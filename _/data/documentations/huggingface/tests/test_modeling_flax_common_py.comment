['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' assumed parallelism: 8','line_number':55,'multiline':False]['text':' make sure that at least one token is attended to for each batch','line_number':97,'multiline':False]['text':' If Both parameters and batch normalization statistics are present','line_number':106,'multiline':False]['text':' Extract only parameters for the specified head prefix (if specified) and add batch statistics','line_number':108,'multiline':False]['text':' Only parameters are present','line_number':116,'multiline':False]['text':' hack for now until we have AutoModel classes','line_number':138,'multiline':False]['text':' (Copied from tests.test_modeling_common.ModelTesterMixin.check_pt_flax_outputs)','line_number':182,'multiline':False]['text':' Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).','line_number':198,'multiline':False]['text':' convert to the case of `tuple`','line_number':210,'multiline':False]['text':' appending each key to the current (string) `name`','line_number':211,'multiline':False]['text':' Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)','line_number':217,'multiline':False]['text':' case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)','line_number':227,'multiline':False]['text':' case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`','line_number':234,'multiline':False]['text':' Using `np.asarray` gives `ValueError: assignment destination is read-only` at the line `fx_outputs[fx_nans] = 0`.','line_number':245,'multiline':False]['text':' deal with NumPy's scalars to make replacing nan values by 0 work.','line_number':253,'multiline':False]['text':' It might be better to put this inside the for loop below (because we modify the config there).','line_number':278,'multiline':False]['text':' But logically, it is fine.','line_number':279,'multiline':False]['text':' Output all for aggressive testing','line_number':284,'multiline':False]['text':' prepare inputs','line_number':288,'multiline':False]['text':' load corresponding PyTorch class','line_number':292,'multiline':False]['text':' Skip the "Flax" at the beginning','line_number':293,'multiline':False]['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':297,'multiline':False]['text':' So we disable `use_cache` here for PyTorch model.','line_number':298,'multiline':False]['text':' send pytorch model to the correct device','line_number':305,'multiline':False]['text':' Output all for aggressive testing','line_number':336,'multiline':False]['text':' prepare inputs','line_number':340,'multiline':False]['text':' load corresponding PyTorch class','line_number':344,'multiline':False]['text':' Skip the "Flax" at the beginning','line_number':345,'multiline':False]['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':349,'multiline':False]['text':' So we disable `use_cache` here for PyTorch model.','line_number':350,'multiline':False]['text':' make sure weights are tied in PyTorch','line_number':356,'multiline':False]['text':' send pytorch model to the correct device','line_number':359,'multiline':False]['text':' send pytorch model to the correct device','line_number':376,'multiline':False]['text':' verify that normal save_pretrained works as expected','line_number':399,'multiline':False]['text':' the config file (and the generation config file, if it can generate) should be saved','line_number':403,'multiline':False]['text':' verify that save_pretrained for distributed training','line_number':415,'multiline':False]['text':' with `params=params` works as expected','line_number':416,'multiline':False]['text':' check that all base model weights are loaded correctly','line_number':436,'multiline':False]['text':' check that all base model weights are loaded correctly','line_number':458,'multiline':False]['text':' convert Flax model to PyTorch model','line_number':481,'multiline':False]['text':' Skip the "Flax" at the beginning','line_number':482,'multiline':False]['text':' check that all base model weights are loaded correctly','line_number':486,'multiline':False]['text':' save pt model','line_number':488,'multiline':False]['text':' convert Flax model to PyTorch model','line_number':510,'multiline':False]['text':' Skip the "Flax" at the beginning','line_number':511,'multiline':False]['text':' check that all base model weights are loaded correctly','line_number':515,'multiline':False]['text':' convert Flax model to PyTorch model','line_number':539,'multiline':False]['text':' Skip the "Flax" at the beginning','line_number':540,'multiline':False]['text':' check that all base model weights are loaded correctly','line_number':544,'multiline':False]['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':584,'multiline':False]['text':' check that output_hidden_states also work using config','line_number':650,'multiline':False]['text':' check that output_attentions also work using config','line_number':677,'multiline':False]['text':' Question Answering model returns start_logits and end_logits','line_number':694,'multiline':False]['text':' start_logits and end_logits instead of only 1 output','line_number':696,'multiline':False]['text':' decoder attentions','line_number':700,'multiline':False]['text':' cross attentions','line_number':709,'multiline':False]['text':' Check attention is always last and order is fine','line_number':722,'multiline':False]['text':' Fails when we don't set ignore_mismatched_sizes=True','line_number':758,'multiline':False]['text':' check if all params are still in float32 when dtype of computation is half-precision','line_number':789,'multiline':False]['text':' cast all params to bf16','line_number':803,'multiline':False]['text':' test if all params are in bf16','line_number':806,'multiline':False]['text':' test masking','line_number':810,'multiline':False]['text':' choose a random param','line_number':812,'multiline':False]['text':' don't cast the key','line_number':813,'multiline':False]['text':' test if all params are in bf16 except key','line_number':818,'multiline':False]['text':' cast all params to fp16','line_number':831,'multiline':False]['text':' test if all params are in fp16','line_number':834,'multiline':False]['text':' test masking','line_number':838,'multiline':False]['text':' choose a random param','line_number':840,'multiline':False]['text':' don't cast the key','line_number':841,'multiline':False]['text':' test if all params are in fp16 except key','line_number':846,'multiline':False]['text':' cast all params to fp16 and back to fp32','line_number':859,'multiline':False]['text':' test if all params are in fp32','line_number':863,'multiline':False]['text':' test masking','line_number':868,'multiline':False]['text':' choose a random param','line_number':870,'multiline':False]['text':' don't cast the key','line_number':871,'multiline':False]['text':' cast to fp16 and back to fp32 with mask','line_number':874,'multiline':False]['text':' test if all params are in fp32 except key','line_number':878,'multiline':False]['text':' convert weights to fp16 and save','line_number':892,'multiline':False]['text':' load the weights again and check if they are still in fp16','line_number':897,'multiline':False]['text':' convert weights to bf16 and save','line_number':909,'multiline':False]['text':' load the weights again and check if they are still in fp16','line_number':914,'multiline':False]['text':' The main input is the name of the argument after `self`','line_number':923,'multiline':False]['text':' Prepare head mask','line_number':946,'multiline':False]['text':' Remove NaN','line_number':956,'multiline':False]['text':' Check we don't have more than 25% nans (arbitrary)','line_number':958,'multiline':False]['text':' encoder-decodere models have only 2 layers in each modules','line_number':964,'multiline':False]['text':' Check that accesing parmas raises an ValueError when _do_init is False','line_number':981,'multiline':False]['text':' Check if we params can be properly initialized when calling init_weights','line_number':985,'multiline':False]['text':' Check if all required parmas are initialized','line_number':988,'multiline':False]['text':' Check if the shapes match','line_number':991,'multiline':False]['text':' Check that setting params raises an ValueError when _do_init is False','line_number':1000,'multiline':False]['text':' Check if we can do a forward pass','line_number':1004,'multiline':False]['text':' Check if all required parmas are loaded','line_number':1014,'multiline':False]['text':' Check if the shapes match','line_number':1017,'multiline':False]['text':' init the model','line_number':1027,'multiline':False]['text':' save the model in the temporary directory','line_number':1030,'multiline':False]['text':' load the saved model with _do_init=False','line_number':1031,'multiline':False]['text':' Check that accesing parmas raises an ValueError when _do_init is False','line_number':1036,'multiline':False]['text':' Check if all required parmas are loaded','line_number':1040,'multiline':False]['text':' Check that setting params raises an ValueError when _do_init is False','line_number':1043,'multiline':False]['text':' Check if init_weights initializes missing keys from from_pretrained','line_number':1047,'multiline':False]['text':' Check if all required parmas are loaded','line_number':1058,'multiline':False]['text':' the model above is the same as the model below, just a sharded version.','line_number':1063,'multiline':False]['text':' We use the same folder for various sizes to make sure a new save erases the old checkpoint.','line_number':1072,'multiline':False]['text':' Get each shard file and its size','line_number':1076,'multiline':False]['text':' Check there is an index but no regular weight file','line_number':1084,'multiline':False]['text':' Check a file is bigger than max_size only when it has a single weight','line_number':1088,'multiline':False]['text':' Note: pickle adds some junk so the weight of the file can end up being slightly bigger than','line_number':1094,'multiline':False]['text':' the size asked for (since we count parameters)','line_number':1095,'multiline':False]['text':' Check the index and the shard files found match','line_number':1101,'multiline':False]['text':' Finally, check the model can be reloaded','line_number':1109,'multiline':False]['text':' prepare inputs','line_number':1126,'multiline':False]['text':' ensure that the dicts of outputs contain the same keys','line_number':1139,'multiline':False]['text':' ensure that the outputs remain precisely equal','line_number':1145,'multiline':False]