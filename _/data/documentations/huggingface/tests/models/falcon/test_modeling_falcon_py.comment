['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' first forward pass','line_number':214,'multiline':False]['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':224,'multiline':False]['text':' append to next input_ids and','line_number':228,'multiline':False]['text':' select random slice','line_number':248,'multiline':False]['text':' test that outputs are equal for slice','line_number':255,'multiline':False]['text':' TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146','line_number':302,'multiline':False]['text':' Falcon can have different numbers of KV-heads than the number of query heads, so we need','line_number':366,'multiline':False]['text':' to override this test to use the right head counts.','line_number':367,'multiline':False]['text':' If it doesn't support cache, pass the test','line_number':371,'multiline':False]['text':' If "past_key_values" is not returned, pass the test (e.g. RWKV uses a different cache name and format)','line_number':380,'multiline':False]['text':' K V for the decoder = 2','line_number':402,'multiline':False]['text':' Fixed seed at init time so the two models get the same random weights','line_number':416,'multiline':False]['text':' Fixed seed at init time so the two models get the same random weights','line_number':423,'multiline':False]['text':' Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original','line_number':431,'multiline':False]['text':' maximum sequence length, so the outputs for the short input should match.','line_number':432,'multiline':False]['text':' The output should be different for long inputs','line_number':438,'multiline':False]['text':' make sure that all models have enough positions for generation','line_number':459,'multiline':False]['text':' NOTE: This check is disabled for Falcon as the non-SDPA/SDPA implementation is in the same class (legacy reason).','line_number':487,'multiline':False]['text':' for name, submodule in model_eager.named_modules():','line_number':488,'multiline':False]['text':'     if "SdpaAttention" in submodule.__class__.__name__:','line_number':489,'multiline':False]['text':'         raise ValueError("The eager model should not have SDPA attention layers")','line_number':490,'multiline':False]['text':' has_sdpa = False','line_number':492,'multiline':False]['text':' for name, submodule in model_sdpa.named_modules():','line_number':493,'multiline':False]['text':'     if "SdpaAttention" in submodule.__class__.__name__:','line_number':494,'multiline':False]['text':'         has_sdpa = True','line_number':495,'multiline':False]['text':'         break','line_number':496,'multiline':False]['text':' if not has_sdpa:','line_number':497,'multiline':False]['text':'     raise ValueError("The SDPA model should have SDPA attention layers")','line_number':498,'multiline':False]['text':' Just test that a large cache works as expected','line_number':500,'multiline':False]['text':' The big models are way too big for the CI, so we use tiny random models that resemble their','line_number':533,'multiline':False]['text':' architectures but with much smaller and fewer layers','line_number':534,'multiline':False]['text':' We just test that these run without errors - the models are randomly initialized','line_number':542,'multiline':False]['text':' and so the actual text outputs will be garbage','line_number':543,'multiline':False]['text':' The big models are way too big for the CI, so we use tiny random models that resemble their','line_number':550,'multiline':False]['text':' architectures but with much smaller and fewer layers','line_number':551,'multiline':False]['text':' Test results are the same with and without cache','line_number':564,'multiline':False]['text':' should generate the rest of the sequence','line_number':580,'multiline':False]['text':' forces left-padding on `test_text`','line_number':586,'multiline':False]['text':' left-padding exists','line_number':592,'multiline':False]