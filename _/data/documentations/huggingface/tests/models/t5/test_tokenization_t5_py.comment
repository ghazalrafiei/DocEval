['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2018 Google T5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' We have a SentencePiece fixture for testing','line_number':49,'multiline':False]['text':' check if input_ids are returned and no decoder_input_ids','line_number':202,'multiline':False]['text':' Since T5 does NOT have a max input length,','line_number':226,'multiline':False]['text':' this test should be changed to the following in Transformers v5:','line_number':227,'multiline':False]['text':' self.assertEqual(batch.input_ids.shape, (2, 8001))','line_number':228,'multiline':False]['text':' the following checks allow us to verify that our test works as expected, i.e. that the tokenizer takes','line_number':331,'multiline':False]['text':' into account the new value of additional_special_tokens given in the "tokenizer_config.json" and','line_number':332,'multiline':False]['text':' "special_tokens_map.json" files','line_number':333,'multiline':False]['text':' self.assertIn("an_additional_special_token",tokenizer_without_change_in_init.get_vocab()) # ByT5Tokenization no vocab','line_number':340,'multiline':False]['text':' Now we test that we can change the value of additional_special_tokens in the from_pretrained','line_number':348,'multiline':False]['text':' overwritten from `test_tokenization_common` since T5 has no max length','line_number':363,'multiline':False]['text':' We should have at least one default checkpoint for each tokenizer','line_number':365,'multiline':False]['text':' We should specify the max input length as well (used in some part to list the pretrained checkpoints)','line_number':366,'multiline':False]['text':' fmt: skip','line_number':372,'multiline':False]['text':' TODO ArthurZ the above is necessary as addedTokens / intialization sucks. Trie is not correctly created','line_number':441,'multiline':False]['text':' So the extra ids are split....','line_number':442,'multiline':False]['text':' make sure `'▁'` is prepended, and outputs match sp_model's','line_number':446,'multiline':False]['text':' `sentencepiece.NormalizerSpec.add_dummy_prefix` attribute','line_number':447,'multiline':False]['text':' make sure the extra spaces are eaten','line_number':468,'multiline':False]['text':' sentencepiece.NormalizerSpec.remove_extra_whitespaces attribute','line_number':469,'multiline':False]['text':' `'▁'` is also a whitespace','line_number':477,'multiline':False]['text':' no extra space added','line_number':481,'multiline':False]['text':' here t5x does not eat with lstrip, so there is and extra ▁He in the original one','line_number':484,'multiline':False]['text':' spaces are eaten by spm','line_number':487,'multiline':False]['text':' make sure that the output after the extra id is the same as if','line_number':488,'multiline':False]['text':' extra_id was not there','line_number':489,'multiline':False]['text':' spaces are eaten by spm even if not start','line_number':493,'multiline':False]['text':' Make sure that `tokenizer.tokenize` is similar to','line_number':496,'multiline':False]['text':' adding the equivalent special token to the vocab','line_number':497,'multiline':False]['text':' spaces are not longer eaten by rstrip and lstrip','line_number':512,'multiline':False]['text':' test with a begin of word like `▁He`','line_number':515,'multiline':False]['text':' spaces are eaten by rstrip / lstrip, so this is expected. Don't strip otherwise you break','line_number':518,'multiline':False]['text':' Make sure this does not happen if we don't strip','line_number':522,'multiline':False]['text':' the first `' '` after `'No'` is eaten by spm:','line_number':528,'multiline':False]['text':' TODO @ArthurZucker fix the 3 commented tests with #23909','line_number':543,'multiline':False]['text':' "Bonjour<extra_id_0>.",  # this will fail. In T5 the special token has to be at the end.','line_number':546,'multiline':False]['text':' because in T5 they add `_<extra_id_0>` to the vocab, not `<extra_id_0>`.','line_number':547,'multiline':False]['text':' "Hey <extra_id_0> I love you", # this will fail, we strip left, to _I vs I','line_number':549,'multiline':False]['text':' "Hey <extra_id_0>▁He", # this will fail for the same reason, we replace `_` then strip','line_number':550,'multiline':False]['text':' Test with umt5','line_number':555,'multiline':False]['text':' Test with T5','line_number':571,'multiline':False]