['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' let's use a vocab size that's way bigger than BERT's one','line_number':52,'multiline':False]['text':' NOTE: this is not a model parameter, just an input','line_number':53,'multiline':False]['text':' we set has_text_modality to False as the config has no vocab_size attribute','line_number':245,'multiline':False]['text':' expected_num_layers equals num_hidden_layers of the deep encoder + 1, + 2 for the first shallow encoder, + 2','line_number':281,'multiline':False]['text':' for the final shallow encoder','line_number':282,'multiline':False]['text':' the expected length of the hidden_states of the first and final shallow encoders','line_number':289,'multiline':False]['text':' is equal to the seq_length','line_number':290,'multiline':False]['text':' the expected length of the hidden_states of the deep encoder need to be updated','line_number':296,'multiline':False]['text':' for CANINE since the seq length is downsampled','line_number':297,'multiline':False]['text':' check that output_hidden_states also work using config','line_number':309,'multiline':False]['text':' we add + 2 due to the 2 shallow encoders','line_number':331,'multiline':False]['text':' check that output_attentions also work using config','line_number':334,'multiline':False]['text':' we add + 2 due to the 2 shallow encoders','line_number':343,'multiline':False]['text':' Check attention is always last and order is fine','line_number':352,'multiline':False]['text':' To be sure we have no Nan','line_number':454,'multiline':False]['text':' Prepare head_mask','line_number':460,'multiline':False]['text':' Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior)','line_number':461,'multiline':False]['text':' Test that we can get a gradient back for importance score computation','line_number':475,'multiline':False]['text':' Remove Nan','line_number':485,'multiline':False]['text':' Check we don't have more than 25% nans (arbitrary)','line_number':489,'multiline':False]['text':' remove them (the test is less complete)','line_number':492,'multiline':False]['text':' ViT does not use inputs_embeds','line_number':503,'multiline':False]['text':' this one corresponds to the first example of the TydiQA dev set (in Swahili)','line_number':540,'multiline':False]['text':' fmt: off','line_number':541,'multiline':False]['text':' fmt: on','line_number':545,'multiline':False]['text':' verify sequence output','line_number':551,'multiline':False]['text':' verify pooled output','line_number':565,'multiline':False]