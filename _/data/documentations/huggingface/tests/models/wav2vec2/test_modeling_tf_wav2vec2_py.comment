['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' use a spawn pool, which should trigger a warning if different than fork','line_number':93,'multiline':False]['text':' force batch_decode to internally create a spawn pool, which should trigger a warning if different than fork','line_number':100,'multiline':False]['text':' this is most likely not correctly set yet','line_number':135,'multiline':False]['text':' test does not pass for models making use of `group_norm`','line_number':211,'multiline':False]['text':' check: https://github.com/pytorch/fairseq/issues/3227','line_number':212,'multiline':False]['text':' convert values that are over input_lengths to padding','line_number':222,'multiline':False]['text':' convert values that are over input_lengths to padding','line_number':247,'multiline':False]['text':' pad input','line_number':268,'multiline':False]['text':' freeze feature encoder','line_number':285,'multiline':False]['text':' overwrite because input_values != input_ids','line_number':340,'multiline':False]['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':347,'multiline':False]['text':' overwrite because input_values != input_ids','line_number':353,'multiline':False]['text':' TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC','line_number':433,'multiline':False]['text':' TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC','line_number':438,'multiline':False]['text':' We override the base test here to skip loss calculation for Wav2Vec2 models because the loss is massive with','line_number':443,'multiline':False]['text':' the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT','line_number':444,'multiline':False]['text':' Output all for aggressive testing','line_number':452,'multiline':False]['text':' Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency','line_number':456,'multiline':False]['text':' of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.','line_number':457,'multiline':False]['text':' TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.','line_number':458,'multiline':False]['text':' Skip the "TF" at the beginning','line_number':461,'multiline':False]['text':' Check we can load pt model in tf and vice-versa with model => model functions','line_number':469,'multiline':False]['text':' Original test: check without `labels`','line_number':477,'multiline':False]['text':' Check we can load pt model in tf and vice-versa with checkpoint => model functions','line_number':480,'multiline':False]['text':' Original test: check without `labels`','line_number':494,'multiline':False]['text':' overwrite because input_values != input_ids','line_number':517,'multiline':False]['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':524,'multiline':False]['text':' overwrite because input_values != input_ids','line_number':530,'multiline':False]['text':' TODO (Joao): fix me','line_number':589,'multiline':False]['text':' TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC','line_number':618,'multiline':False]['text':' TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC','line_number':623,'multiline':False]['text':' We override the base test here to skip loss calculation for Wav2Vec2 models because the loss is massive with','line_number':628,'multiline':False]['text':' the default labels and frequently overflows to inf or exceeds numerical tolerances between TF/PT','line_number':629,'multiline':False]['text':' Output all for aggressive testing','line_number':637,'multiline':False]['text':' Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency','line_number':641,'multiline':False]['text':' of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.','line_number':642,'multiline':False]['text':' TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.','line_number':643,'multiline':False]['text':' Skip the "TF" at the beginning','line_number':646,'multiline':False]['text':' Check we can load pt model in tf and vice-versa with model => model functions','line_number':654,'multiline':False]['text':' Original test: check without `labels`','line_number':662,'multiline':False]['text':' Check we can load pt model in tf and vice-versa with checkpoint => model functions','line_number':665,'multiline':False]['text':' Original test: check without `labels`','line_number':679,'multiline':False]['text':' because of overlap mask don't have to add up exactly to `mask_prob * sequence_length`, but have to be smaller or equal','line_number':705,'multiline':False]['text':' clean-up as much as possible GPU memory occupied by PyTorch','line_number':715,'multiline':False]['text':' automatic decoding with librispeech','line_number':720,'multiline':False]['text':' test user-managed pool','line_number':823,'multiline':False]['text':' user-managed pool + num_processes should trigger a warning','line_number':829,'multiline':False]['text':' s3prl logits for the same batch','line_number':928,'multiline':False]