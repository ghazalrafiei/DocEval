['text':' input_mask = tf.reshape(input_mask, [-1, MAX_SEQ_LEN])','line_number':35,'multiline':False]['text':' The TF tokenizers are usually going to be used as pretrained tokenizers from existing model checkpoints,','line_number':45,'multiline':False]['text':' so that's what we focus on here.','line_number':46,'multiline':False]['text':' convert them to numpy to avoid messing with ragged tensors','line_number':72,'multiline':False]['text':' Build model with some sample inputs','line_number':96,'multiline':False]['text':' We may see small differences because the loaded model is compiled, so we need an epsilon for the test','line_number':102,'multiline':False]['text':' Build model with some sample inputs','line_number':109,'multiline':False]['text':' for the test to run','line_number':121,'multiline':False]