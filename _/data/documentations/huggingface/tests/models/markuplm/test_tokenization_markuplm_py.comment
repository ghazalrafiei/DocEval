['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt','line_number':53,'multiline':False]['text':' fmt: skip','line_number':54,'multiline':False]['text':' , "/html/body/div/li[1]/div/span"]','line_number':89,'multiline':False]['text':' We usually have added tokens from the start in tests because our vocab fixtures are','line_number':133,'multiline':False]['text':' smaller than the original vocabs - let's not assert this','line_number':134,'multiline':False]['text':' self.assertEqual(vocab_size, all_size)','line_number':135,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':215,'multiline':False]['text':' Test 'longest' and 'no_padding' don't do anything','line_number':226,'multiline':False]['text':' Test right padding','line_number':259,'multiline':False]['text':' Test left padding','line_number':278,'multiline':False]['text':' test 1: single sequence','line_number':347,'multiline':False]['text':' Method is implemented (e.g. not GPT-2)','line_number':353,'multiline':False]['text':' test 2: two sequences','line_number':359,'multiline':False]['text':' Method is implemented (e.g. not GPT-2)','line_number':365,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':379,'multiline':False]['text':' Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':384,'multiline':False]['text':' FIXME: the next line should be padding(max_length) to avoid warning','line_number':388,'multiline':False]['text':' Check that nothing is done when a maximum length is not specified','line_number':396,'multiline':False]['text':' Encode - Simple input','line_number':415,'multiline':False]['text':' Encode - Pair input','line_number':428,'multiline':False]['text':' Encode_plus - Simple input','line_number':448,'multiline':False]['text':' Encode_plus - Pair input','line_number':467,'multiline':False]['text':' Batch_encode_plus - Simple input','line_number':492,'multiline':False]['text':' Batch_encode_plus - Pair input','line_number':541,'multiline':False]['text':' Using pad on single examples after tokenization','line_number':576,'multiline':False]['text':' Using pad on single examples after tokenization','line_number':588,'multiline':False]['text':' Using pad after tokenization','line_number':597,'multiline':False]['text':' Using pad after tokenization','line_number':607,'multiline':False]['text':' Tests that all call wrap to encode_plus and batch_encode_plus','line_number':618,'multiline':False]['text':' Test not batched','line_number':622,'multiline':False]['text':' Test not batched pairs','line_number':628,'multiline':False]['text':' Test batched','line_number':634,'multiline':False]['text':' Tests that all encoded values have the correct size','line_number':641,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':662,'multiline':False]['text':' check 'longest' is unsensitive to a max length','line_number':680,'multiline':False]['text':' check 'no_padding' is unsensitive to a max length','line_number':693,'multiline':False]['text':' Test that padded sequences are equivalent between batch_encode_plus and encode_plus','line_number':711,'multiline':False]['text':' Right padding tests','line_number':713,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':721,'multiline':False]['text':' Left padding tests','line_number':737,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':746,'multiline':False]['text':' empty_tokens = tokenizer([""], [[]], padding=True, pad_to_multiple_of=8)','line_number':771,'multiline':False]['text':' for key, value in empty_tokens.items():','line_number':773,'multiline':False]['text':'     self.assertEqual(len(value) % 8, 0, f"BatchEncoding.{key} is not multiple of 8")','line_number':774,'multiline':False]['text':' Should also work with truncation','line_number':782,'multiline':False]['text':' truncation to something which is not a multiple of pad_to_multiple_of raises an error','line_number':789,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':811,'multiline':False]['text':' Input tokens id','line_number':819,'multiline':False]['text':' Generate output','line_number':824,'multiline':False]['text':' Generate pair output','line_number':829,'multiline':False]['text':' add_prefix_space=False,','line_number':845,'multiline':False]['text':' Testing single inputs','line_number':862,'multiline':False]['text':' safety check on max_len default value so we are sure the test works','line_number':875,'multiline':False]['text':' Now let's start the test','line_number':881,'multiline':False]['text':' Isolate this from the other tests because we save additional tokens/etc','line_number':885,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':909,'multiline':False]['text':' RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':914,'multiline':False]['text':' LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':925,'multiline':False]['text':' RIGHT & LEFT PADDING - Check that nothing is done for 'longest' and 'no_padding'','line_number':936,'multiline':False]['text':' test 1: single sequence','line_number':968,'multiline':False]['text':' Assert that the token type IDs have the same length as the input IDs','line_number':973,'multiline':False]['text':' Assert that the token type IDs have the same length as the attention mask','line_number':976,'multiline':False]['text':' test 2: two sequences (question + nodes)','line_number':982,'multiline':False]['text':' Assert that the token type IDs have the same length as the input IDs','line_number':987,'multiline':False]['text':' Assert that the token type IDs have the same length as the attention mask','line_number':990,'multiline':False]['text':' No pair','line_number':1003,'multiline':False]['text':' Assert there is the same number of tokens and offsets','line_number':1014,'multiline':False]['text':' Assert there is online added_tokens special_tokens','line_number':1017,'multiline':False]['text':' Pairs','line_number':1020,'multiline':False]['text':' Assert there is the same number of tokens and offsets','line_number':1035,'multiline':False]['text':' Assert there is online added_tokens special_tokens','line_number':1038,'multiline':False]['text':' Make sure the model contains at least the full vocabulary size in its embedding matrix','line_number':1064,'multiline':False]['text':' Build sequence','line_number':1072,'multiline':False]['text':' This should not fail','line_number':1078,'multiline':False]['text':' saves some time','line_number':1080,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1089,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1107,'multiline':False]['text':' Ensure basic input match','line_number':1117,'multiline':False]['text':' Ensure truncation match','line_number':1141,'multiline':False]['text':' Ensure truncation with stride match','line_number':1152,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1169,'multiline':False]['text':' tokenize()','line_number':1198,'multiline':False]['text':' encode()','line_number':1203,'multiline':False]['text':' encode_plus()','line_number':1208,'multiline':False]['text':' # batch_encode_plus','line_number':1217,'multiline':False]['text':' Ensure that the input IDs are less than the max length defined.','line_number':1235,'multiline':False]['text':' Ensure that the input IDs are still truncated when no max_length is specified','line_number':1242,'multiline':False]['text':' A Tensor cannot be build by sequences which are not the same size','line_number':1253,'multiline':False]['text':' We want to have sequence 0 and sequence 1 are tagged','line_number':1300,'multiline':False]['text':' respectively with 0 and 1 token_ids','line_number':1301,'multiline':False]['text':' (regardless of whether the model use token type ids)','line_number':1302,'multiline':False]['text':' We use this assumption in the QA pipeline among other place','line_number':1303,'multiline':False]['text':' TODO this is only possible for slow currently','line_number':1350,'multiline':False]['text':' This feature only exists for fast tokenizers','line_number':1363,'multiline':False]['text':' Test we can use the new tokenizer with something not seen during training','line_number':1370,'multiline':False]['text':' original expected result "this is the" seems contradicts to roberta-based tokenizer','line_number':1376,'multiline':False]['text':' We check that the parameters of the tokenizer remained the same','line_number':1384,'multiline':False]['text':' Check we have the same number of added_tokens for both pair and non-pair inputs.','line_number':1385,'multiline':False]['text':' Check we have the correct max_length for both pair and non-pair inputs.','line_number':1389,'multiline':False]['text':' Assert the set of special tokens match as we didn't ask to change them','line_number':1393,'multiline':False]['text':' This feature only exists for fast tokenizers','line_number':1402,'multiline':False]['text':' Test with a special tokens map','line_number':1407,'multiline':False]['text':' Create a new mapping from the special tokens defined in the original tokenizer','line_number':1417,'multiline':False]['text':' Get the private one to avoid unnecessary warnings.','line_number':1422,'multiline':False]['text':' Train new tokenizer','line_number':1427,'multiline':False]['text':' Check the changes','line_number':1432,'multiline':False]['text':' Get the private one to avoid unnecessary warnings.','line_number':1434,'multiline':False]['text':' Check if the AddedToken / string format has been kept','line_number':1445,'multiline':False]['text':' The special token must appear identically in the list of the new tokenizer.','line_number':1448,'multiline':False]['text':' The special token must appear in the list of the new tokenizer as an object of type AddedToken with','line_number':1454,'multiline':False]['text':' the same parameters as the old AddedToken except the content that the user has requested to change.','line_number':1455,'multiline':False]['text':' The special token must appear identically in the list of the new tokenizer.','line_number':1478,'multiline':False]['text':' The special token must appear in the list of the new tokenizer as an object of type string.','line_number':1485,'multiline':False]['text':' Test we can use the new tokenizer with something not seen during training','line_number':1488,'multiline':False]['text':' same as line 1399','line_number':1494,'multiline':False]['text':' only test prepare_for_model for the slow tokenizer','line_number':1503,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1516,'multiline':False]['text':' rename encoded batch to "inputs"','line_number':1531,'multiline':False]['text':' Renaming `input_ids` to `inputs`','line_number':1538,'multiline':False]['text':' Single example','line_number':1572,'multiline':False]['text':' Batch of examples','line_number':1590,'multiline':False]['text':' For these 2 examples, 3 training examples will be created','line_number':1591,'multiline':False]['text':' toks_str = [t[1] for t in toks]','line_number':1630,'multiline':False]['text':' Ensure consistency','line_number':1633,'multiline':False]['text':' an extra blank will cause inconsistency: ["a","b",] & "a b"','line_number':1635,'multiline':False]['text':' slow part fixed, fast part not','line_number':1652,'multiline':False]['text':' Build a sequence from our model's vocabulary','line_number':1656,'multiline':False]['text':' We are not using the special tokens - a bit too hard to test all the tokenizers with this','line_number':1685,'multiline':False]['text':' TODO try this again later','line_number':1686,'multiline':False]['text':' Test with max model input length','line_number':1689,'multiline':False]['text':' assertgreater -> assertgreaterequal','line_number':1695,'multiline':False]['text':' Simple','line_number':1707,'multiline':False]['text':' Simple','line_number':1737,'multiline':False]['text':' Simple with no truncation','line_number':1752,'multiline':False]['text':' Reset warnings','line_number':1753,'multiline':False]['text':' Check the order of Sequence of input ids, overflowing tokens and xpath_tags_seq sequence with truncation','line_number':1785,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1847,'multiline':False]['text':' add_prefix_space=False,','line_number':1858,'multiline':False]['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':1878,'multiline':False]['text':' add_prefix_space=False,','line_number':1889,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1900,'multiline':False]['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':1928,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1959,'multiline':False]['text':' ISSUE HAPPENS HERE ↓','line_number':1973,'multiline':False]['text':' add_prefix_space=False,','line_number':1998,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2000,'multiline':False]['text':' Test with max model input length','line_number':2041,'multiline':False]['text':' Simple','line_number':2052,'multiline':False]['text':' Simple with no truncation','line_number':2080,'multiline':False]['text':' Reset warnings','line_number':2081,'multiline':False]['text':' Check the order of Sequence of input ids, overflowing tokens, xpath_tags_seq and xpath_subs_seq sequence with truncation','line_number':2109,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2121,'multiline':False]['text':' test slow tokenizer','line_number':2168,'multiline':False]['text':' test fast tokenizer','line_number':2177,'multiline':False]['text':' There are 3 cases:','line_number':2190,'multiline':False]['text':' CASE 1: document image classification (training + inference), document image token classification (inference),','line_number':2191,'multiline':False]['text':' in which case only nodes and normalized bounding xpaths are provided to the tokenizer','line_number':2192,'multiline':False]['text':' CASE 2: document image token classification (training),','line_number':2193,'multiline':False]['text':' in which case one also provides word labels to the tokenizer','line_number':2194,'multiline':False]['text':' CASE 3: document image visual question answering (inference),','line_number':2195,'multiline':False]['text':' in which case one also provides a question to the tokenizer','line_number':2196,'multiline':False]['text':' We need to test all 3 cases both on batched and non-batched inputs.','line_number':2198,'multiline':False]['text':' CASE 1: not batched','line_number':2200,'multiline':False]['text':' fmt: skip','line_number':2203,'multiline':False]['text':' CASE 1: batched','line_number':2210,'multiline':False]['text':' fmt: skip','line_number':2213,'multiline':False]['text':' CASE 2: not batched','line_number':2220,'multiline':False]['text':' fmt: skip','line_number':2224,'multiline':False]['text':' CASE 2: batched','line_number':2231,'multiline':False]['text':' fmt: skip','line_number':2235,'multiline':False]['text':' CASE 3: not batched','line_number':2242,'multiline':False]['text':' fmt: skip','line_number':2245,'multiline':False]['text':' CASE 3: batched','line_number':2252,'multiline':False]['text':' fmt: skip','line_number':2255,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2273,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2291,'multiline':False]['text':' We want to assert there are no warnings, but the 'assertLogs' method does not support that.','line_number':2297,'multiline':False]['text':' Therefore, we are adding a dummy warning, and then we will assert it is the only warning.','line_number':2298,'multiline':False]