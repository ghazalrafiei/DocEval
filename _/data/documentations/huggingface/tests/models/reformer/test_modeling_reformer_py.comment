['text':' coding=utf-8 # Copyright 2020 Huggingface','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' 2 * hidden_size because we use reversible resnet layers','line_number':206,'multiline':False]['text':' no special position embeddings','line_number':243,'multiline':False]['text':' need to set chunk length equal sequence length to be certain that chunking works','line_number':248,'multiline':False]['text':' set all position encodings to zero so that postions don't matter','line_number':254,'multiline':False]['text':' normal padded','line_number':265,'multiline':False]['text':' shifted padded','line_number':275,'multiline':False]['text':' Batch x SeqLen x hiddenSize','line_number':298,'multiline':False]['text':' get random tensors','line_number':300,'multiline':False]['text':' now the random seeds for attention and feed forward is initialized','line_number':304,'multiline':False]['text':' forward tensors with dropout','line_number':305,'multiline':False]['text':' disable dropout','line_number':335,'multiline':False]['text':' Batch x SeqLen x hiddenSize','line_number':383,'multiline':False]['text':' only use last 10 inputs for generation','line_number':434,'multiline':False]['text':' force chunk length to be bigger than input_ids','line_number':439,'multiline':False]['text':' return saved cache','line_number':473,'multiline':False]['text':' calculate last output with and without cache','line_number':476,'multiline':False]['text':' select random slice idx','line_number':480,'multiline':False]['text':' outputs should be similar within range','line_number':483,'multiline':False]['text':' reformer cannot keep gradients in attentions or hidden states','line_number':592,'multiline':False]['text':' reformer cannot resize embeddings that easily','line_number':596,'multiline':False]['text':' check attn size','line_number':655,'multiline':False]['text':' check hidden size','line_number':680,'multiline':False]['text':' and it's not used enough to be worth fixing :)','line_number':686,'multiline':False]['text':' TODO: Fix the failed tests','line_number':717,'multiline':False]['text':' `QAPipelineTests` fails for a few models when the slower tokenizer are used.','line_number':726,'multiline':False]['text':' (The slower tokenizers were never used for pipeline tests before the pipeline testing rework)','line_number':727,'multiline':False]['text':' TODO: check (and possibly fix) the `QAPipelineTests` with slower tokenizer','line_number':728,'multiline':False]['text':' sanotheu','line_number':764,'multiline':False]['text':' attn_layers=[lsh,lsh,lsh,lsh],','line_number':765,'multiline':False]['text':' check attn size','line_number':808,'multiline':False]['text':' check hidden size','line_number':833,'multiline':False]['text':' and it's not used enough to be worth fixing :)','line_number':847,'multiline':False]['text':' check last grads to cover all proable errors','line_number':1214,'multiline':False]['text':' check last grads to cover all proable errors','line_number':1253,'multiline':False]