['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Make sure `loss` exist','line_number':250,'multiline':False]['text':' make the decoder inputs a different shape from the encoder inputs to harden the test','line_number':305,'multiline':False]['text':' Similar to `check_encoder_decoder_model_output_attentions`, but with `output_attentions` triggered from the','line_number':333,'multiline':False]['text':' config file. Contrarily to most models, changing the model's config won't work -- the defaults are loaded','line_number':334,'multiline':False]['text':' from the inner models' configurations.','line_number':335,'multiline':False]['text':' model config -> won't work','line_number':341,'multiline':False]['text':' inner model config -> will work','line_number':356,'multiline':False]['text':' Generate until max length','line_number':375,'multiline':False]['text':' Bert does not have a bos token id, so use pad_token_id instead','line_number':381,'multiline':False]['text':' Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).','line_number':403,'multiline':False]['text':' convert to the case of `tuple`','line_number':415,'multiline':False]['text':' appending each key to the current (string) `names`','line_number':416,'multiline':False]['text':' Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)','line_number':422,'multiline':False]['text':' case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)','line_number':428,'multiline':False]['text':' case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `names`','line_number':435,'multiline':False]['text':' deal with NumPy's scalars to make replacing nan values by 0 work.','line_number':453,'multiline':False]['text':' other general float inputs','line_number':485,'multiline':False]['text':' send pytorch inputs to the correct device','line_number':496,'multiline':False]['text':' send pytorch model to the correct device','line_number':501,'multiline':False]['text':' Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences','line_number':504,'multiline':False]['text':' tf models returned loss is usually a tensor rather than a scalar.','line_number':511,'multiline':False]['text':' (see `hf_compute_loss`: it uses `tf.keras.losses.Reduction.NONE`)','line_number':512,'multiline':False]['text':' Change it here to a scalar to match PyTorch models' loss','line_number':513,'multiline':False]['text':' PT -> TF','line_number':525,'multiline':False]['text':' Output all for aggressive testing','line_number':536,'multiline':False]['text':' All models tested in this file have attentions','line_number':538,'multiline':False]['text':' Output all for aggressive testing','line_number':553,'multiline':False]['text':' TODO: A generalizable way to determine this attribute','line_number':555,'multiline':False]['text':' Make sure model is built before saving','line_number':559,'multiline':False]['text':' TODO Matt: PT doesn't support loading TF safetensors - remove the arg and from_tf=True when it does','line_number':563,'multiline':False]['text':' Keep only common arguments','line_number':614,'multiline':False]['text':' Output all for aggressive testing','line_number':629,'multiline':False]['text':' All models tested in this file have attentions','line_number':632,'multiline':False]['text':' `encoder_hidden_states` is not used in model call/forward','line_number':637,'multiline':False]['text':' Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency','line_number':640,'multiline':False]['text':' of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.','line_number':641,'multiline':False]['text':' Make sure no all 0s attention masks - to avoid failure at this moment.','line_number':645,'multiline':False]['text':' Put `1` at the beginning of sequences to make it still work when combining causal attention masks.','line_number':646,'multiline':False]['text':' TODO: remove this line once a fix regarding large negative values for attention mask is done.','line_number':647,'multiline':False]['text':' Original test: check without `labels` and  without `enc_to_dec_proj` projection','line_number':658,'multiline':False]['text':' check with `labels`','line_number':663,'multiline':False]['text':' check `enc_to_dec_proj` work as expected','line_number':667,'multiline':False]['text':' make sure that cross attention layers are added','line_number':743,'multiline':False]['text':'  disable cache for now','line_number':745,'multiline':False]['text':' workaround to load from pt','line_number':774,'multiline':False]['text':' Test with the TF checkpoint','line_number':792,'multiline':False]['text':' make sure that cross attention layers are added','line_number':843,'multiline':False]['text':' disable cache for now','line_number':845,'multiline':False]['text':' workaround to load from pt','line_number':875,'multiline':False]['text':' make sure that cross attention layers are added','line_number':935,'multiline':False]['text':'  disable cache for now','line_number':937,'multiline':False]['text':' make sure that cross attention layers are added','line_number':996,'multiline':False]['text':'  disable cache for now','line_number':998,'multiline':False]['text':' # This should be enabled once we upload the TF version of','line_number':1049,'multiline':False]['text':' # "patrickvonplaten/bert2bert-cnn_dailymail-fp16" to the Hub.','line_number':1050,'multiline':False]['text':' model = self.get_encoderdecoder_model()','line_number':1051,'multiline':False]['text':' self._check_configuration_tie(model)','line_number':1052,'multiline':False]['text':' create two random BERT models for bert2bert & initialize weights (+cross_attention weights)','line_number':1072,'multiline':False]['text':' create two random BERT models for bert2bert & initialize weights (+cross_attention weights)','line_number':1115,'multiline':False]['text':' PyTorch => TensorFlow','line_number':1129,'multiline':False]['text':' Make sure `from_pretrained` following `save_pretrained` work and give the same result','line_number':1140,'multiline':False]['text':' TensorFlow => PyTorch','line_number':1150,'multiline':False]['text':' Since most of HF's models don't have pretrained cross-attention layers, they are randomly','line_number':1170,'multiline':False]['text':' initialized even if we create models using `from_pretrained` method.','line_number':1171,'multiline':False]['text':' For the tests, the decoder need to be a model with pretrained cross-attention layers.','line_number':1172,'multiline':False]['text':' So we create pretrained models (without `load_weight_prefix`), save them, and later,','line_number':1173,'multiline':False]['text':' we load them using `from_pretrained`.','line_number':1174,'multiline':False]['text':' (we don't need to do this for encoder, but let's make the code more similar between encoder/decoder)','line_number':1175,'multiline':False]['text':' It's necessary to specify `add_cross_attention=True` here.','line_number':1177,'multiline':False]['text':' check that the from pretrained methods work','line_number':1192,'multiline':False]['text':' Create the model using `__init__` with loaded ``pretrained`` encoder / decoder','line_number':1201,'multiline':False]