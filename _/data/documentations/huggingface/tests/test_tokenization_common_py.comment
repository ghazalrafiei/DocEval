['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2019 HuggingFace Inc.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' the list of different pairs of tokens_list','line_number':163,'multiline':False]['text':' check of sampling is done','line_number':166,'multiline':False]['text':' check if converting back to original text works','line_number':173,'multiline':False]['text':' set to True to test a sentencepiece tokenizer','line_number':192,'multiline':False]['text':' set to True to ignore casing when testing a sentencepiece tokenizer','line_number':195,'multiline':False]['text':' test_sentencepiece must also be set to True','line_number':196,'multiline':False]['text':' Tokenizer.filter makes it possible to filter which Tokenizer to case based on all the','line_number':200,'multiline':False]['text':' information available in Tokenizer (name, rust class, python class, vocab key name)','line_number':201,'multiline':False]['text':' Let's just test the first pretrained vocab for speed','line_number':215,'multiline':False]['text':' the length of the tokenizer does not always represent the tokens that it can encode: what if there are holes?','line_number':231,'multiline':False]['text':' toks_str = [t[1] for t in toks]','line_number':242,'multiline':False]['text':' Ensure consistency','line_number':245,'multiline':False]['text':' to pin the tokenizer version','line_number':330,'multiline':False]['text':' Ensure we match max_length','line_number':347,'multiline':False]['text':' Ensure the number of padded tokens is the same','line_number':351,'multiline':False]['text':' Switch from batch_encode_plus format:   {'input_ids': [[...], [...]], ...}','line_number':384,'multiline':False]['text':' to the list of examples/ encode_plus format: [{'input_ids': [...], ...}, {'input_ids': [...], ...}]','line_number':385,'multiline':False]['text':' TODO: this test can be combined with `test_sentencepiece_tokenize_and_convert_tokens_to_string` after the latter is extended to all tokenizers.','line_number':391,'multiline':False]['text':' Both methods should add the token to `_additional_special_tokens` and `added_tokens_decoder`','line_number':400,'multiline':False]['text':' next is failing for almost all the Fast tokenizers now.','line_number':412,'multiline':False]['text':' self.assertEqual(token_2[0], SPECIAL_TOKEN_2)','line_number':413,'multiline':False]['text':' TODO: this test could be extended to all tokenizers - not just the sentencepiece','line_number':415,'multiline':False]['text':' check if converting back to original text works','line_number':431,'multiline':False]['text':' Subword regularization is only available for the slow tokenizer.','line_number':470,'multiline':False]['text':' Subword regularization is only available for the slow tokenizer.','line_number':489,'multiline':False]['text':' We want to verify that we will be able to save the tokenizer even if the original files that were used to','line_number':509,'multiline':False]['text':' build the tokenizer have been deleted in the meantime.','line_number':510,'multiline':False]['text':' nlp models','line_number':535,'multiline':False]['text':' speech models','line_number':536,'multiline':False]['text':' first name of model_input_names has to correspond to main model input name','line_number':541,'multiline':False]['text':' to make sure `tokenizer.pad(...)` works correctly','line_number':542,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':585,'multiline':False]['text':' We don't have an exact equivalence on `tokenize()` between Rust and Slow','line_number':593,'multiline':False]['text':' Slow tokenizer only split tokens, Rust tokenizers will replace with <unk>','line_number':594,'multiline':False]['text':' tokens = tokenizer.tokenize(sequence)','line_number':595,'multiline':False]['text':' rust_tokens = rust_tokenizer.tokenize(sequence)','line_number':596,'multiline':False]['text':' self.assertListEqual(tokens, rust_tokens)','line_number':597,'multiline':False]['text':' safety check on max_len default value so we are sure the test works','line_number':697,'multiline':False]['text':' Now let's start the test','line_number':703,'multiline':False]['text':' Isolate this from the other tests because we save additional tokens/etc','line_number':707,'multiline':False]['text':' Isolate this from the other tests because we save additional tokens/etc','line_number':726,'multiline':False]['text':' Test that we can also use the non-legacy saving format for fast tokenizers','line_number':756,'multiline':False]['text':' Isolate this from the other tests because we save additional tokens/etc','line_number':762,'multiline':False]['text':' toks before adding new_toks','line_number':831,'multiline':False]['text':' Rust tokenizers dont't lowercase added tokens at the time calling `tokenizer.add_tokens`,','line_number':839,'multiline':False]['text':' while python tokenizers do, so new_toks 0 and 2 would be treated as the same, so do new_toks 1 and 3.','line_number':840,'multiline':False]['text':' toks_before_adding should be longer','line_number':845,'multiline':False]['text':' Check that none of the special tokens are lowercased','line_number':848,'multiline':False]['text':' Convert the tokenized list to str as some special tokens are tokenized like normal tokens','line_number':850,'multiline':False]['text':' which have a prefix spacee e.g. the mask token of Albert, and cannot match the original','line_number':851,'multiline':False]['text':' special tokens exactly.','line_number':852,'multiline':False]['text':' toks before adding new_toks','line_number':869,'multiline':False]['text':' Length should still be the same','line_number':878,'multiline':False]['text':' But at least the first non-special tokens should differ','line_number':881,'multiline':False]['text':' toks_before_adding should be longer','line_number':883,'multiline':False]['text':' TODO @ArthurZ Nuke this','line_number':886,'multiline':False]['text':' We usually have added tokens from the start in tests (but also otherwise) because our vocab fixtures are','line_number':896,'multiline':False]['text':' smaller than the original vocabs - let's not assert this','line_number':897,'multiline':False]['text':' self.assertEqual(vocab_size, all_size)','line_number':898,'multiline':False]['text':' These are added tokens, they will be normalized....','line_number':992,'multiline':False]['text':' TODO  @ArthurZ Refactor testing as now the do_normalize works for special and non special','line_number':1009,'multiline':False]['text':' We should have at least one default checkpoint for each tokenizer','line_number':1025,'multiline':False]['text':' We should specify the max input length as well (used in some part to list the pretrained checkpoints)','line_number':1026,'multiline':False]['text':' We want to have sequence 0 and sequence 1 are tagged','line_number':1062,'multiline':False]['text':' respectively with 0 and 1 token_ids','line_number':1063,'multiline':False]['text':' (regardless of whether the model use token type ids)','line_number':1064,'multiline':False]['text':' We use this assumption in the QA pipeline among other place','line_number':1065,'multiline':False]['text':' We want to have sequence 0 and sequence 1 are tagged','line_number':1078,'multiline':False]['text':' respectively with 0 and 1 token_ids','line_number':1079,'multiline':False]['text':' (regardless of whether the model use token type ids)','line_number':1080,'multiline':False]['text':' We use this assumption in the QA pipeline among other place','line_number':1081,'multiline':False]['text':' Test we can pass chat_template arg','line_number':1107,'multiline':False]['text':' Check that no error raised when tokenize=True','line_number':1108,'multiline':False]['text':' Test property setter','line_number':1112,'multiline':False]['text':' Test chat_template attribute is used if no arg is passed','line_number':1114,'multiline':False]['text':' Check that no error raised','line_number':1115,'multiline':False]['text':' Test template has persisted','line_number':1121,'multiline':False]['text':' Test output is the same after reloading','line_number':1123,'multiline':False]['text':' Check that no error raised','line_number':1124,'multiline':False]['text':' Method is implemented (e.g. not GPT-2)','line_number':1136,'multiline':False]['text':' Test with max model input length','line_number':1155,'multiline':False]['text':' Simple','line_number':1168,'multiline':False]['text':' Simple with no truncation','line_number':1182,'multiline':False]['text':' Reset warnings','line_number':1183,'multiline':False]['text':' Overflowing tokens','line_number':1208,'multiline':False]['text':' add_prefix_space=False,','line_number':1217,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1220,'multiline':False]['text':' Build a sequence from our model's vocabulary','line_number':1245,'multiline':False]['text':' We are not using the special tokens - a bit too hard to test all the tokenizers with this','line_number':1266,'multiline':False]['text':' TODO try this again later','line_number':1267,'multiline':False]['text':' , add_prefix_space=False)','line_number':1268,'multiline':False]['text':' Test with max model input length','line_number':1270,'multiline':False]['text':' Simple','line_number':1287,'multiline':False]['text':' Simple','line_number':1303,'multiline':False]['text':' Simple with no truncation','line_number':1310,'multiline':False]['text':' Reset warnings','line_number':1311,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1358,'multiline':False]['text':' add_prefix_space=False,','line_number':1368,'multiline':False]['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':1380,'multiline':False]['text':' add_prefix_space=False,','line_number':1390,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1401,'multiline':False]['text':' add_prefix_space=False,','line_number':1411,'multiline':False]['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':1423,'multiline':False]['text':' add_prefix_space=False,','line_number':1433,'multiline':False]['text':' add_prefix_space=False,','line_number':1452,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1454,'multiline':False]['text':' add_prefix_space=False,','line_number':1483,'multiline':False]['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1485,'multiline':False]['text':' def test_encode_input_type(self):','line_number':1506,'multiline':False]['text':'     tokenizers = self.get_tokenizers(do_lower_case=False)','line_number':1507,'multiline':False]['text':'     for tokenizer in tokenizers:','line_number':1508,'multiline':False]['text':'         with self.subTest(f"{tokenizer.__class__.__name__}"):','line_number':1509,'multiline':False]['text':'             sequence = "Let's encode this sequence"','line_number':1510,'multiline':False]['text':'             tokens = sequence.split()  # tokenizer.tokenize(sequence)','line_number':1512,'multiline':False]['text':'             # input_ids = tokenizer.convert_tokens_to_ids(tokens)','line_number':1513,'multiline':False]['text':'             formatted_input = tokenizer.encode(sequence, add_special_tokens=True, add_prefix_space=False)','line_number':1514,'multiline':False]['text':'             self.assertEqual(','line_number':1516,'multiline':False]['text':'                 tokenizer.encode(tokens, is_split_into_words=True, add_special_tokens=True), formatted_input','line_number':1517,'multiline':False]['text':'             )','line_number':1518,'multiline':False]['text':'             # This is not supported with the Rust tokenizers','line_number':1519,'multiline':False]['text':'             # self.assertEqual(tokenizer.encode(input_ids, add_special_tokens=True), formatted_input)','line_number':1520,'multiline':False]['text':' def test_swap_special_token(self):','line_number':1522,'multiline':False]['text':'     tokenizers = self.get_tokenizers(do_lower_case=False)','line_number':1523,'multiline':False]['text':'     for tokenizer in tokenizers:','line_number':1524,'multiline':False]['text':'         with self.subTest(f"{tokenizer.__class__.__name__}"):','line_number':1525,'multiline':False]['text':'             # Our mask token','line_number':1526,'multiline':False]['text':'             mask = "<mask>"','line_number':1527,'multiline':False]['text':'             # We take a single word in the middle of the vocabulary','line_number':1528,'multiline':False]['text':'             all_tokens = sorted(tokenizer.get_vocab().keys())','line_number':1529,'multiline':False]['text':'             word = tokenizer.decode(tokenizer.encode(all_tokens[len(all_tokens)//2], add_special_tokens=False)[:1])','line_number':1530,'multiline':False]['text':'             sequence_0 = "Encode " + word + " sequence"','line_number':1532,'multiline':False]['text':'             sequence_masked_0 = "Encode " + mask + " sequence"','line_number':1533,'multiline':False]['text':'             sequence_1 = word + " this sequence"','line_number':1535,'multiline':False]['text':'             sequence_masked_1 = mask + " this sequence"','line_number':1536,'multiline':False]['text':'             # Add tokens so that masked token isn't split','line_number':1538,'multiline':False]['text':'             # tokens = [AddedToken(t, lstrip=True, normalized=False) for t in sequence.split()]','line_number':1539,'multiline':False]['text':'             # tokenizer.add_tokens(tokens)','line_number':1540,'multiline':False]['text':'             tokenizer.add_special_tokens(','line_number':1541,'multiline':False]['text':'                 {"mask_token": AddedToken(mask, normalized=False)}','line_number':1542,'multiline':False]['text':'             )  # Eat left space on Byte-level BPE tokenizers','line_number':1543,'multiline':False]['text':'             mask_ind = tokenizer.convert_tokens_to_ids(mask)','line_number':1544,'multiline':False]['text':'             # Test first masked sequence','line_number':1546,'multiline':False]['text':'             encoded_0 = tokenizer.encode(sequence_0, add_special_tokens=False)','line_number':1547,'multiline':False]['text':'             encoded_masked = tokenizer.encode(sequence_masked_0, add_special_tokens=False)','line_number':1548,'multiline':False]['text':'             self.assertEqual(len(encoded_masked), len(encoded_0))','line_number':1549,'multiline':False]['text':'             mask_loc = encoded_masked.index(mask_ind)','line_number':1550,'multiline':False]['text':'             encoded_masked[mask_loc] = encoded_0[mask_loc]','line_number':1551,'multiline':False]['text':'             self.assertEqual(encoded_masked, encoded_0)','line_number':1553,'multiline':False]['text':'             # Test second masked sequence','line_number':1555,'multiline':False]['text':'             encoded_1 = tokenizer.encode(sequence_1, add_special_tokens=False)','line_number':1556,'multiline':False]['text':'             encoded_masked = tokenizer.encode(sequence_masked_1, add_special_tokens=False)','line_number':1557,'multiline':False]['text':'             self.assertEqual(len(encoded_masked), len(encoded_1))','line_number':1558,'multiline':False]['text':'             mask_loc = encoded_masked.index(mask_ind)','line_number':1559,'multiline':False]['text':'             encoded_masked[mask_loc] = encoded_1[mask_loc]','line_number':1560,'multiline':False]['text':'             self.assertEqual(encoded_masked, encoded_1)','line_number':1562,'multiline':False]['text':' Testing single inputs','line_number':1569,'multiline':False]['text':' , add_prefix_space=False','line_number':1574,'multiline':False]['text':' add_prefix_space=False,','line_number':1596,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':1693,'multiline':False]['text':' RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1698,'multiline':False]['text':' LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1709,'multiline':False]['text':' RIGHT & LEFT PADDING - Check that nothing is done for 'longest' and 'no_padding'','line_number':1720,'multiline':False]['text':' RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1754,'multiline':False]['text':' Remove EOS/BOS tokens','line_number':1759,'multiline':False]['text':' LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the truncation flag set to True','line_number':1767,'multiline':False]['text':' RIGHT & LEFT PADDING - Check that nothing is done for 'longest' and 'no_truncation'','line_number':1777,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':1814,'multiline':False]['text':' Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1819,'multiline':False]['text':' FIXME: the next line should be padding(max_length) to avoid warning','line_number':1823,'multiline':False]['text':' Check that nothing is done when a maximum length is not specified','line_number':1831,'multiline':False]['text':' Should also work with truncation','line_number':1859,'multiline':False]['text':' truncation to something which is not a multiple of pad_to_multiple_of raises an error','line_number':1864,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':1900,'multiline':False]['text':' Test 'longest' and 'no_padding' don't do anything','line_number':1912,'multiline':False]['text':' Test right padding','line_number':1943,'multiline':False]['text':' Test left padding','line_number':1961,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2004,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2022,'multiline':False]['text':' We want to assert there are no warnings, but the 'assertLogs' method does not support that.','line_number':2028,'multiline':False]['text':' Therefore, we are adding a dummy warning, and then we will assert it is the only warning.','line_number':2029,'multiline':False]['text':' This tests that tokenizers don't impact others. Unfortunately the case where it fails is when','line_number':2039,'multiline':False]['text':' we're loading an S3 configuration from a pre-trained identifier, and we have no way of testing those today.','line_number':2040,'multiline':False]['text':' Tests that all call wrap to encode_plus and batch_encode_plus','line_number':2078,'multiline':False]['text':' Test not batched','line_number':2088,'multiline':False]['text':' Test not batched pairs','line_number':2093,'multiline':False]['text':' Test batched','line_number':2098,'multiline':False]['text':' Test batched pairs','line_number':2103,'multiline':False]['text':' Tests that all encoded values have the correct size','line_number':2109,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2129,'multiline':False]['text':' check 'longest' is unsensitive to a max length','line_number':2143,'multiline':False]['text':' check 'no_padding' is unsensitive to a max length','line_number':2154,'multiline':False]['text':' Canine cannot add tokens which are not codepoints','line_number':2177,'multiline':False]['text':' XXX: This used to split on `extra_id_1` first we're matching','line_number':2180,'multiline':False]['text':' longest first now.','line_number':2181,'multiline':False]['text':' TODO this is tested 10_000 times....','line_number':2195,'multiline':False]['text':' Test that padded sequences are equivalent between batch_encode_plus and encode_plus','line_number':2207,'multiline':False]['text':' Right padding tests','line_number':2209,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2221,'multiline':False]['text':' Left padding tests','line_number':2235,'multiline':False]['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':2248,'multiline':False]['text':' Test when inputs are pretokenized','line_number':2263,'multiline':False]['text':' , add_prefix_space=True)','line_number':2265,'multiline':False]['text':' Prepare a sequence from our tokenizer vocabulary','line_number':2271,'multiline':False]['text':' sequence = " " + sequence  # To be sure the byte-level tokenizers are feeling good','line_number':2273,'multiline':False]['text':' sequence_no_prefix_space = sequence.strip()','line_number':2275,'multiline':False]['text':' Test encode for pretokenized inputs','line_number':2277,'multiline':False]['text':' Test encode_plus for pretokenized inputs','line_number':2286,'multiline':False]['text':' Test batch_encode_plus for pretokenized inputs','line_number':2296,'multiline':False]['text':' Test encode for pretokenized inputs pairs','line_number':2318,'multiline':False]['text':' Test encode_plus for pretokenized inputs pairs','line_number':2330,'multiline':False]['text':' Test batch_encode_plus for pretokenized inputs pairs','line_number':2344,'multiline':False]['text':' A Tensor cannot be build by sequences which are not the same size','line_number':2405,'multiline':False]['text':' if tokenizer does not have pad_token_id, an error should be thrown','line_number':2437,'multiline':False]['text':' add pad_token_id to pass subsequent tests','line_number':2445,'multiline':False]['text':' Make sure the model contains at least the full vocabulary size in its embedding matrix','line_number':2471,'multiline':False]['text':' Build sequence','line_number':2476,'multiline':False]['text':' Ensure that the BatchEncoding.to() method works.','line_number':2481,'multiline':False]['text':' This should not fail','line_number':2485,'multiline':False]['text':' saves some time','line_number':2487,'multiline':False]['text':' if self.test_rust_tokenizer:','line_number':2491,'multiline':False]['text':'     fast_tokenizer = self.get_rust_tokenizer()','line_number':2492,'multiline':False]['text':'     encoded_sequence_fast = fast_tokenizer.encode_plus(sequence, return_tensors="pt")','line_number':2493,'multiline':False]['text':'     batch_encoded_sequence_fast = fast_tokenizer.batch_encode_plus([sequence, sequence], return_tensors="pt")','line_number':2494,'multiline':False]['text':'     # This should not fail','line_number':2495,'multiline':False]['text':'     model(**encoded_sequence_fast)','line_number':2496,'multiline':False]['text':'     model(**batch_encoded_sequence_fast)','line_number':2497,'multiline':False]['text':' Make sure the model contains at least the full vocabulary size in its embedding matrix','line_number':2520,'multiline':False]['text':' Build sequence','line_number':2523,'multiline':False]['text':' This should not fail','line_number':2529,'multiline':False]['text':' TODO: Check if require_torch is the best to test for numpy here ... Maybe move to require_flax when available','line_number':2533,'multiline':False]['text':' Build sequence','line_number':2553,'multiline':False]['text':' TODO: add forward through JAX/Flax when PR is merged','line_number':2559,'multiline':False]['text':' This is currently here to make ruff happy !','line_number':2560,'multiline':False]['text':' TODO: add forward through JAX/Flax when PR is merged','line_number':2574,'multiline':False]['text':' This is currently here to make ruff happy !','line_number':2575,'multiline':False]['text':' Longer text that will definitely require truncation.','line_number':2590,'multiline':False]['text':' this should be ignored (for all but mbart) but not cause an error','line_number':2610,'multiline':False]['text':' max_target_length will default to max_length if not specified','line_number':2616,'multiline':False]['text':' Check is_fast is set correctly','line_number':2634,'multiline':False]['text':' Ensure None raise an error','line_number':2646,'multiline':False]['text':' words, tokens','line_number':2671,'multiline':False]['text':' Assert token_to_word','line_number':2680,'multiline':False]['text':' Assert word_to_tokens','line_number':2689,'multiline':False]['text':' Assert token_to_chars','line_number':2700,'multiline':False]['text':' Assert char_to_token','line_number':2711,'multiline':False]['text':' Assert char_to_word','line_number':2720,'multiline':False]['text':' Assert word_to_chars','line_number':2729,'multiline':False]['text':' Assert token_to_sequence','line_number':2740,'multiline':False]['text':' Pair of input sequences','line_number':2747,'multiline':False]['text':' Assert word_to_tokens','line_number':2771,'multiline':False]['text':' Assert char_to_token','line_number':2797,'multiline':False]['text':' Assert char_to_word','line_number':2819,'multiline':False]['text':' Assert word_to_chars','line_number':2837,'multiline':False]['text':' Assert token_to_sequence','line_number':2855,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':2880,'multiline':False]['text':' Ensure basic input match','line_number':2888,'multiline':False]['text':' Ensure truncation match','line_number':2901,'multiline':False]['text':' Ensure truncation with stride match','line_number':2908,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':2921,'multiline':False]['text':' Check we have the same number of added_tokens for both pair and non-pair inputs.','line_number':2929,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':2939,'multiline':False]['text':' Check we have the correct max_length for both pair and non-pair inputs.','line_number':2947,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':2953,'multiline':False]['text':' sometimes the tokenizer saved online is not the same','line_number':2958,'multiline':False]['text':' Assert the set of special tokens match.','line_number':2962,'multiline':False]['text':' No pair','line_number':3002,'multiline':False]['text':' Assert there is the same number of tokens and offsets','line_number':3009,'multiline':False]['text':' Assert there is online added_tokens special_tokens','line_number':3012,'multiline':False]['text':' Pairs','line_number':3015,'multiline':False]['text':' Assert there is the same number of tokens and offsets','line_number':3022,'multiline':False]['text':' Assert there is online added_tokens special_tokens','line_number':3025,'multiline':False]['text':' Mono sample','line_number':3067,'multiline':False]['text':' Multi sample','line_number':3081,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3097,'multiline':False]['text':' Too hard to test for now','line_number':3106,'multiline':False]['text':' Input string','line_number':3108,'multiline':False]['text':' Test encode for pretokenized inputs','line_number':3112,'multiline':False]['text':' "return_token_type_ids": True,  # Use the defaults for each tokenizers','line_number':3123,'multiline':False]['text':' "return_attention_mask": True,  # Use the defaults for each tokenizers','line_number':3124,'multiline':False]['text':' Not implemented in python tokenizers','line_number':3127,'multiline':False]['text':' "add_special_tokens": False,','line_number':3128,'multiline':False]['text':' "return_token_type_ids": True,  # Use the defaults for each tokenizers','line_number':3132,'multiline':False]['text':' "return_attention_mask": True,  # Use the defaults for each tokenizers','line_number':3133,'multiline':False]['text':' Not implemented in python tokenizers','line_number':3136,'multiline':False]['text':' "add_special_tokens": False,','line_number':3137,'multiline':False]['text':' Test encode_plus for pretokenized inputs','line_number':3139,'multiline':False]['text':' Test batch_encode_plus for pretokenized inputs','line_number':3145,'multiline':False]['text':' Test encode for pretokenized inputs pairs','line_number':3152,'multiline':False]['text':' Test encode_plus for pretokenized inputs','line_number':3161,'multiline':False]['text':' Test batch_encode_plus for pretokenized inputs','line_number':3167,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3179,'multiline':False]['text':' Generate output','line_number':3189,'multiline':False]['text':' Generate pair output','line_number':3194,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3201,'multiline':False]['text':' # Input string','line_number':3208,'multiline':False]['text':' input_simple = tokenizer_p.tokenize("This is a sample input", add_special_tokens=False)','line_number':3209,'multiline':False]['text':' input_pair = tokenizer_p.tokenize("This is a sample pair", add_special_tokens=False)','line_number':3210,'multiline':False]['text':' # Generate output','line_number':3212,'multiline':False]['text':' output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple)','line_number':3213,'multiline':False]['text':' output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple)','line_number':3214,'multiline':False]['text':' self.assertEqual(output_p, output_r)','line_number':3215,'multiline':False]['text':' # Generate pair output','line_number':3217,'multiline':False]['text':' output_r = tokenizer_r.build_inputs_with_special_tokens(input_simple, input_pair)','line_number':3218,'multiline':False]['text':' output_p = tokenizer_p.build_inputs_with_special_tokens(input_simple, input_pair)','line_number':3219,'multiline':False]['text':' self.assertEqual(output_p, output_r)','line_number':3220,'multiline':False]['text':' Input tokens id','line_number':3230,'multiline':False]['text':' Generate output','line_number':3234,'multiline':False]['text':' Generate pair output','line_number':3239,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3246,'multiline':False]['text':' Encode - Simple input','line_number':3257,'multiline':False]['text':' Encode - Pair input','line_number':3269,'multiline':False]['text':' Encode_plus - Simple input','line_number':3288,'multiline':False]['text':' Encode_plus - Pair input','line_number':3314,'multiline':False]['text':' Batch_encode_plus - Simple input','line_number':3338,'multiline':False]['text':' Batch_encode_plus - Pair input','line_number':3383,'multiline':False]['text':' Using pad on single examples after tokenization','line_number':3420,'multiline':False]['text':' Using pad on single examples after tokenization','line_number':3431,'multiline':False]['text':' Using pad after tokenization','line_number':3440,'multiline':False]['text':' Using pad after tokenization','line_number':3453,'multiline':False]['text':' Test padding nested empty lists (in some use-cases, there is no any token id in the `input_ids` list).','line_number':3465,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3472,'multiline':False]['text':' rename encoded batch to "inputs"','line_number':3489,'multiline':False]['text':' Renaming `input_ids` to `inputs`','line_number':3496,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3510,'multiline':False]['text':' make sure that all ".json" files are saved in the correct format','line_number':3523,'multiline':False]['text':' Checks it save with the same files + the tokenizer.json file for the fast one','line_number':3528,'multiline':False]['text':' Checks everything loads correctly in the same way','line_number':3533,'multiline':False]['text':' Check special tokens are set accordingly on Rust and Python','line_number':3537,'multiline':False]['text':' self.assertEqual(getattr(tokenizer_rp, key), getattr(tokenizer_pp, key))','line_number':3540,'multiline':False]['text':' self.assertEqual(getattr(tokenizer_rp, key + "_id"), getattr(tokenizer_pp, key + "_id"))','line_number':3541,'multiline':False]['text':' Save tokenizer rust, legacy_format=True','line_number':3545,'multiline':False]['text':' Checks it save with the same files','line_number':3551,'multiline':False]['text':' Checks everything loads correctly in the same way','line_number':3554,'multiline':False]['text':' Check special tokens are set accordingly on Rust and Python','line_number':3558,'multiline':False]['text':' Save tokenizer rust, legacy_format=False','line_number':3564,'multiline':False]['text':' Checks it saved the tokenizer.json file','line_number':3570,'multiline':False]['text':' Checks everything loads correctly in the same way','line_number':3573,'multiline':False]['text':' Check special tokens are set accordingly on Rust and Python','line_number':3577,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3585,'multiline':False]['text':' pair_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=True)','line_number':3618,'multiline':False]['text':' tokenize()','line_number':3621,'multiline':False]['text':' encode()','line_number':3628,'multiline':False]['text':' encode_plus()','line_number':3635,'multiline':False]['text':' # batch_encode_plus','line_number':3644,'multiline':False]['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':3653,'multiline':False]['text':' in rust fast, you lose the information of the AddedToken when initializing with `additional_special_tokens`','line_number':3684,'multiline':False]['text':' This test no longer support rust tokenizers, because the only file that should be looked','line_number':3702,'multiline':False]['text':' at by the fast tokenizer with the new saving format is `tokenizer_config.json`.','line_number':3703,'multiline':False]['text':' The previous behaviour is very strange too. Fast tokenizer should not save 3 files, but just one. Can never do slow from fast.','line_number':3704,'multiline':False]['text':' only legacy save will check this','line_number':3712,'multiline':False]['text':' the following checks allow us to verify that our test works as expected, i.e. that the tokenizer takes','line_number':3722,'multiline':False]['text':' into account the new value of additional_special_tokens given in the "tokenizer_config.json" and','line_number':3723,'multiline':False]['text':' "special_tokens_map.json" files','line_number':3724,'multiline':False]['text':' TODO ArthurZ ... Ok so for legacy we have to support this I guess..... (special_tokens_map + additional)','line_number':3726,'multiline':False]['text':' Now we test that we can change the value of additional_special_tokens in the from_pretrained','line_number':3739,'multiline':False]['text':' This feature only exists for fast tokenizers','line_number':3755,'multiline':False]['text':' Test we can use the new tokenizer with something not seen during training','line_number':3762,'multiline':False]['text':' We check that the parameters of the tokenizer remained the same','line_number':3772,'multiline':False]['text':' Check we have the same number of added_tokens for both pair and non-pair inputs.','line_number':3773,'multiline':False]['text':' Check we have the correct max_length for both pair and non-pair inputs.','line_number':3777,'multiline':False]['text':' Assert the set of special tokens match as we didn't ask to change them','line_number':3781,'multiline':False]['text':' This feature only exists for fast tokenizers','line_number':3790,'multiline':False]['text':' Test with a special tokens map','line_number':3795,'multiline':False]['text':' Create a new mapping from the special tokens defined in the original tokenizer','line_number':3805,'multiline':False]['text':' Get the private one to avoid unnecessary warnings.','line_number':3810,'multiline':False]['text':' Train new tokenizer','line_number':3815,'multiline':False]['text':' Check the changes','line_number':3820,'multiline':False]['text':' Get the private one to avoid unnecessary warnings.','line_number':3822,'multiline':False]['text':' Check if the AddedToken / string format has been kept','line_number':3833,'multiline':False]['text':' The special token must appear identically in the list of the new tokenizer.','line_number':3836,'multiline':False]['text':' The special token must appear in the list of the new tokenizer as an object of type AddedToken with','line_number':3842,'multiline':False]['text':' the same parameters as the old AddedToken except the content that the user has requested to change.','line_number':3843,'multiline':False]['text':' The special token must appear identically in the list of the new tokenizer.','line_number':3867,'multiline':False]['text':' The special token must appear in the list of the new tokenizer as an object of type string.','line_number':3874,'multiline':False]['text':' Test we can use the new tokenizer with something not seen during training','line_number':3877,'multiline':False]['text':' Some tokenizer will raised an error before reaching the logged warning because there are no','line_number':3897,'multiline':False]['text':' corresponding files to load','line_number':3898,'multiline':False]['text':' Some tokenizers cannot be loaded into the target tokenizer at all and errors are returned,','line_number':3901,'multiline':False]['text':' here we just check that the warning has been logged before the error is raised','line_number':3902,'multiline':False]['text':' Some tokenizers cannot be loaded into the target tokenizer at all and errors are returned,','line_number':3921,'multiline':False]['text':' here we just check that the warning has been logged before the error is raised','line_number':3922,'multiline':False]['text':' Save the fast tokenizer files in a temporary directory','line_number':3937,'multiline':False]['text':' save only fast version','line_number':3939,'multiline':False]['text':' Initialize toy model for the trainer','line_number':3941,'multiline':False]['text':' Load tokenizer from a folder without legacy files','line_number':3944,'multiline':False]['text':' Should not raise an error','line_number':3949,'multiline':False]['text':' we need both slow and fast versions','line_number':3964,'multiline':False]['text':' Here we check that even if we have initialized a fast tokenizer with a tokenizer_file we can','line_number':3970,'multiline':False]['text':' still save only the slow version and use these saved files to rebuild a tokenizer','line_number':3971,'multiline':False]['text':' save only slow version','line_number':3982,'multiline':False]['text':' Should not raise an error','line_number':3988,'multiline':False]['text':' TODO This is ran for all models but only tests bert...','line_number':3991,'multiline':False]['text':' Fast from slow','line_number':4005,'multiline':False]['text':' fast and slow don't have the same output when we don't cleanup','line_number':4013,'multiline':False]['text':' tokenization space. Here `be!` vs `be !` and `go.` vs `go .`','line_number':4014,'multiline':False]['text':' Slow from fast','line_number':4023,'multiline':False]['text':' bloom, gptneox etc only have a fast','line_number':4047,'multiline':False]['text':' if we have subword tokenization or special vocab','line_number':4062,'multiline':False]['text':' Utility to test the added vocab','line_number':4070,'multiline':False]['text':' Load a slow tokenizer from the hub, init with the new token for fast to also include it','line_number':4082,'multiline':False]['text':' We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright','line_number':4126,'multiline':False]