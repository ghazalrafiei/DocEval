['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2019 HuggingFace Inc.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' noqa E402','line_number':74,'multiline':False]['text':' Fake pretrained models for tests','line_number':102,'multiline':False]['text':' linear is a common name between Base and Head on purpose.','line_number':139,'multiline':False]['text':' Not sure this is the intended behavior. TODO fix Lysandre & Thom','line_number':232,'multiline':False]['text':' test that the model can be instantiated with dtype of user's choice - as long as it's a','line_number':299,'multiline':False]['text':' float dtype. To make it happen config.torch_dtype needs to be set before instantiating the','line_number':300,'multiline':False]['text':' model from the config object.','line_number':301,'multiline':False]['text':' XXX: isn't supported','line_number':305,'multiline':False]['text':' model = T5ForConditionalGeneration.from_config(config)','line_number':306,'multiline':False]['text':' torch.set_default_dtype() supports only float dtypes, so will fail with non-float type','line_number':312,'multiline':False]['text':' test that the model can be instantiated with dtype of either','line_number':317,'multiline':False]['text':' 1. explicit from_pretrained's torch_dtype argument','line_number':318,'multiline':False]['text':' 2. via autodiscovery by looking at model weights (torch_dtype="auto")','line_number':319,'multiline':False]['text':' so if a model.half() was saved, we want it to be instantiated as such.','line_number':320,'multiline':False]['text':'','line_number':321,'multiline':False]['text':' test an explicit model class, but also AutoModel separately as the latter goes through a different code path','line_number':322,'multiline':False]['text':' baseline - we know TINY_T5 is fp32 model','line_number':325,'multiline':False]['text':' test the default fp32 save_pretrained => from_pretrained cycle','line_number':337,'multiline':False]['text':' 1. test torch_dtype="auto" via `config.torch_dtype`','line_number':341,'multiline':False]['text':' 2. test torch_dtype="auto" via auto-derivation','line_number':344,'multiline':False]['text':' now remove the torch_dtype entry from config.json and try "auto" again which should','line_number':345,'multiline':False]['text':' perform auto-derivation from weights','line_number':346,'multiline':False]['text':' test forced loading in fp16 (even though the weights are in fp32)','line_number':351,'multiline':False]['text':' test fp16 save_pretrained, loaded with auto-detection','line_number':355,'multiline':False]['text':' 1. test torch_dtype="auto" via `config.torch_dtype`','line_number':358,'multiline':False]['text':' tests `config.torch_dtype` saving','line_number':362,'multiline':False]['text':' 2. test torch_dtype="auto" via auto-derivation','line_number':366,'multiline':False]['text':' now same with using config info','line_number':367,'multiline':False]['text':' 3. now retest that AutoModel behaves the same wrt torch_dtype="auto" as T5ForConditionalGeneration','line_number':372,'multiline':False]['text':' test fp16 save_pretrained, loaded with the explicit fp16','line_number':376,'multiline':False]['text':' test AutoModel separately as it goes through a different path','line_number':380,'multiline':False]['text':' test auto-detection - as currently TINY_T5 doesn't have torch_dtype entry','line_number':381,'multiline':False]['text':' test that the config object didn't get polluted with torch_dtype="auto"','line_number':383,'multiline':False]['text':' there was a bug that after this call we ended up with config.torch_dtype=="auto"','line_number':384,'multiline':False]['text':' now test the outcome','line_number':386,'multiline':False]['text':' test model whose first param is not of a floating type, but int','line_number':391,'multiline':False]['text':' This is the model we will use, total size 340,000 bytes.','line_number':408,'multiline':False]['text':' size 80,000','line_number':410,'multiline':False]['text':' size 160,000','line_number':411,'multiline':False]['text':' size 80,000','line_number':412,'multiline':False]['text':' size 20,000','line_number':413,'multiline':False]['text':' Split is first two layers then last two.','line_number':424,'multiline':False]['text':' Split is first layer, second layer then last 2.','line_number':446,'multiline':False]['text':' We use the same folder for various sizes to make sure a new save erases the old checkpoint.','line_number':476,'multiline':False]['text':' Get each shard file and its size','line_number':480,'multiline':False]['text':' Check there is an index but no regular weight file','line_number':488,'multiline':False]['text':' Check a file is bigger than max_size only when it has a single weight','line_number':492,'multiline':False]['text':' Note: pickle adds some junk so the weight of the file can end up being slightly bigger than','line_number':498,'multiline':False]['text':' the size asked for (since we count parameters)','line_number':499,'multiline':False]['text':' Check the index and the shard files found match','line_number':504,'multiline':False]['text':' Finally, check the model can be reloaded','line_number':512,'multiline':False]['text':' the model above is the same as the model below, just a sharded version.','line_number':519,'multiline':False]['text':' saving will create a variant checkpoint','line_number':664,'multiline':False]['text':' saving shouldn't delete variant checkpoints','line_number':668,'multiline':False]['text':' there should be a normal checkpoint','line_number':672,'multiline':False]['text':' test that we can use `from_pretrained(..., low_cpu_mem_usage=True)` with normal and','line_number':680,'multiline':False]['text':' sharded models','line_number':681,'multiline':False]['text':' test that `from_pretrained(..., low_cpu_mem_usage=True)` uses less cpu memory than default','line_number':694,'multiline':False]['text':' print(f"{max_rss_normal=}")','line_number':701,'multiline':False]['text':' print(f"{max_rss_low_mem=}")','line_number':705,'multiline':False]['text':' print(f"{diff_bytes=}, {diff_percent=}")','line_number':709,'multiline':False]['text':' ideally we would compare that the diff is close to ~1x checkpoint size in bytes, but','line_number':710,'multiline':False]['text':' measuring cpu memory on linux is very tricky and inconsistent, so instead let's check that','line_number':711,'multiline':False]['text':' it's at least 15% less cpu memory consumed','line_number':712,'multiline':False]['text':' if you want to compare things manually, let's first look at the size of the model in bytes','line_number':721,'multiline':False]['text':' model = BertModel.from_pretrained(mname, low_cpu_mem_usage=False)','line_number':722,'multiline':False]['text':' total_numel = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())','line_number':723,'multiline':False]['text':' total_bytes = total_numel * 4  # 420MB','line_number':724,'multiline':False]['text':' Now the diff_bytes should be very close to total_bytes, but the reports are inconsistent.','line_number':725,'multiline':False]['text':' The easiest way to test this is to switch the model and torch.load to do all the work on','line_number':726,'multiline':False]['text':' gpu - that way one can measure exactly the total and peak memory used. Perhaps once we add','line_number':727,'multiline':False]['text':' functionality to load models directly on gpu, this test can be rewritten to use torch's','line_number':728,'multiline':False]['text':' cuda memory tracking and then we should be able to do a much more precise test.','line_number':729,'multiline':False]['text':' With state dict temp offload','line_number':780,'multiline':False]['text':' with disk offload','line_number':814,'multiline':False]['text':' With state dict temp offload','line_number':822,'multiline':False]['text':' A mock response for an HTTP head request to emulate server down','line_number':833,'multiline':False]['text':' Download this model to make sure it's in the cache.','line_number':840,'multiline':False]['text':' Under the mock environment we get a 500 error when trying to reach the model.','line_number':843,'multiline':False]['text':' This check we did call the fake head request','line_number':846,'multiline':False]['text':' This test is for deprecated behavior and can be removed in v5','line_number':863,'multiline':False]['text':' Should not raise anymore','line_number':871,'multiline':False]['text':' test that error if only safetensors is available','line_number':874,'multiline':False]['text':' test that only safetensors if both available and use_safetensors=False','line_number':880,'multiline':False]['text':' test that no safetensors if both available and use_safetensors=True','line_number':893,'multiline':False]['text':' No pytorch_model.bin file, only a model.safetensors','line_number':911,'multiline':False]['text':' Check models are equal','line_number':917,'multiline':False]['text':' Check models are equal','line_number':926,'multiline':False]['text':' No pytorch_model.bin index file, only a model.safetensors index','line_number':935,'multiline':False]['text':' No regular weights file','line_number':938,'multiline':False]['text':' Check models are equal','line_number':944,'multiline':False]['text':' Check models are equal','line_number':953,'multiline':False]['text':' Can load a base model in a model with head','line_number':962,'multiline':False]['text':' It doesn't work if the state dict has a mix of keys of the head and base without prefix though.','line_number':967,'multiline':False]['text':' Base','line_number':980,'multiline':False]['text':' Remove tied weight from state_dict -> model should load with no complain of missing keys','line_number':989,'multiline':False]['text':' With head','line_number':996,'multiline':False]['text':' Should only complain about the missing bias','line_number':1000,'multiline':False]['text':' Loading the model with a new class, we don't get a warning for unexpected weights, just an info','line_number':1009,'multiline':False]['text':' Loading the model with the same class, we do get a warning for unexpected weights','line_number':1018,'multiline':False]['text':' Checking for 1 model(the same one which was described in the issue) .','line_number':1125,'multiline':False]['text':' Note: `joaogante/tiny-random-gpt2-with-generation-config` has a `generation_config.json` containing a dummy','line_number':1145,'multiline':False]['text':' `transformers_version` field set to `foo`. If loading the file fails, this test also fails.','line_number':1146,'multiline':False]['text':' 1. Load without further parameters','line_number':1148,'multiline':False]['text':' 2. Load with `device_map`','line_number':1152,'multiline':False]['text':' This should have opened a PR with the user's account','line_number':1360,'multiline':False]['text':' We now switch the repo visibility to public','line_number':1367,'multiline':False]['text':' We once again call from_pretrained, which should call the bot to open a PR','line_number':1370,'multiline':False]['text':' Push a model on `main`','line_number':1393,'multiline':False]['text':' Push a model on a given revision','line_number':1396,'multiline':False]['text':' Try to convert the model on that revision should raise','line_number':1399,'multiline':False]['text':' Reset repo','line_number':1441,'multiline':False]['text':' Push to hub via save_pretrained','line_number':1444,'multiline':False]['text':' Reset repo','line_number':1481,'multiline':False]['text':' Push to hub via save_pretrained','line_number':1484,'multiline':False]['text':' checks','line_number':1500,'multiline':False]['text':' Can't make an isinstance check because the new_model is from the CustomModel class of a dynamic module','line_number':1507,'multiline':False]['text':' make sure there are no overflows','line_number':1537,'multiline':False]['text':' k * (k+1) / 2 tokens are masked in triangualar masks','line_number':1542,'multiline':False]['text':' at least causal mask + maybe more','line_number':1548,'multiline':False]['text':' k * (k+1) / 2 tokens are masked in triangualar masks','line_number':1557,'multiline':False]['text':' at least causal mask + maybe more','line_number':1564,'multiline':False]['text':' no causal mask if q_len is 1','line_number':1574,'multiline':False]['text':' k * (k+1) / 2 tokens are masked in triangualar masks','line_number':1580,'multiline':False]['text':' k * (k+1) / 2 tokens are masked in triangualar masks','line_number':1587,'multiline':False]['text':' This function computes the # of attention tokens that are added for','line_number':1594,'multiline':False]['text':' the sliding window','line_number':1595,'multiline':False]['text':' auto-regressive use case','line_number':1605,'multiline':False]['text':' special auto-regressive case','line_number':1607,'multiline':False]['text':' non auto-regressive case','line_number':1609,'multiline':False]['text':' same with extra attention masks','line_number':1612,'multiline':False]['text':' check that the mask does not overflow on causal masked tokens','line_number':1617,'multiline':False]['text':' non auto-regressive case','line_number':1623,'multiline':False]['text':' same with extra attention masks','line_number':1626,'multiline':False]['text':' auto-regressive use case','line_number':1632,'multiline':False]['text':' special auto-regressive case','line_number':1634,'multiline':False]['text':' non auto-regressive case','line_number':1636,'multiline':False]['text':' same with extra attention masks','line_number':1639,'multiline':False]['text':' auto-regressive use case','line_number':1647,'multiline':False]['text':' special auto-regressive case','line_number':1649,'multiline':False]['text':' non auto-regressive case','line_number':1651,'multiline':False]['text':' auto-regressive use case','line_number':1657,'multiline':False]['text':' special auto-regressive case','line_number':1659,'multiline':False]['text':' non auto-regressive case','line_number':1661,'multiline':False]