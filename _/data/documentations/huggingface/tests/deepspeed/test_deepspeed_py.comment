['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' noqa','line_number':28,'multiline':False]['text':' noqa','line_number':56,'multiline':False]['text':' hack to restore original logging level pre #21700','line_number':61,'multiline':False]['text':' default torch.distributed port','line_number':67,'multiline':False]['text':' noqa','line_number':119,'multiline':False]['text':' noqa','line_number':121,'multiline':False]['text':' 1. explicitly set --num_nodes=1 just in case these tests end up run on a multi-node setup','line_number':125,'multiline':False]['text':' - it won't be able to handle that','line_number':126,'multiline':False]['text':' 2. for now testing with just 2 gpus max (since some quality tests may give different','line_number':127,'multiline':False]['text':' results with mode gpus because we use very little data)','line_number':128,'multiline':False]['text':' customize the test name generator function as we want both params to appear in the sub-test','line_number':156,'multiline':False]['text':' name, as by default it shows only the first param','line_number':157,'multiline':False]['text':' Cartesian-product of zero stages with models to test','line_number':162,'multiline':False]['text':' reset the ds config global so that tests state doesn't leak','line_number':190,'multiline':False]['text':' test that zero.Init() works correctly under zero3/fp16','line_number':194,'multiline':False]['text':' now remove zero optimization','line_number':214,'multiline':False]['text':' use self.get_config_dict(stage) to use these to ensure the original is not modified','line_number':251,'multiline':False]['text':' The following setting slows things down, so don't enable it by default unless needed by a test.','line_number':256,'multiline':False]['text':' It's in the file as a demo for users since we want everything to work out of the box even if slower.','line_number':257,'multiline':False]['text':' reset the ds config global so that tests state doesn't leak','line_number':268,'multiline':False]['text':' As some tests modify the dict, always make a copy','line_number':272,'multiline':False]['text':' --- These tests are enough to run on one of zero stages --- #','line_number':298,'multiline':False]['text':' Purposefully configure these values to mismatch TrainingArguments values.','line_number':303,'multiline':False]['text':' This currently doesn't cover all keys (but it could)','line_number':304,'multiline':False]['text':' Test various combos','line_number':351,'multiline':False]['text':' 1. DS scheduler + DS optimizer: this is already tested by most other tests','line_number':352,'multiline':False]['text':' 2. HF scheduler + HF optimizer:','line_number':353,'multiline':False]['text':' 3. DS scheduler + HF optimizer:','line_number':354,'multiline':False]['text':' 4. HF scheduler + DS optimizer:','line_number':355,'multiline':False]['text':' force default HF Trainer optimizer','line_number':361,'multiline':False]['text':' force default HF Trainer scheduler','line_number':362,'multiline':False]['text':' force optimizer on the first step','line_number':364,'multiline':False]['text':' force default HF Trainer optimizer','line_number':374,'multiline':False]['text':' force optimizer on the first step','line_number':376,'multiline':False]['text':' force default HF Trainer scheduler','line_number':386,'multiline':False]['text':' force optimizer on the first step','line_number':388,'multiline':False]['text':' this actually doesn't have to be on NVMe, any storage will do since this test only','line_number':397,'multiline':False]['text':' runs a simple check that we can use some directory as if it were NVMe','line_number':398,'multiline':False]['text':' hyperparameter_search requires model_init() to recreate the model for each trial','line_number':414,'multiline':False]['text':' --- These tests need to run on both zero stages --- #','line_number':435,'multiline':False]['text':' non-DS optimizers can be used with ZERO-offload (as long as they have both CPU and GPU implementation (except LAMB))','line_number':439,'multiline':False]['text':' force default HF Trainer optimizer','line_number':441,'multiline':False]['text':' force cpu offload','line_number':442,'multiline':False]['text':' offload is not efficient w/o CPUAdam','line_number':444,'multiline':False]['text':' this setup emulates a notebook where a launcher needs to be emulated by hand','line_number':455,'multiline':False]['text':' note that unittest resets sys.stdout each test, so `CaptureStd` will work here to capture','line_number':457,'multiline':False]['text':' DeepSpeed log if this test happens to run first in this pytest worker. But it will fail if','line_number':458,'multiline':False]['text':' it's run not as a first test as `sys.stdout` will no longer be the same. So we either have','line_number':459,'multiline':False]['text':' to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.','line_number':460,'multiline':False]['text':' with deepspeed's fp16 and dynamic loss scale enabled the optimizer/scheduler steps may','line_number':472,'multiline':False]['text':' not run for the first few dozen steps while loss scale is too large, and thus during','line_number':473,'multiline':False]['text':' that time `get_last_lr` will fail if called during that warm up stage,','line_number':474,'multiline':False]['text':'','line_number':475,'multiline':False]['text':' setting `logging_steps=1` forces an early `trainer._maybe_log_save_evaluate()` which calls','line_number':476,'multiline':False]['text':' `self.lr_scheduler.get_last_lr()` and originally it'd fail on the very first step.','line_number':477,'multiline':False]['text':' XXX: for some reason the following check fails with zero3/fp16 and any/bf16 - not a','line_number':495,'multiline':False]['text':' broken but a different qualitative outcome - as if optimizer did run','line_number':496,'multiline':False]['text':' oddly getting 1.0 for both a and b from 0.0 - there is a bug somewhere','line_number':497,'multiline':False]['text':' print(trainer.model.a.item())','line_number':498,'multiline':False]['text':' print(trainer.model.b.item())','line_number':499,'multiline':False]['text':' need to investigate at some point','line_number':500,'multiline':False]['text':' it's enough that train didn't fail for this test, but we must check that','line_number':504,'multiline':False]['text':' optimizer/scheduler didn't run (since if it did this test isn't testing the right thing)','line_number':505,'multiline':False]['text':' this test measures that we get identical weights and similar loss with:','line_number':510,'multiline':False]['text':' 1. per_device_train_batch_size=8, gradient_accumulation_steps=1','line_number':511,'multiline':False]['text':' 2. per_device_train_batch_size=4, gradient_accumulation_steps=2','line_number':512,'multiline':False]['text':' since the 2nd should produce the effective batch of 1st, with the same results','line_number':513,'multiline':False]['text':'','line_number':514,'multiline':False]['text':' I can get an identical loss for a small train_len=32, plus the power of the initial','line_number':515,'multiline':False]['text':' dynamic loss scale value set to:','line_number':516,'multiline':False]['text':'   "fp16.initial_scale_power": 1','line_number':517,'multiline':False]['text':' plus having the same WarmupLR's warmup_min_lr == warmup_max_lr in the config file','line_number':518,'multiline':False]['text':' but for some reason going to train_len=64 the weights, weights start to mismatch with this setup.','line_number':519,'multiline':False]['text':' the culprit seems to be `initial_scale_power` - putting it back to its default 32 keeps the weights identical','line_number':520,'multiline':False]['text':' make sure the optimizer kicked in - if it hasn't changed from the original value of a then make train_len bigger','line_number':544,'multiline':False]['text':' training with half the batch size but accumulation steps as 2 should give the same','line_number':559,'multiline':False]['text':' weights, but sometimes get a slight difference still of 1e-6','line_number':560,'multiline':False]['text':' Relative difference. See the note above how to get identical loss on a small bs','line_number':564,'multiline':False]['text':' adapted from TrainerIntegrationCommon.check_saved_checkpoints','line_number':568,'multiline':False]['text':' common files','line_number':584,'multiline':False]['text':' ds files','line_number':589,'multiline':False]['text':' filename = os.path.join(path, filename)','line_number':592,'multiline':False]['text':' print(filename)','line_number':593,'multiline':False]['text':' adapted from  TrainerIntegrationTest.test_save_checkpoints','line_number':599,'multiline':False]['text':' force optimizer on the first step','line_number':605,'multiline':False]['text':' XXX:','line_number':606,'multiline':False]['text':' save checkpoints','line_number':610,'multiline':False]['text':' 1. fail to find any checkpoint - due a fresh output_dir','line_number':633,'multiline':False]['text':' 2. fail to find a bogus checkpoint','line_number':641,'multiline':False]['text':' adapted from TrainerIntegrationTest.test_can_resume_training','line_number':651,'multiline':False]['text':' test normal resume for each stage separately, error-handling is tested in a different test','line_number':652,'multiline':False]['text':' ToDo: Currently, hf_optim + hf_scheduler resumes with the correct states and','line_number':654,'multiline':False]['text':' also has same losses for few steps but then slowly diverges. Need to figure it out.','line_number':655,'multiline':False]['text':' force optimizer on the first step','line_number':662,'multiline':False]['text':' XXX:','line_number':663,'multiline':False]['text':' Reinitialize trainer','line_number':690,'multiline':False]['text':' Now check with a later checkpoint that it also works when we span over one epoch','line_number':700,'multiline':False]['text':' Reinitialize trainer and load model','line_number':703,'multiline':False]['text':' Finally, should be able to resume with the same trainer/same deepspeed engine instance','line_number':713,'multiline':False]['text':' XXX: but currently this not possible due DS bug: https://github.com/microsoft/DeepSpeed/issues/1612','line_number':714,'multiline':False]['text':' trainer.train(resume_from_checkpoint=checkpoint)','line_number':715,'multiline':False]['text':' a workaround needs to be used that re-creates the deepspeed engine','line_number':716,'multiline':False]['text':' test that we can load fp32 weights directly from the zero checkpoint into the current model','line_number':720,'multiline':False]['text':' "./xxx", after=False, before=False)','line_number':722,'multiline':False]['text':' test that we can switch from zero2 to zero3 in the same process for example','line_number':754,'multiline':False]['text':' test is_zero, etc.','line_number':755,'multiline':False]['text':' test we can repeat that and with train this time','line_number':766,'multiline':False]['text':' test zero3 is disabled','line_number':771,'multiline':False]['text':' check config obj','line_number':775,'multiline':False]['text':' with accelerate integration below line is additionally required for this test to pass','line_number':779,'multiline':False]['text':' now weakref should gc the global and we shouldn't get anything here','line_number':782,'multiline':False]['text':' Test that forced deepspeed reinit doesn't break the model. the forced re-init after','line_number':789,'multiline':False]['text':' loading the best model in Trainer is there to workaround this bug in Deepspeed','line_number':790,'multiline':False]['text':' https://github.com/microsoft/DeepSpeed/issues/1612','line_number':791,'multiline':False]['text':'','line_number':792,'multiline':False]['text':' The test is derived from a repro script submitted in this Issue:','line_number':793,'multiline':False]['text':' https://github.com/huggingface/transformers/issues/17114','line_number':794,'multiline':False]['text':'','line_number':795,'multiline':False]['text':' One additional feature of this test is that we use a non-AdamW optimizer to test that','line_number':796,'multiline':False]['text':' deepspeed doesn't fallback to AdamW, which would prevent the optimizer states from loading','line_number':797,'multiline':False]['text':' correctly','line_number':798,'multiline':False]['text':' noqa','line_number':800,'multiline':False]['text':' "./xxx", after=False, before=False)','line_number':802,'multiline':False]['text':' will use HF Trainer optimizer','line_number':805,'multiline':False]['text':' will use HF Trainer scheduler','line_number':806,'multiline':False]['text':' offload is not efficient w/o CPUAdam','line_number':807,'multiline':False]['text':' must use this setting to get the reload path exercised','line_number':808,'multiline':False]['text':' crash 1 was here','line_number':873,'multiline':False]['text':' crash 2 was here','line_number':874,'multiline':False]['text':' Tests to devise #','line_number':883,'multiline':False]['text':'','line_number':884,'multiline':False]['text':' 1. predict_with_generate on multigpu - need to figure out how to give input sequences so that','line_number':885,'multiline':False]['text':' the 2 gpus will generate prediction sequences that aren't of the same length - this is because','line_number':886,'multiline':False]['text':' we had to code a special feature to sync the gpus when the predicted sequences aren't of the','line_number':887,'multiline':False]['text':' same length. In general this will tested as a side-effect through a variety of other tests -','line_number':888,'multiline':False]['text':' it'll simply hang trying to synchronize with other gpus if this problem is encountered. So as','line_number':889,'multiline':False]['text':' long as we have a few full tests running on zero3 + predict_with_generate this should be','line_number':890,'multiline':False]['text':' mostly covered.','line_number':891,'multiline':False]['text':'','line_number':892,'multiline':False]['text':' but there are 5 variations on beam search in `generate`- with identical code branched with `if','line_number':893,'multiline':False]['text':' synced_gpus`','line_number':894,'multiline':False]['text':'','line_number':895,'multiline':False]['text':' 2. most tests should probably be run on both: zero2 and zero3 configs','line_number':896,'multiline':False]['text':'','line_number':897,'multiline':False]['text':' testing only zero3 since zero2 makes no sense with inference','line_number':905,'multiline':False]['text':' real model needs too much GPU memory under stage2+fp32, so using tiny random model here -','line_number':917,'multiline':False]['text':' therefore no quality checks, just basic completion checks are done','line_number':918,'multiline':False]['text':' real model needs too much GPU memory under stage2+fp32, so using tiny random model here -','line_number':933,'multiline':False]['text':' therefore no quality checks, just basic completion checks are done','line_number':934,'multiline':False]['text':' do normal training and then resume not from the deepspeed checkpoint but explicitly from','line_number':948,'multiline':False]['text':' the saved model dir','line_number':949,'multiline':False]['text':' 1. normal training','line_number':962,'multiline':False]['text':' 2. now resume explicitly from the saved weights, by passing --model_name_or_path output_dir','line_number':965,'multiline':False]['text':' - i.e. the same path the model was saved to in step 1','line_number':966,'multiline':False]['text':' this is just inference, so no optimizer should be loaded','line_number':977,'multiline':False]['text':' it only works for z3 (makes no sense with z1-z2)','line_number':978,'multiline':False]['text':' XXX: need to do better validation beyond just that the run was successful','line_number':1004,'multiline':False]['text':' we are doing quality testing so using a small real model','line_number':1019,'multiline':False]['text':' currently only works for bool args','line_number':1107,'multiline':False]['text':' keep for quick debug','line_number':1117,'multiline':False]['text':' print(" ".join([f"\nPYTHONPATH={self.src_dir_str}"] +cmd)); die','line_number':1118,'multiline':False]['text':' this test exercises model.resize_token_embeddings() which requires param gathering outside','line_number':1125,'multiline':False]['text':' of forward - it's not used by `run_translation.py`, but it is in `run_clm.py`','line_number':1126,'multiline':False]['text':' keep for quick debug','line_number':1155,'multiline':False]['text':' print(" ".join([f"\nPYTHONPATH={self.src_dir_str}"] +cmd)); die','line_number':1156,'multiline':False]['text':' this test exercises AutoModel.from_config(config) - to ensure zero.Init is called','line_number':1160,'multiline':False]['text':' keep for quick debug','line_number':1186,'multiline':False]['text':' print(" ".join([f"\nPYTHONPATH={self.src_dir_str}"] +cmd)); die','line_number':1187,'multiline':False]