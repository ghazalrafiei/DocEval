['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' noqa','line_number':22,'multiline':False]['text':' noqa','line_number':37,'multiline':False]['text':' default torch.distributed port','line_number':50,'multiline':False]['text':' *** Working Models ***','line_number':55,'multiline':False]['text':' hf tiny model is unsuitable','line_number':76,'multiline':False]['text':' *** To Fix ***','line_number':92,'multiline':False]['text':' *** tiny model issues ***','line_number':95,'multiline':False]['text':' missing model files:','line_number':96,'multiline':False]['text':' missing tokenizer files','line_number':101,'multiline':False]['text':' issues with tokenizer','line_number':106,'multiline':False]['text':' same as ctrl','line_number':108,'multiline':False]['text':' other issues with tiny models','line_number':110,'multiline':False]['text':' multiple issues with either mlm/qa/clas','line_number':111,'multiline':False]['text':' multiple issues with either mlm/qa/clas','line_number':112,'multiline':False]['text':' *** Lacking official examples to test with ***','line_number':114,'multiline':False]['text':' or not working with examples','line_number':115,'multiline':False]['text':' - "dpr"  examples/research_projects/rag-end2end-retriever/','line_number':117,'multiline':False]['text':' - "rag" research_projects','line_number':119,'multiline':False]['text':' - "luke" Entities classes - no plan to make such example','line_number':121,'multiline':False]['text':' - "lxmert" doesn't work with run_qa.py','line_number':123,'multiline':False]['text':' - "clip" nothing under pytorch examples - XXX: Suraj is working on adding some - check by end of Sep','line_number':125,'multiline':False]['text':' - "speech_to_text", nothing under pytorch examples','line_number':127,'multiline':False]['text':' *** Reactive mode ***','line_number':130,'multiline':False]['text':' models with low usage, unstable API, things about to change - do nothing about the following until someone runs into a problem','line_number':131,'multiline':False]['text':' additional notes on tapas','line_number':133,'multiline':False]['text':' 1. "Table must be of type pd.DataFrame" failure','line_number':134,'multiline':False]['text':' TODO: new models to add:','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':' 1. explicitly set --num_nodes=1 just in case these tests end up run on a multi-node setup','line_number':142,'multiline':False]['text':' - it won't be able to handle that','line_number':143,'multiline':False]['text':' 2. for now testing with just 2 gpus max (since some quality tests may give different','line_number':144,'multiline':False]['text':' results with mode gpus because we use very little data)','line_number':145,'multiline':False]['text':' try to cover as many models as possible once (it's enough to run on one task per model)','line_number':165,'multiline':False]['text':' but need a tiny model for each','line_number':166,'multiline':False]['text':'','line_number':167,'multiline':False]['text':' should have "{model_type.upper()}_TINY" corresponding vars defined, e.g., T5_TINY, etc.','line_number':168,'multiline':False]['text':' "mt5", missing model files','line_number':178,'multiline':False]['text':' "camembert", missing model files','line_number':193,'multiline':False]['text':' "reformer", # multiple issues with either mlm/qa/clas','line_number':204,'multiline':False]['text':' "convbert", # missing tokenizer files','line_number':213,'multiline':False]['text':' "layoutlmv2", missing model files','line_number':214,'multiline':False]['text':' "hubert", # missing tokenizer files','line_number':219,'multiline':False]['text':' "ibert", # multiple issues with either mlm/qa/clas','line_number':220,'multiline':False]['text':' "transfo-xl", # tokenizer issues as ctrl','line_number':221,'multiline':False]['text':' "ctrl", # tokenizer issues','line_number':222,'multiline':False]['text':' "openai-gpt", missing model files','line_number':223,'multiline':False]['text':' "tapas", multiple issues','line_number':224,'multiline':False]['text':' # generation special case','line_number':285,'multiline':False]['text':' if task == "gen":','line_number':286,'multiline':False]['text':'     launcher = f"deepspeed --num_nodes 1 --num_gpus 1".split()','line_number':287,'multiline':False]['text':'     args_model += f"--model_type {model}".split()','line_number':288,'multiline':False]['text':'     cmds[f"{task}_{model}"] = launcher + args + args_model','line_number':289,'multiline':False]['text':' else:','line_number':290,'multiline':False]['text':' future preparation:','line_number':302,'multiline':False]['text':' for now test just fp16, as these tests are quite slow','line_number':303,'multiline':False]['text':' FP16 = "fp16"','line_number':304,'multiline':False]['text':' BF16 = "bf16"','line_number':305,'multiline':False]['text':'','line_number':306,'multiline':False]['text':' dtypes = [FP16]','line_number':307,'multiline':False]['text':' so just hardcoding --fp16 for now','line_number':308,'multiline':False]['text':' if is_torch_bf16_gpu_available():','line_number':309,'multiline':False]['text':'     dtypes += [BF16]','line_number':310,'multiline':False]['text':' customize the test name generator function as we want both params to appear in the sub-test','line_number':314,'multiline':False]['text':' name, as by default it shows only the first param','line_number':315,'multiline':False]['text':' Cartesian-product of zero stages with models to test','line_number':320,'multiline':False]['text':' return a ready to run train cmd','line_number':331,'multiline':False]['text':' testing the ability to do a run followed by recovery of full fp32 weights','line_number':347,'multiline':False]['text':' 1. generate the checkpoint','line_number':351,'multiline':False]['text':' keep for quick debug','line_number':353,'multiline':False]['text':' print(" ".join([f"\nPYTHONPATH={self.src_dir_str}"] + cmd)); die','line_number':354,'multiline':False]['text':' 2. test that the fp32 weights get reconsolidated','line_number':357,'multiline':False]['text':' keep for quick debug','line_number':361,'multiline':False]['text':' print(" ".join([f"\nPYTHONPATH={self.src_dir_str}"] +cmd)); die','line_number':362,'multiline':False]['text':' possibly could also test that the resulting saved model is usable but given that we use','line_number':366,'multiline':False]['text':' random models we won't know if it's any good','line_number':367,'multiline':False]