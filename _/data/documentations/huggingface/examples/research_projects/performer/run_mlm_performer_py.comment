['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 The HuggingFace Team All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.','line_number':27,'multiline':False]['text':' Cache the result','line_number':57,'multiline':False]['text':' Adapted from transformers/data/data_collator.py','line_number':198,'multiline':False]['text':' Letting here for now, let's discuss where it should live','line_number':199,'multiline':False]['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':236,'multiline':False]['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':239,'multiline':False]['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':259,'multiline':False]['text':' We only compute loss on masked tokens','line_number':265,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':267,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':271,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':278,'multiline':False]['text':' Hide away tokens which doesn't participate in the optimization','line_number':406,'multiline':False]['text':' Hide away tokens which doesn't participate in the optimization','line_number':429,'multiline':False]['text':' See all possible arguments in src/transformers/training_args.py','line_number':448,'multiline':False]['text':' or by passing the --help flag to this script.','line_number':449,'multiline':False]['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':450,'multiline':False]['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':454,'multiline':False]['text':' let's parse it to get our arguments.','line_number':455,'multiline':False]['text':' Setup logging','line_number':473,'multiline':False]['text':' Log on each process the small summary:','line_number':480,'multiline':False]['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':487,'multiline':False]['text':' Set seed before initializing model.','line_number':490,'multiline':False]['text':' Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)','line_number':493,'multiline':False]['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':494,'multiline':False]['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':495,'multiline':False]['text':'','line_number':496,'multiline':False]['text':' For CSV/JSON files, this script will use the column called 'text' or the first column if no column called','line_number':497,'multiline':False]['text':' 'text' is found. You can easily tweak this behavior (see below).','line_number':498,'multiline':False]['text':'','line_number':499,'multiline':False]['text':' In distributed training, the load_dataset function guarantees that only one local process can concurrently','line_number':500,'multiline':False]['text':' download the dataset.','line_number':501,'multiline':False]['text':' Downloading and loading a dataset from the hub.','line_number':503,'multiline':False]['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':526,'multiline':False]['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':527,'multiline':False]['text':' Load pretrained model and tokenizer','line_number':529,'multiline':False]['text':' Distributed training:','line_number':531,'multiline':False]['text':' The .from_pretrained methods guarantee that only one local process can concurrently','line_number':532,'multiline':False]['text':' download model & vocab.','line_number':533,'multiline':False]['text':' Preprocessing the datasets.','line_number':565,'multiline':False]['text':' First we tokenize all the texts.','line_number':566,'multiline':False]['text':' Remove empty lines','line_number':576,'multiline':False]['text':' Enable tensorboard only on the master node','line_number':595,'multiline':False]['text':' Data collator','line_number':599,'multiline':False]['text':' This one will take care of randomly masking the tokens.','line_number':600,'multiline':False]['text':' Setup optimizer','line_number':603,'multiline':False]['text':' Create learning rate scheduler','line_number':611,'multiline':False]['text':' Create parallel version of the training and evaluation steps','line_number':616,'multiline':False]['text':' Replicate the optimizer on each device','line_number':620,'multiline':False]['text':' Store some constant','line_number':623,'multiline':False]['text':' ======================== Training ================================','line_number':635,'multiline':False]['text':' Create sampling rng','line_number':636,'multiline':False]['text':' Generate an epoch by shuffling sampling indices from the train dataset','line_number':639,'multiline':False]['text':' Avoid using jax.numpy here in case of TPU training','line_number':641,'multiline':False]['text':' Gather the indexes for creating the batch and do a training step','line_number':645,'multiline':False]['text':' Model forward','line_number':650,'multiline':False]['text':' ======================== Evaluating ==============================','line_number':659,'multiline':False]['text':' Avoid using jax.numpy here in case of TPU training','line_number':661,'multiline':False]['text':' Model forward','line_number':670,'multiline':False]['text':' Update progress bar','line_number':680,'multiline':False]['text':' Save metrics','line_number':688,'multiline':False]