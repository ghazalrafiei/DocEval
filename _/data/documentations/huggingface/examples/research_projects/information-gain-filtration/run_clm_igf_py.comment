['text':' Copyright 2022 - Intel Corp. All rights reserved.','line_number':1,'multiline':False]['text':' Authors: Mayank Kumar Raunak, Javier Turek, Nicole Beckage','line_number':2,'multiline':False]['text':' Prerequisite libraries:','line_number':26,'multiline':False]['text':' generates same data everytime','line_number':74,'multiline':False]['text':' generate train_data and objective_set','line_number':76,'multiline':False]['text':' keeps model same across runs','line_number':80,'multiline':False]['text':' model, lm_optimizer, lm_scheduler = recopy_gpt2(model, device, max_steps) # store original model weights','line_number':82,'multiline':False]['text':' can we train on GPU?','line_number':83,'multiline':False]['text':' load pretrained model','line_number':86,'multiline':False]['text':' collect igf pairs and save to file demo.jbl','line_number':92,'multiline':False]['text':' clean up, delete model and data we don't need anymore','line_number':95,'multiline':False]['text':' Load pre-trained model','line_number':123,'multiline':False]['text':' Initialize secondary learner to use embedding weights of model','line_number':126,'multiline':False]['text':' Train secondary learner','line_number':129,'multiline':False]['text':' Compute the performance of the transformer model at the beginning','line_number':201,'multiline':False]['text':' Here we implement the simple non-constant threshold for the predicted IG(X) value','line_number':220,'multiline':False]['text':' We will decay the selectivity of our secondary learner filter from','line_number':221,'multiline':False]['text':' 1 standard deviation above average to 1 below average after 10 batches.','line_number':222,'multiline':False]['text':' If we passed the filter, add the context to the batch!','line_number':229,'multiline':False]['text':' Once the batch is filled with enough contexts, backprop on the batch.','line_number':238,'multiline':False]['text':' Do LM backprop','line_number':242,'multiline':False]['text':' Update learning rate schedule','line_number':245,'multiline':False]['text':' Compute the performance of the transformer model at this batch','line_number':247,'multiline':False]['text':' Break out of the loop after 60 batches','line_number':253,'multiline':False]['text':' save finetuned transformer model','line_number':259,'multiline':False]['text':' Do some cleaning up so we can reinitialize for the next run of this function','line_number':262,'multiline':False]['text':' Required parameters','line_number':271,'multiline':False]['text':' function calls','line_number':395,'multiline':False]['text':' Collecting *n* pairs of context and information gain(X, IG(X)) for training the secondary learner','line_number':396,'multiline':False]['text':' Load train data for secondary learner','line_number':407,'multiline':False]['text':' Train secondary learner','line_number':410,'multiline':False]['text':' load pretrained gpt2 model','line_number':419,'multiline':False]['text':' Generate train and test data to train and evaluate gpt2 model','line_number':423,'multiline':False]['text':' fine-tuning of the gpt2 model using igf (Information Gain Filtration)','line_number':428,'multiline':False]