['text':'#####################################','line_number':52,'multiline':False]['text':'#####################################','line_number':54,'multiline':False]['text':' The dataset needed for RAG must have three columns:','line_number':56,'multiline':False]['text':' - title (string): title of the document','line_number':57,'multiline':False]['text':' - text (string): text of a passage of the document','line_number':58,'multiline':False]['text':' - embeddings (array of dimension d): DPR representation of the passage','line_number':59,'multiline':False]['text':' Let's say you have documents in tab-separated csv files with columns "title" and "text"','line_number':60,'multiline':False]['text':' You can load a Dataset object this way','line_number':63,'multiline':False]['text':' More info about loading csv files in the documentation: https://huggingface.co/docs/datasets/loading_datasets?highlight=csv#csv-files','line_number':68,'multiline':False]['text':' Then split the documents into passages of 100 words','line_number':70,'multiline':False]['text':' And compute the embeddings','line_number':73,'multiline':False]['text':' optional, save as float32 instead of float64 to save space','line_number':78,'multiline':False]['text':' And finally save your dataset','line_number':86,'multiline':False]['text':' from datasets import load_from_disk','line_number':89,'multiline':False]['text':' dataset = load_from_disk(passages_path)  # to reload the dataset','line_number':90,'multiline':False]['text':'#####################################','line_number':92,'multiline':False]['text':'#####################################','line_number':94,'multiline':False]['text':' Let's use the Faiss implementation of HNSW for fast approximate nearest neighbor search','line_number':96,'multiline':False]['text':' And save the index','line_number':100,'multiline':False]['text':' dataset.load_faiss_index("embeddings", index_path)  # to reload the index','line_number':103,'multiline':False]