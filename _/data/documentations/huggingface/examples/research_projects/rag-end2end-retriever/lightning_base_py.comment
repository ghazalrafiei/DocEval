['text':' update this and the import above to support new schedulers from transformers.optimization','line_number':50,'multiline':False]['text':' '': get_constant_schedule,             # not supported for now','line_number':56,'multiline':False]['text':' '': get_constant_schedule_with_warmup, # not supported for now','line_number':57,'multiline':False]['text':' TODO: move to self.save_hyperparameters()','line_number':76,'multiline':False]['text':' self.save_hyperparameters()','line_number':77,'multiline':False]['text':' can also expand arguments into trainer signature for easier reading','line_number':78,'multiline':False]['text':' check this named paramters','line_number':137,'multiline':False]['text':' TODO: consider num_tpu_cores','line_number':168,'multiline':False]['text':' this process can also be done with PL ddp plugging.','line_number':272,'multiline':False]['text':' But still it is experimental (check original RAG, I updated that with pluggin (shamanez))','line_number':273,'multiline':False]['text':' we initialize the retriever only on master worker with RAY. In new pytorch-lightning accelorators are removed.','line_number':277,'multiline':False]['text':' better to use hook functions.','line_number':278,'multiline':False]['text':' check whether new added model paramters are differentiable','line_number':282,'multiline':False]['text':' print(pl_module.model.rag)','line_number':284,'multiline':False]['text':' Log results','line_number':299,'multiline':False]['text':' Log and save results to file','line_number':307,'multiline':False]['text':'  To allow all pl args uncomment the following line','line_number':317,'multiline':False]['text':'  parser = pl.Trainer.add_argparse_args(parser)','line_number':318,'multiline':False]['text':' can pass WandbLogger() here','line_number':364,'multiline':False]['text':' init model','line_number':372,'multiline':False]['text':' add custom checkpoints','line_number':376,'multiline':False]