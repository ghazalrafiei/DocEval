['text':'!/usr/bin/env python','line_number':1,'multiline':False]['text':' coding=utf-8','line_number':2,'multiline':False]['text':' Copyright 2021 The HuggingFace Team All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' Skip incomplete batch.','line_number':191,'multiline':False]['text':' See all possible arguments in src/transformers/training_args.py','line_number':230,'multiline':False]['text':' or by passing the --help flag to this script.','line_number':231,'multiline':False]['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':232,'multiline':False]['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':236,'multiline':False]['text':' let's parse it to get our arguments.','line_number':237,'multiline':False]['text':' Make one log on every process with the configuration for debugging.','line_number':253,'multiline':False]['text':' Setup logging, we only want one process per machine to log things on the screen.','line_number':259,'multiline':False]['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':268,'multiline':False]['text':' Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)','line_number':271,'multiline':False]['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':272,'multiline':False]['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':273,'multiline':False]['text':'','line_number':274,'multiline':False]['text':' For CSV/JSON files, this script will use the column called 'text' or the first column if no column called','line_number':275,'multiline':False]['text':' 'text' is found. You can easily tweak this behavior (see below).','line_number':276,'multiline':False]['text':' Downloading and loading a dataset from the hub.','line_number':278,'multiline':False]['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':306,'multiline':False]['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':307,'multiline':False]['text':' Load pretrained config and tokenizer','line_number':309,'multiline':False]['text':' since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function','line_number':338,'multiline':False]['text':' clm input could be much much longer than block_size','line_number':344,'multiline':False]['text':' Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.','line_number':376,'multiline':False]['text':' Concatenate all texts.','line_number':378,'multiline':False]['text':' We drop the small remainder, we could add padding if the model supported it instead of this drop, you can','line_number':381,'multiline':False]['text':' customize this part to your needs.','line_number':382,'multiline':False]['text':' Split by chunks of max_len.','line_number':385,'multiline':False]['text':' Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder','line_number':393,'multiline':False]['text':' for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower','line_number':394,'multiline':False]['text':' to preprocess.','line_number':395,'multiline':False]['text':'','line_number':396,'multiline':False]['text':' To speed up this part, we use multiprocessing. See the documentation of the map method for more information:','line_number':397,'multiline':False]['text':' https://huggingface.co/docs/datasets/process#map','line_number':398,'multiline':False]['text':' Enable tensorboard only on the master node','line_number':423,'multiline':False]['text':' Initialize our training','line_number':441,'multiline':False]['text':' Store some constant','line_number':445,'multiline':False]['text':' TODO: weights should be initialized in pjitted fun, this won't work for REALLY large models','line_number':452,'multiline':False]['text':' TODO: when loading from pre-trained model we need to make sure the vocab is divisible by num_partitions','line_number':453,'multiline':False]['text':' GPT2's vocab is odd, we need to resize it for fine-tuning','line_number':454,'multiline':False]['text':' Create learning rate schedule','line_number':459,'multiline':False]['text':' Get PartitionSpec for model params','line_number':480,'multiline':False]['text':' Get the PyTree for opt_state, we don't actually initialize the opt_state yet.','line_number':483,'multiline':False]['text':' get PartitionSpec for opt_state, this is very specific to adamw','line_number':487,'multiline':False]['text':' TODO: optax returns different state for different optimizers, how can we handle this generically ?','line_number':488,'multiline':False]['text':' or maybe we don't since in our examples we just use adamw or adafactor','line_number':489,'multiline':False]['text':' pjit the get_initial_state function to shard params and init','line_number':499,'multiline':False]['text':' optimizer state in sharded way','line_number':500,'multiline':False]['text':' hack: move the inital params to CPU to free up device memory','line_number':507,'multiline':False]['text':' TODO: allow loading weights on CPU in pre-trained model','line_number':508,'multiline':False]['text':' mesh defination','line_number':511,'multiline':False]['text':' actually initialize the opt_state','line_number':514,'multiline':False]['text':' cross-entropy with z loss','line_number':518,'multiline':False]['text':' Define gradient update step fn','line_number':534,'multiline':False]['text':' TODO: try to use TrainState instead of passing params and opt_state individually','line_number':535,'multiline':False]['text':' Define eval fn','line_number':554,'multiline':False]['text':' metrics','line_number':558,'multiline':False]['text':' we are not doing 2D parallelism (yet!), this just does model parallelism','line_number':585,'multiline':False]['text':' ======================== Training ================================','line_number':588,'multiline':False]['text':' Create sampling rng','line_number':591,'multiline':False]['text':' Generate an epoch by shuffling sampling indices from the train dataset','line_number':594,'multiline':False]['text':' train','line_number':599,'multiline':False]['text':' Save metrics','line_number':614,'multiline':False]['text':' ======================== Evaluating ==============================','line_number':627,'multiline':False]['text':' normalize eval metrics','line_number':637,'multiline':False]['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':652,'multiline':False]