['text':' XLM and RoBERTa don"t use token_type_ids','line_number':52,'multiline':False]['text':' tensorboard_logs = {"loss": loss, "rate": self.lr_scheduler.get_last_lr()[-1]}','line_number':56,'multiline':False]['text':' HACK(we will not use this anymore soon)','line_number':99,'multiline':False]['text':' XLM and RoBERTa don"t use token_type_ids','line_number':111,'multiline':False]['text':' when stable','line_number':148,'multiline':False]['text':' updating to test_epoch_end instead of deprecated test_end','line_number':154,'multiline':False]['text':' Converting to the dict required by pl','line_number':157,'multiline':False]['text':' https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\','line_number':158,'multiline':False]['text':' pytorch_lightning/trainer/logging.py#L139','line_number':159,'multiline':False]['text':' `val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`','line_number':161,'multiline':False]['text':' Add NER specific options','line_number':166,'multiline':False]['text':' See https://github.com/huggingface/transformers/issues/3159','line_number':210,'multiline':False]['text':' pl use this default format to create a checkpoint:','line_number':211,'multiline':False]['text':' https://github.com/PyTorchLightning/pytorch-lightning/blob/master\','line_number':212,'multiline':False]['text':' /pytorch_lightning/callbacks/model_checkpoint.py#L322','line_number':213,'multiline':False]