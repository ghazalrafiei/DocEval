['text':'!/usr/bin/env python','line_number':1,'multiline':False]['text':' coding=utf-8','line_number':2,'multiline':False]['text':' Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.','line_number':19,'multiline':False]['text':' Will error if the minimal version of Transformers is not installed. Remove at your own risks.','line_number':55,'multiline':False]['text':' A list of all multilingual tokenizer which require src_lang and tgt_lang attributes.','line_number':62,'multiline':False]['text':' accepting both json and jsonl file extensions, as','line_number':254,'multiline':False]['text':' many jsonlines files actually have a .json extension','line_number':255,'multiline':False]['text':' See all possible arguments in src/transformers/training_args.py','line_number':269,'multiline':False]['text':' or by passing the --help flag to this script.','line_number':270,'multiline':False]['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':271,'multiline':False]['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':275,'multiline':False]['text':' let's parse it to get our arguments.','line_number':276,'multiline':False]['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':290,'multiline':False]['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':291,'multiline':False]['text':' Setup logging','line_number':294,'multiline':False]['text':' The default of training_args.log_level is passive, so we set log level at info here to have that default.','line_number':302,'multiline':False]['text':' Log on each process the small summary:','line_number':312,'multiline':False]['text':' Detecting last checkpoint.','line_number':331,'multiline':False]['text':' Set seed before initializing model.','line_number':346,'multiline':False]['text':' Get the datasets: you can either provide your own JSON training and evaluation files (see below)','line_number':349,'multiline':False]['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':350,'multiline':False]['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':351,'multiline':False]['text':'','line_number':352,'multiline':False]['text':' For translation, only JSON files are supported, with one field named "translation" containing two keys for the','line_number':353,'multiline':False]['text':' source and target languages (unless you adapt what follows).','line_number':354,'multiline':False]['text':'','line_number':355,'multiline':False]['text':' In distributed training, the load_dataset function guarantee that only one local process can concurrently','line_number':356,'multiline':False]['text':' download the dataset.','line_number':357,'multiline':False]['text':' Downloading and loading a dataset from the hub.','line_number':359,'multiline':False]['text':' the "json" builder reads both .json and .jsonl files','line_number':378,'multiline':False]['text':' e.g. "parquet"','line_number':380,'multiline':False]['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':387,'multiline':False]['text':' https://huggingface.co/docs/datasets/loading.','line_number':388,'multiline':False]['text':' Load pretrained model and tokenizer','line_number':390,'multiline':False]['text':'','line_number':391,'multiline':False]['text':' Distributed training:','line_number':392,'multiline':False]['text':' The .from_pretrained methods guarantee that only one local process can concurrently','line_number':393,'multiline':False]['text':' download model & vocab.','line_number':394,'multiline':False]['text':' We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch','line_number':420,'multiline':False]['text':' on a small vocab and want a smaller embedding size, remove this test.','line_number':421,'multiline':False]['text':' Set decoder_start_token_id','line_number':426,'multiline':False]['text':' Preprocessing the datasets.','line_number':438,'multiline':False]['text':' We need to tokenize inputs and targets.','line_number':439,'multiline':False]['text':' For translation we set the codes of our source and target languages (only useful for mBART, the others will','line_number':450,'multiline':False]['text':' ignore those attributes).','line_number':451,'multiline':False]['text':' For multilingual translation models like mBART-50 and M2M100 we need to force the target language token','line_number':461,'multiline':False]['text':' as the first generated token. We ask the user to explicitly provide this as --forced_bos_token argument.','line_number':462,'multiline':False]['text':' Get the language codes for input/target.','line_number':468,'multiline':False]['text':' Temporarily set max_target_length for training.','line_number':472,'multiline':False]['text':' Tokenize targets with the `text_target` keyword argument','line_number':488,'multiline':False]['text':' If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore','line_number':491,'multiline':False]['text':' padding in the loss.','line_number':492,'multiline':False]['text':' Data collator','line_number':554,'multiline':False]['text':' Metric','line_number':566,'multiline':False]['text':' Replace -100s used for padding as we can't decode them','line_number':579,'multiline':False]['text':' Some simple post-processing','line_number':585,'multiline':False]['text':' Initialize our Trainer','line_number':596,'multiline':False]['text':' Training','line_number':607,'multiline':False]['text':' Saves the tokenizer too for easy upload','line_number':615,'multiline':False]['text':' Evaluation','line_number':627,'multiline':False]['text':' For xla_spawn (TPUs)','line_number':694,'multiline':False]