['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 The HuggingFace Team All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Build a map example to its corresponding features.','line_number':83,'multiline':False]['text':' The dictionaries we have to fill.','line_number':89,'multiline':False]['text':' Logging.','line_number':95,'multiline':False]['text':' Let's loop over all the examples!','line_number':99,'multiline':False]['text':' Those are the indices of the features associated to the current example.','line_number':101,'multiline':False]['text':' Looping through all the features associated to the current example.','line_number':107,'multiline':False]['text':' We grab the predictions of the model for this feature.','line_number':109,'multiline':False]['text':' This is what will allow us to map some the positions in our logits to span of texts in the original','line_number':112,'multiline':False]['text':' context.','line_number':113,'multiline':False]['text':' Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context','line_number':115,'multiline':False]['text':' available in the current feature.','line_number':116,'multiline':False]['text':' Update minimum null prediction.','line_number':119,'multiline':False]['text':' Go through all possibilities for the `n_best_size` greater start and end logits.','line_number':129,'multiline':False]['text':' Don't consider out-of-scope answers, either because the indices are out of bounds or correspond','line_number':134,'multiline':False]['text':' to part of the input_ids that are not in the context.','line_number':135,'multiline':False]['text':' Don't consider answers with a length that is either < 0 or > max_answer_length.','line_number':145,'multiline':False]['text':' Don't consider answer that don't have the maximum context available (if such information is','line_number':148,'multiline':False]['text':' provided).','line_number':149,'multiline':False]['text':' Add the minimum null prediction','line_number':162,'multiline':False]['text':' Only keep the best `n_best_size` predictions.','line_number':166,'multiline':False]['text':' Add back the minimum null prediction if it was removed because of its low score.','line_number':169,'multiline':False]['text':' Use the offsets to gather the answer text in the original context.','line_number':177,'multiline':False]['text':' In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid','line_number':183,'multiline':False]['text':' failure.','line_number':184,'multiline':False]['text':' Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using','line_number':188,'multiline':False]['text':' the LogSumExp trick).','line_number':189,'multiline':False]['text':' Include the probabilities in our predictions.','line_number':194,'multiline':False]['text':' Pick the best prediction. If the null answer is not possible, this is easy.','line_number':198,'multiline':False]['text':' Otherwise we first need to find the best non-empty prediction.','line_number':202,'multiline':False]['text':' Then we compare to the null prediction using the threshold.','line_number':208,'multiline':False]['text':' To be JSON-serializable.','line_number':210,'multiline':False]['text':' Make `predictions` JSON-serializable by casting np.float back to float.','line_number':216,'multiline':False]['text':' If we have an output_dir, let's save all those dicts.','line_number':222,'multiline':False]['text':' Build a map example to its corresponding features.','line_number':303,'multiline':False]['text':' The dictionaries we have to fill.','line_number':309,'multiline':False]['text':' Logging.','line_number':314,'multiline':False]['text':' Let's loop over all the examples!','line_number':318,'multiline':False]['text':' Those are the indices of the features associated to the current example.','line_number':320,'multiline':False]['text':' Looping through all the features associated to the current example.','line_number':326,'multiline':False]['text':' We grab the predictions of the model for this feature.','line_number':328,'multiline':False]['text':' This is what will allow us to map some the positions in our logits to span of texts in the original','line_number':334,'multiline':False]['text':' context.','line_number':335,'multiline':False]['text':' Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context','line_number':337,'multiline':False]['text':' available in the current feature.','line_number':338,'multiline':False]['text':' Update minimum null prediction','line_number':341,'multiline':False]['text':' Go through all possibilities for the `n_start_top`/`n_end_top` greater start and end logits.','line_number':345,'multiline':False]['text':' Don't consider out-of-scope answers (last part of the test should be unnecessary because of the','line_number':351,'multiline':False]['text':' p_mask but let's not take any risk)','line_number':352,'multiline':False]['text':' Don't consider answers with a length negative or > max_answer_length.','line_number':363,'multiline':False]['text':' Don't consider answer that don't have the maximum context available (if such information is','line_number':366,'multiline':False]['text':' provided).','line_number':367,'multiline':False]['text':' Only keep the best `n_best_size` predictions.','line_number':379,'multiline':False]['text':' Use the offsets to gather the answer text in the original context.','line_number':382,'multiline':False]['text':' In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid','line_number':388,'multiline':False]['text':' failure.','line_number':389,'multiline':False]['text':' Without predictions min_null_score is going to be None and None will cause an exception later','line_number':391,'multiline':False]['text':' Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using','line_number':395,'multiline':False]['text':' the LogSumExp trick).','line_number':396,'multiline':False]['text':' Include the probabilities in our predictions.','line_number':401,'multiline':False]['text':' Pick the best prediction and set the probability for the null answer.','line_number':405,'multiline':False]['text':' Make `predictions` JSON-serializable by casting np.float back to float.','line_number':410,'multiline':False]['text':' If we have an output_dir, let's save all those dicts.','line_number':416,'multiline':False]