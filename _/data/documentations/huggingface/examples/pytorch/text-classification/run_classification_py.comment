['text':'!/usr/bin/env python','line_number':1,'multiline':False]['text':' coding=utf-8','line_number':2,'multiline':False]['text':' Copyright 2020 The HuggingFace Inc. team. All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' You can also adapt this script on your own text classification task. Pointers for this are left as comments.','line_number':17,'multiline':False]['text':' Will error if the minimal version of Transformers is not installed. Remove at your own risks.','line_number':50,'multiline':False]['text':' we will treat the label list as a list of string instead of int, consistent with model.config.label2id','line_number':270,'multiline':False]['text':' See all possible arguments in src/transformers/training_args.py','line_number':276,'multiline':False]['text':' or by passing the --help flag to this script.','line_number':277,'multiline':False]['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':278,'multiline':False]['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':282,'multiline':False]['text':' let's parse it to get our arguments.','line_number':283,'multiline':False]['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':297,'multiline':False]['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':298,'multiline':False]['text':' Setup logging','line_number':301,'multiline':False]['text':' The default of training_args.log_level is passive, so we set log level at info here to have that default.','line_number':309,'multiline':False]['text':' Log on each process the small summary:','line_number':319,'multiline':False]['text':' Detecting last checkpoint.','line_number':326,'multiline':False]['text':' Set seed before initializing model.','line_number':341,'multiline':False]['text':' Get the datasets: you can either provide your own CSV/JSON training and evaluation files, or specify a dataset name','line_number':344,'multiline':False]['text':' to load from huggingface/datasets. In ether case, you can specify a the key of the column(s) containing the text and','line_number':345,'multiline':False]['text':' the key of the column containing the label. If multiple columns are specified for the text, they will be joined togather','line_number':346,'multiline':False]['text':' for the actual text value.','line_number':347,'multiline':False]['text':' In distributed training, the load_dataset function guarantee that only one local process can concurrently','line_number':348,'multiline':False]['text':' download the dataset.','line_number':349,'multiline':False]['text':' Downloading and loading a dataset from the hub.','line_number':351,'multiline':False]['text':' Try print some info about the dataset','line_number':358,'multiline':False]['text':' Loading a dataset from your local files.','line_number':362,'multiline':False]['text':' CSV/JSON training and evaluation files are needed.','line_number':363,'multiline':False]['text':' Get the test dataset: you can provide your own CSV/JSON test file','line_number':366,'multiline':False]['text':' Loading a dataset from local csv files','line_number':382,'multiline':False]['text':' Loading a dataset from local json files','line_number':390,'multiline':False]['text':' See more about loading any type of standard or custom dataset at','line_number':398,'multiline':False]['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':399,'multiline':False]['text':' Trying to have good defaults here, don't hesitate to tweak to your needs.','line_number':431,'multiline':False]['text':' regession requires float as label type, let's cast it if needed','line_number':443,'multiline':False]['text':' classification','line_number':459,'multiline':False]['text':' multi-label classification','line_number':460,'multiline':False]['text':' Trying to find the number of labels in a multi-label classification task','line_number':463,'multiline':False]['text':' We have to deal with common cases that labels appear in the training set but not in the validation/test set.','line_number':464,'multiline':False]['text':' So we build the label list from the union of labels in train/val/test.','line_number':465,'multiline':False]['text':' add the labels that appear in val/test but not in train, throw a warning','line_number':472,'multiline':False]['text':' if label is -1, we throw a warning and remove it from the label list','line_number':477,'multiline':False]['text':' Load pretrained model and tokenizer','line_number':488,'multiline':False]['text':' In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently','line_number':489,'multiline':False]['text':' download model & vocab.','line_number':490,'multiline':False]['text':' Padding strategy','line_number':530,'multiline':False]['text':' We will pad later, dynamically at batch creation, to the max sequence length in each batch','line_number':534,'multiline':False]['text':' for training ,we will update the config with label infos,','line_number':537,'multiline':False]['text':' if do_train is not set, we will use the label infos in the config','line_number':538,'multiline':False]['text':' classification, training','line_number':539,'multiline':False]['text':' update config with label infos','line_number':541,'multiline':False]['text':' classification, but not training','line_number':549,'multiline':False]['text':' regression','line_number':553,'multiline':False]['text':' BCELoss requires float as target type','line_number':564,'multiline':False]['text':' join together text columns into "sentence" column','line_number':572,'multiline':False]['text':' Tokenize the texts','line_number':577,'multiline':False]['text':' Running the preprocessing pipeline on all the datasets','line_number':586,'multiline':False]['text':' remove label column if it exists','line_number':624,'multiline':False]['text':' Log a few random samples from the training set:','line_number':629,'multiline':False]['text':' convert logits to multi-hot encoding','line_number':661,'multiline':False]['text':' Micro F1 is commonly used in multi-label classification','line_number':662,'multiline':False]['text':' Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if','line_number':671,'multiline':False]['text':' we already did the padding.','line_number':672,'multiline':False]['text':' Initialize our Trainer','line_number':680,'multiline':False]['text':' Training','line_number':691,'multiline':False]['text':' Saves the tokenizer too for easy upload','line_number':704,'multiline':False]['text':' Evaluation','line_number':709,'multiline':False]['text':' Removing the `label` columns if exists because it might contains -1 and Trainer won't like that.','line_number':720,'multiline':False]['text':' Convert logits to multi-hot encoding. We compare the logits to 0 instead of 0.5, because the sigmoid is not applied.','line_number':727,'multiline':False]['text':' You can also pass `preprocess_logits_for_metrics=lambda logits, labels: nn.functional.sigmoid(logits)` to the Trainer','line_number':728,'multiline':False]['text':' and set p > 0.5 below (less efficient in this case)','line_number':729,'multiline':False]['text':' recover from multi-hot encoding','line_number':742,'multiline':False]['text':' For xla_spawn (TPUs)','line_number':758,'multiline':False]