['text':'!/usr/bin/env python','line_number':1,'multiline':False]['text':' coding=utf-8','line_number':2,'multiline':False]['text':' Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.','line_number':3,'multiline':False]['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':4,'multiline':False]['text':'','line_number':5,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':6,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':7,'multiline':False]['text':' You may obtain a copy of the License at','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':10,'multiline':False]['text':'','line_number':11,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':12,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':13,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':14,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':15,'multiline':False]['text':' limitations under the License.','line_number':16,'multiline':False]['text':' Hardcoded max length to avoid infinite loop','line_number':62,'multiline':False]['text':' Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia','line_number':77,'multiline':False]['text':' in https://github.com/rusiaaman/XLNet-gen#methodology','line_number':78,'multiline':False]['text':' and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e','line_number':79,'multiline':False]['text':'','line_number':92,'multiline':False]['text':' Functions to prepare models' input','line_number':93,'multiline':False]['text':'','line_number':94,'multiline':False]['text':' kwargs = {"language": None, "mask_token_id": None}','line_number':108,'multiline':False]['text':' Set the language','line_number':110,'multiline':False]['text':' kwargs["language"] = tokenizer.lang2id[language]','line_number':122,'multiline':False]['text':' TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers','line_number':124,'multiline':False]['text':' XLM masked-language modeling (MLM) models need masked token','line_number':125,'multiline':False]['text':' is_xlm_mlm = "mlm" in args.model_name_or_path','line_number':126,'multiline':False]['text':' if is_xlm_mlm:','line_number':127,'multiline':False]['text':'     kwargs["mask_token_id"] = tokenizer.mask_token_id','line_number':128,'multiline':False]['text':' No generation bigger than model size','line_number':157,'multiline':False]['text':' avoid infinite loop','line_number':159,'multiline':False]['text':' Initialize the distributed state.','line_number':338,'multiline':False]['text':' Initialize the model and tokenizer','line_number':346,'multiline':False]['text':' Set the model to the right device','line_number':358,'multiline':False]['text':' Different models need different input formatting and/or extra arguments','line_number':369,'multiline':False]['text':' Remove the batch dimension when returning multiple sequences','line_number':421,'multiline':False]['text':' Decode text','line_number':431,'multiline':False]['text':' Remove all text after the stop token','line_number':434,'multiline':False]['text':' Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing','line_number':437,'multiline':False]