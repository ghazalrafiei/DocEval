['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' Make sure only the first process in distributed training processes the dataset,','line_number':70,'multiline':False]['text':' and the others will use the cache.','line_number':71,'multiline':False]['text':' Truncate in block of block_size','line_number':91,'multiline':False]['text':' Note that we are losing the last truncated example here for the sake of simplicity (no padding)','line_number':95,'multiline':False]['text':' If your dataset is small, first you should look for a bigger one :-) and second you','line_number':96,'multiline':False]['text':' can change this behavior by adding (model specific) padding.','line_number':97,'multiline':False]['text':' Here, we do not cache the features, operating under the assumption','line_number':127,'multiline':False]['text':' that we will soon use fast multithreaded tokenizers from the','line_number':128,'multiline':False]['text':' `tokenizers` repo everywhere =)','line_number':129,'multiline':False]['text':' Here, we do not cache the features, operating under the assumption','line_number':162,'multiline':False]['text':' that we will soon use fast multithreaded tokenizers from the','line_number':163,'multiline':False]['text':' `tokenizers` repo everywhere =)','line_number':164,'multiline':False]['text':' use this method to avoid delimiter '\u2029' to split a line','line_number':168,'multiline':False]['text':' Get ref inf from file','line_number':170,'multiline':False]['text':' TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)','line_number':210,'multiline':False]['text':' file path looks like ./dataset/wiki_1, ./dataset/wiki_2','line_number':211,'multiline':False]['text':' Account for special tokens','line_number':243,'multiline':False]['text':' We *usually* want to fill up the entire sequence since we are padding','line_number':246,'multiline':False]['text':' to `block_size` anyways, so short sequences are generally wasted','line_number':247,'multiline':False]['text':' computation. However, we *sometimes*','line_number':248,'multiline':False]['text':' (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter','line_number':249,'multiline':False]['text':' sequences to minimize the mismatch between pretraining and fine-tuning.','line_number':250,'multiline':False]['text':' The `target_seq_length` is just a rough target however, whereas','line_number':251,'multiline':False]['text':' `block_size` is a hard limit.','line_number':252,'multiline':False]['text':' We DON'T just concatenate all of the tokens from a document into a long','line_number':257,'multiline':False]['text':' sequence and choose an arbitrary split point because this would make the','line_number':258,'multiline':False]['text':' next sentence prediction task too easy. Instead, we split the input into','line_number':259,'multiline':False]['text':' segments "A" and "B" based on the actual "sentences" provided by the user','line_number':260,'multiline':False]['text':' input.','line_number':261,'multiline':False]['text':' a buffer stored current working segments','line_number':263,'multiline':False]['text':' get a segment','line_number':267,'multiline':False]['text':' add a segment to current chunk','line_number':271,'multiline':False]['text':' overall token length','line_number':272,'multiline':False]['text':' if current length goes to the target length or reaches the end of file, start building token a and b','line_number':273,'multiline':False]['text':' `a_end` is how many segments from `current_chunk` go into the `A` (first) sentence.','line_number':276,'multiline':False]['text':' if current chunk has more than 2 sentences, pick part of it `A` (first) sentence','line_number':278,'multiline':False]['text':' token a','line_number':281,'multiline':False]['text':' token b','line_number':286,'multiline':False]['text':' switch tokens_a and tokens_b randomly','line_number':294,'multiline':False]['text':' We want to sometimes truncate from the front and sometimes from the','line_number':310,'multiline':False]['text':' back to add more randomness and avoid biases.','line_number':311,'multiline':False]['text':' add special tokens','line_number':323,'multiline':False]['text':' add token type ids, 0 for sentence a, 1 for sentence b','line_number':325,'multiline':False]['text':' clear current chunk','line_number':334,'multiline':False]['text':' reset current text length','line_number':335,'multiline':False]['text':' go to next line','line_number':336,'multiline':False]['text':' Make sure only the first process in distributed training processes the dataset,','line_number':380,'multiline':False]['text':' and the others will use the cache.','line_number':381,'multiline':False]['text':' Input file format:','line_number':384,'multiline':False]['text':' (1) One sentence per line. These should ideally be actual sentences, not','line_number':385,'multiline':False]['text':' entire paragraphs or arbitrary spans of text. (Because we use the','line_number':386,'multiline':False]['text':' sentence boundaries for the "next sentence prediction" task).','line_number':387,'multiline':False]['text':' (2) Blank lines between documents. Document boundaries are needed so','line_number':388,'multiline':False]['text':' that the "next sentence prediction" task doesn't span between documents.','line_number':389,'multiline':False]['text':'','line_number':390,'multiline':False]['text':' Example:','line_number':391,'multiline':False]['text':' I am very happy.','line_number':392,'multiline':False]['text':' Here is the second sentence.','line_number':393,'multiline':False]['text':'','line_number':394,'multiline':False]['text':' A new document.','line_number':395,'multiline':False]['text':' Empty lines are used as document delimiters','line_number':416,'multiline':False]['text':' We *usually* want to fill up the entire sequence since we are padding','line_number':441,'multiline':False]['text':' to `block_size` anyways, so short sequences are generally wasted','line_number':442,'multiline':False]['text':' computation. However, we *sometimes*','line_number':443,'multiline':False]['text':' (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter','line_number':444,'multiline':False]['text':' sequences to minimize the mismatch between pretraining and fine-tuning.','line_number':445,'multiline':False]['text':' The `target_seq_length` is just a rough target however, whereas','line_number':446,'multiline':False]['text':' `block_size` is a hard limit.','line_number':447,'multiline':False]['text':' a buffer stored current working segments','line_number':452,'multiline':False]['text':' `a_end` is how many segments from `current_chunk` go into the `A`','line_number':462,'multiline':False]['text':' (first) sentence.','line_number':463,'multiline':False]['text':' This should rarely go for more than one iteration for large','line_number':478,'multiline':False]['text':' corpora. However, just to be careful, we try to make sure that','line_number':479,'multiline':False]['text':' the random document is not the same as the document','line_number':480,'multiline':False]['text':' we're processing.','line_number':481,'multiline':False]['text':' We didn't actually use these segments so we "put them back" so','line_number':493,'multiline':False]['text':' they don't go to waste.','line_number':494,'multiline':False]['text':' Actual next','line_number':497,'multiline':False]['text':' add special tokens','line_number':508,'multiline':False]['text':' add token type ids, 0 for sentence a, 1 for sentence b','line_number':510,'multiline':False]