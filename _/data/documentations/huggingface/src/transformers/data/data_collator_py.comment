['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' In this function we'll make the assumption that all `features` in the batch','line_number':64,'multiline':False]['text':' have the same attributes.','line_number':65,'multiline':False]['text':' So we will look at the first element as a proxy for what attributes exist','line_number':66,'multiline':False]['text':' on the whole batch.','line_number':67,'multiline':False]['text':' Special handling for labels.','line_number':113,'multiline':False]['text':' Ensure that tensor is created with the correct type','line_number':114,'multiline':False]['text':' (it should be automatically the case, but let's make sure of it.)','line_number':115,'multiline':False]['text':' Handling of all other possible keys.','line_number':127,'multiline':False]['text':' Again, we will use the first element to figure out which key/values are not None for this model.','line_number':128,'multiline':False]['text':' Special handling for labels.','line_number':149,'multiline':False]['text':' Ensure that tensor is created with the correct type','line_number':150,'multiline':False]['text':' (it should be automatically the case, but let's make sure of it.)','line_number':151,'multiline':False]['text':' Handling of all other possible keys.','line_number':170,'multiline':False]['text':' Again, we will use the first element to figure out which key/values are not None for this model.','line_number':171,'multiline':False]['text':' Special handling for labels.','line_number':188,'multiline':False]['text':' Ensure that tensor is created with the correct type','line_number':189,'multiline':False]['text':' (it should be automatically the case, but let's make sure of it.)','line_number':190,'multiline':False]['text':' Handling of all other possible keys.','line_number':202,'multiline':False]['text':' Again, we will use the first element to figure out which key/values are not None for this model.','line_number':203,'multiline':False]['text':' Conversion to tensors will fail if we have labels as they are not of the same length yet.','line_number':351,'multiline':False]['text':' Conversion to tensors will fail if we have labels as they are not of the same length yet.','line_number':380,'multiline':False]['text':' Tensorize if necessary.','line_number':406,'multiline':False]['text':' Check if padding is necessary.','line_number':412,'multiline':False]['text':' If yes, check if we have a `pad_token`.','line_number':418,'multiline':False]['text':' Creating the full tensor and filling it with our data.','line_number':425,'multiline':False]['text':' Tensorize if necessary.','line_number':442,'multiline':False]['text':' Check if padding is necessary.','line_number':446,'multiline':False]['text':' If yes, check if we have a `pad_token`.','line_number':452,'multiline':False]['text':' Creating the full tensor and filling it with our data.','line_number':459,'multiline':False]['text':' result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)','line_number':463,'multiline':False]['text':' Tensorize if necessary.','line_number':478,'multiline':False]['text':' Check if padding is necessary.','line_number':482,'multiline':False]['text':' If yes, check if we have a `pad_token`.','line_number':488,'multiline':False]['text':' Creating the full tensor and filling it with our data.','line_number':495,'multiline':False]['text':' Checks for TF tensors without needing the import','line_number':511,'multiline':False]['text':' We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the','line_number':563,'multiline':False]['text':' same length to return tensors.','line_number':564,'multiline':False]['text':' prepare decoder_input_ids','line_number':594,'multiline':False]['text':' 1 for a special token, 0 for a normal token in the special tokens mask','line_number':670,'multiline':False]['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':671,'multiline':False]['text':' Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens','line_number':673,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':676,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':681,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':687,'multiline':False]['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':693,'multiline':False]['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':701,'multiline':False]['text':' Cannot directly create as bool','line_number':709,'multiline':False]['text':' Replace self.tokenizer.pad_token_id with -100','line_number':722,'multiline':False]['text':' Makes a copy, just in case','line_number':725,'multiline':False]['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':730,'multiline':False]['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':738,'multiline':False]['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':758,'multiline':False]['text':' We only compute loss on masked tokens','line_number':770,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':772,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':776,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':781,'multiline':False]['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':785,'multiline':False]['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':793,'multiline':False]['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':811,'multiline':False]['text':' Numpy doesn't have bernoulli, so we use a binomial with 1 trial','line_number':822,'multiline':False]['text':' We only compute loss on masked tokens','line_number':824,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':826,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':830,'multiline':False]['text':' indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced','line_number':831,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':840,'multiline':False]['text':' For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]','line_number':876,'multiline':False]['text':' For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]','line_number':906,'multiline':False]['text':' For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]','line_number':934,'multiline':False]['text':' If adding a whole-word mask would exceed the maximum number of','line_number':973,'multiline':False]['text':' predictions, then just skip this candidate.','line_number':974,'multiline':False]['text':' We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)','line_number':1006,'multiline':False]['text':' We only compute loss on masked tokens','line_number':1019,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':1021,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':1025,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':1030,'multiline':False]['text':' We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)','line_number':1047,'multiline':False]['text':' Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens','line_number':1059,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':1062,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':1067,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':1072,'multiline':False]['text':' We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)','line_number':1086,'multiline':False]['text':' We only compute loss on masked tokens','line_number':1098,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':1100,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':1104,'multiline':False]['text':' indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced','line_number':1105,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':1112,'multiline':False]['text':' size of segment_ids varied because randomness, padding zero to the end as the original implementation','line_number':1141,'multiline':False]['text':' We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)','line_number':1169,'multiline':False]['text':' probability be `1` (masked), however in albert model attention mask `0` means masked, revert the value','line_number':1179,'multiline':False]['text':' We only compute loss on masked tokens, -100 is default for CE compute','line_number':1184,'multiline':False]['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':1186,'multiline':False]['text':' 10% of the time, we replace masked input tokens with random word','line_number':1190,'multiline':False]['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':1195,'multiline':False]['text':' maximum length of a span of masked tokens','line_number':1210,'multiline':False]['text':' Creating the mask and target_mapping tensors','line_number':1262,'multiline':False]['text':' Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).','line_number':1267,'multiline':False]['text':' Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)','line_number':1272,'multiline':False]['text':' Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked','line_number':1274,'multiline':False]['text':' Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`','line_number':1276,'multiline':False]['text':' Set `cur_len = cur_len + context_length`','line_number':1279,'multiline':False]['text':' Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,','line_number':1282,'multiline':False]['text':' the i-th predict corresponds to the i-th token.','line_number':1283,'multiline':False]['text':' Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.','line_number':1295,'multiline':False]['text':' We only compute loss on masked tokens','line_number':1299,'multiline':False]['text':' Generate permutation indices i.e. sample a random factorisation order for the sequence. This will','line_number':1304,'multiline':False]['text':' determine which tokens a given token can attend to (encoded in `perm_mask`).','line_number':1305,'multiline':False]['text':' Note: Length of token sequence being permuted has to be less than or equal to reused sequence length','line_number':1306,'multiline':False]['text':' (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,','line_number':1307,'multiline':False]['text':' we assume that reused length is half of sequence length and permutation length is equal to reused length.','line_number':1308,'multiline':False]['text':' This requires that the sequence length be even.','line_number':1309,'multiline':False]['text':' Create a linear factorisation order','line_number':1311,'multiline':False]['text':' Split this into two halves, assuming that half the sequence is reused each time','line_number':1313,'multiline':False]['text':' Permute the two halves such that they do not cross over','line_number':1315,'multiline':False]['text':' Flatten this out into the desired permuted factorisation order','line_number':1317,'multiline':False]['text':' Set the permutation indices of non-masked (non-functional) tokens to the','line_number':1319,'multiline':False]['text':' smallest index (-1) so that:','line_number':1320,'multiline':False]['text':' (1) They can be seen by all other positions','line_number':1321,'multiline':False]['text':' (2) They cannot see masked positions, so there won't be information leak','line_number':1322,'multiline':False]['text':' The logic for whether the i-th token can attend on the j-th token based on the factorisation order:','line_number':1324,'multiline':False]['text':' 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token','line_number':1325,'multiline':False]['text':' 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token','line_number':1326,'multiline':False]['text':' Creating the mask and target_mapping tensors','line_number':1361,'multiline':False]['text':' Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).','line_number':1367,'multiline':False]['text':' Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)','line_number':1372,'multiline':False]['text':' Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked','line_number':1374,'multiline':False]['text':' Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`','line_number':1376,'multiline':False]['text':' Set `cur_len = cur_len + context_length`','line_number':1379,'multiline':False]['text':' Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,','line_number':1382,'multiline':False]['text':' the i-th predict corresponds to the i-th token.','line_number':1383,'multiline':False]['text':' Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.','line_number':1399,'multiline':False]['text':' We only compute loss on masked tokens','line_number':1403,'multiline':False]['text':' Generate permutation indices i.e. sample a random factorisation order for the sequence. This will','line_number':1408,'multiline':False]['text':' determine which tokens a given token can attend to (encoded in `perm_mask`).','line_number':1409,'multiline':False]['text':' Note: Length of token sequence being permuted has to be less than or equal to reused sequence length','line_number':1410,'multiline':False]['text':' (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,','line_number':1411,'multiline':False]['text':' we assume that reused length is half of sequence length and permutation length is equal to reused length.','line_number':1412,'multiline':False]['text':' This requires that the sequence length be even.','line_number':1413,'multiline':False]['text':' Create a linear factorisation order','line_number':1415,'multiline':False]['text':' tf.range is the equivalent of torch.arange','line_number':1416,'multiline':False]['text':' Split this into two halves, assuming that half the sequence is reused each time','line_number':1418,'multiline':False]['text':' Permute the two halves such that they do not cross over','line_number':1420,'multiline':False]['text':' Shuffles along the first dimension','line_number':1421,'multiline':False]['text':' Flatten this out into the desired permuted factorisation order','line_number':1422,'multiline':False]['text':' Set the permutation indices of non-masked (non-functional) tokens to the','line_number':1424,'multiline':False]['text':' smallest index (-1) so that:','line_number':1425,'multiline':False]['text':' (1) They can be seen by all other positions','line_number':1426,'multiline':False]['text':' (2) They cannot see masked positions, so there won't be information leak','line_number':1427,'multiline':False]['text':' The logic for whether the i-th token can attend on the j-th token based on the factorisation order:','line_number':1429,'multiline':False]['text':' 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token','line_number':1430,'multiline':False]['text':' 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token','line_number':1431,'multiline':False]['text':' Creating the mask and target_mapping tensors','line_number':1466,'multiline':False]['text':' Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).','line_number':1471,'multiline':False]['text':' Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)','line_number':1476,'multiline':False]['text':' Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked','line_number':1478,'multiline':False]['text':' Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`','line_number':1480,'multiline':False]['text':' Set `cur_len = cur_len + context_length`','line_number':1483,'multiline':False]['text':' Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,','line_number':1486,'multiline':False]['text':' the i-th predict corresponds to the i-th token.','line_number':1487,'multiline':False]['text':' Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.','line_number':1499,'multiline':False]['text':' We only compute loss on masked tokens','line_number':1503,'multiline':False]['text':' Generate permutation indices i.e. sample a random factorisation order for the sequence. This will','line_number':1508,'multiline':False]['text':' determine which tokens a given token can attend to (encoded in `perm_mask`).','line_number':1509,'multiline':False]['text':' Note: Length of token sequence being permuted has to be less than or equal to reused sequence length','line_number':1510,'multiline':False]['text':' (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,','line_number':1511,'multiline':False]['text':' we assume that reused length is half of sequence length and permutation length is equal to reused length.','line_number':1512,'multiline':False]['text':' This requires that the sequence length be even.','line_number':1513,'multiline':False]['text':' Create a linear factorisation order','line_number':1515,'multiline':False]['text':' Split this into two halves, assuming that half the sequence is reused each time','line_number':1517,'multiline':False]['text':' Permute the two halves such that they do not cross over','line_number':1519,'multiline':False]['text':' Flatten this out into the desired permuted factorisation order','line_number':1521,'multiline':False]['text':' Set the permutation indices of non-masked (non-functional) tokens to the','line_number':1523,'multiline':False]['text':' smallest index (-1) so that:','line_number':1524,'multiline':False]['text':' (1) They can be seen by all other positions','line_number':1525,'multiline':False]['text':' (2) They cannot see masked positions, so there won't be information leak','line_number':1526,'multiline':False]['text':' The logic for whether the i-th token can attend on the j-th token based on the factorisation order:','line_number':1528,'multiline':False]['text':' 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token','line_number':1529,'multiline':False]['text':' 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token','line_number':1530,'multiline':False]