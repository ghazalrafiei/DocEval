['text':' Ensure we have batch axis','line_number':64,'multiline':False]['text':' Compute the score of each tuple(start, end) to be the real answer','line_number':71,'multiline':False]['text':' Remove candidate with end < start and end - start > max_answer_len','line_number':74,'multiline':False]['text':'  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)','line_number':77,'multiline':False]['text':' Ensure padded tokens & question tokens cannot belong to the set of candidate answers.','line_number':120,'multiline':False]['text':' Generate mask','line_number':126,'multiline':False]['text':' Make sure non-context indexes in the tensor cannot contribute to the softmax','line_number':129,'multiline':False]['text':' Normalize logits and spans to retrieve the answer','line_number':133,'multiline':False]['text':' Mask CLS','line_number':143,'multiline':False]['text':' Detect where the actual inputs are','line_number':175,'multiline':False]['text':' Generic compatibility with sklearn and Keras','line_number':183,'multiline':False]['text':' Batched data','line_number':184,'multiline':False]['text':' When user is sending a generator we need to trust it's a valid example','line_number':204,'multiline':False]['text':' Normalize inputs','line_number':209,'multiline':False]['text':' Copy to avoid overriding arguments','line_number':213,'multiline':False]['text':' Set defaults values','line_number':313,'multiline':False]['text':' Convert inputs to features','line_number':389,'multiline':False]['text':' XXX: This is specal, args_parser will not handle anything generator or dataset like','line_number':397,'multiline':False]['text':' For those we expect user to send a simple valid example either directly as a SquadExample or simple dict.','line_number':398,'multiline':False]['text':' So we still need a little sanitation here.','line_number':399,'multiline':False]['text':' Define the side we want to truncate / pad and the text/pair sorting','line_number':423,'multiline':False]['text':' When the input is too long, it's converted in a batch of inputs with overflowing tokens','line_number':438,'multiline':False]['text':' and a stride of overlap between the inputs. If a batch of inputs is given, a special output','line_number':439,'multiline':False]['text':' "overflow_to_sample_mapping" indicate which member of the encoded batch belong to which original batch sample.','line_number':440,'multiline':False]['text':' Here we tokenize examples one-by-one so we don't need to use "overflow_to_sample_mapping".','line_number':441,'multiline':False]['text':' "num_span" is the number of output samples generated from the overflowing tokens.','line_number':442,'multiline':False]['text':' p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)','line_number':445,'multiline':False]['text':' We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)','line_number':446,'multiline':False]['text':' keep the cls_token unmasked (some models use it to indicate unanswerable questions)','line_number':461,'multiline':False]['text':' We don't use the rest of the values - and actually','line_number':474,'multiline':False]['text':' for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample','line_number':475,'multiline':False]['text':' `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported','line_number':516,'multiline':False]['text':' large and positive','line_number':535,'multiline':False]['text':' Convert the answer (tokens) back to the original text','line_number':553,'multiline':False]['text':' Score: score from the model','line_number':554,'multiline':False]['text':' Start: Index of the first character of the answer in the context string','line_number':555,'multiline':False]['text':' End: Index of the character following the last character of the answer in the context string','line_number':556,'multiline':False]['text':' Answer: Plain text of the answer','line_number':557,'multiline':False]['text':' Convert the answer (tokens) back to the original text','line_number':569,'multiline':False]['text':' Score: score from the model','line_number':570,'multiline':False]['text':' Start: Index of the first character of the answer in the context string','line_number':571,'multiline':False]['text':' End: Index of the character following the last character of the answer in the context string','line_number':572,'multiline':False]['text':' Answer: Plain text of the answer','line_number':573,'multiline':False]['text':' Encoding was *not* padded, input_ids *might*.','line_number':577,'multiline':False]['text':' It doesn't make a difference unless we're padding on','line_number':578,'multiline':False]['text':' the left hand side, since now we have different offsets','line_number':579,'multiline':False]['text':' everywhere.','line_number':580,'multiline':False]['text':' Sometimes the max probability token is in the middle of a word so:','line_number':586,'multiline':False]['text':' - we start by finding the right word containing the token with `token_to_word`','line_number':587,'multiline':False]['text':' - then we convert this word in a character span with `word_to_chars`','line_number':588,'multiline':False]['text':' Some tokenizers don't really handle words. Keep to offsets then.','line_number':622,'multiline':False]['text':' Append words if they are in the span','line_number':648,'multiline':False]['text':' Stop if we went over the end of the answer','line_number':658,'multiline':False]['text':' Append the subtokenization length to the running index','line_number':662,'multiline':False]['text':' Join text with spaces','line_number':666,'multiline':False]