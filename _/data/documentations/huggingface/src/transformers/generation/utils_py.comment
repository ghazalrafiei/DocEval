['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':' Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' Non-beam methods','line_number':567,'multiline':False]['text':' Beam methods','line_number':572,'multiline':False]['text':' 1. retrieve all kwargs that are non-None or non-model input related.','line_number':617,'multiline':False]['text':' some encoder-decoder models have different names for model and encoder','line_number':618,'multiline':False]['text':' 2. check whether model_input_name is passed as kwarg','line_number':630,'multiline':False]['text':' if yes and `inputs` is None use kwarg inputs','line_number':631,'multiline':False]['text':' 3. In the presence of `inputs_embeds` for text models:','line_number':641,'multiline':False]['text':' - decoder-only models should complain if the user attempts to pass `inputs_embeds`, but the model','line_number':642,'multiline':False]['text':' doesn't have its forwarding implemented. `inputs_embeds` is kept in `model_kwargs` and can coexist with','line_number':643,'multiline':False]['text':' input_ids (`inputs_embeds` will be used in the 1st generation step, as opposed to `input_ids`)','line_number':644,'multiline':False]['text':' - encoder-decoder models should complain if the user attempts to pass `inputs_embeds` and `input_ids`, and','line_number':645,'multiline':False]['text':' pull the former to inputs. It will be used in place of `input_ids` to get the encoder hidden states.','line_number':646,'multiline':False]['text':' In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of','line_number':658,'multiline':False]['text':' the attention mask) can rely on the actual model input.','line_number':659,'multiline':False]['text':' 4. if `inputs` is still None, try to create `input_ids` from BOS token','line_number':668,'multiline':False]['text':' make dummy input_ids with value -100, as a sanity check ensuring that they won't be used for encoding','line_number':684,'multiline':False]['text':' If there is some tensor in `model_kwargs`, we can infer the batch size from it. This is helpful with','line_number':691,'multiline':False]['text':' soft-prompting or in multimodal implementations built on top of decoder-only language models.','line_number':692,'multiline':False]['text':' Check if input is input_ids and padded -> only then is attention_mask defined','line_number':712,'multiline':False]['text':' 1. get encoder','line_number':721,'multiline':False]['text':' Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device','line_number':723,'multiline':False]['text':' as the inputs.','line_number':724,'multiline':False]['text':' 2. Prepare encoder args and encoder kwargs from model kwargs.','line_number':731,'multiline':False]['text':' 3. make sure that encoder returns `ModelOutput`','line_number':745,'multiline':False]['text':' 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,','line_number':763,'multiline':False]['text':' we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.','line_number':764,'multiline':False]['text':' 2. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.','line_number':772,'multiline':False]['text':' no user input -> use decoder_start_token_id as decoder_input_ids','line_number':778,'multiline':False]['text':' exception: Donut checkpoints have task-specific decoder starts and don't expect a BOS token','line_number':781,'multiline':False]['text':' user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust','line_number':784,'multiline':False]['text':' decoder_attention_mask if provided)','line_number':785,'multiline':False]['text':' Bloom fix: standardizes the cache format when requested','line_number':850,'multiline':False]['text':' update past_key_values','line_number':863,'multiline':False]['text':' update token_type_ids with last value','line_number':870,'multiline':False]['text':' update attention mask','line_number':876,'multiline':False]['text':' update decoder attention mask','line_number':883,'multiline':False]['text':' instantiate warpers list','line_number':930,'multiline':False]['text':' In beam methods, we need to keep at least one non-eos token to explore continuations that might have a','line_number':933,'multiline':False]['text':' better score (i.e. keep len(list(generation_config.eos_token_id)) + 1)','line_number':934,'multiline':False]['text':' the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files','line_number':943,'multiline':False]['text':' all samplers can be found in `generation_utils_samplers.py`','line_number':944,'multiline':False]['text':' `LogitNormalization` should always be the last logit processor, when present','line_number':963,'multiline':False]['text':' Assisted generation may extend some generation modes','line_number':997,'multiline':False]['text':' instantiate processors list','line_number':1023,'multiline':False]['text':' generation starts after the last token that is forced','line_number':1119,'multiline':False]['text':' `LogitNormalization` should always be the last logit processor, when present','line_number':1127,'multiline':False]['text':' 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent','line_number':1252,'multiline':False]['text':' to a beam search approach were the first (and only) beam is always selected','line_number':1253,'multiline':False]['text':' 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being','line_number':1258,'multiline':False]['text':' seq_len - input_length','line_number':1259,'multiline':False]['text':' 3. Optionally normalize the logits (across the vocab dimension)','line_number':1262,'multiline':False]['text':' 4. cut beam_indices to longest beam length','line_number':1268,'multiline':False]['text':' 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards','line_number':1274,'multiline':False]['text':' 6. multiply beam_indices with vocab size to gather correctly from scores','line_number':1277,'multiline':False]['text':' 7. Define which indices contributed to scores','line_number':1280,'multiline':False]['text':' 8. Compute scores','line_number':1284,'multiline':False]['text':' 9. Mask out transition_scores of beams that stopped early','line_number':1287,'multiline':False]['text':' If a `Cache` instance is passed, checks whether the model is compatible with it','line_number':1320,'multiline':False]['text':' Excludes arguments that are handled before calling any model function','line_number':1327,'multiline':False]['text':' `kwargs`/`model_kwargs` is often used to handle optional forward pass inputs like `attention_mask`. If','line_number':1334,'multiline':False]['text':' `prepare_inputs_for_generation` doesn't accept them, then a stricter check can be made ;)','line_number':1335,'multiline':False]['text':' Encoder-Decoder models may also need Encoder arguments from `model_kwargs`','line_number':1339,'multiline':False]['text':' allow encoder kwargs','line_number':1343,'multiline':False]['text':' `MusicgenForConditionalGeneration` has `text_encoder` and `audio_encoder`.','line_number':1345,'multiline':False]['text':' Also, it has `base_model_prefix = "encoder_decoder"` but there is no `self.encoder_decoder`','line_number':1346,'multiline':False]['text':' TODO: A better way to handle this.','line_number':1347,'multiline':False]['text':' allow decoder kwargs','line_number':1355,'multiline':False]['text':' allow assistant_encoder_outputs to be passed if we're doing assisted generating','line_number':1364,'multiline':False]['text':' 1. Max length warnings related to poor parameterization','line_number':1381,'multiline':False]['text':' 20 is the default max_length of the generation config','line_number':1383,'multiline':False]['text':' 2. Min length warnings due to unfeasible parameter combinations','line_number':1399,'multiline':False]['text':' 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call','line_number':1533,'multiline':False]['text':' priority: `generation_config` argument > `model.generation_config` (the default generation config)','line_number':1536,'multiline':False]['text':' legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,','line_number':1538,'multiline':False]['text':' two conditions must be met','line_number':1539,'multiline':False]['text':' 1) the generation config must have been created from the model config (`_from_model_config` field);','line_number':1540,'multiline':False]['text':' 2) the generation config must have seen no modification since its creation (the hash is the same).','line_number':1541,'multiline':False]['text':' All unused kwargs must be model kwargs','line_number':1557,'multiline':False]['text':' 2. Set generation parameters if not already defined','line_number':1561,'multiline':False]['text':' 3. Define model inputs','line_number':1577,'multiline':False]['text':' inputs_tensor has to be defined','line_number':1578,'multiline':False]['text':' model_input_name is defined if model-specific keyword input is passed','line_number':1579,'multiline':False]['text':' otherwise model_input_name is None','line_number':1580,'multiline':False]['text':' all model-specific keyword inputs are removed from `model_kwargs`','line_number':1581,'multiline':False]['text':' 4. Define other model kwargs','line_number':1587,'multiline':False]['text':' decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are','line_number':1590,'multiline':False]['text':' generating the first new token or not, and we only want to use the embeddings for the first new token)','line_number':1591,'multiline':False]['text':' decoder-only models should use left-padding for generation','line_number':1605,'multiline':False]['text':' If `input_ids` was given, check if the last id in any sequence is `pad_token_id`','line_number':1607,'multiline':False]['text':' Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.','line_number':1608,'multiline':False]['text':' if model is encoder decoder encoder_outputs are created','line_number':1620,'multiline':False]['text':' and added to `model_kwargs`','line_number':1621,'multiline':False]['text':' 5. Prepare `input_ids` which will be used for auto-regressive generation','line_number':1626,'multiline':False]['text':' 6. Prepare `max_length` depending on other stopping criteria.','line_number':1642,'multiline':False]['text':' 7. determine generation mode','line_number':1656,'multiline':False]['text':' 8. prepare distribution pre_processing samplers','line_number':1675,'multiline':False]['text':' 9. prepare stopping criteria','line_number':1687,'multiline':False]['text':' 10. go into different generation modes','line_number':1691,'multiline':False]['text':' 11. Get the candidate generator, given the parameterization','line_number':1703,'multiline':False]['text':' 12. run assisted generate','line_number':1713,'multiline':False]['text':' 11. run greedy search','line_number':1730,'multiline':False]['text':' 11. prepare logits warper','line_number':1765,'multiline':False]['text':' 12. expand input_ids with `num_return_sequences` additional sequences per batch','line_number':1768,'multiline':False]['text':' 13. run sample','line_number':1776,'multiline':False]['text':' 11. prepare beam search scorer','line_number':1792,'multiline':False]['text':' 12. interleave input_ids with `num_beams` additional sequences per batch','line_number':1802,'multiline':False]['text':' 13. run beam search','line_number':1809,'multiline':False]['text':' 11. prepare logits warper','line_number':1824,'multiline':False]['text':' 12. prepare beam search scorer','line_number':1827,'multiline':False]['text':' 13. interleave input_ids with `num_beams` additional sequences per batch','line_number':1838,'multiline':False]['text':' 14. run beam sample','line_number':1846,'multiline':False]['text':' 11. prepare beam search scorer','line_number':1862,'multiline':False]['text':' 12. interleave input_ids with `num_beams` additional sequences per batch','line_number':1873,'multiline':False]['text':' 13. run beam search','line_number':1880,'multiline':False]['text':' 11. prepare beam search scorer','line_number':1935,'multiline':False]['text':' 12. interleave input_ids with `num_beams` additional sequences per batch','line_number':1946,'multiline':False]['text':' 13. run beam search','line_number':1953,'multiline':False]['text':' init values','line_number':2070,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':2093,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':2099,'multiline':False]['text':' keep track of which sequences are already finished','line_number':2106,'multiline':False]['text':' used by synced_gpus only','line_number':2109,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':2114,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':2115,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':2117,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':2119,'multiline':False]['text':' if the first step in the loop, encode all the prefix and obtain: (1) past_key_values;','line_number':2123,'multiline':False]['text':' (2) last_hidden_states; (3) logit_for_next_step; (4) update model kwargs for the next step','line_number':2124,'multiline':False]['text':' prepare inputs','line_number':2126,'multiline':False]['text':' encode the given prefix and prepare model inputs; encoder-decoder model process the prefix and save','line_number':2130,'multiline':False]['text':' the `encoder_outputs`','line_number':2131,'multiline':False]['text':' last decoder hidden states will be used to compute the degeneration penalty (cosine similarity with','line_number':2136,'multiline':False]['text':' previous tokens)','line_number':2137,'multiline':False]['text':' next logit for contrastive search to select top-k candidate tokens','line_number':2143,'multiline':False]['text':' Expands model inputs top_k times, for batched forward passes (akin to beam search).','line_number':2153,'multiline':False]['text':' contrastive_search main logic start:','line_number':2173,'multiline':False]['text':' contrastive search decoding consists of two steps: (1) candidate tokens recall; (2) candidate re-rank by','line_number':2174,'multiline':False]['text':' degeneration penalty','line_number':2175,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':2181,'multiline':False]['text':' Replicates the new past_key_values to match the `top_k` candidates','line_number':2199,'multiline':False]['text':' item is either the key or the value matrix','line_number':2203,'multiline':False]['text':' defined in first loop iteration','line_number':2213,'multiline':False]['text':' compute the candidate tokens by the language model and collect their hidden_states','line_number':2216,'multiline':False]['text':' stack hidden states','line_number':2240,'multiline':False]['text':' stack logits','line_number':2249,'multiline':False]['text':' compute the candidate tokens by the language model and collect their hidden_states','line_number':2253,'multiline':False]['text':' assembles top_k_ids into batch of size k','line_number':2254,'multiline':False]['text':' name is different for encoder-decoder and decoder-only models','line_number':2263,'multiline':False]['text':' compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the','line_number':2275,'multiline':False]['text':' model confidence. Keeping `selected_idx` on CPU enables multi-device contrastive search and doesn't','line_number':2276,'multiline':False]['text':' introduce (noticeable) slowdowns on single-device runs.','line_number':2277,'multiline':False]['text':' prepare for the next step: (1) next token_id; (2) past_key_values; (3) last_hidden_states for computing','line_number':2281,'multiline':False]['text':' the degeneration penalty; (4) logits for selecting next top-k candidates; (5) selected tokens scores','line_number':2282,'multiline':False]['text':' (model confidence minus degeneration penalty); (6) decoder hidden_states','line_number':2283,'multiline':False]['text':' generate past_key_values cache of only the selected token','line_number':2294,'multiline':False]['text':' item is either the key or the value matrix','line_number':2313,'multiline':False]['text':' [B, K, num_head, seq_len, esz]','line_number':2315,'multiline':False]['text':' [B, num_head, seq_len, esz]','line_number':2316,'multiline':False]['text':' Rebuilds the relevant parts of the model output for the selected token, for use in the next iteration','line_number':2323,'multiline':False]['text':' contrastive_search main logic end','line_number':2351,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':2354,'multiline':False]['text':' finished sentences should have their next token be a padding token','line_number':2356,'multiline':False]['text':' update generated ids, model inputs, and length for next step','line_number':2362,'multiline':False]['text':' if eos_token was found in one sentence, set sentence to finished','line_number':2370,'multiline':False]['text':' stop when each sentence is finished','line_number':2376,'multiline':False]['text':' stop if we exceed the maximum length','line_number':2380,'multiline':False]['text':' Contrastive search works by forward looking at the next token, so we need to exclude it from','line_number':2391,'multiline':False]['text':' `past_key_values` to be consistent with the other decoding methods','line_number':2392,'multiline':False]['text':' init values','line_number':2532,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':2560,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':2566,'multiline':False]['text':' keep track of which sequences are already finished','line_number':2573,'multiline':False]['text':' used by synced_gpus only','line_number':2576,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':2579,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':2580,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':2582,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':2584,'multiline':False]['text':' prepare model inputs','line_number':2588,'multiline':False]['text':' forward pass to get next token','line_number':2591,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':2600,'multiline':False]['text':' pre-process distribution','line_number':2604,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':2607,'multiline':False]['text':' argmax','line_number':2625,'multiline':False]['text':' finished sentences should have their next token be a padding token','line_number':2628,'multiline':False]['text':' update generated ids, model inputs, and length for next step','line_number':2634,'multiline':False]['text':' if eos_token was found in one sentence, set sentence to finished','line_number':2642,'multiline':False]['text':' stop when each sentence is finished','line_number':2648,'multiline':False]['text':' stop if we exceed the maximum length','line_number':2652,'multiline':False]['text':' init values','line_number':2812,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':2841,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':2847,'multiline':False]['text':' keep track of which sequences are already finished','line_number':2854,'multiline':False]['text':' used by synced_gpus only','line_number':2857,'multiline':False]['text':' auto-regressive generation','line_number':2858,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':2861,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':2862,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':2864,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':2866,'multiline':False]['text':' prepare model inputs','line_number':2870,'multiline':False]['text':' forward pass to get next token','line_number':2873,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':2882,'multiline':False]['text':' pre-process distribution','line_number':2886,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':2890,'multiline':False]['text':' sample','line_number':2908,'multiline':False]['text':' finished sentences should have their next token be a padding token','line_number':2912,'multiline':False]['text':' update generated ids, model inputs, and length for next step','line_number':2918,'multiline':False]['text':' if eos_token was found in one sentence, set sentence to finished','line_number':2926,'multiline':False]['text':' stop when each sentence is finished','line_number':2932,'multiline':False]['text':' stop if we exceed the maximum length','line_number':2936,'multiline':False]['text':' Exception 1: code path for models using the legacy cache format','line_number':2977,'multiline':False]['text':' Exception 2: models with different cache formats. These are limited to `DynamicCache` until their','line_number':2980,'multiline':False]['text':' cache format is standardized, to avoid adding complexity to the codebase.','line_number':2981,'multiline':False]['text':' Standard code path: use the `Cache.reorder_cache`','line_number':2990,'multiline':False]['text':' init values','line_number':3117,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':3156,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':3165,'multiline':False]['text':' initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens','line_number':3172,'multiline':False]['text':' of the first beam are considered to avoid sampling the exact same tokens across all beams.','line_number':3173,'multiline':False]['text':' used by synced_gpus only','line_number':3178,'multiline':False]['text':' record the prompt length of decoder','line_number':3180,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':3183,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':3184,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':3186,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':3188,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':3203,'multiline':False]['text':' (batch_size * num_beams, vocab_size)','line_number':3208,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':3215,'multiline':False]['text':' reshape for beam search','line_number':3233,'multiline':False]['text':' Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.','line_number':3237,'multiline':False]['text':' stateless','line_number':3246,'multiline':False]['text':' increase cur_len','line_number':3275,'multiline':False]['text':' init values','line_number':3461,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':3493,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':3502,'multiline':False]['text':' used by synced_gpus only','line_number':3512,'multiline':False]['text':' record the prompt length of decoder','line_number':3514,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':3517,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':3518,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':3520,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':3522,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':3537,'multiline':False]['text':' (batch_size * num_beams, vocab_size)','line_number':3543,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':3551,'multiline':False]['text':' reshape for beam search','line_number':3569,'multiline':False]['text':' stateless','line_number':3584,'multiline':False]['text':' increase cur_len','line_number':3612,'multiline':False]['text':' init values','line_number':3791,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':3836,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':3842,'multiline':False]['text':' initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in','line_number':3849,'multiline':False]['text':' the same group don't produce same tokens everytime.','line_number':3850,'multiline':False]['text':' used by synced_gpus only','line_number':3855,'multiline':False]['text':' record the prompt length of decoder','line_number':3857,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':3860,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':3861,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':3863,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':3865,'multiline':False]['text':' predicted tokens in cur_len step','line_number':3869,'multiline':False]['text':' indices which will form the beams in the next time step','line_number':3872,'multiline':False]['text':' do one decoder step on all beams of all sentences in batch','line_number':3875,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':3886,'multiline':False]['text':' indices of beams of current group among all sentences in batch','line_number':3896,'multiline':False]['text':' select outputs of beams of current group only','line_number':3905,'multiline':False]['text':' (batch_size * group_size, vocab_size)','line_number':3910,'multiline':False]['text':' reshape for beam search','line_number':3922,'multiline':False]['text':' Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.','line_number':3925,'multiline':False]['text':' stateless','line_number':3934,'multiline':False]['text':' (beam_idx // group_size) -> batch_idx','line_number':3960,'multiline':False]['text':' (beam_idx % group_size) -> offset of idx inside the group','line_number':3961,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':3968,'multiline':False]['text':' increase cur_len','line_number':3996,'multiline':False]['text':' init values','line_number':4181,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':4220,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':4229,'multiline':False]['text':' initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens','line_number':4236,'multiline':False]['text':' of the first beam are considered to avoid sampling the exact same tokens across all beams.','line_number':4237,'multiline':False]['text':' used by synced_gpus only','line_number':4242,'multiline':False]['text':' record the prompt length of decoder','line_number':4244,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':4247,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':4248,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':4250,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':4252,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':4267,'multiline':False]['text':' (batch_size * num_beams, vocab_size)','line_number':4272,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':4282,'multiline':False]['text':' reshape for beam search','line_number':4300,'multiline':False]['text':' Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.','line_number':4304,'multiline':False]['text':' stateless','line_number':4313,'multiline':False]['text':' increase cur_len','line_number':4341,'multiline':False]['text':' handling deprecated arguments','line_number':4512,'multiline':False]['text':' init values','line_number':4530,'multiline':False]['text':' init attention / hidden states / scores tuples','line_number':4554,'multiline':False]['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':4560,'multiline':False]['text':' keep track of which sequences are already finished','line_number':4567,'multiline':False]['text':' other auxiliary variables','line_number':4570,'multiline':False]['text':' used by synced_gpus only','line_number':4573,'multiline':False]['text':' Under synced_gpus the `forward` call must continue until all gpus complete their sequence.','line_number':4576,'multiline':False]['text':' The following logic allows an early break if all peers finished generating their sequence','line_number':4577,'multiline':False]['text':' send 0.0 if we finished, 1.0 otherwise','line_number':4579,'multiline':False]['text':' did all peers finish? the reduced sum will be 0.0 then','line_number':4581,'multiline':False]['text':'  1. Fetch candidate sequences from a `CandidateGenerator`','line_number':4587,'multiline':False]['text':' 2. Use the original model to obtain the next token logits given the candidate sequence. We obtain','line_number':4598,'multiline':False]['text':' `candidate_length + 1` relevant logits from this process: in the event that all candidates are correct,','line_number':4599,'multiline':False]['text':' we use this forward pass to also pick the subsequent logits in the original model.','line_number':4600,'multiline':False]['text':' 2.1. Prepare the model inputs','line_number':4602,'multiline':False]['text':' 2.2. Run a forward pass on the candidate sequence','line_number':4611,'multiline':False]['text':' 2.3. Process the new logits','line_number':4618,'multiline':False]['text':' excludes the input prompt if present','line_number':4619,'multiline':False]['text':' 3. Obtain the next tokens from the original model logits.','line_number':4627,'multiline':False]['text':' 4. Compare the argmax from the original model logits with the assistant forecasted tokens. We can keep','line_number':4634,'multiline':False]['text':' the assistant forecasted tokens until the first mismatch, or until the max length is reached.','line_number':4635,'multiline':False]['text':' 5. Update variables according to the number of matching assistant tokens. Remember: the token generated','line_number':4639,'multiline':False]['text':' by the model after the last candidate match is also valid, as it is generated from a correct sequence.','line_number':4640,'multiline':False]['text':' Because of this last token, assisted generation search reduces to a normal greedy search/sample if there','line_number':4641,'multiline':False]['text':' is no match.','line_number':4642,'multiline':False]['text':' 5.1. Ensure we don't generate beyond max_len or an EOS token','line_number':4644,'multiline':False]['text':' 5.2. Get the valid continuation, after the matching tokens','line_number':4649,'multiline':False]['text':' 5.3. Discard past key values relative to unused assistant tokens','line_number':4656,'multiline':False]['text':' 6. Update the candidate generation strategy if needed','line_number':4660,'multiline':False]['text':' don't waste resources running the code we don't need','line_number':4664,'multiline':False]['text':' Store scores, attentions and hidden_states when required','line_number':4666,'multiline':False]['text':' Assistant: modified to append one tuple element per token, as in the other generation methods.','line_number':4667,'multiline':False]['text':' if eos_token was found in one sentence, set sentence to finished','line_number':4711,'multiline':False]['text':' stop when each sentence is finished','line_number':4720,'multiline':False]['text':' stop if we exceed the maximum length','line_number':4724,'multiline':False]['text':' Retrocompatibility: in our generation functions, the first iteration includes the attention/hidden states for the','line_number':4763,'multiline':False]['text':' prompt.','line_number':4764,'multiline':False]['text':' The first iteration contains the prompt + 1 generated token, let's update the length variables accordingly','line_number':4771,'multiline':False]['text':' [B*K, S]','line_number':4833,'multiline':False]['text':' [B*K]','line_number':4834,'multiline':False]['text':' [B*K]','line_number':4835,'multiline':False]['text':' [B, K]','line_number':4837,'multiline':False]['text':' [B]','line_number':4838,'multiline':False]