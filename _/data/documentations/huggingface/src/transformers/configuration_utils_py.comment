['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' no-format','line_number':50,'multiline':False]['text':' Attributes with defaults','line_number':268,'multiline':False]['text':' Only used by PyTorch models','line_number':272,'multiline':False]['text':' Only used by PyTorch models','line_number':273,'multiline':False]['text':' Only used by TensorFlow models','line_number':275,'multiline':False]['text':' Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.','line_number':279,'multiline':False]['text':' Is decoder is used in encoder-decoder models to differentiate encoder from decoder','line_number':281,'multiline':False]['text':' Parameters for sequence generation','line_number':288,'multiline':False]['text':' Fine-tuning task arguments','line_number':316,'multiline':False]['text':' Keys are always strings in JSON so convert ids to int here.','line_number':333,'multiline':False]['text':' we will start using self.torch_dtype in v5, but to be consistent with','line_number':338,'multiline':False]['text':' from_pretrained's torch_dtype arg convert it to an actual torch.dtype object','line_number':339,'multiline':False]['text':' Tokenizer arguments TODO: eventually tokenizer and models should share the same config','line_number':345,'multiline':False]['text':' task specific arguments','line_number':355,'multiline':False]['text':' regression / multi-label classification','line_number':358,'multiline':False]['text':' TPU arguments','line_number':367,'multiline':False]['text':' Name or path to the pretrained checkpoint','line_number':374,'multiline':False]['text':' Config hash','line_number':376,'multiline':False]['text':' Attention implementation to use, if relevant.','line_number':379,'multiline':False]['text':' Drop the transformers version info','line_number':382,'multiline':False]['text':' Deal with gradient checkpointing','line_number':385,'multiline':False]['text':' Additional attributes without default values','line_number':393,'multiline':False]['text':' Make sure that name_or_path is a string (for JSON encoding)','line_number':407,'multiline':False]['text':' If torchscript is set, force `return_dict=False` to avoid jit errors','line_number':414,'multiline':False]['text':' This property is made private for now (as it cannot be changed and a PreTrainedModel.use_attn_implementation method needs to be implemented.)','line_number':432,'multiline':False]['text':' `config.attn_implementation` should never be None, for backward compatibility.','line_number':435,'multiline':False]['text':' If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be','line_number':474,'multiline':False]['text':' loaded from the Hub.','line_number':475,'multiline':False]['text':' If we save using the predefined names, we can load using `from_pretrained`','line_number':479,'multiline':False]['text':' Some model config classes like CLIP define their own `from_pretrained` without the new argument `token` yet.','line_number':502,'multiline':False]['text':' Get config dict associated with the base config file','line_number':643,'multiline':False]['text':' That config file may point us toward another config file to use.','line_number':648,'multiline':False]['text':' Special case when pretrained_model_name_or_path is a local file','line_number':688,'multiline':False]['text':' Load from local folder or from cache or download from model Hub and cache','line_number':698,'multiline':False]['text':' Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to','line_number':715,'multiline':False]['text':' the original exception.','line_number':716,'multiline':False]['text':' For any other exception, we throw a generic error.','line_number':719,'multiline':False]['text':' Load config dict','line_number':728,'multiline':False]['text':' Those arguments may be passed along for our internal telemetry.','line_number':763,'multiline':False]['text':' We remove them so they don't appear in `return_unused_kwargs`.','line_number':764,'multiline':False]['text':' The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.','line_number':767,'multiline':False]['text':' We remove it from kwargs so that it does not appear in `return_unused_kwargs`.','line_number':771,'multiline':False]['text':' Update config with kwargs if needed','line_number':779,'multiline':False]['text':' To authorize passing a custom subconfig as kwarg in models that have nested configs.','line_number':793,'multiline':False]['text':' get the default config dict','line_number':846,'multiline':False]['text':' get class specific config dict','line_number':849,'multiline':False]['text':' only serialize values that differ from the default config','line_number':854,'multiline':False]['text':' For nested configs we need to clean the diff recursively','line_number':861,'multiline':False]['text':' Needs to be set even if it's not in the diff','line_number':864,'multiline':False]['text':' pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.','line_number':883,'multiline':False]['text':' Transformers version when serializing the model','line_number':910,'multiline':False]['text':' Deal with nested configs like CLIP','line_number':914,'multiline':False]['text':' pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.','line_number':928,'multiline':False]['text':' Defaults to FULL_CONFIGURATION_FILE and then try to look at some newer versions.','line_number':1072,'multiline':False]['text':' No point going further since the versions are sorted.','line_number':1079,'multiline':False]