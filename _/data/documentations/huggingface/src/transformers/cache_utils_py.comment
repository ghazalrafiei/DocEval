['text':' Cache without size limit -> all cache is usable','line_number':47,'multiline':False]['text':' Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache','line_number':48,'multiline':False]['text':'   length, we will need to evict part of the cache (and thus not all cache is usable)','line_number':49,'multiline':False]['text':' Used in `generate` to keep tally of how many tokens the cache has seen','line_number':68,'multiline':False]['text':' Update the number of seen tokens','line_number':118,'multiline':False]['text':' Update the cache','line_number':122,'multiline':False]['text':' Used in `generate` to keep tally of how many tokens the cache has seen','line_number':190,'multiline':False]['text':' Upcast to float32 temporarily for better accuracy','line_number':208,'multiline':False]['text':' Compute the cos and sin required for back- and forward-rotating to one position earlier in the sequence','line_number':212,'multiline':False]['text':' Workaround to make 'key_states.shape[-2] + past_key_value.get_seq_length(self.layer_idx)' <= window_length','line_number':228,'multiline':False]['text':' Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models','line_number':262,'multiline':False]['text':' with partially rotated position embeddings, like Phi or Persimmon.','line_number':263,'multiline':False]['text':' Update the number of seen tokens','line_number':269,'multiline':False]['text':' [bsz, num_heads, seq_len, head_dim]','line_number':273,'multiline':False]['text':' Empty cache','line_number':275,'multiline':False]['text':' Growing cache','line_number':280,'multiline':False]['text':' Shifting cache','line_number':285,'multiline':False]['text':' On RoPE models, we need to recompute the Key rotation as the tokens are shifted','line_number':290,'multiline':False]['text':' Concatenate sink tokens, shifted & rotated tokens (if needed), and new tokens','line_number':304,'multiline':False]