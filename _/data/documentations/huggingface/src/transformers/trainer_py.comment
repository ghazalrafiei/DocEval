['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020-present the HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Integrations must be imported before ML frameworks:','line_number':39,'multiline':False]['text':' isort: off','line_number':40,'multiline':False]['text':' isort: on','line_number':46,'multiline':False]['text':' Name of the files used for checkpointing','line_number':222,'multiline':False]['text':' Those are used as methods of the Trainer in examples.','line_number':315,'multiline':False]['text':' Seed must be set before instantiating the model when using model','line_number':337,'multiline':False]['text':' memory metrics - must set up as early as possible','line_number':345,'multiline':False]['text':' set the correct log level depending on the node','line_number':349,'multiline':False]['text':' force device and distributed setup init explicitly','line_number':353,'multiline':False]['text':' warn users','line_number':394,'multiline':False]['text':' At this stage the model is already loaded','line_number':406,'multiline':False]['text':' one place to sort out whether to place the model on device or not','line_number':428,'multiline':False]['text':' postpone switching model to cuda when:','line_number':429,'multiline':False]['text':' 1. MP - since we are trying to fit a much bigger than 1 gpu model','line_number':430,'multiline':False]['text':' 2. fp16-enabled DeepSpeed loads the model in half the size and it doesn't need .to() anyway,','line_number':431,'multiline':False]['text':'    and we only use deepspeed for training at the moment','line_number':432,'multiline':False]['text':' 3. full bf16 or fp16 eval - since the model needs to be cast to the right dtype first','line_number':433,'multiline':False]['text':' 4. FSDP - same as MP','line_number':434,'multiline':False]['text':' Bnb Quantized models doesn't support `.to` operation.','line_number':451,'multiline':False]['text':' Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs','line_number':458,'multiline':False]['text':' later use `self.model is self.model_wrapped` to check if it's wrapped or not','line_number':462,'multiline':False]['text':' Will be set to True by `self._setup_loggers()` on first call to `self.log()`.','line_number':505,'multiline':False]['text':' Create distant repo and output directory if needed','line_number':508,'multiline':False]['text':' Mixed precision setup','line_number':536,'multiline':False]['text':' Mixed precision setup for SageMaker Model Parallel','line_number':540,'multiline':False]['text':' BF16 + model parallelism in SageMaker: currently not supported, raise an error','line_number':542,'multiline':False]['text':' When there's mismatch between SMP config and trainer argument, use SMP config as truth','line_number':547,'multiline':False]['text':' smp < 1.10 does not support fp16 in trainer.','line_number':556,'multiline':False]['text':' deepspeed and SageMaker Model Parallel manage their own half precision','line_number':571,'multiline':False]['text':' Label smoothing','line_number':583,'multiline':False]['text':' Internal variable to count flos in each process, will be accumulated in `self.state.total_flos` then','line_number':595,'multiline':False]['text':' returned to 0 every time flos need to be logged','line_number':596,'multiline':False]['text':' Internal variables to help with automatic batch size reduction','line_number':604,'multiline':False]['text':' very last','line_number':608,'multiline':False]['text':' torch.compile','line_number':611,'multiline':False]['text':' Moving a model to an XLA device disconnects the tied weights, so we have to retie them.','line_number':691,'multiline':False]['text':' Inspect model forward signature to keep only the arguments it accepts.','line_number':697,'multiline':False]['text':' Labels may be named label or label_ids, the default data collator handles that.','line_number':703,'multiline':False]['text':' Build the sampler.','line_number':754,'multiline':False]['text':' Deprecated code','line_number':810,'multiline':False]['text':' We use the same batch_size as for eval.','line_number':896,'multiline':False]['text':' If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer','line_number':909,'multiline':False]['text':' parse args.optim_args','line_number':985,'multiline':False]['text':' TODO Change dtypes back to M=FP32, Var = BF16, Kahan = False once they can be cast together in torchdistx.','line_number':1083,'multiline':False]['text':' Special case for IterableDatasetShard, we need to dig deeper','line_number':1132,'multiline':False]['text':' no dataset or length, estimate by length of dataloader','line_number':1136,'multiline':False]['text':' Casting value to the proper type','line_number':1179,'multiline':False]['text':' Rebuild the deepspeed config to reflect the updated training parameters','line_number':1193,'multiline':False]['text':' remove mixed precision hooks from the model','line_number':1261,'multiline':False]['text':' conv_bn_folding is disabled as it fails in symbolic tracing, resulting in ipex warnings','line_number':1304,'multiline':False]['text':' Wrapping the base model twice in a DistributedModel will raise an error.','line_number':1321,'multiline':False]['text':' train/eval could be run multiple-times - if already wrapped, don't re-wrap it again','line_number':1326,'multiline':False]['text':' Mixed precision training with apex (torch < 1.6)','line_number':1330,'multiline':False]['text':' Multi-gpu training (should be after apex fp16 initialization) / 8bit models does not support DDP','line_number':1334,'multiline':False]['text':' Note: in torch.distributed mode, there's no point in wrapping the model','line_number':1343,'multiline':False]['text':' inside a DistributedDataParallel as we'll be under `no_grad` anyways.','line_number':1344,'multiline':False]['text':' Distributed training (should be after apex fp16 initialization)','line_number':1348,'multiline':False]['text':' Distributed training using PyTorch FSDP','line_number':1349,'multiline':False]['text':' Transformer layer class to wrap','line_number':1382,'multiline':False]['text':' Apply gradient checkpointing to auto-wrapped sub-modules if specified','line_number':1387,'multiline':False]['text':' Wrap the base model with an outer FSDP wrapper','line_number':1391,'multiline':False]['text':' Patch `xm.optimizer_step` should not reduce gradients in this case,','line_number':1399,'multiline':False]['text':' as FSDP does not need gradient reduction over sharded parameters.','line_number':1400,'multiline':False]['text':' find_unused_parameters breaks checkpointing as per','line_number':1419,'multiline':False]['text':' https://github.com/huggingface/transformers/pull/4659#issuecomment-643356021','line_number':1420,'multiline':False]['text':' memory metrics - must set up as early as possible','line_number':1461,'multiline':False]['text':' Attach NEFTune hooks if necessary','line_number':1468,'multiline':False]['text':' do_train is not a reliable argument, as it might not be set and .train() still called, so','line_number':1472,'multiline':False]['text':' the following is a workaround:','line_number':1473,'multiline':False]['text':' This might change the seed so needs to run first.','line_number':1486,'multiline':False]['text':' Model re-init','line_number':1490,'multiline':False]['text':' Seed must be set before instantiating the model when using model_init.','line_number':1493,'multiline':False]['text':' Reinitializes optimizer and scheduler','line_number':1497,'multiline':False]['text':' Load potential model checkpoint','line_number':1500,'multiline':False]['text':' In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly','line_number':1513,'multiline':False]['text':' If model was re-initialized, put it on the right device and update self.model_wrapped','line_number':1518,'multiline':False]['text':' Disable progress bars when uploading models during checkpoints to avoid polluting stdout','line_number':1529,'multiline':False]['text':' Data loader and number of training steps','line_number':1555,'multiline':False]['text':' Setting up training control variables:','line_number':1558,'multiline':False]['text':' number of training epochs: num_train_epochs','line_number':1559,'multiline':False]['text':' number of training steps per epoch: num_update_steps_per_epoch','line_number':1560,'multiline':False]['text':' total number of training steps to execute: max_steps','line_number':1561,'multiline':False]['text':' May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's','line_number':1576,'multiline':False]['text':' the best we can do.','line_number':1577,'multiline':False]['text':' Rely on max_steps when dataloader does not have a working size','line_number':1589,'multiline':False]['text':' Setting a very large number of epochs so we go as many times as necessary over the iterator.','line_number':1591,'multiline':False]['text':' nn.DataParallel(model) replicates the model, creating new variables and module','line_number':1606,'multiline':False]['text':' references registered here no longer work on other gpus, breaking the module','line_number':1607,'multiline':False]['text':' noqa','line_number':1613,'multiline':False]['text':' We need to reset the scheduler, as its parameters may be different on subsequent calls','line_number':1617,'multiline':False]['text':' Compute absolute values for logging, eval, and save if given as ratio','line_number':1632,'multiline':False]['text':' Activate gradient checkpointing if needed','line_number':1649,'multiline':False]['text':' as the model is wrapped, don't use `accelerator.prepare`','line_number':1660,'multiline':False]['text':' this is for unhandled cases such as','line_number':1661,'multiline':False]['text':' FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX','line_number':1662,'multiline':False]['text':' prepare using `accelerator` prepare','line_number':1668,'multiline':False]['text':' to handle cases wherein we pass "DummyScheduler" such as when it is specified in DeepSpeed config.','line_number':1677,'multiline':False]['text':' for the rest of this function `model` is the outside model, whether it was wrapped or not','line_number':1685,'multiline':False]['text':' backward compatibility','line_number':1689,'multiline':False]['text':' ckpt loading','line_number':1693,'multiline':False]['text':' Check if saved optimizer or scheduler states exist','line_number':1700,'multiline':False]['text':' important: at this point:','line_number':1703,'multiline':False]['text':' self.model         is the Transformers Model','line_number':1704,'multiline':False]['text':' self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),','line_number':1705,'multiline':False]['text':' FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.','line_number':1706,'multiline':False]['text':' Train!','line_number':1708,'multiline':False]['text':' Check if continuing training from a checkpoint','line_number':1726,'multiline':False]['text':' Update the references','line_number':1747,'multiline':False]['text':' use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial','line_number':1753,'multiline':False]['text':' parameter to Train when using DDP.','line_number':1754,'multiline':False]['text':' This should be the same if the state has been saved but in case the training arguments changed, it's safer','line_number':1761,'multiline':False]['text':' to set this after the load.','line_number':1762,'multiline':False]['text':' tr_loss is a tensor to avoid synchronization of TPUs through .item()','line_number':1768,'multiline':False]['text':' _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses','line_number':1770,'multiline':False]['text':' Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.','line_number':1777,'multiline':False]['text':' We just need to begin an iteration to create the randomization of the sampler.','line_number':1786,'multiline':False]['text':' Otherwise we need to call the whooooole sampler cause there is some random operation added','line_number':1790,'multiline':False]['text':' AT THE VERY END!','line_number':1791,'multiline':False]['text':' Reset the past mems state at the beginning of each epoch if necessary.','line_number':1801,'multiline':False]['text':' Skip past any already trained steps if resuming training','line_number':1841,'multiline':False]['text':' if loss is nan or inf simply add the average of previous logged losses','line_number':1864,'multiline':False]['text':' last step in epoch but step is always smaller than gradient_accumulation_steps','line_number':1878,'multiline':False]['text':' the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered','line_number':1881,'multiline':False]['text':' in accelerate. So, explicitly enable sync gradients to True in that case.','line_number':1882,'multiline':False]['text':' Gradient clipping','line_number':1886,'multiline':False]['text':' deepspeed does its own clipping','line_number':1888,'multiline':False]['text':' Revert to normal clipping otherwise, handling Apex or full precision','line_number':1893,'multiline':False]['text':' Optimizer step','line_number':1904,'multiline':False]['text':' Delay optimizer scheduling until metrics are generated','line_number':1908,'multiline':False]['text':' tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)','line_number':1936,'multiline':False]['text':' Clean the state at the end of training','line_number':1947,'multiline':False]['text':' Wait for everyone to get here so we are sure the model has been saved by process 0.','line_number':1952,'multiline':False]['text':' add remaining tr_loss','line_number':1962,'multiline':False]['text':' Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.','line_number':1986,'multiline':False]['text':' Wait for the checkpoint to be uploaded.','line_number':1995,'multiline':False]['text':' After training we make sure to retrieve back the original forward pass method','line_number':1998,'multiline':False]['text':' for the embedding layer by removing the forward post hook.','line_number':1999,'multiline':False]['text':' If the model is on the GPU, it still works!','line_number':2074,'multiline':False]['text':' If the 'user_content.pt' file exists, load with the new smp api.','line_number':2077,'multiline':False]['text':' Checkpoint must have been saved with the new smp api.','line_number':2078,'multiline':False]['text':' If the 'user_content.pt' file does NOT exist, load with the old smp api.','line_number':2083,'multiline':False]['text':' Checkpoint must have been saved with the old smp api.','line_number':2084,'multiline':False]['text':' Required for smp to not auto-translate state_dict from hf to smp (is already smp).','line_number':2090,'multiline':False]['text':' release memory','line_number':2093,'multiline':False]['text':' We load the model state dict on the CPU to avoid an OOM error.','line_number':2098,'multiline':False]['text':' workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963','line_number':2104,'multiline':False]['text':' which takes *args instead of **kwargs','line_number':2105,'multiline':False]['text':' release memory','line_number':2107,'multiline':False]['text':' Load adapters following PR # 24096','line_number':2111,'multiline':False]['text':' If train a model using PEFT & LoRA, assume that adapter have been saved properly.','line_number':2113,'multiline':False]['text':' We load the sharded checkpoint','line_number':2126,'multiline':False]['text':' If the 'user_content.pt' file exists, load with the new smp api.','line_number':2156,'multiline':False]['text':' Checkpoint must have been saved with the new smp api.','line_number':2157,'multiline':False]['text':' If the 'user_content.pt' file does NOT exist, load with the old smp api.','line_number':2165,'multiline':False]['text':' Checkpoint must have been saved with the old smp api.','line_number':2166,'multiline':False]['text':' If train a model using PEFT & LoRA, assume that adapter have been saved properly.','line_number':2176,'multiline':False]['text':' Load_adapter has no return value present, modify it when appropriate.','line_number':2180,'multiline':False]['text':' We load the model state dict on the CPU to avoid an OOM error.','line_number':2195,'multiline':False]['text':' If the model is on the GPU, it still works!','line_number':2201,'multiline':False]['text':' workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963','line_number':2202,'multiline':False]['text':' which takes *args instead of **kwargs','line_number':2203,'multiline':False]['text':' all_gather + mean() to get average loss over all processes','line_number':2239,'multiline':False]['text':' reset tr_loss to zero','line_number':2242,'multiline':False]['text':' Run delayed LR scheduler now that metrics are populated','line_number':2269,'multiline':False]['text':' Load RNG states from `checkpoint`','line_number':2281,'multiline':False]['text':' In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we','line_number':2333,'multiline':False]['text':' want to save except FullyShardedDDP.','line_number':2334,'multiline':False]['text':' assert unwrap_model(model) is self.model, "internal model should be a reference to self.model"','line_number':2335,'multiline':False]['text':' Save model checkpoint','line_number':2337,'multiline':False]['text':' Save optimizer and scheduler','line_number':2356,'multiline':False]['text':' Save RNG state','line_number':2358,'multiline':False]['text':' Determine the new best metric / best model checkpoint','line_number':2361,'multiline':False]['text':' Save the Trainer state','line_number':2377,'multiline':False]['text':' Place checkpoint in final location after all saving is finished.','line_number':2384,'multiline':False]['text':' First wait for everyone to finish writing','line_number':2385,'multiline':False]['text':' Then go through the rewriting process starting on process 0','line_number':2387,'multiline':False]['text':' Maybe delete some older checkpoints.','line_number':2393,'multiline':False]['text':' Save RNG state in non-distributed training','line_number':2398,'multiline':False]['text':' In non distributed, we save the global CUDA RNG state (will take care of DataParallel)','line_number':2406,'multiline':False]['text':' A process can arrive here before the process 0 has a chance to save the model, in which case output_dir may','line_number':2420,'multiline':False]['text':' not yet exist.','line_number':2421,'multiline':False]['text':' under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed','line_number':2447,'multiline':False]['text':' config `stage3_gather_16bit_weights_on_model_save` is True','line_number':2448,'multiline':False]['text':' save fsdp specific ckpt for resuming from ckpt','line_number':2451,'multiline':False]['text':' deepspeed.save_checkpoint above saves model/optim/sched','line_number':2457,'multiline':False]['text':' Save SCHEDULER & SCALER','line_number':2460,'multiline':False]['text':' deepspeed loads optimizer/lr_scheduler together with the model in deepspeed_init','line_number':2479,'multiline':False]['text':' Load in optimizer and scheduler states','line_number':2503,'multiline':False]['text':' On TPU we have to take some extra precautions to properly load the states on the right device.','line_number':2505,'multiline':False]['text':' Optimizer checkpoint was saved with smp >= 1.10','line_number':2519,'multiline':False]['text':' Optimizer checkpoint was saved with smp < 1.10','line_number':2524,'multiline':False]['text':' We use the CPU when training on one GPU to avoid OOM for GPU RAM when training big models.','line_number':2535,'multiline':False]['text':' In distributed training however, we load directly on each GPU and risk the GPU OOM as it's more','line_number':2536,'multiline':False]['text':' likely to get OOM on CPU (since we load num_gpu times the optimizer state','line_number':2537,'multiline':False]['text':' NLP models inputs are int/uint and those get adjusted to the right dtype of the','line_number':2664,'multiline':False]['text':' embedding. Other models such as wav2vec2's inputs are already float and thus','line_number':2665,'multiline':False]['text':' may need special handling to match the dtypes of the model','line_number':2666,'multiline':False]['text':' mean() to average on multi-gpu parallel training','line_number':2734,'multiline':False]['text':' Save past state if it exists','line_number':2755,'multiline':False]['text':' TODO: this needs to be fixed and made cleaner later.','line_number':2756,'multiline':False]['text':' We don't use .loss here since the model may return tuples instead of ModelOutput.','line_number':2776,'multiline':False]['text':' Special case for SageMaker ModelParallel since there process_index is dp_process_index, not the global','line_number':2793,'multiline':False]['text':' process index.','line_number':2794,'multiline':False]['text':' Calling the state_dict needs to be done on the wrapped model and on all processes.','line_number':2813,'multiline':False]['text':' 'user_content.pt' indicates model state_dict saved with smp >= 1.10','line_number':2819,'multiline':False]['text':' remove the dummy state_dict','line_number':2840,'multiline':False]['text':' Push to the Hub when `save_model` is called by the user.','line_number':2847,'multiline':False]['text':' Save a trained model and configuration using `save_pretrained()`.','line_number':2859,'multiline':False]['text':' They can then be reloaded using `from_pretrained()`','line_number':2860,'multiline':False]['text':' If we are executing this function, we are the process zero, so we don't check for that.','line_number':2882,'multiline':False]['text':' Save a trained model and configuration using `save_pretrained()`.','line_number':2888,'multiline':False]['text':' They can then be reloaded using `from_pretrained()`','line_number':2889,'multiline':False]['text':' Good practice: save your training arguments together with the trained model','line_number':2912,'multiline':False]['text':' Storing the number of floating-point operations that went into the model','line_number':2916,'multiline':False]['text':' Make sure we don't delete the best model.','line_number':2943,'multiline':False]['text':' Check if we should delete older checkpoint(s)','line_number':2957,'multiline':False]['text':' If save_total_limit=1 with load_best_model_at_end=True, we could end up deleting the last checkpoint, which','line_number':2962,'multiline':False]['text':' we don't do to allow resuming.','line_number':2963,'multiline':False]['text':' memory metrics - must set up as early as possible','line_number':3008,'multiline':False]['text':' No point gathering the predictions if there are no metrics, otherwise we defer to','line_number':3018,'multiline':False]['text':' self.args.prediction_loss_only','line_number':3019,'multiline':False]['text':' tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)','line_number':3040,'multiline':False]['text':' memory metrics - must set up as early as possible','line_number':3084,'multiline':False]['text':' if eval is called w/o train, handle model prep here','line_number':3128,'multiline':False]['text':' for the rest of this function `model` is the outside model, whether it was wrapped or not','line_number':3144,'multiline':False]['text':' backward compatibility','line_number':3148,'multiline':False]['text':' if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called','line_number':3152,'multiline':False]['text':' while ``train`` is running, cast it to the right dtype first and then put on device','line_number':3153,'multiline':False]['text':' Do this before wrapping.','line_number':3172,'multiline':False]['text':' Initialize containers','line_number':3178,'multiline':False]['text':' losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)','line_number':3179,'multiline':False]['text':' losses/preds/labels on CPU (final containers)','line_number':3185,'multiline':False]['text':' Will be useful when we have an iterable dataset so don't know its length.','line_number':3190,'multiline':False]['text':' Main evaluation loop','line_number':3193,'multiline':False]['text':' Update the observed num examples','line_number':3195,'multiline':False]['text':' For batch samplers, batch_size is not known by the dataloader in advance.','line_number':3199,'multiline':False]['text':' Prediction step','line_number':3203,'multiline':False]['text':' Update containers on host','line_number':3211,'multiline':False]['text':' Gather all tensors and put them back on the CPU if we have done enough accumulation steps.','line_number':3238,'multiline':False]['text':' Set back to None to begin a new accumulation','line_number':3259,'multiline':False]['text':' After all calls to `.gather_function`, reset to `gather_for_metrics`:','line_number':3262,'multiline':False]['text':' Clean the state at the end of the evaluation loop','line_number':3265,'multiline':False]['text':' Gather all remaining tensors and put them back on the CPU','line_number':3268,'multiline':False]['text':' Number of samples','line_number':3284,'multiline':False]['text':' The instance check is weird and does not actually check for the type, but whether the dataset has the right','line_number':3287,'multiline':False]['text':' methods. Therefore we need to make sure it also has the attribute.','line_number':3288,'multiline':False]['text':' both len(dataloader.dataset) and len(dataloader) fail','line_number':3294,'multiline':False]['text':' Metrics!','line_number':3299,'multiline':False]['text':' To be JSON-serializable, we need to remove numpy types or zero-d tensors','line_number':3310,'multiline':False]['text':' Prefix all keys with metric_key_prefix + '_'','line_number':3318,'multiline':False]['text':' For CLIP-like models capable of returning loss values.','line_number':3375,'multiline':False]['text':' If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`','line_number':3376,'multiline':False]['text':' is `True` in `model.forward`.','line_number':3377,'multiline':False]['text':' labels may be popped when computing the loss (label smoothing for instance) so we grab them first.','line_number':3390,'multiline':False]['text':' TODO: this needs to be fixed and made cleaner later.','line_number':3436,'multiline':False]['text':' Only on process zero','line_number':3471,'multiline':False]['text':' Only push from one node.','line_number':3550,'multiline':False]['text':' If we haven't finished the last push, we don't do this one unless args.hub_always_push=True.','line_number':3553,'multiline':False]['text':' To avoid a new synchronization of all model weights, we just copy the file from the checkpoint folder','line_number':3558,'multiline':False]['text':' Saving the tokenizer is fast and we don't know how many files it may have spawned, so we resave it to be sure.','line_number':3565,'multiline':False]['text':' Same for the training arguments','line_number':3568,'multiline':False]['text':' In case the user calls this method with args.push_to_hub = False','line_number':3636,'multiline':False]['text':' Needs to be executed on all processes for TPU training, but will only save on the processed determined by','line_number':3640,'multiline':False]['text':' self.args.should_save.','line_number':3641,'multiline':False]['text':' Only push from one node.','line_number':3644,'multiline':False]['text':' Wait for the current upload to be finished.','line_number':3650,'multiline':False]['text':'','line_number':3661,'multiline':False]['text':' Deprecated code','line_number':3662,'multiline':False]['text':'','line_number':3663,'multiline':False]['text':' if eval is called w/o train, handle model prep here','line_number':3685,'multiline':False]['text':' for the rest of this function `model` is the outside model, whether it was wrapped or not','line_number':3701,'multiline':False]['text':' backward compatibility','line_number':3705,'multiline':False]['text':' if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called','line_number':3709,'multiline':False]['text':' while ``train`` is running, cast it to the right dtype first and then put on device','line_number':3710,'multiline':False]['text':' The actual number of eval_sample can be greater than num_examples in distributed settings (when we pass','line_number':3731,'multiline':False]['text':' a batch size to the sampler)','line_number':3732,'multiline':False]['text':' Gather all tensors and put them back on the CPU if we have done enough accumulation steps.','line_number':3767,'multiline':False]['text':' Set back to None to begin a new accumulation','line_number':3775,'multiline':False]['text':' Clean the state at the end of the evaluation loop','line_number':3779,'multiline':False]['text':' Gather all remaining tensors and put them back on the CPU','line_number':3782,'multiline':False]['text':' To be JSON-serializable, we need to remove numpy types or zero-d tensors','line_number':3804,'multiline':False]['text':' Prefix all keys with metric_key_prefix + '_'','line_number':3810,'multiline':False]['text':' Make sure we only do this on the main process','line_number':3835,'multiline':False]['text':' Get current .gitignore content','line_number':3841,'multiline':False]['text':' Add the patterns to .gitignore','line_number':3848,'multiline':False]['text':' Write the .gitignore file if it has changed','line_number':3857,'multiline':False]['text':' avoid race condition with git status','line_number':3865,'multiline':False]['text':' create accelerator object','line_number':3877,'multiline':False]['text':' some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag','line_number':3884,'multiline':False]['text':' deepspeed and accelerate flags covering both trainer args and accelerate launcher','line_number':3887,'multiline':False]['text':' post accelerator creation setup','line_number':3891,'multiline':False]