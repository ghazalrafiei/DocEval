['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' torchrun support','line_number':76,'multiline':False]['text':' https://github.com/pytorch/xla/pull/3609','line_number':77,'multiline':False]['text':' just an alias for adamw_bnb_8bit','line_number':149,'multiline':False]['text':' TODO: `TrainingArguments` users rely on it being fully mutable. In the future see if we can narrow this to a few keys: https://github.com/huggingface/transformers/pull/25903','line_number':159,'multiline':False]['text':' Do not touch this type annotation or it will stop working in CLI','line_number':1057,'multiline':False]['text':' Do not touch this type annotation or it will stop working in CLI','line_number':1076,'multiline':False]['text':' XXX: enable when pytorch==2.0.1 comes out - we want to give it time to get all the bugs sorted out','line_number':1091,'multiline':False]['text':' if is_torch_available() and version.parse(version.parse(torch.__version__).base_version) >= version.parse("2.1.0"):','line_number':1092,'multiline':False]['text':'     default_optim = "adamw_torch_fused"','line_number':1093,'multiline':False]['text':' and update the doc above to:','line_number':1094,'multiline':False]['text':' optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `"adamw_torch_fused"` (for torch<2.1.0 `"adamw_torch"`):','line_number':1095,'multiline':False]['text':' Deprecated arguments','line_number':1190,'multiline':False]['text':' expand paths, if not os.makedirs("~/bar") will make directory','line_number':1310,'multiline':False]['text':' in the current directory instead of the actual home','line_number':1311,'multiline':False]['text':' see https://github.com/huggingface/transformers/issues/10628','line_number':1312,'multiline':False]['text':' Go back to the underlying string or we won't be able to instantiate `IntervalStrategy` on it.','line_number':1329,'multiline':False]['text':' eval_steps has to be defined and non-zero, fallbacks to logging_steps if the latter is non-zero','line_number':1348,'multiline':False]['text':' logging_steps must be non-zero for logging_strategy that is other than 'no'','line_number':1359,'multiline':False]['text':' Sanity checks for load_best_model_at_end: we require save and eval strategies to be compatible.','line_number':1376,'multiline':False]['text':' Work around floating point precision issues','line_number':1391,'multiline':False]['text':' cpu','line_number':1433,'multiline':False]['text':' gpu','line_number':1437,'multiline':False]['text':' npu','line_number':1442,'multiline':False]['text':' xpu','line_number':1451,'multiline':False]['text':' there is a bug in fp16/AMP in pt-2.0.0','line_number':1486,'multiline':False]['text':' accelerate integration for torch compile','line_number':1532,'multiline':False]['text':' set env vars for accelerate','line_number':1534,'multiline':False]['text':' no need to assert on else','line_number':1564,'multiline':False]['text':' if training args is specified, it will override the one specified in the accelerate config','line_number':1566,'multiline':False]['text':' Import at runtime to avoid a circular import.','line_number':1583,'multiline':False]['text':' if fsdp_config["transformer_layer_cls_to_wrap"] is specified as a string, convert it to a list with a single object','line_number':1630,'multiline':False]['text':' store XLA fsdp configuration parameters into a dictionary','line_number':1658,'multiline':False]['text':' apply appropriate string to torch.dtype conversions for parameters','line_number':1660,'multiline':False]['text':' accelerate integration for FSDP','line_number':1671,'multiline':False]['text':' set environment variable for FSDP sharding strategy','line_number':1682,'multiline':False]['text':' - must be run very last in arg parsing, since it will use a lot of these settings.','line_number':1722,'multiline':False]['text':' - must be run before the model is created.','line_number':1723,'multiline':False]['text':' will be used later by the Trainer','line_number':1728,'multiline':False]['text':' note: leave self.deepspeed unmodified in case a user relies on it not to be modified)','line_number':1729,'multiline':False]['text':' Accelerate DeepSpeed Plugin','line_number':1733,'multiline':False]['text':' Accelerate DeepSpeed Plugin','line_number':1739,'multiline':False]['text':' Remove deprecated arguments. That code should be removed once','line_number':1788,'multiline':False]['text':' those deprecated arguments are removed from TrainingArguments. (TODO: v5)','line_number':1789,'multiline':False]['text':' Need to do similar for Accelerator init','line_number':1865,'multiline':False]['text':' Already set _n_gpu','line_number':1887,'multiline':False]['text':' if n_gpu is > 1 we'll use nn.DataParallel.','line_number':1921,'multiline':False]['text':' If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`','line_number':1922,'multiline':False]['text':' Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will','line_number':1923,'multiline':False]['text':' trigger an error that a device index is missing. Index 0 takes into account the','line_number':1924,'multiline':False]['text':' GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`','line_number':1925,'multiline':False]['text':' will use the first GPU in that env, i.e. GPU#1','line_number':1926,'multiline':False]['text':' Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at','line_number':1928,'multiline':False]['text':' the default value.','line_number':1929,'multiline':False]['text':' Make sure `self._n_gpu` is properly setup.','line_number':1953,'multiline':False]['text':' convert to int','line_number':2062,'multiline':False]['text':' tell all replicas to wait','line_number':2118,'multiline':False]['text':' the wait is over','line_number':2128,'multiline':False]['text':' filter out fields that are defined as field(init=False)','line_number':2151,'multiline':False]['text':' The following methods are there to simplify the instantiation of `TrainingArguments`','line_number':2182,'multiline':False]