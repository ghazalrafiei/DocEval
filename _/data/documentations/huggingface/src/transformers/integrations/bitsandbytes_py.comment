['text':' Recurse if needed','line_number':43,'multiline':False]['text':' Support models using `Conv1D` in place of `nn.Linear` (e.g. gpt2) by transposing the weight matrix prior to quantization.','line_number':89,'multiline':False]['text':' Since weights are saved in the correct "orientation", we skip transposing when loading.','line_number':90,'multiline':False]['text':' Check if the current key is not in the `modules_to_not_convert`','line_number':133,'multiline':False]['text':' Store the module class in case we need to transpose the weight later','line_number':167,'multiline':False]['text':' Force requires grad to False to avoid unexpected errors','line_number':169,'multiline':False]['text':' Remove the last key for recursion','line_number':179,'multiline':False]['text':' For backward compatibility','line_number':225,'multiline':False]['text':' For backward compatiblity','line_number':234,'multiline':False]['text':' Create a copy of the model and tie the weights, then','line_number':254,'multiline':False]['text':' check if it contains tied weights','line_number':255,'multiline':False]['text':' this has 0 cost since it is done inside `init_empty_weights` context manager`','line_number':256,'multiline':False]['text':' For compatibility with Accelerate < 0.18','line_number':260,'multiline':False]['text':' If there is not tied weights, we want to keep the lm_headï¼ˆoutput_embedding) in full precision','line_number':267,'multiline':False]['text':' otherwise, no tied weights, no output embedding defined, simply keep the last module in full precision','line_number':274,'multiline':False]['text':' add last module together with tied weights','line_number':277,'multiline':False]['text':' remove ".weight" from the keys','line_number':281,'multiline':False]