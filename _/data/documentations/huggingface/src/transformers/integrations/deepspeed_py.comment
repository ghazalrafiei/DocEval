['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' Check we're not importing a "deepspeed" directory somewhere but the actual library by trying to grab the version','line_number':38,'multiline':False]['text':' AND checking it has an author field in the metadata that is HuggingFace.','line_number':39,'multiline':False]['text':' Inherits from a dummy `object` if accelerate is not available, so that python succeeds to import this file.','line_number':51,'multiline':False]['text':' Deepspeed glue code will never inherit this dummy object as it checks if accelerate is available.','line_number':52,'multiline':False]['text':' set global weakref object','line_number':74,'multiline':False]['text':' DeepSpeed does:','line_number':137,'multiline':False]['text':' train_batch_size = world_size * train_micro_batch_size_per_gpu * gradient_accumulation_steps','line_number':138,'multiline':False]['text':' not a trainer arg','line_number':152,'multiline':False]['text':' total_num_steps - will get set in trainer_config_finalize','line_number':154,'multiline':False]['text':' fp16','line_number':156,'multiline':False]['text':' deepspeed uses shared storage by default. Let's override this setting if save_on_each_node == True','line_number':163,'multiline':False]['text':' amp: similar to the pytorch native amp - it has a bunch of optional params but we won't set','line_number':167,'multiline':False]['text':' any here unless the user did the work','line_number':168,'multiline':False]['text':' apex: delegates amp work to apex (which needs to be available), but it cannot be used with any','line_number':175,'multiline':False]['text':' ZeRO features','line_number':176,'multiline':False]['text':' deepspeed's default mode is fp16 unless there is a config that says differently','line_number':182,'multiline':False]['text':' zero','line_number':196,'multiline':False]['text':' deal with config keys that use `auto` value and rely on model's hidden_size','line_number':198,'multiline':False]['text':' if there are many hidden sizes pick the largest one','line_number':210,'multiline':False]['text':' automatically assign the optimal config values based on model config','line_number':222,'multiline':False]['text':' scheduler','line_number':226,'multiline':False]['text':' keep the config object global to be able to access it anywhere during TrainingArguments life-cycle','line_number':238,'multiline':False]['text':' this is a special weakref global object to allow us to get to Deepspeed config from APIs','line_number':243,'multiline':False]['text':' that don't have an easy way to get to the Deepspeed config outside of the Trainer domain.','line_number':244,'multiline':False]['text':' will go away automatically when HfDeepSpeedConfig is destroyed (when TrainingArguments is destroyed)','line_number':246,'multiline':False]['text':' useful for unit tests to ensure the global state doesn't leak - call from `tearDown` method','line_number':251,'multiline':False]['text':' Optimizer + Scheduler','line_number':278,'multiline':False]['text':' Currently supported combos:','line_number':279,'multiline':False]['text':' 1. DS scheduler + DS optimizer: Yes','line_number':280,'multiline':False]['text':' 2. HF scheduler + HF optimizer: Yes','line_number':281,'multiline':False]['text':' 3. DS scheduler + HF optimizer: Yes','line_number':282,'multiline':False]['text':' 4. HF scheduler + DS optimizer: No','line_number':283,'multiline':False]['text':'','line_number':284,'multiline':False]['text':' Unless Offload is enabled in which case it's:','line_number':285,'multiline':False]['text':' 1. DS scheduler + DS optimizer: Yes','line_number':286,'multiline':False]['text':' 2. HF scheduler + HF optimizer: Mostly*','line_number':287,'multiline':False]['text':' 3. DS scheduler + HF optimizer: Mostly*','line_number':288,'multiline':False]['text':' 4. HF scheduler + DS optimizer: Yes','line_number':289,'multiline':False]['text':'','line_number':290,'multiline':False]['text':' Mostly*: All non-native DeepSpeed optimizers that have both CPU and GPU implementation should work (except LAMB)','line_number':291,'multiline':False]['text':' ds supports Adam, OneBitAdam, and Lamb optimizers and can import other optimizers from torch.','line_number':308,'multiline':False]['text':' But trainer uses AdamW by default.','line_number':309,'multiline':False]['text':' To use other optimizers requires voiding warranty with: `zero_allow_untested_optimizer`','line_number':311,'multiline':False]['text':' resume config update - some bits like `model` and `num_training_steps` only become available during train','line_number':361,'multiline':False]['text':' set the Deepspeed log level consistent with the Trainer','line_number':364,'multiline':False]['text':' only Z3 makes sense for the inference','line_number':368,'multiline':False]['text':' in case the training config is re-used for inference','line_number':372,'multiline':False]['text':' important for when deepspeed_init is used as re-init','line_number':378,'multiline':False]['text':' keep for quick debug:','line_number':384,'multiline':False]['text':' from pprint import pprint; pprint(config)','line_number':385,'multiline':False]['text':' it's possible that the user is trying to resume from model_path, which doesn't necessarily','line_number':391,'multiline':False]['text':' contain a deepspeed checkpoint. e.g. examples just check if the dir exists and assume it's','line_number':392,'multiline':False]['text':' a resume from a checkpoint and not just a local pretrained weight. So we check here if the','line_number':393,'multiline':False]['text':' path contains what looks like a deepspeed checkpoint','line_number':394,'multiline':False]['text':' this magically updates self.optimizer and self.lr_scheduler','line_number':401,'multiline':False]