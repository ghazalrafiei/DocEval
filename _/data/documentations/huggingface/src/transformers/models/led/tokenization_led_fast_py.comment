['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 Iz Beltagy, Matthew E. Peters, Arman Cohan and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast.__init__','line_number':137,'multiline':False]['text':' we have to specify that this tokens is special otherwise adding it will reset the normalized flag to `False` in `add_special_tokens`','line_number':155,'multiline':False]['text':' the pre_tokenizer is already updated in the GPT2TokenizerFast `__init__`','line_number':186,'multiline':False]['text':' The lists 'sep' and 'cls' must be cased in tuples for the object `post_processor_class`','line_number':192,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast.mask_token with BART->LED','line_number':214,'multiline':False]['text':' Mask token behave like a normal word, i.e. include the space before it','line_number':236,'multiline':False]['text':' So we set lstrip to True','line_number':237,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast._batch_encode_plus','line_number':241,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast._encode_plus','line_number':253,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast.save_vocabulary','line_number':265,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast.build_inputs_with_special_tokens','line_number':270,'multiline':False]['text':' Copied from transformers.models.bart.tokenization_bart_fast.BartTokenizerFast.create_token_type_ids_from_sequences with BART->LED','line_number':278,'multiline':False]['text':' Copied from transformers.models.led.tokenization_led.LEDTokenizer._pad','line_number':302,'multiline':False]['text':' Load from model defaults','line_number':319,'multiline':False]['text':' `global_attention_mask` need to have the same length as other (sequential) inputs.','line_number':325,'multiline':False]['text':' Use `-1` since `0` in `global_attention_mask` means `local attention` instead of `not to attend`','line_number':332,'multiline':False]