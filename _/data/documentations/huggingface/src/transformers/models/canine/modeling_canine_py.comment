['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 Google AI The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all CANINE models at https://huggingface.co/models?filter=canine','line_number':58,'multiline':False]['text':' Support up to 16 hash functions.','line_number':61,'multiline':False]['text':' Load weights from TF model','line_number':115,'multiline':False]['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v','line_number':127,'multiline':False]['text':' which are not required for using pretrained model','line_number':128,'multiline':False]['text':' also discard the cls weights (which were used for the next sentence prediction pre-training task)','line_number':129,'multiline':False]['text':' if first scope name starts with "bert", change it to "encoder"','line_number':146,'multiline':False]['text':' remove "embeddings" middle name of HashBucketCodepointEmbedders','line_number':149,'multiline':False]['text':' rename segment_embeddings to token_type_embeddings','line_number':152,'multiline':False]['text':' rename initial convolutional projection layer','line_number':155,'multiline':False]['text':' rename final convolutional projection layer','line_number':158,'multiline':False]['text':' character embeddings','line_number':205,'multiline':False]['text':' self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load','line_number':213,'multiline':False]['text':' any TensorFlow checkpoint file','line_number':214,'multiline':False]['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':218,'multiline':False]['text':' self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load','line_number':312,'multiline':False]['text':' any TensorFlow checkpoint file','line_number':313,'multiline':False]['text':' `cls_encoding`: [batch, 1, hidden_size]','line_number':317,'multiline':False]['text':' char_encoding has shape [batch, char_seq, hidden_size]','line_number':320,'multiline':False]['text':' We transpose it to be [batch, hidden_size, char_seq]','line_number':321,'multiline':False]['text':' Truncate the last molecule in order to reserve a position for [CLS].','line_number':327,'multiline':False]['text':' Often, the last position is never used (unless we completely fill the','line_number':328,'multiline':False]['text':' text buffer). This is important in order to maintain alignment on TPUs','line_number':329,'multiline':False]['text':' (i.e. a multiple of 128).','line_number':330,'multiline':False]['text':' We also keep [CLS] as a separate sequence position since we always','line_number':333,'multiline':False]['text':' want to reserve a position (and the model capacity that goes along','line_number':334,'multiline':False]['text':' with that) in the deep BERT stack.','line_number':335,'multiline':False]['text':' `result`: [batch, molecule_seq, molecule_dim]','line_number':336,'multiline':False]['text':' self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load','line_number':360,'multiline':False]['text':' any TensorFlow checkpoint file','line_number':361,'multiline':False]['text':' inputs has shape [batch, mol_seq, molecule_hidden_size+char_hidden_final]','line_number':370,'multiline':False]['text':' we transpose it to be [batch, molecule_hidden_size+char_hidden_final, mol_seq]','line_number':371,'multiline':False]['text':' PyTorch < 1.9 does not support padding="same" (which is used in the original implementation),','line_number':374,'multiline':False]['text':' so we pad the tensor manually before passing it to the conv layer','line_number':375,'multiline':False]['text':' based on https://github.com/google-research/big_transfer/blob/49afe42338b62af9fbe18f0258197a33ee578a6b/bit_tf2/models.py#L36-L38','line_number':376,'multiline':False]['text':' `result`: shape (batch_size, char_seq_len, hidden_size)','line_number':382,'multiline':False]['text':' Limit transformer query seq and attention mask to these character','line_number':391,'multiline':False]['text':' positions to greatly reduce the compute cost. Typically, this is just','line_number':392,'multiline':False]['text':' done for the MLM training task.','line_number':393,'multiline':False]['text':' TODO add support for MLM','line_number':394,'multiline':False]['text':' If this is instantiated as a cross-attention module, the keys','line_number':440,'multiline':False]['text':' and values come from an encoder; the attention mask needs to be','line_number':441,'multiline':False]['text':' such that the encoder's padding tokens are not attended to.','line_number':442,'multiline':False]['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':449,'multiline':False]['text':' fp16 compatibility','line_number':458,'multiline':False]['text':' if attention_mask is 3D, do the following:','line_number':471,'multiline':False]['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':473,'multiline':False]['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':474,'multiline':False]['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':475,'multiline':False]['text':' Apply the attention mask (precomputed for all layers in CanineModel forward() function)','line_number':477,'multiline':False]['text':' Normalize the attention scores to probabilities.','line_number':480,'multiline':False]['text':' This is actually dropping out entire tokens to attend to, which might','line_number':483,'multiline':False]['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':484,'multiline':False]['text':' Mask heads if we want to','line_number':487,'multiline':False]['text':' additional arguments related to local attention','line_number':551,'multiline':False]['text':' Prune linear layers','line_number':575,'multiline':False]['text':' Update hyper params and store pruned heads','line_number':581,'multiline':False]['text':' Create chunks (windows) that we will attend *from* and then concatenate them.','line_number':600,'multiline':False]['text':' We must skip this first position so that our output sequence is the','line_number':604,'multiline':False]['text':' correct length (this matters in the *from* sequence only).','line_number':605,'multiline':False]['text':' Determine the chunks (windows) that will will attend *to*.','line_number':613,'multiline':False]['text':' next, compute attention scores for each pair of windows and concatenate','line_number':627,'multiline':False]['text':' `attention_mask`: <float>[batch_size, from_seq, to_seq]','line_number':633,'multiline':False]['text':' `attention_mask_chunk`: <float>[batch_size, from_seq_chunk, to_seq_chunk]','line_number':634,'multiline':False]['text':' add attentions if we output them','line_number':655,'multiline':False]['text':' add attentions if we output them','line_number':657,'multiline':False]['text':' add self attentions if we output attention weights','line_number':733,'multiline':False]['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':831,'multiline':False]['text':' to the first token.','line_number':832,'multiline':False]['text':' The output weights are the same as the input embeddings, but there is','line_number':861,'multiline':False]['text':' an output-only bias for each token.','line_number':862,'multiline':False]['text':' Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`','line_number':867,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':903,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':904,'multiline':False]['text':' shallow/low-dim transformer encoder to get a initial character encoding','line_number':990,'multiline':False]['text':' deep transformer encoder','line_number':1002,'multiline':False]['text':' shallow/low-dim transformer encoder to get a final character encoding','line_number':1005,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1010,'multiline':False]['text':' We don't assume that `from_tensor` is a mask (although it could be). We','line_number':1038,'multiline':False]['text':' don't actually care if we attend *from* padding tokens (only *to* padding)','line_number':1039,'multiline':False]['text':' tokens so we create a tensor of all ones.','line_number':1040,'multiline':False]['text':' Here we broadcast along two dimensions to create the mask.','line_number':1043,'multiline':False]['text':' first, make char_attention_mask 3D by adding a channel dim','line_number':1051,'multiline':False]['text':' next, apply MaxPool1d to get pooled_molecule_mask of shape (batch_size, 1, mol_seq_len)','line_number':1055,'multiline':False]['text':' finally, squeeze to get tensor of shape (batch_size, mol_seq_len)','line_number':1060,'multiline':False]['text':' `repeated`: [batch_size, almost_char_seq_len, molecule_hidden_size]','line_number':1071,'multiline':False]['text':' So far, we've repeated the elements sufficient for any `char_seq_length`','line_number':1074,'multiline':False]['text':' that's a multiple of `downsampling_rate`. Now we account for the last','line_number':1075,'multiline':False]['text':' n elements (n < `downsampling_rate`), i.e. the remainder of floor','line_number':1076,'multiline':False]['text':' division. We do this by repeating the last molecule a few extra times.','line_number':1077,'multiline':False]['text':' +1 molecule to compensate for truncation.','line_number':1082,'multiline':False]['text':' `repeated`: [batch_size, char_seq_len, molecule_hidden_size]','line_number':1087,'multiline':False]['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1134,'multiline':False]['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1135,'multiline':False]['text':' Prepare head mask if needed','line_number':1144,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':1145,'multiline':False]['text':' attention_probs has shape bsz x n_heads x N x N','line_number':1146,'multiline':False]['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':1147,'multiline':False]['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':1148,'multiline':False]['text':' `input_char_embeddings`: shape (batch_size, char_seq, char_dim)','line_number':1151,'multiline':False]['text':' Contextualize character embeddings using shallow Transformer.','line_number':1159,'multiline':False]['text':' We use a 3D attention mask for the local attention.','line_number':1160,'multiline':False]['text':' `input_char_encoding`: shape (batch_size, char_seq_len, char_dim)','line_number':1161,'multiline':False]['text':' Downsample chars to molecules.','line_number':1173,'multiline':False]['text':' The following lines have dimensions: [batch, molecule_seq, molecule_dim].','line_number':1174,'multiline':False]['text':' In this transformation, we change the dimensionality from `char_dim` to','line_number':1175,'multiline':False]['text':' `molecule_dim`, but do *NOT* add a resnet connection. Instead, we rely on','line_number':1176,'multiline':False]['text':' the resnet connections (a) from the final char transformer stack back into','line_number':1177,'multiline':False]['text':' the original char transformer stack and (b) the resnet connections from','line_number':1178,'multiline':False]['text':' the final char transformer stack back into the deep BERT stack of','line_number':1179,'multiline':False]['text':' molecules.','line_number':1180,'multiline':False]['text':'','line_number':1181,'multiline':False]['text':' Empirically, it is critical to use a powerful enough transformation here:','line_number':1182,'multiline':False]['text':' mean pooling causes training to diverge with huge gradient norms in this','line_number':1183,'multiline':False]['text':' region of the model; using a convolution here resolves this issue. From','line_number':1184,'multiline':False]['text':' this, it seems that molecules and characters require a very different','line_number':1185,'multiline':False]['text':' feature space; intuitively, this makes sense.','line_number':1186,'multiline':False]['text':' Deep BERT encoder','line_number':1189,'multiline':False]['text':' `molecule_sequence_output`: shape (batch_size, mol_seq_len, mol_dim)','line_number':1190,'multiline':False]['text':' Upsample molecules back to characters.','line_number':1202,'multiline':False]['text':' `repeated_molecules`: shape (batch_size, char_seq_len, mol_hidden_size)','line_number':1203,'multiline':False]['text':' Concatenate representations (contextualized char embeddings and repeated molecules):','line_number':1206,'multiline':False]['text':' `concat`: shape [batch_size, char_seq_len, molecule_hidden_size+char_hidden_final]','line_number':1207,'multiline':False]['text':' Project representation dimension back to hidden_size','line_number':1210,'multiline':False]['text':' `sequence_output`: shape (batch_size, char_seq_len, hidden_size])','line_number':1211,'multiline':False]['text':' Apply final shallow Transformer','line_number':1214,'multiline':False]['text':' `sequence_output`: shape (batch_size, char_seq_len, hidden_size])','line_number':1215,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1271,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1367,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1459,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1564,'multiline':False]['text':' If we are on multi-GPU, split add a dimension','line_number':1622,'multiline':False]['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1627,'multiline':False]