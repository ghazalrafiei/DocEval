['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The Pop2Piano Authors and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' using the normal Pop2PianoLayerNorm','line_number':57,'multiline':False]['text':' See all Pop2Piano models at https://huggingface.co/models?filter=pop2piano','line_number':69,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Pop2Piano','line_number':147,'multiline':False]['text':' Pop2Piano uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':158,'multiline':False]['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':159,'multiline':False]['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':160,'multiline':False]['text':' half-precision inputs is done in fp32','line_number':161,'multiline':False]['text':' convert into half-precision if necessary','line_number':166,'multiline':False]['text':' noqa','line_number':174,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->Pop2Piano,t5->pop2piano','line_number':179,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->Pop2Piano','line_number':202,'multiline':False]['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':218,'multiline':False]['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':219,'multiline':False]['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':220,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->Pop2Piano','line_number':232,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5Attention with T5->Pop2Piano,t5->pop2piano','line_number':251,'multiline':False]['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':265,'multiline':False]['text':' Prune linear layers','line_number':282,'multiline':False]['text':' Update hyper params','line_number':287,'multiline':False]['text':' now relative_position is in the range [0, inf)','line_number':321,'multiline':False]['text':' half of the buckets are for exact increments in positions','line_number':323,'multiline':False]['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':327,'multiline':False]['text':' shape (query_length, key_length)','line_number':346,'multiline':False]['text':' shape (query_length, key_length)','line_number':348,'multiline':False]['text':' shape (query_length, key_length, num_heads)','line_number':353,'multiline':False]['text':' shape (1, num_heads, query_length, key_length)','line_number':354,'multiline':False]['text':' Input is (batch_size, seq_length, dim)','line_number':372,'multiline':False]['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':373,'multiline':False]['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':374,'multiline':False]['text':' self-attn','line_number':399,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':400,'multiline':False]['text':' cross-attn','line_number':403,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':404,'multiline':False]['text':' self-attn','line_number':409,'multiline':False]['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':410,'multiline':False]['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':413,'multiline':False]['text':' the provided `key_value_states` to support prefix tuning','line_number':414,'multiline':False]['text':' cross-attn','line_number':415,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':416,'multiline':False]['text':' cross-attn','line_number':419,'multiline':False]['text':' get query states','line_number':423,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':424,'multiline':False]['text':' get key/value states','line_number':426,'multiline':False]['text':' compute scores','line_number':434,'multiline':False]['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':437,'multiline':False]['text':' if key and values are already calculated','line_number':449,'multiline':False]['text':' we want only the last query position bias','line_number':450,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':455,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':467,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':470,'multiline':False]['text':' Mask heads if we want to','line_number':472,'multiline':False]['text':' (batch_size, seq_length, dim)','line_number':476,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->Pop2Piano,t5->pop2piano','line_number':487,'multiline':False]['text':' add attentions if we output them','line_number':516,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->Pop2Piano,t5->pop2piano','line_number':520,'multiline':False]['text':' add attentions if we output them','line_number':553,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5Block with T5->Pop2Piano,t5->pop2piano','line_number':557,'multiline':False]['text':' Keep self-attention outputs and relative position weights','line_number':611,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':613,'multiline':False]['text':' the actual query length is unknown for cross attention','line_number':624,'multiline':False]['text':' if using past key value states. Need to inject it here','line_number':625,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':644,'multiline':False]['text':' Combine self attn and cross attn key value states','line_number':653,'multiline':False]['text':' Keep cross-attention outputs and relative position weights','line_number':657,'multiline':False]['text':' Apply Feed Forward layer','line_number':660,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':663,'multiline':False]['text':' hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':679,'multiline':False]['text':' Used for testing weights initialization','line_number':697,'multiline':False]['text':' Mesh TensorFlow embeddings initialization','line_number':703,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':704,'multiline':False]['text':' Mesh TensorFlow FF initialization','line_number':709,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':710,'multiline':False]['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':711,'multiline':False]['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':729,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':730,'multiline':False]['text':' shift inputs to the right','line_number':750,'multiline':False]['text':' Item assignment is not supported natively for proxies.','line_number':752,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':762,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5Stack.__init__ with T5->Pop2Piano,t5->pop2piano','line_number':769,'multiline':False]['text':' Initialize weights and apply final processing','line_number':782,'multiline':False]['text':' Model parallel','line_number':784,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5Stack.get_input_embeddings','line_number':789,'multiline':False]['text':' Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings','line_number':793,'multiline':False]['text':' required mask seq length can be calculated via length of past','line_number':840,'multiline':False]['text':' initialize past_key_values with `None` if past does not exist','line_number':855,'multiline':False]['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':859,'multiline':False]['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':860,'multiline':False]['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':863,'multiline':False]['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':864,'multiline':False]['text':' Prepare head mask if needed','line_number':881,'multiline':False]['text':' past_key_value is always None with gradient checkpointing','line_number':910,'multiline':False]['text':' layer_outputs is a tuple with:','line_number':929,'multiline':False]['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':930,'multiline':False]['text':' We share the position biases between the layers - the first layer store them','line_number':936,'multiline':False]['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':937,'multiline':False]['text':' (cross-attention position bias), (cross-attention weights)','line_number':938,'multiline':False]['text':' append next layer key value states','line_number':942,'multiline':False]['text':' Add last layer','line_number':954,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1037,'multiline':False]['text':' since self.mel_conditioner adds a new array at the front of inputs_embeds we need to do the same for attention_mask to keep the shapes same','line_number':1103,'multiline':False]['text':' Encode if needed (training, first prediction pass)','line_number':1146,'multiline':False]['text':' Convert encoder inputs in embeddings if needed','line_number':1148,'multiline':False]['text':' get decoder inputs from shifting lm labels to the right','line_number':1168,'multiline':False]['text':' Decode','line_number':1171,'multiline':False]['text':' Rescale output before projecting on vocab','line_number':1190,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1191,'multiline':False]['text':' check for composer_to_feature_token','line_number':1277,'multiline':False]['text':' to control the variation of generated MIDI tokens we concatenate mel-conditioner tokens(which depends on composer_token)','line_number':1292,'multiline':False]['text':' at the front of input_features.','line_number':1293,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':1321,'multiline':False]['text':' if decoder past is not included in output','line_number':1340,'multiline':False]['text':' speedy decoding is disabled and no need to reorder','line_number':1341,'multiline':False]['text':' get the correct batch idx from layer past batch dim','line_number':1348,'multiline':False]['text':' batch dim of `past` is at 2nd position','line_number':1349,'multiline':False]['text':' need to set correct `past` for each of the four key / value states','line_number':1352,'multiline':False]