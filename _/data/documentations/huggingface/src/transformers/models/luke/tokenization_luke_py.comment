['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright Studio-Ouisa and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode','line_number':153,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.get_pairs','line_number':178,'multiline':False]['text':' Mask token behave like a normal word, i.e. include the space before it','line_number':326,'multiline':False]['text':' how to handle errors in decoding','line_number':332,'multiline':False]['text':' Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions','line_number':342,'multiline':False]['text':' we add 2 special tokens for downstream tasks','line_number':345,'multiline':False]['text':' for more information about lstrip and rstrip, see https://github.com/huggingface/transformers/pull/2778','line_number':346,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.vocab_size with Roberta->Luke, RoBERTa->LUKE','line_number':411,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_vocab with Roberta->Luke, RoBERTa->LUKE','line_number':415,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.bpe with Roberta->Luke, RoBERTa->LUKE','line_number':421,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._tokenize with Roberta->Luke, RoBERTa->LUKE','line_number':464,'multiline':False]['text':' Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)','line_number':471,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._convert_token_to_id with Roberta->Luke, RoBERTa->LUKE','line_number':475,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._convert_id_to_token with Roberta->Luke, RoBERTa->LUKE','line_number':480,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.convert_tokens_to_string with Roberta->Luke, RoBERTa->LUKE','line_number':485,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.build_inputs_with_special_tokens with Roberta->Luke, RoBERTa->LUKE','line_number':492,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask with Roberta->Luke, RoBERTa->LUKE','line_number':518,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences with Roberta->Luke, RoBERTa->LUKE','line_number':546,'multiline':False]['text':' Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.prepare_for_tokenization with Roberta->Luke, RoBERTa->LUKE','line_number':570,'multiline':False]['text':' Input type checking for clearer error','line_number':643,'multiline':False]['text':' prepare_for_model will create the attention_mask and token_type_ids','line_number':778,'multiline':False]['text':' input_ids is a list of tuples (one for each example in the batch)','line_number':840,'multiline':False]['text':' whitespace should be prepended to the following token','line_number':958,'multiline':False]['text':' add special tokens to input ids','line_number':1014,'multiline':False]['text':' we pad in batch afterward','line_number':1130,'multiline':False]['text':' we pad in batch afterward','line_number':1135,'multiline':False]['text':' we pad in batch afterward','line_number':1136,'multiline':False]['text':' We convert the whole batch to tensors at the end','line_number':1141,'multiline':False]['text':' Backward compatibility for 'truncation_strategy', 'pad_to_max_length'','line_number':1215,'multiline':False]['text':' Compute lengths','line_number':1225,'multiline':False]['text':' Load from model defaults','line_number':1247,'multiline':False]['text':' Compute the total size of the returned word encodings','line_number':1255,'multiline':False]['text':' Truncation: Handle max sequence length and max_entity_length','line_number':1258,'multiline':False]['text':' truncate words up to max_length','line_number':1261,'multiline':False]['text':' Add special tokens','line_number':1274,'multiline':False]['text':' 1 * <s> token','line_number':1278,'multiline':False]['text':' 1 * <s> token & 2 * <sep> tokens','line_number':1279,'multiline':False]['text':' Build output dictionary','line_number':1286,'multiline':False]['text':' Set max entity length','line_number':1296,'multiline':False]['text':' truncate entities up to max_entity_length','line_number':1327,'multiline':False]['text':' Check lengths','line_number':1370,'multiline':False]['text':' Padding','line_number':1373,'multiline':False]['text':' If we have a list of dicts, let's convert it in a dict of lists','line_number':1455,'multiline':False]['text':' We do this to allow using this method as a collate_fn function in PyTorch Dataloader','line_number':1456,'multiline':False]['text':' The model's main input name, usually `input_ids`, has be passed for padding','line_number':1460,'multiline':False]['text':' If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects','line_number':1474,'multiline':False]['text':' and rebuild them afterwards if no return_tensors is specified','line_number':1475,'multiline':False]['text':' Note that we lose the specific device the tensor may be on for PyTorch','line_number':1476,'multiline':False]['text':' first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.','line_number':1480,'multiline':False]['text':' At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.','line_number':1486,'multiline':False]['text':' Convert padding_strategy in PaddingStrategy','line_number':1503,'multiline':False]['text':' Load from model defaults','line_number':1591,'multiline':False]['text':' Initialize attention mask if not present.','line_number':1616,'multiline':False]