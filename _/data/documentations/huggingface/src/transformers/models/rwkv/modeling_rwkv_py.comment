['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 Bo Peng and HuggingFace Inc. team.','line_number':2,'multiline':False]['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' See all RWKV models at https://huggingface.co/models?filter=rwkv','line_number':58,'multiline':False]['text':' Only load the kernel if it's not been loaded yet or if we changed the context length','line_number':73,'multiline':False]['text':' The CUDA kernel will fill this tensor.','line_number':130,'multiline':False]['text':' g stands for grad','line_number':162,'multiline':False]['text':' The CUDA kernel will fill those tensors.','line_number':167,'multiline':False]['text':' For CPU fallback. Will be slower and probably take more memory than the custom CUDA kernel if not executed','line_number':204,'multiline':False]['text':' within a torch.no_grad.','line_number':205,'multiline':False]['text':' For numerical stability','line_number':215,'multiline':False]['text':'    real_numerator_state = num_state * torch.exp(max_state)','line_number':216,'multiline':False]['text':'    real_denominator_state = den_state * torch.exp(max_state)','line_number':217,'multiline':False]['text':' wkv computation at time t','line_number':225,'multiline':False]['text':' Update state for next iteration','line_number':233,'multiline':False]['text':' Launching the CUDA kernel for just one token will actually be slower (there is no for loop in the CPU version','line_number':249,'multiline':False]['text':' in this case).','line_number':250,'multiline':False]['text':' TODO: maybe jit, otherwise move inside forward','line_number':288,'multiline':False]['text':' Mix hidden with the previous timestep to produce key, value, receptance','line_number':290,'multiline':False]['text':' 0 to 1','line_number':420,'multiline':False]['text':' 1 to ~0','line_number':421,'multiline':False]['text':' 1 to ~0','line_number':456,'multiline':False]['text':' Initialize weights and apply final processing','line_number':608,'multiline':False]['text':' noqa','line_number':626,'multiline':False]['text':' Layers should be rescaled for inference only.','line_number':712,'multiline':False]['text':' Deal with quantization statistics','line_number':722,'multiline':False]['text':' re-quantize the model:','line_number':748,'multiline':False]['text':' we need to put it first on CPU then back to the device','line_number':749,'multiline':False]['text':' this will create an overhead :/','line_number':750,'multiline':False]['text':' We set requires_grad=False as we cannot compute gradients on top of 4bit parameters anyway and to avoid','line_number':751,'multiline':False]['text':' bugs with bnb','line_number':752,'multiline':False]['text':' Initialize weights and apply final processing','line_number':772,'multiline':False]['text':' only last token for inputs_ids if the state is passed along.','line_number':782,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':786,'multiline':False]['text':' noqa','line_number':804,'multiline':False]['text':' move labels to correct device to enable model parallelism','line_number':836,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':838,'multiline':False]['text':' Flatten the tokens','line_number':841,'multiline':False]