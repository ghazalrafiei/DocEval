['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]['text':' limitations under the License.','line_number':19,'multiline':False]['text':' noqa','line_number':58,'multiline':False]['text':' This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.','line_number':62,'multiline':False]['text':' It means that the function will not be traced through and simply appear as a node in the graph.','line_number':63,'multiline':False]['text':' cat along the layers?','line_number':97,'multiline':False]['text':' cast the expert indices to int64, otherwise one-hot encoding will fail','line_number':104,'multiline':False]['text':' For a given token, determine if it was routed to a given expert.','line_number':113,'multiline':False]['text':' cast to float32 otherwise mean will fail','line_number':116,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':124,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Mixtral','line_number':137,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Mixtral','line_number':155,'multiline':False]['text':' Build here to make `torch.jit.trace` work.','line_number':166,'multiline':False]['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':176,'multiline':False]['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':182,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.rotate_half','line_number':192,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':200,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.repeat_kv','line_number':229,'multiline':False]['text':' Copied from transformers.models.mistral.modeling_mistral.MistralAttention with Mistral->Mixtral','line_number':242,'multiline':False]['text':' Specific to RoPE models','line_number':326,'multiline':False]['text':' repeat k/v heads if n_kv_heads < n_heads','line_number':329,'multiline':False]['text':' upcast attention to fp32','line_number':349,'multiline':False]['text':' Copied from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with Mistral->Mixtral','line_number':371,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':379,'multiline':False]['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':383,'multiline':False]['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':384,'multiline':False]['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':385,'multiline':False]['text':' overwrite attention_mask with padding_mask','line_number':403,'multiline':False]['text':' Because the input can be padded, the absolute sequence length depends on the max position id.','line_number':425,'multiline':False]['text':' Activate slicing cache only if the config has a value `sliding_windows` attribute','line_number':444,'multiline':False]['text':' Specific to RoPE models','line_number':469,'multiline':False]['text':' repeat k/v heads if n_kv_heads < n_heads','line_number':472,'multiline':False]['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':477,'multiline':False]['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':478,'multiline':False]['text':' cast them back in float16 just to be sure everything works as expected.','line_number':479,'multiline':False]['text':' Handle the case where the model is quantized','line_number':482,'multiline':False]['text':' Reashape to the expected shape for Flash Attention','line_number':498,'multiline':False]['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':556,'multiline':False]['text':' Contains at least one padding token in the sequence','line_number':559,'multiline':False]['text':' On the first iteration we need to properly re-create the padding mask','line_number':624,'multiline':False]['text':' by slicing it on the proper place','line_number':625,'multiline':False]['text':' There is a memcpy here, that is very bad.','line_number':646,'multiline':False]['text':' The -q_len: slice assumes left padding.','line_number':650,'multiline':False]['text':' gating','line_number':707,'multiline':False]['text':' router_logits: (batch * sequence_length, n_experts)','line_number':716,'multiline':False]['text':' we cast back to the input dtype','line_number':722,'multiline':False]['text':' One hot encode the selected experts to create an expert mask','line_number':729,'multiline':False]['text':' this will be used to easily index which expert is going to be sollicitated','line_number':730,'multiline':False]['text':' Loop over all available experts in the model and perform the computation on each expert','line_number':733,'multiline':False]['text':' in torch it is faster to index using lists than torch tensors','line_number':741,'multiline':False]['text':' Index the correct hidden states and compute the expert hidden state for','line_number':745,'multiline':False]['text':' the current expert. We need to make sure to multiply the output hidden','line_number':746,'multiline':False]['text':' states by `routing_weights` on the corresponding tokens (top-1 and top-2)','line_number':747,'multiline':False]['text':' However `index_add_` only support torch tensors for indexing so we'll use','line_number':751,'multiline':False]['text':' the `top_x` tensor here.','line_number':752,'multiline':False]['text':' Self Attention','line_number':805,'multiline':False]['text':' Fully Connected','line_number':816,'multiline':False]['text':' Copied from transformers.models.mistral.modeling_mistral.MistralPreTrainedModel with Mistral->Mixtral','line_number':857,'multiline':False]['text':' Copied from transformers.models.mistral.modeling_mistral.MistralModel with MISTRAL->MIXTRAL,Mistral->Mixtral','line_number':950,'multiline':False]['text':' Initialize weights and apply final processing','line_number':972,'multiline':False]['text':' Ignore copy','line_number':981,'multiline':False]['text':' retrieve input_ids and inputs_embeds','line_number':1007,'multiline':False]['text':' 2d mask is passed through the layers','line_number':1047,'multiline':False]['text':' 4d mask is passed through the layers','line_number':1050,'multiline':False]['text':' decoder layers','line_number':1068,'multiline':False]['text':' add hidden states from the last decoder layer','line_number':1113,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1147,'multiline':False]['text':' Ignore copy','line_number':1170,'multiline':False]['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1221,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':1241,'multiline':False]['text':' Flatten the tokens','line_number':1244,'multiline':False]['text':' Enable model parallelism','line_number':1248,'multiline':False]['text':' Omit tokens covered by past_key_values','line_number':1279,'multiline':False]['text':' Keep only the unprocessed tokens:','line_number':1289,'multiline':False]['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':1290,'multiline':False]['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':1291,'multiline':False]['text':' input)','line_number':1292,'multiline':False]['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':1295,'multiline':False]['text':' input_ids based on the past_length.','line_number':1296,'multiline':False]['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':1299,'multiline':False]['text':' If we are about to go beyond the maximum cache length, we need to crop the input attention mask.','line_number':1301,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':1311,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1317,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Mixtral, LLAMA->MIXTRAL','line_number':1358,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1366,'multiline':False]