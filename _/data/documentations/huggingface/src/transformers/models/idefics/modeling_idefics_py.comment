['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]['text':' limitations under the License.','line_number':19,'multiline':False]['text':' See all Idefics models at https://huggingface.co/models?filter=idefics','line_number':54,'multiline':False]['text':' must have this key set to at least None','line_number':191,'multiline':False]['text':' update token_type_ids with last value','line_number':197,'multiline':False]['text':' update attention masks','line_number':202,'multiline':False]['text':' Get the precomputed image_hidden_states','line_number':213,'multiline':False]['text':' only last token for inputs_ids if past is defined in kwargs','line_number':221,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':231,'multiline':False]['text':' Explicitely setting it to true to avoid any mistakes','line_number':267,'multiline':False]['text':' Derived from https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding','line_number':274,'multiline':False]['text':' Clone so that we don't modify the original input_ids later on','line_number':359,'multiline':False]['text':' for successful lookup replace input_ids with 0, the results of these will be discarded anyway','line_number':365,'multiline':False]['text':' overwrite the records with high indices','line_number':369,'multiline':False]['text':' Derived from https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear','line_number':384,'multiline':False]['text':' this was adapted from LlamaRMSNorm','line_number':448,'multiline':False]['text':' convert into half-precision if necessary','line_number':462,'multiline':False]['text':' this was adapted from LlamaRotaryEmbedding','line_number':472,'multiline':False]['text':' Build here to make `torch.jit.trace` work.','line_number':483,'multiline':False]['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':493,'multiline':False]['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':499,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':516,'multiline':False]['text':' this was adapted from LlamaMLP','line_number':545,'multiline':False]['text':' this was adapted from LlamaAttention','line_number':563,'multiline':False]['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':650,'multiline':False]['text':' Note that, in this case, `kv_len` == `kv_seq_len`','line_number':660,'multiline':False]['text':' [bsz, nh, t, hd]','line_number':672,'multiline':False]['text':' reuse k, v, self_attention','line_number':675,'multiline':False]['text':' SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,','line_number':691,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/112577.','line_number':692,'multiline':False]['text':' The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.','line_number':704,'multiline':False]['text':' this was adapted from LlamaDecoderLayer','line_number':728,'multiline':False]['text':' Self Attention','line_number':775,'multiline':False]['text':' Fully Connected','line_number':787,'multiline':False]['text':' Self Attention','line_number':917,'multiline':False]['text':' Fill in zeros for cross_attention hidden_states of tokens attending to no images','line_number':925,'multiline':False]['text':' Fully Connected','line_number':929,'multiline':False]['text':' important: this ported version of Idefics isn't meant for training from scratch - only','line_number':976,'multiline':False]['text':' inference and fine-tuning - so the proper init weights code has been removed - the m4 code','line_number':977,'multiline':False]['text':' base should be used for training from scratch and it contains the correct code.','line_number':978,'multiline':False]['text':' Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa','line_number':989,'multiline':False]['text':' We remove the checks on `is_torch_sdpa_available()` and `cls._supports_sdpa` as Falcon supports SDPA from torch==2.0.0 (no requirement on 2.1).','line_number':992,'multiline':False]['text':' Perceiver Resampler','line_number':1094,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1117,'multiline':False]['text':' retrieve input_ids and inputs_embeds','line_number':1173,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':1191,'multiline':False]['text':' fp16 compatibility','line_number':1206,'multiline':False]['text':' Get sequence from the vision encoder','line_number':1210,'multiline':False]['text':' # Hack to use the model in full language modeling mode','line_number':1233,'multiline':False]['text':' image_attention_mask = torch.zeros(batch_size, seq_length, 1, dtype=torch.long, device=image_hidden_states.device)','line_number':1234,'multiline':False]['text':' Make image_attention_mask compatible with hidden states','line_number':1235,'multiline':False]['text':' cross_attention_gate:','line_number':1250,'multiline':False]['text':' For any tokens attending to no images, the hidden_states comming out of the cross-attention should be zeroed-out.','line_number':1251,'multiline':False]['text':' `image_attention_mask` has shape [bsz, 1, num_images, hidden_size] with elements equal to either 0.0 or a very negative number.','line_number':1252,'multiline':False]['text':' If any of the elements are 0.0, then the token is attending to at least one image and the gate value is 1. Otherwise the gate value is 0.','line_number':1253,'multiline':False]['text':' `cross_attention_gate` has shape [bsz, seq_len] with elements equal to either 0.0 or 1.0.','line_number':1254,'multiline':False]['text':' embed positions','line_number':1261,'multiline':False]['text':' decoder layers','line_number':1279,'multiline':False]['text':' TODO(ls): Add cross attention values to respective lists','line_number':1305,'multiline':False]['text':' not implemented','line_number':1316,'multiline':False]['text':' add hidden states from the last decoder layer','line_number':1382,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1419,'multiline':False]['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1513,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':1537,'multiline':False]['text':' Flatten the tokens','line_number':1545,'multiline':False]