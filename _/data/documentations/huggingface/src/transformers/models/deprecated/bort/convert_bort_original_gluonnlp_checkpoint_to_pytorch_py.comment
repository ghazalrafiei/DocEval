['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020, The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Original Bort configuration','line_number':60,'multiline':False]['text':' Let's construct the original Bort model here','line_number':80,'multiline':False]['text':' Taken from official BERT implementation, see:','line_number':81,'multiline':False]['text':' https://github.com/alexa/bort/blob/master/bort/bort.py','line_number':82,'multiline':False]['text':' Vocab information needs to be fetched first','line_number':99,'multiline':False]['text':' It's the same as RoBERTa, so RobertaTokenizer can be used later','line_number':100,'multiline':False]['text':' Specify download folder to Gluonnlp's vocab','line_number':103,'multiline':False]['text':' Build our config ðŸ¤—','line_number':124,'multiline':False]['text':' 2 = BERT, 1 = RoBERTa','line_number':138,'multiline':False]['text':' 2 = BERT, 1 = RoBERTa','line_number':139,'multiline':False]['text':' Parameter mapping table (Gluonnlp to Transformers)','line_number':147,'multiline':False]['text':' * denotes layer index','line_number':148,'multiline':False]['text':'','line_number':149,'multiline':False]['text':' | Gluon Parameter                                                | Transformers Parameter','line_number':150,'multiline':False]['text':' | -------------------------------------------------------------- | ----------------------','line_number':151,'multiline':False]['text':' | `encoder.layer_norm.beta`                                      | `bert.embeddings.LayerNorm.bias`','line_number':152,'multiline':False]['text':' | `encoder.layer_norm.gamma`                                     | `bert.embeddings.LayerNorm.weight`','line_number':153,'multiline':False]['text':' | `encoder.position_weight`                                      | `bert.embeddings.position_embeddings.weight`','line_number':154,'multiline':False]['text':' | `word_embed.0.weight`                                          | `bert.embeddings.word_embeddings.weight`','line_number':155,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_key.bias`     | `bert.encoder.layer.*.attention.self.key.bias`','line_number':156,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_key.weight`   | `bert.encoder.layer.*.attention.self.key.weight`','line_number':157,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_query.bias`   | `bert.encoder.layer.*.attention.self.query.bias`','line_number':158,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_query.weight` | `bert.encoder.layer.*.attention.self.query.weight`','line_number':159,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_value.bias`   | `bert.encoder.layer.*.attention.self.value.bias`','line_number':160,'multiline':False]['text':' | `encoder.transformer_cells.*.attention_cell.proj_value.weight` | `bert.encoder.layer.*.attention.self.value.weight`','line_number':161,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.ffn_2.bias`                   | `bert.encoder.layer.*.attention.output.dense.bias`','line_number':162,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.ffn_2.weight`                 | `bert.encoder.layer.*.attention.output.dense.weight`','line_number':163,'multiline':False]['text':' | `encoder.transformer_cells.*.layer_norm.beta`                  | `bert.encoder.layer.*.attention.output.LayerNorm.bias`','line_number':164,'multiline':False]['text':' | `encoder.transformer_cells.*.layer_norm.gamma`                 | `bert.encoder.layer.*.attention.output.LayerNorm.weight`','line_number':165,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.ffn_1.bias`                   | `bert.encoder.layer.*.intermediate.dense.bias`','line_number':166,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.ffn_1.weight`                 | `bert.encoder.layer.*.intermediate.dense.weight`','line_number':167,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.layer_norm.beta`              | `bert.encoder.layer.*.output.LayerNorm.bias`','line_number':168,'multiline':False]['text':' | `encoder.transformer_cells.*.ffn.layer_norm.gamma`             | `bert.encoder.layer.*.output.LayerNorm.weight`','line_number':169,'multiline':False]['text':' | `encoder.transformer_cells.*.proj.bias`                        | `bert.encoder.layer.*.output.dense.bias`','line_number':170,'multiline':False]['text':' | `encoder.transformer_cells.*.proj.weight`                      | `bert.encoder.layer.*.output.dense.weight`','line_number':171,'multiline':False]['text':' Helper function to convert MXNET Arrays to PyTorch','line_number':173,'multiline':False]['text':' Check param shapes and map new HF param back','line_number':177,'multiline':False]['text':' Inspired by RoBERTa conversion script, we just zero them out (Bort does not use them)','line_number':203,'multiline':False]['text':' self attention','line_number':211,'multiline':False]['text':' self attention output','line_number':234,'multiline':False]['text':' intermediate','line_number':250,'multiline':False]['text':' output','line_number':260,'multiline':False]['text':' Save space and energy ðŸŽ„','line_number':276,'multiline':False]['text':' Compare output of both models','line_number':279,'multiline':False]['text':' Get gluon output','line_number':284,'multiline':False]['text':' Get Transformer output (save and reload model again)','line_number':288,'multiline':False]['text':' Required parameters','line_number':311,'multiline':False]