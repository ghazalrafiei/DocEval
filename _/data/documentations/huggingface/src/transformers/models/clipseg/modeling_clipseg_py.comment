['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 The OpenAI Team Authors and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all CLIPSeg models at https://huggingface.co/models?filter=clipseg','line_number':47,'multiline':False]['text':' contrastive loss function, adapted from','line_number':51,'multiline':False]['text':' https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html','line_number':52,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->clipseg','line_number':57,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->CLIPSeg','line_number':65,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.__init__ with CLIP->CLIPSeg','line_number':148,'multiline':False]['text':' we interpolate the position embeddings in 2D','line_number':176,'multiline':False]['text':' shape = [*, width, grid, grid]','line_number':192,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->CLIPSeg','line_number':208,'multiline':False]['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':217,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->CLIPSeg','line_number':242,'multiline':False]['text':' get query proj','line_number':279,'multiline':False]['text':' apply the causal_attention_mask first','line_number':298,'multiline':False]['text':' this operation is a bit akward, but it's required to','line_number':319,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':320,'multiline':False]['text':' In order to do so, attn_weights have to reshaped','line_number':321,'multiline':False]['text':' twice and have to be reused in the following','line_number':322,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->CLIPSeg','line_number':347,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->CLIPSeg','line_number':363,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoder with CLIP->CLIPSeg','line_number':562,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer.__init__ with CLIP->CLIPSeg','line_number':661,'multiline':False]['text':' For `pooled_output` computation','line_number':670,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer.forward with clip->clipseg, CLIP->CLIPSeg','line_number':675,'multiline':False]['text':' CLIPSeg's text model uses causal mask, prepare it here.','line_number':703,'multiline':False]['text':' https://github.com/openai/CLIPSeg/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clipseg/model.py#L324','line_number':704,'multiline':False]['text':' expand attention_mask','line_number':708,'multiline':False]['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':710,'multiline':False]['text':' The `eos_token_id` was incorrect before PR #24773: Let's keep what have been done here.','line_number':726,'multiline':False]['text':' A CLIPSeg model with such `eos_token_id` in the config can't work correctly with extra new tokens added','line_number':727,'multiline':False]['text':' ------------------------------------------------------------','line_number':728,'multiline':False]['text':' text_embeds.shape = [batch_size, sequence_length, transformer.width]','line_number':729,'multiline':False]['text':' take features from the eot embedding (eot_token is the highest number in each sequence)','line_number':730,'multiline':False]['text':' casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14','line_number':731,'multiline':False]['text':' The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)','line_number':737,'multiline':False]['text':' We need to get the first position of `eos_token_id` value (`pad_token_ids` might equal to `eos_token_id`)','line_number':740,'multiline':False]['text':' Initialize weights and apply final processing','line_number':765,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPVisionTransformer.__init__ with CLIP->CLIPSeg','line_number':813,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPVisionTransformer.forward','line_number':826,'multiline':False]['text':' Initialize weights and apply final processing','line_number':879,'multiline':False]['text':' Initialize weights and apply final processing','line_number':957,'multiline':False]['text':' Use CLIPSEG model's config for some fields (if specified) instead of those of vision & text components.','line_number':986,'multiline':False]['text':' Use CLIPSEG model's config for some fields (if specified) instead of those of vision & text components.','line_number':1037,'multiline':False]['text':' pooled_output','line_number':1051,'multiline':False]['text':' Use CLIPSEG model's config for some fields (if specified) instead of those of vision & text components.','line_number':1093,'multiline':False]['text':' normalized features','line_number':1122,'multiline':False]['text':' cosine similarity as logits','line_number':1126,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer.__init__ with CLIP->CLIPSeg','line_number':1156,'multiline':False]['text':' remove cls token and reshape to [batch_size, reduce_dim, seq_len]','line_number':1288,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1326,'multiline':False]['text':' compute conditional embeddings from texts','line_number':1338,'multiline':False]['text':' compute conditional embeddings from images','line_number':1346,'multiline':False]['text':' step 1: forward the query images through the frozen CLIP vision encoder','line_number':1404,'multiline':False]['text':' we need the intermediate hidden states','line_number':1409,'multiline':False]['text':' we add +1 here as the hidden states also include the initial embeddings','line_number':1415,'multiline':False]['text':' update vision_outputs','line_number':1418,'multiline':False]['text':' step 2: compute conditional embeddings, either from text, images or an own provided embedding','line_number':1431,'multiline':False]['text':' step 3: forward both the pooled output and the activations through the lightweight decoder to predict masks','line_number':1451,'multiline':False]['text':' move labels to the correct device to enable PP','line_number':1463,'multiline':False]