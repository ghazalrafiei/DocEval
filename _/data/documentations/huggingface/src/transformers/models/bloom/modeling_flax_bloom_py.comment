['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 HuggingFace Inc. Team and Bigscience Workshop. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Note: the Alibi tensor will added to the attention bias that will be applied to the query, key product of attention','line_number':141,'multiline':False]['text':' therefore, Alibi will have to be of shape (batch_size, num_heads, query_length, key_length)','line_number':142,'multiline':False]['text':' => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)','line_number':143,'multiline':False]['text':' so that the query_length dimension will then be broadcast correctly.','line_number':144,'multiline':False]['text':' This is more or less identical to T5's relative position bias:','line_number':145,'multiline':False]['text':' https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527','line_number':146,'multiline':False]['text':' Copied from transformers.models.gptj.modeling_flax_gptj.FlaxGPTJAttention._concatenate_to_cache','line_number':186,'multiline':False]['text':' detect if we're initializing by absence of existing cache data.','line_number':193,'multiline':False]['text':' update key, value caches with our new 1d spatial slices','line_number':201,'multiline':False]['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key','line_number':210,'multiline':False]['text':' positions that have already been generated and cached, not the remaining zero elements.','line_number':211,'multiline':False]['text':' proj q, k, v','line_number':231,'multiline':False]['text':' for fast decoding causal attention mask should be shifted','line_number':238,'multiline':False]['text':' fast decoding for generate requires special attention_mask','line_number':243,'multiline':False]['text':' broadcast causal attention mask & attention mask to fit for merge','line_number':252,'multiline':False]['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':263,'multiline':False]['text':' and cache the keys and values step by step.','line_number':264,'multiline':False]['text':' transform boolean mask into float mask','line_number':268,'multiline':False]['text':' Cast in fp32 if the original dtype is different from fp32','line_number':278,'multiline':False]['text':' Cast back in the original dtype if the native dtype is not fp32','line_number':291,'multiline':False]['text':' layer norm before saving residual if config calls for it','line_number':366,'multiline':False]['text':' self-attention','line_number':372,'multiline':False]['text':' set residual based on config','line_number':389,'multiline':False]['text':' init input tensors','line_number':425,'multiline':False]['text':' init input variables to retrieve cache','line_number':452,'multiline':False]['text':' Handle any PRNG if needed','line_number':485,'multiline':False]['text':' If past_key_values are passed then cache is already initialized a private flag init_cache has to be passed','line_number':492,'multiline':False]['text':' down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be','line_number':493,'multiline':False]['text':' changed by FlaxBloomAttention module','line_number':494,'multiline':False]['text':' add updated cache to model output','line_number':514,'multiline':False]['text':' this contains possible `None` values - `FlaxBloomModule` will filter them out','line_number':566,'multiline':False]['text':' word embeddings (no positional embedding layer)','line_number':579,'multiline':False]['text':' post-embedding layernorm','line_number':587,'multiline':False]['text':' transformer layers','line_number':590,'multiline':False]['text':' final layernorm','line_number':593,'multiline':False]['text':' do post-embedding layernorm','line_number':607,'multiline':False]['text':' build alibi depending on `attention_mask`','line_number':610,'multiline':False]['text':' Copied from transformers.models.gpt_neo.modeling_flax_gpt_neo.FlaxGPTNeoModel with GPTNeo->Bloom','line_number':646,'multiline':False]['text':' initializing the cache','line_number':712,'multiline':False]['text':' Note that usually one would have to put 0's in the attention_mask for','line_number':716,'multiline':False]['text':' x > input_ids.shape[-1] and x < cache_length. But since Bloom uses a causal mask,','line_number':717,'multiline':False]['text':' those positions are masked anyway. Thus, we can create a single static attention_mask here,','line_number':718,'multiline':False]['text':' which is more efficient for compilation','line_number':719,'multiline':False]