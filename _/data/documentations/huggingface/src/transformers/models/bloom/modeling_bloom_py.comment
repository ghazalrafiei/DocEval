['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 HuggingFace Inc. team and BigScience workshop.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Note: alibi will added to the attention bias that will be applied to the query, key product of attention','line_number':90,'multiline':False]['text':' => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)','line_number':91,'multiline':False]['text':' => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)','line_number':92,'multiline':False]['text':' => the query_length dimension will then be broadcasted correctly','line_number':93,'multiline':False]['text':' This is more or less identical to T5's relative position bias:','line_number':94,'multiline':False]['text':' https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527','line_number':95,'multiline':False]['text':' x is a tuple of 1 element, needs to unpack it first','line_number':143,'multiline':False]['text':' sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243','line_number':145,'multiline':False]['text':' Layer-wise attention scaling','line_number':201,'multiline':False]['text':' What we want to achieve is:','line_number':235,'multiline':False]['text':' batch_size * num_heads, seq_length, head_dim -> batch_size, seq_length, num_heads * head_dim','line_number':236,'multiline':False]['text':' First view to decompose the batch size','line_number':240,'multiline':False]['text':' batch_size * num_heads, seq_length, head_dim -> batch_size, num_heads, seq_length, head_dim','line_number':241,'multiline':False]['text':' batch_size, num_heads, seq_length, head_dim -> batch_size, seq_length, num_heads, head_dim','line_number':244,'multiline':False]['text':' batch_size, seq_length, num_heads, head_dim -> batch_size, seq_length, num_heads * head_dim','line_number':247,'multiline':False]['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':261,'multiline':False]['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':263,'multiline':False]['text':' concatenate along seq_length dimension:','line_number':273,'multiline':False]['text':'  - key: [batch_size * self.num_heads, head_dim, kv_length]','line_number':274,'multiline':False]['text':'  - value: [batch_size * self.num_heads, kv_length, head_dim]','line_number':275,'multiline':False]['text':' [batch_size * num_heads, q_length, kv_length]','line_number':286,'multiline':False]['text':' we use `torch.Tensor.baddbmm` instead of `torch.baddbmm` as the latter isn't supported by TorchScript v1.11','line_number':287,'multiline':False]['text':' change view to [batch_size, num_heads, q_length, kv_length]','line_number':295,'multiline':False]['text':' cast attention scores to fp32, compute scaled softmax and cast back to initial dtype - [batch_size, num_heads, q_length, kv_length]','line_number':298,'multiline':False]['text':' `float16` has a minimum value of -65504.0, whereas `bfloat16` and `float32` have a minimum value of `-3.4e+38`','line_number':300,'multiline':False]['text':' [batch_size, num_heads, q_length, kv_length]','line_number':306,'multiline':False]['text':' change view [batch_size x num_heads, q_length, kv_length]','line_number':312,'multiline':False]['text':' matmul: [batch_size * num_heads, q_length, head_dim]','line_number':315,'multiline':False]['text':' change view [batch_size, q_length, num_heads * head_dim]','line_number':318,'multiline':False]['text':' aggregate results across tp ranks. See here: https://github.com/pytorch/pytorch/issues/76232','line_number':321,'multiline':False]['text':' hidden_states: [batch_size, seq_length, hidden_size]','line_number':398,'multiline':False]['text':' Layer norm at the beginning of the transformer layer.','line_number':400,'multiline':False]['text':' Layer norm post the self attention.','line_number':403,'multiline':False]['text':' Self attention.','line_number':409,'multiline':False]['text':' Get residual','line_number':427,'multiline':False]['text':' MLP.','line_number':433,'multiline':False]['text':' hidden_states, present, attentions','line_number':441,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':457,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':458,'multiline':False]['text':' key: [batch_size * num_heads, head_dim, seq_length] -> [batch_size, num_heads, head_dim, seq_length]','line_number':480,'multiline':False]['text':' value: [batch_size * num_heads, seq_length, head_dim] -> [batch_size, num_heads, seq_length, head_dim]','line_number':481,'multiline':False]['text':' key:  [batch_size, num_heads, head_dim, seq_length] -> [batch_size * num_heads, head_dim, seq_length]','line_number':499,'multiline':False]['text':' value: [batch_size, num_heads, seq_length, head_dim] -> [batch_size * num_heads, seq_length, head_dim]','line_number':500,'multiline':False]['text':' Embedding + LN Embedding','line_number':591,'multiline':False]['text':' Transformer blocks','line_number':595,'multiline':False]['text':' Final Layer Norm','line_number':598,'multiline':False]['text':' Initialize weights and apply final processing','line_number':603,'multiline':False]['text':' `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`','line_number':635,'multiline':False]['text':' Prepare head mask if needed','line_number':663,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':664,'multiline':False]['text':' attention_probs has shape batch_size x num_heads x N x N','line_number':665,'multiline':False]['text':' head_mask has shape n_layer x batch x num_heads x N x N','line_number':666,'multiline':False]['text':' Compute alibi tensor: check build_alibi_tensor documentation','line_number':685,'multiline':False]['text':' Add last hidden state','line_number':739,'multiline':False]['text':' Initialize weights and apply final processing','line_number':771,'multiline':False]['text':' only last tokens for input_ids if past is not None','line_number':788,'multiline':False]['text':' Some generation methods already pass only the last input ID','line_number':792,'multiline':False]['text':' Default to old behavior: keep only final ID','line_number':796,'multiline':False]['text':' the cache may be in the stardard format (e.g. in contrastive search), convert to bloom's format if needed','line_number':801,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':805,'multiline':False]['text':' `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`','line_number':847,'multiline':False]['text':' move labels to correct device to enable model parallelism','line_number':875,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':877,'multiline':False]['text':' Flatten the tokens','line_number':881,'multiline':False]['text':' Get a copy of `beam_idx` on all the devices where we need those indices.','line_number':911,'multiline':False]['text':' Initialize weights and apply final processing','line_number':947,'multiline':False]['text':' `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`','line_number':977,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1081,'multiline':False]['text':' `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`','line_number':1111,'multiline':False]['text':' move labels to correct device to enable model parallelism','line_number':1140,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1173,'multiline':False]['text':' If we are on multi-GPU, split add a dimension','line_number':1222,'multiline':False]['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1227,'multiline':False]