['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The Fairseq Authors, Microsoft Research, and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' General docstring','line_number':48,'multiline':False]['text':' Base docstring','line_number':51,'multiline':False]['text':' CTC docstring','line_number':55,'multiline':False]['text':' Frame class docstring','line_number':59,'multiline':False]['text':' Speaker Verification docstring','line_number':63,'multiline':False]['text':' See all WavLM models at https://huggingface.co/models?filter=wavlm','line_number':71,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices','line_number':75,'multiline':False]['text':' epsilon is used for probabilistic rounding','line_number':111,'multiline':False]['text':' make sure num masked span <= sequence_length','line_number':119,'multiline':False]['text':' make sure num_masked span is also <= input_length - (mask_length - 1)','line_number':123,'multiline':False]['text':' compute number of masked spans in batch','line_number':129,'multiline':False]['text':' SpecAugment mask to fill','line_number':136,'multiline':False]['text':' compute num of masked spans for this input','line_number':146,'multiline':False]['text':' get random indices to mask','line_number':149,'multiline':False]['text':' pick first sampled index that will serve as a dummy index to pad vector','line_number':154,'multiline':False]['text':' to ensure same dimension for all batches due to probabilistic rounding','line_number':155,'multiline':False]['text':' Picking first sample just pads those vectors twice.','line_number':156,'multiline':False]['text':' this case can only happen if `input_length` is strictly smaller then','line_number':158,'multiline':False]['text':' `sequence_length` in which case the last token has to be a padding','line_number':159,'multiline':False]['text':' token which we can use as a dummy mask id','line_number':160,'multiline':False]['text':' expand masked indices to masked spans','line_number':172,'multiline':False]['text':' add offset to the starting indexes so that indexes now create a span','line_number':178,'multiline':False]['text':' ensure that we cannot have indices larger than sequence_length','line_number':185,'multiline':False]['text':' scatter indices to mask','line_number':189,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->WavLM','line_number':195,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->WavLM','line_number':217,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->WavLM','line_number':245,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->WavLM','line_number':270,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->WavLM','line_number':310,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->WavLM','line_number':322,'multiline':False]['text':' make sure hidden_states require grad for gradient_checkpointing','line_number':351,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->WavLM','line_number':378,'multiline':False]['text':' non-projected hidden states are needed for quantization','line_number':387,'multiline':False]['text':' first pass of attention layer creates position bias','line_number':444,'multiline':False]['text':' Compute relative position bias:','line_number':451,'multiline':False]['text':' 1) get reshape hidden_states','line_number':452,'multiline':False]['text':' 2) project hidden states','line_number':456,'multiline':False]['text':' 3) compute gate for position bias from projected hidden states','line_number':460,'multiline':False]['text':' 4) apply gate to position bias to compute gated position_bias','line_number':464,'multiline':False]['text':' self-attention assumes q = k = v','line_number':482,'multiline':False]['text':' disable bias and add_zero_attn','line_number':486,'multiline':False]['text':' PyTorch 1.3.0 has F.multi_head_attention_forward defined','line_number':490,'multiline':False]['text':' so no problem with backwards compatibility','line_number':491,'multiline':False]['text':' [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]','line_number':516,'multiline':False]['text':' IMPORTANT: Attention weights are averaged weights','line_number':520,'multiline':False]['text':' here which should not be the case. This is an open issue','line_number':521,'multiline':False]['text':' on PyTorch: https://github.com/pytorch/pytorch/issues/32590','line_number':522,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->WavLM','line_number':560,'multiline':False]['text':' make sure padded tokens output 0','line_number':687,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':702,'multiline':False]['text':' under deepspeed zero3 all gpus must run in sync','line_number':707,'multiline':False]['text':' make sure padded tokens are not attended to','line_number':772,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':786,'multiline':False]['text':' under deepspeed zero3 all gpus must run in sync','line_number':791,'multiline':False]['text':' XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication','line_number':792,'multiline':False]['text':' storage for codebook variables (codewords)','line_number':846,'multiline':False]['text':' can be decayed for training','line_number':852,'multiline':False]['text':' project to codevector dim','line_number':864,'multiline':False]['text':' sample code vector probs via gumbel in differentiateable way','line_number':869,'multiline':False]['text':' compute perplexity','line_number':873,'multiline':False]['text':' take argmax in non-differentiable way','line_number':879,'multiline':False]['text':' comptute hard codevector distribution (one hot)','line_number':880,'multiline':False]['text':' use probs to retrieve codevectors','line_number':890,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Adapter with Wav2Vec2->WavLM','line_number':898,'multiline':False]['text':' feature dim might need to be down-projected','line_number':903,'multiline':False]['text':' down project hidden_states if necessary','line_number':914,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AdapterLayer with Wav2Vec2->WavLM','line_number':930,'multiline':False]['text':' gumbel softmax requires special init','line_number':962,'multiline':False]['text':' 1D convolutional layer output length formula taken','line_number':1003,'multiline':False]['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':1004,'multiline':False]['text':' Effectively attention_mask.sum(-1), but not inplace to be able to run','line_number':1019,'multiline':False]['text':' on inference mode.','line_number':1020,'multiline':False]['text':' these two operations makes sure that all values before the output lengths idxs are attended to','line_number':1031,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM, WavLMBaseModelOutput->Wav2Vec2BaseModelOutput','line_number':1098,'multiline':False]['text':' model only needs masking vector if mask prob is > 0.0','line_number':1106,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1117,'multiline':False]['text':' `config.apply_spec_augment` can set masking to False','line_number':1150,'multiline':False]['text':' generate indices & apply SpecAugment along time axis','line_number':1154,'multiline':False]['text':' apply SpecAugment along time axis with given mask_time_indices','line_number':1158,'multiline':False]['text':' generate indices & apply SpecAugment along feature axis','line_number':1172,'multiline':False]['text':' compute reduced attention_mask corresponding to feature vectors','line_number':1212,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM','line_number':1250,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1272,'multiline':False]['text':' Note that `tie_weights` is usually used to tie input and output embedding weights. The method is re-purposed to','line_number':1283,'multiline':False]['text':' correctly load adapter layers for WavLM so that we do not have to introduce a new API to','line_number':1284,'multiline':False]['text':' [`PreTrainedModel`]. While slightly hacky, WavLM never has to tie input and output embeddings, so that it is','line_number':1285,'multiline':False]['text':' ok to repurpose this function here.','line_number':1286,'multiline':False]['text':' retrieve loss input_lengths from attention_mask','line_number':1368,'multiline':False]['text':' assuming that padded tokens are filled with -100','line_number':1374,'multiline':False]['text':' when not being attended to','line_number':1375,'multiline':False]['text':' ctc_loss doesn't support fp16','line_number':1380,'multiline':False]['text':' transformer layers + input embeddings','line_number':1419,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1425,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_extractor','line_number':1428,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->wavlm','line_number':1441,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_base_model with wav2vec2->wavlm','line_number':1449,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->WavLM, wav2vec2->wavlm','line_number':1465,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM','line_number':1534,'multiline':False]['text':' transformer layers + input embeddings','line_number':1544,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss','line_number':1641,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer','line_number':1665,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM','line_number':1698,'multiline':False]['text':' transformer layers + input embeddings','line_number':1704,'multiline':False]['text':' 1D convolutional layer output length formula taken','line_number':1752,'multiline':False]['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':1753,'multiline':False]['text':' Statistic Pooling','line_number':1809,'multiline':False]