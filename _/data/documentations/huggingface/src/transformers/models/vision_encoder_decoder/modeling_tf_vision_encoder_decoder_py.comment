['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.encoder_decoder.modeling_tf_encoder_decoder.shift_tokens_right','line_number':148,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':160,'multiline':False]['text':' "Verify that `labels` has only positive values and -100"','line_number':165,'multiline':False]['text':' Make sure the assertion op is called by wrapping the result in an identity no-op','line_number':168,'multiline':False]['text':' initialize with config','line_number':212,'multiline':False]['text':' make sure that the individual model's config refers to the shared config','line_number':235,'multiline':False]['text':' so that the updates to the config will be synced','line_number':236,'multiline':False]['text':' encoder outputs might need to be projected to different dimension for decoder','line_number':240,'multiline':False]['text':' Matt: The TF and PT weights don't align because our TF base classes have an extra layer compared to PT models','line_number':294,'multiline':False]['text':' (the main model stem is in the MainLayer class). If we remove that layer, then weight names sync up as normal.','line_number':295,'multiline':False]['text':' However, the name of that extra layer is the name of the MainLayer in the base model. We make the assumption','line_number':296,'multiline':False]['text':' here that the config model_type is the same as the name of the MainLayer. I don't know of anywhere that's','line_number':297,'multiline':False]['text':' not the case, and I wasn't sure how else to go from the config to the correct MainLayer name!','line_number':298,'multiline':False]['text':' This override is only needed in the case where we're crossloading weights from PT. However, since weights are','line_number':300,'multiline':False]['text':' often safetensors now, we don't know if we're going to be crossloading until we sniff the weights file.','line_number':301,'multiline':False]['text':' Therefore, we specify tf_to_pt_weight_rename anyway, and let the super method figure out if it needs it','line_number':302,'multiline':False]['text':' or not.','line_number':303,'multiline':False]['text':' remove encoder, decoder kwargs from kwargs','line_number':381,'multiline':False]['text':' Load and initialize the encoder and decoder','line_number':387,'multiline':False]['text':' The distinction between encoder and decoder at the model level is made','line_number':388,'multiline':False]['text':' by the value of the flag `is_decoder` that we need to set correctly.','line_number':389,'multiline':False]['text':' Make sure these 2 `tf.keras.Model` have fixed names so `from_pretrained` could load model weights correctly.','line_number':448,'multiline':False]['text':' instantiate config with corresponding kwargs','line_number':454,'multiline':False]['text':' Let the user be responsible for the expected format.','line_number':524,'multiline':False]['text':' Add arguments to encoder from `kwargs_encoder`','line_number':541,'multiline':False]['text':' Handle the case where the inputs are passed as a single dict which contains `labels`.','line_number':550,'multiline':False]['text':' The `labels` shouldn't be passed to `self.encoder` below, because it is a based model without this','line_number':551,'multiline':False]['text':' parameter (otherwise, an error occurs when `input_processing` is called inside `self.encoder.call()`).','line_number':552,'multiline':False]['text':' handle the init case where `dummy_inputs` returns a dict containing `decoder_input_ids`.','line_number':556,'multiline':False]['text':' handle the init case where `dummy_inputs` returns a dict containing `decoder_input_ids`.','line_number':559,'multiline':False]['text':' optionally project encoder_hidden_states','line_number':567,'multiline':False]['text':' Add arguments to decoder from `kwargs_decoder`','line_number':596,'multiline':False]['text':' Compute loss independent from decoder (as some shift the logits inside them)','line_number':603,'multiline':False]['text':' The starting index of the remaining elements in `decoder_outputs`','line_number':613,'multiline':False]['text':' needs to be passed to make Keras.layer.__call__ happy','line_number':668,'multiline':False]['text':' TODO (joao): the `TFBaseModelOutput` wrapper should not be needed after the generate refactor is complete','line_number':672,'multiline':False]