['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.encoder_decoder.modeling_encoder_decoder.shift_tokens_right','line_number':36,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':49,'multiline':False]['text':' initialize with config','line_number':187,'multiline':False]['text':' make sure input & output embeddings is not tied','line_number':188,'multiline':False]['text':' make sure that the individual model's config refers to the shared config','line_number':212,'multiline':False]['text':' so that the updates to the config will be synced','line_number':213,'multiline':False]['text':' encoder outputs might need to be projected to different dimension for decoder','line_number':217,'multiline':False]['text':' a workaround to load from tensorflow checkpoint','line_number':273,'multiline':False]['text':' Using `_tf_model` won't work, because the weight names in the encoder/decoder of `_tf_model` get','line_number':274,'multiline':False]['text':' extended before saving those components. For example, The name of `_tf_model.encoder.vit` is','line_number':275,'multiline':False]['text':' `[top model name]/encoder/vit`, but the name of `tf_model.encoder.vit` is `[top model name]/vit`. The','line_number':276,'multiline':False]['text':' [top model name] is handled (stripped) by the conversion method, and the former case gets extra `encoder`,','line_number':277,'multiline':False]['text':' which should not occur when we want to save the components alone.','line_number':278,'multiline':False]['text':' There was a (very) ugly potential fix, which wasn't integrated to `transformers`: see','line_number':279,'multiline':False]['text':'   https://github.com/huggingface/transformers/pull/13222/commits/dbb3c9de76eee235791d2064094654637c99f36d#r697304245','line_number':280,'multiline':False]['text':'   (the change in `src/transformers/modeling_tf_utils.py`)','line_number':281,'multiline':False]['text':' Using `tf_model` instead','line_number':287,'multiline':False]['text':' Make sure models are built','line_number':290,'multiline':False]['text':' Get the variable correspondence between `_tf_model` and `encoder` and `decoder`','line_number':294,'multiline':False]['text':' assign weight values to `encoder` and `decoder` from `_tf_model`','line_number':309,'multiline':False]['text':' Deal with `enc_to_dec_proj`','line_number':317,'multiline':False]['text':' This is only for copying some specific attributes of this particular model.','line_number':342,'multiline':False]['text':' At the moment fast initialization is not supported for composite models','line_number':351,'multiline':False]['text':' remove encoder, decoder kwargs from kwargs','line_number':439,'multiline':False]['text':' Load and initialize the encoder and decoder','line_number':445,'multiline':False]['text':' The distinction between encoder and decoder at the model level is made','line_number':446,'multiline':False]['text':' by the value of the flag `is_decoder` that we need to set correctly.','line_number':447,'multiline':False]['text':' instantiate config with corresponding kwargs','line_number':508,'multiline':False]['text':' make sure input & output embeddings is not tied','line_number':511,'multiline':False]['text':' optionally project encoder_hidden_states','line_number':589,'multiline':False]['text':' else:','line_number':596,'multiline':False]['text':' Decode','line_number':604,'multiline':False]['text':' Compute loss independent from decoder (as some shift the logits inside them)','line_number':619,'multiline':False]['text':' apply decoder cache reordering here','line_number':669,'multiline':False]