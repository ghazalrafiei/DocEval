['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' General docstring','line_number':54,'multiline':False]['text':' Base docstring','line_number':57,'multiline':False]['text':' CTC docstring','line_number':61,'multiline':False]['text':' See all Wav2Vec2Conformer models at https://huggingface.co/models?filter=wav2vec2-conformer','line_number':68,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput with Wav2Vec2->Wav2Vec2Conformer','line_number':73,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices','line_number':115,'multiline':False]['text':' epsilon is used for probabilistic rounding','line_number':151,'multiline':False]['text':' make sure num masked span <= sequence_length','line_number':159,'multiline':False]['text':' make sure num_masked span is also <= input_length - (mask_length - 1)','line_number':163,'multiline':False]['text':' compute number of masked spans in batch','line_number':169,'multiline':False]['text':' SpecAugment mask to fill','line_number':176,'multiline':False]['text':' compute num of masked spans for this input','line_number':186,'multiline':False]['text':' get random indices to mask','line_number':189,'multiline':False]['text':' pick first sampled index that will serve as a dummy index to pad vector','line_number':194,'multiline':False]['text':' to ensure same dimension for all batches due to probabilistic rounding','line_number':195,'multiline':False]['text':' Picking first sample just pads those vectors twice.','line_number':196,'multiline':False]['text':' this case can only happen if `input_length` is strictly smaller then','line_number':198,'multiline':False]['text':' `sequence_length` in which case the last token has to be a padding','line_number':199,'multiline':False]['text':' token which we can use as a dummy mask id','line_number':200,'multiline':False]['text':' expand masked indices to masked spans','line_number':212,'multiline':False]['text':' add offset to the starting indexes so that indexes now create a span','line_number':218,'multiline':False]['text':' ensure that we cannot have indices larger than sequence_length','line_number':225,'multiline':False]['text':' scatter indices to mask','line_number':229,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2._sample_negative_indices','line_number':235,'multiline':False]['text':' generate indices of the positive vectors themselves, repeat them `num_negatives` times','line_number':244,'multiline':False]['text':' get `num_negatives` random vector indices from the same utterance','line_number':247,'multiline':False]['text':' avoid sampling the same positive vector, but keep the distribution uniform','line_number':260,'multiline':False]['text':' remap to actual indices','line_number':263,'multiline':False]['text':' correct for batch size','line_number':266,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->Wav2Vec2Conformer','line_number':272,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->Wav2Vec2Conformer','line_number':294,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->Wav2Vec2Conformer','line_number':322,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->Wav2Vec2Conformer','line_number':347,'multiline':False]['text':' Embeddings are computed in the dtype of the inv_freq constant','line_number':409,'multiline':False]['text':' Computed embeddings are cast to the dtype of the hidden state inputs','line_number':416,'multiline':False]['text':' Reset the positional encodings','line_number':432,'multiline':False]['text':' self.pe contains both positive and negative parts','line_number':434,'multiline':False]['text':' the length of self.pe is 2 * input_len - 1','line_number':435,'multiline':False]['text':' Suppose `i` is the position of query vector and `j` is the','line_number':440,'multiline':False]['text':' position of key vector. We use positive relative positions when keys','line_number':441,'multiline':False]['text':' are to the left (i>j) and negative relative positions otherwise (i<j).','line_number':442,'multiline':False]['text':' Reverse the order of positive indices and concat both positive and','line_number':454,'multiline':False]['text':' negative indices. This is used to support the shifting trick','line_number':455,'multiline':False]['text':' as in https://arxiv.org/abs/1901.02860','line_number':456,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->Wav2Vec2Conformer','line_number':471,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->Wav2Vec2Conformer','line_number':483,'multiline':False]['text':' make sure hidden_states require grad for gradient_checkpointing','line_number':515,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->Wav2Vec2Conformer','line_number':531,'multiline':False]['text':' non-projected hidden states are needed for quantization','line_number':540,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->Wav2Vec2Conformer','line_number':547,'multiline':False]['text':' exchange the temporal dimension and the feature dimension','line_number':612,'multiline':False]['text':' GLU mechanism','line_number':615,'multiline':False]['text':' => (batch, 2*channel, dim)','line_number':616,'multiline':False]['text':' => (batch, channel, dim)','line_number':618,'multiline':False]['text':' 1D Depthwise Conv','line_number':621,'multiline':False]['text':' linear transformation for positional encoding','line_number':652,'multiline':False]['text':' these two learnable bias are used in matrix c and matrix d','line_number':654,'multiline':False]['text':' as described in https://arxiv.org/abs/1901.02860 Section 3.3','line_number':655,'multiline':False]['text':' self-attention mechanism','line_number':666,'multiline':False]['text':' make sure query/key states can be != value states','line_number':669,'multiline':False]['text':' project query_key_states and value_states','line_number':680,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':685,'multiline':False]['text':' apply relative_position_embeddings to qk scores','line_number':696,'multiline':False]['text':' as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860','line_number':697,'multiline':False]['text':' apply attention_mask if necessary','line_number':704,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':708,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':712,'multiline':False]['text':' => (batch, time1, hidden_size)','line_number':715,'multiline':False]['text':' rotate hidden_states with rotary embeddings','line_number':728,'multiline':False]['text':' 1. project positional embeddings','line_number':741,'multiline':False]['text':' => (batch, head, 2*time1-1, d_k)','line_number':742,'multiline':False]['text':' 2. Add bias to query','line_number':750,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':751,'multiline':False]['text':' 3. attention score: first compute matrix a and matrix c','line_number':756,'multiline':False]['text':' as described in https://arxiv.org/abs/1901.02860 Section 3.3','line_number':757,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':758,'multiline':False]['text':' 4. then compute matrix b and matrix d','line_number':761,'multiline':False]['text':' => (batch, head, time1, 2*time1-1)','line_number':762,'multiline':False]['text':' 5. shift matrix b and matrix d','line_number':765,'multiline':False]['text':' 6. sum matrices','line_number':773,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':774,'multiline':False]['text':' Feed-forward 1','line_number':788,'multiline':False]['text':' Self-Attention','line_number':792,'multiline':False]['text':' Conformer Convolution','line_number':797,'multiline':False]['text':' Feed-forward 2','line_number':800,'multiline':False]['text':' 1. Feed-Forward 1 layer','line_number':814,'multiline':False]['text':' 2. Self-Attention layer','line_number':821,'multiline':False]['text':' 3. Convolutional Layer','line_number':832,'multiline':False]['text':' 4. Feed-Forward 2 Layer','line_number':837,'multiline':False]['text':' make sure padded tokens output 0','line_number':877,'multiline':False]['text':' extend attention_mask','line_number':880,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':900,'multiline':False]['text':' under deepspeed zero3 all gpus must run in sync','line_number':905,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer with Wav2Vec2->Wav2Vec2Conformer','line_number':942,'multiline':False]['text':' storage for codebook variables (codewords)','line_number':960,'multiline':False]['text':' can be decayed for training','line_number':966,'multiline':False]['text':' project to codevector dim','line_number':984,'multiline':False]['text':' sample code vector probs via gumbel in differentiateable way','line_number':989,'multiline':False]['text':' compute perplexity','line_number':994,'multiline':False]['text':' take argmax in non-differentiable way','line_number':1000,'multiline':False]['text':' comptute hard codevector distribution (one hot)','line_number':1001,'multiline':False]['text':' use probs to retrieve codevectors','line_number':1011,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Adapter with Wav2Vec2->Wav2Vec2Conformer','line_number':1019,'multiline':False]['text':' feature dim might need to be down-projected','line_number':1024,'multiline':False]['text':' down project hidden_states if necessary','line_number':1035,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AdapterLayer with Wav2Vec2->Wav2Vec2Conformer','line_number':1051,'multiline':False]['text':' Wav2Vec2ForPreTraining last 2 linear layers need standard Linear init.','line_number':1083,'multiline':False]['text':' gumbel softmax requires special init','line_number':1089,'multiline':False]['text':' 1D convolutional layer output length formula taken','line_number':1135,'multiline':False]['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':1136,'multiline':False]['text':' Effectively attention_mask.sum(-1), but not inplace to be able to run','line_number':1151,'multiline':False]['text':' on inference mode.','line_number':1152,'multiline':False]['text':' these two operations makes sure that all values before the output lengths idxs are attended to','line_number':1163,'multiline':False]['text':' model only needs masking vector if mask prob is > 0.0','line_number':1237,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1245,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.freeze_feature_encoder','line_number':1248,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states','line_number':1256,'multiline':False]['text':' `config.apply_spec_augment` can set masking to False','line_number':1268,'multiline':False]['text':' generate indices & apply SpecAugment along time axis','line_number':1272,'multiline':False]['text':' apply SpecAugment along time axis with given mask_time_indices','line_number':1276,'multiline':False]['text':' generate indices & apply SpecAugment along feature axis','line_number':1290,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.forward with wav2vec2->wav2vec2_conformer','line_number':1311,'multiline':False]['text':' compute reduced attention_mask corresponding to feature vectors','line_number':1331,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer','line_number':1369,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1380,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.set_gumbel_temperature','line_number':1383,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.freeze_feature_encoder with wav2vec2->wav2vec2_conformer','line_number':1390,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.compute_contrastive_logits','line_number':1399,'multiline':False]['text':' apply temperature','line_number':1416,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,wav2vec2_conformer-base->wav2vec2-conformer-rel-pos-large','line_number':1422,'multiline':False]['text':' 1. project all transformed features (including masked) to final vq dim','line_number':1504,'multiline':False]['text':' 2. quantize all (unmasked) extracted features and project to final vq dim','line_number':1507,'multiline':False]['text':' compute reduced attention_mask correponding to feature vectors','line_number':1511,'multiline':False]['text':' for training, we sample negatives','line_number':1525,'multiline':False]['text':' 3. sample K negatives (distractors) quantized states for contrastive loss','line_number':1526,'multiline':False]['text':' if attention_mask is passed, make sure that padded feature vectors cannot be sampled','line_number':1527,'multiline':False]['text':' sample negative quantized vectors BTC => (BxT)C','line_number':1528,'multiline':False]['text':' 4. compute logits, corresponding to `logs = sim(c_t, [q_t, \sim{q}_t]) / \kappa`','line_number':1536,'multiline':False]['text':' of equation (3) in https://arxiv.org/pdf/2006.11477.pdf','line_number':1537,'multiline':False]['text':' 5. if a negative vector is identical to the positive (i.e. when codebook utilization is low),','line_number':1545,'multiline':False]['text':' its cosine similarity will be masked','line_number':1546,'multiline':False]['text':' 6. compute contrastive loss \mathbf{L}_m = cross_entropy(logs) =','line_number':1552,'multiline':False]['text':' -log(exp(sim(c_t, q_t)/\kappa) / \sum_{\sim{q}} exp(sim(c_t, \sim{q})/\kappa))','line_number':1553,'multiline':False]['text':' 7. compute diversity loss: \mathbf{L}_d','line_number':1558,'multiline':False]['text':' 8. \mathbf{L} = \mathbf{L}_m + \alpha * \mathbf{L}_d','line_number':1562,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer','line_number':1587,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1608,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.freeze_feature_encoder with wav2vec2->wav2vec2_conformer','line_number':1611,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer','line_number':1627,'multiline':False]['text':' retrieve loss input_lengths from attention_mask','line_number':1665,'multiline':False]['text':' assuming that padded tokens are filled with -100','line_number':1671,'multiline':False]['text':' when not being attended to','line_number':1672,'multiline':False]['text':' ctc_loss doesn't support fp16','line_number':1677,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer','line_number':1708,'multiline':False]['text':' transformer layers + input embeddings','line_number':1717,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1723,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->wav2vec2_conformer','line_number':1726,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER','line_number':1749,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER','line_number':1819,'multiline':False]['text':' transformer layers + input embeddings','line_number':1828,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.freeze_feature_encoder with wav2vec2->wav2vec2_conformer','line_number':1836,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.freeze_base_model with wav2vec2->wav2vec2_conformer','line_number':1844,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.forward with wav2vec2->wav2vec2_conformer','line_number':1860,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss','line_number':1915,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer','line_number':1939,'multiline':False]['text':' transformer layers + input embeddings','line_number':1977,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.freeze_feature_encoder with wav2vec2->wav2vec2_conformer','line_number':1992,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.freeze_base_model with wav2vec2->wav2vec2_conformer','line_number':2000,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector._get_tdnn_output_lengths with wav2vec2->wav2vec2_conformer','line_number':2009,'multiline':False]['text':' 1D convolutional layer output length formula taken','line_number':2016,'multiline':False]['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':2017,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER','line_number':2032,'multiline':False]['text':' Statistic Pooling','line_number':2073,'multiline':False]