['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The Meta AI Authors and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all SAM models at https://huggingface.co/models?filter=sam','line_number':47,'multiline':False]['text':' Input projections','line_number':254,'multiline':False]['text':' Separate into heads','line_number':260,'multiline':False]['text':' SamAttention','line_number':265,'multiline':False]['text':' batch_size * point_batch_size  x N_heads x N_tokens x N_tokens','line_number':269,'multiline':False]['text':' Get output','line_number':273,'multiline':False]['text':' Self attention block','line_number':344,'multiline':False]['text':' Cross attention block, tokens attending to image embedding','line_number':353,'multiline':False]['text':' MLP block','line_number':362,'multiline':False]['text':' Cross attention block, image embedding attending to tokens','line_number':367,'multiline':False]['text':' Prepare queries','line_number':454,'multiline':False]['text':' Apply transformer blocks and final layernorm','line_number':458,'multiline':False]['text':' Apply the final attenion layer from the points to the image','line_number':471,'multiline':False]['text':' Should be (1, 32) + (4, 32) = (5, 32)','line_number':620,'multiline':False]['text':' Should be (batch_size, point_size, 5, 32)','line_number':623,'multiline':False]['text':' Matt: The original Torch code checked that the sum of sparse_prompt_embeddings equalled 0. However, this only','line_number':625,'multiline':False]['text':'       happens when the sparse prompt embeddings are an empty tensor with shape[1] == 0. I replaced','line_number':626,'multiline':False]['text':'       it with an explicit shape check to avoid data-dependent control flow which breaks XLA.','line_number':627,'multiline':False]['text':' TODO Matt: What is going on here? Why is a non-trainable weight randomly initialized?','line_number':692,'multiline':False]['text':' assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape','line_number':714,'multiline':False]['text':' outputs d_1 x ... x d_n x channel shape','line_number':719,'multiline':False]['text':' Convert to channels-last','line_number':736,'multiline':False]['text':' Convert back to channels-first','line_number':745,'multiline':False]['text':' This class needs an explicit build method because it isn't called with the standard dummy inputs','line_number':749,'multiline':False]['text':' We must explicitly build the mask embed because it isn't touched by the standard dummy inputs','line_number':803,'multiline':False]['text':' Shift to center of pixel','line_number':817,'multiline':False]['text':' Shift to center of pixel','line_number':845,'multiline':False]['text':' initialize relative positional embeddings','line_number':936,'multiline':False]['text':' Interpolate rel pos if needed.','line_number':971,'multiline':False]['text':' Interpolate rel pos.','line_number':973,'multiline':False]['text':' Scale the coords with short length if shapes for q and k are different.','line_number':983,'multiline':False]['text':' qkv with shape (3, batch_size, nHead, height * width, channel)','line_number':1037,'multiline':False]['text':' q, k, v with shape (batch_size * nHead, height * width, channel)','line_number':1040,'multiline':False]['text':' Initialize absolute positional embedding with pretrain image size.','line_number':1240,'multiline':False]['text':' channel x height x width','line_number':1438,'multiline':False]['text':' Ensures that later checks pass even with an all-None shape from the serving signature','line_number':1551,'multiline':False]['text':' repeat with batch size','line_number':1562,'multiline':False]