['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 Google AI and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all OwlViT models at https://huggingface.co/models?filter=owlvit','line_number':49,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.contrastive_loss with clip->owlvit','line_number':57,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->owlvit','line_number':62,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr._upcast','line_number':107,'multiline':False]['text':' Protects from numerical overflows in multiplications by upcasting to the equivalent higher type','line_number':109,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.box_area','line_number':116,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.box_iou','line_number':133,'multiline':False]['text':' [N,M,2]','line_number':138,'multiline':False]['text':' [N,M,2]','line_number':139,'multiline':False]['text':' [N,M,2]','line_number':141,'multiline':False]['text':' [N,M]','line_number':142,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.generalized_box_iou','line_number':150,'multiline':False]['text':' degenerate boxes gives inf / nan results','line_number':158,'multiline':False]['text':' so do an early check','line_number':159,'multiline':False]['text':' [N,M,2]','line_number':169,'multiline':False]['text':' shape = [batch_size, num_channels, height, width]','line_number':296,'multiline':False]['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':312,'multiline':False]['text':' get query proj','line_number':373,'multiline':False]['text':' apply the causal_attention_mask first','line_number':392,'multiline':False]['text':' this operation is a bit akward, but it's required to','line_number':413,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':414,'multiline':False]['text':' In order to do so, attn_weights have to reshaped','line_number':415,'multiline':False]['text':' twice and have to be reused in the following','line_number':416,'multiline':False]['text':' For int8 compatibility, sometimes the `attn_probs` are in `fp32`','line_number':424,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->OwlViT','line_number':444,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->OwlViT','line_number':460,'multiline':False]['text':' num_samples, seq_len = input_shape  where num_samples = batch_size * num_max_text_queries','line_number':798,'multiline':False]['text':' OWLVIT's text model uses causal mask, prepare it here.','line_number':799,'multiline':False]['text':' https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324','line_number':800,'multiline':False]['text':' expand attention_mask','line_number':804,'multiline':False]['text':' [num_samples, seq_len] -> [num_samples, 1, tgt_seq_len, src_seq_len]','line_number':806,'multiline':False]['text':' take features from the end of tokens embedding (end of token is the highest number in each sequence)','line_number':821,'multiline':False]['text':' casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14','line_number':822,'multiline':False]['text':' Initialize weights and apply final processing','line_number':845,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':881,'multiline':False]['text':' Cast the input to the expected `dtype`','line_number':919,'multiline':False]['text':' Initialize weights and apply final processing','line_number':956,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1032,'multiline':False]['text':' Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1060,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':1063,'multiline':False]['text':' Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1096,'multiline':False]['text':' Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1146,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':1160,'multiline':False]['text':' normalized features','line_number':1174,'multiline':False]['text':' cosine similarity as logits and set it on the correct device','line_number':1178,'multiline':False]['text':' Normalize image and text features','line_number':1258,'multiline':False]['text':' Get class predictions','line_number':1262,'multiline':False]['text':' Apply a learnable shift and scale to logits','line_number':1265,'multiline':False]['text':' Computes normalized xy corner coordinates from feature_map.','line_number':1296,'multiline':False]['text':' Flatten (h, w, 2) -> (h*w, 2)','line_number':1308,'multiline':False]['text':' The box center is biased to its position on the feature grid','line_number':1317,'multiline':False]['text':' Unnormalize xy','line_number':1321,'multiline':False]['text':' The box size is biased to the patch size','line_number':1324,'multiline':False]['text':' Compute box bias','line_number':1328,'multiline':False]['text':' Bounding box detection head [batch_size, num_boxes, 4].','line_number':1347,'multiline':False]['text':' Compute the location of each token on the grid and use it to compute a bias for the bbox prediction','line_number':1350,'multiline':False]['text':' Encode text and image','line_number':1382,'multiline':False]['text':' Get image embeddings','line_number':1392,'multiline':False]['text':' Resize class token','line_number':1396,'multiline':False]['text':' Merge image embedding with class tokens','line_number':1400,'multiline':False]['text':' Resize to [batch_size, num_patches, num_patches, hidden_size]','line_number':1404,'multiline':False]['text':' Get OwlViTModel vision embeddings (same as CLIP)','line_number':1422,'multiline':False]['text':' Apply post_layernorm to last_hidden_state, return non-projected output','line_number':1425,'multiline':False]['text':' Resize class token','line_number':1429,'multiline':False]['text':' Merge image embedding with class tokens','line_number':1433,'multiline':False]['text':' Resize to [batch_size, num_patches, num_patches, hidden_size]','line_number':1437,'multiline':False]['text':' Loop over query images','line_number':1455,'multiline':False]['text':' If there are no overlapping boxes, fall back to generalized IoU','line_number':1465,'multiline':False]['text':' Use an adaptive threshold to include all boxes within 80% of the best IoU','line_number':1469,'multiline':False]['text':' Compute feature maps for the input and query images','line_number':1538,'multiline':False]['text':' Get top class embedding and best box index for each query image in batch','line_number':1551,'multiline':False]['text':' Predict object classes [batch_size, num_patches, num_queries+1]','line_number':1554,'multiline':False]['text':' Predict object boxes','line_number':1557,'multiline':False]['text':' Embed images and text queries','line_number':1637,'multiline':False]['text':' Text and vision model outputs','line_number':1646,'multiline':False]['text':' Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]','line_number':1653,'multiline':False]['text':' If first token is 0, then this is a padded query [batch_size, num_queries].','line_number':1657,'multiline':False]['text':' Predict object classes [batch_size, num_patches, num_queries+1]','line_number':1661,'multiline':False]['text':' Predict object boxes','line_number':1664,'multiline':False]