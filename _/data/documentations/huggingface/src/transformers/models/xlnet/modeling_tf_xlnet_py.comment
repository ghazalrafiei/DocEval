['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.','line_number':2,'multiline':False]['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' See all XLNet models at https://huggingface.co/models?filter=xlnet','line_number':65,'multiline':False]['text':' x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))','line_number':138,'multiline':False]['text':' content based attention score','line_number':146,'multiline':False]['text':' position based attention score','line_number':149,'multiline':False]['text':' segment based attention score','line_number':153,'multiline':False]['text':' merge attention scores and perform masking','line_number':160,'multiline':False]['text':' attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask','line_number':163,'multiline':False]['text':' attention probability','line_number':169,'multiline':False]['text':' Mask heads if we want to','line_number':174,'multiline':False]['text':' attention output','line_number':178,'multiline':False]['text':' post-attention projection (back to `d_model`)','line_number':188,'multiline':False]['text':' Two-stream attention with relative positional encoding.','line_number':214,'multiline':False]['text':' content based attention score','line_number':215,'multiline':False]['text':' content-based key head','line_number':221,'multiline':False]['text':' content-based value head','line_number':224,'multiline':False]['text':' position-based key head','line_number':227,'multiline':False]['text':' h-stream','line_number':230,'multiline':False]['text':' content-stream query head','line_number':231,'multiline':False]['text':' core attention ops','line_number':234,'multiline':False]['text':' post processing','line_number':250,'multiline':False]['text':' g-stream','line_number':253,'multiline':False]['text':' query-stream query head','line_number':254,'multiline':False]['text':' core attention ops','line_number':257,'multiline':False]['text':' post processing','line_number':292,'multiline':False]['text':' Multi-head attention with relative positional encoding','line_number':299,'multiline':False]['text':' content heads','line_number':305,'multiline':False]['text':' positional heads','line_number':310,'multiline':False]['text':' core attention ops','line_number':313,'multiline':False]['text':' post processing','line_number':329,'multiline':False]['text':' Add again attentions if there are there','line_number':421,'multiline':False]['text':' The output weights are the same as the input embeddings, but there is','line_number':440,'multiline':False]['text':' an output-only bias for each token.','line_number':441,'multiline':False]['text':' cache hidden states into memory.','line_number':557,'multiline':False]['text':' If `use_mems` is active but no `mem_len` is defined, the model behaves like GPT-2 at inference time','line_number':562,'multiline':False]['text':' and returns all of the past and current hidden states.','line_number':563,'multiline':False]['text':' If `use_mems` is active and `mem_len` is defined, the model returns the last `mem_len` hidden','line_number':566,'multiline':False]['text':' states. This is the preferred setting for training and long-form generation.','line_number':567,'multiline':False]['text':' if `use_mems` is active and `mem_len` is defined, the model','line_number':570,'multiline':False]['text':' beg, end = klen - 1, -qlen','line_number':594,'multiline':False]['text':' beg, end = klen - 1, -1','line_number':597,'multiline':False]['text':' the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end','line_number':651,'multiline':False]['text':' but we want a unified interface in the library with the batch size on the first dimension','line_number':652,'multiline':False]['text':' so we move here the first dimension (batch) to the end','line_number':653,'multiline':False]['text':' Attention mask','line_number':675,'multiline':False]['text':' causal attention mask','line_number':676,'multiline':False]['text':' data mask: input mask & perm mask','line_number':685,'multiline':False]['text':' all mems can be attended to','line_number':703,'multiline':False]['text':' Word embeddings and prepare h & g hidden states','line_number':723,'multiline':False]['text':' else:  # We removed the inp_q input which was same as target mapping','line_number':732,'multiline':False]['text':'     inp_q_ext = inp_q[:, :, None]','line_number':733,'multiline':False]['text':'     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k','line_number':734,'multiline':False]['text':' Segment embedding','line_number':739,'multiline':False]['text':' Convert `token_type_ids` to one-hot `seg_mat`','line_number':741,'multiline':False]['text':' `1` indicates not in the same segment [qlen x klen x bsz]','line_number':748,'multiline':False]['text':' Positional encoding','line_number':757,'multiline':False]['text':' Prepare head mask if needed','line_number':761,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':762,'multiline':False]['text':' attention_probs has shape bsz x n_heads x N x N','line_number':763,'multiline':False]['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)','line_number':764,'multiline':False]['text':' and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]','line_number':765,'multiline':False]['text':' cache new mems','line_number':778,'multiline':False]['text':' Add last hidden state','line_number':801,'multiline':False]['text':' Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)','line_number':807,'multiline':False]['text':' when target_mapping is provided, there are 2-tuple of attentions','line_number':819,'multiline':False]['text':' generate fails to convert to a graph with XLNet','line_number':1243,'multiline':False]['text':' Add dummy token at the end (no attention on this one)','line_number':1254,'multiline':False]['text':' At every pass, the attention values for the new token and the two last generated tokens','line_number':1258,'multiline':False]['text':' are computed, the rest is reloaded from the `past` cache. A purely auto-regressive model would have','line_number':1259,'multiline':False]['text':' offset = 1; offset = 2 seems to have slightly better computation.','line_number':1260,'multiline':False]['text':' Build permutation mask so that previous tokens don't see last token','line_number':1268,'multiline':False]['text':' We'll only predict the last token','line_number':1274,'multiline':False]['text':' if past is defined in model kwargs then use it for faster decoding','line_number':1286,'multiline':False]