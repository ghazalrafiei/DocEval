['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2018 The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':163,'multiline':False]['text':' initialize with config','line_number':206,'multiline':False]['text':' make sure that the individual model's config refers to the shared config','line_number':233,'multiline':False]['text':' so that the updates to the config will be synced','line_number':234,'multiline':False]['text':' encoder outputs might need to be projected to different dimension for decoder','line_number':238,'multiline':False]['text':' tie encoder, decoder weights if config set accordingly','line_number':257,'multiline':False]['text':' tie encoder & decoder if needed','line_number':261,'multiline':False]['text':' tie encoder and decoder base model','line_number':263,'multiline':False]['text':' a workaround to load from tensorflow checkpoint','line_number':299,'multiline':False]['text':' Using `_tf_model` won't work, because the weight names in the encoder/decoder of `_tf_model` get','line_number':300,'multiline':False]['text':' extended before saving those components. For example, The name of `_tf_model.encoder.vit` is','line_number':301,'multiline':False]['text':' `[top model name]/encoder/vit`, but the name of `tf_model.encoder.vit` is `[top model name]/vit`. The','line_number':302,'multiline':False]['text':' [top model name] is handled (stripped) by the conversion method, and the former case gets extra `encoder`,','line_number':303,'multiline':False]['text':' which should not occur when we want to save the components alone.','line_number':304,'multiline':False]['text':' There was a (very) ugly potential fix, which wasn't integrated to `transformers`: see','line_number':305,'multiline':False]['text':'   https://github.com/huggingface/transformers/pull/13222/commits/dbb3c9de76eee235791d2064094654637c99f36d#r697304245','line_number':306,'multiline':False]['text':'   (the change in `src/transformers/modeling_tf_utils.py`)','line_number':307,'multiline':False]['text':' Using `tf_model` instead','line_number':311,'multiline':False]['text':' Make sure models are built','line_number':314,'multiline':False]['text':' Get the variable correspondence between `_tf_model` and `encoder` and `decoder`','line_number':318,'multiline':False]['text':' assign weight values to `encoder` and `decoder` from `_tf_model`','line_number':333,'multiline':False]['text':' Deal with `enc_to_dec_proj`','line_number':341,'multiline':False]['text':' This is only for copying some specific attributes of this particular model.','line_number':366,'multiline':False]['text':' At the moment fast initialization is not supported for composite models','line_number':375,'multiline':False]['text':' remove encoder, decoder kwargs from kwargs','line_number':462,'multiline':False]['text':' Load and initialize the encoder and decoder','line_number':468,'multiline':False]['text':' The distinction between encoder and decoder at the model level is made','line_number':469,'multiline':False]['text':' by the value of the flag `is_decoder` that we need to set correctly.','line_number':470,'multiline':False]['text':' instantiate config with corresponding kwargs','line_number':531,'multiline':False]['text':' optionally project encoder_hidden_states','line_number':608,'multiline':False]['text':' Decode','line_number':622,'multiline':False]['text':' Compute loss independent from decoder (as some shift the logits inside them)','line_number':637,'multiline':False]['text':' apply decoder cache reordering here','line_number':689,'multiline':False]