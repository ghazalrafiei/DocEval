['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 LongT5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right','line_number':58,'multiline':False]['text':' pad tensor to multiple of block_len','line_number':84,'multiline':False]['text':' [batch_size, num_blocks, block_len] -> [batch_size, num_blocks + 2, block_len]','line_number':100,'multiline':False]['text':' We use indexing approach here:','line_number':105,'multiline':False]['text':' https://numpy.org/doc/stable/user/basics.indexing.html#dealing-with-variable-numbers-of-indices-within-programs','line_number':106,'multiline':False]['text':' [batch_size, num_blocks, 3 * block_len, ...]','line_number':111,'multiline':False]['text':' [block_len, 3 * block_len]','line_number':118,'multiline':False]['text':' [batch_size, num_blocks, block_len]','line_number':132,'multiline':False]['text':' [batch_size, num_block, 3 * block_len]','line_number':134,'multiline':False]['text':' [batch_size, num_block, block_len, 3 * block_len]','line_number':139,'multiline':False]['text':' [batch_size, 1, num_block, block_len, 3 * block_len]','line_number':142,'multiline':False]['text':' set padding tokens to -1','line_number':172,'multiline':False]['text':' [batch_size, seq_len]','line_number':174,'multiline':False]['text':' [batch_size, seq_len // global_block_size]','line_number':178,'multiline':False]['text':' (batch..., seq_len, global_seq_len))','line_number':199,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5LayerNorm with T5->LongT5','line_number':204,'multiline':False]['text':' layer norm should always be calculated in float32','line_number':218,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5DenseActDense with T5->LongT5','line_number':225,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5DenseGatedActDense with T5->LongT5','line_number':257,'multiline':False]['text':' the dtype of the computation','line_number':260,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5LayerFF with T5->LongT5','line_number':296,'multiline':False]['text':' the dtype of the computation','line_number':299,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Attention with T5->LongT5','line_number':319,'multiline':False]['text':' the dtype of the computation','line_number':324,'multiline':False]['text':' now relative_position is in the range [0, inf)','line_number':392,'multiline':False]['text':' half of the buckets are for exact increments in positions','line_number':394,'multiline':False]['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':398,'multiline':False]['text':' detect if we're initializing by absence of existing cache data.','line_number':438,'multiline':False]['text':' update key, value caches with our new 1d spatial slices','line_number':446,'multiline':False]['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key positions','line_number':455,'multiline':False]['text':' that have already been generated and cached, not the remaining zero elements.','line_number':456,'multiline':False]['text':' if key and values are already calculated, only the last query position bias should be taken','line_number':478,'multiline':False]['text':' q, k, v projections','line_number':504,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':505,'multiline':False]['text':' reshape to (batch_size, seq_length, n_heads, head_dim)','line_number':509,'multiline':False]['text':' counter-act scaling in dot_product_attention_weights function','line_number':514,'multiline':False]['text':' for fast decoding causal attention mask should be shifted','line_number':517,'multiline':False]['text':' create causal attention_mask; attention_mask has to be defined when model is causal','line_number':521,'multiline':False]['text':' fast decoding for generate requires special attention_mask','line_number':525,'multiline':False]['text':' broadcast causal attention mask & attention mask to fit for merge','line_number':534,'multiline':False]['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':545,'multiline':False]['text':' and cache the keys and values step by step.','line_number':546,'multiline':False]['text':' replace masked positions with -10_000','line_number':552,'multiline':False]['text':' compute position bias (only for first layer)','line_number':562,'multiline':False]['text':' create dropout rng','line_number':570,'multiline':False]['text':' Softmax(QK^T)','line_number':575,'multiline':False]['text':' multiply with value states','line_number':587,'multiline':False]['text':' bring back to (batch_size, seq_length, d_model)','line_number':590,'multiline':False]['text':' apply output matrix','line_number':593,'multiline':False]['text':' the dtype of the computation','line_number':607,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Attention._relative_position_bucket','line_number':657,'multiline':False]['text':' now relative_position is in the range [0, inf)','line_number':677,'multiline':False]['text':' half of the buckets are for exact increments in positions','line_number':679,'multiline':False]['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':683,'multiline':False]['text':' position_bias shape: # (1, 1, n_heads, block_len, 3 * block_len)','line_number':717,'multiline':False]['text':' q, k, v projections','line_number':741,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':742,'multiline':False]['text':' reshape to (batch_size, seq_length, n_heads, head_dim)','line_number':746,'multiline':False]['text':' Split into blocks -> (batch_size, num_blocks, block_len, n_heads, head_dim)','line_number':751,'multiline':False]['text':' Concatenate 3 blocks for keys and values -> (batch_size, num_blocks, 3 * block_len, n_heads, dim_per_head)','line_number':756,'multiline':False]['text':' counter-act scaling in dot_product_attention_weights function','line_number':760,'multiline':False]['text':' replace masked positions with -10_000','line_number':766,'multiline':False]['text':' compute position bias (only for first layer)','line_number':774,'multiline':False]['text':' create dropout rng','line_number':780,'multiline':False]['text':' Softmax(QK^T)','line_number':785,'multiline':False]['text':' multiply with value states','line_number':797,'multiline':False]['text':' bring back to (batch_size, seq_length, d_model)','line_number':800,'multiline':False]['text':' apply output matrix','line_number':804,'multiline':False]['text':' the dtype of the computation','line_number':818,'multiline':False]['text':' Relativen attention bias & Layer norm for global attention','line_number':868,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Attention._relative_position_bucket','line_number':880,'multiline':False]['text':' now relative_position is in the range [0, inf)','line_number':900,'multiline':False]['text':' half of the buckets are for exact increments in positions','line_number':902,'multiline':False]['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':906,'multiline':False]['text':' (batch_size, 1, 1, seq_len, global_seq_len)','line_number':934,'multiline':False]['text':' (batch_size, seq_len, global_seq_len)','line_number':941,'multiline':False]['text':' (batch_size, seq_len, global_seq_len, num_heads)','line_number':949,'multiline':False]['text':' (batch_size, 1, num_heads, seq_len, global_seq_len)','line_number':952,'multiline':False]['text':' (batch_size, num_heads, seq_len, global_seq_len)','line_number':954,'multiline':False]['text':' position_bias shape: # (1, 1, n_heads, block_len, 3 * block_len)','line_number':965,'multiline':False]['text':' Prepare components for transient-global attention','line_number':989,'multiline':False]['text':' Obtain block_ids and global_segment_ids','line_number':990,'multiline':False]['text':' global_seq_len := seq_len // self.global_block_size','line_number':991,'multiline':False]['text':' shapes: (batch_size, seq_len) & (batch_size, global_seq_len)','line_number':992,'multiline':False]['text':' Create global inputs','line_number':997,'multiline':False]['text':' q, k, v projections','line_number':1002,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':1003,'multiline':False]['text':' reshape to (batch_size, seq_length, n_heads, head_dim)','line_number':1007,'multiline':False]['text':' Get global/side key/value_states','line_number':1012,'multiline':False]['text':' reshape to (batch_size, global_seq_len, n_heads, head_dim)','line_number':1016,'multiline':False]['text':' Split into blocks -> (batch_size, num_blocks, block_len, n_heads, head_dim)','line_number':1020,'multiline':False]['text':' Concatenate 3 blocks for keys and values -> (batch_size, num_blocks, 3 * block_len, n_heads, dim_per_head)','line_number':1025,'multiline':False]['text':' Tile side inputs across local key/value blocks','line_number':1029,'multiline':False]['text':' New shape: (batch_size, num_blocks, global_seq_len, n_heads, dim_per_head)','line_number':1030,'multiline':False]['text':' Concatenate "local" and "side"/"global" key/value states to allow each token to attend global aggregated ones','line_number':1036,'multiline':False]['text':' New shape: (batch_size, num_blocks, 3 * block_len + global_seq_len, n_heads, dim_per_head)','line_number':1037,'multiline':False]['text':' counter-act scaling in dot_product_attention_weights function','line_number':1041,'multiline':False]['text':' compute position bias (only for first layer)','line_number':1055,'multiline':False]['text':' Calculate global/side bias - shape: # (batch_size, num_heads, seq_len, global_seq_len)','line_number':1060,'multiline':False]['text':' create dropout rng','line_number':1068,'multiline':False]['text':' Softmax(QK^T)','line_number':1073,'multiline':False]['text':' multiply with value states','line_number':1085,'multiline':False]['text':' bring back to (batch_size, seq_length, d_model)','line_number':1088,'multiline':False]['text':' apply output matrix','line_number':1092,'multiline':False]['text':' the dtype of the computation','line_number':1108,'multiline':False]['text':' to accept init_cache kwargs','line_number':1126,'multiline':False]['text':' add attentions if we output them','line_number':1137,'multiline':False]['text':' the dtype of the computation','line_number':1146,'multiline':False]['text':' to accept init_cache kwargs','line_number':1164,'multiline':False]['text':' add attentions if we output them','line_number':1175,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5LayerSelfAttention with T5->LongT5','line_number':1179,'multiline':False]['text':' the dtype of the computation','line_number':1183,'multiline':False]['text':' add attentions if we output them','line_number':1216,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5LayerCrossAttention with T5->LongT5','line_number':1220,'multiline':False]['text':' the dtype of the computation','line_number':1223,'multiline':False]['text':' add attentions if we output them','line_number':1252,'multiline':False]['text':' the dtype of the computation','line_number':1259,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Block.__call__ with T5->LongT5','line_number':1289,'multiline':False]['text':' Keep self-attention outputs and relative position weights','line_number':1312,'multiline':False]['text':' Keep cross-attention outputs and relative position weights','line_number':1326,'multiline':False]['text':' Apply Feed Forward layer','line_number':1329,'multiline':False]['text':' returns hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights),','line_number':1336,'multiline':False]['text':' (cross-attention position bias), (cross-attention weights)','line_number':1337,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5LayerCollection with T5->LongT5','line_number':1341,'multiline':False]['text':' the dtype of the computation','line_number':1345,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5BlockCollection with T5->LongT5','line_number':1377,'multiline':False]['text':' the dtype of the computation','line_number':1380,'multiline':False]['text':' Prepare head mask if needed','line_number':1418,'multiline':False]['text':' We share the position biases between the layers - the first layer store them','line_number':1443,'multiline':False]['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1444,'multiline':False]['text':' (cross-attention position bias), (cross-attention weights)','line_number':1445,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Stack with T5->LongT5','line_number':1464,'multiline':False]['text':' the dtype of the computation','line_number':1468,'multiline':False]['text':' Add last layer','line_number':1513,'multiline':False]['text':' init input tensors','line_number':1696,'multiline':False]['text':' prepare encoder inputs','line_number':1750,'multiline':False]['text':' prepare decoder inputs','line_number':1754,'multiline':False]['text':' Handle any PRNG if needed','line_number':1758,'multiline':False]['text':' init input variables to retrieve cache','line_number':1788,'multiline':False]['text':' we only need to call the decoder to init the cache','line_number':1806,'multiline':False]['text':' Handle any PRNG if needed','line_number':1847,'multiline':False]['text':' Handle any PRNG if needed','line_number':1921,'multiline':False]['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be','line_number':1928,'multiline':False]['text':' passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that','line_number':1929,'multiline':False]['text':' it can be changed by FlaxLongT5Attention module','line_number':1930,'multiline':False]['text':' add updated cache to model output','line_number':1960,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Module with T5->LongT5','line_number':2017,'multiline':False]['text':' the dtype of the computation','line_number':2020,'multiline':False]['text':' Encode if needed (training, first prediction pass)','line_number':2070,'multiline':False]['text':' Decode','line_number':2080,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5Model with T5->LongT5','line_number':2107,'multiline':False]['text':' Copied from transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule with T5->LongT5','line_number':2142,'multiline':False]['text':' the dtype of the computation','line_number':2145,'multiline':False]['text':' Encode','line_number':2201,'multiline':False]['text':' Decode','line_number':2213,'multiline':False]['text':' Rescale output before projecting on vocab','line_number':2228,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':2229,'multiline':False]['text':' Handle any PRNG if needed','line_number':2309,'multiline':False]['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be','line_number':2316,'multiline':False]['text':' passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that','line_number':2317,'multiline':False]['text':' it can be changed by FlaxLongT5Attention module','line_number':2318,'multiline':False]['text':' Rescale output before projecting on vocab','line_number':2336,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':2337,'multiline':False]['text':' add updated cache to model output','line_number':2378,'multiline':False]['text':' initializing the cache','line_number':2396,'multiline':False]['text':' Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.','line_number':2400,'multiline':False]['text':' But since the decoder uses a causal mask, those positions are masked anyways.','line_number':2401,'multiline':False]['text':' Thus we can create a single static attention_mask here, which is more efficient for compilation','line_number':2402,'multiline':False]