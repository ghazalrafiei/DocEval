['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The Google Flax Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' detect if we're initializing by absence of existing cache data.','line_number':168,'multiline':False]['text':' update key, value caches with our new 1d spatial slices','line_number':176,'multiline':False]['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.','line_number':185,'multiline':False]['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':202,'multiline':False]['text':' for the decoder','line_number':203,'multiline':False]['text':' combine masks if needed','line_number':233,'multiline':False]['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':246,'multiline':False]['text':' and cache the keys and values step by step.','line_number':247,'multiline':False]['text':' transform boolean mask into float mask','line_number':251,'multiline':False]['text':' usual dot product attention','line_number':261,'multiline':False]['text':' residual connection','line_number':341,'multiline':False]['text':' output_attn: a, (attentions)','line_number':342,'multiline':False]['text':' residual connection','line_number':344,'multiline':False]['text':' Cross-Attention Block','line_number':347,'multiline':False]['text':' add one self-attention block for cross-attention','line_number':349,'multiline':False]['text':' residual connection','line_number':365,'multiline':False]['text':' add cross attentions if we output attention weights','line_number':367,'multiline':False]['text':' residual connection','line_number':372,'multiline':False]['text':' init input tensors','line_number':403,'multiline':False]['text':' init input variables to retrieve cache','line_number':446,'multiline':False]['text':' Handle any PRNG if needed','line_number':493,'multiline':False]['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be changed by FlaxGPT2Attention module','line_number':500,'multiline':False]['text':' add updated cache to model output','line_number':523,'multiline':False]['text':' this contains possible `None` values - `FlaxGPT2Module` will filter them out','line_number':581,'multiline':False]['text':' initializing the cache','line_number':746,'multiline':False]['text':' Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.','line_number':750,'multiline':False]['text':' But since GPT2 uses a causal mask, those positions are masked anyways.','line_number':751,'multiline':False]['text':' Thus we can create a single static attention_mask here, which is more efficient for compilation','line_number':752,'multiline':False]