['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all SeamlessM4T models at https://huggingface.co/models?filter=seamless_m4t','line_number':55,'multiline':False]['text':'########### UTILS ################','line_number':213,'multiline':False]['text':' Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids','line_number':216,'multiline':False]['text':' The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.','line_number':227,'multiline':False]['text':' Copied from transformers.models.bart.modeling_bart.shift_tokens_right','line_number':233,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':244,'multiline':False]['text':' attribute kwargs to models','line_number':294,'multiline':False]['text':' If the key is already in a specific config, then it's been set with a','line_number':305,'multiline':False]['text':' submodules specific value and we don't override','line_number':306,'multiline':False]['text':'########### SPEECH ENCODER related code ################','line_number':314,'multiline':False]['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->SeamlessM4TConformer, feat_extract_activation->speech_encoder_hidden_act','line_number':317,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRotaryPositionalEmbedding with Wav2Vec2->SeamlessM4T, num_attention_heads->speech_encoder_attention_heads','line_number':357,'multiline':False]['text':' Embeddings are computed in the dtype of the inv_freq constant','line_number':380,'multiline':False]['text':' Computed embeddings are cast to the dtype of the hidden state inputs','line_number':387,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRelPositionalEmbedding with Wav2Vec2->SeamlessM4T','line_number':392,'multiline':False]['text':' Reset the positional encodings','line_number':404,'multiline':False]['text':' self.pe contains both positive and negative parts','line_number':406,'multiline':False]['text':' the length of self.pe is 2 * input_len - 1','line_number':407,'multiline':False]['text':' Suppose `i` is the position of query vector and `j` is the','line_number':412,'multiline':False]['text':' position of key vector. We use positive relative positions when keys','line_number':413,'multiline':False]['text':' are to the left (i>j) and negative relative positions otherwise (i<j).','line_number':414,'multiline':False]['text':' Reverse the order of positive indices and concat both positive and','line_number':426,'multiline':False]['text':' negative indices. This is used to support the shifting trick','line_number':427,'multiline':False]['text':' as in https://arxiv.org/abs/1901.02860','line_number':428,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSamePadLayer with Wav2Vec2->SeamlessM4T','line_number':443,'multiline':False]['text':' non-projected hidden states are needed for quantization','line_number':463,'multiline':False]['text':' Ensure that we do not leak padded positions in depthwise convolution.','line_number':534,'multiline':False]['text':' Put 0 where necessary','line_number':535,'multiline':False]['text':' exchange the temporal dimension and the feature dimension','line_number':539,'multiline':False]['text':' GLU mechanism','line_number':542,'multiline':False]['text':' => (batch, 2*channel, dim)','line_number':543,'multiline':False]['text':' => (batch, channel, dim)','line_number':545,'multiline':False]['text':' 1D Depthwise Conv','line_number':548,'multiline':False]['text':' linear transformation for positional encoding','line_number':579,'multiline':False]['text':' these two learnable bias are used in matrix c and matrix d','line_number':581,'multiline':False]['text':' as described in https://arxiv.org/abs/1901.02860 Section 3.3','line_number':582,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention.forward','line_number':586,'multiline':False]['text':' self-attention mechanism','line_number':594,'multiline':False]['text':' make sure query/key states can be != value states','line_number':597,'multiline':False]['text':' project query_key_states and value_states','line_number':608,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':613,'multiline':False]['text':' apply relative_position_embeddings to qk scores','line_number':624,'multiline':False]['text':' as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860','line_number':625,'multiline':False]['text':' apply attention_mask if necessary','line_number':632,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':636,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':640,'multiline':False]['text':' => (batch, time1, hidden_size)','line_number':643,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_rotary_embedding','line_number':649,'multiline':False]['text':' rotate hidden_states with rotary embeddings','line_number':657,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_relative_embeddings','line_number':669,'multiline':False]['text':' 1. project positional embeddings','line_number':671,'multiline':False]['text':' => (batch, head, 2*time1-1, d_k)','line_number':672,'multiline':False]['text':' 2. Add bias to query','line_number':680,'multiline':False]['text':' => (batch, head, time1, d_k)','line_number':681,'multiline':False]['text':' 3. attention score: first compute matrix a and matrix c','line_number':686,'multiline':False]['text':' as described in https://arxiv.org/abs/1901.02860 Section 3.3','line_number':687,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':688,'multiline':False]['text':' 4. then compute matrix b and matrix d','line_number':691,'multiline':False]['text':' => (batch, head, time1, 2*time1-1)','line_number':692,'multiline':False]['text':' 5. shift matrix b and matrix d','line_number':695,'multiline':False]['text':' 6. sum matrices','line_number':703,'multiline':False]['text':' => (batch, head, time1, time2)','line_number':704,'multiline':False]['text':' Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4T, attention_dropout->speech_encoder_dropout, torch.nn->nn','line_number':713,'multiline':False]['text':' Feed-forward 1','line_number':719,'multiline':False]['text':' Self-Attention','line_number':723,'multiline':False]['text':' Conformer Convolution','line_number':728,'multiline':False]['text':' Feed-forward 2','line_number':731,'multiline':False]['text':' 1. Feed-Forward 1 layer','line_number':746,'multiline':False]['text':' 2. Self-Attention layer','line_number':753,'multiline':False]['text':' 3. Convolutional Layer','line_number':764,'multiline':False]['text':' 4. Feed-Forward 2 Layer','line_number':769,'multiline':False]['text':' make sure padded tokens output 0','line_number':813,'multiline':False]['text':' extend attention_mask','line_number':815,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':835,'multiline':False]['text':' under deepspeed zero3 all gpus must run in sync','line_number':842,'multiline':False]['text':' 1. residual convolution','line_number':888,'multiline':False]['text':' Self-Attention','line_number':899,'multiline':False]['text':' Feed-forward','line_number':911,'multiline':False]['text':' Apply pooling to the residual to match the sequence length of the','line_number':931,'multiline':False]['text':' multi-head attention output.','line_number':932,'multiline':False]['text':' (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)','line_number':933,'multiline':False]['text':' (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)','line_number':937,'multiline':False]['text':' Apply pooling before feeding to the multihead-attention layer.','line_number':941,'multiline':False]['text':' (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)','line_number':942,'multiline':False]['text':' (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)','line_number':946,'multiline':False]['text':' The rest of the computation is identical to a vanilla Transformer','line_number':959,'multiline':False]['text':' encoder layer.','line_number':960,'multiline':False]['text':' down project hidden_states if necessary','line_number':984,'multiline':False]['text':'########### TEXT / UNITS related code ################','line_number':992,'multiline':False]['text':' Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding','line_number':995,'multiline':False]['text':' in forward put the weights on the correct dtype and device of the param','line_number':1009,'multiline':False]['text':' zero pad','line_number':1028,'multiline':False]['text':' Create the position ids from the input token ids. Any padded tokens remain padded.','line_number':1041,'multiline':False]['text':' expand embeddings if needed','line_number':1049,'multiline':False]['text':' Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart->SeamlessM4T','line_number':1077,'multiline':False]['text':' if encoder_hidden_states are provided this layer is used as a cross-attention layer','line_number':1122,'multiline':False]['text':' for the decoder','line_number':1123,'multiline':False]['text':' get query proj','line_number':1128,'multiline':False]['text':' get key, value proj','line_number':1130,'multiline':False]['text':' `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`','line_number':1131,'multiline':False]['text':' is checking that the `sequence_length` of the `past_key_value` is the same as','line_number':1132,'multiline':False]['text':' the provided `encoder_hidden_states` to support prefix tuning','line_number':1133,'multiline':False]['text':' reuse k,v, cross_attentions','line_number':1139,'multiline':False]['text':' cross_attentions','line_number':1143,'multiline':False]['text':' reuse k, v, self_attention','line_number':1147,'multiline':False]['text':' self_attention','line_number':1153,'multiline':False]['text':' if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.','line_number':1158,'multiline':False]['text':' Further calls to cross_attention layer can then reuse all cross-attention','line_number':1159,'multiline':False]['text':' key/value_states (first "if" case)','line_number':1160,'multiline':False]['text':' if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of','line_number':1161,'multiline':False]['text':' all previous decoder key/value_states. Further calls to uni-directional self-attention','line_number':1162,'multiline':False]['text':' can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)','line_number':1163,'multiline':False]['text':' if encoder bi-directional self-attention `past_key_value` is always `None`','line_number':1164,'multiline':False]['text':' this operation is a bit awkward, but it's required to','line_number':1192,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':1193,'multiline':False]['text':' In order to do so, attn_weights have to be reshaped','line_number':1194,'multiline':False]['text':' twice and have to be reused in the following','line_number':1195,'multiline':False]['text':' Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be','line_number':1214,'multiline':False]['text':' partitioned across GPUs when using tensor-parallelism.','line_number':1215,'multiline':False]['text':' Copied from transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense with NllbMoe->SeamlessM4T,DenseActDense->FeedForwardNetwork, d_model->hidden_size','line_number':1223,'multiline':False]['text':' Self Attention','line_number':1370,'multiline':False]['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':1371,'multiline':False]['text':' add present self-attn cache to positions 1,2 of present_key_value tuple','line_number':1373,'multiline':False]['text':' Cross-Attention Block','line_number':1383,'multiline':False]['text':' cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple','line_number':1390,'multiline':False]['text':' add cross-attn to positions 3,4 of present_key_value tuple','line_number':1403,'multiline':False]['text':' Fully Connected','line_number':1406,'multiline':False]['text':'########### SUB-MODELS related code ################','line_number':1424,'multiline':False]['text':' 1. First, let's compute last_hidden_states from hidden_states.','line_number':1506,'multiline':False]['text':' For each generation step, takes the hidden state from the last layer.','line_number':1507,'multiline':False]['text':' shape: (batch_size*vocab_size*num_return_sequences, # generation_steps, hidden_dim)','line_number':1508,'multiline':False]['text':' 2. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent','line_number':1511,'multiline':False]['text':' to a beam search approach were the first (and only) beam is always selected','line_number':1512,'multiline':False]['text':' in that case, return directly last_hidden_states','line_number':1513,'multiline':False]['text':' 3. cut beam_indices to longest beam length','line_number':1517,'multiline':False]['text':' 4. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards anyways','line_number':1523,'multiline':False]['text':' 5. expand beam_indices to last_hidden_states dim','line_number':1526,'multiline':False]['text':' 6. select the right candidate for each beam','line_number':1530,'multiline':False]['text':' in other words, new_last_hidden_states[i,j,k] = last_hidden_states[beam_indices[i,j,k], j, k] for all i, j, k','line_number':1531,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1554,'multiline':False]['text':' inspired from MBart and NllbMoe','line_number':1608,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1665,'multiline':False]['text':' retrieve input_ids and inputs_embeds','line_number':1719,'multiline':False]['text':' expand attention_mask','line_number':1743,'multiline':False]['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':1745,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':1754,'multiline':False]['text':' skip the layer','line_number':1758,'multiline':False]['text':' if embed_tokens defined, use its shape instead','line_number':1818,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1843,'multiline':False]['text':' retrieve input_ids and inputs_embeds','line_number':1924,'multiline':False]['text':' past_key_values_length','line_number':1937,'multiline':False]['text':' expand encoder attention mask','line_number':1947,'multiline':False]['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':1949,'multiline':False]['text':' embed positions','line_number':1954,'multiline':False]['text':' decoder layers','line_number':1968,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':1975,'multiline':False]['text':' add hidden states from the last decoder layer','line_number':2019,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2057,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':2091,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':2099,'multiline':False]['text':' update config - used principaly for bos_token_id etc.','line_number':2149,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2160,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':2256,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':2261,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':2276,'multiline':False]['text':'########### VOCODER related code ################','line_number':2289,'multiline':False]['text':' Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock','line_number':2309,'multiline':False]['text':' Input: B x T x C; Output: B x T','line_number':2395,'multiline':False]['text':' remove seq-len dim since this collapses to 1','line_number':2471,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2498,'multiline':False]['text':' take care of edge cases where no padding or too many padding','line_number':2507,'multiline':False]['text':' 1D convolutional layer output length formula taken','line_number':2521,'multiline':False]['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':2522,'multiline':False]['text':' conv_pre','line_number':2530,'multiline':False]['text':' upsampler','line_number':2533,'multiline':False]['text':' resblock','line_number':2541,'multiline':False]['text':' conv_post','line_number':2552,'multiline':False]['text':' B x C x T','line_number':2578,'multiline':False]['text':' if batched sample, need to interleave per sample, and pad -> loss of parallelism','line_number':2582,'multiline':False]['text':'########### WHOLE MODEL related code ################','line_number':2634,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2660,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':2732,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':2742,'multiline':False]['text':' prepare text_decoder_input_ids','line_number':2854,'multiline':False]['text':' overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.','line_number':2856,'multiline':False]['text':' also accept __xxx__','line_number':2861,'multiline':False]['text':' tgt_lang gets priority over decoder input ids','line_number':2868,'multiline':False]['text':' only a warning, otherwise errors appear in the tests','line_number':2877,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':2903,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':2908,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':2920,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2948,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':3017,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':3034,'multiline':False]['text':' overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.','line_number':3144,'multiline':False]['text':' also accept __xxx__','line_number':3155,'multiline':False]['text':' tgt_lang gets priority over decoder input ids','line_number':3162,'multiline':False]['text':' only a warning, otherwise errors appear in the tests','line_number':3171,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':3196,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':3201,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':3213,'multiline':False]['text':' Initialize weights and apply final processing','line_number':3243,'multiline':False]['text':' if encoder_outputs is not None, it's probably used within a .generate method so no need to warn','line_number':3309,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':3323,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':3333,'multiline':False]['text':' also accept __xxx__','line_number':3437,'multiline':False]['text':' overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.','line_number':3460,'multiline':False]['text':' first generation','line_number':3466,'multiline':False]['text':' prepare second generation','line_number':3470,'multiline':False]['text':' take care of num_return_sequences','line_number':3476,'multiline':False]['text':' take most probable hidden states per batch of return_sequences','line_number':3477,'multiline':False]['text':' (batch_size*num_return_sequences, ...) -> (batch_size,...)','line_number':3478,'multiline':False]['text':' get decoder last hidden state - must do a pass through the text decoder','line_number':3487,'multiline':False]['text':' Compute new attention mask','line_number':3496,'multiline':False]['text':' Compute t2u decoder_input_ids','line_number':3501,'multiline':False]['text':' second generation','line_number':3509,'multiline':False]['text':' get rid of t2u_decoder_input_ids','line_number':3513,'multiline':False]['text':' replace eos per pad','line_number':3515,'multiline':False]['text':' offset of control symbols','line_number':3517,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':3548,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':3553,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':3565,'multiline':False]['text':' Initialize weights and apply final processing','line_number':3593,'multiline':False]['text':' if encoder_outputs is not None, it's probably used within a .generate method so no need to warn','line_number':3657,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':3671,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':3688,'multiline':False]['text':' also accept __xxx__','line_number':3789,'multiline':False]['text':' overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.','line_number':3811,'multiline':False]['text':' first generation','line_number':3817,'multiline':False]['text':' prepare second generation','line_number':3821,'multiline':False]['text':' get last_hidden_state from encoder','line_number':3825,'multiline':False]['text':' input modality = speech so new attention mask for the decoder','line_number':3828,'multiline':False]['text':' take care of num_return_sequences','line_number':3837,'multiline':False]['text':' take most probable hidden states per batch of return_sequences','line_number':3838,'multiline':False]['text':' (batch_size*num_return_sequences, ...) -> (batch_size,...)','line_number':3839,'multiline':False]['text':' get decoder last hidden state - must do a pass through the text decoder','line_number':3848,'multiline':False]['text':' Compute new attention mask','line_number':3857,'multiline':False]['text':' Compute t2u decoder_input_ids','line_number':3862,'multiline':False]['text':' second generation','line_number':3870,'multiline':False]['text':' get rid of t2u_decoder_input_ids','line_number':3874,'multiline':False]['text':' replace eos per pad','line_number':3876,'multiline':False]['text':' offset of control symbols','line_number':3878,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':3904,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':3919,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':3924,'multiline':False]['text':' Initialize weights and apply final processing','line_number':3958,'multiline':False]['text':' these models already call post_init in their initialization','line_number':3965,'multiline':False]['text':' if encoder_outputs is not None, it's probably used within a .generate method so no need to warn','line_number':4059,'multiline':False]['text':' if encoder_outputs is not None, it's probably used within a .generate method so no need to warn','line_number':4076,'multiline':False]['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':4090,'multiline':False]['text':' input modality = speech so new attention mask','line_number':4099,'multiline':False]['text':' decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)','line_number':4108,'multiline':False]['text':' also accept __xxx__','line_number':4226,'multiline':False]['text':' overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.','line_number':4254,'multiline':False]['text':' tgt_lang gets priority over decoder input ids','line_number':4256,'multiline':False]['text':' first generation','line_number':4262,'multiline':False]['text':' prepare second generation','line_number':4279,'multiline':False]['text':' get encoder last hidden states','line_number':4283,'multiline':False]['text':' get last_hidden_state from encoder - must do a pass through the speech encoder','line_number':4285,'multiline':False]['text':' input modality = speech so new attention mask for the decoder','line_number':4290,'multiline':False]['text':' take care of num_return_sequences','line_number':4301,'multiline':False]['text':' take most probable hidden states per batch of return_sequences','line_number':4302,'multiline':False]['text':' (batch_size*num_return_sequences, ...) -> (batch_size,...)','line_number':4303,'multiline':False]['text':' get decoder last hidden state - must do a pass through the text decoder','line_number':4312,'multiline':False]['text':' Compute new attention mask','line_number':4321,'multiline':False]['text':' Compute t2u decoder_input_ids','line_number':4326,'multiline':False]['text':' second generation','line_number':4334,'multiline':False]['text':' get rid of t2u_decoder_input_ids','line_number':4338,'multiline':False]['text':' replace eos per pad','line_number':4340,'multiline':False]['text':' offset of control symbols','line_number':4342,'multiline':False]['text':' cut decoder_input_ids if past is used','line_number':4373,'multiline':False]['text':' encoder_outputs is defined. input_ids not needed','line_number':4378,'multiline':False]['text':' cached cross_attention states don't have to be reordered -> they are always the same','line_number':4390,'multiline':False]