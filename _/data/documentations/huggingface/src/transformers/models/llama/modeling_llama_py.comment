['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]['text':' limitations under the License.','line_number':19,'multiline':False]['text':' noqa','line_number':56,'multiline':False]['text':' This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.','line_number':59,'multiline':False]['text':' It means that the function will not be traced through and simply appear as a node in the graph.','line_number':60,'multiline':False]['text':' Build here to make `torch.jit.trace` work.','line_number':133,'multiline':False]['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':143,'multiline':False]['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':149,'multiline':False]['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':172,'multiline':False]['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':198,'multiline':False]['text':' Specific to RoPE models','line_number':407,'multiline':False]['text':' upcast attention to fp32','line_number':428,'multiline':False]['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':466,'multiline':False]['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':467,'multiline':False]['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':468,'multiline':False]['text':' LlamaFlashAttention2 attention does not support output_attentions','line_number':481,'multiline':False]['text':' overwrite attention_mask with padding_mask','line_number':487,'multiline':False]['text':' Flash attention requires the input to have the shape','line_number':498,'multiline':False]['text':' batch_size x seq_length x head_dim x hidden_dim','line_number':499,'multiline':False]['text':' therefore we just need to keep the original shape','line_number':500,'multiline':False]['text':' Specific to RoPE models','line_number':512,'multiline':False]['text':' TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache','line_number':515,'multiline':False]['text':' to be able to avoid many of these transpose/reshape/view.','line_number':516,'multiline':False]['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':523,'multiline':False]['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':524,'multiline':False]['text':' cast them back in the correct dtype just to be sure everything works as expected.','line_number':525,'multiline':False]['text':' This might slowdown training & inference so it is recommended to not cast the LayerNorms','line_number':526,'multiline':False]['text':' in fp32. (LlamaRMSNorm handles it correctly)','line_number':527,'multiline':False]['text':' Handle the case where the model is quantized','line_number':531,'multiline':False]['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':584,'multiline':False]['text':' Contains at least one padding token in the sequence','line_number':587,'multiline':False]['text':' There is a memcpy here, that is very bad.','line_number':639,'multiline':False]['text':' The -q_len: slice assumes left padding.','line_number':643,'multiline':False]['text':' Adapted from LlamaAttention.forward','line_number':664,'multiline':False]['text':' TODO: Improve this warning with e.g. `model.config.attn_implementation = "manual"` once this is implemented.','line_number':675,'multiline':False]['text':' Specific to RoPE models','line_number':707,'multiline':False]['text':' SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,','line_number':719,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/112577.','line_number':720,'multiline':False]['text':' The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.','line_number':732,'multiline':False]['text':' Self Attention','line_number':795,'multiline':False]['text':' Fully Connected','line_number':807,'multiline':False]['text':' Initialize weights and apply final processing','line_number':963,'multiline':False]['text':' retrieve input_ids and inputs_embeds','line_number':993,'multiline':False]['text':' 2d mask is passed through the layers','line_number':1028,'multiline':False]['text':' output_attentions=True can not be supported when using SDPA, and we fall back on','line_number':1031,'multiline':False]['text':' the manual implementation that requires a 4D causal mask in all cases.','line_number':1032,'multiline':False]['text':' 4d mask is passed through the layers','line_number':1040,'multiline':False]['text':' embed positions','line_number':1045,'multiline':False]['text':' decoder layers','line_number':1048,'multiline':False]['text':' add hidden states from the last decoder layer','line_number':1087,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1113,'multiline':False]['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1180,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':1204,'multiline':False]['text':' Flatten the tokens','line_number':1207,'multiline':False]['text':' Enable model parallelism','line_number':1211,'multiline':False]['text':' Keep only the unprocessed tokens:','line_number':1239,'multiline':False]['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':1240,'multiline':False]['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':1241,'multiline':False]['text':' input)','line_number':1242,'multiline':False]['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':1245,'multiline':False]['text':' input_ids based on the past_length.','line_number':1246,'multiline':False]['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':1249,'multiline':False]['text':' If we are about to go beyond the maximum cache length, we need to crop the input attention mask.','line_number':1251,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':1261,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1267,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1315,'multiline':False]