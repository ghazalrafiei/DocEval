['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The Eleuther AI and HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' noqa','line_number':54,'multiline':False]['text':' This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.','line_number':57,'multiline':False]['text':' It means that the function will not be traced through and simply appear as a node in the graph.','line_number':58,'multiline':False]['text':' See all GPTNeo models at https://huggingface.co/models?filter=gpt_neo','line_number':72,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':78,'multiline':False]['text':' Load weights from TF model','line_number':105,'multiline':False]['text':' skip "gpt2/"','line_number':129,'multiline':False]['text':' if vocab is padded, then trim off the padding embeddings','line_number':154,'multiline':False]['text':' init the final linear layer using word embeddings','line_number':163,'multiline':False]['text':' local causal self attention is a sliding window where each token can only attend to the previous','line_number':181,'multiline':False]['text':' window_size tokens. This is implemented by updating the causal mask such that for each token','line_number':182,'multiline':False]['text':' all other tokens are masked except the previous window_size tokens.','line_number':183,'multiline':False]['text':' (batch, head, seq_length, head_features)','line_number':214,'multiline':False]['text':' Keep the attention weights computation in fp32 to avoid overflow issues','line_number':225,'multiline':False]['text':' Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.','line_number':234,'multiline':False]['text':' Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`','line_number':235,'multiline':False]['text':' Apply the attention mask','line_number':240,'multiline':False]['text':' Mask heads if we want to','line_number':247,'multiline':False]['text':' a, present, (attentions)','line_number':293,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':303,'multiline':False]['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':307,'multiline':False]['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':308,'multiline':False]['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':309,'multiline':False]['text':' Flash attention requires the input to have the shape','line_number':345,'multiline':False]['text':' batch_size x seq_length x head_dim x hidden_dim','line_number':346,'multiline':False]['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':353,'multiline':False]['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':354,'multiline':False]['text':' cast them back in the correct dtype just to be sure everything works as expected.','line_number':355,'multiline':False]['text':' This might slowdown training & inference so it is recommended to not cast the LayerNorms','line_number':356,'multiline':False]['text':' in fp32. (LlamaRMSNorm handles it correctly)','line_number':357,'multiline':False]['text':' Handle the case where the model is quantized','line_number':360,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':390,'multiline':False]['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':416,'multiline':False]['text':' Contains at least one padding token in the sequence','line_number':419,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input','line_number':450,'multiline':False]['text':' There is a memcpy here, that is very bad.','line_number':472,'multiline':False]['text':' The -q_len: slice assumes left padding.','line_number':476,'multiline':False]['text':' in MLP: intermediate_size= 4 * hidden_size','line_number':531,'multiline':False]['text':' output_attn: a, present, (attentions)','line_number':576,'multiline':False]['text':' residual connection','line_number':578,'multiline':False]['text':' residual connection','line_number':584,'multiline':False]['text':' hidden_states, present, (attentions, cross_attentions)','line_number':592,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':615,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':616,'multiline':False]['text':' Initialize weights and apply final processing','line_number':727,'multiline':False]['text':' Prepare head mask if needed','line_number':789,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':790,'multiline':False]['text':' attention_probs has shape bsz x num_heads x N x N','line_number':791,'multiline':False]['text':' head_mask has shape n_layer x batch x num_heads x N x N','line_number':792,'multiline':False]['text':' Attention mask.','line_number':800,'multiline':False]['text':' 2d mask is passed through the layers','line_number':802,'multiline':False]['text':' 4d mask is passed through the layers','line_number':805,'multiline':False]['text':' Add last hidden state','line_number':860,'multiline':False]['text':' Initialize weights and apply final processing','line_number':890,'multiline':False]['text':' Omit tokens covered by past_key_values','line_number':901,'multiline':False]['text':' Some generation methods already pass only the last input ID','line_number':905,'multiline':False]['text':' Default to old behavior: keep only final ID','line_number':909,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':920,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':926,'multiline':False]['text':' move labels to correct device to enable model parallelism','line_number':992,'multiline':False]['text':' Compute loss in fp32 to match with mesh-tf version','line_number':994,'multiline':False]['text':' https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179','line_number':995,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':998,'multiline':False]['text':' Flatten the tokens','line_number':1001,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1057,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1179,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1262,'multiline':False]['text':' If we are on multi-GPU, split add a dimension','line_number':1319,'multiline':False]['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1324,'multiline':False]