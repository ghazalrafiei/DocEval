['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The Suno AI Authors and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' noqa','line_number':57,'multiline':False]['text':' See all Bark models at https://huggingface.co/models?filter=bark','line_number':69,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':73,'multiline':False]['text':' adapted from GPTNeoSelfAttention and Bark code','line_number':87,'multiline':False]['text':' BarkSelfAttention can have two attention type, i.e full attention or causal attention','line_number':88,'multiline':False]['text':' regularization','line_number':93,'multiline':False]['text':' key, query, value projections for all heads, but in a batch','line_number':108,'multiline':False]['text':' output projection','line_number':110,'multiline':False]['text':' Copied from transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoSelfAttention._split_heads','line_number':119,'multiline':False]['text':' (batch, head, seq_length, head_features)','line_number':126,'multiline':False]['text':' re-assemble all head outputs side by side','line_number':133,'multiline':False]['text':' (batch, num_heads, seq_len, attn_head_size) -> (batch, seq_len, num_heads*attn_head_size)','line_number':134,'multiline':False]['text':' unlike GPTNeo's SelfAttention, divide by the square root of the dimension of the query and the key','line_number':141,'multiline':False]['text':' fill the upper left part of the attention weights with inf','line_number':147,'multiline':False]['text':' Apply the attention mask','line_number':154,'multiline':False]['text':' Mask heads if we want to','line_number':161,'multiline':False]['text':' (batch, num_heads, seq_len, seq_len) x (batch, num_heads, seq_len, attn_head_size)','line_number':165,'multiline':False]['text':' -> (batch, num_heads, seq_len, attn_head_size)','line_number':166,'multiline':False]['text':' calculate query, key, values for all heads in batch and move head forward to be the batch dim','line_number':180,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':218,'multiline':False]['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':222,'multiline':False]['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':223,'multiline':False]['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':224,'multiline':False]['text':' Flash attention requires the input to have the shape','line_number':233,'multiline':False]['text':' batch_size x seq_length x head_dim x hidden_dim - (batch, seq_length, head, head_features)','line_number':234,'multiline':False]['text':' re-assemble all head outputs side by side','line_number':241,'multiline':False]['text':' (batch, seq_len, num_heads, attn_head_size) -> (batch, seq_len, num_heads*attn_head_size)','line_number':242,'multiline':False]['text':' calculate query, key, values for all heads in batch and move head forward to be the batch dim','line_number':257,'multiline':False]['text':' (batch, head, seq_length, head_features) -> (batch, seq_length, head, head_features)','line_number':265,'multiline':False]['text':' and merge on seq_length','line_number':268,'multiline':False]['text':'  (batch, head, seq_length, head_features)','line_number':273,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':291,'multiline':False]['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':317,'multiline':False]['text':' Contains at least one padding token in the sequence','line_number':320,'multiline':False]['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input','line_number':351,'multiline':False]['text':' There is a memcpy here, that is very bad.','line_number':373,'multiline':False]['text':' The -q_len: slice assumes left padding.','line_number':377,'multiline':False]['text':' if causal, uses handmade LayerNorm, so that the layerNorm bias is optional','line_number':430,'multiline':False]['text':' this handmade layerNorm is used to stick with Bark choice of leaving optional bias in','line_number':431,'multiline':False]['text':' AutoRegressive models (corresponding to the "Text" and the "Coarse" modules)','line_number':432,'multiline':False]['text':' output_attn: output, present_key_values, (attn_weights)','line_number':463,'multiline':False]['text':' hidden_states, ((present), attentions)','line_number':476,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':492,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':493,'multiline':False]['text':' if has _hf_hook, has been offloaded so the device has to be found in the hook','line_number':515,'multiline':False]['text':' GPT2-like autoregressive model','line_number':657,'multiline':False]['text':' initialize as an autoregressive GPT-like model','line_number':665,'multiline':False]['text':' Initialize weights and apply final processing','line_number':679,'multiline':False]['text':' Omit tokens covered by past_key_values','line_number':695,'multiline':False]['text':' Some generation methods already pass only the last input ID','line_number':699,'multiline':False]['text':' Default to old behavior: keep only final ID','line_number':703,'multiline':False]['text':' input_embeds have already been used and is not required anymore','line_number':708,'multiline':False]['text':' ensure that attention_mask and position_ids shapes are aligned with the weird Bark hack of reducing','line_number':716,'multiline':False]['text':' sequence length on the first forward pass','line_number':717,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':724,'multiline':False]['text':' Verify if input_embeds already exists','line_number':771,'multiline':False]['text':' then compute embeddings.','line_number':772,'multiline':False]['text':' we want to return the input_embeds in priority so that it is in line with a weird hack','line_number':776,'multiline':False]['text':' of Bark which concatenate two bits of the input_embeds on the first forward pass of the semantic model','line_number':777,'multiline':False]['text':' token embeddings of shape (b, t, n_embd)','line_number':780,'multiline':False]['text':' shape (1, seq_length)','line_number':800,'multiline':False]['text':' position embeddings of shape (1, t, n_embd)','line_number':802,'multiline':False]['text':' Attention mask.','line_number':804,'multiline':False]['text':' [bsz, to_seq_length] -> [bsz, 1, 1, to_seq_length]','line_number':812,'multiline':False]['text':' from_seq_length is 1 to easily broadcast','line_number':813,'multiline':False]['text':' Prepare head mask if needed','line_number':816,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':817,'multiline':False]['text':' attention_probs has shape bsz x num_heads x N x N','line_number':818,'multiline':False]['text':' head_mask has shape num_layers x batch x num_heads x N x N','line_number':819,'multiline':False]['text':' Add last hidden state','line_number':872,'multiline':False]['text':' Necessary for beam_search','line_number':906,'multiline':False]['text':' pass input_ids in order to stay consistent with the transformers generate method even though it is not used','line_number':1006,'multiline':False]['text':' (except to get the input seq_len - that's why we keep the first 257 tokens)','line_number':1007,'multiline':False]['text':' size: 10048','line_number':1014,'multiline':False]['text':' take the generated semantic tokens','line_number':1016,'multiline':False]['text':' clone to avoid modifying history_prompt.coarse_prompt','line_number':1064,'multiline':False]['text':' offset x_coarse_history','line_number':1067,'multiline':False]['text':' offset','line_number':1070,'multiline':False]['text':' flatten x_coarse_history','line_number':1073,'multiline':False]['text':' e.g: after SEMANTIC_VOCAB_SIZE (10000), 1024 tokens dedicated to first codebook, 1024 next tokens','line_number':1079,'multiline':False]['text':' dedicated to second codebook.','line_number':1080,'multiline':False]['text':' trim histories correctly','line_number':1083,'multiline':False]['text':' bit of a hack for time alignment (sounds better) - from Bark original implementation','line_number':1096,'multiline':False]['text':' shape: (batch_size, 0)','line_number':1100,'multiline':False]['text':' replace semantic_pad_token (eos_tok and pad_tok here) with coarse_semantic_pad_token i.e the pad_token','line_number':1151,'multiline':False]['text':' used in the next model','line_number':1152,'multiline':False]['text':' pad from right side','line_number':1196,'multiline':False]['text':' non-causal gpt-like model with one embedding layer and one lm_head for each codebook of Encodec','line_number':1254,'multiline':False]['text':' initialize a modified non causal GPT-like model','line_number':1258,'multiline':False]['text':' note that for there is one embedding layer and one lm_head for each codebook of Encodec','line_number':1259,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1281,'multiline':False]['text':' one embedding layers for each codebook','line_number':1285,'multiline':False]['text':' one embedding layers for each codebook','line_number':1289,'multiline':False]['text':' one lm_head for each codebook','line_number':1293,'multiline':False]['text':' one lm_head for each codebook','line_number':1297,'multiline':False]['text':' if word embeddings are not tied, make sure that lm head is resized as well','line_number':1311,'multiline':False]['text':' Update base model and current model config','line_number':1349,'multiline':False]['text':' Tie weights again if needed','line_number':1355,'multiline':False]['text':' self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight','line_number':1373,'multiline':False]['text':' an additionnal idx corresponding to the id of the codebook that will be predicted','line_number':1384,'multiline':False]['text':' the input_embeddings are the sum of the j previous codebooks embeddings before','line_number':1411,'multiline':False]['text':' the current codebook_idx codebook','line_number':1412,'multiline':False]['text':' forward the GPT model itself','line_number':1414,'multiline':False]['text':' token embeddings of shape (b, t, n_embd)','line_number':1418,'multiline':False]['text':' shape (1, seq_length)','line_number':1430,'multiline':False]['text':' position embeddings of shape (1, t, n_embd)','line_number':1432,'multiline':False]['text':' Attention mask.','line_number':1434,'multiline':False]['text':' [bsz, to_seq_length] -> [bsz, 1, 1, to_seq_length]','line_number':1441,'multiline':False]['text':' from_seq_length is 1 to easily broadcast','line_number':1442,'multiline':False]['text':' Add last hidden state','line_number':1472,'multiline':False]['text':' since we don't really use GenerationConfig through the fine model (autoencoder)','line_number':1531,'multiline':False]['text':' and since only temperature is used from the classic GenerationConfig parameters','line_number':1532,'multiline':False]['text':' manually impose the kwargs priority over the generation config','line_number':1533,'multiline':False]['text':' shape: (batch, n_coarse_codebooks * seq_len)','line_number':1539,'multiline':False]['text':' new_shape: (batch, seq_len, n_coarse_codebooks)','line_number':1540,'multiline':False]['text':' brings ids into the range [0, codebook_size -1]','line_number':1543,'multiline':False]['text':' transpose to get to shape (seq_len, n_fine_codebooks)','line_number':1549,'multiline':False]['text':' pad the last 6th codebooks','line_number':1555,'multiline':False]['text':' prepend history if available (max max_fine_history_length)','line_number':1563,'multiline':False]['text':' len of the fine_history that has been added to fine_input','line_number':1567,'multiline':False]['text':' need to pad if too short (since non-causal model)','line_number':1573,'multiline':False]['text':' we can be lazy about fractional loop and just keep overwriting codebooks.','line_number':1578,'multiline':False]['text':' seems that coarse_output.shape[1] - (max_fine_input_length - n_history) is equal to minus n_remove_from_end','line_number':1579,'multiline':False]['text':' So if we needed to pad because too short, n_loops is always 1 (because n_remove_from_end > 0)','line_number':1580,'multiline':False]['text':' If not, we loop over at least twice.','line_number':1581,'multiline':False]['text':' apply softmax','line_number':1602,'multiline':False]['text':' reshape to 2D: (batch_size, seq_len, codebook_size) -> (batch_size*seq_len, codebook_size)','line_number':1604,'multiline':False]['text':' multinomial then reshape : (batch_size*seq_len)-> (batch_size,seq_len)','line_number':1606,'multiline':False]['text':' transfer into fine_input','line_number':1612,'multiline':False]['text':' for bark_model, device must be verified on its sub-models','line_number':1668,'multiline':False]['text':' if has _hf_hook, has been offloaded so the device has to be found in the hook','line_number':1669,'multiline':False]['text':' otherwise we don't see the memory savings (but they probably exist)','line_number':1699,'multiline':False]['text':' this layer is used outside the first foward pass of semantic so need to be loaded before semantic','line_number':1701,'multiline':False]['text':' We'll offload the last model manually.','line_number':1716,'multiline':False]['text':' encodec uses LSTMs which behaves differently with appended padding','line_number':1726,'multiline':False]['text':' decoding with encodec takes around 0.1% of the total generation time','line_number':1727,'multiline':False]['text':' to keep generation quality, we break batching','line_number':1728,'multiline':False]['text':' squeeze the codebook dimension','line_number':1733,'multiline':False]['text':' TODO (joao):workaround until nested generation config is compatible with PreTrained Model','line_number':1787,'multiline':False]['text':' todo: dict','line_number':1788,'multiline':False]['text':' if "attention_mask" is set, it should not be passed to CoarseModel and FineModel','line_number':1794,'multiline':False]['text':' If the key is already in a specific config, then it's been set with a','line_number':1811,'multiline':False]['text':' submodules specific value and we don't override','line_number':1812,'multiline':False]['text':' 1. Generate from the semantic model','line_number':1820,'multiline':False]['text':' 2. Generate from the coarse model','line_number':1828,'multiline':False]['text':' (batch_size, seq_len*coarse_codebooks) -> (batch_size, seq_len)','line_number':1842,'multiline':False]['text':' 3. "generate" from the fine model','line_number':1845,'multiline':False]['text':' Manually offload fine_acoustics to CPU','line_number':1857,'multiline':False]['text':' and load codec_model to GPU','line_number':1858,'multiline':False]['text':' since bark doesn't use codec_model forward pass','line_number':1859,'multiline':False]['text':' 4. Decode the output and generate audio array','line_number':1863,'multiline':False]['text':' Offload codec_model to CPU','line_number':1867,'multiline':False]