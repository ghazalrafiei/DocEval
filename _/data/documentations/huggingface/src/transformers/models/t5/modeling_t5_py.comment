['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':'###################################################','line_number':57,'multiline':False]['text':' This dict contains ids and associated url','line_number':58,'multiline':False]['text':' for the pretrained weights provided with the models','line_number':59,'multiline':False]['text':'###################################################','line_number':60,'multiline':False]['text':' See all T5 models at https://huggingface.co/models?filter=t5','line_number':67,'multiline':False]['text':'###################################################','line_number':71,'multiline':False]['text':' This is a conversion method from TF 1.0 to PyTorch','line_number':72,'multiline':False]['text':' More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28','line_number':73,'multiline':False]['text':'###################################################','line_number':74,'multiline':False]['text':' Load weights from TF model','line_number':90,'multiline':False]['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v','line_number':102,'multiline':False]['text':' which are not required for using pretrained model','line_number':103,'multiline':False]['text':'###################################################','line_number':180,'multiline':False]['text':' PyTorch Models are constructed by sub-classing','line_number':181,'multiline':False]['text':' - torch.nn.Module for the layers and','line_number':182,'multiline':False]['text':' - PreTrainedModel for the models (it-self a sub-class of nn.Module)','line_number':183,'multiline':False]['text':'###################################################','line_number':184,'multiline':False]['text':' T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':248,'multiline':False]['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':249,'multiline':False]['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':250,'multiline':False]['text':' half-precision inputs is done in fp32','line_number':251,'multiline':False]['text':' convert into half-precision if necessary','line_number':256,'multiline':False]['text':' noqa','line_number':266,'multiline':False]['text':' using the normal T5LayerNorm','line_number':270,'multiline':False]['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':316,'multiline':False]['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':317,'multiline':False]['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':318,'multiline':False]['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':361,'multiline':False]['text':' Prune linear layers','line_number':378,'multiline':False]['text':' Update hyper params','line_number':383,'multiline':False]['text':' now relative_position is in the range [0, inf)','line_number':417,'multiline':False]['text':' half of the buckets are for exact increments in positions','line_number':419,'multiline':False]['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':423,'multiline':False]['text':' shape (query_length, key_length)','line_number':442,'multiline':False]['text':' shape (query_length, key_length)','line_number':444,'multiline':False]['text':' shape (query_length, key_length, num_heads)','line_number':449,'multiline':False]['text':' shape (1, num_heads, query_length, key_length)','line_number':450,'multiline':False]['text':' Input is (batch_size, seq_length, dim)','line_number':468,'multiline':False]['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':469,'multiline':False]['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':470,'multiline':False]['text':' self-attn','line_number':495,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':496,'multiline':False]['text':' cross-attn','line_number':499,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':500,'multiline':False]['text':' self-attn','line_number':505,'multiline':False]['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':506,'multiline':False]['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':509,'multiline':False]['text':' the provided `key_value_states` to support prefix tuning','line_number':510,'multiline':False]['text':' cross-attn','line_number':511,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':512,'multiline':False]['text':' cross-attn','line_number':515,'multiline':False]['text':' get query states','line_number':519,'multiline':False]['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':520,'multiline':False]['text':' get key/value states','line_number':522,'multiline':False]['text':' compute scores','line_number':530,'multiline':False]['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':533,'multiline':False]['text':' if key and values are already calculated','line_number':545,'multiline':False]['text':' we want only the last query position bias','line_number':546,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':551,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':563,'multiline':False]['text':' (batch_size, n_heads, seq_length, key_length)','line_number':566,'multiline':False]['text':' Mask heads if we want to','line_number':568,'multiline':False]['text':' (batch_size, seq_length, dim)','line_number':572,'multiline':False]['text':' add attentions if we output them','line_number':611,'multiline':False]['text':' add attentions if we output them','line_number':647,'multiline':False]['text':' Keep self-attention outputs and relative position weights','line_number':704,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':706,'multiline':False]['text':' the actual query length is unknown for cross attention','line_number':717,'multiline':False]['text':' if using past key value states. Need to inject it here','line_number':718,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':737,'multiline':False]['text':' Combine self attn and cross attn key value states','line_number':746,'multiline':False]['text':' Keep cross-attention outputs and relative position weights','line_number':750,'multiline':False]['text':' Apply Feed Forward layer','line_number':753,'multiline':False]['text':' clamp inf values to enable fp16 training','line_number':756,'multiline':False]['text':' hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':772,'multiline':False]['text':' Used for testing weights initialization','line_number':820,'multiline':False]['text':' Mesh TensorFlow embeddings initialization','line_number':827,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':828,'multiline':False]['text':' Mesh TensorFlow FF initialization','line_number':843,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':844,'multiline':False]['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':845,'multiline':False]['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':863,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':864,'multiline':False]['text':' shift inputs to the right','line_number':885,'multiline':False]['text':' Item assignment is not supported natively for proxies.','line_number':887,'multiline':False]['text':' replace possible -100 values in labels by `pad_token_id`','line_number':897,'multiline':False]['text':' Initialize weights and apply final processing','line_number':916,'multiline':False]['text':' Model parallel','line_number':918,'multiline':False]['text':' Check validity of device_map','line_number':932,'multiline':False]['text':' Load onto devices','line_number':940,'multiline':False]['text':' Set embed_tokens to first layer','line_number':946,'multiline':False]['text':' Set final layer norm to last device','line_number':948,'multiline':False]['text':' Model parallel','line_number':988,'multiline':False]['text':' required mask seq length can be calculated via length of past','line_number':1020,'multiline':False]['text':' initialize past_key_values with `None` if past does not exist','line_number':1027,'multiline':False]['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1034,'multiline':False]['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1035,'multiline':False]['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1038,'multiline':False]['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1039,'multiline':False]['text':' Prepare head mask if needed','line_number':1058,'multiline':False]['text':' Model parallel','line_number':1073,'multiline':False]['text':' Ensure that attention_mask is always on the same device as hidden_states','line_number':1076,'multiline':False]['text':' past_key_value is always None with gradient checkpointing','line_number':1105,'multiline':False]['text':' layer_outputs is a tuple with:','line_number':1124,'multiline':False]['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1125,'multiline':False]['text':' We share the position biases between the layers - the first layer store them','line_number':1131,'multiline':False]['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1132,'multiline':False]['text':' (cross-attention position bias), (cross-attention weights)','line_number':1133,'multiline':False]['text':' append next layer key value states','line_number':1137,'multiline':False]['text':' Model Parallel: If it's the last layer for that device, put things on the next device','line_number':1146,'multiline':False]['text':' Add last layer','line_number':1155,'multiline':False]['text':' Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1331,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1366,'multiline':False]['text':' Model parallel','line_number':1369,'multiline':False]['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1480,'multiline':False]['text':' Encode if needed (training, first prediction pass)','line_number':1486,'multiline':False]['text':' Set device for model parallelism','line_number':1506,'multiline':False]['text':' Decode','line_number':1517,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1575,'multiline':False]['text':' Model parallel','line_number':1578,'multiline':False]['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1697,'multiline':False]['text':' Encode if needed (training, first prediction pass)','line_number':1703,'multiline':False]['text':' Convert encoder inputs in embeddings if needed','line_number':1705,'multiline':False]['text':' get decoder inputs from shifting lm labels to the right','line_number':1728,'multiline':False]['text':' Set device for model parallelism','line_number':1731,'multiline':False]['text':' Decode','line_number':1742,'multiline':False]['text':' Set device for model parallelism','line_number':1760,'multiline':False]['text':' Rescale output before projecting on vocab','line_number':1767,'multiline':False]['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1768,'multiline':False]['text':' move labels to correct device to enable PP','line_number':1776,'multiline':False]['text':' TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666','line_number':1779,'multiline':False]['text':' cut decoder_input_ids if past_key_values is used','line_number':1810,'multiline':False]['text':' Some generation methods already pass only the last input ID','line_number':1814,'multiline':False]['text':' Default to old behavior: keep only final ID','line_number':1818,'multiline':False]['text':' if decoder past is not included in output','line_number':1839,'multiline':False]['text':' speedy decoding is disabled and no need to reorder','line_number':1840,'multiline':False]['text':' get the correct batch idx from layer past batch dim','line_number':1847,'multiline':False]['text':' batch dim of `past` is at 2nd position','line_number':1848,'multiline':False]['text':' need to set correct `past` for each of the four key / value states','line_number':1851,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1886,'multiline':False]['text':' Model parallel','line_number':1889,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2004,'multiline':False]['text':' Copied from models.bart.modeling_bart.BartModel.forward different to other models, T5 automatically creates','line_number':2044,'multiline':False]['text':' decoder_input_ids from input_ids if no decoder_input_ids are provided','line_number':2045,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2153,'multiline':False]['text':' Copied from models.bart.modeling_bart.BartModel.forward','line_number':2214,'multiline':False]['text':'   different to other models, T5 automatically creates decoder_input_ids from','line_number':2215,'multiline':False]['text':'   input_ids if no decoder_input_ids are provided','line_number':2216,'multiline':False]['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':2229,'multiline':False]['text':' Encode if needed (training, first prediction pass)','line_number':2235,'multiline':False]['text':' Decode','line_number':2255,'multiline':False]['text':' If we are on multi-GPU, split add a dimension','line_number':2280,'multiline':False]['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':2285,'multiline':False]