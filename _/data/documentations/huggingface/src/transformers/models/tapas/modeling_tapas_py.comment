['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 Google Research and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' large models','line_number':60,'multiline':False]['text':' base models','line_number':66,'multiline':False]['text':' small models','line_number':72,'multiline':False]['text':' mini models','line_number':78,'multiline':False]['text':' tiny models','line_number':84,'multiline':False]['text':' See all TAPAS models at https://huggingface.co/models?filter=tapas','line_number':90,'multiline':False]['text':' Load weights from TF model','line_number':147,'multiline':False]['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculate m and v','line_number':159,'multiline':False]['text':' which are not required for using pretrained model','line_number':160,'multiline':False]['text':' in case the model is TapasForSequenceClassification, we skip output_bias and output_weights','line_number':175,'multiline':False]['text':' since these are not used for classification','line_number':176,'multiline':False]['text':' in case the model is TapasModel, we skip output_bias, output_weights, output_bias_cls and output_weights_cls','line_number':181,'multiline':False]['text':' since this model does not have MLM and NSP heads','line_number':182,'multiline':False]['text':' in case the model is TapasForMaskedLM, we skip the pooler','line_number':187,'multiline':False]['text':' if first scope name starts with "bert", change it to "tapas"','line_number':192,'multiline':False]['text':' cell selection heads','line_number':205,'multiline':False]['text':' aggregation head','line_number':217,'multiline':False]['text':' classification head','line_number':224,'multiline':False]['text':' Added a check to see whether the array is a scalar (because bias terms in Tapas checkpoints can be','line_number':253,'multiline':False]['text':' scalar => should first be converted to numpy arrays)','line_number':254,'multiline':False]['text':' we do not include config.disabled_features and config.disable_position_embeddings from the original implementation','line_number':269,'multiline':False]['text':' word embeddings','line_number':270,'multiline':False]['text':' position embeddings','line_number':272,'multiline':False]['text':' token type embeddings','line_number':274,'multiline':False]['text':' self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load','line_number':281,'multiline':False]['text':' any TensorFlow checkpoint file','line_number':282,'multiline':False]['text':' create absolute position embeddings','line_number':298,'multiline':False]['text':' when self.config.reset_position_index_per_cell is set to True, create relative position embeddings','line_number':301,'multiline':False]['text':' shape (batch_size, seq_len)','line_number':303,'multiline':False]['text':' shape (batch_size, seq_len)','line_number':305,'multiline':False]['text':' shape (batch_size, seq_len)','line_number':307,'multiline':False]['text':' shape (max_rows * max_columns,). First absolute position for every cell','line_number':309,'multiline':False]['text':' ? shape (batch_size, seq_len). First absolute position of the cell for every token','line_number':311,'multiline':False]['text':' shape (1, seq_len)','line_number':313,'multiline':False]['text':' If this is instantiated as a cross-attention module, the keys','line_number':377,'multiline':False]['text':' and values come from an encoder; the attention mask needs to be','line_number':378,'multiline':False]['text':' such that the encoder's padding tokens are not attended to.','line_number':379,'multiline':False]['text':' reuse k,v, cross_attentions','line_number':383,'multiline':False]['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':405,'multiline':False]['text':' Apply the attention mask is (precomputed for all layers in TapasModel forward() function)','line_number':409,'multiline':False]['text':' Normalize the attention scores to probabilities.','line_number':412,'multiline':False]['text':' This is actually dropping out entire tokens to attend to, which might','line_number':415,'multiline':False]['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':416,'multiline':False]['text':' Mask heads if we want to','line_number':419,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertSelfOutput','line_number':435,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads','line_number':457,'multiline':False]['text':' Prune linear layers','line_number':465,'multiline':False]['text':' Update hyper params and store pruned heads','line_number':471,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertAttention.forward','line_number':476,'multiline':False]['text':' add attentions if we output them','line_number':497,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertIntermediate','line_number':501,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertOutput','line_number':517,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertLayer.forward','line_number':547,'multiline':False]['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':558,'multiline':False]['text':' if decoder, the last output is tuple of self-attn cache','line_number':569,'multiline':False]['text':' add self attentions if we output attention weights','line_number':574,'multiline':False]['text':' cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple','line_number':584,'multiline':False]['text':' add cross attentions if we output attention weights','line_number':596,'multiline':False]['text':' add cross-attn cache to positions 3,4 of present_key_value tuple','line_number':598,'multiline':False]['text':' if decoder, return the attn key/values as the last output','line_number':607,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk','line_number':613,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertPooler','line_number':683,'multiline':False]['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':691,'multiline':False]['text':' to the first token.','line_number':692,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->Tapas','line_number':699,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Tapas','line_number':717,'multiline':False]['text':' The output weights are the same as the input embeddings, but there is','line_number':723,'multiline':False]['text':' an output-only bias for each token.','line_number':724,'multiline':False]['text':' Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`','line_number':729,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Tapas','line_number':738,'multiline':False]['text':' Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights','line_number':759,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':763,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':764,'multiline':False]['text':' Initialize weights and apply final processing','line_number':859,'multiline':False]['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':942,'multiline':False]['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':943,'multiline':False]['text':' If a 2D ou 3D attention mask is provided for the cross-attention','line_number':946,'multiline':False]['text':' we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]','line_number':947,'multiline':False]['text':' Prepare head mask if needed','line_number':957,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':958,'multiline':False]['text':' attention_probs has shape bsz x n_heads x N x N','line_number':959,'multiline':False]['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':960,'multiline':False]['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':961,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1003,'multiline':False]['text':' -100 index = padding token','line_number':1085,'multiline':False]['text':' base model','line_number':1112,'multiline':False]['text':' dropout (only used when training)','line_number':1115,'multiline':False]['text':' cell selection heads','line_number':1118,'multiline':False]['text':' init_cell_selection_weights_to_zero: Whether the initial weights should be','line_number':1120,'multiline':False]['text':' set to 0. This ensures that all tokens have the same prior probability.','line_number':1121,'multiline':False]['text':' here, a truncated normal is used in the original implementation','line_number':1128,'multiline':False]['text':' here, a truncated normal is used in the original implementation','line_number':1132,'multiline':False]['text':' aggregation head','line_number':1136,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1140,'multiline':False]['text':' Construct indices for the table.','line_number':1240,'multiline':False]['text':' Masks.','line_number':1271,'multiline':False]['text':' Table cells only, without question tokens and table headers.','line_number':1276,'multiline':False]['text':' torch.FloatTensor[batch_size, seq_length]','line_number':1279,'multiline':False]['text':' Mask for cells that exist in the table (i.e. that are not padding).','line_number':1282,'multiline':False]['text':' Compute logits per token. These are used to select individual cells.','line_number':1285,'multiline':False]['text':' Compute logits per column. These are used to select a column.','line_number':1288,'multiline':False]['text':' Aggregation logits','line_number':1300,'multiline':False]['text':' Total loss calculation','line_number':1305,'multiline':False]['text':' Semi-supervised cell selection in case of no aggregation:','line_number':1312,'multiline':False]['text':' If the answer (the denotation) appears directly in the table we might','line_number':1313,'multiline':False]['text':' select the answer without applying any aggregation function. There are','line_number':1314,'multiline':False]['text':' some ambiguous cases, see utils._calculate_aggregate_mask for more info.','line_number':1315,'multiline':False]['text':' `aggregate_mask` is 1 for examples where we chose to aggregate and 0','line_number':1316,'multiline':False]['text':'  for examples where we chose to select the answer directly.','line_number':1317,'multiline':False]['text':' `labels` encodes the positions of the answer appearing in the table.','line_number':1318,'multiline':False]['text':' <float32>[batch_size]','line_number':1326,'multiline':False]['text':' Cell selection log-likelihood','line_number':1337,'multiline':False]['text':' Compute cell selection loss per example.','line_number':1343,'multiline':False]['text':' Supervised cell selection','line_number':1361,'multiline':False]['text':' For the not supervised case, do not assign loss for cell selection','line_number':1367,'multiline':False]['text':' Semi-supervised regression loss and supervised loss for aggregations','line_number':1370,'multiline':False]['text':' Note that `aggregate_mask` is None if the setting is supervised.','line_number':1373,'multiline':False]['text':' Set aggregation labels to zeros','line_number':1391,'multiline':False]['text':' Add regression loss for numeric answers which require aggregation.','line_number':1405,'multiline':False]['text':' Zero loss for examples with answer_loss > cutoff.','line_number':1417,'multiline':False]['text':' if no label ids are provided, set them to zeros in order to properly compute logits','line_number':1428,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1462,'multiline':False]['text':' Beginning of everything related to segmented tensors','line_number':1579,'multiline':False]['text':' returns a torch.Size object','line_number':1605,'multiline':False]['text':' first, check whether the indices of the index represent scalar values (i.e. not vectorized)','line_number':1670,'multiline':False]['text':' torch.gather expects index to have the same number of dimensions as values','line_number':1677,'multiline':False]['text':' this means we have a vectorized version','line_number':1680,'multiline':False]['text':' we have to adjust the index','line_number':1681,'multiline':False]['text':' first, get batch_size as scalar tensor','line_number':1702,'multiline':False]['text':' next, create offset as 1-D tensor of length batch_size,','line_number':1704,'multiline':False]['text':' and multiply element-wise by num segments (to offset different elements in the batch) e.g. if batch size is 2: [0, 64]','line_number':1705,'multiline':False]['text':' typically range(1,2)','line_number':1708,'multiline':False]['text':' create a rank 1 tensor vector containing batch_shape (e.g. [2])','line_number':1732,'multiline':False]['text':' create a rank 0 tensor (scalar) containing num_segments (e.g. 64)','line_number':1734,'multiline':False]['text':' create a rank 1 vector with num_segments elements','line_number':1739,'multiline':False]['text':' new_tensor is just a vector of [1 64] for example (assuming only 1 batch dimension)','line_number':1744,'multiline':False]['text':' equivalent (in Numpy:)','line_number':1750,'multiline':False]['text':' indices = torch.as_tensor(np.tile(indices.numpy(), multiples.tolist()))','line_number':1751,'multiline':False]['text':' Flatten the batch dimensions, as segments ops (scatter) do not support batching.','line_number':1773,'multiline':False]['text':' However if `values` has extra dimensions to the right keep them','line_number':1774,'multiline':False]['text':' unflattened. Segmented ops support vector-valued operations.','line_number':1775,'multiline':False]['text':' torch.Size object','line_number':1777,'multiline':False]['text':' changed "view" by "reshape" in the following line','line_number':1781,'multiline':False]['text':' Unflatten the values.','line_number':1789,'multiline':False]['text':' End of everything related to segmented tensors','line_number':1914,'multiline':False]['text':' First, compute the token logits (batch_size, seq_len) - without temperature','line_number':1942,'multiline':False]['text':' Next, average the logits per cell (batch_size, max_num_cols*max_num_rows)','line_number':1945,'multiline':False]['text':' Finally, average the logits per column (batch_size, max_num_cols)','line_number':1948,'multiline':False]['text':' Mask columns that do not appear in the example.','line_number':1955,'multiline':False]['text':' Part 1: column loss','line_number':1995,'multiline':False]['text':' First find the column we should select. We use the column with maximum number of selected cells.','line_number':1997,'multiline':False]['text':' shape of labels_per_column is (batch_size, max_num_cols). It contains the number of label ids for every column, for every example','line_number':1999,'multiline':False]['text':' shape (batch_size,)','line_number':2000,'multiline':False]['text':' Check if there are no selected cells in the column. In that case the model','line_number':2001,'multiline':False]['text':' should predict the special column id 0, which means "select nothing".','line_number':2002,'multiline':False]['text':' no_cell_selected is of shape (batch_size,) and equals True','line_number':2005,'multiline':False]['text':' if an example of the batch has no cells selected (i.e. if there are no labels set to 1 for that example)','line_number':2006,'multiline':False]['text':' shape (batch_size, max_num_cols)','line_number':2011,'multiline':False]['text':' Part 2: cell loss','line_number':2014,'multiline':False]['text':' Reduce the labels and logits to per-cell from per-token.','line_number':2016,'multiline':False]['text':' logits_per_cell: shape (batch_size, max_num_rows*max_num_cols) i.e. (batch_size, 64*32)','line_number':2017,'multiline':False]['text':' labels_per_cell: shape (batch_size, 64*32), indicating whether each cell should be selected (1) or not (0)','line_number':2019,'multiline':False]['text':' Mask for the selected column.','line_number':2024,'multiline':False]['text':' column_id_for_cells: shape (batch_size, 64*32), indicating to which column each cell belongs','line_number':2025,'multiline':False]['text':' column_mask: shape (batch_size, 64*32), equal to 1 if cell belongs to column to be selected','line_number':2027,'multiline':False]['text':' Compute the log-likelihood for cells, but only for the selected column.','line_number':2034,'multiline':False]['text':' shape (batch_size, 64*32)','line_number':2035,'multiline':False]['text':' shape(batch_size, 64*32)','line_number':2036,'multiline':False]['text':' We need to normalize the loss by the number of cells in the column.','line_number':2040,'multiline':False]['text':' Set the probs outside the selected column (selected by the *model*)','line_number':2050,'multiline':False]['text':' to 0. This ensures backwards compatibility with models that select','line_number':2051,'multiline':False]['text':' cells from multiple columns.','line_number':2052,'multiline':False]['text':' shape (batch_size,)','line_number':2055,'multiline':False]['text':' selected_column_mask: shape (batch_size, 64*32), equal to 1 if cell belongs to column selected by the model','line_number':2057,'multiline':False]['text':' Never select cells with the special column id 0.','line_number':2064,'multiline':False]['text':' torch.FloatTensor(batch_size,)','line_number':2123,'multiline':False]['text':' Index 0 corresponds to "no aggregation".','line_number':2127,'multiline':False]['text':' Cell selection examples according to current model.','line_number':2130,'multiline':False]['text':' Examples with non-empty cell selection supervision.','line_number':2133,'multiline':False]['text':' torch.where is not equivalent to tf.where (in tensorflow 1)','line_number':2136,'multiline':False]['text':' hence the added .view on the condition to match the shape of the first tensor','line_number':2137,'multiline':False]['text':' Prepare "no aggregation" targets for cell selection examples.','line_number':2176,'multiline':False]['text':' Use aggregation supervision as the target.','line_number':2179,'multiline':False]['text':' torch.FloatTensor[batch_size]','line_number':2185,'multiline':False]['text':' Accumulate loss only for examples requiring cell selection','line_number':2188,'multiline':False]['text':' (no aggregation).','line_number':2189,'multiline':False]['text':' Index 0 corresponds to "no aggregation".','line_number':2210,'multiline':False]['text':' Predict some aggregation in case of an answer that needs aggregation.','line_number':2212,'multiline':False]['text':' This increases the probability of all aggregation functions, in a way','line_number':2213,'multiline':False]['text':' similar to MML, but without considering whether the function gives the','line_number':2214,'multiline':False]['text':' correct answer.','line_number':2215,'multiline':False]['text':' Add aggregation loss for numeric answers that need aggregation.','line_number':2252,'multiline':False]['text':' The token logits where already divided by the temperature and used for','line_number':2282,'multiline':False]['text':' computing cell selection errors so we need to multiply it again here','line_number':2283,'multiline':False]['text':' <float32>[batch_size, seq_length]','line_number':2291,'multiline':False]['text':' Mask non-numeric table values to zero.','line_number':2296,'multiline':False]['text':' The sum of all probabilities except that correspond to other cells','line_number':2302,'multiline':False]['text':' Ex here stands for expectation, more explicitly the expectation of the sum of N-1 Bernoulli random variables plus','line_number':2303,'multiline':False]['text':' the constant 1, which is computed as adding all N expected values and subtracting the extra one. It corresponds to X_c','line_number':2304,'multiline':False]['text':' in Appendix D of the original TAPAS paper which is trying to approximate the average of a random set.','line_number':2305,'multiline':False]['text':' The sum of all probabilities except that correspond to other cells','line_number':2309,'multiline':False]['text':' <float32>[batch_size, num_aggregation_labels - 1]','line_number':2323,'multiline':False]['text':' <float32>[batch_size, num_aggregation_labels - 1]','line_number':2326,'multiline':False]['text':' PyTorch does not currently support Huber loss with custom delta so we define it ourself','line_number':2344,'multiline':False]['text':' shape (batch_size,)','line_number':2346,'multiline':False]['text':' float32 (batch_size,)','line_number':2386,'multiline':False]['text':' float32 (batch_size,)','line_number':2391,'multiline':False]