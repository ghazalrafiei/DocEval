['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The Kakao Enterprise Authors and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' General docstring','line_number':41,'multiline':False]['text':' See all VITS models at https://huggingface.co/models?filter=vits','line_number':47,'multiline':False]['text':' and all MMS models at https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts','line_number':48,'multiline':False]['text':' find the roots of a quadratic equation','line_number':311,'multiline':False]['text':' last one is not necessary','line_number':368,'multiline':False]['text':' Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock','line_number':434,'multiline':False]['text':' remove a useless vflow','line_number':816,'multiline':False]['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':908,'multiline':False]['text':' for the decoder','line_number':909,'multiline':False]['text':' get query proj','line_number':913,'multiline':False]['text':' self_attention','line_number':916,'multiline':False]['text':' this operation is a bit awkward, but it's required to','line_number':960,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':961,'multiline':False]['text':' In order to do so, attn_weights have to be reshaped','line_number':962,'multiline':False]['text':' twice and have to be reused in the following','line_number':963,'multiline':False]['text':' Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be','line_number':988,'multiline':False]['text':' partitioned aross GPUs when using tensor-parallelism.','line_number':989,'multiline':False]['text':' Concat columns of pad to shift from relative to absolute indexing.','line_number':1008,'multiline':False]['text':' Concat extra elements so to add up to shape (len+1, 2*len-1).','line_number':1011,'multiline':False]['text':' Reshape and slice out the padded elements.','line_number':1015,'multiline':False]['text':' Pad along column','line_number':1023,'multiline':False]['text':' Add 0's in the beginning that will skew the elements after reshape','line_number':1027,'multiline':False]['text':' expand attention_mask','line_number':1134,'multiline':False]['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':1136,'multiline':False]['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':1147,'multiline':False]['text':' under deepspeed zero3 all gpus must run in sync','line_number':1152,'multiline':False]['text':' This is used only for training.','line_number':1346,'multiline':False]['text':' These parameters control the synthesised speech properties','line_number':1349,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1354,'multiline':False]['text':' Create a padding mask for the output lengths of shape (batch, 1, max_output_length)','line_number':1450,'multiline':False]['text':' Reconstruct an attention tensor of shape (batch, 1, out_length, in_length)','line_number':1455,'multiline':False]['text':' Expand prior distribution','line_number':1465,'multiline':False]