['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 Microsoft Research and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all X-CLIP models at https://huggingface.co/models?filter=x-clip','line_number':46,'multiline':False]['text':' contrastive loss function, adapted from','line_number':50,'multiline':False]['text':' https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html','line_number':51,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->x_clip','line_number':56,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->XCLIP','line_number':106,'multiline':False]['text':' shape = [*, width, grid, grid]','line_number':133,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->XCLIP','line_number':142,'multiline':False]['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':151,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPAttention with CLIP->XCLIP','line_number':176,'multiline':False]['text':' get query proj','line_number':213,'multiline':False]['text':' apply the causal_attention_mask first','line_number':232,'multiline':False]['text':' this operation is a bit akward, but it's required to','line_number':253,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':254,'multiline':False]['text':' In order to do so, attn_weights have to reshaped','line_number':255,'multiline':False]['text':' twice and have to be reused in the following','line_number':256,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->XCLIP','line_number':281,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->XCLIP','line_number':297,'multiline':False]['text':' Copied from transformers.models.beit.modeling_beit.drop_path','line_number':348,'multiline':False]['text':' work with diff dim tensors, not just 2D ConvNets','line_number':362,'multiline':False]['text':' binarize','line_number':364,'multiline':False]['text':' Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->XCLIP','line_number':369,'multiline':False]['text':' add dummy sequence dimension','line_number':433,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoder with CLIP->XCLIP','line_number':618,'multiline':False]['text':' X_CLIP's text model uses causal mask, prepare it here.','line_number':754,'multiline':False]['text':' https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324','line_number':755,'multiline':False]['text':' expand attention_mask','line_number':759,'multiline':False]['text':' [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]','line_number':761,'multiline':False]['text':' text_embeds.shape = [batch_size, sequence_length, transformer.width]','line_number':776,'multiline':False]['text':' take features from the eot embedding (eot_token is the highest number in each sequence)','line_number':777,'multiline':False]['text':' Initialize weights and apply final processing','line_number':797,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1007,'multiline':False]['text':' add position embeddings','line_number':1126,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1288,'multiline':False]['text':' Use X_CLIP model's config for some fields (if specified) instead of those of vision & text components.','line_number':1317,'multiline':False]['text':' Use X_CLIP model's config for some fields (if specified) instead of those of vision & text components.','line_number':1421,'multiline':False]['text':' Use X_CLIP model's config for some fields (if specified) instead of those of vision & text components.','line_number':1550,'multiline':False]['text':' normalized features','line_number':1601,'multiline':False]['text':' cosine similarity as logits','line_number':1605,'multiline':False]