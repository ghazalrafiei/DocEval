['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all VipLlava models at https://huggingface.co/models?filter=vipllava','line_number':43,'multiline':False]['text':' Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->VipLlava','line_number':48,'multiline':False]['text':' Copied from transformers.models.llava.modeling_llava.LlavaPreTrainedModel with Llava->VipLlava,llava->vipllava','line_number':134,'multiline':False]['text':' important: this ported version of VipLlava isn't meant for training from scratch - only','line_number':143,'multiline':False]['text':' inference and fine-tuning - so the proper init weights code has been removed - the original codebase','line_number':144,'multiline':False]['text':' https://github.com/haotian-liu/LLaVA/tree/main/vipllava should serve for that purpose','line_number':145,'multiline':False]['text':' Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration with LLAVA->VIPLLAVA,Llava->VipLlava','line_number':235,'multiline':False]['text':' update vocab size','line_number':272,'multiline':False]['text':' 1. Create a mask to know where special image tokens are','line_number':284,'multiline':False]['text':' Compute the maximum embed dimension','line_number':287,'multiline':False]['text':' 2. Compute the positions where text should be written','line_number':291,'multiline':False]['text':' Calculate new positions for text tokens in merged image-text sequence.','line_number':292,'multiline':False]['text':' `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.','line_number':293,'multiline':False]['text':' `torch.cumsum` computes how each image token shifts subsequent text token positions.','line_number':294,'multiline':False]['text':' - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.','line_number':295,'multiline':False]['text':' offset for left padding','line_number':299,'multiline':False]['text':' 3. Create the full embedding, already padded to the maximum position','line_number':302,'multiline':False]['text':' In case the Vision model or the Language model has been offloaded to CPU, we need to manually','line_number':309,'multiline':False]['text':' set the corresponding tensors into their correct target device.','line_number':310,'multiline':False]['text':' 4. Fill the embeddings based on the mask. If we have ["hey" "<image>", "how", "are"]','line_number':319,'multiline':False]['text':' we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features','line_number':320,'multiline':False]['text':' 5. Fill the embeddings corresponding to the images. Anything that is still zeros needs filling','line_number':324,'multiline':False]['text':' Ignore copy','line_number':341,'multiline':False]['text':' 1. Extra the input embeddings','line_number':398,'multiline':False]['text':' 2. Merge text and images','line_number':401,'multiline':False]['text':' For VIP-llava, the image features are computed this way','line_number':404,'multiline':False]['text':' We select the features from index 1: for the layers -2, -5, -8, -11 and 6','line_number':405,'multiline':False]['text':' In case input_ids.shape[1] == 1 & pixel_values==None & past_key_values != None, we are in the case of','line_number':416,'multiline':False]['text':' generation with cache','line_number':417,'multiline':False]['text':' Retrieve the first layer to inspect the logits and mask out the hidden states','line_number':419,'multiline':False]['text':' that are set to 0','line_number':420,'multiline':False]['text':' Get the target length','line_number':423,'multiline':False]['text':' Zero-out the places where we don't need to attend','line_number':432,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':453,'multiline':False]['text':' Flatten the tokens','line_number':461,'multiline':False]['text':' Keep only the unprocessed tokens:','line_number':489,'multiline':False]['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':490,'multiline':False]['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':491,'multiline':False]['text':' input)','line_number':492,'multiline':False]['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':495,'multiline':False]['text':' input_ids based on the past_length.','line_number':496,'multiline':False]['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':499,'multiline':False]['text':' If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the','line_number':502,'multiline':False]['text':' older attention values, as their corresponding values are not part of the input.','line_number':503,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':509,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':515,'multiline':False]