['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2020 The Trax Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]['text':' limitations under the License.','line_number':15,'multiline':False]['text':' See all Reformer models at https://huggingface.co/models?filter=reformer','line_number':56,'multiline':False]['text':' Define named tuples for nn.Modules here','line_number':60,'multiline':False]['text':' this function scales the vector so that torch.argsort is stable.','line_number':75,'multiline':False]['text':' torch.argsort is not stable on its own','line_number':76,'multiline':False]['text':' create weights','line_number':135,'multiline':False]['text':' create expanded shapes','line_number':137,'multiline':False]['text':' create tensor and init','line_number':142,'multiline':False]['text':' broadcast weights to correct shape','line_number':146,'multiline':False]['text':' permute weights so that 2D correctly drops dims 1 and 2','line_number':165,'multiline':False]['text':' drop entire matrix of last two dims (prev dims 1 and 2)','line_number':167,'multiline':False]['text':' compute how many columns are needed','line_number':189,'multiline':False]['text':' cut to columns that are needed','line_number':193,'multiline':False]['text':' select correct position encodings','line_number':199,'multiline':False]['text':' dropout','line_number':262,'multiline':False]['text':' add positional embeddings','line_number':265,'multiline':False]['text':' projection matrices','line_number':350,'multiline':False]['text':' save mask value here. Need fp32 and fp16 mask values','line_number':354,'multiline':False]['text':' num hashes can optionally be overwritten by user','line_number':375,'multiline':False]['text':' check if cache shall be used and that hidden states are already cached','line_number':380,'multiline':False]['text':' get query vector','line_number':389,'multiline':False]['text':' split key & value vectors by num hashes to apply','line_number':408,'multiline':False]['text':' self attention on each separately','line_number':409,'multiline':False]['text':' repeat query vectors across hash dimension','line_number':424,'multiline':False]['text':' project hidden_states to query_key and value','line_number':433,'multiline':False]['text':' if query key is not already split','line_number':438,'multiline':False]['text':' cache buckets for next incremental decoding','line_number':447,'multiline':False]['text':' free memory','line_number':451,'multiline':False]['text':' LSH attention only makes sense if chunked attention should be performed','line_number':464,'multiline':False]['text':' set `num_buckets` on the fly, recommended way to do it','line_number':466,'multiline':False]['text':' use cached buckets for backprop only','line_number':470,'multiline':False]['text':' hash query key vectors into buckets','line_number':472,'multiline':False]['text':' make sure buckets has correct shape for LSH attention','line_number':475,'multiline':False]['text':' make sure bucket idx is not longer then sequence length','line_number':486,'multiline':False]['text':' cluster query key value vectors according to hashed buckets','line_number':489,'multiline':False]['text':' use max sequence length','line_number':513,'multiline':False]['text':' get sequence length indices','line_number':516,'multiline':False]['text':' scale key vectors','line_number':521,'multiline':False]['text':' set query_vectors to query key vectors if LSH self attention','line_number':525,'multiline':False]['text':' free memory','line_number':528,'multiline':False]['text':' get attention probs','line_number':531,'multiline':False]['text':' free memory','line_number':543,'multiline':False]['text':' re-order out_vectors and logits','line_number':546,'multiline':False]['text':' sort clusters back to correct ordering','line_number':548,'multiline':False]['text':' sum up all hash rounds','line_number':552,'multiline':False]['text':' free memory','line_number':571,'multiline':False]['text':' free memory','line_number':574,'multiline':False]['text':' only relevant for inference and no bias => we can use einsum here','line_number':601,'multiline':False]['text':' only relevant for inference and no bias => we can use einsum here','line_number':609,'multiline':False]['text':' See https://arxiv.org/pdf/1509.02897.pdf','line_number':616,'multiline':False]['text':' We sample a different random rotation for each round of hashing to','line_number':617,'multiline':False]['text':' decrease the probability of hash misses.','line_number':618,'multiline':False]['text':' Factorize the hash if self.num_buckets is a list or tuple','line_number':626,'multiline':False]['text':' remove gradient','line_number':635,'multiline':False]['text':' for determinism','line_number':639,'multiline':False]['text':' create a random self.attention_head_size x num_hashes x num_buckets/2','line_number':643,'multiline':False]['text':' Output dim: Batch_Size x Num_Attn_Heads x Num_Hashes x Seq_Len x Num_Buckets/2','line_number':645,'multiline':False]['text':' Get the buckets for them and combine.','line_number':652,'multiline':False]['text':' add an extra bucket for padding tokens only','line_number':666,'multiline':False]['text':' assign padding tokens extra bucket','line_number':668,'multiline':False]['text':' buckets is now (Batch_size x Num_Attn_Heads x Num_Hashes x Seq_Len).','line_number':676,'multiline':False]['text':' Next we add offsets so that bucket numbers from different hashing rounds don't overlap.','line_number':677,'multiline':False]['text':' expand to batch size and num attention heads','line_number':681,'multiline':False]['text':' no gradients are needed','line_number':688,'multiline':False]['text':' hash-based sort','line_number':690,'multiline':False]['text':' create simple indices to scatter to, to have undo sort','line_number':693,'multiline':False]['text':' get undo sort','line_number':700,'multiline':False]['text':' `num_buckets` should be set to 2 * sequence_length // chunk_length as recommended in paper','line_number':707,'multiline':False]['text':' make sure buckets are power of 2','line_number':709,'multiline':False]['text':' factorize `num_buckets` if `num_buckets` becomes too large','line_number':712,'multiline':False]['text':' set num buckets in config to be properly saved','line_number':722,'multiline':False]['text':' look at previous and following chunks if chunked attention','line_number':737,'multiline':False]['text':' get logits and dots','line_number':742,'multiline':False]['text':' (BS, NumAttn, NumHash x NumChunk, Chunk_L x Hidden),(BS, NumAttn, NumHash x NumChunk, Chunk_L * (Num_bef + Num_aft + 1) x Hidden) -> (BS, NumAttn, NumHash x NumChunk, Chunk_L, Chunk_L * (1 + Num_bef + Num_aft))','line_number':743,'multiline':False]['text':' free memory','line_number':746,'multiline':False]['text':' if chunked attention split bucket idxs to query and key','line_number':749,'multiline':False]['text':' get correct mask values depending on precision','line_number':768,'multiline':False]['text':' free memory','line_number':788,'multiline':False]['text':' Self mask is ALWAYS applied.','line_number':791,'multiline':False]['text':' From the reformer paper (https://arxiv.org/pdf/2001.04451.pdf):','line_number':792,'multiline':False]['text':' " While attention to the future is not allowed, typical implementations of the','line_number':793,'multiline':False]['text':' Transformer do allow a position to attend to itself.','line_number':794,'multiline':False]['text':' Such behavior is undesirable in a shared-QK formulation because the dot-product','line_number':795,'multiline':False]['text':' of a query vector with itself will almost always be greater than the dot product of a','line_number':796,'multiline':False]['text':' query vector with a vector at another position. We therefore modify the masking','line_number':797,'multiline':False]['text':' to forbid a token from attending to itself, except in situations','line_number':798,'multiline':False]['text':' where a token has no other valid attention targets (e.g. the first token in a sequence) "','line_number':799,'multiline':False]['text':' apply self_mask','line_number':805,'multiline':False]['text':' free memory','line_number':808,'multiline':False]['text':' dots shape is `[batch_size, num_attn_heads, num_hashes * seq_len // chunk_length, chunk_length, chunk_length * (1 + num_chunks_before + num_chunks_after)]`','line_number':812,'multiline':False]['text':' free memory','line_number':815,'multiline':False]['text':' dropout','line_number':818,'multiline':False]['text':' Mask heads if we want to','line_number':821,'multiline':False]['text':' attend values','line_number':825,'multiline':False]['text':' free memory','line_number':828,'multiline':False]['text':' merge chunk length','line_number':831,'multiline':False]['text':' attention mask for LSH','line_number':841,'multiline':False]['text':' if chunked attention, the attention mask has to correspond to LSH order','line_number':843,'multiline':False]['text':' expand attn_mask to fit with key_value_bucket_idx shape','line_number':846,'multiline':False]['text':' extract attention mask from LSH sorted key_indices','line_number':849,'multiline':False]['text':' Causal mask','line_number':854,'multiline':False]['text':' add attention mask if not None','line_number':858,'multiline':False]['text':' concat hidden states','line_number':869,'multiline':False]['text':' batch_size hidden','line_number':872,'multiline':False]['text':' check if cached buckets include pad bucket','line_number':876,'multiline':False]['text':' if pad bucket was cached => need to increase num buckets for caching','line_number':879,'multiline':False]['text':' retrieve query buckets','line_number':882,'multiline':False]['text':' concat buckets','line_number':887,'multiline':False]['text':' hash-based sort','line_number':890,'multiline':False]['text':' bucket_idx has shape: BatchSize x NumAttnHeads x NumHashes x SequenceLength','line_number':893,'multiline':False]['text':' find indices of new bucket indices','line_number':904,'multiline':False]['text':' expand relevant bucket indices to its chunks','line_number':907,'multiline':False]['text':' adapt bucket_idx for batch and hidden states for index select','line_number':911,'multiline':False]['text':' add batch offset','line_number':917,'multiline':False]['text':' select all relevant hidden states','line_number':921,'multiline':False]['text':' reshape hidden states and bucket_idx to correct output','line_number':924,'multiline':False]['text':' get relevant indices of where chunk starts and its size','line_number':953,'multiline':False]['text':' expand start indices and add correct chunk offset via arange','line_number':957,'multiline':False]['text':' make sure that circular logic holds via % seq len','line_number':963,'multiline':False]['text':' expand indices and set indices correctly','line_number':966,'multiline':False]['text':' save sorted_bucket_idx for backprop','line_number':1005,'multiline':False]['text':' undo sort to have correct order for next layer','line_number':1009,'multiline':False]['text':' get parameters saved in ctx','line_number':1017,'multiline':False]['text':' reverse sort of forward','line_number':1021,'multiline':False]['text':' return grad and `None` fillers for last 2 forward args','line_number':1025,'multiline':False]['text':' projection matrices','line_number':1044,'multiline':False]['text':' save mask value here','line_number':1051,'multiline':False]['text':' check if cache shall be used and that hidden states are already cached','line_number':1068,'multiline':False]['text':' only query vector for last token','line_number':1079,'multiline':False]['text':' compute key and value for relevant chunk','line_number':1081,'multiline':False]['text':' free memory','line_number':1085,'multiline':False]['text':' project hidden_states to query, key and value','line_number':1088,'multiline':False]['text':' split last dim into `config.num_attention_heads` and `config.attention_head_size`','line_number':1093,'multiline':False]['text':' normalize key vectors','line_number':1114,'multiline':False]['text':' get sequence length indices','line_number':1117,'multiline':False]['text':' if one should do normal n^2 self-attention','line_number':1122,'multiline':False]['text':' if input should be chunked','line_number':1125,'multiline':False]['text':' chunk vectors','line_number':1127,'multiline':False]['text':' B x Num_Attn_Head x Seq_Len // chunk_len x chunk_len  x  attn_head_size','line_number':1128,'multiline':False]['text':' chunk indices','line_number':1151,'multiline':False]['text':' append chunks before and after','line_number':1155,'multiline':False]['text':' query-key matmul: QK^T','line_number':1162,'multiline':False]['text':' free memory','line_number':1165,'multiline':False]['text':' get mask tensor depending on half precision or not','line_number':1173,'multiline':False]['text':' free memory','line_number':1181,'multiline':False]['text':' softmax','line_number':1184,'multiline':False]['text':' free memory','line_number':1188,'multiline':False]['text':' dropout','line_number':1191,'multiline':False]['text':' Mask heads if we want to','line_number':1194,'multiline':False]['text':' attend values','line_number':1198,'multiline':False]['text':' free memory','line_number':1201,'multiline':False]['text':' merge chunk length','line_number':1204,'multiline':False]['text':' chunk attention mask and look before and after','line_number':1225,'multiline':False]['text':' create attn_mask','line_number':1232,'multiline':False]['text':' Causal mask','line_number':1235,'multiline':False]['text':' add attention mask if not None','line_number':1239,'multiline':False]['text':' get correct attn layers','line_number':1280,'multiline':False]['text':' make sure cached hidden states is set to None for backward pass','line_number':1306,'multiline':False]['text':' use cached buckets for backprob if buckets not None for LSHSelfAttention','line_number':1312,'multiline':False]['text':' add buckets if necessary','line_number':1324,'multiline':False]['text':' cache hidden states for future use','line_number':1330,'multiline':False]['text':' padded input should not be cached','line_number':1333,'multiline':False]['text':' padded input should not be cached','line_number':1343,'multiline':False]['text':' compute attention feed forward output','line_number':1349,'multiline':False]['text':' dropout requires to have the same','line_number':1419,'multiline':False]['text':' seed for forward and backward pass','line_number':1420,'multiline':False]['text':' randomize seeds','line_number':1432,'multiline':False]['text':' use cuda generator if available','line_number':1433,'multiline':False]['text':' GPU','line_number':1435,'multiline':False]['text':' CPU','line_number':1439,'multiline':False]['text':' randomize seeds','line_number':1449,'multiline':False]['text':' use cuda generator if available','line_number':1450,'multiline':False]['text':' GPU','line_number':1452,'multiline':False]['text':' CPU','line_number':1456,'multiline':False]['text':' every forward pass we sample a different seed','line_number':1474,'multiline':False]['text':' for dropout and save for forward fn in backward pass','line_number':1475,'multiline':False]['text':' to have correct dropout','line_number':1476,'multiline':False]['text':' Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0)','line_number':1492,'multiline':False]['text':' Y_1 = X_1 + f(X_2)','line_number':1493,'multiline':False]['text':' free memory','line_number':1496,'multiline':False]['text':' every forward pass we sample a different seed','line_number':1499,'multiline':False]['text':' for dropout and save seed for forward fn in backward','line_number':1500,'multiline':False]['text':' to have correct dropout','line_number':1501,'multiline':False]['text':' Y_2 = X_2 + g(Y_1)','line_number':1504,'multiline':False]['text':' Implements the backward pass for reversible ResNets.','line_number':1524,'multiline':False]['text':' A good blog post on how this works can be found here:','line_number':1525,'multiline':False]['text':' Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0)','line_number':1526,'multiline':False]['text':' This code is heavily inspired by https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reversible.py','line_number':1527,'multiline':False]['text':' set seed to have correct dropout','line_number':1537,'multiline':False]['text':' g(Y_1)','line_number':1539,'multiline':False]['text':' X_2 = Y_2 - g(Y_1)','line_number':1544,'multiline':False]['text':' set seed to have correct dropout','line_number':1554,'multiline':False]['text':' f(X_2)','line_number':1556,'multiline':False]['text':' use cached buckets for backprob if buckets not None for LSHSelfAttention','line_number':1557,'multiline':False]['text':' X_1 = Y_1 - f(X_2)','line_number':1567,'multiline':False]['text':' split duplicated tensor','line_number':1608,'multiline':False]['text':' Add last layer','line_number':1634,'multiline':False]['text':' attach params to ctx for backward','line_number':1638,'multiline':False]['text':' Concatenate 2 RevNet outputs','line_number':1645,'multiline':False]['text':' retrieve params from ctx for backward','line_number':1652,'multiline':False]['text':' create tuple','line_number':1655,'multiline':False]['text':' free memory','line_number':1663,'multiline':False]['text':' pop last buckets from stack','line_number':1672,'multiline':False]['text':' backprop','line_number':1676,'multiline':False]['text':' num of return vars has to match num of forward() args','line_number':1690,'multiline':False]['text':' return gradient for hidden_states arg and None for other args','line_number':1691,'multiline':False]['text':' Reformer is using Rev Nets, thus last layer outputs are concatenated and','line_number':1701,'multiline':False]['text':' Layer Norm is done over 2 * hidden_size','line_number':1702,'multiline':False]['text':' hidden_states and attention lists to be filled if wished','line_number':1717,'multiline':False]['text':' init cached hidden states if necessary','line_number':1721,'multiline':False]['text':' concat same tensor for reversible ResNet','line_number':1725,'multiline':False]['text':' Apply layer norm to concatenated hidden states','line_number':1742,'multiline':False]['text':' Apply dropout','line_number':1745,'multiline':False]['text':' Reformer is using Rev Nets, thus last layer outputs are concatenated and','line_number':1759,'multiline':False]['text':' Layer Norm is done over 2 * hidden_size','line_number':1760,'multiline':False]['text':' To tie those two weights if they get disconnected (on TPU or when the bias is resized)','line_number':1775,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':1808,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':1809,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1987,'multiline':False]['text':' noqa: F841','line_number':2035,'multiline':False]['text':' noqa: F841','line_number':2038,'multiline':False]['text':' prepare head mask','line_number':2050,'multiline':False]['text':' original sequence length for padding','line_number':2053,'multiline':False]['text':' if needs padding','line_number':2056,'multiline':False]['text':' pad input','line_number':2076,'multiline':False]['text':' start index for position encoding depends on incremental decoding','line_number':2088,'multiline':False]['text':' if padding was applied','line_number':2114,'multiline':False]['text':' Extend `attention_mask`','line_number':2154,'multiline':False]['text':' Extend `input_ids` with padding to match least common multiple chunk_length','line_number':2168,'multiline':False]['text':' Pad position ids if given','line_number':2173,'multiline':False]['text':' Extend `inputs_embeds` with padding to match least common multiple chunk_length','line_number':2179,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2206,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':2263,'multiline':False]['text':' Flatten the tokens','line_number':2266,'multiline':False]['text':' only last token for inputs_ids if past is defined in kwargs','line_number':2285,'multiline':False]['text':' buckets','line_number':2301,'multiline':False]['text':' hidden states','line_number':2307,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2326,'multiline':False]['text':' no causal mask','line_number':2411,'multiline':False]['text':' -100 index = padding token','line_number':2422,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2455,'multiline':False]['text':' take <s> token (equiv. to [CLS])','line_number':2575,'multiline':False]['text':' 2 * config.hidden_size because we use reversible residual layers','line_number':2597,'multiline':False]['text':' Initialize weights and apply final processing','line_number':2600,'multiline':False]['text':' no causal mask','line_number':2642,'multiline':False]['text':' If we are on multi-GPU, split add a dimension','line_number':2657,'multiline':False]['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':2662,'multiline':False]