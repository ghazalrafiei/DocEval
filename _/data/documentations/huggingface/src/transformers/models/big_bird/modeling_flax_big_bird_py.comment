['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The Google Flax Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' the dtype of the computation','line_number':198,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertEmbeddings.setup','line_number':200,'multiline':False]['text':' Embed','line_number':224,'multiline':False]['text':' Sum all embeddings','line_number':232,'multiline':False]['text':' Layer Norm','line_number':235,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertSelfAttention with Bert->BigBird','line_number':241,'multiline':False]['text':' the dtype of the computation','line_number':245,'multiline':False]['text':' Copied from transformers.models.bart.modeling_flax_bart.FlaxBartAttention._concatenate_to_cache','line_number':283,'multiline':False]['text':' detect if we're initializing by absence of existing cache data.','line_number':290,'multiline':False]['text':' update key, value caches with our new 1d spatial slices','line_number':298,'multiline':False]['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.','line_number':307,'multiline':False]['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':325,'multiline':False]['text':' for the decoder','line_number':326,'multiline':False]['text':' get query proj','line_number':330,'multiline':False]['text':' get key, value proj','line_number':332,'multiline':False]['text':' cross_attentions','line_number':334,'multiline':False]['text':' self_attention','line_number':338,'multiline':False]['text':' handle cache prepare causal attention mask','line_number':346,'multiline':False]['text':' combine masks if needed','line_number':359,'multiline':False]['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':368,'multiline':False]['text':' and cache the keys and values step by step.','line_number':369,'multiline':False]['text':' Convert the boolean attention mask to an attention bias.','line_number':375,'multiline':False]['text':' attention mask in the form of attention bias','line_number':377,'multiline':False]['text':' Mask heads if we want to','line_number':402,'multiline':False]['text':' BigBird block-sparse attention as suggested in paper','line_number':543,'multiline':False]['text':' ITC:','line_number':545,'multiline':False]['text':'     global tokens: 2 x block_size','line_number':546,'multiline':False]['text':'     window tokens: 3 x block_size','line_number':547,'multiline':False]['text':'     random tokens: num_rand_tokens x block_size','line_number':548,'multiline':False]['text':' ETC:','line_number':550,'multiline':False]['text':'     global tokens: extra_globals_tokens + 2 x block_size','line_number':551,'multiline':False]['text':'     window tokens: 3 x block_size','line_number':552,'multiline':False]['text':'     random tokens: num_rand_tokens x block_size','line_number':553,'multiline':False]['text':' Note:','line_number':555,'multiline':False]['text':'     1) Currently, ETC is not supported.','line_number':556,'multiline':False]['text':'     2) Window size is fixed to 3 blocks & it can be changed only by','line_number':557,'multiline':False]['text':'     changing `block_size`.','line_number':558,'multiline':False]['text':'     3) Number of global blocks are fixed (2 blocks here) & global tokens can be','line_number':559,'multiline':False]['text':'     controlled only by `block_size`.','line_number':560,'multiline':False]['text':' attention is calculated separately for q[0], q[1], q[2:-2], q[-2], q[-1] in order to use special trick of','line_number':562,'multiline':False]['text':' shifting tokens (for calculating sliding attention). hence following code can be divided into 5 parts.','line_number':563,'multiline':False]['text':' old plans used in paper','line_number':582,'multiline':False]['text':' 1st PART','line_number':628,'multiline':False]['text':' 1st block (global block) attention scores','line_number':629,'multiline':False]['text':' q[0] x (k[0], k[1], k[2], k[3], k[4] .... )','line_number':630,'multiline':False]['text':' [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, to_seq_len]','line_number':632,'multiline':False]['text':' [bsz, n_heads, from_block_size, to_seq_len]','line_number':637,'multiline':False]['text':' [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]','line_number':639,'multiline':False]['text':' 2nd PART','line_number':643,'multiline':False]['text':' 2nd block attention scores','line_number':644,'multiline':False]['text':' q[1] x (sliding_keys, random_keys, global_keys)','line_number':645,'multiline':False]['text':' sliding key blocks -> 2nd, 3rd blocks','line_number':646,'multiline':False]['text':' global key blocks -> 1st block','line_number':647,'multiline':False]['text':' [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]','line_number':658,'multiline':False]['text':' [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]','line_number':668,'multiline':False]['text':' [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]','line_number':670,'multiline':False]['text':' ==> [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]','line_number':671,'multiline':False]['text':' [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]','line_number':692,'multiline':False]['text':' [bsz, n_heads, from_block_size, (4+r)*to_block_size] x [bsz, n_heads, (4+r)*to_block_size, -1]','line_number':694,'multiline':False]['text':'  ==> [bsz, n_heads, from_block_size, -1]','line_number':695,'multiline':False]['text':' 3rd PART','line_number':699,'multiline':False]['text':' Middle blocks attention scores','line_number':700,'multiline':False]['text':' q[-2:2] x (sliding_keys, random_keys, global_keys)','line_number':701,'multiline':False]['text':' sliding attn is calculated using special trick of shifting tokens as discussed in paper','line_number':702,'multiline':False]['text':' random keys are generated by taking random indices as per `rand_attn`','line_number':703,'multiline':False]['text':' global keys -> 1st & last block','line_number':704,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]','line_number':708,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]','line_number':712,'multiline':False]['text':' sliding attention scores for q[-2:2]','line_number':715,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [b, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]','line_number':716,'multiline':False]['text':'     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, 3*to_block_size]','line_number':718,'multiline':False]['text':' randn attention scores for q[-2:2]','line_number':721,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]','line_number':722,'multiline':False]['text':' x [bsz, n_heads, from_seq_len//from_block_size-4, n_rand_blocks*to_block_size, -1]','line_number':723,'multiline':False]['text':'     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, n_rand_blocks*to_block_size]','line_number':725,'multiline':False]['text':' Including 1st block (since it's global)','line_number':728,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [bsz, n_heads, to_block_size, -1]','line_number':729,'multiline':False]['text':'  ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size]','line_number':730,'multiline':False]['text':' Including last block (since it's global)','line_number':734,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1] x [bsz, n_heads, to_block_size, -1]','line_number':735,'multiline':False]['text':'  ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size]','line_number':736,'multiline':False]['text':' masking padded tokens','line_number':740,'multiline':False]['text':' completing attention scores matrix for all q[-2:2]','line_number':746,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, (5+n_rand_blocks)*to_block_size]','line_number':749,'multiline':False]['text':' safely doing softmax since attention matrix is completed','line_number':751,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, (5+n_rand_blocks)*to_block_size]','line_number':754,'multiline':False]['text':' contribution of sliding keys','line_number':756,'multiline':False]['text':' [bsz, n_heads, m//from_block_size-4, from_block_size, 3*to_block_size]','line_number':757,'multiline':False]['text':' x [bsz, n_heads, from_seq_len//from_block_size-4, 3*to_block_size, -1]','line_number':758,'multiline':False]['text':'     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]','line_number':762,'multiline':False]['text':' adding contribution of random keys','line_number':764,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, n_rand_blocks*to_block_size]','line_number':765,'multiline':False]['text':' x [bsz, n_heads, from_seq_len//from_block_size-4, n_rand_blocks*to_block_size, -1]','line_number':766,'multiline':False]['text':'     ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]','line_number':772,'multiline':False]['text':' adding contribution of global keys','line_number':774,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size] x [bsz, n_heads, to_block_size, -1]','line_number':775,'multiline':False]['text':'  ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]','line_number':776,'multiline':False]['text':' [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, to_block_size] x [bsz, n_heads, to_block_size, -1]','line_number':780,'multiline':False]['text':' ==> [bsz, n_heads, from_seq_len//from_block_size-4, from_block_size, -1]','line_number':781,'multiline':False]['text':' 4th PART','line_number':786,'multiline':False]['text':' last 2nd token attention scores','line_number':787,'multiline':False]['text':' q[-2] x (sliding_keys, random_keys, global_keys)','line_number':788,'multiline':False]['text':' sliding key blocks -> last 3 blocks','line_number':789,'multiline':False]['text':' global key block -> 1st block','line_number':790,'multiline':False]['text':' random key block -> based on indices stored in `randn_attn`','line_number':791,'multiline':False]['text':' [bsz, n_heads, (4+n_random_blocks)*to_block_size, -1]','line_number':802,'multiline':False]['text':' [bsz, n_heads, (4+r)*to_block_size, -1]','line_number':812,'multiline':False]['text':' [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]','line_number':814,'multiline':False]['text':' ==> [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]','line_number':815,'multiline':False]['text':' [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size]','line_number':836,'multiline':False]['text':' [bsz, n_heads, from_block_size, (4+n_rand_blocks)*to_block_size] x [bsz, n_heads, (4+n_rand_blocks)*to_block_size, -1]','line_number':838,'multiline':False]['text':' ==> [bsz, n_heads, from_block_size, -1]','line_number':839,'multiline':False]['text':' 5th PART','line_number':843,'multiline':False]['text':' last block (global) attention scores','line_number':844,'multiline':False]['text':' q[-1] x (k[0], k[1], k[2], k[3], .... )','line_number':845,'multiline':False]['text':' [bsz, n_heads, from_block_size, -1] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, to_seq_len]','line_number':847,'multiline':False]['text':' [bsz, n_heads, from_block_size, n]','line_number':851,'multiline':False]['text':' [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]','line_number':853,'multiline':False]['text':' combining representations of all tokens','line_number':857,'multiline':False]['text':' params.shape[:batch_dims] + indices.shape + params.shape[batch_dims+1:]','line_number':885,'multiline':False]['text':' using this method when from_seq_length in [1024, 3072, 4096]','line_number':985,'multiline':False]['text':' deterministic nor randomness','line_number':990,'multiline':False]['text':' shorthand','line_number':999,'multiline':False]['text':' Missing -3: should have been sliced till last-3','line_number':1012,'multiline':False]['text':' Missing -4: should have been sliced till last-4','line_number':1016,'multiline':False]['text':' using this method when from_seq_length not in [1024, 3072, 4096]','line_number':1073,'multiline':False]['text':' Total number of blocks in the mmask','line_number':1081,'multiline':False]['text':' Number of blocks per plan','line_number':1083,'multiline':False]['text':' till when to follow plan','line_number':1085,'multiline':False]['text':' Random Attention adjacency list','line_number':1088,'multiline':False]['text':' deterministic','line_number':1094,'multiline':False]['text':' We will go iteratively over the plan blocks and pick random number of','line_number':1100,'multiline':False]['text':' Attention blocks from the legally allowed blocks','line_number':1101,'multiline':False]['text':' set the row for all from_blocks starting from 0 to','line_number':1105,'multiline':False]['text':' plan_block_length[plan_idx-1]','line_number':1106,'multiline':False]['text':' column indx start fromm plan_block_length[plan_idx-1] and ends at','line_number':1107,'multiline':False]['text':' plan_block_length[plan_idx]','line_number':1108,'multiline':False]['text':' list of to_blocks from which to choose random attention','line_number':1212,'multiline':False]['text':' permute the blocks','line_number':1214,'multiline':False]['text':' illegal blocks for the current block id, using window','line_number':1217,'multiline':False]['text':' Add blocks at the start and at the end','line_number':1220,'multiline':False]['text':' The second from_block cannot choose random attention on second last to_block','line_number':1224,'multiline':False]['text':' The second last from_block cannot choose random attention on second to_block','line_number':1228,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertSelfOutput with Bert->BigBird','line_number':1242,'multiline':False]['text':' the dtype of the computation','line_number':1245,'multiline':False]['text':' Attention mask comes in as attention_mask.shape == (*batch_sizes, kv_length)','line_number':1292,'multiline':False]['text':' FLAX expects: attention_mask.shape == (*batch_sizes, 1, 1, kv_length) such that it is broadcastable','line_number':1293,'multiline':False]['text':' with attn_weights.shape == (*batch_sizes, num_heads, q_length, kv_length)','line_number':1294,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertIntermediate with Bert->BigBird','line_number':1323,'multiline':False]['text':' the dtype of the computation','line_number':1326,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertOutput with Bert->BigBird','line_number':1342,'multiline':False]['text':' the dtype of the computation','line_number':1345,'multiline':False]['text':' the dtype of the computation','line_number':1366,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertLayer.__call__ with Bert->BigBird','line_number':1377,'multiline':False]['text':' Self Attention','line_number':1389,'multiline':False]['text':' Cross-Attention Block','line_number':1400,'multiline':False]['text':' the dtype of the computation','line_number':1426,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertLayerCollection.__call__ with Bert->BigBird','line_number':1442,'multiline':False]['text':' Check if head_mask has a correct number of layers specified if desired','line_number':1460,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertEncoder with Bert->BigBird','line_number':1507,'multiline':False]['text':' the dtype of the computation','line_number':1510,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertPredictionHeadTransform with Bert->BigBird','line_number':1547,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertLMPredictionHead with Bert->BigBird, np.ndarray->jnp.ndarray','line_number':1563,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertOnlyMLMHead with Bert->BigBird','line_number':1587,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainedModel.enable_gradient_checkpointing','line_number':1642,'multiline':False]['text':' init input tensors','line_number':1651,'multiline':False]['text':' Copied from transformers.models.bart.modeling_flax_bart.FlaxBartDecoderPreTrainedModel.init_cache','line_number':1698,'multiline':False]['text':' init input variables to retrieve cache','line_number':1708,'multiline':False]['text':' init input tensors if not passed','line_number':1743,'multiline':False]['text':' Handle any PRNG if needed','line_number':1756,'multiline':False]['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed','line_number':1767,'multiline':False]['text':' down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be','line_number':1768,'multiline':False]['text':' changed by FlaxBigBirdAttention module','line_number':1769,'multiline':False]['text':' add updated cache to model output','line_number':1793,'multiline':False]['text':' the dtype of the computation','line_number':1822,'multiline':False]['text':' if pooled is None, don't return it','line_number':1872,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertModel with Bert->BigBird','line_number':1890,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingModule with Bert->BigBird','line_number':1898,'multiline':False]['text':' Model','line_number':1924,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForPreTraining with Bert->BigBird','line_number':1967,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLMModule with Bert->BigBird','line_number':2000,'multiline':False]['text':' Model','line_number':2027,'multiline':False]['text':' Compute the prediction scores','line_number':2046,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLM with Bert->BigBird','line_number':2060,'multiline':False]['text':' take <s> token (equiv. to [CLS])','line_number':2085,'multiline':False]['text':' Model','line_number':2117,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassification with Bert->BigBird','line_number':2150,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForMultipleChoiceModule with Bert->BigBird','line_number':2163,'multiline':False]['text':' Model','line_number':2196,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassificationModule with Bert->BigBird','line_number':2262,'multiline':False]['text':' Model','line_number':2295,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassification with Bert->BigBird','line_number':2329,'multiline':False]['text':' Model','line_number':2389,'multiline':False]['text':' removing question tokens from the competition','line_number':2407,'multiline':False]['text':' assuming input_ids format: <cls> <question> <sep> context <sep>','line_number':2469,'multiline':False]['text':' setting lengths logits to `-inf`','line_number':2477,'multiline':False]['text':' init input tensors if not passed','line_number':2484,'multiline':False]['text':' Handle any PRNG if needed','line_number':2488,'multiline':False]['text':' q_lengths -> (bz, 1)','line_number':2513,'multiline':False]['text':' Model','line_number':2556,'multiline':False]['text':' Compute the prediction scores','line_number':2578,'multiline':False]['text':' Copied from transformers.models.bert.modeling_flax_bert.FlaxBertForCausalLM with Bert->BigBird','line_number':2599,'multiline':False]['text':' initializing the cache','line_number':2604,'multiline':False]['text':' Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.','line_number':2608,'multiline':False]['text':' But since the decoder uses a causal mask, those positions are masked anyway.','line_number':2609,'multiline':False]['text':' Thus, we can create a single static attention_mask here, which is more efficient for compilation','line_number':2610,'multiline':False]