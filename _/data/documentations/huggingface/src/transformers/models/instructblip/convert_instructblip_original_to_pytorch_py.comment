['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' pip3 install salesforce-lavis','line_number':26,'multiline':False]['text':' I'm actually installing a slightly modified version: pip3 install git+https://github.com/nielsrogge/LAVIS.git@fix_lavis_float32 (there's also the fix_lavis branch)','line_number':27,'multiline':False]['text':' also note: to convert Vicuna checkpoints, we had to include /home/niels/python_projects/checkpoints/FastChat/vicuna-7b in lavis/configs/models/blip2/blip2_instruct_vicuna7b.yaml','line_number':28,'multiline':False]['text':' same for Vicuna-13b','line_number':29,'multiline':False]['text':' here we list all keys to be renamed (original name on the left, our name on the right)','line_number':56,'multiline':False]['text':' fmt: off','line_number':59,'multiline':False]['text':' vision encoder','line_number':61,'multiline':False]['text':' QFormer','line_number':82,'multiline':False]['text':' fmt: on','line_number':86,'multiline':False]['text':' read in original q and v biases','line_number':97,'multiline':False]['text':' next, set bias in the state dict','line_number':101,'multiline':False]['text':' make sure the models have proper bos_token_id and eos_token_id set (important for generation)','line_number':110,'multiline':False]['text':' seems like flan-T5 models don't have bos_token_id properly set?','line_number':111,'multiline':False]['text':' the authors add one special "[DEC]" token to the vocab of Q-Former, hence vocab size = 30522 + 1','line_number':123,'multiline':False]['text':' the following was used in the original implementation:','line_number':141,'multiline':False]['text':' tokenizer = LlamaTokenizer.from_pretrained("huggyllama/llama-7b", use_fast=False, truncation_side="left")','line_number':142,'multiline':False]['text':' tokenizer.add_special_tokens({"pad_token": "[PAD]"})','line_number':143,'multiline':False]['text':' tokenizer.add_special_tokens({"bos_token": "</s>"})','line_number':144,'multiline':False]['text':' tokenizer.add_special_tokens({"eos_token": "</s>"})','line_number':145,'multiline':False]['text':' tokenizer.add_special_tokens({"unk_token": "</s>"})','line_number':146,'multiline':False]['text':' load original model','line_number':164,'multiline':False]['text':' update state dict keys','line_number':174,'multiline':False]['text':' some keys can be renamed efficiently','line_number':180,'multiline':False]['text':' read in qv biases','line_number':197,'multiline':False]['text':' note: weights get loaded in torch.float32 by default','line_number':200,'multiline':False]['text':' create processor','line_number':206,'multiline':False]['text':' make sure processor creates exact same pixel values','line_number':217,'multiline':False]['text':' assert values','line_number':239,'multiline':False]['text':' important: we need to cast the weights of the HF model to the appropriate type','line_number':248,'multiline':False]['text':' convert output id 0 to 2 (eos_token_id)','line_number':262,'multiline':False]['text':' TODO add this in the generate method?','line_number':263,'multiline':False]