['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 Deepmind and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all Perceiver models at https://huggingface.co/models?filter=perceiver','line_number':56,'multiline':False]['text':' Thanks, Phil Wang','line_number':180,'multiline':False]['text':' Q and K must have the same number of channels.','line_number':198,'multiline':False]['text':' Default to preserving Q's input's shape.','line_number':199,'multiline':False]['text':' V's num_channels determines the shape of the output of QKV-attention.','line_number':202,'multiline':False]['text':' Default to the same number of channels used in the key-query operation.','line_number':203,'multiline':False]['text':' Layer normalization','line_number':216,'multiline':False]['text':' Projection matrices','line_number':220,'multiline':False]['text':' Project queries, keys and values to a common feature dimension. If this is instantiated as a cross-attention module,','line_number':244,'multiline':False]['text':' the keys and values come from the inputs; the attention mask needs to be such that the inputs's non-relevant tokens are not attended to.','line_number':245,'multiline':False]['text':' Reshape channels for multi-head attention.','line_number':257,'multiline':False]['text':' We reshape from (batch_size, time, channels) to (batch_size, num_heads, time, channels per head)','line_number':258,'multiline':False]['text':' Take the dot product between the queries and keys to get the raw attention scores.','line_number':263,'multiline':False]['text':' Apply the attention mask (precomputed for all layers in PerceiverModel forward() function)','line_number':273,'multiline':False]['text':' Normalize the attention scores to probabilities.','line_number':276,'multiline':False]['text':' This is actually dropping out entire tokens to attend to, which might','line_number':279,'multiline':False]['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':280,'multiline':False]['text':' Mask heads if we want to','line_number':283,'multiline':False]['text':' MultiHead attention','line_number':323,'multiline':False]['text':' dense block','line_number':348,'multiline':False]['text':' Prune linear layers','line_number':366,'multiline':False]['text':' Update hyper params and store pruned heads','line_number':372,'multiline':False]['text':' Output projection','line_number':395,'multiline':False]['text':' Optionally include a residual to the original queries.','line_number':398,'multiline':False]['text':' Consider omitting the residual if the semantics of query and output','line_number':399,'multiline':False]['text':' are different, e.g. if queries are positions and outputs are pixels.','line_number':400,'multiline':False]['text':' add attentions if we output them','line_number':404,'multiline':False]['text':' add attentions if we output attention weights','line_number':475,'multiline':False]['text':' residual connection','line_number':481,'multiline':False]['text':' Check that we can use multihead-attention with these shapes.','line_number':500,'multiline':False]['text':' Construct the cross attention layer.','line_number':512,'multiline':False]['text':' Construct a single block of self-attention layers.','line_number':525,'multiline':False]['text':' We get deeper architectures by applying this block more than once.','line_number':526,'multiline':False]['text':' Apply the cross-attention between the latents (hidden_states) and inputs:','line_number':558,'multiline':False]['text':' Apply the block of self-attention layers more than once:','line_number':572,'multiline':False]['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':621,'multiline':False]['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':622,'multiline':False]['text':' Initialize weights and apply final processing','line_number':735,'multiline':False]['text':' If no attention mask is provided, make them all ones','line_number':878,'multiline':False]['text':' Make the attention mask broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':881,'multiline':False]['text':' Prepare head mask if needed','line_number':884,'multiline':False]['text':' 1.0 in head_mask indicate we keep the head','line_number':885,'multiline':False]['text':' attention_probs has shape bsz x n_heads x N x N','line_number':886,'multiline':False]['text':' input head_mask has shape [num_heads] or [num_blocks x num_heads]','line_number':887,'multiline':False]['text':' and head_mask is converted to shape [num_blocks x batch x num_heads x N x N]','line_number':888,'multiline':False]['text':' add cross-attentions of decoder','line_number':926,'multiline':False]['text':' we need to define the seq_len of the inputs beforehand','line_number':971,'multiline':False]['text':' Initialize weights and apply final processing','line_number':983,'multiline':False]['text':' -100 index = padding token','line_number':1072,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1107,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1242,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1386,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1530,'multiline':False]['text':' position_encoding_kwargs','line_number':1669,'multiline':False]['text':' decoder kwargs','line_number':1681,'multiline':False]['text':' We query the decoder using the first frame features','line_number':1684,'multiline':False]['text':' rather than a standard decoder position encoding.','line_number':1685,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1691,'multiline':False]['text':' Autoencoding, don't pass inputs to the queries.','line_number':1830,'multiline':False]['text':' Autoencoding, don't pass inputs to the queries.','line_number':1847,'multiline':False]['text':' Modality specific decoders are used ONLY to generate queries.','line_number':1849,'multiline':False]['text':' All modalties are decoded together using a unified decoder.','line_number':1850,'multiline':False]['text':' Autoencoding, don't pass inputs to the queries.','line_number':1854,'multiline':False]['text':' Autoencoding, don't pass inputs to the queries.','line_number':1871,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1902,'multiline':False]['text':' Below: position encodings','line_number':1994,'multiline':False]['text':' We don't use the index_dims argument, as this is only known during the forward pass','line_number':2018,'multiline':False]['text':' Optionally, project the position encoding to a target dimension:','line_number':2025,'multiline':False]['text':' Below: Perceiver decoders','line_number':2031,'multiline':False]['text':' (batch_size, num_latents, d_latents) -> (batch_size, d_latents)','line_number':2070,'multiline':False]['text':' (batch_size, d_latents) -> (batch_size, config.num_labels)','line_number':2072,'multiline':False]['text':' The following 2 arguments are ignored if position_encoding_type == 'none':','line_number':2118,'multiline':False]['text':' If `none`, the decoder will not construct any position encodings.','line_number':2135,'multiline':False]['text':' You should construct your own when querying the decoder.','line_number':2136,'multiline':False]['text':' for multimodal autoencoding, we don't need the decoder cross-attention and final layer','line_number':2154,'multiline':False]['text':' so then we will set position_encoding_only to True','line_number':2155,'multiline':False]['text':' Queries come from elsewhere','line_number':2172,'multiline':False]['text':' Queries come from elsewhere','line_number':2185,'multiline':False]['text':' subsampled_points are the indices if the inputs would be flattened','line_number':2188,'multiline':False]['text':' however, the inputs aren't flattened, that's why we use unravel_index','line_number':2189,'multiline':False]['text':' to get the indices for the unflattened array','line_number':2190,'multiline':False]['text':' unravel_index returns a tuple (x_idx, y_idx, ...)','line_number':2191,'multiline':False]['text':' stack to get the [n, d] tensor of coordinates','line_number':2192,'multiline':False]['text':' Map these coordinates to [-1, 1]','line_number':2196,'multiline':False]['text':' Construct the position encoding.','line_number':2199,'multiline':False]['text':' Optionally project them to a target dimension.','line_number':2207,'multiline':False]['text':' Construct the position encoding.','line_number':2214,'multiline':False]['text':' Optionally project them to a target dimension.','line_number':2222,'multiline':False]['text':' Cross-attention decoding.','line_number':2239,'multiline':False]['text':' key, value: B x N x K; query: B x M x K','line_number':2240,'multiline':False]['text':' Attention maps -> B x N x M','line_number':2241,'multiline':False]['text':' Output -> B x M x K','line_number':2242,'multiline':False]['text':' Predict a single logit array.','line_number':2281,'multiline':False]['text':' B x 1 x num_classes -> B x num_classes','line_number':2303,'multiline':False]['text':' Output flow and rescale.','line_number':2338,'multiline':False]['text':' B, T, H, W','line_number':2362,'multiline':False]['text':' Build the decoder components:','line_number':2364,'multiline':False]['text':' T*H*W','line_number':2370,'multiline':False]['text':' Apply a predictable ordering to the modalities','line_number':2412,'multiline':False]['text':' Partition the flat inputs among the different modalities','line_number':2485,'multiline':False]['text':' Obtain modality-specific decoders' queries','line_number':2488,'multiline':False]['text':' Get input_without_pos for this modality if it exists.','line_number':2493,'multiline':False]['text':' Pad all queries with trainable position encodings to make them have the same channels','line_number':2505,'multiline':False]['text':' Apply a predictable ordering to the modalities','line_number':2513,'multiline':False]['text':' B x 1 x num_classes -> B x num_classes','line_number':2525,'multiline':False]['text':' Below: IO pre- and post-processor classes for Perceiver.','line_number':2531,'multiline':False]['text':' split up dimensions (height by spatial_block_size, width by spatial_block_size)','line_number':2542,'multiline':False]['text':' move blocks to last dimension: (batch_size, H//bs, W//bs, bs, bs, C)','line_number':2551,'multiline':False]['text':' concatenate blocks along channel dimension: (batch_size, H//bs, W//bs, bs*bs*C)','line_number':2553,'multiline':False]['text':' split up dimensions (time by temporal_block_size, height by spatial_block_size, width by spatial_block_size)','line_number':2563,'multiline':False]['text':' move blocks to last dimension: (batch_size, T//ts, H//bs, W//bs, ts, bs, bs, C)','line_number':2574,'multiline':False]['text':' concatenate blocks along channel dimension: (batch_size, T//ts, H//bs, W//bs, ts*bs*bs*C)','line_number':2576,'multiline':False]['text':' Nyquist frequency at the target resolution:','line_number':2673,'multiline':False]['text':' Get frequency bands for each spatial dimension.','line_number':2678,'multiline':False]['text':' Output is size [n, d * num_bands]','line_number':2679,'multiline':False]['text':' Output is size [n, d * num_bands]','line_number':2684,'multiline':False]['text':' Output is size [n, 2 * d * num_bands]','line_number':2687,'multiline':False]['text':' Concatenate the raw input positions.','line_number':2691,'multiline':False]['text':' Adds d bands to the encoding.','line_number':2693,'multiline':False]['text':' equivalent to `torch.broadcast_to(pos[None], (batch_size,) + pos.shape)`','line_number':2782,'multiline':False]['text':' but `torch.broadcast_to` cannot be converted to ONNX','line_number':2783,'multiline':False]['text':' Just a warning label: you probably don't want your spatial features to','line_number':2787,'multiline':False]['text':' have a different spatial layout than your pos coordinate system.','line_number':2788,'multiline':False]['text':' But feel free to override if you think it'll work!','line_number':2789,'multiline':False]['text':' Flatten batch dim','line_number':2894,'multiline':False]['text':' Slice up modalities by their sizes.','line_number':2923,'multiline':False]['text':' to be supported: 'conv', 'patches', 'pixels'','line_number':2971,'multiline':False]['text':' Architecture parameters:','line_number':2974,'multiline':False]['text':' only relevant when conv_after_patching = True','line_number':3050,'multiline':False]['text':' Downsampling with conv is currently restricted','line_number':3075,'multiline':False]['text':' spatial_downsample is unconstrained for 1x1 convolutions.','line_number':3096,'multiline':False]['text':' Position embeddings','line_number':3100,'multiline':False]['text':' Optional convolutional layer after patches.','line_number':3109,'multiline':False]['text':' Let's assume that the number of resolutions (in the context of image preprocessing)','line_number':3116,'multiline':False]['text':' of the input data is 2 or 3 depending on whether we are processing image or video respectively.','line_number':3117,'multiline':False]['text':' In this case, for convenience, we will declare is_temporal variable,','line_number':3118,'multiline':False]['text':' which will show whether the data has a temporal dimension or not.','line_number':3119,'multiline':False]['text':' position embedding','line_number':3122,'multiline':False]['text':' inputs','line_number':3130,'multiline':False]['text':' Flatten input features to a 1D index dimension if necessary.','line_number':3158,'multiline':False]['text':' Construct the position encoding.','line_number':3162,'multiline':False]['text':' Optionally project them to a target dimension.','line_number':3168,'multiline':False]['text':' Reshape pos to match the input feature shape','line_number':3172,'multiline':False]['text':' if the network takes non-1D inputs','line_number':3173,'multiline':False]['text':' Convnet image featurization.','line_number':3184,'multiline':False]['text':' Downsamples spatially by a factor of 4','line_number':3185,'multiline':False]['text':' map inputs to self.out_channels','line_number':3189,'multiline':False]['text':' if requested, downsamples in the crudest way','line_number':3193,'multiline':False]['text':' Space2depth featurization.','line_number':3204,'multiline':False]['text':' Video: B x T x C x H x W','line_number':3205,'multiline':False]['text':' for flow','line_number':3211,'multiline':False]['text':' Optionally apply conv layer.','line_number':3214,'multiline':False]['text':' move channels to last dimension, as the _build_network_inputs method below expects this','line_number':3218,'multiline':False]['text':' Size for each modality, only needed for multimodal','line_number':3227,'multiline':False]['text':' Add a dummy index dimension.','line_number':3250,'multiline':False]['text':' No position encodings, so the 1st (input) and 3rd (inputs_without_pos)','line_number':3253,'multiline':False]['text':' outputs are identical.','line_number':3254,'multiline':False]['text':' Position embeddings','line_number':3306,'multiline':False]['text':' position embedding','line_number':3316,'multiline':False]['text':' Construct the position encoding.','line_number':3330,'multiline':False]['text':' Optionally project them to a target dimension.','line_number':3336,'multiline':False]['text':' Size for each modality, only needed for multimodal','line_number':3350,'multiline':False]['text':' preprocess each modality using the respective preprocessor.','line_number':3405,'multiline':False]['text':' pad to the same common_channel_size.','line_number':3410,'multiline':False]['text':' mask if required','line_number':3420,'multiline':False]['text':' Apply a predictable ordering to the modalities','line_number':3431,'multiline':False]['text':' Finally, concatenate along the time dimension','line_number':3434,'multiline':False]