['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all Llava models at https://huggingface.co/models?filter=llava','line_number':45,'multiline':False]['text':' Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->Llava','line_number':50,'multiline':False]['text':' important: this ported version of Llava isn't meant for training from scratch - only','line_number':136,'multiline':False]['text':' inference and fine-tuning - so the proper init weights code has been removed - the original codebase','line_number':137,'multiline':False]['text':' https://github.com/haotian-liu/LLaVA/tree/main/llava should serve for that purpose','line_number':138,'multiline':False]['text':' update vocab size','line_number':264,'multiline':False]['text':' 1. Create a mask to know where special image tokens are','line_number':276,'multiline':False]['text':' Compute the maximum embed dimension','line_number':279,'multiline':False]['text':' 2. Compute the positions where text should be written','line_number':283,'multiline':False]['text':' Calculate new positions for text tokens in merged image-text sequence.','line_number':284,'multiline':False]['text':' `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.','line_number':285,'multiline':False]['text':' `torch.cumsum` computes how each image token shifts subsequent text token positions.','line_number':286,'multiline':False]['text':' - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.','line_number':287,'multiline':False]['text':' offset for left padding','line_number':291,'multiline':False]['text':' 3. Create the full embedding, already padded to the maximum position','line_number':294,'multiline':False]['text':' In case the Vision model or the Language model has been offloaded to CPU, we need to manually','line_number':301,'multiline':False]['text':' set the corresponding tensors into their correct target device.','line_number':302,'multiline':False]['text':' 4. Fill the embeddings based on the mask. If we have ["hey" "<image>", "how", "are"]','line_number':311,'multiline':False]['text':' we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features','line_number':312,'multiline':False]['text':' 5. Fill the embeddings corresponding to the images. Anything that is still zeros needs filling','line_number':316,'multiline':False]['text':' 1. Extra the input embeddings','line_number':395,'multiline':False]['text':' 2. Merge text and images','line_number':398,'multiline':False]['text':' this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.','line_number':401,'multiline':False]['text':' In case input_ids.shape[1] == 1 & pixel_values==None & past_key_values != None, we are in the case of','line_number':420,'multiline':False]['text':' generation with cache','line_number':421,'multiline':False]['text':' Retrieve the first layer to inspect the logits and mask out the hidden states','line_number':423,'multiline':False]['text':' that are set to 0','line_number':424,'multiline':False]['text':' Get the target length','line_number':427,'multiline':False]['text':' Zero-out the places where we don't need to attend','line_number':436,'multiline':False]['text':' Shift so that tokens < n predict n','line_number':457,'multiline':False]['text':' Flatten the tokens','line_number':465,'multiline':False]['text':' Keep only the unprocessed tokens:','line_number':493,'multiline':False]['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':494,'multiline':False]['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':495,'multiline':False]['text':' input)','line_number':496,'multiline':False]['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':499,'multiline':False]['text':' input_ids based on the past_length.','line_number':500,'multiline':False]['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':503,'multiline':False]['text':' If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the','line_number':506,'multiline':False]['text':' older attention values, as their corresponding values are not part of the input.','line_number':507,'multiline':False]['text':' create position_ids on the fly for batch generation','line_number':513,'multiline':False]['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':519,'multiline':False]