['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 IBM & Hugging Face. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all PatchTST models at https://huggingface.co/models?filter=patchtst','line_number':38,'multiline':False]['text':' Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PatchTST','line_number':42,'multiline':False]['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':91,'multiline':False]['text':' for the decoder','line_number':92,'multiline':False]['text':' get query proj','line_number':97,'multiline':False]['text':' get key, value proj','line_number':99,'multiline':False]['text':' `past_key_value[0].shape[2] == key_value_states.shape[1]`','line_number':100,'multiline':False]['text':' is checking that the `sequence_length` of the `past_key_value` is the same as','line_number':101,'multiline':False]['text':' the provided `key_value_states` to support prefix tuning','line_number':102,'multiline':False]['text':' reuse k,v, cross_attentions','line_number':108,'multiline':False]['text':' cross_attentions','line_number':112,'multiline':False]['text':' reuse k, v, self_attention','line_number':116,'multiline':False]['text':' self_attention','line_number':122,'multiline':False]['text':' if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.','line_number':127,'multiline':False]['text':' Further calls to cross_attention layer can then reuse all cross-attention','line_number':128,'multiline':False]['text':' key/value_states (first "if" case)','line_number':129,'multiline':False]['text':' if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of','line_number':130,'multiline':False]['text':' all previous decoder key/value_states. Further calls to uni-directional self-attention','line_number':131,'multiline':False]['text':' can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)','line_number':132,'multiline':False]['text':' if encoder bi-directional self-attention `past_key_value` is always `None`','line_number':133,'multiline':False]['text':' this operation is a bit awkward, but it's required to','line_number':170,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':171,'multiline':False]['text':' In order to do so, attn_weights have to be reshaped','line_number':172,'multiline':False]['text':' twice and have to be reused in the following','line_number':173,'multiline':False]['text':' Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be','line_number':192,'multiline':False]['text':' partitioned across GPUs when using tensor-parallelism.','line_number':193,'multiline':False]['text':' output: (batch_size, d_model, sequence_length)','line_number':218,'multiline':False]['text':' noise in [0, 1], bs x 1 x  L','line_number':258,'multiline':False]['text':' bs x num_channels x time','line_number':259,'multiline':False]['text':' noise in [0, 1], bs x num_channels x L','line_number':261,'multiline':False]['text':' mask: [bs x num_channels x num_patch]','line_number':264,'multiline':False]['text':' sort noise for each sample','line_number':268,'multiline':False]['text':' ascend: small is keep, large is remove','line_number':269,'multiline':False]['text':' ids_restore: [bs x num_channels x L]','line_number':270,'multiline':False]['text':' mask: [bs x num_channels x num_patches x patch_length]','line_number':273,'multiline':False]['text':' mask: [bs x num_channels x num_patch x patch_len]','line_number':341,'multiline':False]['text':' get the number of patches','line_number':369,'multiline':False]['text':' output: [bs x new_sequence_length x num_channels]','line_number':388,'multiline':False]['text':' output: [bs x num_patches x num_input_channels x patch_length]','line_number':390,'multiline':False]['text':' output: [bs x num_input_channels x num_patches x patch_length]','line_number':392,'multiline':False]['text':' mask: [bs x num_input_channels x num_patch]','line_number':452,'multiline':False]['text':' Multi-Head attention','line_number':466,'multiline':False]['text':' Add & Norm of the sublayer 1','line_number':473,'multiline':False]['text':' Add & Norm of the sublayer 2','line_number':482,'multiline':False]['text':' Position-wise Feed-Forward','line_number':492,'multiline':False]['text':' Add & Norm of sublayer 3','line_number':500,'multiline':False]['text':' First sublayer: attention across time','line_number':524,'multiline':False]['text':' hidden_states: [(bs*num_channels) x sequence_length x d_model]','line_number':525,'multiline':False]['text':'# Norm and Multi-Head attention and Add residual connection','line_number':529,'multiline':False]['text':' Add: residual connection with residual dropout','line_number':533,'multiline':False]['text':'# Multi-Head attention and Add residual connection and Norm - Standard Transformer from BERT','line_number':536,'multiline':False]['text':' hidden_states: [(bs*num_channels) x sequence_length x d_model]','line_number':540,'multiline':False]['text':' hidden_state: [bs x num_channels x sequence_length x d_model]','line_number':543,'multiline':False]['text':' second sublayer: attention across variable at any given time','line_number':546,'multiline':False]['text':' hidden_state: [bs x sequence_length x num_channels x d_model]','line_number':548,'multiline':False]['text':' hidden_state: [(bs*sequence_length) x num_channels x d_model]','line_number':550,'multiline':False]['text':'# Norm and Multi-Head attention and Add residual connection','line_number':553,'multiline':False]['text':' Add: residual connection with residual dropout','line_number':557,'multiline':False]['text':'# Multi-Head attention and Add residual connection and Norm','line_number':560,'multiline':False]['text':' hidden_states: [(bs*sequence_length) x num_channels x d_model]','line_number':564,'multiline':False]['text':' Reshape hidden state','line_number':567,'multiline':False]['text':' hidden_state: [bs x sequence_length x num_channels x d_model]','line_number':568,'multiline':False]['text':' hidden_state: [bs x num_channels x sequence_length x d_model]','line_number':570,'multiline':False]['text':' Third sublayer: mixing across hidden','line_number':573,'multiline':False]['text':' hidden_state: [(batch_size*num_channels) x sequence_length x d_model]','line_number':574,'multiline':False]['text':'# Norm and Position-wise Feed-Forward and Add residual connection','line_number':577,'multiline':False]['text':' Add: residual connection with residual dropout','line_number':578,'multiline':False]['text':'# Position-wise Feed-Forward and Add residual connection and Norm','line_number':581,'multiline':False]['text':' Add: residual connection with residual dropout','line_number':582,'multiline':False]['text':' [bs x num_channels x sequence_length x d_model]','line_number':585,'multiline':False]['text':' initialize cls_token','line_number':606,'multiline':False]['text':' initialize positional encoding','line_number':609,'multiline':False]['text':' Input encoding: projection of feature vectors onto a d-dim vector space','line_number':633,'multiline':False]['text':' Input encoding','line_number':649,'multiline':False]['text':' x: [bs x num_channels  x num_patches x d_model]','line_number':657,'multiline':False]['text':' cls_token: [1 x num_input_channels x 1 x d_model]','line_number':674,'multiline':False]['text':' postional encoding: [num_patches x d_model]','line_number':677,'multiline':False]['text':' Positional dropout','line_number':679,'multiline':False]['text':' Positional encoding','line_number':686,'multiline':False]['text':' patch_input: [bs x num_channels x num_patches x d_model]','line_number':706,'multiline':False]['text':' append cls token where cls_token: [1 x num_channels x 1 x d_model]','line_number':708,'multiline':False]['text':' get the same copy of cls_token for all the samples in batch: [bs x num_channels x 1 x d_model]','line_number':710,'multiline':False]['text':' hidden_state: [bs x num_channels x (num_patches+1) x d_model]','line_number':712,'multiline':False]['text':' hidden_state: [bs x num_channels x num_patches x d_model]','line_number':715,'multiline':False]['text':' Input embedding: projection of feature vectors onto a d-dim vector space','line_number':729,'multiline':False]['text':' Positional encoding','line_number':731,'multiline':False]['text':' Encoder','line_number':733,'multiline':False]['text':' Initialize weights and apply final processing','line_number':736,'multiline':False]['text':' Input embedding','line_number':760,'multiline':False]['text':' Positional encoding','line_number':762,'multiline':False]['text':' get hidden state. hidden_state shape is [bs x num_channels x num_patches x d_model]','line_number':772,'multiline':False]['text':' or [bs x num_channels x (num_patches+1) x d_model] if use cls_token','line_number':773,'multiline':False]['text':' append attention matrix at each layer','line_number':775,'multiline':False]['text':' return past_values, hidden_states','line_number':778,'multiline':False]['text':' Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll','line_number':967,'multiline':False]['text':' Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average','line_number':975,'multiline':False]['text':' Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesStdScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST','line_number':1000,'multiline':False]['text':' Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesMeanScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST','line_number':1036,'multiline':False]['text':' If `default_scale` is provided, we use it, otherwise we use the scale','line_number':1069,'multiline':False]['text':' of the batch.','line_number':1070,'multiline':False]['text':' apply default scale where there are no observations','line_number':1078,'multiline':False]['text':' ensure the scale is at least `self.minimum_scale`','line_number':1081,'multiline':False]['text':' Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesNOPScaler with TimeSeriesTransformer->PatchTST,TimeSeries->PatchTST','line_number':1091,'multiline':False]['text':' get num_patches information from PatchTSTPatchify','line_number':1158,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1167,'multiline':False]['text':' x: tensor [bs x sequence_length x num_input_channels]','line_number':1233,'multiline':False]['text':' patched_values: [bs x num_input_channels x num_patches x patch_length] for pretrain','line_number':1236,'multiline':False]['text':' [bs x num_channels x num_patches x patch_length]','line_number':1285,'multiline':False]['text':' remove the first cls token','line_number':1287,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1303,'multiline':False]['text':' past_values: [bs x num_channels x num_patches x d_model] or','line_number':1377,'multiline':False]['text':' [bs x num_channels x (num_patches+1) x d_model] if use cls_token','line_number':1378,'multiline':False]['text':' last_hidden_state: [bs x num_channels x num_patches x patch_length] or','line_number':1387,'multiline':False]['text':' [bs x num_channels x (num_patches+1) x patch_length] if use cls_token','line_number':1388,'multiline':False]['text':' calculate masked_loss','line_number':1391,'multiline':False]['text':' use the first output token, pooled_embedding: bs x num_channels x d_model','line_number':1426,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1429,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1432,'multiline':False]['text':' pooled_embedding: bs x num_channels * d_model','line_number':1436,'multiline':False]['text':' output: bs x n_classes','line_number':1438,'multiline':False]['text':' Turn off masking','line_number':1451,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1459,'multiline':False]['text':' if each channel has its own head','line_number':1562,'multiline':False]['text':' use linear head','line_number':1569,'multiline':False]['text':' use distribution head','line_number':1572,'multiline':False]['text':' all the channels share the same head','line_number':1576,'multiline':False]['text':' use linear head','line_number':1579,'multiline':False]['text':' use distribution head','line_number':1582,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1597,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1601,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1604,'multiline':False]['text':' pooled_embedding: [bs x num_channels x num_patches x d_model]','line_number':1607,'multiline':False]['text':' pooled_embedding: [bs x (d_model * num_patches)] or [bs x d_model)]','line_number':1613,'multiline':False]['text':' pooled_embedding: [bs x forecast_len]','line_number':1616,'multiline':False]['text':'  or tuple ([bs x forecast_len], [bs x forecast_len]) if using distribution head','line_number':1617,'multiline':False]['text':' output: [bs x num_channels x forecast_len]','line_number':1620,'multiline':False]['text':' pooled_embedding: [bs x num_channels x (d_model * num_patches)] or [bs x num_channels x d_model)]','line_number':1623,'multiline':False]['text':' output: [bs x num_channels x forecast_len] or','line_number':1626,'multiline':False]['text':' tuple ([bs x num_channels x forecast_len], [bs x num_channels x forecast_len]) if using distribution head','line_number':1627,'multiline':False]['text':' output: ([bs x forecast_len x num_channels], [bs x forecast_len x num_channels])','line_number':1631,'multiline':False]['text':' [bs x forecast_len x num_channels]','line_number':1634,'multiline':False]['text':' Turn off masking','line_number':1646,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1669,'multiline':False]['text':' get model output','line_number':1735,'multiline':False]['text':' get output head','line_number':1743,'multiline':False]['text':' take average of the loss','line_number':1759,'multiline':False]['text':' get number of samples','line_number':1804,'multiline':False]['text':' get model output','line_number':1807,'multiline':False]['text':' get distribution','line_number':1815,'multiline':False]['text':' get samples: list of [bs x forecast_len x num_channels]','line_number':1819,'multiline':False]['text':' samples: [bs x num_samples x forecast_len x num_channels]','line_number':1821,'multiline':False]['text':' use the first output token, pooled_embedding: [bs x num_channels x d_model]','line_number':1862,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1865,'multiline':False]['text':' pooled_embedding: [bs x num_channels x d_model]','line_number':1868,'multiline':False]['text':' flatten the input','line_number':1872,'multiline':False]['text':' pooled_embedding: bs x (num_channels * d_model)','line_number':1873,'multiline':False]['text':' projection','line_number':1875,'multiline':False]['text':' output: bs x output_dim or a tuple of this shape for distribution head','line_number':1876,'multiline':False]['text':' apply sigmoid to bound the output if required','line_number':1878,'multiline':False]['text':' linear head','line_number':1879,'multiline':False]['text':' Turn off masking','line_number':1892,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1912,'multiline':False]['text':' get output head. y_hat is of shape [bs x num_targets] or tuple of this shape','line_number':1970,'multiline':False]['text':' take average of the loss','line_number':1978,'multiline':False]['text':' get number of samples','line_number':2017,'multiline':False]['text':' get model output','line_number':2020,'multiline':False]['text':' get distribution','line_number':2028,'multiline':False]['text':' get samples: list of [bs x num_targets]','line_number':2030,'multiline':False]['text':' samples: [bs x num_samples x num_targets]','line_number':2032,'multiline':False]