['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2023 Google AI and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' See all Owlv2 models at https://huggingface.co/models?filter=owlv2','line_number':49,'multiline':False]['text':' See all OWLv2 models at https://huggingface.co/models?filter=owlv2','line_number':52,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.contrastive_loss with clip->owlv2','line_number':56,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->owlv2','line_number':61,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr._upcast','line_number':106,'multiline':False]['text':' Protects from numerical overflows in multiplications by upcasting to the equivalent higher type','line_number':108,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.box_area','line_number':115,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.box_iou','line_number':132,'multiline':False]['text':' [N,M,2]','line_number':137,'multiline':False]['text':' [N,M,2]','line_number':138,'multiline':False]['text':' [N,M,2]','line_number':140,'multiline':False]['text':' [N,M]','line_number':141,'multiline':False]['text':' Copied from transformers.models.detr.modeling_detr.generalized_box_iou','line_number':149,'multiline':False]['text':' degenerate boxes gives inf / nan results','line_number':157,'multiline':False]['text':' so do an early check','line_number':158,'multiline':False]['text':' [N,M,2]','line_number':168,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput with OwlViT->Owlv2,OWL-ViT->OWLv2','line_number':229,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionEmbeddings with OwlViT->Owlv2','line_number':278,'multiline':False]['text':' shape = [batch_size, num_channels, height, width]','line_number':301,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextEmbeddings with OwlViT->Owlv2','line_number':311,'multiline':False]['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':318,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTAttention with OwlViT->Owlv2','line_number':343,'multiline':False]['text':' get query proj','line_number':380,'multiline':False]['text':' apply the causal_attention_mask first','line_number':399,'multiline':False]['text':' this operation is a bit akward, but it's required to','line_number':420,'multiline':False]['text':' make sure that attn_weights keeps its gradient.','line_number':421,'multiline':False]['text':' In order to do so, attn_weights have to reshaped','line_number':422,'multiline':False]['text':' twice and have to be reused in the following','line_number':423,'multiline':False]['text':' For int8 compatibility, sometimes the `attn_probs` are in `fp32`','line_number':431,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Owlv2','line_number':451,'multiline':False]['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->Owlv2','line_number':467,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTPreTrainedModel with OwlViT->Owlv2,owlvit->owlv2','line_number':518,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTEncoder with OwlViT->Owlv2','line_number':686,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextTransformer with OWLVIT->OWLV2,OwlViT->Owlv2','line_number':776,'multiline':False]['text':' num_samples, seq_len = input_shape  where num_samples = batch_size * num_max_text_queries','line_number':810,'multiline':False]['text':' OWLV2's text model uses causal mask, prepare it here.','line_number':811,'multiline':False]['text':' https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324','line_number':812,'multiline':False]['text':' expand attention_mask','line_number':816,'multiline':False]['text':' [num_samples, seq_len] -> [num_samples, 1, tgt_seq_len, src_seq_len]','line_number':818,'multiline':False]['text':' take features from the end of tokens embedding (end of token is the highest number in each sequence)','line_number':833,'multiline':False]['text':' casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14','line_number':834,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextModel with google/owlvit-base-patch32->google/owlv2-base-patch16, OWLVIT->OWLV2,OwlViT->Owlv2','line_number':851,'multiline':False]['text':' Initialize weights and apply final processing','line_number':858,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':894,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionTransformer with OWLVIT->OWLV2,OwlViT->Owlv2','line_number':904,'multiline':False]['text':' Cast the input to the expected `dtype`','line_number':933,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTVisionModel with OWLVIT->OWLV2,OwlViT->Owlv2,google/owlvit-base-patch32->google/owlv2-base-patch16','line_number':963,'multiline':False]['text':' Initialize weights and apply final processing','line_number':971,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTModel with google/owlvit-base-patch32->google/owlv2-base-patch16-ensemble, OWLVIT->OWLV2,OwlViT->Owlv2,owlvit->owlv2,OWL-ViT->OWLv2','line_number':1015,'multiline':False]['text':' Initialize weights and apply final processing','line_number':1048,'multiline':False]['text':' Use OWLv2 model's config for some fields (if specified) instead of those of vision & text components.','line_number':1076,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':1079,'multiline':False]['text':' Use OWLv2 model's config for some fields (if specified) instead of those of vision & text components.','line_number':1112,'multiline':False]['text':' Use OWLv2 model's config for some fields (if specified) instead of those of vision & text components.','line_number':1162,'multiline':False]['text':' Get embeddings for all text queries in all batch samples','line_number':1176,'multiline':False]['text':' normalized features','line_number':1190,'multiline':False]['text':' cosine similarity as logits and set it on the correct device','line_number':1194,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTBoxPredictionHead with OwlViT->Owlv2','line_number':1230,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTClassPredictionHead with OwlViT->Owlv2','line_number':1250,'multiline':False]['text':' Normalize image and text features','line_number':1276,'multiline':False]['text':' Get class predictions','line_number':1280,'multiline':False]['text':' Apply a learnable shift and scale to logits','line_number':1283,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.normalize_grid_corner_coordinates','line_number':1314,'multiline':False]['text':' Computes normalized xy corner coordinates from feature_map.','line_number':1316,'multiline':False]['text':' Flatten (h, w, 2) -> (h*w, 2)','line_number':1328,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.compute_box_bias','line_number':1350,'multiline':False]['text':' The box center is biased to its position on the feature grid','line_number':1352,'multiline':False]['text':' Unnormalize xy','line_number':1356,'multiline':False]['text':' The box size is biased to the patch size','line_number':1359,'multiline':False]['text':' Compute box bias','line_number':1363,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.box_predictor','line_number':1367,'multiline':False]['text':' Bounding box detection head [batch_size, num_boxes, 4].','line_number':1383,'multiline':False]['text':' Compute the location of each token on the grid and use it to compute a bias for the bbox prediction','line_number':1386,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.class_predictor','line_number':1391,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.image_text_embedder with owlvit->owlv2','line_number':1411,'multiline':False]['text':' Encode text and image','line_number':1420,'multiline':False]['text':' Get image embeddings','line_number':1430,'multiline':False]['text':' Resize class token','line_number':1434,'multiline':False]['text':' Merge image embedding with class tokens','line_number':1438,'multiline':False]['text':' Resize to [batch_size, num_patches, num_patches, hidden_size]','line_number':1442,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.image_embedder with owlvit->owlv2, OwlViTModel->Owlv2Model','line_number':1454,'multiline':False]['text':' Get Owlv2Model vision embeddings (same as CLIP)','line_number':1461,'multiline':False]['text':' Apply post_layernorm to last_hidden_state, return non-projected output','line_number':1464,'multiline':False]['text':' Resize class token','line_number':1468,'multiline':False]['text':' Merge image embedding with class tokens','line_number':1472,'multiline':False]['text':' Resize to [batch_size, num_patches, num_patches, hidden_size]','line_number':1476,'multiline':False]['text':' Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.embed_image_query','line_number':1487,'multiline':False]['text':' Loop over query images','line_number':1495,'multiline':False]['text':' If there are no overlapping boxes, fall back to generalized IoU','line_number':1505,'multiline':False]['text':' Use an adaptive threshold to include all boxes within 80% of the best IoU','line_number':1509,'multiline':False]['text':' Compute feature maps for the input and query images','line_number':1608,'multiline':False]['text':' Get top class embedding and best box index for each query image in batch','line_number':1621,'multiline':False]['text':' Predict object classes [batch_size, num_patches, num_queries+1]','line_number':1624,'multiline':False]['text':' Predict object boxes','line_number':1627,'multiline':False]['text':' Embed images and text queries','line_number':1724,'multiline':False]['text':' Text and vision model outputs','line_number':1733,'multiline':False]['text':' Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]','line_number':1740,'multiline':False]['text':' If first token is 0, then this is a padded query [batch_size, num_queries].','line_number':1744,'multiline':False]['text':' Predict object classes [batch_size, num_patches, num_queries+1]','line_number':1748,'multiline':False]['text':' Predict objectness','line_number':1751,'multiline':False]['text':' Predict object boxes','line_number':1754,'multiline':False]