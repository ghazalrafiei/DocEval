['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2022 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode','line_number':51,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.get_pairs','line_number':79,'multiline':False]['text':' language code lookup by name, with a few language aliases','line_number':197,'multiline':False]['text':' how to handle errors in decoding','line_number':303,'multiline':False]['text':' Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions','line_number':319,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe with GPT2 -> Whisper','line_number':346,'multiline':False]['text':' Copied from transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.build_inputs_with_special_tokens','line_number':447,'multiline':False]['text':' We don't expect to process pairs, but leave the pair logic for API consistency','line_number':452,'multiline':False]['text':' Copied from transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.get_special_tokens_mask','line_number':455,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize with GPT2 -> Whisper','line_number':486,'multiline':False]['text':' Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)','line_number':493,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id with GPT2 -> Whisper','line_number':497,'multiline':False]['text':' either there are no timestamps or there are no consecutive ones','line_number':564,'multiline':False]['text':' we add the final timestamp if it is not already in the list','line_number':567,'multiline':False]['text':' strip timestamp tokens from the text output','line_number':576,'multiline':False]['text':' legacy method to decode timestamps when not included in the tokenizer vocabulary','line_number':688,'multiline':False]['text':' retrieve offsets','line_number':695,'multiline':False]['text':' To avoid mixing byte-level and unicode for byte-level BPT','line_number':713,'multiline':False]['text':' we need to build string separately for added tokens and byte-level tokens','line_number':714,'multiline':False]['text':' cf. https://github.com/huggingface/transformers/issues/1133','line_number':715,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string with GPT2 -> Whisper','line_number':742,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.prepare_for_tokenization with GPT2 -> Whisper','line_number':787,'multiline':False]['text':' Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.default_chat_template','line_number':795,'multiline':False]['text':' prefix tokens are of the form: <|startoftranscript|> <|lang_id|> <|task|> <|notimestamps|>','line_number':810,'multiline':False]['text':' we don't want to force the bos token at position 1, as this is the starting token','line_number':811,'multiline':False]['text':' when we generate, so we slice the prefix tokens to: <|lang_id|> <|task|> <|notimestamps|>','line_number':812,'multiline':False]['text':' to get the forced tokens','line_number':813,'multiline':False]['text':' Check for special tokens','line_number':831,'multiline':False]['text':' =========== Overview ============','line_number':859,'multiline':False]['text':' - iterate over all outputs','line_number':860,'multiline':False]['text':' - all tokens within output','line_number':861,'multiline':False]['text':' - Each token can be','line_number':862,'multiline':False]['text':'   - language token','line_number':863,'multiline':False]['text':'   - special token','line_number':864,'multiline':False]['text':'   - timestamp token','line_number':865,'multiline':False]['text':'   - text token','line_number':866,'multiline':False]['text':' - We accumulate the text tokens.','line_number':867,'multiline':False]['text':' - We split on end timestamps','line_number':868,'multiline':False]['text':' - Lots of complexity comes from stride and timestamps','line_number':869,'multiline':False]['text':' Welcome to the state machine !','line_number':876,'multiline':False]['text':' - iterate over all outputs','line_number':887,'multiline':False]['text':' We can drop everything to Python list, it's going to make','line_number':889,'multiline':False]['text':' our lives easier','line_number':890,'multiline':False]['text':' Those keep track of timestamps within strides','line_number':895,'multiline':False]['text':' Which need to be skipped and resolve all tokens in a single','line_number':896,'multiline':False]['text':' chunk.','line_number':897,'multiline':False]['text':' Offset the timings to account for the other `model_outputs`.','line_number':903,'multiline':False]['text':' Keeping track of timestamps within strides','line_number':907,'multiline':False]['text':' We're going to NOT split on those, and delay until we're','line_number':908,'multiline':False]['text':' out of BOTH stride. Otherwise lots of issues occur and','line_number':909,'multiline':False]['text':' corner cases','line_number':910,'multiline':False]['text':' There can be several token in the right stride','line_number':916,'multiline':False]['text':' But the last one is ALWAYS going to be skipped','line_number':917,'multiline':False]['text':' - all tokens within output','line_number':928,'multiline':False]['text':' 4 possible states for each token','line_number':930,'multiline':False]['text':' - 1/ Language code','line_number':931,'multiline':False]['text':' - 2/ all other special tokens (which we ignore)','line_number':932,'multiline':False]['text':' - 3/ Timestamp','line_number':933,'multiline':False]['text':' - 4/ Regular text','line_number':934,'multiline':False]['text':' Either language code or other','line_number':936,'multiline':False]['text':' Removing outer shell <|XX|>','line_number':938,'multiline':False]['text':' 1/ Indeed some language','line_number':942,'multiline':False]['text':' TODO Handle when language is different from the previous','line_number':943,'multiline':False]['text':' one, and we cannot use timestamped tokens to create chunks','line_number':944,'multiline':False]['text':' Flush all our temporary context','line_number':952,'multiline':False]['text':' 2/ This is a regular special token, ignoring it','line_number':959,'multiline':False]['text':' 3/ Timestamp token','line_number':962,'multiline':False]['text':' Whisper outputted a timestamp token, but it falls within','line_number':966,'multiline':False]['text':' our stride, so we're going to skip it for the time being','line_number':967,'multiline':False]['text':' and resolve this later','line_number':968,'multiline':False]['text':' Skip is necessary because timestamp tokens always come','line_number':969,'multiline':False]['text':' by pair, so we need to skip the next one too (which would mark the start of another chunk).','line_number':970,'multiline':False]['text':' This is the end of the timestamp chunk','line_number':977,'multiline':False]['text':' This is a bug in timestamp token output','line_number':979,'multiline':False]['text':' where we're taking the duplicate token','line_number':980,'multiline':False]['text':' as a stop where it should be a start.','line_number':981,'multiline':False]['text':' This is an issue in the underlying model output','line_number':982,'multiline':False]['text':' Let's just skip it so it becomes de-factor','line_number':983,'multiline':False]['text':' a start agin','line_number':984,'multiline':False]['text':' Handling merges.','line_number':988,'multiline':False]['text':' Flush all our temporary context','line_number':1003,'multiline':False]['text':' 4/ Regular token','line_number':1010,'multiline':False]['text':' We just append to the list of all tokens so we can handle','line_number':1011,'multiline':False]['text':' merges later and decode into text.','line_number':1012,'multiline':False]['text':' should never happen','line_number':1019,'multiline':False]['text':' Leftover tokens','line_number':1025,'multiline':False]['text':' Happens when we don't use timestamps','line_number':1043,'multiline':False]['text':' Preparing and cleaning up the pipeline output','line_number':1055,'multiline':False]['text':' It would be much harder to do O(n) because of fault tolerance.','line_number':1079,'multiline':False]['text':' We actually have a really good property which is that the total sequence','line_number':1080,'multiline':False]['text':' MUST be those subsequences in order.','line_number':1081,'multiline':False]['text':' If token_timestamp_sequences is provided, will split those sequences in','line_number':1082,'multiline':False]['text':' exactly the same way.','line_number':1083,'multiline':False]['text':' index = 0','line_number':1094,'multiline':False]['text':' Here we're sliding matches','line_number':1097,'multiline':False]['text':' [a, b, c, d]','line_number':1098,'multiline':False]['text':'          [c, d, f]','line_number':1099,'multiline':False]['text':' =        [c] == [d]','line_number':1100,'multiline':False]['text':'','line_number':1101,'multiline':False]['text':' [a, b, c, d]','line_number':1102,'multiline':False]['text':'       [c, d, f]','line_number':1103,'multiline':False]['text':' =     [c, d] == [c, d]','line_number':1104,'multiline':False]['text':'','line_number':1105,'multiline':False]['text':'','line_number':1106,'multiline':False]['text':' [a, b, c, d]','line_number':1107,'multiline':False]['text':'    [c, d, f]','line_number':1108,'multiline':False]['text':'','line_number':1109,'multiline':False]['text':' =  [b, c, d] == [c, d, f]','line_number':1110,'multiline':False]['text':'','line_number':1111,'multiline':False]['text':' [a, b, c, d]','line_number':1112,'multiline':False]['text':' [c, d, f]','line_number':1113,'multiline':False]['text':'','line_number':1114,'multiline':False]['text':' [a, b, c] == [c, d, f]','line_number':1115,'multiline':False]['text':'','line_number':1116,'multiline':False]['text':' [a, b, c, d]','line_number':1117,'multiline':False]['text':' [d, f]','line_number':1118,'multiline':False]['text':'','line_number':1119,'multiline':False]['text':' [a, b] == [d, f]','line_number':1120,'multiline':False]['text':'','line_number':1121,'multiline':False]['text':' [a, b, c, d]','line_number':1122,'multiline':False]['text':' [f]','line_number':1123,'multiline':False]['text':'','line_number':1124,'multiline':False]['text':' [a] == [f]','line_number':1125,'multiline':False]['text':' epsilon to favor long perfect matches','line_number':1128,'multiline':False]['text':' Slightly convoluted because we don't want out of bound indices','line_number':1131,'multiline':False]['text':' This will be necessary for a small conflict resolution optimization','line_number':1132,'multiline':False]['text':' later','line_number':1133,'multiline':False]['text':' We can only match subsequences of the same size.','line_number':1142,'multiline':False]['text':' This is a small conflict optimization since those sequences overlap','line_number':1156,'multiline':False]['text':' in audio.','line_number':1157,'multiline':False]['text':' We're going to give more confidence to the left sequence','line_number':1158,'multiline':False]['text':' for the left of the overlap,','line_number':1159,'multiline':False]['text':' and to the right of the sequence, for the right of the overlap','line_number':1160,'multiline':False]['text':' These languages don't typically use spaces.','line_number':1212,'multiline':False]['text':' prepend punctuations','line_number':1278,'multiline':False]['text':' append punctuations','line_number':1293,'multiline':False]['text':' remove elements that are now empty','line_number':1308,'multiline':False]