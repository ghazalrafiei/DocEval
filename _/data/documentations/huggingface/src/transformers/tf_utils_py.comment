['text':' Copyright 2022 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' TODO: When the issue linked above gets sorted, add a check on TF version here and use the original function if','line_number':68,'multiline':False]['text':' it has the fix. After we drop the support for unfixed versions, remove this function.','line_number':69,'multiline':False]['text':' This is a very simplified functional layernorm, designed to duplicate','line_number':74,'multiline':False]['text':' the functionality of PyTorch nn.functional.layer_norm when this is needed to port','line_number':75,'multiline':False]['text':' models in Transformers.','line_number':76,'multiline':False]['text':' Get mean and variance on the axis to be normalized','line_number':81,'multiline':False]['text':' Reshape scale and weight to have the same rank as inputs, but with 1 dimensions','line_number':85,'multiline':False]['text':' on every dimension except axis','line_number':86,'multiline':False]['text':' Compute layer normalization using the batch_normalization','line_number':92,'multiline':False]['text':' function.','line_number':93,'multiline':False]['text':' Replicates the behavior of torch.flatten in TF','line_number':106,'multiline':False]['text':' If end_dim or start_dim is negative, count them from the end','line_number':108,'multiline':False]['text':' Catches stray NumPy inputs','line_number':134,'multiline':False]['text':' T5 has a mask that can compare sequence ids, we can simulate this here with this transposition','line_number':139,'multiline':False]['text':' Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow','line_number':140,'multiline':False]['text':' /transformer/transformer_layers.py#L270','line_number':141,'multiline':False]['text':' encoder_extended_attention_mask = (encoder_extended_attention_mask ==','line_number':142,'multiline':False]['text':' encoder_extended_attention_mask.transpose(-1, -2))','line_number':143,'multiline':False]['text':' Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`','line_number':188,'multiline':False]['text':' because in that case even chunking the array would not make the saving','line_number':189,'multiline':False]['text':' possible.','line_number':190,'multiline':False]['text':' Expecting this to never be true.','line_number':193,'multiline':False]['text':' This will never loop forever thanks to the test above.','line_number':206,'multiline':False]