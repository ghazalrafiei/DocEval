['text':' coding=utf-8','line_number':1,'multiline':False]['text':' Copyright 2021 The HuggingFace Inc. team.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]['text':' limitations under the License.','line_number':14,'multiline':False]['text':'####################','line_number':42,'multiline':False]['text':' PyTorch => Flax #','line_number':43,'multiline':False]['text':'####################','line_number':44,'multiline':False]['text':' noqa: F401','line_number':52,'multiline':False]['text':' model is sharded and pytorch_checkpoint_path already contains the list of .pt shard files','line_number':76,'multiline':False]['text':' layer norm','line_number':93,'multiline':False]['text':' batch norm layer mean','line_number':98,'multiline':False]['text':' batch norm layer var','line_number':103,'multiline':False]['text':' embedding','line_number':108,'multiline':False]['text':' conv layer','line_number':113,'multiline':False]['text':' linear layer','line_number':119,'multiline':False]['text':' old PyTorch layer norm weight','line_number':125,'multiline':False]['text':' old PyTorch layer norm bias','line_number':130,'multiline':False]['text':' New `weight_norm` from https://github.com/huggingface/transformers/pull/24030','line_number':135,'multiline':False]['text':' convert pytorch tensor to numpy','line_number':149,'multiline':False]['text':' numpy currently does not support bfloat16, need to go over float32 in this case to not lose precision','line_number':150,'multiline':False]['text':' noqa: F401','line_number':152,'multiline':False]['text':' use params dict if the model contains batch norm layers','line_number':168,'multiline':False]['text':' add batch_stats keys,values to dict','line_number':175,'multiline':False]['text':' Need to change some parameters name to match Flax names','line_number':189,'multiline':False]['text':' remove base model prefix if necessary','line_number':194,'multiline':False]['text':' Correctly rename weight parameters','line_number':199,'multiline':False]['text':' add model prefix if necessary','line_number':204,'multiline':False]['text':' add batch stats if the model contains batchnorm layers','line_number':216,'multiline':False]['text':' remove num_batches_tracked key','line_number':221,'multiline':False]['text':' also add unexpected weight so that warning is thrown','line_number':226,'multiline':False]['text':' also add unexpected weight so that warning is thrown','line_number':232,'multiline':False]['text':'###########################','line_number':240,'multiline':False]['text':' Sharded Pytorch => Flax #','line_number':241,'multiline':False]['text':'###########################','line_number':242,'multiline':False]['text':' Load the index','line_number':248,'multiline':False]['text':' load using msgpack utils','line_number':251,'multiline':False]['text':' use params dict if the model contains batch norm layers and then add batch_stats keys,values to dict','line_number':257,'multiline':False]['text':' Need to change some parameters name to match Flax names','line_number':273,'multiline':False]['text':' remove base model prefix if necessary','line_number':277,'multiline':False]['text':' Correctly rename weight parameters','line_number':282,'multiline':False]['text':' add model prefix if necessary','line_number':286,'multiline':False]['text':' add batch stats if the model contains batchnorm layers','line_number':298,'multiline':False]['text':' remove num_batches_tracked key','line_number':306,'multiline':False]['text':' also add unexpected weight so that warning is thrown','line_number':311,'multiline':False]['text':' also add unexpected weight so that warning is thrown','line_number':315,'multiline':False]['text':'####################','line_number':320,'multiline':False]['text':' Flax => PyTorch #','line_number':321,'multiline':False]['text':'####################','line_number':322,'multiline':False]['text':' import correct flax class','line_number':330,'multiline':False]['text':' load flax weight dict','line_number':333,'multiline':False]['text':' noqa: F401','line_number':351,'multiline':False]['text':' check if we have bf16 weights','line_number':360,'multiline':False]['text':' convert all weights to fp32 if the are bf16 since torch.from_numpy can-not handle bf16','line_number':363,'multiline':False]['text':' and bf16 is not fully supported in PT yet.','line_number':364,'multiline':False]['text':' keep track of unexpected & missing keys','line_number':383,'multiline':False]['text':' adapt flax_key to prepare for loading from/to base model only','line_number':391,'multiline':False]['text':' rename flax weights to PyTorch format','line_number':397,'multiline':False]['text':' conv layer','line_number':399,'multiline':False]['text':' linear layer','line_number':403,'multiline':False]['text':' adding batch stats from flax batch norm to pt','line_number':409,'multiline':False]['text':' Remove the params/batch_stats header','line_number':416,'multiline':False]['text':' We also need to look at `pt_model_dict` and see if there are keys requiring further transformation.','line_number':420,'multiline':False]['text':' New `weight_norm` from https://github.com/huggingface/transformers/pull/24030','line_number':422,'multiline':False]['text':' add weight to pytorch dict','line_number':445,'multiline':False]['text':' remove from missing keys','line_number':448,'multiline':False]['text':' weight is not expected by PyTorch model','line_number':451,'multiline':False]['text':' re-transform missing_keys to list','line_number':456,'multiline':False]