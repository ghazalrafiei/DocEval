['text':'!/usr/bin/env python','line_number':1,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This a `torch.distributed` diagnostics script that checks that all GPUs in the cluster (one or','line_number':4,'multiline':False]['text':' many nodes) can talk to each other via nccl and allocate gpu memory.','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':' To run first adjust the number of processes and nodes:','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' You may need to add --master_addr $MASTER_ADDR --master_port $MASTER_PORT if using a custom addr:port','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':' You can also use the rdzv API: --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT --rdzv_backend c10d','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' use torch.distributed.launch instead of torch.distributed.run for torch < 1.9','line_number':15,'multiline':False]['text':'','line_number':16,'multiline':False]['text':' If you get a hanging in `barrier` calls you have some network issues, you may try to debug this with:','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':' NCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py','line_number':19,'multiline':False]['text':'','line_number':20,'multiline':False]['text':' which should tell you what's going on behind the scenes.','line_number':21,'multiline':False]['text':'','line_number':22,'multiline':False]['text':'','line_number':23,'multiline':False]['text':' This script can be run via `srun` in the SLURM environment as well. Here is a SLURM script that','line_number':24,'multiline':False]['text':' runs on 2 nodes of 4 gpus per node:','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' #SBATCH --job-name=test-nodes        # name','line_number':27,'multiline':False]['text':' #SBATCH --nodes=2                    # nodes','line_number':28,'multiline':False]['text':' #SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!','line_number':29,'multiline':False]['text':' #SBATCH --cpus-per-task=10           # number of cores per tasks','line_number':30,'multiline':False]['text':' #SBATCH --gres=gpu:4                 # number of gpus','line_number':31,'multiline':False]['text':' #SBATCH --time 0:05:00               # maximum execution time (HH:MM:SS)','line_number':32,'multiline':False]['text':' #SBATCH --output=%x-%j.out           # output file name','line_number':33,'multiline':False]['text':'','line_number':34,'multiline':False]['text':' GPUS_PER_NODE=4','line_number':35,'multiline':False]['text':' MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)','line_number':36,'multiline':False]['text':' MASTER_PORT=6000','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \','line_number':39,'multiline':False]['text':' --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \','line_number':40,'multiline':False]['text':' --master_addr $MASTER_ADDR --master_port $MASTER_PORT \','line_number':41,'multiline':False]['text':' torch-distributed-gpu-test.py'','line_number':42,'multiline':False]['text':'','line_number':43,'multiline':False]['text':' test distributed','line_number':71,'multiline':False]['text':' test cuda is available and can allocate memory','line_number':76,'multiline':False]['text':' global rank','line_number':80,'multiline':False]