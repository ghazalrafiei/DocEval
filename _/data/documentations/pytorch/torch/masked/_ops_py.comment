['text':' A workaround to support both TorchScript and MyPy:','line_number':4,'multiline':False]['text':' The JIT doesn't understand Union, nor torch.dtype here','line_number':19,'multiline':False]['text':' All masked reduction/normalization operations have the same','line_number':26,'multiline':False]['text':' signatures. Here we introduce docstring templates that are applied','line_number':27,'multiline':False]['text':' to docstrings of reduction/normalization functions via','line_number':28,'multiline':False]['text':' _apply_docstring_templates decorator.','line_number':29,'multiline':False]['text':' Expose function as public symbol','line_number':47,'multiline':False]['text':' argument name sufficies separated by double underscore will','line_number':166,'multiline':False]['text':' be removed in the final documentation string.','line_number':167,'multiline':False]['text':' Default example data:','line_number':287,'multiline':False]['text':' add function name to operation names dictionaries','line_number':331,'multiline':False]['text':' one-line representation of a tensor:','line_number':340,'multiline':False]['text':' multi-line representation of a tensor with indent','line_number':344,'multiline':False]['text':' add function name to operation names dictionaries','line_number':371,'multiline':False]['text':' Apply function name info to docstring templates:','line_number':381,'multiline':False]['text':' Apply docstring templates to function doctring:','line_number':392,'multiline':False]['text':' lstrip module name when present','line_number':416,'multiline':False]['text':' Strictly speaking, the identity value of the mean operation','line_number':432,'multiline':False]['text':' is the mean of the input. Since the mean value depends on','line_number':433,'multiline':False]['text':' the dim argument and it may be a non-scalar tensor, we','line_number':434,'multiline':False]['text':' consider the identity value of the mean operation ambiguous.','line_number':435,'multiline':False]['text':' Moreover, the mean value of empty input is undefined.','line_number':436,'multiline':False]['text':' We use NaN for now because the implementation is currently using torch.nanmedian','line_number':445,'multiline':False]['text':' and NaN is the identity for that function since it gets ignored','line_number':446,'multiline':False]['text':' Currently, `dim=()` in reductions operations means "reduce','line_number':458,'multiline':False]['text':' over all dimensions" while in future, it will read "no','line_number':459,'multiline':False]['text':' reduce". See https://github.com/pytorch/pytorch/issues/29137','line_number':460,'multiline':False]['text':' When gh-29137 is resolved, this if-block must be deleted.','line_number':461,'multiline':False]['text':' Flatted N-D indices to 1-D indices','line_number':479,'multiline':False]['text':' Support torch.any with tuple dim argument.','line_number':488,'multiline':False]['text':' Workaround of https://github.com/pytorch/pytorch/issues/56586','line_number':489,'multiline':False]['text':' TODO: eliminate this restriction','line_number':524,'multiline':False]['text':' For set operations on sparse tensor indices, we'll convert','line_number':528,'multiline':False]['text':' multi-dimensional indices to 1-D indices for efficiency.','line_number':529,'multiline':False]['text':' the set of mask flat indices that define masked-in elements:','line_number':537,'multiline':False]['text':' the set of input flat indices of specified and masked-in elements:','line_number':558,'multiline':False]['text':' the indices and values of masked-in elements','line_number':564,'multiline':False]['text':' apply mask to the dense part of the input values:','line_number':569,'multiline':False]['text':' the set of flat indices of unspecified input and masked-in elements:','line_number':576,'multiline':False]['text':' the indices of masked-in zero elements','line_number':581,'multiline':False]['text':' construct result','line_number':585,'multiline':False]['text':' the input is coalesced, hence input_flat_indices are ordered','line_number':588,'multiline':False]['text':' and the result is guaranteed to be coalesced:','line_number':589,'multiline':False]['text':' appending zero elements leads to uncoalesced sparse tensor','line_number':604,'multiline':False]['text':' promote dtype if specified','line_number':630,'multiline':False]['text':' Reduce dense dimensions','line_number':652,'multiline':False]['text':' FIXME: Implement reductions for dense dimensions for ops with non-zero reduction identities','line_number':658,'multiline':False]['text':' Reduce sparse dimensions','line_number':663,'multiline':False]['text':' IndexError: amax(): Expected reduction dim 0 to have non-zero size.','line_number':666,'multiline':False]['text':' sum()/prod() return the reduction identity when dim has size 0 but amax()/amin() do not','line_number':667,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/61901','line_number':668,'multiline':False]['text':' zero out reduced sparse dimensions if keepdim = True','line_number':679,'multiline':False]['text':' ensures that the call to torch.unique folds duplicated indices together while preserving the dimension','line_number':680,'multiline':False]['text':' remove reduced sparse dimensions if keepdim = False','line_number':683,'multiline':False]['text':' Use scatter_reduce to reduce items in the new_values tensor that correspond to the same indices in new_indices','line_number':694,'multiline':False]['text':' lexsort indices and get index tensor for scatter reduction','line_number':696,'multiline':False]['text':' FIXME: temporary workaround for issue with bfloat16/float16 remove when acctype is implemented for scatter_reduce','line_number':705,'multiline':False]['text':' Currently, while sparse CSR is always 2D with no dense dimensions keepdim must be True','line_number':735,'multiline':False]['text':' FIXME: when dense dimensions are implemented for CSR tensors','line_number':736,'multiline':False]['text':' promote dtype if specified','line_number':754,'multiline':False]['text':' all intervals new_crow_indices[i] - new_crow_indices[i-1] are 1','line_number':776,'multiline':False]['text':' except for where crow_indices[i] == crow_indices[i-1] where the interval remains as 0','line_number':777,'multiline':False]['text':' type: ignore[attr-defined]','line_number':787,'multiline':False]['text':' amax and amin do not support dtype kwarg','line_number':794,'multiline':False]['text':' TODO: implement sparse CSR specific where operator for efficiency','line_number':816,'multiline':False]['text':' default mask','line_number':898,'multiline':False]['text':' mask shape must match with input shape','line_number':902,'multiline':False]['text':' Broadcasting of CSR tensors is not implemented. Working','line_number':914,'multiline':False]['text':' around by using COO layout.','line_number':915,'multiline':False]['text':' mask layout must match with input layout','line_number':920,'multiline':False]['text':' sparse mask must be coalesced','line_number':933,'multiline':False]['text':' mask is a boolean tensor','line_number':937,'multiline':False]['text':' lstrip ord argument','line_number':971,'multiline':False]['text':' type: ignore[union-attr]','line_number':1025,'multiline':False]['text':' __doc__ is generated by _apply_docstring_templates decorator','line_number':1040,'multiline':False]['text':' promote integer types to int64 when output dtype is not specified','line_number':1042,'multiline':False]['text':' csr.to(dtype=torch.int64) is not implemented, so','line_number':1051,'multiline':False]['text':' using coo.to on input to ensure the promoted dtype','line_number':1052,'multiline':False]['text':' __doc__ is generated by _apply_docstring_templates decorator','line_number':1093,'multiline':False]['text':' promote integer types to int64 when output dtype is not specified','line_number':1095,'multiline':False]['text':' csr.to(dtype=torch.int64) is not implemented, so','line_number':1104,'multiline':False]['text':' using coo.to on input to ensure the promoted dtype','line_number':1105,'multiline':False]['text':' Workaround https://github.com/pytorch/pytorch/issues/56586','line_number':1122,'multiline':False]['text':' See comment in the sparse_csr branch, the same issue arises for sparse_coo tensors','line_number':1130,'multiline':False]['text':' mask is None corresponds to all-True mask. The','line_number':1139,'multiline':False]['text':' unspecified elements in the CSR tensor correspond to','line_number':1140,'multiline':False]['text':' zero values. Hence, the prod reduction result is','line_number':1141,'multiline':False]['text':' automatically zero unless all elements are specified.','line_number':1142,'multiline':False]['text':' A semi-optimal way to take this into account is to use:','line_number':1143,'multiline':False]['text':'','line_number':1144,'multiline':False]['text':'   masked_prod(csr, ..., mask=None) == torch._sparse_csr_prod(csr, ...) * all(csr.nonzero(), ...)','line_number':1145,'multiline':False]['text':'','line_number':1146,'multiline':False]['text':' but that requires implementing `all` and `nonzero`','line_number':1147,'multiline':False]['text':' support for sparse csr tensors.','line_number':1148,'multiline':False]['text':' See comment in the sparse_csr branch of prod, a similar issue arises here','line_number':1229,'multiline':False]['text':' where unspecified elements along a dimension may need to be reduced with the result','line_number':1230,'multiline':False]['text':' See comment in the sparse_csr branch of prod, a similar issue arises here','line_number':1279,'multiline':False]['text':' where unspecified elements along a dimension may need to be reduced with the result','line_number':1280,'multiline':False]['text':' TODO: compute count analytically','line_number':1381,'multiline':False]['text':' Cannot use _apply_docstring_templates as it is only set up for reductions and normalizations','line_number':1479,'multiline':False]['text':' TODO: compute count analytically','line_number':1605,'multiline':False]['text':' TODO: replace torch.subtract/divide/square/maximum with','line_number':1621,'multiline':False]['text':' masked subtract/divide/square/maximum when these will be','line_number':1622,'multiline':False]['text':' available.','line_number':1623,'multiline':False]['text':' TODO: eliminate mask_input as unnecessary when using masked divide.','line_number':1785,'multiline':False]['text':' TODO: replace torch.maximum with masked maximum when available.','line_number':1789,'multiline':False]['text':' TODO: replace torch.divide with masked divide when available.','line_number':1791,'multiline':False]