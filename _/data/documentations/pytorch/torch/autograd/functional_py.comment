['text':' Utility functions','line_number':9,'multiline':False]['text':' Ensures that inp is a tuple of Tensors','line_number':22,'multiline':False]['text':' Returns whether or not the original inp was a tuple and the tupled version of the input','line_number':23,'multiline':False]['text':' Unpacks a potentially nested tuple of Tensors','line_number':49,'multiline':False]['text':' to_unpack should be a single boolean or a tuple of two booleans.','line_number':50,'multiline':False]['text':' It is used to:','line_number':51,'multiline':False]['text':' - invert _as_tuple when res should match the inp given to _as_tuple','line_number':52,'multiline':False]['text':' - optionally remove nesting of two tuples created by multiple calls to _as_tuple','line_number':53,'multiline':False]['text':' Preprocess the inputs to make sure they require gradient','line_number':67,'multiline':False]['text':' inputs is a tuple of Tensors to preprocess','line_number':68,'multiline':False]['text':' create_graph specifies if the user wants gradients to flow back to the Tensors in inputs','line_number':69,'multiline':False]['text':' need_graph specifies if we internally want gradients to flow back to the Tensors in res','line_number':70,'multiline':False]['text':' Note that we *always* create a new Tensor object to be able to see the difference between','line_number':71,'multiline':False]['text':' inputs given as arguments and the same Tensors automatically captured by the user function.','line_number':72,'multiline':False]['text':' Check this issue for more details on how that can happen: https://github.com/pytorch/pytorch/issues/32576','line_number':73,'multiline':False]['text':' Create at least a new Tensor object in a differentiable way','line_number':77,'multiline':False]['text':' Use .view_as() to get a shallow copy','line_number':79,'multiline':False]['text':' We cannot use view for sparse Tensors so we clone','line_number':82,'multiline':False]['text':' Postprocess the generated Tensors to avoid returning Tensors with history when the user did not','line_number':90,'multiline':False]['text':' request it.','line_number':91,'multiline':False]['text':' This assumes that other is the correct shape, and v should match','line_number':102,'multiline':False]['text':' Both are assumed to be tuples of Tensors','line_number':103,'multiline':False]['text':' Used to make all the necessary checks to raise nice errors in strict mode.','line_number':123,'multiline':False]['text':' This can only be reached for grad_inputs.','line_number':131,'multiline':False]['text':' Version of autograd.grad that accepts `None` in outputs and do not compute gradients for them.','line_number':173,'multiline':False]['text':' This has the extra constraint that inputs has to be a tuple','line_number':174,'multiline':False]['text':' No differentiable output, we don't need to call the autograd engine','line_number':189,'multiline':False]['text':' Used to detect None in the grads and depending on the flags, either replace them','line_number':204,'multiline':False]['text':' with Tensors full of 0s of the appropriate size based on the refs or raise an error.','line_number':205,'multiline':False]['text':' strict and create graph allow us to detect when it is appropriate to raise an error','line_number':206,'multiline':False]['text':' stage gives us information of which backward call we consider to give good error message','line_number':207,'multiline':False]['text':' Public API','line_number':258,'multiline':False]['text':' Cleanup objects and return them to the user','line_number':347,'multiline':False]['text':' The backward is linear so the value of grad_outputs is not important as','line_number':438,'multiline':False]['text':' it won't appear in the double backward graph. We only need to ensure that','line_number':439,'multiline':False]['text':' it does not contain inf or nan.','line_number':440,'multiline':False]['text':' Cleanup objects and return them to the user','line_number':460,'multiline':False]['text':' This function:','line_number':472,'multiline':False]['text':' - constructs a N=sum(tensor_numels) standard basis. i.e. an NxN identity matrix.','line_number':473,'multiline':False]['text':' - Splits the identity matrix into chunks with each chunk size determined by `tensor_numels`.','line_number':474,'multiline':False]['text':' - Each chunk corresponds to one tensor. The chunk has the same dtype and','line_number':475,'multiline':False]['text':'   device as the tensor','line_number':476,'multiline':False]['text':'','line_number':477,'multiline':False]['text':' For example, with tensor_numels = [1, 2, 1], this function returns:','line_number':478,'multiline':False]['text':' ( tensor([[1],     tensor([[0, 0],      tensor([[0],','line_number':479,'multiline':False]['text':'           [0],             [1, 0],              [0],','line_number':480,'multiline':False]['text':'           [0],             [0, 1],              [0],','line_number':481,'multiline':False]['text':'           [0]])  ,         [0, 0]])  ,          [1]])  )','line_number':482,'multiline':False]['text':'','line_number':483,'multiline':False]['text':' Precondition: tensor_numels == tuple(tensor.numel() for tensor in tensors)','line_number':484,'multiline':False]['text':' Precondition: tensors always has at least one element.','line_number':485,'multiline':False]['text':'','line_number':486,'multiline':False]['text':' See NOTE: [Computing jacobian with vmap and grad for multiple tensors]','line_number':487,'multiline':False]['text':' for context behind this function. All the pre-conditions are guarded for','line_number':488,'multiline':False]['text':' in torch.autograd.functional.jacobian.','line_number':489,'multiline':False]['text':' See NOTE: [Computing jacobian with vmap and grad for multiple outputs]','line_number':516,'multiline':False]['text':' Step 1: Prepare tangents','line_number':519,'multiline':False]['text':' Step 2: Compute vmap over computation with dual tensors','line_number':522,'multiline':False]['text':' Step 3: for each of the output tangents, split along dim 0','line_number':547,'multiline':False]['text':' We need to transpose the Jacobian because in forward AD, the','line_number':552,'multiline':False]['text':' batch dimension represents that of the inputs','line_number':553,'multiline':False]['text':' noqa: C409','line_number':556,'multiline':False]['text':' Omit [Step 4] because everything is already transposed w/ forward AD','line_number':561,'multiline':False]['text':' NOTE: [Computing jacobian with vmap and grad for multiple outputs]','line_number':688,'multiline':False]['text':'','line_number':689,'multiline':False]['text':' Let's consider f(x) = (x**2, x.sum()) and let x = torch.randn(3).','line_number':690,'multiline':False]['text':' It turns out we can compute the jacobian of this function with a single','line_number':691,'multiline':False]['text':' call to autograd.grad by using vmap over the correct grad_outputs.','line_number':692,'multiline':False]['text':'','line_number':693,'multiline':False]['text':' Firstly, one way to compute the jacobian is to stack x**2 and x.sum()','line_number':694,'multiline':False]['text':' into a 4D vector. E.g., use g(x) = torch.stack([x**2, x.sum()])','line_number':695,'multiline':False]['text':'','line_number':696,'multiline':False]['text':' To get the first row of the jacobian, we call','line_number':697,'multiline':False]['text':' >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([1, 0, 0, 0]))','line_number':698,'multiline':False]['text':' To get the 2nd row of the jacobian, we call','line_number':699,'multiline':False]['text':' >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([0, 1, 0, 0]))','line_number':700,'multiline':False]['text':' and so on.','line_number':701,'multiline':False]['text':'','line_number':702,'multiline':False]['text':' Using vmap, we can vectorize all 4 of these computations into one by','line_number':703,'multiline':False]['text':' passing the standard basis for R^4 as the grad_output.','line_number':704,'multiline':False]['text':' vmap(partial(autograd.grad, g(x), x))(torch.eye(4)).','line_number':705,'multiline':False]['text':'','line_number':706,'multiline':False]['text':' Now, how do we compute the jacobian *without stacking the output*?','line_number':707,'multiline':False]['text':' We can just split the standard basis across the outputs. So to','line_number':708,'multiline':False]['text':' compute the jacobian of f(x), we'd use','line_number':709,'multiline':False]['text':' >>> autograd.grad(f(x), x, grad_outputs=_construct_standard_basis_for(...))','line_number':710,'multiline':False]['text':' The grad_outputs looks like the following:','line_number':711,'multiline':False]['text':' ( torch.tensor([[1, 0, 0],','line_number':712,'multiline':False]['text':'                 [0, 1, 0],','line_number':713,'multiline':False]['text':'                 [0, 0, 1],','line_number':714,'multiline':False]['text':'                 [0, 0, 0]]),','line_number':715,'multiline':False]['text':'   torch.tensor([[0],','line_number':716,'multiline':False]['text':'                 [0],','line_number':717,'multiline':False]['text':'                 [0],','line_number':718,'multiline':False]['text':'                 [1]]) )','line_number':719,'multiline':False]['text':'','line_number':720,'multiline':False]['text':' But we're not done yet!','line_number':721,'multiline':False]['text':' >>> vmap(partial(autograd.grad(f(x), x, grad_outputs=...)))','line_number':722,'multiline':False]['text':' returns a Tensor of shape [4, 3]. We have to remember to split the','line_number':723,'multiline':False]['text':' jacobian of shape [4, 3] into two:','line_number':724,'multiline':False]['text':' - one of shape [3, 3] for the first output','line_number':725,'multiline':False]['text':' - one of shape [   3] for the second output','line_number':726,'multiline':False]['text':' Step 1: Construct grad_outputs by splitting the standard basis','line_number':728,'multiline':False]['text':' Step 2: Call vmap + autograd.grad','line_number':733,'multiline':False]['text':' Step 3: The returned jacobian is one big tensor per input. In this step,','line_number':754,'multiline':False]['text':' we split each Tensor by output.','line_number':755,'multiline':False]['text':' Step 4: Right now, `jacobian` is a List[List[Tensor]].','line_number':766,'multiline':False]['text':' The outer List corresponds to the number of inputs,','line_number':767,'multiline':False]['text':' the inner List corresponds to the number of outputs.','line_number':768,'multiline':False]['text':' We need to exchange the order of these and convert to tuples','line_number':769,'multiline':False]['text':' before returning.','line_number':770,'multiline':False]['text':' mypy complains that expression and variable have different types due to the empty list','line_number':783,'multiline':False]['text':' type: ignore[assignment]','line_number':784,'multiline':False]['text':' type: ignore[operator]','line_number':818,'multiline':False]['text':' _grad_preprocess requires create_graph=True and input to require_grad','line_number':949,'multiline':False]['text':' or else the input will be detached','line_number':950,'multiline':False]