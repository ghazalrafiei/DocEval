['text':' Do an op to force AccumulateGrad lazy creation and get it','line_number':158,'multiline':False]['text':' Note that output_nr default to 0 which is the right value','line_number':161,'multiline':False]['text':' for the AccumulateGrad node.','line_number':162,'multiline':False]['text':' See NOTE: [disabled_error_message invariant]','line_number':354,'multiline':False]['text':' or grad accumulator','line_number':411,'multiline':False]['text':' On the first call, compute the actual nb_calls and buffer','line_number':429,'multiline':False]['text':' type: ignore[attr-defined]','line_number':430,'multiline':False]['text':' NOTE [Allow mutation on tensors saved for backward]','line_number':465,'multiline':False]['text':'','line_number':466,'multiline':False]['text':' 1. Tensor gets saved for backward','line_number':467,'multiline':False]['text':'    - remember the python object id and the version of the tensor','line_number':468,'multiline':False]['text':'    - remember aliasing information (data_ptr of base + version)','line_number':469,'multiline':False]['text':'    - save the original so we control its lifetime','line_number':470,'multiline':False]['text':' 2. Any time a tensor gets in-placed','line_number':471,'multiline':False]['text':'    - for each tensor aliased to it:','line_number':472,'multiline':False]['text':'      - check using its object id and version to see if it has been saved','line_number':473,'multiline':False]['text':'      - if it has been saved, clone it','line_number':474,'multiline':False]['text':'      - delete the reference to the original','line_number':475,'multiline':False]['text':' 3. during backward','line_number':476,'multiline':False]['text':'    - if the clone exists, the tensor must've been modified in-place','line_number':477,'multiline':False]['text':' Tensors saved for backward have an entry in _tid_to_weakhandle','line_number':498,'multiline':False]['text':' Save aliasing information','line_number':501,'multiline':False]['text':' NB: The same tensor (of the same version) can be saved multiple times','line_number':504,'multiline':False]['text':' Store an additional strong reference to the handle','line_number':510,'multiline':False]['text':' We know that if tid is in sid_to_tid, then it must also be in','line_number':547,'multiline':False]['text':' tid_to_weakhandle. However, it is possible for the tensor to be','line_number':548,'multiline':False]['text':' saved at one point, but cleared by backward before it is modified','line_number':549,'multiline':False]['text':' in-place. Consider the following example:','line_number':550,'multiline':False]['text':'','line_number':551,'multiline':False]['text':' >>> a = torch.randn(2, 3, requires_grad=True).clone()','line_number':552,'multiline':False]['text':' >>> out = (a**2).sum()','line_number':553,'multiline':False]['text':' >>> out.backward()','line_number':554,'multiline':False]['text':' >>> a.sin_()','line_number':555,'multiline':False]['text':' The same exact tensor has been cloned already','line_number':559,'multiline':False]