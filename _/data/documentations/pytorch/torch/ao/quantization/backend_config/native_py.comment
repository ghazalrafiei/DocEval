['text':' ===================','line_number':32,'multiline':False]['text':' |  DTYPE CONFIGS  |','line_number':33,'multiline':False]['text':' ===================','line_number':34,'multiline':False]['text':' weighted op int8 dtype config','line_number':36,'multiline':False]['text':' this is config for ops that has quantized weights, like linear, conv','line_number':37,'multiline':False]['text':' currently the dtype check is not yet enabled, so we provided the dtype_configs but','line_number':62,'multiline':False]['text':' it is not really used yet,','line_number':63,'multiline':False]['text':' we will enable it a bit later after we moved everything to backend_config_dict','line_number':64,'multiline':False]['text':' currently the dtype check is not yet enabled, so we provided the dtype_configs but','line_number':73,'multiline':False]['text':' it is not really used yet,','line_number':74,'multiline':False]['text':' we will enable it a bit later after we moved everything to backend_config_dict','line_number':75,'multiline':False]['text':' Needed for LayerNorm and f.layer_norm, since currently the kernel only supports float weights','line_number':79,'multiline':False]['text':' =====================','line_number':100,'multiline':False]['text':' |  BACKEND CONFIGS  |','line_number':101,'multiline':False]['text':' =====================','line_number':102,'multiline':False]['text':' TODO: express this BackendConfig as a union of the FBGEMM and QNNPACK BackendConfigs','line_number':158,'multiline':False]