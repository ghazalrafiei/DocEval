['text':' _is_output_of_quantized_pattern:','line_number':49,'multiline':False]['text':'  * Node as output node of a fusion pattern.','line_number':50,'multiline':False]['text':'  * The fusion pattern supports int8 data type.','line_number':51,'multiline':False]['text':'  * The fusion pattern has inputs annotated to insert observer.','line_number':52,'multiline':False]['text':' Operations that:','line_number':56,'multiline':False]['text':' 1. Operations are optimized to run with int8 when int8 input provided.','line_number':57,'multiline':False]['text':' 2. Operations do not support int8 input and produce fp32 output.','line_number':58,'multiline':False]['text':' Operations support the int8 data type and exclude operations such as conv and linear.','line_number':68,'multiline':False]['text':' A superset of int8_in_int8_out_ops_pt2e incorporating additional operators.','line_number':69,'multiline':False]['text':' The node has not been annotated, directly return False','line_number':117,'multiline':False]['text':' TODO: Add more supported operators here.','line_number':125,'multiline':False]['text':' Append Conv Optional(Add) Optioinal(ReLU)','line_number':133,'multiline':False]['text':' add','line_number':136,'multiline':False]['text':' relu','line_number':137,'multiline':False]['text':' Append Conv ReLU','line_number':141,'multiline':False]['text':' type: ignore[list-item]','line_number':142,'multiline':False]['text':' Append Conv Add','line_number':144,'multiline':False]['text':' type: ignore[list-item]','line_number':145,'multiline':False]['text':' Append Conv Add ReLU','line_number':147,'multiline':False]['text':' type: ignore[list-item]','line_number':148,'multiline':False]['text':' Copy from x86 default qconfig from torch/ao/quantization/qconfig.py','line_number':172,'multiline':False]['text':' reduce_range=False','line_number':176,'multiline':False]['text':' Only support per channel quant for now','line_number':190,'multiline':False]['text':' type: ignore[dict-item]','line_number':191,'multiline':False]['text':' 0 corresponding to weight shape = (oc, ic, kh, kw) of conv','line_number':197,'multiline':False]['text':' type: ignore[assignment]','line_number':228,'multiline':False]['text':' type: ignore[union-attr]','line_number':357,'multiline':False]['text':' type: ignore[union-attr]','line_number':362,'multiline':False]['text':' type: ignore[index]','line_number':367,'multiline':False]['text':' Step1: Recipe of fusion patterns like conv/linear.','line_number':392,'multiline':False]['text':' Annotate QAT specific pattern: mainly due to BN not folded in prepare_qat','line_number':394,'multiline':False]['text':' Step2: Recipe to propagate annotation for patterns beside conv/linear.','line_number':399,'multiline':False]['text':' Go through all the nodes from start to end.','line_number':400,'multiline':False]['text':' Recipe refer to https://github.com/intel/intel-extension-for-pytorch/blob/','line_number':401,'multiline':False]['text':' 90d19323d96afc53fcc22ba5a7bb3fb07fdd6c1c/intel_extension_for_pytorch/quantization/_recipe.py#L538','line_number':402,'multiline':False]['text':' Step3: For quantizable ops, such as maxpool2d, we need to quantize its output if it is quantized','line_number':406,'multiline':False]['text':' in inputs. So, we can fuse dq-operator-q into a quantized op.','line_number':407,'multiline':False]['text':' Refer to https://github.com/intel/intel-extension-for-pytorch/blob/','line_number':408,'multiline':False]['text':' 90d19323d96afc53fcc22ba5a7bb3fb07fdd6c1c/intel_extension_for_pytorch/quantization/_recipe.py#L487','line_number':409,'multiline':False]['text':' Annotate QAT Specific patterns','line_number':418,'multiline':False]['text':' Conv BN pattern should only has 1 user.','line_number':447,'multiline':False]['text':' TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.','line_number':479,'multiline':False]['text':' type: ignore[arg-type]','line_number':480,'multiline':False]['text':' Conv BN pattern should only has 1 user.','line_number':506,'multiline':False]['text':' TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.','line_number':536,'multiline':False]['text':' type: ignore[arg-type]','line_number':537,'multiline':False]['text':' Extend the fused_partitions if partitions is not empty','line_number':558,'multiline':False]['text':' TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.','line_number':582,'multiline':False]['text':' type: ignore[arg-type]','line_number':583,'multiline':False]['text':' TODO<leslie> Remove the annotate of output in QAT when qat util support pattern matcher.','line_number':617,'multiline':False]['text':' type: ignore[arg-type]','line_number':618,'multiline':False]['text':' Conv2d + add + unary op','line_number':639,'multiline':False]['text':' Conv Node should only has 1 user node','line_number':649,'multiline':False]['text':' No conv node found to be fused with add','line_number':663,'multiline':False]['text':' Conv2d + add','line_number':684,'multiline':False]['text':' Conv Node should only has 1 user node','line_number':694,'multiline':False]['text':' No conv node found to be fused with add','line_number':709,'multiline':False]['text':' Extend the fused_partitions if partitions is not empty','line_number':736,'multiline':False]['text':' skip annotation if it is already annotated','line_number':773,'multiline':False]['text':' There has the case of cat same nodes: torch.cat([input0, input0], 1)','line_number':817,'multiline':False]['text':' Propagate annotation to quantizable patterns.','line_number':830,'multiline':False]['text':' Ensure all the inputs connect to fusion pattern or quantized node','line_number':838,'multiline':False]['text':' Recipe of maxpool2d: check input arg[0] of maxpool2d is quantized or not','line_number':845,'multiline':False]['text':' Get the quantization_annotation from getitem_node','line_number':909,'multiline':False]['text':' Annotate the output_qspec of getitem_node','line_number':919,'multiline':False]['text':' skip annotation if it is already annotated','line_number':949,'multiline':False]