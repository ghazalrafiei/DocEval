['text':' TODO: replace all usages with these constants','line_number':37,'multiline':False]['text':' TODO: derive this map from the BackendConfig','line_number':44,'multiline':False]['text':' default_per_channel_weight_observer is not currently compatible with fbgemm backend','line_number':72,'multiline':False]['text':' so we have to modify the weight observer to default_weight_observer or another','line_number':73,'multiline':False]['text':' per tensor supported observer.','line_number':74,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/47535','line_number':75,'multiline':False]['text':' currently layernorm only supports float weights','line_number':81,'multiline':False]['text':' we have to add this because otherwise there will be a extra quantize-dequantize pair','line_number':82,'multiline':False]['text':' Use special observers for ops with fixed qparams','line_number':98,'multiline':False]['text':' TODO Currently it's required that separate ops in a fused op/module have the same qconfig.','line_number':112,'multiline':False]['text':'      Need to be able to support fusion of ops with different qconfigs','line_number':113,'multiline':False]['text':' TODO: add assert for backend choices','line_number':126,'multiline':False]['text':' In increasing match priority:','line_number':214,'multiline':False]['text':' TODO: remove this','line_number':294,'multiline':False]['text':' TODO: remove this','line_number':321,'multiline':False]