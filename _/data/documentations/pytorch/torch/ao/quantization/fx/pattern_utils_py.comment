['text':' TODO(future PR): fix the typing on QuantizeHandler (currently a circular dependency)','line_number':14,'multiline':False]['text':' pattern for conv bn fusion','line_number':17,'multiline':False]['text':' Mapping from pattern to activation_post_process(observer/fake_quant) constructor for output activation','line_number':30,'multiline':False]['text':' e.g. pattern: torch.sigmoid,','line_number':31,'multiline':False]['text':'      output_activation_post_process: default_fixed_qparams_range_0to1_fake_quant','line_number':32,'multiline':False]['text':' Register pattern for both static quantization and qat','line_number':36,'multiline':False]['text':' Get patterns for both static quantization and qat','line_number':46,'multiline':False]['text':' a map from pattern to output activation post process constructor','line_number':50,'multiline':False]['text':' e.g. torch.sigmoid -> default_affine_fixed_qparam_fake_quant','line_number':51,'multiline':False]['text':' Example use of register pattern function:','line_number':58,'multiline':False]['text':' @_register_fusion_pattern(torch.nn.ReLU, (torch.nn.BatchNorm2d, torch.nn.Conv2d)))','line_number':59,'multiline':False]['text':' class ConvOrLinearBNReLUFusion():','line_number':60,'multiline':False]['text':'     def __init__(...):','line_number':61,'multiline':False]['text':'         ...','line_number':62,'multiline':False]['text':'','line_number':63,'multiline':False]