['text':' F.channel_shuffle and torch.channel_shuffle are essentially the same thing','line_number':115,'multiline':False]['text':' so we only need to put one of them here','line_number':116,'multiline':False]['text':' Mapping from reference module class to the replacement static quantized module class for lowering','line_number':216,'multiline':False]['text':' Mapping from reference module class to the replacement dynamic quantized module class for lowering','line_number':224,'multiline':False]['text':' Mapping from reference module class to the replacement weight only quantized module class for lowering','line_number':234,'multiline':False]['text':' TODO: correct the namespace for these modules','line_number':235,'multiline':False]['text':' TODO: merge with STATIC_LOWER_MODULE_MAP after we merge','line_number':241,'multiline':False]['text':' _lower_static_weighted_ref_module and special_pattern_replacement','line_number':242,'multiline':False]['text':' Mapping from fused module class to a 2-tuple of:','line_number':263,'multiline':False]['text':'   1) The inner reference module class','line_number':264,'multiline':False]['text':'   2) The replacement static quantized module class for lowering','line_number':265,'multiline':False]['text':' TODO: LinearLeakyReLU is registered as global but it is only fused and','line_number':268,'multiline':False]['text':' lowered when ondnn's backend config is used. Maybe need to separate','line_number':269,'multiline':False]['text':' registration and lowering functions for different backends in the future.','line_number':270,'multiline':False]['text':' The difference between STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP and STATIC_LOWER_FUSED_MODULE_MAP:','line_number':278,'multiline':False]['text':' The refer node inside STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP has 2 inputs.','line_number':279,'multiline':False]['text':' Mapping from fused module class to a 2-tuple of:','line_number':280,'multiline':False]['text':'   1) The inner reference module class','line_number':281,'multiline':False]['text':'   2) The replacement static quantized module class for lowering','line_number':282,'multiline':False]['text':' Mapping from fused module class to a 2-tuple of:','line_number':288,'multiline':False]['text':'   1) The inner reference module class','line_number':289,'multiline':False]['text':'   2) The replacement dynamic quantized module class for lowering','line_number':290,'multiline':False]['text':' Mapping from a functional to lower to a 2-tuple of','line_number':295,'multiline':False]['text':'   1) The quantized version of the op','line_number':296,'multiline':False]['text':'   2) The quantized version of the op fused with relu, if it exists, else None','line_number':297,'multiline':False]['text':' Mapping from a functional to a dictionary, where the key is a 2-tuple of','line_number':319,'multiline':False]['text':' (input_activation_dtype, weight_dtype) and the value is a 2-tuple of','line_number':320,'multiline':False]['text':'   1) The dynamically quantized version of the op','line_number':321,'multiline':False]['text':'   2) The dynamically quantized version of the op fused with relu, if it exists, else None','line_number':322,'multiline':False]['text':' dynamic conv + relu is not available yet','line_number':330,'multiline':False]['text':' TODO: add tests for lowering these ops','line_number':354,'multiline':False]['text':' type: ignore[attr-defined]','line_number':373,'multiline':False]['text':' type: ignore[attr-defined] # noqa: B950','line_number':381,'multiline':False]['text':' pop the packed param attributesn','line_number':385,'multiline':False]['text':' map from folded node name to the prepacked weight name','line_number':399,'multiline':False]['text':' get packed weights','line_number':401,'multiline':False]['text':' remove folded nodes and replace the prepacking node with getattr','line_number':414,'multiline':False]['text':' add a prepacked attribute to root','line_number':425,'multiline':False]['text':' replace prepack node with a getattr node','line_number':432,'multiline':False]['text':' remove the foled node','line_number':436,'multiline':False]['text':' copy other nodes','line_number':439,'multiline':False]['text':' Match quantize node','line_number':485,'multiline':False]['text':' Handle cases where the node is wrapped in a ReLU','line_number':492,'multiline':False]['text':' Match reference module or functional','line_number':503,'multiline':False]['text':' Match dequantize node(s). Both of the following conditions must pass:','line_number':513,'multiline':False]['text':' (1) All `torch.fx.Node`s at the matching indices must be a dequantize node','line_number':514,'multiline':False]['text':' (2) There must be at least one dequantize node','line_number':515,'multiline':False]['text':' Match quantize node','line_number':555,'multiline':False]['text':' Match reference module or functional','line_number':565,'multiline':False]['text':' This pass only support op of "call_module"','line_number':570,'multiline':False]['text':' Check ref_node has 2 input nodes, both are dq node.','line_number':576,'multiline':False]['text':' Step 0: Find nodes that match this pattern (dequantize - ref module - quantize)','line_number':596,'multiline':False]['text':' type: ignore[arg-type]','line_number':599,'multiline':False]['text':' Step 1: Change this pattern to use the corresponding quantized module','line_number':610,'multiline':False]['text':' For fused modules, we also check whether the inner module is a reference module','line_number':611,'multiline':False]['text':' If so, we replace the entire fused module with the corresponding quantized module','line_number':612,'multiline':False]['text':' type: ignore[index]','line_number':615,'multiline':False]['text':' replace reference module with quantized module','line_number':622,'multiline':False]['text':' Step 2: Reroute around dq_node, and remove q_node and its args','line_number':626,'multiline':False]['text':'                                            (dequantize \','line_number':651,'multiline':False]['text':' Step 0: Find nodes that match this pattern (dequantize - ref module - quantize)','line_number':652,'multiline':False]['text':' type: ignore[arg-type]','line_number':655,'multiline':False]['text':' Step 1: Change this pattern to use the corresponding quantized module','line_number':666,'multiline':False]['text':' For fused modules, we also check whether the inner module is a reference module','line_number':667,'multiline':False]['text':' If so, we replace the entire fused module with the corresponding quantized module','line_number':668,'multiline':False]['text':' type: ignore[index]','line_number':671,'multiline':False]['text':' replace reference module with quantized module','line_number':678,'multiline':False]['text':' Step 2: Reroute around dq_node, and remove q_node and its args','line_number':682,'multiline':False]['text':' type: ignore[assignment]','line_number':732,'multiline':False]['text':' TODO: maybe define a WeightedDynamicallyQuantizedModule','line_number':733,'multiline':False]['text':' type: ignore[attr-defined]','line_number':734,'multiline':False]['text':' replace reference module with dynamically quantized module','line_number':736,'multiline':False]['text':' TODO: WeightedQuantizedModule is currently assuming static quant apis','line_number':756,'multiline':False]['text':' with output_scale, output_zero_point in from_reference, we may want to','line_number':757,'multiline':False]['text':' relax that, or rename this','line_number':758,'multiline':False]['text':' TODO: maybe define a WeightedWeightOnlyQuantizedModule','line_number':759,'multiline':False]['text':' type: ignore[union-attr]','line_number':760,'multiline':False]['text':' replace reference module with dynamically quantized module','line_number':762,'multiline':False]['text':' Step 0: Find nodes that match this pattern (dequantize - functional op - quantize)','line_number':775,'multiline':False]['text':' Step 1: Replace quantized weights with packed weights, which will be folded later','line_number':793,'multiline':False]['text':' Use the right prepack op and prepare the corresponding args','line_number':794,'multiline':False]['text':' Linear prepack args: (quantized weights[, bias])','line_number':795,'multiline':False]['text':' Conv prepack args: (quantized weights[, bias, stride, padding, dilation, groups])','line_number':796,'multiline':False]['text':' type: ignore[arg-type]','line_number':802,'multiline':False]['text':' For conv1d, the stride, padding, and dilation args may be ints,','line_number':803,'multiline':False]['text':' in which case we need to convert them to tuples','line_number':804,'multiline':False]['text':' type: ignore[arg-type]','line_number':810,'multiline':False]['text':' For conv_transpose1d, the stride, padding, and dilation args may be ints,','line_number':811,'multiline':False]['text':' in which case we need to convert them to tuples','line_number':812,'multiline':False]['text':' Note prepack_args[5] is groups.','line_number':814,'multiline':False]['text':' swap dilation and groups','line_number':818,'multiline':False]['text':' prepack op has arguments: {w, b, stride, padding, output_padding, dilation, groups}','line_number':819,'multiline':False]['text':' transposed conv op has arguments: {x, w, b, stride, padding, output_padding, groups, dilation}','line_number':820,'multiline':False]['text':' kwargs of the func node are needed for prepack op (i.e., quantized::linear_prepack)','line_number':826,'multiline':False]['text':' They are not needed for compute op (i.e., quantized::linear)','line_number':827,'multiline':False]['text':' F.linear uses 'bias' key for bias while qlinear_prepack uses 'B' for bias','line_number':829,'multiline':False]['text':' Step 2: Replace reference pattern with the corresponding quantized op','line_number':836,'multiline':False]['text':' type: ignore[index]','line_number':837,'multiline':False]['text':' conv_transpose does not support fusion with relu yet. q_relu_func is None in such cases','line_number':838,'multiline':False]['text':' kwargs for func_node has been moved to kwargs for prepack op','line_number':844,'multiline':False]['text':' Move func_node after output_zp_node in the graph','line_number':847,'multiline':False]['text':' Clean up: Remove quantize node, and the relu node if it exists','line_number':850,'multiline':False]['text':' we want to search in reserved order so that we can match the larger patterns first','line_number':867,'multiline':False]['text':' e.g. we want to match linear - relu before linear.','line_number':868,'multiline':False]['text':' Step 0: Find nodes that match this pattern','line_number':871,'multiline':False]['text':' (quantize_per_tensor_dynamic - dequantize - dynamically quantized op)','line_number':872,'multiline':False]['text':' We search for the pattern backwards, starting with the quantize node','line_number':873,'multiline':False]['text':' Quantize node args: (func, scale, zp, dtype)','line_number':874,'multiline':False]['text':' Handle cases where the functional op is wrapped in a ReLU','line_number':876,'multiline':False]['text':' Linear args: (dequantized inputs, dequantized weights[, bias])','line_number':886,'multiline':False]['text':' Conv args: (dequantized inputs, dequantized weights[, bias, stride, padding, dilation, groups])','line_number':887,'multiline':False]['text':' Step 1: Try to select reference pattern with the corresponding quantized op','line_number':911,'multiline':False]['text':' Step 2: Replace quantized weights with packed weights, which will be folded later','line_number':924,'multiline':False]['text':' Use the right prepack op and prepare the corresponding args','line_number':925,'multiline':False]['text':' Linear prepack args: (quantized weights[, bias])','line_number':926,'multiline':False]['text':' Conv prepack args: (quantized weights[, bias, stride, padding, dilation, groups])','line_number':927,'multiline':False]['text':' For conv1d, the stride, padding, and dilation args may be ints,','line_number':933,'multiline':False]['text':' in which case we need to convert them to tuples','line_number':934,'multiline':False]['text':' Step 3: Replace reference pattern with the corresponding quantized op','line_number':944,'multiline':False]['text':' Step 4: Remove the relu node if it exists','line_number':954,'multiline':False]['text':' Step 0: Find nodes that match this pattern (dequantize - ref module - quantize)','line_number':964,'multiline':False]['text':' Step 1: Remove dequant nodes','line_number':972,'multiline':False]['text':' Step 2: Swap binary op to quantized binary op','line_number':984,'multiline':False]['text':' prepare the args for quantized binary op','line_number':988,'multiline':False]['text':' (x, y)','line_number':989,'multiline':False]['text':' (x, y, scale, zero_point)','line_number':991,'multiline':False]['text':' add scale and zero_point arguments for Tensor - Tensor operation','line_number':992,'multiline':False]['text':' insert a call to quantized binary op and remove the original binary op','line_number':995,'multiline':False]['text':' Step 3: Remove quantize node, binary op node, and relu node if any','line_number':1003,'multiline':False]['text':' get output scale/zero_point/dtype from the quantize node','line_number':1019,'multiline':False]['text':' ref_node, scale_node, zero_point_node, dtype = q_node.args','line_number':1020,'multiline':False]['text':' TODO: add safety checks that users for the ref_node and dq_node needs to be one','line_number':1021,'multiline':False]['text':' TODO: add a warning or error out here? (bc-breaking if error out)','line_number':1024,'multiline':False]['text':' warnings.warn(','line_number':1025,'multiline':False]['text':'     "Only reference patterns are currently supported for {dtype} dtype with {op} op"','line_number':1026,'multiline':False]['text':'     "".format(dtype=dtypes, op=ref_node))','line_number':1027,'multiline':False]['text':' TODO: add a warning or error out here? (bc-breaking if error out)','line_number':1032,'multiline':False]['text':' This check includes all supported ops','line_number':1035,'multiline':False]['text':' TODO: enable we have patterns that needs to swap the modules','line_number':1054,'multiline':False]['text':' type:ignore[union-attr]','line_number':1064,'multiline':False]['text':' replace reference module with quantized module','line_number':1065,'multiline':False]['text':' reroute around dq node:','line_number':1069,'multiline':False]['text':' store q node args','line_number':1080,'multiline':False]['text':' replace uses of q node with input and remove q node','line_number':1082,'multiline':False]['text':' pass scale/zer_point arguments from quantize_per_tensor to the default node operator','line_number':1089,'multiline':False]['text':' insert an op after the zero_point node so that the scale/zero_point','line_number':1090,'multiline':False]['text':' nodes are is available','line_number':1091,'multiline':False]['text':' remove scale/zero_point node for quantize node','line_number':1110,'multiline':False]['text':' skip the dequantize node','line_number':1126,'multiline':False]['text':' skip the dequantize node','line_number':1141,'multiline':False]