['text':' TODO: move all LSTM util functions from fx/utils.py to this file','line_number':19,'multiline':False]['text':' Build QConfigMapping for the LSTM cell','line_number':81,'multiline':False]['text':' Note: FloatFunctional qconfigs will be configured separately below','line_number':82,'multiline':False]['text':' type: ignore[arg-type]','line_number':83,'multiline':False]['text':' Insert observers into each LSTM cell','line_number':91,'multiline':False]['text':' TODO: maybe make this work for layer_bw as well','line_number':92,'multiline':False]['text':' HACK: Manually replace the activation_post_process following these ops.','line_number':96,'multiline':False]['text':' This is needed for FloatFunctional ops because there is currently no way','line_number':97,'multiline':False]['text':' to configure these ops in FX graph mode quantization today. This is because','line_number':98,'multiline':False]['text':' the FloatFunctional modules simply disappear from the graph after tracing.','line_number':99,'multiline':False]['text':' In the future, we should rewrite quantizable LSTM without FloatFunctionals.','line_number':100,'multiline':False]['text':' gates.add','line_number':102,'multiline':False]['text':' fgate_cx.mul','line_number':103,'multiline':False]['text':' igate_cgate.mul','line_number':104,'multiline':False]['text':' fgate_cx_igate_cgate.add','line_number':105,'multiline':False]['text':' ogate_cy.mul','line_number':106,'multiline':False]['text':' e.g. (torch.add, 1)','line_number':111,'multiline':False]['text':' Neither torch.add nor torch.mul','line_number':119,'multiline':False]['text':' type: ignore[union-attr]','line_number':156,'multiline':False]['text':' type: ignore[arg-type]','line_number':157,'multiline':False]['text':' HACK: Manually remove input quantize nodes and output dequantize nodes,','line_number':159,'multiline':False]['text':' since custom modules expect quint8 inputs and outputs for now. Note that','line_number':160,'multiline':False]['text':' this functionality is supposedly handled through PrepareCustomConfig's','line_number':161,'multiline':False]['text':' `set_input_quantized_indexes` and `set_output_quantized_indexes`, but that','line_number':162,'multiline':False]['text':' API doesn't currently handle tuple inputs and outputs, so we have to do','line_number':163,'multiline':False]['text':' this manually for now. In the future we should (1) relax the restriction','line_number':164,'multiline':False]['text':' on custom module input/output dtypes, and (2) expand support for complex','line_number':165,'multiline':False]['text':' input/output structures.','line_number':166,'multiline':False]['text':' Remove quantize(x), quantize(hidden[0]), and quantize(hidden[1])','line_number':170,'multiline':False]['text':' Remove all dequantize nodes in the output tuple','line_number':176,'multiline':False]