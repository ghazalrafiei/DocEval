['text':' Makes sure that quantized_decomposed ops are registered','line_number':14,'multiline':False]['text':' noqa: F401','line_number':15,'multiline':False]['text':' Example inputs for conv-bn1d patterns','line_number':39,'multiline':False]['text':' x','line_number':41,'multiline':False]['text':' conv_weight','line_number':42,'multiline':False]['text':' conv_bias','line_number':43,'multiline':False]['text':' bn_weight','line_number':44,'multiline':False]['text':' bn_bias','line_number':45,'multiline':False]['text':' bn_running_mean','line_number':46,'multiline':False]['text':' bn_running_var','line_number':47,'multiline':False]['text':' Example inputs for conv-bn2d patterns','line_number':50,'multiline':False]['text':' x','line_number':52,'multiline':False]['text':' conv_weight','line_number':53,'multiline':False]['text':' conv_bias','line_number':54,'multiline':False]['text':' bn_weight','line_number':55,'multiline':False]['text':' bn_bias','line_number':56,'multiline':False]['text':' bn_running_mean','line_number':57,'multiline':False]['text':' bn_running_var','line_number':58,'multiline':False]['text':' Note: we won't need this op anymore after batch norm consolidation','line_number':154,'multiline':False]['text':' For now, we need to continue to support it because it gives better','line_number':155,'multiline':False]['text':' training numerics than `_native_batch_norm_legit`','line_number':156,'multiline':False]['text':' conv args: input, weight, bias, stride, padding, dilation, ...','line_number':187,'multiline':False]['text':' eval bn args: input, weight, bias, running mean, running var, momentum, eps','line_number':192,'multiline':False]['text':' train bn args: input, weight, bias, running mean, running var, training, momentum, eps','line_number':193,'multiline':False]['text':' type: ignore[union-attr]','line_number':194,'multiline':False]['text':' update the weight and bias for conv','line_number':210,'multiline':False]['text':' filling in the default bias argument','line_number':212,'multiline':False]['text':' calling data since the fused_weight and fused_bias are nn.Parameter','line_number':216,'multiline':False]['text':' type: ignore[arg-type]','line_number':222,'multiline':False]['text':' type: ignore[arg-type]','line_number':225,'multiline':False]['text':' NOTE: here we assume the bias of conv is not quantized!','line_number':228,'multiline':False]['text':' native_batch_norm has 3 outputs, we expect getitem calls on the output','line_number':232,'multiline':False]['text':' and we want to replace the uses of getitem 0 with the output of conv','line_number':233,'multiline':False]['text':'','line_number':234,'multiline':False]['text':' Before:','line_number':235,'multiline':False]['text':' conv -> bn - (first output) -> users1','line_number':236,'multiline':False]['text':'          \ - (second output) -> users2','line_number':237,'multiline':False]['text':'          \ - (third output) -> users3','line_number':238,'multiline':False]['text':' After:','line_number':239,'multiline':False]['text':' conv -> (first output) -> users1','line_number':240,'multiline':False]['text':'       bn -','line_number':241,'multiline':False]['text':'          \ - (second output) -> users2','line_number':242,'multiline':False]['text':'          \ - (third output) -> users3','line_number':243,'multiline':False]['text':' if users2 and users3 are empty then bn will be removed through dead code elimination','line_number':244,'multiline':False]['text':' fuse conv bn weights, inplace modification of the graph_module and graph','line_number':251,'multiline':False]['text':' TODO: move this information to fx node itself','line_number':269,'multiline':False]['text':' TODO: Handle this in export itself and don't wrap the model in another GraphModule','line_number':492,'multiline':False]['text':' in prepare and convert','line_number':493,'multiline':False]['text':' type: ignore[method-assign]','line_number':505,'multiline':False]['text':' type: ignore[method-assign]','line_number':506,'multiline':False]