['text':' noqa: F401','line_number':9,'multiline':False]['text':' without using quant_min/max in clamp, the traced graph will not have quant_mi/max args.','line_number':61,'multiline':False]['text':' This results in failure to match the pattern.','line_number':62,'multiline':False]['text':' Therefore, we call a torch.ops.aten.clamp here','line_number':63,'multiline':False]['text':' always set bias to None so that the same representation can work for the case','line_number':69,'multiline':False]['text':' no matter if bias_scale == x_scale * weight_scale or not','line_number':70,'multiline':False]['text':' TODO: change to mul.Scalar','line_number':77,'multiline':False]['text':' Note: we are quantizing bias with these scales without signal from user, but it might be OK','line_number':78,'multiline':False]['text':' TODO: change to mul.Scalar when we make x_scale/weight_scale etc. Scalar values','line_number':82,'multiline':False]['text':' decomposed representation for quantize_per_tensor','line_number':123,'multiline':False]['text':' TODO: use out_dtype(mul, ...) here when the op is ready','line_number':124,'multiline':False]['text':' fp32','line_number':125,'multiline':False]['text':' round modes might be different here','line_number':126,'multiline':False]['text':' pytorch is rounding to even, which is also common for most of the backends','line_number':127,'multiline':False]['text':' fp32','line_number':128,'multiline':False]['text':' int32','line_number':129,'multiline':False]['text':' int32','line_number':130,'multiline':False]['text':' clamp works for fp32, int32 and int8 dtypes','line_number':131,'multiline':False]['text':' int32','line_number':132,'multiline':False]['text':' always set bias to None so that the same representation can work for the case','line_number':139,'multiline':False]['text':' no matter if bias_scale == x_scale * weight_scale or not','line_number':140,'multiline':False]['text':' without using quant_min/max in clamp, the traced graph will not have quant_mi/max args.','line_number':206,'multiline':False]['text':' This results in failure to match the pattern.','line_number':207,'multiline':False]['text':' Therefore, we call a torch.ops.aten.clamp here','line_number':208,'multiline':False]['text':' always set bias to None so that the same representation can work for the case','line_number':214,'multiline':False]['text':' no matter if bias_scale == x_scale * weight_scale or not','line_number':215,'multiline':False]['text':' Note: we are quantizing bias with these scales without signal from user, but it might be OK','line_number':222,'multiline':False]['text':' bias quantization to int32 uses bias_scale = x_scale * weight_scale due to:','line_number':224,'multiline':False]['text':' Take linear calculation for example','line_number':225,'multiline':False]['text':' Out_(i, j)_fp32 = Sum_(over k)[X_(i, k)_fp32 * W_(i, k)_fp32] + bias_(i)_fp32','line_number':226,'multiline':False]['text':' Represent X, W fp32 as their dequant transforms','line_number':227,'multiline':False]['text':' A_fp32 = (A_q - A_zero_point)/A_scale','line_number':228,'multiline':False]['text':' Out_(i, j)_fp32 = Sum_(over k)[(X_(i, k)_fp32 - X_zp) * X_scale * (W_(i, k)_fp32 - W_zp) * W_scale] + bias_(i)_fp32','line_number':229,'multiline':False]['text':' Factor out X_scale and W_scale','line_number':230,'multiline':False]['text':' Out_(i, j)_fp32 = ((X_scale * W_scale) * Sum_(over k)[(X_(i, k)_fp32 - X_zp) * (W_(i, k)_fp32 - W_zp)]) + bias_(i)_fp32','line_number':231,'multiline':False]['text':' In order to addition of bias_(i)_fp32 inside, we must do','line_number':232,'multiline':False]['text':' Out_(i, j)_fp32 = (X_scale * W_scale) * (Sum_(over k)[(X_(i, k)_fp32 - X_zp) * (W_(i, k)_fp32 - W_zp)] + (1 / (X_scale * W_scale)) * bias_(i)_fp32)W_scale  # noqa: B950','line_number':233,'multiline':False]['text':' Note we had to multiply bias_fp32 qith X_scale * W_scale = bias_scale','line_number':234,'multiline':False]['text':' Thus bias quantization to int32 must be with X_scale * W_scale','line_number':235,'multiline':False]['text':' Unsqueeze to match broadcast dims','line_number':238,'multiline':False]['text':' Unfortnuately I cannot do bias_i32.unsqueeze(0) due to literal matching nightmare','line_number':239,'multiline':False]['text':' in graph pattern replacement','line_number':240,'multiline':False]['text':' TODO: change to mul.Scalar when we make x_scale/weight_scale etc. Scalar values','line_number':244,'multiline':False]['text':' TODO: change this to mul.Scalar?','line_number':287,'multiline':False]['text':' out_i32 = torch.ops.aten.clamp(out_i32, out_zero_point)','line_number':291,'multiline':False]['text':' TODO: use out_dtype op','line_number':327,'multiline':False]['text':' to preserve x_quant_min, x_quant_max in the graph for pattern matching','line_number':368,'multiline':False]['text':' TODO: use out_dtype(mul, ...) here when the op is ready','line_number':397,'multiline':False]['text':' fp32','line_number':398,'multiline':False]['text':' round modes might be different here','line_number':399,'multiline':False]['text':' pytorch is rounding to even, which is also common for most of the backends','line_number':400,'multiline':False]['text':' fp32','line_number':401,'multiline':False]['text':' int32','line_number':402,'multiline':False]['text':' int32','line_number':403,'multiline':False]['text':' clamp works for fp32, int32 and int8 dtypes','line_number':404,'multiline':False]['text':' int32','line_number':405,'multiline':False]['text':' without using quant_min/max in clamp, the traced graph will not have quant_mi/max args.','line_number':422,'multiline':False]['text':' This results in failure to match the pattern.','line_number':423,'multiline':False]['text':' Therefore, we call a torch.ops.aten.clamp here','line_number':424,'multiline':False]['text':' TODO: use out_dtype op','line_number':426,'multiline':False]['text':' note: x_i8.to(torch.int32) does not work here','line_number':427,'multiline':False]['text':' TODO: debug the implementation later when torchdynamo time out issue is resolved','line_number':428,'multiline':False]['text':' the following will be replaced as placeholders','line_number':462,'multiline':False]['text':' the following will be replaced as placeholders','line_number':469,'multiline':False]['text':' in order to preserve the quant_min/quant_max args for pattern matching (e.g. matching for int4 quantized ops)','line_number':470,'multiline':False]['text':' we call a torch.ops.aten.clamp here','line_number':471,'multiline':False]['text':' example inputs used for exporting the pattern into GraphModule','line_number':493,'multiline':False]['text':' post transformation on the exported pattern and replacement GraphModule','line_number':497,'multiline':False]['text':' type: ignore[arg-type, assignment]','line_number':587,'multiline':False]['text':' type: ignore[arg-type]','line_number':588,'multiline':False]['text':' type: ignore[arg-type, assignment]','line_number':589,'multiline':False]['text':' type: ignore[arg-type]','line_number':590,'multiline':False]['text':' type: ignore[attr-defined]','line_number':595,'multiline':False]['text':' type: ignore[attr-defined]','line_number':596,'multiline':False]