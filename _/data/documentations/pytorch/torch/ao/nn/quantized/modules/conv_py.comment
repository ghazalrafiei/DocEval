['text':' All subclasses have this signature - See PR #49702s','line_number':39,'multiline':False]['text':' Initialize as NCHW. set_weight will internally transpose to NHWC.','line_number':68,'multiline':False]['text':' ===== Serialization methods =====','line_number':109,'multiline':False]['text':' The special consideration here is that we have to unpack the weights into','line_number':110,'multiline':False]['text':' their regular QTensor form for serialization. Packed weights should not','line_number':111,'multiline':False]['text':' live outside the process in which they were created, rather they should be','line_number':112,'multiline':False]['text':' derived from the QTensor weight.','line_number':113,'multiline':False]['text':'   self','line_number':114,'multiline':False]['text':'   |--- weight : Tensor','line_number':115,'multiline':False]['text':'   |--- bias : Tensor','line_number':116,'multiline':False]['text':'','line_number':117,'multiline':False]['text':' TODO: maybe change to this when https://github.com/pytorch/pytorch/pull/32958 is landed','line_number':118,'multiline':False]['text':'   self','line_number':119,'multiline':False]['text':'   |--- _packed_params : Conv2dPackedParamsBase or Conv3dPackedParamsBase','line_number':120,'multiline':False]['text':' ===== Deserialization methods =====','line_number':150,'multiline':False]['text':' Counterpart to the serialization methods, we must pack the serialized','line_number':151,'multiline':False]['text':' QTensor weight into its packed format for use by the FBGEMM ops.','line_number':152,'multiline':False]['text':' the __init__ call used is the one from derived classes and not the one from _ConvNd','line_number':204,'multiline':False]['text':' dynamic quantization doesn't need scale/zero_point','line_number':210,'multiline':False]['text':' assert type(mod) == cls.__QAT_MODULE, " nnq." + cls.__name__ + \','line_number':220,'multiline':False]['text':' ".from_float only works for " + cls.__QAT_MODULE.__name__','line_number':221,'multiline':False]['text':' type: ignore[arg-type]','line_number':255,'multiline':False]['text':' type: ignore[arg-type]','line_number':256,'multiline':False]['text':' type: ignore[arg-type]','line_number':257,'multiline':False]['text':' type: ignore[arg-type]','line_number':258,'multiline':False]['text':' type: ignore[arg-type]','line_number':260,'multiline':False]['text':' Subclasses of _ConvNd needs to call _init rather than __init__. See','line_number':330,'multiline':False]['text':' discussion on PR #49702','line_number':331,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':359,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':360,'multiline':False]['text':' Padding in Conv1d is stored as (p, p), need to get (p,)','line_number':364,'multiline':False]['text':' Subclasses of _ConvNd need to call _init rather than __init__. See','line_number':433,'multiline':False]['text':' discussion on PR #49702','line_number':434,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':460,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':461,'multiline':False]['text':' Subclasses of _ConvNd need to call _init rather than __init__. See','line_number':535,'multiline':False]['text':' discussion on PR #49702','line_number':536,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':562,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':563,'multiline':False]['text':' === Transposed Convolutions ===','line_number':583,'multiline':False]['text':' Subclasses of _ConvNd need to call _init rather than __init__. See','line_number':597,'multiline':False]['text':' discussion on PR #49702','line_number':598,'multiline':False]['text':' derived classes override cls._FLOAT_MODULE attribute','line_number':618,'multiline':False]['text':' type: ignore[attr-defined]','line_number':620,'multiline':False]['text':' the __init__ call used is the one from derived classes and not the one from _ConvTransposeNd','line_number':629,'multiline':False]['text':' type: ignore[call-arg]','line_number':630,'multiline':False]['text':' dynamic quantization doesn't need scale/zero_point','line_number':635,'multiline':False]['text':' type: ignore[arg-type]','line_number':654,'multiline':False]['text':' type: ignore[arg-type]','line_number':655,'multiline':False]['text':' type: ignore[arg-type]','line_number':656,'multiline':False]['text':' type: ignore[arg-type]','line_number':657,'multiline':False]['text':' type: ignore[arg-type]','line_number':659,'multiline':False]['text':' type: ignore[arg-type]','line_number':660,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':752,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':753,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':843,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':844,'multiline':False]['text':' Temporarily using len(shape) instead of ndim due to JIT issue','line_number':936,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/23890','line_number':937,'multiline':False]