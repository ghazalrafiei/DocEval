['text':' TODO enable float32 support when adding cuSPARSELt as a backend','line_number':19,'multiline':False]['text':' torch.float32: _SEMI_STRUCTURED_SPARSE_CONFIG(32, 32)','line_number':20,'multiline':False]['text':' type: ignore[assignment]','line_number':114,'multiline':False]['text':' type: ignore[assignment]','line_number':115,'multiline':False]['text':' type: ignore[assignment]','line_number':116,'multiline':False]['text':' type: ignore[assignment]','line_number':117,'multiline':False]['text':' type: ignore[attr-defined]','line_number':119,'multiline':False]['text':' if original tensor is passed in, we need to compress it and store the compressed representation.','line_number':156,'multiline':False]['text':' TODO right now we have unified checks and constraints for cuSPARSELt and CUTLASS, these are not actually the same.','line_number':158,'multiline':False]['text':' We should consolidate similar checks here and leave backend specific checks like shape in the op implementation.','line_number':159,'multiline':False]['text':' check device','line_number':161,'multiline':False]['text':' check dim','line_number':168,'multiline':False]['text':' check dtype','line_number':175,'multiline':False]['text':' check shape','line_number':182,'multiline':False]['text':' TODO in the future we can add in padding to support dimensions that aren't perfect multiples','line_number':191,'multiline':False]['text':' use cuSPARSELt','line_number':207,'multiline':False]['text':' set values','line_number':210,'multiline':False]['text':' type: ignore[override]','line_number':248,'multiline':False]['text':' only 2d matmul','line_number':271,'multiline':False]['text':' check shape','line_number':274,'multiline':False]['text':' Since this code runs below autograd, a detach corresponds to only returning a new object','line_number':305,'multiline':False]['text':' Because we cannot go from the compressed representation back to the dense representation currently,','line_number':316,'multiline':False]['text':' we just keep track of how many times we have been transposed. Depending on whether the sparse matrix','line_number':317,'multiline':False]['text':' is the first or second argument, we expect an even / odd number of calls to transpose respectively.','line_number':318,'multiline':False]['text':' transpose shape','line_number':322,'multiline':False]['text':' handle addmm','line_number':330,'multiline':False]['text':' Currently, we only support the first matrix being sparse for addmm/mm in cuSPARSELT and CUTLASS.','line_number':334,'multiline':False]['text':' CUTLASS only supports the first input to be sparse for a given matmul.','line_number':335,'multiline':False]['text':' cuSPARSELt does not have this limitation, although our implementation is only for sparse first.','line_number':336,'multiline':False]['text':' We support second matrix sparse matmul by taking advantage of some transpose properties:','line_number':338,'multiline':False]['text':' This is also why we want an odd number of transposed for second matrix sparse vs an even number','line_number':339,'multiline':False]['text':' of transpose calss for first matrix sparse.','line_number':340,'multiline':False]['text':' F.linear(x) = addmm(bias, input, weight.t()) = b + xW' = (b + xW')''','line_number':341,'multiline':False]['text':'        = (W''x' + b')' = (Wx' + b')' = addmm(bias.T, weight, input).T','line_number':342,'multiline':False]['text':' type: ignore[arg-type]','line_number':353,'multiline':False]['text':' handle mm','line_number':357,'multiline':False]['text':' first element sparse','line_number':361,'multiline':False]['text':' type: ignore[arg-type]','line_number':372,'multiline':False]['text':' second element sparse','line_number':376,'multiline':False]['text':' type: ignore[arg-type]','line_number':387,'multiline':False]['text':' When torch is run with inference mode, pytorch does not decompose torch.ops.aten.linear into a .t() and addmm(),','line_number':391,'multiline':False]['text':' so we must match the aten.linear op. In this case, we need to explicitly handle collapsing to 2d matmul','line_number':392,'multiline':False]['text':' TODO see if there's a way to force pytorch to decompose the op so we don't have to handle this here.','line_number':393,'multiline':False]['text':' this is a noop if already padded','line_number':400,'multiline':False]['text':' type: ignore[arg-type]','line_number':414,'multiline':False]['text':' handle values','line_number':421,'multiline':False]['text':' handle indices','line_number':430,'multiline':False]