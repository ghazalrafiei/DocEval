['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' No FSDP wrapping','line_number':38,'multiline':False]['text':' FSDP recursive wrapping','line_number':40,'multiline':False]['text':' TODO: FSDP non-recursive wrapping','line_number':42,'multiline':False]['text':' NONRECURSIVE = auto()','line_number':43,'multiline':False]['text':' Move model to CUDA before passing to the FSDP constructor','line_number':47,'multiline':False]['text':' Move model to CUDA after passing to the FSDP constructor','line_number':49,'multiline':False]['text':' Keep on CPU','line_number':51,'multiline':False]['text':' Include names for debugging convenience','line_number':100,'multiline':False]['text':' For non-FSDP roots, some parts of the model state on rank 0 may','line_number':151,'multiline':False]['text':' not be on CPU, so we move everything to CPU to avoid issues like:','line_number':152,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/77113.','line_number':153,'multiline':False]['text':' Ensure that the state is on CUDA','line_number':161,'multiline':False]['text':' share the embedding and output projection weights','line_number':238,'multiline':False]['text':' type: ignore[arg-type]','line_number':246,'multiline':False]['text':' keep everything deterministic','line_number':256,'multiline':False]['text':' T x B','line_number':257,'multiline':False]['text':' T x B','line_number':258,'multiline':False]['text':' type: ignore[operator]','line_number':263,'multiline':False]['text':' Default to the `ModuleWrapPolicy`','line_number':316,'multiline':False]['text':' keep everything deterministic','line_number':394,'multiline':False]['text':' Does not wrap with top-level FSDP','line_number':440,'multiline':False]['text':' This `__init__` only differs from `NestedWrappedModule.__init__` in that','line_number':496,'multiline':False]['text':' the last two `nn.Linear` layers are FSDP wrapped in a `nn.Sequential`','line_number':497,'multiline':False]['text':' container. This arrangement results in all elements of the last two parameters','line_number':498,'multiline':False]['text':' residing on a single rank. Freezing all parameters except those two allows us','line_number':499,'multiline':False]['text':' to verify that `ShardedGradScaler` accommodates situations where some ranks','line_number':500,'multiline':False]['text':' have no (non-zero sized) parameter shards.','line_number':501,'multiline':False]['text':' The parameters that should remain unfrozen are in `module.2.1`. The regex','line_number':552,'multiline':False]['text':' pattern below matches the relevant parameter names both with and without','line_number':553,'multiline':False]['text':' an interstitial FSDP module indicator (`_fsdp_wrapped_module`) present.','line_number':554,'multiline':False]['text':' Give each rank different expert parameters','line_number':705,'multiline':False]['text':' type: ignore[attr-defined]','line_number':714,'multiline':False]['text':' Keep all other parameters the same across ranks','line_number':717,'multiline':False]['text':' we create a process group of size 1 for the expert params','line_number':723,'multiline':False]['text':' world size 1 means no shard','line_number':726,'multiline':False]['text':' type: ignore[assignment]','line_number':727,'multiline':False]['text':' type: ignore[assignment]','line_number':728,'multiline':False]['text':' This patch covers any `import torch..._reshard` uses.','line_number':749,'multiline':False]['text':' Manually reduce gradients if not wrapped in FullyShardedDataParallel','line_number':759,'multiline':False]['text':' these params don't need grad reduction','line_number':764,'multiline':False]['text':' Does not wrap with top-level FSDP','line_number':805,'multiline':False]['text':' Convert the config mapping to a list to have a fixed order','line_number':840,'multiline':False]['text':' Map keyword to chosen value','line_number':845,'multiline':False]['text':' Set TORCH_NCCL_DESYNC_DEBUG=0 to disable the NCCL `workCleanupLoop()`,','line_number':868,'multiline':False]['text':' which can cause unit test flakiness:','line_number':869,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/90848','line_number':870,'multiline':False]['text':' Specify gloo backend to make 'init_process_group()' succeed,','line_number':906,'multiline':False]['text':' Actual tests will be skipped if there is no enough GPUs.','line_number':907,'multiline':False]['text':' Execute barrier prior to running test to ensure that every process','line_number':926,'multiline':False]['text':' has finished initialization and that the following test','line_number':927,'multiline':False]['text':' immediately exiting due to a skip doesn't cause flakiness.','line_number':928,'multiline':False]['text':' use SGD with momentum instead of Adam, since Adam is scale invariant','line_number':958,'multiline':False]['text':' and this makes it bad for tests','line_number':959,'multiline':False]['text':' Inputs always cuda regardless of cpu offloading, or model.device','line_number':964,'multiline':False]['text':' Post-forward, if CPU offloading model param should be on CPU.','line_number':972,'multiline':False]['text':' If not resharding after forward, the parameters are still','line_number':976,'multiline':False]['text':' exposed as unsharded views into the GPU flat parameter','line_number':977,'multiline':False]['text':' Params should always be on CPU','line_number':982,'multiline':False]['text':' FSDP loss is fp16, DDP AMP loss is fp32','line_number':996,'multiline':False]['text':' Post-backward, if CPU offloading model params should be on CPU.','line_number':1002,'multiline':False]['text':' Params should always be on CPU','line_number':1005,'multiline':False]['text':' Unscale the gradients and step','line_number':1007,'multiline':False]['text':' Update the scale factor','line_number':1009,'multiline':False]['text':' if save_model, simulate save + load.','line_number':1011,'multiline':False]['text':' Zero params, if save/load state_dict did not work properly, this','line_number':1014,'multiline':False]['text':' would break the parity test with DDP.','line_number':1015,'multiline':False]['text':' Establish reference behavior with DDP','line_number':1064,'multiline':False]['text':' Check against FSDP behavior','line_number':1090,'multiline':False]['text':' Enforce that we wrap with top-level FSDP since we are comparing','line_number':1113,'multiline':False]['text':' assuming a data parallel reference and some test models may not','line_number':1114,'multiline':False]['text':' do so in their `init()` method','line_number':1115,'multiline':False]['text':' Change the model parameter dtype after FSDP initialization','line_number':1118,'multiline':False]['text':' Offloading parameters with `CUDA_AFTER` should raise an error during','line_number':1123,'multiline':False]['text':' lazy initialization due to the parameter devices not being CPU;','line_number':1124,'multiline':False]['text':' otherwise, all parameter devices should be CPU','line_number':1125,'multiline':False]['text':' No need to check for parameter and loss parity if expecting an error','line_number':1158,'multiline':False]['text':' Check parameter devices are CPU if offloading to CPU before calling','line_number':1161,'multiline':False]['text':' `get_full_params()`, which will cast the parameters to FP32','line_number':1162,'multiline':False]['text':' Do not check dtype since the reference DDP loss may not be the same','line_number':1168,'multiline':False]['text':' dtype as the FSDP loss in the case of mixed precision','line_number':1169,'multiline':False]['text':' Do not check for parameter parity if using mixed precision since (1)','line_number':1171,'multiline':False]['text':' the DDP parameters are in FP16 (from `half()`) while the FSDP','line_number':1172,'multiline':False]['text':' parameters are in FP32 (from `summon_full_params()`) and (2) DDP runs','line_number':1173,'multiline':False]['text':' the optimizer in FP16 while FSDP runs it in FP32','line_number':1174,'multiline':False]['text':' TODO: Disable checking the parameters for pure FP16 due to floating','line_number':1175,'multiline':False]['text':' point inaccuracy. Note that this means that the backward pass is not','line_number':1176,'multiline':False]['text':' checked: https://github.com/pytorch/pytorch/issues/90784','line_number':1177,'multiline':False]