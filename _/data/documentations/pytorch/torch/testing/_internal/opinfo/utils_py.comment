['text':' Better way to acquire devices?','line_number':48,'multiline':False]['text':' Class to tag the dynamically generated types.','line_number':53,'multiline':False]['text':' Returns the supported dtypes for the given operator and device_type pair.','line_number':58,'multiline':False]['text':' If `sample_inputs_fn` doesn't support sampling for a given','line_number':71,'multiline':False]['text':' `dtype`, we assume that the `dtype` is not supported.','line_number':72,'multiline':False]['text':' We raise a warning, so that user knows that this was the case','line_number':73,'multiline':False]['text':' and can investigate if there was an issue with the `sample_inputs_fn`.','line_number':74,'multiline':False]['text':' We assume the dtype is supported','line_number':80,'multiline':False]['text':' only if all samples pass for the given dtype.','line_number':81,'multiline':False]['text':' dtype is not supported','line_number':87,'multiline':False]['text':' Function returns the appropriate dispatch function (from COMPLETE_DTYPES_DISPATCH and EXTENSIBLE_DTYPE_DISPATCH)','line_number':98,'multiline':False]['text':' and its string representation for the passed `dtypes`.','line_number':99,'multiline':False]['text':' CUDA is not available, dtypes will be empty.','line_number':102,'multiline':False]['text':' Short circuit if we get an exact match.','line_number':108,'multiline':False]['text':' If user passed dtypes which are lower than the lowest','line_number':124,'multiline':False]['text':' dispatch type available (not likely but possible in code path).','line_number':125,'multiline':False]['text':' Detect if the OpInfo entry acquired dtypes dynamically','line_number':136,'multiline':False]['text':' using `get_supported_dtypes`.','line_number':137,'multiline':False]['text':' Wrapper that passes PyTorch's default scalar','line_number':153,'multiline':False]['text':'   type as an argument to the wrapped NumPy','line_number':154,'multiline':False]['text':'   unary ufunc when given an integer input.','line_number':155,'multiline':False]['text':'   This mimicks PyTorch's integer->floating point','line_number':156,'multiline':False]['text':'   type promotion.','line_number':157,'multiline':False]['text':'','line_number':158,'multiline':False]['text':' This is necessary when NumPy promotes','line_number':159,'multiline':False]['text':'   integer types to double, since PyTorch promotes','line_number':160,'multiline':False]['text':'   integer types to the default scalar type.','line_number':161,'multiline':False]['text':' Helper to determine if promotion is needed','line_number':163,'multiline':False]['text':' As the default dtype can change, acquire it when function is called.','line_number':177,'multiline':False]['text':' NOTE: Promotion in PyTorch is from integer types to the default dtype','line_number':178,'multiline':False]['text':' Copy keys into a set','line_number':208,'multiline':False]['text':' NumPy reductions don't accept dim=0 for scalar inputs','line_number':217,'multiline':False]['text':' so we convert it to None if and only if dim is equivalent','line_number':218,'multiline':False]['text':' Unsqueeze reduced dimensions if NumPy does not support keepdims','line_number':244,'multiline':False]