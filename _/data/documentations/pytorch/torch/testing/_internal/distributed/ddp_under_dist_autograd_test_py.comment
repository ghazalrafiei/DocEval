['text':'!/usr/bin/env python3','line_number':1,'multiline':False]['text':' Trainers + the master + the remote worker','line_number':35,'multiline':False]['text':' Don't apply DDP','line_number':43,'multiline':False]['text':' Apply DDP to the top level nn.Module','line_number':45,'multiline':False]['text':' Embed DDP inside the top level nn.Module','line_number':47,'multiline':False]['text':' add the handlers to the logger','line_number':61,'multiline':False]['text':' Return a linear module with predefined parameters.','line_number':108,'multiline':False]['text':' The same size of mini batch.','line_number':167,'multiline':False]['text':' Split into microbatches, and trim to simulate uneven inputs.','line_number':231,'multiline':False]['text':' Every example has another one that has exactly the same features but an','line_number':279,'multiline':False]['text':' opposite value. Therefore, their grads cancel each other in all-reduce.','line_number':280,'multiline':False]['text':' Split the examples among NUM_TRAINERS trainers','line_number':290,'multiline':False]['text':' The name has to be consistent with that in 'dist_init' decorator.','line_number':322,'multiline':False]['text':' The name has to be consistent with that in 'dist_init' decorator.','line_number':326,'multiline':False]['text':' new_group needs to be called on ranks.','line_number':339,'multiline':False]['text':' new_group needs to be called on ranks.','line_number':412,'multiline':False]['text':' Half the trainers will deplete inputs earlier than the rest.','line_number':420,'multiline':False]['text':' When there are uneven inputs, it is not necessary that grads','line_number':436,'multiline':False]['text':' cancel each other out, since some trainers contribute 0 grad.','line_number':437,'multiline':False]['text':' Destroy process groups','line_number':454,'multiline':False]['text':' Send shutdown signals.','line_number':458,'multiline':False]['text':' Common utils for both CPU and CUDA test suites','line_number':496,'multiline':False]['text':' The name has to be consistent with that in 'dist_init' decorator.','line_number':503,'multiline':False]['text':' Each trainer uses a different random seed. Otherwise, they are going','line_number':514,'multiline':False]['text':' to have exactly the same initial model parameters, input, and','line_number':515,'multiline':False]['text':' therefore grads. That means the grads will be the same before and','line_number':516,'multiline':False]['text':' after DDP's all-reduce.','line_number':517,'multiline':False]['text':' Postfix file_name with "pg" since file_name is also used by RPC agent','line_number':521,'multiline':False]['text':' Odd ranks join early if simulate_uneven_inputs.','line_number':529,'multiline':False]['text':' Use distributed autograd. The gradients will be in RPC context map.','line_number':539,'multiline':False]['text':' Use local autograd. The gradients will be in each variable's '.grad'.','line_number':549,'multiline':False]['text':' The gradients should be the same','line_number':554,'multiline':False]['text':' test with simulating uneven inputs in DDP','line_number':576,'multiline':False]['text':' Each trainer uses a different random seed. Otherwise, they are going','line_number':582,'multiline':False]['text':' to have exactly the same initial model parameters, input, and','line_number':583,'multiline':False]['text':' therefore grads. That means the grads will be the same before and','line_number':584,'multiline':False]['text':' after DDP's all-reduce.','line_number':585,'multiline':False]['text':' Different inputs for each','line_number':597,'multiline':False]['text':' Run local.','line_number':601,'multiline':False]['text':' Each trainer uses a different random seed. Otherwise, they are going','line_number':615,'multiline':False]['text':' to have exactly the same initial model parameters, input, and','line_number':616,'multiline':False]['text':' therefore grads. That means the grads will be the same before and','line_number':617,'multiline':False]['text':' after DDP's all-reduce.','line_number':618,'multiline':False]['text':' Use two different remote device input string, w/ and w/o the default','line_number':627,'multiline':False]['text':' device string "cpu", respectively.','line_number':628,'multiline':False]['text':' Start with the same parameters for remote and local','line_number':634,'multiline':False]['text':' Run local case.','line_number':637,'multiline':False]['text':' Run remote case.','line_number':644,'multiline':False]['text':' Each trainer uses a different random seed. Otherwise, they are going','line_number':667,'multiline':False]['text':' to have exactly the same initial model parameters, input, and','line_number':668,'multiline':False]['text':' therefore grads. That means the grads will be the same before and','line_number':669,'multiline':False]['text':' after DDP's all-reduce.','line_number':670,'multiline':False]['text':' Start with the same parameters for remote and local','line_number':683,'multiline':False]['text':' Start with the same parameters for remote and local','line_number':693,'multiline':False]['text':' Run local case.','line_number':699,'multiline':False]['text':' Run remote case.','line_number':706,'multiline':False]