['text':' Right now we test up to 3-layer nested rpc calls.','line_number':29,'multiline':False]['text':' rpc_done[1] and ctx_ids[1] represent rpc is done in prev rank, and context id','line_number':30,'multiline':False]['text':' sent from prev rank respectively.','line_number':31,'multiline':False]['text':' rpc_done[2] and ctx_ids[2] represents for prev of prev rank.','line_number':32,'multiline':False]['text':' rpc_done[3] and ctx_ids[3] represents for prev of prev of prev rank.','line_number':33,'multiline':False]['text':' rpc_done[0] and ctx_ids[0] represents for current rank, but mostly not used.','line_number':34,'multiline':False]['text':' Send rpc done info and context_id to','line_number':42,'multiline':False]['text':' dst_rank = (self.rank + rank_distance) % self.world_size','line_number':43,'multiline':False]['text':' we don't need a lock here since the GIL is held while executing remote','line_number':44,'multiline':False]['text':' python UDFs, so access is serialized across several workers.','line_number':45,'multiline':False]['text':' This method must be called on the rref owner, and verifies that the grad of','line_number':63,'multiline':False]['text':' rref tensor equals to the given grad.','line_number':64,'multiline':False]['text':' after dist autograd context is cleaned up, it should be cleaned up on other','line_number':141,'multiline':False]['text':' nodes. This helper allows timeout_seconds for those RPCs to be completed, and','line_number':142,'multiline':False]['text':' ensures that all the contexts have been cleaned up in that timeframe.any','line_number':143,'multiline':False]['text':' all contexts have been cleaned up if trying to retrieve any context resulted in a RuntimeError.','line_number':157,'multiline':False]['text':' This function creates a dis autograd context, run rpc_sync on the given ps,','line_number':162,'multiline':False]['text':' and then blocks until the ps has verified the grads are correctly accumulated.','line_number':163,'multiline':False]['text':' prevent deleting dist autograd context','line_number':172,'multiline':False]['text':' This function is the same as _run_trainer, except rpc calls torchscript','line_number':176,'multiline':False]['text':' function "my_script_ref_add" instead of python function "my_rref_add"','line_number':177,'multiline':False]['text':' prevent deleting dist autograd context','line_number':186,'multiline':False]['text':' Run the operation locally.','line_number':208,'multiline':False]['text':' Run the operation using rpc_sync','line_number':209,'multiline':False]['text':' Run the operation using remote.','line_number':210,'multiline':False]['text':' Run the operation using rpc_async','line_number':211,'multiline':False]['text':' Common utils for both CPU and CUDA test suites','line_number':214,'multiline':False]['text':' Verify grads were accumulated appropriately.','line_number':258,'multiline':False]['text':' Verify graph for current context id.','line_number':297,'multiline':False]['text':' Wait for the prev rank to be done with rpc.','line_number':312,'multiline':False]['text':' Verify graph for previous context id.','line_number':314,'multiline':False]['text':' this barrier is needed so one worker does not clean up their','line_number':319,'multiline':False]['text':' autograd context before another worker tries to access it.','line_number':320,'multiline':False]['text':' autograd context should be cleaned up by now.','line_number':323,'multiline':False]['text':' No autograd context available.','line_number':327,'multiline':False]['text':' 3-layer nested calls','line_number':331,'multiline':False]['text':' Barrier to ensure all RPCs are done.','line_number':360,'multiline':False]['text':' Barrier to ensure all set_rpc_done have completed.','line_number':370,'multiline':False]['text':' For self.rank, it has 4 graphs to verify','line_number':373,'multiline':False]['text':' One is for current context id when this rank send first rpc call.','line_number':374,'multiline':False]['text':' Second one is for prev context id when this rank make 1st nested','line_number':375,'multiline':False]['text':' call.','line_number':376,'multiline':False]['text':' Third one is for prev prev context id when this rank make','line_number':377,'multiline':False]['text':' 2nd nested call.','line_number':378,'multiline':False]['text':' Last one is for prev prev prev context id when this rank','line_number':379,'multiline':False]['text':' execute the torch.add() operator.','line_number':380,'multiline':False]['text':' Verify first graph for current context id.','line_number':382,'multiline':False]['text':' Verify second graph for 1st nested call.','line_number':397,'multiline':False]['text':' Verify third graph for 2nd nested call.','line_number':401,'multiline':False]['text':' verify last graph for rpc call execution.','line_number':405,'multiline':False]['text':' this barrier is needed so one worker does not clean up their','line_number':410,'multiline':False]['text':' autograd context before another worker tries to access it.','line_number':411,'multiline':False]['text':' Rank0->Rank1->Rank0','line_number':414,'multiline':False]['text':' For self.rank, it has 2 graphs to verify.','line_number':460,'multiline':False]['text':' One is for current context id when this rank send first rpc','line_number':461,'multiline':False]['text':' call and execute the torch.add() operator.','line_number':462,'multiline':False]['text':' Another one is for prev context id when this rank make','line_number':463,'multiline':False]['text':' nested call.','line_number':464,'multiline':False]['text':' Verify two pairs of send and recv functions for nested','line_number':480,'multiline':False]['text':' call','line_number':481,'multiline':False]['text':' this barrier is needed so one worker does not clean up their','line_number':485,'multiline':False]['text':' autograd context before another worker tries to access it.','line_number':486,'multiline':False]['text':' Wait for the prev rank to be done with rpc.','line_number':520,'multiline':False]['text':' NB: RRef.to_here() always passes the autograd context to the','line_number':522,'multiline':False]['text':' the callee, as the caller does not know whether the return','line_number':523,'multiline':False]['text':' value would contain a requires_grad tensor or not.','line_number':524,'multiline':False]['text':'','line_number':525,'multiline':False]['text':' rpc/remote with udf (_set_rpc_done here) also always passes the','line_number':526,'multiline':False]['text':' autograd context to the callee due to the same reason.','line_number':527,'multiline':False]['text':' Verify appropriate tensors have been attached the autograd graph.','line_number':555,'multiline':False]['text':' Verify that the worker id has been recorded in the context','line_number':564,'multiline':False]['text':' test that in dist autograd, in the case that tensors communicated over RPC do','line_number':573,'multiline':False]['text':' NOT require grad, we still cleanup the dist autograd contexts created','line_number':574,'multiline':False]['text':' on other nodes. This is because the autograd context is still','line_number':575,'multiline':False]['text':' communicated over RPC even if tensor arguments do not require grad, as','line_number':576,'multiline':False]['text':'  it is possible that the response could.','line_number':577,'multiline':False]['text':' the thread's context id should be cleaned up','line_number':597,'multiline':False]['text':' Ensure all peers have finished mutating the','line_number':600,'multiline':False]['text':' `known_context_ids` set.','line_number':601,'multiline':False]['text':' check that all contexts have been cleaned up.','line_number':603,'multiline':False]['text':' Now populate .grad with local autograd engine and','line_number':621,'multiline':False]['text':' verify dist autograd doesn't mess with it.','line_number':622,'multiline':False]['text':' The current rank first creates a tensor on the rref_owner, and then passes','line_number':638,'multiline':False]['text':' the rref with another tensor to the callee to run either my_rref_add or','line_number':639,'multiline':False]['text':' my_nested_rref_add, depending on whether the callee is the rref owner.','line_number':640,'multiline':False]['text':' The grad of tensor lives on the current rank, and the grad of the rref','line_number':641,'multiline':False]['text':' tensor lives on the rref owner.','line_number':642,'multiline':False]['text':' verify grads on caller','line_number':672,'multiline':False]['text':' verify grads on rref owner','line_number':677,'multiline':False]['text':' In this test, every rank will serve as a parameter server (ps) and a','line_number':686,'multiline':False]['text':' driver, and then kicks off trainers on the other three ranks. So, we have:','line_number':687,'multiline':False]['text':' ps = rank0 with trainers = rank1/2/3','line_number':688,'multiline':False]['text':' ps = rank2 with trainers = rank2/3/0','line_number':689,'multiline':False]['text':' ps = rank3 with trainers = rank3/0/1','line_number':690,'multiline':False]['text':' ps = rank4 with trainers = rank0/1/2','line_number':691,'multiline':False]['text':'','line_number':692,'multiline':False]['text':' These four test ps-trainer groups run on completely separate autograd','line_number':693,'multiline':False]['text':' graphs, but they share the same set of underlying RpcAgents.','line_number':694,'multiline':False]['text':' create rref on self','line_number':709,'multiline':False]['text':' kick off forward and backward pass on three other workers (trainers)','line_number':715,'multiline':False]['text':' check if the trainers have done with their backward pass','line_number':727,'multiline':False]['text':' trainers are done and holding the context for verification','line_number':731,'multiline':False]['text':' make sure grads are accumulated for the same tensors and values','line_number':734,'multiline':False]['text':' are all correct','line_number':735,'multiline':False]['text':' unblock trainers','line_number':742,'multiline':False]['text':' wait until all trainers are done','line_number':745,'multiline':False]['text':' Multiple RPCs between different nodes.','line_number':751,'multiline':False]['text':' Run the same code locally and with dist autograd and verify gradients','line_number':783,'multiline':False]['text':' are same.','line_number':784,'multiline':False]['text':' Run the same code locally and with dist autograd and verify gradients','line_number':798,'multiline':False]['text':' are same.','line_number':799,'multiline':False]['text':' Run backward twice.','line_number':830,'multiline':False]['text':' Now run distributed autograd.','line_number':845,'multiline':False]['text':' Run backward in a loop multiple times.','line_number':890,'multiline':False]['text':' For current context, this rank sends t1 and t2 tensors to dst_rank,','line_number':894,'multiline':False]['text':' then get t3 = torch.add(t1, t2) result tensor.','line_number':895,'multiline':False]['text':' For the current context in this rank, it expects graph like this:','line_number':896,'multiline':False]['text':'  send function:','line_number':897,'multiline':False]['text':'              rpcSendBackward','line_number':898,'multiline':False]['text':'                  /          \','line_number':899,'multiline':False]['text':'  t1.AccumulateGrad         t2.AccumulateGrad','line_number':900,'multiline':False]['text':'','line_number':901,'multiline':False]['text':'  recv function:','line_number':902,'multiline':False]['text':'','line_number':903,'multiline':False]['text':'            |','line_number':904,'multiline':False]['text':'          t3.rpcRecvBackward','line_number':905,'multiline':False]['text':'','line_number':906,'multiline':False]['text':' Retrieve the next functions in the graph.','line_number':910,'multiline':False]['text':' We should now hit t1 and t2 in the autograd graph.','line_number':914,'multiline':False]['text':' Test recv functions.','line_number':922,'multiline':False]['text':' Run the same code locally and with dist autograd and verify gradients','line_number':925,'multiline':False]['text':' are same.','line_number':926,'multiline':False]['text':' For a context passed from previous nested chain calls, this rank','line_number':942,'multiline':False]['text':' receives two tensors t1 and t2, executes torch.add(t1, t2) and sends','line_number':943,'multiline':False]['text':' result tensor t3 back.','line_number':944,'multiline':False]['text':' For this context in this rank, it expects graph like this:','line_number':945,'multiline':False]['text':'  send and recv functions:','line_number':946,'multiline':False]['text':'       rpcSendBackward','line_number':947,'multiline':False]['text':'           |','line_number':948,'multiline':False]['text':'          t3.AddBackward0','line_number':949,'multiline':False]['text':'          /             \','line_number':950,'multiline':False]['text':' t1.recvRpcBackward    t2.recvRpcBackward','line_number':951,'multiline':False]['text':' Verify next function is AddBackward0','line_number':953,'multiline':False]['text':' Verify the next two functions are the same recv backward function.','line_number':959,'multiline':False]['text':' For a context passed from previous nested chain calls, this rank','line_number':970,'multiline':False]['text':' receives two tensors t1 and t2, forwards t1 and t2 tensors using','line_number':971,'multiline':False]['text':' nested rpc call to next dst. In return route, receive result tensor t3','line_number':972,'multiline':False]['text':' from next dst and forwarding t3 back to previous calls.','line_number':973,'multiline':False]['text':' For this context in this rank, it expects graph like this:','line_number':974,'multiline':False]['text':'  send and recv functions for receiving and forwarding t1 and t2:','line_number':975,'multiline':False]['text':'       rpcSendBackward','line_number':976,'multiline':False]['text':'          /          \','line_number':977,'multiline':False]['text':' t1.recvRpcBackward    t2.recvRpcBackward','line_number':978,'multiline':False]['text':'  send and recv functions for receiving and forwarding t3:','line_number':979,'multiline':False]['text':'       rpcSendBackward','line_number':980,'multiline':False]['text':'             |','line_number':981,'multiline':False]['text':'           t3.recvRpcBackward','line_number':982,'multiline':False]['text':' For send function when making nest rpc call,','line_number':987,'multiline':False]['text':' next functions of the send function are two recv functions','line_number':988,'multiline':False]['text':' for received two tensors from previous call','line_number':989,'multiline':False]['text':' For send function when returning response to previous call','line_number':1000,'multiline':False]['text':' next function of the send function is the recv function','line_number':1001,'multiline':False]['text':' for received tensor result returned from nested call','line_number':1002,'multiline':False]['text':' Sparse tests only work with TensorPipeAgent.','line_number':1012,'multiline':False]['text':' Run equivalent of _nested_python_udf locally.','line_number':1204,'multiline':False]['text':' requires_grad = True to record send/recv functions','line_number':1239,'multiline':False]['text':' Run backward twice.','line_number':1245,'multiline':False]['text':' Run backward twice to test accumulation of sparse gradients.','line_number':1257,'multiline':False]['text':' Verify max possible id.','line_number':1273,'multiline':False]['text':' First 16 bits should be worker_id.','line_number':1286,'multiline':False]['text':' Nested contexts not supported.','line_number':1300,'multiline':False]['text':' Wait for the prev rank to be done with rpc.','line_number':1366,'multiline':False]['text':' due to the above get_gradients call, ensure that dist autograd','line_number':1372,'multiline':False]['text':' contexts aren't cleaned up until all workers exit context managers','line_number':1373,'multiline':False]['text':' if no tensors require grad, we should still record worker_ids, as','line_number':1421,'multiline':False]['text':' the autograd context ID is still passed to other workers.','line_number':1422,'multiline':False]['text':' all worker_ids in dst_ranks should be recorded.','line_number':1430,'multiline':False]['text':' worker_ids should be recorded when tensors do require grad','line_number':1435,'multiline':False]['text':' all worker_ids in dst_ranks should be recorded.','line_number':1445,'multiline':False]['text':' There should be at least 1 send and recv_events each, corresponding to send/recv functions executed.','line_number':1466,'multiline':False]['text':' The CPU total for backward event should be great than send and recv, since','line_number':1469,'multiline':False]['text':' applying those functions in the backwards pass is a subset of the entire backward pass.','line_number':1470,'multiline':False]['text':' This should throw an error since matrix sizes don't match.','line_number':1481,'multiline':False]['text':' TODO, need more investigation','line_number':1564,'multiline':False]['text':' there is rref leak when shutting down, suspect it is because','line_number':1565,'multiline':False]['text':' ref as arg is passed to pybind boundary, and the ref is not garbage','line_number':1566,'multiline':False]['text':' collected by python when calling shutdown()','line_number':1567,'multiline':False]['text':' We don't use the result of an RPC function, as a result the','line_number':1650,'multiline':False]['text':' backward pass would hang in the "FAST" mode.','line_number':1651,'multiline':False]['text':' Run backward, this would hang forever.','line_number':1658,'multiline':False]['text':' Run the test in a thread which would never finish.','line_number':1663,'multiline':False]['text':' Wait for 10s.','line_number':1669,'multiline':False]['text':' Verify thread is still alive (indicating backward hasn't completed yet).','line_number':1671,'multiline':False]['text':' Perform some ops before error simulation.','line_number':1679,'multiline':False]['text':' Run multiple round trips across different nodes and verify the','line_number':1683,'multiline':False]['text':' original node receives an error thrown on a node deep in the chain.','line_number':1684,'multiline':False]['text':' Run backwards, and validate we receive an error.','line_number':1701,'multiline':False]['text':' 5 seconds','line_number':1710,'multiline':False]['text':' Wait for all RPCs to be done.','line_number':1720,'multiline':False]['text':' Kill all odd rank nodes.','line_number':1723,'multiline':False]['text':' Wait for all other nodes to die.','line_number':1726,'multiline':False]['text':' Shutdown sequence is not very well defined and as a result','line_number':1731,'multiline':False]['text':' we might see any error given by get_shutdown_error_regex()','line_number':1732,'multiline':False]['text':' Run backwards, and validate we receive an error since all','line_number':1734,'multiline':False]['text':' other nodes are dead.','line_number':1735,'multiline':False]['text':' Exit all other nodes.','line_number':1738,'multiline':False]['text':' dummy context_id','line_number':1746,'multiline':False]['text':' Run the same code locally and with dist autograd and verify gradients','line_number':1852,'multiline':False]['text':' are same.','line_number':1853,'multiline':False]['text':' Set a short timeout to quickly time out failed RPCs.','line_number':1907,'multiline':False]['text':' 5 seconds','line_number':1908,'multiline':False]['text':' Kill rank 2 (last hop of nested rpc) and verify rank 0 receives an error.','line_number':1924,'multiline':False]['text':' Wait for rank 2 to die.','line_number':1930,'multiline':False]['text':' Shutdown sequence is not very well defined and as a result','line_number':1933,'multiline':False]['text':' we might see any error given by get_shutdown_error_regex().','line_number':1934,'multiline':False]['text':' Run backwards, and validate we receive an error since rank 2 is dead.','line_number':1936,'multiline':False]['text':' Mark rank 0 is done in the store, since the RPC framework on','line_number':1939,'multiline':False]['text':' some nodes might be broken at this point.','line_number':1940,'multiline':False]['text':' Wait for backward to finish on rank 0.','line_number':1943,'multiline':False]['text':' Run equivalent of _nested_python_udf locally.','line_number':1955,'multiline':False]['text':' Release the context to simulate error (use barrier before releasing','line_number':1974,'multiline':False]['text':' context to ensure all nodes execute the backward function).','line_number':1975,'multiline':False]['text':' Verify all contexts are cleaned up.','line_number':1981,'multiline':False]['text':' Send the context id to all nodes.','line_number':2004,'multiline':False]['text':' Verify all context ids have been received.','line_number':2016,'multiline':False]['text':' Call MyBackwardFunc as the first op of the backward pass to','line_number':2024,'multiline':False]['text':' ensure we release the context early in the backward pass.','line_number':2025,'multiline':False]['text':' dummy context_id','line_number':2029,'multiline':False]['text':' HACK: Killing workers since otherwise the autograd engine gets stuck on','line_number':2036,'multiline':False]['text':' other nodes. The proper fix would be addressing:','line_number':2037,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/27643, which would inform','line_number':2038,'multiline':False]['text':' other nodes about the failure.','line_number':2039,'multiline':False]['text':' The autograd engine gets stuck on other nodes since they're waiting to','line_number':2040,'multiline':False]['text':' receive gradients from the node that received an error (and as a','line_number':2041,'multiline':False]['text':' result it didn't execute the rest of the graph).','line_number':2042,'multiline':False]['text':' Hard to validate exact numbers because of the distributed nature.','line_number':2085,'multiline':False]['text':' We can't use a barrier() here since that would block the single','line_number':2086,'multiline':False]['text':' CPU thread available for autograd and can cause deadlocks.','line_number':2087,'multiline':False]['text':' Call custom function in middle of backward pass to ensure all','line_number':2108,'multiline':False]['text':' nodes are still waiting on a backward().','line_number':2109,'multiline':False]['text':' Need atleast one context and not more than 4.','line_number':2124,'multiline':False]['text':' Validate information','line_number':2136,'multiline':False]['text':' only have `num_current_backward_passes` and `num_autograd contexts`','line_number':2140,'multiline':False]['text':' All contexts should be cleaned up.','line_number':2145,'multiline':False]['text':' All other ranks schedule work on rank 0.','line_number':2171,'multiline':False]['text':' Run backward twice.','line_number':2189,'multiline':False]['text':' Run backward twice.','line_number':2196,'multiline':False]['text':' Verify the gradients are same for local and remote execution.','line_number':2200,'multiline':False]['text':' Run backward in a loop multiple times.','line_number':2240,'multiline':False]['text':' Recovered from error.','line_number':2246,'multiline':False]['text':' Sync before resetting flag.','line_number':2253,'multiline':False]['text':' Reset the flag.','line_number':2256,'multiline':False]['text':' Double the gradient.','line_number':2262,'multiline':False]['text':' create autograd function that saves grad pointer as class static','line_number':2280,'multiline':False]['text':' non-contiguous grad should be copied','line_number':2317,'multiline':False]['text':' test case that should trigger no copy for a','line_number':2324,'multiline':False]['text':' Verify there was no clone.','line_number':2330,'multiline':False]['text':' Test case that should trigger copy for both of a,b. This is','line_number':2333,'multiline':False]['text':' different in the distributed autograd case since we hold','line_number':2334,'multiline':False]['text':' a reference to all grads in a vector until all accumulation is done.','line_number':2335,'multiline':False]['text':' check a,b uses different grad buffer','line_number':2342,'multiline':False]['text':' both should be copied.','line_number':2344,'multiline':False]['text':' create autograd function that saves grad pointer as class static','line_number':2350,'multiline':False]['text':' Create a sparse tensor with non-contiguous indices and values','line_number':2372,'multiline':False]['text':' and return as grad.','line_number':2373,'multiline':False]['text':' test case that should trigger no copy for a.','line_number':2388,'multiline':False]['text':' check a uses the same buffer','line_number':2396,'multiline':False]['text':' Run backwards multiple times.','line_number':2399,'multiline':False]['text':' non-contiguous indices and value, we should trigger a copy.','line_number':2403,'multiline':False]['text':' check a,b uses different grad buffer','line_number':2412,'multiline':False]['text':' Verify we cloned both grads.','line_number':2414,'multiline':False]['text':' Run backwards multiple times to verify accumulation.','line_number':2418,'multiline':False]['text':' create autograd function that saves grad pointer as class static','line_number':2424,'multiline':False]['text':' indices() and values() return views, so holding onto','line_number':2437,'multiline':False]['text':' references of them would not increment refcount of indices','line_number':2438,'multiline':False]['text':' and values inside the sparse tensor.','line_number':2439,'multiline':False]['text':' grad would be stolen, since static_grad_indices_ref and','line_number':2458,'multiline':False]['text':' static_grad_values_ref are holding onto views and don't bump the','line_number':2459,'multiline':False]['text':' refcount.','line_number':2460,'multiline':False]['text':' Register post hooks','line_number':2478,'multiline':False]['text':' due to slow add, the continuation of this backward pass will be','line_number':2515,'multiline':False]['text':' invoked by the previous rpc.remote thread which does not have a','line_number':2516,'multiline':False]['text':' valid context_id. So, this can test whether we propagate','line_number':2517,'multiline':False]['text':' thread_local states properly when jumping across threads on the','line_number':2518,'multiline':False]['text':' server side.','line_number':2519,'multiline':False]['text':' Run a few iterations.','line_number':2550,'multiline':False]['text':' Root is CPU','line_number':2554,'multiline':False]['text':' Autograd graph consists of CPU -> GPU -> CPU execution.','line_number':2563,'multiline':False]['text':' Run a few iterations.','line_number':2574,'multiline':False]['text':' Root is CPU','line_number':2578,'multiline':False]['text':' Autograd graph consists of CPU -> GPU -> CPU execution.','line_number':2586,'multiline':False]['text':' Reusing a simplified helper function from DistAutogradTest to ensure','line_number':2594,'multiline':False]['text':' autograd context is successfully cleaned up even when RPCs are failing.','line_number':2595,'multiline':False]['text':' test that in dist autograd, in the case that tensors communicated over RPC do','line_number':2599,'multiline':False]['text':' NOT require grad, we still cleanup the dist autograd contexts created','line_number':2600,'multiline':False]['text':' on other nodes. This is because the autograd context is still','line_number':2601,'multiline':False]['text':' communicated over RPC even if tensor arguments do not require grad, as','line_number':2602,'multiline':False]['text':' it is possible that the response could.','line_number':2603,'multiline':False]['text':' the thread's context id should be cleaned up','line_number':2612,'multiline':False]['text':' Ensure all peers have finished mutating the','line_number':2615,'multiline':False]['text':' `known_context_ids` set.','line_number':2616,'multiline':False]['text':' check that all contexts have been cleaned up.','line_number':2618,'multiline':False]['text':' no faulty_messages defined so this fails all retryable messages - see','line_number':2622,'multiline':False]['text':' faulty_rpc_agent_test_fixture.py for the list of retryable messages.','line_number':2623,'multiline':False]['text':' The reverse of this device mapping should be used for the backward pass.','line_number':2658,'multiline':False]['text':' The reverse of this device mapping should be used for the backward pass.','line_number':2701,'multiline':False]['text':' Run local autograd','line_number':2716,'multiline':False]['text':' Run distributed autograd','line_number':2722,'multiline':False]['text':' Compare grads.','line_number':2728,'multiline':False]['text':' this is master','line_number':2749,'multiline':False]['text':' local iteration','line_number':2761,'multiline':False]['text':' remote iteration','line_number':2765,'multiline':False]