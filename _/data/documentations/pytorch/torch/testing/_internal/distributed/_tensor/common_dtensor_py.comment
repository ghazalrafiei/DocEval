['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' We use this as a proxy for "multiple GPUs exist"','line_number':42,'multiline':False]['text':' when we actually have multiple GPUs, relax the requirement to smaller counts.','line_number':44,'multiline':False]['text':' The builtin @skip_if_no_gpu relies on os.environ['WORLD_SIZE'] being set.','line_number':75,'multiline':False]['text':' pyre-ignore[16]','line_number':101,'multiline':False]['text':' pyre-ignore[16]','line_number':102,'multiline':False]['text':' set device for nccl pg for collectives','line_number':105,'multiline':False]['text':' Wait for all ranks to reach here before starting shutdown.','line_number':110,'multiline':False]['text':' FIXME dist.barrier deadlocks with multiple threads and NCCL: https://github.com/pytorch/pytorch/issues/95895','line_number':111,'multiline':False]['text':' dist.all_reduce(torch.zeros((1,), device="cuda" if torch.cuda.is_available() else "cpu"))','line_number':112,'multiline':False]['text':' FIXME can't use the above all_reduce as it causes hangs on bionic and focal. It hangs:','line_number':113,'multiline':False]['text':'  test_dtensor.py  -- DTensorMeshTest.test_dtensor_device_mesh_device_conversion','line_number':114,'multiline':False]['text':' pyre-ignore[2]:','line_number':122,'multiline':False]['text':' pyre can't find assertTrue anymore?','line_number':127,'multiline':False]['text':' wrapper to initialize comms (processgroup)','line_number':138,'multiline':False]['text':' pyre-ignore[6]','line_number':142,'multiline':False]['text':' type: ignore[misc]','line_number':144,'multiline':False]['text':' if backend not specified, and cuda available, then use nccl, else gloo','line_number':146,'multiline':False]['text':' type: ignore[misc]','line_number':153,'multiline':False]['text':' Convert the config mapping to a list to have a fixed order','line_number':179,'multiline':False]['text':' Map keyword to chosen value','line_number':184,'multiline':False]['text':' This is a class for converting args/kwargs of an op into distributed args/kwargs','line_number':208,'multiline':False]['text':' TODO: dist tensor need to support quantized and sparse','line_number':246,'multiline':False]['text':' tensors, quantized tensor might be relatively easy, but','line_number':247,'multiline':False]['text':' sparse tensor have special layouts that we need to possibly','line_number':248,'multiline':False]['text':' deal with, until we are clear about them, we don't officially','line_number':249,'multiline':False]['text':' support them.','line_number':250,'multiline':False]['text':' We need a way to test if a tensor is batched but there','line_number':262,'multiline':False]['text':' is no official APi to do it','line_number':263,'multiline':False]['text':' torch._C._is_batched(t),','line_number':264,'multiline':False]['text':' c10d collective does not support bool tensor','line_number':273,'multiline':False]['text':' for bool tensor we treat it as replicated','line_number':274,'multiline':False]['text':' only generating choices with: replicate, or sharding','line_number':276,'multiline':False]['text':' evenly on a dimension that could be sharded','line_number':277,'multiline':False]['text':' TODO: add multi mesh choices','line_number':283,'multiline':False]['text':' all_choices = itertools.product(','line_number':284,'multiline':False]['text':'     *(self.mesh.ndim * [sharding_choices])','line_number':285,'multiline':False]['text':' )','line_number':286,'multiline':False]['text':' scalar tensor by default will be replicated','line_number':335,'multiline':False]['text':' distribute non-scalar tensors','line_number':338,'multiline':False]['text':' type: ignore[assignment]','line_number':341,'multiline':False]['text':' Blindly converting tensor subclasses to dist tensor can cause','line_number':349,'multiline':False]['text':' unpredictable problems, we explicitly disable this conversion','line_number':350,'multiline':False]['text':' for now (i.e. we don't support DTensor holding tensor subclass','line_number':351,'multiline':False]['text':' until there's a strong reason later).','line_number':352,'multiline':False]