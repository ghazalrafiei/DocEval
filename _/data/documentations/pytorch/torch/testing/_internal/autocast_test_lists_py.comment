['text':' weight_ih','line_number':13,'multiline':False]['text':' weight_hh','line_number':14,'multiline':False]['text':' bias_ih','line_number':15,'multiline':False]['text':' bias_hh','line_number':16,'multiline':False]['text':' returns args as a tuple','line_number':18,'multiline':False]['text':' Supplies ops and arguments for test_autocast_* in test/test_cuda.py','line_number':21,'multiline':False]['text':' Utility arguments, created as one-element tuples','line_number':25,'multiline':False]['text':' The lists below organize ops that autocast needs to test.','line_number':46,'multiline':False]['text':' self.list_name corresponds to test_autocast_list_name in test/test_cuda.py.','line_number':47,'multiline':False]['text':' Each op is associated with a tuple of valid arguments.','line_number':48,'multiline':False]['text':' In addition, cudnn conv ops are not supported on ROCm and hence will','line_number':49,'multiline':False]['text':' be skipped by passing TEST_WITH_ROCM flag to those ops in self.torch_fp16 list.','line_number':50,'multiline':False]['text':' Some ops implement built-in type promotion.  These don't need autocasting,','line_number':52,'multiline':False]['text':' but autocasting relies on their promotion, so we include tests to double-check.','line_number':53,'multiline':False]['text':' The remaining lists organize ops that autocast treats explicitly.','line_number':80,'multiline':False]['text':' deprecated _convolution','line_number':82,'multiline':False]['text':' the current  _convolution','line_number':85,'multiline':False]['text':' _thnn_fused_lstm_cell and _thnn_fused_gru_cell are not Python-exposed as far as I can tell.','line_number':115,'multiline':False]['text':' ("_thnn_fused_lstm_cell", mat0_fp32 + mat1_fp32 + mat2_fp32 + pointwise0_fp32 + pointwise1_fp32),','line_number':116,'multiline':False]['text':' ("_thnn_fused_gru_cell", mat0_fp32 + mat1_fp32 + mat2_fp32 + pointwise0_fp32 + pointwise1_fp32),','line_number':117,'multiline':False]['text':' ("pow", (1.7,) + pointwise0_fp16), # This variant has a backend, but is not documented in the API.','line_number':140,'multiline':False]['text':' these need magma','line_number':147,'multiline':False]['text':' ("norm", mat0_fp16, {"p": "nuc"}),','line_number':148,'multiline':False]['text':' ("norm", mat0_fp16, {"p": "nuc", "dim": 0}),','line_number':149,'multiline':False]['text':' Supplies ops and arguments for test_autocast_* in test/test_cpu.py','line_number':235,'multiline':False]['text':' Utility arguments, created as one-element tuples','line_number':239,'multiline':False]['text':' The lists below organize ops that autocast needs to test.','line_number':274,'multiline':False]['text':' self.list_name corresponds to test_autocast_list_name in test/test_cpu.py.','line_number':275,'multiline':False]['text':' Each op is associated with a tuple of valid arguments.','line_number':276,'multiline':False]['text':' Some ops implement built-in type promotion.  These don't need autocasting,','line_number':278,'multiline':False]['text':' but autocasting relies on their promotion, so we include tests to double-check.','line_number':279,'multiline':False]['text':' The remaining lists organize ops that autocast treats explicitly.','line_number':303,'multiline':False]