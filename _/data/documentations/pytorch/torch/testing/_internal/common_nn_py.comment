['text':' NOTE [How to check NN module / functional API parity between Python and C++ frontends]','line_number':44,'multiline':False]['text':'','line_number':45,'multiline':False]['text':' The way to check API parity is to add parity tests for the NN module / functional of interest.','line_number':46,'multiline':False]['text':' Here are the detailed steps:','line_number':47,'multiline':False]['text':'','line_number':48,'multiline':False]['text':' For NN module:','line_number':49,'multiline':False]['text':' 1. Make sure you already have a test dict with the module configuration you want to test.','line_number':50,'multiline':False]['text':' 2. Add `cpp_constructor_args` entry to the test dict, with its value exactly matching','line_number':51,'multiline':False]['text':'    the Python module constructor arguments. For example, if in the test dict we pass','line_number':52,'multiline':False]['text':'    `(10, 8)` to `torch.nn.Linear` constructor, then we should pass `torch::nn::LinearOptions(10, 8)`','line_number':53,'multiline':False]['text':'    as the corresponding C++ constructor argument to `torch::nn::Linear`.','line_number':54,'multiline':False]['text':' 3. If in the process of performing the above step you referenced any variables','line_number':55,'multiline':False]['text':'    in the `cpp_constructor_args` entry, you must add `cpp_var_map` entry','line_number':56,'multiline':False]['text':'    to the test dict to make sure that those variables are populated with the right Python values.','line_number':57,'multiline':False]['text':'    For example, if the Python constructor call is','line_number':58,'multiline':False]['text':'    `torch.nn.FractionalMaxPool2d(2, output_ratio=0.5, _random_samples=random_samples)`,','line_number':59,'multiline':False]['text':'    the corresponding C++ constructor argument is','line_number':60,'multiline':False]['text':'    `torch::nn::FractionalMaxPool2dOptions(2).output_ratio(0.5)._random_samples(random_samples)`,','line_number':61,'multiline':False]['text':'    and the `cpp_var_map` entry must be','line_number':62,'multiline':False]['text':'    `{'random_samples': random_samples}` in order to populate the C++ variable `random_samples`','line_number':63,'multiline':False]['text':'    used in the C++ constructor argument with the Python tensor value `random_samples`.','line_number':64,'multiline':False]['text':'','line_number':65,'multiline':False]['text':' For NN functional:','line_number':66,'multiline':False]['text':' 1. Make sure you already have a test dict with the functional configuration you want to test.','line_number':67,'multiline':False]['text':' 2. If the test dict's `constructor` entry looks like `wrap_functional(F.some_functional_name, ...)`,','line_number':68,'multiline':False]['text':'    then you must add `cpp_options_args` entry to the test dict, with its value exactly matching the Python','line_number':69,'multiline':False]['text':'    functional optional arguments. For example, if the test dict's `constructor` entry is','line_number':70,'multiline':False]['text':'    `wrap_functional(F.interpolate, size=12, scale_factor=None, mode='nearest')`,','line_number':71,'multiline':False]['text':'    then the `cpp_options_args` entry should be','line_number':72,'multiline':False]['text':'    "F::InterpolateFuncOptions().size(std::vector<int64_t>({12})).scale_factor(c10::nullopt).mode(torch::kNearest)".','line_number':73,'multiline':False]['text':' 3. Otherwise, if the test dict's `constructor` entry looks like','line_number':74,'multiline':False]['text':'    `wrap_functional(lambda i: F.some_functional_name(...))`,','line_number':75,'multiline':False]['text':'    then you must add `cpp_function_call` entry to the test dict, with its value exactly matching the Python','line_number':76,'multiline':False]['text':'    functional function call. For example, if the test dict's `constructor` entry is','line_number':77,'multiline':False]['text':'    `wrap_functional(lambda i: F.poisson_nll_loss(i, t.type_as(i), reduction='none'))`,','line_number':78,'multiline':False]['text':'    then the `cpp_function_call` entry should be','line_number':79,'multiline':False]['text':'    "F::poisson_nll_loss(i, t.to(i.options()), F::PoissonNLLLossFuncOptions().reduction(torch::kNone))".','line_number':80,'multiline':False]['text':' 4. If in the process of performing the above two steps you referenced any variables','line_number':81,'multiline':False]['text':'    in the `cpp_options_args` or `cpp_function_call` entry, you must','line_number':82,'multiline':False]['text':'    add `cpp_var_map` entry to the test dict to make sure that those variables','line_number':83,'multiline':False]['text':'    are populated with the right Python values. For example, if the test dict's `constructor` entry is','line_number':84,'multiline':False]['text':'    `wrap_functional(lambda i: F.poisson_nll_loss(i, t.type_as(i), reduction='none'))`,','line_number':85,'multiline':False]['text':'    then the `cpp_function_call` entry should be','line_number':86,'multiline':False]['text':'    "F::poisson_nll_loss(i, t.to(i.options()), F::PoissonNLLLossFuncOptions().reduction(torch::kNone))".','line_number':87,'multiline':False]['text':'    Notice that there are two variables `i` and `t` that need to have their values provided,','line_number':88,'multiline':False]['text':'    and the way to do so is to add a `cpp_var_map` entry: `cpp_var_map={'i': '_get_input()', 't': t}`.','line_number':89,'multiline':False]['text':'    (Note that for `i`, since we want it to take the Python input value, we pass '_get_input()' string as value','line_number':90,'multiline':False]['text':'    and the C++ parity test mechanism will populate `i` with the Python input value correctly.)','line_number':91,'multiline':False]['text':'','line_number':92,'multiline':False]['text':' There are also a few optional flags in the test dict to control the C++ parity test behavior:','line_number':93,'multiline':False]['text':'','line_number':94,'multiline':False]['text':' - `test_cpp_api_parity`: if `False`, skips the C++ parity test for this test dict. Default: True.','line_number':95,'multiline':False]['text':' - `has_parity`: if `False`, expects this test dict to fail the C++ parity test. Default: True.','line_number':96,'multiline':False]['text':' TODO: reference function','line_number':142,'multiline':False]['text':' TODO(#50743): Figure out the error. "RuntimeError: Unrecognized tensor type ID: Batched"','line_number':149,'multiline':False]['text':' Generates rand tensor with non-equal values. This ensures that duplicate','line_number':156,'multiline':False]['text':' values won't be causing test failure for modules like MaxPooling.','line_number':157,'multiline':False]['text':' size should be small, otherwise randperm fails / long overflows.','line_number':158,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/5006','line_number':2261,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/5006','line_number':2288,'multiline':False]['text':' trigger the last-dim algo in CUDA','line_number':2296,'multiline':False]['text':' trigger special case of spatial CUDA algo','line_number':2313,'multiline':False]['text':' regular spatial algorithm','line_number':2321,'multiline':False]['text':' regular spatial algorithm','line_number':2329,'multiline':False]['text':' trigger the last-dim algo in CUDA','line_number':2364,'multiline':False]['text':' trigger special case of spatial CUDA algo','line_number':2372,'multiline':False]['text':' regular spatial algorithm','line_number':2380,'multiline':False]['text':' TODO(#50743): figure out the error','line_number':2515,'multiline':False]['text':' RuntimeError: The size of tensor a (6) must match the size of tensor b (4)','line_number':2516,'multiline':False]['text':' at non-singleton dimension 2','line_number':2517,'multiline':False]['text':' add conv padding mode tests:','line_number':2623,'multiline':False]['text':' conv signature:','line_number':2627,'multiline':False]['text':'     in_channels, out_channels, kernel_size, stride=1,','line_number':2628,'multiline':False]['text':'     padding=0, dilation=1, groups=1,','line_number':2629,'multiline':False]['text':'     bias=True, padding_mode='zeros'','line_number':2630,'multiline':False]['text':' FIXME: remove after implementing reflection pad 3d','line_number':2633,'multiline':False]['text':'        https://github.com/pytorch/pytorch/issues/27655','line_number':2634,'multiline':False]['text':' simplified from `(4 + 2 * p - 3) // 2 + 1`','line_number':2639,'multiline':False]['text':' Check that non linear activations work with no batch dimensions','line_number':2661,'multiline':False]['text':' For RRelu, test that compare CPU and GPU results fail because RNG','line_number':2673,'multiline':False]['text':' is different between CPU and GPU','line_number':2674,'multiline':False]['text':' TODO: This code can path can be removed if #61309 is resolved','line_number':2798,'multiline':False]['text':' loss is normalized by the weights to be consistent with nll_loss_nd','line_number':2799,'multiline':False]['text':' when beta <= 0 we should just use l1_loss','line_number':2853,'multiline':False]['text':' make everything 2-dimensional','line_number':2894,'multiline':False]['text':' we know we have (1, C) X (1, C) -> (1,), so squeeze will get us','line_number':2912,'multiline':False]['text':' back to correct dimensionality','line_number':2913,'multiline':False]['text':' this directly follows Graves et al's paper, in contrast to the production implementation, it does not use log-space','line_number':3015,'multiline':False]['text':' we need the accuracy as we are not in logspace','line_number':3020,'multiline':False]['text':' reduction is 'sum' or 'mean' which results in a scalar','line_number':3104,'multiline':False]['text':' Check that regression criterion work with no batch dimensions','line_number':3108,'multiline':False]['text':' Check that classification criterion work with no batch dimensions','line_number':3139,'multiline':False]['text':' List of tuples of (name, input_fn, target_fn)','line_number':3140,'multiline':False]['text':' For MarginRankingLoss, input_fn : (x1, x2) and target_fn : target','line_number':3157,'multiline':False]['text':' For TripletMarginLoss, input_fn : (anchor, positive) and target_fn : negative','line_number':3159,'multiline':False]['text':' TODO : Fix these discrepancies','line_number':3170,'multiline':False]['text':' _forward is defined in classes inheriting from NNTestCase','line_number':3197,'multiline':False]['text':' make non grad zeros','line_number':3257,'multiline':False]['text':' Tensors will accumulate gradient from multiple steps','line_number':3266,'multiline':False]['text':' get_numerical_jacobian returns a list of tuples but we require a tensor','line_number':3297,'multiline':False]['text':' TODO: compare structure (ensure analytic jacobian has correct shape)','line_number':3313,'multiline':False]['text':' type: ignore[type-var]','line_number':3315,'multiline':False]['text':' TODO: do this with in-memory files as soon as torch.save will support it','line_number':3432,'multiline':False]['text':' Always making only the last dimension noncontiguous is easy to hide','line_number':3449,'multiline':False]['text':' bugs because .view(-1) will still work. So try to find a dim with size','line_number':3450,'multiline':False]['text':' > 1 and make that non-contiguous, i.e., stack + select on the','line_number':3451,'multiline':False]['text':' dimension directly after that.','line_number':3452,'multiline':False]['text':' check no scalars, can't make non-contig','line_number':3464,'multiline':False]['text':' Some ops, e.g., nn.Flatten, return gradient that shares','line_number':3485,'multiline':False]['text':' storage with the grad_output. Hence we copy here.','line_number':3486,'multiline':False]['text':' Run backwards on CPU and GPU and compare results','line_number':3532,'multiline':False]['text':' Run double-backwards on CPU and GPU and compare results','line_number':3542,'multiline':False]['text':' We mix output into the second backwards computation so that','line_number':3568,'multiline':False]['text':' torch.autograd.grad doesn't complain that some inputs','line_number':3569,'multiline':False]['text':' are unreachable (which can happen if you differentiate','line_number':3570,'multiline':False]['text':' only on the gradient.','line_number':3571,'multiline':False]['text':' type: ignore[arg-type]','line_number':3596,'multiline':False]['text':' type: ignore[misc]','line_number':3609,'multiline':False]['text':' gradcheck doesn't support operators that take in dense inputs but','line_number':3634,'multiline':False]['text':' return sparse parameters. This only happens in the case of nn.Embedding','line_number':3635,'multiline':False]['text':' and nn.EmbeddingBag. Instead, we call `self.check_jacobian`, which','line_number':3636,'multiline':False]['text':' is a slightly different version of gradcheck that can handle this.','line_number':3637,'multiline':False]['text':' check if module can be printed','line_number':3661,'multiline':False]['text':' check if the inplace variant of the module gives the same result','line_number':3665,'multiline':False]['text':' as the out-of-place','line_number':3666,'multiline':False]['text':' check_inplace doesn't support multiple input tensors, since we don't have any modules','line_number':3668,'multiline':False]['text':' that modify the inputs in-place and that accept more than one input','line_number':3669,'multiline':False]['text':' check that cuda() moves module parameters to correct GPU device,','line_number':3704,'multiline':False]['text':' and that float() casts parameters correctly','line_number':3705,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3709,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3716,'multiline':False]['text':' check that float()/double() casters work correctly','line_number':3718,'multiline':False]['text':' TODO: torch.complex32 when properly supported','line_number':3728,'multiline':False]['text':' to float','line_number':3737,'multiline':False]['text':' and back to double','line_number':3743,'multiline':False]['text':' check that cuda() moves module parameters to correct GPU device,','line_number':3750,'multiline':False]['text':' and that float() casts parameters correctly','line_number':3751,'multiline':False]['text':' to GPU0','line_number':3753,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3757,'multiline':False]['text':' to CPU','line_number':3759,'multiline':False]['text':' back to GPU0','line_number':3765,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3769,'multiline':False]['text':' test that forwards of module runs correctly without cuDNN','line_number':3771,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3775,'multiline':False]['text':' test cross-GPU transfer works','line_number':3778,'multiline':False]['text':' to GPU1','line_number':3779,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3784,'multiline':False]['text':' test double()','line_number':3787,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3791,'multiline':False]['text':' test half()','line_number':3793,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3798,'multiline':False]['text':' type: ignore[misc]','line_number':3809,'multiline':False]['text':' TODO: check that criterions don't ignore grad_output','line_number':3810,'multiline':False]['text':' Check that these methods don't raise errors','line_number':3835,'multiline':False]['text':' type: ignore[misc]','line_number':3859,'multiline':False]['text':' Convert input, target and module parameters to dtype','line_number':3885,'multiline':False]['text':' GPU setup','line_number':3892,'multiline':False]['text':' torch.HalfTensor doesn't support most operations, converting back to default','line_number':3897,'multiline':False]['text':' Loss modules with weights require consistent input/module weight types','line_number':3901,'multiline':False]['text':' dtype used to be able to be None, so set precision in this way instead of a precision map','line_number':3906,'multiline':False]['text':' dtype used to be able to be None, so set precision in this way instead of a precision map','line_number':3914,'multiline':False]['text':' fp32 compute','line_number':3931,'multiline':False]['text':' bfloat16 compute','line_number':3939,'multiline':False]