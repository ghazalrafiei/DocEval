['text':' List of all namespaces containing modules to test.','line_number':29,'multiline':False]['text':' Modules that shouldn't be tested for one reason or another.','line_number':38,'multiline':False]['text':' abstract base class','line_number':40,'multiline':False]['text':' deprecated','line_number':41,'multiline':False]['text':' deprecated','line_number':42,'multiline':False]['text':' aliases to nn.MaxPool2d','line_number':43,'multiline':False]['text':' aliases to nn.MaxPool2d','line_number':44,'multiline':False]['text':' List of all module classes to test.','line_number':47,'multiline':False]['text':' type: ignore[attr-defined]','line_number':49,'multiline':False]['text':' Dict of module class -> common name. Useful for making test names more intuitive.','line_number':53,'multiline':False]['text':' Example: torch.nn.modules.linear.Linear -> "nn.Linear"','line_number':54,'multiline':False]['text':' type: ignore[attr-defined]','line_number':57,'multiline':False]['text':' Deal with any aliases by preferring earlier names.','line_number':61,'multiline':False]['text':' Specifies the modes (i.e. train, eval) to test over.','line_number':66,'multiline':False]['text':' If train and eval modes don't differ for the module, don't bother using more than one.','line_number':88,'multiline':False]['text':' Construct the test name; device / dtype parts are handled outside.','line_number':107,'multiline':False]['text':' See [Note: device and dtype suffix placement]','line_number':108,'multiline':False]['text':' Construct parameter kwargs to pass to the test.','line_number':113,'multiline':False]['text':' Provides an error message for debugging before rethrowing the exception','line_number':129,'multiline':False]['text':' Example: "nn.Linear"','line_number':136,'multiline':False]['text':' Inputs to pass during construction','line_number':156,'multiline':False]['text':' Inputs to pass to forward()','line_number':157,'multiline':False]['text':' Description for this set of inputs','line_number':158,'multiline':False]['text':' Reference with signature: reference_fn(module, parameters, *args, **kwargs)','line_number':159,'multiline':False]['text':' Copy inputs to avoid undesired side effects from calling the reference.','line_number':165,'multiline':False]['text':' Note that module parameters are passed in for convenience.','line_number':168,'multiline':False]['text':' Class object for the module under test','line_number':202,'multiline':False]['text':' Function to generate module inputs','line_number':204,'multiline':False]['text':' Indicates which tests to skip','line_number':205,'multiline':False]['text':' Additional decorators to apply to generated tests','line_number':206,'multiline':False]['text':' dtypes this function is expected to work with','line_number':207,'multiline':False]['text':' whether the op supports second order gradients','line_number':208,'multiline':False]['text':' tolerance for nondeterminism while performing gradcheck','line_number':209,'multiline':False]['text':' whether converting module to channels last will generate','line_number':210,'multiline':False]['text':' channels last output','line_number':211,'multiline':False]['text':' whether the module has differing behavior between train and eval','line_number':212,'multiline':False]['text':' Function to generate module inputs that error','line_number':213,'multiline':False]['text':' Start of module inputs functions.','line_number':243,'multiline':False]['text':' TODO: Uncomment when negative weights is supported.','line_number':347,'multiline':False]['text':' negative_weight = make_weight(10)','line_number':348,'multiline':False]['text':' negative_weight[0] = -1','line_number':349,'multiline':False]['text':' cases.append(('weights_negative', {'weight': negative_weight}))','line_number':350,'multiline':False]['text':' TODO: add pos_weight to the definition here and corresponding SampleInputs','line_number':1490,'multiline':False]['text':' Reuse the TransformerEncoderLayer samples since the forward args are nearly the same.','line_number':2336,'multiline':False]['text':' Construct a TransformerEncoderLayer object to pass to TransformerEncoder.','line_number':2340,'multiline':False]['text':' Note: TransformerEncoderLayer takes a "src_mask" while','line_number':2347,'multiline':False]['text':' TransformerEncoder takes a "mask"; rename kwarg appropriately.','line_number':2348,'multiline':False]['text':' Samples below are for validating the no-batch-dim support.','line_number':2379,'multiline':False]['text':' Samples below are for validating the no-batch-dim support.','line_number':2448,'multiline':False]['text':' Using same mask for tgt and memory','line_number':2452,'multiline':False]['text':' Samples below are for validating the no-batch-dim support.','line_number':2488,'multiline':False]['text':' Using same mask for tgt and memory','line_number':2493,'multiline':False]['text':' Currently all samples below are for validating the no-batch-dim support.','line_number':2545,'multiline':False]['text':' Currently all samples below are for validating the no-batch-dim support.','line_number':2576,'multiline':False]['text':' RNN also supports `nonlinearity` argument.','line_number':2593,'multiline':False]['text':' `tanh` is the default, so we check with `relu`','line_number':2594,'multiline':False]['text':' Currently all samples below are for validating the no-batch-dim support.','line_number':2607,'multiline':False]['text':' user won't have access to inp so won't be able to get its grads','line_number':2626,'multiline':False]['text':' Currently all samples below are for validating the no-batch-dim support.','line_number':2633,'multiline':False]['text':' Currently all samples below are for validating the no-batch-dim support.','line_number':2695,'multiline':False]['text':' All these operators share similar issues on cuDNN and MIOpen','line_number':3063,'multiline':False]['text':' RuntimeError: Batching rule not implemented for aten::_cudnn_rnn_backward.','line_number':3065,'multiline':False]['text':' We could not generate a fallback','line_number':3066,'multiline':False]['text':' NotImplementedError: the derivative for '_cudnn_rnn_backward' is not implemented.','line_number':3071,'multiline':False]['text':' Double backwards is not supported for CuDNN RNNs due to limitations in the CuDNN API','line_number':3072,'multiline':False]['text':' CUDNN GRU doesn't accept non-contiguous hx','line_number':3077,'multiline':False]['text':' MIOPEN GRU doesn't accept non-contiguous hx (this is dispatched to miopen only for float).','line_number':3082,'multiline':False]['text':' Start of module error inputs functions.','line_number':3097,'multiline':False]['text':' Database of ModuleInfo entries in alphabetical order.','line_number':3274,'multiline':False]['text':' Fails on MPS backend if input/output sizes are not divisible','line_number':3279,'multiline':False]['text':' Fails on MPS backend if input/output sizes are not divisible','line_number':3286,'multiline':False]['text':' Fails on backward check if output size is 1x1','line_number':3288,'multiline':False]['text':' not supported on MPS backend','line_number':3301,'multiline':False]['text':' not supported on MPS backend','line_number':3320,'multiline':False]['text':' The difference between channels last backward and','line_number':3332,'multiline':False]['text':' channels first backward of AvgPool2d on CUDA is too large','line_number':3333,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107201','line_number':3334,'multiline':False]['text':' No channels_last support for AvgPool1d as it does not take 4D inputs','line_number':3347,'multiline':False]['text':' not supported on MPS backend','line_number':3349,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':3356,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':3357,'multiline':False]['text':' tracking here rather than in the list in test_aotdispatch.py as eval mode passes','line_number':3359,'multiline':False]['text':' RuntimeError: tried to get Double out of SymInt','line_number':3360,'multiline':False]['text':' torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default','line_number':3366,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':3377,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':3378,'multiline':False]['text':' tracking here rather than in the list in test_aotdispatch.py as eval mode passes','line_number':3380,'multiline':False]['text':' RuntimeError: tried to get Double out of SymInt','line_number':3381,'multiline':False]['text':' torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default','line_number':3387,'multiline':False]['text':' not supported on MPS backend','line_number':3398,'multiline':False]['text':' tracking here rather than in the list in test_aotdispatch.py as eval mode passes','line_number':3400,'multiline':False]['text':' RuntimeError: tried to get Double out of SymInt','line_number':3401,'multiline':False]['text':' torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default','line_number':3407,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3424,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3426,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3438,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3440,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3443,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3444,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':3447,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 8005','line_number':3459,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3461,'multiline':False]['text':' Conv3d is not supported on MPS backend','line_number':3463,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3465,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3466,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3478,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3480,'multiline':False]['text':' Not implmented for chalf on CPU','line_number':3484,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/73502','line_number':3487,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3500,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3502,'multiline':False]['text':' Fails on backward check because ViewAsRealBackward apply contiguous for grad','line_number':3506,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3509,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3510,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':3513,'multiline':False]['text':' Not implemented for chalf on CPU','line_number':3516,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/73502','line_number':3519,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 8005','line_number':3532,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3534,'multiline':False]['text':' ConvTranspose3d is not supported on MPS backend','line_number':3536,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3538,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3539,'multiline':False]['text':' These fail only on ROCm','line_number':3541,'multiline':False]['text':' Not implmented for chalf on CPU','line_number':3544,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/73502','line_number':3547,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3558,'multiline':False]['text':' not supported on MPS backend','line_number':3571,'multiline':False]['text':' not supported on MPS backend','line_number':3579,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3586,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3593,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3602,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3604,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3606,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3607,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3620,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3622,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3624,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3625,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3627,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3628,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':3631,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 8005','line_number':3643,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3645,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3647,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3648,'multiline':False]['text':' LazyConv3d is not supported on MPS backend','line_number':3650,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3652,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3653,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3664,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3666,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3668,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3669,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 7603','line_number':3681,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3683,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3685,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3686,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3689,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3690,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':3693,'multiline':False]['text':' channels_last support on cuda requires cudnn >= 8005','line_number':3705,'multiline':False]['text':' Failure on ROCM for float32 issue #70125','line_number':3707,'multiline':False]['text':' Lazy modules don't currently play well with ModuleInfo tests on the meta device.','line_number':3709,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/70505 for more info.','line_number':3710,'multiline':False]['text':' LazyConvTranspose3d is not supported on MPS backend','line_number':3712,'multiline':False]['text':' This was wrongly being skipped before and needs investigation.','line_number':3714,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80247','line_number':3715,'multiline':False]['text':' No channels_last support for Linear currently.','line_number':3725,'multiline':False]['text':' No channels_last support for Bilinear currently.','line_number':3739,'multiline':False]['text':' Fails on backward check on MPS','line_number':3755,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':3756,'multiline':False]['text':' not supported on MPS backend','line_number':3787,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3793,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/115588','line_number':3796,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3804,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3811,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3818,'multiline':False]['text':' 'aten::multilabel_margin_loss_forward' is not currently implemented for the MPS device.','line_number':3820,'multiline':False]['text':' derivative for aten::multilabel_margin_loss_backward is not implemented','line_number':3822,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3828,'multiline':False]['text':' 'aten::multi_margin_loss' is not currently implemented for the MPS device.','line_number':3830,'multiline':False]['text':' RuntimeError: derivative for aten::multi_margin_loss_backward is not implemented','line_number':3832,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3838,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3845,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3852,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3859,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3865,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3871,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3878,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3885,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3892,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3899,'multiline':False]['text':' No channels_last support for loss functions.','line_number':3906,'multiline':False]['text':' The operator aten::_ctc_loss is not currently implemented for the MPS device.','line_number':3908,'multiline':False]['text':' derivative for aten::_ctc_loss_backward is not implemented','line_number':3910,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/115585','line_number':3913,'multiline':False]['text':' Tracking at https://github.com/pytorch/pytorch/issues/98089','line_number':3931,'multiline':False]['text':' No channels_last support for GroupNorm currently.','line_number':3935,'multiline':False]['text':' not supported on MPS backend','line_number':3944,'multiline':False]['text':' Fails on backward check on MPS','line_number':3951,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':3952,'multiline':False]['text':' No channels_last support for InstanceNorm1d currently.','line_number':3971,'multiline':False]['text':' No channels_last support for InstanceNorm2d currently.','line_number':3979,'multiline':False]['text':' not supported on MPS backend','line_number':3986,'multiline':False]['text':' No channels_last support for InstanceNorm3d currently.','line_number':3988,'multiline':False]['text':' uses avg_pool3d which is not supported on MPS backend','line_number':3994,'multiline':False]['text':' No channels_last support for LayerNorm currently.','line_number':4001,'multiline':False]['text':' TransformerEncoder takes the same inputs as TransformerEncoderLayer','line_number':4004,'multiline':False]['text':' Not implemented for SDPA backward derivative','line_number':4009,'multiline':False]['text':' No channels_last support for TransformerEncoderLayer currently.','line_number':4014,'multiline':False]['text':' Doesn't support device / dtype kwargs directly because it is just a','line_number':4016,'multiline':False]['text':' container of TransformerEncoderLayers.','line_number':4017,'multiline':False]['text':' Not implemented for SDPA backward derivative','line_number':4028,'multiline':False]['text':' No channels_last support for TransformerEncoderLayer currently.','line_number':4033,'multiline':False]['text':' Not implemented for SDPA backward derivative','line_number':4040,'multiline':False]['text':' No channels_last support for TransformerDecoderLayer currently.','line_number':4045,'multiline':False]['text':' Not implemented for SDPA backward derivative','line_number':4052,'multiline':False]['text':' No channels_last support for Transformer currently.','line_number':4057,'multiline':False]['text':' No channels_last support for MultiheadAttention currently.','line_number':4065,'multiline':False]['text':' Fails on backward check on MPS','line_number':4079,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':4080,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':4097,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':4098,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':4104,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':4105,'multiline':False]['text':' Fails on backward check on MPS','line_number':4130,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':4131,'multiline':False]['text':' no channels last support for Softmax2d currently','line_number':4159,'multiline':False]['text':' no channels last support for LogSoftmax currently','line_number':4166,'multiline':False]['text':' no channels last support for Softmin currently','line_number':4173,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':4179,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':4180,'multiline':False]['text':' not supported on MPS backend','line_number':4186,'multiline':False]['text':' Fails on backward check on MPS','line_number':4198,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':4199,'multiline':False]['text':' Fails on backward check on MPS','line_number':4212,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107214','line_number':4213,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':4225,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':4226,'multiline':False]['text':' not supported on MPS backend','line_number':4232,'multiline':False]['text':' LSTM with projections is not currently supported with MPS','line_number':4255,'multiline':False]['text':' test fails on MPS backend and is being investigated.','line_number':4311,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100914','line_number':4312,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':4324,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':4331,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':4351,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':4363,'multiline':False]['text':' Fails with channels last test on MPS backend','line_number':4370,'multiline':False]