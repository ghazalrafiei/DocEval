['text':' TODO: We add this to prevent dymamo from tracing into map_wrapper,','line_number':30,'multiline':False]['text':' remove the wrapper call when it's ready.','line_number':31,'multiline':False]['text':' type: ignore[arg-type]','line_number':41,'multiline':False]['text':' type: ignore[arg-type]','line_number':42,'multiline':False]['text':' type: ignore[arg-type]','line_number':43,'multiline':False]['text':' Note: We create "clean" environments for make_fx by suspending all dispatch keys','line_number':55,'multiline':False]['text':' between Autograd and Python key. Currently, we only suspend functionalization but more can be','line_number':56,'multiline':False]['text':' added when required. Will encounter two problems if we don't suspend functionalization:','line_number':57,'multiline':False]['text':'','line_number':58,'multiline':False]['text':' 1. make_fx fails to capture operations on input: the inputs are wrapped as _to_functional_tensor_wrapper,','line_number':59,'multiline':False]['text':' but they will be unwrapped before entering ProxyTorchDispatchMode as part of the dispatching.','line_number':60,'multiline':False]['text':' However, it's the outside wrapper that tracer creates proxies for. This casuses tracer fail to','line_number':61,'multiline':False]['text':' fetch the proxy for the inputs and fail to capture any operations on them.','line_number':62,'multiline':False]['text':'','line_number':63,'multiline':False]['text':' 2. make_fx fails to capture output: the outputs after ProxyTorchDispatchMode are further','line_number':64,'multiline':False]['text':' wrapped as FunctionalTensorWrapper in Functionalize key after return. However, the tracer','line_number':65,'multiline':False]['text':' only associates the inner tensor with proxy in ProxyTorchDispatchMode. Therefore,','line_number':66,'multiline':False]['text':' when creating the output node, it fails to associate the wrapped tensor with its proxy.','line_number':67,'multiline':False]['text':' Instead, it will create _tensor_constant as output.','line_number':68,'multiline':False]['text':' clone of a functional tensor produces a functional tensor','line_number':83,'multiline':False]['text':' but we want to avoid it so we clone a non-functional version','line_number':84,'multiline':False]['text':' need to handle both types of functionalization here:','line_number':90,'multiline':False]['text':' these are the tensors that came from the user,','line_number':91,'multiline':False]['text':' which could be either FunctionalTensorWrapper or FunctionalTensor','line_number':92,'multiline':False]['text':' In order to keep map functional for backward graph,','line_number':146,'multiline':False]['text':' we clone outputs that are aliasing inputs','line_number':147,'multiline':False]['text':' type: ignore[arg-type]','line_number':197,'multiline':False]['text':' Backward graph can return None output when forward inputs doesn't require grad.','line_number':303,'multiline':False]['text':' When we eagerly execute backward graph, we need to call _stack_pytree on its output,','line_number':304,'multiline':False]['text':' therefore we need to deal with None output.','line_number':305,'multiline':False]['text':' type: ignore[arg-type]','line_number':306,'multiline':False]