['text':' See https://github.com/pytorch/pytorch/issues/75462','line_number':37,'multiline':False]['text':' Should not be used, this is kept only for BC of loading old serialized Tensor subclasses','line_number':47,'multiline':False]['text':' Tensor does define __setstate__ even though it doesn't define','line_number':61,'multiline':False]['text':' __getstate__. So only use __setstate__ if it is NOT the one defined','line_number':62,'multiline':False]['text':' on Tensor','line_number':63,'multiline':False]['text':' NB: If you subclass Tensor, and want to share the subclassed class','line_number':74,'multiline':False]['text':' across processes, you must also update torch/multiprocessing/reductions.py','line_number':75,'multiline':False]['text':' to define a ForkingPickler serialization mode for the class.','line_number':76,'multiline':False]['text':'','line_number':77,'multiline':False]['text':' NB: If you add a new method to Tensor, you must update','line_number':78,'multiline':False]['text':' torch/_C/__init__.pyi.in to add a type annotation for your method;','line_number':79,'multiline':False]['text':' otherwise, it will not show up in autocomplete.','line_number':80,'multiline':False]['text':' TODO: skipping storage copy is wrong for meta, as meta','line_number':96,'multiline':False]['text':' does accurate alias tracking; however, the code below','line_number':97,'multiline':False]['text':' doesn't work because of','line_number':98,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/47442','line_number':99,'multiline':False]['text':' Update the test in test_serialization if you remove 'meta' from here','line_number':100,'multiline':False]['text':' quantizer_params can be different type based on torch attribute','line_number':124,'multiline':False]['text':' TODO: Once we decide to break serialization FC, no longer','line_number':149,'multiline':False]['text':' need to wrap with TypedStorage','line_number':150,'multiline':False]['text':' Plain Tensors don't have slots','line_number':201,'multiline':False]['text':' type: ignore[attr-defined]','line_number':202,'multiline':False]['text':' Fast path for regular tensor without Python state.','line_number':215,'multiline':False]['text':' For internal use only, to avoid raising deprecation warning','line_number':240,'multiline':False]['text':' See Note [Don't serialize hooks]','line_number':249,'multiline':False]['text':' Note: Numpy array is chosen to be the rebuild component for XLA, MTIA, ORT Tensors.','line_number':252,'multiline':False]['text':' We considered a few options:','line_number':253,'multiline':False]['text':' 1. CPU tensor can't be used here.','line_number':254,'multiline':False]['text':'    Otherwise in torch.load CPU storage is reconstructed with randomly','line_number':255,'multiline':False]['text':'    initialized data, moved onto backend device, and then storage is updated','line_number':256,'multiline':False]['text':'    to the serialized content. This works perfectly for CPU/CUDA but not these backends;','line_number':257,'multiline':False]['text':'    their tensors are disconnected with storage so they don't get the update.','line_number':258,'multiline':False]['text':' 2. Python list is not a good fit due to performance reason.','line_number':259,'multiline':False]['text':'    `tolist()` converts every single element in the tensor into python objects','line_number':260,'multiline':False]['text':'    and serialize them one by one.','line_number':261,'multiline':False]['text':' Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn't','line_number':266,'multiline':False]['text':' support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,','line_number':267,'multiline':False]['text':' this would reconstruct the BFloat16 tensor from numpy.','line_number':268,'multiline':False]['text':' NB: This implementation BREAKS storage sharing.  Current','line_number':279,'multiline':False]['text':' hypothesis is that no one cares for meta tensors.','line_number':280,'multiline':False]['text':' quantizer_params can be different type based on torch attribute','line_number':289,'multiline':False]['text':' convert scales and zero points to tuple to avoid recursive calls','line_number':303,'multiline':False]['text':' when/if we get multi-axis quantized tensors in the future, the shape','line_number':304,'multiline':False]['text':' is recoverable from the main tensor shape','line_number':305,'multiline':False]['text':' TODO: Once we decide to break serialization FC, no longer','line_number':316,'multiline':False]['text':' need to wrap with TypedStorage','line_number':317,'multiline':False]['text':' NB: values() currently returns the storage as a buffer in an unsafe way.','line_number':371,'multiline':False]['text':' Ideally, we'd use a private API for this instead. TODO: Switch to this if','line_number':372,'multiline':False]['text':' we ever get around to adding it.','line_number':373,'multiline':False]['text':' TODO: Once we decide to break serialization FC, no longer','line_number':410,'multiline':False]['text':' need to wrap with TypedStorage','line_number':411,'multiline':False]['text':' type: ignore[assignment]','line_number':412,'multiline':False]['text':' type: ignore[assignment]','line_number':417,'multiline':False]['text':' previously was self._backward_hooks','line_number':425,'multiline':False]['text':' type: ignore[assignment]','line_number':428,'multiline':False]['text':' type: ignore[assignment]','line_number':432,'multiline':False]['text':' Warning: this method is NOT called when you torch.load() a tensor;','line_number':439,'multiline':False]['text':' that is managed by _rebuild_tensor_v2','line_number':440,'multiline':False]['text':' legacy serialization of Tensor','line_number':444,'multiline':False]['text':' legacy serialization of Variable','line_number':448,'multiline':False]['text':' The setting of _backward_hooks is expected to be a no-op.','line_number':451,'multiline':False]['text':' See Note [Don't serialize hooks]','line_number':452,'multiline':False]['text':' All strings are unicode in Python 3.','line_number':460,'multiline':False]['text':' If get_infos is True, then we don't need to check for errors and vice versa','line_number':764,'multiline':False]['text':' type: ignore[attr-defined]','line_number':894,'multiline':False]['text':' NB: we use 'imap' and not 'map' here, so that in Python 2 we get a','line_number':1013,'multiline':False]['text':' generator and don't eagerly perform all the indexes.  This could','line_number':1014,'multiline':False]['text':' save us work, and also helps keep trace ordering deterministic','line_number':1015,'multiline':False]['text':' (e.g., if you zip(*hiddens), the eager map will force all the','line_number':1016,'multiline':False]['text':' indexes of hiddens[0] before hiddens[1], while the generator','line_number':1017,'multiline':False]['text':' map will interleave them.)','line_number':1018,'multiline':False]['text':' NB: We have intentionally skipped __torch_function__ dispatch here.','line_number':1019,'multiline':False]['text':' See gh-54457','line_number':1020,'multiline':False]['text':' Do NOT handle __torch_function__ here as user's default','line_number':1035,'multiline':False]['text':' implementation that handle most functions will most likely do it wrong.','line_number':1036,'multiline':False]['text':' It can be easily overridden by defining this method on the user','line_number':1037,'multiline':False]['text':' subclass if needed.','line_number':1038,'multiline':False]['text':' deprecated','line_number':1045,'multiline':False]['text':' property only available dense, cuda tensors','line_number':1049,'multiline':False]['text':' Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`','line_number':1055,'multiline':False]['text':' prefer Tensor ops over numpy ones','line_number':1056,'multiline':False]['text':' Wrap Numpy array again in a suitable tensor when done, to support e.g.','line_number':1066,'multiline':False]['text':' `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`','line_number':1067,'multiline':False]['text':' Workaround, torch has no built-in bool tensor','line_number':1074,'multiline':False]['text':' type hint doesn't understand the __contains__ result array','line_number':1090,'multiline':False]['text':' type: ignore[union-attr]','line_number':1091,'multiline':False]['text':' TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185','line_number':1105,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1106,'multiline':False]['text':' raise AttributeError for unsupported tensors, so that','line_number':1108,'multiline':False]['text':' hasattr(cpu_tensor, "__cuda_array_interface__") is False.','line_number':1109,'multiline':False]['text':' RuntimeError, matching tensor.__array__() behavior.','line_number':1124,'multiline':False]['text':' CUDA devices are little-endian and tensors are stored in native byte','line_number':1131,'multiline':False]['text':' order. 1-byte entries are endian-agnostic.','line_number':1132,'multiline':False]['text':' __cuda_array_interface__ v2 requires the strides to be omitted','line_number':1150,'multiline':False]['text':' (either not set or set to None) for C-contiguous arrays.','line_number':1151,'multiline':False]['text':' read-only is false','line_number':1156,'multiline':False]['text':' Note [rename_ / rename API]','line_number':1292,'multiline':False]['text':' The Python API for these is different from the C++ API. In Python:','line_number':1293,'multiline':False]['text':' 1) tensor.rename(*names) takes a vararglist of names','line_number':1294,'multiline':False]['text':' 2) tensor.rename(**rename_map) takes a map of names to rename.','line_number':1295,'multiline':False]['text':' C++ is static, making it difficult to implement similar behavior.','line_number':1296,'multiline':False]['text':' See Note [rename_ / rename API]','line_number':1338,'multiline':False]['text':' See Note [rename_ / rename API]','line_number':1390,'multiline':False]['text':' DLPack capsules can't capture all of PyTorch's semantics,','line_number':1447,'multiline':False]['text':' so we prohibit exporting tensors that would lose their properties like','line_number':1448,'multiline':False]['text':' requires_grad and having the conjugate bit set.','line_number':1449,'multiline':False]['text':' Stream pointers in CUDA/ROCm are uniquely numbered and can','line_number':1462,'multiline':False]['text':' be retrieved from their integer value.','line_number':1463,'multiline':False]['text':' NB: This logic handles the special case values for default','line_number':1467,'multiline':False]['text':' streams and must be kept in sync with from_dlpack in','line_number':1468,'multiline':False]['text':' torch/utils/dlpack.py','line_number':1469,'multiline':False]['text':' Only synchronize on different streams','line_number':1476,'multiline':False]['text':' Also handles things like namedtuples','line_number':1515,'multiline':False]