['text':' Python 3.10+','line_number':18,'multiline':False]['text':' This is a stricter implementation than zipfile.is_zipfile().','line_number':110,'multiline':False]['text':' zipfile.is_zipfile() is True if the magic number appears anywhere in the','line_number':111,'multiline':False]['text':' binary. Since we expect the files here to be generated by torch.save or','line_number':112,'multiline':False]['text':' torch.jit.save, it's safe to only check the start bytes and avoid','line_number':113,'multiline':False]['text':' collisions and assume the zip has only 1 file.','line_number':114,'multiline':False]['text':' See bugs.python.org/issue28494.','line_number':115,'multiline':False]['text':' Read the first few bytes and match against the ZIP file signature','line_number':118,'multiline':False]['text':' Cast module version fields to match the types of the required version','line_number':189,'multiline':False]['text':' PyTorchFileWriter only supports ascii filename.','line_number':467,'multiline':False]['text':' For filenames with non-ascii characters, we rely on Python','line_number':468,'multiline':False]['text':' for writing out the file.','line_number':469,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/54354','line_number':583,'multiline':False]['text':' The first line of this docstring overrides the one Sphinx generates for the','line_number':584,'multiline':False]['text':' documentation. We need it so that Sphinx doesn't leak `pickle`s path from','line_number':585,'multiline':False]['text':' the build environment (e.g. `<module 'pickle' from '/leaked/path').','line_number':586,'multiline':False]['text':' Since loading storages that view the same data with different dtypes is','line_number':641,'multiline':False]['text':' not supported, we need to keep track of the dtype associated with each','line_number':642,'multiline':False]['text':' storage data_ptr and throw an error if the dtype is ever different.','line_number':643,'multiline':False]['text':' TODO: This feature could be added in the future','line_number':644,'multiline':False]['text':' FIXME: the docs say that persistent_id should only return a string','line_number':648,'multiline':False]['text':' but torch store returns tuples. This works only in the binary protocol','line_number':649,'multiline':False]['text':' see','line_number':650,'multiline':False]['text':' https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects','line_number':651,'multiline':False]['text':' https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537','line_number':652,'multiline':False]['text':' saving the source is optional, so we can ignore any errors','line_number':661,'multiline':False]['text':' TODO: Once we decide to break serialization FC, this case','line_number':671,'multiline':False]['text':' can be deleted','line_number':672,'multiline':False]['text':' If storage is allocated, ensure that any other saved storages','line_number':689,'multiline':False]['text':' pointing to the same data all have the same dtype. If storage is','line_number':690,'multiline':False]['text':' not allocated, don't perform this check','line_number':691,'multiline':False]['text':' Offset is always 0, but we keep it for backwards compatibility','line_number':703,'multiline':False]['text':' with the old serialization format (which supported storage views)','line_number':704,'multiline':False]['text':' TODO: There's an issue here with FC. It might be impossible to','line_number':709,'multiline':False]['text':' solve, but it's worth noting. Imagine we save a list `[storage,','line_number':710,'multiline':False]['text':' tensor]`, where `tensor.storage()` is the same as `storage`, and','line_number':711,'multiline':False]['text':' `tensor.element_size() > 1`. Let's say that `tensor.dtype ==','line_number':712,'multiline':False]['text':' torch.float`.  The storage will be serialized with element size','line_number':713,'multiline':False]['text':' of 1, since we're choosing to serialize the first occurance of','line_number':714,'multiline':False]['text':' a duplicate storage. Since this legacy serialization format saves','line_number':715,'multiline':False]['text':' the numel of the storage, rather than nbytes directly, we'll be','line_number':716,'multiline':False]['text':' effectively saving nbytes in this case.  We'll be able to load it','line_number':717,'multiline':False]['text':' and the tensor back up with no problems in _this_ and future','line_number':718,'multiline':False]['text':' versions of pytorch, but in older versions, here's the problem:','line_number':719,'multiline':False]['text':' the storage will be loaded up as a UntypedStorage, and then the','line_number':720,'multiline':False]['text':' FloatTensor will loaded and the UntypedStorage will be assigned to','line_number':721,'multiline':False]['text':' it. Since the storage dtype does not match the tensor dtype, this','line_number':722,'multiline':False]['text':' will cause an error.  If we reverse the list, like `[tensor,','line_number':723,'multiline':False]['text':' storage]`, then we will save the `tensor.storage()` as a faked','line_number':724,'multiline':False]['text':' `FloatStorage`, and the saved size will be the correct','line_number':725,'multiline':False]['text':' dtype-specific numel count that old versions expect. `tensor`','line_number':726,'multiline':False]['text':' will be able to load up properly in old versions, pointing to','line_number':727,'multiline':False]['text':' a FloatStorage. However, `storage` is still being translated to','line_number':728,'multiline':False]['text':' a UntypedStorage, and it will try to resolve to the same','line_number':729,'multiline':False]['text':' FloatStorage that `tensor` contains. This will also cause an','line_number':730,'multiline':False]['text':' error. It doesn't seem like there's any way around this.','line_number':731,'multiline':False]['text':' Probably, we just cannot maintain FC for the legacy format if the','line_number':732,'multiline':False]['text':' saved list contains both a tensor and a storage that point to the','line_number':733,'multiline':False]['text':' same data.  We should still be able to maintain FC for lists of','line_number':734,'multiline':False]['text':' just tensors, as long as all views share the same dtype as the','line_number':735,'multiline':False]['text':' tensor they are viewing.','line_number':736,'multiline':False]['text':' Since loading storages that view the same data with different dtypes is','line_number':784,'multiline':False]['text':' not supported, we need to keep track of the dtype associated with each','line_number':785,'multiline':False]['text':' storage data_ptr and throw an error if the dtype is ever different.','line_number':786,'multiline':False]['text':' TODO: This feature could be added in the future','line_number':787,'multiline':False]['text':' FIXME: the docs say that persistent_id should only return a string','line_number':791,'multiline':False]['text':' but torch store returns tuples. This works only in the binary protocol','line_number':792,'multiline':False]['text':' see','line_number':793,'multiline':False]['text':' https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects','line_number':794,'multiline':False]['text':' https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537','line_number':795,'multiline':False]['text':' TODO: Once we decide to break serialization FC, this case','line_number':799,'multiline':False]['text':' can be deleted','line_number':800,'multiline':False]['text':' If storage is allocated, ensure that any other saved storages','line_number':813,'multiline':False]['text':' pointing to the same data all have the same dtype. If storage is','line_number':814,'multiline':False]['text':' not allocated, don't perform this check','line_number':815,'multiline':False]['text':' Write the pickle data for `obj`','line_number':837,'multiline':False]['text':' Write byte order marker','line_number':845,'multiline':False]['text':' Write each tensor to a file named tensor/the_tensor_key in the zip archive','line_number':852,'multiline':False]['text':' given that we copy things around anyway, we might use storage.cpu()','line_number':856,'multiline':False]['text':' this means to that to get tensors serialized, you need to implement','line_number':857,'multiline':False]['text':' .cpu() on the underlying Storage','line_number':858,'multiline':False]['text':' Now that it is on the CPU we can directly copy it into the zip file','line_number':861,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/54354','line_number':875,'multiline':False]['text':' The first line of this docstring overrides the one Sphinx generates for the','line_number':876,'multiline':False]['text':' documentation. We need it so that Sphinx doesn't leak `pickle`s path from','line_number':877,'multiline':False]['text':' the build environment (e.g. `<module 'pickle' from '/leaked/path').','line_number':878,'multiline':False]['text':' Add ability to force safe only weight loads via environment variable','line_number':978,'multiline':False]['text':' make flipping default BC-compatible','line_number':989,'multiline':False]['text':' The zipfile reader is going to advance the current file position.','line_number':1000,'multiline':False]['text':' If we want to actually tail call to torch.jit.load, we need to','line_number':1001,'multiline':False]['text':' reset back to the original position.','line_number':1002,'multiline':False]['text':' Register pickling support for layout instances such as','line_number':1043,'multiline':False]['text':' torch.sparse_coo, etc','line_number':1044,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1048,'multiline':False]['text':' There are yet not good way to type annotate function attributes https://github.com/python/mypy/issues/2087','line_number':1055,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1056,'multiline':False]['text':' type: ignore[name-defined]','line_number':1065,'multiline':False]['text':' saving the source is optional, so we can ignore any errors','line_number':1078,'multiline':False]['text':' Ignore containers that don't have any sources saved','line_number':1120,'multiline':False]['text':' TODO: Once we decide to break serialization FC, we can','line_number':1138,'multiline':False]['text':' stop wrapping with TypedStorage','line_number':1139,'multiline':False]['text':' TODO: Once we decide to break serialization FC, we can','line_number':1150,'multiline':False]['text':' stop wrapping with TypedStorage','line_number':1151,'multiline':False]['text':' skip next 4 bytes; legacy encoding treated ndim as 8 bytes','line_number':1165,'multiline':False]['text':' Ignore containers that don't have any sources saved','line_number':1188,'multiline':False]['text':' TODO: Once we decide to break serialization FC, we can','line_number':1202,'multiline':False]['text':' stop wrapping with TypedStorage','line_number':1203,'multiline':False]['text':' TODO: Once we decide to break serialization FC, we can','line_number':1222,'multiline':False]['text':' stop wrapping with TypedStorage','line_number':1223,'multiline':False]['text':' legacy_load requires that f has fileno()','line_number':1240,'multiline':False]['text':' only if offset is zero we can attempt the legacy tar file loader','line_number':1241,'multiline':False]['text':' .zip is used for torch.jit.save and will throw an un-pickling error here','line_number':1246,'multiline':False]['text':' if not a tarfile, reset file offset and proceed','line_number':1249,'multiline':False]['text':' When using encoding='bytes' in Py3, some **internal** keys stored as','line_number':1288,'multiline':False]['text':' strings in Py2 are loaded as bytes. This function decodes them with','line_number':1289,'multiline':False]['text':' ascii encoding, one that Py3 uses by default.','line_number':1290,'multiline':False]['text':'','line_number':1291,'multiline':False]['text':' NOTE: This should only be used on internal keys (e.g., `typename` and','line_number':1292,'multiline':False]['text':'       `location` in `persistent_load` below!','line_number':1293,'multiline':False]['text':' check if byteswapping is needed','line_number':1338,'multiline':False]['text':' Default behaviour was changed','line_number':1358,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/101688','line_number':1359,'multiline':False]['text':' swap here if byteswapping is needed','line_number':1374,'multiline':False]['text':' TODO: Once we decide to break serialization FC, we can','line_number':1379,'multiline':False]['text':' stop wrapping with TypedStorage','line_number':1380,'multiline':False]['text':' See https://github.com/pytorch/pytorch/pull/51633','line_number':1413,'multiline':False]['text':' Need to subclass Unpickler instead of directly monkey-patching the find_class method','line_number':1417,'multiline':False]['text':' because it's marked readonly in pickle.','line_number':1418,'multiline':False]['text':' The type: ignore is because mypy can't statically determine the type of this class.','line_number':1419,'multiline':False]['text':' type: ignore[name-defined]','line_number':1420,'multiline':False]['text':' from https://stackoverflow.com/questions/13398462/unpickling-python-objects-with-a-changed-module-path/13405732','line_number':1421,'multiline':False]['text':' Lets us override the imports that pickle uses when unpickling an object.','line_number':1422,'multiline':False]['text':' This is useful for maintaining BC if we change a module path that tensor instantiation relies on.','line_number':1423,'multiline':False]['text':' Load the data (which may in turn use `persistent_load` to load tensors)','line_number':1433,'multiline':False]