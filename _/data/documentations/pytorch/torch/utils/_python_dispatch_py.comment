['text':' TODO: Limitations and things about enable_torch_dispatch_mode we should fix before exposing it:','line_number':12,'multiline':False]['text':' - We need a better user-facing api for _DisableTorchDispatch that','line_number':13,'multiline':False]['text':'   is able to selectively disable __torch_dispatch__ of a particular class.','line_number':14,'multiline':False]['text':' - It doesn't work with the tensor constructors (torch.tensor, torch.Tensor)','line_number':15,'multiline':False]['text':' - Better name (see https://github.com/pytorch/pytorch/pull/63496#discussion_r694091694)','line_number':16,'multiline':False]['text':' Today, mode keys are not used at all in the per-dispatch-key-mode logic (for pre-dispatch)','line_number':64,'multiline':False]['text':' We should probably revisit this.','line_number':65,'multiline':False]['text':' Return a user mode on the stack if there are any','line_number':77,'multiline':False]['text':' See Note [Not Caching Per-Dispatch-Key Mode Handlers]','line_number':90,'multiline':False]['text':' Clear the cache of every op that has been used so far, for this particular key.','line_number':91,'multiline':False]['text':' per-dispatch-key-mode-stack do not currently handle "always running infra modes last".','line_number':105,'multiline':False]['text':' In practice this doesn't matter, since ProxyTorchDispatchMode is the only mode','line_number':106,'multiline':False]['text':' that we push onto these per-dispatch-key-mode-stacks.','line_number':107,'multiline':False]['text':' Manually disable proxy and fake modes, if any are active','line_number':125,'multiline':False]['text':' NB: Purposefully guard here to simplify the inner / outer symbols.','line_number':197,'multiline':False]['text':' Using sym_eq() for symbolic comparison can result in an expression that's too','line_number':198,'multiline':False]['text':' difficult to guard on, so we use == here.','line_number':199,'multiline':False]['text':' This is hopefully a reasonable assert:','line_number':227,'multiline':False]['text':' subclasses that rely on this API for output aliasing','line_number':228,'multiline':False]['text':' should always return wrapper tensor subclasses for us to manually alias.','line_number':229,'multiline':False]['text':' in theory if a subclass that needs this API wants to sometimes return','line_number':230,'multiline':False]['text':' plain tensors, we could remove the assert and just not perform the aliasing,','line_number':231,'multiline':False]['text':' but it seems safer to learn more about this case first.','line_number':232,'multiline':False]['text':' Need to run under no_dispatch, because we explicitly do **not**','line_number':238,'multiline':False]['text':' want our subclass to intercept the set_() call.','line_number':239,'multiline':False]['text':' instead, our subclass should directly have its storage swapped out.','line_number':240,'multiline':False]['text':' See Note: [Fake Tensor Dispatch Keys]','line_number':242,'multiline':False]['text':' we're borrowing the way it modifies dispatch key TLS.','line_number':243,'multiline':False]['text':' directly calling this overload, and passing ret.shape, because we **explicitly**','line_number':247,'multiline':False]['text':' don't want to reset the sizes on ret, if the storage implies a size change.','line_number':248,'multiline':False]['text':' Why?','line_number':249,'multiline':False]['text':' The purpose of this API is *not* to change the size/strides of our output- we assume it's already correct.','line_number':250,'multiline':False]['text':' We just want to "fix up" the storage aliasing, without modifying or output's metadata.','line_number':251,'multiline':False]['text':' Example: out = inp.expand(inp.shape[0], inp.shape[0])','line_number':252,'multiline':False]['text':'     This requires swapping the storage of out to be the same as inp,','line_number':253,'multiline':False]['text':'     but we do *not* want it to change the sizes/strides that were compute for out.','line_number':254,'multiline':False]['text':' This abstracts over the fact that in return_and_correct_aliasing,','line_number':275,'multiline':False]['text':' we sometimes use torchgen schema parsing (for aten ops, since torchscript's schema parsing is sometimes buggy),','line_number':276,'multiline':False]['text':' and sometimes use torchscript schema parsing (for custom ops, for which torchgen parsing is untested).','line_number':277,'multiline':False]['text':' Can't import torch._ops.OpOverload due to circular reference','line_number':289,'multiline':False]['text':' Given an OpOverload, returns schema information on it.','line_number':292,'multiline':False]['text':' This is cached for efficiency, since it can involve running torchgen','line_number':293,'multiline':False]['text':' For ATen ops: use torchgen (since torchscript parser doesn't handle alias annotations','line_number':297,'multiline':False]['text':' properly for some ops that output tensorlists)','line_number':298,'multiline':False]['text':' remove the aten:: namespace, which is added by the torchscript parser,','line_number':302,'multiline':False]['text':' and torchgen doesn't know how to handle','line_number':303,'multiline':False]['text':' the torchscript parser ends up converting int[2]=1 into int[2]=[1, 1],','line_number':306,'multiline':False]['text':' which torchgen chokes on.','line_number':307,'multiline':False]['text':' for aten::rot90','line_number':310,'multiline':False]['text':' For non-aten ops, torchgen is untested so we rely on torchscript schema parsing','line_number':324,'multiline':False]['text':' Caching here because torchgen parsing is definitely not fast, and this function is called','line_number':355,'multiline':False]['text':' once for every op in the graph during functionalization.','line_number':356,'multiline':False]['text':' torchscript allows for complicated alias sets, but our dispatcher ops only really involve simple aliasing','line_number':363,'multiline':False]['text':' For any dispatcher op with an output alias, we expect it to map to exactly one alias in the schema's input arguments.','line_number':376,'multiline':False]['text':' Fix up the storages of any outs so that they point to the same storage as the input,','line_number':384,'multiline':False]['text':' if func is a view op.','line_number':385,'multiline':False]['text':' For inplace_view ops in particular, we'll try hard to make sure that the wrapper subclass's','line_number':388,'multiline':False]['text':' metadata is set correctly.','line_number':389,'multiline':False]['text':' no_dispatch() to make sure that we secretly change the metadata on the wrapper,','line_number':391,'multiline':False]['text':' but don't end up dispatching the op anywhere else.','line_number':392,'multiline':False]['text':' Assumption: we have a very small number of inplace_view ops that follow a strict schema:','line_number':394,'multiline':False]['text':' there is only a single argument that gets its metadata mutated.','line_number':395,'multiline':False]['text':' This check exists because we generally *do* want to update the metadata of any wrapper subclasses,','line_number':397,'multiline':False]['text':' but FunctionalTensor is special: it overrides all size/stride calls to plumb to the inner tensor.','line_number':398,'multiline':False]['text':' so we don't actually need to update the metadata (and attempting to do so causes errors)','line_number':399,'multiline':False]['text':' See Note: [Fake Tensor Dispatch Keys]','line_number':403,'multiline':False]['text':' we're borrowing the way it modifies dispatch key TLS.','line_number':404,'multiline':False]['text':' Next: we need to make sure to return inputs directly, if the output is a mutable alias (e.g. add_()).','line_number':412,'multiline':False]['text':' simple case: none of our outputs have mutable aliases, so we can return the output as-is','line_number':414,'multiline':False]['text':' simplifying assumption: we don't have **any** ops with return types like "-> (Tensor(a!), Tensor)"','line_number':418,'multiline':False]['text':' In the multi-return case, all aten ops return a tuple / list, so cast accordingly.','line_number':425,'multiline':False]