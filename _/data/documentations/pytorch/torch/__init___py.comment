['text':' multipy/deploy is setting this import before importing torch, this is the most','line_number':20,'multiline':False]['text':' reliable way we have to detect if we're running within deploy.','line_number':21,'multiline':False]['text':' https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137','line_number':22,'multiline':False]['text':' TODO(torch_deploy) figure out how to freeze version.py in fbcode build','line_number':31,'multiline':False]['text':'###############################################################################','line_number':63,'multiline':False]['text':' Load the extension module','line_number':64,'multiline':False]['text':'###############################################################################','line_number':65,'multiline':False]['text':' When users create a virtualenv that inherits the base environment,','line_number':72,'multiline':False]['text':' we will need to add the corresponding library directory into','line_number':73,'multiline':False]['text':' DLL search directories. Otherwise, it will rely on `PATH` which','line_number':74,'multiline':False]['text':' is dependent on user settings.','line_number':75,'multiline':False]['text':' Should only be called on Linux if default path resolution have failed','line_number':148,'multiline':False]['text':' See Note [Global dependencies]','line_number':166,'multiline':False]['text':' Can only happen for wheel with cuda libs as PYPI deps','line_number':178,'multiline':False]['text':' As PyTorch is not purelib, but nvidia-*-cu12 is','line_number':179,'multiline':False]['text':' Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a','line_number':203,'multiline':False]['text':' few circumstances:','line_number':204,'multiline':False]['text':'','line_number':205,'multiline':False]['text':'   1. You're in a build environment (e.g., fbcode) where','line_number':206,'multiline':False]['text':'      libtorch_global_deps is not available, but you still need','line_number':207,'multiline':False]['text':'      to get mkl to link in with RTLD_GLOBAL or it will just','line_number':208,'multiline':False]['text':'      not work.','line_number':209,'multiline':False]['text':'','line_number':210,'multiline':False]['text':'   2. You're trying to run PyTorch under UBSAN and you need','line_number':211,'multiline':False]['text':'      to ensure that only one copy of libtorch is loaded, so','line_number':212,'multiline':False]['text':'      vptr checks work properly','line_number':213,'multiline':False]['text':'','line_number':214,'multiline':False]['text':' If you're using this setting, you must verify that all the libraries','line_number':215,'multiline':False]['text':' you load consistently use the same libstdc++, or you may have','line_number':216,'multiline':False]['text':' mysterious segfaults.','line_number':217,'multiline':False]['text':'','line_number':218,'multiline':False]['text':' noqa: F403','line_number':221,'multiline':False]['text':' Easy way.  You want this most of the time, because it will prevent','line_number':226,'multiline':False]['text':' C++ symbols from libtorch clobbering C++ symbols from other','line_number':227,'multiline':False]['text':' libraries, leading to mysterious segfaults.','line_number':228,'multiline':False]['text':'','line_number':229,'multiline':False]['text':' If building in an environment where libtorch_global_deps isn't available','line_number':230,'multiline':False]['text':' like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will','line_number':231,'multiline':False]['text':' want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False','line_number':232,'multiline':False]['text':'','line_number':233,'multiline':False]['text':' See Note [Global dependencies]','line_number':234,'multiline':False]['text':' noqa: F403','line_number':237,'multiline':False]['text':' Appease the type checker; ordinarily this binding is inserted by the','line_number':239,'multiline':False]['text':' torch._C module initialization code in C','line_number':240,'multiline':False]['text':' This field MUST be named node; C++ binding code assumes that this','line_number':252,'multiline':False]['text':' class has a field named node that stores SymNode','line_number':253,'multiline':False]['text':' Magic methods installed by torch.fx.experimental.sym_node','line_number':265,'multiline':False]['text':' We could support constant SymInts as well, but not doing it for now','line_number':302,'multiline':False]['text':' This field MUST be named node; C++ binding code assumes that this','line_number':313,'multiline':False]['text':' class has a field named node that stores SymNode','line_number':314,'multiline':False]['text':' Magic methods installed by torch.fx.experimental.sym_node','line_number':320,'multiline':False]['text':' This field MUST be named node; C++ binding code assumes that this','line_number':364,'multiline':False]['text':' class has a field named node that stores SymNode','line_number':365,'multiline':False]['text':' Magic methods installed by torch.fx.experimental.sym_node','line_number':374,'multiline':False]['text':' We very carefully define __sym_not__, and not a number of other','line_number':381,'multiline':False]['text':' plausible alternatives:','line_number':382,'multiline':False]['text':'','line_number':383,'multiline':False]['text':'   - We do not override __not__ because this is not a real magic','line_number':384,'multiline':False]['text':'     method; you cannot override the meaning of the not builtin in','line_number':385,'multiline':False]['text':'     Python.  We use the name 'sym_not' to clarify that in user code you','line_number':386,'multiline':False]['text':'     cannot use the builtin not or operator.not_ or operator.__not__ and','line_number':387,'multiline':False]['text':'     hit this magic method; you must use our custom sym_not operator.','line_number':388,'multiline':False]['text':'','line_number':389,'multiline':False]['text':'   - We do not override the __invert__ method because SymBool is','line_number':390,'multiline':False]['text':'     meant to be usable in situations where bool is expected.  However,','line_number':391,'multiline':False]['text':'     bitwise negation ~a does the wrong thing with booleans (because','line_number':392,'multiline':False]['text':'     bool is a subclass of int, so ~1 = -2 which is not falseish.)','line_number':393,'multiline':False]['text':'     This would be a giant footgun, so we get around it by defining','line_number':394,'multiline':False]['text':'     our own operator.  Note that bitwise and/or do the right thing,','line_number':395,'multiline':False]['text':'     so we reuse the conventional operators there for readability.','line_number':396,'multiline':False]['text':'','line_number':397,'multiline':False]['text':' type: ignore[operator]','line_number':430,'multiline':False]['text':' type: ignore[operator]','line_number':447,'multiline':False]['text':' type: ignore[arg-type, call-overload]','line_number':463,'multiline':False]['text':' type: ignore[operator]','line_number':464,'multiline':False]['text':' NB: If you actually care about preserving output type exactly','line_number':475,'multiline':False]['text':' if you do something like max(0, 0.0), it is NOT sound to treat','line_number':476,'multiline':False]['text':' min/max as commutative','line_number':477,'multiline':False]['text':' type: ignore[operator]','line_number':479,'multiline':False]['text':' type: ignore[operator]','line_number':491,'multiline':False]['text':' Drop in replacement for math.sqrt','line_number':493,'multiline':False]['text':' Check to see if we can load C extensions, and if not provide some guidance','line_number':513,'multiline':False]['text':' on what the problem might be.','line_number':514,'multiline':False]['text':' _initExtension is chosen (arbitrarily) as a sentinel.','line_number':516,'multiline':False]['text':' The __file__ check only works for Python 3.7 and above.','line_number':521,'multiline':False]['text':' If __file__ is not None the cause is unknown, so just re-raise.','line_number':535,'multiline':False]['text':' type: ignore[arg-type]','line_number':541,'multiline':False]['text':' TODO: fix their module from C++ side','line_number':543,'multiline':False]['text':' issue 109438 / pr 109940. Prevent TensorBase from being copied into torch.','line_number':547,'multiline':False]['text':' issue 38137 and python issue 43367. Submodules of a C extension are','line_number':551,'multiline':False]['text':' non-standard, and attributes of those submodules cannot be pickled since','line_number':552,'multiline':False]['text':' pickle expect to be able to import them as "from _C.sub import attr"','line_number':553,'multiline':False]['text':' which fails with "_C is not a package','line_number':554,'multiline':False]['text':' submodule','line_number':558,'multiline':False]['text':'###############################################################################','line_number':563,'multiline':False]['text':' Define basic utilities','line_number':564,'multiline':False]['text':'###############################################################################','line_number':565,'multiline':False]['text':' NOTE: builtins.int is used here because int in this scope resolves','line_number':913,'multiline':False]['text':' to torch.int','line_number':914,'multiline':False]['text':'###############################################################################','line_number':1044,'multiline':False]['text':' Define error checking functions','line_number':1045,'multiline':False]['text':'###############################################################################','line_number':1046,'multiline':False]['text':' These error checking functions must be kept consistent with their C++','line_number':1048,'multiline':False]['text':' equivalents. Their C++ equivalents are mentioned where applicable.','line_number':1049,'multiline':False]['text':' noqa: F811','line_number':1051,'multiline':False]['text':' error_type must be a subclass of Exception and not subclass of Warning','line_number':1059,'multiline':False]['text':' noqa: F811','line_number':1076,'multiline':False]['text':' This is responsible for the expect_true','line_number':1103,'multiline':False]['text':' noqa: F811','line_number':1108,'multiline':False]['text':' noqa: F811','line_number':1125,'multiline':False]['text':' noqa: F811','line_number':1142,'multiline':False]['text':' noqa: F811','line_number':1159,'multiline':False]['text':' noqa: F811','line_number':1176,'multiline':False]['text':' C++ equivalent: `TORCH_CHECK_TENSOR_ALL`','line_number':1186,'multiline':False]['text':' noqa: F811','line_number':1187,'multiline':False]['text':'###############################################################################','line_number':1205,'multiline':False]['text':' Define numeric constants','line_number':1206,'multiline':False]['text':'###############################################################################','line_number':1207,'multiline':False]['text':' For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and','line_number':1209,'multiline':False]['text':' NumPy consistency (https://numpy.org/devdocs/reference/constants.html)','line_number':1210,'multiline':False]['text':'###############################################################################','line_number':1214,'multiline':False]['text':' Define Storage and Tensor classes','line_number':1215,'multiline':False]['text':'###############################################################################','line_number':1216,'multiline':False]['text':' NOTE: New <type>Storage classes should never be added. When adding a new','line_number':1221,'multiline':False]['text':' dtype, use torch.storage.TypedStorage directly.','line_number':1222,'multiline':False]['text':' The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()','line_number':1402,'multiline':False]['text':' If you edit these imports, please update torch/__init__.py.in as well','line_number':1405,'multiline':False]['text':'###############################################################################','line_number':1410,'multiline':False]['text':' Initialize extension','line_number':1411,'multiline':False]['text':'###############################################################################','line_number':1412,'multiline':False]['text':' Initializing the extension shadows the built-in python float / int classes;','line_number':1425,'multiline':False]['text':' store them for later use by SymInt / SymFloat.','line_number':1426,'multiline':False]['text':' Shared memory manager needs to know the exact location of manager executable','line_number':1430,'multiline':False]['text':' Appease the type checker: it can't deal with direct setting of globals().','line_number':1434,'multiline':False]['text':' Note that we will see "too many" functions when reexporting this way; there','line_number':1435,'multiline':False]['text':' is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions','line_number':1436,'multiline':False]['text':' so that this import is good enough','line_number':1437,'multiline':False]['text':' Some type signatures pulled in from _VariableFunctions here clash with','line_number':1439,'multiline':False]['text':' signatures already imported. For now these clashes are ignored; see','line_number':1440,'multiline':False]['text':' PR #43339 for details.','line_number':1441,'multiline':False]['text':' type: ignore[assignment, misc] # noqa: F403','line_number':1442,'multiline':False]['text':' Fixup segment_reduce visibility','line_number':1443,'multiline':False]['text':' Ops not to be exposed in `torch` namespace,','line_number':1447,'multiline':False]['text':' mostly helper ops.','line_number':1448,'multiline':False]['text':' Hide some APIs that should not be public','line_number':1458,'multiline':False]['text':' TODO: Once the undocumented FC window is passed, remove the line bellow','line_number':1460,'multiline':False]['text':'###############################################################################','line_number':1469,'multiline':False]['text':' Import TorchDynamo's lazy APIs to avoid circular dependenices','line_number':1470,'multiline':False]['text':'###############################################################################','line_number':1471,'multiline':False]['text':' needs to be before from .functional import * to avoid circular dependencies','line_number':1473,'multiline':False]['text':'###############################################################################','line_number':1476,'multiline':False]['text':' Import interface functions defined in Python','line_number':1477,'multiline':False]['text':'###############################################################################','line_number':1478,'multiline':False]['text':' needs to be after the above ATen bindings so we can overwrite from Python side','line_number':1480,'multiline':False]['text':' noqa: F403','line_number':1481,'multiline':False]['text':'###############################################################################','line_number':1484,'multiline':False]['text':' Remove unnecessary members','line_number':1485,'multiline':False]['text':'###############################################################################','line_number':1486,'multiline':False]['text':'###############################################################################','line_number':1491,'multiline':False]['text':' Define _assert','line_number':1492,'multiline':False]['text':'###############################################################################','line_number':1493,'multiline':False]['text':' needs to be before the submodule imports to avoid circular dependencies','line_number':1495,'multiline':False]['text':'###############################################################################','line_number':1505,'multiline':False]['text':' Import most common subpackages','line_number':1506,'multiline':False]['text':'###############################################################################','line_number':1507,'multiline':False]['text':' Use the redundant form so that type checkers know that these are a part of','line_number':1509,'multiline':False]['text':' the public API. The "regular" import lines are there solely for the runtime','line_number':1510,'multiline':False]['text':' side effect of adding to the imported module's members for other users.','line_number':1511,'multiline':False]['text':' Quantized, sparse, AO, etc. should be last to get imported, as nothing','line_number':1546,'multiline':False]['text':' is expected to depend on them.','line_number':1547,'multiline':False]['text':' nn.quant* depends on ao -- so should be after those.','line_number':1549,'multiline':False]['text':' attach docstrings to torch and tensor functions','line_number':1557,'multiline':False]['text':' Import the ops "namespace"','line_number':1567,'multiline':False]['text':' quantization depends on torch.fx','line_number':1572,'multiline':False]['text':' Import quantization','line_number':1573,'multiline':False]['text':' Import the quasi random sampler','line_number':1576,'multiline':False]['text':' If you are seeing this, it means that this call site was not checked if','line_number':1579,'multiline':False]['text':' the memory format could be preserved, and it was switched to old default','line_number':1580,'multiline':False]['text':' behaviour of contiguous','line_number':1581,'multiline':False]['text':' Register fork handler to initialize OpenMP in child processes (see gh-28389)','line_number':1584,'multiline':False]['text':' Import tools that require fully imported torch (for applying','line_number':1589,'multiline':False]['text':' torch.jit.script as a decorator, for instance):','line_number':1590,'multiline':False]['text':' These were previously defined in native_functions.yaml and appeared on the','line_number':1593,'multiline':False]['text':' `torch` namespace, but we moved them to c10 dispatch to facilitate custom','line_number':1594,'multiline':False]['text':' class usage. We add these lines here to preserve backward compatibility.','line_number':1595,'multiline':False]['text':' Import experimental masked operations support. See','line_number':1601,'multiline':False]['text':' [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more','line_number':1602,'multiline':False]['text':' information.','line_number':1603,'multiline':False]['text':' Import removed ops with error message about removal','line_number':1606,'multiline':False]['text':' type: ignore[misc]','line_number':1607,'multiline':False]['text':' type: ignore[misc]','line_number':1613,'multiline':False]['text':' FIXME: CUDA Graph does not work well with CUPTI teardown.','line_number':1626,'multiline':False]['text':'   1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11)','line_number':1627,'multiline':False]['text':'   2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12)','line_number':1628,'multiline':False]['text':' Workaround: turn off CUPTI teardown when using CUDA Graphs.','line_number':1629,'multiline':False]['text':' only pass the args if they non-empty','line_number':1698,'multiline':False]['text':' Temporary until we get proper support for python 3.12','line_number':1803,'multiline':False]['text':' Decorator mode','line_number':1807,'multiline':False]['text':' Make sure the device_type represent a supported device type for torch.','line_number':1844,'multiline':False]['text':' expose return_types','line_number':1854,'multiline':False]['text':' Enable CUDA Sanitizer','line_number':1860,'multiline':False]['text':' Populate magic methods on SymInt and SymFloat','line_number':1866,'multiline':False]['text':' The function _sparse_coo_tensor_unsafe is removed from PyTorch','line_number':1873,'multiline':False]['text':' Python API (v. 1.13), here we temporarily provide its replacement','line_number':1874,'multiline':False]['text':' with a deprecation warning.','line_number':1875,'multiline':False]['text':' TODO: remove the function for PyTorch v 1.15.','line_number':1876,'multiline':False]['text':' Register MPS specific decomps','line_number':1884,'multiline':False]['text':' Deprecated attributes','line_number':1904,'multiline':False]['text':' Import the following modules during type checking to enable code intelligence features,','line_number':1913,'multiline':False]['text':' such as auto-completion in tools like pylance, even when these modules are not explicitly','line_number':1914,'multiline':False]['text':' imported in user code.','line_number':1915,'multiline':False]['text':' ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit','line_number':1925,'multiline':False]['text':' Deprecated attrs','line_number':1930,'multiline':False]['text':' Lazy modules','line_number':1937,'multiline':False]