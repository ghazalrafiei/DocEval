['text':' noqa: C101','line_number':1,'multiline':False]['text':' add some debug printouts','line_number':12,'multiline':False]['text':' add inf and NaN checkers','line_number':15,'multiline':False]['text':' Whether to disable a progress bar for autotuning','line_number':18,'multiline':False]['text':' Whether to enable printing the source code for each future','line_number':21,'multiline':False]['text':' use fx aot graph codegen cache','line_number':24,'multiline':False]['text':' use cpp wrapper instead of python wrapper','line_number':27,'multiline':False]['text':' dead code elimination','line_number':30,'multiline':False]['text':' assume weight tensors are fixed size','line_number':33,'multiline':False]['text':' put correctness assertions in generated code','line_number':36,'multiline':False]['text':' enable loop reordering based on input orders','line_number':40,'multiline':False]['text':' reuse a kernel input as the output','line_number':43,'multiline':False]['text':' reuse a buffer for an unrelated purpose','line_number':46,'multiline':False]['text':' Enable pooled allocations for non-output tensors','line_number':49,'multiline':False]['text':' How to organize memory under memory_planning=True:','line_number':52,'multiline':False]['text':' - "none": do not try to pool storage, just reuse','line_number':53,'multiline':False]['text':' - "intermediates": all non-outputs share storage, outputs each get unique storage','line_number':54,'multiline':False]['text':' - "outputs": two pools, one for intermediates (freed on return) and one for outputs','line_number':55,'multiline':False]['text':' - "combined": a single pool for both intermediates and outputs','line_number':56,'multiline':False]['text':' codegen benchmark harness','line_number':59,'multiline':False]['text':' fuse pointwise into templates','line_number':62,'multiline':False]['text':' do epilogue fusions before other fusions','line_number':65,'multiline':False]['text':' enable pattern match+replace optimizations','line_number':68,'multiline':False]['text':' register custom graph optimization pass hook. so far, pre/post passes are','line_number':71,'multiline':False]['text':' only applied before/after pattern_matcher in post_grad_passes.','line_number':72,'multiline':False]['text':'','line_number':73,'multiline':False]['text':' def my_custom_pre_pass(graph: torch.fx.graph.Graph):','line_number':74,'multiline':False]['text':'     # my custom graph optimization pass','line_number':75,'multiline':False]['text':'     ...','line_number':76,'multiline':False]['text':'','line_number':77,'multiline':False]['text':' def my_custom_post_pass(graph: torch.fx.graph.Graph):','line_number':78,'multiline':False]['text':'     # my custom graph optimization pass','line_number':79,'multiline':False]['text':'     ...','line_number':80,'multiline':False]['text':'','line_number':81,'multiline':False]['text':' torch._inductor.config.post_grad_custom_pre_pass = my_custom_pre_pass','line_number':82,'multiline':False]['text':' torch._inductor.config.post_grad_custom_post_pass = my_custom_post_pass','line_number':83,'multiline':False]['text':' Registers a custom pregrad pass. Note that the pre-grad IR is 1.','line_number':87,'multiline':False]['text':' non-functional, 2. non-normalized, and 3. prone to change. Ideally we should','line_number':88,'multiline':False]['text':' use post-grad passes.','line_number':89,'multiline':False]['text':' Optimize away split cat patterns (Experimental)','line_number':92,'multiline':False]['text':' Optimize conv-batchnorm if batchnorm is in eval mode. Slightly reduces numerical stability.','line_number':95,'multiline':False]['text':' Enable predispatch aten IR for export','line_number':98,'multiline':False]['text':' Deprecated','line_number':101,'multiline':False]['text':' Deprecated','line_number':104,'multiline':False]['text':' Pre grad group/batch fusion and options in order, set to empty dict to disable fusion.','line_number':107,'multiline':False]['text':' Call `torch._inductor.fx_passes.group_batch_fusion.list_group_batch_fusions()` to see available fusions.','line_number':108,'multiline':False]['text':' Post grad group/batch fusion and options, set to empty dict to disable fusion.','line_number':118,'multiline':False]['text':' Call `torch._inductor.fx_passes.group_batch_fusion.list_group_batch_fusions(False)` to see available fusions.','line_number':119,'multiline':False]['text':' enable reordering pass for improving memory locality','line_number':122,'multiline':False]['text':' Scale down RBLOCK for better occupancy','line_number':125,'multiline':False]['text':' this forces fusion for int_mm with mul. Needed when you want to avoid realizing the int32','line_number':128,'multiline':False]['text':' but the mul gets fused with other pointwise ops instead.','line_number':129,'multiline':False]['text':' for pattern torch.mm(a, b.to(dtype)) with cuda tensors,','line_number':132,'multiline':False]['text':' enable torch._inductor.kernel.mm.tuned_mixed_mm fused kernel.','line_number':133,'multiline':False]['text':' Autotune will compare perf with normal cast->then->mm option','line_number':134,'multiline':False]['text':' for pattern torch.mm(a, b.to(dtype)) with cuda tensors, always use','line_number':137,'multiline':False]['text':' torch._inductor.kernel.mm.tuned_mixed_mm's fused kernel.','line_number':138,'multiline':False]['text':' Autotune will not compare with normal cast->then->mm option.','line_number':139,'multiline':False]['text':' (if force_mixed_mm is true, the use_mixed_mm flag will be ignored)','line_number':140,'multiline':False]['text':' enable reordering pass for increasing overlap between compute and communication','line_number':143,'multiline':False]['text':' passes (in execution order) for increasing overlap between compute and communication','line_number':146,'multiline':False]['text':' for built-in passes, use string name; for user-defined passes, pass in the function handle','line_number':147,'multiline':False]['text':' runtime estimation function for ops','line_number':154,'multiline':False]['text':' for built-in estimation function, pass in "default"; for user-defined estimation function, pass in the function handle','line_number':155,'multiline':False]['text':' unit: GB/s, uni-directional P2P bandwidth per card','line_number':158,'multiline':False]['text':' default value is NVLink','line_number':159,'multiline':False]['text':' unit: GB/s, uni-directional P2P bandwidth per node','line_number':162,'multiline':False]['text':' default value is InfiniBand','line_number':163,'multiline':False]['text':' enable slow autotuning passes to select algorithms','line_number':166,'multiline':False]['text':' enable slow autotuning passes to select pointwise/reductions algorithms','line_number':169,'multiline':False]['text':' enable slow autotuning passes to select gemm algorithms','line_number':172,'multiline':False]['text':' force cublas and triton to use the same precision; cublas supports TF32 for matmul operations','line_number':175,'multiline':False]['text':' when m, n, k are multiples of 16, 16, 8, whereas triton supports TF32 for matmul operations','line_number':176,'multiline':False]['text':' for any combinations of m, n, k, regardless of their alignment. setting this flag will ensure','line_number':177,'multiline':False]['text':' that triton does not use TF32 wherever cublas would not use TF32','line_number':178,'multiline':False]['text':' Specify candidate backends for gemm autotune.','line_number':182,'multiline':False]['text':' Possible choices are combinations of: ATen, Triton, CUTLASS.','line_number':183,'multiline':False]['text':' ATen: default Pytorch ATen kernels.','line_number':184,'multiline':False]['text':' Triton: Triton templates defined in torch inductor.','line_number':185,'multiline':False]['text':' CUTLASS: Cutlass templates and kernels.','line_number':186,'multiline':False]['text':' the value used as a fallback for the unbacked SymInts','line_number':191,'multiline':False]['text':' that can appear in the input shapes (e.g., in autotuning)','line_number':192,'multiline':False]['text':' enable searching global and local cache regardless of `max_autotune`','line_number':195,'multiline':False]['text':' We will disable creating subprocess for autotuning if this is False','line_number':200,'multiline':False]['text':' If autotuning in subprocess, whether to use multiple devices','line_number':203,'multiline':False]['text':' Whether to keep the output strides the same as eager after layout optimization.','line_number':222,'multiline':False]['text':' Enabling this will let compiler print warning messages if a generated triton','line_number':225,'multiline':False]['text':' kernel has inputs with mixed layouts.  This is helpful for perf debugging','line_number':226,'multiline':False]['text':' since kernel with mixed layout inputs may run much slower then one whose inputs','line_number':227,'multiline':False]['text':' have uniform layouts.','line_number':228,'multiline':False]['text':' control store vs recompute heuristic','line_number':231,'multiline':False]['text':' For fanouts, rematerialization can lead to exponential blowup. So, have','line_number':232,'multiline':False]['text':' smaller threshold','line_number':233,'multiline':False]['text':' Threshold to prevent excessive accumulation of ops in one buffer during lowering','line_number':237,'multiline':False]['text':' fallback to eager for random/dropout, this is slow but useful for debugging','line_number':240,'multiline':False]['text':' automatically create fallbacks when encountering an unhandled op','line_number':243,'multiline':False]['text':' fuse even in cases without common reads','line_number':246,'multiline':False]['text':' For each fused kernel in the wrapper, comment with the nodes that get fused.','line_number':249,'multiline':False]['text':' Useful for debugging fusion.','line_number':250,'multiline':False]['text':' how many nodes to allow into a single fusion','line_number':255,'multiline':False]['text':' max number of inputs to generate cat as a pointwise op with masked laods','line_number':258,'multiline':False]['text':' replace small reductions with pointwise, disable with `= 1`','line_number':261,'multiline':False]['text':' Add extra comments to output code (causes compile cache misses)','line_number':264,'multiline':False]['text':' Convert 1x1 convs into matmuls','line_number':267,'multiline':False]['text':' Enable split reductions for better utilization when the dimension','line_number':270,'multiline':False]['text':' being reduced over is large (by splitting it)','line_number':271,'multiline':False]['text':' Enable constant and index_expr folding','line_number':276,'multiline':False]['text':' we always add constants into graph.constants without','line_number':279,'multiline':False]['text':' performing any constant-inlining optimization','line_number':280,'multiline':False]['text':' assert that indirect indexing does not read / write out of bounds','line_number':283,'multiline':False]['text':' constant folding on the joint graph','line_number':286,'multiline':False]['text':' Enable indirect_indexing asserts for decompositions and lowerings','line_number':289,'multiline':False]['text':' warnings intended for PyTorch developers, disable for point releases','line_number':292,'multiline':False]['text':' The multiprocessing start method to use for inductor workers in the codecache.','line_number':296,'multiline':False]['text':' TODO: fork is not safe in a multithreaded environment, we should evaluate changing','line_number':297,'multiline':False]['text':' the default to spawn.','line_number':298,'multiline':False]['text':' gemm autotuning global cache dir','line_number':326,'multiline':False]['text':' If kernel is fused, the name is generated from the origin node op names','line_number':342,'multiline':False]['text':' for larger kernels limit this','line_number':343,'multiline':False]['text':' Pad input tensors of matmul/bmm/addmm to leverage Tensor Cores in NVIDIA GPUs','line_number':346,'multiline':False]['text':' Fx-based linear/matmul/bmm + permute/transpose vertical fusion','line_number':349,'multiline':False]['text':' Mark the wrapper call in PyTorch profiler','line_number':352,'multiline':False]['text':' Generate hook calls to torch._inductor.hooks.run_intermediate_hooks for','line_number':355,'multiline':False]['text':' every intermediate for which we can correlate it with an intermediate','line_number':356,'multiline':False]['text':' from the original FX graph','line_number':357,'multiline':False]['text':' Populate traceback field on IRNode; good for debugging why origin_node is','line_number':360,'multiline':False]['text':' not populated, or finding out where an IRNode was constructed','line_number':361,'multiline':False]['text':' used for debugging to make sure config is properly set','line_number':364,'multiline':False]['text':' Specify a file where we print out the profiling results.','line_number':370,'multiline':False]['text':' None means we do not dump results to a file.','line_number':371,'multiline':False]['text':' TODO: remove later','line_number':374,'multiline':False]['text':' Freezing will attempt to inline weights as constants in optimization','line_number':378,'multiline':False]['text':' and run constant folding and other optimizations on them. After freezing, weights','line_number':379,'multiline':False]['text':' can no longer be updated.','line_number':380,'multiline':False]['text':' Make freezing invalidate the eager Parameters of nn modules, to avoid memory overhead','line_number':383,'multiline':False]['text':' of potentially keeping multiple copies of weights.','line_number':384,'multiline':False]['text':' Kill switch for allowing temporary tensors to be allocated as stack arrays. Tests','line_number':387,'multiline':False]['text':' should be run with this flag both on and off to make sure we have coverage.','line_number':388,'multiline':False]['text':' Enables an alternate DSO interface (the "minimal ArrayRef interface") intended','line_number':391,'multiline':False]['text':' to maximize performance for use cases that it can accommodate at the expense of','line_number':392,'multiline':False]['text':' generality. In brief:','line_number':393,'multiline':False]['text':' - inputs and outputs are ArrayRefTensor<T> (note that strides are required, but the','line_number':394,'multiline':False]['text':'   tensor must be contiguous)','line_number':395,'multiline':False]['text':' - constant handling is unchanged because it is not a per-inference-iteration bottleneck','line_number':396,'multiline':False]['text':'','line_number':397,'multiline':False]['text':' When the DSO is generated in this mode, the usual interface will also be supported,','line_number':398,'multiline':False]['text':' but performance for that interface may be degraded.','line_number':399,'multiline':False]['text':' config specific to codegen/cpp.py','line_number':403,'multiline':False]['text':' set to torch.get_num_threads()','line_number':405,'multiline':False]['text':' Do not generate loops when the condition doesn't hold, like:','line_number':408,'multiline':False]['text':' for(long i0=4096; i0<4096; i0+=1)','line_number':409,'multiline':False]['text':' Assume number of threads is dynamic, don't specialize thread number.','line_number':412,'multiline':False]['text':' Kernels don't recompile on thread number changes with this flag on.','line_number':413,'multiline':False]['text':' For single-threaded workload, turning it on would incur a slight','line_number':414,'multiline':False]['text':' performance degradation.','line_number':415,'multiline':False]['text':' download gcc12 from conda-forge if conda is installed','line_number':421,'multiline':False]['text':' "g++-12",','line_number':422,'multiline':False]['text':' "g++-11",','line_number':423,'multiline':False]['text':' "g++-10",','line_number':424,'multiline':False]['text':' "clang++",','line_number':425,'multiline':False]['text':' "g++.par",','line_number':427,'multiline':False]['text':' Allow kernel performance profiling via PyTorch profiler','line_number':429,'multiline':False]['text':' enable weight prepacking to get a better performance; may lead to large memory footprint','line_number':432,'multiline':False]['text':' Inject a bug into our relu implementation; useful for testing our repro','line_number':435,'multiline':False]['text':' extraction and minification functionality.','line_number':436,'multiline':False]['text':' Valid values: "compile_error", "runtime_error", "accuracy"','line_number':437,'multiline':False]['text':' If None, autodetect whether or not AVX512/AVX2 can be used.  Otherwise,','line_number':441,'multiline':False]['text':' force usage as specified, without testing.','line_number':442,'multiline':False]['text':' similar to config.triton.descriptive_names','line_number':445,'multiline':False]['text':' how many nodes to allow into a single horizontal fusion','line_number':448,'multiline':False]['text':' Make scatter_reduce fallback when reduce is sum to avoid performance regression','line_number':451,'multiline':False]['text':' using atomic_add.','line_number':452,'multiline':False]['text':' Use funsafe-math-optimizations when compiling','line_number':455,'multiline':False]['text':' config specific to codegen/triton.py','line_number':459,'multiline':False]['text':' Use cudagraphs on output code','line_number':461,'multiline':False]['text':' Use cudagraph trees for memory pooling if `cudagraphs` is True','line_number':464,'multiline':False]['text':' assertions not on the fast path, steady state','line_number':467,'multiline':False]['text':' TODO - need to debug why this prevents cleanup','line_number':470,'multiline':False]['text':' assertions on the fast path','line_number':473,'multiline':False]['text':' skip warmup for cudagraph trees','line_number':476,'multiline':False]['text':' Synchronize before and after every compiled graph.','line_number':479,'multiline':False]['text':' Synchronize after every kernel launch, to help pinpoint bugs','line_number':482,'multiline':False]['text':' Always load full blocks (rather than broadcasting inside the block)','line_number':485,'multiline':False]['text':' limit tiling dimensions','line_number':488,'multiline':False]['text':' use triton.autotune for pointwise ops with complex layouts','line_number':491,'multiline':False]['text':' this should only be disabled for debugging/testing','line_number':492,'multiline':False]['text':' max autotune gemm with cublasLt','line_number':495,'multiline':False]['text':' should we stop a fusion to allow better tiling?','line_number':498,'multiline':False]['text':' should we give different names to kernels','line_number':502,'multiline':False]['text':' Note: This is orthogonal to descriptive_names - this is deciding whether','line_number':503,'multiline':False]['text':' our triton kernel names should all be `triton_` (to maximize caching) or','line_number':504,'multiline':False]['text':' whether they should be unique.','line_number':505,'multiline':False]['text':' should we put op names in kernel names','line_number':508,'multiline':False]['text':' False: No special names (just triton__1, triton__2, etc.)','line_number':509,'multiline':False]['text':' "torch": Maps to the fx op in the Dynamo graph (module name, method name, etc.)','line_number':510,'multiline':False]['text':' "original_aten": Maps to the highest-level aten op (i.e. pre-decompositions)','line_number':511,'multiline':False]['text':' "inductor_node": Maps to the node name in the FX graph passed to Inductor','line_number':512,'multiline':False]['text':' use alternate codegen for smaller reductions','line_number':515,'multiline':False]['text':' hint to Triton when arguments are divisible by 16','line_number':520,'multiline':False]['text':' theses are not enforced, but they are used by asserts in triton_heuristics.py','line_number':523,'multiline':False]['text':' NOTE: mobilevit_s in timm_models required X to be set to the higher value 2048','line_number':524,'multiline':False]['text':' Store the generated cubin files for cpp wrapper code to load','line_number':527,'multiline':False]['text':' the max number of spills we allow for the configs we benchmark.','line_number':530,'multiline':False]['text':' Setting this to 0 means we skip a config if it spills even a single','line_number':531,'multiline':False]['text':' register.','line_number':532,'multiline':False]['text':' Setting it to a larger value allows a config spilling a small amount','line_number':533,'multiline':False]['text':' of registers being benchmarked.','line_number':534,'multiline':False]['text':'','line_number':535,'multiline':False]['text':' NOTE: triton will always report >0 register spills for kernels using sin/cos.','line_number':536,'multiline':False]['text':' (check this issue https://github.com/openai/triton/issues/1756 )','line_number':537,'multiline':False]['text':' So far we see a fixed 8 spilled registers for kernels using sin/cos.','line_number':538,'multiline':False]['text':' Raise the threshold to 16 to be safe.','line_number':539,'multiline':False]['text':' We should revisit this once we understand more of the source of register spills.','line_number':540,'multiline':False]['text':' Inject a bug into our relu implementation; useful for testing our repro','line_number':543,'multiline':False]['text':' extraction and minification functionality.','line_number':544,'multiline':False]['text':' Valid values: "compile_error", "runtime_error", "accuracy"','line_number':545,'multiline':False]['text':' AOTInductor output path','line_number':550,'multiline':False]['text':' If an absolute path is specified, the generated lib files will be stored under the directory;','line_number':551,'multiline':False]['text':' If a relative path is specified, it will be used as a subdirectory under the default caching path;','line_number':552,'multiline':False]['text':' If not specified, a temp directory will be created under the default caching path.','line_number':553,'multiline':False]['text':' If the specified path contains something like "model.so", the sub-string will be used','line_number':554,'multiline':False]['text':' to name the generated library.','line_number':555,'multiline':False]['text':' Wether to codegen abi compatible model.so','line_number':560,'multiline':False]['text':' Serialized tree spec for flattening inputs','line_number':563,'multiline':False]['text':' Serialized tree spec for flattening outputs','line_number':566,'multiline':False]['text':' CUDA arch to use for CUDA template kernel compilation.','line_number':571,'multiline':False]['text':' e.g. "70", "75", "80", "90", etc.','line_number':572,'multiline':False]['text':' When arch is None, Inductor uses torch.cuda.get_device_capability(0).','line_number':573,'multiline':False]['text':' CUDA version to use for CUDA template kernel compilation.','line_number':576,'multiline':False]['text':' e.g. "11.4", "12.1", etc.','line_number':577,'multiline':False]['text':' When version is None, Inductor uses torch.version.cuda.','line_number':578,'multiline':False]['text':' Optimization level for the host compiler.','line_number':581,'multiline':False]['text':' Whether to enable device LTO (link-time-optimization).','line_number':584,'multiline':False]['text':' Whether to keep intermediate files dring compilation.','line_number':587,'multiline':False]['text':' Whether to enable debug info, e.g. line number, cutlass debug info.','line_number':590,'multiline':False]['text':' Whether to use fast math.','line_number':593,'multiline':False]['text':' Path to the CUTLASS repo root directory.','line_number':596,'multiline':False]['text':' The default path only works under PyTorch local development environment.','line_number':597,'multiline':False]['text':' Configures the maximum number of CUTLASS configs to profile in max_autotune.','line_number':605,'multiline':False]['text':' By default it's None, so that all CUTLASS configs are tuned.','line_number':606,'multiline':False]['text':' This is mainly used to reduce test time in CI.','line_number':607,'multiline':False]['text':' Path to CUDA NVCC.','line_number':610,'multiline':False]['text':' NVCC search order:','line_number':611,'multiline':False]['text':' 1) cuda_cxx set in this config','line_number':612,'multiline':False]['text':' 2）CUDACXX environment variable','line_number':613,'multiline':False]['text':' 3）CUDA_HOME environment variable','line_number':614,'multiline':False]['text':' 4) default system search PATH.','line_number':615,'multiline':False]['text':' If set to True, it will ensure that only GEMM ops capable of','line_number':618,'multiline':False]['text':' epilogue fusion via CUTLASS Epilogue Visitor Trees ( EVT )','line_number':619,'multiline':False]['text':' are enabled for the CUTLASS backend.','line_number':620,'multiline':False]['text':' create a directory containing lots of debug information','line_number':624,'multiline':False]['text':' master switch for all debugging flags below','line_number':626,'multiline':False]['text':' Save debug information to a temporary directory','line_number':629,'multiline':False]['text':' If not specified, a temp directory will be created by system','line_number':630,'multiline':False]['text':' Save python logger call >=logging.DEBUG','line_number':633,'multiline':False]['text':' Save python logger call >=logging.INFO','line_number':636,'multiline':False]['text':' Save input FX graph (post decomps, pre optimization)','line_number':639,'multiline':False]['text':' Save FX graph after transformations','line_number':642,'multiline':False]['text':' Save TorchInductor IR before fusion pass','line_number':645,'multiline':False]['text':' Save TorchInductor IR after fusion pass','line_number':648,'multiline':False]['text':' Copy generated code to trace dir','line_number':651,'multiline':False]['text':' SVG figure showing post-fusion graph','line_number':654,'multiline':False]['text':' SVG figure showing fx with fusion','line_number':657,'multiline':False]['text':' We draw our fx graphs with the "record" shape attribute by default.','line_number':660,'multiline':False]['text':' Sometimes, when the graph is very complex, we may hit dot errors like below:','line_number':661,'multiline':False]['text':'   "flat edge between adjacent nodes one of which has a record shape -','line_number':662,'multiline':False]['text':'    replace records with HTML-like labels"','line_number':663,'multiline':False]['text':' and thus fail to generate a graph. So, let's give the user an option','line_number':664,'multiline':False]['text':' to specify the shape attribute for the dot graph. For example, passing','line_number':665,'multiline':False]['text':' INDUCTOR_DOT_GRAPH_SHAPE_SVG = "none" would let us generate HTML-like lables','line_number':666,'multiline':False]['text':' to workaround the above failure.','line_number':667,'multiline':False]['text':' Store cProfile (see snakeviz to view)','line_number':670,'multiline':False]['text':' Upload the .tar.gz file','line_number':673,'multiline':False]['text':' Needs to be overriden based on specific environment needs','line_number':674,'multiline':False]['text':' workaround: "Can't pickle <function ...>"','line_number':679,'multiline':False]['text':' noqa: F401, F403','line_number':684,'multiline':False]['text':' adds patch, save_config, etc','line_number':688,'multiline':False]