['text':' NOQA: F401','line_number':36,'multiline':False]['text':' TODO(jansel): ezyang says we won't need this in the future, try removing it','line_number':114,'multiline':False]['text':' based on https://github.com/pytorch/pytorch/blob/9e3eb329df8f701/c10/core/ScalarType.h#L28','line_number':115,'multiline':False]['text':' TODO(jansel): add quantized types?','line_number':130,'multiline':False]['text':'  _(c10::qint8, QInt8) /* 12 */','line_number':131,'multiline':False]['text':' _(c10::quint8, QUInt8) /* 13 */','line_number':132,'multiline':False]['text':' _(c10::qint32, QInt32) /* 14 */','line_number':133,'multiline':False]['text':' _(c10::quint4x2, QUInt4x2) /* 16 */','line_number':134,'multiline':False]['text':' _(c10::quint2x4, QUInt2x4) /* 17 */','line_number':135,'multiline':False]['text':' type: ignore[attr-defined]','line_number':151,'multiline':False]['text':' construct a tmp tensor to feed into torch.result_type','line_number':170,'multiline':False]['text':' FIXME that's a crude approximation for promoting args','line_number':200,'multiline':False]['text':' sometimes args are an immutable list so we can't mutate them','line_number':210,'multiline':False]['text':' TODO maybe we need to use pytrees here','line_number':273,'multiline':False]['text':' explicitly assert for "out=" ops for better error messages','line_number':278,'multiline':False]['text':' kwargs tensors not supported yet unless it's a fallback op','line_number':282,'multiline':False]['text':' prefer shorter formula','line_number':341,'multiline':False]['text':' group by device, whether any of the inputs are dynamic, and whether their types match','line_number':442,'multiline':False]['text':' (proxy for type promotion)','line_number':443,'multiline':False]['text':' broadcast scalar inputs to match length of list inputs','line_number':477,'multiline':False]['text':' Because we may promote tensor type from float16 or bfloat16','line_number':548,'multiline':False]['text':' to float, we will need to pass the original src dtype (i.e. x_dtype),','line_number':549,'multiline':False]['text':' which is used for correctly constructing type conversion before bitcast,','line_number':550,'multiline':False]['text':' which requires the bitwidth of the input tensor type is the same as the','line_number':551,'multiline':False]['text':' target type.','line_number':552,'multiline':False]['text':' AOT autograd handles this for us','line_number':676,'multiline':False]['text':' squeeze does nothing if the size isn't 1','line_number':697,'multiline':False]['text':' TODO: It would be better to realize the input if any of its sizes','line_number':775,'multiline':False]['text':' are unbacked, because typically the size will be non-zero.  However,','line_number':776,'multiline':False]['text':' this cannot be done directly as below as we'll choke on the size_hint','line_number':777,'multiline':False]['text':' here','line_number':778,'multiline':False]['text':' maybe realize input before broadcasting it','line_number':782,'multiline':False]['text':' maybe realize the input','line_number':844,'multiline':False]['text':' ATen specifies int[1] type for shifts and dims which expands integers to tuples of length 1','line_number':894,'multiline':False]['text':' Takes care of the case when dims is not specified (default)','line_number':909,'multiline':False]['text':' By default, the tensor is flattened before shifting, after which the original shape is restored','line_number':910,'multiline':False]['text':' TODO: Avoid guarding on shape here','line_number':925,'multiline':False]['text':' as_strided ignores views','line_number':948,'multiline':False]['text':' (inclusive, exclusive)','line_number':978,'multiline':False]['text':' if we're concatting [4], [2]','line_number':1012,'multiline':False]['text':' when we index the second tensor for 5 we want to index 5 - 4','line_number':1013,'multiline':False]['text':' this value should be unused','line_number':1020,'multiline':False]['text':' TODO <leslie> Remove this fallback when we support vectorization','line_number':1050,'multiline':False]['text':' code gen with uint8 data type directly.','line_number':1051,'multiline':False]['text':' Unrealized inputs will not be storage and layouts, and we dont want to realize','line_number':1068,'multiline':False]['text':' them in case we want to fuse','line_number':1069,'multiline':False]['text':' TODO: We don't have to guard on sizes per se, but the number','line_number':1168,'multiline':False]['text':' of splits must stay constant','line_number':1169,'multiline':False]['text':' TODO: don't guard on static shape here','line_number':1253,'multiline':False]['text':' There are some types (CPU) which we accept as input but not as','line_number':1625,'multiline':False]['text':' output.','line_number':1626,'multiline':False]['text':' Complex views are supported with IR ComplexView','line_number':1630,'multiline':False]['text':' Custom fallback lowering','line_number':1646,'multiline':False]['text':' We should be able to remove this special case once `disable_cpp_codegen` is killed.','line_number':1650,'multiline':False]['text':' only skip codegen if there is a cpu output, not input','line_number':1674,'multiline':False]['text':' if fallback_random, we allow not decomposing random','line_number':1688,'multiline':False]['text':' Note: 'warn' is holdover from when this was a warning, but for ops that previously','line_number':1694,'multiline':False]['text':' set warn=False we do not want a CI error.','line_number':1695,'multiline':False]['text':' Ignore the 'suppress errors' configs in CI, as this particular warning happens on startup anyway and is not','line_number':1696,'multiline':False]['text':' likely to be triggered preferentially on one CI config over another.','line_number':1697,'multiline':False]['text':' stride arg is optional and will be used in future for distributed random','line_number':1743,'multiline':False]['text':' ops. Currently, its unused.','line_number':1744,'multiline':False]['text':' Both seed and offset in the philox_rand op are tensors.','line_number':1755,'multiline':False]['text':' torch seed and offsets are of type int64, but tl.rand accepts int32','line_number':1756,'multiline':False]['text':' Get the offset'd position','line_number':1759,'multiline':False]['text':' This shouldn't be called in general','line_number':1809,'multiline':False]['text':' only warn once per graph','line_number':1821,'multiline':False]['text':' The entire boundaries tensor needs to be used by ops.bucketize, so we','line_number':1954,'multiline':False]['text':' need to realize it into global memory; or in other words, we can't','line_number':1955,'multiline':False]['text':' guarantee that boundaries.get_name() (used below) will exist unless','line_number':1956,'multiline':False]['text':' we call boundaries.realize().','line_number':1957,'multiline':False]['text':' TODO(jansel): we should implement decomps or lowerings for these','line_number':2021,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/327','line_number':2022,'multiline':False]['text':' sdpa requires dense last dimension','line_number':2039,'multiline':False]['text':' contiguous stride order','line_number':2050,'multiline':False]['text':' This is the minimum alignment required by SDPA kernels for attention_bias.','line_number':2053,'multiline':False]['text':' This value can be found in pytorch/aten/src/ATen/native/transformers/attention.cpp preprocess_mask','line_number':2054,'multiline':False]['text':' This correctly handles the forward case:','line_number':2067,'multiline':False]['text':' input is padded, requiring_stride_order will unwrap the view and unpad.','line_number':2070,'multiline':False]['text':' Would be nice to be able to require certain padding from inductor ir, nyi','line_number':2071,'multiline':False]['text':' TODO: This is done, just need to enable support in TorchInductor for complex types.','line_number':2133,'multiline':False]['text':' The following were added as a result of https://github.com/pytorch/pytorch/pull/94039 to pass tests','line_number':2136,'multiline':False]['text':' It's not necessarily a priority to implement these','line_number':2137,'multiline':False]['text':' these are data-dependent, highly unlikely you can write a lowering','line_number':2263,'multiline':False]['text':' fails accuracy on test_torch.py, and explicit fallback required to avoid warn=True on implicit','line_number':2269,'multiline':False]['text':' Register with type_promotion_kind None.','line_number':2273,'multiline':False]['text':' For example, fp16.copy_(fp32) should **not** promote the first input's dtype.','line_number':2274,'multiline':False]['text':' TODO(jansel): memory format','line_number':2291,'multiline':False]['text':' selecting every element is the same as just src.clone()','line_number':2377,'multiline':False]['text':' inline small tensors','line_number':2457,'multiline':False]['text':' This is interesting!  Most lowerings return tensors, so you can just','line_number':2510,'multiline':False]['text':' return the buffer you allocated and it will get used (or not used, if','line_number':2511,'multiline':False]['text':' it's dead.)  But _local_scalar_dense (aka item) returns an int,','line_number':2512,'multiline':False]['text':' not a Tensor, so you would have a type mismatch if you return a buffer;','line_number':2513,'multiline':False]['text':' we are obligated to return a sympy expression instead.  However,','line_number':2514,'multiline':False]['text':' we need to actually codegen the .item() call somehow.  We do this','line_number':2515,'multiline':False]['text':' by registering a faux buffer for the DynamicScalar IR node, which is','line_number':2516,'multiline':False]['text':' solely responsible for generating this .item().  The buffer is','line_number':2517,'multiline':False]['text':' not used for anything (notice we discard it); at codegen time,','line_number':2518,'multiline':False]['text':' the "buffer" just gets assigned None.','line_number':2519,'multiline':False]['text':' torch.zeros, torch.ones, etc','line_number':2562,'multiline':False]['text':' explicitly set ranges to zeros in order to make a NopKernelSchedulerNode','line_number':2675,'multiline':False]['text':' sparse_grad doesn't affect forward computation,','line_number':2721,'multiline':False]['text':' and backward tracing is taken care of by AOT Autograd','line_number':2722,'multiline':False]['text':' Eager allows indices to be CPU tensor when running on CUDA','line_number':2790,'multiline':False]['text':' FIXME: Calling to_device(x, device) should work but','line_number':2791,'multiline':False]['text':' test_advancedindex_mixed_cpu_devices still fails','line_number':2792,'multiline':False]['text':' Note that behavior of indexing differs when there are non consecutive','line_number':2809,'multiline':False]['text':' tensors. In this case, the tensor index is pulled to the beginning.','line_number':2810,'multiline':False]['text':'','line_number':2811,'multiline':False]['text':' Suppose a = torch.arange(3 * 4 * 5 * 6 * 7).view(3, 4, 5, 6, 7)','line_number':2812,'multiline':False]['text':'         x = torch.tensor[1,2]','line_number':2813,'multiline':False]['text':' Then, a[:,x,:,x,:] will have shape 2,3,5,7 as due to x,:,x then 2 will','line_number':2814,'multiline':False]['text':' be pulled to the front.','line_number':2815,'multiline':False]['text':' no guards on output size, all the guards are set in broadcast_tensors','line_number':2877,'multiline':False]['text':' We can use the first one since they are all required to be the same size','line_number':2879,'multiline':False]['text':' Fallback to ATen for boolean indexing','line_number':2913,'multiline':False]['text':' All the indexing decompositions are written in terms of index, index_put, and index_put_','line_number':2925,'multiline':False]['text':' We cannot have this lowering as a decomposition as it introduces','line_number':2926,'multiline':False]['text':' mutation in the graph, which is bad for Aot Autograd. Aot Autograd runs dead','line_number':2927,'multiline':False]['text':' code elimination and common subexpression elimination optimizations, which','line_number':2928,'multiline':False]['text':' assume graphs to be side-effect free. More details at','line_number':2929,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/1235','line_number':2930,'multiline':False]['text':' and','line_number':2931,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/1863','line_number':2932,'multiline':False]['text':' tl.atomic_add does NOT support the following types','line_number':2979,'multiline':False]['text':' Dispatch to masked fill for single boolean index with single value','line_number':2984,'multiline':False]['text':' Fallback in torch deterministic mode','line_number':2995,'multiline':False]['text':' Fallback if there is a boolean index','line_number':2999,'multiline':False]['text':' self is an scalar Tensor','line_number':3008,'multiline':False]['text':' Note that code will only get here when dtype is uint32','line_number':3019,'multiline':False]['text':' self is an scalar Tensor','line_number':3031,'multiline':False]['text':' We can use the first one since they are all required to be the same size','line_number':3035,'multiline':False]['text':' all guards are set above during broadcast_tensors and expand','line_number':3051,'multiline':False]['text':' iter_ranges,','line_number':3057,'multiline':False]['text':' self is captured from the end of the function, so it may have 0 dim','line_number':3229,'multiline':False]['text':' src is a scalar','line_number':3242,'multiline':False]['text':' TODO: Need to support more reduction type','line_number':3249,'multiline':False]['text':' zero out the corresponding elements first','line_number':3254,'multiline':False]['text':' self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0','line_number':3270,'multiline':False]['text':' self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1','line_number':3271,'multiline':False]['text':' self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2','line_number':3272,'multiline':False]['text':' elements are reused','line_number':3300,'multiline':False]['text':' Nearest Exact: input_index = round(scale * (output_index + 0.5) - 0.5)','line_number':3315,'multiline':False]['text':'                            = floor(scale * (output_index + 0.5))','line_number':3316,'multiline':False]['text':' Nearest: input_index = floor(scale * output_index)','line_number':3317,'multiline':False]['text':' dot product between xs and cs','line_number':3452,'multiline':False]['text':' TODO(Lezcano) Here we may not need to set-up a device_size','line_number':3473,'multiline':False]['text':' Areas after reflection:','line_number':3525,'multiline':False]['text':'','line_number':3526,'multiline':False]['text':'   top-left    |   top     |   top-right','line_number':3527,'multiline':False]['text':' -----------------------------------------','line_number':3528,'multiline':False]['text':'   left        |   center  |   right','line_number':3529,'multiline':False]['text':' -----------------------------------------','line_number':3530,'multiline':False]['text':'   bottom-left |   bottom  |   bottom-right','line_number':3531,'multiline':False]['text':'','line_number':3532,'multiline':False]['text':' The center area is the original matrix. Other areas are reflections.','line_number':3533,'multiline':False]['text':' Accumulate gradients from different areas','line_number':3539,'multiline':False]['text':' If some of the padding is negative, center load is not always valid','line_number':3540,'multiline':False]['text':' If the upper bound is less than the lower bound, we can get rid of one accumulation.','line_number':3551,'multiline':False]['text':' This happens when the padding size is zero.','line_number':3552,'multiline':False]['text':' note - dims pre-canonicalized','line_number':3588,'multiline':False]['text':' if padding is a complicated expression, hoist it','line_number':3619,'multiline':False]['text':' Sliding windows must start within the input or left padding','line_number':3722,'multiline':False]['text':' ceil mode is actually a no-op, lets guard on that','line_number':3726,'multiline':False]['text':' Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.','line_number':3777,'multiline':False]['text':' TODO(jansel): should we force these to be realized?','line_number':3817,'multiline':False]['text':' we will read this many times, so make sure it is computed','line_number':3845,'multiline':False]['text':' some classes don't have `get_stride`','line_number':3850,'multiline':False]['text':' TODO will need a better way of determining if inputs are channels-last','line_number':3851,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3853,'multiline':False]['text':' type: ignore[attr-defined]','line_number':3854,'multiline':False]['text':' don't codegen channels-last when autotune is not enabled, it's very slow','line_number':3881,'multiline':False]['text':' Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.','line_number':3911,'multiline':False]['text':' don't need mask for 0, 0','line_number':3961,'multiline':False]['text':' no-op if the same input and output','line_number':4062,'multiline':False]['text':' Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.','line_number':4093,'multiline':False]['text':' TODO: should we force these to be realized?','line_number':4115,'multiline':False]['text':' Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.','line_number':4215,'multiline':False]['text':' TODO(jansel): optimize to do `int(x<h)` rather than `x<h?1:0`','line_number':4254,'multiline':False]['text':' TODO(jansel): should we force these to be realized?','line_number':4263,'multiline':False]['text':' we will read this many times, so make sure it is computed','line_number':4296,'multiline':False]['text':' Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.','line_number':4326,'multiline':False]['text':' Only realize if reduction isn't unrolled','line_number':4519,'multiline':False]['text':' compute in higher-precision until end of mean lowering','line_number':4547,'multiline':False]['text':' Instead of unrolling welford, just unroll the simpler two-step var','line_number':4585,'multiline':False]['text':' Type promotion ensures all tensor arguments have the same type','line_number':4715,'multiline':False]['text':' Optimize away small fixed powers, or for integers avoid falling back to ATen','line_number':4719,'multiline':False]['text':' ops.pow doesn't work for integers','line_number':4743,'multiline':False]['text':' introduce a copy to handle views','line_number':4763,'multiline':False]['text':' Fast path, just swing the data pointer','line_number':4775,'multiline':False]['text':' floordiv and truncdiv need special handling for integer tensors on Triton,','line_number':4812,'multiline':False]['text':' see the discussion at https://github.com/openai/triton/issues/605','line_number':4813,'multiline':False]['text':' NOTE: prims.div maps to a / b in C, so performs truncation division on','line_number':4833,'multiline':False]['text':'   integer inputs and true division for floating and complex inputs.','line_number':4834,'multiline':False]['text':' these are only encountered as outputs of the graph','line_number':5115,'multiline':False]['text':' reinplacing epilogue copies improves compile time','line_number':5116,'multiline':False]['text':' by removing extra buffers sent to the scheduler.','line_number':5117,'multiline':False]['text':' Note [Can val be an int?]','line_number':5205,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':5206,'multiline':False]['text':' In principle, someone could construct an FX graph where','line_number':5207,'multiline':False]['text':' a call to size/stride has a val that is a plain int (not','line_number':5208,'multiline':False]['text':' SymInt).  However, we will maintain the invariant that','line_number':5209,'multiline':False]['text':' this is not possible: if you are constructing an FX graph','line_number':5210,'multiline':False]['text':' where there is a call to size/stride that returns an','line_number':5211,'multiline':False]['text':' int, but you KNOW that int must always be a constant,','line_number':5212,'multiline':False]['text':' then you do not need trace that call at all (and just','line_number':5213,'multiline':False]['text':' constant propagate the integer as is.)','line_number':5214,'multiline':False]['text':' See Note [Can val be an int?]','line_number':5222,'multiline':False]['text':' TODO(jansel): decompose into `variable.grad += new_grad` when variable.grad is defined','line_number':5249,'multiline':False]['text':' populate lowerings defined in kernel/*','line_number':5423,'multiline':False]