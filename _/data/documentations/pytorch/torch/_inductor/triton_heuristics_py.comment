['text':' Triton codegen tries to codegen set of AutotuneHints.','line_number':68,'multiline':False]['text':' Enum.__repr__ looks like "<AutotuneHint.ELEMENTS_PER_WARP_32: 0>""','line_number':69,'multiline':False]['text':' which isn't valid python.','line_number':70,'multiline':False]['text':' Enum.__str__ will just return "AutotuneHint.ELEMENTS_PER_WARP_32".','line_number':71,'multiline':False]['text':' Autotuning can give different benchmarking results from run to run, and','line_number':115,'multiline':False]['text':' therefore we disable autotuning when use_deterministic flag is on.','line_number':116,'multiline':False]['text':' passed directly to triton','line_number':133,'multiline':False]['text':' metadata not relevant to triton','line_number':139,'multiline':False]['text':' pre-create the profiler context manager to reduce latency','line_number':170,'multiline':False]['text':' Skip the config if we run out of resource','line_number':187,'multiline':False]['text':' Disable for AMDGPU as Triton is not ready to return n_regs for a compiled_binary.','line_number':207,'multiline':False]['text':' make sure rblock is not too small','line_number':222,'multiline':False]['text':' each SM of A100 has 65536 32-bit registers. To maximize','line_number':226,'multiline':False]['text':' the theoretical occupancy, we need run 2048 threads on each','line_number':227,'multiline':False]['text':' SM. So each thread should use no more than 65536 / 2048','line_number':228,'multiline':False]['text':' = 32 registers. In cases where occupancy matters, and each','line_number':229,'multiline':False]['text':' thread uses too many registers, reduce RBLOCK to reduce','line_number':230,'multiline':False]['text':' the register usage.','line_number':231,'multiline':False]['text':' For kernel https://gist.github.com/shunting314/e4cccc031fe30d378b9b23c08c238cbd','line_number':232,'multiline':False]['text':' from PLBartForCausalLM, latency improve from','line_number':233,'multiline':False]['text':' 7.795ms to 4.883ms.','line_number':234,'multiline':False]['text':'','line_number':235,'multiline':False]['text':' Previously we set max_blocks_per_sm to 'max_threads_per_multi_processo / (32 * num_warps)'','line_number':246,'multiline':False]['text':' The formula below is a tighter upper bound since we have the assumption that','line_number':247,'multiline':False]['text':'   nreg > device_prop.regs_per_multiprocessor // device_prop.max_threads_per_multi_processor','line_number':248,'multiline':False]['text':' due to the if condition above and:','line_number':249,'multiline':False]['text':'   regs_per_multiprocessor / nreg_per_block','line_number':250,'multiline':False]['text':'   = regs_per_multiprocessor / (nreg * 32 * num_warps)','line_number':251,'multiline':False]['text':'   < regs_per_multiprocessor / ((regs_per_multiprocessor / max_threads_per_multi_processor) * 32 * num_warps)','line_number':252,'multiline':False]['text':'   = max_threads_per_multi_processor / (32 * num_warps)','line_number':253,'multiline':False]['text':' Using a tigher upper bound can reveal more optimization opportunities.','line_number':254,'multiline':False]['text':' no need to improve occupancy','line_number':263,'multiline':False]['text':' Setting device_type="hip" required on ROCm to pass down to triton','line_number':286,'multiline':False]['text':' load binary to the correct device','line_number':300,'multiline':False]['text':' need to initialize context','line_number':302,'multiline':False]['text':' store this global variable to avoid the high overhead of reading it when calling run','line_number':352,'multiline':False]['text':' clone inplace buffers to avoid autotune contaminating them if','line_number':392,'multiline':False]['text':' the kernel does in-place stores. avoid cloning other buffers because','line_number':393,'multiline':False]['text':' it leads to increase memory use','line_number':394,'multiline':False]['text':' unique kernel name','line_number':450,'multiline':False]['text':' User defined triton kernels will have arbitrary kwarg names','line_number':463,'multiline':False]['text':' There is some divergence between CUDA and ROCm here.','line_number':470,'multiline':False]['text':' On ROCm's triton we only have the the path to the binary, not the binary itself.','line_number':471,'multiline':False]['text':' For ROCm we will copy the binary to the new location instead of writing to file','line_number':472,'multiline':False]['text':' skip triton template','line_number':495,'multiline':False]['text':' guard the record_function_ctx and only call it if profiling is currently','line_number':556,'multiline':False]['text':' in progress, to reduce latency when profiler is not turned on. Note that','line_number':557,'multiline':False]['text':' the "if" statement (instead of, say, a contextlib.nullcontext) is intentional;','line_number':558,'multiline':False]['text':' it is faster than entering and exiting a context manager, even if the context','line_number':559,'multiline':False]['text':' manager is a nullcontext.','line_number':560,'multiline':False]['text':' sort perf numbers in descending order, i.e. placing the','line_number':616,'multiline':False]['text':' most runtime-heavy kernels at the top of the list','line_number':617,'multiline':False]['text':' also display the runtime percentage for each kernel','line_number':625,'multiline':False]['text':' on disk caching logic','line_number':738,'multiline':False]['text':' Remove XBLOCK from config if it's not a function argument.','line_number':769,'multiline':False]['text':' This way, coordinate descent tuning will not try to tune it.','line_number':770,'multiline':False]['text':'','line_number':771,'multiline':False]['text':' Context: When TritonKernel.no_x_dim is True, we hardcode XBLOCK to 1.','line_number':772,'multiline':False]['text':' Ideally we want to read this from some device config','line_number':862,'multiline':False]['text':' for a 2d size_hints [a, b], a should be mapped to YBLOCK rather than XBLOCK','line_number':864,'multiline':False]['text':' shrink sizes to size hints','line_number':873,'multiline':False]['text':' if we are below original block size, scale up where we can;','line_number':880,'multiline':False]['text':' or if the calculated grid size is larger than the limit, we bump up the corresponding dimension','line_number':881,'multiline':False]['text':' we are going to arrive at 2 warps only if bs was too small due to','line_number':906,'multiline':False]['text':' numel being too small. However to workaround some ptx bugs we still','line_number':907,'multiline':False]['text':' want at least 4 warps if there's enough elements per thread','line_number':908,'multiline':False]['text':' given that this is a rare situation, don't expect this to affect perf','line_number':909,'multiline':False]['text':' in general','line_number':910,'multiline':False]['text':' see https://github.com/pytorch/pytorch/pull/97950','line_number':911,'multiline':False]['text':' Increase x to satisfy min_elem_per_thread requirements.','line_number':917,'multiline':False]['text':' shrink sizes to size hints','line_number':944,'multiline':False]['text':' if we are below original block size, scale up where we can','line_number':948,'multiline':False]['text':' shrink sizes to size hints','line_number':973,'multiline':False]['text':' if we are below original block size, scale up where we can','line_number':978,'multiline':False]['text':' ~8% better for fp16','line_number':1061,'multiline':False]['text':' skip all these cases','line_number':1124,'multiline':False]['text':' halve the XBLOCK/RBLOCK compared to outer_config','line_number':1169,'multiline':False]['text':' TODO: this may only be beneficial when each iteration of the reduction','line_number':1170,'multiline':False]['text':' is quite heavy. E.g. https://gist.github.com/shunting314/189a8ef69f90db9d614a823385147a72','line_number':1171,'multiline':False]['text':' TODO(jansel): we should be able to improve these heuristics','line_number':1197,'multiline':False]['text':' we don't need RBLOCK for persistent reduction','line_number':1209,'multiline':False]