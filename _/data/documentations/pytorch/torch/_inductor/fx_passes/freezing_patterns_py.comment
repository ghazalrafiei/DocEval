['text':' First pass_patterns[0] are applied, then [1], then [2]','line_number':24,'multiline':False]['text':' We need a few rounds of binary folding to get rid of all the','line_number':42,'multiline':False]['text':' unnecessary nodes, but may need a good method to chose the rounds number.','line_number':43,'multiline':False]['text':' works like: conv+binary+binary.','line_number':44,'multiline':False]['text':' Make sure meta['val'] is properly set for all nodes','line_number':51,'multiline':False]['text':' If we don't have binary folding, we don't need to run the pass again.','line_number':54,'multiline':False]['text':' TODO: remove the need to run fake_tensor_prop on the whole model.','line_number':55,'multiline':False]['text':' The CPU weight packing always assume the conv's weight is channels last,','line_number':68,'multiline':False]['text':' So make sure the layout_optimization is on when doing it.','line_number':69,'multiline':False]['text':' workaround https://github.com/pytorch/pytorch/issues/97894','line_number':116,'multiline':False]