['text':' attn_weight = torch.dropout(attn_weight, dropout_p)','line_number':109,'multiline':False]['text':' in real workloads inputs to matmul are permuted','line_number':146,'multiline':False]['text':' causing matmul to expand to a series of expand and clone calls','line_number':147,'multiline':False]['text':' we want the same to happen during pattern tracing','line_number':148,'multiline':False]['text':' sdpa prefers inputs in permuted format','line_number':161,'multiline':False]['text':' it makes a copy to put them in this format','line_number':162,'multiline':False]['text':' if they aren't already','line_number':163,'multiline':False]['text':' to make replacement efficient ensure that inputs to sdpa','line_number':164,'multiline':False]['text':' are in required order','line_number':165,'multiline':False]['text':' attn_mask,','line_number':174,'multiline':False]['text':' no dropout version of pattern 7','line_number':181,'multiline':False]['text':' attn_mask,','line_number':201,'multiline':False]['text':' attn_mask,','line_number':229,'multiline':False]['text':' no dropout version of 9','line_number':236,'multiline':False]['text':' attn_mask,','line_number':257,'multiline':False]['text':' Mainly for huggingface models','line_number':264,'multiline':False]['text':' Has attn_mask add.','line_number':334,'multiline':False]['text':' attn_mask_node may be a float/int number.','line_number':337,'multiline':False]['text':' Make sure attn_mask.dtype == query.dtype or attn_mask.dtype == torch.bool','line_number':341,'multiline':False]['text':' Note: args[1] of the scale_factor_node is always the scale_factor for the current patterns.','line_number':354,'multiline':False]['text':' make sure the scale_factor a float/int. SymInt?','line_number':356,'multiline':False]['text':' type: ignore[attr-defined]','line_number':381,'multiline':False]['text':' workaround https://github.com/pytorch/pytorch/issues/97894','line_number':391,'multiline':False]['text':' sizes/values don't actually matter for initial trace','line_number':396,'multiline':False]['text':' once we get a possible match we re-trace with the actual values and verify the match still holds','line_number':397,'multiline':False]['text':' workaround https://github.com/pytorch/pytorch/issues/97894','line_number':403,'multiline':False]['text':' 0.113377 is a "magic" value that lets us recover the lost input arg relationship','line_number':404,'multiline':False]['text':' we could also generate all these patterns in 3d.. TODO','line_number':407,'multiline':False]['text':' softmax will generate a dtype conversion on inputs if they are in half,','line_number':412,'multiline':False]['text':' but will not in float, so we generate a pattern for both','line_number':413,'multiline':False]['text':' XXX: when adding a new pattern, re-run `gen_attention_patterns` so the pattern','line_number':513,'multiline':False]['text':' gets serialized to a python file and does not require tracing at runtime.','line_number':514,'multiline':False]['text':' functools.partial insufficient because we look at signature downstream','line_number':533,'multiline':False]