['text':' only insert to_dtype if is_bf16 is True','line_number':72,'multiline':False]['text':' binary_op(other, computation_op)','line_number':178,'multiline':False]['text':' binary_op(computation_op, other)','line_number':182,'multiline':False]['text':' fusion pattern is always in the form of computation_op + to_float32 + unary_op + to_bfloat16','line_number':207,'multiline':False]['text':' inp is a Number','line_number':246,'multiline':False]['text':' computation_args += ["none", [], ""]','line_number':261,'multiline':False]['text':' inp is a Number','line_number':289,'multiline':False]['text':' check alpha is one.','line_number':340,'multiline':False]['text':' check args[0] and args[1] is not same','line_number':354,'multiline':False]['text':' Make sure the other is not an alias or mutation(fx side doesn't has such info).','line_number':446,'multiline':False]['text':' convert reshape+linear+reshape to a single linear for applying fusion path.','line_number':638,'multiline':False]['text':' check linear's input's shape[:-1] == reshape_2[:-1]','line_number':673,'multiline':False]['text':' and check product(reshape_2[:-1]) == reshape_1[0]','line_number':674,'multiline':False]['text':' TODO: Haozhe investigate how add guard here','line_number':676,'multiline':False]['text':' convert linear+bias to a single linear for applying fusion path.','line_number':710,'multiline':False]['text':' Weights should be Constant','line_number':738,'multiline':False]['text':' Meta info for weights and inputs should be available','line_number':744,'multiline':False]['text':' Check device','line_number':748,'multiline':False]['text':' Check dtype','line_number':755,'multiline':False]['text':' TODO: Support dynamic shape case for MKLDNN conv transpose.','line_number':792,'multiline':False]['text':' doesn't support group_depthwise_conv_transpose.','line_number':797,'multiline':False]['text':' Port from: aten/src/ATen/native/Convolution.cpp:is_output_padding_big','line_number':800,'multiline':False]['text':' weight_idx is 1 for aten.mm and is 2 for aten.addmm','line_number':815,'multiline':False]['text':' on x86, for fp32, mkl should be enabled and batch_size should not be a free symbol.','line_number':825,'multiline':False]['text':' on aarch64, use mkldnn op for fp32 as well if acl is enabled','line_number':826,'multiline':False]['text':' input','line_number':871,'multiline':False]['text':' weight0','line_number':872,'multiline':False]['text':' weight1','line_number':873,'multiline':False]['text':' weight2','line_number':874,'multiline':False]['text':' weight3','line_number':875,'multiline':False]['text':' hx_','line_number':876,'multiline':False]['text':' cx_','line_number':877,'multiline':False]['text':' reverse','line_number':878,'multiline':False]['text':' batch_sizes','line_number':879,'multiline':False]['text':' mode','line_number':880,'multiline':False]['text':' hidden_size','line_number':881,'multiline':False]['text':' num_layers','line_number':882,'multiline':False]['text':' has_biases','line_number':883,'multiline':False]['text':' bidirectional','line_number':884,'multiline':False]['text':' batch_first','line_number':885,'multiline':False]['text':' train','line_number':886,'multiline':False]['text':' output_padding','line_number':905,'multiline':False]['text':' For dynamic shape case, we need to pack weight in runtime.','line_number':917,'multiline':False]['text':' For bfloat16 dynamic shape path, using input size hint to pack weight for a better performance.','line_number':1007,'multiline':False]['text':' TODO: aarch64: enable op fusion for acl once it supports fused operators. Disabling it for now.','line_number':1076,'multiline':False]['text':' Otherwise even the matmul or innerproduct can not be accelerated with acl','line_number':1077,'multiline':False]