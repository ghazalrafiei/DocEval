['text':' Could expand this to check deeper properties','line_number':116,'multiline':False]['text':' (e.g. TensorBox points to View or StorageBox)','line_number':117,'multiline':False]['text':' Be picky about the accepted data structure (don't use pytree here)','line_number':137,'multiline':False]['text':' [None] makes sure the cpp wrapper codegen will generate something like','line_number':227,'multiline':False]['text':' {c10::nullopt} instead of {}','line_number':228,'multiline':False]['text':' this can get *very* long','line_number':272,'multiline':False]['text':' The abstract method declarations below serve to convince mypy that all IRNode instances have these functions','line_number':321,'multiline':False]['text':' defined, while having no effect at runtime. We cannot create stub implementations here because other parts of','line_number':322,'multiline':False]['text':' the code dynamically check for defined attributes.','line_number':323,'multiline':False]['text':' Make zero-element loops into a no-op','line_number':444,'multiline':False]['text':' self.dtype represents the dst dtype','line_number':571,'multiline':False]['text':' type: ignore[call-arg]','line_number':576,'multiline':False]['text':' We don't support unbacked symints','line_number':651,'multiline':False]['text':' do heuristics that's close to eager mode for split inner reduction','line_number':669,'multiline':False]['text':' we leak reduction autotune configs here, and will need to refactor to avoid this later','line_number':670,'multiline':False]['text':' don't split if there are enough outputs','line_number':673,'multiline':False]['text':' prefer even splits, but never smalle than min_elements_per_thread','line_number':688,'multiline':False]['text':' prefer even splits','line_number':696,'multiline':False]['text':' TODO the best heuristic currently has XBLOCK (corresponding to numel_hint) 128','line_number':705,'multiline':False]['text':' extend to even smaller number of outputs','line_number':706,'multiline':False]['text':' comes from heuristics, refactor to not leak here','line_number':709,'multiline':False]['text':' prefer even splits','line_number':730,'multiline':False]['text':' easy cases','line_number':739,'multiline':False]['text':' No need to split.','line_number':743,'multiline':False]['text':' Only handles the case where keep_dim = False.','line_number':750,'multiline':False]['text':' Otherwise, we need to propagate reduction dim info to the stage where','line_number':751,'multiline':False]['text':' the intermediate loader of the first Reduction is generated.','line_number':752,'multiline':False]['text':' If the input_node or its dependent nodes are also Reduction nodes,','line_number':771,'multiline':False]['text':' use reduction_sizes of this node or its dependent nodes directly.','line_number':772,'multiline':False]['text':' try finding the full size producer','line_number':803,'multiline':False]['text':' TODO this will fail for something like ((1, N) * (N, 1)).sum()','line_number':804,'multiline':False]['text':' this would also possibly be wrong for producers with the different contiguity but we hope those cases are rare','line_number':805,'multiline':False]['text':' TODO determine splits when all inputs are broadcast','line_number':829,'multiline':False]['text':' type: ignore[arg-type]','line_number':876,'multiline':False]['text':' type: ignore[arg-type]','line_number':877,'multiline':False]['text':' type: ignore[override]','line_number':895,'multiline':False]['text':' N.B. This is a hack to generate the literal of the given type','line_number':910,'multiline':False]['text':' Ideally, we should be fixing `def constant` in triton.py','line_number':911,'multiline':False]['text':' but it breaks due to hardcoded dtypes in other places','line_number':912,'multiline':False]['text':' "all" is desugared to `!any(!val)`','line_number':927,'multiline':False]['text':' this reduction is actually a pointwise op','line_number':945,'multiline':False]['text':' triton doesn't support reduce to single element well, so break it up','line_number':974,'multiline':False]['text':' intermediate reduction in split can contain complex indexing,','line_number':986,'multiline':False]['text':' and num_splits will fail to correctly set the hint','line_number':987,'multiline':False]['text':' reuse the passed hint if available','line_number':988,'multiline':False]['text':' type: ignore[arg-type]','line_number':994,'multiline':False]['text':' triton doesn't support reduce to single element well, so break it up','line_number':1011,'multiline':False]['text':' triton will automatically compute reductions in fp32 if reducing over fp16/bf16','line_number':1159,'multiline':False]['text':' within the kernel. keep the intermediate in fp32 so as to keep the whole reduction','line_number':1160,'multiline':False]['text':' in fp32 and not reduce precision by breaking up the kernel into multiple layers','line_number':1161,'multiline':False]['text':' TODO(jansel): realize the reduction so we can do dynamic indexing','line_number':1219,'multiline':False]['text':' type: ignore[override]','line_number':1331,'multiline':False]['text':' TODO: Unrolled reduction','line_number':1384,'multiline':False]['text':' if (','line_number':1385,'multiline':False]['text':'     isinstance(reduction_numel, sympy.Integer)','line_number':1386,'multiline':False]['text':'     and V.graph.sizevars.size_hint(reduction_numel)','line_number':1387,'multiline':False]['text':'     < config.unroll_reductions_threshold','line_number':1388,'multiline':False]['text':'     and sympy_product(ranges) != 1','line_number':1389,'multiline':False]['text':' ):','line_number':1390,'multiline':False]['text':'     return Pointwise.create(','line_number':1391,'multiline':False]['text':'         device,','line_number':1392,'multiline':False]['text':'         dst_dtype,','line_number':1393,'multiline':False]['text':'         cls._unroll_reduction_fn(','line_number':1394,'multiline':False]['text':'             inner_fn, reduction_ranges, reduction_type, src_dtype','line_number':1395,'multiline':False]['text':'         ),','line_number':1396,'multiline':False]['text':'         ranges,','line_number':1397,'multiline':False]['text':'     )','line_number':1398,'multiline':False]['text':' triton doesn't support reduce to single element well, so break it up','line_number':1400,'multiline':False]['text':' intermediate reduction in split can contain complex indexing,','line_number':1411,'multiline':False]['text':' and num_splits will fail to correctly set the hint','line_number':1412,'multiline':False]['text':' reuse the passed hint if available','line_number':1413,'multiline':False]['text':' triton doesn't support reduce to single element well, so break it up','line_number':1417,'multiline':False]['text':' type: ignore[override]','line_number':1453,'multiline':False]['text':' If we need mask, then "welford_reduce" doesn't work because','line_number':1474,'multiline':False]['text':' masked inputs shouldn't count towards the welford weight','line_number':1475,'multiline':False]['text':' welford_reduce turns one input into three outputs, which are combined with welford_combine','line_number':1536,'multiline':False]['text':' HACK we mimick reduction','line_number':1551,'multiline':False]['text':' return self.scan_op','line_number':1564,'multiline':False]['text':' TODO: CPU support','line_number':1604,'multiline':False]['text':' TODO: ROCm support','line_number':1608,'multiline':False]['text':' Scan with a single element is just a copy','line_number':1614,'multiline':False]['text':' TODO: Support splitting','line_number':1634,'multiline':False]['text':' TODO: custom splitting heuristic for scan','line_number':1671,'multiline':False]['text':' making the base of x contiguous or stride_ordered will not necessarily make','line_number':1723,'multiline':False]['text':' the ReinterpretView either, so don't pass along those arguments','line_number':1724,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1805,'multiline':False]['text':' zero out broadcast dimension','line_number':1879,'multiline':False]['text':' redirect to a generic view','line_number':1961,'multiline':False]['text':' Skip pointless views','line_number':2030,'multiline':False]['text':' TODO: a new class for FixedTransferLayout that output layout is constrained by input layout','line_number':2047,'multiline':False]['text':' realize x; otherwise, the dynamic_reshape_indexer below will fail','line_number':2050,'multiline':False]['text':' due to the size_hint's inability to process unbacked SymInts','line_number':2051,'multiline':False]['text':' optimistic algorithm failed, lets do a fallback','line_number':2086,'multiline':False]['text':' re-add','line_number':2110,'multiline':False]['text':' re-add','line_number':2112,'multiline':False]['text':' reinterpret_tensor is similar to as_strided except:','line_number':2213,'multiline':False]['text':' - offset is added to the existing offset (rather than replacing it)','line_number':2214,'multiline':False]['text':' - view tracking is disabled similar to unsafe_view','line_number':2215,'multiline':False]['text':' Fast path','line_number':2252,'multiline':False]['text':' redirect to a generic view','line_number':2271,'multiline':False]['text':' ignore dimensions of size 1, they dont affect layout','line_number':2410,'multiline':False]['text':' since we may have removed dimensions, need to re-sort & re-index order','line_number':2424,'multiline':False]['text':' reorder the stride given order','line_number':2427,'multiline':False]['text':' check if it is in ascending order','line_number':2431,'multiline':False]['text':' create channels_last order(NCHW, NCDHW, the C is the first order).','line_number':2438,'multiline':False]['text':' This is janky, I figured out what fields to populate by just running','line_number':2623,'multiline':False]['text':' the model I was interested in and adding properties/methods as needed.','line_number':2624,'multiline':False]['text':' This doesn't inherit from Layout because Layout assumes you have stuff','line_number':2625,'multiline':False]['text':' like sizes, but I don't really have anything here.','line_number':2626,'multiline':False]['text':'','line_number':2627,'multiline':False]['text':' If you have an ir.Node with NoneLayout, you probably need to setup','line_number':2628,'multiline':False]['text':' dependencies manually in scheduler','line_number':2629,'multiline':False]['text':' type: ignore[attr-defined]','line_number':2655,'multiline':False]['text':' NOTE: We must realize users of `dst` before we realize `src`, since','line_number':2682,'multiline':False]['text':' realization order determines scheduling order. Otherwise, src's','line_number':2683,'multiline':False]['text':' mutation would be scheduled before the existing users of dst!','line_number':2684,'multiline':False]['text':' We copy the contents of src into dst. In most cases this should','line_number':2690,'multiline':False]['text':' be fused into a single kernel by the scheduler.','line_number':2691,'multiline':False]['text':' NOTE: We cannot change src's layout to mutate dst directly as this','line_number':2692,'multiline':False]['text':' would alias src to dst, which is not correct as further mutations to','line_number':2693,'multiline':False]['text':' dst would effect users of src. However if there are no more users of','line_number':2694,'multiline':False]['text':' dst, we can alias src to dst.','line_number':2695,'multiline':False]['text':' Name is sometimes None; e.g., ForceInPlace, where there isn't','line_number':2723,'multiline':False]['text':' a meaningful name','line_number':2724,'multiline':False]['text':' Multi-output buffers will define 'outputs: List[Buffer]'. Confusingly,','line_number':2728,'multiline':False]['text':' MultiOutput does NOT define this!','line_number':2729,'multiline':False]['text':' Loading from a zero-element buffer is a no-op','line_number':2789,'multiline':False]['text':' So this is a little unusual.  In principle, you could imagine','line_number':2833,'multiline':False]['text':' defining a MultiOutputLayout buffer so that it DOES define','line_number':2834,'multiline':False]['text':' unbacked symints.  However, we can't easily tell what symints','line_number':2835,'multiline':False]['text':' such a buffer defines, because MultiOutputLayout doesn't actually','line_number':2836,'multiline':False]['text':' define any useful information about what it returns.','line_number':2837,'multiline':False]['text':'','line_number':2838,'multiline':False]['text':' An easier and better approach is to delay the symint allocation','line_number':2839,'multiline':False]['text':' to the MultiOutput IR nodes, which are when we actually extract','line_number':2840,'multiline':False]['text':' out the buffers and know what their sizes are.','line_number':2841,'multiline':False]['text':'','line_number':2842,'multiline':False]['text':' There are two subleties here:','line_number':2843,'multiline':False]['text':'','line_number':2844,'multiline':False]['text':' 1. Suppose you have a kernel that produces out1: (i0,), out2: (i0,)','line_number':2845,'multiline':False]['text':'    Both of these actually count as defs!  The scheduler will just','line_number':2846,'multiline':False]['text':'    arbitrarily pick one of these as the canonical definer and','line_number':2847,'multiline':False]['text':'    ensure it stays live.  It's not a big deal if we pick the','line_number':2848,'multiline':False]['text':'    wrong one because tuple accesses are cheap, and all this means','line_number':2849,'multiline':False]['text':'    is we accidentally keep a MultiOutput node live when it wasn't','line_number':2850,'multiline':False]['text':'    strictly necessary.','line_number':2851,'multiline':False]['text':'','line_number':2852,'multiline':False]['text':' 2. Suppose you have a MultiOutput buffer whose size is (i0,), but','line_number':2853,'multiline':False]['text':'    the MultiOutputLayout buffer it is projecting from isn't actually','line_number':2854,'multiline':False]['text':'    dynamic; it has i0 as one of the arguments.  We cannot tell this','line_number':2855,'multiline':False]['text':'    directly from MultiOutput, we have to look at the input buffer's','line_number':2856,'multiline':False]['text':'    uses to work this out.  No big deal.','line_number':2857,'multiline':False]['text':' This kernel defines all unbacked symbols... that it didn't get in as','line_number':2861,'multiline':False]['text':' arguments!','line_number':2862,'multiline':False]['text':' NB: If it is possible for other ir node types to return unbacked','line_number':2888,'multiline':False]['text':' symints, you need to make sure their codegen calls this method.','line_number':2889,'multiline':False]['text':' Don't forget to update get_unbacked_symbol_defs too.','line_number':2890,'multiline':False]['text':' Returns False by default.','line_number':2924,'multiline':False]['text':' wrap scalar to 0-d tensor for cpp wrapper','line_number':2964,'multiline':False]['text':' Ordinarily, we'd like to just peek at the arguments list,','line_number':3004,'multiline':False]['text':' but ComputedBuffers have no argument list.','line_number':3005,'multiline':False]['text':'','line_number':3006,'multiline':False]['text':' Morally, this logic needs to be synchronized with the','line_number':3007,'multiline':False]['text':' KernelArgs.size calls, which are responsible for making symbols make','line_number':3008,'multiline':False]['text':' there way as kernel arguments (and it is precisely passing in one of','line_number':3009,'multiline':False]['text':' those symbols that establishes a dependency).  However, we haven't','line_number':3010,'multiline':False]['text':' started codegen yet so we can't directly reuse that logic.','line_number':3011,'multiline':False]['text':'','line_number':3012,'multiline':False]['text':' For now, I'm just yoloing with the size of the buffer.  Not sure if','line_number':3013,'multiline':False]['text':' it is enough.','line_number':3014,'multiline':False]['text':'','line_number':3015,'multiline':False]['text':' One thing you might wonder is if this is enough for a ComputedBuffer','line_number':3016,'multiline':False]['text':' denoting a reduction over i0.  Empirically, it is enough, but for an','line_number':3017,'multiline':False]['text':' unusual reason: we only need accurate dependencies for item() call,','line_number':3018,'multiline':False]['text':' but it's impossible to end up with a reduction over i0 from an','line_number':3019,'multiline':False]['text':' item() call without a regular non-reduction buffer first.','line_number':3020,'multiline':False]['text':' Inline constants and index_expressions','line_number':3028,'multiline':False]['text':' can be inlined','line_number':3034,'multiline':False]['text':' only consider reads to buffer of same size','line_number':3065,'multiline':False]['text':' ignore StarDeps because they don't contribute stride information','line_number':3066,'multiline':False]['text':' the reordering_reindex in reads' simplify_reorder_and_tile','line_number':3145,'multiline':False]['text':' type: ignore[has-type]','line_number':3151,'multiline':False]['text':' for NHWC: reindex0([0,1,2,3]) = [0,2,3,1], reindex1([0,1,2,3]) = [0,3,2,1]','line_number':3157,'multiline':False]['text':' sizes, reindex1, prune = _simplify_loops(x_vars, sizes, index_formulas)','line_number':3165,'multiline':False]['text':' x_vars = prune(x_vars)','line_number':3166,'multiline':False]['text':' sizes, reindex2 = self._apply_loop_reordering(x_vars, sizes, memory_addrs)','line_number':3167,'multiline':False]['text':' remember the reordering if not have loop collapse.','line_number':3179,'multiline':False]['text':' retrace the loop body with simplification and reordering applied','line_number':3182,'multiline':False]['text':' consider both layout(strides) and reordering(reordering_reindex)','line_number':3216,'multiline':False]['text':' if len(order) != len(strides), do not reorder','line_number':3221,'multiline':False]['text':' type: ignore[name-defined]','line_number':3315,'multiline':False]['text':' Global memory (in bytes) needed for this template.','line_number':3318,'multiline':False]['text':' If any of the inputs is in CL format, use CL format for the output','line_number':3411,'multiline':False]['text':' use CL stride for the output','line_number':3420,'multiline':False]['text':' unwrap a TensorBox','line_number':3466,'multiline':False]['text':' Attempt to turn this into a ReinterpretView rather than assert.','line_number':3475,'multiline':False]['text':' This has concessions around layout, as as_storage_and_layout','line_number':3476,'multiline':False]['text':' can cause us to go from flexible to fixed layout.','line_number':3477,'multiline':False]['text':' unwrap a TensorBox','line_number':3484,'multiline':False]['text':' ExternKernelAlloc has specific requirements for output layout, should create a copy','line_number':3488,'multiline':False]['text':' introduce a copy','line_number':3493,'multiline':False]['text':' freeze layout otherwise our output stride calculation might','line_number':3576,'multiline':False]['text':' become incorrect','line_number':3577,'multiline':False]['text':' We don't have generic shape formulas, so just burn in the','line_number':3582,'multiline':False]['text':' shapes and run an example input.','line_number':3583,'multiline':False]['text':' TODO(jansel): replace this with dynamic shape formulas','line_number':3584,'multiline':False]['text':' We need to retain the constant values of fake tensors that we originally','line_number':3587,'multiline':False]['text':' propagated the graph with, because for some operators running without a','line_number':3588,'multiline':False]['text':' constant would trigger an error / DataDependentException','line_number':3589,'multiline':False]['text':' TODO: Unconditionally do this, not just when example_output has','line_number':3612,'multiline':False]['text':' unbacked symbols','line_number':3613,'multiline':False]['text':' NOTE: Don't use extract_read_writes here as it fails when','line_number':3630,'multiline':False]['text':' make_loader() inlines the computation','line_number':3631,'multiline':False]['text':' TODO(jansel): impose layout preference on realized buffer','line_number':3688,'multiline':False]['text':' Layout doesn't matter','line_number':3705,'multiline':False]['text':' require x to have the layout as strided_ordered as order','line_number':3708,'multiline':False]['text':' fix flexiblelayout to be FixedLayout with stride_order','line_number':3713,'multiline':False]['text':' TODO - Storage to InputBuffer','line_number':3732,'multiline':False]['text':' FIXME: we should unconditionally fill self.kwargs with missing default values','line_number':3795,'multiline':False]['text':' instead of carrying an extra self.ordered_kwargs_for_cpp_kernel','line_number':3796,'multiline':False]['text':' FIXME We should let ExternKernel have access to the cpp schema where possible.','line_number':3805,'multiline':False]['text':' iter_ranges = _size of output tensor, reduce_range = [] because no reduction','line_number':3836,'multiline':False]['text':' manually generate index formula for conv','line_number':3843,'multiline':False]['text':' reorder index vars according to stride','line_number':3849,'multiline':False]['text':' assign new variables each dimension to deal with numbering mismatches','line_number':3861,'multiline':False]['text':' d0, d1, d2 could become d0, d2 -- which won't match d0, d1','line_number':3862,'multiline':False]['text':' NB: It's not necessary to check regular inputs as we automatically','line_number':3870,'multiline':False]['text':' have dependencies on them','line_number':3871,'multiline':False]['text':' Definition of kernel','line_number':4000,'multiline':False]['text':' in C++ wrapper, we don't pass constexpr args, as they don't','line_number':4007,'multiline':False]['text':' get added as parameters to the PTX code compiled from the','line_number':4008,'multiline':False]['text':' user-defined Triton kernel (only non-constexpr args do)','line_number':4009,'multiline':False]['text':' Call to kernel','line_number':4012,'multiline':False]['text':' UserDefinedTritonKernel does not return anything, but rather','line_number':4025,'multiline':False]['text':' modifies input in place, do not let it get DCEd','line_number':4026,'multiline':False]['text':' type: ignore[arg-type]','line_number':4053,'multiline':False]['text':' If we are autotuning, not all arguments will be passed','line_number':4063,'multiline':False]['text':' type: ignore[arg-type]','line_number':4133,'multiline':False]['text':' type: ignore[arg-type]','line_number':4164,'multiline':False]['text':' Follow aten/src/ATen/native/ReductionType.h:get_operator_enum','line_number':4181,'multiline':False]['text':' type: ignore[arg-type]','line_number':4255,'multiline':False]['text':' type: ignore[arg-type]','line_number':4303,'multiline':False]['text':' TODO: handle bools carefully','line_number':4355,'multiline':False]['text':' type: ignore[arg-type]','line_number':4357,'multiline':False]['text':' Special case for boolean.  For Reasons(TM), we don't represent','line_number':4362,'multiline':False]['text':' boolean variables directly in sympy; instead, we generate an','line_number':4363,'multiline':False]['text':' indicator integer variable which we then convert to a boolean by','line_number':4364,'multiline':False]['text':' testing i0 == 1.  We have to identify the underlying indicator','line_number':4365,'multiline':False]['text':' variable, and then bind i0 to the appropriate integer value','line_number':4366,'multiline':False]['text':' based on the runtime boolean.','line_number':4367,'multiline':False]['text':' We need output buffers for generating kernel arguments in the','line_number':4415,'multiline':False]['text':' abi-compatible mode, where we retrieve outputs by pass each individual','line_number':4416,'multiline':False]['text':' output through the abi-compatible interface.','line_number':4417,'multiline':False]['text':' These checks are here because ops that return aliasing tensors will','line_number':4442,'multiline':False]['text':' return type Tensor& instead of Tensor, but codegen will always write','line_number':4443,'multiline':False]['text':' type Tensor on the LHS.','line_number':4444,'multiline':False]['text':' Generate abi-compatible kernel names for shim kernels.','line_number':4478,'multiline':False]['text':' Each individual shim kernel may have its own versioning rule.','line_number':4479,'multiline':False]['text':' However, we don't expect we would end up with too many of such rules.','line_number':4480,'multiline':False]['text':' For sdpa, we need the v2 version only if any optional','line_number':4486,'multiline':False]['text':' kwarg is missing.','line_number':4487,'multiline':False]['text':' Now we setup abi_compatible_kernel after self.kernel','line_number':4511,'multiline':False]['text':' and kwargs are adjusted appropriately.','line_number':4512,'multiline':False]['text':' Previously, we want to maintain forward-compatibility by skipping','line_number':4525,'multiline':False]['text':' default args in the serialized artifacts in fbcode. However,','line_number':4526,'multiline':False]['text':' some of our shim interfaces require default values being set.','line_number':4527,'multiline':False]['text':' Discussed with Sherlock offline and we decided to allow serializing','line_number':4528,'multiline':False]['text':' default args into the C++ wrapper code for now. We will refine this','line_number':4529,'multiline':False]['text':' part if we see real FC requirement. More details related to FC','line_number':4530,'multiline':False]['text':' can be found at:','line_number':4531,'multiline':False]['text':' https://docs.google.com/document/d/1FzWm-sHYwmRi3x_g036kOxd99KaYquUsA-L5JwOn8ys/edit?usp=sharing','line_number':4532,'multiline':False]['text':' Some positional args are not provided, need to use their default value in cpp wrapper','line_number':4536,'multiline':False]['text':' let self.codegen_kwargs handle kwargs','line_number':4544,'multiline':False]['text':' Remove None','line_number':4556,'multiline':False]['text':' TODO - some fallbacks are still OpOverloadPackets','line_number':4567,'multiline':False]['text':' TODO - some fallbacks are still OpOverloadPackets','line_number':4573,'multiline':False]['text':' TODO - use op._schema.arguments alias_info to figure out','line_number':4577,'multiline':False]['text':' precise list','line_number':4578,'multiline':False]['text':' ProxyExecutor Design Note','line_number':4582,'multiline':False]['text':' We export the ExternFallbackNodes (for custom ops) into a serialized file','line_number':4583,'multiline':False]['text':' and run it with a host side proxy executor to address the ABI problem','line_number':4584,'multiline':False]['text':' This is currently only implemented for fbcode. Eventually, we will also make this work for OSS.','line_number':4585,'multiline':False]['text':' Detailed design doc can be found at','line_number':4586,'multiline':False]['text':' https://docs.google.com/document/d/1wC4DOZFaYym2t1Esz0X5yxlLI3RDnSiyRbUus3bkJ64/edit?usp=sharing','line_number':4587,'multiline':False]['text':' type: ignore[arg-type]','line_number':4595,'multiline':False]['text':' type: ignore[arg-type]','line_number':4596,'multiline':False]['text':' serialize_outputs','line_number':4598,'multiline':False]['text':' For single Tensor','line_number':4601,'multiline':False]['text':' For single TensorList','line_number':4612,'multiline':False]['text':' type: ignore[union-attr]','line_number':4623,'multiline':False]['text':' For tuple returns, e.g "-> (Tensor, Tensor)" or "-> (Tesnor, Tensor[])"','line_number':4628,'multiline':False]['text':' Aten Fallback Ops','line_number':4653,'multiline':False]['text':' Calling with the default kernel name can lead to ambiguous behavior like the following example.','line_number':4666,'multiline':False]['text':' repeat_interleave(const at::Tensor & repeats, c10::optional<int64_t> output_size=c10::nullopt)','line_number':4667,'multiline':False]['text':' repeat_interleave(const at::Tensor & self, int64_t repeats,','line_number':4668,'multiline':False]['text':'       c10::optional<int64_t> dim=c10::nullopt, c10::optional<int64_t> output_size=c10::nullopt)','line_number':4669,'multiline':False]['text':' For non-aten OpOverload, i.e. custom ops','line_number':4701,'multiline':False]['text':' type: ignore[assignment]','line_number':4801,'multiline':False]['text':' Signal to codegen that our output buffer isn't safe to reuse','line_number':4818,'multiline':False]['text':' We need output buffers for generating kernel arguments in the','line_number':4833,'multiline':False]['text':' abi-compatible mode, where we retrieve outputs by pass each individual','line_number':4834,'multiline':False]['text':' output through the abi-compatible interface.','line_number':4835,'multiline':False]['text':' Given an input MultiOutputLayout buffer, indexes out an actual buffer','line_number':4875,'multiline':False]['text':' from that result.  This doesn't actually produce multiple outputs,','line_number':4876,'multiline':False]['text':' that's MultiOutputLayout!','line_number':4877,'multiline':False]['text':' cpp wrapper code needs to use std::get<> to access a tuple','line_number':4884,'multiline':False]['text':' Port from aten/src/ATen/native/ConvUtils.h: _conv_input_size','line_number':4943,'multiline':False]['text':' The size of prepacked_weight is the prepacked weight size of deconv:','line_number':4967,'multiline':False]['text':'   Groups > 1:  [g*o, i/g, ...]','line_number':4968,'multiline':False]['text':'   Groups == 1: [o, i, ...]','line_number':4969,'multiline':False]['text':' Returns original weight size in [i, o, ...]','line_number':4970,'multiline':False]['text':' TODO <Leslie> cleaned up the fake_tensor trace as Linear implementation','line_number':4993,'multiline':False]['text':' When transposed, the size of the prepacked oneDNN weight is different','line_number':5010,'multiline':False]['text':' from the PyTorch weight. We're not able to run aten conv with such','line_number':5011,'multiline':False]['text':' size. We infer the output size from the input params here:','line_number':5012,'multiline':False]['text':' The weight has been transposed during the qlinear weight prepack process.','line_number':5083,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/4979f9c0d72490970e2019bb1d2284f83d93f76b/','line_number':5084,'multiline':False]['text':' aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp#L291','line_number':5085,'multiline':False]['text':' Due to constrain of op.call, other (Tensor&) should be at input[0]','line_number':5275,'multiline':False]['text':' TODO: op.call: input[0] should be at::Tensor&','line_number':5288,'multiline':False]['text':' type: ignore[arg-type]','line_number':5356,'multiline':False]['text':' This op mutates in place which means that the result is not the','line_number':5361,'multiline':False]['text':' target but rather the input that is being mutated','line_number':5362,'multiline':False]['text':' init reorders the inputs, so inputs[1] becomes packed.inputs[0]','line_number':5363,'multiline':False]['text':' If batch_first, x has been permuted in lstm before entering the mkldnn_rnn_layer.','line_number':5673,'multiline':False]['text':' Make sure x is contiguous in batch_first case.','line_number':5674,'multiline':False]['text':' batch_first is handled in the lstm OP. When entering','line_number':5687,'multiline':False]['text':' rnn_layer here, we'll always have batch_first = False','line_number':5688,'multiline':False]['text':' Parser the inputs and constant','line_number':5793,'multiline':False]['text':' packed_weight','line_number':5852,'multiline':False]['text':' swap padding and stride to align with functional conv arg order','line_number':5881,'multiline':False]['text':' in _prepare_convolution_fusion_create, we use x.dtype (uint8) to create kernel_layout','line_number':5903,'multiline':False]['text':' if we set output_dtype is not None, the output buf should be output_dtype instead of uint8.','line_number':5904,'multiline':False]['text':' Parser the inputs and constant','line_number':5969,'multiline':False]['text':' packed_weight','line_number':6040,'multiline':False]['text':' swap padding and stride to align with functional conv arg order','line_number':6080,'multiline':False]['text':' in _prepare_convolution_fusion_create, we use x.dtype (uint8) to create kernel_layout','line_number':6104,'multiline':False]['text':' if output_dtype is not None, the output buf should be dtype output_dtype instead of uint8.','line_number':6105,'multiline':False]['text':' Parser the inputs and constant','line_number':6159,'multiline':False]['text':' packed_weight','line_number':6210,'multiline':False]['text':' in _prepare_linear_fusion_create, we use x.dtype (uint8) to create kernel_layout','line_number':6244,'multiline':False]['text':' if we set fp32_output, the output buf should be dtype float32 instead of uint8.','line_number':6245,'multiline':False]['text':' type: ignore[attr-defined]','line_number':6277,'multiline':False]['text':' a list of heavy ops','line_number':6374,'multiline':False]['text':' Skip the check for non Pointwise instances','line_number':6411,'multiline':False]['text':' call super() with a placeholder to avoid constructing a','line_number':6430,'multiline':False]['text':' GraphModule which is very expensive (it does codegen).','line_number':6431,'multiline':False]['text':' Doing a local import to avoid dumping all the code here','line_number':6481,'multiline':False]['text':' type: ignore[name-defined]','line_number':6568,'multiline':False]['text':' This indirection is just a cute way to get IndexPropagation to','line_number':6678,'multiline':False]['text':' unwrap the return value.','line_number':6679,'multiline':False]['text':' strip `; del var0` suffixes to make output prettier','line_number':6692,'multiline':False]['text':' wait op still needs to produce a 'buffer' that represents the tensor output.','line_number':6725,'multiline':False]['text':' this is a symbolic gesture, and it gets handled by WrapperCodegen.','line_number':6726,'multiline':False]['text':' codegen outputs a '# reuse' line that assigns the input buffer here ('input_collective')','line_number':6727,'multiline':False]['text':' to a new name (`self.get_name()`) and `del`s the old name.','line_number':6728,'multiline':False]['text':' TODO(whc) i'm not sure what's going on here, this probably means I missed something upstream','line_number':6733,'multiline':False]['text':' Signal to codegen that our output buffer isn't safe to reuse','line_number':6741,'multiline':False]['text':' The generated `_wait_tensor` op mutates the input tensor','line_number':6745,'multiline':False]['text':' factor so the boilerplate can be handled in CollectiveKernel.codegen','line_number':6768,'multiline':False]['text':' factor so the boilerplate can be handled in CollectiveKernel.codegen','line_number':6772,'multiline':False]['text':' extract references to our args in string form for codegen output','line_number':6791,'multiline':False]['text':' TODO: avoid more than one ref of the same pg (even though they are cached inside the api)','line_number':6797,'multiline':False]['text':' NOTE: As seen in issue #108780, output buffers of out-of-place collectives','line_number':6842,'multiline':False]['text':' could be incorrectly reused. As a safety measure, here we just ban the reuse of them.','line_number':6843,'multiline':False]['text':' TODO: A better fix is to figure out how to propagate the aliases properly,','line_number':6844,'multiline':False]['text':' so that the buffer is only reused after all its users have consumed it.','line_number':6845,'multiline':False]['text':' new_size[0] *= group_size','line_number':6867,'multiline':False]['text':' type: ignore[arg-type]','line_number':6967,'multiline':False]['text':' type: ignore[arg-type]','line_number':7007,'multiline':False]['text':' type: ignore[arg-type]','line_number':7043,'multiline':False]['text':' return cls.create_output_nodes(packed, outputs)','line_number':7157,'multiline':False]['text':' TODO(yifu): replace the CollectiveKernel IR hierarchy with _CollectiveKernel.','line_number':7213,'multiline':False]['text':' This is identical to FallbackKernel.set_cpp_kernel(), minus the','line_number':7221,'multiline':False]['text':' part that checks against input aliasing and mutation.','line_number':7222,'multiline':False]['text':' NOTE: [In-Place Collective Safety]','line_number':7237,'multiline':False]['text':' Between the initiation and completion of an in-place collective, the','line_number':7238,'multiline':False]['text':' input buffers are subject to both volatile reads and volatile writes.','line_number':7239,'multiline':False]['text':' They must not be read, written to or reused by another kernel. To ensure','line_number':7240,'multiline':False]['text':' the constraints, we model collective -> wait_tensor as as two-step','line_number':7241,'multiline':False]['text':' mutation of the input buffers.','line_number':7242,'multiline':False]['text':' NOTE: [Out-of-Place Collective Safety]','line_number':7266,'multiline':False]['text':' Between the initiation and completion of an out-of-place collective:','line_number':7267,'multiline':False]['text':'','line_number':7268,'multiline':False]['text':' Input buffers:','line_number':7269,'multiline':False]['text':' - Are subject to volatile reads','line_number':7270,'multiline':False]['text':' - Can be read by another kernel','line_number':7271,'multiline':False]['text':' - Must not be written to or reused by another kernel','line_number':7272,'multiline':False]['text':'','line_number':7273,'multiline':False]['text':' Output buffers:','line_number':7274,'multiline':False]['text':' - Are subject to volatile writes','line_number':7275,'multiline':False]['text':' - Must not be read, written to or reused by another kernel','line_number':7276,'multiline':False]['text':'','line_number':7277,'multiline':False]['text':' To ensure the safety of input buffers without sacrificing read','line_number':7278,'multiline':False]['text':' availability, we add input buffers as read deps of wait_tensor kernels.','line_number':7279,'multiline':False]['text':'','line_number':7280,'multiline':False]['text':' To ensure the safety of output buffers, we model wait_tensor as a','line_number':7281,'multiline':False]['text':' mutation to the output buffer. Note we also assumes the user program being','line_number':7282,'multiline':False]['text':' correct and the output buffer is not consumed by kernels other than','line_number':7283,'multiline':False]['text':' wait_tensor.','line_number':7284,'multiline':False]['text':'','line_number':7285,'multiline':False]['text':' TODO(yifu): add a pre-grad pass to validate the correctness of collective','line_number':7286,'multiline':False]['text':' usage in the user program.','line_number':7287,'multiline':False]['text':' Out-of-place single-output','line_number':7336,'multiline':False]['text':' Out-of-place multi-output','line_number':7339,'multiline':False]['text':' In-place requires no additional deps handling for volatile','line_number':7345,'multiline':False]['text':' reads since the inputs are mutated.','line_number':7346,'multiline':False]['text':' See [Out-of-Place Collective Safety].','line_number':7369,'multiline':False]['text':' NB: recursive structure here reflects val_to_arg_str, avoid','line_number':7376,'multiline':False]['text':' calling free_unbacked_symbols on "exotic" types that don't get pexpr','line_number':7377,'multiline':False]['text':' treatment','line_number':7378,'multiline':False]['text':' This branch should be impossible in return position','line_number':7381,'multiline':False]['text':' This branch is impossible in constant-args position','line_number':7389,'multiline':False]['text':' TODO: might be necessary to do some pretty printing on','line_number':7450,'multiline':False]['text':' split sizes','line_number':7451,'multiline':False]