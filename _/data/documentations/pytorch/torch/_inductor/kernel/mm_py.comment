['text':' options to tune from','line_number':124,'multiline':False]['text':' If we are not autotuning, we can swap to a FlexibleLayout','line_number':148,'multiline':False]['text':' in order to get fusion optimizations to kick in, e.g. ConcatFusion','line_number':149,'multiline':False]['text':' TODO: Re-enable eager mode implementation once cuBLAS is fixed','line_number':167,'multiline':False]['text':' unexpand inp to make sure fused addmm from cublasLt is used','line_number':220,'multiline':False]['text':' can't use triton kernel unless one of these is true','line_number':266,'multiline':False]['text':' This op is a special case of the int_mm op which we use based on the pattern','line_number':282,'multiline':False]['text':' _int_mm -> mul (defined in ../fx_passes/post_grad.py) in order to prevent','line_number':283,'multiline':False]['text':' realization of the int32 _int_mm output by forcing fusion with the mul op.','line_number':284,'multiline':False]['text':' This is only used when config.force_fuse_int_mm_with_mul = True','line_number':285,'multiline':False]