['text':' torch.float16, # TODO: implement this','line_number':77,'multiline':False]['text':' TODO: this should not be needed once #93059 lands','line_number':126,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/94031#discussion_r1096044816','line_number':127,'multiline':False]['text':' TODO: make a dedicated UnknownSource for this?','line_number':128,'multiline':False]['text':' NB: This is using the legacy default behavior from','line_number':129,'multiline':False]['text':' create_symbolic_sizes_strides_storage_offset but we hope we can','line_number':130,'multiline':False]['text':' just delete this entirely','line_number':131,'multiline':False]['text':' CUDACombinedScheduling combines Triton and CUDA C++ scheduling for CUDA devices via delegation','line_number':165,'multiline':False]['text':' we do our own error wrapping','line_number':193,'multiline':False]['text':' type: ignore[assignment]','line_number':215,'multiline':False]['text':' See `ProxyExecutor Design Note` in ir.py for more details','line_number':216,'multiline':False]['text':' type: ignore[assignment]','line_number':221,'multiline':False]['text':' type: ignore[assignment]','line_number':233,'multiline':False]['text':' This is the cache key for the compiled artifact','line_number':239,'multiline':False]['text':' This is the path in the filesystem where the compiled artifact is stored','line_number':240,'multiline':False]['text':' This is the linemap used by the profiler to mark custom compiled kernels getting run','line_number':245,'multiline':False]['text':' Used if lowering encounters cases where cudagraphs are not supported','line_number':246,'multiline':False]['text':' NHWC perf issue on ROCm5.7 first noted here https://github.com/pytorch/pytorch/pull/110319','line_number':272,'multiline':False]['text':' For cpu backend and mkldnn enabled, we always using channels_last for a better performance.','line_number':276,'multiline':False]['text':' Followering models are skipped due to this:','line_number':288,'multiline':False]['text':' jx_nest_base','line_number':289,'multiline':False]['text':' volo_d1_224','line_number':290,'multiline':False]['text':' only grouped convolutions benchmarked as slower in conv samples for inference only','line_number':320,'multiline':False]['text':' average benchmarked channels last speedup / slowdown, < 1 is speedup.','line_number':349,'multiline':False]['text':' taken from the set of convolution inputs in benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/','line_number':350,'multiline':False]['text':' To regenerate these numbers follow https://gist.github.com/eellison/55d7a6ed6f39829d68ac56f95f4df5bb','line_number':351,'multiline':False]['text':' TODO - get different values per hardware','line_number':358,'multiline':False]['text':' Channels last layout can dramatically hurt grouped conv perf. E.g.','line_number':374,'multiline':False]['text':' Conv with arguments like','line_number':375,'multiline':False]['text':'   {"input_shape": [32, 224, 112, 112], "weight_shape": [224, 112, 3, 3],','line_number':376,'multiline':False]['text':'    "stride": [2, 2], "padding": [1, 1], "groups": 2}','line_number':377,'multiline':False]['text':' slows down 31x using channels last..','line_number':378,'multiline':False]['text':' But a lot of timm models use depthwise separable convolution which will','line_number':380,'multiline':False]['text':' result in grouped convolution with in-channel size == 1.','line_number':381,'multiline':False]['text':' For those grouped convolution, channels last still helps a lot.','line_number':382,'multiline':False]['text':' E.g.','line_number':383,'multiline':False]['text':' Conv with arguments','line_number':384,'multiline':False]['text':'   {"input_shape": [128, 58, 56, 56], "weight_shape": [58, 1, 3, 3],','line_number':385,'multiline':False]['text':'    "stride": [2, 2], "padding": [1, 1], "groups": 58}','line_number':386,'multiline':False]['text':' get 1.86x speedup with channels last layout.','line_number':387,'multiline':False]['text':'','line_number':388,'multiline':False]['text':' The following heuristics skip using channels-last if the model contains','line_number':389,'multiline':False]['text':' grouped convolution with in-channels > 1.','line_number':390,'multiline':False]['text':' For some models that contain convolution with larger in-channel than out-channel, applying','line_number':397,'multiline':False]['text':' channels last hurts performance.','line_number':398,'multiline':False]['text':' Following models are skipped due to this:','line_number':399,'multiline':False]['text':' - pytorch_unet','line_number':400,'multiline':False]['text':' - phlippe_densenet (slightly worse)','line_number':401,'multiline':False]['text':' - Background_Matting (1.22x -> 0.821x)','line_number':402,'multiline':False]['text':' - pytorch_CycleGAN_and_pix2pix (1.597x -> 1.294x)','line_number':403,'multiline':False]['text':' Following models are skipped due to this:','line_number':410,'multiline':False]['text':' - functorch_maml_omniglot','line_number':411,'multiline':False]['text':' need a second pass to add downstream nodes of those channel last nodes to the sets.','line_number':448,'multiline':False]['text':' This pass is especially needed to avoid mix-layout kernel inputs in backward pass.','line_number':449,'multiline':False]['text':'','line_number':450,'multiline':False]['text':' Let's say a conv-batchnorm 's output is passed to relu whose output is in turn returned','line_number':451,'multiline':False]['text':' from the fwd graph. Without this second pass, we will force relu's output to be contiguous.','line_number':452,'multiline':False]['text':' Then in the kernel in backward pass, the contiguous output of relu may be mix with other channels last','line_number':453,'multiline':False]['text':' tensors and passed to a kernel.','line_number':454,'multiline':False]['text':'','line_number':455,'multiline':False]['text':' This pass improve yolov3 training speedup from 1.116x (worse than disabling layout optimization speedup 1.196x) to 1.457x.','line_number':456,'multiline':False]['text':' It also improves dla102 training speedup from 1.240x (worse than disabling layout optimization speedup 1.523x) to 1.835x .','line_number':457,'multiline':False]['text':' This also helps the following models:','line_number':458,'multiline':False]['text':' - res2net101_26w_4s','line_number':459,'multiline':False]['text':' - res2net50_14w_8s','line_number':460,'multiline':False]['text':' - sebotnet33ts_256','line_number':461,'multiline':False]['text':' Skip empty CPU tensor so that CUDA graphs can succeed, see https://github.com/pytorch/pytorch/pull/114144','line_number':524,'multiline':False]['text':' We may generate a var name for each constant in the codegen.','line_number':586,'multiline':False]['text':' Let's only keep sane characters.','line_number':587,'multiline':False]['text':' todo(chilli): We can remove the last check once we turn buffers into','line_number':635,'multiline':False]['text':' static shape tensors. That's a hack to workaround Inductor believing','line_number':636,'multiline':False]['text':' the buffer should be static but us passing in a fake tensor with','line_number':637,'multiline':False]['text':' symbolic shapes.','line_number':638,'multiline':False]['text':' the first N inputs are weights','line_number':640,'multiline':False]['text':' TODO(jansel): handle input aliasing','line_number':644,'multiline':False]['text':' passthrough lowerings from .pattern_matcher','line_number':661,'multiline':False]['text':' There isn't a good way to dynamically patch this in','line_number':683,'multiline':False]['text':' since AOT Autograd already ran.  The error message tells','line_number':684,'multiline':False]['text':' the user how to fix it.','line_number':685,'multiline':False]['text':' this is a constant','line_number':707,'multiline':False]['text':' tensor lowering has constant inlining logic','line_number':717,'multiline':False]['text':' one of our inputs was mutated, need to turn that into a copy','line_number':763,'multiline':False]['text':' replace output with mutated input','line_number':765,'multiline':False]['text':' TODO: this is sus, it probably should be handled in the','line_number':817,'multiline':False]['text':' lowerings themselves similarly to sym_size/sym-stride','line_number':818,'multiline':False]['text':' require the same stride order for dense outputs,','line_number':828,'multiline':False]['text':' 1. user-land view() will not throw because inductor','line_number':829,'multiline':False]['text':' output different strides than eager','line_number':830,'multiline':False]['text':' long term the solution is to make view() always succeed','line_number':831,'multiline':False]['text':' with infallible strides.','line_number':832,'multiline':False]['text':' 2: as_strided ops, we need make sure its input has same size/stride with','line_number':833,'multiline':False]['text':' eager model to align with eager behavior.','line_number':834,'multiline':False]['text':' Realize so that outputs are correctly aliased','line_number':849,'multiline':False]['text':' requiring a stride order for a non-dense output wouldn't','line_number':857,'multiline':False]['text':' recreate the same strides, and would fail with view, defer for now.','line_number':858,'multiline':False]['text':' Realize if (1) any user need inputs realized, or (2) there is','line_number':870,'multiline':False]['text':' already too many reads and rematerializing can be bad.','line_number':871,'multiline':False]['text':' This inclusion is somewhat controversial (from','line_number':877,'multiline':False]['text':' discussion between Horace, Natalia, and Elias).','line_number':878,'multiline':False]['text':' Currently, it's not very clear why this is helpful.','line_number':879,'multiline':False]['text':' The general idea here is that even though a node may','line_number':880,'multiline':False]['text':' have FlexibleLayout, we still often *treat* it as if','line_number':881,'multiline':False]['text':' it was contiguous. This appears to sometimes result in','line_number':882,'multiline':False]['text':' suboptimal behavior.','line_number':883,'multiline':False]['text':'','line_number':884,'multiline':False]['text':' When we do a better job selecting layout, we should','line_number':885,'multiline':False]['text':' revisit this.','line_number':886,'multiline':False]['text':' TODO(jansel): introduce a store vs inline choice','line_number':917,'multiline':False]['text':' Realize if the IRNode already has accumulated lots of reads','line_number':920,'multiline':False]['text':' Prevent excessive accumulation in a computed buffer, when','line_number':922,'multiline':False]['text':' there are multiple branches each with small number of memory','line_number':923,'multiline':False]['text':' reads, but they converge to a user.','line_number':924,'multiline':False]['text':' Realize if a Pointwise has too much stuff to be inlined.','line_number':927,'multiline':False]['text':' As this may cause RecursionError during Inductor's evaluation.','line_number':928,'multiline':False]['text':' Use inner fn as a rough proxy. Good enough.','line_number':932,'multiline':False]['text':' This is not complete, but it doesn't have to be: origin_node','line_number':936,'multiline':False]['text':' tracking is best effort.  The logic here critically relies on direct','line_number':937,'multiline':False]['text':' TensorBox -> StorageBox denoting a non-view; we don't bother trying','line_number':938,'multiline':False]['text':' to get views to work.  Feel free to add any extra cases as needed.','line_number':939,'multiline':False]['text':'','line_number':940,'multiline':False]['text':' Note: we can't YOLO tree_map over this result, because if there are','line_number':941,'multiline':False]['text':' buffers or a view involved, we might not be able to validly assign','line_number':942,'multiline':False]['text':' the origin_node here.','line_number':943,'multiline':False]['text':' Not really multi-output, can straightforwardly recurse in','line_number':953,'multiline':False]['text':' TODO(Eikan): Only support mixing cpu and other device now.','line_number':995,'multiline':False]['text':' first pass','line_number':1013,'multiline':False]['text':' Need concrete value to run dynamic shapes and tune the result','line_number':1019,'multiline':False]['text':' second pass','line_number':1035,'multiline':False]['text':' TODO: reuse self.scheduler from the first pass to speed up the second pass','line_number':1036,'multiline':False]['text':' cpu','line_number':1042,'multiline':False]['text':' Logged twice as per https://github.com/pytorch/pytorch/pull/99038#discussion_r1167826029','line_number':1086,'multiline':False]['text':' TODO. Revisit this once the logging API is more mature','line_number':1087,'multiline':False]['text':' Directly return the file path with the compiled code','line_number':1120,'multiline':False]['text':' dynamo wraps unspec variable as 0d CPU tensor,','line_number':1136,'multiline':False]['text':' need to convert to scalar during codegen (triton only)','line_number':1137,'multiline':False]