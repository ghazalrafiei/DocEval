['text':' float16 math is done in float32 inside the kernel','line_number':137,'multiline':False]['text':' We'll use this to track which masks the variable needs when used for indirect indexing','line_number':166,'multiline':False]['text':' When making a variable that is going to be used in indirect indexing','line_number':170,'multiline':False]['text':' if a where clause is used it should mean that the result is always a','line_number':171,'multiline':False]['text':' valid index, so you shouldn't include any of the dependent variables','line_number':172,'multiline':False]['text':' in the resulting load mask','line_number':173,'multiline':False]['text':' most of the time index vars don't need masks associated with them','line_number':180,'multiline':False]['text':' however, when index vars are used to compute indices for indirect reads','line_number':181,'multiline':False]['text':' those reads should subsequently be masked,','line_number':182,'multiline':False]['text':' No data type conversion is needed. No requirements on min_elem_per_thread.','line_number':195,'multiline':False]['text':' fp8 data type conversions has min_elem_per_thread requirements.','line_number':198,'multiline':False]['text':' Refer to Triton implementations here:','line_number':199,'multiline':False]['text':' https://github.com/openai/triton/blob/10f59d8ce04052521c1bc0cb3a3f8b98918fc7e3/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp#L10.','line_number':200,'multiline':False]['text':' Triton doesn't support type conversions between fp8_e4m3 and fp8_e5m2.','line_number':205,'multiline':False]['text':' No requirements on min_elem_per_thread.','line_number':215,'multiline':False]['text':' Both dtype and src_dtype are set. This is used by torch to(dtype=dtype).','line_number':219,'multiline':False]['text':' It takes the maximum min_elem_per_thread if there are multiple fp8 conversions','line_number':220,'multiline':False]['text':' in the same kernel.','line_number':221,'multiline':False]['text':' to work around llvm uint conversion semantics','line_number':230,'multiline':False]['text':' that produces 0's for negative values','line_number':231,'multiline':False]['text':' We may promote float16 or bfloat16 to float32 and cause the','line_number':238,'multiline':False]['text':' bitwidth of dtype to be different from the input tensor (i.e. float32).','line_number':239,'multiline':False]['text':' In such as case, we will have to convert the input tensor to','line_number':240,'multiline':False]['text':' its src_type, perform bitcast, and then convert the bit-casted','line_number':241,'multiline':False]['text':' tensor back to float to ensure we use values with the right precision.','line_number':242,'multiline':False]['text':' Float constants are always f32 in triton','line_number':258,'multiline':False]['text':' NOTE: We use a tensor here in order to get the expected type.','line_number':261,'multiline':False]['text':' Otherwise, e.g. float64 constants would be trunctated to float32.','line_number':262,'multiline':False]['text':' NB: this only triggers runtime error as long as input','line_number':307,'multiline':False]['text':' is not all zero','line_number':308,'multiline':False]['text':' XX: This is wrong for the value -0.0 in floating point','line_number':508,'multiline':False]['text':' See the comment in lowering.div_mode. a and b are integer type.','line_number':545,'multiline':False]['text':' Similar to div_floor_kernel_cuda in pytorch core.','line_number':546,'multiline':False]['text':' Notice that // in triton behaves as truncdiv instead of floordiv','line_number':547,'multiline':False]['text':' See the comment in lowering.div_mode. a and b are integer type.','line_number':568,'multiline':False]['text':' Notice that // in triton behaves as truncdiv instead of floordiv','line_number':569,'multiline':False]['text':' NOTE: Cannot use shape=[] as it's not supported by triton-rocm','line_number':587,'multiline':False]['text':' We could use shape=[1] instead but starting with the correct','line_number':588,'multiline':False]['text':' ndim avoids extra `tt.expand_dim` ops appearing in the triton IR.','line_number':589,'multiline':False]['text':' This is called from CSEProxy.__getattr__,  so we'll set the bounds there','line_number':597,'multiline':False]['text':' Take dtype from result to prevent accidental promotion','line_number':610,'multiline':False]['text':' Store all the nodes in one flat list','line_number':688,'multiline':False]['text':' This is for re-ordering program ID in triton mm template','line_number':690,'multiline':False]['text':' pid_cache["tl.program_id(0)"] = pid_m','line_number':691,'multiline':False]['text':' fill in unused index var','line_number':749,'multiline':False]['text':' fill in unused index var','line_number':754,'multiline':False]['text':' no need to "roffset = "','line_number':783,'multiline':False]['text':' type: ignore[assignment]','line_number':825,'multiline':False]['text':' type: ignore[method-assign]','line_number':826,'multiline':False]['text':' lift non-reduction stores outside loop','line_number':836,'multiline':False]['text':' for dynamic shapes, find parts of indexing expressions that have to be precomputed','line_number':844,'multiline':False]['text':' Template code to function name','line_number':869,'multiline':False]['text':' Don't duplicate existing helpers','line_number':890,'multiline':False]['text':' type: ignore[assignment]','line_number':906,'multiline':False]['text':' type: ignore[assignment]','line_number':931,'multiline':False]['text':' A set of autotuning hints to pass as part of triton_meta','line_number':949,'multiline':False]['text':' define this in a closure to make cache local to object','line_number':952,'multiline':False]['text':' Not static','line_number':984,'multiline':False]['text':' will need to recompile if we cross a larger power of 2 boundary','line_number':989,'multiline':False]['text':' reduction indexing goes inside a loop','line_number':1014,'multiline':False]['text':' workaround for this issue:','line_number':1018,'multiline':False]['text':' https://gist.github.com/jansel/6527126f781559095c5531f98a4235a7','line_number':1019,'multiline':False]['text':' calling codegen_body() will flush all the pending buffers','line_number':1030,'multiline':False]['text':' and write out a reduction loop','line_number':1031,'multiline':False]['text':' flush out any code before opening the next loop','line_number':1037,'multiline':False]['text':' guard on the last item out','line_number':1064,'multiline':False]['text':' scroll to next group with remaining elements','line_number':1088,'multiline':False]['text':' need to break size in two','line_number':1092,'multiline':False]['text':' tmpX  means indirect indexing','line_number':1158,'multiline':False]['text':' Note. This may not be correct when there is indirect indexing','line_number':1162,'multiline':False]['text':' Non-iterated variables, e.g. strides','line_number':1169,'multiline':False]['text':' If the index variables only iterate over a subset of the kernel','line_number':1175,'multiline':False]['text':' numels, then it must be broadcasted.','line_number':1176,'multiline':False]['text':' if simple replacements didn't get rid of floor/ceil, try full subs','line_number':1225,'multiline':False]['text':' last resort, if no range vars are in the expr, hoist it','line_number':1228,'multiline':False]['text':' TODO instead of trying to blindly find complicated exprs, we should hoist the','line_number':1229,'multiline':False]['text':' inputs/outputs sizes and strides, but at the time indexing is generated','line_number':1230,'multiline':False]['text':' kernel inputs and outputs are not set yet, we'd need a deeper refactor','line_number':1231,'multiline':False]['text':' to do it this way','line_number':1232,'multiline':False]['text':' for nested exprs, atoms yields top level first (?)','line_number':1236,'multiline':False]['text':' so if everything goes fine, lower level replacements will come up empty','line_number':1237,'multiline':False]['text':' indirect indexing','line_number':1255,'multiline':False]['text':' var is one of xN, yN or rN','line_number':1261,'multiline':False]['text':' Masks are superfluous if we only have one element','line_number':1312,'multiline':False]['text':' Masks are superfluous if numel is a multiple of BLOCK','line_number':1316,'multiline':False]['text':' (We use the fact that BLOCK is required by triton to be a power of 2)','line_number':1317,'multiline':False]['text':' Optional optimization: if block divides numel exactly, we will','line_number':1321,'multiline':False]['text':' never need to do a masked load to handle stragglers at the end.','line_number':1322,'multiline':False]['text':' It's faster to avoid masking at all.  But it is sound to always','line_number':1323,'multiline':False]['text':' mask.','line_number':1324,'multiline':False]['text':' if indexing expression is complicated, we precompute it on the host side','line_number':1339,'multiline':False]['text':' and send the result as a kernel argument','line_number':1340,'multiline':False]['text':' TODO(jansel): do we need a reshape here?','line_number':1360,'multiline':False]['text':' Keep the variable in cache if were going to reuse it. Equiv., if any of the following hold','line_number':1416,'multiline':False]['text':'  1) We are doing broadcasting','line_number':1417,'multiline':False]['text':'  2) It is a non-coalesced load. The intuition is that if it's','line_number':1418,'multiline':False]['text':'  non-coalesced, we will likely load each element multiple times in','line_number':1419,'multiline':False]['text':'  practice.','line_number':1420,'multiline':False]['text':'  3) It will be used later and it won't be CSE'd. Equiv., if all the following hold','line_number':1421,'multiline':False]['text':'   3.1) We are in a reduction loop','line_number':1422,'multiline':False]['text':'   3.2) Its not its last use','line_number':1423,'multiline':False]['text':'   3.3) This load will not be lifted to the body','line_number':1424,'multiline':False]['text':'','line_number':1425,'multiline':False]['text':' "other" below is a workaround for https://github.com/openai/triton/issues/737','line_number':1446,'multiline':False]['text':' for bool, even though it's likely subject to the same bug, setting `other` leads','line_number':1447,'multiline':False]['text':' to LLVM errors so we are skipping it for now','line_number':1448,'multiline':False]['text':' Workaround for https://github.com/openai/triton/issues/2151','line_number':1468,'multiline':False]['text':' tl.load returns int8 when loading from pointer to int1','line_number':1469,'multiline':False]['text':' NOTE: Currently causes hangs on bool UTs for ROCm','line_number':1470,'multiline':False]['text':' Masked loads must come after the mask is computed','line_number':1474,'multiline':False]['text':' can lift a common load outside of reduction loop','line_number':1482,'multiline':False]['text':' One exception is when this is an indirect_load.','line_number':1483,'multiline':False]['text':' Guard against write-after-read corruption in triton.','line_number':1507,'multiline':False]['text':' See # https://github.com/openai/triton/issues/1615','line_number':1508,'multiline':False]['text':' This triton bug means that a load which is broadcasted over multiple','line_number':1509,'multiline':False]['text':' warps may see the result of a store that happens later in the triton','line_number':1510,'multiline':False]['text':' program. The workaround is to add a barrier before storing, which','line_number':1511,'multiline':False]['text':' enforces that all warps have already read the data.','line_number':1512,'multiline':False]['text':' Triton performance for bucketize_binary_search is much better when the number','line_number':1540,'multiline':False]['text':' of threads equals the number of elements.','line_number':1541,'multiline':False]['text':' If we're trying to use a bucketize kernel, we should make sure that an','line_number':1542,'multiline':False]['text':' autotuning config with num_elements_per_warp=32 exists.','line_number':1543,'multiline':False]['text':' noqa: B950 line too long','line_number':1561,'multiline':False]['text':' Say we have','line_number':1590,'multiline':False]['text':'     tmp0 = ops.constant(1, torch.int64)','line_number':1591,'multiline':False]['text':'     tmp1 = ops.reduction(torch.int64, torch.int64, "sum", tmp0)','line_number':1592,'multiline':False]['text':' tmp0 in the triton code is either a scalar, or single-element tensor','line_number':1593,'multiline':False]['text':' so if we emit tl.sum directly, it will only give 1 instead of RBLOCK * 1','line_number':1594,'multiline':False]['text':' To avoid this, we broadcast to the expected shape first.','line_number':1595,'multiline':False]['text':' For persistent reductions, don't bother with','line_number':1660,'multiline':False]['text':' welford's algorithm since it uses more registers, and','line_number':1661,'multiline':False]['text':' taking two reductions doesn't increase memory usage.','line_number':1662,'multiline':False]['text':' This is only really used for aten.any. It changes the','line_number':1778,'multiline':False]['text':' final reduction of a non-persistent reduction from','line_number':1779,'multiline':False]['text':'     tmp5 = triton_helpers.max(_tmp5, 1)[:, None]','line_number':1780,'multiline':False]['text':' to','line_number':1781,'multiline':False]['text':'     tmp5 = triton_helpers.max(_tmp5.to(tl.int8), 1)[:, None].to(tl.int1)','line_number':1782,'multiline':False]['text':' which is needed because tl.reduce doesn't support tl.int1','line_number':1783,'multiline':False]['text':' Lift IR function into a triton function in the global namespace','line_number':1816,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1903,'multiline':False]['text':' last range tree is always reduction','line_number':1928,'multiline':False]['text':' invalidate any caches that came from inside the reduction loop','line_number':1935,'multiline':False]['text':' noqa: B950 line too long','line_number':1963,'multiline':False]['text':' note that random seed is put in V.graph.constants','line_number':1966,'multiline':False]['text':' noqa: B950 line too long','line_number':1969,'multiline':False]['text':' Force the seed_offset to be 0 so calls to the same kernel','line_number':1974,'multiline':False]['text':' using different seed offset will have the same benchmark harness.','line_number':1975,'multiline':False]['text':' We can dedup kernel definitions in this case.','line_number':1976,'multiline':False]['text':' no-op to ensure context','line_number':1997,'multiline':False]['text':' benchmark all configs','line_number':2017,'multiline':False]['text':' no-op to ensure context','line_number':2024,'multiline':False]['text':' noqa: B950 line too long','line_number':2026,'multiline':False]['text':' This default heuristic hint was picked carefully: it is','line_number':2069,'multiline':False]['text':' large, to ensure that we don't shrink the block size (since','line_number':2070,'multiline':False]['text':' if you don't have many elements, it'd be wasteful to pick a','line_number':2071,'multiline':False]['text':' large block size).  Since we don't know how many elements we','line_number':2072,'multiline':False]['text':' might have, we should be OK with some inefficiency to make','line_number':2073,'multiline':False]['text':' sure we handle the large case well.  8192 is the largest','line_number':2074,'multiline':False]['text':' block size we support, so we pick that.','line_number':2075,'multiline':False]['text':'','line_number':2076,'multiline':False]['text':' If we have a better hint for unbacked SymInts (e.g., because','line_number':2077,'multiline':False]['text':' a user told us, or we are tracking upper bounds) we could','line_number':2078,'multiline':False]['text':' use that here.','line_number':2079,'multiline':False]['text':' maps actual expression to SizeArg if its in sizevars replacements','line_number':2109,'multiline':False]['text':' constexpr version causes issues, see','line_number':2157,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/pull/1362','line_number':2158,'multiline':False]['text':' triton_meta["constants"][len(argdefs)] = V.graph.sizevars.size_hint(','line_number':2159,'multiline':False]['text':'     tree.numel','line_number':2160,'multiline':False]['text':' )','line_number':2161,'multiline':False]['text':' argdefs.append(f"{tree.prefix}numel: tl.constexpr")','line_number':2162,'multiline':False]['text':' input, output and 2 args','line_number':2195,'multiline':False]['text':' no_x_dim is sympy.logic.boolalg.BooleanTrue','line_number':2265,'multiline':False]['text':' dynamo wraps unspec variable as 0d CPU tensor, need convert to scalar','line_number':2298,'multiline':False]['text':' TODO(jansel): if there are constants, we shouldn't bother passing them as args','line_number':2303,'multiline':False]['text':' even if input buffer and output buffer have different layout,','line_number':2348,'multiline':False]['text':' this can be a layout conversion kernel. No need to warn for','line_number':2349,'multiline':False]['text':' the mix layouts.','line_number':2350,'multiline':False]['text':' ignore the tensor if only 1 dimension is non-zero','line_number':2358,'multiline':False]['text':' Only allow fusion for TritonTemplates for now.','line_number':2453,'multiline':False]['text':' Fusion for CUDATemplates are not supported.','line_number':2454,'multiline':False]['text':' check for a bad combined tiling','line_number':2460,'multiline':False]['text':' swap args to hit the case above','line_number':2515,'multiline':False]['text':' Writes with a reduced shape, meaning they are only present once the','line_number':2525,'multiline':False]['text':' reduction loop has ended','line_number':2526,'multiline':False]['text':' A scan is modelled as a reduction in the scheduler but has a','line_number':2546,'multiline':False]['text':' full sized output that can be used inside the loop body','line_number':2547,'multiline':False]['text':' flush out any other runnable nodes to reduce number of loops','line_number':2560,'multiline':False]['text':' need to start a new reduction loop','line_number':2596,'multiline':False]['text':' Allow for unhinted e as long as we can still statically prove','line_number':2641,'multiline':False]['text':' (e.g., via ValueRanges) that it is still in bounds','line_number':2642,'multiline':False]['text':' Otherwise, the hint MUST exist and be in range','line_number':2645,'multiline':False]['text':' Any use of a MultiOutputLayout will create a buffer with a','line_number':2651,'multiline':False]['text':' Layout whose sizes are accounted for','line_number':2652,'multiline':False]['text':' Only install guards for 32-bit indexing as there is no correctness','line_number':2662,'multiline':False]['text':' issue with using 64-bit for everything','line_number':2663,'multiline':False]['text':' Gather all used buffer names','line_number':2671,'multiline':False]['text':' Get buffers objects','line_number':2680,'multiline':False]['text':' In theory we can separately check xnumel and rnumel are <= int_max','line_number':2698,'multiline':False]['text':' but some indexers do use the full linear index so we need to be','line_number':2699,'multiline':False]['text':' conservative here.','line_number':2700,'multiline':False]['text':' We probably should look what are the nodes inside a foreach','line_number':2748,'multiline':False]['text':' schedule node','line_number':2749,'multiline':False]['text':' Not every node in the schedule will actually be live on output;','line_number':2796,'multiline':False]['text':' we can't check dead buffers.','line_number':2797,'multiline':False]['text':' TODO - use split ranges ?','line_number':2832,'multiline':False]['text':' use the original src_code as the key','line_number':2851,'multiline':False]['text':' DESCRIPTIVE_NAME is used for profiling purposes; it shows the full kernel name','line_number':2855,'multiline':False]['text':' even when unique_kernel_names is turned off. Meanwhile, KERNEL_NAME is sometimes set','line_number':2856,'multiline':False]['text':' to "triton_" to maximize caching opportunities (when unique_kernel_names = False).','line_number':2857,'multiline':False]['text':' TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does','line_number':2861,'multiline':False]['text':' not use BracesBuffer, so we have no good indicator of a C++ buffer atm.','line_number':2862,'multiline':False]['text':' finalize must be called after adding epilogue above','line_number':2894,'multiline':False]['text':' TODO: Maybe unify CUDATemplateKernel to also use PartialRender for flexible epilogue fusion.','line_number':2896,'multiline':False]['text':' isinstance(dep, MemoryDep): this filters out StarDeps. StarDeps refer to reads','line_number':2968,'multiline':False]['text':' that need to access the entire tensor; they don't contribute read indexing','line_number':2969,'multiline':False]['text':' information (and practically, they don't have dep.index so they can't be used','line_number':2970,'multiline':False]['text':' for stride_hints below','line_number':2971,'multiline':False]['text':' if this is a broadcasted tensor and all dimensions after split are broadcast,','line_number':2994,'multiline':False]['text':' this is not a real split','line_number':2995,'multiline':False]['text':' score by number of elements','line_number':3004,'multiline':False]['text':' ngimel said contiguous writes is more important than reads','line_number':3011,'multiline':False]['text':' TODO(jansel): should we tile reductions?','line_number':3038,'multiline':False]['text':' do perf hint here if stride-1 dim is not being reduced','line_number':3039,'multiline':False]['text':' Consider adding a third dimension of tiling, but only','line_number':3059,'multiline':False]['text':' when a1 is a multiple of b1; otherwise, you have a lot','line_number':3060,'multiline':False]['text':' of stragglers which is annoying to generate code for.','line_number':3061,'multiline':False]['text':'','line_number':3062,'multiline':False]['text':' NB: More than three max tiles is not enabled by default.','line_number':3063,'multiline':False]['text':' Add one 3D tiling choice','line_number':3065,'multiline':False]['text':' swap so a0 is bigger','line_number':3072,'multiline':False]['text':' only 1 choice for now','line_number':3079,'multiline':False]['text':' empty last_usage. May cause more aggressive 'evict_last'. Should be fine.','line_number':3116,'multiline':False]['text':' call once to trigger the compilation','line_number':3156,'multiline':False]['text':' skip benchmarking the kernel if there are register spills','line_number':3162,'multiline':False]['text':' We have to clone the inplace updated arguments to avoid earlier calls','line_number':3165,'multiline':False]['text':' generating out of range indices for later calls.','line_number':3166,'multiline':False]['text':' higher is better','line_number':3181,'multiline':False]['text':' Don't tile stuff outside the main reduction loop','line_number':3213,'multiline':False]