['text':' The code generated by Inductor consists of two main parts: kernel code and wrapper code.','line_number':58,'multiline':False]['text':' For any new backend looking to integrate with Inductor, customization of these two main','line_number':59,'multiline':False]['text':' parts are necessary to generate its specific code.','line_number':60,'multiline':False]['text':'','line_number':61,'multiline':False]['text':' Kernel code generation is determined by different Scheduling. Consequently, a new','line_number':62,'multiline':False]['text':' backend needs to provide a custom Scheduling for its unique kernel code generation. Currently,','line_number':63,'multiline':False]['text':' CppScheduling and TritonScheduling serve the C++/OpenMP and Triton backends, respectively.','line_number':64,'multiline':False]['text':'','line_number':65,'multiline':False]['text':' For the Wrapper, Inductor provides a WrapperCodeGen class to generate the Python wrapper code','line_number':66,'multiline':False]['text':' that bridges kernels. This allows out-of-tree backends to inherit from WrapperCodeGen,','line_number':67,'multiline':False]['text':' and override specific member functions to create backend-specific Python wrapper code.','line_number':68,'multiline':False]['text':'','line_number':69,'multiline':False]['text':' Other classes, such as CppKernel and TritonKernel, used for code generation, typically form part','line_number':70,'multiline':False]['text':' of the logic for either Scheduling or WrapperCodeGen. So the Scheduling and WrapperCodeGen interfaces','line_number':71,'multiline':False]['text':' provide flexibility to the backend. A backend can choose to implement these classes from scratch,','line_number':72,'multiline':False]['text':' or reuse them by extending and overriding as necessary. And Inductor provides the registration API,','line_number':73,'multiline':False]['text':' register_backend_for_device, to equip a new backend at runtime.','line_number':74,'multiline':False]['text':'','line_number':75,'multiline':False]['text':' Intel has developed a new backend on top of Triton to support Intel GPUs, leveraging these interfaces.','line_number':76,'multiline':False]['text':' This backend can be used as a reference:','line_number':77,'multiline':False]['text':' https://github.com/intel/intel-extension-for-pytorch/blob/5dcc9d57e5422cf295e1a1ee97896d6b6a554a85/intel_extension_for_pytorch/_inductor/__init__.py#L9','line_number':78,'multiline':False]['text':' added contiguous index prevents reordering','line_number':98,'multiline':False]['text':' we can infer output node if it only have 1 arg','line_number':182,'multiline':False]['text':' For masked_subblock, we use output's dtype to represent','line_number':231,'multiline':False]['text':' the dtype of this subgraph. For other cases, graph_dtype','line_number':232,'multiline':False]['text':' might be None','line_number':233,'multiline':False]['text':' don't put extra parens for strings that are already wrapped in parens','line_number':287,'multiline':False]['text':' GreaterThan:          >=','line_number':317,'multiline':False]['text':' StrictlyGreaterThan:  >','line_number':318,'multiline':False]['text':' Go figure...','line_number':319,'multiline':False]['text':' Pow() confuses triton','line_number':347,'multiline':False]['text':' NB: Remember this is sizevar computation!  You don't typically','line_number':349,'multiline':False]['text':' expect to have to do floating point computation including exponents','line_number':350,'multiline':False]['text':' in sizevar compute.  Instead of adding support for floating','line_number':351,'multiline':False]['text':' point pow, you should make upstream retranslate the Sympy expression','line_number':352,'multiline':False]['text':' into Tensor expressions earlier and do that instead.','line_number':353,'multiline':False]['text':' exp == 0','line_number':365,'multiline':False]['text':' used to trigger cse','line_number':399,'multiline':False]['text':' TODO(fdrocha): this is currently not being used anywhere,','line_number':438,'multiline':False]['text':' pending on moving triton pin past 972b761','line_number':439,'multiline':False]['text':' Includes inplace buffers, excludes removed buffers.  Essentially,','line_number':692,'multiline':False]['text':' after you do a call into this kernel, which buffers actually contain','line_number':693,'multiline':False]['text':' updated data?  Modeled off of python_argdefs.','line_number':694,'multiline':False]['text':' In the abi_compatible model, we just return the buf here.','line_number':738,'multiline':False]['text':' We will form correct call args later in wrapper.generate_kernel_all.','line_number':739,'multiline':False]['text':' Note(fdrocha): reduction_cache is not being cloned, not sure if this is intentional','line_number':779,'multiline':False]['text':' If the expressions were always created with all the information, we could','line_number':804,'multiline':False]['text':' assert expr.bounds == bounds, but sometimes the expression is created','line_number':805,'multiline':False]['text':' with the loose ValueRanges.unknown(), so we need to tighten the bounds','line_number':806,'multiline':False]['text':' We assert if we've not been able to prove the bound','line_number':847,'multiline':False]['text':' FooBar interview question','line_number':851,'multiline':False]['text':' The conditions need to be in parens because of Python's operator precedence.','line_number':855,'multiline':False]['text':' It'd be less error-prone to use and/or/not, which is suported by triton','line_number':856,'multiline':False]['text':' set in set_current_node','line_number':911,'multiline':False]['text':' Upper bounds for indirect_indexing and their str representation','line_number':914,'multiline':False]['text':' key: the buffer to write','line_number':920,'multiline':False]['text':' value: the buffer to read and whose memory can be reused for','line_number':921,'multiline':False]['text':'   the buffer specified by key','line_number':922,'multiline':False]['text':' Set minimum number of elements processed per thread.','line_number':924,'multiline':False]['text':' put the load in the compute section as it might have deps','line_number':964,'multiline':False]['text':' type: ignore[misc]','line_number':1007,'multiline':False]['text':' TritonTemplateKernel has no current_node','line_number':1009,'multiline':False]['text':' type: ignore[has-type]','line_number':1020,'multiline':False]['text':' Skip CSE since this doesn't return an expression','line_number':1030,'multiline':False]['text':' Take the negative part of the bound and add size to it','line_number':1037,'multiline':False]['text':' Then take union of that and the positive part','line_number':1038,'multiline':False]['text':' This is a tighter bound than that of a generic ops.where, as we have info on the cond','line_number':1039,'multiline':False]['text':' We don't have a good way of representing the empty range','line_number':1042,'multiline':False]['text':' Mixed negative and non-negative','line_number':1048,'multiline':False]['text':' An assertion line may have been written already, if so just','line_number':1060,'multiline':False]['text':' update the max size.','line_number':1061,'multiline':False]['text':' A load from an invalidated store requires us to','line_number':1088,'multiline':False]['text':' keep the actual buffer around','line_number':1089,'multiline':False]['text':' only the triton kernel requires mask','line_number':1174,'multiline':False]['text':' adds the necessary kernel args for index expressions','line_number':1178,'multiline':False]['text':' and renames variables in index expressions to kernel arg names','line_number':1179,'multiline':False]['text':' Load value as mask','line_number':1201,'multiline':False]['text':' Load uint8 value as float32','line_number':1208,'multiline':False]['text':' type: ignore[name-defined]','line_number':1252,'multiline':False]