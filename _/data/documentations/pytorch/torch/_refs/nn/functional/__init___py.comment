['text':' type: ignore[attr-defined]','line_number':68,'multiline':False]['text':' From paper: Self-Normalizing Neural Networks (https://arxiv.org/pdf/1706.02515.pdf)','line_number':114,'multiline':False]['text':' alpha = - SELU.alpha * SELU.scale, here','line_number':115,'multiline':False]['text':' SELU.alpha = 1.6732632423543772848170429916717 and','line_number':116,'multiline':False]['text':' SELU.scale = 1.0507009873554804934193349852946','line_number':117,'multiline':False]['text':' nb. We use the name of the first argument used in the unary references','line_number':133,'multiline':False]['text':' celu is implemented specially because it has an alpha argument','line_number':148,'multiline':False]['text':' celu is very similar to elu','line_number':149,'multiline':False]['text':' type: ignore[arg-type]','line_number':173,'multiline':False]['text':' nb. This should be factored out into a can_cast aux function','line_number':228,'multiline':False]['text':' input shape is (N, C, *), so we flatten all inner dimensions except (N, C)','line_number':287,'multiline':False]['text':' Forwarding alias: the functional variant doesn't support the out kwarg','line_number':380,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':381,'multiline':False]['text':' for compat when using TorchRefsMode(strict=True)','line_number':385,'multiline':False]['text':' The error is for compat with regular PyTorch, which has this behavior','line_number':388,'multiline':False]['text':' deprecated.  For PrimTorch, it's fine to drop support for deprecated','line_number':389,'multiline':False]['text':' behavior because it requires explicit opt in.  This error is to inform','line_number':390,'multiline':False]['text':' users how to update their calls.','line_number':391,'multiline':False]['text':' type: ignore[call-overload]','line_number':393,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':396,'multiline':False]['text':' for compat when using TorchRefsMode(strict=True)','line_number':400,'multiline':False]['text':' The error is for compat with regular PyTorch, which has this behavior','line_number':403,'multiline':False]['text':' deprecated.  For PrimTorch, it's fine to drop support for deprecated','line_number':404,'multiline':False]['text':' behavior because it requires explicit opt in.  This error is to inform','line_number':405,'multiline':False]['text':' users how to update their calls.','line_number':406,'multiline':False]['text':' type: ignore[call-overload]','line_number':408,'multiline':False]['text':' softplus is implemented specially because it has beta and threshold arguments','line_number':411,'multiline':False]['text':' type: ignore[arg-type]','line_number':439,'multiline':False]['text':' Formula for reference,','line_number':452,'multiline':False]['text':' hardshrink(x) = x if x > lambd','line_number':453,'multiline':False]['text':'               = x if x < -lambd','line_number':454,'multiline':False]['text':'               = 0 otherwise','line_number':455,'multiline':False]['text':' Formula for reference,','line_number':463,'multiline':False]['text':' softshrink(x) = x - lambd if x > lambd','line_number':464,'multiline':False]['text':'               = x + lambd if x < -lambd','line_number':465,'multiline':False]['text':'               = 0 otherwise','line_number':466,'multiline':False]['text':' We implement this in one torch.where to generate better code in the backward','line_number':471,'multiline':False]['text':' see https://github.com/pytorch/pytorch/pull/107052#discussion_r1293748211','line_number':472,'multiline':False]['text':' Losses','line_number':476,'multiline':False]['text':' reduction == "none"','line_number':495,'multiline':False]['text':' This helper function maps depreciated arguments, "size_average" and "reduce"','line_number':504,'multiline':False]['text':' to their corresponding "reduction" string argument','line_number':505,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':522,'multiline':False]['text':' TODO: Raise exception instead of converting value.  This is only for','line_number':538,'multiline':False]['text':' primTorch since it can drop support for deprecated arguments.','line_number':539,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':540,'multiline':False]['text':' TODO: Raise exception instead of converting value.  This is only for','line_number':563,'multiline':False]['text':' primTorch since it can drop support for deprecated arguments.','line_number':564,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':565,'multiline':False]['text':' Forwarding alias: the functional variant doesn't support the out kwarg','line_number':579,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':580,'multiline':False]['text':' for compat when using TorchRefsMode(strict=True)','line_number':584,'multiline':False]['text':' The error is for compat with regular PyTorch, which has this behavior','line_number':587,'multiline':False]['text':' deprecated.  For PrimTorch, it's fine to drop support for deprecated','line_number':588,'multiline':False]['text':' behavior because it requires explicit opt in.  This error is to inform','line_number':589,'multiline':False]['text':' users how to update their calls.','line_number':590,'multiline':False]['text':' type: ignore[call-overload]','line_number':592,'multiline':False]['text':' loss_without_reduction = max(0, −target * (input1 − input2) + margin)','line_number':603,'multiline':False]['text':' TODO: Raise exception instead of converting value.  This is only for','line_number':626,'multiline':False]['text':' primTorch since it can drop support for deprecated arguments.','line_number':627,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':628,'multiline':False]['text':' loss_without_reduction = input if y == 1','line_number':642,'multiline':False]['text':'                        = max(0, margin - input) if y == -1','line_number':643,'multiline':False]['text':' TODO: Enable data-dependent checks with debug mode','line_number':674,'multiline':False]['text':' TODO: This check does not work with FakeTensor inputs; See Issue #85834','line_number':675,'multiline':False]['text':' Explicit cast for class_check to bool; See Issue #78071','line_number':676,'multiline':False]['text':' implicit batch size = 1','line_number':703,'multiline':False]['text':' input (1 batch size, C classes)','line_number':704,'multiline':False]['text':' input (N batch size, C classes)','line_number':707,'multiline':False]['text':' 3D case (N batch size, C classe, K dimensions)','line_number':711,'multiline':False]['text':' input (N batch size, C classes, K)','line_number':712,'multiline':False]['text':' calculate weighted mean of the loss function','line_number':727,'multiline':False]['text':' TODO: raise exception instead of converting value','line_number':754,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':755,'multiline':False]['text':' Convert these options for consistency with the eager mode','line_number':756,'multiline':False]['text':' The expected behavior when the target and input have zero elements:','line_number':760,'multiline':False]['text':'   reduction = 'none' --- tensor([])','line_number':761,'multiline':False]['text':'   reduction = 'sum'  --- tensor(0.)','line_number':762,'multiline':False]['text':'   reduction = 'mean' --- tensor(nan)','line_number':763,'multiline':False]['text':' Mean reduction on empty tensors produces NaN. See the discussion in','line_number':764,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/64572#issuecomment-926504162','line_number':765,'multiline':False]['text':' The _nll_loss_nd helper function handles the most common cases.','line_number':774,'multiline':False]['text':' ndim == 1 (Single Example)','line_number':775,'multiline':False]['text':'   => Batch Size: 1, Input: (C), Target: ()','line_number':776,'multiline':False]['text':' ndim == 2 (k = 1)','line_number':777,'multiline':False]['text':'   => Batch Size: N, Input: (N, C), Target: (N)','line_number':778,'multiline':False]['text':' ndim == 3 (k > 1)','line_number':779,'multiline':False]['text':'   => Batch Size: N, Input: (N, C, K), Target: (N, K)','line_number':780,'multiline':False]['text':' For ndim > 3, we reshape the input and target to 3-D case.','line_number':784,'multiline':False]['text':' Input (N batch-size, C classes, k-dimensions)','line_number':785,'multiline':False]['text':' Target (N batch-size, k-dimensions)','line_number':786,'multiline':False]['text':' reshape flattened inner-dim to original k-dimensions','line_number':806,'multiline':False]['text':' TODO: This ref supports int reduction and out kwarg to be compatible with ATen:','line_number':810,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/83931','line_number':811,'multiline':False]['text':' TODO: Could be rewritten to support complex:','line_number':812,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/85041','line_number':813,'multiline':False]['text':' type: ignore[arg-type]','line_number':831,'multiline':False]['text':' type: ignore[arg-type]','line_number':838,'multiline':False]['text':' tanhshrink does not use _make_elementwise_unary_reference because it does not support out','line_number':841,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':881,'multiline':False]['text':' No elementwise type promotion - core op doesn't explicitly type promote','line_number':882,'multiline':False]['text':' TODO: Raise exception instead of converting value.  This is only for','line_number':896,'multiline':False]['text':' primTorch since it can drop support for deprecated arguments.','line_number':897,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':898,'multiline':False]['text':' torch.nn.functional.triplet_margin_with_distance_loss has no ref defined','line_number':901,'multiline':False]['text':' since it's a pure Python implementation.  Use this helper instead.','line_number':902,'multiline':False]['text':' Pure Python impl - don't register decomp and don't add a ref.  Defined as a','line_number':914,'multiline':False]['text':' helper here since triplet_margin_loss can be nicely implemented with it.','line_number':915,'multiline':False]['text':' The distance swap is described in the paper "Learning shallow','line_number':947,'multiline':False]['text':' convolutional feature descriptors with triplet losses" by V. Balntas, E.','line_number':948,'multiline':False]['text':' Riba et al.  If True, and if the positive example is closer to the','line_number':949,'multiline':False]['text':' negative example than the anchor is, swaps the positive example and the','line_number':950,'multiline':False]['text':' anchor in the loss computation.','line_number':951,'multiline':False]['text':' preserve legacy behavior of boundaries not causing type promotion','line_number':981,'multiline':False]['text':' type: ignore[arg-type]','line_number':983,'multiline':False]['text':' type: ignore[arg-type]','line_number':984,'multiline':False]['text':' type: ignore[arg-type]','line_number':989,'multiline':False]['text':' CompositeImplicitAutograd - don't register decomp','line_number':1023,'multiline':False]['text':' TODO: Raise exception instead of converting value.  This is only for','line_number':1042,'multiline':False]['text':' primTorch since it can drop support for deprecated arguments.','line_number':1043,'multiline':False]['text':' msg = "size_average and reduce args are deprecated, please use reduction argument."','line_number':1044,'multiline':False]['text':' avoid inplace add','line_number':1056,'multiline':False]['text':' See https://github.com/pytorch/pytorch/pull/81142#discussion_r918220126','line_number':1113,'multiline':False]['text':' It may be better to use clamp here, but we use hardtanh to replicate','line_number':1114,'multiline':False]['text':' the behavior of the existing implementation','line_number':1115,'multiline':False]['text':' For p == 2 we can use an efficient implementation, but other values of p','line_number':1157,'multiline':False]['text':' require creating a much bigger tensor for an intermediate step','line_number':1158,'multiline':False]['text':' Needed as aten.{celu_,elu_...} exist (even if they don't have the in-place kwarg)','line_number':1169,'multiline':False]