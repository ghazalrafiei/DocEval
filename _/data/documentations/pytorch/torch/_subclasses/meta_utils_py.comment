['text':' Import the following modules during type checking to enable code intelligence features,','line_number':24,'multiline':False]['text':' Do not import unconditionally, as they import sympy and importing sympy is very slow','line_number':25,'multiline':False]['text':' inference mode can trigger this','line_number':35,'multiline':False]['text':' TODO: test if is resizable (no direct query for this atm)','line_number':75,'multiline':False]['text':' TODO: audit AutogradMeta to see if it matches','line_number':76,'multiline':False]['text':' TODO: test forward AD','line_number':77,'multiline':False]['text':' This is a class for converting multiple tensors into meta tensors which','line_number':82,'multiline':False]['text':' share the same view/storage structure.  The operation model is you allocate','line_number':83,'multiline':False]['text':' one of these, and then call it repeatedly on all the tensors you want to','line_number':84,'multiline':False]['text':' convert.  It's important to use the same object for tensors you want to','line_number':85,'multiline':False]['text':' share storage because this is how we correlate shared storages to the same','line_number':86,'multiline':False]['text':' meta storages. This class will hold weak references to cached tenosrs','line_number':87,'multiline':False]['text':' and tensor storages.','line_number':88,'multiline':False]['text':' if for some reason we have aquired many storages which have not expired','line_number':116,'multiline':False]['text':' even though a tensor with their storage has expired (aliasing or otherwise)','line_number':117,'multiline':False]['text':' check for expired storages less often so as to bound the amount of work we','line_number':118,'multiline':False]['text':' do checking for expired storages','line_number':119,'multiline':False]['text':' hold a weak ref to self, otherwise it will be kept alive','line_number':128,'multiline':False]['text':' by the del_ten closure','line_number':129,'multiline':False]['text':' tensor outlives the converter','line_number':138,'multiline':False]['text':' on shutdown, tensor_ref_key may not be in memo','line_number':142,'multiline':False]['text':' [expired-storages]','line_number':147,'multiline':False]['text':' NB: even though the tensor has died,','line_number':148,'multiline':False]['text':' the deallocation of its storage can take longer,','line_number':149,'multiline':False]['text':' even when the storage has no other uses/views.','line_number':150,'multiline':False]['text':' In this case, the StorageWeakRef object will be kept alive','line_number':151,'multiline':False]['text':' longer than it needs to be, however the storage itself','line_number':152,'multiline':False]['text':' will be deallocated. We retain the possibly dead storages','line_number':153,'multiline':False]['text':' and periodically check if any of them are expired and','line_number':154,'multiline':False]['text':' can be freed.','line_number':155,'multiline':False]['text':' NB: doesn't actually return a storage, because meta storage is','line_number':161,'multiline':False]['text':' not supported','line_number':162,'multiline':False]['text':' NB: TypedStorage is freshly allocated and cannot be used as hash','line_number':164,'multiline':False]['text':' key index.','line_number':165,'multiline':False]['text':' Use a Weak Ref to s in order to not leak memory','line_number':167,'multiline':False]['text':' This function assumes that it's possible to do the conversion','line_number':175,'multiline':False]['text':' NB: name here is used in a conventional way by Dynamo; it corresponds','line_number':176,'multiline':False]['text':' precisely to the Source.name() of the tensor we're fakeifying and','line_number':177,'multiline':False]['text':' corresponds to a valid Python expression.  When we construct sub-names','line_number':178,'multiline':False]['text':' as part of this process, we will maintain this invariant!  (Even though','line_number':179,'multiline':False]['text':' other users of this may not need it this property to be upheld.)','line_number':180,'multiline':False]['text':' TODO: make a dedicated UnknownSource for this?','line_number':192,'multiline':False]['text':' This indicates you set no_dispatch() before calling into this','line_number':197,'multiline':False]['text':' function.  This is an error: we may be creating fake tensors and','line_number':198,'multiline':False]['text':' will perform operations on them which need fake tensor mode to','line_number':199,'multiline':False]['text':' be active.  You will segfault if you are in a no_dispatch() block.','line_number':200,'multiline':False]['text':' When we make as_strided calls, we end up generating a guard','line_number':207,'multiline':False]['text':' that the new as_strided tensor is in bounds for the old storage','line_number':208,'multiline':False]['text':' for the base (since as_strided calls can "bust" out of their','line_number':209,'multiline':False]['text':' bounding box.)  This guard is unnecessary: if a user is able','line_number':210,'multiline':False]['text':' to provide us a tensor with the view base setup this way, we','line_number':211,'multiline':False]['text':' don't need to produce a guard, because the fact that they','line_number':212,'multiline':False]['text':' were able to produce the view base means its in bounds.','line_number':213,'multiline':False]['text':'','line_number':214,'multiline':False]['text':' Now, ordinarily, this guard would be harmless.  However, the','line_number':215,'multiline':False]['text':' generated guard refers to variables bound on the base variable.','line_number':216,'multiline':False]['text':' At the moment, Dynamo doesn't actually guard on x._base, because','line_number':217,'multiline':False]['text':' according to Voz this results in a lot of spurious invalidations,','line_number':218,'multiline':False]['text':' and also if the user doesn't directly make use of _base, its','line_number':219,'multiline':False]['text':' pointless anyway (because programs should be parametric over','line_number':220,'multiline':False]['text':' whether or not the input tensor is a view or not--unless you're','line_number':221,'multiline':False]['text':' mutating the input, but that's a whole 'nother ballgame).  So','line_number':222,'multiline':False]['text':' for expediency, we suppress these guards so we don't have to','line_number':223,'multiline':False]['text':' deal with this (yet, anyway.)','line_number':224,'multiline':False]['text':'','line_number':225,'multiline':False]['text':' NB: An old version of this code suppressed guards for ALL operations','line_number':226,'multiline':False]['text':' happening during meta conversion, not just as_strided calls.','line_number':227,'multiline':False]['text':' This is too aggressive: we do duck sizing and 0/1 simplification','line_number':228,'multiline':False]['text':' as we allocate variables, and we do need to register guards for','line_number':229,'multiline':False]['text':' these cases.','line_number':230,'multiline':False]['text':' Don't reallocate the sizes; the shape envs are the same,','line_number':241,'multiline':False]['text':' so reuse the old sizes/strides/etc','line_number':242,'multiline':False]['text':' see expired-storages','line_number':267,'multiline':False]['text':' Note [is_coalesced is dispatched]','line_number':288,'multiline':False]['text':' Strangely enough, is_coalesced() is a dispatched operator,','line_number':289,'multiline':False]['text':' which means that it will get caught by fake tensor mode.','line_number':290,'multiline':False]['text':' Ordinarily this would error, but there's some logic in','line_number':291,'multiline':False]['text':' fake tensor ensure this doesn't happen.','line_number':292,'multiline':False]['text':' Construct views in two steps: recursively meta-fy their','line_number':317,'multiline':False]['text':' base, and then create view(s) off that.  NB: doing it','line_number':318,'multiline':False]['text':' directly from storage is WRONG because this won't cause','line_number':319,'multiline':False]['text':' version counters to get shared.','line_number':320,'multiline':False]['text':' In some situations, MetaConverter may be called in a','line_number':351,'multiline':False]['text':' context where autograd is disabled.  For the _is_view','line_number':352,'multiline':False]['text':' assert to pass, we have to setup the autograd view','line_number':353,'multiline':False]['text':' metadata anyway.  Do this by reenabling the','line_number':354,'multiline':False]['text':' ADInplaceOrView key.  This is kind of a hack.','line_number':355,'multiline':False]['text':' This is not guaranteed to succeed.  If it fails, it','line_number':370,'multiline':False]['text':' means there is another dtype-converting view function','line_number':371,'multiline':False]['text':' that hasn't been handled here','line_number':372,'multiline':False]['text':' This is very tricky.  Naively, you might expect this','line_number':375,'multiline':False]['text':' to hold:','line_number':376,'multiline':False]['text':'','line_number':377,'multiline':False]['text':'   if t.requires_grad and not safe_is_leaf(t)','line_number':378,'multiline':False]['text':'       assert t._base.requires_grad','line_number':379,'multiline':False]['text':'','line_number':380,'multiline':False]['text':' But it's not true!  As you can see in the following','line_number':381,'multiline':False]['text':' program:','line_number':382,'multiline':False]['text':'','line_number':383,'multiline':False]['text':'   x = torch.zeros(4)','line_number':384,'multiline':False]['text':'   y = x.view(1, 4)','line_number':385,'multiline':False]['text':'   y.requires_grad = True','line_number':386,'multiline':False]['text':'   z = y.view(1, 1, 4)','line_number':387,'multiline':False]['text':'   assert z._base is x','line_number':388,'multiline':False]['text':'','line_number':389,'multiline':False]['text':' So we may have to do *two* views out of the base to','line_number':390,'multiline':False]['text':' recreate this situation.','line_number':391,'multiline':False]['text':' Nested tensors do not support as_strided, and','line_number':394,'multiline':False]['text':' hence,always have _view_func available.','line_number':395,'multiline':False]['text':'','line_number':396,'multiline':False]['text':' The unsafe version of _view_func omits','line_number':397,'multiline':False]['text':' checking whether the base passed in has the same','line_number':398,'multiline':False]['text':' metadata as the original base the view_func','line_number':399,'multiline':False]['text':' was originally executed with. (1) It is OK here,','line_number':400,'multiline':False]['text':' because we're calling it on the meta-ified base,','line_number':401,'multiline':False]['text':' so the metadata is guaranteed to be the same.','line_number':402,'multiline':False]['text':' (2) It is necessary because we don't actually','line_number':403,'multiline':False]['text':' want to guard on the base's metadata here.','line_number':404,'multiline':False]['text':' Leaf views that track view metadata are created by','line_number':415,'multiline':False]['text':' creating a view inside a no_grad block','line_number':416,'multiline':False]['text':' As it's a leaf, we can directly assign requires_grad','line_number':419,'multiline':False]['text':' Easy case, just run the view op','line_number':423,'multiline':False]['text':' NB: We don't actaully faithfully replicate','line_number':427,'multiline':False]['text':' autograd connectivity, but that doesn't matter','line_number':428,'multiline':False]['text':' today. See following for more info:','line_number':429,'multiline':False]['text':' https://gist.github.com/soulitzer/e03f015b314c3f5fcf80888c69390913','line_number':430,'multiline':False]['text':' Obscure case.  Create a leaf view and give it the','line_number':432,'multiline':False]['text':' correct requires_grad, then do the final view.','line_number':433,'multiline':False]['text':' NB: Can't have a non-leaf without requiring grad!','line_number':434,'multiline':False]['text':' The CreationMeta influences whether or not inplace','line_number':441,'multiline':False]['text':' mutation is an error or not.  So we need to make','line_number':442,'multiline':False]['text':' sure we properly propagate this as well.','line_number':443,'multiline':False]['text':' If we have a subclass that desugars into dense tensors,','line_number':465,'multiline':False]['text':' perform our callback on each inner tensor.','line_number':466,'multiline':False]['text':' Note: transform_subclass will use __tensor_unflatten__ to generate','line_number':468,'multiline':False]['text':' a fresh subclass wrapper. We assume that if the inner tensors of','line_number':469,'multiline':False]['text':' the subclass are given symbolic sizes, their sizes will be used','line_number':470,'multiline':False]['text':' to construct the (symbolic) sizes of the wrapper tensor.','line_number':471,'multiline':False]['text':' Fake up some autograd history.','line_number':506,'multiline':False]['text':' preserve_format is the default, but we want to','line_number':508,'multiline':False]['text':' emphasize how important it is to preserve','line_number':509,'multiline':False]['text':' format here','line_number':510,'multiline':False]['text':' Graph-Break for wrapped tensors','line_number':513,'multiline':False]['text':' You're normal and happy, install the fresh storage into the memo','line_number':526,'multiline':False]['text':' You're in crazy town; somehow you gave us a tensor','line_number':529,'multiline':False]['text':' that wasn't a view, but had nonzero storage offset,','line_number':530,'multiline':False]['text':' nontrivial strides (such that clone() couldn't','line_number':531,'multiline':False]['text':' preserve them), or already aliases with another','line_number':532,'multiline':False]['text':' tensor's storage.  The most typical way to end','line_number':533,'multiline':False]['text':' up here is with set_.  So use set_ to bludgeon this','line_number':534,'multiline':False]['text':' in.','line_number':535,'multiline':False]['text':' NB: In principle, this should always work, but there','line_number':537,'multiline':False]['text':' is some subtle difference in the autograd metadata','line_number':538,'multiline':False]['text':' that means we will backprop the set_ call, even if','line_number':539,'multiline':False]['text':' r is declared as an input to grad.','line_number':540,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/87956','line_number':541,'multiline':False]['text':' for the reproducer.','line_number':542,'multiline':False]['text':' NB: The in_kernel_invocation_manager here is necessary','line_number':543,'multiline':False]['text':' for fake tensor.  If we run the set_ call with fake','line_number':544,'multiline':False]['text':' tensor on, r will improperly report that it is NOT a','line_number':545,'multiline':False]['text':' meta tensor but a cpu tensor, and then the set_ call','line_number':546,'multiline':False]['text':' will fail due to device mismatch.  no_dispatch() is','line_number':547,'multiline':False]['text':' not enough, because the fake tensor will still claim','line_number':548,'multiline':False]['text':' to be a CPU tensor and you'll end up in the CPU','line_number':549,'multiline':False]['text':' kernel.  Arguably this is a hack; a cleaner way to','line_number':550,'multiline':False]['text':' solve this is to have a FakeStorage concept which','line_number':551,'multiline':False]['text':' would report it's CPU device--no problem now!  But','line_number':552,'multiline':False]['text':' this is difficult to do because we don't have storage','line_number':553,'multiline':False]['text':' subclasses.  Relevant test is','line_number':554,'multiline':False]['text':' DynamicShapesFunctionTests::test_add_dynamic_shapes in','line_number':555,'multiline':False]['text':' test/dynamo/test_dynamic_shapes.py','line_number':556,'multiline':False]['text':' This can be skipped if necessary for performance reasons','line_number':581,'multiline':False]['text':' TODO: zero tensors?  We appear to have eliminated them by','line_number':596,'multiline':False]['text':' excluding complex for now','line_number':597,'multiline':False]['text':' We need a way to test if a tensor is batched but there','line_number':608,'multiline':False]['text':' is no official APi to do it','line_number':609,'multiline':False]['text':' torch._C._is_batched(t),','line_number':610,'multiline':False]['text':' TODO: sparse should support meta','line_number':613,'multiline':False]['text':' NB technically to('meta') does work but our logging','line_number':614,'multiline':False]['text':' instrumentation will see the meta conversions and the','line_number':615,'multiline':False]['text':' tests all break so we just exclude this.  In any case','line_number':616,'multiline':False]['text':' the to conversion isn't really right anyhow.','line_number':617,'multiline':False]['text':' the case of AOTAutograd','line_number':630,'multiline':False]['text':' torch.func.functionalize','line_number':645,'multiline':False]['text':' NB: Cannot directly use Parameter constructor','line_number':672,'multiline':False]['text':' because that would force a detach, not desirable','line_number':673,'multiline':False]['text':' non-Tensor types don't count as hit or miss','line_number':680,'multiline':False]