['text':' This wrapper exists to support variadic args.','line_number':73,'multiline':False]['text':' type: ignore[attr-defined]','line_number':76,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':104,'multiline':False]['text':' TODO Move this to C++ once the jit has better support for torch.Size.','line_number':105,'multiline':False]['text':' with implementation above, torch.jit.trace hardcodes the sizes which makes subsequent replays fail','line_number':133,'multiline':False]['text':' Overwriting reason:','line_number':186,'multiline':False]['text':' This dispatches to two ATen functions depending on the type of','line_number':187,'multiline':False]['text':' split_size_or_sections. The branching code is in _tensor.py, which we','line_number':188,'multiline':False]['text':' call here.','line_number':189,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':332,'multiline':False]['text':' Convert the subscript list format which is an interleaving of operand and its subscripts','line_number':341,'multiline':False]['text':' list with an optional output subscripts list at the end (see documentation for more details on this)','line_number':342,'multiline':False]['text':' to the equation string format by creating the equation string from the subscripts list and grouping the','line_number':343,'multiline':False]['text':' input operands into a tensorlist (List[Tensor]).','line_number':344,'multiline':False]['text':' Parse subscripts for input operands','line_number':354,'multiline':False]['text':' Parse optional output subscripts (provided when the number of arguments is odd)','line_number':357,'multiline':False]['text':' the old interface of passing the operands as one list argument','line_number':371,'multiline':False]['text':' recurse incase operands contains value that has torch function','line_number':373,'multiline':False]['text':' in the original implementation this line is omitted','line_number':374,'multiline':False]['text':' the path for contracting 0 or 1 time(s) is already optimized','line_number':378,'multiline':False]['text':' or the user has disabled using opt_einsum','line_number':379,'multiline':False]['text':' type: ignore[attr-defined]','line_number':380,'multiline':False]['text':' flatten path for dispatching to C++','line_number':386,'multiline':False]['text':' type: ignore[attr-defined]','line_number':388,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':391,'multiline':False]['text':' The JIT doesn't understand Union, so only add type annotation for mypy','line_number':393,'multiline':False]['text':' the old interface of passing the operands as one list argument','line_number':499,'multiline':False]['text':' type: ignore[assignment]','line_number':500,'multiline':False]['text':' Continue allowing call of old method that takes no indexing','line_number':502,'multiline':False]['text':' kwarg for forward compatibility reasons.','line_number':503,'multiline':False]['text':'','line_number':504,'multiline':False]['text':' Remove this two weeks after landing.','line_number':505,'multiline':False]['text':' type: ignore[attr-defined]','line_number':507,'multiline':False]['text':' NOTE: Do not edit. This code will be removed once the forward-compatibility','line_number':652,'multiline':False]['text':'       period is over for PR #73432','line_number':653,'multiline':False]['text':' type: ignore[attr-defined]','line_number':660,'multiline':False]['text':' These _impl functions return a variable number of tensors as output with','line_number':747,'multiline':False]['text':' __torch_function__; tuple unpacking is done already rather than being','line_number':748,'multiline':False]['text':' done by the caller of the _impl function','line_number':749,'multiline':False]['text':' type: ignore[attr-defined]','line_number':970,'multiline':False]['text':' type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]','line_number':976,'multiline':False]['text':' type: (Tensor, bool, bool, bool, Optional[int]) -> Tensor','line_number':986,'multiline':False]['text':' type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]','line_number':996,'multiline':False]['text':' The return type of unique depends on `return_inverse`, and `return_counts` so in order to','line_number':1023,'multiline':False]['text':' resolve the output type in TorchScript we need to statically know the value of both parameters','line_number':1024,'multiline':False]['text':' type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]','line_number':1038,'multiline':False]['text':' type: (Tensor, bool, bool, Optional[int]) -> Tensor','line_number':1048,'multiline':False]['text':' type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]','line_number':1058,'multiline':False]['text':' The return type of unique depends on `return_inverse`, and `return_counts` so in order to','line_number':1085,'multiline':False]['text':' resolve the output type in TorchScript we need to statically know the value of both parameters','line_number':1086,'multiline':False]['text':' There's no good way to use this type annotation without breaking JIT','line_number':1100,'multiline':False]['text':' overloads. So leave untyped for mypy for now.','line_number':1101,'multiline':False]['text':' noqa: F811','line_number':1107,'multiline':False]['text':' noqa: F811','line_number':1108,'multiline':False]['text':' noqa: F811','line_number':1111,'multiline':False]['text':' noqa: F811','line_number':1112,'multiline':False]['text':' noqa: F811','line_number':1115,'multiline':False]['text':' noqa: F811','line_number':1116,'multiline':False]['text':' noqa: F811','line_number':1120,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1208,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1210,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1242,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1245,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1278,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1281,'multiline':False]['text':' type: (Tensor, Tensor, float, str) -> (Tensor)','line_number':1285,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1330,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1332,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1334,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1367,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1372,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1405,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1410,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1451,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1456,'multiline':False]['text':' There's no good way to use this type annotation; cannot rename norm() to','line_number':1461,'multiline':False]['text':' _norm_impl() in a way that doesn't break JIT overloads. So leave untyped','line_number':1462,'multiline':False]['text':' for mypy for now.','line_number':1463,'multiline':False]['text':'    def norm(input: Tensor,','line_number':1464,'multiline':False]['text':'             p: Optional[Union[str, Number]] = "fro",','line_number':1465,'multiline':False]['text':'             dim: Optional[Union[int, List[int]]] = None,','line_number':1466,'multiline':False]['text':'             keepdim: bool = False,','line_number':1467,'multiline':False]['text':'             out: Optional[Tensor] = None,','line_number':1468,'multiline':False]['text':'             dtype: _dtype = None) -> Tensor:','line_number':1469,'multiline':False]['text':'        return _norm_impl(input, p, dim, keepdim, out, dtype)','line_number':1470,'multiline':False]['text':' TODO: type dim as BroadcastingList when','line_number':1472,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/33782 is fixed','line_number':1473,'multiline':False]['text':' type: (Tensor, str, Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor','line_number':1476,'multiline':False]['text':' noqa: F811','line_number':1479,'multiline':False]['text':' noqa: F811','line_number':1480,'multiline':False]['text':' type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor','line_number':1481,'multiline':False]['text':' noqa: F811','line_number':1484,'multiline':False]['text':' noqa: F811','line_number':1485,'multiline':False]['text':' type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor','line_number':1486,'multiline':False]['text':' noqa: F811','line_number':1489,'multiline':False]['text':' noqa: F811','line_number':1490,'multiline':False]['text':' type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor','line_number':1491,'multiline':False]['text':' noqa: F811','line_number':1495,'multiline':False]['text':' NB. All the repeated code and weird python is to please TorchScript.','line_number':1593,'multiline':False]['text':'     For a more compact implementation see the relevant function in `_refs/__init__.py`','line_number':1594,'multiline':False]['text':' We don't do this for MPS or sparse tensors','line_number':1596,'multiline':False]['text':' type: ignore[assignment]','line_number':1605,'multiline':False]['text':' Here we either call the nuclear norm, or we call matrix_norm with some arguments','line_number':1614,'multiline':False]['text':' that will throw an error','line_number':1615,'multiline':False]['text':' NB. p should be Union[str, number], not Optional!','line_number':1623,'multiline':False]['text':' catch default case','line_number':1632,'multiline':False]['text':' noqa: C416 TODO: rewrite as list(range(m))','line_number':1638,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1639,'multiline':False]['text':' TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed','line_number':1641,'multiline':False]['text':' remove the overloads where dim is an int and replace with BraodcastingList1','line_number':1642,'multiline':False]['text':' and remove next four lines, replace _dim with dim','line_number':1643,'multiline':False]['text':' type: ignore[assignment]','line_number':1650,'multiline':False]['text':' type: ignore[arg-type]','line_number':1660,'multiline':False]['text':' type: ignore[arg-type]','line_number':1662,'multiline':False]['text':' type: ignore[arg-type]','line_number':1668,'multiline':False]['text':' type: ignore[arg-type]','line_number':1670,'multiline':False]['text':' type: ignore[arg-type]','line_number':1673,'multiline':False]['text':' type: ignore[arg-type]','line_number':1675,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1683,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1685,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1688,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1690,'multiline':False]['text':' This wrapper exists to support variadic args.','line_number':1811,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1816,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1818,'multiline':False]['text':' type: (Tensor, bool, bool, Any) -> Tuple[Tensor, Tensor, Tensor]','line_number':1822,'multiline':False]['text':' If get_infos is True, then we don't need to check for errors and vice versa','line_number':1917,'multiline':False]['text':' type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor, Tensor]]) -> Tuple[Tensor, Tensor, Tensor]','line_number':1935,'multiline':False]['text':' A_LU, pivots, infos','line_number':1946,'multiline':False]['text':' type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]','line_number':1950,'multiline':False]['text':' need to check for torch_function here so that we exit if','line_number':1951,'multiline':False]['text':' A_LU, pivots','line_number':1962,'multiline':False]['text':' The return type of lu depends on `get_infos`, so in order to resolve the output type','line_number':1964,'multiline':False]['text':' of lu in TorchScript we need to statically know the value of `get_infos`','line_number':1965,'multiline':False]