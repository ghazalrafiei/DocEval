['text':' AccumulateGrad is used in the Autograd engine to handle gradient updates.','line_number':156,'multiline':False]['text':' There are two possible cases:','line_number':157,'multiline':False]['text':' 1) This is a newly created gradient Tensor. In that case there is nothing','line_number':158,'multiline':False]['text':'    to accumulate, so autograd simply detaches the Tensor.','line_number':159,'multiline':False]['text':'','line_number':160,'multiline':False]['text':' 2) There is a preexisting gradient Tensor and we need to add the newly','line_number':161,'multiline':False]['text':'    computed update. This is done with an in-place add (aten::add_) op.','line_number':162,'multiline':False]['text':'    (The underscore suffix denotes "in-place".)','line_number':163,'multiline':False]['text':' TODO(robieta): Move away from load bearing names','line_number':167,'multiline':False]['text':' We directly instrument `torch.nn.Module` and `torch.optim.Optimizer`','line_number':177,'multiline':False]['text':' NOTE: The values captured by the python tracer are cached; they can be','line_number':178,'multiline':False]['text':'       used to build up labels but do not imply that a Tensor was live at','line_number':179,'multiline':False]['text':'       a particular time.','line_number':180,'multiline':False]['text':' Tensor','line_number':250,'multiline':False]['text':'','line_number':252,'multiline':False]['text':' TensorList','line_number':253,'multiline':False]['text':'','line_number':255,'multiline':False]['text':' Scalar and uncaptured inputs.','line_number':256,'multiline':False]['text':' Profiler only records a subset of possible argument types. If we','line_number':297,'multiline':False]['text':' reach this point then the schema must call for a type that profiler','line_number':298,'multiline':False]['text':' does not record. Thus, the schema can only be a match if `observed`','line_number':299,'multiline':False]['text':' is also None.','line_number':300,'multiline':False]['text':' TODO(robieta):','line_number':305,'multiline':False]['text':'   _jit_get_schemas_for_operator is quite expensive. (~100us / call)','line_number':306,'multiline':False]['text':'   Consider adding `functools.lru_cache` if that becomes an issue.','line_number':307,'multiline':False]['text':' Schema lookup will throw if `name` is malformed. (For example,','line_number':310,'multiline':False]['text':' schemas must be namespaced and schema lookup will fail if name','line_number':311,'multiline':False]['text':' does not include "::".) We simply catch the exception and return','line_number':312,'multiline':False]['text':' `None` to denote that `name` cannot be an operator name.','line_number':313,'multiline':False]['text':'','line_number':314,'multiline':False]['text':' Note that record_function annotations also go through this path,','line_number':315,'multiline':False]['text':' so it is expected that some names will not correspond to PyTorch','line_number':316,'multiline':False]['text':' operators.','line_number':317,'multiline':False]['text':' It is possible to resize Storage in PyTorch, however we','line_number':371,'multiline':False]['text':' key on data pointer so most resizes will be treated as a','line_number':372,'multiline':False]['text':' change in storage. The one corner case that cannot be','line_number':373,'multiline':False]['text':' handled is `realloc` which successfully resizes the','line_number':374,'multiline':False]['text':' storage. At time of writing this is not done anywhere in','line_number':375,'multiline':False]['text':' the core PyTorch codebase.','line_number':376,'multiline':False]['text':' Scalars are represented as zero dim Tensors','line_number':386,'multiline':False]['text':' Make sure the version bumping behavior matches what we expect.','line_number':429,'multiline':False]['text':' Start by populating edges from op inputs and outputs.','line_number':436,'multiline':False]['text':' Tensor','line_number':442,'multiline':False]['text':' TensorList','line_number':447,'multiline':False]['text':' We consider an op to be mutated if we encounter a schema where it','line_number':459,'multiline':False]['text':' is a mutable argument OR if it is ambiguous. (We never explicitly','line_number':460,'multiline':False]['text':' see it in any schema.)','line_number':461,'multiline':False]['text':' Then handle deletions. Note that deleting a Tensor implicitly adds','line_number':465,'multiline':False]['text':' it as an input edge.','line_number':466,'multiline':False]['text':' And finally handle allocations. This step must be last, because the','line_number':475,'multiline':False]['text':' previous two steps optimistically add input edges.','line_number':476,'multiline':False]['text':' We don't need to sort the inputs, but it makes debugging and unit tests nicer.','line_number':481,'multiline':False]['text':' MyPy can't see through `is_allocation` to know that','line_number':487,'multiline':False]['text':' `v.input_version` is not None.','line_number':488,'multiline':False]['text':' Check that each (Tensor, version) pair has a unique creation node','line_number':527,'multiline':False]['text':' And check that `self._nodes` forms a valid topologically sorted DAG.','line_number':535,'multiline':False]['text':' Used by unit tests to check internals. (And consequently by','line_number':625,'multiline':False]['text':' MemoryProfile.lookup) This should not be used in any other capacity.','line_number':626,'multiline':False]['text':' We are guaranteed to exit because there is a finite set of','line_number':790,'multiline':False]['text':' TensorAndID pairs. In practice we do not expect to loop more than','line_number':791,'multiline':False]['text':' three times: once to identify the core parameter update loop,','line_number':792,'multiline':False]['text':' once to fold the first step into that loop, and a third time','line_number':793,'multiline':False]['text':' where no new elements are added.','line_number':794,'multiline':False]['text':' Gradients are straightforward to detect. We directly check the','line_number':801,'multiline':False]['text':' `.grad` property in the Python tracer, and we can detect any new','line_number':802,'multiline':False]['text':' gradient Tensors from `AccumulateGrad` ops.','line_number':803,'multiline':False]['text':' Similarly, temporary Tensors are easy to identify and are useful to','line_number':808,'multiline':False]['text':' flag since they can make memory use "spikier" than one would','line_number':809,'multiline':False]['text':' otherwise expect.','line_number':810,'multiline':False]['text':' All of this analysis is predicated on using at least one training','line_number':842,'multiline':False]['text':' step (or parameters from the python tracer) to partition the graph.','line_number':843,'multiline':False]['text':' Absent that we cannot determine which Tensors are inputs and which','line_number':844,'multiline':False]['text':' ones are part of the model.','line_number':845,'multiline':False]['text':' We only want to annotate Tensors which actually contribute to the','line_number':848,'multiline':False]['text':' model calculation.','line_number':849,'multiline':False]['text':' Don't include Tensors created in the backward pass, as these are','line_number':861,'multiline':False]['text':' generally Autograd implementation details rather than proper inputs.','line_number':862,'multiline':False]['text':' Determine which Tensors might be parameters based on forward pass','line_number':903,'multiline':False]['text':' data flow. Note this these are only candidates; we filter nodes that','line_number':904,'multiline':False]['text':' we know are part of the backward pass but that doesn't guarantee that','line_number':905,'multiline':False]['text':' they are part of the forward pass.','line_number':906,'multiline':False]['text':' Don't check nodes in the backward pass.','line_number':915,'multiline':False]['text':'','line_number':919,'multiline':False]['text':' and only check nodes which depend on an input.','line_number':920,'multiline':False]['text':' Require that each parameter eventually contributes to the value of a gradient','line_number':926,'multiline':False]['text':' and depends on a gradient.','line_number':937,'multiline':False]['text':'','line_number':957,'multiline':False]['text':' Stop filling when we reach the backward pass.','line_number':958,'multiline':False]['text':' Convert timestamps from ns to us, to match trace events.','line_number':1022,'multiline':False]['text':' Save the smallest timestamp to populate pre-existing allocs.','line_number':1026,'multiline':False]['text':' Handle timestep','line_number':1030,'multiline':False]['text':' Handle memory and categories','line_number':1039,'multiline':False]['text':' TODO: Write a faster serialize (orjson not available in CI)','line_number':1061,'multiline':False]['text':' Check if user has matplotlib installed, return gracefully if not.','line_number':1138,'multiline':False]['text':' For this timeline, start at 0 to match Chrome traces.','line_number':1157,'multiline':False]['text':' Plot memory timeline as stacked data','line_number':1164,'multiline':False]['text':' Usually training steps are in magnitude of ms.','line_number':1173,'multiline':False]['text':' Embed the memory timeline image into the HTML file','line_number':1185,'multiline':False]