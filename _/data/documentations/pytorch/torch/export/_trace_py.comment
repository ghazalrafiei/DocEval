['text':' TODO properly use the cached fake tensor','line_number':86,'multiline':False]['text':' Append a root module to every nn_module_stack.','line_number':114,'multiline':False]['text':' TODO Figure out why sometimes we have root sometimes we don't.','line_number':127,'multiline':False]['text':' TODO(zhxchen17) Remove this.','line_number':147,'multiline':False]['text':' Since the graph module is flattened (no module heirarchy), we','line_number':199,'multiline':False]['text':' need to noramlize the module by replacing "." with "_". If we','line_number':200,'multiline':False]['text':' don't, it will try to save the weight to a submodule which no','line_number':201,'multiline':False]['text':' longer exists.','line_number':202,'multiline':False]['text':' Replace state dict attr names with the fqn','line_number':206,'multiline':False]['text':' Replace graph getattr nodes with the correct name','line_number':218,'multiline':False]['text':' We convert to nn.Module because __call__ of ExportedProgram','line_number':252,'multiline':False]['text':' is untracable right now.','line_number':253,'multiline':False]['text':' noqa: TRY200','line_number':272,'multiline':False]['text':' noqa: TRY200','line_number':274,'multiline':False]['text':' user inputs are always added in the end','line_number':322,'multiline':False]['text':' type: ignore[arg-type]','line_number':361,'multiline':False]['text':' type: ignore[return-value]','line_number':365,'multiline':False]['text':' TODO(zhxchen17) Revisit if this is needed later.','line_number':374,'multiline':False]['text':' This _reparametrize_module makes sure inputs and module.params/buffers have the same fake_mode,','line_number':376,'multiline':False]['text':' otherwise aot_export_module will error out because it sees a mix of fake_modes.','line_number':377,'multiline':False]['text':' And we want aot_export_module to use the fake_tensor mode in dynamo to keep the pipeline easy to reason about.','line_number':378,'multiline':False]['text':' NOTE: aot_export adds symint metadata for placeholders with int values;','line_number':384,'multiline':False]['text':' since these become specialized, we replace such metadata with the original values','line_number':385,'multiline':False]['text':' type: ignore[arg-type]','line_number':411,'multiline':False]['text':' type: ignore[arg-type]','line_number':412,'multiline':False]['text':' type: ignore[arg-type]','line_number':413,'multiline':False]['text':' type: ignore[arg-type]','line_number':414,'multiline':False]['text':' type: ignore[arg-type]','line_number':415,'multiline':False]['text':' type: ignore[arg-type, union-attr]','line_number':416,'multiline':False]['text':' type: ignore[arg-type, union-attr]','line_number':417,'multiline':False]['text':' type: ignore[arg-type, union-attr]','line_number':418,'multiline':False]['text':' don't need to restore because we will do it later','line_number':565,'multiline':False]['text':' We detect the fake_mode by looking at gm_torch_level's placeholders, this is the fake_mode created in dynamo.','line_number':570,'multiline':False]['text':' First, we want to pass through the graph to try populating','line_number':578,'multiline':False]['text':' val field for getattr if there is anything missing.','line_number':579,'multiline':False]['text':' THis can happen when quantization adds extra params and forgets','line_number':580,'multiline':False]['text':' to update "val"','line_number':581,'multiline':False]['text':' Checks if it is not a HigherOrderOp branch or a module','line_number':585,'multiline':False]['text':' When aot_export lifts the params, we lose the nn_module_stack','line_number':594,'multiline':False]['text':' and source_fn from the param nodes as they are treated as fresh inputs','line_number':595,'multiline':False]['text':' Therefore, we manually extract them before calling into aot_export','line_number':596,'multiline':False]['text':' If the call_function uses param as input, we also need to update params' meta','line_number':619,'multiline':False]['text':' with this call_function node's meta.','line_number':620,'multiline':False]['text':' This is basically the same flow as torch.fx.traceback.preserve_meta()','line_number':621,'multiline':False]['text':' Fix the graph output signature to be tuple if scalar','line_number':631,'multiline':False]['text':' aot_export expect the return type to always be a tuple.','line_number':634,'multiline':False]['text':' type: ignore[attr-defined]','line_number':638,'multiline':False]['text':' Restore FQN of param/buffers','line_number':649,'multiline':False]['text':' TODO unfortunately preserving graph-level metadata is not','line_number':666,'multiline':False]['text':' working well with aot_export. So we manually copy it.','line_number':667,'multiline':False]['text':' (The node-level meta is addressed above.)','line_number':668,'multiline':False]['text':' Note: aot_export_module doesn't accept kwargs, we'd like to reorder the kwargs as an OrderedDict','line_number':676,'multiline':False]['text':' to follow the order in orig_args and correctly call module','line_number':677,'multiline':False]['text':' After aot_export, set the param/buffer metadata back into placeholders','line_number':690,'multiline':False]['text':' Technically, users can still construct this data from param names','line_number':691,'multiline':False]['text':' without relying on this metadata','line_number':692,'multiline':False]['text':' The unbacked symint symbols are updated in aot_export','line_number':706,'multiline':False]['text':' so we serialize them here instead of inside dynamo','line_number':707,'multiline':False]['text':' TODO(zhxchen17) Return empty state_dict for functions.','line_number':752,'multiline':False]