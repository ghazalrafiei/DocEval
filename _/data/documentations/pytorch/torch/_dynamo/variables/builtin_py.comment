['text':' function -> ([forward name, reverse name, in-place name], in-place op)','line_number':199,'multiline':False]['text':' NB: The follow binary operators are not supported for now, since the','line_number':223,'multiline':False]['text':' corresponding magic methods aren't defined on SymInt / SymFloat:','line_number':224,'multiline':False]['text':' operator.matmul','line_number':225,'multiline':False]['text':' divmod','line_number':226,'multiline':False]['text':' operator.and_','line_number':227,'multiline':False]['text':' operator.or_','line_number':228,'multiline':False]['text':' operator.xor','line_number':229,'multiline':False]['text':' Multiple dispatch mechanism defining custom binop behavior for certain type','line_number':236,'multiline':False]['text':' combinations. Handlers are attempted in order, and will be used if the type checks','line_number':237,'multiline':False]['text':' match. They are expected to have the signature:','line_number':238,'multiline':False]['text':' fn(tx, arg0: VariableTracker, arg1: VariableTracker, options) -> VariableTracker','line_number':239,'multiline':False]['text':' Override table contains: op_fn -> [list of handlers]','line_number':241,'multiline':False]['text':' User-defined args (highest precedence)','line_number':252,'multiline':False]['text':' Manually handle reversing logic if needed (e.g. call __radd__)','line_number':261,'multiline':False]['text':' TODO: If we expand this to handle tensor args, we need to manually','line_number':263,'multiline':False]['text':' handle cases like this:','line_number':264,'multiline':False]['text':'','line_number':265,'multiline':False]['text':' class A(int):','line_number':266,'multiline':False]['text':'     def __radd__(self, other):','line_number':267,'multiline':False]['text':'         print("woof")','line_number':268,'multiline':False]['text':' torch.randn(3) + A(3)','line_number':269,'multiline':False]['text':'','line_number':270,'multiline':False]['text':' In this example, A.__radd__() is not called -> nothing is printed, because','line_number':271,'multiline':False]['text':' Tensor.__add__ only does a subtype test against int, ignoring the subclass.','line_number':272,'multiline':False]['text':' To be fully correct, we should not call A.__radd__() here, and there may be','line_number':273,'multiline':False]['text':' other cases to reason about and add exceptions for.','line_number':274,'multiline':False]['text':' Dynamic shape args','line_number':299,'multiline':False]['text':' NB: Prefer out-of-place op when calling in-place op to generate valid graph','line_number':318,'multiline':False]['text':' Special cases - lower precedence but still prefer these over constant folding','line_number':326,'multiline':False]['text':' List-like addition (e.g. [1, 2] + [3, 4])','line_number':328,'multiline':False]['text':' NB: Prefer the tuple-specific logic over base logic because of','line_number':336,'multiline':False]['text':' some SizeVariable weirdness. Specifically, the tuple-specific logic','line_number':337,'multiline':False]['text':' drops the subclass type (e.g. SizeVariable) and returns TupleVariables.','line_number':338,'multiline':False]['text':' Handler doesn't apply','line_number':366,'multiline':False]['text':' List-like expansion (e.g. [1, 2, 3] * 3)','line_number':390,'multiline':False]['text':' Return first handler that matches the type checks','line_number':420,'multiline':False]['text':' args[0] is list and args[1] is unspec','line_number':502,'multiline':False]['text':' In-place operators like += usually mustate tensor','line_number':523,'multiline':False]['text':' values, but in the edge case of immutable values they','line_number':524,'multiline':False]['text':' re-bind the variable.','line_number':525,'multiline':False]['text':'','line_number':526,'multiline':False]['text':' The easiest way to keep the graph consistent in this','line_number':527,'multiline':False]['text':' scenario is to de-sugar eagerly.','line_number':528,'multiline':False]['text':' Standard indexing will force specialization due to','line_number':532,'multiline':False]['text':' __index__.  Rewrite as a regular torch op which will','line_number':533,'multiline':False]['text':' trace fine','line_number':534,'multiline':False]['text':' Interaction between ndarray and tensors:','line_number':541,'multiline':False]['text':'   We prefer the tensor op whenever there are tensors involved','line_number':542,'multiline':False]['text':' Work around for vision_maskrcnn due to precision difference','line_number':585,'multiline':False]['text':' specialize the dividend when float divide by tensor','line_number':586,'multiline':False]['text':' Handle cases like int(torch.seed())','line_number':596,'multiline':False]['text':' Also handle sym_float to sym_int cases','line_number':597,'multiline':False]['text':' Handle `str` on a user defined function','line_number':617,'multiline':False]['text':' Handle binary ops (e.g. __add__ / __radd__, __iadd__, etc.)','line_number':621,'multiline':False]['text':' NB: Tensor args are handled above and not here','line_number':622,'multiline':False]['text':' Try to find a handler for the arg types; otherwise, fall through to constant handler','line_number':624,'multiline':False]['text':' Actually, we will handle this just fine','line_number':654,'multiline':False]['text':' constant fold','line_number':658,'multiline':False]['text':' expand iterable','line_number':689,'multiline':False]['text':' result of an item call is a scalar convert to a tensor','line_number':710,'multiline':False]['text':' Dynamic input does not get resolved, rather, gets stored as call_function','line_number':714,'multiline':False]['text':' convert min/max to torch ops','line_number':728,'multiline':False]['text':' return unspec if both a, b are unspec or const','line_number':749,'multiline':False]['text':' otherwise return tensor','line_number':780,'multiline':False]['text':' Call arg.__abs__()','line_number':793,'multiline':False]['text':' None no-ops this handler and lets the driving function proceed','line_number':807,'multiline':False]['text':' For non-list iterators, we will guard on vars that','line_number':833,'multiline':False]['text':' determine the control flow','line_number':834,'multiline':False]['text':' TODO This should probably be treated as a dict, or dicts should also be treated here','line_number':837,'multiline':False]['text':' Only `OrderedDict.fromkeys` accepts `value` passed by keyword','line_number':929,'multiline':False]['text':' UserDefinedObject with C extensions can have torch.Tensor attributes,','line_number':1021,'multiline':False]['text':' so break graph.','line_number':1022,'multiline':False]['text':' handle __instancecheck__ defined in user class','line_number':1029,'multiline':False]['text':' Special case for sum on tuple of floats and ints','line_number':1074,'multiline':False]['text':' re-read a pending side effect?','line_number':1134,'multiline':False]['text':' We are going to be raising this tensor as grapharg. So, ensure','line_number':1173,'multiline':False]['text':' that we have real grad value instead of fake tensor value.','line_number':1174,'multiline':False]['text':' Walk through the inputs of the subgraph and find if we already','line_number':1175,'multiline':False]['text':' have the original tensor stored in the graphargs.','line_number':1176,'multiline':False]['text':' There is a rare edge case in which','line_number':1192,'multiline':False]['text':' we seem to get symbol mismatches','line_number':1193,'multiline':False]['text':' for jagged tensor comparison.','line_number':1194,'multiline':False]['text':' See PYTORCH_TEST_WITH_DYNAMO=1 python test/test_nestedtensor.py','line_number':1195,'multiline':False]['text':'   -k test_dropout_backward_layout_torch_jagged_cpu','line_number':1196,'multiline':False]['text':' We lazily update the grad on the example to its real state as tracked by fake tensor.','line_number':1204,'multiline':False]['text':' This allocation is fine - it is just a hint. It will not make it to runtime, but it coerces','line_number':1205,'multiline':False]['text':' the underlying value to always be correct.','line_number':1206,'multiline':False]['text':' TODO(voz): Make it work properly','line_number':1283,'multiline':False]['text':' Remove the old reference in tracked fakes - if we don't do this','line_number':1289,'multiline':False]['text':' new .data value size and shape differences will cause','line_number':1290,'multiline':False]['text':' tracked fakes to produce incorrect guards. This is sound because the TensorVariable','line_number':1291,'multiline':False]['text':' coming out of set_() below will be a new one, and get','line_number':1292,'multiline':False]['text':' installed in tracked fakes.','line_number':1293,'multiline':False]['text':' Step 1 - disable grads','line_number':1301,'multiline':False]['text':' Step 2 - call `set_`','line_number':1303,'multiline':False]['text':' Step 3 - drop the version counter - this is a step required to get','line_number':1313,'multiline':False]['text':' .data setting to play correctly with the autograd engine.','line_number':1314,'multiline':False]['text':' Esentially, dynamo is trying to faithful preserve the (absurd)','line_number':1315,'multiline':False]['text':' behavior of .data= from eager mode','line_number':1316,'multiline':False]['text':' This handles options prop, guards and ends with a clone','line_number':1331,'multiline':False]['text':' Step 4 - replace all reference to the current object with the new one','line_number':1332,'multiline':False]['text':' get_fake_val will get the same fake tensor','line_number':1357,'multiline':False]['text':' same tensor identiy, setattr is a no-op','line_number':1360,'multiline':False]['text':' FIXME (tmanlaibaatar) this is utter hack to unblock HuggingFace export','line_number':1369,'multiline':False]['text':' Export generally doesn't want to allow mutations on objects directly,','line_number':1370,'multiline':False]['text':' but we don't have good way to do this rn. For now, we make it an undefined','line_number':1371,'multiline':False]['text':' behaviour and just set attributes directly on the PretrainedConfig object','line_number':1372,'multiline':False]['text':' for now.','line_number':1373,'multiline':False]['text':' neg is a constant fold function, so we only get here if constant fold is not valid','line_number':1451,'multiline':False]['text':' None no-ops this handler and lets the driving function proceed','line_number':1459,'multiline':False]['text':' Note, we have a rare BaseListVariable subtype mismatch with valid comparison','line_number':1527,'multiline':False]['text':' x = torch.randn([3, 3])','line_number':1528,'multiline':False]['text':' x.size() == (3, 3) # True','line_number':1529,'multiline':False]['text':' (3, 3) == x.size() # True','line_number':1530,'multiline':False]['text':' Mismatch in BaseListVariable subclasses','line_number':1537,'multiline':False]['text':' Mismatch in BaseListVariable subclasses','line_number':1542,'multiline':False]['text':' not broadcastable, can't be compared','line_number':1568,'multiline':False]['text':' handle Ndarrays and Tensors','line_number':1572,'multiline':False]['text':' If the two objects are of different type, we can safely return False','line_number':1602,'multiline':False]['text':' Rely on constant_handler','line_number':1612,'multiline':False]['text':' None no-ops this handler and lets the driving function proceed','line_number':1625,'multiline':False]['text':' Rely on constant_handler','line_number':1629,'multiline':False]['text':' None no-ops this handler and lets the driving function proceed','line_number':1642,'multiline':False]