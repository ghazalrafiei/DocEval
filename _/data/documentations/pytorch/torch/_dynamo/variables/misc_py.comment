['text':' directly get attr from self.typevar if true','line_number':39,'multiline':False]['text':' We default to the python type of the object. However, if this is','line_number':56,'multiline':False]['text':' a `type` or subclass of `type`, then the original object represents','line_number':57,'multiline':False]['text':' the user defined type.','line_number':58,'multiline':False]['text':' Walk the mro tuple to find out the actual class where the','line_number':69,'multiline':False]['text':' attribute resides.','line_number':70,'multiline':False]['text':' Equivalent of something like type(L['self']).__mro__[1].attr_name','line_number':75,'multiline':False]['text':' TODO(jansel): there is a small chance this could trigger user code, prevent that','line_number':82,'multiline':False]['text':' Check if getattr is a constant. If not, delay the actual work by','line_number':86,'multiline':False]['text':' wrapping the result in GetAttrVariable. Mostly super is called with a','line_number':87,'multiline':False]['text':' method, so most of the work is delayed to call_function.','line_number':88,'multiline':False]['text':'','line_number':89,'multiline':False]['text':' We could have just implemented a const_getattr. However, super is','line_number':90,'multiline':False]['text':' special when it comes to finding sources. Compared to other VTs, super','line_number':91,'multiline':False]['text':' requires the attr name to walk the mro and find the actual source (and','line_number':92,'multiline':False]['text':' not just AttrSource).','line_number':93,'multiline':False]['text':' To support the comptime.print_graph convenience accessors','line_number':192,'multiline':False]['text':' TODO: support an expression form as well','line_number':204,'multiline':False]['text':' We have to manually bind the freevars ourselves','line_number':212,'multiline':False]['text':' We could automatically promote free variables into','line_number':223,'multiline':False]['text':' ComptimeVar but this is confusing if you access','line_number':224,'multiline':False]['text':' a free variable that we actually DO have the runtime','line_number':225,'multiline':False]['text':' value for','line_number':226,'multiline':False]['text':' tuple(make_cell(ComptimeVar(i)) for i in fn.closure.items)','line_number':227,'multiline':False]['text':' closure variable created by an inlined function','line_number':246,'multiline':False]['text':' Note - this is the same check used in autograd/function.py, except inverted.','line_number':351,'multiline':False]['text':' If we want to support functorch transforms here, we will need to enable this.','line_number':352,'multiline':False]['text':' type: ignore[attr-defined]','line_number':361,'multiline':False]['text':' type: ignore[attr-defined]','line_number':365,'multiline':False]['text':' NOTE [On Tracing autograd.Function w/ grad]','line_number':375,'multiline':False]['text':' The complex system described here revolves around the soundness evaluation of an autograd.Function in','line_number':376,'multiline':False]['text':' PyTorch. The system follows a well-defined strategy for tracing, which involves three key steps: tracing','line_number':377,'multiline':False]['text':' forward, tracing backward, and if both are sound the potential recording of an "apply" operation into the','line_number':378,'multiline':False]['text':' graph.We trace forward, and evaluate soundness. Soundness, in this context, refers to the absence of side','line_number':379,'multiline':False]['text':' effects, the avoidance of lifting new arguments into the trace, the production of a single tensor output,','line_number':380,'multiline':False]['text':' and a limited input scope confined to contexts, tensors, and constants. If the forward trace is sound,','line_number':381,'multiline':False]['text':' we install any guards accumulated from tracing. If not, we graph break. We trace backward, and evaluate','line_number':382,'multiline':False]['text':' for soundness, same as forward, except with more strictness. We enable a strict mode on the tx, and','line_number':383,'multiline':False]['text':' reject certain ops when running under this strict mode. If both the forward and backward traces are sound,','line_number':384,'multiline':False]['text':' we write the autograd functionâ€™s apply into the graph.','line_number':385,'multiline':False]['text':' For tracing forward and backward, we use UserFunctionVariable. Although it does not directly contribute','line_number':387,'multiline':False]['text':' to soundness evaluation, it plus a  GlobalSource makes sure we can produce valid guards,','line_number':388,'multiline':False]['text':' and that we can inline properly here. Inlining is required in order to be able to ensure that the','line_number':389,'multiline':False]['text':' soundness evaluation works as described above.','line_number':390,'multiline':False]['text':' If fwd and backward are sound, we want apply in the graph.','line_number':420,'multiline':False]['text':' We don't want backward because we are tracing forwards.','line_number':421,'multiline':False]['text':' Allowlist a few popular classes(e.g, collections.OrderedDict) calls in skip files.','line_number':751,'multiline':False]['text':' Fold through the functions(e.g, collections.namedtuple)','line_number':767,'multiline':False]['text':' that inputs & outputs are all python constants','line_number':768,'multiline':False]['text':' Default to operator.add','line_number':819,'multiline':False]['text':' noqa: TRY200','line_number':838,'multiline':False]['text':' noqa: TRY200','line_number':908,'multiline':False]['text':' The first arg, a callable (the ctor below will assert on types)','line_number':942,'multiline':False]['text':' guards for the produced FunctoolsPartialVariable are installed in FunctoolsPartialVariable ctor from the','line_number':945,'multiline':False]['text':' args and keywords','line_number':946,'multiline':False]['text':' some internal details do leak from tnp','line_number':1027,'multiline':False]['text':' which are not part of numpy API.','line_number':1028,'multiline':False]['text':' lookup method name in tnp. Things like np.dtype(float) are not supported yet.','line_number':1054,'multiline':False]['text':' We are dealing with a callable.','line_number':1059,'multiline':False]['text':' TODO(larryliu0820): currently assuming all numpy.* functions are returning a ndarray that can be','line_number':1075,'multiline':False]['text':'  wrapped by NumpyNdarrayVariable which is wrong!','line_number':1076,'multiline':False]['text':' this handles numpy dtype attribute such as np.float32. TODO(larryliu0820): we should split NumpyVariable','line_number':1100,'multiline':False]['text':'  into NumpyVariable for instances/objects and NumpyVariable for types.','line_number':1101,'multiline':False]['text':' retrieve attribute str. E.g., "float32" if given np.float32','line_number':1103,'multiline':False]['text':' get tnp equivalent','line_number':1106,'multiline':False]['text':' returning a string here because we are assuming all `dtype` kwargs for numpy','line_number':1108,'multiline':False]['text':' functions can take an equivalent string and the behavior of the function would','line_number':1109,'multiline':False]['text':' be the same as taking a numpy dtype.','line_number':1110,'multiline':False]['text':' Used to keep track of NULLs pushed on the stack for Python 3.11 function calls','line_number':1116,'multiline':False]