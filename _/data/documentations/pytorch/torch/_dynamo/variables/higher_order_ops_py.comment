['text':' A more read-able syntax sugar for creating a UserFunctionVariable for f','line_number':76,'multiline':False]['text':' and run call_function on it. Make it return a function to preserve the calling','line_number':77,'multiline':False]['text':' convention of the original f.','line_number':78,'multiline':False]['text':' Store the invocation as a call','line_number':97,'multiline':False]['text':' Transform variable back into a list (previously made into a tuple by','line_number':109,'multiline':False]['text':' speculate_subgraph function) so as to respect the pytree API typing.','line_number':110,'multiline':False]['text':' This arg is not used in the body of the higher order op.','line_number':151,'multiline':False]['text':' Currently, this new input is added to make the calls','line_number':152,'multiline':False]['text':' happy, which expect a fixed number of arguments. In','line_number':153,'multiline':False]['text':' future, we can clean this up.','line_number':154,'multiline':False]['text':' Weird special case, we probably want to delete it or fold it','line_number':157,'multiline':False]['text':' into the next case (of `a` being placeable into a graph)','line_number':158,'multiline':False]['text':' If `a` can be put into a graph','line_number':162,'multiline':False]['text':' If `a` cannot be put into a graph','line_number':175,'multiline':False]['text':' HOPs work much better if they use speculate_subgraph(manually_set_subgraph_inputs=False).','line_number':177,'multiline':False]['text':' See NOTE [HigherOrderOperator tracing design] for details of the design','line_number':186,'multiline':False]['text':' source_target is the .value of HigherOrderOpVariable and is the','line_number':194,'multiline':False]['text':' target of the proxy that we created for the higherOrderOperator.','line_number':195,'multiline':False]['text':' NOTE [Temporary argument `manually_set_subgraph_inputs`]','line_number':199,'multiline':False]['text':' If manually_set_subgraph_inputs=True, then we manually add','line_number':200,'multiline':False]['text':' the `sub_args` to `subgraph`, if False then we rely','line_number':201,'multiline':False]['text':' on tracer's lifting mechanism to lift these args.','line_number':202,'multiline':False]['text':' NOTE: Default `True` is temporary and plan is','line_number':203,'multiline':False]['text':'       to always lift args in future and remove this','line_number':204,'multiline':False]['text':'       argument.','line_number':205,'multiline':False]['text':' Pass in an originating tracer - this is needed for preserving context','line_number':209,'multiline':False]['text':' across fwd-bwd for autograd.Function','line_number':210,'multiline':False]['text':' See NOTE [Temporary argument `manually_set_subgraph_inputs`]','line_number':216,'multiline':False]['text':' ensure guards on args get installed in parent subgraph','line_number':224,'multiline':False]['text':' Captured variables are tracked in side-effects','line_number':254,'multiline':False]['text':' and they show up in output graph incorrectly.','line_number':255,'multiline':False]['text':' It is ok to undo this side-effect tracking','line_number':256,'multiline':False]['text':' as speculate_subgraph will allow only','line_number':257,'multiline':False]['text':' pure functions.','line_number':258,'multiline':False]['text':' Flatten the speculated subgraph output.','line_number':263,'multiline':False]['text':' Actually, transform the list (returned by flatten) into a tuple','line_number':267,'multiline':False]['text':' for dynamo consistency.','line_number':268,'multiline':False]['text':' Register output to graph','line_number':271,'multiline':False]['text':' Modeled off of compile_and_call_fx_graph','line_number':272,'multiline':False]['text':' TODO: support pytree output','line_number':273,'multiline':False]['text':' We check always_restore because we dont use the output or side effects of always_restore code,','line_number':274,'multiline':False]['text':' like bwd.','line_number':275,'multiline':False]['text':' Nothing left to do here','line_number':277,'multiline':False]['text':' The output proxies might not belong to this SubgraphTracer','line_number':287,'multiline':False]['text':' (if they are free variables that were never lifted)','line_number':288,'multiline':False]['text':' so lift them here.','line_number':289,'multiline':False]['text':' TODO(voz): Support fake tensor dispatch for recursive','line_number':434,'multiline':False]['text':' ops - see torch/dispatch/_dispatcher.py','line_number':435,'multiline':False]['text':' predicate','line_number':441,'multiline':False]['text':' operands','line_number':449,'multiline':False]['text':' branches','line_number':460,'multiline':False]['text':' true_fn','line_number':471,'multiline':False]['text':' false_fn','line_number':483,'multiline':False]['text':' Our strategy for tracing the true/false branches of cond','line_number':485,'multiline':False]['text':' are to checkpoint our graphstate, run the true branch,','line_number':486,'multiline':False]['text':' roll it back to the checkpoint, and run the false','line_number':487,'multiline':False]['text':' branch, and then merge the graphstates.  Well, perhaps','line_number':488,'multiline':False]['text':' "merge" is too strong a word: we mostly assert that','line_number':489,'multiline':False]['text':' the resulting graphstates have to be the same.','line_number':490,'multiline':False]['text':'','line_number':491,'multiline':False]['text':' We only permit guards to diverge (we union the guards from','line_number':492,'multiline':False]['text':' both branches).  In particular, this means that side','line_number':493,'multiline':False]['text':' effects are NOT permitted inside true/false branches; this','line_number':494,'multiline':False]['text':' would be difficult to implement, because of the path','line_number':495,'multiline':False]['text':' explosion problem.','line_number':496,'multiline':False]['text':' NB: 0 is predicate','line_number':499,'multiline':False]['text':' TODO: Support kwargs','line_number':501,'multiline':False]['text':' We check the meta data associated with meta["example_value"]','line_number':548,'multiline':False]['text':' The nn module attributes are guaranteed to be registered into the top-level graph module during','line_number':563,'multiline':False]['text':' higher order op speculation. Therefore, get_attr nodes in two branches with the same','line_number':564,'multiline':False]['text':' target refer to the same attribute and we can safely deduplicate them with their target.','line_number':565,'multiline':False]['text':'','line_number':566,'multiline':False]['text':' Note: ideally, dynamo should just create a single proxy for the same attribute of a nn module. But','line_number':567,'multiline':False]['text':' true_branch and false_branch belong to two separate tracing contexts, they may register the same','line_number':568,'multiline':False]['text':' attribute to top level seperately. This creates two get_attr proxies for the same attribute','line_number':569,'multiline':False]['text':' that have different meta data such as stack_trace (one stack trace for the true_branch,','line_number':570,'multiline':False]['text':' and the other for false_branch). It seems better to discard the proxy explicitly in cond','line_number':571,'multiline':False]['text':' than make dynamo create a single proxy for the same get_attr target.','line_number':572,'multiline':False]['text':' Let's say we capture cond(pred, true_fn, false_fn, (x,))','line_number':622,'multiline':False]['text':' With mannually_set_graph_input set to False,','line_number':623,'multiline':False]['text':' true_fn has lifted variables x, a, b, c','line_number':624,'multiline':False]['text':' false_fn has lifted variables x, a, b, d','line_number':625,'multiline':False]['text':' Then fixup_branch_inps make sure both branches have the same signature, i.e.:','line_number':626,'multiline':False]['text':' - true_fn(x, a, b, c_true_branch, d_false_branch)','line_number':627,'multiline':False]['text':' - false_fn(x, a, b, c_true_branch, d_false_branch)','line_number':628,'multiline':False]['text':'','line_number':629,'multiline':False]['text':' More formally, the signature has three parts in the following order:','line_number':630,'multiline':False]['text':' 1. used in both branches: x, a, b','line_number':631,'multiline':False]['text':' 2. only used in true branches: c, suffixed with _true_branch','line_number':632,'multiline':False]['text':' 3. only used in false branches: d, suffixed with _false_branch','line_number':633,'multiline':False]['text':' Within each part, we re-order the nodes by name to have a derterministic ordering for testing.','line_number':634,'multiline':False]['text':' Override with new_ph if there exists a old placeholder.','line_number':641,'multiline':False]['text':' replace_all_uses_with doesn't clean users. Clean it mannually so that we could erase it.','line_number':645,'multiline':False]['text':' We pick true_shared but it shouldn't matter','line_number':684,'multiline':False]['text':' To get the example output from map() we will need to provide at least one sample to','line_number':727,'multiline':False]['text':' the loop body. In our case we will always use xs[0], and our map() won't support zero','line_number':728,'multiline':False]['text':' sized tensor during tracing.','line_number':729,'multiline':False]['text':' TODO: Support kwargs','line_number':734,'multiline':False]['text':' right now we only supports num_mapped = 1','line_number':764,'multiline':False]['text':' This is operator for delegation within Executorch which calls a','line_number':779,'multiline':False]['text':' specific function in the given lowered module with the given','line_number':780,'multiline':False]['text':' operators. The actual operator is defined in the Executorch codebase.','line_number':781,'multiline':False]['text':' This is a bad hierarchical violation since','line_number':782,'multiline':False]['text':' executorch_call_delegate sits at a higher level than dynamo, but','line_number':783,'multiline':False]['text':' there's no real solution to this issue yet.','line_number':784,'multiline':False]['text':' NOTE [Guaranteeing the 1-1 correspondence of FakeTensors and real tensors]:','line_number':800,'multiline':False]['text':' executorch modules promise not to alias inputs and outputs.','line_number':801,'multiline':False]['text':' Thus, output FakeTensors will correctly not alias input FakeTensors.','line_number':802,'multiline':False]['text':' Store the invocation as a call','line_number':809,'multiline':False]['text':' TODO: Support `fn` with kwargs.','line_number':829,'multiline':False]['text':' [NOTE] Here we are (roughly) modelling the following','line_number':836,'multiline':False]['text':'','line_number':837,'multiline':False]['text':'   grad_fn = torch.func.grad(fn, argnums=.., has_aux=..)','line_number':838,'multiline':False]['text':'   grad_output = grad_fn(x)','line_number':839,'multiline':False]['text':' get arguments','line_number':842,'multiline':False]['text':' Since speculate_subgraph doesn't support kwargs, we can't handle this for now.','line_number':846,'multiline':False]['text':' Trace through the `func`','line_number':851,'multiline':False]['text':' NOTE [HACK: Enable autograd while tracing function]','line_number':852,'multiline':False]['text':' `torch.func.grad` should not be affected by `no_grad` outside of `grad`.','line_number':853,'multiline':False]['text':' So, we enable_grad right before the function to which `grad` is applied','line_number':854,'multiline':False]['text':' (the parts explicitly disabled with `no_grad` inside the function are still disabled).','line_number':855,'multiline':False]['text':' Eg.','line_number':856,'multiline':False]['text':' def f(x):','line_number':857,'multiline':False]['text':'     with no_grad():  # This will disable grad tracking under it.','line_number':858,'multiline':False]['text':'        y = x * 2','line_number':859,'multiline':False]['text':'','line_number':860,'multiline':False]['text':'     return x ** 2 - y  # grad tracking should be enabled irrespective of outside `no_grad`.','line_number':861,'multiline':False]['text':'','line_number':862,'multiline':False]['text':' with no_grad():  # This will not disable grad tracking inside of grad(f).','line_number':863,'multiline':False]['text':'     grad_o = torch.func.grad(f)(x)','line_number':864,'multiline':False]['text':' TODO: Support kwargs','line_number':865,'multiline':False]['text':' See NOTE [HACK: Enable autograd while tracing function]','line_number':873,'multiline':False]['text':' Model `grad_fn = grad(fn, *grad_args, **grad_kwargs)`','line_number':889,'multiline':False]['text':' Pass lifted freevars to the call to `grad_fn`','line_number':898,'multiline':False]['text':' Call grad_fn with inputs.','line_number':904,'multiline':False]['text':' grad_output = grad_fn(*grad_fn_args, **grad_fn_kwargs)','line_number':905,'multiline':False]['text':' `grad_fn(*grad_fn_args, **grad_fn_kwargs)`','line_number':908,'multiline':False]['text':' Output of grad_fn is','line_number':909,'multiline':False]['text':' For has_aux=False, Tuple[gradients of inputs indicated by argnums].','line_number':910,'multiline':False]['text':' For has_aux=True, Tuple[Tuple[gradients of inputs indicated by argnums], aux values]','line_number':911,'multiline':False]['text':' NOTE: example_value should match `grad_output`.','line_number':912,'multiline':False]['text':' case : has_aux = True','line_number':946,'multiline':False]['text':' NOTE: Currently speculate subgraph allows body_r to be','line_number':947,'multiline':False]['text':' Tensor or Tuple/List of Tensor.','line_number':948,'multiline':False]['text':' Since `grad` expects output with has_aux','line_number':949,'multiline':False]['text':' to be (output, aux), only valid output currently is','line_number':950,'multiline':False]['text':' (output, some_tensor)','line_number':951,'multiline':False]['text':' Call contiguous on all the computed grads.','line_number':958,'multiline':False]['text':' case: has_aux.value = True','line_number':971,'multiline':False]['text':' fx_proxy -> Tuple(grads, aux)','line_number':972,'multiline':False]['text':' unpack args','line_number':1005,'multiline':False]['text':' Trace into tree_flatten with the list of batch_input_args.','line_number':1029,'multiline':False]['text':' Transform in_dims into a list if it's not an integer literal.','line_number':1034,'multiline':False]['text':' Trace into _broadcast_to_and_flatten with the transformed in_dims.','line_number':1041,'multiline':False]['text':' We want to pass unbatched input to speculate subgraph.','line_number':1046,'multiline':False]['text':' So we loop through the inputs and select only one sample','line_number':1047,'multiline':False]['text':' from the batch.','line_number':1048,'multiline':False]['text':' Ban ops like `stride`, `storage_offset` in the traced functions.','line_number':1063,'multiline':False]['text':' NOTE: We are conservatively banning more ops (vmap should be able','line_number':1064,'multiline':False]['text':'       to handle a few of them).','line_number':1065,'multiline':False]['text':' trace through the function with unbatched inputs.','line_number':1067,'multiline':False]['text':' Returns a ListVariable, since that's where we started flattening.','line_number':1071,'multiline':False]['text':' However, we really want to pass the inner Python list as argument.','line_number':1072,'multiline':False]['text':' body_lifted_variable should not be treated as batched.','line_number':1089,'multiline':False]['text':' So here we update `in_dims` to reflect that.','line_number':1090,'multiline':False]['text':' NOTE: updated_in_dims is flat list, it is ok for now','line_number':1091,'multiline':False]['text':'       as speculate_subgraph does not supports functions with non-Tensor args.','line_number':1092,'multiline':False]['text':'       (so we graph-break above)','line_number':1093,'multiline':False]['text':' vmap_proxy corresponds to `vmap_proxy = vmap(fn, *vmap_args, **vmap_kwargs)`','line_number':1106,'multiline':False]['text':' We compute the example_value by actually calling','line_number':1119,'multiline':False]['text':' `vmap` with FakeTensors.','line_number':1120,'multiline':False]['text':' NOTE: `body_graph` might have operators which','line_number':1129,'multiline':False]['text':' will create new tensors. So it is required','line_number':1130,'multiline':False]['text':' that we run `vmap` under FakeMode.','line_number':1131,'multiline':False]['text':' proxy corresponds to `call = vmap_proxy(*batched_fn_args, **batched_fn_kwargs)`','line_number':1142,'multiline':False]['text':' The fwd_bwd_tracer is owned by AutogradFunctionVariable and passed','line_number':1156,'multiline':False]['text':' in for speculation. It allows us to share tracing information about proxies','line_number':1157,'multiline':False]['text':' across fwd bwd, such as when users stash tensors on a context.','line_number':1158,'multiline':False]['text':' TODO(jansel): BUG!!! we aren't copying on the line below, so the post-pre check below is pointless','line_number':1184,'multiline':False]['text':' In eager-mode PyTorch, if we only compute first-order gradients,','line_number':1186,'multiline':False]['text':' then the grad_mode is False during the backward pass.','line_number':1187,'multiline':False]['text':' torch.compile assumes that we only compute first-order gradients,','line_number':1188,'multiline':False]['text':' so we want to speculate the backward pass with the grad mode disabled.','line_number':1189,'multiline':False]['text':' TODO: Support kwargs','line_number':1194,'multiline':False]['text':' Backwards should never, ever be stored!','line_number':1208,'multiline':False]['text':' Nothing left to do here','line_number':1221,'multiline':False]['text':' don't add call module to parent graph if speculating forward','line_number':1224,'multiline':False]['text':' return the result directly','line_number':1225,'multiline':False]['text':' Store the invocation as a call','line_number':1239,'multiline':False]['text':' See NOTE [HigherOrderOperator tracing design] for more details','line_number':1254,'multiline':False]['text':' function','line_number':1262,'multiline':False]['text':' Since, we call `speculate_subgraph` with `manually_set_subgraph_inputs=False`,','line_number':1281,'multiline':False]['text':' all the arguments are lifted.','line_number':1282,'multiline':False]['text':' This flattens the kwargs into lifted args','line_number':1297,'multiline':False]['text':' This is a simplified implementation of this operator just for tracing.','line_number':1325,'multiline':False]['text':' Actual implementation may also first promote the arguments','line_number':1326,'multiline':False]['text':' Store the invocation as a call','line_number':1329,'multiline':False]['text':' Here we use checkpoint_kwargs (and not gmod kwargs). gmod_kwargs are','line_number':1356,'multiline':False]['text':' already flattened above and managed inside the fx graph.','line_number':1357,'multiline':False]['text':' Store the invocation as a call','line_number':1373,'multiline':False]['text':' Transform variable back into a list (previously made into a tuple by','line_number':1388,'multiline':False]['text':' speculate_subgraph function) so as to respect the pytree API typing.','line_number':1389,'multiline':False]