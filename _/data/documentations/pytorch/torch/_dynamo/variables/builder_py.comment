['text':' TODO: storing a SymInt here but not a FakeTensor is a pretty strange','line_number':173,'multiline':False]['text':' thing to do.  Probably should have example (which stores an int) and','line_number':174,'multiline':False]['text':' fake_example','line_number':175,'multiline':False]['text':' UnspecializedPythonVariable often masquerades as a tensor.','line_number':179,'multiline':False]['text':' We MUST NOT generate shape guard code','line_number':180,'multiline':False]['text':' that actually tries to access tensor properties on these values.','line_number':181,'multiline':False]['text':' is_tensor lets us tell if this graph arg actually is a tensor','line_number':182,'multiline':False]['text':' or not.','line_number':183,'multiline':False]['text':' Sometimes, the Tensor we pass to example is freshly allocated (smh).','line_number':185,'multiline':False]['text':' Then we cannot only keep a weak reference to it.  This lets you','line_number':186,'multiline':False]['text':' stash a strong reference too.','line_number':187,'multiline':False]['text':' We zero-one specialize shapes, so specialize these constants','line_number':265,'multiline':False]['text':' too','line_number':266,'multiline':False]['text':' NB: There used to be more constants here, but honestly it was','line_number':269,'multiline':False]['text':' pretty confusing.  Note we specialize floats by default, and','line_number':270,'multiline':False]['text':' DON'T specialize ints by default.  This all only matters with','line_number':271,'multiline':False]['text':' dynamic_shapes','line_number':272,'multiline':False]['text':' NB: Careful not to close over self to avoid ref cycle from lru_cache','line_number':296,'multiline':False]['text':' import here to avoid circular dependencies','line_number':370,'multiline':False]['text':' Handle exact type() match','line_number':384,'multiline':False]['text':' Handle exact id() match','line_number':389,'multiline':False]['text':' Note - There are some nested values where types mismatch!','line_number':394,'multiline':False]['text':' We want to get those out and wrap those.','line_number':395,'multiline':False]['text':' Everything else (NB: order matters!)','line_number':398,'multiline':False]['text':' For SUPPORTED_NODES, we guard on the dictionary version (PEP509)','line_number':407,'multiline':False]['text':' under the assumption that the values themselves don't change.','line_number':408,'multiline':False]['text':' It is faster to guard on 'false' property than to guard','line_number':429,'multiline':False]['text':' on actual dict keys, but we can't do this fast guard in general because','line_number':430,'multiline':False]['text':' it omits a crucial type check that ensures the value is actually still a dict at runtime.','line_number':431,'multiline':False]['text':' Why is this OK for (specialized) nnmodules? We set up a setattr hook','line_number':433,'multiline':False]['text':' to check for module property mutations, which does a reasonable,','line_number':434,'multiline':False]['text':' but not completely secure job ensuring a property wasn't changed.','line_number':435,'multiline':False]['text':' store key variables in global location for reconstruction','line_number':440,'multiline':False]['text':' non-atomic literals','line_number':472,'multiline':False]['text':' For frozenset, we can guard by object ID instead of value','line_number':477,'multiline':False]['text':' equality, this allows us to handle non-literal values','line_number':478,'multiline':False]['text':' typing.List, typing.Mapping, etc.','line_number':514,'multiline':False]['text':' numpy array scalars: convert to 0D arrays','line_number':521,'multiline':False]['text':' NB: These can't be put in type_dispatch, they have to run later','line_number':531,'multiline':False]['text':' handle aliased autograd function `apply` calls','line_number':571,'multiline':False]['text':' TODO: this doing it manually is bad','line_number':624,'multiline':False]['text':' TODO: see if we need to add custom guard instead of a simple ID_MATCH','line_number':633,'multiline':False]['text':' TODO: see if we need to add custom guard instead of a simple ID_MATCH','line_number':637,'multiline':False]['text':' TODO: see if we need to add custom guard instead of a simple ID_MATCH','line_number':641,'multiline':False]['text':' Note: the idea here is to re-use the infra we've built for SymInt by simulating the','line_number':648,'multiline':False]['text':' user provided SymBool with a SymInt in dynamo.','line_number':649,'multiline':False]['text':' Concretely,','line_number':651,'multiline':False]['text':' 1. We create a SymInt in dynamo's shape_env, whose source is constructed as ConvertIntSource(self.source).','line_number':652,'multiline':False]['text':' so that guards on the SymInts can be effectively applied on the original SymBool in user program.','line_number':653,'multiline':False]['text':' 2. We create a SymBool based on the SymInt in dynamo's ShapeEnv. Because the original user program','line_number':654,'multiline':False]['text':' depends on the value being a SymBool. This allows dynamo to interpret the user's program correctly.','line_number':655,'multiline':False]['text':' No kernel idx provided','line_number':692,'multiline':False]['text':' No grid provided','line_number':693,'multiline':False]['text':' don't let MethodTypes fall through to UserDefinedObject,','line_number':746,'multiline':False]['text':' which doesn't support 'CALL_FUNCTION'','line_number':747,'multiline':False]['text':' TODO(whc): Why do we limit this to methods on NNModules?','line_number':749,'multiline':False]['text':' I don't have a good reason for this, but it preserves the existing behavior','line_number':750,'multiline':False]['text':' for MBartForConditionalGeneration, which generates many graph breaks and OOMs otherwise.','line_number':751,'multiline':False]['text':' I suspect we probably want to relax this check and dig deeper there.','line_number':752,'multiline':False]['text':' In order to construct a MethodVariable in Dynamo, we start with an actual method obj from python,','line_number':754,'multiline':False]['text':' but need to separately wrap its underlying `__func__` and its `self` argument.  We wrap `self` here','line_number':755,'multiline':False]['text':' and then `__func__` gets wrapped inside UserMethodVariable.','line_number':756,'multiline':False]['text':' don't allow STORE_ATTR mutation with custom __setattr__','line_number':800,'multiline':False]['text':' only allow Parameter and another specific Tensor can be used as dict key','line_number':805,'multiline':False]['text':' One can index a tensor with a list/tuple. Therefore, we need to','line_number':834,'multiline':False]['text':' have a stricter match.','line_number':835,'multiline':False]['text':' created dynamically, don't specialize on it','line_number':894,'multiline':False]['text':' don't allow STORE_ATTR mutation with custom __setattr__','line_number':898,'multiline':False]['text':' See note [Dynamo treats FSDP wrapped modules as UnspecializedNNModule]','line_number':907,'multiline':False]['text':' in fully_sharded_data_parallel.py for more information','line_number':908,'multiline':False]['text':' we can't do this assert inside FSDP constructor,','line_number':910,'multiline':False]['text':' since we don't know yet whether dynamo will be used','line_number':911,'multiline':False]['text':' Note on FSDP guarding','line_number':916,'multiline':False]['text':' 1. We expect FSDP wrapping mutates an nn module irreversably (no way to de-wrap).','line_number':917,'multiline':False]['text':' 2. Eager FSDP already assumes (requires, but without enforcement) that users don't mutate their','line_number':918,'multiline':False]['text':'    model parameters/structure after FSDP wrapping, because FSDP wouldn't notice or update its FlatParams.','line_number':919,'multiline':False]['text':'','line_number':920,'multiline':False]['text':' Due to (1), once we enter this path we expect not to go back nor have to guard on type','line_number':921,'multiline':False]['text':' or _is_fsdp_managed_module.','line_number':922,'multiline':False]['text':'','line_number':923,'multiline':False]['text':' TODO(whc) We could add a guard on the opposite case, where a user compiled/ran','line_number':924,'multiline':False]['text':' pre-FSDP-wrapped model, then wrapped, to ensure that we recompile with the FSDP handling.','line_number':925,'multiline':False]['text':'','line_number':926,'multiline':False]['text':' Due to (2), we skip guards on inner contents of fsdp_managed modules, by using FSDPNNModuleSource as the','line_number':927,'multiline':False]['text':' guard source.  This behavior is gated on config.skip_fsdp_guards.','line_number':928,'multiline':False]['text':'','line_number':929,'multiline':False]['text':' ID_MATCH is required to disambiguate cases as simple as a unit test that constructs 2 models and wraps','line_number':930,'multiline':False]['text':' them differently with different FSDP configs.  (test_dynamo_distributed.py -k test_fsdp_aot_eager)','line_number':931,'multiline':False]['text':' Guards are added inside register_attr_or_module','line_number':939,'multiline':False]['text':' unspecializing int by default, but still','line_number':953,'multiline':False]['text':' specialize for the following conditions','line_number':954,'multiline':False]['text':' Assume integers from global variables want to be specialized','line_number':957,'multiline':False]['text':' Assume that integers that came from NN modules want to be','line_number':959,'multiline':False]['text':' specialized (as we don't expect users to be changing the','line_number':960,'multiline':False]['text':' NN modules on the fly)','line_number':961,'multiline':False]['text':' We cannot already be tracking the tensor, which implies','line_number':982,'multiline':False]['text':' it would have already been wrapped','line_number':983,'multiline':False]['text':' Guards are added inside register_attr_or_module','line_number':1001,'multiline':False]['text':' Ordinarily, we would fakeify a tensor so that it can get dynamic','line_number':1005,'multiline':False]['text':' shapes and be computed on without triggering actual operations.','line_number':1006,'multiline':False]['text':' However, how can we fakeify a tensor subclass?  Ordinary','line_number':1007,'multiline':False]['text':' inheritance (nor multiple inheritance) won't work work.','line_number':1008,'multiline':False]['text':'','line_number':1009,'multiline':False]['text':' Instead, our plan is to *manually simulate* the tensor subclass','line_number':1010,'multiline':False]['text':' inheriting from a fake tensor with dynamo.  This means our','line_number':1011,'multiline':False]['text':' data representation for a tensor subclass will be a fake tensor','line_number':1012,'multiline':False]['text':' + tensor subclass type + any extra data the subclass may have','line_number':1013,'multiline':False]['text':' been storing on the tensor.  Because all Python accesses are','line_number':1014,'multiline':False]['text':' mediated through TensorWithTFOverrideVariable, we can ensure','line_number':1015,'multiline':False]['text':' that we dispatch differently, e.g., according to','line_number':1016,'multiline':False]['text':' __torch_function__','line_number':1017,'multiline':False]['text':'','line_number':1018,'multiline':False]['text':' To simplify things for now, the __dict__ tracking bits haven't','line_number':1019,'multiline':False]['text':' been implemented yet, but they can be added into this design at','line_number':1020,'multiline':False]['text':' a later point in time.','line_number':1021,'multiline':False]['text':' NB: this just says we accessed a tensor from the same source again','line_number':1032,'multiline':False]['text':' (e.g., a tensor lives in a global foo, and we LOAD_GLOBAL it twice).','line_number':1033,'multiline':False]['text':' This is distinct from two distinct sources mapping to the same','line_number':1034,'multiline':False]['text':' Tensor (per id())!  No guard is necessary here.  See below for the','line_number':1035,'multiline':False]['text':' other case.','line_number':1036,'multiline':False]['text':' By this point, we should have deduplicated all tensors','line_number':1041,'multiline':False]['text':' tx.output has multiple tracers if we're introspecting HigherOrderOperator.','line_number':1044,'multiline':False]['text':' When we've discovered an untracked tensor, then we actually need','line_number':1045,'multiline':False]['text':' to get Dynamo to track the tensor (which is what this function does)','line_number':1046,'multiline':False]['text':' and put it as a graph input on the root tracer. Later on,','line_number':1047,'multiline':False]['text':' if the input is actually used in the body of the HigherOrderOperator,','line_number':1048,'multiline':False]['text':' then the relevant SubgraphTracer will lift it to being an input of','line_number':1049,'multiline':False]['text':' the subgraph.','line_number':1050,'multiline':False]['text':' See NOTE [HigherOrderOperator tracing design] for more details.','line_number':1051,'multiline':False]['text':' install guards for subclass inner tensors','line_number':1089,'multiline':False]['text':' Note: this information is conveyed via subclass_type now','line_number':1101,'multiline':False]['text':' failed to convert to tensor, graph break','line_number':1130,'multiline':False]['text':' We do this because we want the full behavior of guarding the numpy ndarray as if it were','line_number':1133,'multiline':False]['text':' a tensor. It's a little annoying to make a VT to throw out, but there's so many side effects here','line_number':1134,'multiline':False]['text':' that there's not another great way to do this atm.','line_number':1135,'multiline':False]['text':' This creates the right graphargs, as well as registration for guards in tensor names and shape env.','line_number':1136,'multiline':False]['text':' is_unspecialized should be true because we are wrapping a np.ndarray as argument input, and it needs to be','line_number':1153,'multiline':False]['text':' converted to a tensor.','line_number':1154,'multiline':False]['text':' NB: We do not do float.  For motivation, see','line_number':1182,'multiline':False]['text':' https://docs.google.com/document/d/1INSCdYu1PxXcr43HrD82OudeEuS-qxQe1yZmLg2wy6A/edit','line_number':1183,'multiline':False]['text':' but the general idea is that we generate kernels that can','line_number':1184,'multiline':False]['text':' take unspecialized floats and use them in sizevar computation','line_number':1185,'multiline':False]['text':' If specialize_int is False, also return','line_number':1192,'multiline':False]['text':' a constant (but this should have been handled','line_number':1193,'multiline':False]['text':' in the caller, TBH)','line_number':1194,'multiline':False]['text':' Note - this essentially means that if this name gets reused as a tensor,','line_number':1200,'multiline':False]['text':' it will start fully dynamic. That should always be a safe option, and not awfully inefficient.','line_number':1201,'multiline':False]['text':' Alternatively, if we want to improve pef here, we can add a third state of unset, but I am not','line_number':1202,'multiline':False]['text':' sure that is necessary for now.','line_number':1203,'multiline':False]['text':' TODO: This should be dynamic, as we in general do not','line_number':1217,'multiline':False]['text':' know if bare integers are actually going to be sizevars','line_number':1218,'multiline':False]['text':' and it is inappropriate to eagerly duck size them with','line_number':1219,'multiline':False]['text':' real sizevars','line_number':1220,'multiline':False]['text':' assume_static_by_default','line_number':1225,'multiline':False]['text':' TODO: dynamic_dim = DimDynamic.STATIC should work but','line_number':1226,'multiline':False]['text':' for some reason it doesn't','line_number':1227,'multiline':False]['text':' Note: Unfortunate split due to some gross classes existing that subclass TensorVariable','line_number':1327,'multiline':False]['text':' Should be compositional instead','line_number':1328,'multiline':False]['text':'','line_number':1329,'multiline':False]['text':' This is a horribly complicated function that does too many things, to','line_number':1330,'multiline':False]['text':' explain what it does, let's first talk about the classic usage wrap_fx_proxy','line_number':1331,'multiline':False]['text':' for a TensorVariable.  There are two primary modes of use:','line_number':1332,'multiline':False]['text':'','line_number':1333,'multiline':False]['text':'   1. Wrapping a pre-existing Tensor.  In this case, example_value is set','line_number':1334,'multiline':False]['text':'      to the pre-existing Tensor.  (Note that this example_value will NOT','line_number':1335,'multiline':False]['text':'      be the final example_value we put into node.meta['example_value'],','line_number':1336,'multiline':False]['text':'      instead it is converted into a fake tensor using','line_number':1337,'multiline':False]['text':'      wrap_to_fake_tensor_and_record and registered as a graph input.)','line_number':1338,'multiline':False]['text':'','line_number':1339,'multiline':False]['text':'   2. "Wrapping" the result of some Tensor operation Dynamo traced over. In','line_number':1340,'multiline':False]['text':'      this case, example_value is None (and we are going to figure it out','line_number':1341,'multiline':False]['text':'      ourselves using FakeTensors, via get_fake_value, which will run','line_number':1342,'multiline':False]['text':'      the operation represented by the (singular!) FX node referenced by','line_number':1343,'multiline':False]['text':'      the passed in proxy.)','line_number':1344,'multiline':False]['text':'','line_number':1345,'multiline':False]['text':' The expectation is you end up with a Tensor output, and everything is','line_number':1346,'multiline':False]['text':' straightforwardly traced into the graph.','line_number':1347,'multiline':False]['text':'','line_number':1348,'multiline':False]['text':' In all cases, the returned `TensorVariable` subclass will have an `example_value`','line_number':1349,'multiline':False]['text':' and that `example_value` must be a `FakeTensor` produced by the currently running','line_number':1350,'multiline':False]['text':' instance of Dynamo.','line_number':1351,'multiline':False]['text':'','line_number':1352,'multiline':False]['text':' Upon closer inspection, you may notice that there are a slurry of non-Tensor','line_number':1353,'multiline':False]['text':' output cases.  What gives?  Well, we sometimes trace operations into the','line_number':1354,'multiline':False]['text':' graph that don't involve tensors.','line_number':1355,'multiline':False]['text':'','line_number':1356,'multiline':False]['text':'   * Some operators return tuples; we need to recursively handle their','line_number':1357,'multiline':False]['text':'     contents','line_number':1358,'multiline':False]['text':'','line_number':1359,'multiline':False]['text':'   * Some operators have side effects that will affect subsequent AOTAutograd','line_number':1360,'multiline':False]['text':'     tracing but don't otherwise return anything.','line_number':1361,'multiline':False]['text':'','line_number':1362,'multiline':False]['text':'   * Some operators return symbolic ints/floats/bools which can go in the','line_number':1363,'multiline':False]['text':'     graph and be traced (but only if they're actually symbolic!  If they're','line_number':1364,'multiline':False]['text':'     static you don't want to put them in the graph, which means you','line_number':1365,'multiline':False]['text':'     shouldn't call this function.)','line_number':1366,'multiline':False]['text':'','line_number':1367,'multiline':False]['text':' The common theme is that you only use this function WHEN YOU ARE TRACING','line_number':1368,'multiline':False]['text':' SOMETHING INTO THE GRAPH.  This is sort of obvious, because you can't call','line_number':1369,'multiline':False]['text':' this function without a proxy.','line_number':1370,'multiline':False]['text':' tensor subclasses will not be converted to FakeTensors and need to be cloned','line_number':1386,'multiline':False]['text':' Is functional tensor fakeified by this instance of Dynamo','line_number':1390,'multiline':False]['text':' NB: ensure strides are preserved','line_number':1396,'multiline':False]['text':' only allow_non_graph_fake in this instance because we handle the non-fake','line_number':1403,'multiline':False]['text':' cases properly below.','line_number':1404,'multiline':False]['text':' Handle recursive calls here','line_number':1407,'multiline':False]['text':' The legacy behavior for real value cache with subclasses was','line_number':1413,'multiline':False]['text':' to perform a clone WITHOUT preserving the subclass.  It's','line_number':1414,'multiline':False]['text':' not entirely clear this is what you actually want though.','line_number':1415,'multiline':False]['text':' NB: If we're ignoring subclass, then the expectation is you will','line_number':1420,'multiline':False]['text':' take the returned TensorVariable and wrap it into a more','line_number':1421,'multiline':False]['text':' accurate TensorVariable that is able to track subclass-ness;','line_number':1422,'multiline':False]['text':' otherwise this is wrong!','line_number':1423,'multiline':False]['text':' NB: In most (all?) cases, this does not actually do a clone.','line_number':1449,'multiline':False]['text':' (WARNING: this means that if we mutate metadata on the fake','line_number':1450,'multiline':False]['text':' tensor, the stored example value will update too!)','line_number':1451,'multiline':False]['text':' TODO: not sure about this fake mode test','line_number':1455,'multiline':False]['text':' nn.MultiheadAttention() can return None, see issue #175','line_number':1491,'multiline':False]['text':' NB: Keep the old proxy around.  See SizeVariable for an','line_number':1508,'multiline':False]['text':' explanation why','line_number':1509,'multiline':False]['text':' some mac builds are missing torch.distributed.get_rank()','line_number':1564,'multiline':False]['text':' This always wants to be in the graph, even if the constraint','line_number':1567,'multiline':False]['text':' results in a constant int','line_number':1568,'multiline':False]['text':' Tracks the sources of all fake tensors we wrap in Dynamo.','line_number':1581,'multiline':False]['text':' Used by shape guard computation.','line_number':1582,'multiline':False]['text':' Is None when fake is SymInt','line_number':1587,'multiline':False]['text':' Performs automatic dynamic dim determination.','line_number':1599,'multiline':False]['text':' Returns a SymbolicContext','line_number':1600,'multiline':False]['text':' Get symbolic context for outer tensor','line_number':1611,'multiline':False]['text':' Get symbolic contexts for inner tensors','line_number':1616,'multiline':False]['text':' mapping from attr -> symbolic context','line_number':1618,'multiline':False]['text':' We preserve the dynamism of inputs. For example, when users call','line_number':1643,'multiline':False]['text':' make_fx(torch.cond, tracing_mode="symbolic")(*args), inputs have SymInt sizes.','line_number':1644,'multiline':False]['text':' Prep for automatic dynamic','line_number':1658,'multiline':False]['text':' If there is no entry for this source, add the tensor to frame state with its current static size.','line_number':1661,'multiline':False]['text':' E.g., {} -> {"x": [2, 4]}','line_number':1662,'multiline':False]['text':' If there is already an entry, and the dim mismatches, replace the frame state entry with None.','line_number':1669,'multiline':False]['text':' E.g. {"x": [2, 3, 4]} -> {"x": None}','line_number':1670,'multiline':False]['text':' If there is already an entry, and the dim matches, for every size in the frame state which','line_number':1679,'multiline':False]['text':' disagrees with the current static size, replace it with None. E.g., {"x": [2, 3]} -> {"x": [2, None]}','line_number':1680,'multiline':False]['text':' TODO: index export_constraints ahead of time so we don't have to','line_number':1692,'multiline':False]['text':' do a linear scan every time here','line_number':1693,'multiline':False]['text':' We process constraint ranges for each shared dimension separately','line_number':1722,'multiline':False]['text':' so that we can directly check range constraint violations on them','line_number':1723,'multiline':False]['text':' without looking up which other shared dimensions have this info.','line_number':1724,'multiline':False]['text':' In other words, for this t_id, we will have processed all of its','line_number':1725,'multiline':False]['text':' constraint ranges, no matter where / how they were specified, by','line_number':1726,'multiline':False]['text':' by the end of this loop.','line_number':1727,'multiline':False]['text':' NB: mark dynamic has precedence over static','line_number':1737,'multiline':False]['text':' NB: both static and dynamic have precedence over','line_number':1742,'multiline':False]['text':' Reflect the user directive in the frame_state','line_number':1747,'multiline':False]['text':' For dynamic, apply None always','line_number':1748,'multiline':False]['text':' We will process constraints first, as they will imply that we','line_number':1753,'multiline':False]['text':' have a dynamic dimension','line_number':1754,'multiline':False]['text':' Precedence: export constraints > eager constraints','line_number':1755,'multiline':False]['text':' Now, figure out if the dim is dynamic/duck/static','line_number':1771,'multiline':False]['text':' NB: We could assert static_shapes is False here, but it','line_number':1778,'multiline':False]['text':' seems better to allow the user to override symbolic_context in this','line_number':1779,'multiline':False]['text':' case','line_number':1780,'multiline':False]['text':' See note [Tensor Fakification and Symbol Caching]','line_number':1799,'multiline':False]['text':' Parent contexts are passed in when we are recursively creating','line_number':1816,'multiline':False]['text':' fake tensors for subclasses. A better design would be not to create a','line_number':1817,'multiline':False]['text':' parent/child relationship, but to recursively call _automatic_dynamic','line_number':1818,'multiline':False]['text':' as we recursively call wrap_to_fake_tensor_and_record. This runs','line_number':1819,'multiline':False]['text':' into bugs around how meta_utils knows and works to create fake tensors','line_number':1820,'multiline':False]['text':' with tensor subclasses. Ideally, dynamo would drive both the recursive','line_number':1821,'multiline':False]['text':' wrap_to_fake_tensor_and_record and _automatic_dynamic policy creation.','line_number':1822,'multiline':False]['text':' This is always valid to call, and useful for recursive calls.','line_number':1887,'multiline':False]