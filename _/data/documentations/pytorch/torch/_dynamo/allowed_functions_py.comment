['text':' lazy init','line_number':79,'multiline':False]['text':' lazy init','line_number':84,'multiline':False]['text':' extract all dtypes from torch','line_number':131,'multiline':False]['text':' Distributed APIs don't work well with torch.compile.','line_number':143,'multiline':False]['text':' Helper function to dump the torch name rule map generated based on','line_number':152,'multiline':False]['text':' the heuristic defined in gen_allowed_objs_and_ids.','line_number':153,'multiline':False]['text':' Add obj to ctx_mamager_classes set if it's a torch context manager class.','line_number':189,'multiline':False]['text':' This is used to generate the ctx manager class list based on heuristic.','line_number':190,'multiline':False]['text':' In some platforms, these functions were loaded as classes instead of functions.','line_number':202,'multiline':False]['text':' To mitigate these weired cases, we need this special check.','line_number':203,'multiline':False]['text':' Add obj to c_binding_in_graph_functions set or non_c_binding_in_graph_functions set','line_number':210,'multiline':False]['text':' if it's a torch function or method.','line_number':211,'multiline':False]['text':' This is used to generate the in graph function list based on heuristic.','line_number':212,'multiline':False]['text':' torch.nn.modules.rnn is disallowed because these modules internally','line_number':243,'multiline':False]['text':' flatten their parameters.  This flattening process will call','line_number':244,'multiline':False]['text':' Tensor.set_ with a Storage, and Storages cannot be traced with','line_number':245,'multiline':False]['text':' AOTAutograd; so we need to graph-break. To ensure this, we inline','line_number':246,'multiline':False]['text':' these functions, rather than keep them opaque-ly in the graph.','line_number':247,'multiline':False]['text':' Dynamo allows all builtins into the graph and does not attempt','line_number':337,'multiline':False]['text':' to introspect into them. We don't want to allow instances of','line_number':338,'multiline':False]['text':' HigherOrderOperator into the graph all the time (Dynamo needs','line_number':339,'multiline':False]['text':' to introspect the body functions of these HigherOrderOperator','line_number':340,'multiline':False]['text':' first, decide they are safe, and then allow them into the graph).','line_number':341,'multiline':False]['text':' So we exclude HigherOrderOperator from being a builtin.','line_number':342,'multiline':False]['text':' We want to trace through `grad` and `vmap`','line_number':348,'multiline':False]['text':' torch.Tensor.{fn}','line_number':392,'multiline':False]['text':' If the module is already imported, eagerly run init','line_number':487,'multiline':False]['text':' Module is not yet imported, delay processing until needed','line_number':492,'multiline':False]['text':' torch.ops is populated lazily so we don't necessarily have them in','line_number':519,'multiline':False]['text':' _allowed_function_ids.  Figure it out by testing the type instead','line_number':520,'multiline':False]['text':' in those cases','line_number':521,'multiline':False]