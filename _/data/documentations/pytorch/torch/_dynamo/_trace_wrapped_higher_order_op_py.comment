['text':' trace_wrapped(*args, fn) is equivalent to fn(*args), but with a twist:','line_number':14,'multiline':False]['text':' if you make_fx trace through this call, we will not actually trace into fn; instead,','line_number':15,'multiline':False]['text':' we will directly insert it as a call_function to fn in the graph.','line_number':16,'multiline':False]['text':' (Unlike make_fx, Dynamo WILL inline into fn.)','line_number':17,'multiline':False]['text':' You can think of this as a one off allow_in_graph equivalent for proxy tensor tracing.','line_number':18,'multiline':False]['text':'','line_number':19,'multiline':False]['text':' Because proxy tensor tracing does not actually run the function, there are','line_number':20,'multiline':False]['text':' requirements on the behavior of fn. We are still figuring it out, but here is the current state:','line_number':21,'multiline':False]['text':'','line_number':22,'multiline':False]['text':' 1) fn SHOULD only take a single argument, which must be a tensor','line_number':23,'multiline':False]['text':' 2) fn MUST return a new tensor with the same metadata as the original tensor','line_number':24,'multiline':False]['text':'    (e.g., zeros_like(input) is a permissible implementation of fn).','line_number':25,'multiline':False]['text':'    This is verified via an extra assert that is inserted into the traced graph.','line_number':26,'multiline':False]['text':' 3) fn MAY have side effects, but it MAY NOT perform metadata mutation on other tensors','line_number':27,'multiline':False]['text':'    participating in proxy tensor tracing (it MAY mutate other tensors, it MAY mutate Python state)','line_number':28,'multiline':False]['text':' These requirements stem from the requirement that we need to continue performing proxy tensor tracing,','line_number':29,'multiline':False]['text':' which assumes accurate fake tensor metadata, without actually running fn.','line_number':30,'multiline':False]['text':' In the future, we may allow for a "meta" function associated with fn to allow for more interesting input-output patterns.','line_number':31,'multiline':False]['text':'','line_number':32,'multiline':False]['text':' Note that tensors / Python state are allowed to be mutated.','line_number':33,'multiline':False]['text':' This is relaxed constraint is not always sound, but it is sound for backward tracing with fake','line_number':34,'multiline':False]['text':' tensors as it takes place in AOTAutograd, as the backward pass is guaranteed not to depend on concrete','line_number':35,'multiline':False]['text':' tensor values (via fake tensor) or Python state (because the autograd engine doesn't depend on Python).','line_number':36,'multiline':False]['text':'','line_number':37,'multiline':False]['text':' The intended use case for this function is to allow AOTAutograd to defer complex','line_number':38,'multiline':False]['text':' backward hooks to compiled autograd. AOTAutograd performs a make_fx trace which preserves','line_number':39,'multiline':False]['text':' the function call as is in the graph, and only when we Dynamo through the backward graph in','line_number':40,'multiline':False]['text':' compiled autograd do we inline into the function.','line_number':41,'multiline':False]['text':' We have a little shortcut here, wherein we DO NOT yet run a meta func, and so','line_number':76,'multiline':False]['text':' we take on an assumption that input and output meta matches. As such, we must introduce','line_number':77,'multiline':False]['text':' a runtime assert','line_number':78,'multiline':False]