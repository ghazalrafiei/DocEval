['text':' to configure logging for dynamo, aot, and inductor','line_number':11,'multiline':False]['text':' use the following API in the torch._logging module','line_number':12,'multiline':False]['text':' torch._logging.set_logs(dynamo=<level>, aot=<level>, inductor<level>)','line_number':13,'multiline':False]['text':' or use the environment variable TORCH_LOGS="dynamo,aot,inductor" (use a prefix + to indicate higher verbosity)','line_number':14,'multiline':False]['text':' see this design doc for more detailed info','line_number':15,'multiline':False]['text':' Design doc: https://docs.google.com/document/d/1ZRfTWKa8eaPq1AxaiHrq4ASTPouzzlPiuquSBEJYwS8/edit#','line_number':16,'multiline':False]['text':' the name of a file to write the logs to','line_number':17,'multiline':False]['text':' [@compile_ignored: debug]','line_number':18,'multiline':False]['text':' [@compile_ignored: debug] Verbose will print full stack traces on warnings and errors','line_number':21,'multiline':False]['text':' [@compile_ignored: runtime_behaviour] verify the correctness of optimized backend','line_number':24,'multiline':False]['text':' need this many ops to create an FX graph','line_number':27,'multiline':False]['text':' turn on/off DCE pass','line_number':30,'multiline':False]['text':' disable (for a function) when cache reaches this size','line_number':33,'multiline':False]['text':' controls the maximum number of cache entries with a guard on same ID_MATCH'd','line_number':35,'multiline':False]['text':' object. It also controls the maximum size of cache entries if they don't have','line_number':36,'multiline':False]['text':' any ID_MATCH'd guards.','line_number':37,'multiline':False]['text':' [@compile_ignored: runtime_behaviour]','line_number':38,'multiline':False]['text':' [@compile_ignored: runtime_behaviour] controls the maximum number of entries for a code object.','line_number':41,'multiline':False]['text':' whether or not to specialize on int inputs.  This only has an effect with','line_number':44,'multiline':False]['text':' dynamic_shapes; when dynamic_shapes is False, we ALWAYS specialize on int','line_number':45,'multiline':False]['text':' inputs.  Note that assume_static_by_default will also cause ints to get','line_number':46,'multiline':False]['text':' specialized, so this is mostly useful for export, where we want inputs','line_number':47,'multiline':False]['text':' to be dynamic, but accesses to ints should NOT get promoted into inputs.','line_number':48,'multiline':False]['text':' legacy config, does nothing now!','line_number':51,'multiline':False]['text':' This is a temporarily flag, which changes the behavior of dynamic_shapes=True.','line_number':54,'multiline':False]['text':' When assume_static_by_default is True, we only allocate symbols for shapes marked dynamic via mark_dynamic.','line_number':55,'multiline':False]['text':' NOTE - this flag can be removed once we can run dynamic_shapes=False w/ the mark_dynamic API','line_number':56,'multiline':False]['text':' see [Note - on the state of mark_dynamic]','line_number':57,'multiline':False]['text':' This flag changes how dynamic_shapes=True works, and is meant to be used in conjunction','line_number':60,'multiline':False]['text':' with assume_static_by_default=True.','line_number':61,'multiline':False]['text':' With this flag enabled, we always compile a frame as fully static for the first time, and, if we fail','line_number':62,'multiline':False]['text':' any guards due to wobbles in shape, we recompile with *all* the wobbled shapes as being marked dynamic.','line_number':63,'multiline':False]['text':' This flag changes how the shapes of parameters are treated.','line_number':66,'multiline':False]['text':' If this flag is set to True, then the shapes of torch.nn.Parameter as well as of torch.Tensor are attempted to be dynamic','line_number':67,'multiline':False]['text':' If this flag is set to False, then the shapes of torch.nn.Parameter are assumed to be static,','line_number':68,'multiline':False]['text':' while the shapes of torch.Tensor are assumed to be dynamic.','line_number':69,'multiline':False]['text':' This flag ensures that the shapes of a nn module are always assumed to be static','line_number':72,'multiline':False]['text':' If the flag is set to True, then the shapes of a nn.module are assumed to be static','line_number':73,'multiline':False]['text':' If the flag is set to False, then the shapes of a nn.module can be dynamic','line_number':74,'multiline':False]['text':' Typically, if you mark_dynamic a dimension, we will error if the dimension','line_number':77,'multiline':False]['text':' actually ended up getting specialized.  This knob changes the behavior so','line_number':78,'multiline':False]['text':' that we don't error at all.  This is helpful for our CI where I'm using a','line_number':79,'multiline':False]['text':' heuristic to mark batch dimensions as dynamic and the heuristic may get it','line_number':80,'multiline':False]['text':' wrong.','line_number':81,'multiline':False]['text':' Set this to False to assume nn.Modules() contents are immutable (similar assumption as freezing)','line_number':84,'multiline':False]['text':' Uses CPython internal dictionary tags to detect mutation. There is some','line_number':87,'multiline':False]['text':' overlap between guard_nn_modules_using_dict_tags and guard_nn_modules flag.','line_number':88,'multiline':False]['text':' guard_nn_modules unspecializes the nn module instance and adds guard for each','line_number':89,'multiline':False]['text':' relevant member of the nn modules. On the other hand,','line_number':90,'multiline':False]['text':' guard_nn_modules_using_dict_tags specializes on each nn module instance but','line_number':91,'multiline':False]['text':' uses low overhead dict version matching to detect mutations, obviating the','line_number':92,'multiline':False]['text':' need to guard on members of the nn modules. With','line_number':93,'multiline':False]['text':' guard_nn_modules_using_dict_tags, the guard_nn_modules is not really required','line_number':94,'multiline':False]['text':' but kept around for debugging and discussing unspecializing nn module','line_number':95,'multiline':False]['text':' variables.','line_number':96,'multiline':False]['text':' TODO(janimesh, voz): Remove both of these flags (or atleast guard_nn_modules)','line_number':97,'multiline':False]['text':' once we have reached stability for the guard_nn_modules_using_dict_tags.','line_number':98,'multiline':False]['text':' This feature doesn't really work.  We offer this flag for experimental','line_number':101,'multiline':False]['text':' purposes / if you want to help us build out support.','line_number':102,'multiline':False]['text':'','line_number':103,'multiline':False]['text':' torchdynamo has very limited support for tensor subclasses that implement','line_number':104,'multiline':False]['text':' __torch_function__.  Our current support is limited to tensor subclasses','line_number':105,'multiline':False]['text':' that DO NOT store metadata on the tensor (in general, dynamo does not','line_number':106,'multiline':False]['text':' support Python code that stores extra attributes on tensors at present).','line_number':107,'multiline':False]['text':' If your tensor subclass purely changes function call behavior via','line_number':108,'multiline':False]['text':' __torch_function__, you can allow torchdynamo to trace into it by','line_number':109,'multiline':False]['text':' adding it to traceable_tensor_subclasses.  We don't do any safety checks,','line_number':110,'multiline':False]['text':' so it is up to you to ensure that your subclass is well behaved.  See also','line_number':111,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/1948','line_number':112,'multiline':False]['text':'','line_number':113,'multiline':False]['text':' We do NOT currently support __torch_dispatch__.  The implementation is','line_number':114,'multiline':False]['text':' currently buggy, the main show stopper for nontrivial use is','line_number':115,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/1952','line_number':116,'multiline':False]['text':' Suppress errors in torch._dynamo.optimize, instead forcing a fallback to eager.','line_number':119,'multiline':False]['text':' This is a good way to get your model to work one way or another, but you may','line_number':120,'multiline':False]['text':' lose optimization opportunities this way.  Devs, if your benchmark model is failing','line_number':121,'multiline':False]['text':' this way, you should figure out why instead of suppressing it.','line_number':122,'multiline':False]['text':' Record and write an execution record of the current frame to a file','line_number':125,'multiline':False]['text':' if an exception is encountered','line_number':126,'multiline':False]['text':' @compile_ignored[debug]','line_number':127,'multiline':False]['text':' Rewrite assert statement in python with torch._assert','line_number':130,'multiline':False]['text':' Disable dynamo','line_number':133,'multiline':False]['text':' [@compile_ignored: runtime_behaviour] Get a cprofile trace of Dynamo','line_number':136,'multiline':False]['text':' legacy config, does nothing now!','line_number':139,'multiline':False]['text':' If a string representing a PyTorch module is in this ignorelist,','line_number':142,'multiline':False]['text':' the `allowed_functions.is_allowed` function will not consider it','line_number':143,'multiline':False]['text':' when creating a list of PyTorch functions that will appear in','line_number':144,'multiline':False]['text':' FX IR.','line_number':145,'multiline':False]['text':' Debug Flag to try minifier at different stages. Possible values are {None, "aot", "dynamo"}','line_number':154,'multiline':False]['text':' None - Minifier is switched off','line_number':155,'multiline':False]['text':' dynamo - Runs minifier on the TorchDynamo produced graphs, if compilation fails','line_number':156,'multiline':False]['text':' aot - Runs minifier on the Aot Autograd produced graphs, if compilation fails','line_number':157,'multiline':False]['text':' [@compile_ignored: debug]','line_number':158,'multiline':False]['text':' Compiler compilation debug info','line_number':161,'multiline':False]['text':' 1: Dumps the original graph out to repro.py if compilation fails','line_number':162,'multiline':False]['text':' 2: Dumps a minifier_launcher.py if compilation fails.','line_number':163,'multiline':False]['text':' 3: Always dumps a minifier_launcher.py. Good for segfaults.','line_number':164,'multiline':False]['text':' 4: Dumps a minifier_launcher.py if the accuracy fails.','line_number':165,'multiline':False]['text':' [@compile_ignored: debug]','line_number':166,'multiline':False]['text':' By default, we try to detect accuracy failure by running both forward','line_number':169,'multiline':False]['text':' and backward of a torchdynamo produced graph (if you are using repro_after','line_number':170,'multiline':False]['text':' 'dynamo').  This setting forces us to only test the forward graph and','line_number':171,'multiline':False]['text':' not the backward graph.  This can be helpful if you're trying to debug','line_number':172,'multiline':False]['text':' an inference only problem, but the minifier seems to be choking on the','line_number':173,'multiline':False]['text':' backwards step','line_number':174,'multiline':False]['text':' TODO: Detect this situation automatically so the user doesn't need','line_number':175,'multiline':False]['text':' to manually configure this','line_number':176,'multiline':False]['text':' [@compile_ignored: debug]','line_number':177,'multiline':False]['text':' The tolerance we should use when testing if a compiled graph','line_number':180,'multiline':False]['text':' has diverged so that we should treat it as an accuracy failure','line_number':181,'multiline':False]['text':' [@compile_ignored: debug]','line_number':182,'multiline':False]['text':' If True, when testing if two models are the same, we will test them against','line_number':185,'multiline':False]['text':' a third fp64 reference and only report a problem if the RMSE relative to the','line_number':186,'multiline':False]['text':' fp64 is greater.  However, this will use more memory; you may disable this','line_number':187,'multiline':False]['text':' if memory usage is too high.','line_number':188,'multiline':False]['text':' [@compile_ignored: runtime_behaviour]','line_number':189,'multiline':False]['text':' Not all backends support scalars. Some calls on torch.Tensor (like .item()) return a scalar type.','line_number':192,'multiline':False]['text':' When this flag is set to False, we introduce a graph break instead of capturing.','line_number':193,'multiline':False]['text':' This requires dynamic_shapes to be True.','line_number':194,'multiline':False]['text':' Not all backends support operators that have dynamic output shape (e.g.,','line_number':197,'multiline':False]['text':' nonzero, unique).  When this flag is set to False, we introduce a graph','line_number':198,'multiline':False]['text':' break instead of capturing.  This requires dynamic_shapes to be True.','line_number':199,'multiline':False]['text':' If you set this to True, you probably also want capture_scalar_outputs','line_number':200,'multiline':False]['text':' (these are separated for historical reasons).','line_number':201,'multiline':False]['text':' By default, dynamo will treat all ints as backed SymInts, which means (1) it','line_number':204,'multiline':False]['text':' will wait to see the int change over multiple runs before generalizing and','line_number':205,'multiline':False]['text':' (2) it will still always 0/1 specialize an int.  When true, this knob','line_number':206,'multiline':False]['text':' forces dynamo to treat _length_per_key and _offset_per_key on','line_number':207,'multiline':False]['text':' KeyedJaggedTensor from torchrec as size-like unbacked SymInts, so that','line_number':208,'multiline':False]['text':' they (1) generalize immediately and (2) unsoundly never compare equal to','line_number':209,'multiline':False]['text':' 0/1.  This is not on by default as AOTAutograd/Inductor cannot currently','line_number':210,'multiline':False]['text':' compile this code; however, this can be useful for export.','line_number':211,'multiline':False]['text':' Should almost always be true in prod. This relaxes the requirement that cond's true_fn and','line_number':214,'multiline':False]['text':' false_fn produces code with identical guards.','line_number':215,'multiline':False]['text':' Automatically split model graph into pieces to match DDP bucket sizes','line_number':218,'multiline':False]['text':' to allow DDP comm/compute overlap.  Disable to allow DDP models to','line_number':219,'multiline':False]['text':' run without graph-breaks, but also without comm/compute overlap.','line_number':220,'multiline':False]['text':' set torch._dynamo.config.log_level to INFO or DEBUG for more info','line_number':221,'multiline':False]['text':' about optimize_ddp behavior.','line_number':222,'multiline':False]['text':' Whether to skip guarding on FSDP-managed modules','line_number':225,'multiline':False]['text':' Make dynamo skip guarding on hooks on nn modules','line_number':228,'multiline':False]['text':' Note: unsafe: if your model actually has hooks and you remove them, or doesn't and  you add them,','line_number':229,'multiline':False]['text':' dynamo will not notice and will execute whichever version you first compiled.','line_number':230,'multiline':False]['text':' If True, raises exception if TorchDynamo is called with a context manager','line_number':233,'multiline':False]['text':' If True, raise when aot autograd is unsafe to use','line_number':236,'multiline':False]['text':' If true, error if you torch.jit.trace over a dynamo-optimized function.','line_number':239,'multiline':False]['text':' If false, silently suppress dynamo','line_number':240,'multiline':False]['text':' If true, error with a better message if we symbolically trace over a','line_number':243,'multiline':False]['text':' dynamo-optimized function. If false, silently suppress dynamo.','line_number':244,'multiline':False]['text':' Disables graph breaking on rnn. YMMV with backends.','line_number':247,'multiline':False]['text':' If true, error if we try to compile a function that has','line_number':250,'multiline':False]['text':' been seen before.','line_number':251,'multiline':False]['text':' [@compile_ignored: runtime_behaviour]','line_number':252,'multiline':False]['text':' [@compile_ignored: debug] Whether to report any guard failures (deprecated: does not do anything)','line_number':255,'multiline':False]['text':' [@compile_ignored: debug] root folder of the project','line_number':258,'multiline':False]['text':' Trace through NumPy or graphbreak','line_number':261,'multiline':False]['text':' Trace through torch.distributed code','line_number':264,'multiline':False]['text':' Default NumPy dtypes when tracing with torch.compile','line_number':267,'multiline':False]['text':' We default to 64bits. For efficiency, one may want to change these to float32','line_number':268,'multiline':False]['text':' use numpy's PRNG if True, pytorch otherwise','line_number':273,'multiline':False]['text':' [@compile_ignored: debug]','line_number':281,'multiline':False]['text':' [@compile_ignored: debug]','line_number':284,'multiline':False]['text':' [@compile_ignored: debug]','line_number':288,'multiline':False]['text':' [@compile_ignored: debug]','line_number':292,'multiline':False]['text':' [@compile_ignored: debug]','line_number':296,'multiline':False]['text':' workaround: "cannot pickle PyCapsule"','line_number':300,'multiline':False]['text':' workaround: "cannot pickle module"','line_number':302,'multiline':False]['text':' When True, only ops that have the torch.Tag.pt2_compliant tag','line_number':306,'multiline':False]['text':' will be allowed into the graph; all other ops will be disallowed','line_number':307,'multiline':False]['text':' and will fall back to eager-mode PyTorch. Useful to ensure','line_number':308,'multiline':False]['text':' correctness of custom ops.','line_number':309,'multiline':False]['text':' enable/disable dynamo tracing for `torch.func` transforms','line_number':314,'multiline':False]['text':' If to log Dynamo compilation metrics into log files (for OSS) and Scuba tables (for fbcode).','line_number':317,'multiline':False]['text':' simulates what would happen if we didn't have support for BUILD_SET opcode,','line_number':320,'multiline':False]['text':' used for testing','line_number':321,'multiline':False]['text':' support `context_fn` in torch.utils.checkpoint.checkpoint API under torch.compile().','line_number':337,'multiline':False]['text':' WARNING: this is an experimental flag and is subject to change.','line_number':338,'multiline':False]['text':' noqa: F401, F403','line_number':342,'multiline':False]