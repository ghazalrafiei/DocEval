['text':' Early load resource_sharer to prevent a partially initialized instance','line_number':13,'multiline':False]['text':' from being inherited in a forked child process. The reduce_storage method','line_number':14,'multiline':False]['text':' requires this module indirectly through DupFd(). The built-in mp.Queue','line_number':15,'multiline':False]['text':' class pickles arguments in a background thread which may overlap with the','line_number':16,'multiline':False]['text':' fork.','line_number':17,'multiline':False]['text':' Save a direct reference to _free_weak_ref because the `torch` module','line_number':34,'multiline':False]['text':' might be cleared during Python shutdown before this module is cleared.','line_number':35,'multiline':False]['text':' type: ignore[attr-defined]','line_number':36,'multiline':False]['text':' type: ignore[attr-defined]','line_number':42,'multiline':False]['text':' type: ignore[attr-defined]','line_number':46,'multiline':False]['text':' free_dead_references() is called if the len exceeds the current','line_number':64,'multiline':False]['text':' limit. The limit scales with the number of remaining live objects.','line_number':65,'multiline':False]['text':' `fork` inherits lock state, so in case we fork when the lock is held,','line_number':67,'multiline':False]['text':' we register a function to reset the lock to a new object to avoid','line_number':68,'multiline':False]['text':' possible deadlocks, following python multiprocessing library design.','line_number':69,'multiline':False]['text':' mapping from handles to StorageWeakRef objects','line_number':96,'multiline':False]['text':' we have to pass requires_grad into constructor, rather than set it as an','line_number':113,'multiline':False]['text':' attribute later, because it's an important check for Integer Tensors to','line_number':114,'multiline':False]['text':' have requires_grad=False (or else they raise an error)','line_number':115,'multiline':False]['text':' If storage_handle is None, storage points to nullptr.','line_number':139,'multiline':False]['text':' We already ref counting this Storage, but producer needs new ref-counters to be released.','line_number':162,'multiline':False]['text':' It is crucial for integer tensors to receive','line_number':181,'multiline':False]['text':' the requires_grad=False as an argument in the constructor','line_number':182,'multiline':False]['text':' Note [CUDA IPC and the caching allocator]','line_number':202,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':203,'multiline':False]['text':' When you send a CUDA tensor over IPC, you might expect that you will','line_number':204,'multiline':False]['text':' get out the same storage from the other end.  However, the CUDA caching','line_number':205,'multiline':False]['text':' allocator makes it difficult to preserve this invariant.  Consider','line_number':206,'multiline':False]['text':' the following situation: a tensor of size 0x100 points to offset 0x20 of','line_number':207,'multiline':False]['text':' a storage at 0xA100 of size 0x100.  (For simplicity, all of these','line_number':208,'multiline':False]['text':' sizes are given in bytes).  HOWEVER, with the caching allocator, this storage','line_number':209,'multiline':False]['text':' might be part of a larger cudaMalloc allocation 0xA000 of size 0x4000.','line_number':210,'multiline':False]['text':'','line_number':211,'multiline':False]['text':' When we want to send this CUDA tensor over IPC, we must send the','line_number':212,'multiline':False]['text':' *entire* cudaMalloc allocation, i.e., the 0xA000 region, not just','line_number':213,'multiline':False]['text':' the storage 0xA100 (because that is what CUDA supports).  So, on the','line_number':214,'multiline':False]['text':' other end, there simply isn't any way to say, "Wait, you gave me','line_number':215,'multiline':False]['text':' a bigger region (0xA000) than the one I wanted (0xA100)".','line_number':216,'multiline':False]['text':'','line_number':217,'multiline':False]['text':' OK, so if you sent the cudaMalloc allocation, can you just wrap that up as','line_number':218,'multiline':False]['text':' one storage itself? No, because this cudaMalloc allocation might contain','line_number':219,'multiline':False]['text':' storages of mixed types: float, bytes, double... If you make the entire','line_number':220,'multiline':False]['text':' allocation a single storage of a type A, we'll hit an error when constructing','line_number':221,'multiline':False]['text':' a tensor of type B on the storage.','line_number':222,'multiline':False]['text':'','line_number':223,'multiline':False]['text':' cudaIpcMemHandle is an identifier to access the sender cudaMalloc allocation on the','line_number':224,'multiline':False]['text':' receiver side. However, cudaIpcMemHandles from each device in a given process may','line_number':225,'multiline':False]['text':' only be opened by one context per device per other process.','line_number':226,'multiline':False]['text':' If we open and close a memory handle multiples times in a process, CUDA is allowed','line_number':227,'multiline':False]['text':' to give it a different address; similarly, once we close the memory, we're not','line_number':228,'multiline':False]['text':' allowed to access it(and the storage/tensor built on top of it), even if it is','line_number':229,'multiline':False]['text':' still live in the original process. As we cannot make a cudaMalloc allocation','line_number':230,'multiline':False]['text':' to a single storage in one go, this requires us to cache the device pointer for','line_number':231,'multiline':False]['text':' each cudaIpcMemHandle on C++ side to reconstruct types of storages, while keep','line_number':232,'multiline':False]['text':' the old ones alives.','line_number':233,'multiline':False]['text':' See [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html]','line_number':234,'multiline':False]['text':'','line_number':235,'multiline':False]['text':' This is fine, because all we need to do is to save our position in the allocation,','line_number':236,'multiline':False]['text':' and reconstruct storage and tensor from it.','line_number':237,'multiline':False]['text':' 0xA000 ->  -------CUDA Allocation------','line_number':238,'multiline':False]['text':'           |                            |','line_number':239,'multiline':False]['text':'           |                            |','line_number':240,'multiline':False]['text':'           |                            |','line_number':241,'multiline':False]['text':'           |                            |','line_number':242,'multiline':False]['text':' 0xA100 ->  --------storage1 begin------','line_number':243,'multiline':False]['text':'           |                            |','line_number':244,'multiline':False]['text':' 0xA120 ->  --------tensor1 begin ------','line_number':245,'multiline':False]['text':'           |                            |','line_number':246,'multiline':False]['text':'           |                            |','line_number':247,'multiline':False]['text':'           |                            |','line_number':248,'multiline':False]['text':'           |                            |','line_number':249,'multiline':False]['text':'           |                            |','line_number':250,'multiline':False]['text':' 0xA160 ->  --------tensor1 end---------','line_number':251,'multiline':False]['text':'           |                            |','line_number':252,'multiline':False]['text':'           |                            |','line_number':253,'multiline':False]['text':'           |                            |','line_number':254,'multiline':False]['text':' 0xA200 ->  --------storage1 end--------','line_number':255,'multiline':False]['text':'           |                            |','line_number':256,'multiline':False]['text':' 0xE000 ->  --------CUDA allocation-----','line_number':257,'multiline':False]['text':'','line_number':258,'multiline':False]['text':' To send tensor1, the following info are required from sender to receiver for','line_number':259,'multiline':False]['text':' storage recontruction.','line_number':260,'multiline':False]['text':'   1. cudaIpcMemHandle of 0xA000(which can be mapped to a basePtr in receiver process).','line_number':261,'multiline':False]['text':'      basePtr may not be exactly 0xA000 since it's a different process.','line_number':262,'multiline':False]['text':'   2. offset(0xA100) of storage1 in the CUDA allocation.','line_number':263,'multiline':False]['text':'   3. size of storage1(0x100).','line_number':264,'multiline':False]['text':'','line_number':265,'multiline':False]['text':' On receiver side:','line_number':266,'multiline':False]['text':'   1. Get the devPtr of the MemHandle to access the memory, reconstruct a storage','line_number':267,'multiline':False]['text':'      of the same type using (basePtr, offset, size).','line_number':268,'multiline':False]['text':'   2. we can reconstruct the tensor on top of the reconstructed storage','line_number':269,'multiline':False]['text':'   Tensor(size=0x040, offset=0x020, storage=Storage(data=basePtr+0xA100, size=0x0100))','line_number':270,'multiline':False]['text':'','line_number':271,'multiline':False]['text':' This strategy has a few implications:','line_number':272,'multiline':False]['text':'','line_number':273,'multiline':False]['text':' 1. When we serialize a CUDA tensor for IPC, we cannot do it all in one','line_number':274,'multiline':False]['text':'    go (non-compositionally), and this requires to have a global map','line_number':275,'multiline':False]['text':'    memHandle -> devPtr for each process.','line_number':276,'multiline':False]['text':'','line_number':277,'multiline':False]['text':' 2. We MUST NOT let the new IPC tensor be resizable.  Originally, a resize','line_number':278,'multiline':False]['text':'    of the storage beyond 0x100 would merely have caused us to do a','line_number':279,'multiline':False]['text':'    reallocation.  You don't really want to do this, but if you did,','line_number':280,'multiline':False]['text':'    all that would happen is that you would lose IPC sharing.  But if','line_number':281,'multiline':False]['text':'    you do this in the new world, we will happily let you write out of','line_number':282,'multiline':False]['text':'    bounds of your "allocation", clobbering unrelated data in the cached','line_number':283,'multiline':False]['text':'    allocator block.  BAD!','line_number':284,'multiline':False]['text':'','line_number':285,'multiline':False]['text':' By the way, in old versions of PyTorch, we supported this situation','line_number':286,'multiline':False]['text':' natively using a "storage view", which permitted multiple storages to be','line_number':287,'multiline':False]['text':' views on each other.  But this was the *only* use of storage views, so we','line_number':288,'multiline':False]['text':' eliminated it so that we could just use tensor views to implement the same','line_number':289,'multiline':False]['text':' thing.','line_number':290,'multiline':False]['text':'','line_number':291,'multiline':False]['text':' TODO: Handle distinguishing between subclass and non-subclass versions of NT better','line_number':293,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/110543','line_number':294,'multiline':False]['text':' _backward_hooks purposely omitted here, see','line_number':324,'multiline':False]['text':' Note [Don't serialize hooks]','line_number':325,'multiline':False]['text':' tensor offset in its storage','line_number':332,'multiline':False]['text':' identifier which CUDA allocation is the storage in.','line_number':336,'multiline':False]['text':' size(in bytes) of the storage','line_number':337,'multiline':False]['text':' offset(in bytes) of the storage in the CUDA allocation','line_number':338,'multiline':False]['text':' _backward_hooks purposely omitted here, see Note [Don't serialize hooks]','line_number':347,'multiline':False]['text':' Returns a tuple which uniquely identifies a file descriptor. In Mac OS,','line_number':480,'multiline':False]['text':' this doesn't work with shared memory handles, which is why we don't','line_number':481,'multiline':False]['text':' support the "file_descriptor" sharing method on that platform.','line_number':482,'multiline':False]['text':' Use for torch.storage.TypedStorage','line_number':535,'multiline':False]['text':' Use for child classes of torch.storage.TypedStorage, like torch.FloatStorage','line_number':544,'multiline':False]['text':' This is special cased because Empty tensors','line_number':564,'multiline':False]['text':' (with size 0) cannot be mmapped.','line_number':565,'multiline':False]['text':' type: ignore[assignment]','line_number':572,'multiline':False]['text':' TODO: Maybe this should be in tensor_classes? :)','line_number':592,'multiline':False]