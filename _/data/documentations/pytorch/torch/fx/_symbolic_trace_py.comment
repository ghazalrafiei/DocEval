['text':' type: ignore[attr-defined]','line_number':26,'multiline':False]['text':' These need to run in global scope to handle nested calls correctly','line_number':36,'multiline':False]['text':' type: ignore[call-overload]','line_number':99,'multiline':False]['text':' type: ignore[misc]','line_number':102,'multiline':False]['text':' type: ignore[misc]','line_number':118,'multiline':False]['text':' Python-3.11+ code signature','line_number':127,'multiline':False]['text':' type: ignore[attr-defined]','line_number':141,'multiline':False]['text':' type: ignore[attr-defined]','line_number':144,'multiline':False]['text':' type: ignore[arg-type]','line_number':185,'multiline':False]['text':' we need to insert placeholder nodes for *args and **kwargs','line_number':190,'multiline':False]['text':' we can't call this function normally, otherwise it would try to unpack them','line_number':191,'multiline':False]['text':' instead, let's make python think that args and kwargs are normal variables','line_number':192,'multiline':False]['text':' Provide a hey for user to identify placeholder node during analysis','line_number':216,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/54354','line_number':233,'multiline':False]['text':' The first line of this docstring overrides the one Sphinx generates for the','line_number':234,'multiline':False]['text':' documentation. We need it so that Sphinx doesn't leak `math`s path from the','line_number':235,'multiline':False]['text':' build environment (e.g. `<module 'math' from '/leaked/path').','line_number':236,'multiline':False]['text':' Not checking BC on this API because the default value for `autowrap_modules`','line_number':249,'multiline':False]['text':' includes the local filepath to the `math` module, which would jitter','line_number':250,'multiline':False]['text':' across machines.','line_number':251,'multiline':False]['text':' This method's signature is overridden by the first line of this class'','line_number':259,'multiline':False]['text':' docstring. If this method's signature is modified, the signature that','line_number':260,'multiline':False]['text':' overrides it also should be modified accordingly.','line_number':261,'multiline':False]['text':' Functions we will eagerly wrap when we see them while tracing','line_number':287,'multiline':False]['text':' this captures both `math.sqrt()` and `from math import sqrt` automatically','line_number':288,'multiline':False]['text':' Python modules to apply autowrap to at the start, in addition to','line_number':296,'multiline':False]['text':' modules we see while tracing','line_number':297,'multiline':False]['text':' Maps the containing module's name to the operator name','line_number':303,'multiline':False]['text':' Records the module call stack','line_number':305,'multiline':False]['text':' Mapping of node name to module scope','line_number':307,'multiline':False]['text':' The base tracer is used to construct Graphs when there is no associated','line_number':338,'multiline':False]['text':' module hierarchy, so it can never create parameter references.','line_number':339,'multiline':False]['text':' The default tracer adds the ability to refer to parameters when','line_number':340,'multiline':False]['text':' tracing modules.','line_number':341,'multiline':False]['text':' For NamedTuple instances that appear literally as args, we emit','line_number':355,'multiline':False]['text':' a node to construct the NamedTuple and use that Node as the argument.','line_number':356,'multiline':False]['text':' Tensors do not have a reliable string repr() from which they can be','line_number':361,'multiline':False]['text':' constructed (and we probably don't want to rely on that, either), so','line_number':362,'multiline':False]['text':' for any constant Tensor values we encounter, first search for if they','line_number':363,'multiline':False]['text':' are an attribute of some module in the module hierarchy. If so, emit','line_number':364,'multiline':False]['text':' a get_attr to retrieve that tensor. Otherwise, we'll store away the','line_number':365,'multiline':False]['text':' tensor value into a special attribute on the Module s.t. we can','line_number':366,'multiline':False]['text':' retrieve it with a get_attr.','line_number':367,'multiline':False]['text':' Tensor was not found in the Module hierarchy, stow it away in a','line_number':371,'multiline':False]['text':' special attribute and set the qualname to refer to that','line_number':372,'multiline':False]['text':' This is an instance of a proxyable class for which we did not','line_number':386,'multiline':False]['text':' witness its construction. Intern this as a constant attribute','line_number':387,'multiline':False]['text':' TODO: binary search','line_number':389,'multiline':False]['text':' Prefer the O(1) algorithm','line_number':439,'multiline':False]['text':' O(N^2) fallback in the case that we didn't store the submodule','line_number':446,'multiline':False]['text':' paths.','line_number':447,'multiline':False]['text':' module_stack is an ordered dict so writing then deleting the','line_number':490,'multiline':False]['text':' entry is equivalent to push/pop on a list','line_number':491,'multiline':False]['text':' type: ignore[arg-type]','line_number':543,'multiline':False]['text':' This method will be refactored','line_number':564,'multiline':False]['text':' In some cases, a function or method has been decorated with a wrapper','line_number':572,'multiline':False]['text':' defined via ``functools.wraps``. In this case, the outer code object','line_number':573,'multiline':False]['text':' will likely not contain the actual parameters we care about, so unwrap','line_number':574,'multiline':False]['text':' the function to get to the innermost callable.','line_number':575,'multiline':False]['text':' skip self','line_number':589,'multiline':False]['text':' Transfer attrs in the case where you're using a placeholder other','line_number':612,'multiline':False]['text':' than the singleton PH (PH has no attributes to transfer).','line_number':613,'multiline':False]['text':' Proxies were created out of the placeholders.','line_number':614,'multiline':False]['text':' Transfer any metadata (put on the placeholders in the form of','line_number':615,'multiline':False]['text':' attributes set by the user) from the placeholder to the','line_number':616,'multiline':False]['text':' underlying nodes (the proxy is unwrapped by the user, but','line_number':617,'multiline':False]['text':' the metadata should hold).','line_number':618,'multiline':False]['text':' Union[int, bool] == bool in Python <= 3.6','line_number':622,'multiline':False]['text':' type: ignore[assignment]','line_number':652,'multiline':False]['text':' This covers the very specific case where we are passing in flat','line_number':661,'multiline':False]['text':' concrete_args as a tuple, but our traced fn takes (*args, **kwargs).','line_number':662,'multiline':False]['text':' In this case, just take the concrete_args and pass them through.','line_number':663,'multiline':False]['text':' Transfer attrs in the case where you're using a placeholder other','line_number':673,'multiline':False]['text':' than the singleton PH (PH has no attributes to transfer).','line_number':674,'multiline':False]['text':' Proxies were created out of the placeholders.','line_number':675,'multiline':False]['text':' Transfer any metadata (put on the placeholders in the form of','line_number':676,'multiline':False]['text':' attributes set by the user) from the placeholder to the','line_number':677,'multiline':False]['text':' underlying nodes (the proxy is unwrapped by the user, but','line_number':678,'multiline':False]['text':' the metadata should hold).','line_number':679,'multiline':False]['text':' TODO: type annotations for *args and **kwargs','line_number':695,'multiline':False]['text':' In the case that we have pytree-flattened inputs in','line_number':704,'multiline':False]['text':' `concrete_args`, generate a flattening wrapper around the','line_number':705,'multiline':False]['text':' original root function and return that.','line_number':706,'multiline':False]['text':' When we encounter a Tensor value that's not a parameter, we look if it','line_number':781,'multiline':False]['text':' is some other attribute on the model. Construct a dict mapping Tensor','line_number':782,'multiline':False]['text':' values to the qualified name here for efficiency. This is used downstream','line_number':783,'multiline':False]['text':' in create_arg','line_number':784,'multiline':False]['text':' run before it gets patched','line_number':798,'multiline':False]['text':' Reduce number of get_attr calls','line_number':805,'multiline':False]['text':' Method dispatch on parameters is not recorded unless it's directly used.','line_number':807,'multiline':False]['text':' Thus, we need to insert a proxy when __getattr__ requests a parameter.','line_number':808,'multiline':False]['text':' allow duplicate patches to support the case of nested calls','line_number':827,'multiline':False]['text':' _autowrap_search contains modules, which cannot be deepcopied.','line_number':857,'multiline':False]['text':' Dictionary of (id(globals dict), function name) => globals_dict to patch for','line_number':871,'multiline':False]['text':' the purposes of the wrap() API.','line_number':872,'multiline':False]['text':' We key by the globals dict id and function name to ensure we're wrapping a given','line_number':873,'multiline':False]['text':' function only once.','line_number':874,'multiline':False]['text':' List of methods on classes to wrap (class type, function name)','line_number':877,'multiline':False]['text':' this currently only works for Tensor.* methods that aren't traced properly','line_number':878,'multiline':False]['text':' This change is needed to trace models like PositionalEmbedding from BERT:','line_number':882,'multiline':False]['text':' https://github.com/pytorch/benchmark/blob/master/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/embedding/position.py','line_number':883,'multiline':False]['text':' but causes issues in quantization documented here:','line_number':884,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/50710','line_number':885,'multiline':False]['text':' once that is fixed we can make this the default behavior.','line_number':886,'multiline':False]['text':' type: ignore[attr-defined]','line_number':986,'multiline':False]['text':' already patched, no need to do it again','line_number':990,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1003,'multiline':False]['text':' already patched, no need to do it again','line_number':1006,'multiline':False]['text':' unpatch in reverse order to handle duplicates correctly','line_number':1026,'multiline':False]['text':' to make mypy happy','line_number':1105,'multiline':False]['text':' consider implementing Callable version of this via _autowrap_function_ids / _autowrap_search','line_number':1120,'multiline':False]['text':' semantics would be slightly different, but would add support `from x import wrapped_function`','line_number':1121,'multiline':False]