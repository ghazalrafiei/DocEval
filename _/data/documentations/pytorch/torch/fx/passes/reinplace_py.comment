['text':' check if op is a view','line_number':27,'multiline':False]['text':' check if op is a view','line_number':35,'multiline':False]['text':' check if op is a multi-output view','line_number':37,'multiline':False]['text':' Stores a bunch of metadata related to functionalization each node.','line_number':45,'multiline':False]['text':' Relevant metadata:','line_number':46,'multiline':False]['text':' n.meta['fake_result']: FakeTensor (same type as the output of the node, but with FakeTenors instead of Tensors)','line_number':47,'multiline':False]['text':'   The fake tensor output from running the current node','line_number':48,'multiline':False]['text':' n.meta['view_of']: Node','line_number':49,'multiline':False]['text':'   If the current node n is a view of some base tensor, the 'view_of' field tells us which','line_number':50,'multiline':False]['text':'   view node was used to generate the current node (a view tensor).','line_number':51,'multiline':False]['text':'   This information actually makes `fake_result` redundant, but we can use `fake_result`','line_number':52,'multiline':False]['text':'   to sanity check that our aliasing information is correct.','line_number':53,'multiline':False]['text':' (1) Update metadata with the list of nodes that are used by this node','line_number':63,'multiline':False]['text':' copy_() doesn't read from its first argument; it writes to it, overwriting previous data.','line_number':64,'multiline':False]['text':' We don't want to treat it as "being used as an input".','line_number':65,'multiline':False]['text':' (2) Update metadata to track aliasing information about view tensor nodes.','line_number':70,'multiline':False]['text':' Check if we returned a multi-output view,','line_number':79,'multiline':False]['text':' and we're now grabbing the individual views from the output.','line_number':80,'multiline':False]['text':'','line_number':81,'multiline':False]['text':' For multi-output views, we want to map each output view to the base,','line_number':82,'multiline':False]['text':' but this mapping involves two separate nodes in FX IR.','line_number':83,'multiline':False]['text':' e.g. "a, b = x_1.split(...)" becomes:','line_number':84,'multiline':False]['text':'    %split_tensor : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%x_1, 2), kwargs = {})','line_number':85,'multiline':False]['text':'    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split_tensor, 0), kwargs = {})','line_number':86,'multiline':False]['text':'    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split_tensor, 1), kwargs = {})','line_number':87,'multiline':False]['text':' And we'd like to set:','line_number':88,'multiline':False]['text':'    getitem1.meta['view_of'] = x_1','line_number':89,'multiline':False]['text':' Note: we could also track indexing info here for multi-output views.','line_number':94,'multiline':False]['text':' I don't think this metadata is strictly needed for de-functionalization.','line_number':95,'multiline':False]['text':' We're linking the current node with its first argument as views.','line_number':100,'multiline':False]['text':' Assert here that this is actually the case, and their storages are the same.','line_number':101,'multiline':False]['text':' for the inplace op, its first argument should be mutable','line_number':123,'multiline':False]['text':' and its remaining arguments shouldn't be.','line_number':125,'multiline':False]['text':' TODO: this should be beefed up to be able to properly re-inplace with:','line_number':129,'multiline':False]['text':' - mutating ops (e.g. _fused_moving_avg_obs_fq_helper)','line_number':130,'multiline':False]['text':' - out= ops (e.g. angle -> angle.out)','line_number':131,'multiline':False]['text':' TODO: we should also figure this info out using torchgen.','line_number':132,'multiline':False]['text':' __module__ seems broken; it returns torch._ops.aten which doesn't exist','line_number':134,'multiline':False]['text':' Some view ops have inplace variants (as_strided_, etc),','line_number':137,'multiline':False]['text':' but we do NOT want the reinplacing pass to directly add these into the program.','line_number':138,'multiline':False]['text':' (they'll require extra special handling, aren't aren't really useful for perf anyway)','line_number':139,'multiline':False]['text':' Just because foo() and foo_() are both existing operators,','line_number':157,'multiline':False]['text':' They aren't guaranteed to have compatible schemas.','line_number':158,'multiline':False]['text':' For example, pow.Scalar(Scalar self, Tensor exponent) has no valid inplace variant,','line_number':159,'multiline':False]['text':' Even though several overloads of pow_ exist.','line_number':160,'multiline':False]['text':' This function, given a set of set of (aliased) tensor nodes,','line_number':174,'multiline':False]['text':' Returns any nodes in the graph that *use* any of the aliases, that occur *after* op_index','line_number':175,'multiline':False]['text':' in the node ordering.','line_number':176,'multiline':False]['text':' get all nodes that use the current alias','line_number':184,'multiline':False]['text':' We only care about usages after the current node','line_number':187,'multiline':False]['text':' We also don't care about intermediate view ops.','line_number':190,'multiline':False]['text':' They only matter if their output is then used elsewhere','line_number':191,'multiline':False]['text':' (either in an out-of-place op, or as an output to the function).','line_number':192,'multiline':False]['text':' Given an op that we're trying to re-inplace, "b = foo(a)",','line_number':199,'multiline':False]['text':' And given a {view}_scatter op that shows up later in the graph, "y = {view}_scatter(base, x, args...)"','line_number':200,'multiline':False]['text':' Then re-inplacing `foo()` would allow us to remove the `{view}_scatter` op entirely, IF:','line_number':201,'multiline':False]['text':' If there are any aliases in the alias_set(a) that satisfy:','line_number':202,'multiline':False]['text':' (1) The base of "alias", "alias_base", has the same size/stride/offset metadata as "base"','line_number':203,'multiline':False]['text':' (2) The output of running {view}(alias, args...) gives you the same size/stride/offset metadata','line_number':204,'multiline':False]['text':'     as "alias"','line_number':205,'multiline':False]['text':' Go through them in node order, so we can see chains of view_scatter ops.','line_number':213,'multiline':False]['text':' Check that this view_inverse op actually corresponds to taking doing the inverse','line_number':223,'multiline':False]['text':' of one of our existing self_alias nodes.','line_number':224,'multiline':False]['text':' We're looking for some alias of the self arg, "alias",','line_number':227,'multiline':False]['text':' that was created from some op `alias = foo(base, args...)`','line_number':228,'multiline':False]['text':' such that the current _scatter op "inverts" that foo call.','line_number':229,'multiline':False]['text':' We can check that by running the original op again, and checking that the strides match.','line_number':230,'multiline':False]['text':' The we're trying to re-use the args from the view_scatter call inside of the corresponding','line_number':235,'multiline':False]['text':' view op, which might throw. This just indicates that view_scatter op isn't a valid inverse','line_number':236,'multiline':False]['text':' of the current alias we're looking at.','line_number':237,'multiline':False]['text':' If the alias and its base both have matching metadata, then this view_scatter op is valid to re-inplace.','line_number':240,'multiline':False]['text':' Useful debug printing','line_number':453,'multiline':False]['text':' def _print(x):','line_number':454,'multiline':False]['text':' if isinstance(x, FakeTensor):','line_number':455,'multiline':False]['text':' print(f'fake_result: {StorageWeakRef(x._typed_storage()).cdata}')','line_number':456,'multiline':False]['text':' for n in gm.graph.nodes:','line_number':458,'multiline':False]['text':' print(n.format_node())','line_number':459,'multiline':False]['text':' if hasattr(n, 'meta'):','line_number':460,'multiline':False]['text':' print(f'node_idx: {n.meta["node_idx"]}')','line_number':461,'multiline':False]['text':' if 'fake_result' in n.meta:','line_number':462,'multiline':False]['text':' tree_map(_print, n.meta['fake_result'])','line_number':463,'multiline':False]['text':' if 'view_of' in n.meta:','line_number':464,'multiline':False]['text':' print(f'view_of: {str(n.meta["view_of"])}')','line_number':465,'multiline':False]['text':' print()','line_number':466,'multiline':False]['text':' We need to know which nodes correspond to inputs (or their aliases)','line_number':468,'multiline':False]['text':' so we know not to re-inplace them.','line_number':469,'multiline':False]['text':' NOTE: later, we'll need to add an optimization for fully recovering performance','line_number':470,'multiline':False]['text':' on programs that mutate inputs.','line_number':471,'multiline':False]['text':' We also need to know for a given node, what are all of its aliasing nodes.','line_number':478,'multiline':False]['text':' Tree-mapping because some ops can return lists of tensors.','line_number':482,'multiline':False]['text':' inplace-ify functional ops, subject to the constraints written below.','line_number':488,'multiline':False]['text':' Today, the re-inplace pass on directly acts on:','line_number':493,'multiline':False]['text':' - functional ops with an inplace variant','line_number':494,'multiline':False]['text':' - {view}_scatter ops that can be potentially removed from the graph.','line_number':495,'multiline':False]['text':' Both of these ops take in tensor first args, so filtering on this condition','line_number':496,'multiline':False]['text':' makes the later code simpler.','line_number':497,'multiline':False]['text':' We should revisit this at some point though, particularly when we also want','line_number':498,'multiline':False]['text':' the reinplacer to be able to handle out= and mutable operators','line_number':499,'multiline':False]['text':' and tensorlist first args (like `_foreach_` ops).','line_number':500,'multiline':False]['text':' Step 1a: Check that the self argument we're attempting to reinplace','line_number':508,'multiline':False]['text':' has the same size/stride as the output.','line_number':509,'multiline':False]['text':' For example, we shouldn't try to reinplace torch.add(scalar_tensor, larger_tensor)','line_number':510,'multiline':False]['text':' As it would require resizing scalar_tensor.','line_number':511,'multiline':False]['text':' (We could potentially swizzle this into larger_tensor.add_(scalar_tensor),','line_number':512,'multiline':False]['text':' this is probably an optimization to revisit later).','line_number':513,'multiline':False]['text':' We also cannot re-inplace on tensors that have internal memory overlap.','line_number':524,'multiline':False]['text':' e.g. torch.ones(1).expand(4, 4).add_(1)','line_number':525,'multiline':False]['text':' Here, we (optimistically) assume that a.resize(b) is valid to re-inplace,','line_number':528,'multiline':False]['text':' Since users should never really be calling the functional "torch.ops.aten.resize"','line_number':529,'multiline':False]['text':' op directly in their programs.','line_number':530,'multiline':False]['text':' Step 1b: ensure that the op we're trying to re-inplace isn't a program input','line_number':534,'multiline':False]['text':' TODO: later, add the optimization for handling `copy_()` calls in the graph.','line_number':538,'multiline':False]['text':' Step 1c:','line_number':541,'multiline':False]['text':' Calling stuff like aten.mul_(a, a) isn't guaranteed to be sound,','line_number':542,'multiline':False]['text':' so we prevent re-inplacing in this case.','line_number':543,'multiline':False]['text':' First, we find all later usages of any of the aliases of self_arg.','line_number':549,'multiline':False]['text':' Then, we check if any of those later usages are actually view_scatter ops','line_number':551,'multiline':False]['text':' that are safe to fully remove.','line_number':552,'multiline':False]['text':' Step 2: Check to see if the input to the op is re-used later in the graph.','line_number':555,'multiline':False]['text':' If not (same goes for its aliases), then this op is safe to re-in place.','line_number':556,'multiline':False]['text':' This is a slightly roundabout way to check that there are no later usages of the current self argument.','line_number':557,'multiline':False]['text':' (later_view_inverse_node_usages corresponds to "view_scatter" nodes that we are allowed to delete)','line_number':558,'multiline':False]['text':' Step 3a: Special handling for when we see *_scatter operators.','line_number':563,'multiline':False]['text':' When we see an operator like `b = torch.slice_scatter(a, ...)`,','line_number':564,'multiline':False]['text':' instead of trying to "inplace" it into a.slice_scatter_(..._),','line_number':565,'multiline':False]['text':' we would prefer to remove it from the graph entirely,','line_number':566,'multiline':False]['text':' and instead copy_() the slice directly into the larger tensor.','line_number':567,'multiline':False]['text':' See the description of the algorithm for a full example.','line_number':568,'multiline':False]['text':' Before:','line_number':571,'multiline':False]['text':'   base_updated = torch.ops.aten.slice_scatter.default(base, mutated_slice, args...)','line_number':572,'multiline':False]['text':' After:','line_number':573,'multiline':False]['text':'   slice = torch.ops.aten.slice.default(base, args...)','line_number':574,'multiline':False]['text':'   slice.copy_(mutated_slice)','line_number':575,'multiline':False]['text':' Add the slice_scatter node to our "nodes to delete" list.','line_number':583,'multiline':False]['text':' Step 3b: Check to see if this operator has an inplace variant.','line_number':588,'multiline':False]['text':' And if so, replace it with its inplace variant.','line_number':592,'multiline':False]['text':' At this point, 'storage_to_nodes' will be stale.','line_number':595,'multiline':False]['text':' Now that we're inplacing `b = foo(a)`, we need to effectively','line_number':596,'multiline':False]['text':' union together the dict values for b and a's storage.','line_number':597,'multiline':False]['text':' Hmm... morally I think we also want to keep the `fake_result` metadata','line_number':598,'multiline':False]['text':' up to date here, but I'm not sure how easy it is to do.','line_number':599,'multiline':False]['text':' Maybe it's fine to wait until the end of the pass to update it.','line_number':600,'multiline':False]['text':' Need to remember the view_scatter view nodes we found so we can remove them alter.','line_number':605,'multiline':False]['text':' Step 4:','line_number':608,'multiline':False]['text':' Now that we've replaced b = a.foo() with a.foo_(),','line_number':609,'multiline':False]['text':' We need to replace any later usages of "b" with "a"','line_number':610,'multiline':False]['text':' First, replace usages of "b" with "a"','line_number':623,'multiline':False]['text':' Second, update our storage_to_nodes data structure.','line_number':627,'multiline':False]['text':' This will happen if we're updating a view op, e.g.','line_number':640,'multiline':False]['text':' e.g. replacing','line_number':641,'multiline':False]['text':'     x = view(old)','line_number':642,'multiline':False]['text':'     x = view(new)','line_number':643,'multiline':False]['text':' When that happens, we need to make sure to keep our','line_number':644,'multiline':False]['text':' storage mapping up to date.','line_number':645,'multiline':False]['text':'','line_number':646,'multiline':False]['text':' We're checking for len(...) == 1 here because all view ops are guaranteed to return either a single tensor,','line_number':647,'multiline':False]['text':' or multiple tensors that all share the same storage.','line_number':648,'multiline':False]['text':' We can't just check equality because we might encounter FX nodes that return zero tensor outputs.','line_number':649,'multiline':False]['text':' Technically, "old_ref" and all its aliases will remain','line_number':660,'multiline':False]['text':' in our mapping.','line_number':661,'multiline':False]['text':' That should be fine though, since we deleted "old"','line_number':662,'multiline':False]['text':' from the graph at this point.','line_number':663,'multiline':False]['text':' Step 4: delete any _scatter nodes that we de-functionalized','line_number':667,'multiline':False]['text':' Need to take care not to delete any of these nodes until after *all* modifications','line_number':668,'multiline':False]['text':' to the graph are finished.','line_number':669,'multiline':False]