['text':' Use try-except to initialize package-dependent global variables.','line_number':33,'multiline':False]['text':' type: ignore[import]','line_number':35,'multiline':False]['text':' type: ignore[import]','line_number':36,'multiline':False]['text':' This is not use directly in DORT but needed by underlying exporter,','line_number':38,'multiline':False]['text':' so we still need to check if it exists.','line_number':39,'multiline':False]['text':' TODO: select a good default based on the capabilities of the host','line_number':88,'multiline':False]['text':' e.g. DML on Windows, etc.','line_number':89,'multiline':False]['text':' ort pytorch device is mapped to NPU OrtDevice type','line_number':116,'multiline':False]['text':' Uncomment the following lines to print out development info.','line_number':123,'multiline':False]['text':' logging.basicConfig(level=logging.WARNING)','line_number':124,'multiline':False]['text':' logger.setLevel(logging.WARNING)','line_number':125,'multiline':False]['text':' Use extra_support_dict[op_name] = None to indicate','line_number':137,'multiline':False]['text':' we support op_name with all input types. Otherwise,','line_number':138,'multiline':False]['text':' see support_dict (type: SupportDict) in operator_support.py','line_number':139,'multiline':False]['text':' for specifying supported types.','line_number':140,'multiline':False]['text':' OperatorSupport.is_node_supported returns True for non-callable nodes.','line_number':147,'multiline':False]['text':' Since ORT can't execute them, we return False here to override the base','line_number':148,'multiline':False]['text':' behavior.','line_number':149,'multiline':False]['text':' This is the and the only place to decide if aten op is supported.','line_number':152,'multiline':False]['text':' If node.target is not in support_dict, we still want to check if torch.jit.script','line_number':165,'multiline':False]['text':' can convert it to ONNX equivalence. Let's use base mechanism to do this.','line_number':166,'multiline':False]['text':' See extra_support_dict  for supported ops.','line_number':167,'multiline':False]['text':' aten._to_copy doesn't have exporter so we replace it with aten.to.','line_number':205,'multiline':False]['text':' This aten::_to_copy looks like ONNX Cast, so other kwargs are ignored.','line_number':235,'multiline':False]['text':' This change could lead to invalid FX graph but it doesn't matter, as long as the downstream backend,','line_number':236,'multiline':False]['text':' ONNXRuntime, can execute the exported ONNX graph.','line_number':237,'multiline':False]['text':' Output node is unique. Let's retrieve output values from','line_number':276,'multiline':False]['text':' this node's input list. And then just return.','line_number':277,'multiline':False]['text':' Output arguments with example value (type: torch.Tensor) in the `graph_module`.','line_number':287,'multiline':False]['text':' output_arg must have tensor for its device information.','line_number':291,'multiline':False]['text':' Otherwise, skip it.','line_number':292,'multiline':False]['text':' Lowest priority.','line_number':303,'multiline':False]['text':' Higher priority than CPU but lower than','line_number':306,'multiline':False]['text':' other specialized EPs.','line_number':307,'multiline':False]['text':' Highest priority.','line_number':309,'multiline':False]['text':' preallocate output pytorch Tensors and use the buffers affined to the torch device for the output ortvalue.','line_number':377,'multiline':False]['text':' Because the output ortvalue is not allocated and owned by ort, it does not need to convert the output ortvalue','line_number':378,'multiline':False]['text':' to torch Tensor transferring the ownership.','line_number':379,'multiline':False]['text':' type: ignore[name-defined]','line_number':437,'multiline':False]['text':' type: ignore[name-defined]','line_number':439,'multiline':False]['text':' Carrier of ONNX model and its executor.','line_number':444,'multiline':False]['text':' For the ONNX model stored in self.session, self.input_names[i] is the','line_number':446,'multiline':False]['text':' name of the i-th positional input.','line_number':447,'multiline':False]['text':' self.input_name[i]'s type information is stored in self.input_value_infos[i].','line_number':449,'multiline':False]['text':' type: ignore[name-defined]','line_number':450,'multiline':False]['text':' Similar to self.input_names, but for outputs.','line_number':451,'multiline':False]['text':' Similar to self.input_value_infos but for outputs.','line_number':453,'multiline':False]['text':' type: ignore[name-defined]','line_number':454,'multiline':False]['text':' For the ONNX model stored in self.session, self.input_devices[i] is the','line_number':455,'multiline':False]['text':' i-th positional input's device.','line_number':456,'multiline':False]['text':' Similar to self.input_devices, but for outputs.','line_number':458,'multiline':False]['text':' This is the outputs of executing the original torch.fx.GraphModule with example inputs','line_number':460,'multiline':False]['text':' (i.e., args passed into OrtBackend._ort_acclerated_call).','line_number':461,'multiline':False]['text':' Compare the args and the input schema in ONNX model and','line_number':467,'multiline':False]['text':' return the first match.','line_number':468,'multiline':False]['text':' All sessions (and their related information) created by exporting the same GraphModule','line_number':492,'multiline':False]['text':' with different inputs.','line_number':493,'multiline':False]['text':' All execution information for ONNX models exported from the same `graph_module`','line_number':503,'multiline':False]['text':' with different inputs.','line_number':504,'multiline':False]['text':' Returns the first session that accepts this input schema.','line_number':509,'multiline':False]['text':' No reusable session found.','line_number':511,'multiline':False]['text':' preallocate_output allows for allocating output torch Tensor buffers and feeding them to InferenceSession','line_number':565,'multiline':False]['text':' in order to avoid internal allocation of output buffers in InferenceSession.','line_number':566,'multiline':False]['text':' If output ortvalue returned from InferenceSession is allocated internally,','line_number':567,'multiline':False]['text':' it needs to be converted to torch Tensor for return, and the torch Tensor should hold the ownership.','line_number':568,'multiline':False]['text':' When a custom torch device is used with a custom aten allocator, the conversion from ortvalue to torch Tensor','line_number':569,'multiline':False]['text':' should be supported, which is currently done through dlpack. Note that dlpack might not support a custom torch device.','line_number':570,'multiline':False]['text':' It can be avoided by allowing for preallocation for output buffers allocated by a custom aten allocator,','line_number':571,'multiline':False]['text':' and use the preallocated output buffers for InferenceSession not holding any ownership for them.','line_number':572,'multiline':False]['text':' TODO(wschin): Make it to inference session level flag.','line_number':573,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106869.','line_number':574,'multiline':False]['text':' options.export_options contains information shared between exporter and DORT.','line_number':609,'multiline':False]['text':' For example, they should use the same decomposition table when','line_number':610,'multiline':False]['text':'  1. capturing FX graph in torch.compile (see how we create aot_ort in register_backend.py)','line_number':611,'multiline':False]['text':'  2. call exporter's API to convert `torch.fx.GraphModule` to ONNX model','line_number':612,'multiline':False]['text':'     (see onnxfunction_dispatcher passed to FxOnnxInterpreter.run below).','line_number':613,'multiline':False]['text':'','line_number':614,'multiline':False]['text':' Convert user-facing option to internal option used by ONNX exporter','line_number':615,'multiline':False]['text':' to access required information.','line_number':616,'multiline':False]['text':' Some useful fields:','line_number':617,'multiline':False]['text':' - Decomposition table for decomposing FX operators in exporter is','line_number':618,'multiline':False]['text':'   self._resolved_onnx_exporter_options.decomposition_table.','line_number':619,'multiline':False]['text':' - self._resolved_onnx_exporter_options.onnx_registry records what','line_number':620,'multiline':False]['text':'   aten/prim ops are supported by exporter and their exporters (type: callable).','line_number':621,'multiline':False]['text':'  Given DORT's computation flow:','line_number':630,'multiline':False]['text':'   1. OrtOperatorSupport uses support_dict and extra_support_dict to select operators','line_number':631,'multiline':False]['text':'      and send them to DORT.','line_number':632,'multiline':False]['text':'   2. Then, DORT exports the selected sub-graphs into ONNX.','line_number':633,'multiline':False]['text':'   3. Finally DORT calls ORT to do the computation.','line_number':634,'multiline':False]['text':'  OrtOperatorSupport and create_onnx_friendly_decomposition_table(...)','line_number':635,'multiline':False]['text':'  must use the same support_dict. If the support_dict here contains something not','line_number':636,'multiline':False]['text':'  supported by exporter, exporter will fails in step 2 since the selected graphs may','line_number':637,'multiline':False]['text':'  contains unsupported operators such as aten::_who_you_are.','line_number':638,'multiline':False]['text':'  This restriction is automatically done since DORT and exporter shares the same','line_number':639,'multiline':False]['text':'  self._resolved_onnx_exporter_options.','line_number':640,'multiline':False]['text':' TODO(wschin): this is a naive implementation of cache without proper guard','line_number':651,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106868.','line_number':652,'multiline':False]['text':' Conceptually, this filed is a 2-layer dictionary','line_number':654,'multiline':False]['text':'   GraphModule 0','line_number':655,'multiline':False]['text':'     ONNX Model 0 (with ORT InferenceSession and related information. type: OrtExecutionInfoPerSession)','line_number':656,'multiline':False]['text':'     ONNX Model 1','line_number':657,'multiline':False]['text':'     ...','line_number':658,'multiline':False]['text':'   GraphModule 1','line_number':659,'multiline':False]['text':'     ONNX Model 2 (with ORT InferenceSession and related information. type: OrtExecutionInfoPerSession)','line_number':660,'multiline':False]['text':'     ONNX Model 3','line_number':661,'multiline':False]['text':'     ...','line_number':662,'multiline':False]['text':'   ...','line_number':663,'multiline':False]['text':' , which caches all previous compilation result so that we can reuse them.','line_number':664,'multiline':False]['text':' ONNX Model 0 and 1 are exported from the same GraphModule 0 but with different inputs','line_number':665,'multiline':False]['text':' (e.g., tensors with different ranks). GraphModule 0 and GraphModule 1 are different','line_number':666,'multiline':False]['text':' graphs captured by Dynamo and sent to OrtBackend.compile.','line_number':667,'multiline':False]['text':' Function which invokes ORT do to the real computation.','line_number':674,'multiline':False]['text':' If user feeds CUDA tensor as input argument,','line_number':687,'multiline':False]['text':' we want to use CUDA EP.','line_number':688,'multiline':False]['text':' Thus, `eps_from_args` (deduced from input arguments)','line_number':689,'multiline':False]['text':' has highest priority.','line_number':690,'multiline':False]['text':' If there is no EP in input arguments, we deduce EP from','line_number':693,'multiline':False]['text':' graph_module's outputs. Those outputs may come from','line_number':694,'multiline':False]['text':' FakeTensorProp or Dynamo's built-in symbolic shape inference.','line_number':695,'multiline':False]['text':' It's first time seeing such as graph. Let's make a new session','line_number':734,'multiline':False]['text':' (type: onnxruntime.InferenceSession) for it.','line_number':735,'multiline':False]['text':' Generate reference outputs. They are used to indicate output','line_number':741,'multiline':False]['text':' tensors' types and devices when calling ORT.','line_number':742,'multiline':False]['text':'','line_number':743,'multiline':False]['text':' WARNING: The downstream code should not change prim_outputs and','line_number':744,'multiline':False]['text':' this backend should always produces output with schema identical to prim_outputs'.','line_number':745,'multiline':False]['text':' No pre-allocation when dynamic shape is enabled.','line_number':748,'multiline':False]['text':' Select outputs with "val" information. Without "val",','line_number':754,'multiline':False]['text':' it's not possible access output_arg.meta["val"].device.','line_number':755,'multiline':False]['text':' When FakeTensorProp fails, it is not possible to preallocate output buffers','line_number':770,'multiline':False]['text':' because the output shapes are not inferred.','line_number':771,'multiline':False]['text':' rethrow FakeTensorProb failure because it is not yet currently handled.','line_number':774,'multiline':False]['text':' Create the object to iterate through the nodes in graph one-by-one','line_number':777,'multiline':False]['text':' and calls the corresponding ONNX exporter for each node.','line_number':778,'multiline':False]['text':' Cast FX variables if they will result schema-mismatch when searching','line_number':782,'multiline':False]['text':' for ONNX operator. E.g., add(double_tensor, int_tensor) is fine in PyTorch,','line_number':783,'multiline':False]['text':' but ONNX expects add(double_tensor, double_tensor).','line_number':784,'multiline':False]['text':' Start the per-node exporting process. It's conceptually a for loop','line_number':788,'multiline':False]['text':' scanning through the nodes in the graph.','line_number':789,'multiline':False]['text':' Convert the exported result to ONNX ModelProto.','line_number':795,'multiline':False]['text':' Initialize a ORT session to execute this ONNX model.','line_number':800,'multiline':False]['text':' Note that TorchDynamo assumes all inputs/outputs are on the','line_number':801,'multiline':False]['text':' same device, but it's subject to change (very likely with','line_number':802,'multiline':False]['text':' dynamic shape support), so we add execution providers','line_number':803,'multiline':False]['text':' based on the logic in _select_eps: (explicitly preferred EPs,','line_number':804,'multiline':False]['text':' EPs inferred from inputs or graph, and the fallback default EP)/','line_number':805,'multiline':False]['text':'','line_number':806,'multiline':False]['text':' TODO(wschin): enable external allocators.','line_number':807,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106867','line_number':808,'multiline':False]['text':' Cache ORT session. It's reused for the same "graph_module".','line_number':815,'multiline':False]['text':' Generate ONNX model and extract its input and output names.','line_number':816,'multiline':False]['text':' Cache devices for inputs and outputs. They are used to invoke','line_number':820,'multiline':False]['text':' ORT session. Output devices indicate where (e.g., GPU or CPU)','line_number':821,'multiline':False]['text':' to store outputs','line_number':822,'multiline':False]['text':' ORT always returns a tuple of outputs. If the original output is a tensor,','line_number':845,'multiline':False]['text':' ORT output's first element must be extracted and returned. Otherwise, type','line_number':846,'multiline':False]['text':' mismatch may happen in downstream computation.','line_number':847,'multiline':False]['text':' Compute baseline.','line_number':868,'multiline':False]['text':' Ensure every output tensor is close to the corresponding baseline.','line_number':875,'multiline':False]['text':' Deferred import since CapabilityBasedPartitioner is not decorated with','line_number':883,'multiline':False]['text':' @compatibility; importing it at the module level will result in the test','line_number':884,'multiline':False]['text':' failing: pytest test/test_fx.py -k test_public_api_surface','line_number':885,'multiline':False]['text':' because this module is imported into torch.onnx.','line_number':886,'multiline':False]['text':' FX graph based partitioning based on ONNX supported ops.','line_number':889,'multiline':False]['text':' Given a graph module','line_number':890,'multiline':False]['text':'  GraphModule0','line_number':891,'multiline':False]['text':'   node_0','line_number':892,'multiline':False]['text':'   node_1','line_number':893,'multiline':False]['text':'   node_2','line_number':894,'multiline':False]['text':'   node_3','line_number':895,'multiline':False]['text':'   node_4','line_number':896,'multiline':False]['text':' If only node_2 is not supported by ONNX, this graph module will be partitioned into','line_number':897,'multiline':False]['text':'  GraphModule0','line_number':898,'multiline':False]['text':'   GraphModule1','line_number':899,'multiline':False]['text':'    node_0','line_number':900,'multiline':False]['text':'    node_1','line_number':901,'multiline':False]['text':'   node_2','line_number':902,'multiline':False]['text':'   GraphModule2','line_number':903,'multiline':False]['text':'    node_3','line_number':904,'multiline':False]['text':'    node_4','line_number':905,'multiline':False]['text':' by calling CapabilityBasedPartitioner.partition_and_fuse.','line_number':906,'multiline':False]['text':' Then, GraphModule1's and GraphModule2's forward method (GraphModule._wrapped_call)','line_number':907,'multiline':False]['text':' will be replaced by OrtBackend._ort_accelerated_call to delegate computation to ORT.','line_number':908,'multiline':False]['text':' TODO(wschin): this is required for removing aten::_to_copy in _replace_to_copy_with_to.','line_number':913,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106871.','line_number':914,'multiline':False]['text':' Overriding fused_module's __call__() function with ort_acclerated_call()','line_number':924,'multiline':False]['text':' This loop goes through all graph partitions (each of them is an ONNX-representable graph)','line_number':925,'multiline':False]['text':' and override their _wrapped_call function with _ort_accelerated_call.','line_number':926,'multiline':False]['text':' Inside _ort_accelerated_call, the partition's graph is exported into ONNX and executed by ORT.','line_number':927,'multiline':False]['text':' TODO(wschin): use a better way to identify fused submodule','line_number':929,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106872.','line_number':930,'multiline':False]['text':' self.ort_acclerated_call is responsible for exporting graph to ONNX,','line_number':933,'multiline':False]['text':' creating ORT session, and running ORT session.','line_number':934,'multiline':False]['text':' onnxruntime.SessionOptions is a pybind11 object, cannot be pickled,','line_number':984,'multiline':False]['text':' and holds too much potential state to reasonably check manually;','line_number':985,'multiline':False]['text':' ort_session_options is provided at all, the backend does not participate','line_number':986,'multiline':False]['text':' in caching.','line_number':987,'multiline':False]['text':' Similarly, some objects in ExportOptions are too stateful to use for','line_number':994,'multiline':False]['text':' caching. We should revisit this.','line_number':995,'multiline':False]['text':' We can't account for how the two option sets may differ, so it's not safe to reuse.','line_number':1007,'multiline':False]