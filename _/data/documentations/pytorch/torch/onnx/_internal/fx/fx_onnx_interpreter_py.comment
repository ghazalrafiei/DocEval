['text':' type: ignore[import]','line_number':10,'multiline':False]['text':' type: ignore[import]','line_number':11,'multiline':False]['text':' 1. fx_node_arg is a torch.fx.Node, which means','line_number':109,'multiline':False]['text':'    fx_node_arg stands for the output of that torch.fx.Node.','line_number':110,'multiline':False]['text':' 2. fx_node_arg (variable in torch.fx.Graph) is be mapped to','line_number':111,'multiline':False]['text':'    torch.jit.Value, fx_name_to_onnxscript_value[fx_node_arg.name],','line_number':112,'multiline':False]['text':'    in TorchScript graph.','line_number':113,'multiline':False]['text':' This intends to handle dynamic axes. for example, if the input size of op.Expand','line_number':120,'multiline':False]['text':' is dynamic, each dimension would be variable (i.e., sym variable in Pytorch','line_number':121,'multiline':False]['text':' FX graph. Note that sym variable is mapped to tensor in ONNX Script world)','line_number':122,'multiline':False]['text':' calculated by other operators.','line_number':123,'multiline':False]['text':' NOTE: op.Concat doesn't support scalar, so we need to wrap it with','line_number':137,'multiline':False]['text':' dim, and onnx-script will promote it to tensor(int64)','line_number':138,'multiline':False]['text':' Concat all the elements in the sequence.','line_number':140,'multiline':False]['text':' shapes are mapped to tensors in ONNX graph (TorchScriptGraph),','line_number':141,'multiline':False]['text':' so list of sym_ints is concatenated to a tensor before calling ONNX op.','line_number':142,'multiline':False]['text':' For example:','line_number':144,'multiline':False]['text':'    inputs: [[2], [4], fx.Node(SymIntA), [1], fx.Node(SymIntB)]','line_number':145,'multiline':False]['text':'    outputs: op.Concat([op.Constant(2), op.Constant(4), TorchScriptTensor(A), op.Constant(1), TorchScriptTensor(B)])','line_number':146,'multiline':False]['text':' onnx-script auto wraps python number with op.Constants,','line_number':148,'multiline':False]['text':' so we don't need to specifically process them.','line_number':149,'multiline':False]['text':' type: ignore[type-var]','line_number':151,'multiline':False]['text':' type: ignore[union-attr]','line_number':152,'multiline':False]['text':' type: ignore[union-attr]','line_number':153,'multiline':False]['text':' NOTE: if device is specified in kwargs (not consumed), it's free to ignored. But','line_number':176,'multiline':False]['text':' if it's in args, we need to set it to string for dispatcher to match schema.','line_number':177,'multiline':False]['text':' torch.device is not supported by onnxscript (no op). We turn it into','line_number':179,'multiline':False]['text':' a string.','line_number':180,'multiline':False]['text':' all other cases, we do nothing.','line_number':183,'multiline':False]['text':' We omit if dtype is not provided, because onnxscript handles the','line_number':202,'multiline':False]['text':' default case.','line_number':203,'multiline':False]['text':' ex: aten::split - in onnx_dtype: seq(tensor)','line_number':229,'multiline':False]['text':' onnxscript_values is a single tensor, but expected_values is a list of tensors.','line_number':230,'multiline':False]['text':' There is no shape/type from None.','line_number':239,'multiline':False]['text':' NOTE: according to https://github.com/pytorch/pytorch/blob/main/torch/_meta_registrations.py,','line_number':240,'multiline':False]['text':' None could be a valid value for return type, so we need to handle it.','line_number':241,'multiline':False]['text':' e.g. the function: meta__scaled_dot_product_flash() in cpu mode.','line_number':242,'multiline':False]['text':' aten::sym_size output is a int, not a tensor, which stands','line_number':245,'multiline':False]['text':' for the size of one dim. We treat it as 1-D tensor.','line_number':246,'multiline':False]['text':' Like torch.view_as_real, we flatten complex tensors to real tensors with','line_number':257,'multiline':False]['text':' additional last dimension of 2','line_number':258,'multiline':False]['text':' complex64 -> float32, complex128 -> float64, etc.','line_number':260,'multiline':False]['text':' Dispatcher needs to know the value is complex','line_number':264,'multiline':False]['text':' We set node output sizes to be dynamic to continue the model conversion,','line_number':267,'multiline':False]['text':' and inputs are also set to be dynamic in add_input().','line_number':268,'multiline':False]['text':' naming','line_number':272,'multiline':False]['text':' TODO(titaiwang): aten::sym_size has overload, but fx graph is using','line_number':285,'multiline':False]['text':' overloadpacket for some reasons.','line_number':286,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/97201','line_number':287,'multiline':False]['text':' We manually assigned overload for aten::sym_size.','line_number':288,'multiline':False]['text':' type: ignore[union-attr]','line_number':290,'multiline':False]['text':' type: ignore[union-attr]','line_number':292,'multiline':False]['text':' This function assumes the order of arguments in FX op is the','line_number':294,'multiline':False]['text':' same as the order of arguments in TorchScript op.','line_number':295,'multiline':False]['text':' Get default from schema.','line_number':308,'multiline':False]['text':' THIS SHOULD BE THE ONLY STATE IN THIS CLASS (constraint from diagnosticS API)','line_number':371,'multiline':False]['text':' TODO: Diagnostics API should be revised to get rid of this attribute.','line_number':372,'multiline':False]['text':' DO NOT add other class-level attributes.','line_number':373,'multiline':False]['text':' Record stack trace of node in diagnostic.','line_number':411,'multiline':False]['text':' If parent_onnxscript_graph is provided, we assume fx_graph_module is a','line_number':490,'multiline':False]['text':' submodule representing a forward call of an nn.Module.','line_number':491,'multiline':False]['text':' Compose package and version where the nn.Module is defined as domain name','line_number':492,'multiline':False]['text':' for the local function.','line_number':493,'multiline':False]['text':' Leave as default domain name for the root module.','line_number':506,'multiline':False]['text':' In the following loop, a TorchScript graph is created to','line_number':515,'multiline':False]['text':' represent the input FX graph with ONNX symbols (e.g., onnx::add).','line_number':516,'multiline':False]['text':' To connect the values to nodes in the TorchScript graph, we maintain','line_number':517,'multiline':False]['text':' fx_name_to_onnxscript_value. Basically, we want to translate','line_number':518,'multiline':False]['text':'   fx_tensor_x (type: torch.fx.Node) -> fx_node_1 -> fx_tensor_y (type: torch.fx.Node)','line_number':519,'multiline':False]['text':' to','line_number':520,'multiline':False]['text':'   fx_name_to_onnxscript_value[fx_tensor_x.name] -> onnx_node_1 -> fx_name_to_onnxscript_value[fx_tensor_y.name]','line_number':521,'multiline':False]['text':' TODO: Fix FakeTensorMode limitation asap','line_number':530,'multiline':False]['text':' We want to pass list of ints and floats to TorchScript graph correctly','line_number':531,'multiline':False]['text':' in _export_fx_to_ts, so we must disable FakeTensorMode. Otherwise, graph may','line_number':532,'multiline':False]['text':' receive FakeTensor and results runtime error. In addition, TorchScript-based','line_number':533,'multiline':False]['text':' ONNX exporter used in _ts_graph_to_onnx_model_in_protobuf is not compatible','line_number':534,'multiline':False]['text':' with FakeTensorMode.','line_number':535,'multiline':False]['text':' node_fixed_shape is only used on op_level_debug purpose.','line_number':537,'multiline':False]['text':' Input of graph.','line_number':567,'multiline':False]['text':' The node.meta["val"] is generated by FakeTensorProp.','line_number':568,'multiline':False]['text':' NOTE: add_input() intends to create nodes with shape/type','line_number':569,'multiline':False]['text':' NOTE: During the tracing, when inputs are constants, they are represented','line_number':571,'multiline':False]['text':' by nodes with node.meta['val'] being None (nn.Module to dynamo_export)','line_number':572,'multiline':False]['text':' or nodes with node.meta['val'] being a builtin value (ExportedProgram to dynamo_export).','line_number':573,'multiline':False]['text':' Nonethless, the nodes are not consumed by others, so we don't need to','line_number':574,'multiline':False]['text':' create a TorchScriptTensor for them.','line_number':575,'multiline':False]['text':' NOTE: ONNX doesn't support tensor of complex64/complex128, so we','line_number':581,'multiline':False]['text':' convert them to float32/float64 with real representation.','line_number':582,'multiline':False]['text':' aten ops and other stateless functions.','line_number':626,'multiline':False]['text':' type: ignore[union-attr,index]','line_number':628,'multiline':False]['text':' type: ignore[union-attr,index]','line_number':630,'multiline':False]['text':' type: ignore[index]','line_number':632,'multiline':False]['text':' Map FX inputs to ONNX inputs and fill optional inputs with default values.','line_number':643,'multiline':False]['text':' torch_args and torch_kwargs are for op-level validation','line_number':644,'multiline':False]['text':' Dispatch to ONNX op through OpShema. The input argument dtypes are compared to','line_number':653,'multiline':False]['text':' function signature in OpSchema, and find the best matched overload.','line_number':654,'multiline':False]['text':' type: ignore[no-redef]','line_number':662,'multiline':False]['text':' Assign type and shape from fx graph.','line_number':669,'multiline':False]['text':' One fx node could produce multiple outputs (e.g., tuple of tensors); in','line_number':671,'multiline':False]['text':' that case, v is a tuple of TorchScriptTensors.','line_number':672,'multiline':False]['text':' NOTE(titaiwang): We bypass two kinds of ops as it's not meaningful to','line_number':676,'multiline':False]['text':' validate them with op level debug.','line_number':677,'multiline':False]['text':' 1. aten::sym_size: The op is simply get item from a list of tensors.','line_number':678,'multiline':False]['text':' 2. BuiltinFunction: It doesn't supported tensor','line_number':679,'multiline':False]['text':' ONNX can't represent collection types (e.g., dictionary, tuple of tuple of','line_number':712,'multiline':False]['text':' tensor, etc), we flatten the collection and register each element as output.','line_number':713,'multiline':False]['text':' TODO(wechi): Support call_method.','line_number':724,'multiline':False]['text':' TODO: We may want to consider other naming styles. The goal is to be stable and','line_number':781,'multiline':False]['text':' unique such that it can be easily identified in case of kernel substitution.','line_number':782,'multiline':False]['text':' Example for current style is combination of qualified module class name and','line_number':783,'multiline':False]['text':' module attribute name: `torch_nn_modules_conv_Conv2d_conv1`.','line_number':784,'multiline':False]['text':' Other naming styles such as qualified module class name made unique can also','line_number':785,'multiline':False]['text':' be considered.','line_number':786,'multiline':False]['text':' type: ignore[no-redef]','line_number':789,'multiline':False]['text':' Skip op_level_validation for call_module. Subgraph nodes are validated individually.','line_number':803,'multiline':False]['text':' TODO: Constant tensors and buffer/weights are both categorized into `get_attr`,','line_number':819,'multiline':False]['text':' but they are different to ONNX. We need to distinguish them.','line_number':820,'multiline':False]['text':' Constant tensors should become ONNX constants in the graph, while buffers/weights ONNX initializers.','line_number':821,'multiline':False]['text':' For now they are all converted to ONNX initializers.','line_number':822,'multiline':False]['text':' Parameter/buffer name cannot contain "."','line_number':828,'multiline':False]['text':' Revert from "/" to restore namespace formatting.','line_number':829,'multiline':False]