['text':' Add new placeholder nodes in the order specified by the inputs','line_number':69,'multiline':False]['text':' Can't use node_copy here as we may be turning previous call_function into placeholders','line_number':72,'multiline':False]['text':' Construct the forward module','line_number':140,'multiline':False]['text':' Keep symints separate from tensors, passed between fwd/bwd graphs, and in the right order.','line_number':141,'multiline':False]['text':' This is to filter out saved values that don't actually end up being used by the backwards pass','line_number':153,'multiline':False]['text':' Now that we have the finalized list of saved values, we need to ensure','line_number':166,'multiline':False]['text':' we propagate all symbols which are referenced by backwards inputs.','line_number':167,'multiline':False]['text':' These are not directly used in the graph but are required for downstream','line_number':168,'multiline':False]['text':' sizevar assignment','line_number':169,'multiline':False]['text':' Some symbols may already be bound in the directly saved_sym_nodes,','line_number':174,'multiline':False]['text':' keep track of them so we don't re-bind them','line_number':175,'multiline':False]['text':' Now go through all of the prospective backward inputs and track any','line_number':184,'multiline':False]['text':' other symbols we need to bind','line_number':185,'multiline':False]['text':' NB: Deterministic order please!','line_number':191,'multiline':False]['text':' NB: For well formed graphs, the symbol should always be present,','line_number':193,'multiline':False]['text':' but we also have ways to produce ill-formed graphs, e.g., direct','line_number':194,'multiline':False]['text':' make_fx usages, so don't choke in this case','line_number':195,'multiline':False]['text':' Update saved_sym_nodes that are now reordered to have all bindings at','line_number':202,'multiline':False]['text':' front. This can also be used later on to figure out the position of saved','line_number':203,'multiline':False]['text':' sym nodes in the output of fwd graph.','line_number':204,'multiline':False]['text':' Now, we re-generate the fwd/bwd graphs.','line_number':208,'multiline':False]['text':' NB: This might increase compilation time, but I doubt it matters','line_number':209,'multiline':False]['text':' Symints must be kept separate from tensors so that PythonFunction only calls','line_number':267,'multiline':False]['text':' save_for_backward on tensors and stashes symints in autograd .ctx','line_number':268,'multiline':False]['text':' Since we can't save tuple of tensor values, we need to flatten out what we're saving','line_number':274,'multiline':False]['text':' If we have a tensor in the forward, where only its sizes/strides are needed in the backward,','line_number':282,'multiline':False]['text':' and not the actual tensor data,','line_number':283,'multiline':False]['text':' then it will be a lot cheaper to save only the sizes/strides, and not the actual tensor.','line_number':284,'multiline':False]['text':'','line_number':285,'multiline':False]['text':' Note that saving the tensor could also cause compilation problems:','line_number':286,'multiline':False]['text':' If the user mutated an input in the forward and uses its sizes/strides in the backward,','line_number':287,'multiline':False]['text':' then we would be obligated to clone the input before saving it to appease autograd.','line_number':288,'multiline':False]['text':' (This is how we originally found this bug).','line_number':289,'multiline':False]['text':' NB: The fallback values here are meaningless, maybe we should respect','line_number':317,'multiline':False]['text':' torch._inductor.config.unbacked_symint_fallback (but this is a','line_number':318,'multiline':False]['text':' layering violation)','line_number':319,'multiline':False]['text':' Only needed since we don't always trace with fake tensors.','line_number':327,'multiline':False]['text':' Used for some investigative purposes','line_number':338,'multiline':False]['text':' currently aot autograd uses packet not overload','line_number':359,'multiline':False]['text':' Base case','line_number':369,'multiline':False]['text':' Handle output node','line_number':374,'multiline':False]['text':' Get the depth of args and set the depth of this node','line_number':382,'multiline':False]['text':' factory ops like full, rand might not have any input args','line_number':384,'multiline':False]['text':' Add new placeholder nodes in the order specified by the inputs','line_number':422,'multiline':False]['text':' Can't use node_copy here as we may be turning previous call_function into placeholders','line_number':426,'multiline':False]['text':' Populate depth for the nodes. Depth is the distance from the inputs.','line_number':435,'multiline':False]['text':' Bias traversal towards the nodes that have higher depth - prioritizes','line_number':444,'multiline':False]['text':' critical path first.','line_number':445,'multiline':False]['text':' Find first bwd node in the graph','line_number':451,'multiline':False]['text':' Build the graph op-by-op by starting from the node all the way to the end','line_number':462,'multiline':False]['text':' The output node is already built by the traversal.','line_number':466,'multiline':False]['text':' During user-driven activation checkpointing, we have to ensure that a rng','line_number':472,'multiline':False]['text':' op in fwd yields the same output as the recomputed rng op in the bwd.  To','line_number':473,'multiline':False]['text':' do this, we use functionalize wrappers to wrap the random ops and share','line_number':474,'multiline':False]['text':' rng state between the fwd and bwd graphs.','line_number':475,'multiline':False]['text':' There are 3 main steps to do this','line_number':477,'multiline':False]['text':' Step 1 - Construct a mapping of rng node between the fwd and its counterpart in bwd.','line_number':478,'multiline':False]['text':' Step 2 - Modify the fwd pass such that','line_number':479,'multiline':False]['text':'   1) Replace rand with run_and_save_rng_state wrapper','line_number':480,'multiline':False]['text':'   2) Replace the users of the original op with the output[1] of this op.','line_number':481,'multiline':False]['text':'   3) Collect all the rng_state - output[0] of each op, and make them','line_number':482,'multiline':False]['text':'   output nodes. Special care needs to be taken here because fwd outputs','line_number':483,'multiline':False]['text':'   has symints at the very end.','line_number':484,'multiline':False]['text':' Step 3 - Modify the bwd pass such that','line_number':485,'multiline':False]['text':'   1) Add the input nodes just before the tangents for the stashed rng states','line_number':486,'multiline':False]['text':'   2) Replace rand with run_with_save_rng_state wrappers','line_number':487,'multiline':False]['text':'   3) Use the stashed states as inputs to these ops','line_number':488,'multiline':False]['text':' Unique id to generate name','line_number':490,'multiline':False]['text':' Step 1 - Construct a mapping of rng node between the fwd and its counterpart in bwd.','line_number':527,'multiline':False]['text':' Step 2 - Modify the fwd pass such that','line_number':554,'multiline':False]['text':' Step 3 - Modify the bwd pass such that','line_number':572,'multiline':False]['text':' Add the rng states in the output of the fwd graph. AOT Autograd assumes','line_number':591,'multiline':False]['text':' that symints are at the end of forward graph outputs. So, insert the new','line_number':592,'multiline':False]['text':' rng states accordingly.','line_number':593,'multiline':False]['text':'  add the CSE pass','line_number':660,'multiline':False]['text':' networkx blows up on graphs with no required backward nodes','line_number':698,'multiline':False]['text':' Since there's nothing to partition anyway, and the default partitioner can "handle"','line_number':699,'multiline':False]['text':' this case, send our graph over to the default partitioner.','line_number':700,'multiline':False]['text':' compiler == "nvfuser" is the default set of recomputable ops','line_number':715,'multiline':False]['text':' noqa: E501,B950','line_number':716,'multiline':False]['text':' noqa: E501,B950','line_number':719,'multiline':False]['text':' Natalia said that we should allow recomputing indexing :)','line_number':721,'multiline':False]['text':' noqa: E501,B950','line_number':739,'multiline':False]['text':' If a node *must* be materialized in the backwards pass, then we','line_number':783,'multiline':False]['text':' should never recompute it. This is a pretty subtle point.  In','line_number':784,'multiline':False]['text':' general, the assumption we make is that recomputing a node in the','line_number':785,'multiline':False]['text':' backwards pass is "free". However, if a node must be materialized','line_number':786,'multiline':False]['text':' in the backwards pass, then recomputing it is never free.','line_number':787,'multiline':False]['text':' Arbitrary hack that sometimes seems to help things. The above','line_number':791,'multiline':False]['text':' modification appears to have made this heuristic a lot less critical','line_number':792,'multiline':False]['text':' for performance.','line_number':793,'multiline':False]['text':' TODO: Investigate why this hack helps.','line_number':794,'multiline':False]['text':' TODO: Investigate the interaction with compiler assisted','line_number':795,'multiline':False]['text':' activation checkpointing. Removing the heuristic improves both','line_number':796,'multiline':False]['text':' memory footprint and speedup.','line_number':797,'multiline':False]['text':' If the output of an op is 4x smaller (arbitrary choice),','line_number':801,'multiline':False]['text':' then we don't allow recomputation.','line_number':802,'multiline':False]['text':' We can perform "memory fusion" into a cat, but cat cannot be a','line_number':808,'multiline':False]['text':' producer to a fusion','line_number':809,'multiline':False]['text':' Heuristic to bias towards nodes closer to the backwards pass','line_number':823,'multiline':False]['text':' Complete guess about current value','line_number':824,'multiline':False]['text':' mem_sz = int(mem_sz + node.dist_from_bw)','line_number':826,'multiline':False]['text':' If a node can't be recomputed (too expensive or involves randomness),','line_number':845,'multiline':False]['text':' we prevent it from being recomputed by adding an inf edge to the source','line_number':846,'multiline':False]['text':' We only need to ban nodes in the fw pass, as those are the only ones that would be recomputed.','line_number':847,'multiline':False]['text':' Checks if a node is actually a tuple. Can be simplified to just an isinstance check if we always use faketensors.','line_number':851,'multiline':False]['text':' Creates the weights on the "node" edge','line_number':862,'multiline':False]['text':' To make this stuff deterministic','line_number':885,'multiline':False]['text':' save_for_backward on tensors and stashes symints in autograd .ctx','line_number':888,'multiline':False]['text':' NB: saved_sym_nodes will be mutated to reflect the actual saved symbols','line_number':891,'multiline':False]