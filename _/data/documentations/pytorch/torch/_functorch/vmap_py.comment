['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' Checks that all args-to-be-batched have the same batch dim size','line_number':48,'multiline':False]['text':' If value is a tuple, check it has length `num_elements`.','line_number':68,'multiline':False]['text':' If value is not a tuple, make a tuple with `value` repeated `num_elements` times','line_number':69,'multiline':False]['text':' Creates BatchedTensors for every Tensor in arg that should be batched.','line_number':126,'multiline':False]['text':' Returns the (potentially) batched arguments and the batch_size.','line_number':127,'multiline':False]['text':' See NOTE [Ignored _remove_batch_dim, _add_batch_dim]','line_number':132,'multiline':False]['text':' out_dim is non None','line_number':149,'multiline':False]['text':' Undos the batching (and any batch dimensions) associated with the `vmap_level`.','line_number':158,'multiline':False]['text':' Some weird edge case requires us to spell out the following','line_number':173,'multiline':False]['text':' see test_out_dims_edge_case','line_number':174,'multiline':False]['text':' Not all callables have __name__, in fact, only static functions/methods do.','line_number':216,'multiline':False]['text':' A callable created via functools.partial or an nn.Module, to name some','line_number':217,'multiline':False]['text':' examples, don't have a __name__.','line_number':218,'multiline':False]['text':' torch.package, Python 3.11, and torch.jit-less environments are unhappy with','line_number':226,'multiline':False]['text':' decompositions. Only load them when needed if possible.','line_number':227,'multiline':False]['text':' use an alternate way to register an operator into the decomposition table','line_number':241,'multiline':False]['text':' _register_jit_decomposition doesn't work for some operators, e.g. addr,','line_number':242,'multiline':False]['text':'  because the Tensor types generated cannot be unioned by torchscript','line_number':243,'multiline':False]['text':' decomp should be type OpOverload','line_number':244,'multiline':False]['text':' If chunk_size is not specified.','line_number':277,'multiline':False]['text':' remainder chunk','line_number':285,'multiline':False]['text':' transpose chunk dim and flatten structure','line_number':302,'multiline':False]['text':' chunks_flat_args is a list of flatten args','line_number':303,'multiline':False]['text':' chunks_output is a list of chunked outputs','line_number':309,'multiline':False]['text':' flatten chunked outputs:','line_number':310,'multiline':False]['text':' transpose chunk dim and flatten structure','line_number':319,'multiline':False]['text':' flat_output_chunks is flat list of chunks','line_number':320,'multiline':False]['text':' concat chunks on out_dim','line_number':326,'multiline':False]['text':' release tensors','line_number':332,'multiline':False]['text':' Applies vmap on chunked_input and returns concatenated output over the chunks.','line_number':338,'multiline':False]['text':' The way we compute split the input in `_get_chunked_inputs`,','line_number':346,'multiline':False]['text':' we may get a tensor with `0` batch-size. We skip any computation','line_number':347,'multiline':False]['text':' in that case.','line_number':348,'multiline':False]['text':' Eg.','line_number':349,'multiline':False]['text':' >>> chunk_size = 1','line_number':350,'multiline':False]['text':' >>> batch_size = 6','line_number':351,'multiline':False]['text':' >>> t = torch.zeros(batch_size, 1)','line_number':352,'multiline':False]['text':' >>> t.tensor_split([1, 2, 3, 4, 5, 6])','line_number':353,'multiline':False]['text':' (tensor([[0.]]), tensor([[0.]]), tensor([[0.]]), tensor([[0.]]),','line_number':354,'multiline':False]['text':'  tensor([[0.]]), tensor([[0.]]), tensor([], size=(0, 1)))','line_number':355,'multiline':False]['text':' chunked output tensors are held by both `flat_output_chunks` and `chunks_output`.','line_number':369,'multiline':False]['text':' eagerly remove the reference from `chunks_output`.','line_number':370,'multiline':False]['text':' concat chunks on out_dim','line_number':373,'multiline':False]['text':' finally unflatten the output','line_number':376,'multiline':False]['text':' Vmap refactored helper functions:','line_number':380,'multiline':False]['text':' `restore_vmap` is a private helper function. It is vmap but has the following','line_number':397,'multiline':False]['text':' differences:','line_number':398,'multiline':False]['text':' - instead of returning outputs, it returns an (outputs, out_dims) tuple.','line_number':399,'multiline':False]['text':'   out_dims is a pytree of same shape as outputs and contains Optional[int]','line_number':400,'multiline':False]['text':'   specifying where the vmapped dimension, if it exists, is in the corresponding output.','line_number':401,'multiline':False]['text':' - does no validation on in_dims or inputs (vmap expects at least one Tensor to be vmapped).','line_number':402,'multiline':False]['text':'   restore_vmap allows for no inputs to have the vmap dimension','line_number':403,'multiline':False]['text':' - does no validation on outputs (vmap expects only Tensor outputs)','line_number':404,'multiline':False]['text':'   restore_vmap allows for return of arbitrary outputs (not just Tensors)','line_number':405,'multiline':False]['text':'','line_number':406,'multiline':False]['text':' The TL;DR is that restore_vmap is more general than vmap and has a slightly','line_number':407,'multiline':False]['text':' different API. The relaxations are so that we can "pause" vmap in the middle','line_number':408,'multiline':False]['text':' of its execution and then "restore" it later (this is what we do in','line_number':409,'multiline':False]['text':' the generate_vmap_rule=True implementation of autograd.Function).','line_number':410,'multiline':False]['text':'','line_number':411,'multiline':False]['text':' restore_vmap can be technically used in the implementation of vmap, but doing','line_number':412,'multiline':False]['text':' that refactor is a bit technically challenging because:','line_number':413,'multiline':False]['text':' - vmap couples the tensor-wrapping code with error checking','line_number':414,'multiline':False]['text':' - vmap's tensor unwrapping code is in C++; we would need to rewrite part of it','line_number':415,'multiline':False]['text':'   in python because it overlaps with unwrap_batched','line_number':416,'multiline':False]