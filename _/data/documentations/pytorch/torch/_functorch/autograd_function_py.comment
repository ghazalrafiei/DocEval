['text':' autograd.Function technically runs before the regular PyTorch dispatcher.','line_number':22,'multiline':False]['text':' This is how features like autocast and torch_dispatch (e.g. PythonTLSSnapshot)','line_number':23,'multiline':False]['text':' work with it. One day we might decide to change this, but until then,','line_number':24,'multiline':False]['text':' we need to give the illusion that autograd.Function runs before those things.','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' We do this by using creating a custom HigherOrderOperator that only functorch','line_number':27,'multiline':False]['text':' dispatches specially.','line_number':28,'multiline':False]['text':' When custom_function_call is done dispatching through functorch,','line_number':34,'multiline':False]['text':' it should just invoke the autograd.Function. This is consistent','line_number':35,'multiline':False]['text':' with the autograd.Function behavior of being invoked before the','line_number':36,'multiline':False]['text':' PyTorch dispatcher.','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' This will lead us into trouble later down the line, but this is','line_number':39,'multiline':False]['text':' pre-existing. There is an invariant that a function traced by','line_number':40,'multiline':False]['text':' make_fx should have the same behavior when provided the same','line_number':41,'multiline':False]['text':' Tensor. However, make_fx sees autograd.Function as a composite','line_number':42,'multiline':False]['text':' (because autograd.Function happens before the Python dispatch key)','line_number':43,'multiline':False]['text':' and only traces the forward pass.','line_number':44,'multiline':False]['text':' "custom_function_call"','line_number':50,'multiline':False]['text':' This is the mechanism for an autograd.Function that works with functorch transforms.','line_number':51,'multiline':False]['text':' It wraps an autograd.Function; interactions with functorch transforms are defined','line_number':52,'multiline':False]['text':' via PyDispatcher and HigherOrderOperator rather than through the traditional PyTorch','line_number':53,'multiline':False]['text':' dispatcher.','line_number':54,'multiline':False]['text':' The grad rule for custom_function_call is to construct a new _SingleLevelFunction','line_number':58,'multiline':False]['text':' (autograd.Function that only works with a single layer (level) of functorch) that:','line_number':59,'multiline':False]['text':' - unwraps the inputs','line_number':60,'multiline':False]['text':' - redispatches to custom_function_call','line_number':61,'multiline':False]['text':' - wraps the outputs','line_number':62,'multiline':False]['text':' and whose backward pass calls the original autograd.Function's backward.','line_number':63,'multiline':False]['text':'','line_number':64,'multiline':False]['text':' Why do we need to redispatch to custom_function_call?','line_number':65,'multiline':False]['text':' -----------------------------------------------------','line_number':66,'multiline':False]['text':' This is consistent with how ATen operators work with functorch's grad transform:','line_number':67,'multiline':False]['text':' they always redispatch to the original operator.','line_number':68,'multiline':False]['text':' Consider torch.sin, and let's say we do grad0(grad1(torch.sin))(x)','line_number':69,'multiline':False]['text':'','line_number':70,'multiline':False]['text':' grad1 will:','line_number':71,'multiline':False]['text':' - set up the autograd graph','line_number':72,'multiline':False]['text':' - unwrap the inputs','line_number':73,'multiline':False]['text':' - redispatch to at::sin (*)','line_number':74,'multiline':False]['text':' - rewrap the outputs on the return','line_number':75,'multiline':False]['text':'','line_number':76,'multiline':False]['text':' On the redispatch in (*), grad0 will:','line_number':77,'multiline':False]['text':' - set up the autograd graph','line_number':78,'multiline':False]['text':' - unwrap the inputs','line_number':79,'multiline':False]['text':' - redispatch to at::sin','line_number':80,'multiline':False]['text':' - rewrap the outputs on the return','line_number':81,'multiline':False]['text':'','line_number':82,'multiline':False]['text':' To "set up the autograd graph", we generate a _SingleLevelFunction','line_number':83,'multiline':False]['text':' and apply it.','line_number':84,'multiline':False]['text':' Both enable_grad() and _set_fwd_grad_enabled() are necessary no matter','line_number':102,'multiline':False]['text':' the transform. _SingleLevelFunction will turn off both fwd and bwd','line_number':103,'multiline':False]['text':' gradient computation and we need to turn it back on here.','line_number':104,'multiline':False]['text':' See NOTE [mark_dirty object identity check]','line_number':108,'multiline':False]['text':' backward is only used if the transform is TransformType.Grad','line_number':121,'multiline':False]['text':' jvp is only used if the transform is TransformType.Jvp','line_number':126,'multiline':False]['text':' This is the sequence of magic words to dynamically generate a Subclass with','line_number':131,'multiline':False]['text':' a given name. A Tensor's .grad_fn field has a class name that is the original','line_number':132,'multiline':False]['text':' autograd.Function's name + Backward, so we do this to generate some','line_number':133,'multiline':False]['text':' meaningful name.','line_number':134,'multiline':False]['text':' wrap_outputs_maintaining_identity handles outputs from the vmap,','line_number':148,'multiline':False]['text':' backward (vjp), and jvp staticmethod. The way it distinguishes','line_number':149,'multiline':False]['text':' between the vmap case and the {backward, jvp} case is if the out_dims','line_number':150,'multiline':False]['text':' are specified or not.','line_number':151,'multiline':False]['text':'','line_number':152,'multiline':False]['text':' NB: we cannot use out_dims=None as the deciding factor. This because','line_number':153,'multiline':False]['text':' out_dims=None can still happen in the vmap staticmethod! What the','line_number':154,'multiline':False]['text':' user is saying in that case is that their output does not have a','line_number':155,'multiline':False]['text':' dimension that is being vmapped over, which is valid.','line_number':156,'multiline':False]['text':' NOTE [mark_dirty object identity check]','line_number':159,'multiline':False]['text':' autograd.Function's ctx.mark_dirty expect a returned input','line_number':160,'multiline':False]['text':' to have the same object identity as the input.','line_number':161,'multiline':False]['text':' Mode-only functorch will greatly simplify this logic.','line_number':162,'multiline':False]['text':' _broadcast_to_and_flatten returns None if it is unable to broadcast.','line_number':180,'multiline':False]['text':' TODO: update following link from master to stable once that's out','line_number':181,'multiline':False]['text':' type: ignore[index]','line_number':202,'multiline':False]['text':' NOTE: [functorch vjp and autograd interaction]','line_number':209,'multiline':False]['text':' There's an edge case with the functorch vjp and autograd interaction','line_number':210,'multiline':False]['text':' that will eventually be fixed by mode-only functorch.','line_number':211,'multiline':False]['text':' The TL;DR is that there's no way to unwrap a dead GradTensorWrapper,','line_number':212,'multiline':False]['text':' so we (the framework) need to do it manually. Regular PyTorch operators','line_number':213,'multiline':False]['text':' automatically do so this is consistent.','line_number':214,'multiline':False]['text':'','line_number':215,'multiline':False]['text':' class MyExp(torch.autograd.Function):','line_number':216,'multiline':False]['text':'     @staticmethod','line_number':217,'multiline':False]['text':'     def forward(x):','line_number':218,'multiline':False]['text':'         return x.exp()','line_number':219,'multiline':False]['text':'','line_number':220,'multiline':False]['text':'     @staticmethod','line_number':221,'multiline':False]['text':'     def setup_context(ctx, inputs, output):','line_number':222,'multiline':False]['text':'         y = output','line_number':223,'multiline':False]['text':'         ctx.save_for_backward(y)','line_number':224,'multiline':False]['text':'','line_number':225,'multiline':False]['text':'     @staticmethod','line_number':226,'multiline':False]['text':'     def backward(gy):','line_number':227,'multiline':False]['text':'         y, = ctx.saved_tensors()','line_number':228,'multiline':False]['text':'         return MyMul.apply(gy, y)','line_number':229,'multiline':False]['text':'','line_number':230,'multiline':False]['text':' x = torch.randn([], requires_grad=True)','line_number':231,'multiline':False]['text':' gy = torch.randn([], requires_grad=True)','line_number':232,'multiline':False]['text':' _, vjp_fn = vjp(MySin.apply, x)','line_number':233,'multiline':False]['text':' result = vjp_fn(gy)','line_number':234,'multiline':False]['text':'','line_number':235,'multiline':False]['text':' MyMul is an autograd.Function that is not shown here.','line_number':236,'multiline':False]['text':' It saves a `y` for backward (since gy requires grad).','line_number':237,'multiline':False]['text':'','line_number':238,'multiline':False]['text':' in vjp_fn(gy), we get:','line_number':239,'multiline':False]['text':' > MyMul.apply(gy, GradTensorWrapper(y, level=dead))','line_number':240,'multiline':False]['text':' Because the y that is saved for backward by MyExp is a GradTensorWrapper','line_number':241,'multiline':False]['text':' but is now dead since we are outside the vjp context.','line_number':242,'multiline':False]['text':'','line_number':243,'multiline':False]['text':' PyTorch dispatcher operations, upon seeing a dead GradTensorWrapper,','line_number':244,'multiline':False]['text':' will automatically unwrap the GradTensorWrapper when applied.','line_number':245,'multiline':False]['text':' But since autograd.Function technically sits above the regular PyTorch','line_number':246,'multiline':False]['text':' dispatcher, it doesn't get this treatment. So we manually do','line_number':247,'multiline':False]['text':' the unwrapping to be consistent with regular PyTorch dispatcher operations.','line_number':248,'multiline':False]['text':' TODO: Update link to stable once that's out','line_number':274,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/92029','line_number':275,'multiline':False]['text':' TODO: Update link to stable once that's out','line_number':286,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/92029','line_number':287,'multiline':False]['text':' If none of the tensors are batched at the current level, then we skip the','line_number':302,'multiline':False]['text':' current level. This saves the user from needing to handle this case in','line_number':303,'multiline':False]['text':' their vmap staticmethod (and is consistent with our C++ batching rule API)','line_number':304,'multiline':False]['text':' See NOTE [mark_dirty object identity check]','line_number':314,'multiline':False]['text':' The following values are saved from the forward() and setup_context()','line_number':344,'multiline':False]['text':' and used in backward().','line_number':345,'multiline':False]['text':' Why do we save the values out here instead of on the ctx object?','line_number':346,'multiline':False]['text':' - out_dims: There's no way to retrieve this from forward()','line_number':347,'multiline':False]['text':' - input_shapes, saved_tensors_bdims: I'm a bit scared of nesting','line_number':348,'multiline':False]['text':'   vmap(vmap( but not completely sure if it is a problem. If we','line_number':349,'multiline':False]['text':'   assigned those fields to the ctx object, the worry is that they','line_number':350,'multiline':False]['text':'   get overwritten.','line_number':351,'multiline':False]['text':' wrapped_ctx.save_for_backward will:','line_number':368,'multiline':False]['text':' - unwrap batchedtensors into (tensor, bdim)','line_number':369,'multiline':False]['text':' - save_for_backward(*unwrapped_tensors)','line_number':370,'multiline':False]['text':' - assign the bdims to wrapped_ctx._pt_saved_tensors_bdims','line_number':371,'multiline':False]['text':' input_shapes are used for reductify later to reduce expanded gradients','line_number':375,'multiline':False]['text':' to the correct shape.','line_number':376,'multiline':False]['text':' See NOTE: [Why can't we rely on autograd to reduce expanded gradients?]','line_number':377,'multiline':False]['text':' for more details','line_number':378,'multiline':False]['text':' See NOTE: [Why do we need to run setup_context under a vmap?]','line_number':385,'multiline':False]['text':' tangents might be None, so we need to replace','line_number':450,'multiline':False]['text':' the corresponding in_dims with None.','line_number':451,'multiline':False]['text':' NOTE: [Why do we need to run setup_context under a vmap?]','line_number':460,'multiline':False]['text':' Consider the following autograd.Function','line_number':461,'multiline':False]['text':'','line_number':462,'multiline':False]['text':' class Sum(torch.autograd.Function):','line_number':463,'multiline':False]['text':'    @staticmethod','line_number':464,'multiline':False]['text':'    def forward(x):','line_number':465,'multiline':False]['text':'        return x.sum()','line_number':466,'multiline':False]['text':'    @staticmethod','line_number':467,'multiline':False]['text':'    def setup_context(ctx, inputs, outputs):','line_number':468,'multiline':False]['text':'        ctx.x_shape = inputs[0]','line_number':469,'multiline':False]['text':'    @staticmethod','line_number':470,'multiline':False]['text':'    def backward(ctx, gy):','line_number':471,'multiline':False]['text':'        return gy.expand(ctx.x_shape)','line_number':472,'multiline':False]['text':'','line_number':473,'multiline':False]['text':' x = torch.randn(B, 4)','line_number':474,'multiline':False]['text':' in_dims = 0','line_number':475,'multiline':False]['text':' vmap(Sum.apply, in_dims)(x)','line_number':476,'multiline':False]['text':'','line_number':477,'multiline':False]['text':' Let’s assume for a moment that we didn’t vmap setup_context in VmappedSum:','line_number':478,'multiline':False]['text':'','line_number':479,'multiline':False]['text':' class VmappedSum(torch.autograd.Function):','line_number':480,'multiline':False]['text':'    @staticmethod','line_number':481,'multiline':False]['text':'    def forward(x):','line_number':482,'multiline':False]['text':'        return vmap(Sum.forward, in_dims)(x)','line_number':483,'multiline':False]['text':'','line_number':484,'multiline':False]['text':'    @staticmethod','line_number':485,'multiline':False]['text':'    def setup_context(ctx, inputs, outputs):','line_number':486,'multiline':False]['text':'        Sum.setup_context(ctx, inputs, outputs)','line_number':487,'multiline':False]['text':'','line_number':488,'multiline':False]['text':'    @staticmethod','line_number':489,'multiline':False]['text':'    def backward(ctx, gy):','line_number':490,'multiline':False]['text':'        def backward_no_context(gy):','line_number':491,'multiline':False]['text':'            return gy.expand(ctx.x_shape)','line_number':492,'multiline':False]['text':'','line_number':493,'multiline':False]['text':'        dims = (0,)','line_number':494,'multiline':False]['text':'        gx = vmap(backward_no_context, dims)(gy)','line_number':495,'multiline':False]['text':'        return gx','line_number':496,'multiline':False]['text':'','line_number':497,'multiline':False]['text':' We end up saving [B, 4] as x_shape. In the backward, gy has shape [B],','line_number':498,'multiline':False]['text':' and we’re doing:','line_number':499,'multiline':False]['text':'','line_number':500,'multiline':False]['text':' def backward_no_context(gy):','line_number':501,'multiline':False]['text':'     return gy.expand([B, 4])','line_number':502,'multiline':False]['text':'','line_number':503,'multiline':False]['text':' gx = vmap(backward_no_context, dims)(gy: "Tensor[B]")','line_number':504,'multiline':False]['text':'','line_number':505,'multiline':False]['text':' This gives us the wrong result (gx has shape [B, B, 4], but it should','line_number':506,'multiline':False]['text':' have shape [4]). Performing vmap over setup_context means the shape','line_number':507,'multiline':False]['text':' saved has shape [4] and leads to a correct result shape for gx.','line_number':508,'multiline':False]['text':' Wraps a ctx object. Forwards all attr accesses to the underlying object','line_number':510,'multiline':False]['text':' except for the attrs in _pt_attrs','line_number':511,'multiline':False]['text':' Wraps ctx to create a new ctx object that overrides saved_tensors.','line_number':536,'multiline':False]['text':' NOTE: [Why can't we rely on autograd to reduce expanded gradients?]','line_number':598,'multiline':False]['text':' For reverse-mode AD,','line_number':599,'multiline':False]['text':' given a grad_input and input, it is valid for the user to return a','line_number':600,'multiline':False]['text':' grad_input that has a broadcasted shape when compared to the input.','line_number':601,'multiline':False]['text':' In this situation, autograd automatically reduces the grad_input to','line_number':602,'multiline':False]['text':' the shape of the input.','line_number':603,'multiline':False]['text':'','line_number':604,'multiline':False]['text':' However, when input_bdim is not None, we have problems.','line_number':605,'multiline':False]['text':'','line_number':606,'multiline':False]['text':' [example 1]','line_number':607,'multiline':False]['text':' grad_input: Tensor[3, 4], input: Tensor[B, 4]','line_number':608,'multiline':False]['text':' We can expand grad_input to Tensor[B, 3, 4], but that isn't broadcastable','line_number':609,'multiline':False]['text':' from [B, 4].','line_number':610,'multiline':False]['text':'','line_number':611,'multiline':False]['text':' [example 2]','line_number':612,'multiline':False]['text':' grad_input: Tensor[3, B, 4], input: Tensor[B, 4]','line_number':613,'multiline':False]['text':' We can swizzle grad_input to Tensor[B, 3, 4], but that isn't broadcastable','line_number':614,'multiline':False]['text':' from [B, 4].','line_number':615,'multiline':False]['text':'','line_number':616,'multiline':False]['text':' This means that we need to also reduce the grad_input to the shape of the','line_number':617,'multiline':False]['text':' input. This behavior is controlled by the `target_shape_without_bdim_to_reduce_to` flag;','line_number':618,'multiline':False]['text':' if not-None then we do the reducing manually, otherwise, we do not do a reduction.','line_number':619,'multiline':False]