['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' TODO: Remove the following hack for namedtuples','line_number':75,'multiline':False]['text':' Version of autograd.grad that handles outputs that don't depend on inputs','line_number':110,'multiline':False]['text':' NOTE [grad and vjp interaction with no_grad]','line_number':132,'multiline':False]['text':'','line_number':133,'multiline':False]['text':' def f(x):','line_number':134,'multiline':False]['text':'   with torch.no_grad():','line_number':135,'multiline':False]['text':'     c = x ** 2','line_number':136,'multiline':False]['text':'   return x - c','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':' The thing to consider is if enable_grad is on/off before grad gets called.','line_number':139,'multiline':False]['text':'','line_number':140,'multiline':False]['text':' Case 1: enable_grad is on.','line_number':141,'multiline':False]['text':' grad(f)(x)','line_number':142,'multiline':False]['text':' In this case, `grad` should respect the inner torch.no_grad.','line_number':143,'multiline':False]['text':'','line_number':144,'multiline':False]['text':' Case 2: enable_grad is off','line_number':145,'multiline':False]['text':' with torch.no_grad():','line_number':146,'multiline':False]['text':'   grad(f)(x)','line_number':147,'multiline':False]['text':' In this case, `grad` should respect the inner torch.no_grad, but not the','line_number':148,'multiline':False]['text':' outer one. This is because `grad` is a "function transform": its result','line_number':149,'multiline':False]['text':' should not depend on the result of a context manager outside of `f`.','line_number':150,'multiline':False]['text':'','line_number':151,'multiline':False]['text':' This gives us the following desired behavior:','line_number':152,'multiline':False]['text':' - (nested) grad transforms must obey torch.no_grad inside them','line_number':153,'multiline':False]['text':' - (nested) grad transforms should not obey torch.no_grad outside them','line_number':154,'multiline':False]['text':'','line_number':155,'multiline':False]['text':' To achieve this behavior, upon entering grad/vjp:','line_number':156,'multiline':False]['text':' - we save the current ("previous") is_grad_enabled (*)','line_number':157,'multiline':False]['text':' - we unconditionally enable grad.','line_number':158,'multiline':False]['text':'','line_number':159,'multiline':False]['text':' Inside DynamicLayerBackFallback, when we're temporarily popping `grad` layer','line_number':160,'multiline':False]['text':' off the stack:','line_number':161,'multiline':False]['text':' - if grad_mode is disabled, then we do nothing. (there is a torch.no_grad','line_number':162,'multiline':False]['text':'   active, all subsequent grad transforms must obey it).','line_number':163,'multiline':False]['text':' - if grad_mode is enabled, and the previous is_grad_enabled (*) is False,','line_number':164,'multiline':False]['text':'   then we temporarily restore the previous `is_grad_enabled`. This is','line_number':165,'multiline':False]['text':'   because we're crossing the boundary from a `grad` outside the','line_number':166,'multiline':False]['text':'   no_grad to a `grad` inside the no_grad.','line_number':167,'multiline':False]['text':'','line_number':168,'multiline':False]['text':' NB: vjp has some interesting behavior because the vjp's callable can be called','line_number':169,'multiline':False]['text':' under a different grad_mode than the forward computation...','line_number':170,'multiline':False]['text':'','line_number':171,'multiline':False]['text':' NB: forward-mode AD: forward-mode AD doesn't respect torch.no_grad, but','line_number':172,'multiline':False]['text':' it respects c10::AutoFwGradMode. We've implemented the same logic for','line_number':173,'multiline':False]['text':' our jvp transform (it will have special handling if FwGradMode is disabled).','line_number':174,'multiline':False]['text':' How do we increment and decrement the nesting? I don't think we can.','line_number':177,'multiline':False]['text':' This is the same function as vjp but also accepts an argnums argument','line_number':280,'multiline':False]['text':' All args are the same as vjp except for the added argument','line_number':281,'multiline':False]['text':' argnums (Optional[int or tuple[int]]): Optional, specifies the argument(s) to compute gradients with respect to.','line_number':282,'multiline':False]['text':'         If None, computes the gradients with respect to all inputs (used for vjp). Default: None','line_number':283,'multiline':False]['text':'','line_number':284,'multiline':False]['text':' WARN: Users should NOT call this function directly and should just be calling vjp.','line_number':285,'multiline':False]['text':' It is only separated so that inputs passed to jacrev but not differentiated get the correct wrappers.','line_number':286,'multiline':False]['text':'','line_number':287,'multiline':False]['text':' NOTE: All error messages are produced as if vjp was being called, even if this was called by jacrev','line_number':288,'multiline':False]['text':'','line_number':289,'multiline':False]['text':' Returns the same two elements as :func:`vjp` but the function returned, vjp_fn, returns a tuple of VJPs','line_number':290,'multiline':False]['text':' for only the primal elements given by argnums.','line_number':291,'multiline':False]['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':294,'multiline':False]['text':' jacrev and jacfwd don't support complex functions','line_number':353,'multiline':False]['text':' Helper function to throw appropriate error.','line_number':354,'multiline':False]['text':' See NOTE: [Computing jacobian with vmap and vjp for multiple outputs]','line_number':506,'multiline':False]['text':' NB: vjp already checks that all outputs are tensors','line_number':511,'multiline':False]['text':' Step 1: Construct grad_outputs by splitting the standard basis','line_number':512,'multiline':False]['text':' Helper function to compute chunked Jacobian','line_number':519,'multiline':False]['text':' The intermediate chunked calculation are only','line_number':520,'multiline':False]['text':' scoped at this function level.','line_number':521,'multiline':False]['text':' sanity check.','line_number':527,'multiline':False]['text':' Behaviour with `chunk_size=1` is same as `for-loop`','line_number':536,'multiline':False]['text':' i.e. user shouldn't deal with the limitations of vmap.','line_number':537,'multiline':False]['text':' chunk_size is None or chunk_size != 1','line_number':539,'multiline':False]['text':' Short-circuit if we used a single chunk','line_number':550,'multiline':False]['text':' Concatenate chunks.','line_number':553,'multiline':False]['text':' Iterate and concat the jacobians of different','line_number':555,'multiline':False]['text':' inputs.','line_number':556,'multiline':False]['text':' Helper function to compute chunked Jacobian','line_number':564,'multiline':False]['text':' The intermediate chunked calculation are only','line_number':565,'multiline':False]['text':' scoped at this function level.','line_number':566,'multiline':False]['text':' Don't pre-allocate if we have a single chunk.','line_number':569,'multiline':False]['text':' sanity check.','line_number':577,'multiline':False]['text':' Behaviour with `chunk_size=1` is same as `for-loop`','line_number':586,'multiline':False]['text':' i.e. user shouldn't deal with the limitations of vmap.','line_number':587,'multiline':False]['text':' chunk_size is None or chunk_size != 1','line_number':589,'multiline':False]['text':' Short-circuit if we have a single chunk.','line_number':594,'multiline':False]['text':' and out_vec_size == 1','line_number':596,'multiline':False]['text':' Since we squeezed the output dim','line_number':597,'multiline':False]['text':' Step 2: The returned jacobian is one big tensor per input. In this step,','line_number':611,'multiline':False]['text':' we split each Tensor by output.','line_number':612,'multiline':False]['text':' Step 3: Right now, `jacobian` is a List[List[Tensor]].','line_number':620,'multiline':False]['text':' The outer List corresponds to the number of primals,','line_number':621,'multiline':False]['text':' the inner List corresponds to the number of outputs.','line_number':622,'multiline':False]['text':' We need to:','line_number':623,'multiline':False]['text':' a. Exchange the order of the outer List and inner List','line_number':624,'multiline':False]['text':' b. tree_unflatten the inner Lists (which correspond to the primals)','line_number':625,'multiline':False]['text':' c. handle the argnums=int case','line_number':626,'multiline':False]['text':' d. tree_unflatten the outer List (which corresponds to the outputs)','line_number':627,'multiline':False]['text':' NOTE: [Computing jacobian with vmap and vjp for multiple outputs]','line_number':642,'multiline':False]['text':'','line_number':643,'multiline':False]['text':' Let's consider f(x) = (x**2, x.sum()) and let x = torch.randn(3).','line_number':644,'multiline':False]['text':' It turns out we can compute the jacobian of this function with a single','line_number':645,'multiline':False]['text':' call to autograd.grad by using vmap over the correct grad_outputs.','line_number':646,'multiline':False]['text':'','line_number':647,'multiline':False]['text':' Firstly, one way to compute the jacobian is to stack x**2 and x.sum()','line_number':648,'multiline':False]['text':' into a 4D vector. E.g., use g(x) = torch.stack([x**2, x.sum()])','line_number':649,'multiline':False]['text':'','line_number':650,'multiline':False]['text':' To get the first row of the jacobian, we call','line_number':651,'multiline':False]['text':' >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([1, 0, 0, 0]))','line_number':652,'multiline':False]['text':' To get the 2nd row of the jacobian, we call','line_number':653,'multiline':False]['text':' >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([0, 1, 0, 0]))','line_number':654,'multiline':False]['text':' and so on.','line_number':655,'multiline':False]['text':'','line_number':656,'multiline':False]['text':' Using vmap, we can vectorize all 4 of these computations into one by','line_number':657,'multiline':False]['text':' passing the standard basis for R^4 as the grad_output.','line_number':658,'multiline':False]['text':' vmap(partial(autograd.grad, g(x), x))(torch.eye(4)).','line_number':659,'multiline':False]['text':'','line_number':660,'multiline':False]['text':' Now, how do we compute the jacobian *without stacking the output*?','line_number':661,'multiline':False]['text':' We can just split the standard basis across the outputs. So to','line_number':662,'multiline':False]['text':' compute the jacobian of f(x), we'd use','line_number':663,'multiline':False]['text':' >>> autograd.grad(f(x), x, grad_outputs=_construct_standard_basis_for(...))','line_number':664,'multiline':False]['text':' The grad_outputs looks like the following:','line_number':665,'multiline':False]['text':' ( torch.tensor([[1, 0, 0],','line_number':666,'multiline':False]['text':'                 [0, 1, 0],','line_number':667,'multiline':False]['text':'                 [0, 0, 1],','line_number':668,'multiline':False]['text':'                 [0, 0, 0]]),','line_number':669,'multiline':False]['text':'   torch.tensor([[0],','line_number':670,'multiline':False]['text':'                 [0],','line_number':671,'multiline':False]['text':'                 [0],','line_number':672,'multiline':False]['text':'                 [1]]) )','line_number':673,'multiline':False]['text':'','line_number':674,'multiline':False]['text':' But we're not done yet!','line_number':675,'multiline':False]['text':' >>> vmap(partial(autograd.grad(f(x), x, grad_outputs=...)))','line_number':676,'multiline':False]['text':' returns a Tensor of shape [4, 3]. We have to remember to split the','line_number':677,'multiline':False]['text':' jacobian of shape [4, 3] into two:','line_number':678,'multiline':False]['text':' - one of shape [3, 3] for the first output','line_number':679,'multiline':False]['text':' - one of shape [   3] for the second output','line_number':680,'multiline':False]['text':' This function:','line_number':684,'multiline':False]['text':' - constructs a N=sum(tensor_numels) standard basis. i.e. an NxN identity matrix.','line_number':685,'multiline':False]['text':' - Splits the identity matrix into chunks with each chunk size determined by `tensor_numels`.','line_number':686,'multiline':False]['text':' - Each chunk corresponds to one tensor. The chunk has the same dtype and','line_number':687,'multiline':False]['text':'   device as the tensor','line_number':688,'multiline':False]['text':'','line_number':689,'multiline':False]['text':' For example, with tensor_numels = [1, 2, 1], this function returns:','line_number':690,'multiline':False]['text':' ( tensor([[1],     tensor([[0, 0],      tensor([[0],','line_number':691,'multiline':False]['text':'           [0],             [1, 0],              [0],','line_number':692,'multiline':False]['text':'           [0],             [0, 1],              [0],','line_number':693,'multiline':False]['text':'           [0]])  ,         [0, 0]])  ,          [1]])  )','line_number':694,'multiline':False]['text':'','line_number':695,'multiline':False]['text':' Precondition: tensor_numels == tuple(tensor.numel() for tensor in tensors)','line_number':696,'multiline':False]['text':' Precondition: tensors always has at least one element.','line_number':697,'multiline':False]['text':'','line_number':698,'multiline':False]['text':' See NOTE: [Computing jacobian with vmap and grad for multiple tensors]','line_number':699,'multiline':False]['text':' for context behind this function.','line_number':700,'multiline':False]['text':' NOTE: Argument `chunk_size` is used to generate chunked basis instead of','line_number':701,'multiline':False]['text':'       one huge basis matrix. `chunk_size` dictates the maximum size of the','line_number':702,'multiline':False]['text':'       basis matrix along dim=0.','line_number':703,'multiline':False]['text':' chunk_size is None or chunk_size >= total_numel','line_number':710,'multiline':False]['text':' This is the same function as jvp but also accepts an argnums argument','line_number':933,'multiline':False]['text':' Most args are the same as jvp except for the added argument','line_number':934,'multiline':False]['text':' argnums (Optional[int or tuple[int]]): Optional, specifies the argument(s) to compute gradients with respect to.','line_number':935,'multiline':False]['text':'         If None, computes the gradients with respect to all inputs (used for jvp). Default: None','line_number':936,'multiline':False]['text':' Because of this, tangents must be of length argnums and matches up to the corresponding primal whose index is','line_number':937,'multiline':False]['text':' given by argnums','line_number':938,'multiline':False]['text':'','line_number':939,'multiline':False]['text':' WARN: Users should NOT call this function directly and should just be calling jvp.','line_number':940,'multiline':False]['text':' It is only separated so that inputs passed to jacfwd but not differentiated get the correct wrappers.','line_number':941,'multiline':False]['text':'','line_number':942,'multiline':False]['text':' NOTE: All error messages are produced as if jvp was being called, even if this was called by jacfwd','line_number':943,'multiline':False]['text':'','line_number':944,'multiline':False]['text':' Returns the same two elements as :func:`jvp` but the returned tuple, ``jvp_out``, only has JVPs with respect to','line_number':945,'multiline':False]['text':' the primals given by argnums','line_number':946,'multiline':False]['text':' output[0] is the output of `func(*args)`','line_number':1131,'multiline':False]['text':' aux is in the standard basis format, e.g. NxN matrix','line_number':1142,'multiline':False]['text':' We need to fetch the first element as original `func` output','line_number':1143,'multiline':False]['text':' Most probably below output check can never raise an error','line_number':1149,'multiline':False]['text':' as jvp should test the output before','line_number':1150,'multiline':False]['text':' assert_non_empty_output(jac_outs, 'jacfwd(f, ...)(*args)')','line_number':1151,'multiline':False]['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':1249,'multiline':False]['text':' NB: need create_graph so that backward pass isn't run in no_grad mode','line_number':1276,'multiline':False]['text':' If it's not a functional tensor, just return it.','line_number':1326,'multiline':False]['text':' This can happen if we functionalize a fn that returns a global,','line_number':1327,'multiline':False]['text':' which was never wrapped properly.','line_number':1328,'multiline':False]['text':' Sync any pending updates on the output tensor','line_number':1330,'multiline':False]['text':' Call sync_() on the inputs, to ensure that any pending mutations have been applied.','line_number':1527,'multiline':False]['text':' And if any mutations were applied to the inputs, we need to propagate them back to the user.','line_number':1530,'multiline':False]['text':' Note: We evaluate `fn` twice.','line_number':1582,'multiline':False]['text':' Once for returning the output and other while','line_number':1583,'multiline':False]['text':' tracing the graph.','line_number':1584,'multiline':False]['text':' If this becomes a bottle-neck, we should update','line_number':1585,'multiline':False]['text':' make_fx such that it also returns the output.','line_number':1586,'multiline':False]['text':' tangents for tracing','line_number':1593,'multiline':False]['text':' function to trace','line_number':1596,'multiline':False]['text':' Hold only the meta-data regarding the primals.','line_number':1609,'multiline':False]['text':' jvp_fn : callable to return','line_number':1634,'multiline':False]['text':'   It takes care of checking the argspec of tangents,','line_number':1635,'multiline':False]['text':'   calling the folded fx graph and unflattening fx graph output','line_number':1636,'multiline':False]['text':' const folded graph can return flat output,','line_number':1646,'multiline':False]['text':' so transform output.','line_number':1647,'multiline':False]