['text':' FuncTorchInterpreter is the Python version of Interpreter (recall that','line_number':46,'multiline':False]['text':' the DynamicLayerStack is a stack of interpreters).','line_number':47,'multiline':False]['text':' It is a wrapper around the actual C++ Interpreter object.','line_number':48,'multiline':False]['text':'','line_number':49,'multiline':False]['text':' Keep the methods in sync with aten/src/ATen/functorch/Interpreter.h','line_number':50,'multiline':False]['text':' Process an operation. eg for vmap, this is invoking a batching rule.','line_number':55,'multiline':False]['text':' Conceptually this is analogous to Interpreter::process in C++','line_number':56,'multiline':False]['text':' lower an operation from this Interpreter to the next Interpreter on the stack.','line_number':61,'multiline':False]['text':' Concretely, this involves temporarily popping the current Interpreter.','line_number':62,'multiline':False]['text':' Conceptually this is analogous to Interpreter::sendToNextInterpreter in C++','line_number':63,'multiline':False]['text':' NOTE: [Interpreter cdata vs cptr]','line_number':86,'multiline':False]['text':' cdata is a generic CInterpreter. We wrap it in a CVmapInterpreterPtr','line_number':87,'multiline':False]['text':' so that we can access methods specific to the vmap interpreter','line_number':88,'multiline':False]['text':' See NOTE: [Interpreter cdata vs cptr]','line_number':121,'multiline':False]['text':' GradInterpreter has custom lower because of the no_grad interaction','line_number':134,'multiline':False]['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':135,'multiline':False]['text':' This logic is mirrored from C++ GradInterpreterPtr::sendToNextInterpreter','line_number':136,'multiline':False]['text':' See NOTE: [Interpreter cdata vs cptr]','line_number':150,'multiline':False]['text':' Jvp has custom lower because of the no_fwd_grad interaction','line_number':163,'multiline':False]['text':' See NOTE [grad and vjp interaction with no_grad] for related info.','line_number':164,'multiline':False]['text':' This logic is mirrored from C++ JvpInterpreterPtr::sendToNextInterpreter','line_number':165,'multiline':False]['text':' In traditional PyTorch operators, DispatchKey::FuncTorchTensorWrapper's','line_number':211,'multiline':False]['text':' unwrap_dead_tensors fallback handles unwrapping dead tensor wrappers.','line_number':212,'multiline':False]['text':' PyDispatcher sidesteps the PyTorch dispatcher when dealing with functorch','line_number':213,'multiline':False]['text':' transforms, so we manually unwrap the dead tensors here.','line_number':214,'multiline':False]['text':' This logic won't need to exist when we have mode-only functorch.','line_number':215,'multiline':False]