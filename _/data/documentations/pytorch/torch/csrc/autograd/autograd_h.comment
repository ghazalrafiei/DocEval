['text':'/ Computes the sum of gradients of given tensors with respect to graph leaves.','line_number':8,'multiline':False]['text':'/','line_number':9,'multiline':False]['text':'/ The graph is differentiated using the chain rule. If any of ``tensors``','line_number':10,'multiline':False]['text':'/ are non-scalar (i.e. their data has more than one element) and require','line_number':11,'multiline':False]['text':'/ gradient, then the Jacobian-vector product would be computed, in this case','line_number':12,'multiline':False]['text':'/ the function additionally requires specifying `grad_tensors`. It should be a','line_number':13,'multiline':False]['text':'/ sequence of matching length, that contains the "vector" in the','line_number':14,'multiline':False]['text':'/ Jacobian-vector product, usually the gradient of the differentiated function','line_number':15,'multiline':False]['text':'/ w.r.t. corresponding tensors','line_number':16,'multiline':False]['text':'/ (`torch::Tensor()` is an acceptable value for all tensors that don't need','line_number':17,'multiline':False]['text':'/ gradient tensors).','line_number':18,'multiline':False]['text':'/','line_number':19,'multiline':False]['text':'/ This function accumulates gradients in the leaves - you might need to zero','line_number':20,'multiline':False]['text':'/ them before calling it.','line_number':21,'multiline':False]['text':'/','line_number':22,'multiline':False]['text':'/ \param tensors Tensors of which the derivative will be computed.','line_number':23,'multiline':False]['text':'/ \param grad_tensors The "vector" in the Jacobian-vector product, usually','line_number':24,'multiline':False]['text':'/ gradients','line_number':25,'multiline':False]['text':'/     w.r.t. each element of corresponding tensors. `torch::Tensor()` values','line_number':26,'multiline':False]['text':'/     can be specified for scalar Tensors or ones that don't require grad. If','line_number':27,'multiline':False]['text':'/     a `torch::Tensor()` value would be acceptable for all grad_tensors, then','line_number':28,'multiline':False]['text':'/     this argument is optional.','line_number':29,'multiline':False]['text':'/ \param retain_graph If `false`, the graph used to compute the grad will be','line_number':30,'multiline':False]['text':'/ freed.','line_number':31,'multiline':False]['text':'/     Note that in nearly all cases setting this option to `true` is not','line_number':32,'multiline':False]['text':'/     needed and often can be worked around in a much more efficient way.','line_number':33,'multiline':False]['text':'/     Defaults to the value of `create_graph`.','line_number':34,'multiline':False]['text':'/ \param create_graph If `true`, graph of the derivative will be constructed,','line_number':35,'multiline':False]['text':'/ allowing','line_number':36,'multiline':False]['text':'/     to compute higher order derivative products. Defaults to `false`.','line_number':37,'multiline':False]['text':'/ \param inputs Inputs w.r.t. which the gradient will be accumulated into','line_number':38,'multiline':False]['text':'/     `at::Tensor::grad`. All other Tensors will be ignored. If not provided,','line_number':39,'multiline':False]['text':'/     the gradient is accumulated into all the leaf Tensors that were used to','line_number':40,'multiline':False]['text':'/     compute param `tensors`.','line_number':41,'multiline':False]['text':'      When inputs are provided and a given input is not a leaf,','line_number':42,'multiline':False]['text':'      the current implementation will call its grad_fn (even though it is not','line_number':43,'multiline':False]['text':'      strictly needed to get this gradients). It is an implementation detail','line_number':44,'multiline':False]['text':'      on which the user should not rely. See','line_number':45,'multiline':False]['text':'      https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for','line_number':46,'multiline':False]['text':'      more details.','line_number':47,'multiline':False]['text':'/ Computes and returns the sum of gradients of outputs with respect to the','line_number':55,'multiline':False]['text':'/ inputs.','line_number':56,'multiline':False]['text':'/','line_number':57,'multiline':False]['text':'/ ``grad_outputs`` should be a sequence of length matching ``output``','line_number':58,'multiline':False]['text':'/ containing the "vector" in Jacobian-vector product, usually the pre-computed','line_number':59,'multiline':False]['text':'/ gradients w.r.t. each of the outputs. If an output doesn't require_grad,','line_number':60,'multiline':False]['text':'/ then the gradient can be ``torch::Tensor()``).','line_number':61,'multiline':False]['text':'/','line_number':62,'multiline':False]['text':'/ \param outputs outputs of the differentiated function.','line_number':63,'multiline':False]['text':'/ \param inputs Inputs w.r.t. which the gradient will be','line_number':64,'multiline':False]['text':'/     returned (and not accumulated into ``at::Tensor::grad``).','line_number':65,'multiline':False]['text':'/ \param grad_outputs The "vector" in the Jacobian-vector product.','line_number':66,'multiline':False]['text':'/     Usually gradients w.r.t. each output. `torch::Tensor()` values can be','line_number':67,'multiline':False]['text':'/     specified for scalar Tensors or ones that don't require grad. If a','line_number':68,'multiline':False]['text':'/     `torch::Tensor()` value would be acceptable for all grad_tensors, then','line_number':69,'multiline':False]['text':'/     this argument is optional. Default: `{}`.','line_number':70,'multiline':False]['text':'/ \param retain_graph If ``false``, the graph used to compute the grad','line_number':71,'multiline':False]['text':'/     will be freed. Note that in nearly all cases setting this option to','line_number':72,'multiline':False]['text':'/     ``true`` is not needed and often can be worked around in a much more','line_number':73,'multiline':False]['text':'/     efficient way. Defaults to the value of ``create_graph``.','line_number':74,'multiline':False]['text':'/ \param create_graph If ``true``, graph of the derivative will','line_number':75,'multiline':False]['text':'/     be constructed, allowing to compute higher order derivative products.','line_number':76,'multiline':False]['text':'/     Default: ``false``.','line_number':77,'multiline':False]['text':'/ \param allow_unused If ``false``, specifying inputs that were not','line_number':78,'multiline':False]['text':'/     used when computing outputs (and therefore their grad is always zero)','line_number':79,'multiline':False]['text':'/     is an error. Defaults to ``false``.','line_number':80,'multiline':False]['text':'/ Creates a new dual level and returns its index. This level index should then','line_number':91,'multiline':False]['text':'/ be used to call into the other functions below. This API supports entering a','line_number':92,'multiline':False]['text':'/ new level before the previous one is exited. We call them nested forward AD','line_number':93,'multiline':False]['text':'/ levels. These can be used to compute higher order derivatives.','line_number':94,'multiline':False]['text':'/ Exits the given level. This will clear up all the gradients from this level','line_number':97,'multiline':False]['text':'/ and all dual Tensors that had gradients for this level will become regular','line_number':98,'multiline':False]['text':'/ Tensors again. This function can only be used to exit the innermost nesting','line_number':99,'multiline':False]['text':'/ level and so exiting must happen in reverse order compared to the entering','line_number':100,'multiline':False]['text':'/ that was done with the function above.','line_number':101,'multiline':False]['text':' namespace forward_ad','line_number':104,'multiline':False]['text':' namespace autograd','line_number':105,'multiline':False]['text':' namespace torch','line_number':106,'multiline':False]