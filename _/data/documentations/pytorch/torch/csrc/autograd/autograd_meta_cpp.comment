['text':' [Forward Grad View/inplace]','line_number':18,'multiline':False]['text':' It is important to us to allow view and inplace to work with dual Tensors.','line_number':19,'multiline':False]['text':' These operations should either compute the right gradient or raise a','line_number':20,'multiline':False]['text':' user-friendly error.','line_number':21,'multiline':False]['text':' The basic case where all Tensors are dual Tensors is as follows:','line_number':23,'multiline':False]['text':'     # Have:','line_number':24,'multiline':False]['text':'     #   foo is a dual Tensor that is not a view','line_number':25,'multiline':False]['text':'     #   bar is a dual Tensor of appropriate size (depending on cases) that is','line_number':26,'multiline':False]['text':'     not a view','line_number':27,'multiline':False]['text':'','line_number':28,'multiline':False]['text':'     # Case 1: no view','line_number':29,'multiline':False]['text':'     foo.copy_(bar)','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':'     # Case 2: with view, propagate from view to base','line_number':32,'multiline':False]['text':'     view = foo[0]','line_number':33,'multiline':False]['text':'     view.copy_(bar)','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':'     # Case 3: with view, propagate from base to view','line_number':36,'multiline':False]['text':'     view = foo[0]','line_number':37,'multiline':False]['text':'     foo.copy_(bar)','line_number':38,'multiline':False]['text':'','line_number':39,'multiline':False]['text':'     # In both cases, the forward grad of foo must be properly updated.','line_number':40,'multiline':False]['text':'     # In the second and third cases, the forward grad of view must match','line_number':41,'multiline':False]['text':'     # the one of foo for the subset they have in common.','line_number':42,'multiline':False]['text':'','line_number':43,'multiline':False]['text':' All these cases can be handled by the following layout constraint on the','line_number':44,'multiline':False]['text':' forward grad:','line_number':45,'multiline':False]['text':'   - A Tensor and its forward grad (for all levels) must have the same','line_number':46,'multiline':False]['text':'   metadata (size, stride','line_number':47,'multiline':False]['text':'     conj/neg bit and storage offset). Storage offset must be in this metadata','line_number':48,'multiline':False]['text':'     because of as_strided. conj/neg bit must be part of this metadata because','line_number':49,'multiline':False]['text':'     of ops like `real`.','line_number':50,'multiline':False]['text':'   - View operations must create a forward grad that is a view of the base's','line_number':51,'multiline':False]['text':'   forward grad.','line_number':52,'multiline':False]['text':'   - Inplace operations must modify the input's forward grad inplace.','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' This layout constraint is ensured in the `set_fw_grad` function below','line_number':55,'multiline':False]['text':' More complex cases arrise when non-dual Tensor interact with dual Tensors.','line_number':57,'multiline':False]['text':' The two most important cases are:','line_number':58,'multiline':False]['text':'','line_number':59,'multiline':False]['text':'     # Have:','line_number':60,'multiline':False]['text':'     #   foo is a regular Tensor that is not a view','line_number':61,'multiline':False]['text':'     #   bar is a dual Tensor of appropriate size (depending on cases) that is','line_number':62,'multiline':False]['text':'     not a view','line_number':63,'multiline':False]['text':'','line_number':64,'multiline':False]['text':'     # Case 4: Changes on the view must propagate to its base','line_number':65,'multiline':False]['text':'     view = foo[0]','line_number':66,'multiline':False]['text':'     # view is still a regular Tensor here','line_number':67,'multiline':False]['text':'     view.copy_(bar)','line_number':68,'multiline':False]['text':'     # Now both view and foo are dual Tensor with appropriate forward grad','line_number':69,'multiline':False]['text':'','line_number':70,'multiline':False]['text':'     # Case 5: Changes on the base must propagate on all its views','line_number':71,'multiline':False]['text':'     view = foo[0]','line_number':72,'multiline':False]['text':'     # view is still a regular Tensor here','line_number':73,'multiline':False]['text':'     base.copy_(bar)','line_number':74,'multiline':False]['text':'     # Now both view and foo are dual Tensor with appropriate forward grad','line_number':75,'multiline':False]['text':'','line_number':76,'multiline':False]['text':'     # NB there is a case 6 involving changes on a view propagating to other','line_number':77,'multiline':False]['text':'     views # but it is fully described by the two others and is skipped in','line_number':78,'multiline':False]['text':'     this discussion.','line_number':79,'multiline':False]['text':'','line_number':80,'multiline':False]['text':' Case 4 is handled by set_fw_grad by properly setting the forward grad of the','line_number':81,'multiline':False]['text':' base if needed. Case 5 is handled in fw_grad by reading the forward grad from','line_number':82,'multiline':False]['text':' the base if needed.','line_number':83,'multiline':False]['text':' Enforcing that the metadata between the primal and tangent are same has two','line_number':87,'multiline':False]['text':' goals:','line_number':88,'multiline':False]['text':' - When properties of the primal are checked in composite op's to determine','line_number':89,'multiline':False]['text':'   control flow, the code path decided upon is also reasonable for the tangent','line_number':90,'multiline':False]['text':' - Make sure that when the same as_strided is applied to both primal and','line_number':91,'multiline':False]['text':'   and tangent, it behaves similarly.','line_number':92,'multiline':False]['text':'','line_number':93,'multiline':False]['text':' We do that by checking:','line_number':94,'multiline':False]['text':'   1) the storages have same properties: size and conj/neg-ness','line_number':95,'multiline':False]['text':'   2) the same indices refer to the same elements in storage','line_number':96,'multiline':False]['text':'      (we are more strict than necessary here to satisfy the goal 1)','line_number':97,'multiline':False]['text':' 1) The storages have the same properties','line_number':102,'multiline':False]['text':' Technically dim and size belong as part of (2), so we shouldn't really care','line_number':110,'multiline':False]['text':' if a zero-numel tensor violates these. But since these properties','line_number':111,'multiline':False]['text':' (unlike offset and strides) often determine control flow in composite ops','line_number':112,'multiline':False]['text':' it is useful to enforce that they match for primal and tangent here so','line_number':113,'multiline':False]['text':' nothing funny happens later (See goal 1).','line_number':114,'multiline':False]['text':' The check below will always be vacuously true for 0-element tensors','line_number':124,'multiline':False]['text':' 2) The same indices refer to the same elements in storage','line_number':129,'multiline':False]['text':' namespace utils','line_number':143,'multiline':False]['text':' This function is will ensure that the fw_grad_ is properly a view of the base','line_number':145,'multiline':False]['text':' for inplace ops on Tensors that do not have forward grad originally.','line_number':146,'multiline':False]['text':' Lazy initialization','line_number':162,'multiline':False]['text':' Setting the forward grad again is only allowed if it is a no-op.','line_number':170,'multiline':False]['text':' We do allow this case to simplify writing codegen for inplace ops.','line_number':171,'multiline':False]['text':' TODO(alband) remove this spurious version counter bump','line_number':187,'multiline':False]['text':' For inplace ops on a Tensor that does not already have a forward grad','line_number':205,'multiline':False]['text':' and is a view, we propagate the tangent to the base and ensure that the','line_number':206,'multiline':False]['text':' new_grad is a view of that base's tangent. This ensure that case 4 from','line_number':207,'multiline':False]['text':' [Forward Grad View/inplace] above works fine What happens in this long','line_number':208,'multiline':False]['text':' if statement is:','line_number':209,'multiline':False]['text':'   - Check if the base already has a grad','line_number':210,'multiline':False]['text':'   - If not, set a new fw_grad for it full of zeros','line_number':211,'multiline':False]['text':'   - Take a view of the base's forward grad','line_number':212,'multiline':False]['text':'   - Copy the given new_grad into this view','line_number':213,'multiline':False]['text':'   - Use this view as the new new_grad','line_number':214,'multiline':False]['text':' Enforce same meta here to make sure that the view op below is','line_number':220,'multiline':False]['text':' always valid','line_number':221,'multiline':False]['text':' TODO extend this special case to when the underlying storage of','line_number':225,'multiline':False]['text':' new_grad can be re-used.','line_number':226,'multiline':False]['text':' Update new_grad to be a view of the base','line_number':234,'multiline':False]['text':' is_inplace_op ','line_number':247,'multiline':True]['text':' Enforce the basic layout constraint','line_number':252,'multiline':False]['text':' TLS that disables forward AD.','line_number':274,'multiline':False]['text':' Ensure that concurrent fw_grad() "reads" are thread safe','line_number':279,'multiline':False]['text':' For view that don't have a forward grad, check if their base has one that','line_number':286,'multiline':False]['text':' has been defined by an inplace operation.','line_number':287,'multiline':False]['text':' This ensure that case 5 from [Forward Grad View/inplace] above works fine','line_number':288,'multiline':False]['text':' This is ok to do as we ONLY modify fw_grad_ and this field is properly','line_number':291,'multiline':False]['text':' locked in all methods','line_number':292,'multiline':False]['text':' Lazy initialization of fw_grad_','line_number':299,'multiline':False]['text':' namespace autograd','line_number':318,'multiline':False]['text':' namespace torch','line_number':319,'multiline':False]