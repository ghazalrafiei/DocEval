['text':' The current evaluating node. This is useful to assign the current node as a','line_number':26,'multiline':False]['text':' parent of new nodes created during the evaluation of this node in anomaly','line_number':27,'multiline':False]['text':' mode.','line_number':28,'multiline':False]['text':' restore the previous evaluating node','line_number':37,'multiline':False]['text':'
 * Fix for #5534: prevent stack overflow on deletion of deep computation graph
 *
 * Sometimes one can end up with a very big computation graph of Nodes
 * and Edges. Each std::shared_ptr<Node> contains a list of Edge, and
 * each Edge contains a std::shared_ptr<Node>. Deleting a
 * std::shared_ptr<Node> can trigger the recursive deletion of other
 * std::shared_ptr<Node>'s: this can stack overflow if the graph
 * is deep enough. Here is an example of such a graph:
 *
 * shared_ptr<Node> -> Edge -> shared_ptr<Node> -> Edge -> ... ->
 * shared_ptr<Node>
 *
 * The solution here is to detect when we are decrementing away the last
 * reference to a Node, and when doing so to buffer up the Node's
 * that will be recursively decremented.  We can then decrement (and free)
 * the original Node without causing a recursive cascade, before
 * draining the buffer applying the same behavior.  This is, in effect,
 * converting recursion to a loop, using a heap buffer in place of the
 * recursive call stack.
 ','line_number':74,'multiline':True]['text':' To avoid stack overflow on large computational graphs,','line_number':96,'multiline':False]['text':' we need to track reference decrementing and freeing','line_number':97,'multiline':False]['text':' on the heap.','line_number':98,'multiline':False]['text':' Reference count is decremented on the loop backedge.','line_number':108,'multiline':False]['text':' namespace autograd','line_number':116,'multiline':False]['text':' namespace torch','line_number':117,'multiline':False]