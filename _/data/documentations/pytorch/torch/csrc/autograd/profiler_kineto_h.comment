['text':' namespace kineto','line_number':17,'multiline':False]['text':' namespace impl','line_number':18,'multiline':False]['text':' namespace profiler','line_number':19,'multiline':False]['text':' Copy fields from result so we can return ArrayRefs.','line_number':72,'multiline':False]['text':' Consolidating events returned directly from Kineto','line_number':78,'multiline':False]['text':' with events manually created by us (e.g. start/stop marks,','line_number':79,'multiline':False]['text':' memory allocation events)','line_number':80,'multiline':False]['text':'
 * This API is used by backends to record latency of events that
 * happened in the backend but were not visible to pytorch runtime.
 * For example, if part of the model is lowered to a dsp backend, then
 * the execution of that part of the model is delegated to the backend.
 * When backend finishes execution it has an option to provide profiling
 * information (latency only at the moment) corresponding to different operators
 * that were executed in the backend.
 * When such events are recorded by backend using this API, the event
 * records will be collected by active kineto profiler. If no kineto profiler
 * is active then the event is ignored.
 * This provides us with a way to generate all the profiling information
 * for a model regardless of where model (or part of it) executed.
 * @param start_time_us: start time in us of the event
 * @param end_time_us: end time in us of the event
 * @param debug_handle: debug handle to correlate this event/op with
 * model level module/source information
 * @param scope: scope of the event, e.g. LITE_INTERPRETER, RECORD_FN etc.
 * @param event_name: name of the event, e.g. op name
 * @param backend_name: name of the backend where the event took place.
 ','line_number':112,'multiline':True]['text':'
 * Same as enableProfiler but with callback to do post-processing of
 * KinetoEvents.
 * enableProfilerWithEventPostProcess enables profiler to capture
 * specified activities, with specified RecordFunction scope, if any.
 * Additionally, it takes a functor that does in-place post processing of
 * events, e.g. populate stack trace or module hierarchy information lazily
 * using debug_handle.
 * Example usage is with lite interpreter that has recording scope of
 * LITE_INTERPRETER. In this case lite interpreter runtime, records debug
 * handles in RecordFunction, along with other information. Debug handles are
 * eventually passed down to KinetoEvent and recorded as part of the event.
 * KinetoEdgeCPUProfiler, in torch/csrc/jit/mobile/profiler_edge.cpp, enables
 * profiler using post-processing callback, via
 * enableProfilerWithEventPostProcess, that takes these debug handles and
 * generates stack trace and module hierarchy information, once profiling is
 * done.
 ','line_number':146,'multiline':True]['text':'debug_handle ','line_number':165,'multiline':True]['text':'jit_stack    ','line_number':166,'multiline':True]['text':'jit_modules  ','line_number':167,'multiline':True]['text':' namespace profiler','line_number':180,'multiline':False]['text':' namespace autograd','line_number':181,'multiline':False]['text':' Experimental.','line_number':186,'multiline':False]['text':' namespace impl','line_number':189,'multiline':False]['text':' namespace profiler','line_number':190,'multiline':False]['text':' namespace torch','line_number':192,'multiline':False]