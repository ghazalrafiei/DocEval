['text':' Chain this view info with the new view op between base and tensor','line_number':60,'multiline':False]['text':' Set `view_func` using the root base as input.','line_number':65,'multiline':False]['text':' `view_func` is used to recover views in backward when either as_strided is','line_number':66,'multiline':False]['text':' not supported or the view function changes the metadata which is not','line_number':67,'multiline':False]['text':' recorded by as_strided See Note [View + Inplace update on base tensor] and','line_number':68,'multiline':False]['text':' [View + Inplace update on view tensor] for more details how we use this','line_number':69,'multiline':False]['text':' function in backward.','line_number':70,'multiline':False]['text':' both current_view and it's parent have a view_func','line_number':72,'multiline':False]['text':' Copy parent view function to gain ownership','line_number':74,'multiline':False]['text':' current_view has a view_func and but it's parent doesn't have one','line_number':81,'multiline':False]['text':' When base is a view but doesn't carry a view_fn in','line_number':91,'multiline':False]['text':' DifferentiableViewMeta, it's a view that doesn't support inplace','line_number':92,'multiline':False]['text':' update, e.g. unbind. In this case we should throw an error when','line_number':93,'multiline':False]['text':' inplace update happens in **forward**. One would naturally think the','line_number':94,'multiline':False]['text':' following function will be first called in backward pass. But the','line_number':95,'multiline':False]['text':' first call site is indeed in **forward** pass when we refresh','line_number':96,'multiline':False]['text':' `grad_fn` triggered by inplace update. Search Note [View + Inplace','line_number':97,'multiline':False]['text':' update for view tensor] to for the call site.','line_number':98,'multiline':False]['text':' if current_view doesn't have a view_func but it's parent has one','line_number':110,'multiline':False]['text':' Copy parent view function to gain ownership','line_number':111,'multiline':False]['text':' namespace','line_number':143,'multiline':False]['text':' This function is called whenever the grad_fn of the tensor is','line_number':162,'multiline':False]['text':' changed. We assume here that new_fn does not yet have hooks of','line_number':163,'multiline':False]['text':' its own.','line_number':164,'multiline':False]['text':'','line_number':165,'multiline':False]['text':' This function does two things:','line_number':166,'multiline':False]['text':' (1) reset the list when grad_fn is updated, so new hooks don't','line_number':167,'multiline':False]['text':'     get erroneously registered to the old grad_fn.','line_number':168,'multiline':False]['text':'     Note that the old cpp_hooks_list_ is still kept alive by the','line_number':169,'multiline':False]['text':'     old grad_fn so hooks registered to the older version of the tensor','line_number':170,'multiline':False]['text':'     will continue to be active.','line_number':171,'multiline':False]['text':' (2) If there is a retains_grad hook registered, move that from the','line_number':172,'multiline':False]['text':'     old cpp_hooks_list_ to the new one','line_number':173,'multiline':False]['text':' See NOTE [ View + Inplace detection ]','line_number':197,'multiline':False]['text':' Do not use handle_view_on_rebase here as check_inplace should have been','line_number':199,'multiline':False]['text':' called before this and either throw an error','line_number':200,'multiline':False]['text':' trigger an update to the view's grad_fn','line_number':215,'multiline':False]['text':' Pass both self and its grad_fn to avoid calling into grad_fn reentrantly','line_number':220,'multiline':False]['text':' NB: we could potentially only update hooks_ if !fn, but it shouldn't','line_number':232,'multiline':False]['text':' matter','line_number':233,'multiline':False]['text':'     and this was the way before, so we keep it like this for now.','line_number':234,'multiline':False]['text':' If grad_fn is null (as is the case for a leaf node), we instead','line_number':286,'multiline':False]['text':' interpret the gradient function to be a gradient accumulator, which will','line_number':287,'multiline':False]['text':' accumulate its inputs into the grad property of the variable. These','line_number':288,'multiline':False]['text':' nodes get suppressed in some situations, see "suppress gradient','line_number':289,'multiline':False]['text':' accumulation" below. Note that only variables which have `requires_grad =','line_number':290,'multiline':False]['text':' True` can have gradient accumulators.','line_number':291,'multiline':False]['text':' For views, make sure this new grad_fn_ is not overwritten unless it is','line_number':303,'multiline':False]['text':' necessary in the VariableHooks::grad_fn below. This logic is only relevant','line_number':304,'multiline':False]['text':' for custom autograd Functions for which multiple operations can happen on a','line_number':305,'multiline':False]['text':' given Tensor before its gradient edge is set when exiting the custom','line_number':306,'multiline':False]['text':' Function.','line_number':307,'multiline':False]['text':' Versions','line_number':322,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':323,'multiline':False]['text':' Hooks','line_number':344,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':345,'multiline':False]['text':' This is a little goofy, but usually this should be a no oop','line_number':361,'multiline':False]['text':' Miscellaneous','line_number':382,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':383,'multiline':False]['text':' NB: could return nullptr','line_number':386,'multiline':False]['text':' NB: return nullptr if self is not a view','line_number':394,'multiline':False]['text':' namespace impl','line_number':403,'multiline':False]['text':'version_counter=','line_number':414,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':415,'multiline':True]['text':'version_counter=','line_number':423,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':424,'multiline':True]['text':' `var.set_data(new_data)` shallow-copies all non-autograd TensorImpl fields','line_number':453,'multiline':False]['text':' from `new_data` to `var`. It requires that `new_data` and `var` have','line_number':454,'multiline':False]['text':' compatible tensor type.','line_number':455,'multiline':False]['text':' Resets gradient accumulator if metadata is out of date','line_number':465,'multiline':False]['text':' Version counter is not shared when we replace a `Variable`'s tensor data','line_number':481,'multiline':False]['text':' by calling `set_data(...)`. The original version of the `Variable` is','line_number':482,'multiline':False]['text':' always preserved. See NOTE [ Version Counter Sharing ] for details.','line_number':483,'multiline':False]['text':'','line_number':484,'multiline':False]['text':' `var.set_data(new_data)` always ignores `var`'s','line_number':485,'multiline':False]['text':' `allow_tensor_metadata_change_`, because users need this API as an escape','line_number':486,'multiline':False]['text':' hatch for changing a tensor's metadata regardless of its','line_number':487,'multiline':False]['text':' `allow_tensor_metadata_change_` value, and the users are responsible for','line_number':488,'multiline':False]['text':' ensuring this is the behavior they want.','line_number':489,'multiline':False]['text':' temporary hack to improve functorch UX.','line_number':506,'multiline':False]['text':' no-op for leaves','line_number':512,'multiline':False]['text':' TODO torch::autograd::backward should take the c10::optional<Tensor>','line_number':558,'multiline':False]['text':' gradient directly instead of us having to unwrap it to Tensor _gradient','line_number':559,'multiline':False]['text':' here.','line_number':560,'multiline':False]['text':' Backward View Variables','line_number':578,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':579,'multiline':False]['text':' See NOTE [ View + Inplace detection ]','line_number':624,'multiline':False]['text':' This is an indirect rebase_history due to another view or the base','line_number':633,'multiline':False]['text':' being modified inplace','line_number':634,'multiline':False]['text':' indirect ','line_number':635,'multiline':True]['text':' Note [View + Inplace update for view tensor]','line_number':637,'multiline':False]['text':' An inplace update happened on Tensor `self` (which is a view).','line_number':638,'multiline':False]['text':' For example:','line_number':639,'multiline':False]['text':'   view_1 = view_op_1(diff_view_meta->base_)','line_number':640,'multiline':False]['text':'   view_2 = view_op_2(view_1)','line_number':641,'multiline':False]['text':'   ...','line_number':642,'multiline':False]['text':'   self = view_op_n(view_n-1)','line_number':643,'multiline':False]['text':'   self = inplace_op(self)','line_number':644,'multiline':False]['text':'','line_number':645,'multiline':False]['text':' For CPU/CUDA backends, we employ one AsStridedBackward0 Node to','line_number':646,'multiline':False]['text':' represent the chain of view backward ops for efficiency.','line_number':647,'multiline':False]['text':'','line_number':648,'multiline':False]['text':' However in XLA backend we don't have full support of','line_number':649,'multiline':False]['text':' AsStridedBackward0, we instead run a full forward pass with a tensor','line_number':650,'multiline':False]['text':' that requires gradient to get proper grad_fn setup, then save it to','line_number':651,'multiline':False]['text':' DifferentiableViewMeta for future use. This is fairly cheap for XLA','line_number':652,'multiline':False]['text':' lazy tensor approach (but would be really expensive for CPU/CUDA). XLA','line_number':653,'multiline':False]['text':' Tensor only run through VariableType dispatch and lower the forward','line_number':654,'multiline':False]['text':' pass to a XLA HLO graph, then we take grad_fn and never materialize the','line_number':655,'multiline':False]['text':' tensor content. So we only construct the graph but not execute it,','line_number':656,'multiline':False]['text':' which is a fairly cheap operation to do.','line_number':657,'multiline':False]['text':'','line_number':658,'multiline':False]['text':' See Note [View + Inplace update for base tensor] for what we do to base','line_number':659,'multiline':False]['text':' tensor when an in-place operation happens.','line_number':660,'multiline':False]['text':'','line_number':661,'multiline':False]['text':' TODO: Potentially the following logic can be replaced by special logic','line_number':662,'multiline':False]['text':' in VariableType_x.cpp','line_number':663,'multiline':False]['text':'       that would provide a way to recreate the grad_fn chain.','line_number':664,'multiline':False]['text':' We can reach this path with grad_mode disabled, e.g. engine','line_number':669,'multiline':False]['text':' Note: sizes(), not base_.sizes(), is','line_number':685,'multiline':False]['text':' intentional','line_number':686,'multiline':False]['text':' Hook will be ignored','line_number':712,'multiline':False]['text':' NB: materialize_autograd_meta unnecessary due to requires grad check','line_number':723,'multiline':False]['text':'is_retains_grad_hooks=','line_number':727,'multiline':True]['text':'/ See NOTE [ View + Inplace detection ] for justification of the logic below','line_number':737,'multiline':False]['text':' Create the header for the error message.','line_number':743,'multiline':False]['text':' create_meta is not necessarily CreationMeta::NO_GRAD_MODE','line_number':770,'multiline':False]['text':' e.g. CreationMeta::IN_CUSTOM_FUNCTION is possible, but we know that','line_number':771,'multiline':False]['text':' if there is no grad_fn, that means that the view was performed in','line_number':772,'multiline':False]['text':' no-grad mode','line_number':773,'multiline':False]['text':' namespace autograd','line_number':826,'multiline':False]['text':' namespace torch','line_number':827,'multiline':False]