['text':' Helper functions for autogenerated code','line_number':36,'multiline':False]['text':' These used to be inlined into the codegened Functions.cpp','line_number':37,'multiline':False]['text':'level ','line_number':72,'multiline':True]['text':'level ','line_number':76,'multiline':True]['text':' R -> C','line_number':164,'multiline':False]['text':' R -> C','line_number':172,'multiline':False]['text':' handle case at 0 where we return a subgradient containing 0','line_number':227,'multiline':False]['text':' NB: We mask fill the NaNs in the output to be zero but still do float','line_number':250,'multiline':False]['text':' division','line_number':251,'multiline':False]['text':'     by zero, which ASAN complains about. One way to appease ASAN is to fill','line_number':252,'multiline':False]['text':'     the problematic values with something arbitrary before the division,','line_number':253,'multiline':False]['text':'     but we decide not to due to the perf hit. Instead we just silence ASAN','line_number':254,'multiline':False]['text':'     where necessary','line_number':255,'multiline':False]['text':' Derivative of amax(abs(self), dim, keepdim) but respecting nans','line_number':273,'multiline':False]['text':' We create a mask of `argmax`: it's argmax if self.abs() == norm or it's','line_number':274,'multiline':False]['text':' NaN','line_number':275,'multiline':False]['text':' See norm_backward above for a note on ignoring the sanitizer','line_number':296,'multiline':False]['text':' NB: currently norm_jvp is also reused for dist's jvp (which haas two','line_number':304,'multiline':False]['text':' differentiable inputs)','line_number':305,'multiline':False]['text':'     but self_t still cannot be a ZT because that would require both self_t','line_number':306,'multiline':False]['text':'     and other_t to be ZT','line_number':307,'multiline':False]['text':' composite compliance?','line_number':309,'multiline':False]['text':' No need to handle the dtype arg as it's handled via broadcasting in the','line_number':446,'multiline':False]['text':' function','line_number':447,'multiline':False]['text':' No need to handle the dtype arg as it's handled via broadcasting in the','line_number':459,'multiline':False]['text':' function','line_number':460,'multiline':False]['text':' Caveats:','line_number':491,'multiline':False]['text':' We define d(a^b)/db at a = 0 and b < 0 to be -inf. This is due to','line_number':492,'multiline':False]['text':' d(a^b)/db -> -inf for a fixed b as a -> +0','line_number':493,'multiline':False]['text':' Currently, tensorflow defines d(a^b)/db = nan for a = 0 and b < 0.','line_number':494,'multiline':False]['text':'','line_number':495,'multiline':False]['text':' We define d(a^b)/db = 0 for a = 0 and b = 0 by continuity as','line_number':496,'multiline':False]['text':' d(a^b)/db = 0 for a > 0 and b -> +0.','line_number':497,'multiline':False]['text':' Currently, tensorflow agrees with us.','line_number':498,'multiline':False]['text':' `.to()` is no-op if dtype is same.','line_number':513,'multiline':False]['text':' masked_select does not work well with functorch, as its shape is','line_number':583,'multiline':False]['text':' data-dependent','line_number':584,'multiline':False]['text':' invert the permutation','line_number':654,'multiline':False]['text':' Optimisation for two common cases','line_number':683,'multiline':False]['text':' we are only using `keepdim=true` path for SymInts for now','line_number':720,'multiline':False]['text':' When input has a zero sized dimension (empty tensor),','line_number':779,'multiline':False]['text':' we don't need to actually compute the grads.','line_number':780,'multiline':False]['text':' So we just reshape `grad` as `input`.','line_number':781,'multiline':False]['text':' note that the gradient for prod is equivalent to:','line_number':806,'multiline':False]['text':' cumprod(exclusive, normal) * cumprod(exclusive, reverse), e.g.:','line_number':807,'multiline':False]['text':' input:                        [    a,     b,     c]','line_number':808,'multiline':False]['text':' cumprod(exclusive, normal):   [1    ,     a, a * b]','line_number':809,'multiline':False]['text':' cumprod(exclusive, reverse):  [b * c,     c,     1]','line_number':810,'multiline':False]['text':' product:                      [b * c, a * c, a * b]','line_number':811,'multiline':False]['text':' and this is safe under input with 0s.','line_number':812,'multiline':False]['text':' For Composite Compliance, always take the safer (and slower) path','line_number':821,'multiline':False]['text':' `prod` reduces the dimension at `dim`,','line_number':847,'multiline':False]['text':' so, unsqueeze `grad` and `result` at dim.','line_number':848,'multiline':False]['text':' For Composite Compliance, always take the safer (and slower) path','line_number':853,'multiline':False]['text':' In general,','line_number':877,'multiline':False]['text':' dX = solve(A, dB - dA_contrib), but this behavior is different for','line_number':878,'multiline':False]['text':' lu_solve. For refer to lu_solve_jvp for more details on this.','line_number':879,'multiline':False]['text':' Trivial case','line_number':884,'multiline':False]['text':' Reference: https://github.com/tensorflow/tensorflow/blob/','line_number':913,'multiline':False]['text':' 2a5910906a0e0f3dbc186ff9db6386d81a63448c/tensorflow/python/ops/math_grad.py#L1832-L1863','line_number':914,'multiline':False]['text':' no trick separating the positive and negative required','line_number':937,'multiline':False]['text':' Mostly taken from logsumexp_jvp','line_number':949,'multiline':False]['text':' NB: for simplicity, we recompute some values that can be reused from','line_number':951,'multiline':False]['text':' forward','line_number':952,'multiline':False]['text':' Use the exp-normalize trick','line_number':956,'multiline':False]['text':' at::max doesn't support complex128','line_number':958,'multiline':False]['text':' R -> C','line_number':1073,'multiline':False]['text':' If input was empty tensor, gradInput should be empty tensor.','line_number':1079,'multiline':False]['text':' R -> C','line_number':1128,'multiline':False]['text':' If input was empty tensor, gradInput should be empty tensor.','line_number':1134,'multiline':False]['text':' 0d case','line_number':1139,'multiline':False]['text':' 2d case','line_number':1142,'multiline':False]['text':' 1d case','line_number':1146,'multiline':False]['text':' clamp: gradients not defined on min and max, so we return the subgradient 1','line_number':1169,'multiline':False]['text':' for these cases.','line_number':1170,'multiline':False]['text':' clamp: gradients not defined on min and max, so we return the subgradient 1','line_number':1190,'multiline':False]['text':' for these cases.','line_number':1191,'multiline':False]['text':' If min > max, min has no gradient','line_number':1217,'multiline':False]['text':' Cannot pass initializer list due to overload ambiguity','line_number':1366,'multiline':False]['text':' This function is used by load_derivatives.py to replace tensor.strides()','line_number':1381,'multiline':False]['text':' calls that appear in derivative formulas. If the tensor has requires_grad','line_number':1382,'multiline':False]['text':' set, this function returns its strides or an empty array if the tensor','line_number':1383,'multiline':False]['text':' is sparse. If requires_grad is not set, an empty array is returned since','line_number':1384,'multiline':False]['text':' there will be no backward pass. There has one special case, if input is','line_number':1385,'multiline':False]['text':' MKLDNN tensor and has requires_grad set, just return an empty array, the','line_number':1386,'multiline':False]['text':' reason is that MKLDNN tensor is a opaque tensor which has not stride info.','line_number':1387,'multiline':False]['text':'','line_number':1388,'multiline':False]['text':' This function only supports the case where `input` is the tensor whose','line_number':1389,'multiline':False]['text':' single derivative is being calculated.','line_number':1390,'multiline':False]['text':'','line_number':1391,'multiline':False]['text':' This function does not support `self` derivatives for inplace functions.','line_number':1392,'multiline':False]['text':'','line_number':1393,'multiline':False]['text':' Args:','line_number':1394,'multiline':False]['text':'  input              Tensor to call .strides() on','line_number':1395,'multiline':False]['text':'  input_name         Name of `input` tensor, from derivative formula','line_number':1396,'multiline':False]['text':' TODO: Ideally, this function would never be called if requires_grad is','line_number':1400,'multiline':False]['text':' not set. Once codegen is updated to avoid the call, we can remove this','line_number':1401,'multiline':False]['text':' check.','line_number':1402,'multiline':False]['text':' if input was column-major, return grad as column-order for efficiency','line_number':1423,'multiline':False]['text':' General fallback, should work for any layout','line_number':1429,'multiline':False]['text':' if input was column-major, return grad as column-order for efficiency','line_number':1442,'multiline':False]['text':' General fallback, should work for any layout','line_number':1448,'multiline':False]['text':' zero must to have mat1 sparsity pattern:','line_number':1465,'multiline':False]['text':' search into x is faster','line_number':1491,'multiline':False]['text':' search into gx is faster','line_number':1494,'multiline':False]['text':' gx.coalesce() is likely faster','line_number':1503,'multiline':False]['text':' x.coalesce() is likely faster','line_number':1506,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':1531,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':1535,'multiline':False]['text':' NOTE: sparse_mask accumulates matches, so the backward step has to','line_number':1544,'multiline':False]['text':' accumulate as well.','line_number':1545,'multiline':False]['text':'accumulate_matches=','line_number':1547,'multiline':True]['text':'
  To implement the backward algorithm for sparse matrix-matrix matmul (SPMM) we
  can start from the following definition for dense tensors:

  c = a @ b
      then
  a_grad = c_grad @ b^H
  b_grad = a^H @ c_grad

  So for sparse matrices we can use the following definition:

  if grad_order == 0:
      a_grad = sparse_matrix_mask(c_grad @ b^H, mask=a)
  else:
      b_grad = sparse_matrix_mask(a^H @ c_grad, mask=b)
  ','line_number':1556,'multiline':True]['text':' NOTE: _sparse_sparse_matmul returns a coalesced gradient,','line_number':1576,'multiline':False]['text':'   // hence there is no need in accumulating matches.','line_number':1577,'multiline':False]['text':'accumulate_matches=','line_number':1580,'multiline':True]['text':'accumulate_matches=','line_number':1583,'multiline':True]['text':'keepdim=','line_number':1601,'multiline':True]['text':'dtype=','line_number':1601,'multiline':True]['text':' vector_norm output is real, so grad_output must also be real','line_number':1605,'multiline':False]['text':'keepdim=','line_number':1610,'multiline':True]['text':'dtype=','line_number':1610,'multiline':True]['text':'keepdim=','line_number':1612,'multiline':True]['text':' For cuda half, calculate norm in float precision then cast','line_number':1632,'multiline':False]['text':' normalization factor to half','line_number':1633,'multiline':False]['text':'is_cuda=','line_number':1635,'multiline':True]['text':'keepdim=','line_number':1642,'multiline':True]['text':'dtype=','line_number':1643,'multiline':True]['text':'keepdim=','line_number':1649,'multiline':True]['text':'keepdim=','line_number':1663,'multiline':True]['text':' Suppress unused variable warning','line_number':1678,'multiline':False]['text':' Reshape gradient (repeat > 1)','line_number':1686,'multiline':False]['text':' Index:      [..., dim    , ...]    [..., dim   ,  dim+1        , ...]','line_number':1687,'multiline':False]['text':' Shape: From [..., dimsize, ...] to [..., repeat, dimsize/repeat, ...]','line_number':1688,'multiline':False]['text':' The gradient tensor at 'dim' is reshaped to 'repeat' times of input','line_number':1689,'multiline':False]['text':' tensor. Then, sum up gradients over repeated tensors along 'dim', and','line_number':1690,'multiline':False]['text':' reduce shape from 'repeat * dimsize/repeat' to 'dimsize/repeat'','line_number':1691,'multiline':False]['text':' ('input_dimsize'). Example:','line_number':1692,'multiline':False]['text':'        Size(3, 2)                                      Size(6, 2)','line_number':1693,'multiline':False]['text':'                                                      [[v1_0, v1_1],','line_number':1694,'multiline':False]['text':'                                                       [v1_2, v1_3],','line_number':1695,'multiline':False]['text':'        [[v0, v1],               repeat(2, 1)          [v1_4, v1_5],','line_number':1696,'multiline':False]['text':'         [v2, v3],              ------------->         [v2_0, v2_1],','line_number':1697,'multiline':False]['text':'         [v4, v5]]                                     [v2_2, v2_3],','line_number':1698,'multiline':False]['text':'                                                       [v2_4, v2_5]]','line_number':1699,'multiline':False]['text':'','line_number':1700,'multiline':False]['text':'    input grad (3, 2)      reshape (2, 3, 2)         output grad (6, 2)','line_number':1701,'multiline':False]['text':'                            [[[g1_0, g1_1],            [[g1_0, g1_1],','line_number':1702,'multiline':False]['text':'                              [g1_2, g1_3],             [g1_2, g1_3],','line_number':1703,'multiline':False]['text':' [[g1_0+g2_0, g1_1+g2_1],     [g1_4, g1_5]],            [g1_4, g1_5],','line_number':1704,'multiline':False]['text':'  [g1_2+g2_2, g1_3+g2_3],     [g2_0, g2_1],            [[g2_0, g2_1],','line_number':1705,'multiline':False]['text':'  [g1_4+g2_4, g1_5+g2_5]]     [g2_2, g2_3],             [g2_2, g2_3],','line_number':1706,'multiline':False]['text':'                              [g2_4, g2_5]]             [g2_4, g2_5]]]','line_number':1707,'multiline':False]['text':'','line_number':1708,'multiline':False]['text':' If gradient tensor is reshaped to [..., dimsize/repeat, repeat, ...] and','line_number':1709,'multiline':False]['text':' then sum over 'dim+1'. The gradient for input is not correctly aligned','line_number':1710,'multiline':False]['text':' with input. Example:','line_number':1711,'multiline':False]['text':'  input grad (3, 2)        reshape (3, 2, 2)        output grad (6, 2)','line_number':1712,'multiline':False]['text':'                           [[[g1_0, g1_1],           [[g1_0, g1_1],','line_number':1713,'multiline':False]['text':'                             [g1_2, g1_3]],           [g1_2, g1_3],','line_number':1714,'multiline':False]['text':' [[g1_0+g1_2, g1_1+g1_3],   [[g1_4, g1_5],            [g1_4, g1_5],','line_number':1715,'multiline':False]['text':'  [g1_4+g2_0, g1_5+g2_1],    [g2_0, g2_1]],           [g2_0, g2_1],','line_number':1716,'multiline':False]['text':'  [g2_2+g2_4, g2_3+g2_5]]   [[g2_2, g2_3],            [g2_2, g2_3],','line_number':1717,'multiline':False]['text':'                             [g2_4, g2_5]]]           [g2_4, g2_5]]','line_number':1718,'multiline':False]['text':' Don't need to reshape gradient into (repeat, input_shape[dim]) (repeat ==','line_number':1723,'multiline':False]['text':' 1)','line_number':1724,'multiline':False]['text':' One-time Reshape & Sum','line_number':1727,'multiline':False]['text':' Reshape gradient to grad_size:','line_number':1728,'multiline':False]['text':'   1. If repeat equals to 1, append input size at that dimension,','line_number':1729,'multiline':False]['text':'   2. If repeat is larger than 1, append both repeat and input size at that','line_number':1730,'multiline':False]['text':'   dimension.','line_number':1731,'multiline':False]['text':' Sum over all "repeat" dimensions from sum_dims:','line_number':1732,'multiline':False]['text':' Example:','line_number':1733,'multiline':False]['text':' Input Size         (2,    3,    4,    5)','line_number':1734,'multiline':False]['text':' repeat             [4,    1,    9,    3]','line_number':1735,'multiline':False]['text':' output/grad Size   (8,    3,    36,   15)','line_number':1736,'multiline':False]['text':' grad_size          [4, 2,    3, 9, 4, 3, 5]','line_number':1737,'multiline':False]['text':' sum_dims           [0,          3,    5]','line_number':1738,'multiline':False]['text':' When repeat 1 time over all original dimensions, the empty sum_dims will','line_number':1740,'multiline':False]['text':' reduce the whole grad tensor into a scalar rather than keeping original','line_number':1741,'multiline':False]['text':' dimensions.','line_number':1742,'multiline':False]['text':' p1m == 1 - p','line_number':1750,'multiline':False]['text':' Use autograd-friendly backward if double backward is required','line_number':1756,'multiline':False]['text':' scale == (1 / (1 - prob))','line_number':1763,'multiline':False]['text':' when n == correction, 2 / (n - correction) is infinity','line_number':1820,'multiline':False]['text':' when self == self.mean(), we return NaN because infinity * 0 = NaN','line_number':1821,'multiline':False]['text':' otherwise, we return infinity because infinity * c = infinity, for all','line_number':1822,'multiline':False]['text':' c > 0','line_number':1823,'multiline':False]['text':'keepdim=','line_number':1839,'multiline':True]['text':' Let A = LL^H','line_number':1902,'multiline':False]['text':' dA = dLL^H + L(dL)^H','line_number':1903,'multiline':False]['text':' L^{-1}dA(L^{-H}) = L^{-1}dL + (L^{-1}dL)^H','line_number':1904,'multiline':False]['text':'               = sym(L^{-1}dL)','line_number':1905,'multiline':False]['text':' where sym(X) = X + X^H','line_number':1906,'multiline':False]['text':' A short computation gives that the inverse of sym is given by','line_number':1907,'multiline':False]['text':' \pi(X) = X.tril() - 0.5*diag(X)','line_number':1908,'multiline':False]['text':' so','line_number':1909,'multiline':False]['text':' dL = L\pi(L^{-1}dA(L^{-H}))','line_number':1910,'multiline':False]['text':' Precondition: dA is symmetric/Hermitian','line_number':1912,'multiline':False]['text':'upper=','line_number':1914,'multiline':True]['text':'left=','line_number':1914,'multiline':True]['text':'upper=','line_number':1915,'multiline':True]['text':'left=','line_number':1915,'multiline':True]['text':' From cholesky_jvp we have that','line_number':1923,'multiline':False]['text':' dL = L\pi(L^{-1}dA(L^-H))','line_number':1924,'multiline':False]['text':'','line_number':1925,'multiline':False]['text':' Let gL be the projection into the lower-triangular gradient wrt L. Taking','line_number':1926,'multiline':False]['text':' adjoints we have gA = L^{-H}\pi^*((L^HgL).tril())L^{-1} where \pi^*(X) =','line_number':1927,'multiline':False]['text':' 0.5 * (X + X^H - diag(X)) The only non-standard point of this derivation is','line_number':1928,'multiline':False]['text':' noting that the adjoint to multiplying on the left by a lower triangular','line_number':1929,'multiline':False]['text':' matrix L is multiplying by L^H and then projecting back to the lower','line_number':1930,'multiline':False]['text':' triangular matrices (hence the .tril() projection) Note that the gradient','line_number':1931,'multiline':False]['text':' is symmetric and not triangular.','line_number':1932,'multiline':False]['text':' Nb. We don't need to compute gL_ = gL.tril() as','line_number':1936,'multiline':False]['text':' tril(L^H gL) = tril(L^H (triu(gL, 1) + tril(gL)))','line_number':1937,'multiline':False]['text':'              = tril(L^H tril(gL)) + tril(L^H triu(gL, 1))','line_number':1938,'multiline':False]['text':'              = tril(L^H tril(gL))','line_number':1939,'multiline':False]['text':' since tril(L^H triu(gL, 1)) = 0, as L^H triu(gL, 1) is upper triangular','line_number':1940,'multiline':False]['text':' Equivalent to 0.5 * (gA + gA^H - diag(gA))','line_number':1942,'multiline':False]['text':'upper=','line_number':1944,'multiline':True]['text':'left=','line_number':1944,'multiline':True]['text':'upper=','line_number':1945,'multiline':True]['text':'left=','line_number':1945,'multiline':True]['text':' If X = (L L^H)^{-1} with L lower-triangular with a real positive diagonal,','line_number':1969,'multiline':False]['text':' then dX = K^H + K, where','line_number':1970,'multiline':False]['text':' K =  L^{-H} dL^{-1} [dL^{-1} = -L^{-1} dL L^{-1}]','line_number':1971,'multiline':False]['text':'   = -L^{-H} L^{-1} dL L^{-1} [L^{-H} L^{-1} = X]','line_number':1972,'multiline':False]['text':'   = -X dL L^{-1} [X = X^H = L^{-H} L^{-1} = L^{-1} L^{-H}]','line_number':1973,'multiline':False]['text':'   = -X dL X L^{H}.','line_number':1974,'multiline':False]['text':' If X = (U^H U)^{-1} with U upper-triangular with a real positive diagonal,','line_number':1975,'multiline':False]['text':' then K becomes','line_number':1976,'multiline':False]['text':' K = -X dU^H X U','line_number':1977,'multiline':False]['text':' The formula for forward AD is adapted from','line_number':1990,'multiline':False]['text':'','line_number':1991,'multiline':False]['text':' Golub, Gene H., and Victor Pereyra. "The Differentiation of Pseudo-Inverses','line_number':1992,'multiline':False]['text':' and Nonlinear Least Squares Problems Whose Variables Separate." SIAM Journal','line_number':1993,'multiline':False]['text':' on Numerical Analysis 10(2). (1973). 413-432. doi: 10.1137/0710036','line_number':1994,'multiline':False]['text':'','line_number':1995,'multiline':False]['text':' We present a short derivation below:','line_number':1996,'multiline':False]['text':' Let Ap := pinv(A), then Ap is the unique matrix such that','line_number':1997,'multiline':False]['text':'','line_number':1998,'multiline':False]['text':' Ap A Ap = Ap [1]','line_number':1999,'multiline':False]['text':' A Ap A = A   [2]','line_number':2000,'multiline':False]['text':'','line_number':2001,'multiline':False]['text':' By differentiating [1] we get:','line_number':2002,'multiline':False]['text':'','line_number':2003,'multiline':False]['text':' dAp = dAp A Ap + Ap dA Ap + Ap A dAp [3]','line_number':2004,'multiline':False]['text':'','line_number':2005,'multiline':False]['text':' In the rhs of [3] the products involving dAp could be expressed as products','line_number':2006,'multiline':False]['text':' of Ap^i, A^j, dA^k with i, j, k in {1, H}, where X^H = X.mH(). To prove that,','line_number':2007,'multiline':False]['text':' note (A Ap)^H = A Ap and (Ap A)^H = Ap A, which could be shown by taking the','line_number':2008,'multiline':False]['text':' product between the SVD decompositions of A and Ap. Consider the','line_number':2009,'multiline':False]['text':' conjugate-transposed [2]: (A Ap A)^H = A^H (A Ap) = A^H. By differentiating','line_number':2010,'multiline':False]['text':' it we get: dA^H A Ap + A^H dA Ap + A^H A dAp = dA^H. By multiplying from the','line_number':2011,'multiline':False]['text':' left by Ap^H and using Ap^H A^H = (A Ap)^H = A Ap: Ap^H dA^H A Ap + A Ap dA','line_number':2012,'multiline':False]['text':' Ap + A Ap A dAp = Ap^H dA^H. By multiplying from the left by Ap and by','line_number':2013,'multiline':False]['text':' applying [1] and [2] repeatedly until impossible we get: Ap Ap^H dA^H A Ap +','line_number':2014,'multiline':False]['text':' Ap dA Ap + Ap A dAp = Ap Ap^H dA^H. By rearranging the terms:','line_number':2015,'multiline':False]['text':'','line_number':2016,'multiline':False]['text':' Ap A dAp = -Ap dA Ap + Ap Ap^H dA^H (I - A Ap) [4],','line_number':2017,'multiline':False]['text':' which is one of the summands in [3].','line_number':2018,'multiline':False]['text':'','line_number':2019,'multiline':False]['text':' Similar, by differentiating the transpose-conjugated [2] written differently,','line_number':2020,'multiline':False]['text':' i.e. (A Ap A)^H = Ap A A^H = A^H we will get an expression for dAp A Ap,','line_number':2021,'multiline':False]['text':' which is','line_number':2022,'multiline':False]['text':'','line_number':2023,'multiline':False]['text':' dAp A Ap = -Ap dA Ap + (I - Ap A) dA^H Ap^H Ap [5].','line_number':2024,'multiline':False]['text':'','line_number':2025,'multiline':False]['text':' By plugging in [4] and [5] into [3] we get the forward AD formula for pinv:','line_number':2026,'multiline':False]['text':'','line_number':2027,'multiline':False]['text':' dAp = -Ap dA Ap + (I - Ap A) dA^H Ap^H Ap + Ap Ap^H dA^H (I - A Ap).','line_number':2028,'multiline':False]['text':' optimization to produce matrices of the smallest dimension','line_number':2035,'multiline':False]['text':' optimization to produce matrices of the smallest dimension','line_number':2054,'multiline':False]['text':' it's possible some of the grads are not defined (represents tensors of all','line_number':2078,'multiline':False]['text':' 0s). Since at::cat can't handle those, let's define them','line_number':2079,'multiline':False]['text':' add 1 to account for batch dim','line_number':2102,'multiline':False]['text':' it's possible some of the grads are not defined (represents tensors of all','line_number':2104,'multiline':False]['text':' 0s). Since at::cat can't handle those, let's define them','line_number':2105,'multiline':False]['text':' subtract 1 to account for batch dim','line_number':2115,'multiline':False]['text':' handle non-empty inputs','line_number':2151,'multiline':False]['text':' handle empty inputs','line_number':2162,'multiline':False]['text':' This is mps-only.','line_number':2168,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2268,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2270,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2273,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2295,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2297,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2299,'multiline':False]['text':' Default eps in binary_cross_entropy for ALL dtypes','line_number':2308,'multiline':False]['text':' TODO: probably change this to a dtype-dependent value','line_number':2309,'multiline':False]['text':' Trivial case','line_number':2330,'multiline':False]['text':' -w * [ pos * y * (1 -sigmoid(x)) - (1 - y) sigmoid(x)] * grad','line_number':2335,'multiline':False]['text':' If there are subclassed tensors use the out of place version','line_number':2337,'multiline':False]['text':' pos_weight might need to be broadcasted, thus mul(target) is not inplace.','line_number':2340,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2341,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2361,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2363,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2366,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2391,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2394,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2399,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2408,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2410,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2413,'multiline':False]['text':' NOTE: [How to write vmap-compatible backward formulas]','line_number':2439,'multiline':False]['text':'','line_number':2440,'multiline':False]['text':' See NOTE: [vmap-incompatible in-place operations] for what it means for an','line_number':2441,'multiline':False]['text':' in-place operation to be incompatible with vmap.','line_number':2442,'multiline':False]['text':'','line_number':2443,'multiline':False]['text':' If an in-place operation used in a backward formula is vmap-incompatible,','line_number':2444,'multiline':False]['text':' then as developers we have the following options:','line_number':2445,'multiline':False]['text':'','line_number':2446,'multiline':False]['text':' - If the in-place operation directly followed the creation of a tensor with','line_number':2447,'multiline':False]['text':'   a factory function like at::zeros(...), we should replace the factory with','line_number':2448,'multiline':False]['text':'   a corresponding grad.new_zeros(...) call. The grad.new_zeros(...) call','line_number':2449,'multiline':False]['text':'   propagates the batch dims to the resulting tensor.','line_number':2450,'multiline':False]['text':'   For example:','line_number':2451,'multiline':False]['text':'     Before: at::zeros(input.sizes(), grad.options()).copy_(grad)','line_number':2452,'multiline':False]['text':'     After:  grad.new_zeros(input.sizes()).copy_(grad)','line_number':2453,'multiline':False]['text':'','line_number':2454,'multiline':False]['text':' - If the in-place operation followed some sequence of operations, if the','line_number':2455,'multiline':False]['text':'   we want to be able to vmap over the backward formula as-is (this is','line_number':2456,'multiline':False]['text':'   usually the case for simple (<15loc) backward formulas), then use','line_number':2457,'multiline':False]['text':'   areAnyTensorSubclassLike  to guard the operation. For example:','line_number':2458,'multiline':False]['text':'             c = a * b','line_number':2459,'multiline':False]['text':'     Before: c.mul_(grad)','line_number':2460,'multiline':False]['text':'     After:  c = !areAnyTensorSubclassLike({c, grad}) ? c.mul_(grad) : c *','line_number':2461,'multiline':False]['text':'     grad','line_number':2462,'multiline':False]['text':'','line_number':2463,'multiline':False]['text':' - If we don't want to vmap directly over the backward formula (e.g., if the','line_number':2464,'multiline':False]['text':'   backward formula is too complicated or has a lot of vmap-incompatible','line_number':2465,'multiline':False]['text':'   operations, then register the backward formula as an operator and','line_number':2466,'multiline':False]['text':'   eventually write a batching rule for it.','line_number':2467,'multiline':False]['text':' gradient wrt input','line_number':2479,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2489,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2491,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2494,'multiline':False]['text':' gradient wrt grad_output','line_number':2512,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2521,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2523,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':2526,'multiline':False]['text':' special case to protect against a divide-by-zero.','line_number':2542,'multiline':False]['text':' NOTE [ as_strided Backward and layout-aware/agnostic autograd ]','line_number':2632,'multiline':False]['text':'','line_number':2633,'multiline':False]['text':' `storage_offset` is ignored for simplicity in this note. If you just want the','line_number':2634,'multiline':False]['text':' full algorithm without explanation, scroll down to bottom of this note.','line_number':2635,'multiline':False]['text':'','line_number':2636,'multiline':False]['text':' Implementing the backward of as_strided is tricky because you have to deal','line_number':2637,'multiline':False]['text':' with mappings that map one memory location to multiple indices, i.e., the','line_number':2638,'multiline':False]['text':' output tensor has multiple indices pointing to **overlapping** memory','line_number':2639,'multiline':False]['text':' addresses. This can happen in all in all sorts of weird cases. For example,','line_number':2640,'multiline':False]['text':'','line_number':2641,'multiline':False]['text':'   x = torch.randn(15)','line_number':2642,'multiline':False]['text':'   x.as_strided([3, 3], [1, 0])  # "expand" case','line_number':2643,'multiline':False]['text':'   x.as_strided([3, 3], [2, 1])  # "size too large" case','line_number':2644,'multiline':False]['text':'   x.as_strided([3, 2], [3, 6])  # res[2, 0] points to 2*3 + 0*6 = 6','line_number':2645,'multiline':False]['text':'                                 # res[0, 1] points to 0*3 + 1*6 = 6','line_number':2646,'multiline':False]['text':'','line_number':2647,'multiline':False]['text':' Here is the general strategy we apply in implementing as_strided backward:','line_number':2648,'multiline':False]['text':'   0. ??? (optimization step. we will talk about this later)','line_number':2649,'multiline':False]['text':'   1. Create some underlying flattened tensor as if it is the base tensor','line_number':2650,'multiline':False]['text':'      representing the contiguous memory storage for both input and output.','line_number':2651,'multiline':False]['text':'   2. Use the output geometry to scatter (or index_add) the gradients into','line_number':2652,'multiline':False]['text':'      this storage tensor.','line_number':2653,'multiline':False]['text':'   3. ??? (fix for input tensor with overlapping memory. we will talk about','line_number':2654,'multiline':False]['text':'           this later)','line_number':2655,'multiline':False]['text':'   4. Return the as_strided view of the storage tensor using input geometry.','line_number':2656,'multiline':False]['text':'','line_number':2657,'multiline':False]['text':' In step (2), if the output tensor doesn't have overlapping memory, we can','line_number':2658,'multiline':False]['text':' safely scatter (`storage.as_strided(output_geometry).copy_(grad)`);','line_number':2659,'multiline':False]['text':' otherwise, we must use `index_add` as gradients at different indices may need','line_number':2660,'multiline':False]['text':' to be summed to a single location.','line_number':2661,'multiline':False]['text':'','line_number':2662,'multiline':False]['text':' For example, in this case:','line_number':2663,'multiline':False]['text':'','line_number':2664,'multiline':False]['text':'   x = torch.randn(3)','line_number':2665,'multiline':False]['text':'   y = x.as_strided([3, 3], [1, 0])  # "expand" case','line_number':2666,'multiline':False]['text':'                                     # size   [ 3, 3]','line_number':2667,'multiline':False]['text':'                                     # stride [ 1, 0]','line_number':2668,'multiline':False]['text':'   y.backward()  # step (1): contiguous storagte tensor `s` of size 3, which','line_number':2669,'multiline':False]['text':'                             is large enough to be used as underlying storage','line_number':2670,'multiline':False]['text':'                             for `x` and `y`.','line_number':2671,'multiline':False]['text':'                               s = [ 0, 0, 0]','line_number':2672,'multiline':False]['text':'                 # step (2): since `y` has overlapping memory, index_add grad','line_number':2673,'multiline':False]['text':'                             into `s` basing on `y`'s geometry, i.e.,','line_number':2674,'multiline':False]['text':'                             s[i * y.stride(0) + j * y.stride(1)] += gy[i, j].','line_number':2675,'multiline':False]['text':'                               s = [ 3, 3, 3]','line_number':2676,'multiline':False]['text':'                 # step (4): as_strided view `s` using `x`'s geometry','line_number':2677,'multiline':False]['text':'                               s = [ 3, 3, 3]','line_number':2678,'multiline':False]['text':'                               grad_input = s.as_strided(x.size(), x.stride())','line_number':2679,'multiline':False]['text':'                                          = s.as_strided([3], [1])','line_number':2680,'multiline':False]['text':'                                          = [ 3, 3, 3]','line_number':2681,'multiline':False]['text':'','line_number':2682,'multiline':False]['text':' This is exactly what we would get if using `expand`. However, here the input','line_number':2683,'multiline':False]['text':' tensor doesn't have overlapping memory. If it does, we must add an extra step','line_number':2684,'multiline':False]['text':' before (4). Considering this case:','line_number':2685,'multiline':False]['text':'','line_number':2686,'multiline':False]['text':'   t = torch.randn(3)','line_number':2687,'multiline':False]['text':'   x = t.expand(3, 3)            # input with overlapping memory','line_number':2688,'multiline':False]['text':'                                 # size   [3, 3]','line_number':2689,'multiline':False]['text':'                                 # stride [0, 1]','line_number':2690,'multiline':False]['text':'   y = x.as_strided([1], [1])    # contiguous output','line_number':2691,'multiline':False]['text':'                                 # size   [1]','line_number':2692,'multiline':False]['text':'                                 # stride [1]','line_number':2693,'multiline':False]['text':'   y.backward()  # step (1): contiguous storage tensor `s` of size 3, which','line_number':2694,'multiline':False]['text':'                             is large enough to be used as underlying storage','line_number':2695,'multiline':False]['text':'                             for `x` and `y`.','line_number':2696,'multiline':False]['text':'                               s = [ 0, 0, 0]','line_number':2697,'multiline':False]['text':'                 # step (2): scatter grad into `s` basing on `y`'s geometry','line_number':2698,'multiline':False]['text':'                               s = [ 1, 0, 0]','line_number':2699,'multiline':False]['text':'                 # step (4): as_strided view `s` using `x`'s geometry','line_number':2700,'multiline':False]['text':'                               s = [ 1, 0, 0]','line_number':2701,'multiline':False]['text':'                               grad_input = s.as_strided([3, 3], [0, 1])','line_number':2702,'multiline':False]['text':'                                          = s.as_strided([3, 3], [0, 1])','line_number':2703,'multiline':False]['text':'                                          = [[ 1, 0, 0],','line_number':2704,'multiline':False]['text':'                                             [ 1, 0, 0],','line_number':2705,'multiline':False]['text':'                                             [ 1, 0, 0]]','line_number':2706,'multiline':False]['text':' Is this result correct?','line_number':2707,'multiline':False]['text':'','line_number':2708,'multiline':False]['text':' `x.as_strided([1], [1])` call is obviously equivalent with','line_number':2709,'multiline':False]['text':' `x[(0,) * x.dim()].view(1)` for any `x`. But autograd through the second','line_number':2710,'multiline':False]['text':' gives gradient `[ [ 1, 0, 0], [ 0, 0, 0], [ 0, 0, 0]]`. For this specific','line_number':2711,'multiline':False]['text':' case, indexing `x` at any index in first column is also equivalent, and','line_number':2712,'multiline':False]['text':' yields a gradient of shape `[3 x 3]` containing eight 0's and one 1. There is','line_number':2713,'multiline':False]['text':' an `x.size(1)`-times difference between these gradients computed from other','line_number':2714,'multiline':False]['text':' PyTorch ops and the gradient we got from as_strided.','line_number':2715,'multiline':False]['text':'','line_number':2716,'multiline':False]['text':' You might conclude that the gradients from as_strided is wrong. However,','line_number':2717,'multiline':False]['text':' let's first see why they are actually reasonable. Consider the pointwise','line_number':2718,'multiline':False]['text':' perturbations by `delta` anywhere in the first column of `x`. It will lead to','line_number':2719,'multiline':False]['text':' a `delta` change in the same memory location, and then `y` will change by','line_number':2720,'multiline':False]['text':' `delta`. So one can say the gradient should be exactly 1 at the first column,','line_number':2721,'multiline':False]['text':' as given by our above procedure.','line_number':2722,'multiline':False]['text':'','line_number':2723,'multiline':False]['text':' In the above computation of numerical gradients, they only match the','line_number':2724,'multiline':False]['text':' analytical results because strides and memory locations are considered in the','line_number':2725,'multiline':False]['text':' forward pass, i.e., this op (including both forward and backward) is','line_number':2726,'multiline':False]['text':' layout-aware.','line_number':2727,'multiline':False]['text':'','line_number':2728,'multiline':False]['text':' However, in PyTorch, most (probably all) other ops (forward and backward) are','line_number':2729,'multiline':False]['text':' layout-agnostic. E.g.,','line_number':2730,'multiline':False]['text':'','line_number':2731,'multiline':False]['text':'   t = torch.randn(1)','line_number':2732,'multiline':False]['text':'   x = t.expand(2)','line_number':2733,'multiline':False]['text':'   y = x.sum()','line_number':2734,'multiline':False]['text':'   y.backward()','line_number':2735,'multiline':False]['text':'','line_number':2736,'multiline':False]['text':' Layout-agnostic autograd (as it is currently in PyTorch) will give you','line_number':2737,'multiline':False]['text':'','line_number':2738,'multiline':False]['text':'   gy = 1','line_number':2739,'multiline':False]['text':'   gx = [ 1, 1]  # SumBackward:    torch.ones_like(x)','line_number':2740,'multiline':False]['text':'   gt = [ 2]     # ExpandBackward: gx.sum()','line_number':2741,'multiline':False]['text':'','line_number':2742,'multiline':False]['text':' Note that `gx = [ 1, 1]`. However, if you perturb any value in `x` by `delta`','line_number':2743,'multiline':False]['text':' (the other will also change by `delta`), `y` will change by `2 * delta`. So','line_number':2744,'multiline':False]['text':' the gradients, if strides are taken into consideration, should be 2.','line_number':2745,'multiline':False]['text':'','line_number':2746,'multiline':False]['text':' Layout-aware autograd should give you','line_number':2747,'multiline':False]['text':'','line_number':2748,'multiline':False]['text':'   gy = 1','line_number':2749,'multiline':False]['text':'   gx = [ 2, 2]  # Because the backward considers the fact that the input `x`','line_number':2750,'multiline':False]['text':'                 # is already expanded.','line_number':2751,'multiline':False]['text':'   gt = [ 2]     # Layout-aware backward of expand is just a slicing because','line_number':2752,'multiline':False]['text':'                 # the previous backward should have already taken care of','line_number':2753,'multiline':False]['text':'                 # strides and made sure that gradients are the same along the','line_number':2754,'multiline':False]['text':'                 # expanded dimension.','line_number':2755,'multiline':False]['text':'','line_number':2756,'multiline':False]['text':' As shown above, these two types are not compatible. Therefore, we must either','line_number':2757,'multiline':False]['text':' make as_strided layout-agnostic, or make all other ops layout-aware.','line_number':2758,'multiline':False]['text':'','line_number':2759,'multiline':False]['text':' It is difficult to support layout-aware autograd (at least in the current','line_number':2760,'multiline':False]['text':' codebase structure), because it would mean','line_number':2761,'multiline':False]['text':'   1. storing tensor geometries of every input tensor for backward','line_number':2762,'multiline':False]['text':'   2. depending on input geometry, the gradient computed from backward change','line_number':2763,'multiline':False]['text':'   3. ideally enforcing gradient of T to always have same strides as T','line_number':2764,'multiline':False]['text':' (although these two methods only differ when it comes to overlapping memory)','line_number':2765,'multiline':False]['text':'','line_number':2766,'multiline':False]['text':' Therefore, we must formulate `as_strided` in a layout-agnostic way, i.e.,','line_number':2767,'multiline':False]['text':' giving the same output regardless of the input layout. We consider','line_number':2768,'multiline':False]['text':' `input.stride()` as a separate independent fixed argument `input_stride`.','line_number':2769,'multiline':False]['text':' Then, `as_strided(input, size, stride)` can be thought of as:','line_number':2770,'multiline':False]['text':'   1. "Scatter" each value of `input` into a "storage" using storage location','line_number':2771,'multiline':False]['text':'      computed from the value's index in `input`, `input.size()` and','line_number':2772,'multiline':False]['text':'      `input_stride`, but if N values end up in the same location, the value','line_number':2773,'multiline':False]['text':'      is average of those N values (they will be the same value anyways).','line_number':2774,'multiline':False]['text':'','line_number':2775,'multiline':False]['text':'      Formal description:','line_number':2776,'multiline':False]['text':'        Denote the set of all input indices that pointing to the same storage','line_number':2777,'multiline':False]['text':'        location `storage[n]` as `S(n)`, i.e.,','line_number':2778,'multiline':False]['text':'','line_number':2779,'multiline':False]['text':'            S(n) = { index : <index, input_stride> == n, index is valid given','line_number':2780,'multiline':False]['text':'            input.size() },','line_number':2781,'multiline':False]['text':'','line_number':2782,'multiline':False]['text':'        where `<x, y>` is the dot product between `x` and `y`.','line_number':2783,'multiline':False]['text':'','line_number':2784,'multiline':False]['text':'        Then, the process is:','line_number':2785,'multiline':False]['text':'','line_number':2786,'multiline':False]['text':'            storage[n] = Avg { S(n) }','line_number':2787,'multiline':False]['text':'','line_number':2788,'multiline':False]['text':'        Note that all values in `S(n)` are the same (they point to the same','line_number':2789,'multiline':False]['text':'        memory location anyways, so this step doesn't change anything, but','line_number':2790,'multiline':False]['text':'        effectively avoids having the dependency on the layout of `input`.','line_number':2791,'multiline':False]['text':'        I.e., the result holds fixed regardless of the layout of `input`, as','line_number':2792,'multiline':False]['text':'        long as `input_stride` is fixed.','line_number':2793,'multiline':False]['text':'','line_number':2794,'multiline':False]['text':'      NOTE: for forward pass, we can equivalently simply select any one of','line_number':2795,'multiline':False]['text':'            `S(n)` as `storage[n]`. However, considering this as an average','line_number':2796,'multiline':False]['text':'            operation makes backward easier (so all values in set','line_number':2797,'multiline':False]['text':'            `{ grad_input[i] : i in S(n) }` are the same, and it can use the','line_number':2798,'multiline':False]['text':'            same geometry as input).','line_number':2799,'multiline':False]['text':'   2. As usual, return the as_strided view of `storage` using required output','line_number':2800,'multiline':False]['text':'      `size` and `stride`.','line_number':2801,'multiline':False]['text':'','line_number':2802,'multiline':False]['text':' To backward through this layout-agnostic version, we simply add the following','line_number':2803,'multiline':False]['text':' step:','line_number':2804,'multiline':False]['text':'   .... (scatter gradients into the storage tensor using output geometry)','line_number':2805,'multiline':False]['text':'   3. For all storage location n, `storage[n] /= |S(n)|`.','line_number':2806,'multiline':False]['text':'   .... (return as_strided view of the storage tensor using input geometry)','line_number':2807,'multiline':False]['text':'','line_number':2808,'multiline':False]['text':' Finally, we note that these general operations are expensive, so we apply the','line_number':2809,'multiline':False]['text':' following optimizations:','line_number':2810,'multiline':False]['text':'   Add step (0): For all output dimension `d` with output stride 0, sum the','line_number':2811,'multiline':False]['text':'                 gradients along dimension `d` (don't keepdim), and remove','line_number':2812,'multiline':False]['text':'                 dimension `d` from output size and stride.','line_number':2813,'multiline':False]['text':'                 (An optimization for "expand" cases so we may avoid step (3))','line_number':2814,'multiline':False]['text':'  Only apply step (3) when input tensor has overlapping memory.','line_number':2815,'multiline':False]['text':'','line_number':2816,'multiline':False]['text':' FULL ALGORITHM:','line_number':2817,'multiline':False]['text':'   0. For all output dimension `d` with output stride 0, sum the gradients','line_number':2818,'multiline':False]['text':'       along dimension `d` (don't keepdim), and remove dimension `d` from','line_number':2819,'multiline':False]['text':'       output size and stride.','line_number':2820,'multiline':False]['text':'   1. Create some underlying flattened tensor as if it is the base tensor','line_number':2821,'multiline':False]['text':'      representing the contiguous memory storage for both input and output.','line_number':2822,'multiline':False]['text':'   2. Use the output geometry to scatter (or index_add) the gradients into','line_number':2823,'multiline':False]['text':'      this storage tensor `storage`.','line_number':2824,'multiline':False]['text':'   3. If input tensor has overlapping memory,','line_number':2825,'multiline':False]['text':'      For all storage location `i`, `storage[i] /= N(i)`, where `N(i)` is the','line_number':2826,'multiline':False]['text':'      number of indices in input geometry pointing to the same storage','line_number':2827,'multiline':False]['text':'      location `i` (i.e., `|S(i)|` in equations above).','line_number':2828,'multiline':False]['text':'   4. Return the as_strided view of the storage tensor using input geometry.','line_number':2829,'multiline':False]['text':'','line_number':2830,'multiline':False]['text':' See NOTE [ Detecting Memory Overlap Within A Strided Tensor ] on how to','line_number':2831,'multiline':False]['text':' roughly detech overlapping memory.','line_number':2832,'multiline':False]['text':' NOTE [ Detecting Memory Overlap Within A Strided Tensor ]','line_number':2834,'multiline':False]['text':'','line_number':2835,'multiline':False]['text':' Checking memory overlap within a strided tensor is the special case of','line_number':2836,'multiline':False]['text':' detecting memory overlap of two strided tensors, where the two tensors start','line_number':2837,'multiline':False]['text':' at the same memory address. The later is HARD (see #8212).','line_number':2838,'multiline':False]['text':'','line_number':2839,'multiline':False]['text':' But even this special case isn't simple. This note describes a check for a','line_number':2840,'multiline':False]['text':' even more constrained simple case where we can be certain that there is no','line_number':2841,'multiline':False]['text':' overlap.','line_number':2842,'multiline':False]['text':'','line_number':2843,'multiline':False]['text':' The checking algorithm can be described as:','line_number':2844,'multiline':False]['text':'   0. Return [ pass check ] if any dimension has size 0','line_number':2845,'multiline':False]['text':'   1. Ignore all dimensions that have size 1','line_number':2846,'multiline':False]['text':'   2. If no remaining dimensions, return [ pass check ]','line_number':2847,'multiline':False]['text':'   3. Sort the remaining dimensions according to the strides decreasingly','line_number':2848,'multiline':False]['text':'   4. Check that for each dimension k,','line_number':2849,'multiline':False]['text':'','line_number':2850,'multiline':False]['text':'           stride[k] > \sum_{ i > k } (size[i] - 1) * stride[i]','line_number':2851,'multiline':False]['text':'','line_number':2852,'multiline':False]['text':'      That is equivalent to, after reordering the dimensions so strides are','line_number':2853,'multiline':False]['text':'      in decreasing order, checking that stride of each dimension is larger','line_number':2854,'multiline':False]['text':'      than the maximum memory offset in a slice at that dimension.','line_number':2855,'multiline':False]['text':'','line_number':2856,'multiline':False]['text':' Obviously this check passes for contiguous tensors ( the dimensions will be','line_number':2857,'multiline':False]['text':' already sorted with LHS = stride[0] = \prod size[i] being exactly 1 larger','line_number':2858,'multiline':False]['text':' than RHS ). Similarly, the check passes for tensors contiguous in all but','line_number':2859,'multiline':False]['text':' the last dimension, and LHS = stride[0] = stride[-1] * \prod size[i] being','line_number':2860,'multiline':False]['text':' exactly stride[-1] larger than RHS. (*)','line_number':2861,'multiline':False]['text':'','line_number':2862,'multiline':False]['text':' We will show that these view operations, including all our view operations','line_number':2863,'multiline':False]['text':' *except for* general as_strided and unfold, also preserve this invariant:','line_number':2864,'multiline':False]['text':'','line_number':2865,'multiline':False]['text':'  alias:      Obviously preserves','line_number':2866,'multiline':False]['text':'','line_number':2867,'multiline':False]['text':'  expand:     All changed dimensions are removed in step (1)','line_number':2868,'multiline':False]['text':'','line_number':2869,'multiline':False]['text':'  view:       Consider the input dimensions as grouped into consecutive','line_number':2870,'multiline':False]['text':'              dimension "blocks", where dimensions are contiguous in each one.','line_number':2871,'multiline':False]['text':'              one. view only works when the output dimensions can also be','line_number':2872,'multiline':False]['text':'              grouped into the same consecutive blocks of same ordering.','line_number':2873,'multiline':False]['text':'','line_number':2874,'multiline':False]['text':'              NB: this means that the number of elements and stride of the','line_number':2875,'multiline':False]['text':'                  last dimension in each block is the same in input and','line_number':2876,'multiline':False]['text':'                  output. (**)','line_number':2877,'multiline':False]['text':'','line_number':2878,'multiline':False]['text':'              Notation:','line_number':2879,'multiline':False]['text':'                Consider a single such block B,','line_number':2880,'multiline':False]['text':'                    ... B_prev[-1]], [ B[0], ..., B[i], ..., B[k] = B[-1] ], [','line_number':2881,'multiline':False]['text':'                    B_next[0], ...','line_number':2882,'multiline':False]['text':'                                start--^^^^                  ^^^^^^^^^^^^--end','line_number':2883,'multiline':False]['text':'                Each B[i] denotes a dimension index such that B[i] = B[0] + i.','line_number':2884,'multiline':False]['text':'','line_number':2885,'multiline':False]['text':'              We first show that in a tensor (i.e., input) satisfies the','line_number':2886,'multiline':False]['text':'              invariant, after sorting, the dimensions within each block','line_number':2887,'multiline':False]['text':'              still remain consecutive. (***)','line_number':2888,'multiline':False]['text':'','line_number':2889,'multiline':False]['text':'                After removing dimensions of size 1, the dimensions within a','line_number':2890,'multiline':False]['text':'                block is already sorted by strides in descending order. So','line_number':2891,'multiline':False]['text':'                sorting all dimensions will not change the relative ordering','line_number':2892,'multiline':False]['text':'                among them.','line_number':2893,'multiline':False]['text':'','line_number':2894,'multiline':False]['text':'                Assume that some block B is not consecutive after sorting,','line_number':2895,'multiline':False]['text':'                i.e., there exists a dimension d between B[0] and B[-1] in','line_number':2896,'multiline':False]['text':'                sorted order.','line_number':2897,'multiline':False]['text':'','line_number':2898,'multiline':False]['text':'                By (*), we know that','line_number':2899,'multiline':False]['text':'                       stride[B[0]]','line_number':2900,'multiline':False]['text':'                    =  \sum_{i > 0}   (size[B[i]] - 1) * stride[B[i]] +','line_number':2901,'multiline':False]['text':'                    stride[B[-1]] <  \sum_{i > 0}   (size[B[i]] - 1) *','line_number':2902,'multiline':False]['text':'                    stride[B[i]] + stride[d]','line_number':2903,'multiline':False]['text':'                    <= \sum_{i > 0}   (size[B[i]] - 1) * stride[B[i]] +','line_number':2904,'multiline':False]['text':'                    (size[d] - 1) * stride[d]','line_number':2905,'multiline':False]['text':'                    <= \sum{j > B[0]} (size[j]    - 1) * stride[j],','line_number':2906,'multiline':False]['text':'','line_number':2907,'multiline':False]['text':'                where the first <   comes from sorting and','line_number':2908,'multiline':False]['text':'                      the second <= comes from the fact that dimension d','line_number':2909,'multiline':False]['text':'                                               exists after step (1) and','line_number':2910,'multiline':False]['text':'                                               thus must have size greater','line_number':2911,'multiline':False]['text':'                                               than 1','line_number':2912,'multiline':False]['text':'                      the third  <= comes from the fact that each term in','line_number':2913,'multiline':False]['text':'                                               the sum is non-negative','line_number':2914,'multiline':False]['text':'','line_number':2915,'multiline':False]['text':'                Then we have a countradiction as the invariant must not be','line_number':2916,'multiline':False]['text':'                satisfied at B[0]. So the original proposition is true.','line_number':2917,'multiline':False]['text':'','line_number':2918,'multiline':False]['text':'              Now that we established the above claim (***), we consider the','line_number':2919,'multiline':False]['text':'              view operation as first sorting the dimensions (i.e., blocks),','line_number':2920,'multiline':False]['text':'              apply the original view (since it only cares dimensions being','line_number':2921,'multiline':False]['text':'              consecutive and contiguous withtin each block), and then undo','line_number':2922,'multiline':False]['text':'              the sort.','line_number':2923,'multiline':False]['text':'','line_number':2924,'multiline':False]['text':'              Consider a single block B in the output,','line_number':2925,'multiline':False]['text':'                  ... ], [ B[0], ..., B[i], ..., B[k] = B[-1] ], [ ...','line_number':2926,'multiline':False]['text':'                    start--^^^^                  ^^^^^^^^^^^^--end','line_number':2927,'multiline':False]['text':'','line_number':2928,'multiline':False]['text':'              By (*), we know that for all i','line_number':2929,'multiline':False]['text':'                  stride[i] = stride[B[-1]] +','line_number':2930,'multiline':False]['text':'                                \sum_{j=i+1}^{k} (size[B[j]] - 1) *','line_number':2931,'multiline':False]['text':'                                stride[B[j]]','line_number':2932,'multiline':False]['text':'','line_number':2933,'multiline':False]['text':'              Then the invariant is obviously satisfied at every dimension','line_number':2934,'multiline':False]['text':'              in this block if it is satisfied at dimension B[-1]. It only','line_number':2935,'multiline':False]['text':'              remains to show that it is satisfied at the last dimension in','line_number':2936,'multiline':False]['text':'              each block.','line_number':2937,'multiline':False]['text':'','line_number':2938,'multiline':False]['text':'              Since the same blocks are present in both input and output','line_number':2939,'multiline':False]['text':'              with the same ordering, we will abuse the notation in the','line_number':2940,'multiline':False]['text':'              following statements.','line_number':2941,'multiline':False]['text':'','line_number':2942,'multiline':False]['text':'              By (*), we know that the following holds for both input and','line_number':2943,'multiline':False]['text':'              output, for any block B:','line_number':2944,'multiline':False]['text':'                    \sum_{i > B[-1]} (size[i] - 1) * stride[i]','line_number':2945,'multiline':False]['text':'                  = \sum_{block B' after B} \prod_{j in B'} size[B[j]] *','line_number':2946,'multiline':False]['text':'                  stride[B'[-1]] = \sum_{block B' after B} numel(B') *','line_number':2947,'multiline':False]['text':'                  stride[B'[-1]].','line_number':2948,'multiline':False]['text':'                    ^^^^^^^^^^^^^^^^^^^^^^^|^^^^^^^^^^^^^^^^^^^^^^^^^^','line_number':2949,'multiline':False]['text':'              By (**), we know that, this quantity in the above equation','line_number':2950,'multiline':False]['text':'              remains the same in input and output. So both','line_number':2951,'multiline':False]['text':'                  \sum_{i > B[-1]} (size[i] - 1) * stride[i]','line_number':2952,'multiline':False]['text':'              and','line_number':2953,'multiline':False]['text':'                  stride[B[-1]]','line_number':2954,'multiline':False]['text':'              are the same in input and output.','line_number':2955,'multiline':False]['text':'','line_number':2956,'multiline':False]['text':'              These two quantities are exactly the LHS and RHS of the','line_number':2957,'multiline':False]['text':'              invariant inequality. Since by assumption the invariant is','line_number':2958,'multiline':False]['text':'              satisfied in input at B[-1], it is also satisfied in output at','line_number':2959,'multiline':False]['text':'              B[-1]. This concludes the proof.','line_number':2960,'multiline':False]['text':'','line_number':2961,'multiline':False]['text':'  squeeze:    Special case of view','line_number':2962,'multiline':False]['text':'','line_number':2963,'multiline':False]['text':'  unsqueeze:  Special case of view','line_number':2964,'multiline':False]['text':'','line_number':2965,'multiline':False]['text':'  slice:      Consider slicing dimension i with step = k >= 1.','line_number':2966,'multiline':False]['text':'','line_number':2967,'multiline':False]['text':'              Let stride' and size' be the output strides and sizes. We have','line_number':2968,'multiline':False]['text':'','line_number':2969,'multiline':False]['text':'                  stride'[i] = k * stride[i]','line_number':2970,'multiline':False]['text':'                  size'[i] <= floor(size[i] / k)','line_number':2971,'multiline':False]['text':'','line_number':2972,'multiline':False]['text':'              If size'[i] = 1, invariant is obviously satisfied as we are','line_number':2973,'multiline':False]['text':'              just removing a dimension (afte step (1)).','line_number':2974,'multiline':False]['text':'','line_number':2975,'multiline':False]['text':'              Assume size'[i] > 1.','line_number':2976,'multiline':False]['text':'','line_number':2977,'multiline':False]['text':'              By assumption, the invariant is satisfied at every dimension','line_number':2978,'multiline':False]['text':'              in input.','line_number':2979,'multiline':False]['text':'','line_number':2980,'multiline':False]['text':'              For any dimension j, if stride[j] > stride[i], we have','line_number':2981,'multiline':False]['text':'                  stride'[j] =  stride[j]','line_number':2982,'multiline':False]['text':'                             >  (size[i] - 1) * stride[i]','line_number':2983,'multiline':False]['text':'                             =  (size[i] / k * k - 1) * k * stride[i] / k','line_number':2984,'multiline':False]['text':'                             =  (size[i] / k - 1 / k) * stride'[i]','line_number':2985,'multiline':False]['text':'                             >= (size'[i]    - 1 / k) * stride'[i]','line_number':2986,'multiline':False]['text':'                             >= stride'[i].','line_number':2987,'multiline':False]['text':'','line_number':2988,'multiline':False]['text':'              If stride[j] < stride[i], we have','line_number':2989,'multiline':False]['text':'                  stride'[j] = stride[j] < stride[i] <= stride'[i].','line_number':2990,'multiline':False]['text':'','line_number':2991,'multiline':False]['text':'              So the sorting order remains unchanged after slice.','line_number':2992,'multiline':False]['text':'','line_number':2993,'multiline':False]['text':'              Since','line_number':2994,'multiline':False]['text':'                     (size'[i] - 1) * stride'[i]','line_number':2995,'multiline':False]['text':'                  =  (floor(size[i] / k) - 1) * k * stride[i]','line_number':2996,'multiline':False]['text':'                  <= (size[i] / k - 1) * k * stride[i]','line_number':2997,'multiline':False]['text':'                  =  (size[i] - k) * stride[i]','line_number':2998,'multiline':False]['text':'                  <= (size[i] - 1) * * stride[i],','line_number':2999,'multiline':False]['text':'              the term from this dimension i in the invariant inequality at','line_number':3000,'multiline':False]['text':'              other dimensions can only decrease after slice. So the','line_number':3001,'multiline':False]['text':'              invariant is preserved.','line_number':3002,'multiline':False]['text':'','line_number':3003,'multiline':False]['text':'  narrow:     Special case of slice','line_number':3004,'multiline':False]['text':'','line_number':3005,'multiline':False]['text':'  select:     narrow + squeeze','line_number':3006,'multiline':False]['text':'','line_number':3007,'multiline':False]['text':'  permute:    Sorting makes permutation of dimensions irrelevant','line_number':3008,'multiline':False]['text':'','line_number':3009,'multiline':False]['text':'  transpose:  Sorting makes swapping dimensions irrelevant','line_number':3010,'multiline':False]['text':'','line_number':3011,'multiline':False]['text':'  diagonal:   Effectively merging two dimensions i and j into a new','line_number':3012,'multiline':False]['text':'              dimension k s.t.','line_number':3013,'multiline':False]['text':'                  stride'[k] =  stride[i] + stride[j]','line_number':3014,'multiline':False]['text':'                  size'[k]   <= min(size[i], size[j]),','line_number':3015,'multiline':False]['text':'              where stride and size are on the input, and stride' and size'','line_number':3016,'multiline':False]['text':'              are on the output.','line_number':3017,'multiline':False]['text':'','line_number':3018,'multiline':False]['text':'              Assuming that size[i] > 1 and size[j] > 1. If any has size 1,','line_number':3019,'multiline':False]['text':'              then this is unsqueeze on that dimension.','line_number':3020,'multiline':False]['text':'','line_number':3021,'multiline':False]['text':'              WLOG, say stride[i] >= stride[j].','line_number':3022,'multiline':False]['text':'','line_number':3023,'multiline':False]['text':'              Each dimension d in input with stride[d] > stride[j] has','line_number':3024,'multiline':False]['text':'                  stride'[d] =  stride[d]','line_number':3025,'multiline':False]['text':'                             >  (size[i] - 1) * stride[i] + (size[j] - 1) *','line_number':3026,'multiline':False]['text':'                             stride[j]','line_number':3027,'multiline':False]['text':'                             >= stride[i] + stride[j]','line_number':3028,'multiline':False]['text':'                             =  stride[k].','line_number':3029,'multiline':False]['text':'              So, considering the sorted dimensions, this is effectively','line_number':3030,'multiline':False]['text':'              removing i, and replacing j with k.','line_number':3031,'multiline':False]['text':'','line_number':3032,'multiline':False]['text':'              For dimensions d with stride[i] < stride[d] < stride[j], the','line_number':3033,'multiline':False]['text':'              term from dimension i is removed in the invariant inequality.','line_number':3034,'multiline':False]['text':'              For dimensions d with stride[d] > stride[j], we have','line_number':3035,'multiline':False]['text':'                     (size'[k] - 1) * stride'[k]','line_number':3036,'multiline':False]['text':'                  <= (min(size[i], size[j]) - 1) * (stride[i] + stride[j])','line_number':3037,'multiline':False]['text':'                  <= (size[i] - 1) * stride[i] + (size[j] - 1) * stride[j],','line_number':3038,'multiline':False]['text':'              so the term from i and j in the invariant can only decrease.','line_number':3039,'multiline':False]['text':'','line_number':3040,'multiline':False]['text':'              So this is generally relaxing the constraint, and thus it','line_number':3041,'multiline':False]['text':'              preserves it.','line_number':3042,'multiline':False]['text':' This implements steps (2)~(4) of the algorithm in','line_number':3044,'multiline':False]['text':' NOTE [ Detecting Memory Overlap Within A Strided Tensor ]','line_number':3045,'multiline':False]['text':' Helper for as_strided_backward','line_number':3046,'multiline':False]['text':' Returns the minimum storage size needed to contain a tensor of sizes,','line_number':3070,'multiline':False]['text':' strides, and storage_offset Helper for as_strided_backward','line_number':3071,'multiline':False]['text':' See NOTE [ as_strided Backward and layout-aware/agnostic autograd ] for','line_number':3088,'multiline':False]['text':' explanation','line_number':3089,'multiline':False]['text':' For output geometry,','line_number':3096,'multiline':False]['text':'   check for size 0 dimensions,','line_number':3097,'multiline':False]['text':'   skip size 1 dimensions,','line_number':3098,'multiline':False]['text':'   reduce grad on expanded dims (stride=0, size>1)','line_number':3099,'multiline':False]['text':' Step (0)     for the algorithm in NOTE [ as_strided Backward and','line_number':3100,'multiline':False]['text':' layout-aware/agnostic autograd ] Step (0)~(1) for the algorithm in NOTE [','line_number':3101,'multiline':False]['text':' Detecting Memory Overlap Within A Strided Tensor ]','line_number':3102,'multiline':False]['text':'              on output geometry','line_number':3103,'multiline':False]['text':' Step (2)~(4) for the algorithm in NOTE [ Detecting Memory Overlap Within A','line_number':3124,'multiline':False]['text':' Strided Tensor ]','line_number':3125,'multiline':False]['text':'              on output geometry','line_number':3126,'multiline':False]['text':' For input geometry,','line_number':3129,'multiline':False]['text':'   check for size 0 dimensions,','line_number':3130,'multiline':False]['text':'   skip size 1 dimensions,','line_number':3131,'multiline':False]['text':' Step (0)~(1) for the algorithm in NOTE [ Detecting Memory Overlap Within A','line_number':3132,'multiline':False]['text':' Strided Tensor ]','line_number':3133,'multiline':False]['text':'              on input geometry','line_number':3134,'multiline':False]['text':' Step (1)~(4) for the algorithm in NOTE [ Detecting Memory Overlap Within A','line_number':3151,'multiline':False]['text':' Strided Tensor ]','line_number':3152,'multiline':False]['text':'              on input geometry','line_number':3153,'multiline':False]['text':' Rest of this function implements','line_number':3156,'multiline':False]['text':' Step (1)~(4) for the algorithm in NOTE [ as_strided Backward and','line_number':3157,'multiline':False]['text':' layout-aware/agnostic autograd ]','line_number':3158,'multiline':False]['text':' TODO: Raise if not all output values are visible in input geometry.','line_number':3159,'multiline':False]['text':'       Technically speaking, if you treat those values as constants, not','line_number':3160,'multiline':False]['text':'       raising is fine, and mathematically correct. However, these values','line_number':3161,'multiline':False]['text':'       really are contained in some base tensor, and by treating them as','line_number':3162,'multiline':False]['text':'       constants we are ignoring this tight dependency. Therefore, it is','line_number':3163,'multiline':False]['text':'       more sensible to raise here.','line_number':3164,'multiline':False]['text':' Step (1): create underlying tensor as "storage"','line_number':3166,'multiline':False]['text':' TODO: symint-ify. Do we need a min() and max() for SymInts?','line_number':3168,'multiline':False]['text':' prepare indices tensor if we will do index_add_ later','line_number':3180,'multiline':False]['text':' TODO: should we symint-ify arange? Need SymScalar.','line_number':3184,'multiline':False]['text':' Step (2): use output geometry to scatter gradients into storage','line_number':3191,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':3193,'multiline':False]['text':' assume that new tensors have 0 storage offset','line_number':3198,'multiline':False]['text':' Step (3): if input tensor has overlapping memory, divide scattered gradient','line_number':3203,'multiline':False]['text':'           at storage[i] by the number of times i shows up in input geometry','line_number':3204,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':3208,'multiline':False]['text':' this will give nan outside visible range','line_number':3214,'multiline':False]['text':' Step (4): return as_strided view of the storage tensor with input geometry','line_number':3216,'multiline':False]['text':' Note [as_strided_scatter backward support]','line_number':3228,'multiline':False]['text':' as_strided_scatter handling for autograd is a beast, and is non-trivial to','line_number':3229,'multiline':False]['text':' implement for arbitrarily strided inputs. Most uses for as_strided with','line_number':3230,'multiline':False]['text':' functionalization only care about the contiguous case anyway, So for now','line_number':3231,'multiline':False]['text':' this is not implemented. When autograd is being used, we ban non-contiguous','line_number':3232,'multiline':False]['text':' inputs. We can assume that the input was a contiguous tensor. Also, we'll','line_number':3233,'multiline':False]['text':' take the perf hit and contiguify grad for now.','line_number':3234,'multiline':False]['text':' if (at::native::get_gelutype_enum(approximate) ==','line_number':3266,'multiline':False]['text':' at::native::GeluType::Tanh) {','line_number':3267,'multiline':False]['text':' left_derivative = f_prime_gh','line_number':3288,'multiline':False]['text':' right_derivative = f_prime_gh + g_prime_fh + h_prime_fg','line_number':3289,'multiline':False]['text':' dgrad_dX = left_derivative + right_derivative','line_number':3290,'multiline':False]['text':' See svd_backward for the derivation','line_number':3352,'multiline':False]['text':' With sym(X) = X + X^H, we implement','line_number':3353,'multiline':False]['text':' dU = U (sym(dX S) / E + i Im(diag(dX)) / (2S))','line_number':3354,'multiline':False]['text':' if m > n','line_number':3355,'multiline':False]['text':'   dU = [dU for m == n] + (I_m - UU^H) dA V S^{-1}','line_number':3356,'multiline':False]['text':' dS = Re(diag(dP))','line_number':3357,'multiline':False]['text':' dV = V (sym(S dX) / E - i Im(diag(dX)) / (2S))','line_number':3358,'multiline':False]['text':' if m < n','line_number':3359,'multiline':False]['text':'   dV = [dV for m == n] + (I_n - VV^H) (dA)^H U S^{-1}','line_number':3360,'multiline':False]['text':' dVh = dV^H','line_number':3361,'multiline':False]['text':' with dP = U^H dA V','line_number':3362,'multiline':False]['text':'      dX = dP - dS','line_number':3363,'multiline':False]['text':'      E_{jk} = S_k^2 - S_j^2 if j != k','line_number':3364,'multiline':False]['text':'               1             otherwise','line_number':3365,'multiline':False]['text':' Checks compute_uv=true','line_number':3367,'multiline':False]['text':' dP = U^H dA V','line_number':3379,'multiline':False]['text':' dX = dP - dS','line_number':3386,'multiline':False]['text':' Any number a != 0 would, as we are just going to use it to compute 0 / a','line_number':3392,'multiline':False]['text':' later on','line_number':3393,'multiline':False]['text':' diag(dP) / (2S)','line_number':3400,'multiline':False]['text':' dU = U (sym(dP S) / E) + i Im(diag(dP)) / (2S)','line_number':3403,'multiline':False]['text':' dU += (I_m - UU^H) dA V S^{-1}','line_number':3412,'multiline':False]['text':' To "fix" the full_matrices case (the full_matrices case should not be','line_number':3416,'multiline':False]['text':' differentiable...)','line_number':3417,'multiline':False]['text':'dim=','line_number':3421,'multiline':True]['text':' dVh = -sym(S dP) / E + i Im(diag(dP)) / (2S)','line_number':3425,'multiline':False]['text':' Perf: We negate the S as it's the smallest tensor in the equation','line_number':3426,'multiline':False]['text':' dVh += S^{-1} U^H dA (I_n - VV^H)','line_number':3435,'multiline':False]['text':' To "fix" the full_matrices case (the full_matrices case should not be','line_number':3439,'multiline':False]['text':' differentiable...)','line_number':3440,'multiline':False]['text':'dim=','line_number':3444,'multiline':True]['text':' Throughout both the real and complex case we assume A has distinct singular','line_number':3459,'multiline':False]['text':' values. Furthermore, if A is rectangular or complex, we assume it's','line_number':3460,'multiline':False]['text':' full-rank.','line_number':3461,'multiline':False]['text':'','line_number':3462,'multiline':False]['text':'','line_number':3463,'multiline':False]['text':' The real case (A \in R)','line_number':3464,'multiline':False]['text':' See e.g. https://j-towns.github.io/papers/svd-derivative.pdf','line_number':3465,'multiline':False]['text':'','line_number':3466,'multiline':False]['text':' Denote by skew(X) = X - X^T, and by A o B the coordinatewise product, then','line_number':3467,'multiline':False]['text':' if m == n','line_number':3468,'multiline':False]['text':'   gA = U [(skew(U^T gU) / E)S + S(skew(V^T gV) / E) + I o gS ]V^T','line_number':3469,'multiline':False]['text':' where E_{jk} = S_k^2 - S_j^2 if j != k and 1 otherwise','line_number':3470,'multiline':False]['text':'','line_number':3471,'multiline':False]['text':' if m > n','line_number':3472,'multiline':False]['text':'   gA = [term in m == n] + (I_m - UU^T)gU S^{-1} V^T','line_number':3473,'multiline':False]['text':' if m < n','line_number':3474,'multiline':False]['text':'   gA = [term in m == n] + U S^{-1} (gV)^T (I_n - VV^T)','line_number':3475,'multiline':False]['text':'','line_number':3476,'multiline':False]['text':'','line_number':3477,'multiline':False]['text':' The complex case (A \in C)','line_number':3478,'multiline':False]['text':' This one is trickier because the svd is not locally unique.','line_number':3479,'multiline':False]['text':' Denote L = diag(e^{i\theta_k}), then we have that if A = USV^H, then (UL,','line_number':3480,'multiline':False]['text':' S, VL) is another valid SVD decomposition of A as A = ULS(VL)^H =','line_number':3481,'multiline':False]['text':' ULSL^{-1}V^H = USV^H, since L, S and L^{-1} commute, since they are all','line_number':3482,'multiline':False]['text':' diagonal.','line_number':3483,'multiline':False]['text':'','line_number':3484,'multiline':False]['text':' Assume wlog that n >= k in what follows, as otherwise we could reason about','line_number':3485,'multiline':False]['text':' A^H. Denote by St_k(C^n) = {A \in C^{n,k} | A^H A = I_k} the complex','line_number':3486,'multiline':False]['text':' Stiefel manifold. What this invariance means is that the svd decomposition','line_number':3487,'multiline':False]['text':' is not a map svd: C^{n x k} -> St_k(C^n) x R^n x St_k(C^k) (where St_k(C^k)','line_number':3488,'multiline':False]['text':' is simply the unitary group U(k)) but a map svd: C^{n x k} -> M x R^n where','line_number':3489,'multiline':False]['text':' M is the manifold given by quotienting St_k(C^n) x U(n) by the action (U,','line_number':3490,'multiline':False]['text':' V) -> (UL, VL) with L as above. Note that M is a manifold, because the','line_number':3491,'multiline':False]['text':' action is free and proper (as U(1)^k \iso (S^1)^k is compact). For this','line_number':3492,'multiline':False]['text':' reason, pi : St_k(C^n) x U(n) -> M forms a principal bundle.','line_number':3493,'multiline':False]['text':'','line_number':3494,'multiline':False]['text':' To think about M, consider the case case k = 1. The, we have the bundle','line_number':3495,'multiline':False]['text':' pi : St_1(C^n) x U(1) -> M','line_number':3496,'multiline':False]['text':' now, St_1(C^n) are just vectors of norm 1 in C^n. That's exactly the sphere','line_number':3497,'multiline':False]['text':' of dimension 2n-1 in C^n \iso R^{2n} S^{2n-1} = { z \in C^n | z^H z = 1}.','line_number':3498,'multiline':False]['text':' Then, in this case, we're quotienting out U(1) completely, so we get that','line_number':3499,'multiline':False]['text':' pi : S^{2n-1} x U(1) -> CP(n-1)','line_number':3500,'multiline':False]['text':' where CP(n-1) is the complex projective space of dimension n-1.','line_number':3501,'multiline':False]['text':' In other words, M is just the complex projective space, and pi is (pretty','line_number':3502,'multiline':False]['text':' similar to) the usual principal bundle from S^{2n-1} to CP(n-1). The case k','line_number':3503,'multiline':False]['text':' > 1 is the same, but requiring a linear independence condition between the','line_number':3504,'multiline':False]['text':' vectors from the different S^{2n-1} or CP(n-1).','line_number':3505,'multiline':False]['text':'','line_number':3506,'multiline':False]['text':' Note that this is a U(1)^k-bundle. In plain words, this means that the','line_number':3507,'multiline':False]['text':' fibres of this bundle, i.e. pi^{-1}(x) for x \in M are isomorphic to U(1) x','line_number':3508,'multiline':False]['text':' ... x U(1). This is obvious as, if pi(U,V) = x, pi^{-1}(x) = {(U','line_number':3509,'multiline':False]['text':' diag(e^{i\theta}), V diag(e^{i\theta})) | \theta \in R^k}','line_number':3510,'multiline':False]['text':'            = {(U diag(z), V diag(z)) | z \in U(1)^k}','line_number':3511,'multiline':False]['text':' since U(1) = {z \in C | |z| = 1}.','line_number':3512,'multiline':False]['text':'','line_number':3513,'multiline':False]['text':' The big issue here is that M with its induced metric is not locally','line_number':3514,'multiline':False]['text':' isometric to St_k(C^n) x U(k). [The why is rather technical, but you can','line_number':3515,'multiline':False]['text':' see that the horizontal distribution is not involutive, and hence','line_number':3516,'multiline':False]['text':' integrable due to Frobenius' theorem] What this means in plain words is','line_number':3517,'multiline':False]['text':' that, no matter how we choose to return the U and V from the SVD, we won't','line_number':3518,'multiline':False]['text':' be able to simply differentiate wrt. U and V and call it a day. An example','line_number':3519,'multiline':False]['text':' of a case where we can do this is when performing an eigendecomposition on','line_number':3520,'multiline':False]['text':' a real matrix that happens to have real eigendecomposition. In this case,','line_number':3521,'multiline':False]['text':' even though you can rescale the eigenvectors by any real number, you can','line_number':3522,'multiline':False]['text':' choose them of norm 1 and call it a day. In the eigenvector case, we are','line_number':3523,'multiline':False]['text':' using that you can isometrically embed S^{n-1} into R^n. In the svd case,','line_number':3524,'multiline':False]['text':' we need to work with the "quotient manifold" M explicitly, which is','line_number':3525,'multiline':False]['text':' slightly more technically challenging.','line_number':3526,'multiline':False]['text':'','line_number':3527,'multiline':False]['text':' Since the columns of U and V are not uniquely defined, but are','line_number':3528,'multiline':False]['text':' representatives of certain classes of equivalence which represent elements','line_number':3529,'multiline':False]['text':' M, the user may not depend on the particular representative that we return','line_number':3530,'multiline':False]['text':' from the SVD. In particular, if the loss function depends on U or V, it','line_number':3531,'multiline':False]['text':' must be invariant under the transformation (U, V) -> (UL, VL) with L =','line_number':3532,'multiline':False]['text':' diag(e^{i\theta})), for every \theta \in R^k. In more geometrical terms,','line_number':3533,'multiline':False]['text':' this means that the loss function should be constant on the fibres, or, in','line_number':3534,'multiline':False]['text':' other words, the gradient along the fibres should be zero. We may see this','line_number':3535,'multiline':False]['text':' by checking that the gradients as element in the tangent space T_{(U,','line_number':3536,'multiline':False]['text':' V)}(St(n,k) x U(k)) are normal to the fibres. Differentiating the map (U,','line_number':3537,'multiline':False]['text':' V) -> (UL, VL), we see that the space tangent to the fibres is given by','line_number':3538,'multiline':False]['text':' Vert_{(U, V)}(St(n,k) x U(k)) = { i[U, V]diag(\theta) | \theta in R^k}','line_number':3539,'multiline':False]['text':' where [U, V] denotes the vertical concatenation of U and V to form an (n+k,','line_number':3540,'multiline':False]['text':' k) matrix. Then, solving <i[U,V]diag(\theta), [S, T]> = 0 for two matrices','line_number':3541,'multiline':False]['text':' S, T \in T_{(U, V)}(St(n,k) x U(k)) where <A, B> = Re tr(A^H B) is the','line_number':3542,'multiline':False]['text':' canonical (real) inner product in C^{n x k} we get that the function is','line_number':3543,'multiline':False]['text':' invariant under action of U(1)^k iff Im(diag(U^H gU + V^H gV)) = 0','line_number':3544,'multiline':False]['text':'','line_number':3545,'multiline':False]['text':' Using this in the derviaton for the forward AD, one sees that, with the','line_number':3546,'multiline':False]['text':' notation from those notes Using this and writing sym(X) = X + X^H, we get','line_number':3547,'multiline':False]['text':' that the forward AD for SVD in the complex case is given by dU = U (sym(dX','line_number':3548,'multiline':False]['text':' S) / E + i Im(diag(dX)) / (2S)) if m > n','line_number':3549,'multiline':False]['text':'   dU = [dU for m == n] + (I_m - UU^H) dA V S^{-1}','line_number':3550,'multiline':False]['text':' dS = Re(diag(dP))','line_number':3551,'multiline':False]['text':' dV = V (sym(S dX) / E - i Im(diag(dX)) / (2S))','line_number':3552,'multiline':False]['text':' if m < n','line_number':3553,'multiline':False]['text':'   dV = [dV for m == n] + (I_n - VV^H) (dA)^H U S^{-1}','line_number':3554,'multiline':False]['text':' dVh = dV^H','line_number':3555,'multiline':False]['text':' with dP = U^H dA V','line_number':3556,'multiline':False]['text':'      dX = dP - dS','line_number':3557,'multiline':False]['text':'      E_{jk} = S_k^2 - S_j^2 if j != k','line_number':3558,'multiline':False]['text':'               1             otherwise','line_number':3559,'multiline':False]['text':'','line_number':3560,'multiline':False]['text':' Similarly, writing skew(X) = X - X^H','line_number':3561,'multiline':False]['text':' the adjoint wrt. the canonical metric is given by','line_number':3562,'multiline':False]['text':' if m == n','line_number':3563,'multiline':False]['text':'   gA = U [((skew(U^H gU) / E) S + i Im(diag(U^H gU)) / S + S ((skew(V^H gV)','line_number':3564,'multiline':False]['text':'   / E)) + I o gS] V^H','line_number':3565,'multiline':False]['text':' if m > n','line_number':3566,'multiline':False]['text':'   gA = [term in m == n] + (I_m - UU^H)gU S^{-1} V^H','line_number':3567,'multiline':False]['text':' if m < n','line_number':3568,'multiline':False]['text':'   gA = [term in m == n] + U S^{-1} (gV)^H (I_n - VV^H)','line_number':3569,'multiline':False]['text':' where we have used that Im(diag(U^H gU)) = - Im(diag(V^h gV)) to group the','line_number':3570,'multiline':False]['text':' diagonal imaginary terms into one that just depends on U^H gU.','line_number':3571,'multiline':False]['text':' Checks compute_uv=true','line_number':3573,'multiline':False]['text':' Trivial case','line_number':3576,'multiline':False]['text':' Optimisation for svdvals: gA = U @ diag(gS) @ Vh','line_number':3584,'multiline':False]['text':' At this point, at least one of gU, gVh is defined','line_number':3589,'multiline':False]['text':' Check for the invariance of the loss function, i.e.','line_number':3596,'multiline':False]['text':' Im(diag(U^H gU)) + Im(diag(V^H gV)) = 0','line_number':3597,'multiline':False]['text':' Rather lax atol and rtol, as we don't want false positives','line_number':3603,'multiline':False]['text':'rtol=','line_number':3605,'multiline':True]['text':'atol=','line_number':3605,'multiline':True]['text':' gA = ((U^H gU) / E) S +  S (((V^H gV) / E) + I o (gS + diag(U^H gU) / (2 *','line_number':3611,'multiline':False]['text':' S))','line_number':3612,'multiline':False]['text':' ret holds everything but the diagonal of gA','line_number':3614,'multiline':False]['text':' Any number a != 0 would, as we are just going to use it to compute 0','line_number':3619,'multiline':False]['text':' / a later on','line_number':3620,'multiline':False]['text':' gVh.defined();','line_number':3631,'multiline':False]['text':' Fill the diagonal','line_number':3635,'multiline':False]['text':' gA = [UgA + (I_m - UU^H)gU S^{-1}]V^H','line_number':3646,'multiline':False]['text':'   gA = U[gA V^H + S^{-1} (gV)^H (I_n - VV^H)]','line_number':3652,'multiline':False]['text':' gA = U gA V^H','line_number':3658,'multiline':False]['text':' https://arxiv.org/pdf/1701.00392.pdf Eq 4.77','line_number':3674,'multiline':False]['text':' For A = VLV^{-1}, denoting the gradients gA, gV and gL, we have','line_number':3675,'multiline':False]['text':' gA = V^{-H}(diag_embed(gL) + (V^H gV -V^HV diag(real(V^H gV))) / E*)V^H','line_number':3676,'multiline':False]['text':' Where:','line_number':3677,'multiline':False]['text':'   - E_{ij} = L_j - L_i if i != j','line_number':3678,'multiline':False]['text':'              1         otherwise','line_number':3679,'multiline':False]['text':'   - diag_embed takes a vector into a diagonal matrix','line_number':3680,'multiline':False]['text':'   - diag zeroes out elements outside of the diagonal','line_number':3681,'multiline':False]['text':' Note: the term '-V^HV diag(real(V^H gV))' comes from the fact that the','line_number':3683,'multiline':False]['text':' eigenvalue decomposition is returned with eigenvectors normalized to have','line_number':3684,'multiline':False]['text':' norm one.','line_number':3685,'multiline':False]['text':' Note: The Hermitian case is a simplification of this formula using that','line_number':3687,'multiline':False]['text':' V^{-1} = V^H and that L is real','line_number':3688,'multiline':False]['text':' This check just can be triggered in the backwards of torch.symeig','line_number':3690,'multiline':False]['text':' Trivial case','line_number':3696,'multiline':False]['text':' Shortcut for linalg.eigvals/eigvalsh','line_number':3701,'multiline':False]['text':' Compute V^-H gL V^H','line_number':3702,'multiline':False]['text':' Check invariance of the loss function wrt the transformation','line_number':3714,'multiline':False]['text':' V -> V * e^{i\phi} for an arbitrary phi in RR^n','line_number':3715,'multiline':False]['text':'rtol=','line_number':3721,'multiline':True]['text':'atol=','line_number':3722,'multiline':True]['text':' Project onto the tangent space at the identity of U(n), that is, the','line_number':3729,'multiline':False]['text':' skew-Hermitian matrices','line_number':3730,'multiline':False]['text':' Project onto the tangent space at V^H V of complex matrices with columns','line_number':3733,'multiline':False]['text':' of norm 1','line_number':3734,'multiline':False]['text':' For CompositeCompliance, if `gL` is subclass but `ret`','line_number':3749,'multiline':False]['text':' is a regular Tensor, then use out-of-place version of diagonal','line_number':3750,'multiline':False]['text':' copy aka `diagonal_scatter`.','line_number':3751,'multiline':False]['text':' Conjugate by V^{-H}','line_number':3761,'multiline':False]['text':' https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf','line_number':3775,'multiline':False]['text':' see also https://arxiv.org/pdf/1701.00392.pdf Eqs. (4.60) and (4.63)','line_number':3776,'multiline':False]['text':' Note that neither of the formulas in these pdfs are correct, as they do not','line_number':3777,'multiline':False]['text':' assume that the eigenvectors are of unit norm. As such, they are missing','line_number':3778,'multiline':False]['text':' the diagonal term in dV dL = diag(dP) dV = dX - V Re(diag V^H dX)) where dP','line_number':3779,'multiline':False]['text':' = V^{-1} dA V dX = V ((dP - diag(dP)) / E) E_{ij} = L_j - L_i if i != j','line_number':3780,'multiline':False]['text':'          1         otherwise','line_number':3781,'multiline':False]['text':' Precondition: if is_hermitian == true, then dA is Hermitian','line_number':3783,'multiline':False]['text':' Equivalent to','line_number':3854,'multiline':False]['text':' B_grad = std::get<0>(at::linalg_lstsq(A.mH(), gX, rcond, driver));','line_number':3855,'multiline':False]['text':' but we avoid this approach as `gelsy` is non-deterministic','line_number':3856,'multiline':False]['text':' dA = dQR + QdR','line_number':3868,'multiline':False]['text':'','line_number':3869,'multiline':False]['text':' Case m >= n','line_number':3870,'multiline':False]['text':' We can put dQ in terms of dR','line_number':3871,'multiline':False]['text':' dQ = dAR^{-1} - QdRR^{-1}','line_number':3872,'multiline':False]['text':' Then we have','line_number':3873,'multiline':False]['text':' Q^H dA R^{-1} = Q^HdQ + dRR^{-1}','line_number':3874,'multiline':False]['text':' where Q^HdQ is skew Hermitian and dRR^{-1} is upper triangular','line_number':3875,'multiline':False]['text':' Define sym(X) = X + X^H','line_number':3876,'multiline':False]['text':' sym(dRR^{-1}) = sym(Q^H dA R^{-1})','line_number':3877,'multiline':False]['text':' and define syminv(X) = triu(X) - 0.5 * diag(X) the inverse of','line_number':3878,'multiline':False]['text':' sym : Triu(k, diag \in \mathbb{R}) -> Her(k) to give','line_number':3879,'multiline':False]['text':' dR = syminv(sym(Q^H dA R^{-1}))R','line_number':3880,'multiline':False]['text':'','line_number':3881,'multiline':False]['text':' Case m < n','line_number':3882,'multiline':False]['text':' Put dR as a function of dQ','line_number':3883,'multiline':False]['text':' dR = Q^H dA - Q^H dQ R','line_number':3884,'multiline':False]['text':' Let X_1 be the main m x m submatrix of a matrix X \in C^{m x n}','line_number':3885,'multiline':False]['text':' Q^H A_1 R_1^{-1} = Q^H dQ + dR_1 R_1^{-1}','line_number':3886,'multiline':False]['text':' Define trilIm(X) = X.tril(-1) + i * Im diag(X)','line_number':3887,'multiline':False]['text':' trilIm(Q^H dQ) = trilIm(Q^H A_1 R_1^{-1})','line_number':3888,'multiline':False]['text':' and define trilIminv(X) = X - X^H - i*Im diag(X). This is the inverse of','line_number':3889,'multiline':False]['text':' trilIm : Skew_C(m) -> Tril(m, imaginary diag)','line_number':3890,'multiline':False]['text':' Note that it is just the inverse when the inputs are skew-Hermitian, not','line_number':3891,'multiline':False]['text':' necessarily when the inputs are arbitrary matrices. We then get dQ = Q','line_number':3892,'multiline':False]['text':' trilImInv(trilIm(Q^H A_1 R_1^{-1}))','line_number':3893,'multiline':False]['text':'upper=','line_number':3918,'multiline':True]['text':'left=','line_number':3918,'multiline':True]['text':'upper=','line_number':3947,'multiline':True]['text':'left=','line_number':3948,'multiline':True]['text':' Nb. We won't be too formal below, as writing this proof formally is a pain','line_number':3962,'multiline':False]['text':' We'll link here a formal writing of all this at some point in the future','line_number':3963,'multiline':False]['text':'','line_number':3964,'multiline':False]['text':' Case m >= n','line_number':3965,'multiline':False]['text':' dQ = dAR^{-1} - Qsyminv(sym(Q^H dA R^{-1}))','line_number':3966,'multiline':False]['text':' dR = syminv(sym(Q^H dA R^{-1}))R','line_number':3967,'multiline':False]['text':'','line_number':3968,'multiline':False]['text':' With the notation from the JVP formula, the only two computations that we','line_number':3969,'multiline':False]['text':' need are syminv*(R) = 0.5 * (R.triu() + R.triu()^H - Re diag(R)) sym*(X) =','line_number':3970,'multiline':False]['text':' 2 * X Using these, after a few simplifications we get that gA = (gQ +','line_number':3971,'multiline':False]['text':' syminvadj(triu(gR R^H - Q^H gQ)))R^{-H}','line_number':3972,'multiline':False]['text':'','line_number':3973,'multiline':False]['text':' Case m < n','line_number':3974,'multiline':False]['text':' dR = Q^H dA - Q^H dQ R','line_number':3975,'multiline':False]['text':' dQ = Q trilImInv(trilIm(Q^H A_1 R_1^{-1}))','line_number':3976,'multiline':False]['text':'','line_number':3977,'multiline':False]['text':' In this case trilIm*(X) = X (it's the trivial embedding)','line_number':3978,'multiline':False]['text':' while trilImInv*(X) = tril(Y) - 0.5 * diag(Y)','line_number':3979,'multiline':False]['text':' with Y = X - X^H','line_number':3980,'multiline':False]['text':'','line_number':3981,'multiline':False]['text':' We also have that if X \in C^{m, n} an dpi(X) = X_1,','line_number':3982,'multiline':False]['text':' projects X into its leading m x m submatrix,','line_number':3983,'multiline':False]['text':' pi*(X) = cat(X, 0_{m,n-m}, dim=-1)','line_number':3984,'multiline':False]['text':'','line_number':3985,'multiline':False]['text':' Using this, we get that','line_number':3986,'multiline':False]['text':' gA = QgR + pi*(Q trilImInv*(Q^H gQ - gR R^H)R_1^{-H})','line_number':3987,'multiline':False]['text':'upper','line_number':4031,'multiline':True]['text':'left','line_number':4031,'multiline':True]['text':'upper','line_number':4043,'multiline':True]['text':'left','line_number':4043,'multiline':True]['text':'dim=','line_number':4046,'multiline':True]['text':' Based on:','line_number':4054,'multiline':False]['text':'','line_number':4055,'multiline':False]['text':' Mathias, Roy.','line_number':4056,'multiline':False]['text':' A Chain Rule for Matrix Functions and Applications.','line_number':4057,'multiline':False]['text':' SIAM J. Matrix Anal. Appl. 17 (1996): 610-620.','line_number':4058,'multiline':False]['text':' Choose between forward (adjoint=false) or backward AD','line_number':4065,'multiline':False]['text':' (adjoint=true)','line_number':4066,'multiline':False]['text':' Given an analytic matrix function, this computes the differential (forward','line_number':4068,'multiline':False]['text':' AD) or the adjoint of the differential (backward AD)','line_number':4069,'multiline':False]['text':' For Composite Compliance, we can't copy a Subclass into a Regular Tensor,','line_number':4077,'multiline':False]['text':' so we use out-of-place ops with equivalent output.','line_number':4078,'multiline':False]['text':' NOTE: We can't use `new_zeros` directly as both `A` and `grad` can','line_number':4079,'multiline':False]['text':' be Tensor Subclass and we don't want to make assumption about which','line_number':4080,'multiline':False]['text':' one to choose for creating output buffer.','line_number':4081,'multiline':False]['text':' eg. if both are BatchedTensor at different level.','line_number':4082,'multiline':False]['text':' adjoint ','line_number':4106,'multiline':True]['text':' This function takes two functions f1 and f2 and a (variadic) list of','line_number':4116,'multiline':False]['text':' tensors, and creates a new tensor of the same shape as the first element of','line_number':4117,'multiline':False]['text':' the list of tensors by applying the function f1 to the tensors for which','line_number':4118,'multiline':False]['text':' the mask is true and f2 to the tensors for which the mask is false This','line_number':4119,'multiline':False]['text':' function is used when we have a formula that works for, say, all','line_number':4120,'multiline':False]['text':' non-singular inputs and another one for when the inputs are singular. See','line_number':4121,'multiline':False]['text':' for example det_backward','line_number':4122,'multiline':False]['text':' Precondition for the n == 0 case to make sense','line_number':4124,'multiline':False]['text':' Equivalent to','line_number':4133,'multiline':False]['text':' ret = torch.empty_like(t)','line_number':4134,'multiline':False]['text':' ret[mask] = f1(t1[mask], ..., tn[mask])','line_number':4135,'multiline':False]['text':' ret[~mask] = f2(t1[~mask], ..., tn[~mask])','line_number':4136,'multiline':False]['text':' (d det)_A(E) = tr(A^{-1}E)*det','line_number':4151,'multiline':False]['text':' We use that the determinant is C^1 to approximate the gradient of singular','line_number':4152,'multiline':False]['text':' inputs Since we never differentiate over forward AD, we don't need to deal','line_number':4153,'multiline':False]['text':' with further gradients, as we do in grad_backward','line_number':4154,'multiline':False]['text':'left=','line_number':4159,'multiline':True]['text':'adjoint=','line_number':4159,'multiline':True]['text':' A.numel() == 0 necessary for the singular case','line_number':4170,'multiline':False]['text':' The gradient G is the matrix solving','line_number':4175,'multiline':False]['text':' A.mH G = det(A).conj() * grad * I','line_number':4176,'multiline':False]['text':' Optimisation, Make it F-transposed as it's what lu_solve expects','line_number':4178,'multiline':False]['text':' Optimisation if we are not going to compute higher-order gradients','line_number':4182,'multiline':False]['text':' The formula is given by the solution of AX = det.conj() * det * I when A','line_number':4184,'multiline':False]['text':' is invertible det is C^1, so if it's not invertible, we can apply a','line_number':4185,'multiline':False]['text':' perturbation to the LU decomposition and use the resulting matrix as a','line_number':4186,'multiline':False]['text':' non-singular approximation','line_number':4187,'multiline':False]['text':'left=','line_number':4192,'multiline':True]['text':'adjoint=','line_number':4192,'multiline':True]['text':' If we want to compute higher-order gradients, we need to recompute the','line_number':4194,'multiline':False]['text':' LU decomposition so that autograd computes the correct gradients wrt','line_number':4195,'multiline':False]['text':' to A (cf. solve_backward)','line_number':4196,'multiline':False]['text':'grad','line_number':4198,'multiline':True]['text':' The derivative may be then computed explicitly by noting that the','line_number':4202,'multiline':False]['text':' gradient of the derivative of the determinant is given in terms of the','line_number':4203,'multiline':False]['text':' adjugate of a matrix. The adjugate of a singular matrix may be computed','line_number':4204,'multiline':False]['text':' as per https://nhigham.com/2020/06/16/what-is-the-adjugate-of-a-matrix/','line_number':4205,'multiline':False]['text':'d','line_number':4207,'multiline':True]['text':' We could use the singular formula for all inputs but we try to filter out','line_number':4215,'multiline':False]['text':' some inputs via the masking, as computing an SVD is about 100 times','line_number':4216,'multiline':False]['text':' slower than computing an lu_solve on GPU','line_number':4217,'multiline':False]['text':' For tensor subclasses, we can't call masked_fmap as it calls','line_number':4218,'multiline':False]['text':' index({mask}) which needs to call item to compute the number of elements','line_number':4219,'multiline':False]['text':' in the result.','line_number':4220,'multiline':False]['text':' No need to handle the singular case separately as we do in det since','line_number':4237,'multiline':False]['text':' this function is not differentiable on singular matrices','line_number':4238,'multiline':False]['text':'left','line_number':4239,'multiline':True]['text':' We compute the complex case, as the real case follows from it','line_number':4258,'multiline':False]['text':' Forward AD','line_number':4259,'multiline':False]['text':' d (logabsdet)_A(E) = Re(tr(A^{-1}E))','line_number':4260,'multiline':False]['text':' d (signdet)_A(E) = sgn * Im(tr(A^{-1}E)) * i','line_number':4261,'multiline':False]['text':' So','line_number':4262,'multiline':False]['text':' d (logabsdet)*_A(g) = gA^{-H}','line_number':4263,'multiline':False]['text':' Now, to compute the adjoint of d(signdet), note that','line_number':4264,'multiline':False]['text':' Re(z * Im(w)) = Re(-Re(z)iw)','line_number':4265,'multiline':False]['text':' So, let g \in C,','line_number':4266,'multiline':False]['text':' <g, d(signdet)_A(E)> = Re(g.conj() * sgn * i * Im(A^{-1}E))','line_number':4267,'multiline':False]['text':'                      = Re(Re(g.conj() * sgn * i) * -i * A^{-1}E)','line_number':4268,'multiline':False]['text':'                      = Re(Im(g.conj() * sgn) * i * A^{-1}E)','line_number':4269,'multiline':False]['text':'                      = <Im(g.conj() * sgn) * -i * A^{-H}, E>','line_number':4270,'multiline':False]['text':' As such,','line_number':4271,'multiline':False]['text':' (d slogabs)*_A(g_sign, g_abs) = (g_abs - g_sign.conj() * sgn) * A^{-H}','line_number':4272,'multiline':False]['text':' In the real case grad_sign is always zero','line_number':4280,'multiline':False]['text':' Cast to complex explicitly','line_number':4295,'multiline':False]['text':' No need to handle the singular case separately here (as we do in det)','line_number':4300,'multiline':False]['text':' since this function is not differentiable on singular matrices','line_number':4301,'multiline':False]['text':' Optimisation, Make it F-transposed as it's what lu_solve expects','line_number':4302,'multiline':False]['text':'left=','line_number':4307,'multiline':True]['text':'adjoint=','line_number':4307,'multiline':True]['text':' If we want to compute further gradients, we need to recompute the LU','line_number':4309,'multiline':False]['text':' decomposition so that autograd computes the correct gradients wrt to A','line_number':4310,'multiline':False]['text':' (cf. solve_backward)','line_number':4311,'multiline':False]['text':' Reference:','line_number':4316,'multiline':False]['text':' https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf','line_number':4317,'multiline':False]['text':' Sec. 2.3.1 Matrix inverse product','line_number':4318,'multiline':False]['text':' The forward AD formula (for left = true) is A^{-1}(B_t - A_tX)','line_number':4386,'multiline':False]['text':' For the derivation see:','line_number':4387,'multiline':False]['text':' [Note: Forward / Backward AD solve_triangular]','line_number':4388,'multiline':False]['text':' [Note: Forward / Backward AD solve_triangular]','line_number':4407,'multiline':False]['text':' Assume left=true for simplicity.','line_number':4408,'multiline':False]['text':' Remark: A solver computes A^{-1}B','line_number':4409,'multiline':False]['text':'','line_number':4410,'multiline':False]['text':' Forward AD:','line_number':4411,'multiline':False]['text':' If f(A) = A^{-1}, differentiating the equation A^{-1}A = I_n gives','line_number':4412,'multiline':False]['text':' (df)_A(E) = -A^{-1}EA^{-1}','line_number':4413,'multiline':False]['text':' As such, if g(A,B) = A^{-1}B,','line_number':4414,'multiline':False]['text':' (dg)_(A,B)(E_A, E_B) = -A^{-1}E_AA^{-1}B + A^{-1}E_B','line_number':4415,'multiline':False]['text':'                      = A^{-1}(E_B - E_AX)','line_number':4416,'multiline':False]['text':' Backward AD:','line_number':4418,'multiline':False]['text':' Denoting the gradients by G_A, G_B, we solve above to give','line_number':4419,'multiline':False]['text':' G_B = A^{-H}G_X','line_number':4420,'multiline':False]['text':' G_A = -A^{-H}G_XX^H = -G_B X^H','line_number':4421,'multiline':False]['text':'','line_number':4422,'multiline':False]['text':' Note that you don't need to store B for forward nor backward','line_number':4423,'multiline':False]['text':'','line_number':4424,'multiline':False]['text':' These formulas work for a general solver of linear equations.','line_number':4425,'multiline':False]['text':' Let's prove now that when A is triangular, G_A is the projection onto the','line_number':4426,'multiline':False]['text':' triangular matrices of the formula above, i.e. simply taking triu (resp.','line_number':4427,'multiline':False]['text':' tril) in the formula above. This is because, since the triangular matrices','line_number':4428,'multiline':False]['text':' form a vector space, the tangent space at any point is itself the space of','line_number':4429,'multiline':False]['text':' triangular matrices. The result follows from a reasoning as that at the end','line_number':4430,'multiline':False]['text':' of [Note: eigh backward] Something similar happens for `unitriangular`,','line_number':4431,'multiline':False]['text':' only that int his case the tangent space is the set of lower-triangular','line_number':4432,'multiline':False]['text':' matrices with zeros on the diagonal.','line_number':4433,'multiline':False]['text':' We always need to comput G_B','line_number':4438,'multiline':False]['text':'upper=','line_number':4464,'multiline':True]['text':'A=','line_number':4494,'multiline':True]['text':' Forward is C2R (onesided)','line_number':4503,'multiline':False]['text':' Think of onesided C2R irfft as','line_number':4504,'multiline':False]['text':'    1. fill the other half by conjugate symmetry','line_number':4505,'multiline':False]['text':'    2. inverse C2C ifft','line_number':4506,'multiline':False]['text':'    3. discard the complex dimension','line_number':4507,'multiline':False]['text':' So backward is','line_number':4508,'multiline':False]['text':'    1. R2C rfft (essentially add dummy complex dimension, and dft)','line_number':4509,'multiline':False]['text':'    2. accumulate gradient by conjugate symmetry','line_number':4510,'multiline':False]['text':'       since rfft results follow conjugate symmetry, we only need to','line_number':4511,'multiline':False]['text':'       double some entries from onesided rfft results, i.e., the ones with','line_number':4512,'multiline':False]['text':'       their reflected indices also landing out of the onesided range. So','line_number':4513,'multiline':False]['text':'       consider the index of last dim:','line_number':4514,'multiline':False]['text':'           i.   idx = 0.','line_number':4515,'multiline':False]['text':'                Reflected to (N - 0) % N = 0. Not doubled.','line_number':4516,'multiline':False]['text':'           ii   0 < idx < floor(N/2) (last).','line_number':4517,'multiline':False]['text':'                N > N - idx > ceil(N/2)','line_number':4518,'multiline':False]['text':'                Reflected to ()','line_number':4519,'multiline':False]['text':'           iii. idx = floor(N/2) = N/2 (last) when N even.','line_number':4520,'multiline':False]['text':'                Reflected to (N - N/2) % N = N/2. Not doubled.','line_number':4521,'multiline':False]['text':'           iv.  idx = floor(N/2) = (N-1)/2 (last) when N odd.','line_number':4522,'multiline':False]['text':'                Reflected to (N - (N-1)/2) % N = (N+1)/2. Doubled.','line_number':4523,'multiline':False]['text':'       Therefore, needs to double','line_number':4524,'multiline':False]['text':'           idx = 1, 2, ..., N/2 - 1     when N even','line_number':4525,'multiline':False]['text':'           idx = 1, 2, ..., (N-1)/2     when N odd','line_number':4526,'multiline':False]['text':'       that is','line_number':4527,'multiline':False]['text':'           idx = 1, 2, ..., N - (floor(N/2) + 1)','line_number':4528,'multiline':False]['text':'               = 1, 2, ..., N - onesided_length','line_number':4529,'multiline':False]['text':'onesided=','line_number':4530,'multiline':True]['text':' also covers case when signal size is zero','line_number':4533,'multiline':False]['text':'forward=','line_number':4546,'multiline':True]['text':' Forward is R2C (onesided)','line_number':4549,'multiline':False]['text':' Think of onesided R2C rfft as','line_number':4550,'multiline':False]['text':'     1. view as complex numbers (fill complex dim with zeros)','line_number':4551,'multiline':False]['text':'     2. C2C fft','line_number':4552,'multiline':False]['text':'     3. discard half of results','line_number':4553,'multiline':False]['text':' So backward is','line_number':4554,'multiline':False]['text':'     1. fill the other half with zeros (with `zero_grad_shape` below)','line_number':4555,'multiline':False]['text':'        (C2C ifft only take twosided inputs so we need to fill here)','line_number':4556,'multiline':False]['text':'     2. inverse C2C ifft','line_number':4557,'multiline':False]['text':'     3. discard the complex dim','line_number':4558,'multiline':False]['text':'forward=','line_number':4573,'multiline':True]['text':' Helper for batchnorm_double_backward','line_number':4576,'multiline':False]['text':' Helper for batchnorm_double_backward','line_number':4586,'multiline':False]['text':' similar to expand_as below, but doesn't do the expand_as; operates as if','line_number':4587,'multiline':False]['text':' reductions were done with keepdim=True','line_number':4588,'multiline':False]['text':' Helper for batchnorm_double_backward','line_number':4600,'multiline':False]['text':' because gamma/ggG/ggB are 1-dimensional and represent dim==1, we can't','line_number':4601,'multiline':False]['text':' do a straight expansion because it won't follow the broadcasting rules.','line_number':4602,'multiline':False]['text':' TODO: Do we have a ScalarOrTensor type?  Would such a thing exist?','line_number':4626,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4630,'multiline':False]['text':' define some terms we will reuse','line_number':4642,'multiline':False]['text':' for half inputs, save_mean, save_invstd are float (ideally, we would cast','line_number':4647,'multiline':False]['text':' everything else, but not now)','line_number':4648,'multiline':False]['text':' calculate gI','line_number':4661,'multiline':False]['text':' add contribution of gamma term to gI','line_number':4682,'multiline':False]['text':' this is the first backward's grad_input','line_number':4698,'multiline':False]['text':' calculate gG','line_number':4710,'multiline':False]['text':' gG is just the first backwards with the gamma term removed (then shaped','line_number':4714,'multiline':False]['text':' properly)','line_number':4715,'multiline':False]['text':' calculate ggO','line_number':4724,'multiline':False]['text':' contribution of input term','line_number':4726,'multiline':False]['text':' printf("M: %ld, N: %ld", M, N);','line_number':4769,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4780,'multiline':False]['text':' for half inputs, save_mean, save_invstd are float','line_number':4797,'multiline':False]['text':' (ideally, we would cast everything else, but not now)','line_number':4798,'multiline':False]['text':' calculate gI','line_number':4806,'multiline':False]['text':' add contribution of gamma term to gI','line_number':4830,'multiline':False]['text':' printf("=== computing gI\n");','line_number':4842,'multiline':False]['text':' this is the grad_input for the first backward function','line_number':4846,'multiline':False]['text':' calculate gG','line_number':4858,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4864,'multiline':False]['text':' calculate ggO','line_number':4868,'multiline':False]['text':' contribution of input term','line_number':4870,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4928,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4995,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':4999,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':5003,'multiline':False]['text':' We must conditionally initialize this using to_dense if sparse, sparse','line_number':5010,'multiline':False]['text':' addition is not supported without exact shape match','line_number':5011,'multiline':False]['text':' The warning only applies to the sparsity of self, dense grad is never','line_number':5016,'multiline':False]['text':' materialized so if self is strided and grad is sparse nothing unexpected','line_number':5017,'multiline':False]['text':' happens memory wise','line_number':5018,'multiline':False]['text':' Although calling self.to_dense() would just return self when it has','line_number':5023,'multiline':False]['text':' strided layout, that would breaks functorch tests.','line_number':5024,'multiline':False]['text':' If grad is sparse we can't divide by the n-d (self + 1).conj(), so we','line_number':5030,'multiline':False]['text':' must multiply by the recipricol, layout of grad is preserved which is','line_number':5031,'multiline':False]['text':' important to gradcheck','line_number':5032,'multiline':False]['text':' Because the backward of pad(input, pads) is just pad(grad_output, [-p for p','line_number':5046,'multiline':False]['text':' in pads])','line_number':5047,'multiline':False]['text':' NOLINTNEXTLINE(modernize-use-transparent-functors)','line_number':5054,'multiline':False]['text':' since first backward takes care of scaling by frequency,','line_number':5063,'multiline':False]['text':' we don't need to worry about it here.','line_number':5064,'multiline':False]['text':' reshape gradient as per the shape of indices','line_number':5067,'multiline':False]['text':' Derivations for the householder_product.backward method.','line_number':5111,'multiline':False]['text':'','line_number':5112,'multiline':False]['text':' Given a sequence of vectors v_1, ..., v_n and a sequence of scalars tau_1,','line_number':5113,'multiline':False]['text':' ..., tau_k, the torch.linalg.householder_product computes the firt n columns','line_number':5114,'multiline':False]['text':' of the following product: Q = (I - tau_1 v_1 v_1^H) ... (I - tau_k v_k','line_number':5115,'multiline':False]['text':' v_k^H). Let','line_number':5116,'multiline':False]['text':'     H_i(sigma) := I - sigma v_i v_i^H, so Q = (H_1(sigma_1) ...','line_number':5117,'multiline':False]['text':'     H_k(sigma_k)[:, :k]; H_i_minus = H_1(tau_1) ... H_{i - 1}(tau_{i - 1}),','line_number':5118,'multiline':False]['text':'     with H_1_minus := I; H_i_plus = H_{i + 1}(tau_{i + 1}) ... H_k(tau_k)','line_number':5119,'multiline':False]['text':'     with H_k_plus := I;','line_number':5120,'multiline':False]['text':'','line_number':5121,'multiline':False]['text':' Forward AD:','line_number':5122,'multiline':False]['text':' dQ = sum_{i = 1}^k H_i_minus (-dtau_i v_i v_i^H - tau_i dv_i v_i^H - tau_i','line_number':5123,'multiline':False]['text':' v_i dv_i^H) H_i_plus.','line_number':5124,'multiline':False]['text':'','line_number':5125,'multiline':False]['text':' Backward AD:','line_number':5126,'multiline':False]['text':' Tr(Q_grad^H dQ) = sum_{i = 1}^k Tr(H_i_plus Q_grad^H H_i_minus (-dtau_i v_i','line_number':5127,'multiline':False]['text':' v_i^H - tau_i dv_i v_i^H - tau_i v_i dv_i^H)). Let K_i := H_i_plus Q_grad^H','line_number':5128,'multiline':False]['text':' H_i_minus, then the gradients are v_i_grad = (-tau_i v_i^H K_i)^H - tau_i K_i','line_number':5129,'multiline':False]['text':' v_i, tau_i_grad = Tr(-v_i^H K_i v_i).conj(). NOTE: the algorithms ignores','line_number':5130,'multiline':False]['text':' that only n columns of Q are observed, so there is no need in recomputing Q','line_number':5131,'multiline':False]['text':' to full completion.','line_number':5132,'multiline':False]['text':'','line_number':5133,'multiline':False]['text':' Note that K_{i + 1} = H_{i + 1}^{-1} K_i H_i, so we can compute v_i_grad,','line_number':5134,'multiline':False]['text':' tau_i_grad one by one by just efficiently updating K_i if that is possible.','line_number':5135,'multiline':False]['text':' Multiplying with H_i from the right could be done with matrix-vector','line_number':5136,'multiline':False]['text':' products, but what about the inverse H_{i + 1}^{-1} and does it even exist?','line_number':5137,'multiline':False]['text':' Luckily, under some assumptions, H_{i + 1}^{-1} exists and admits a','line_number':5138,'multiline':False]['text':' representation as H_i(sigma_i) for some sigma_i, so the left update is also','line_number':5139,'multiline':False]['text':' could be done with matrix-vector and not matrix-matrix products.','line_number':5140,'multiline':False]['text':'','line_number':5141,'multiline':False]['text':' Let H(tau) := I - tau v v^H.','line_number':5142,'multiline':False]['text':' H(tau) has eigenvalues 1 with multiplicity (m - 1) with eigenvectors','line_number':5143,'multiline':False]['text':' orthogonal to v, and an eigenvalue (1 - tau ||v||^2) with the corresponding','line_number':5144,'multiline':False]['text':' eigenvector v / ||v||. If (1 - tau ||v||^2) != 0, H(tau) is invertible. If (1','line_number':5145,'multiline':False]['text':' - tau ||v||^2) != 0, then with sigma defined as sigma := tau / (||v||^2 tau -','line_number':5146,'multiline':False]['text':' 1) we get that H(tau) H(sigma) = H(sigma) H(tau) = I, so H(sigma) is the','line_number':5147,'multiline':False]['text':' inverse of H(tau).','line_number':5148,'multiline':False]['text':'','line_number':5149,'multiline':False]['text':' WARNING: the algorithm below assumes that H_i(tau_i) are all invertible, so','line_number':5150,'multiline':False]['text':' it expects that (1 - tau_i ||v_i||^2) != 0 for all i.','line_number':5151,'multiline':False]['text':' We would like to point out that if there is H_i(tau_i) which is not','line_number':5152,'multiline':False]['text':' invertible, the householder_product is still differentiable! We will not be','line_number':5153,'multiline':False]['text':' able to compute K_i efficiently in such cases, however, as evaluating of each','line_number':5154,'multiline':False]['text':' K_i will amount to calls to ORGQR to be able to compute H_i_plus.','line_number':5155,'multiline':False]['text':' This function computes either the product between','line_number':5157,'multiline':False]['text':' (I - tau u v^H) and K (in-place or not) with `condition_with_I = true`, or','line_number':5158,'multiline':False]['text':' between','line_number':5159,'multiline':False]['text':' (-tau u v^H) and K (out-of-place only) with `condition_with_I = false`.','line_number':5160,'multiline':False]['text':' Parameter `left` controls whether the matrix K is multiplied from the left or','line_number':5161,'multiline':False]['text':' from the right.','line_number':5162,'multiline':False]['text':' Additionally, when the computation is done in-place, we exploit that the','line_number':5163,'multiline':False]['text':' first `k` coordinates of `u_full/v_full` are zeros.','line_number':5164,'multiline':False]['text':' we assume u_full is a vector of dimension (..., m, 1), t is a scalar of','line_number':5175,'multiline':False]['text':' dimension (..., 1)','line_number':5176,'multiline':False]['text':' TODO: matrix-vector products in the code below are dispatched to','line_number':5178,'multiline':False]['text':' matrix-matrix products. We either need to extend matmul to support batched','line_number':5179,'multiline':False]['text':' matrix-vector products, or implement a batched variant of mv. We could','line_number':5180,'multiline':False]['text':' enable mv for inputs which are not batched, but it is not done to eliminate','line_number':5181,'multiline':False]['text':' the code duplication.','line_number':5182,'multiline':False]['text':' returns (I - t u v^H) K or -t u v^H K','line_number':5184,'multiline':False]['text':' returns K (I - t u v^H) or -K t u v^H','line_number':5198,'multiline':False]['text':' NOTE on `flip_order`: when flip_order is true,','line_number':5220,'multiline':False]['text':' the algorithm below reverses the processing direction from','line_number':5221,'multiline':False]['text':' range(k) to range(k - 1, -1, -1) in the main loop, and left/right','line_number':5222,'multiline':False]['text':' Householder projection applications get flipped.','line_number':5223,'multiline':False]['text':' The comments below about the algorithmic details assume flip_order = false.','line_number':5224,'multiline':False]['text':' guard_int is due to irange calls below','line_number':5229,'multiline':False]['text':' forward operates only over the lower triangular part with the assumption','line_number':5232,'multiline':False]['text':' that the diagonal of input is filled with 1s.','line_number':5233,'multiline':False]['text':' compute sigma such that','line_number':5237,'multiline':False]['text':' H(sigma_i) == H(tau_i)^{-1}.','line_number':5238,'multiline':False]['text':' If the input to householder_product comes from GEQRF,','line_number':5239,'multiline':False]['text':' we will never encounter ||v_i||^2 tau_i == 1, so H(tau_i) will always be','line_number':5240,'multiline':False]['text':' invertible. This follows from the documentation','line_number':5241,'multiline':False]['text':' https://www.netlib.org/lapack/lug/node128.html, and tau always satisfying','line_number':5242,'multiline':False]['text':' the condition |tau|^2 ||v||^2 == 2 * Re(tau).','line_number':5243,'multiline':False]['text':' The algorithm updates K by multiplying it from the left/right with','line_number':5251,'multiline':False]['text':' Householder reflectors. If only single backward is run, we modify K','line_number':5252,'multiline':False]['text':' in-place and exploit triangularity of the input. With higher order','line_number':5253,'multiline':False]['text':' derivatives we cannot rewrite the storage of K, hence we use much less','line_number':5254,'multiline':False]['text':' efficient out-of-place methods.','line_number':5255,'multiline':False]['text':'','line_number':5256,'multiline':False]['text':' if only first-order derivative is expected, we can modify K in-place for','line_number':5257,'multiline':False]['text':' better performance','line_number':5258,'multiline':False]['text':' This method exploits that at k-th iteration vector v_k has only elements','line_number':5261,'multiline':False]['text':' v_k[k:] which are non-zero.','line_number':5262,'multiline':False]['text':' v_full is a vector of dimension (..., m, 1), t is a scalar of dimension','line_number':5268,'multiline':False]['text':' (..., 1)','line_number':5269,'multiline':False]['text':'condition_with_I=','line_number':5294,'multiline':True]['text':' K <- H_0^{-1} @ K','line_number':5306,'multiline':False]['text':'left=','line_number':5313,'multiline':True]['text':' For Composite Compliance, we can't copy a Subclass into a Regular Tensor,','line_number':5316,'multiline':False]['text':' so we use out-of-place ops with equivalent output.','line_number':5317,'multiline':False]['text':' NOTE: We can't use `new_zeros` directly as `input`, 'tau' or `grad` can','line_number':5318,'multiline':False]['text':' be Tensor Subclass and we don't want to make assumption about which','line_number':5319,'multiline':False]['text':' one to choose for creating output buffer.','line_number':5320,'multiline':False]['text':' eg. if both are BatchedTensor at different level.','line_number':5321,'multiline':False]['text':' k + 1 if input_grads hold a matrix of zeros for inactive parts of input.','line_number':5323,'multiline':False]['text':' NOTE: narrow will unsqueeze(-1)','line_number':5329,'multiline':False]['text':' K <- H_{i + 1}^{-1} @ K @ H_i','line_number':5335,'multiline':False]['text':'left=','line_number':5341,'multiline':True]['text':'left=','line_number':5342,'multiline':True]['text':' Only first k columns are active in forward.','line_number':5346,'multiline':False]['text':' zero gradients for the inactive input.','line_number':5347,'multiline':False]['text':' NOTE: narrow will unsqueeze(-1)','line_number':5363,'multiline':False]['text':' K <- H_{i + 1}^{-1} @ K @ H_i','line_number':5371,'multiline':False]['text':'left=','line_number':5377,'multiline':True]['text':'left=','line_number':5378,'multiline':True]['text':' forward operates only over the lower-triangular part of the input','line_number':5383,'multiline':False]['text':' excluding the main diagonal, hence the gradient is also lower-triangular.','line_number':5384,'multiline':False]['text':' We refer to the derivations described above the method','line_number':5390,'multiline':False]['text':' `apply_simple_transformation`','line_number':5391,'multiline':False]['text':' forward operates only over the lower triangular part with the assumption','line_number':5401,'multiline':False]['text':' that the diagonal of input is filled with 1s.','line_number':5402,'multiline':False]['text':' compute sigma such that','line_number':5407,'multiline':False]['text':' H(sigma_i) == H(tau_i)^{-1}.','line_number':5408,'multiline':False]['text':' If the input to householder_product comes from GEQRF,','line_number':5409,'multiline':False]['text':' we will never encounter ||v_i||^2 tau_i == 1, so H(tau_i) will always be','line_number':5410,'multiline':False]['text':' invertible. This follows from the documentation','line_number':5411,'multiline':False]['text':' https://www.netlib.org/lapack/lug/node128.html, and tau always satisfying','line_number':5412,'multiline':False]['text':' the condition |tau|^2 ||v||^2 == 2 * Re(tau).','line_number':5413,'multiline':False]['text':' setting `modify_K_in_place = true` causes CUDA memory leaks in OpInfo','line_number':5424,'multiline':False]['text':' tests of forward AD for that reason we ignore `k` by passing zero','line_number':5425,'multiline':False]['text':'k=','line_number':5427,'multiline':True]['text':'modify_K_in_place=','line_number':5432,'multiline':True]['text':'condition_with_I=','line_number':5433,'multiline':True]['text':' computes (-t u v^H) K','line_number':5437,'multiline':False]['text':' since ``modify_K_in_place = false`, we can ignore `k` and pass','line_number':5443,'multiline':False]['text':' arbitrary value','line_number':5444,'multiline':False]['text':'k=','line_number':5446,'multiline':True]['text':'modify_K_in_place=','line_number':5451,'multiline':True]['text':'condition_with_I=','line_number':5452,'multiline':True]['text':'left=','line_number':5453,'multiline':True]['text':'left=','line_number':5469,'multiline':True]['text':' `H_minus_dH_i_H_plus` = H_1 * ... * H_{i-1} dH_i * H_{i+1} * ...','line_number':5471,'multiline':False]['text':' For Composite Compliance, if `intermediate` is a Tensor-Subclass,','line_number':5476,'multiline':False]['text':' we use out-of-place variant of add.','line_number':5477,'multiline':False]['text':'left=','line_number':5484,'multiline':True]['text':' Assume left = true and transpose = false. The case with','line_number':5514,'multiline':False]['text':' left = false and transpose = true is very much similar with just','line_number':5515,'multiline':False]['text':' transposed arguments passed into householder_product_backward.','line_number':5516,'multiline':False]['text':' Ormqr computes B = H_1 * ... * H_k * A.','line_number':5517,'multiline':False]['text':' The sensivity wrt H_i is given by (see notes in','line_number':5518,'multiline':False]['text':' householder_product_backward) Tr(H_i_plus B B_grad^H H_i_minus dH_i),','line_number':5519,'multiline':False]['text':' so, since householder_product_backward respects `for i in range(k)`, we','line_number':5520,'multiline':False]['text':' could reuse householder_product_backward with','line_number':5521,'multiline':False]['text':' householder_product_backward.grad = grad and','line_number':5522,'multiline':False]['text':' householder_product_backward.result = result.','line_number':5523,'multiline':False]['text':' Assuming left = false and transpose = false. The case with','line_number':5529,'multiline':False]['text':' left = true and transpose = true is very much similar with just','line_number':5530,'multiline':False]['text':' transposed arguments passed into householder_product_backward.','line_number':5531,'multiline':False]['text':' In this case Ormqr computes B = H_1 * ... * H_k * A and the sensitivity','line_number':5532,'multiline':False]['text':' wrt H_i becomes Tr(H_i_plus B_grad^H B H_i_minus dH_k).','line_number':5533,'multiline':False]['text':' We could see that the role of `grad` and `result` in','line_number':5534,'multiline':False]['text':' householder_product_backward gets "swapped" and "transposed" and that','line_number':5535,'multiline':False]['text':' in order to compute H_k_grad efficiently we would need to compute grads','line_number':5536,'multiline':False]['text':' in reversed order (`for i in range(k - 1, -1, -1)`). Hence we reuse','line_number':5537,'multiline':False]['text':' householder_product_backward with householder_product_backward.grad =','line_number':5538,'multiline':False]['text':' result.mH, householder_product_backward.result = grad.mH,','line_number':5539,'multiline':False]['text':' householder_product_backward.flip_order = true.','line_number':5540,'multiline':False]['text':'flip_order=','line_number':5544,'multiline':True]['text':' For x = 0, the correct gradient is 0.5,','line_number':5569,'multiline':False]['text':' however due to floating point computation we get NaN.','line_number':5570,'multiline':False]['text':' So we manually update gradient for x=0','line_number':5571,'multiline':False]['text':' Following `where` is needed as `where` computes gradients,','line_number':5575,'multiline':False]['text':' even for the part which didn't affect the output.','line_number':5576,'multiline':False]['text':' Look at https://github.com/pytorch/pytorch/issues/52248','line_number':5577,'multiline':False]['text':' Update if and when this is fixed.','line_number':5578,'multiline':False]['text':' For x = 0, the correct gradient is 0.5,','line_number':5592,'multiline':False]['text':' however due to floating point computation we get NaN.','line_number':5593,'multiline':False]['text':' So we manually update gradient for x=0','line_number':5594,'multiline':False]['text':' Following `where` is needed as `where` computes gradients,','line_number':5598,'multiline':False]['text':' even for the part which didn't affect the output.','line_number':5599,'multiline':False]['text':' Look at https://github.com/pytorch/pytorch/issues/52248','line_number':5600,'multiline':False]['text':' Update if and when this is fixed.','line_number':5601,'multiline':False]['text':' lu_solve is a map (LU, P, B) -> (PLU)^{-1} B,','line_number':5612,'multiline':False]['text':' where LU = L + U - I and P is a permutation matrix, and is fixed.','line_number':5613,'multiline':False]['text':'','line_number':5614,'multiline':False]['text':' Let 1 = ones_like(LU),','line_number':5615,'multiline':False]['text':' 1_U = 1.triu(),','line_number':5616,'multiline':False]['text':' 1_L = 1.tril(-1)','line_number':5617,'multiline':False]['text':' * := the Hadamard (element-wise) product','line_number':5618,'multiline':False]['text':'','line_number':5619,'multiline':False]['text':' Forward AD:','line_number':5620,'multiline':False]['text':'','line_number':5621,'multiline':False]['text':' Let X := U^{-1} L^{-1} P^T B be the output of the function.','line_number':5622,'multiline':False]['text':' Also, the LU input of the function could be represented as','line_number':5623,'multiline':False]['text':' LU = (L - I) + U.','line_number':5624,'multiline':False]['text':'','line_number':5625,'multiline':False]['text':' Differentiating LU = L + U - I produces:','line_number':5626,'multiline':False]['text':' dLU = dL + dU.','line_number':5627,'multiline':False]['text':' Noting that dL and dU are lower- and upper-triangular, respectively,','line_number':5628,'multiline':False]['text':' and that the diagonal of L is never explicitly exposed, so','line_number':5629,'multiline':False]['text':' diag(dL) = 0, it follows','line_number':5630,'multiline':False]['text':' dL = dLU * 1_L,','line_number':5631,'multiline':False]['text':' dU = dLU * 1_U.','line_number':5632,'multiline':False]['text':'','line_number':5633,'multiline':False]['text':' Differentiating X = U^{-1} L^{-1} P^T B produces:','line_number':5634,'multiline':False]['text':' dX = dU^{-1} L^{-1} P^T B + U^{-1} dL^{-1} P^T B + U^{-1} L^{-1} P^T dB','line_number':5635,'multiline':False]['text':' Note that for any invertible matrix A we have A A^{-1} = I, hence','line_number':5636,'multiline':False]['text':' dA A^{-1} + A dA^{-1} = 0 => dA^{-1} = -A^{-1} dA A^{-1}.','line_number':5637,'multiline':False]['text':' Inserting it back into the definition of dX gives:','line_number':5638,'multiline':False]['text':' dX = -U^{-1} dU U^{-1} L^{-1} P^T B - U^{-1} L^{-1} dL L^{-1} P^T B + U^{-1}','line_number':5639,'multiline':False]['text':' L^{-1} P^T dB dX = -U^{-1} dU X - U^{-1} L^{-1} dL U X + U^{-1} L^{-1} P^T dB','line_number':5640,'multiline':False]['text':'','line_number':5641,'multiline':False]['text':' Backward AD:','line_number':5642,'multiline':False]['text':'','line_number':5643,'multiline':False]['text':' Using the definition of dL, dU from above:','line_number':5644,'multiline':False]['text':' Tr(L_grad^H dL) + Tr(U_grad^H dU) = Tr(L_grad^H (dLU * 1_L)) + Tr(U_grad^H','line_number':5645,'multiline':False]['text':' (dLU * 1_U))','line_number':5646,'multiline':False]['text':'                                   = [using Tr(A (B * C)) = Tr((A * B^T) C)','line_number':5647,'multiline':False]['text':'                                   = Tr((L_grad^H * 1_L^T) dLU) + Tr((U_grad^H','line_number':5648,'multiline':False]['text':'                                   * 1_U^T) dLU),','line_number':5649,'multiline':False]['text':' hence','line_number':5650,'multiline':False]['text':' LU_grad = L_grad * 1_L + U_grad * 1_U (!!!)','line_number':5651,'multiline':False]['text':'','line_number':5652,'multiline':False]['text':' Then, transposing the formula for dX above we get:','line_number':5653,'multiline':False]['text':' B_grad = P L^{-H} U^{-H} X_grad = lu_solve(X_grad, LU_data, LU_pivots,','line_number':5654,'multiline':False]['text':' /*adjoint=*/true) U_grad = -U^{-H} X_grad X^H L_grad = L^{-H} U_grad U^H','line_number':5655,'multiline':False]['text':' After inserting U_grad and L_grad into (!!!) we get the value for LU_grad.','line_number':5656,'multiline':False]['text':' From linalg_lu_solve_jvp we have that:','line_number':5665,'multiline':False]['text':' left = True, adjoint = True: A^HX = B','line_number':5666,'multiline':False]['text':' left = True, adjoint = False: AX = B','line_number':5667,'multiline':False]['text':' left = False, adjoint = True: AX^H = B^H','line_number':5668,'multiline':False]['text':' left = False, adjoint = False: A^HX^H = B^H','line_number':5669,'multiline':False]['text':' let op_1(A) = A^H or op_1(A) = A according to the list above','line_number':5670,'multiline':False]['text':' same with op_2(X) and op_3(B)','line_number':5671,'multiline':False]['text':' We have that letting S = lu_solve(LU, pivots, dB, left, adjoint)','line_number':5672,'multiline':False]['text':' the JVP formula reads','line_number':5673,'multiline':False]['text':' if left != adjoint:','line_number':5674,'multiline':False]['text':'   dX = op_2(-U^{-1}(dU + L^{-1}dL U)op_2(X)) + S','line_number':5675,'multiline':False]['text':' else:','line_number':5676,'multiline':False]['text':'   dX = op_2(op_1(-op_3(X)^H P(LdUU^{-1} + dL)L^{-1} P^T)) + S','line_number':5677,'multiline':False]['text':' So computing the adjoint of this operation we get that, using an auxiliary','line_number':5678,'multiline':False]['text':' variable gR if left != adjoint:','line_number':5679,'multiline':False]['text':'   gR = U^{-H}op_2(-gX)op_2(X)^H','line_number':5680,'multiline':False]['text':'   gU = gR.triu()','line_number':5681,'multiline':False]['text':'   gL = (L^{-H} gR U^H).tril(-1)','line_number':5682,'multiline':False]['text':' else:','line_number':5683,'multiline':False]['text':'   gR = -P^T op_3(X)op_1(op_2(gX))PL^{-H}','line_number':5684,'multiline':False]['text':'   gL = gR.tril(-1)','line_number':5685,'multiline':False]['text':'   gU = (L^H gR U^{-H}).triu()','line_number':5686,'multiline':False]['text':' gLU = gL + gU','line_number':5687,'multiline':False]['text':'unpack_data=','line_number':5691,'multiline':True]['text':'unpack_pivots=','line_number':5691,'multiline':True]['text':' TODO Optimise the order of the operations to avoid operating on large','line_number':5692,'multiline':False]['text':' tensors unnecessarily','line_number':5693,'multiline':False]['text':'      The logic should be: if n < k == left then multiply the gX and X first','line_number':5694,'multiline':False]['text':'      (as it's done now) Otherwise multiply them last','line_number':5695,'multiline':False]['text':' gR = U^{-H}op_2(-gX)op_2(X)^H','line_number':5697,'multiline':False]['text':'upper','line_number':5701,'multiline':True]['text':' gL = (L^{-H} gR U^H).tril(-1)','line_number':5702,'multiline':False]['text':'upper','line_number':5706,'multiline':True]['text':'left','line_number':5707,'multiline':True]['text':'unitriangular','line_number':5708,'multiline':True]['text':' gR = -P^T op_3(X)op_1(op_2(gX))P','line_number':5713,'multiline':False]['text':' gR = gR L^{-H}','line_number':5716,'multiline':False]['text':'upper','line_number':5718,'multiline':True]['text':'left','line_number':5718,'multiline':True]['text':'unitriangular','line_number':5718,'multiline':True]['text':' gU = (L^H gR U^{-H}).triu()','line_number':5719,'multiline':False]['text':'upper','line_number':5721,'multiline':True]['text':'left','line_number':5721,'multiline':True]['text':' We write the derivation in terms of some adjoint operations, as otherwise','line_number':5735,'multiline':False]['text':' we would need to write down 4 different proofs with 4 different','line_number':5736,'multiline':False]['text':' implementations and it'd be painful to derive and maintain Below, we just','line_number':5737,'multiline':False]['text':' use that X -> X^H is linear, so it commutes with the derivative The','line_number':5738,'multiline':False]['text':' derivation follows by differentiating op_1(PLU)op_2(X) = op_3(B)','line_number':5739,'multiline':False]['text':' left = True, adjoint = True: A^HX = B','line_number':5741,'multiline':False]['text':' left = True, adjoint = False: AX = B','line_number':5742,'multiline':False]['text':' left = False, adjoint = True: AX^H = B^H','line_number':5743,'multiline':False]['text':' left = False, adjoint = False: A^HX^H = B^H','line_number':5744,'multiline':False]['text':' let op_1(A) = A^H or op_1(A) = A according to the list above','line_number':5745,'multiline':False]['text':' same with op_2(X) and op_3(B)','line_number':5746,'multiline':False]['text':' We have that letting S = lu_solve(LU, pivots, dB, left, adjoint)','line_number':5747,'multiline':False]['text':' the JVP formula reads','line_number':5748,'multiline':False]['text':' dX = op_2(op_1(-U^{-1}(dUU^{-1} + L^{-1}dL)L^{-1} P^T)op_3(B)) + S','line_number':5749,'multiline':False]['text':' We see that when left != adjoint, op_1(A) = A, and we can substitute','line_number':5754,'multiline':False]['text':' A^{-1}op_3(B) by op_2(X) dX = op_2(-U^{-1}(dU + L^{-1}dL U)op_2(X)) + S','line_number':5755,'multiline':False]['text':' Let R = -U^{-1}(dU + L^{-1}dL U)','line_number':5756,'multiline':False]['text':'upper','line_number':5760,'multiline':True]['text':'left','line_number':5761,'multiline':True]['text':'unitriangular','line_number':5762,'multiline':True]['text':'upper','line_number':5765,'multiline':True]['text':' dX = op_2(R op_2(X)) + S','line_number':5766,'multiline':False]['text':' We see that when left == adjoint, op_1(A) = A^H','line_number':5769,'multiline':False]['text':' dX = op_2(op_1(-op_3(B)^H U^{-1}(dUU^{-1} + L^{-1}dL)L^{-1} P^T)) + S','line_number':5770,'multiline':False]['text':' Now, note that whenever adjoint == left, we have that','line_number':5771,'multiline':False]['text':' op_3(B)^H A^{-1} = op_3(X)^H','line_number':5772,'multiline':False]['text':' We can then rewrite the formula above in terms of X as','line_number':5773,'multiline':False]['text':' dX = op_2(op_1(-op_3(X)^H P(LdUU^{-1} + dL)L^{-1} P^T)) + S','line_number':5774,'multiline':False]['text':' Compute V = op_3(X)^H','line_number':5776,'multiline':False]['text':' Compute the inner parens LdUU^{-1} + dL','line_number':5778,'multiline':False]['text':'upper','line_number':5780,'multiline':True]['text':'left','line_number':5780,'multiline':True]['text':' dX = op_2(op_1(-op_3(X)^H PRL^{-1} P^T)) + S','line_number':5782,'multiline':False]['text':'upper','line_number':5786,'multiline':True]['text':'left','line_number':5787,'multiline':True]['text':'unitriangular','line_number':5788,'multiline':True]['text':' dX = op_2(R^H) + S','line_number':5790,'multiline':False]['text':' For left=True (left=False is analogous)','line_number':5804,'multiline':False]['text':' dX = A^{-1}(dB - dAX)','line_number':5805,'multiline':False]['text':' [NumPy compat] Case where the rhs is a vector.','line_number':5807,'multiline':False]['text':' We denote with an underscore vectors that have been converted to matrices','line_number':5808,'multiline':False]['text':' by `unsqueeze(-1)`','line_number':5809,'multiline':False]['text':' This case is disallowed in the primal operation as A.shape = (*, 1, 1)','line_number':5818,'multiline':False]['text':'adjoint','line_number':5825,'multiline':True]['text':' for X = A^{-1}B','line_number':5837,'multiline':False]['text':' gB = A^{-H}gX','line_number':5838,'multiline':False]['text':' gA = -gB X^H','line_number':5839,'multiline':False]['text':' [NumPy compat] Case where the rhs is a vector.','line_number':5846,'multiline':False]['text':' We denote with an underscore vectors that have been converted to matrices','line_number':5847,'multiline':False]['text':' by `unsqueeze(-1)`','line_number':5848,'multiline':False]['text':' If the user is going to compute higher order gradients, then we need to','line_number':5857,'multiline':False]['text':' recompute the LU and the pivots','line_number':5858,'multiline':False]['text':'adjoint','line_number':5865,'multiline':True]['text':' Getters for the principal and complementary part of the matrices','line_number':5903,'multiline':False]['text':'dim=','line_number':5925,'multiline':True]['text':'dim=','line_number':5935,'multiline':True]['text':'dim=','line_number':5946,'multiline':True]['text':'level','line_number':5966,'multiline':True]['text':'level','line_number':5991,'multiline':True]['text':' Basically copy of cat_jvp above','line_number':6002,'multiline':False]['text':' TODO: consolidate with the logic of cat_jvp','line_number':6003,'multiline':False]['text':'level','line_number':6017,'multiline':True]['text':' Generic formula when no 0. is involved','line_number':6030,'multiline':False]['text':' Note that we have to use at::where below as we are removing nans','line_number':6033,'multiline':False]['text':' For input (a, 0, b, 0, c) with gradients (t0, t1, t2, t3, t4)','line_number':6039,'multiline':False]['text':' The output of cumprod is (a, 0, 0, 0, 0)','line_number':6040,'multiline':False]['text':' The gradient we want to compute is (t0, a*t1, a*b*t1, 0, 0)','line_number':6041,'multiline':False]['text':' We do this by:','line_number':6042,'multiline':False]['text':' Get a mask of all zeros (0, 1, 0, 1, 0)','line_number':6043,'multiline':False]['text':' Get a mask of the first zero for each dim (0, 1, 0, 0, 0)','line_number':6045,'multiline':False]['text':' Get the new grad value that should be used after any zero happened:','line_number':6048,'multiline':False]['text':' (X, a*t1, a*b*t1, 0, 0) = cumprod((a, t1, b, 0, c))','line_number':6049,'multiline':False]['text':' Get a mask of everything after the first zero: (0, 1, 1, 1, 1)','line_number':6052,'multiline':False]['text':' Do the final replacement','line_number':6055,'multiline':False]['text':' Helper for {batch,layer,group}_norms below','line_number':6061,'multiline':False]['text':' Computes the jvp for `1 / input.std(dims, keepdim)`','line_number':6062,'multiline':False]['text':' Helper for {batch,layer,group}_norms below only','line_number':6086,'multiline':False]['text':' Computes the jvp for `(input - input.mean(dims)) * input.invstd(dims)`','line_number':6087,'multiline':False]['text':' Helper for {batch,layer,group}_norms below only','line_number':6112,'multiline':False]['text':' Computes the jvp for `input * weight + bias` where weight and bias may be','line_number':6113,'multiline':False]['text':' undefined Possibly modifies the input inplace','line_number':6114,'multiline':False]['text':' We allow input_p to be optional because if weight_p isn't defined,','line_number':6121,'multiline':False]['text':' it may be possible to avoid computing input_p','line_number':6122,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':6126,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':6129,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':6133,'multiline':False]['text':'level=','line_number':6185,'multiline':True]['text':'level=','line_number':6186,'multiline':True]['text':'weight_p=','line_number':6264,'multiline':True]['text':'weight_t=','line_number':6265,'multiline':True]['text':'bias_p=','line_number':6266,'multiline':True]['text':'bias_t=','line_number':6267,'multiline':True]['text':'running_mean=','line_number':6268,'multiline':True]['text':'running_var=','line_number':6269,'multiline':True]['text':'train=','line_number':6272,'multiline':True]['text':'eps=','line_number':6273,'multiline':True]['text':'dims=','line_number':6323,'multiline':True]['text':'numel=','line_number':6324,'multiline':True]['text':'keepdim=','line_number':6325,'multiline':True]['text':' Let A in \C^{m \times n}, then its pivoted LU decomposition is','line_number':6346,'multiline':False]['text':' A = P L U, where P is a permutation matrix.','line_number':6347,'multiline':False]['text':'','line_number':6348,'multiline':False]['text':' Useful notation:','line_number':6349,'multiline':False]['text':' Let o denote the elementwise, or Hadamard, product.','line_number':6350,'multiline':False]['text':' k := min(m, n)','line_number':6351,'multiline':False]['text':' 1 := ones(k, k),','line_number':6352,'multiline':False]['text':' 1_U = 1.tril();','line_number':6353,'multiline':False]['text':' 1_L = 1 - 1_U (note the diagonal is zero)','line_number':6354,'multiline':False]['text':' For a matrix A, A^H := A.mH()','line_number':6355,'multiline':False]['text':'','line_number':6356,'multiline':False]['text':' Below we derive the backward algorithm for the case when m <= n.','line_number':6357,'multiline':False]['text':' The case m > n could be obtained using the same idea.','line_number':6358,'multiline':False]['text':' Since we assume m <= n, the LU decomposition of A could be written as','line_number':6359,'multiline':False]['text':' A = (A1 | A2) = P L (U1 | U2) where A1, U1 in \C^{m \times m}, A2, U2 in','line_number':6360,'multiline':False]['text':' \C^{m, n - m}','line_number':6361,'multiline':False]['text':'','line_number':6362,'multiline':False]['text':' Forward AD:','line_number':6363,'multiline':False]['text':'','line_number':6364,'multiline':False]['text':' dA = P dL U + P L dU => [left-multiply P^T]','line_number':6365,'multiline':False]['text':' (P^T dA1 | P^T dA2) = (dL U1 + L dU1 | dL U2 + L dU2) (*)','line_number':6366,'multiline':False]['text':' From (*):','line_number':6367,'multiline':False]['text':' P^T dA1 = dL U1 + L dU1 => [left-multiply by L^{-1}, right-multiply by','line_number':6368,'multiline':False]['text':' U1^{-1}] L^{-1} P^T dA1 U1^{-1} = L^{-1} dL + dU1 U1^{-1} (**). Note, L is','line_number':6369,'multiline':False]['text':' lower-triangular, and so is its inverse, hence L^{-1} dL is lower-triangular.','line_number':6370,'multiline':False]['text':' Also, since the diagonal of L (all ones) is never exposed explicitly (packed','line_number':6371,'multiline':False]['text':' representation), the diagonal of dL is zero, and hence diag(L^{-1} dL) = 0.','line_number':6372,'multiline':False]['text':' Assuming that U1 is full-rank, similarly, dU1 U1^{-1} is upper-triangular.','line_number':6373,'multiline':False]['text':' Combining these observations we conclude:','line_number':6374,'multiline':False]['text':'','line_number':6375,'multiline':False]['text':' L^{-1} dL = (L^{-1} P^T dA1 U1^{-1}) o 1_L,','line_number':6376,'multiline':False]['text':' dU1 U1^{-1} = (L^{-1} P^T dA1 U1^{-1}) o 1_U.','line_number':6377,'multiline':False]['text':'','line_number':6378,'multiline':False]['text':' Hence,','line_number':6379,'multiline':False]['text':' dL = L [(L^{-1} P^T dA1 U1^{-1}) o 1_L],','line_number':6380,'multiline':False]['text':' dU1 = [(L^{-1} P^T dA1 U1^{-1}) o 1_U] U1.','line_number':6381,'multiline':False]['text':' As for dU2, from (*) it follows','line_number':6382,'multiline':False]['text':' P^T dA2 = dL U2 + L dU2 =>','line_number':6383,'multiline':False]['text':' dU2 = L^{-1} (P^T dA2 - dL U2).','line_number':6384,'multiline':False]['text':'','line_number':6385,'multiline':False]['text':' Backward AD:','line_number':6386,'multiline':False]['text':'','line_number':6387,'multiline':False]['text':' The following equality comes very handy:','line_number':6388,'multiline':False]['text':' Tr(A (B o C)) = Tr((A o B^T) C) (!)','line_number':6389,'multiline':False]['text':' or in other words, given that X -> B o X is a pointwise operation','line_number':6390,'multiline':False]['text':' its Jacobian is diagonal, so its differential is self-adjoint','line_number':6391,'multiline':False]['text':' <A, B o C> = <A o B, C>','line_number':6392,'multiline':False]['text':'','line_number':6393,'multiline':False]['text':' Tr(A_grad^H dA) = Tr(L_grad^H dL) + Tr(U_grad^H dU), then','line_number':6394,'multiline':False]['text':'','line_number':6395,'multiline':False]['text':' Tr(L_grad^H dL) = Tr(L_grad^H L [(L^{-1} P^T dA1 U1^{-1}) o 1_L] = [using','line_number':6396,'multiline':False]['text':' (!)]','line_number':6397,'multiline':False]['text':'                 = Tr((L_grad^H L o 1_L^T) L^{-1} P^T dA1 U1^{-1}) = [using','line_number':6398,'multiline':False]['text':'                 the cyclic property of Tr] = Tr(U1^{-1} (L_grad^H L o 1_L^T)','line_number':6399,'multiline':False]['text':'                 L^{-1} P^T dA1)','line_number':6400,'multiline':False]['text':'','line_number':6401,'multiline':False]['text':' Similar, using (!) and the cyclic property of the trace operator:','line_number':6402,'multiline':False]['text':' Tr(U_grad^H dU) = Tr(U1_grad^H dU1) + Tr(U2_grad^H dU2)','line_number':6403,'multiline':False]['text':'                 = Tr(U1^{-1} (U1 U1_grad^H o 1_U^T) L^{-1} P^T dA1)','line_number':6404,'multiline':False]['text':'                   + Tr(U2_grad^H L^{-1} P^T dA2)','line_number':6405,'multiline':False]['text':'                   - Tr(U1^{-1} (U2 U2_grad^H o 1_L^T) L^{-1} P^T dA1)','line_number':6406,'multiline':False]['text':'','line_number':6407,'multiline':False]['text':' By combining the matrices to the left from dA1 and dA2 and then applying','line_number':6408,'multiline':False]['text':' conjugate transposition, we finally arrive at:','line_number':6409,'multiline':False]['text':'','line_number':6410,'multiline':False]['text':' A1_grad = P L^{-H} [L^H L_grad o 1_L + U1_grad U1^H o 1_U - U2_grad U2^H o','line_number':6411,'multiline':False]['text':' 1_L] U1^{-H}, A2_grad = P L^{-H} U2_grad','line_number':6412,'multiline':False]['text':' Return early if there's nothing to do','line_number':6421,'multiline':False]['text':' L.shape == (..., m, k)','line_number':6426,'multiline':False]['text':' U.shape == (..., k, n)','line_number':6427,'multiline':False]['text':' A_grad = P L^{-H} [L^H L_grad o 1_L + U_grad U^H o 1_U] U^{-H},','line_number':6433,'multiline':False]['text':'upper=','line_number':6442,'multiline':True]['text':'left=','line_number':6443,'multiline':True]['text':'upper=','line_number':6447,'multiline':True]['text':'left=','line_number':6448,'multiline':True]['text':'unitriangular=','line_number':6449,'multiline':True]['text':' Wide case','line_number':6453,'multiline':False]['text':' A1_grad = P L^{-H} [U1_grad + (L^H L_grad o 1_L - U_grad U^H o 1_U)','line_number':6454,'multiline':False]['text':' U1^{-H}) U^{-H}] A2_grad = P L^{-H}  U2_grad','line_number':6455,'multiline':False]['text':'upper=','line_number':6471,'multiline':True]['text':'left=','line_number':6472,'multiline':True]['text':'dim=','line_number':6476,'multiline':True]['text':'upper=','line_number':6482,'multiline':True]['text':'left=','line_number':6483,'multiline':True]['text':'unitriangular=','line_number':6484,'multiline':True]['text':'dim=','line_number':6487,'multiline':True]['text':' Tall case','line_number':6494,'multiline':False]['text':' A1_grad = P [L1_grad + L^{-H} (U_grad U^H o 1_U - L^H L_grad o','line_number':6495,'multiline':False]['text':' 1_L)]U^{-H} A2_grad = P  L2_grad U^{-H}','line_number':6496,'multiline':False]['text':'upper=','line_number':6513,'multiline':True]['text':'left=','line_number':6514,'multiline':True]['text':'unitriangular=','line_number':6515,'multiline':True]['text':'dim=','line_number':6519,'multiline':True]['text':'upper=','line_number':6525,'multiline':True]['text':'left=','line_number':6526,'multiline':True]['text':'dim=','line_number':6529,'multiline':True]['text':'unpack_data=','line_number':6544,'multiline':True]['text':'unpack_pivots','line_number':6544,'multiline':True]['text':' L.shape == (..., m, k)','line_number':6546,'multiline':False]['text':' U.shape == (..., k, n)','line_number':6547,'multiline':False]['text':'L_grad=','line_number':6554,'multiline':True]['text':'U_grad=','line_number':6554,'multiline':True]['text':' This function is based on the forward AD derivations outlined','line_number':6557,'multiline':False]['text':' in the description to the linalg_lu_backward function.','line_number':6558,'multiline':False]['text':' similar to the backward implementation, we also consider block structures','line_number':6573,'multiline':False]['text':' such as: for a matrix A of size m x n we decompose it as A = (A1 | A2) with','line_number':6574,'multiline':False]['text':' A1 of size m x m if m <= n and A = (A1^T | A2^T)^T with A1 of size n x n if','line_number':6575,'multiline':False]['text':' m > n.','line_number':6576,'multiline':False]['text':' We form using two triangular_solve the matrix, the second one in place','line_number':6581,'multiline':False]['text':' dK = L1^{-1} PdA1 U2^{-1}','line_number':6582,'multiline':False]['text':'upper=','line_number':6584,'multiline':True]['text':'left=','line_number':6584,'multiline':True]['text':'unitriangular','line_number':6584,'multiline':True]['text':' TODO We should be able to do this in-place. At the moment it raises:','line_number':6586,'multiline':False]['text':'  RuntimeError: linalg_solve_triangular(): functions with out=...','line_number':6587,'multiline':False]['text':'  arguments don't support automatic differentiation, but one of the','line_number':6588,'multiline':False]['text':'  arguments requires grad.','line_number':6589,'multiline':False]['text':'  at::linalg_solve_triangular_out(dK, U1, dK, /*upper=*/true,','line_number':6591,'multiline':False]['text':'  /*left=*/false);','line_number':6592,'multiline':False]['text':'upper=','line_number':6593,'multiline':True]['text':'left=','line_number':6593,'multiline':True]['text':' we only need to update dU2 defined as','line_number':6601,'multiline':False]['text':' dU2 := L1^{-1} PdA2 - dK.tril(-1) U2)','line_number':6602,'multiline':False]['text':'upper=','line_number':6607,'multiline':True]['text':'left=','line_number':6607,'multiline':True]['text':'unitriangular','line_number':6607,'multiline':True]['text':'dim=','line_number':6610,'multiline':True]['text':' we only need to update dL2 defined as','line_number':6612,'multiline':False]['text':' dL2 := PdA2 U^{-1} - L2 dK.triu()','line_number':6613,'multiline':False]['text':'upper=','line_number':6617,'multiline':True]['text':'left=','line_number':6617,'multiline':True]['text':'dim=','line_number':6620,'multiline':True]['text':'unpack_data=','line_number':6630,'multiline':True]['text':'unpack_pivots=','line_number':6630,'multiline':True]['text':' NB: for simplicity, we recompute some values that can be reused from','line_number':6649,'multiline':False]['text':' forward','line_number':6650,'multiline':False]['text':' Use the exp-normalize trick','line_number':6654,'multiline':False]['text':' amax fails if numel() == 0, in which case it doesn't matter anyway','line_number':6656,'multiline':False]['text':' NB: it's OK for logsumexp_jvp to be reused for formulas like','line_number':6663,'multiline':False]['text':' softmax/log_softmax','line_number':6664,'multiline':False]['text':'     that only have one differentiable input, because that means self_t are','line_number':6665,'multiline':False]['text':'     never zerotensors','line_number':6666,'multiline':False]['text':' This function only exists because cuDNN does not support bias gradient','line_number':6684,'multiline':False]['text':' computation and it's not easy to slice a std::tuple to return only grad_input','line_number':6685,'multiline':False]['text':' / grad_weight from convolution_backward. It will be removed when the','line_number':6686,'multiline':False]['text':' cudnn_convolution and cudnn_convolution_transpose go away.','line_number':6687,'multiline':False]['text':' Just call the general backward and ignore the bias gradient part.','line_number':6703,'multiline':False]['text':' The function is linear','line_number':6733,'multiline':False]['text':'  auto mask = x == restore_reduced_dims(result, dim, keepdim);','line_number':6735,'multiline':False]['text':'  return at::where(mask, dx, 0.).sum(dim, keepdim) / mask.sum(dim,','line_number':6736,'multiline':False]['text':'  keepdim);','line_number':6737,'multiline':False]['text':' Not implemented','line_number':6751,'multiline':False]['text':' FIXME: complex gradients not handled correctly','line_number':6767,'multiline':False]['text':' For now this is ok as scatter_reduce isn't added to the whitelist','line_number':6768,'multiline':False]['text':' in tools/autograd/gen_variable_type.py','line_number':6769,'multiline':False]['text':' Explicitly compute exclusive prod for elements in self/src that are 0','line_number':6779,'multiline':False]['text':' For src positions with src_single_zero, grad * result.gather(dim,index) /','line_number':6790,'multiline':False]['text':' src.masked_fill(src_zero, 1) would incorrectly propagate zeros as the','line_number':6791,'multiline':False]['text':' gradient','line_number':6792,'multiline':False]['text':' GradMode::is_enabled() - adding the autograd Node is a no-op if autograd','line_number':6800,'multiline':False]['text':' is disabled; this also avoids having the item() call in the usual case.','line_number':6801,'multiline':False]['text':' num inputs ','line_number':6805,'multiline':True]['text':' Evenly distribute gradient when there are multiple max/mins','line_number':6819,'multiline':False]['text':' Handle R->C copies without raising a warning','line_number':6845,'multiline':False]['text':'non_blocking=','line_number':6852,'multiline':True]['text':'copy=','line_number':6852,'multiline':True]['text':' FIXME: index_add's backward formula has a special case for source.dim == 0','line_number':6866,'multiline':False]['text':' but this case seems to throw the error "IndexError: dimension specified as','line_number':6867,'multiline':False]['text':' 0 but tensor has no dimensions" look into whether this case is reachable','line_number':6868,'multiline':False]['text':' and should be covered here','line_number':6869,'multiline':False]['text':' For src positions with src_single_zero, (grad *','line_number':6885,'multiline':False]['text':' result).index_select(dim,index) / source.masked_fill(src_zero, 1) would','line_number':6886,'multiline':False]['text':' incorrectly propagate zeros as the gradient','line_number':6887,'multiline':False]['text':' GradMode::is_enabled() - adding the autograd Node is a no-op if autograd','line_number':6896,'multiline':False]['text':' is disabled this also avoids having the item() call in the usual case','line_number':6897,'multiline':False]['text':' num inputs ','line_number':6901,'multiline':True]['text':' For Composite Compliance,','line_number':6942,'multiline':False]['text':' if `grad` and `indices` are CCT but `grad_self` is not','line_number':6943,'multiline':False]['text':' then we use the out-of-place variant of `put`.','line_number':6944,'multiline':False]['text':' Path for strided and nested','line_number':6956,'multiline':False]['text':' num_bias_gates of LSTM ','line_number':7021,'multiline':True]['text':' num_bias_gates of LSTM ','line_number':7023,'multiline':True]['text':' Re-calculate gates and hidden states during one layer, which will be used','line_number':7029,'multiline':False]['text':' in backward.','line_number':7030,'multiline':False]['text':' namespace details','line_number':7104,'multiline':False]['text':' namespace generated','line_number':7105,'multiline':False]['text':' namespace autograd','line_number':7106,'multiline':False]['text':' namespace torch','line_number':7107,'multiline':False]