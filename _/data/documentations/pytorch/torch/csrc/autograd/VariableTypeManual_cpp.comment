['text':' namespace','line_number':82,'multiline':False]['text':' Taken from codegened version','line_number':113,'multiline':False]['text':' Modified from original codegen','line_number':132,'multiline':False]['text':' We explicitly want to ignore the forward grad at the given level','line_number':133,'multiline':False]['text':' End modified from original codegen','line_number':135,'multiline':False]['text':' NB: We need a manual variable type kernel so that set_fw_grad properly','line_number':140,'multiline':False]['text':' detects that _make_dual is not a forward-differentiable view','line_number':141,'multiline':False]['text':'','line_number':142,'multiline':False]['text':' This function can be used to create a dual Tensor that holds a tangent to','line_number':143,'multiline':False]['text':' compute forward mode gradients. Note that the dual Tensor's primal is a view','line_number':144,'multiline':False]['text':' of the given primal and the given tangent is used as-is. This function is','line_number':145,'multiline':False]['text':' backward differentiable.','line_number':146,'multiline':False]['text':' is_inplace_op ','line_number':178,'multiline':True]['text':' We don't have an outplace copy, so this can't be generated automatically','line_number':182,'multiline':False]['text':' TODO: once copy is exposed in Declarations.yaml we may be able to bind','line_number':188,'multiline':False]['text':' it automatically','line_number':189,'multiline':False]['text':' level ','line_number':226,'multiline':True]['text':' is_inplace_op ','line_number':226,'multiline':True]['text':' level ','line_number':247,'multiline':True]['text':' Handle fw grad','line_number':273,'multiline':False]['text':' level ','line_number':274,'multiline':True]['text':' Detach the forward grads by not setting anything on the result','line_number':290,'multiline':False]['text':' See NOTE [ View + Inplace detection ]','line_number':298,'multiline':False]['text':' I think the choice here is conservative.  In principle, doing','line_number':308,'multiline':False]['text':' an in-place detach should give us the ability to just clear','line_number':309,'multiline':False]['text':' the autograd meta.  But this function ONLY resets requires_grad,','line_number':310,'multiline':False]['text':' grad_fn and output_nr; there's other metadata like debug name','line_number':311,'multiline':False]['text':' and hooks which aren't cleared.  Is this function supposed to','line_number':312,'multiline':False]['text':' clear those too? I'm not too sure, so I'm leaving it be for now.','line_number':313,'multiline':False]['text':' Ops in the following registration list are registered as','line_number':323,'multiline':False]['text':'   (1) CompositeImplicitAutograd kernels','line_number':324,'multiline':False]['text':'   (2) Autograd kernels','line_number':325,'multiline':False]['text':'   (3) CompositeExplicitAutograd kernels and additionally Autograd kernels','line_number':326,'multiline':False]['text':' The reason for (3) is that ops that also use dispatch (e.g. register','line_number':327,'multiline':False]['text':' CPU/CUDA/QuantizedCPU kernels) will skip picking up CompositeImplicitAutograd','line_number':328,'multiline':False]['text':' kernels for Autograd, so we register them to both CompositeExplicitAutograd','line_number':329,'multiline':False]['text':' and Autograd instead. See','line_number':330,'multiline':False]['text':' https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword','line_number':331,'multiline':False]['text':' for more details.','line_number':332,'multiline':False]['text':' Invariant:','line_number':333,'multiline':False]['text':' - Ops registered to CompositeImplicitAutograd or CompositeExplicitAutograd','line_number':334,'multiline':False]['text':' below must match `MANUAL_BACKEND` set in tools/autograd/gen_variable_type.py.','line_number':335,'multiline':False]['text':'   and they have manual_kernel_registration=True in native_functions.yaml.','line_number':336,'multiline':False]['text':' - Ops registered to DispatchKey::Autograd below must be included in','line_number':337,'multiline':False]['text':' `MANUAL_AUTOGRAD` in tools/autograd/gen_variable_type.py','line_number':338,'multiline':False]['text':' namespace','line_number':367,'multiline':False]['text':' namespace VariableType','line_number':368,'multiline':False]['text':' namespace autograd','line_number':369,'multiline':False]['text':' Hold sizes to verify if we actually resize `self`.','line_number':397,'multiline':False]['text':' Explicitly copy data, since resizing can move original data','line_number':398,'multiline':False]['text':' and make references invalid.','line_number':399,'multiline':False]['text':' If `self` was resized, increment the version.','line_number':409,'multiline':False]['text':' Hold sizes to verify if we actually resize `self`.','line_number':421,'multiline':False]['text':' Explicitly copy data, since resizing can move original data','line_number':422,'multiline':False]['text':' and make references invalid.','line_number':423,'multiline':False]['text':' If `self` was resized, increment the version.','line_number':434,'multiline':False]['text':' NB: we can't make detach() a normal view operator because the codegen','line_number':447,'multiline':False]['text':' generates allow_tensor_metadata_change = True for them. In the future we','line_number':448,'multiline':False]['text':' should have an option for this in the codegen.','line_number':449,'multiline':False]['text':' base ','line_number':451,'multiline':True]['text':' output ','line_number':452,'multiline':True]['text':' is_bw_differentiable ','line_number':453,'multiline':True]['text':' is_fw_differentiable ','line_number':454,'multiline':True]['text':' view_func ','line_number':455,'multiline':True]['text':' creation_meta ','line_number':456,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':457,'multiline':True]['text':' base ','line_number':478,'multiline':True]['text':' output ','line_number':479,'multiline':True]['text':' is_bw_differentiable ','line_number':480,'multiline':True]['text':' is_fw_differentiable ','line_number':481,'multiline':True]['text':' view_func ','line_number':482,'multiline':True]['text':' creation_meta ','line_number':483,'multiline':True]['text':' NB: This does not redispatch any further','line_number':488,'multiline':False]['text':' base ','line_number':506,'multiline':True]['text':' output ','line_number':507,'multiline':True]['text':' is_bw_differentiable ','line_number':508,'multiline':True]['text':' is_fw_differentiable ','line_number':509,'multiline':True]['text':' view_func ','line_number':510,'multiline':True]['text':' creation_meta ','line_number':511,'multiline':True]['text':' namespace','line_number':543,'multiline':False]['text':' namespace ADInplaceOrView','line_number':544,'multiline':False]['text':' namespace torch','line_number':545,'multiline':False]