['text':' Return undefined tensor.','line_number':23,'multiline':False]['text':' This function has two main goals:','line_number':31,'multiline':False]['text':'  1) Use the user-provided jvp function to populate the outputs' forward','line_number':32,'multiline':False]['text':'  gradient 2) Perform error checking to ensure that view and inplace ops are','line_number':33,'multiline':False]['text':'  properly handled','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' For 1) we have to:','line_number':36,'multiline':False]['text':'  - Create a variable_list of grad_inputs based on the function inputs','line_number':37,'multiline':False]['text':'  - Call the user jvp function with these to get the grad_outputs','line_number':38,'multiline':False]['text':'  - Set the forward grad field on each output based on these grad_outputs','line_number':39,'multiline':False]['text':'','line_number':40,'multiline':False]['text':' For 2) we want to check the following:','line_number':41,'multiline':False]['text':'  - If an output is a view, then the generated forward grad must be a view as','line_number':42,'multiline':False]['text':'  well and','line_number':43,'multiline':False]['text':'    the output's base's forward grad must be the output's forward grad's base.','line_number':44,'multiline':False]['text':'  - If an input was modified inplace (it must be an output as well) we make','line_number':45,'multiline':False]['text':'  sure that its','line_number':46,'multiline':False]['text':'    forward grad was also modified inplace and already present on the','line_number':47,'multiline':False]['text':'    corresponding output.','line_number':48,'multiline':False]['text':' TODO handle multiple levels here','line_number':57,'multiline':False]['text':' The tracking info below are used to perform the view and inplace checks.','line_number':63,'multiline':False]['text':' They are lazily initialized to reduce the cost of this function in the','line_number':64,'multiline':False]['text':' common case where the user is not using forward mode AD.','line_number':65,'multiline':False]['text':' Extract the input's forward gradients and record any info we will need','line_number':91,'multiline':False]['text':' later','line_number':92,'multiline':False]['text':' If no input has forward grad, nothing to do here','line_number':110,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':121,'multiline':False]['text':' contrary to backward mode, we don't allow returning too many gradients','line_number':123,'multiline':False]['text':' If there was already a forward grad for that input','line_number':163,'multiline':False]['text':' Just make sure that it is modified inplace and returned as-is','line_number':164,'multiline':False]['text':' If that Tensor didn't had gradients already, set the newly returned','line_number':176,'multiline':False]['text':' one We could also use inputs[inp_idx] here as it is the same as out','line_number':177,'multiline':False]['text':' is_inplace_op ','line_number':178,'multiline':True]['text':' At this point, outputs[i] cannot be one of the input (raw_outputs[i]','line_number':181,'multiline':False]['text':' might be but was changed by the backward code)','line_number':182,'multiline':False]['text':' If the output is a view','line_number':187,'multiline':False]['text':' And it is a view of an input (either that input is its base or they','line_number':191,'multiline':False]['text':' have a common base)','line_number':192,'multiline':False]['text':' If the matching input has a forward grad, the user should have','line_number':199,'multiline':False]['text':' returned a view of that Tensor','line_number':200,'multiline':False]['text':' If the matching input's grad is a view, ensure that the','line_number':213,'multiline':False]['text':' out_grad is a view of the same base','line_number':214,'multiline':False]['text':' If the matching input's grad is not a view, then it must be the','line_number':225,'multiline':False]['text':' output gradient's base','line_number':226,'multiline':False]['text':' We have a view op where the input didn't have a forward grad but','line_number':233,'multiline':False]['text':' the user returned one for the output To ensure that we maintain','line_number':234,'multiline':False]['text':' the view/inplace constraints, we consider this as an inplace op','line_number':235,'multiline':False]['text':' This case CANNOT happen in codegen as all view ops are mapping','line_number':236,'multiline':False]['text':' from one Tensor to one Tensor and so the output of the view','line_number':237,'multiline':False]['text':' cannot have a forward grad if the base does not.','line_number':238,'multiline':False]['text':' is_inplace_op ','line_number':239,'multiline':True]['text':' is_inplace_op ','line_number':245,'multiline':True]['text':' This is called below in _process_backward_mode_ad in two places:','line_number':253,'multiline':False]['text':'','line_number':254,'multiline':False]['text':' (1) An input has been returned, but it wasn't modified. Return it as a view','line_number':255,'multiline':False]['text':' so that we can attach a new grad_fn to the Variable.','line_number':256,'multiline':False]['text':' Run in no_grad mode to mimic the behavior of the forward.','line_number':257,'multiline':False]['text':'','line_number':258,'multiline':False]['text':' (2) Though it is not necessary for the purposes of attaching grad_fn, we','line_number':259,'multiline':False]['text':' also call this function when an output is non-differentiable (and does not','line_number':260,'multiline':False]['text':' require grad). to help custom forward AD UX more consistent. We'd like to','line_number':261,'multiline':False]['text':' uniformly say that returning an input as-is is treated as if','line_number':262,'multiline':False]['text':' `self.view_as(self)` were returned for that output.','line_number':263,'multiline':False]['text':'','line_number':264,'multiline':False]['text':' Alternatively, we could have not disabled forward grad while performing','line_number':265,'multiline':False]['text':' this view, but it would mean that the user defined jvp may be silently','line_number':266,'multiline':False]['text':' ignored.','line_number':267,'multiline':False]['text':' We thread through this view_as_self_fn lambda so that in the case we are a','line_number':270,'multiline':False]['text':' Python custom function (rather than a cpp one), we can properly call the','line_number':271,'multiline':False]['text':' view_as from python so that torch function logic can still trigger.','line_number':272,'multiline':False]['text':' Sets the grad_fn and output_nr of an output Variable.','line_number':292,'multiline':False]['text':' Return detached aliases of inputs, instead of changing their','line_number':308,'multiline':False]['text':' requires_grad property.','line_number':309,'multiline':False]['text':' If var is a view of one of the inputs of the custom autograd Function,','line_number':315,'multiline':False]['text':' we don't detach it in a no_grad block. This is so that we can mimic the','line_number':316,'multiline':False]['text':' behavior of returning a view from a no_grad block:','line_number':317,'multiline':False]['text':'   x = torch.randn(3, requires_grad=True)','line_number':318,'multiline':False]['text':'   with torch.no_grad():','line_number':319,'multiline':False]['text':'       y = x.view(-1)','line_number':320,'multiline':False]['text':' Here, `y` requires_grad (!).','line_number':321,'multiline':False]['text':' No need to mark as modified Tensors that are not inputs.','line_number':328,'multiline':False]['text':' If the input is a view, the rebase will need to rewrite the graph and','line_number':334,'multiline':False]['text':' this only works if we have a single output to this Function.','line_number':335,'multiline':False]['text':' If the input was modified, transplant the grad_fn in the graph:','line_number':346,'multiline':False]['text':' grad_fn <- variable <- self  ==>  grad_fn <- self <- variable','line_number':347,'multiline':False]['text':' For dirty_inputs check','line_number':367,'multiline':False]['text':' We put a undefined_input placeholder for outputs that are not tensor and','line_number':372,'multiline':False]['text':' for when the output tensor is not differentiable (see below)','line_number':373,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':383,'multiline':False]['text':' For deprecation cycle. Can be removed after 1.6. In the case where we','line_number':412,'multiline':False]['text':' detected a view in no grad mode during the forward, only warn the user','line_number':413,'multiline':False]['text':' (do not change the flag if we return and input that is a view as is). See','line_number':414,'multiline':False]['text':' NOTE [ View + Inplace detection ] for why we replace everything by a','line_number':415,'multiline':False]['text':' warning.','line_number':416,'multiline':False]['text':' is_view() => diff_view_meta','line_number':418,'multiline':False]['text':' If multiple differentiable outputs are returned, we do not allow views to','line_number':431,'multiline':False]['text':' be modified inplace See NOTE [ View + Inplace detection ] for more details','line_number':432,'multiline':False]['text':' All the modified Tensors must be returned as is for the rewrite to be','line_number':444,'multiline':False]['text':' valid.','line_number':445,'multiline':False]['text':' This must happen after the backward processing as we expect the','line_number':480,'multiline':False]['text':' computations happening here to track backward mode gradients.','line_number':481,'multiline':False]['text':' The logic for handling saved variables here is the same as','line_number':528,'multiline':False]['text':' python_function.cpp See _save_variables() and unpack_saved_variables()','line_number':529,'multiline':False]['text':' Allow empty variables to be saved','line_number':535,'multiline':False]['text':' namespace autograd','line_number':603,'multiline':False]['text':' namespace torch','line_number':604,'multiline':False]