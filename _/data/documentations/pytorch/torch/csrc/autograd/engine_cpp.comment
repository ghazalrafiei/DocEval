['text':' True for children forked after engine's thread pool init','line_number':55,'multiline':False]['text':' Called in the forked child if engine's thread pool has already been','line_number':57,'multiline':False]['text':' initialized','line_number':58,'multiline':False]['text':' Should be called before unsafe for forks (thread pool) calls','line_number':63,'multiline':False]['text':' namespace','line_number':101,'multiline':False]['text':' Threads spawned by the engine are assigned a 'worker_device' specifying','line_number':103,'multiline':False]['text':' what device they process work for. This variable is initialized at:','line_number':104,'multiline':False]['text':' 1. thread creation time for CUDA, XLA device threads, as they are','line_number':105,'multiline':False]['text':'    spinning threads waiting for works on their device.','line_number':106,'multiline':False]['text':' 2. before the graph task execution for CPU threads, as for each','line_number':107,'multiline':False]['text':'    backward call we use the caller thread to drive engine execution.','line_number':108,'multiline':False]['text':' This is used when handling reentrant backwards calls;','line_number':109,'multiline':False]['text':' See Note [Reentrant backwards]','line_number':110,'multiline':False]['text':' This variable is true if ALL invocations in the stack of re-entrant engine','line_number':113,'multiline':False]['text':' invocations are imperative backwards. This special variable is needed for the','line_number':114,'multiline':False]['text':' gradient checkpointing feature only.','line_number':115,'multiline':False]['text':' Number of nested reentrant backwards calls currently on this thread','line_number':118,'multiline':False]['text':' For all device threads (i.e. CUDA, XLA), total_depth represents the total','line_number':121,'multiline':False]['text':' nested','line_number':122,'multiline':False]['text':'   reentrant backwards depths over all device threads.','line_number':123,'multiline':False]['text':' For CPU devices, it is the total depth associated with the original backward','line_number':124,'multiline':False]['text':' call.','line_number':125,'multiline':False]['text':' The current GraphTask being executed by this thread. This helps','line_number':128,'multiline':False]['text':' queue_callback() to find the target GraphTask to append final callbacks.','line_number':129,'multiline':False]['text':' Every autograd worker thread is associated with a ready queue, which','line_number':133,'multiline':False]['text':' specifies the stream of work of this thread to do. This shared_ptr is a','line_number':134,'multiline':False]['text':' thread_local pointer to each thread's ready_queue, and it should be','line_number':135,'multiline':False]['text':' initialized via the Engine::init_local_ready_queue() call in each','line_number':136,'multiline':False]['text':' corresponding thread before execution.','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':' The CUDA, XLA threads are shared among all invocations of backwards via','line_number':139,'multiline':False]['text':' device_ready_queues_, while the caller thread is dedicated to processing work','line_number':140,'multiline':False]['text':' for devices returning true in should_run_in_cpu_ready_queue (most notably the','line_number':141,'multiline':False]['text':' CPU device). So any given graph task maintains its own cpu_ready_queue_ where','line_number':142,'multiline':False]['text':' you should send work for it to be done.','line_number':143,'multiline':False]['text':'','line_number':144,'multiline':False]['text':' For reentrant backward calls, if we spawn new thread from the current thread','line_number':145,'multiline':False]['text':' because we reached the maximum depth, the new thread will just reuse the same','line_number':146,'multiline':False]['text':' ReadyQueue with the parent thread for performance improvement.','line_number':147,'multiline':False]['text':' see Note [Reentrant backwards] for more details.','line_number':148,'multiline':False]['text':' Note [Reentrant backwards]','line_number':152,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':153,'multiline':False]['text':' To understand the reentrant backwards problem, we have to notice two','line_number':154,'multiline':False]['text':' aspects of how the autograd engine is implemented today:','line_number':155,'multiline':False]['text':'','line_number':156,'multiline':False]['text':'  1. When you call Engine::execute(), you want to block until','line_number':157,'multiline':False]['text':'  differentiation finishes so that you can get the final result variables','line_number':158,'multiline':False]['text':'  of the backwards pass.','line_number':159,'multiline':False]['text':'','line_number':160,'multiline':False]['text':'  2. The engine operates by having a single worker thread per work queue,','line_number':161,'multiline':False]['text':'  and every work queue is pinned to a specific device where the','line_number':162,'multiline':False]['text':'  operation is executed.','line_number':163,'multiline':False]['text':'','line_number':164,'multiline':False]['text':' The problem is, suppose that you call backward() inside of a worker','line_number':165,'multiline':False]['text':' thread.  By property (1), we're supposed to block until the nested task','line_number':166,'multiline':False]['text':' finishes.  However, by property (2), this worker thread is on the','line_number':167,'multiline':False]['text':' hook for processing the tasks assigned to it; we better not block,','line_number':168,'multiline':False]['text':' because then all of our backward executions (including the one we','line_number':169,'multiline':False]['text':' just started) will deadlock!','line_number':170,'multiline':False]['text':'','line_number':171,'multiline':False]['text':' We maintain a pool of threads waiting for work to do','line_number':172,'multiline':False]['text':' When a reentrant backwards call occurs, the current thread blocks','line_number':173,'multiline':False]['text':' and a thread from the pool is woken up to complete the blocking tasks and an','line_number':174,'multiline':False]['text':' any other tasks that would have been assigned to that worker. If there are no','line_number':175,'multiline':False]['text':' threads available, a new thread is spawned. The new thread will continue','line_number':176,'multiline':False]['text':' processing tasks from the same ReadyQueue as the parent worker','line_number':177,'multiline':False]['text':'','line_number':178,'multiline':False]['text':' When the GraphTask is finished, the parent worker thread that is waiting on','line_number':179,'multiline':False]['text':' the task is notified and the current thread returns to the pool.','line_number':180,'multiline':False]['text':' Note [Streaming backwards]','line_number':182,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':183,'multiline':False]['text':' On CUDA devices the autograd engine's device operations are run on the','line_number':184,'multiline':False]['text':' same stream that ran them in forward. This requires automatically','line_number':185,'multiline':False]['text':' syncing the streams so that function A finishes producing its','line_number':186,'multiline':False]['text':' output before function B consumes it.','line_number':187,'multiline':False]['text':'','line_number':188,'multiline':False]['text':' This synchronization occurs when outputs are placed into input buffers.','line_number':189,'multiline':False]['text':' The functions corresponding to input buffer positions have metadata','line_number':190,'multiline':False]['text':' recording their streams from forward, and during backward this','line_number':191,'multiline':False]['text':' data is used to sync the producer's stream with the consumer's.','line_number':192,'multiline':False]['text':'','line_number':193,'multiline':False]['text':' When a CUDA function is run either all its inputs were accumulated on the','line_number':194,'multiline':False]['text':' stream used to run the function OR the inputs are on different devices','line_number':195,'multiline':False]['text':' and the function is responsible for properly acquiring them.','line_number':196,'multiline':False]['text':'','line_number':197,'multiline':False]['text':' User-facing stream semantics of a backward() (or torch.autograd.grad())','line_number':198,'multiline':False]['text':' call with respect to surrounding ops are the same as for any other call.','line_number':199,'multiline':False]['text':' See "Stream semantics of backward passes" on','line_number':200,'multiline':False]['text':' https://pytorch.org/docs/stable/notes/cuda.html','line_number':201,'multiline':False]['text':'','line_number':202,'multiline':False]['text':' Internally, backward() runs ops (including leaf nodes) on side threads.','line_number':203,'multiline':False]['text':' And streams are thread local. So GraphTask achieves the above semantics by','line_number':204,'multiline':False]['text':'  1. remembering the current streams on all active CUDA devices','line_number':205,'multiline':False]['text':'     in the user-facing thread (aka, the thread that called execute() to','line_number':206,'multiline':False]['text':'     launch the GraphTask)','line_number':207,'multiline':False]['text':'  2. remembering the "leaf streams" (streams each backward leaf node ran on)','line_number':208,'multiline':False]['text':'  3. during exec_post_processing, for each leaf stream, sync the remembered','line_number':209,'multiline':False]['text':'     current streams (on the leaf stream's device) with that','line_number':210,'multiline':False]['text':'     leaf stream.','line_number':211,'multiline':False]['text':' The graph task is no longer valid indicating an error. As a result, we','line_number':218,'multiline':False]['text':' try to move this to the front of the queue to ensure the autograd','line_number':219,'multiline':False]['text':' engine threads pick up this error soon.','line_number':220,'multiline':False]['text':' Lock mutex for writing to heap_','line_number':238,'multiline':False]['text':' Lock mutex for accesses to heap_','line_number':259,'multiline':False]['text':' Lock mutex for accesses to heap_','line_number':265,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':268,'multiline':False]['text':' Lock mutex for accesses to heap_','line_number':275,'multiline':False]['text':' Send shutdown tasks to all device_ready_queues_ if no backward tasks are','line_number':287,'multiline':False]['text':' running Even though readyQueue should be empty, shutdown tasks have the','line_number':288,'multiline':False]['text':' highest priority','line_number':289,'multiline':False]['text':' Under some conditions, autograd threads can hang on shutdown','line_number':295,'multiline':False]['text':' Do not wait for them to shutdown indefinitely but rely on timeout','line_number':296,'multiline':False]['text':' Do not wait for termination of global threads on Windows','line_number':307,'multiline':False]['text':' Because CRT terminates DLL threads before calling','line_number':308,'multiline':False]['text':' global object destructors','line_number':309,'multiline':False]['text':' Set a deadline for how long it is OK to wait device threads to shutdown','line_number':313,'multiline':False]['text':' Otherwise threads are leaked','line_number':325,'multiline':False]['text':' Note [Allocating GPUs to autograd threads]','line_number':357,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':358,'multiline':False]['text':' What's our strategy here?  Originally, the autograd engine was written','line_number':359,'multiline':False]['text':' with only CUDA in mind.  We allocate one thread to handle all CPU','line_number':360,'multiline':False]['text':' operations, and a thread per CUDA device.','line_number':361,'multiline':False]['text':'','line_number':362,'multiline':False]['text':' But what if we have OTHER devices?  There are two plausible','line_number':363,'multiline':False]['text':' strategies:','line_number':364,'multiline':False]['text':'','line_number':365,'multiline':False]['text':'  - We can allocate threads equal to max(num_cuda_devices, num_xla_devices,','line_number':366,'multiline':False]['text':'    ...) and colocate cuda device 0 with xla device 0','line_number':367,'multiline':False]['text':'  - We can allocate threads equal to sum(num_cuda_devices, num_xla_devices,','line_number':368,'multiline':False]['text':'    ...) keeping everyone separate.','line_number':369,'multiline':False]['text':'','line_number':370,'multiline':False]['text':' We don't have any good reason to prefer one or the other, so we've','line_number':371,'multiline':False]['text':' arbitrarily picked to colocate devices.  Maybe the other approach is','line_number':372,'multiline':False]['text':' better.','line_number':373,'multiline':False]['text':' initialize each device thread's thread local ready queue with the ready','line_number':376,'multiline':False]['text':' queue that is created before the thread initialization','line_number':377,'multiline':False]['text':' Decrement the count during shutdown if we incremented earlier.','line_number':383,'multiline':False]['text':' The current graph task's exec_info is being used to trim unnecessary edegs','line_number':400,'multiline':False]['text':' during node evaluation, see `Node.task_should_compute_output()` function.','line_number':401,'multiline':False]['text':' NB: The engine itself does not use the outputs of this function.','line_number':423,'multiline':False]['text':' We could potentially check if there is only a single device here','line_number':430,'multiline':False]['text':' but explicitly require this context doesn't seem bad either','line_number':431,'multiline':False]['text':' Do a copy since we mutate it later','line_number':441,'multiline':False]['text':' Implementation notes:','line_number':454,'multiline':False]['text':' - We need count dependencies even though we have sequence_nr, because','line_number':455,'multiline':False]['text':'   in the accumulate_grad case we cannot assume the outputs to have higher','line_number':456,'multiline':False]['text':'   sequence_nr than the inputs','line_number':457,'multiline':False]['text':' - Don't need to check topological_nr because we have exec_info','line_number':458,'multiline':False]['text':' NOTE: graph_tasks do not necessarily form a stack. Imagine this','line_number':486,'multiline':False]['text':' case:','line_number':487,'multiline':False]['text':'','line_number':488,'multiline':False]['text':'    +----> Eval1','line_number':489,'multiline':False]['text':'  Root','line_number':490,'multiline':False]['text':'    +----> Eval2','line_number':491,'multiline':False]['text':'','line_number':492,'multiline':False]['text':' Once Root is executed, both Eval1 and Eval2 are added to the ready queue.','line_number':493,'multiline':False]['text':' Next, Eval1 is run and this causes the worker to enter thread_main again.','line_number':494,'multiline':False]['text':' Then, it pops the next task from the queue, but at this point it is Eval2.','line_number':495,'multiline':False]['text':' It enters thread_main once again, but now with graph_task of Eval2, which is','line_number':496,'multiline':False]['text':' completely unrelated to that of Eval1 (it's not a recursive call).','line_number':497,'multiline':False]['text':' It's all ok and is handled right now, but it should be accounted for','line_number':498,'multiline':False]['text':' in case this code is to be changed.','line_number':499,'multiline':False]['text':'','line_number':500,'multiline':False]['text':' thread_main is used by:','line_number':501,'multiline':False]['text':' 1). autograd threads for devices (i.e. CUDA, XLA)','line_number':502,'multiline':False]['text':' 2). the caller/owning thread of the backward call on CPU (sync mode)','line_number':503,'multiline':False]['text':' 3). Renetrant backward that invoked by either 1) or 2)','line_number':504,'multiline':False]['text':' The exit conditions are different for the above three cases.','line_number':505,'multiline':False]['text':' For 1), we are spinning on running the thread_main on device autograd','line_number':506,'multiline':False]['text':'         threads throughout the Engine lifetime, thread_main will get','line_number':507,'multiline':False]['text':'         terminated during Engine destruction by pushing shutdown tasks','line_number':508,'multiline':False]['text':' For 2), the owning thread of the backward call drives the thread_main','line_number':509,'multiline':False]['text':'         synchronously until the graph_task of that owning thread is','line_number':510,'multiline':False]['text':'         completed and exit the thread_main to continue executing the','line_number':511,'multiline':False]['text':'         result of caller's code.','line_number':512,'multiline':False]['text':' For 3), the reentrant backward that invokes','line_number':513,'multiline':False]['text':'         thread_main, either from 1) or 2), will not spin and will exit as','line_number':514,'multiline':False]['text':'         long as graph_task is completed and notify the owning thread as','line_number':515,'multiline':False]['text':'         needed.','line_number':516,'multiline':False]['text':' When graph_task is nullptr, this is a long running thread that processes','line_number':518,'multiline':False]['text':' tasks (ex: device threads). When graph_task is non-null (ex: reentrant','line_number':519,'multiline':False]['text':' backwards, user thread), this function is expected to exit once that','line_number':520,'multiline':False]['text':' graph_task complete.','line_number':521,'multiline':False]['text':' local_ready_queue should already been initialized when we get into','line_number':523,'multiline':False]['text':' thread_main','line_number':524,'multiline':False]['text':' local_graph_task represents the graph_task we retrieve from the queue.','line_number':527,'multiline':False]['text':' The outer graph_task represents the overall graph_task we need to execute','line_number':528,'multiline':False]['text':' for reentrant execution.','line_number':529,'multiline':False]['text':' Scope this block of execution since NodeTask is not needed after this','line_number':532,'multiline':False]['text':' block and can be deallocated (release any references to grad tensors','line_number':533,'multiline':False]['text':' as part of inputs_).','line_number':534,'multiline':False]['text':' This will only work if the worker is running a non backward task','line_number':536,'multiline':False]['text':' TODO Needs to be fixed this to work in all cases','line_number':537,'multiline':False]['text':' GraphTask for function is no longer valid, skipping further','line_number':545,'multiline':False]['text':' execution.','line_number':546,'multiline':False]['text':' Set the ThreadLocalState before calling the function.','line_number':553,'multiline':False]['text':' NB: The ThreadLocalStateGuard doesn't set the grad_mode because','line_number':554,'multiline':False]['text':' GraphTask always saves ThreadLocalState without grad_mode.','line_number':555,'multiline':False]['text':' The guard sets the thread_local current_graph_task on construction','line_number':561,'multiline':False]['text':' and restores it on exit. The current_graph_task variable helps','line_number':562,'multiline':False]['text':' queue_callback() to find the target GraphTask to append final','line_number':563,'multiline':False]['text':' callbacks.','line_number':564,'multiline':False]['text':' See Note [ Persisting PyErr state across autograd engine threads ]','line_number':580,'multiline':False]['text':' Decrement the outstanding tasks.','line_number':586,'multiline':False]['text':' Check if we've completed execution.','line_number':589,'multiline':False]['text':' The current worker thread finish the graph_task, but the owning thread','line_number':594,'multiline':False]['text':' of the graph_task might be sleeping on pop() if it does not have work.','line_number':595,'multiline':False]['text':' So we need to send a dummy function task to the owning thread just to','line_number':596,'multiline':False]['text':' ensure that it's not sleeping, so that we can exit the thread_main.','line_number':597,'multiline':False]['text':' If it has work, it might see that graph_task->outstanding_tasks_ == 0','line_number':598,'multiline':False]['text':' before it gets to the task, but it's a no-op anyway.','line_number':599,'multiline':False]['text':'','line_number':600,'multiline':False]['text':' NB: This is not necessary if the current thread is the owning thread.','line_number':601,'multiline':False]['text':' Synchronize outstanding_tasks_ with queue mutex','line_number':603,'multiline':False]['text':' Reentrant call will re-use the graph_task's owner thread ready_queue for','line_number':612,'multiline':False]['text':' queueing tasks (NOTE: this is not true in the async_mode of the engine).','line_number':613,'multiline':False]['text':' While we can create separate ready queue for each new reentrant','line_number':614,'multiline':False]['text':' thread, but sharing the same cpu_ready_queue with parent thread is a','line_number':615,'multiline':False]['text':' performance improvement and cuda thread still have to do the same thing.','line_number':616,'multiline':False]['text':' set the local_ready_queue to the ready queue on the graph_task->owner_','line_number':636,'multiline':False]['text':' device','line_number':637,'multiline':False]['text':' NOLINTNEXTLINE(performance-unnecessary-value-param)','line_number':646,'multiline':False]['text':' Allow only one thread one attempt to process this logic.','line_number':659,'multiline':False]['text':' Future is already marked complete, or being marked as such.','line_number':661,'multiline':False]['text':' In case the marking complete is only in progress, we add a','line_number':662,'multiline':False]['text':' wait() to guarantee the future is marked complete on exit.','line_number':663,'multiline':False]['text':' Run post processing, before marking the future as complete.','line_number':669,'multiline':False]['text':' Drop lock prior to completing, to avoid holding across callbacks.','line_number':670,'multiline':False]['text':' Need to unlock before we call markCompleted to avoid holding locks','line_number':676,'multiline':False]['text':' when the callbacks are called.','line_number':677,'multiline':False]['text':' set the thread_local current_graph_task_ as more callbacks can be installed','line_number':690,'multiline':False]['text':' by existing final callbacks.','line_number':691,'multiline':False]['text':' Lock mutex during each iteration for accessing final_callbacks.size()','line_number':693,'multiline':False]['text':' Unlocking is necessary, because the callback can register','line_number':694,'multiline':False]['text':' more callbacks (or they can be registered from other threads','line_number':695,'multiline':False]['text':' while it's waiting.','line_number':696,'multiline':False]['text':' caller_current_streams_ with nullopt entries removed','line_number':699,'multiline':False]['text':' See Note [Streaming backwards].','line_number':702,'multiline':False]['text':' Syncs caller_current_stream with leaf streams, so final_callbacks may use','line_number':703,'multiline':False]['text':' any grad on its device's current stream.','line_number':704,'multiline':False]['text':' stash_current_streams() stashed streams for all device IDs that already','line_number':707,'multiline':False]['text':' had a CUDA context before the GraphTask executed. For inactive devices,','line_number':708,'multiline':False]['text':' it stashed a c10::nullopt. I don't expect GraphTask's backward pass ran','line_number':709,'multiline':False]['text':' leaf nodes on any new devices, so the stashed streams should be enough.','line_number':710,'multiline':False]['text':' If leaf_stream.device_index() happens to be for a new device,','line_number':711,'multiline':False]['text':' operator* on the c10::nullopt should throw an error.','line_number':712,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':714,'multiline':False]['text':' final_callbacks run on the per-device caller_current_streams (the ambient','line_number':733,'multiline':False]['text':' streams surrounding the user's call to backward()). This has two','line_number':734,'multiline':False]['text':' benefits:','line_number':735,'multiline':False]['text':'  1. caller_current_streams have been synced with leaf_streams, so','line_number':736,'multiline':False]['text':'  callbacks may','line_number':737,'multiline':False]['text':'     safely access any grad.','line_number':738,'multiline':False]['text':'  2. The callback's results can safely be used on (user-facing)','line_number':739,'multiline':False]['text':'  caller_current_streams','line_number':740,'multiline':False]['text':'     after backward().','line_number':741,'multiline':False]['text':' Set the ThreadLocalState before calling the function.','line_number':744,'multiline':False]['text':' NB: The ThreadLocalStateGuard doesn't set the grad_mode because GraphTask','line_number':745,'multiline':False]['text':' always saves ThreadLocalState without grad_mode.','line_number':746,'multiline':False]['text':' WARNING: Don't use a range-for loop here because more callbacks may be','line_number':749,'multiline':False]['text':' added in between callback calls, so iterators may become invalidated.','line_number':750,'multiline':False]['text':' NOLINTNEXTLINE(modernize-loop-convert)','line_number':751,'multiline':False]['text':' NB: We MUST NOT construct the guard for device CPU,','line_number':805,'multiline':False]['text':' as in some settings we compile with cuda, but','line_number':806,'multiline':False]['text':' have lazy stubs for CUDA functionality (so actually','line_number':807,'multiline':False]['text':' attempting to setup a guard(CPU_DEVICE) will cause an','line_number':808,'multiline':False]['text':' error, because it will still query GetDevice).','line_number':809,'multiline':False]['text':'','line_number':810,'multiline':False]['text':' Don't use DeviceGuard here because its destructor may be called before the','line_number':811,'multiline':False]['text':' device is reset. This is fine because the device is thread local.','line_number':812,'multiline':False]['text':' FIXME: TestJit.test_ge_optimized fails this assertion.','line_number':845,'multiline':False]['text':' std::stringstream ss;','line_number':846,'multiline':False]['text':' ss << "undefined gradient at index " << i;','line_number':847,'multiline':False]['text':' TORCH_CHECK(false, format_error(ss.str()));','line_number':848,'multiline':False]['text':' TODO: Currently we only support (*, Sparse) combination for','line_number':879,'multiline':False]['text':' (tensor.layout(), tensor.grad.layout()) In future, there will be an','line_number':880,'multiline':False]['text':' opportunity to support more combinations of layouts if they are','line_number':881,'multiline':False]['text':' composable (example., operations like addition etc., are well defined','line_number':882,'multiline':False]['text':' between tensors of different layouts.), as well as all parts of','line_number':883,'multiline':False]['text':' autograd like AccumulateGrad correctly handle this. We allow grad to be','line_number':884,'multiline':False]['text':' Strided when metadata is SparseCsr','line_number':885,'multiline':False]['text':' quick hack for: https://github.com/pytorch/pytorch/issues/65016 but','line_number':898,'multiline':False]['text':' should be eventually removed','line_number':899,'multiline':False]['text':' We should not build graph for Tensors that are not differentiable','line_number':912,'multiline':False]['text':' In functions/accumulate_grad.cpp, there is some logic to check the','line_number':934,'multiline':False]['text':' conditions under which the incoming gradient can be stolen directly','line_number':935,'multiline':False]['text':' (which elides a deep copy) instead of cloned. One of these conditions','line_number':936,'multiline':False]['text':' is that the incoming gradient's refcount must be 1 (nothing else is','line_number':937,'multiline':False]['text':' referencing the same data).  Stashing inputs_copy here bumps the','line_number':938,'multiline':False]['text':' refcount, so if post hooks are employed, it's actually still ok for','line_number':939,'multiline':False]['text':' accumulate_grad.cpp to steal the gradient if the refcount is 2.','line_number':940,'multiline':False]['text':'','line_number':941,'multiline':False]['text':' "new_grad.use_count() <= 1 + !post_hooks().empty()" in','line_number':942,'multiline':False]['text':' accumulate_grad.cpp accounts for this, but also creates a silent','line_number':943,'multiline':False]['text':' dependency between engine.cpp (ie, this particular engine','line_number':944,'multiline':False]['text':' implementation) and accumulate_grad.cpp.','line_number':945,'multiline':False]['text':'','line_number':946,'multiline':False]['text':' If you change the logic here, make sure it's compatible with','line_number':947,'multiline':False]['text':' accumulate_grad.cpp.','line_number':948,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-use-after-move)','line_number':962,'multiline':False]['text':' The InputBuffer::adds that supplied incoming grads took pains to','line_number':973,'multiline':False]['text':' ensure they're safe to consume in the context of the present','line_number':974,'multiline':False]['text':' func's stream (if applicable). So we guard onto that stream','line_number':975,'multiline':False]['text':' before working with the grads in any capacity.','line_number':976,'multiline':False]['text':' If exec_info_ is not empty, we have to instrument the execution','line_number':980,'multiline':False]['text':' We always want to call tensor pre-hooks, but want to avoid calling it','line_number':986,'multiline':False]['text':' twice. needed_ = True indicates that we will call tensor pre-hooks','line_number':987,'multiline':False]['text':' later.','line_number':988,'multiline':False]['text':'','line_number':989,'multiline':False]['text':' See NOTE [Hooks ordering] for more context.','line_number':990,'multiline':False]['text':' Lock mutex for writing to graph_task->captured_vars_.','line_number':996,'multiline':False]['text':' NOTE [Deprecated capture hooks]','line_number':1001,'multiline':False]['text':' No need to take graph_task->mutex_ here, we already hold it','line_number':1007,'multiline':False]['text':' Skip execution if we don't need to execute the function.','line_number':1013,'multiline':False]['text':' Note: doesn't acquire the mutex','line_number':1026,'multiline':False]['text':' Records leaf stream (if applicable)','line_number':1027,'multiline':False]['text':' See Note [Streaming backwards]','line_number':1028,'multiline':False]['text':' Lock mutex for the accesses to GraphTask dependencies_, not_ready_ and','line_number':1050,'multiline':False]['text':' cpu_ready_queue_ below','line_number':1051,'multiline':False]['text':' Check if the next function is ready to be computed','line_number':1060,'multiline':False]['text':' Skip functions that aren't supposed to be executed','line_number':1076,'multiline':False]['text':' No buffers have been allocated for the function','line_number':1083,'multiline':False]['text':' Accumulates into buffer','line_number':1086,'multiline':False]['text':' The function already has a buffer','line_number':1099,'multiline':False]['text':' Accumulates into buffer','line_number':1102,'multiline':False]['text':' Computes the mininum topological number among all the outputs','line_number':1117,'multiline':False]['text':' Computes the number of dependencies for each function which requires grad','line_number':1133,'multiline':False]['text':' Queue contains all nodes that will start propagating gradients.','line_number':1138,'multiline':False]['text':' We no longer have to expand functions that don't require grad.','line_number':1139,'multiline':False]['text':' Collects current streams for devices where this process has a context,','line_number':1161,'multiline':False]['text':' so GraphTask::exec_post_processing can sync them with leaf_streams.','line_number':1162,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':1176,'multiline':False]['text':' Allows us to assert no other threads are in backwards','line_number':1188,'multiline':False]['text':' accumulate_grad is true if and only if the frontend call was to','line_number':1193,'multiline':False]['text':' grad(), not backward(). grad() returns the sum of the gradients','line_number':1194,'multiline':False]['text':' w.r.t. the inputs and thus needs the inputs to be present.','line_number':1195,'multiline':False]['text':' A fresh first time Engine::execute call should start on the CPU device,','line_number':1199,'multiline':False]['text':' initialize a new thread local ready queue on CPU or reuse the existing one','line_number':1200,'multiline':False]['text':' (if there is one allocated already, i.e. consecutive backward calls,','line_number':1201,'multiline':False]['text':' re-entrant backward calls), then memoize the local_ready_queue in GraphTask','line_number':1202,'multiline':False]['text':' Store root nodes so we can traverse through the graph later','line_number':1206,'multiline':False]['text':' e.g., for get_current_graph_task_execution_order','line_number':1207,'multiline':False]['text':' keep_graph ','line_number':1214,'multiline':True]['text':' create_graph ','line_number':1215,'multiline':True]['text':' depth ','line_number':1216,'multiline':True]['text':' cpu_ready_queue ','line_number':1217,'multiline':True]['text':' graph_roots ','line_number':1218,'multiline':True]['text':' If we receive a single root, skip creating extra root node','line_number':1220,'multiline':False]['text':' Now compute the dependencies for all executable functions','line_number':1227,'multiline':False]['text':' see [Note: Compiled Autograd]','line_number':1236,'multiline':False]['text':' Queue the root','line_number':1247,'multiline':False]['text':' Avoid a refcount bump for the Future, since we check for refcount in','line_number':1267,'multiline':False]['text':' DistEngine (see TORCH_INTERNAL_ASSERT(futureGrads.use_count() == 1)','line_number':1268,'multiline':False]['text':' in dist_engine.cpp).','line_number':1269,'multiline':False]['text':' Lock mutex for GraphTask.','line_number':1290,'multiline':False]['text':' worker_device == NO_DEVICE it's a CPU thread and it's trying to drive the','line_number':1295,'multiline':False]['text':' autograd engine with corresponding GraphTask, and its NOT a re-entrant call','line_number':1296,'multiline':False]['text':' We set the worker_device to CPU_DEVICE only if worker_device was','line_number':1298,'multiline':False]['text':' previously NO_DEVICE. Setting it to CPU afterwards allow us to detect','line_number':1299,'multiline':False]['text':' whether this is a re-entrant call or not.','line_number':1300,'multiline':False]['text':' set the graph_task owner to the current device','line_number':1303,'multiline':False]['text':' Now that all the non-thread safe fields of the graph_task have been','line_number':1306,'multiline':False]['text':' populated, we can enqueue it.','line_number':1307,'multiline':False]['text':' The owning thread start to drive the engine execution for any CPU task','line_number':1311,'multiline':False]['text':' that was just pushed or will be added later from other worker threads','line_number':1312,'multiline':False]['text':' reset the worker_device after the completion of the graph_task, this is','line_number':1316,'multiline':False]['text':' so that the initial state of the engine remains the same across every','line_number':1317,'multiline':False]['text':' backward() or grad() call, we don't need to reset local_ready_queue as we','line_number':1318,'multiline':False]['text':' could possibly reuse it for new backward calls.','line_number':1319,'multiline':False]['text':' If worker_device is any devices (i.e. CPU, CUDA): this is a re-entrant','line_number':1322,'multiline':False]['text':'    backward call from that device.','line_number':1323,'multiline':False]['text':' Now that all the non-thread safe fields of the graph_task have been','line_number':1326,'multiline':False]['text':' populated, we can enqueue it.','line_number':1327,'multiline':False]['text':' See Note [Reentrant backwards]','line_number':1332,'multiline':False]['text':' If reached the max depth, switch to a different thread','line_number':1333,'multiline':False]['text':' Total depth needs to be updated only in this codepath, since it is','line_number':1336,'multiline':False]['text':' not used in the block above (when we call add_thread_pool_task).','line_number':1337,'multiline':False]['text':' In the codepath above, GraphTask.reentrant_depth_ is used to','line_number':1338,'multiline':False]['text':' bootstrap total_depth in the other thread.','line_number':1339,'multiline':False]['text':' Get back to work while we wait for our new graph_task to','line_number':1342,'multiline':False]['text':' complete!','line_number':1343,'multiline':False]['text':' The graph task should have completed and the associated future should','line_number':1350,'multiline':False]['text':' be marked completed as well since 'thread_main' above is a call','line_number':1351,'multiline':False]['text':' blocking an autograd engine thread.','line_number':1352,'multiline':False]['text':' graph_task_exec_post_processing is done when the Future is marked as','line_number':1356,'multiline':False]['text':' completed in mark_as_completed_and_run_post_processing.','line_number':1357,'multiline':False]['text':' note that when python is present, this base engine will be overriden','line_number':1361,'multiline':False]['text':' with a PythonEngine. Because this typically happens before get_default_engine','line_number':1362,'multiline':False]['text':' is called, this base engine will never be created.','line_number':1363,'multiline':False]['text':' if ready_queue provided in the caller, use the caller's ready_queue to','line_number':1405,'multiline':False]['text':' initialize local_ready_queue','line_number':1406,'multiline':False]['text':' otherwise if local_ready_queue not allocated, allocate a new ready_queue','line_number':1409,'multiline':False]['text':' CPU ready queue is per GraphTask, but CUDA device ready queues are shared','line_number':1414,'multiline':False]['text':' across all graph tasks','line_number':1415,'multiline':False]['text':' return the cpu ready queue passed in','line_number':1422,'multiline':False]['text':' See Note [Allocating GPUs to autograd threads]','line_number':1430,'multiline':False]['text':' return the cpu ready queue passed in','line_number':1439,'multiline':False]['text':' See Note [Allocating GPUs to autograd threads]','line_number':1447,'multiline':False]['text':' NB: This function would become obsolete if we truly allocated a CPU','line_number':1448,'multiline':False]['text':' thread per device, rather than colocate.','line_number':1449,'multiline':False]['text':' First always initialize the thread pool for re-entrant threads','line_number':1455,'multiline':False]['text':' Second, create special threads for each non-CPU device','line_number':1458,'multiline':False]['text':' See Note [Allocating GPUs to autograd threads]','line_number':1459,'multiline':False]['text':' Only record the number of devices for device that don't run on the','line_number':1463,'multiline':False]['text':' cpu ready queue.','line_number':1464,'multiline':False]['text':' If there are no device except cpu, no need to create worker threads','line_number':1470,'multiline':False]['text':' Since we're about to create threads, forking is not possible anymore','line_number':1475,'multiline':False]['text':' allocate one thread for every GPU device (but colocate GPUs of different','line_number':1478,'multiline':False]['text':' types), and pre-allocate the device_ready_queues_ to ensure safe reading on','line_number':1479,'multiline':False]['text':' it.','line_number':1480,'multiline':False]['text':' Wait for the threads to start','line_number':1490,'multiline':False]['text':' There may already be some items on the graphtasks_queue_ added by other','line_number':1502,'multiline':False]['text':' threads but not enough workers to get to the new task that will be','line_number':1503,'multiline':False]['text':' added','line_number':1504,'multiline':False]['text':' Don't need to be holding the lock while actually creating the thread','line_number':1509,'multiline':False]['text':' If we're creating a new thread, forking is not allowed anymore','line_number':1512,'multiline':False]['text':' This works even if new thread is created because wait() will test the','line_number':1517,'multiline':False]['text':' predicate before waiting','line_number':1518,'multiline':False]['text':' Remembers current streams on all devices where a context has been created.','line_number':1522,'multiline':False]['text':' Only called if Engine::execute detects at least one node runs on a cuda','line_number':1523,'multiline':False]['text':' stream.','line_number':1524,'multiline':False]['text':' If the build targets ROCM, stash streams for all visible devices','line_number':1532,'multiline':False]['text':' unconditionally, to work around','line_number':1533,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/59750.','line_number':1534,'multiline':False]['text':' TODO: Remove ROCM-specific behavior when','line_number':1535,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/59750 is fixed.','line_number':1536,'multiline':False]['text':' Populates exec_info so nodes that should be executed have','line_number':1555,'multiline':False]['text':' `exec_info[node].needed_ = true` Only nodes that have a path to any edge in','line_number':1556,'multiline':False]['text':' `outputs` should be executed. The code below populates exec_info using','line_number':1557,'multiline':False]['text':' recursion, but the actual code does this iteratively. Refer to the','line_number':1558,'multiline':False]['text':' numbering to see how the actual code corresponds. A difference to note is','line_number':1559,'multiline':False]['text':' that in the iterative version, when you are working with the current Node,','line_number':1560,'multiline':False]['text':' you are responsible to update your parent's is_needed after all your','line_number':1561,'multiline':False]['text':' children have been updated.','line_number':1562,'multiline':False]['text':'','line_number':1563,'multiline':False]['text':' is_needed = {fn: True for fn in outputs}             # (0)','line_number':1564,'multiline':False]['text':' seen = {}','line_number':1565,'multiline':False]['text':' def compute_is_needed(fn):','line_number':1566,'multiline':False]['text':'   for next_edge in fn.next_edges:','line_number':1567,'multiline':False]['text':'     child_fn = next_edge.fn','line_number':1568,'multiline':False]['text':'     if child_fn in seen and is_needed[child_fn]:     # (1)','line_number':1569,'multiline':False]['text':'       is_needed[fn] = true','line_number':1570,'multiline':False]['text':'     else:','line_number':1571,'multiline':False]['text':'       seen.add(child_fn)','line_number':1572,'multiline':False]['text':'       if compute_is_needed(child_fn):','line_number':1573,'multiline':False]['text':'         is_needed[fn] = true                         # (2)','line_number':1574,'multiline':False]['text':'                                                      # (3) exit for-loop','line_number':1575,'multiline':False]['text':'   return is_needed[fn]','line_number':1576,'multiline':False]['text':' compute_is_needed(graph_root)','line_number':1577,'multiline':False]['text':'','line_number':1578,'multiline':False]['text':' NB: you might be wondering why we don't populate `seen` with outputs. We','line_number':1579,'multiline':False]['text':' cannot because in the case where two outputs lie on the same path, we still','line_number':1580,'multiline':False]['text':' need to explore past the first output or we would miss the nodes that are','line_number':1581,'multiline':False]['text':' required to compute the second output.','line_number':1582,'multiline':False]['text':' (0) `is_needed` above corresponds to `exec_info_[fn].needed_`','line_number':1585,'multiline':False]['text':' if called through `.backward()` we directly set `needed_` for all the','line_number':1589,'multiline':False]['text':' outputs to true','line_number':1590,'multiline':False]['text':' otherwise it is `.grad()` and we set exec_info[fn].captures_ instead','line_number':1593,'multiline':False]['text':' In terms of populating the rest of exec_info though, you can basically','line_number':1594,'multiline':False]['text':' think of this as the same as setting `needed_` is true directly.','line_number':1595,'multiline':False]['text':' (1) next child exists AND has already been seen','line_number':1637,'multiline':False]['text':' (2) next child exists but has not been seen','line_number':1644,'multiline':False]['text':' child created before the first output means this child cannot have','line_number':1646,'multiline':False]['text':' an edge to output','line_number':1647,'multiline':False]['text':' (3) no next child exists for `fn` means its `needed` has already been','line_number':1652,'multiline':False]['text':' finalized. pop stack and update parent','line_number':1653,'multiline':False]['text':' namespace autograd','line_number':1662,'multiline':False]['text':' namespace torch','line_number':1663,'multiline':False]