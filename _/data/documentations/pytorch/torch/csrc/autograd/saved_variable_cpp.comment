['text':' Note [Inference tensor cannot be saved for backward]','line_number':25,'multiline':False]['text':' Invariant:','line_number':26,'multiline':False]['text':'   You can't save an inference tensor for backwards.','line_number':27,'multiline':False]['text':' If an inference tensor was saved for backward in an autograd session and','line_number':28,'multiline':False]['text':' then you reenter inference mode and make an inplace update to the tensor','line_number':29,'multiline':False]['text':' without bumping version_counter, it'll lead to silent wrong result when','line_number':30,'multiline':False]['text':' you do backward() for the previous autograd session.  Technically we','line_number':31,'multiline':False]['text':' don't have to check here since it'll fail when querying `current_version`','line_number':32,'multiline':False]['text':' on the inference tensor, but we can give a much better error message','line_number':33,'multiline':False]['text':' here.','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' Note in the documentation we say "inference tensor cannot participate','line_number':36,'multiline':False]['text':' in autograd" which is more restrictive than the invariant.  In practice','line_number':37,'multiline':False]['text':' the check is more permissive and only error out when an inference tensor','line_number':38,'multiline':False]['text':' is saved for backward.  Whether a tensor is saved for backward is','line_number':39,'multiline':False]['text':' determined by derivative formula and thus varies op by op, so by saying','line_number':40,'multiline':False]['text':' "no inference tensor in autograd" it's easier for users to understand and','line_number':41,'multiline':False]['text':' follow.','line_number':42,'multiline':False]['text':' Avoid wrapped numbers from being leaked to the user','line_number':62,'multiline':False]['text':' If the variable is a leaf or is not an output, we can safely save the','line_number':69,'multiline':False]['text':' original variable without running the risk of reference cycles.','line_number':70,'multiline':False]['text':' 1. If the variable is not an output, its grad_fn has already been fully','line_number':71,'multiline':False]['text':' created and in particular will be a different Node than the one','line_number':72,'multiline':False]['text':' we are currently constructing (the one that owns this SavedVariable).','line_number':73,'multiline':False]['text':' 2. If the variable is a leaf, it only has weak reference to the','line_number':74,'multiline':False]['text':' grad_accumulator which cannot create a cycle. In those cases, we save the','line_number':75,'multiline':False]['text':' original variable and don't need further processing.','line_number':76,'multiline':False]['text':' Only do this if we actually need to.','line_number':85,'multiline':False]['text':' Save output number, version counter and fw_grad if needed','line_number':91,'multiline':False]['text':' TODO(albanD) This needs to be updated when moving to multiple levels','line_number':103,'multiline':False]['text':' level ','line_number':104,'multiline':True]['text':' level ','line_number':107,'multiline':True]['text':' We want grad_fn here to provide the most helpful debug message to the user','line_number':139,'multiline':False]['text':' if versions don't match','line_number':140,'multiline':False]['text':' This issue was introduced when we added logic to save the original','line_number':147,'multiline':False]['text':' because now we rely on data_.grad_fn(), but can be unreliable if the','line_number':148,'multiline':False]['text':' autograd_meta of that saved tensor is cleared with an in-place detach.','line_number':149,'multiline':False]['text':' As a simple fix, we choose to disallow that behavior here even though','line_number':150,'multiline':False]['text':' it makes behavior inconsistent depending on whether you are saving','line_number':151,'multiline':False]['text':' input or output.','line_number':152,'multiline':False]['text':' Only check version counter in the case without hooks','line_number':160,'multiline':False]['text':' If user provides hooks, we can't track versions through the hooks','line_number':161,'multiline':False]['text':' The version counter is correct.','line_number':198,'multiline':False]['text':' Additionally, if we deal with a non-leaf variable, we have its correct','line_number':199,'multiline':False]['text':' grad_fn.','line_number':200,'multiline':False]['text':' If we have the original variable, we simply return it','line_number':202,'multiline':False]['text':' NB: saved views are unpacked as normal Variables (not views) even though','line_number':209,'multiline':False]['text':' they still share the same storage. This works only because we never call','line_number':210,'multiline':False]['text':' in-place functions on unpacked variables.','line_number':211,'multiline':False]['text':' NB: var here is never a view so there is no need to make anything special','line_number':222,'multiline':False]['text':' for the case where the saved Tensor was a view. This whole argument relies','line_number':223,'multiline':False]['text':' on the fact that the Tensor returned by this function is never','line_number':224,'multiline':False]['text':' modified in-place.','line_number':225,'multiline':False]['text':' TODO(albanD) This needs to be updated when moving to multiple levels','line_number':227,'multiline':False]['text':' level ','line_number':228,'multiline':True]['text':' level ','line_number':229,'multiline':True]['text':' is_inplace_op ','line_number':229,'multiline':True]['text':' If we didn't save the original variable, we already saved metadata','line_number':273,'multiline':False]['text':' namespace autograd','line_number':288,'multiline':False]['text':' namespace torch','line_number':289,'multiline':False]