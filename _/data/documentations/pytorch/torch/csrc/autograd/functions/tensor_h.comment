['text':' Note [View + Inplace update for base tensor]','line_number':27,'multiline':False]['text':'','line_number':28,'multiline':False]['text':' This note covers a few important topics related to view + inplace handling.','line_number':29,'multiline':False]['text':'   - It explains what is the CopySlices Node and why we need it.','line_number':30,'multiline':False]['text':'   - It explains the considerations on what is saved for backward in','line_number':31,'multiline':False]['text':'   CopySlices.','line_number':32,'multiline':False]['text':'   - It explains why we need to sometimes change the exec_info of the current','line_number':33,'multiline':False]['text':'   backward','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' What is CopySlices?','line_number':36,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' We support autograd with inplace mutation; e.g., if you write x.mul_(2)','line_number':39,'multiline':False]['text':' the autograd will work as if you now had multiple Tensors under the hood and','line_number':40,'multiline':False]['text':' you did','line_number':41,'multiline':False]['text':'   x = t.clone()','line_number':42,'multiline':False]['text':'   x0 = x','line_number':43,'multiline':False]['text':'   x1 = x0 * 2','line_number':44,'multiline':False]['text':'   x = x1','line_number':45,'multiline':False]['text':' As you can see here, after this operation, x.grad_fn now points to x1.grad_fn','line_number':46,'multiline':False]['text':' (the MulBackward node) and this node points to x's original grad_fn (which is','line_number':47,'multiline':False]['text':' also x0.grad_fn). It is important to keep in mind that after the inplace,','line_number':48,'multiline':False]['text':' there is no Tensor object that represents the x0 state anymore. But the graph','line_number':49,'multiline':False]['text':' for it is still around in autograd (in case x was used before being modified','line_number':50,'multiline':False]['text':' inplace). See Example 1 in','line_number':51,'multiline':False]['text':' https://docs.google.com/drawings/d/1-T5DyYfChMX1ONQkY-zU-hj_ayQ2zmA5CBOKDWqvEhE','line_number':52,'multiline':False]['text':' We call this rebasing the history of the Tensor.','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' Now, a difficult situation is what happens if x is a differentiable view','line_number':55,'multiline':False]['text':' of a base b.','line_number':56,'multiline':False]['text':'   b = t.clone()','line_number':57,'multiline':False]['text':'   x = b.select(0, 0)','line_number':58,'multiline':False]['text':'   x *= 2','line_number':59,'multiline':False]['text':' With the same approach as above, this will become','line_number':60,'multiline':False]['text':'   b = t.clone()','line_number':61,'multiline':False]['text':'   x = b.select(0, 0)','line_number':62,'multiline':False]['text':'   b0 = b','line_number':63,'multiline':False]['text':'   x0 = x','line_number':64,'multiline':False]['text':'   x1 = x0 * 2','line_number':65,'multiline':False]['text':'   b1 = b0.select_scatter(x1, 0, 0)','line_number':66,'multiline':False]['text':'   x2 = b1.select(0, 0)','line_number':67,'multiline':False]['text':'   x = x2','line_number':68,'multiline':False]['text':'   b = b1','line_number':69,'multiline':False]['text':' As you can see here, not only we need to modify x's grad_fn, we also need to','line_number':70,'multiline':False]['text':' modify the one from b. We also need to ensure that the new grad_fn on x is','line_number':71,'multiline':False]['text':' linked to b's new grad_fn. The chain the select_scatter, multiplication and','line_number':72,'multiline':False]['text':' select is what CopySlices does, all wrapped into a single Node.','line_number':73,'multiline':False]['text':'','line_number':74,'multiline':False]['text':' See Example 1 in','line_number':75,'multiline':False]['text':' https://docs.google.com/drawings/d/1-T5DyYfChMX1ONQkY-zU-hj_ayQ2zmA5CBOKDWqvEhE','line_number':76,'multiline':False]['text':'','line_number':77,'multiline':False]['text':' What do we need to save in CopySlices to run backward?','line_number':78,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':79,'multiline':False]['text':'','line_number':80,'multiline':False]['text':' We need to perform grad_view = fn(grad_view), but out-of-place.','line_number':81,'multiline':False]['text':' view_fn_ is an optional lambda function saved in DifferentiableViewMeta','line_number':82,'multiline':False]['text':' from forward pass, so that we can recover we when as_strided is not','line_number':83,'multiline':False]['text':' supported. It preserves the invariants:','line_number':84,'multiline':False]['text':'   view = view_fn_(base)','line_number':85,'multiline':False]['text':'   grad_view = view_fn_(grad_base)','line_number':86,'multiline':False]['text':'','line_number':87,'multiline':False]['text':' When as_strided is supported (e.g. strided CPU/CUDA Tensors), view_fn_','line_number':88,'multiline':False]['text':' is empty and we save TensorGeometry(view) instead.','line_number':89,'multiline':False]['text':' With the TensorGeometry information we can use `as_strided` call which','line_number':90,'multiline':False]['text':' is more efficient to recover views in backward.','line_number':91,'multiline':False]['text':'','line_number':92,'multiline':False]['text':' For example:','line_number':93,'multiline':False]['text':'   view_1 = view_op_1(base)','line_number':94,'multiline':False]['text':'   view_2 = view_op_2(view_1)','line_number':95,'multiline':False]['text':'   ...','line_number':96,'multiline':False]['text':'   view_n = view_op_n(view_n-1)','line_number':97,'multiline':False]['text':'   view_n = inplace_op(view_n)','line_number':98,'multiline':False]['text':'','line_number':99,'multiline':False]['text':' In CPU/CUDA case where we support efficient as_strided implementation,','line_number':100,'multiline':False]['text':' grad_view_n can be calculated through 1 step.','line_number':101,'multiline':False]['text':'','line_number':102,'multiline':False]['text':'   grad_view_n = grad_base.as_strided(view_sizes, view_strides, view_offset);','line_number':103,'multiline':False]['text':'','line_number':104,'multiline':False]['text':' But in XLA backend where we don't have full support of as_strided,','line_number':105,'multiline':False]['text':' it has to save a chained lambda function view_fn_, to exactly','line_number':106,'multiline':False]['text':' replay how the view was done in forward.','line_number':107,'multiline':False]['text':'','line_number':108,'multiline':False]['text':'   view_fn_ = view_op_n(...(view_op_2(view_op_1())))','line_number':109,'multiline':False]['text':'   grad_view_n = view_fn_(grad_base)','line_number':110,'multiline':False]['text':'','line_number':111,'multiline':False]['text':' This chain view_fn_ works as long as forward view ops are implemented,','line_number':112,'multiline':False]['text':' e.g XLA simulates view without a real Storage behind Tensor, but it's less','line_number':113,'multiline':False]['text':' efficient than the as_strided one so we should be careful to only use it when','line_number':114,'multiline':False]['text':' necessary.','line_number':115,'multiline':False]['text':'','line_number':116,'multiline':False]['text':'   - For CPU/CUDA we save TensorGeometry of both base and view tensors,','line_number':117,'multiline':False]['text':'     That's all we need to pass into as_strided.','line_number':118,'multiline':False]['text':'     E.g. int[] sizes, int[] strides, and int storage_offset.','line_number':119,'multiline':False]['text':'   - For XLA we use view_fn_, which captures all forward view op arguments','line_number':120,'multiline':False]['text':'     by **value**.','line_number':121,'multiline':False]['text':'     E.g for at::narrow, int dim, int start, in length are saved.','line_number':122,'multiline':False]['text':'','line_number':123,'multiline':False]['text':' Theoretically we could also save Tensor `view` in CopySlices Node, but','line_number':124,'multiline':False]['text':' it's far more expensive than what we currently save.','line_number':125,'multiline':False]['text':'   1. We cannot afford keeping large tensors alive to recover views only.','line_number':126,'multiline':False]['text':'   2. There are inplace checks when Tensors are loaded back to make sure','line_number':127,'multiline':False]['text':'      they haven't been changed (including size metadata).','line_number':128,'multiline':False]['text':' So saving metadata like TensorGeometry/view arguments is much better','line_number':129,'multiline':False]['text':' because it is minimal information needed to recover views, as well as it','line_number':130,'multiline':False]['text':' allows the user to modify the original Tensor without preventing the','line_number':131,'multiline':False]['text':' backward pass from running.','line_number':132,'multiline':False]['text':'','line_number':133,'multiline':False]['text':' Why do we manually change exec_info in the apply?','line_number':134,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':135,'multiline':False]['text':'','line_number':136,'multiline':False]['text':' Using the same example as before,','line_number':137,'multiline':False]['text':'   b = t.clone()','line_number':138,'multiline':False]['text':'   x = b.select(0, 0)','line_number':139,'multiline':False]['text':'   x *= y','line_number':140,'multiline':False]['text':'','line_number':141,'multiline':False]['text':' You can see the visualization at','line_number':142,'multiline':False]['text':' https://docs.google.com/drawings/d/1Bx-Hcz-zlIv7PabQqnPhUIVIs9F8WWi48svqMsAUMFs','line_number':143,'multiline':False]['text':' which contains the wrapped MulBackward Node and show what it links to.','line_number':144,'multiline':False]['text':' Since a backward can happen between any subset of the inputs (t and y) and','line_number':145,'multiline':False]['text':' outputs (o, x, b). It is possible to get into a state where CopySlices's 0th','line_number':146,'multiline':False]['text':' next function (CloneBackward) needs gradient but MulBackward's 0th next','line_number':147,'multiline':False]['text':' function (SelectBackward) is not. This happens if you do autograd.grad','line_number':148,'multiline':False]['text':' between x and t for example.','line_number':149,'multiline':False]['text':' In such a case, we do need to mark SelectBackward as requiring gradient such','line_number':150,'multiline':False]['text':' that, during the execution of MulBackward, we will actually compute gradient','line_number':151,'multiline':False]['text':' for the 0th input.','line_number':152,'multiline':False]['text':'','line_number':153,'multiline':False]['text':' All the other next functions are always shared (this is asserted in the apply','line_number':154,'multiline':False]['text':' code) and so nothing needs to be done for them.','line_number':155,'multiline':False]['text':' See Note [View + Inplace update for view tensor] for what we do to view','line_number':157,'multiline':False]['text':' tensor when an in-place operation happens.','line_number':158,'multiline':False]['text':' common code between apply/apply_with_saved','line_number':166,'multiline':False]['text':' view and view_fn are redundant and view_fn will be used if available.','line_number':178,'multiline':False]['text':' See Note [View + Inplace update for base tensor] for details.','line_number':179,'multiline':False]['text':' namespace autograd','line_number':185,'multiline':False]['text':' namespace torch','line_number':186,'multiline':False]