['text':' Environment variable which controls whether we perform a NCCL healt check','line_number':30,'multiline':False]['text':' which ensures communicators are healthy at the beginning of init.','line_number':31,'multiline':False]['text':' Environment variable which controls whether or not wait() is blocking or','line_number':36,'multiline':False]['text':' non-blocking.','line_number':37,'multiline':False]['text':' Environment variable which controls whether or not we perform Async Error','line_number':42,'multiline':False]['text':' Handling with NCCL.','line_number':43,'multiline':False]['text':' Environment Variable to control whether dumping debug info on watchdog','line_number':48,'multiline':False]['text':' timeout is enabled. This variable must be set together with','line_number':49,'multiline':False]['text':' TORCH_NCCL_ENABLE_MONITORING=1 and TORCH_NCCL_TRACE_BUFFER_SIZE > 0.','line_number':50,'multiline':False]['text':' Environment Variable to control whether Desync Debug is enabled.','line_number':54,'multiline':False]['text':' This variable must be set together with TORCH_NCCL_ASYNC_ERROR_HANDLING.','line_number':55,'multiline':False]['text':' NoHandling: do not handle asynchronous NCCL errors','line_number':80,'multiline':False]['text':' TearDown: tear down process upon error, see `WorkNCCL::handleException`','line_number':81,'multiline':False]['text':' CleanUpOnly: just clean up collectives and abort communicators without','line_number':82,'multiline':False]['text':' tearing down process SkipCleanUp: (this is a temporary option and can be','line_number':83,'multiline':False]['text':' removed in future) tear down process without cleaning up NCCL communicators.','line_number':84,'multiline':False]['text':' This should be used as a last resort in case `ncclCommAbort` itself is','line_number':85,'multiline':False]['text':' hanging','line_number':86,'multiline':False]['text':' If set, ProcessGroupNCCL doesn't use recordStream calls to ensure','line_number':98,'multiline':False]['text':' caching allocator safety for tensors used on both user-facing and','line_number':99,'multiline':False]['text':' internal comm streams.','line_number':100,'multiline':False]['text':' Instead, it stashes live references to those tensors until after','line_number':101,'multiline':False]['text':' user-facing streams are synced with comm streams.','line_number':102,'multiline':False]['text':' See stashed_for_allocator_safety_ below.','line_number':103,'multiline':False]['text':' If set, ProcessGroupNCCL registers postAlloc and preFree hooks to cuda cache','line_number':107,'multiline':False]['text':' allocator so that whenever a tensor is allocated or freed, ProcessGroupNCCL','line_number':108,'multiline':False]['text':' can register/deregister the tensor on all available NCCL communicators.','line_number':109,'multiline':False]['text':' ProcessGroupNCCL implements NCCL bindings for c10d.','line_number':114,'multiline':False]['text':'','line_number':115,'multiline':False]['text':' All functions of the class are expected to be called in the same order','line_number':116,'multiline':False]['text':' across all processes in the process group.  This is the only way that we','line_number':117,'multiline':False]['text':' can guarantee to match up the same calls among all processes.','line_number':118,'multiline':False]['text':'','line_number':119,'multiline':False]['text':' All NCCL functions provided by this class are asynchronous functions. More','line_number':120,'multiline':False]['text':' specifically, each NCCL call is scheduled on a separate CUDA stream that is','line_number':121,'multiline':False]['text':' different from the current CUDA stream. This is for the purpose of','line_number':122,'multiline':False]['text':' achieving potentially concurrency and better performance. As a result,','line_number':123,'multiline':False]['text':' it is the callers' responsibility to make sure that the CUDA stream their','line_number':124,'multiline':False]['text':' code works on needs to wait for the NCCL operation from','line_number':125,'multiline':False]['text':' this class.','line_number':126,'multiline':False]['text':'','line_number':127,'multiline':False]['text':' This can be done by calling:','line_number':128,'multiline':False]['text':'','line_number':129,'multiline':False]['text':' either WorkNCCL::wait() or WorkNCCL::synchronize(), both achieves the same','line_number':130,'multiline':False]['text':' functionality and are synonyms.','line_number':131,'multiline':False]['text':'','line_number':132,'multiline':False]['text':' Also note that WorkNCCL::finishedGPUExecution() is a helper function only','line_number':133,'multiline':False]['text':' provided by ProcessGroupNCCL to check if the NCCL operation of WorkNCCL has','line_number':134,'multiline':False]['text':' finished execution on the GPU (not just scheduled).','line_number':135,'multiline':False]['text':'','line_number':136,'multiline':False]['text':' Example on using the NCCL process group','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':'   ProcessGroupNCCL pg(store, rank, size);','line_number':139,'multiline':False]['text':'   std::shared_ptr<WorkNCCL> work = pg.allreduce(tensors);','line_number':140,'multiline':False]['text':'','line_number':141,'multiline':False]['text':'   // At this point, NCCL kernel has already by queued successfully','line_number':142,'multiline':False]['text':'   // Now, let current stream wait for the NCCL to finish, this function is','line_number':143,'multiline':False]['text':'   // async operation as well','line_number':144,'multiline':False]['text':'','line_number':145,'multiline':False]['text':'   work->wait()','line_number':146,'multiline':False]['text':'','line_number':147,'multiline':False]['text':'   // Now continue on other work in the current stream.','line_number':148,'multiline':False]['text':' Constructor takes a list of CUDA devices','line_number':155,'multiline':False]['text':' Copy constructor doing partial copy without outputs_. Cleanup thread','line_number':165,'multiline':False]['text':' monitors and removes finished works. However it will deadlock when','line_number':166,'multiline':False]['text':' destructs outputs_ tensors who are view tensors in autograd graph.','line_number':167,'multiline':False]['text':' Checks if the NCCL kernel has started to execute.','line_number':172,'multiline':False]['text':' Checks if request has completed. In this specific case of NCCL, it checks','line_number':175,'multiline':False]['text':' if the NCCL operation has completed on the GPU in its own NCCL stream.','line_number':176,'multiline':False]['text':' Non-blocking operation.','line_number':177,'multiline':False]['text':' Same as calling synchronize() for NCCL work.','line_number':182,'multiline':False]['text':' Let current stream wait on the completing of the NCCL work','line_number':187,'multiline':False]['text':' Throws on exceptions. Blocking operation, which will wait for work','line_number':188,'multiline':False]['text':' completion.','line_number':189,'multiline':False]['text':' Synchronize streams by blocking each on the NCCL stream','line_number':192,'multiline':False]['text':' Helper function to handle exception (throw if needed).','line_number':195,'multiline':False]['text':' Helper function that checks if the NCCL kernels have finished','line_number':198,'multiline':False]['text':' execution on the GPUs','line_number':199,'multiline':False]['text':' Get a Future object that will be marked as completed internally.','line_number':202,'multiline':False]['text':' Helper function that sets an exception_ptr on the WorkNCCL object.','line_number':209,'multiline':False]['text':' Helper function that returns True if the WorkNCCL object has timed out','line_number':212,'multiline':False]['text':' and False otherwise.','line_number':213,'multiline':False]['text':' In case of timeout, set exception on the WorkNCCL object.','line_number':214,'multiline':False]['text':' The cached list of CUDA devices to operate on','line_number':221,'multiline':False]['text':' The start CUDA events of NCCL operator tracking this work item on','line_number':224,'multiline':False]['text':' multiple CUDA devices. These start CUDA events are needed by desync','line_number':225,'multiline':False]['text':' debugging if enabled.','line_number':226,'multiline':False]['text':' The end CUDA events of NCCL operator tracking this work item on','line_number':229,'multiline':False]['text':' multiple CUDA devices.','line_number':230,'multiline':False]['text':' The NCCL communicators used for this work item.','line_number':233,'multiline':False]['text':' Tensors used for barrier op','line_number':236,'multiline':False]['text':' Clone of blockingWait_ from ProcessGroupNCCL.','line_number':239,'multiline':False]['text':' Clone of avoidRecordStreams_ from ProcessGroupNCCL.','line_number':242,'multiline':False]['text':' Clone of opTimeout_ from ProcessGroupNCCL.','line_number':245,'multiline':False]['text':' Time point representing when the work started.','line_number':248,'multiline':False]['text':' Record the collective sequential number.','line_number':251,'multiline':False]['text':' Indicates if the nccl start event has been updated to the store trace.','line_number':254,'multiline':False]['text':' This will be used by desync debug.','line_number':255,'multiline':False]['text':' Record collective sizes for debug. We only record the size on the first','line_number':258,'multiline':False]['text':' device as multi-device per process is deprecated','line_number':259,'multiline':False]['text':' Wrapper method for the static checkForNCCLErrors which can be overridden','line_number':263,'multiline':False]['text':' for tests.','line_number':264,'multiline':False]['text':' Helper function for synchronize','line_number':273,'multiline':False]['text':' Checks for NCCL errors and sets an appropriate exception_ptr.','line_number':276,'multiline':False]['text':' Just checks whether GPU execution has started, without modifying','line_number':279,'multiline':False]['text':' exception_ptr.','line_number':280,'multiline':False]['text':' Just checks whether GPU execution has completed, without modifying','line_number':283,'multiline':False]['text':' exception_ptr.','line_number':284,'multiline':False]['text':' Reference to the store so that we can write aborted communicators','line_number':287,'multiline':False]['text':' to the store.','line_number':288,'multiline':False]['text':' Store a reference to NCCL collective's outputs, used by result and to','line_number':291,'multiline':False]['text':' give a more descriptive message when representing the Work as a string.','line_number':292,'multiline':False]['text':' TORCH_NCCL_AVOID_RECORD_STREAMS implementation helper.','line_number':295,'multiline':False]['text':' Stores references to participating non-output tensors (ie inputs,','line_number':296,'multiline':False]['text':' flattened intermediates).','line_number':297,'multiline':False]['text':' We'll clear this list in synchronizeStreams, just after user-facing','line_number':298,'multiline':False]['text':' stream(s) are synced with the nccl work stream(s).','line_number':299,'multiline':False]['text':' By keeping these refs (as well as outputs_) alive until after the','line_number':300,'multiline':False]['text':' collective's work rejoins the user-facing streams, we achieve','line_number':301,'multiline':False]['text':' caching allocator safety without any recordStream calls.','line_number':302,'multiline':False]['text':' For in-place collectives, some refs stashed here may alias outputs_,','line_number':303,'multiline':False]['text':' but that doesn't do any harm.','line_number':304,'multiline':False]['text':' The future returned by getFuture.','line_number':307,'multiline':False]['text':' unique id used to tell the trace buffer that this','line_number':311,'multiline':False]['text':' work has completed','line_number':312,'multiline':False]['text':' Constructor takes a list of WorkNCCL works','line_number':321,'multiline':False]['text':' Same as calling synchronize() for NCCL work.','line_number':329,'multiline':False]['text':' The cached list of CUDA devices to operate on','line_number':333,'multiline':False]['text':' NOTE: timeout in ProcessGroupNCCL::Options denote the timeout for','line_number':340,'multiline':False]['text':' operations. This is only used when blockingWait_ is enabled.','line_number':341,'multiline':False]['text':' return intrusive_ptr of the object','line_number':344,'multiline':False]['text':' Schedule NCCL operations on high priority CUDA streams','line_number':350,'multiline':False]['text':' Configure ranks','line_number':354,'multiline':False]['text':' Optional "parent" backend and color to create communicators from','line_number':358,'multiline':False]['text':' via `ncclCommSplit`','line_number':359,'multiline':False]['text':' If you wish to create multiple process groups, each with a potentially','line_number':365,'multiline':False]['text':' different rank and size, you can do so by passing a new store instance','line_number':366,'multiline':False]['text':' to each one. If you have only a single store object, you can','line_number':367,'multiline':False]['text':' use the `c10d::PrefixStore` to derive scoped instances.','line_number':368,'multiline':False]['text':' This is also what the Python API in torch.distributed does.','line_number':369,'multiline':False]['text':'','line_number':370,'multiline':False]['text':' The process group instance keeps a reference to the store because','line_number':371,'multiline':False]['text':' it may be used long after the constructor runs. In fact, the constructor','line_number':372,'multiline':False]['text':' doesn't create any NCCL communicators. A single NCCL communicator can','line_number':373,'multiline':False]['text':' only be used on a specific set of devices, and are therefore created','line_number':374,'multiline':False]['text':' on-demand when a collective runs. If another collective is executed later,','line_number':375,'multiline':False]['text':' against a different set of devices, the process group creates another NCCL','line_number':376,'multiline':False]['text':' communicator. These NCCL communicators are cached and reused if possible.','line_number':377,'multiline':False]['text':'','line_number':378,'multiline':False]['text':' This constructor includes the deprecated `groupName` argument.','line_number':385,'multiline':False]['text':' If you have existing code that uses the `groupName`, you can replace','line_number':386,'multiline':False]['text':' it by specifying a `c10d::PrefixStore(groupName, store)` for store.','line_number':387,'multiline':False]['text':' Unsupported Ops','line_number':511,'multiline':False]['text':' Agrees on an initial sequence number for the whole group by having rank 0','line_number':526,'multiline':False]['text':' create it and broadcast it to other ranks using the store.','line_number':527,'multiline':False]['text':' Retrieves the current sequence number for the whole group, which should be','line_number':530,'multiline':False]['text':' in sync. If the returned number is not consistent across the group, it','line_number':531,'multiline':False]['text':' may indicate that there is some sort of collective desynchronization.','line_number':532,'multiline':False]['text':' Return the total number of splits the communicators held by this process','line_number':535,'multiline':False]['text':' group have performed.','line_number':536,'multiline':False]['text':' Provide an API for users to define their own ways to store NCCL debug info.','line_number':545,'multiline':False]['text':' Provides an API to abort the ProcessGroup (similar to ncclCommAbort)','line_number':550,'multiline':False]['text':' instead of relying on ProcessGroupNCCL destructor.','line_number':551,'multiline':False]['text':' Helper that broadcasts nccl unique ID to all ranks through the store','line_number':561,'multiline':False]['text':' Helper that either looks up the cached NCCL communicators or creates','line_number':568,'multiline':False]['text':' a new set of NCCL communicators as a cache entry','line_number':569,'multiline':False]['text':' Wrapper method which can be overridden for tests.','line_number':577,'multiline':False]['text':' Helper that encapsulates work shared across all collective communication','line_number':599,'multiline':False]['text':' primitives.  The callbacks have the following signatures:','line_number':600,'multiline':False]['text':'','line_number':601,'multiline':False]['text':'    ncclResult_t fn(at::Tensor& input, at::Tensor& output,','line_number':602,'multiline':False]['text':'                    ncclComm_t, at::cuda::CUDAStream&);','line_number':603,'multiline':False]['text':'    void {pre,post}(std::vector<at::cuda::CUDAStream&>);','line_number':604,'multiline':False]['text':' Helper that encapsulates work shared across point-to-point communication','line_number':625,'multiline':False]['text':' primitives. It is the same structure as the helper used for collective','line_number':626,'multiline':False]['text':' communication primitives.','line_number':627,'multiline':False]['text':' Checks for NCCL errors on each of the communicators and returns an','line_number':649,'multiline':False]['text':' appropriate exception_ptr (nullptr if no errors).','line_number':650,'multiline':False]['text':' Function that runs as part of a separate thread and checks for errors on','line_number':654,'multiline':False]['text':' NCCL communicators. We need a separate thread to check for NCCL errors','line_number':655,'multiline':False]['text':' since we can't rely on the user calling certain methods like wait(),','line_number':656,'multiline':False]['text':' isCompleted() etc. to detect and remediate errors. In addition to this, we','line_number':657,'multiline':False]['text':' need a mechanism to safely abort and remove NCCL communicators from our','line_number':658,'multiline':False]['text':' cache. This can be done cleanly by having a thread for the ProcessGroupNCCL','line_number':659,'multiline':False]['text':' class. Attempting to modify the communicator cache from the WorkNCCL class','line_number':660,'multiline':False]['text':' might run into issues with object lifetime since the ProcessGroupNCCL','line_number':661,'multiline':False]['text':' object might get destroyed before the WorkNCCL object.','line_number':662,'multiline':False]['text':' Return the CUDA device most likely associated with this backend.','line_number':665,'multiline':False]['text':' If we aren't bound to a specific device, there is no strict','line_number':666,'multiline':False]['text':' guarantee that this heuristic is the correct assignment of ranks','line_number':667,'multiline':False]['text':' to GPUs that Python layers use, but in practice it tends to be.','line_number':668,'multiline':False]['text':' Fortunately we don't rely on this for correctness of any tensor','line_number':669,'multiline':False]['text':' operations, just for ancillary uses like health checks and','line_number':670,'multiline':False]['text':' barriers.','line_number':671,'multiline':False]['text':' Performs a health check by initializing dummy NCCL communicators and then','line_number':674,'multiline':False]['text':' destroying them. This will help indicate and signal any NCCL-related issues','line_number':675,'multiline':False]['text':' prior to the first collective. The actual initialization and subsequent','line_number':676,'multiline':False]['text':' destruction is ran on a separate thread and the main thread is signalled','line_number':677,'multiline':False]['text':' about timeouts/errors to report to the application.','line_number':678,'multiline':False]['text':' Destroys initialized NCCL communicators in devNCCLComMap_ given by input','line_number':681,'multiline':False]['text':' key. Throws if there are no communicators to destroy. Also removes','line_number':682,'multiline':False]['text':' communicators from the cache and clears used device indices.','line_number':683,'multiline':False]['text':' Watchdog's inside loop.','line_number':686,'multiline':False]['text':' Takes care of cleaning up completed work, and aborting upon failure or','line_number':687,'multiline':False]['text':' timeout.','line_number':688,'multiline':False]['text':' In the timeout case and we will dump debug info such as the NCCL flight','line_number':693,'multiline':False]['text':' recorder to storage. Down the road, if we have more complicated or blocking','line_number':694,'multiline':False]['text':' operations, we might need to use a side thread to do it.','line_number':695,'multiline':False]['text':' Desync debug helper','line_number':698,'multiline':False]['text':' Desync debug helper','line_number':701,'multiline':False]['text':' Function that runs as part of a separate thread aside from watchdog','line_number':705,'multiline':False]['text':' thread because we need to check the heartbeat from watchdog thread','line_number':706,'multiline':False]['text':' so that when we get stuck in some NCCL/CUDA calls,','line_number':707,'multiline':False]['text':' we can dump the debugging information and abort the process.','line_number':708,'multiline':False]['text':' Function that directly trigger std::abort so that the whole process','line_number':711,'multiline':False]['text':' gets terminated.','line_number':712,'multiline':False]['text':' Create a thread that dumps debug info to the file specified as','line_number':715,'multiline':False]['text':' ${TORCH_NCCL_DEBUG_INFO_TEMP_FILE}{$RANK}','line_number':716,'multiline':False]['text':' Serializes all dumping activity, but allows concurrent calls.','line_number':717,'multiline':False]['text':' Each call returns a future, which can be checked or waited on','line_number':718,'multiline':False]['text':' for dump completion.','line_number':719,'multiline':False]['text':' When watchdog timeout, this function will be called and return debug info','line_number':722,'multiline':False]['text':' for users. For now we only get information from retrieveDesyncReport.','line_number':723,'multiline':False]['text':' We are working on enabling more useful debug information for watchdog','line_number':724,'multiline':False]['text':' timeout.','line_number':725,'multiline':False]['text':' The store is used to broadcast the NCCL unique ID of rank 0.','line_number':730,'multiline':False]['text':' The number of NCCL communicators that have been created during','line_number':737,'multiline':False]['text':' the lifetime of this process group. This sequence number is','line_number':738,'multiline':False]['text':' used to scope keys used in the store.','line_number':739,'multiline':False]['text':' The store keys to trace the last NCCL collective kernel CUDA events - start','line_number':742,'multiline':False]['text':' event and end event respectively. These are used to do desync root cause','line_number':743,'multiline':False]['text':' analysis.','line_number':744,'multiline':False]['text':' The NCCL communicator that the process group has cached.','line_number':748,'multiline':False]['text':'','line_number':749,'multiline':False]['text':' For collective operations:','line_number':750,'multiline':False]['text':' The key is a list of GPU devices that an operation is operating on','line_number':751,'multiline':False]['text':' The GPU devices are stored in a device sequence and the cache NCCL','line_number':752,'multiline':False]['text':' communicator is associated with this GPU device sequence','line_number':753,'multiline':False]['text':'','line_number':754,'multiline':False]['text':' e.g. If the process group op only uses device 0, then the value of','line_number':755,'multiline':False]['text':' the used device string stored (value of the hashmap) would be "0".','line_number':756,'multiline':False]['text':'','line_number':757,'multiline':False]['text':'      If the process group op uses device 0 - 7 and the each tensor of the','line_number':758,'multiline':False]['text':'      input tensor list is on device, 0, 1, 2, 3, 4, 5, 6, 7 separately,','line_number':759,'multiline':False]['text':'      then the value of the used device string (key) stored would be','line_number':760,'multiline':False]['text':'      "0,1,2,3,4,5,6,7"','line_number':761,'multiline':False]['text':'','line_number':762,'multiline':False]['text':'      If the process group op uses device 0 - 7 and the each tensor of the','line_number':763,'multiline':False]['text':'      input tensor list is on device, 0, 4, 5, 6, 7, 1, 2, 3 separately,','line_number':764,'multiline':False]['text':'      then the value of the used device string stored would be','line_number':765,'multiline':False]['text':'      "0,4,5,6,7,1,2,3"','line_number':766,'multiline':False]['text':'','line_number':767,'multiline':False]['text':'      Note that the order of the device for the tensor list matters.','line_number':768,'multiline':False]['text':'','line_number':769,'multiline':False]['text':' For point-to-point operations:','line_number':770,'multiline':False]['text':' The key is a string of my current rank and the peer process rank.','line_number':771,'multiline':False]['text':' e.g. If process 1 and process 2 are involved in a point-to-point','line_number':772,'multiline':False]['text':' communication, the key will be "1:2" on both processes. Note: this is for','line_number':773,'multiline':False]['text':' the scenario where there is only 1 GPU per process. When it comes to','line_number':774,'multiline':False]['text':' multiple GPUs per process, this part may need to redesigned.','line_number':775,'multiline':False]['text':' The NCCL communicators currently in process of being initialized.','line_number':779,'multiline':False]['text':' Map from ncclUniqueId to appropriate communicator.','line_number':783,'multiline':False]['text':' Mutex to guard maps like devNCCLCommMap_ and ncclIdToCommMap_.','line_number':787,'multiline':False]['text':' Heartbeat of watchdog thread.','line_number':790,'multiline':False]['text':' The time interval used for deciding whether there is no watchdog heartbeat.','line_number':793,'multiline':False]['text':' Size of ring buffer where we store NCCL Traces for debugging.','line_number':796,'multiline':False]['text':' We gate the heartbeat monitor thread so that we can roll it out gradually.','line_number':799,'multiline':False]['text':' Monitor thread which checks the heartbeat of Watchdog thread.','line_number':802,'multiline':False]['text':' If the monitor thread finds there is no heartbeat, it will dump debug info','line_number':803,'multiline':False]['text':' and then kill the watchdog thread to avoid hang.','line_number':804,'multiline':False]['text':' Watchdog thread which looks for errors on the cached NCCL communicators.','line_number':807,'multiline':False]['text':' Whether or not we should terminate the watchdog and workCleanup threads.','line_number':812,'multiline':False]['text':' Whether or not we should terminate the heartbeat monitoring threads.','line_number':815,'multiline':False]['text':' Whether we are in the shutdown mode when we are trying to get debug info,','line_number':818,'multiline':False]['text':' such as desync report.','line_number':819,'multiline':False]['text':' Whether there are hooks pending to be fired','line_number':822,'multiline':False]['text':' Mutex to Guard workMetaList_','line_number':825,'multiline':False]['text':' Mutex to Guard monitorWakeUpCV_','line_number':828,'multiline':False]['text':' Mutex to Guard the check of writeDebugInfo_','line_number':833,'multiline':False]['text':' Condition Variable for watchdog thread sleep','line_number':836,'multiline':False]['text':' Condition Variable for monitor thread to wake up early','line_number':839,'multiline':False]['text':' Vector to Store WorkNCCL pointers','line_number':842,'multiline':False]['text':' Mutex to Guard workMetaList_','line_number':847,'multiline':False]['text':' Condition Variable for watchdog thread sleep','line_number':850,'multiline':False]['text':' Add Work Pointer to workVector','line_number':855,'multiline':False]['text':' The CUDA streams used by NCCL kernels','line_number':858,'multiline':False]['text':' The CUDA events used to sync NCCL streams','line_number':862,'multiline':False]['text':' Device Indexes used for all collectives in this group','line_number':865,'multiline':False]['text':' Flag to denote if a coalescing groupStart/groupEnd block is active','line_number':868,'multiline':False]['text':' Stores device indexes for all collectives run inside a coalescing block','line_number':871,'multiline':False]['text':' Stores communicators for all collectives run inside a coalescing block','line_number':874,'multiline':False]['text':' map from the key: "group name + pg counter (ID)" to the','line_number':877,'multiline':False]['text':' unique NCCL ID count. This needs to be group and pg specific','line_number':878,'multiline':False]['text':'','line_number':879,'multiline':False]['text':' For each process group, we need a uniform unique NCCL ID counter to ensure','line_number':880,'multiline':False]['text':' that NCCL operation in this process group can be completed successfully.','line_number':881,'multiline':False]['text':' Since each process group ID belongs to a group name, the key to this map','line_number':882,'multiline':False]['text':' is a combination of group name and ProcessGroupNCCL ID.','line_number':883,'multiline':False]['text':' map from group name to the pg counter (ID) within that group','line_number':886,'multiline':False]['text':'','line_number':887,'multiline':False]['text':' For each group with the "group name" (which is the key), we need to','line_number':888,'multiline':False]['text':' keep track of a unique process group ID when creating a new','line_number':889,'multiline':False]['text':' ProcessGroupNCCL for this "group name". Therefore, the value of this','line_number':890,'multiline':False]['text':' map keeps the unique ProcessGroupNCCL's ID for a specific group with','line_number':891,'multiline':False]['text':' the "group name". The reason we need a per-group process group ID counter','line_number':892,'multiline':False]['text':' is that different group can have different ranks and we need ensure that','line_number':893,'multiline':False]['text':' each group has its own uniform process group ID for all its ranks.','line_number':894,'multiline':False]['text':' Whether or not wait() and synchronize() are blocking operations that wait','line_number':897,'multiline':False]['text':' for the operation to complete.','line_number':898,'multiline':False]['text':' Whether or not to hook the cache allocator to register all allocated','line_number':901,'multiline':False]['text':' tensors','line_number':902,'multiline':False]['text':' Whether or not the workCleanupThread is used to perform async error','line_number':905,'multiline':False]['text':' handling.','line_number':906,'multiline':False]['text':' Whether or not to enable timeout root cause analysis.','line_number':909,'multiline':False]['text':' Whether or not to dump debug info on timeout','line_number':912,'multiline':False]['text':' Whether or not to create start CUDAEvent and enable timing for start','line_number':915,'multiline':False]['text':' and end events. Note that enableTiming_ is always true if desyncDebug_','line_number':916,'multiline':False]['text':' is set to true.','line_number':917,'multiline':False]['text':' Whether or not TORCH_NCCL_AVOID_RECORD_STREAMS was set','line_number':920,'multiline':False]['text':' Set of communicators that this process group has aborted and their','line_number':923,'multiline':False]['text':' ncclUniqueId has been written to the store. We don't need a lock','line_number':924,'multiline':False]['text':' for this map since only the watchdog thread accesses this set. The','line_number':925,'multiline':False]['text':' set contains the string representation of ncclUniqueId.','line_number':926,'multiline':False]['text':' The number of active ncclGroupStart() calls. This counter will be increased','line_number':929,'multiline':False]['text':' by 1 when ncclGroupStart() is called and decreased by 1 when ncclGroupEnd()','line_number':930,'multiline':False]['text':' is called.','line_number':931,'multiline':False]['text':' Counting for the sequential number of NCCL collective call.','line_number':934,'multiline':False]['text':' The callback function to store NCCL debug info.','line_number':939,'multiline':False]['text':' namespace c10d','line_number':949,'multiline':False]['text':' USE_C10D_NCCL','line_number':951,'multiline':False]