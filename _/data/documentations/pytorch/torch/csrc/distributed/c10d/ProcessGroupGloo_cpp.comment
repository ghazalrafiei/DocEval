['text':' See Note in monitoredBarrier','line_number':125,'multiline':False]['text':' If no more remaining time, return -1 to indicate to caller.','line_number':132,'multiline':False]['text':' Emit a LOG(ERROR) and throws using TORCH_CHECK with the given messages.','line_number':140,'multiline':False]['text':' For monitoredBarrier, checks remaining time left to finish processing ranks','line_number':148,'multiline':False]['text':' and throws error if timeout.','line_number':149,'multiline':False]['text':' Bitwise AND with SFINAE guard for integral types.','line_number':211,'multiline':False]['text':' Bitwise OR with SFINAE guard for integral types.','line_number':224,'multiline':False]['text':' Bitwise XOR with SFINAE guard for integral types.','line_number':237,'multiline':False]['text':'resizable=','line_number':329,'multiline':True]['text':' This function initializes a vector of CUDA streams, one for every','line_number':334,'multiline':False]['text':' tensor in the input tensor vector, and ensures that these streams are','line_number':335,'multiline':False]['text':' synchronized with the current default streams. This is needed so','line_number':336,'multiline':False]['text':' that new work on the new streams is serialized w.r.t. all operations','line_number':337,'multiline':False]['text':' on the tensors.','line_number':338,'multiline':False]['text':' Record event on current stream','line_number':348,'multiline':False]['text':' Get a non-default stream to execute asynchronous CUDA operations','line_number':351,'multiline':False]['text':' on for this device. This ensures that the default stream used','line_number':352,'multiline':False]['text':' by the caller is not occupied by c10d related operations.','line_number':353,'multiline':False]['text':'isHighPriority=','line_number':355,'multiline':True]['text':' Ensure the new stream is synchronized with the current stream.','line_number':356,'multiline':False]['text':' `tensors` are created on a different stream. Hence, they must record','line_number':359,'multiline':False]['text':' new streams in this Work to prevent being freed before the Work finishes.','line_number':360,'multiline':False]['text':' We will need to coalesce first, which means new tensors will','line_number':368,'multiline':False]['text':' be allocated on the streams we just allocated, and there','line_number':369,'multiline':False]['text':' is no need to record them separately.','line_number':370,'multiline':False]['text':' This function initializes a vector of CUDA streams, one per device,','line_number':378,'multiline':False]['text':' and ensures that these streams are synchronized with the current default','line_number':379,'multiline':False]['text':' streams. It is assumed that the tensors in the nested tensor vectors are','line_number':380,'multiline':False]['text':' on the same device.','line_number':381,'multiline':False]['text':' Ensure that the tensors in the nested tensor vectors are on the same','line_number':386,'multiline':False]['text':' device.','line_number':387,'multiline':False]['text':' Record event on current stream','line_number':405,'multiline':False]['text':' Get a non-default stream to execute asynchronous CUDA operations','line_number':408,'multiline':False]['text':' on for this output. This ensures that the default stream used','line_number':409,'multiline':False]['text':' by the caller is not occupied by c10d related operations.','line_number':410,'multiline':False]['text':'isHighPriority=','line_number':412,'multiline':True]['text':' Ensure the new stream is synchronized with the current stream.','line_number':413,'multiline':False]['text':' `tensors` are created on a different stream. Hence, they must record','line_number':417,'multiline':False]['text':' new streams in this Work to prevent being freed before the Work','line_number':418,'multiline':False]['text':' finishes.','line_number':419,'multiline':False]['text':' namespace','line_number':427,'multiline':False]['text':' static','line_number':429,'multiline':False]['text':' FIXME: We need to call it here since Future completion requires all','line_number':441,'multiline':False]['text':' the work to be synchronized to CUDA.','line_number':442,'multiline':False]['text':' namespace','line_number':488,'multiline':False]['text':' The work will be started and completed by different threads.','line_number':498,'multiline':False]['text':' Profiler: Pass nullptr as profilingTitle to parent constructor to','line_number':525,'multiline':False]['text':' replace default profiler implementation with async version that reports','line_number':526,'multiline':False]['text':' correct timestamps for work that is asynchronously executed.','line_number':527,'multiline':False]['text':' Completes the Work object and throws the exception.','line_number':581,'multiline':False]['text':' Completes the Work object and throws the exception.','line_number':628,'multiline':False]['text':' Gloo assumes that this machine's hostname can always be resolved','line_number':648,'multiline':False]['text':' to an address. If it doesn't it throws a runtime error saying','line_number':649,'multiline':False]['text':' that it can't be resolved. Instead of catching it, we choose','line_number':650,'multiline':False]['text':' to proactively check if an address can be resolved, so we can','line_number':651,'multiline':False]['text':' gracefully fall back to an alternative if it doesn't.','line_number':652,'multiline':False]['text':' namespace','line_number':685,'multiline':False]['text':' Use the hostname to resolve the network address to','line_number':705,'multiline':False]['text':' use. Note: if the hostname does not resolve to an address (e.g.','line_number':706,'multiline':False]['text':' because of misconfigured /etc/hosts file), this will not work.','line_number':707,'multiline':False]['text':' Use this machine's hostname if it resolves to an address.','line_number':715,'multiline':False]['text':' Otherwise, use the loopback address.','line_number':720,'multiline':False]['text':' Use the hostname to resolve the network address to','line_number':732,'multiline':False]['text':' use. Note: if the hostname does not resolve to an address (e.g.','line_number':733,'multiline':False]['text':' because of misconfigured /etc/hosts file), this will not work.','line_number':734,'multiline':False]['text':' Use this machine's hostname if it resolves to an address.','line_number':742,'multiline':False]['text':' Otherwise, use the loopback address.','line_number':747,'multiline':False]['text':' Create and connect a context for every device.','line_number':771,'multiline':False]['text':'','line_number':772,'multiline':False]['text':' Note that the same device can be specified multiple times, either','line_number':773,'multiline':False]['text':' the same object, or the same logical device as different objects.','line_number':774,'multiline':False]['text':' Either mode is fine and only has performance implications.','line_number':775,'multiline':False]['text':'','line_number':776,'multiline':False]['text':' Using the same object multiple times means all contexts share a','line_number':777,'multiline':False]['text':' single I/O thread. If you use different objects for the same','line_number':778,'multiline':False]['text':' logical device they will have independent I/O threads. The latter','line_number':779,'multiline':False]['text':' option is needed if you have a fast NIC that cannot be saturated','line_number':780,'multiline':False]['text':' by a single I/O thread.','line_number':781,'multiline':False]['text':'','line_number':782,'multiline':False]['text':' TORCH_CHECK to print the cpp stacktrace.','line_number':792,'multiline':False]['text':' Every worker thread stores the AsyncWork object it's currently','line_number':799,'multiline':False]['text':' working on in the workInProgress_ vector. It must have size equal','line_number':800,'multiline':False]['text':' to the number of workers such that they can simply index into it','line_number':801,'multiline':False]['text':' using the worker index they are started with.','line_number':802,'multiline':False]['text':' Queue is empty, signal stop','line_number':817,'multiline':False]['text':' Release lock to allow threads to terminate','line_number':820,'multiline':False]['text':' Wait for worker threads to terminate','line_number':825,'multiline':False]['text':' Notify after releasing the lock so that the waiter','line_number':853,'multiline':False]['text':' does not immediately block.','line_number':854,'multiline':False]['text':' Notify after releasing the lock so that the waiter','line_number':868,'multiline':False]['text':' does not immediately block.','line_number':869,'multiline':False]['text':' Copy to non-root tensors','line_number':914,'multiline':False]['text':' Create pinned host side tensors.','line_number':936,'multiline':False]['text':' non_blocking ','line_number':941,'multiline':True]['text':' Synchronize with copy operation if applicable.','line_number':946,'multiline':False]['text':' Run broadcast on host side tensors.','line_number':951,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':954,'multiline':False]['text':' non_blocking ','line_number':958,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':964,'multiline':False]['text':' namespace','line_number':977,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':996,'multiline':False]['text':' reduce coalesced, flattened tensors.','line_number':1090,'multiline':False]['text':' separate and reshape tensors.','line_number':1095,'multiline':False]['text':' We share dimensionality about the sparse tensors before collecting','line_number':1128,'multiline':False]['text':' their contents. We assume here that the maximum number of sparse','line_number':1129,'multiline':False]['text':' and dense dimensions is 4. This is stored in a contiguous piece of','line_number':1130,'multiline':False]['text':' memory so that we can easily run allgather on it.','line_number':1131,'multiline':False]['text':'','line_number':1132,'multiline':False]['text':' The layout of this memory is as follows:','line_number':1133,'multiline':False]['text':'','line_number':1134,'multiline':False]['text':'   - [0:4]: sparse dims','line_number':1135,'multiline':False]['text':'   - [4:8]: dense dims','line_number':1136,'multiline':False]['text':'   -   [8]: nnz','line_number':1137,'multiline':False]['text':'','line_number':1138,'multiline':False]['text':' Construct from an existing metadata tensor to facilitate structured','line_number':1143,'multiline':False]['text':' access to metadata from peers, after gathering it.','line_number':1144,'multiline':False]['text':' Populate the metadata.','line_number':1152,'multiline':False]['text':' Sparse sizes','line_number':1173,'multiline':False]['text':' Dense sizes','line_number':1180,'multiline':False]['text':' Sparse allreduce is implemented with allgather on indices and values.','line_number':1199,'multiline':False]['text':' Every process then sums the resulting sparse tensors locally.','line_number':1200,'multiline':False]['text':' The nnz for sparse tensors may be different across processes, so first','line_number':1201,'multiline':False]['text':' we run allgather on the nnz, and then allgather with max(nnz).','line_number':1202,'multiline':False]['text':' TODO: This is a massive hack!  There is some confusion about','line_number':1204,'multiline':False]['text':' Variable/Tensor inside the body of this function.  Turning off','line_number':1205,'multiline':False]['text':' grad smooths over the confusion for now.  This fixes','line_number':1206,'multiline':False]['text':' test/test_c10d_gloo.py ProcessGroupGlooTest.test_sparse_allreduce_basics','line_number':1207,'multiline':False]['text':'','line_number':1208,'multiline':False]['text':' The correct fix is to stop allocating tensors that are not variables,','line_number':1209,'multiline':False]['text':' but to conveniently do this c10d must depend on torch not ATen','line_number':1210,'multiline':False]['text':' Perform local reduction if we have multiple inputs.','line_number':1214,'multiline':False]['text':' Need to coalesce before we can access indices and values.','line_number':1219,'multiline':False]['text':' Gather metadata information from all ranks.','line_number':1222,'multiline':False]['text':' Sanity check dimensionality across ranks.','line_number':1225,'multiline':False]['text':' Gather all indices and all values.','line_number':1237,'multiline':False]['text':' Perform global reduction.','line_number':1241,'multiline':False]['text':' Coalesce for good measure.','line_number':1251,'multiline':False]['text':' This copy is needed when we run a multi-gpu version of reduce (multiple','line_number':1258,'multiline':False]['text':' inputs per rank).','line_number':1259,'multiline':False]['text':' Prepare metadata vector (1 entry per rank)','line_number':1271,'multiline':False]['text':' Populate data for this rank','line_number':1278,'multiline':False]['text':' Allgather metadata','line_number':1281,'multiline':False]['text':' tensors copied from cuda may not be contiguous, get a contiguous','line_number':1304,'multiline':False]['text':' tensor before use its data_ptr','line_number':1305,'multiline':False]['text':' Allgatherv indices.','line_number':1308,'multiline':False]['text':' Compile indices tensor per rank.','line_number':1316,'multiline':False]['text':' There are nnz #dense_dim()-dimensional tensors per rank.','line_number':1334,'multiline':False]['text':' Allgatherv indices.','line_number':1350,'multiline':False]['text':' tensors copied from cuda may not be contiguous, get a contiguous','line_number':1352,'multiline':False]['text':' tensor before use its data_ptr','line_number':1353,'multiline':False]['text':' Compile values tensor per rank.','line_number':1361,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':1392,'multiline':False]['text':' Synchronize with copy operations.','line_number':1402,'multiline':False]['text':' Run allreduce on host side tensors.','line_number':1407,'multiline':False]['text':' non_blocking ','line_number':1413,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':1419,'multiline':False]['text':' Kick off copy from CUDA tensors to CPU tensors.','line_number':1442,'multiline':False]['text':' Note that both coalescing the sparse tensor and copying it to CPU','line_number':1443,'multiline':False]['text':' memory must be performed asynchronously, or we block the caller.','line_number':1444,'multiline':False]['text':'non_blocking=','line_number':1450,'multiline':True]['text':' Synchronize with copy operations.','line_number':1455,'multiline':False]['text':' Run allreduce on host side tensors.','line_number':1460,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':1463,'multiline':False]['text':'non_blocking=','line_number':1467,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':1473,'multiline':False]['text':' namespace','line_number':1486,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':1504,'multiline':False]['text':' all reduce sparse calls into default allreduce which','line_number':1553,'multiline':False]['text':' implemented with all_gathering indices and values','line_number':1554,'multiline':False]['text':' we do ths we do not have a native cuda implementation','line_number':1555,'multiline':False]['text':' tensors will be flattened and concatenated (coalesced). This means that','line_number':1567,'multiline':False]['text':' input','line_number':1568,'multiline':False]['text':' tensors must have the same device, layout and type.','line_number':1569,'multiline':False]['text':' invalid arguments are detected early here before any calls to nextTag()','line_number':1585,'multiline':False]['text':' which result in the collectiveCounter_ being incremented.','line_number':1586,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':1700,'multiline':False]['text':' Synchronize with copy operations.','line_number':1710,'multiline':False]['text':' Run reduce on host side tensors.','line_number':1715,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':1718,'multiline':False]['text':' non_blocking ','line_number':1722,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':1728,'multiline':False]['text':' namespace','line_number':1741,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':1760,'multiline':False]['text':' Use single flattened input tensor.','line_number':1829,'multiline':False]['text':' Use single flat output tensor.','line_number':1833,'multiline':False]['text':' The first dimension corresponds to the index into outputs[N],','line_number':1834,'multiline':False]['text':' so copying into the actual output later is easy.','line_number':1835,'multiline':False]['text':' Unflatten into output tensors.','line_number':1840,'multiline':False]['text':' Note: current CUDA implementation holds the assumption that the','line_number':1853,'multiline':False]['text':' tensors in the nested output tensor vectors are on the same device.','line_number':1854,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':1867,'multiline':False]['text':' Synchronize with copy operations.','line_number':1885,'multiline':False]['text':' Run allgather on host side tensors.','line_number':1894,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':1897,'multiline':False]['text':' non_blocking ','line_number':1902,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':1909,'multiline':False]['text':' namespace','line_number':1926,'multiline':False]['text':' first all reduce input','line_number':1936,'multiline':False]['text':' TODO we can probably make this fully async by chaining the allreduce','line_number':1937,'multiline':False]['text':' future.','line_number':1938,'multiline':False]['text':' Note: current CUDA implementation holds the assumption that the','line_number':1960,'multiline':False]['text':' tensors in the nested output tensor vectors are on the same device.','line_number':1961,'multiline':False]['text':' Expect all input/output tensors to have the same type and sizes','line_number':1992,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':2005,'multiline':False]['text':' Use single flattened input tensor.','line_number':2064,'multiline':False]['text':' Compute total number of elements we need to allocate for all tensors','line_number':2068,'multiline':False]['text':' requested.','line_number':2069,'multiline':False]['text':' Use single flat output tensor.','line_number':2075,'multiline':False]['text':' namespace','line_number':2098,'multiline':False]['text':' unused ','line_number':2103,'multiline':True]['text':' Expect i'th tensor of each list from 'output_lists' match i'th tensor','line_number':2118,'multiline':False]['text':' from 'input_list' in type and size.','line_number':2119,'multiline':False]['text':' Set single temporary tensor on root process.','line_number':2193,'multiline':False]['text':' This is later scattered to the separate output tensors.','line_number':2194,'multiline':False]['text':' Set single input tensor on all processes.','line_number':2201,'multiline':False]['text':' Unflatten into output tensors on root process.','line_number':2205,'multiline':False]['text':' Note: current CUDA implementation holds the assumptions:','line_number':2218,'multiline':False]['text':'     - inputs.size() is 1','line_number':2219,'multiline':False]['text':'     - outputs.size() is 1','line_number':2220,'multiline':False]['text':'     - the size of the nested output tensors is world size, i.e.,','line_number':2221,'multiline':False]['text':'       outputs[0].size, is world size','line_number':2222,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':2236,'multiline':False]['text':' Synchronize with copy operations.','line_number':2254,'multiline':False]['text':' Run gather on host side tensors.','line_number':2263,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':2266,'multiline':False]['text':' non_blocking ','line_number':2271,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':2278,'multiline':False]['text':' namespace','line_number':2295,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':2337,'multiline':False]['text':' Set list of input tensors on root process','line_number':2399,'multiline':False]['text':' Set single output tensor on all processes','line_number':2404,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':2427,'multiline':False]['text':' Synchronize with copy operations.','line_number':2446,'multiline':False]['text':' Run scatter on host side tensors.','line_number':2454,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':2457,'multiline':False]['text':' non_blocking ','line_number':2461,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':2467,'multiline':False]['text':' namespace','line_number':2484,'multiline':False]['text':' If the user gave us a CUDA tensor then CUDA must be loaded.','line_number':2525,'multiline':False]['text':' Gloo alltoall','line_number':2591,'multiline':False]['text':' Gloo alltoallv','line_number':2598,'multiline':False]['text':' Kick off copy from CUDA tensors to pinned CPU tensors.','line_number':2643,'multiline':False]['text':' Synchronize with copy operations.','line_number':2653,'multiline':False]['text':' Run alltoall on host side tensors.','line_number':2657,'multiline':False]['text':' Kick off copy back to the CUDA tensors.','line_number':2660,'multiline':False]['text':' non_blocking ','line_number':2663,'multiline':True]['text':' Synchronize with the copy back to CUDA tensors.','line_number':2668,'multiline':False]['text':' namespace','line_number':2683,'multiline':False]['text':' unused ','line_number':2690,'multiline':True]['text':' Construct unbound buffer.','line_number':2760,'multiline':False]['text':' The work captures the tensor to prevent it being deallocated and','line_number':2766,'multiline':False]['text':' the unbound buffer to synchronize on completion of the send.','line_number':2767,'multiline':False]['text':' Construct unbound buffer.','line_number':2780,'multiline':False]['text':' The work captures the tensor to prevent it being deallocated and','line_number':2786,'multiline':False]['text':' the unbound buffer to synchronize on completion of the recv.','line_number':2787,'multiline':False]['text':' Construct unbound buffer.','line_number':2800,'multiline':False]['text':' Build list of ranks that this operation can recv from. In these','line_number':2804,'multiline':False]['text':' bindings we don't differentiate between ranks and can receive','line_number':2805,'multiline':False]['text':' from any other process in the group.','line_number':2806,'multiline':False]['text':' The work captures the tensor to prevent it being deallocated and','line_number':2816,'multiline':False]['text':' the unbound buffer to synchronize on completion of the recv.','line_number':2817,'multiline':False]['text':' Wait on prior work to complete','line_number':2850,'multiline':False]['text':' namespace','line_number':2864,'multiline':False]['text':' Snapshot all in progress and pending work as weak_ptr.','line_number':2869,'multiline':False]['text':' When executing a barrier, we need to ensure that all prior work','line_number':2870,'multiline':False]['text':' has completed before completing itself.','line_number':2871,'multiline':False]['text':' Use default timeout if no timeout was specified.','line_number':2892,'multiline':False]['text':' only enforce timeout on rank 0. This is so that other ranks aren't timed','line_number':2899,'multiline':False]['text':' out first, bringing down the job without reporting which rank timed out.','line_number':2900,'multiline':False]['text':' Mappings of rank to recvWork/sendWork respectively.','line_number':2920,'multiline':False]['text':' Kick off recvWork and wait to unblock sendWork->wait() from non-zero ranks.','line_number':2923,'multiline':False]['text':' Failed/hanging ranks will not ack this call, letting rank 0 know about the','line_number':2924,'multiline':False]['text':' failure.','line_number':2925,'multiline':False]['text':' Note: if waitAllRanks=false, we recompute the time remaining in','line_number':2935,'multiline':False]['text':' barrier and use this recomputed time in wait(). However, if','line_number':2936,'multiline':False]['text':' waitAllRanks=true, we use the original timeout, since if we use','line_number':2937,'multiline':False]['text':' up the entire timeout waiting for response from rank n, then we','line_number':2938,'multiline':False]['text':' won't have any timeout left to query ranks beginning with n + 1.','line_number':2939,'multiline':False]['text':' If we are collecting all failed ranks, check if we need to throw if','line_number':2966,'multiline':False]['text':' some ranks have not responded.','line_number':2967,'multiline':False]['text':' Ensure all ranks from 1, ... WORLD_SIZE -1 have been successfully','line_number':2968,'multiline':False]['text':' processed.','line_number':2969,'multiline':False]['text':' If we've reached here successfully, this means all ranks have acked in','line_number':2994,'multiline':False]['text':' monitoredBarrier. Unblock all ranks now by responding to their recv(). This','line_number':2995,'multiline':False]['text':' ensures that this is a true barrier in that all ranks  exit it successfully','line_number':2996,'multiline':False]['text':' or none of them do.','line_number':2997,'multiline':False]['text':' Gloo just starts sequence numbers at 0.','line_number':3006,'multiline':False]['text':' Nothing to do to enable timing','line_number':3013,'multiline':False]['text':' namespace c10d','line_number':3016,'multiline':False]['text':' USE_C10D_GLOO','line_number':3018,'multiline':False]