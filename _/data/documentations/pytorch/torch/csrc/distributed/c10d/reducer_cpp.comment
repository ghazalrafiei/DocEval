['text':' Macro that wraps TORCH_CHECK with DDP logging.','line_number':29,'multiline':False]['text':' namespace','line_number':38,'multiline':False]['text':' NOLINT','line_number':40,'multiline':False]['text':' unused ','line_number':51,'multiline':True]['text':' If cpu_end_time is not recorded in this iteration,','line_number':56,'multiline':False]['text':' avg_time will return invalid value.','line_number':57,'multiline':False]['text':' For some cases like DDP runs on non-sync mode, backward compute','line_number':58,'multiline':False]['text':' end time can not be recorded in this iteration and thus can not','line_number':59,'multiline':False]['text':' calculate the valid avg_time.','line_number':60,'multiline':False]['text':' In this case, skip calculating the avg_time and return.','line_number':61,'multiline':False]['text':' namespace','line_number':87,'multiline':False]['text':' Check whether the module is multi_device_module','line_number':130,'multiline':False]['text':' For CUDA, record events only for single device module.','line_number':145,'multiline':False]['text':' If `expect_sparse_gradients` is not specified, initialize it such that','line_number':151,'multiline':False]['text':' we do not expect sparse gradients for any parameter.','line_number':152,'multiline':False]['text':' Initialize variable bucketing.','line_number':158,'multiline':False]['text':' This can be reinitialized later after capturing runtime information.','line_number':159,'multiline':False]['text':' All variables are expected to have their `grad_fn` set to the gradient','line_number':165,'multiline':False]['text':' accumulation function (since they are leafs in the autograd graph).','line_number':166,'multiline':False]['text':' We store pointers to these functions such that we can check if they are','line_number':167,'multiline':False]['text':' used in an autograd pass. If they are not, we know their grad tensors','line_number':168,'multiline':False]['text':' can be marked as ready for reduction.','line_number':169,'multiline':False]['text':' The gradient accumulator function is lazily initialized once.','line_number':176,'multiline':False]['text':' Therefore we can use its presence in the autograd graph as','line_number':177,'multiline':False]['text':' evidence that the parameter has participated in an iteration.','line_number':178,'multiline':False]['text':' Hook to execute after the gradient accumulator has executed.','line_number':184,'multiline':False]['text':' unused ','line_number':189,'multiline':True]['text':' Map raw function pointer to parameter index.','line_number':199,'multiline':False]['text':' This is used later on when the autograd graph is traversed','line_number':200,'multiline':False]['text':' to check for parameters for which no gradient is computed, if','line_number':201,'multiline':False]['text':' find_unused_parameters=True.','line_number':202,'multiline':False]['text':' Note that the mapping of gradient accumulator to variable should be','line_number':203,'multiline':False]['text':' one to one as we deduplicate shared parameters before constructing','line_number':204,'multiline':False]['text':' Reducer.','line_number':205,'multiline':False]['text':' The gradient accumulator is stored as weak_ptr in the autograd','line_number':212,'multiline':False]['text':' metadata of the variable, so we have to keep it alive here for','line_number':213,'multiline':False]['text':' the raw pointer to be valid.','line_number':214,'multiline':False]['text':' Initialize backward stats vector.','line_number':226,'multiline':False]['text':' See Note [Skip allreducing local_used_map_dev]','line_number':232,'multiline':False]['text':' Note [Skip allreducing local_used_map_dev]','line_number':238,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':239,'multiline':False]['text':' If find_unused_parameters_ is set to false, there is no need to allreduce','line_number':240,'multiline':False]['text':' local_used_map_dev_, because all parameters will be reduced anyway.','line_number':241,'multiline':False]['text':' Therefore, we can avoid allocating memory for local_used_map and','line_number':242,'multiline':False]['text':' local_used_map_dev_ if find_unused_parameters_ is false.','line_number':243,'multiline':False]['text':' Note [DDP Communication Hook]','line_number':245,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':246,'multiline':False]['text':' If DDP communication hook is not registered, the reducer reduces the buckets','line_number':247,'multiline':False]['text':' by just calling allreduce. If registered, it calls the hook and uses future','line_number':248,'multiline':False]['text':' work handle. If registered, reducer also skips dividing grads by world size.','line_number':249,'multiline':False]['text':' The reason for this is that the communication hook is expected to completely','line_number':250,'multiline':False]['text':' override how we perform communication and the user should have complete','line_number':251,'multiline':False]['text':' control over how the grads are handled.','line_number':252,'multiline':False]['text':'','line_number':253,'multiline':False]['text':' DDP communication hook is an enhancement that provides a hook which can be','line_number':254,'multiline':False]['text':' used to override how DDP communicates gradients across ranks, this can be','line_number':255,'multiline':False]['text':' used for algorithms like Gradient Compression/GossipGrad. This hook can be','line_number':256,'multiline':False]['text':' registered from Python API using `register_comm_hook`. `PythonCommHook`','line_number':257,'multiline':False]['text':' enables registering a Python hook and is a subclass of `CommHookInterface`.','line_number':258,'multiline':False]['text':' Additionally, there are also some built-in C++ hook implementations that can','line_number':259,'multiline':False]['text':' be specified by calling `register_builtin_comm_hook` from Python API.','line_number':260,'multiline':False]['text':' Deliberately don't pin the memory even if local_used_map_dev_ will','line_number':288,'multiline':False]['text':' be cuda. See Note [local_used_map_ -> local_used_map_dev copying]','line_number':289,'multiline':False]['text':' This tensor needs to be on the same device as the replica params because','line_number':292,'multiline':False]['text':' backend such as NCCL may not support CPU tensors, and hence it might not','line_number':293,'multiline':False]['text':' work if we always put it on CPU. The dist backend for MTIA doesn't support','line_number':294,'multiline':False]['text':' int32 allreduce for now, so it has to be placed on CPU.','line_number':295,'multiline':False]['text':' Ensure that the gradient type matches the bucket type, or mixed precision','line_number':305,'multiline':False]['text':' type if we are training with mixed precision.','line_number':306,'multiline':False]['text':' AccumulateGrad doesn't HAVE to obey the grad layout contract.','line_number':318,'multiline':False]['text':' The penalty for disobedience is reduced performance, not numerical','line_number':319,'multiline':False]['text':' death. Warnings here help diagnose poor DDP performance.','line_number':320,'multiline':False]['text':' Copy the contents of the gradient tensor to the corresponding part of the','line_number':349,'multiline':False]['text':' bucket's flattened gradient tensor.','line_number':350,'multiline':False]['text':' If the gradient is not set, we assume it wasn't computed as part of the','line_number':351,'multiline':False]['text':' current backwards pass, and we zero the part of the bucket it would','line_number':352,'multiline':False]['text':' otherwise hold.','line_number':353,'multiline':False]['text':' When gradient_as_bucket_view_ is false, or even when','line_number':357,'multiline':False]['text':' gradient_as_bucket_view_ is true, in rare cases users may set grad to','line_number':358,'multiline':False]['text':' be None after every iteration. In these cases, grad and bucket_view are','line_number':359,'multiline':False]['text':' pointing to different storages and thus need to copy grads to','line_number':360,'multiline':False]['text':' bucket_view. If gradient_as_bucket_view_ is set as true, let grad point','line_number':361,'multiline':False]['text':' to bucket_view. If grad has already been set as views of buckets in','line_number':362,'multiline':False]['text':' previous iterations, no copy is needed.','line_number':363,'multiline':False]['text':' Divides while copying into the bucket view to save one scan over','line_number':369,'multiline':False]['text':' all the input parameters.','line_number':370,'multiline':False]['text':' If DDP is running with create_graph=True, gradients require_grad','line_number':376,'multiline':False]['text':' themselves in order to compute higher order derivatives. However,','line_number':377,'multiline':False]['text':' DDP will not sync up these gradients currently (see','line_number':378,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/63812).','line_number':379,'multiline':False]['text':' Let grad point to bucket_view buffer.','line_number':401,'multiline':False]['text':' The grad is modified and need to be written back.','line_number':403,'multiline':False]['text':' If grad and bucket view point to the same storage, no need to copy.','line_number':407,'multiline':False]['text':' Gradient is undefined. When find_unused_parameters=True, ensure it is','line_number':413,'multiline':False]['text':' not marked as locally used, otherwise we will be allreducing zero's','line_number':414,'multiline':False]['text':' instead of not touching .grad field of parameter.','line_number':415,'multiline':False]['text':' The grad is not modified and doesn't need to be written back.','line_number':427,'multiline':False]['text':' Copy the indices of sparse metadata','line_number':445,'multiline':False]['text':' For indices we are using the ones set by sparse_metadata','line_number':460,'multiline':False]['text':' Sparse tensors cannot be grouped together with other sparse tensors in a','line_number':464,'multiline':False]['text':' single reduction operation like we can for dense tensors. Therefore, the','line_number':465,'multiline':False]['text':' `offsets` and `lengths` vectors in the bucket struct are empty, and','line_number':466,'multiline':False]['text':' there is no pre-existing accumulation tensor.','line_number':467,'multiline':False]['text':' Directly assign the sparse tensor to the `gradients` field.','line_number':468,'multiline':False]['text':' If no DDP comm hook is registered, the allreduce only sums up the','line_number':470,'multiline':False]['text':' value, and a separate division is required.','line_number':471,'multiline':False]['text':' The grad is modified in place and needs to be written back.','line_number':475,'multiline':False]['text':' If it was scheduled, wait on allreduce in forward pass that tells us','line_number':532,'multiline':False]['text':' division factor based on no. of currently participating processes.','line_number':533,'multiline':False]['text':' PyProcessGroup::PyWork doesn't expose value, so fetch it from the','line_number':539,'multiline':False]['text':' future','line_number':540,'multiline':False]['text':' Guard against the results being empty','line_number':543,'multiline':False]['text':' This is called before training and converts the gradients to the dtype they','line_number':551,'multiline':False]['text':' should be reduced in.','line_number':552,'multiline':False]['text':' Right now delay_all_reduce is only called when static_graph_=true and','line_number':560,'multiline':False]['text':' num_iterations_==1.','line_number':561,'multiline':False]['text':' launch all reduce local used map','line_number':570,'multiline':False]['text':' prepare to set unused_parameters_, if it is static graph,','line_number':573,'multiline':False]['text':' unused_parameters_ will not change after 1st iteration.','line_number':574,'multiline':False]['text':' copy all gradients to buckets','line_number':578,'multiline':False]['text':' set unused_parameters_','line_number':580,'multiline':False]['text':' To avoid confusion around why static graph is picking up','line_number':592,'multiline':False]['text':' some parameters as unused on a rank vs not, we log','line_number':593,'multiline':False]['text':' unused parameter names for each rank for better','line_number':594,'multiline':False]['text':' debugability when TORCH_DISTRIBUTED_DEBUG is set to','line_number':595,'multiline':False]['text':' INFO or DETAIL','line_number':596,'multiline':False]['text':' construct one string to output','line_number':598,'multiline':False]['text':' Add the param_name','line_number':606,'multiline':False]['text':' Each rank prints out all the unused parameters detected','line_number':611,'multiline':False]['text':' launch all reduces for all buckets','line_number':623,'multiline':False]['text':' The function `autograd_hook` is called after the gradient for a','line_number':635,'multiline':False]['text':' model parameter has been accumulated into its gradient tensor.','line_number':636,'multiline':False]['text':' This function is only to be called from the autograd thread.','line_number':637,'multiline':False]['text':' Ignore if we don't expect to be called.','line_number':644,'multiline':False]['text':' This may be the case if the user wants to accumulate gradients','line_number':645,'multiline':False]['text':' for number of iterations before reducing them.','line_number':646,'multiline':False]['text':' See Note [Skip allreducing local_used_map_dev]','line_number':653,'multiline':False]['text':' Since it gets here, this param has been used for this iteration. We want','line_number':655,'multiline':False]['text':' to mark it in local_used_map_. During no_sync session, the same var can','line_number':656,'multiline':False]['text':' be set multiple times, which is OK as does not affect correctness. As','line_number':657,'multiline':False]['text':' long as it is used once during no_sync session, it is marked as used.','line_number':658,'multiline':False]['text':' Only set it as locally used if the grad is defined. Otherwise, hooks can','line_number':659,'multiline':False]['text':' be fired  with undefined grads, such as when not all outputs are used in','line_number':660,'multiline':False]['text':' DDP when computing loss. In this case, we don't want to mark it as','line_number':661,'multiline':False]['text':' locally used to ensure we don't touch the parameter's .grad field.','line_number':662,'multiline':False]['text':' The gradient is never modified.','line_number':668,'multiline':False]['text':' If `find_unused_parameters_` is true there may be model parameters that','line_number':678,'multiline':False]['text':' went unused when computing the model output, they won't be part of the','line_number':679,'multiline':False]['text':' autograd graph, and won't receive gradients. These parameters are','line_number':680,'multiline':False]['text':' discovered in the `prepare_for_backward` function and their indexes stored','line_number':681,'multiline':False]['text':' in the `unused_parameters_` vector.','line_number':682,'multiline':False]['text':' Rebuild bucket only if 1) it is the first time to rebuild bucket 2)','line_number':690,'multiline':False]['text':' static_graph_ is true or find_unused_parameters_ is false,','line_number':691,'multiline':False]['text':' 3) this backward pass needs to run allreduce.','line_number':692,'multiline':False]['text':' Here, we just dump tensors and their parameter indices into','line_number':693,'multiline':False]['text':' rebuilt_params_ and rebuilt_param_indices_ based on gradient arriving','line_number':694,'multiline':False]['text':' order, and then at the end of finalize_backward(), buckets will be','line_number':695,'multiline':False]['text':' rebuilt based on rebuilt_params_ and rebuilt_param_indices_, and then','line_number':696,'multiline':False]['text':' will be broadcasted and initialized.','line_number':697,'multiline':False]['text':' If it is static graph, after 1st iteration, check if a variable','line_number':698,'multiline':False]['text':' is ready for communication based on numGradHooksTriggeredMap_.','line_number':699,'multiline':False]['text':' Finally mark variable for which this function was originally called.','line_number':712,'multiline':False]['text':' Finally mark variable for which this function was originally called.','line_number':719,'multiline':False]['text':' See Note [Skip allreducing local_used_map_dev]','line_number':725,'multiline':False]['text':' H2D from local_used_map_ to local_used_map_dev_','line_number':726,'multiline':False]['text':' Note [local_used_map_ -> local_used_map_dev copying]','line_number':728,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':729,'multiline':False]['text':' We do async H2D to avoid the blocking overhead. The async copy and','line_number':730,'multiline':False]['text':' allreduce respect the current stream, so will be sequenced','line_number':731,'multiline':False]['text':' correctly.','line_number':732,'multiline':False]['text':'','line_number':733,'multiline':False]['text':' Correct sequencing with respect to host operations is also','line_number':734,'multiline':False]['text':' essential. The H2D copy_ is stream ordered, while the host's','line_number':735,'multiline':False]['text':' changes to local_used_map_ are host ordered. If a large backlog of','line_number':736,'multiline':False]['text':' cuda-stream work pushes the copy_ far into the future, and if no','line_number':737,'multiline':False]['text':' blocking calls occur between now and finalize_backward()** such','line_number':738,'multiline':False]['text':' that finalize_backward() re-zeroes local_used_map_ on the host','line_number':739,'multiline':False]['text':' before the stream executes the copy_, copy_ will read those zeros','line_number':740,'multiline':False]['text':' instead of the values we thought we told it to read here. Copying','line_number':741,'multiline':False]['text':' local_used_map_ to a pinned temporary (which the pinned caching','line_number':742,'multiline':False]['text':' allocator should supply asynchronously) avoids this nasty, rare','line_number':743,'multiline':False]['text':' race condition.','line_number':744,'multiline':False]['text':'','line_number':745,'multiline':False]['text':' ** In the hoped-for case where all params are used, DDP itself','line_number':746,'multiline':False]['text':' won't do any blocking work between now and the re-zeroing, so the','line_number':747,'multiline':False]['text':' danger is real.','line_number':748,'multiline':False]['text':'','line_number':749,'multiline':False]['text':' Defensively ensures local_used_map_tmp is distinct from','line_number':750,'multiline':False]['text':' local_used_map_','line_number':751,'multiline':False]['text':' pinned_memory ','line_number':757,'multiline':True]['text':' Paranoid asserts here because in some workloads, the pinned','line_number':758,'multiline':False]['text':' allocator behaves in a way we don't understand, and may be bugged.','line_number':759,'multiline':False]['text':' See https://github.com/pytorch/pytorch/pull/54474','line_number':760,'multiline':False]['text':' MTIA probably will have special logic in the future, following code might','line_number':767,'multiline':False]['text':' be changed drastically. Therefore, a new if case is created for MTIA, for','line_number':768,'multiline':False]['text':' now, the implementation is similar to the CUDA one, except for','line_number':769,'multiline':False]['text':' the pin memory step.','line_number':770,'multiline':False]['text':' Cannot simply access variable via `bucket.variables[variable_index]` since','line_number':788,'multiline':False]['text':' return value is used in `runGradCallbackForVariable()` which does not','line_number':789,'multiline':False]['text':' accept const tensors.','line_number':790,'multiline':False]['text':' Something is wrong if all variables contained in this bucket have','line_number':796,'multiline':False]['text':' already been marked as ready.','line_number':797,'multiline':False]['text':' We don't expect the same variable to be marked ready twice.','line_number':798,'multiline':False]['text':' Report index of param that has been marked twice. In debug mode, also','line_number':803,'multiline':False]['text':' report fully qualified parameter name.','line_number':804,'multiline':False]['text':' param_names_ is empty in debug mode.','line_number':816,'multiline':False]['text':' Any time we mark a variable ready (be it in line due to unused parameters,','line_number':875,'multiline':False]['text':' or via an autograd hook), we require a call to the finalize function. If','line_number':876,'multiline':False]['text':' this doesn't happen before the next iteration (or call to','line_number':877,'multiline':False]['text':' `prepare_for_backwards`), we know something is wrong.','line_number':878,'multiline':False]['text':' TODO(@pietern): Make this work for both CPU/CUDA tensors.','line_number':892,'multiline':False]['text':' When using CPU tensors we don't need to do this.','line_number':893,'multiline':False]['text':' Record event so that we can wait for all of them.','line_number':894,'multiline':False]['text':' auto& event = bucket.events[bucket_index.intra_bucket_index];','line_number':895,'multiline':False]['text':' event.record();','line_number':896,'multiline':False]['text':' Check if this was the final gradient for this bucket.','line_number':898,'multiline':False]['text':' Run finalizer function and kick off reduction for local_used_map once the','line_number':903,'multiline':False]['text':' final bucket was marked ready.','line_number':904,'multiline':False]['text':' Check that all buckets were completed and had their work kicked off.','line_number':915,'multiline':False]['text':' TODO(@pietern): Ensure proper synchronization with the CUDA events','line_number':944,'multiline':False]['text':' that recorded copies into this `gradients` tensor. If these copies are','line_number':945,'multiline':False]['text':' executed on non-default streams, the current stream for the device','line_number':946,'multiline':False]['text':' that holds the `gradients` tensor must wait on these events.','line_number':947,'multiline':False]['text':'','line_number':948,'multiline':False]['text':' As long as autograd uses the default stream for every device,','line_number':949,'multiline':False]['text':' these operations are implicitly sequenced, and we don't need to','line_number':950,'multiline':False]['text':' do any extra synchronization here.','line_number':951,'multiline':False]['text':' Check if we have cached mapping previously.','line_number':969,'multiline':False]['text':' Grab bucket index where gradient is located using variable_locators_.','line_number':978,'multiline':False]['text':' Grab the actual model parameter.','line_number':980,'multiline':False]['text':' Called when the bucket at the specified index is ready to be reduced.','line_number':998,'multiline':False]['text':' Buckets are reduced in sequence. Ignore this bucket if','line_number':1002,'multiline':False]['text':' it's not its turn to be reduced.','line_number':1003,'multiline':False]['text':' Keep going, until we either:','line_number':1008,'multiline':False]['text':' - have kicked off reduction for all buckets, or','line_number':1009,'multiline':False]['text':' - found a bucket that's not yet ready for reduction.','line_number':1010,'multiline':False]['text':' Append instead of overwrite so that this method can be called multiple','line_number':1024,'multiline':False]['text':' times in one iteration.','line_number':1025,'multiline':False]['text':' If initialize_buckets is called inside DDP constructor, then','line_number':1035,'multiline':False]['text':' it does not matter rpc context ptr is nullptr or not, as grad','line_number':1036,'multiline':False]['text':' will not be mutated.','line_number':1037,'multiline':False]['text':' If initialize_buckets is called during training loop, e.g, inside','line_number':1038,'multiline':False]['text':' rebuild_buckets(), since grad could be mutated and be pointed to','line_number':1039,'multiline':False]['text':' bucket_view, then it needs to check rpc context ptr is nullptr or not,','line_number':1040,'multiline':False]['text':' If rpc context ptr is nullptr, mutate variable.grad(); otherwise,','line_number':1041,'multiline':False]['text':' mutate grad in rpc context.','line_number':1042,'multiline':False]['text':' This shouldn't be called if we're expecting autograd hooks to fire.','line_number':1048,'multiline':False]['text':' Clear current bucket assignment.','line_number':1054,'multiline':False]['text':' Ensure we have a bucket index for every variable.','line_number':1058,'multiline':False]['text':' Iterate over buckets.','line_number':1061,'multiline':False]['text':' TODO(@pietern): Validate indices.','line_number':1067,'multiline':False]['text':' Must be non-empty, unique, and unique across buckets.','line_number':1068,'multiline':False]['text':' Variables that expect sparse gradients must have their own bucket.','line_number':1074,'multiline':False]['text':' The start index of the variable in the flattened tensor.','line_number':1095,'multiline':False]['text':' Reserve enough space for the per-variable fields stored in the bucket','line_number':1098,'multiline':False]['text':' for efficiency.','line_number':1099,'multiline':False]['text':' Iterate over bucket variables.','line_number':1106,'multiline':False]['text':' Allocate the bucket's flattened `gradients` tensor.','line_number':1137,'multiline':False]['text':' Make gradient type in the reduced precision if mixed precision is','line_number':1138,'multiline':False]['text':' enabled. This ensures that the type is correct when e.g. rebuilding','line_number':1139,'multiline':False]['text':' buckets.','line_number':1140,'multiline':False]['text':' Note:  "Gradient Layout Contract"','line_number':1146,'multiline':False]['text':'','line_number':1147,'multiline':False]['text':' Here, create views into the `gradients` tensor for each variable's','line_number':1148,'multiline':False]['text':' grad. Views serve as entry points to `copy_()` each grad's data in/out','line_number':1149,'multiline':False]['text':' of the flattened `gradients` tensor.','line_number':1150,'multiline':False]['text':'','line_number':1151,'multiline':False]['text':' Gradients may have dense memory but non-row-major-contiguous strides','line_number':1152,'multiline':False]['text':' (e.g. channels_last or channels_last_3d). For coalesced accesses','line_number':1153,'multiline':False]['text':' during copy_s, it's beneficial for each view's layout to match its','line_number':1154,'multiline':False]['text':' grad's layout.','line_number':1155,'multiline':False]['text':'','line_number':1156,'multiline':False]['text':' Specifically, we expect torch/csrc/autograd/functions/accumulate_grad.h','line_number':1157,'multiline':False]['text':' produces grads that obey the "Gradient Layout Contract":','line_number':1158,'multiline':False]['text':'   (1) if variable.is_non_overlapping_and_dense(), the stashed grad's','line_number':1159,'multiline':False]['text':'       strides match variable.','line_number':1160,'multiline':False]['text':'   (2) else, stashed grad is rowmajor contiguous.','line_number':1161,'multiline':False]['text':' and create views to match.','line_number':1162,'multiline':False]['text':'','line_number':1163,'multiline':False]['text':' If AccumulateGrad breaks the contract, and produces a grad with an','line_number':1164,'multiline':False]['text':' unexpected layout, performance will degrade due to poor memory access','line_number':1165,'multiline':False]['text':' patterns when copy_ing grad data in and out of its bucket view.','line_number':1166,'multiline':False]['text':' However, numerics remain correct, because the bucket view is the same','line_number':1167,'multiline':False]['text':' on either end of the raw allreduce.  bucket_view_in.copy(grad)','line_number':1168,'multiline':False]['text':' tranposes','line_number':1169,'multiline':False]['text':' (+ densifies) to the bucket view's layout, the data is allreduced,','line_number':1170,'multiline':False]['text':' then grad.copy_(bucket_view_out) transposes it back to grad's layout.','line_number':1171,'multiline':False]['text':'','line_number':1172,'multiline':False]['text':' The only way the numerics can go haywire is if the bucket views','line_number':1173,'multiline':False]['text':' themselves have different layouts across processes.','line_number':1174,'multiline':False]['text':' Bucket views' sizes and strides are set based on param layouts, using','line_number':1175,'multiline':False]['text':' the same logic that (we expect) AccumulateGrad uses for their grads.','line_number':1176,'multiline':False]['text':' Therefore, the only way a bucket view could have different layouts in','line_number':1177,'multiline':False]['text':' different processes is if its param has a different layout in','line_number':1178,'multiline':False]['text':' different processes. We can check that param layouts match across','line_number':1179,'multiline':False]['text':' processes in Reducer's constructor by allreducing some metadata.','line_number':1180,'multiline':False]['text':' Checking just once won't catch if someone messes with','line_number':1181,'multiline':False]['text':' param layouts over time, but not messing with params after DDP','line_number':1182,'multiline':False]['text':' construction is already a documented constraint.','line_number':1183,'multiline':False]['text':' Map participating variables to this bucket.','line_number':1187,'multiline':False]['text':' (see Note:  "Gradient Layout Contract" in initialize_buckets).','line_number':1202,'multiline':False]['text':' If the param's memory is dense, match its layout, anticipating','line_number':1210,'multiline':False]['text':' the autograd engine (AccumulateGrad) will also create gradients','line_number':1211,'multiline':False]['text':' matching its layout.','line_number':1212,'multiline':False]['text':' Fall back to a C-style contiguous view, again anticipating','line_number':1216,'multiline':False]['text':' AccumulateGrad will do the same when stashing grads for non-dense','line_number':1217,'multiline':False]['text':' params.','line_number':1218,'multiline':False]['text':' By default `bucket_views_out` and `bucket_views_in` are','line_number':1222,'multiline':False]['text':' essentially the same thing.','line_number':1223,'multiline':False]['text':' If gradient_as_bucket_view_ is set as true, then there are two cases to','line_number':1226,'multiline':False]['text':' handle: initialize_bucket_views could be called inside initialize_buckets','line_number':1227,'multiline':False]['text':' when rebuild_buckets, if grad has already been defined/calculated in','line_number':1228,'multiline':False]['text':' previous iteration, old grad needs to be copied into new bucket_view and','line_number':1229,'multiline':False]['text':' let grad point to the new bucket_view, initialize_bucket_views could also','line_number':1230,'multiline':False]['text':' be called inside initialize_buckets during construction. Grads are not','line_number':1231,'multiline':False]['text':' defined during construction time, in this case, do not let grad point to','line_number':1232,'multiline':False]['text':' bucket_view, because grads should be kept as being undefined for globally','line_number':1233,'multiline':False]['text':' unused parameters.','line_number':1234,'multiline':False]['text':' The grad is modified and needs to be written back.','line_number':1241,'multiline':False]['text':' The grad is not modified and does not need to be written back.','line_number':1244,'multiline':False]['text':' (see Note:  "Gradient Layout Contract" in initialize_buckets).','line_number':1251,'multiline':False]['text':' If the param's memory is dense, match its layout, anticipating','line_number':1261,'multiline':False]['text':' the autograd engine (AccumulateGrad) will also create gradients','line_number':1262,'multiline':False]['text':' matching its layout.','line_number':1263,'multiline':False]['text':' Fall back to a C-style contiguous view, again anticipating','line_number':1267,'multiline':False]['text':' AccumulateGrad will do the same when stashing grads for non-dense','line_number':1268,'multiline':False]['text':' params.','line_number':1269,'multiline':False]['text':' Reset num_buckets_ready_ at the beginning of backward computation','line_number':1286,'multiline':False]['text':' in each iteration.','line_number':1287,'multiline':False]['text':' Traverse the autograd graph starting at the specified output.','line_number':1299,'multiline':False]['text':' All parameters for which we have a pointer to their gradient accumulation','line_number':1300,'multiline':False]['text':' functions, but don't show up in the autograd graph will be marked ready for','line_number':1301,'multiline':False]['text':' for reduction as soon as the first autograd hook is called. This is not','line_number':1302,'multiline':False]['text':' done immediately because the model output may be ignored, and we only','line_number':1303,'multiline':False]['text':' want to start performing reductions on `torch.autograd.backward()`.','line_number':1304,'multiline':False]['text':' Seed queue with the grad functions of all outputs.','line_number':1314,'multiline':False]['text':' Traverse the autograd graph starting at the specified output.','line_number':1322,'multiline':False]['text':' Find accumulator functions that don't show up in this graph.','line_number':1336,'multiline':False]['text':' If the accumulator function is present in the graph, we know','line_number':1338,'multiline':False]['text':' a gradient will be computed for the corresponding parameter.','line_number':1339,'multiline':False]['text':' Warn user about unnecessary perf hit if all parameters were used in','line_number':1357,'multiline':False]['text':' forward.','line_number':1358,'multiline':False]['text':' Graph is still static if the set of unused parameters did not change.','line_number':1371,'multiline':False]['text':' Log graph is not static. Logger takes care of ensuring this is done','line_number':1376,'multiline':False]['text':' only once to avoid overhead.','line_number':1377,'multiline':False]['text':' Reset accounting.','line_number':1394,'multiline':False]['text':' Clear gradient ready order as it can be different in the next iteration.','line_number':1396,'multiline':False]['text':' Reset unused parameter accounting.','line_number':1401,'multiline':False]['text':' Reset per iteration marked ready parameters.','line_number':1403,'multiline':False]['text':' If static graph is not set, search graph to detect unused parameters.','line_number':1406,'multiline':False]['text':' When static graph is set, unused_parameters_ will be detected and will','line_number':1407,'multiline':False]['text':' not change after 1st iteration.','line_number':1408,'multiline':False]['text':' If static_graph_ = false and find_unused_parameters_ is false,','line_number':1409,'multiline':False]['text':' we assume that autograd hooks for ALL variables will be called,','line_number':1410,'multiline':False]['text':' and we don't have to search the autograd graph for presence of these hooks.','line_number':1411,'multiline':False]['text':' If a parameter is globally unused, we keep its grad untouched.','line_number':1425,'multiline':False]['text':' Creates grad according to the "Gradient Layout Contract"','line_number':1428,'multiline':False]['text':' (see torch/csrc/autograd/functions/accumulate_grad.h)','line_number':1429,'multiline':False]['text':' The grad is modified and needs to be written back.','line_number':1435,'multiline':False]['text':' The grad is not modified.','line_number':1438,'multiline':False]['text':' A bucket with one or more dense tensors needs to be unflattened.','line_number':1466,'multiline':False]['text':' See Note [Skip allreducing local_used_map_dev]','line_number':1472,'multiline':False]['text':' Determine if this param has been used globally or not.','line_number':1474,'multiline':False]['text':'','line_number':1475,'multiline':False]['text':' If the variable was used locally, it is also used globally and then','line_number':1476,'multiline':False]['text':' we don't need to wait for the reduction. Otherwise we lazily wait for','line_number':1477,'multiline':False]['text':' the reduction to complete, only when we see a variable that was','line_number':1478,'multiline':False]['text':' unused locally. Then we end up delaying the synchronization point','line_number':1479,'multiline':False]['text':' that local_used_work_->wait() implies. If we don't have any unused','line_number':1480,'multiline':False]['text':' parameters at all, we can skip waiting for the work to complete','line_number':1481,'multiline':False]['text':' altogether, and cause negligible performance overhead for models','line_number':1482,'multiline':False]['text':' where all parameters are used. Such lazily waiting means minimizing','line_number':1483,'multiline':False]['text':' performance impact for the big majority of models where all','line_number':1484,'multiline':False]['text':' parameters are always used. Then we only pay the overhead cost if','line_number':1485,'multiline':False]['text':' there is indeed a parameter that is locally unused, because we need','line_number':1486,'multiline':False]['text':' to check if it's also globally unused.','line_number':1487,'multiline':False]['text':' Note: global_unused might not be global yet. As we lazily wait for','line_number':1489,'multiline':False]['text':' the reduction to complete, it becomes really global only if we get to','line_number':1490,'multiline':False]['text':' the point as below where we wait for the reduction work, make D2H','line_number':1491,'multiline':False]['text':' copy, and update global_unused with the real global consensus, i.e.','line_number':1492,'multiline':False]['text':' local_used_map_reduced_ is true.','line_number':1493,'multiline':False]['text':' Wait for local_used_map reduction to complete.','line_number':1496,'multiline':False]['text':' D2H from local_used_map_dev_ to local_used_map_','line_number':1498,'multiline':False]['text':' Blocking copy, if local_used_map_dev_ is cuda','line_number':1499,'multiline':False]['text':' Return early if optimizer has already run.','line_number':1509,'multiline':False]['text':' If a communication hook is registered, then `bucket_view_out` stores','line_number':1521,'multiline':False]['text':' the allreduced results in a newly allocated tensor, so we copy','line_number':1522,'multiline':False]['text':' `bucket_view_out` back to `bucket_view_in` for this gradient.','line_number':1523,'multiline':False]['text':' Return early if optimizer has already run.','line_number':1529,'multiline':False]['text':' If a parameter is globally unused, we keep its grad untouched.','line_number':1532,'multiline':False]['text':' If grad is globally used but locally unused, let grad point to','line_number':1534,'multiline':False]['text':' bucket_view_in','line_number':1535,'multiline':False]['text':' The grad is modified and needs to be written back.','line_number':1550,'multiline':False]['text':' The grad is not modified.','line_number':1553,'multiline':False]['text':' No longer expect autograd hooks to fire after this function returns.','line_number':1561,'multiline':False]['text':' reset for the next iteration','line_number':1564,'multiline':False]['text':' No longer require call to finalize after this function returns.','line_number':1567,'multiline':False]['text':' Wait for asynchronous reduction to complete, and unflatten the bucket's','line_number':1571,'multiline':False]['text':' flattened `gradients` tensor.','line_number':1572,'multiline':False]['text':' See Note [DDP Communication Hook]','line_number':1574,'multiline':False]['text':' sparse metadata is set so the bucket should have sparse_tensor_indices','line_number':1584,'multiline':False]['text':' Reinitialize only `bucket_views_out` with the future_result by','line_number':1600,'multiline':False]['text':' following the same logic in `initialize_buckets`.','line_number':1601,'multiline':False]['text':' Unset allreduce division factor, as it may change in next backwards pass','line_number':1605,'multiline':False]['text':' when running with DDP join mode.','line_number':1606,'multiline':False]['text':' We don't need to finalize the sparse bucket since the sparse grad and','line_number':1610,'multiline':False]['text':' the bucket essentially point to the same storage. As a result, once','line_number':1611,'multiline':False]['text':' the allreduce is done, the sparse grads are automatically updated.','line_number':1612,'multiline':False]['text':' See Note [Skip allreducing local_used_maps_dev]','line_number':1622,'multiline':False]['text':' Due to the lazy wait, it is possible that reduction of the current','line_number':1624,'multiline':False]['text':' iteration is still going when the one for next iteration gets kicked off.','line_number':1625,'multiline':False]['text':' For such case, we want to wait explicitly to make sure the reduction does','line_number':1626,'multiline':False]['text':' complete before kicking off next one. Otherwise the previous one may','line_number':1627,'multiline':False]['text':' interfere, write to the device-side memory and clobber the content of','line_number':1628,'multiline':False]['text':' local_unused_maps_dev_.','line_number':1629,'multiline':False]['text':' Reset unused parameter accounting.','line_number':1636,'multiline':False]['text':' See Note [local_used_map_ -> local_used_map_dev copying]','line_number':1637,'multiline':False]['text':' Under distributed autograd','line_number':1659,'multiline':False]['text':' We should set 'new_context_ptr' even if it's nullptr. That means the','line_number':1667,'multiline':False]['text':' reducer is under a local backward run.','line_number':1668,'multiline':False]['text':' Set the shared ptr to the context only if it's set first time.','line_number':1671,'multiline':False]['text':' All call sites should use the same context ptr.','line_number':1672,'multiline':False]['text':' Use an atomic to avoid data race from multiple threads.','line_number':1673,'multiline':False]['text':' Group indices and num_bucket together into indices_tensor','line_number':1695,'multiline':False]['text':' Broadcast this tensor first, as its size is equal among all processes','line_number':1696,'multiline':False]['text':' Copy CPU tensor to device tensor, as the process_group_ could be NCCL and','line_number':1708,'multiline':False]['text':' it can only broadcast device tensors.','line_number':1709,'multiline':False]['text':'non_blocking=','line_number':1711,'multiline':True]['text':'non_blocking=','line_number':1714,'multiline':True]['text':' Update num_buckets after receiving it from rank 0','line_number':1716,'multiline':False]['text':' Broadcast bucket_sizes','line_number':1719,'multiline':False]['text':' For rank != 0, it is possible that local num buckets bucket_sizes.size()','line_number':1723,'multiline':False]['text':' is smaller than broadcasted num_buckets','line_number':1724,'multiline':False]['text':'non_blocking=','line_number':1729,'multiline':True]['text':'non_blocking=','line_number':1734,'multiline':True]['text':' Clear bucket_indices first, and then update bucket_indices using received','line_number':1736,'multiline':False]['text':' num_buckets, bucket_sizes_tensor and indices_tensor from rank 0','line_number':1737,'multiline':False]['text':' Ensure reduction for previous backwards pass is finished. If user's model','line_number':1754,'multiline':False]['text':' has unused parameters for example, this will raise an error recommending to','line_number':1755,'multiline':False]['text':' run with find_unused_parameters=True, instead of the size mismatch','line_number':1756,'multiline':False]['text':' exception below.','line_number':1757,'multiline':False]['text':' Reverse so that first_bucket_bytes_cap_ (smaller bucket) becomes the last','line_number':1788,'multiline':False]['text':' bucket. We cannot simply pass in {bucket_bytes_cap_,','line_number':1789,'multiline':False]['text':' first_bucket_bytes_cap} as the bucket order as we would immediately','line_number':1790,'multiline':False]['text':' advance to the 2nd element after the first bucket, whereas we only want','line_number':1791,'multiline':False]['text':' the last bucket to have a smaller size.','line_number':1792,'multiline':False]['text':' Reverse again because buckets were rebuilt in the opposite of gradient','line_number':1805,'multiline':False]['text':' ready order.','line_number':1806,'multiline':False]['text':' For rebuilt bucket indices, it needs to be synced across all ranks.','line_number':1819,'multiline':False]['text':' Broadcast the newly rebuilt bucket indices from rank 0 in default.','line_number':1820,'multiline':False]['text':' After syncing up rebuilt bucket indices, initialize buckets for reducer.','line_number':1821,'multiline':False]['text':' See Note [DDP Communication Hook]','line_number':1838,'multiline':False]['text':' See Note [DDP Communication Hook]','line_number':1848,'multiline':False]['text':' Check that any prior reduction has finished.','line_number':1872,'multiline':False]['text':' The variable `require_finalize_` is true until all gradients','line_number':1873,'multiline':False]['text':' have been computed and reduction of all buckets has been kicked off.','line_number':1874,'multiline':False]['text':' Collect unmarked parameter indices, additionally, in debug mode retrieve','line_number':1876,'multiline':False]['text':' parameter names.','line_number':1877,'multiline':False]['text':' We should have some unmarked parameter indices, otherwise we would not','line_number':1879,'multiline':False]['text':' have run into this error branch.','line_number':1880,'multiline':False]['text':' Parameters may have been unused in forward pass, or not all outputs','line_number':1909,'multiline':False]['text':' were used in producing loss.','line_number':1910,'multiline':False]['text':' Note that it does not really matter whether unused_parameters_.empty(),','line_number':1918,'multiline':False]['text':' since user may have enabled detection but this particular iteration','line_number':1919,'multiline':False]['text':' could have used or not used all parameters.','line_number':1920,'multiline':False]['text':' Without debug mode, log unmarked_param_indices, as well as','line_number':1936,'multiline':False]['text':' recommendation to use debug mode to print parameter names.','line_number':1937,'multiline':False]['text':' Retrieve set of parameter names that did not receive gradient.','line_number':1945,'multiline':False]['text':' In debug mode, log param names and indices that went unused.','line_number':1954,'multiline':False]['text':' when static_graph_ is set as true, always initialize_local_used_map','line_number':2022,'multiline':False]['text':' and detect the global unused parameters in the first iteration.','line_number':2023,'multiline':False]['text':' Tensors may be coalesced into buckets. Buckets must contain tensors of','line_number':2029,'multiline':False]['text':' the same type, on the same device, so a bucket can identified by a','line_number':2030,'multiline':False]['text':' composite key of a tensor's type identifier and its device.','line_number':2031,'multiline':False]['text':' See torch/csrc/utils/hash.h for dispatch code.','line_number':2039,'multiline':False]['text':' namespace','line_number':2049,'multiline':False]['text':' Either expect_sparse_gradient is not specified or it has as many elements','line_number':2058,'multiline':False]['text':' as the vector with tensors.','line_number':2059,'multiline':False]['text':' Store bucket indices and their sizes together, because we later sort the','line_number':2064,'multiline':False]['text':' resulting indices by minimum tensor index and want to keep sizes','line_number':2065,'multiline':False]['text':' consistent.','line_number':2066,'multiline':False]['text':' Sparse tensors go in their own bucket, so they do not have an enforced size','line_number':2068,'multiline':False]['text':' limit.','line_number':2069,'multiline':False]['text':' Keep iterator into the size_limit vector by tensor type and device.','line_number':2073,'multiline':False]['text':' This is done so that we can use the consecutive bucket limits per type.','line_number':2074,'multiline':False]['text':' Keep vector of indices and size accumulator by tensor type and device.','line_number':2081,'multiline':False]['text':' when tensor_indices is empty, the index of tensors[i] assigned to','line_number':2094,'multiline':False]['text':' bucket is i, otherwise the tensor index is tensor_indices[i].','line_number':2095,'multiline':False]['text':' If we expect a sparse gradient to be produced for this tensor, it cannot','line_number':2100,'multiline':False]['text':' be grouped together with other gradients and gets its own bucket.','line_number':2101,'multiline':False]['text':' Initialize bucket size limit iterator if necessary.','line_number':2113,'multiline':False]['text':' Advance to the next bucket size limit for this type/device.','line_number':2125,'multiline':False]['text':' Add remaining buckets.','line_number':2133,'multiline':False]['text':' If tensor_indices is not empty, the order of the tensors is in the gradient','line_number':2141,'multiline':False]['text':' ready order, so no need to sort.','line_number':2142,'multiline':False]['text':' If tensor_indices is empty, sort resulting buckets by the minimum tensor','line_number':2143,'multiline':False]['text':' index they include. We assume that the order of the tensors is the order in','line_number':2144,'multiline':False]['text':' which they are used (or the reverse order in which their gradients are','line_number':2145,'multiline':False]['text':' produced). This sorting step ensures that the buckets are ready in','line_number':2146,'multiline':False]['text':' consecutive order.','line_number':2147,'multiline':False]['text':' Return bucket indices and size limits as separate entries in tuple, as some','line_number':2164,'multiline':False]['text':' APIs only need to consume bucket indices.','line_number':2165,'multiline':False]['text':' Verifies corresponding params in the model replica have the same','line_number':2177,'multiline':False]['text':' sizes/strides across processes.','line_number':2178,'multiline':False]['text':' First verify number of parameters to avoid inconsistent inputs into','line_number':2183,'multiline':False]['text':' broadcast which can cause a crash.','line_number':2184,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/73547','line_number':2185,'multiline':False]['text':' Note: Not using tensor building API because of','line_number':2190,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/74114','line_number':2191,'multiline':False]['text':' Allgather and verify parameter size.','line_number':2195,'multiline':False]['text':' Continue with parameter shape verification.','line_number':2223,'multiline':False]['text':' Technically, process 0 is the broadcast source, so only process 0 needs','line_number':2232,'multiline':False]['text':' to populate metadata.  But no harm keeping work aligned across processes.','line_number':2233,'multiline':False]['text':' Technically, process 0 doesn't need to double-check metadata, because it','line_number':2249,'multiline':False]['text':' was the source.  But no harm keeping work aligned.','line_number':2250,'multiline':False]['text':'non_blocking=','line_number':2252,'multiline':True]['text':' Remove all hooks on variables registered by this Reducer. This is necessary','line_number':2291,'multiline':False]['text':' to make DDP failure recoverable. Otherwise, multiple Reducer instances','line_number':2292,'multiline':False]['text':' (from recoveries) will add their hooks to the original model, and those','line_number':2293,'multiline':False]['text':' hooks will try to invoke methods on a deleted Reducer objects.','line_number':2294,'multiline':False]['text':' Force rebuild of buckets.','line_number':2319,'multiline':False]['text':' Ensure forward can run despite previous backward not succeeding.','line_number':2324,'multiline':False]['text':' Unset allreduce division factor, as it may change in next backwards pass','line_number':2328,'multiline':False]['text':' when running with DDP join mode.','line_number':2329,'multiline':False]['text':' namespace c10d','line_number':2333,'multiline':False]