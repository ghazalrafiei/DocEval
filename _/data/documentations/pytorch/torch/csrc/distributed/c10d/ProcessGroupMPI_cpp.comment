['text':' Needed for CUDA-aware check','line_number':13,'multiline':False]['text':' Op mapping','line_number':31,'multiline':False]['text':' Type mapping','line_number':38,'multiline':False]['text':' Checking CUDA-aware MPI support, currently we only support CUDA aware','line_number':49,'multiline':False]['text':' MPI ops through Open MPI','line_number':50,'multiline':False]['text':' Run time check','line_number':52,'multiline':False]['text':' !defined(MPIX_CUDA_AWARE_SUPPORT)','line_number':59,'multiline':False]['text':' MPIX_CUDA_AWARE_SUPPORT','line_number':61,'multiline':False]['text':' Checking the input tensor's validity','line_number':64,'multiline':False]['text':' namespace','line_number':100,'multiline':False]['text':' request_ == MPI_REQUEST_NULL; the work has completed','line_number':152,'multiline':False]['text':' Populate exception if request was not successful','line_number':153,'multiline':False]['text':' unused ','line_number':175,'multiline':True]['text':' AsyncWork needs to manually call profiling end callbacks if they are set,','line_number':177,'multiline':False]['text':' since it does not call ProcessGroup::finish().','line_number':178,'multiline':False]['text':' AsyncWork needs to manually call profiling end callbacks if they are set,','line_number':190,'multiline':False]['text':' since it does not call ProcessGroup::finish().','line_number':191,'multiline':False]['text':' Always return true, because abort API is not implemented.','line_number':201,'multiline':False]['text':' Static global states','line_number':220,'multiline':False]['text':' We only want to initialize once','line_number':223,'multiline':False]['text':' Initialize MPI environment','line_number':232,'multiline':False]['text':' Once initialization','line_number':258,'multiline':False]['text':' If no ranks are specified, assume we're creating the root group','line_number':268,'multiline':False]['text':' `MPI_Comm_create` can be flaky in certain cases.','line_number':275,'multiline':False]['text':' See: https://github.com/pytorch/pytorch/issues/53899','line_number':276,'multiline':False]['text':' Fetch rank and world size for this group (MPI_COMM_WORLD or new)','line_number':292,'multiline':False]['text':' If this process is not part of the group, we don't construct a','line_number':303,'multiline':False]['text':' process group instance. This is in line with the semantics of the','line_number':304,'multiline':False]['text':' other process group types.','line_number':305,'multiline':False]['text':' Start the worker thread accepting MPI calls','line_number':319,'multiline':False]['text':' Queue is empty, signal stop','line_number':333,'multiline':False]['text':' Release lock to allow threads to terminate','line_number':336,'multiline':False]['text':' Join the single worker thread','line_number':340,'multiline':False]['text':' unused ','line_number':529,'multiline':True]['text':' unused ','line_number':530,'multiline':True]['text':' unused ','line_number':531,'multiline':True]['text':' copy the flattened output tensors to the outputs','line_number':587,'multiline':False]['text':' copy the input tensors to the flatten large send buffer','line_number':648,'multiline':False]['text':' We can use alltoall','line_number':705,'multiline':False]['text':' Need alltoallv','line_number':738,'multiline':False]['text':'unused ','line_number':936,'multiline':True]['text':'unused ','line_number':937,'multiline':True]['text':'unused ','line_number':938,'multiline':True]['text':' namespace c10d','line_number':942,'multiline':False]['text':' USE_C10D_MPI','line_number':944,'multiline':False]