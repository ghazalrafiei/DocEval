['text':' DistAutogradContext which stores information for a single distributed','line_number':18,'multiline':False]['text':' autograd pass on a worker.','line_number':19,'multiline':False]['text':' Retrieves the autograd context id for this context.','line_number':26,'multiline':False]['text':' Records a 'send' autograd function for this context with the provided','line_number':29,'multiline':False]['text':' message id.','line_number':30,'multiline':False]['text':' Records a 'recv' autograd function for this context with the provided','line_number':35,'multiline':False]['text':' message id.','line_number':36,'multiline':False]['text':' Given an autograd_message_id, retrieve the appropriate send function.','line_number':41,'multiline':False]['text':' Return all send functions for this context.','line_number':45,'multiline':False]['text':' Return all recv functions for this context.','line_number':49,'multiline':False]['text':' Adds a future message recording an outstanding RPC.','line_number':53,'multiline':False]['text':' Returns all gradients.','line_number':56,'multiline':False]['text':' This function gives a mutable grad reference to the callback.','line_number':59,'multiline':False]['text':' If the callback returns true, it means the grad in the context','line_number':60,'multiline':False]['text':' needs to be updated.','line_number':61,'multiline':False]['text':' records the workerID of a node that we sent an RPC to.','line_number':71,'multiline':False]['text':' workerIDs are added here when we attach a send function to this autograd','line_number':72,'multiline':False]['text':' context','line_number':73,'multiline':False]['text':' Retrieves a set containing the known workerIds for this context','line_number':76,'multiline':False]['text':' These are the different workers that this context has sent RPCs to.','line_number':77,'multiline':False]['text':' Record that we would like to accumulate the provided gradient on the given','line_number':86,'multiline':False]['text':' variable.','line_number':87,'multiline':False]['text':' Retrieve the GraphTask.','line_number':93,'multiline':False]['text':' Set the appropriate graph task for the backward pass. Can be called only','line_number':96,'multiline':False]['text':' once.','line_number':97,'multiline':False]['text':' Resets the graph task to ensure we can run another distributed backward','line_number':100,'multiline':False]['text':' pass for the same autograd context.','line_number':101,'multiline':False]['text':' Waits for all outstanding RPCs for this context to finish and clears all','line_number':104,'multiline':False]['text':' outstanding rpcs held in this context. This should be called only once.','line_number':105,'multiline':False]['text':' Record an event to mark the completion of gradient computation. These','line_number':110,'multiline':False]['text':' events will later help to properly synchronize gradients consumptions','line_number':111,'multiline':False]['text':' in getGradients(). We need these events because backward and','line_number':112,'multiline':False]['text':' optimizer.step are separate RPC calls, and will occur on different CUDA','line_number':113,'multiline':False]['text':' streams. Without synchronization, it is possible that gradients are','line_number':114,'multiline':False]['text':' consumed before they are ready.','line_number':115,'multiline':False]['text':' Set containing known worker IDs, used in cleaning up autograd context.','line_number':120,'multiline':False]['text':' Whenever a sendRpcBackward is attached to the autograd graph for this','line_number':121,'multiline':False]['text':' context, the destination is added here.','line_number':122,'multiline':False]['text':' Map from autograd_message_id to appropriate 'send' autograd function.','line_number':125,'multiline':False]['text':' Map from autograd_message_id to appropriate 'recv' autograd function.','line_number':129,'multiline':False]['text':' Gradients accumulated in this context so far. The key is the variable on','line_number':133,'multiline':False]['text':' which the gradient needs to be accumulated and the value is the gradient','line_number':134,'multiline':False]['text':' that needs to be accumulated on that variable..','line_number':135,'multiline':False]['text':' See comments for recordGradEvent(c10::Device device);','line_number':138,'multiline':False]['text':' The autograd GraphTask for the backward pass on this node for this context.','line_number':142,'multiline':False]['text':' List of futures for RPCs initiated by this node to propagate gradients to','line_number':145,'multiline':False]['text':' other nodes. The distributed autograd engine on this node can return','line_number':146,'multiline':False]['text':' successfully only if all these futures are done and are successful.','line_number':147,'multiline':False]['text':' Lock to protect concurrent modification of the context.','line_number':150,'multiline':False]['text':' This class stores a shared_ptr to a DistAutogradContext instance in a','line_number':156,'multiline':False]['text':' thread local variable. The instance is given by the call site. The class','line_number':157,'multiline':False]['text':' doesn't know the current context. It's just a util class.','line_number':158,'multiline':False]['text':' Store 'new_context' to the thread local variable maintained by this class.','line_number':161,'multiline':False]['text':' Retrieve the stored DistAutogradContext instance.','line_number':165,'multiline':False]['text':' namespace autograd','line_number':172,'multiline':False]['text':' namespace distributed','line_number':173,'multiline':False]['text':' namespace torch','line_number':174,'multiline':False]