['text':' Accumulate multiple grads on the same variable.','line_number':82,'multiline':False]['text':' Gradients are computed using the forward streams. Local autograd','line_number':86,'multiline':False]['text':' engine uses AccumulateGrad function to retrieve and apply forward','line_number':87,'multiline':False]['text':' stream during the backward computation. In distributed autograd,','line_number':88,'multiline':False]['text':' we directly call AccumulateGrad::accumulateGrad, and skip the','line_number':89,'multiline':False]['text':' CUDA stream restoration from autograd function. Hence, we manually','line_number':90,'multiline':False]['text':' call it here to get the streams correct.','line_number':91,'multiline':False]['text':' No higher order gradients supported in distributed autograd.','line_number':97,'multiline':False]['text':' TODO: Need to bump 'num_expected_refs' here when we support post_hooks for','line_number':100,'multiline':False]['text':' distributed autograd as part of','line_number':101,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/33482','line_number':102,'multiline':False]['text':' If we have an error, let the local autograd engine know about it.','line_number':140,'multiline':False]['text':' If there's an error, we want to setError() on the future,','line_number':201,'multiline':False]['text':' unless another error has already been sent - use a CAS to','line_number':202,'multiline':False]['text':' guard.','line_number':203,'multiline':False]['text':'','line_number':204,'multiline':False]['text':' Don't decrement num remaining here! (We don't need to, since','line_number':205,'multiline':False]['text':' memory handling is separate). If we simply don't decrement on','line_number':206,'multiline':False]['text':' errors, reaching 0 means that there were no errors - and hence,','line_number':207,'multiline':False]['text':' we can just markCompleted() without any other checking there.','line_number':208,'multiline':False]['text':' block current streams before accessing gradients to make sure that','line_number':240,'multiline':False]['text':' gradient computations are finished before use.','line_number':241,'multiline':False]['text':' Needs to update the grad in the map.','line_number':264,'multiline':False]['text':' namespace','line_number':272,'multiline':False]['text':' static','line_number':284,'multiline':False]['text':' namespace autograd','line_number':289,'multiline':False]['text':' namespace distributed','line_number':290,'multiline':False]['text':' namespace torch','line_number':291,'multiline':False]