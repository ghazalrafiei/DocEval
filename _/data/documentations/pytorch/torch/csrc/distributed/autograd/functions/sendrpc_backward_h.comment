['text':' As part of our distributed autograd implementation, whenever we send an RPC','line_number':9,'multiline':False]['text':' from one node to another, we add a 'SendRpcBackward' autograd function to the','line_number':10,'multiline':False]['text':' autograd graph. This is more or less a placeholder function that is used to','line_number':11,'multiline':False]['text':' kickoff the autograd engine on the current worker on the backward pass. The','line_number':12,'multiline':False]['text':' edges for this autograd function are the inputs to the RPC method.','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' During the backward pass, this function is queued for execution in the','line_number':15,'multiline':False]['text':' autograd engine which eventually runs the rest of the autograd graph.','line_number':16,'multiline':False]['text':' SendRpcBackward is actually the root of an autograd graph on the local','line_number':22,'multiline':False]['text':' node. As a result, it doesn't receive any 'inputs', but rather the RPC','line_number':23,'multiline':False]['text':' framework passes gradients over to this function to kickoff local autograd','line_number':24,'multiline':False]['text':' computation.','line_number':25,'multiline':False]['text':' Retrieve the grads for the function.','line_number':28,'multiline':False]['text':' namespace autograd','line_number':35,'multiline':False]['text':' namespace distributed','line_number':36,'multiline':False]['text':' namespace torch','line_number':37,'multiline':False]