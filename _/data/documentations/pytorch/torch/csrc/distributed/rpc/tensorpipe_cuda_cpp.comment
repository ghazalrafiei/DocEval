['text':' The cuda_ipc channels use cudaMemcpy to transmit CUDA tensor across processes','line_number':28,'multiline':False]['text':' The cuda_gdr channel sends CUDA memory over InfiniBand using GPUDirect RDMA.','line_number':41,'multiline':False]['text':' It directly registers the user-provided tensor with libibverbs, an operation','line_number':42,'multiline':False]['text':' which is expensive the first time, but it then caches the registration in','line_number':43,'multiline':False]['text':' order to amortize the cost and get low latency for subsequent transfers. A','line_number':44,'multiline':False]['text':' ready-to-send/ready-to-receive handshake is still needed before the transfer','line_number':45,'multiline':False]['text':' in order to ensure readiness and to agree on the device indices and thus the','line_number':46,'multiline':False]['text':' queue pair to use. It automatically pairs each GPU to the "closest" NIC if','line_number':47,'multiline':False]['text':' there are multiple of them (closest = longest prefix match in PCI tree).','line_number':48,'multiline':False]['text':' The cuda_xth channel supports same-process GPU-to-GPU comm','line_number':59,'multiline':False]['text':' The cuda_basic is the fallback channel for GPU-to-GPU comm','line_number':69,'multiline':False]['text':' record tensor data ptrs on TensorPipe streams, so that the tensors','line_number':83,'multiline':False]['text':' won't be destructed before TensorPipe finishing sending them.','line_number':84,'multiline':False]['text':' CUDACachingAllocator will call recordStream accordingly on the current','line_number':107,'multiline':False]['text':' stream.','line_number':108,'multiline':False]['text':' namespace','line_number':128,'multiline':False]['text':' namespace rpc','line_number':129,'multiline':False]['text':' namespace distributed','line_number':130,'multiline':False]['text':' namespace torch','line_number':131,'multiline':False]