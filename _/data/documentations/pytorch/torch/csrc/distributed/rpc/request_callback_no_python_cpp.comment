['text':' When request message has autograd info, processMessage() will set up valid','line_number':26,'multiline':False]['text':' current context id properly. This struct is used to clean up current context','line_number':27,'multiline':False]['text':' id after processMessage() is done.','line_number':28,'multiline':False]['text':' We need two futures here because it could pause twice when processing a','line_number':57,'multiline':False]['text':' RPC message:','line_number':58,'multiline':False]['text':'  1) waiting for all RRefs in the arguments to become confirmed;','line_number':59,'multiline':False]['text':'  2) waiting for processRpc to finish.','line_number':60,'multiline':False]['text':' Deserialize PythonUDF here to trigger RRef unpickling','line_number':64,'multiline':False]['text':' std::function must be copyable, hence hae to cast the unique_ptr to','line_number':71,'multiline':False]['text':' a shared_ptr here.','line_number':72,'multiline':False]['text':' unused ','line_number':75,'multiline':True]['text':' The cost of pre-request check is minimal thanks to','line_number':76,'multiline':False]['text':' std::shared_lock. The cost is in magnitude','line_number':77,'multiline':False]['text':' of 10us.','line_number':78,'multiline':False]['text':' If server global profiler is enabled, we further pay the','line_number':81,'multiline':False]['text':' cost of thread local profiler state initialization.','line_number':82,'multiline':False]['text':' Initialize thread-local profiler state from process-global','line_number':84,'multiline':False]['text':' profiler state.','line_number':85,'multiline':False]['text':' Response message has been sent at this moment, this post-response','line_number':94,'multiline':False]['text':' work doesn't affect RPC trip time.','line_number':95,'multiline':False]['text':' Restore thread-local profiler state.','line_number':97,'multiline':False]['text':' Put thread_local event_lists into the process-global profiler','line_number':99,'multiline':False]['text':' state.','line_number':100,'multiline':False]['text':' Pass a dummy message ID since it will be overwritten anyways.','line_number':132,'multiline':False]['text':' unused ','line_number':156,'multiline':True]['text':' unused ','line_number':162,'multiline':True]['text':' Creating an owner RRef on self, should already exist in owners map','line_number':174,'multiline':False]['text':' forceCreated ','line_number':176,'multiline':True]['text':' Caller is a user and callee is the owner, add fork','line_number':181,'multiline':False]['text':'','line_number':182,'multiline':False]['text':' NB: rrefId == forkId is true if and only if calling remote to self.','line_number':183,'multiline':False]['text':' In that case both the caller and the callee will access the','line_number':184,'multiline':False]['text':' OwnerRRef. Hence, on the callee side (here), it should not call','line_number':185,'multiline':False]['text':' addForkOfOwner as it is not a fork. To allow callee to distinguish','line_number':186,'multiline':False]['text':' when this request is sent to self, the caller will set forkId using','line_number':187,'multiline':False]['text':' rrefId (OwnerRRef does not have a forkId anyway).','line_number':188,'multiline':False]['text':' Need to reverse the device map for the backward pass of distributed','line_number':291,'multiline':False]['text':' autograd.','line_number':292,'multiline':False]['text':' Attach 'recv' autograd function.','line_number':298,'multiline':False]['text':' For this recv thread on server side, before processRpc(),','line_number':304,'multiline':False]['text':' set current_context_id_ to be context_id passed from client.','line_number':305,'multiline':False]['text':' In this way, if there is nested rpc call in python rpc call, original','line_number':306,'multiline':False]['text':' context_id from client can be passed in the chain calls.','line_number':307,'multiline':False]['text':' Process the original RPC.','line_number':315,'multiline':False]['text':' Kick off processing for the nested RPC command.','line_number':317,'multiline':False]['text':' wrappedRpcResponseFuture will be a Future<T> to the result.','line_number':318,'multiline':False]['text':' The original future needs to be marked as completed when the wrapped','line_number':323,'multiline':False]['text':' one completes, with the autograd context information wrapped.','line_number':324,'multiline':False]['text':' As this callback can be invoked by a different thread, we have to','line_number':328,'multiline':False]['text':' make sure that the thread_local states in the previous thread is','line_number':329,'multiline':False]['text':' correctly propagated.','line_number':330,'multiline':False]['text':' NB: The execution of TorchScript functions can also run on a','line_number':331,'multiline':False]['text':' different thread, which is addressed by','line_number':332,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/36395','line_number':333,'multiline':False]['text':' NB: when adding async UDF support, we should also propagate','line_number':334,'multiline':False]['text':' thread_local states there.','line_number':335,'multiline':False]['text':' TODO: Land on a general solution for RPC ThreadLocalState. See','line_number':336,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/38510','line_number':337,'multiline':False]['text':' Propagate error to responseFuture if we had one.','line_number':341,'multiline':False]['text':' Retrieve the appropriate autograd context.','line_number':364,'multiline':False]['text':' Lookup the appropriate 'send' function to enqueue.','line_number':368,'multiline':False]['text':' Attach the gradients to the send function.','line_number':372,'multiline':False]['text':' Now execute the autograd graph using the "distributed engine."','line_number':375,'multiline':False]['text':' Our response is satisfied when the rpcs come back.','line_number':379,'multiline':False]['text':' release the context if it still exists on this thread. We need to','line_number':395,'multiline':False]['text':' check if it exists since it may have been deleted by an in-flight','line_number':396,'multiline':False]['text':' RPC. This can create nested RPCs if there are other nodes that get','line_number':397,'multiline':False]['text':' notified to clean up their context.','line_number':398,'multiline':False]['text':' If requested with CUDA from caller but CUDA is not available on this','line_number':419,'multiline':False]['text':' machine, fallback to CPU and log a warning instead of crashing.','line_number':420,'multiline':False]['text':' Enable the profiler with the config from the sender.','line_number':435,'multiline':False]['text':' When enabling on the main thread, ensure profiler states are cleaned','line_number':436,'multiline':False]['text':' up, but defer consolidation of all profiled events to the continuation','line_number':437,'multiline':False]['text':' below.','line_number':438,'multiline':False]['text':' cleanup TLS state ','line_number':440,'multiline':True]['text':' consolidate events ','line_number':440,'multiline':True]['text':' Kick off processing for nested work and get Future<T> result in','line_number':446,'multiline':False]['text':' wrappedRpcResponseFuture','line_number':447,'multiline':False]['text':' TODO: https://github.com/pytorch/pytorch/issues/55757','line_number':451,'multiline':False]['text':' Defer consolidation of profiler events until async work has','line_number':457,'multiline':False]['text':' completed (such as async UDF)','line_number':458,'multiline':False]['text':' On continuation thread, don't clean up profiler states, since','line_number':463,'multiline':False]['text':' they will be cleaned up by main thread, and consolidate all','line_number':464,'multiline':False]['text':' events so we obtain asynchronously run events.','line_number':465,'multiline':False]['text':' Propagate error','line_number':469,'multiline':False]['text':' No need to propagate remote events in the case of an error.','line_number':470,'multiline':False]['text':' Exiting the scope will disable the profiler on this thread with the','line_number':486,'multiline':False]['text':' options specified above.','line_number':487,'multiline':False]['text':' TODO: RpcCommandBase should have an abstract execute() method that we can','line_number':500,'multiline':False]['text':' call here instead of having another switch statement here. Even better we','line_number':501,'multiline':False]['text':' could have abstract classes RpcRequest and RpcResp which inherit from','line_number':502,'multiline':False]['text':' RpcCommandBase and RpcRequest declares the abstract method execute() that','line_number':503,'multiline':False]['text':' we can call here. RpcResponse could have an abstract method to convert it','line_number':504,'multiline':False]['text':' to a python object.','line_number':505,'multiline':False]['text':' Adding node information to the error here since all processed RPC','line_number':562,'multiline':False]['text':' requests should be going through this function.','line_number':563,'multiline':False]['text':' namespace rpc','line_number':627,'multiline':False]['text':' namespace distributed','line_number':628,'multiline':False]['text':' namespace torch','line_number':629,'multiline':False]