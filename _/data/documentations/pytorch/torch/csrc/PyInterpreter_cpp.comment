['text':' NB: This is a macro and not a template function (like it was before)','line_number':17,'multiline':False]['text':' because passing in constexpr char* as template argument breaks some','line_number':18,'multiline':False]['text':' versions of MSVC that are being used internally at Meta.','line_number':19,'multiline':False]['text':' MSVC 14.16.27023 (vs2017_15.9)','line_number':20,'multiline':False]['text':' TODO: Need to make this work for StorageImpl too. I imagine I'll want to','line_number':40,'multiline':False]['text':' operate upon a PyObjectSlot rather than a TensorImpl','line_number':41,'multiline':False]['text':' NB: this is defined in python_dispatch.cpp','line_number':52,'multiline':False]['text':' NB: intentionally leaks the PyInterpreter, as there may still be','line_number':134,'multiline':False]['text':' references to it that are live, living in objects that aren't being','line_number':135,'multiline':False]['text':' destructed while Python is being cleaned up.','line_number':136,'multiline':False]['text':' WARNING: MUST NOT BE TENSOR ARGS','line_number':157,'multiline':False]['text':' TODO: there should be a shorter way to spell this','line_number':167,'multiline':False]['text':' TODO: fix the constness of target','line_number':168,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':171,'multiline':False]['text':' NB: this may not be a python tensor if you got here from a mode!','line_number':175,'multiline':False]['text':' TORCH_INTERNAL_ASSERT(isPythonTensor(self_t));','line_number':176,'multiline':False]['text':' NOTE [PyInterpreter::decref takes a `has_pyobj_slot` arg]','line_number':202,'multiline':False]['text':' Before calling PyInterpreter::decref, we must statically know if the','line_number':203,'multiline':False]['text':' pyobj has a PyObjectSlot or not.','line_number':204,'multiline':False]['text':' - If it has a PyObjectSlot, we need to be careful about PyObject resurrection','line_number':205,'multiline':False]['text':' - If it does not have a PyObjectSlot, we can freely decref','line_number':206,'multiline':False]['text':' One alternative to this is using PyObject_IsInstance','line_number':207,'multiline':False]['text':' to get at this information. However, we don't want to risk an incorrect','line_number':208,'multiline':False]['text':' `__instancecheck__` changing the semantics here.','line_number':209,'multiline':False]['text':' Leak the pyobj if not initialized.  This can happen if we are running','line_number':212,'multiline':False]['text':' exit handlers that are destructing tensors with residual (owned)','line_number':213,'multiline':False]['text':' PyObjects stored in them.','line_number':214,'multiline':False]['text':' Two possibilities:','line_number':219,'multiline':False]['text':' 1. We are decref-ing an object that has a PyObjectSlot, like a Tensor or','line_number':220,'multiline':False]['text':' Storage. Then we must be careful about PyObject resurrection (see','line_number':221,'multiline':False]['text':' THPVariable_clear).','line_number':222,'multiline':False]['text':' 2. We are decref-ing some other Python object. We don't do','line_number':223,'multiline':False]['text':' PyObject resurrection on non-Tensors, so we just carry on as usual','line_number':224,'multiline':False]['text':' It's still alive!  This can happen if a weak ref resurrected','line_number':227,'multiline':False]['text':' the PyObject without flipping ownership.  At this point it is','line_number':228,'multiline':False]['text':' too late to rescue the object, so just stub out the PyObject','line_number':229,'multiline':False]['text':' so that it fails on subsequent uses.  Don't raise an error here;','line_number':230,'multiline':False]['text':' you're probably in a destructor.','line_number':231,'multiline':False]['text':' Parse the name into namespace and name (no overload_name)','line_number':253,'multiline':False]['text':' TODO: put this into the library','line_number':254,'multiline':False]['text':' Make me some null terminated strings','line_number':260,'multiline':False]['text':' Not all DispatchKeys are pybind'ed into Python and we do not have infra','line_number':284,'multiline':False]['text':' to ensure this, so just pass a string back to Python.','line_number':285,'multiline':False]['text':' The plan: convert all the arguments back into PyObjects,','line_number':296,'multiline':False]['text':' extracting out the tensor handles, then call','line_number':297,'multiline':False]['text':' handle_torch_function_no_python_arg_parser','line_number':298,'multiline':False]['text':' NB: at the point arguments are pushed to the stack, ALL defaults','line_number':299,'multiline':False]['text':' are already present','line_number':300,'multiline':False]['text':' Find overloaded tensors','line_number':307,'multiline':False]['text':' TODO: if necessary, can optimize to cache the cache lookup','line_number':351,'multiline':False]['text':' TODO: if necessary, can optimize OpOverload to have slots','line_number':352,'multiline':False]['text':' TODO: allow this to be non-owning','line_number':359,'multiline':False]['text':' Slow path','line_number':363,'multiline':False]['text':' NB: not redispatch, as that will permanently remove the python','line_number':367,'multiline':False]['text':' dispatcher for subsequent redispatches','line_number':368,'multiline':False]['text':' For backwards compatibility','line_number':424,'multiline':False]['text':' NB: intentionally suffixed with _format to avoid','line_number':474,'multiline':False]['text':' triggering matches against "_like" suffix','line_number':475,'multiline':False]['text':'ignore_hermetic_tls=','line_number':572,'multiline':True]['text':' Note [Tensor Subclass custom size/stride caching strategy]','line_number':579,'multiline':False]['text':' Tensor subclasses can use __torch_dispatch__ to override size/stride calls.','line_number':580,'multiline':False]['text':' However, this presents a problem:','line_number':581,'multiline':False]['text':' (1) When you return a custom (maybe symbolic) size/stride','line_number':582,'multiline':False]['text':'     from python, we need to stash this fresh vector of ints/symints','line_number':583,'multiline':False]['text':'     somewhere so that it has the same lifetime as the tensor.','line_number':584,'multiline':False]['text':' (2) If the subclass experiences a metadata mutation,','line_number':585,'multiline':False]['text':'     this stashed vector is no longer valid, so we need to allocate a fresh','line_number':586,'multiline':False]['text':'     buffer to store the new sizes the next time someone asks for them.','line_number':587,'multiline':False]['text':'','line_number':588,'multiline':False]['text':' We handle this in the same way that `TensorImpl::sizes_default()`','line_number':589,'multiline':False]['text':' handles its buffer: we simply reallocate the buffer whenever','line_number':590,'multiline':False]['text':' the number of dimensions changes due to a resize.','line_number':591,'multiline':False]['text':' Notable, we do *not* reallocate the buffer if the values changed,','line_number':592,'multiline':False]['text':' but the number of dimensions stayed the same (e.g. `.transpose_()`).','line_number':593,'multiline':False]['text':' We do the smallvector optimization here: any time the new_size is <=5,','line_number':616,'multiline':False]['text':' we always allocate our buffer to size 5, so that if the next resize','line_number':617,'multiline':False]['text':' is also to <=5 elements, we don't need to reallocate.','line_number':618,'multiline':False]['text':' Note: I tried removing this optimization and tripped ASAN','line_number':619,'multiline':False]['text':' in a batchnorm kernel here:','line_number':620,'multiline':False]['text':' https://pipelinesghubeus21.actions.githubusercontent.com/mBh68xKhi8LyM7tp3vECvYXNFvuV4gyVGgmYCteuEZP9JH92QN/_apis/pipelines/1/runs/3373307/signedlogcontent/790?urlExpires=2023-09-15T21%3A13%3A51.4327798Z&urlSigningMethod=HMACV1&urlSignature=tDeX7ZqaARVU5NNwyr5yYqqkWq3A2j4z8FFdqYwGr0Q%3D','line_number':621,'multiline':False]['text':' We should fix this instead.','line_number':622,'multiline':False]['text':' We need to resize if:','line_number':624,'multiline':False]['text':' (1) we haven't allocated our buffer at all yet','line_number':625,'multiline':False]['text':' (2) Our buffer size is different from the new size','line_number':626,'multiline':False]['text':'     (note: we use the small vector optimization, where our buffer','line_number':627,'multiline':False]['text':'     is always allocated to at least size 5, and any resizes','line_number':628,'multiline':False]['text':'     within the <= 5 regime to not require a reallocation).','line_number':629,'multiline':False]['text':' If our current buffer is not the right size (either because we haven't','line_number':634,'multiline':False]['text':' allocated it yet, or there was a metadata mutation that changed the','line_number':635,'multiline':False]['text':' number of dims of the tensor), allocate a fresh buffer. Note that this','line_number':636,'multiline':False]['text':' will trash the previous buffer if there already was one, invalidating any','line_number':637,'multiline':False]['text':' existing SymIntArrayRef's from an old .sym_size() call.','line_number':638,'multiline':False]['text':' This is the smallvector optimization','line_number':641,'multiline':False]['text':' Set the buffer','line_number':651,'multiline':False]['text':' Set the len buffer','line_number':653,'multiline':False]['text':' Overwrite the buffer with our new values, but only if any of them changed','line_number':662,'multiline':False]['text':' (due to a metadata mutation).','line_number':663,'multiline':False]['text':' This is technically not thread safe, because the update happens lazily.','line_number':664,'multiline':False]['text':' The original metadata mutation call on the tensor might have been thread','line_number':665,'multiline':False]['text':' safe (e.g. a .resize_() call), but we won't actually mutate the size','line_number':666,'multiline':False]['text':' buffer until the first call to .sizes() which the user might not access','line_number':667,'multiline':False]['text':' in a thread-safe way. For now we are not explicitly locking, but maybe we','line_number':668,'multiline':False]['text':' should.','line_number':669,'multiline':False]['text':' Quick sanity assert that our buffer size is large enough','line_number':671,'multiline':False]['text':' to compare against all the elements in the new buffer.','line_number':672,'multiline':False]['text':' if our SymInts are symbolic, we are *not* doing an equality check on','line_number':681,'multiline':False]['text':' the symints. we just want to see if the nodes are the same. this is','line_number':682,'multiline':False]['text':' because we don't want to introduce any guards here.','line_number':683,'multiline':False]['text':' The correct data is now stored at the buffer - read and return it.','line_number':695,'multiline':False]['text':' See Note [Tensor Subclass custom size/stride caching strategy]','line_number':787,'multiline':False]['text':' We need to squeeze SymIntNodes and ints into `SymInts`','line_number':904,'multiline':False]['text':' since it's a format `sym_strides()` are stored in','line_number':905,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':923,'multiline':False]['text':' anonymous namespace','line_number':933,'multiline':False]