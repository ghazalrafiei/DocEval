['text':' Note [Replicating Modules]','line_number':29,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' Module replication is implemented in the following two steps:','line_number':32,'multiline':False]['text':' 1) create a module replica on each destination device using Module.clone().','line_number':33,'multiline':False]['text':' 2) manually add a gradient edge pointing from every parameter X in every','line_number':34,'multiline':False]['text':'    module replica to the same parameter X in the original module, using','line_number':35,'multiline':False]['text':'    ReduceAdd as the grad_fn.','line_number':36,'multiline':False]['text':'','line_number':37,'multiline':False]['text':' ReduceAdd can ONLY be used during the backward pass of data parallel. Forward','line_number':38,'multiline':False]['text':' pass cannot use this function as it does not setup gradient function and','line_number':39,'multiline':False]['text':' history at all. Do NOT try to use ReduceAdd for any other purposes.','line_number':40,'multiline':False]['text':'','line_number':41,'multiline':False]['text':' NB: An alternative is to add Broadcast and ReduceAddCoalesce to','line_number':42,'multiline':False]['text':' torch/csrc/autograd/functions/comm.cpp as normal autograd functions,','line_number':43,'multiline':False]['text':' implement a Replicatable (like cloneable) class and add it as a friend class','line_number':44,'multiline':False]['text':' in Module.h. In the forward pass, the Replicatable could use the Broadcast','line_number':45,'multiline':False]['text':' function to replicate every module parameter and set gradient functions using','line_number':46,'multiline':False]['text':' ReduceAddCoalesce (like how it is implemented in Python). However, unlike in','line_number':47,'multiline':False]['text':' Python, where changes to Linear._parameters["weight"] would also apply to','line_number':48,'multiline':False]['text':' Linear.weight (using Linear as an example), Linear.weight and','line_number':49,'multiline':False]['text':' Linear.parameters_["weight"] are two tensor objects pointing to the same','line_number':50,'multiline':False]['text':' TensorImpl. Assigning a new tensor to Linear.parameters_["weight"] will not','line_number':51,'multiline':False]['text':' change Linear.weight. To make this work, we will have to:','line_number':52,'multiline':False]['text':' 1) force every module to also inherit from Replicatable','line_number':53,'multiline':False]['text':' 2) force every module to implement an additional function, e.g.,','line_number':54,'multiline':False]['text':'    Replicatable::load_params(), to pick up changes from parameters_ to their','line_number':55,'multiline':False]['text':'    own member fields.','line_number':56,'multiline':False]['text':' This will be an overkill as Replicatable will only be used in data_parallel,','line_number':57,'multiline':False]['text':' not even ddp.','line_number':58,'multiline':False]['text':' Autograd function for the replicate step in data parallel. This is only used','line_number':60,'multiline':False]['text':' in data parallel, and should not be exposed as a user API.','line_number':61,'multiline':False]['text':' TODO: use nccl reduce','line_number':89,'multiline':False]['text':' namespace','line_number':100,'multiline':False]['text':' A friend function to Module, it recursively sets gradient edges pointing from','line_number':102,'multiline':False]['text':' every parameter X in every module replica to the same parameter X in the','line_number':103,'multiline':False]['text':' original module. See [Replicating Modules]','line_number':104,'multiline':False]['text':'recurse=','line_number':110,'multiline':True]['text':'recurse=','line_number':119,'multiline':True]['text':' recursively set gradient edges for all children','line_number':137,'multiline':False]['text':'/ Replicates a module on the given list of devices.','line_number':144,'multiline':False]['text':'/ A replica is created by calling `clone()` on the module. For this, the','line_number':145,'multiline':False]['text':'/ module must inherit from `nn::Cloneable`, or define its own `clone()`','line_number':146,'multiline':False]['text':'/ method, which is expected to perform a deep copy of the module.','line_number':147,'multiline':False]['text':' Configure gradient edges to point from replcia parameters to original','line_number':158,'multiline':False]['text':' module parameters. See [Replicating Modules]','line_number':159,'multiline':False]['text':'/ Replicates a module holder on the given list of devices.','line_number':164,'multiline':False]['text':'/ This method allows calling `replicate()` with a module holder, such as','line_number':165,'multiline':False]['text':'/ `Linear`.','line_number':166,'multiline':False]['text':'/ Applies the given inputs to the given modules in a parallel fashion.','line_number':175,'multiline':False]['text':'/ Conceptually, a thread is spawned for each `(module, input)` pair, in which','line_number':176,'multiline':False]['text':'/ `forward()` is called on the module with its corresponding input. The','line_number':177,'multiline':False]['text':'/ outputs of the individual calls are stored in a vector and returned.','line_number':178,'multiline':False]['text':'/','line_number':179,'multiline':False]['text':'/ The first exception caught by any thread is stashed and rethrown after all','line_number':180,'multiline':False]['text':'/ threads have completed their operation.','line_number':181,'multiline':False]['text':'/','line_number':182,'multiline':False]['text':'/ Further remarks:','line_number':183,'multiline':False]['text':'/ 1. The length of the module container must match the length of the inputs.','line_number':184,'multiline':False]['text':'/ 2. If a list of devices is supplied, it must match the list of modules in','line_number':185,'multiline':False]['text':'/ length. Each device will be set to the current default device during the','line_number':186,'multiline':False]['text':'/ invocation of the respective module. This means any tensors allocated on the','line_number':187,'multiline':False]['text':'/ default device inside the module will be constructed on this device.','line_number':188,'multiline':False]['text':' std::exception_ptr can be passed between threads:','line_number':205,'multiline':False]['text':' > An instance of std::exception_ptr may be passed to another function,','line_number':206,'multiline':False]['text':' > possibly on another thread, where the exception may be rethrown [...].','line_number':207,'multiline':False]['text':' https://en.cppreference.com/w/cpp/error/exception_ptr','line_number':208,'multiline':False]['text':'begin=','line_number':212,'multiline':True]['text':'end=','line_number':213,'multiline':True]['text':'grain_size=','line_number':214,'multiline':True]['text':'/ Evaluates `module(input)` in parallel across the given `devices`. If','line_number':240,'multiline':False]['text':'/ `devices` is not supplied, the invocation is parallelized across all','line_number':241,'multiline':False]['text':'/ available CUDA devices. If `output_device` is supplied, the final, combined','line_number':242,'multiline':False]['text':'/ tensor will be placed on this device. If not, it defaults to the first','line_number':243,'multiline':False]['text':'/ device in `devices`.','line_number':244,'multiline':False]['text':'/','line_number':245,'multiline':False]['text':'/ In detail, this method performs the following four distinct steps:','line_number':246,'multiline':False]['text':'/ 1. *Scatter* the input to the given devices,','line_number':247,'multiline':False]['text':'/ 2. *Replicate* (deep clone) the model on each device,','line_number':248,'multiline':False]['text':'/ 3. *Evaluate* each module with its input on its device,','line_number':249,'multiline':False]['text':'/ 4. *Gather* the outputs of each replica into a single output tensor, located','line_number':250,'multiline':False]['text':'/ on the `output_device`.','line_number':251,'multiline':False]['text':'chunk_sizes=','line_number':279,'multiline':True]['text':' Input tensor might not be big enough to scale across all available devices','line_number':281,'multiline':False]['text':' namespace parallel','line_number':295,'multiline':False]['text':' namespace nn','line_number':296,'multiline':False]['text':' namespace torch','line_number':297,'multiline':False]