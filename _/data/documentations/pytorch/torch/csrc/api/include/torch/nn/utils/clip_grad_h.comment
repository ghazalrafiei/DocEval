['text':' Clips gradient norm of a vector of Tensors.','line_number':11,'multiline':False]['text':' See','line_number':12,'multiline':False]['text':' https://pytorch.org/docs/stable/nn.html?highlight=clip_grad_norm#torch.nn.utils.clip_grad_norm_','line_number':13,'multiline':False]['text':' for more details about this module.','line_number':14,'multiline':False]['text':'','line_number':15,'multiline':False]['text':' Difference with the python version: unlike the python version, even when','line_number':16,'multiline':False]['text':' skipping the finiteness checks (error_if_nonfinite = false), this function','line_number':17,'multiline':False]['text':' will introduce a device <=> CPU synchronization (for devices where that makes','line_number':18,'multiline':False]['text':' sense!) in order to return a CPU-side `double`. This C++ version therefore','line_number':19,'multiline':False]['text':' cannot be run fully asynchronously w.r.t. the device of the gradients.','line_number':20,'multiline':False]['text':' When possible (ie when skipping the finiteness check), we avoid','line_number':63,'multiline':False]['text':' synchronizing the CPU and the gradients' device until the very end to','line_number':64,'multiline':False]['text':' preserve async execution on the device. When checking for finite-ness, this','line_number':65,'multiline':False]['text':' optional ensures we only sync once.','line_number':66,'multiline':False]['text':' min ','line_number':82,'multiline':True]['text':' max ','line_number':82,'multiline':True]['text':' A wrapper around clip_grad_norm_ that allows us to call the function with a','line_number':93,'multiline':False]['text':' braced-init-list of Tensors.','line_number':94,'multiline':False]['text':' A wrapper around clip_grad_norm_ that allows us to call the function with a','line_number':104,'multiline':False]['text':' single Tensor.','line_number':105,'multiline':False]['text':' Clips gradient of an iterable of parameters at specified value.','line_number':116,'multiline':False]['text':' Gradients are modified in-place.','line_number':117,'multiline':False]['text':' See https://pytorch.org/docs/stable/nn.html#clip-grad-value','line_number':118,'multiline':False]['text':' for more details about this module.','line_number':119,'multiline':False]['text':' A wrapper around clip_grad_value_ that allows us to call the function with a','line_number':130,'multiline':False]['text':' braced-init-list of Tensors.','line_number':131,'multiline':False]['text':' A wrapper around clip_grad_value_ that allows us to call the function with a','line_number':138,'multiline':False]['text':' single Tensor.','line_number':139,'multiline':False]['text':' namespace utils','line_number':145,'multiline':False]['text':' namespace nn','line_number':146,'multiline':False]['text':' namespace torch','line_number':147,'multiline':False]