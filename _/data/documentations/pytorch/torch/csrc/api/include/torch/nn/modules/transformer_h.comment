['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Transformer ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':16,'multiline':False]['text':'/ A transformer model. User is able to modify the attributes as needed. The','line_number':18,'multiline':False]['text':'/ architecture is based on the paper "Attention Is All You Need". Ashish','line_number':19,'multiline':False]['text':'/ Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N','line_number':20,'multiline':False]['text':'/ Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.','line_number':21,'multiline':False]['text':'/ In Advances in Neural Information Processing Systems, pages 6000-6010.','line_number':22,'multiline':False]['text':'/','line_number':23,'multiline':False]['text':'/ See https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html to','line_number':24,'multiline':False]['text':'/ learn about the exact behavior of this transformer model','line_number':25,'multiline':False]['text':'/','line_number':26,'multiline':False]['text':'/ See the documentation for `torch::nn::Transformer` class to learn what','line_number':27,'multiline':False]['text':'/ constructor arguments are supported for this encoder layer model','line_number':28,'multiline':False]['text':'/','line_number':29,'multiline':False]['text':'/ Example:','line_number':30,'multiline':False]['text':'/ ```','line_number':31,'multiline':False]['text':'/ Transformer trans(TransformerOptions(512, 8));','line_number':32,'multiline':False]['text':'/ ```','line_number':33,'multiline':False]['text':'/ forward function for Transformer Module','line_number':38,'multiline':False]['text':'/ Args:','line_number':39,'multiline':False]['text':'/   src: the sequence to the encoder (required).','line_number':40,'multiline':False]['text':'/   tgt: the sequence to the decoder (required).','line_number':41,'multiline':False]['text':'/   src_mask: the additive mask for the src sequence (optional).','line_number':42,'multiline':False]['text':'/   tgt_mask: the additive mask for the tgt sequence (optional).','line_number':43,'multiline':False]['text':'/   memory_mask: the additive mask for the encoder output (optional).','line_number':44,'multiline':False]['text':'/   src_key_padding_mask: the ByteTensor mask for src keys per batch','line_number':45,'multiline':False]['text':'/   (optional). tgt_key_padding_mask: the ByteTensor mask for tgt keys per','line_number':46,'multiline':False]['text':'/   batch (optional). memory_key_padding_mask: the ByteTensor mask for','line_number':47,'multiline':False]['text':'/   memory keys per batch (optional).','line_number':48,'multiline':False]['text':'/','line_number':49,'multiline':False]['text':'/ Shape:','line_number':50,'multiline':False]['text':'/   src: `(S, N, E)`','line_number':51,'multiline':False]['text':'/   tgt: `(T, N, E)`','line_number':52,'multiline':False]['text':'/   src_mask: `(S, S)`','line_number':53,'multiline':False]['text':'/   tgt_mask: `(T, T)`','line_number':54,'multiline':False]['text':'/   memory_mask: `(T, S)`','line_number':55,'multiline':False]['text':'/   src_key_padding_mask: `(N, S)`','line_number':56,'multiline':False]['text':'/   tgt_key_padding_mask: `(N, T)`','line_number':57,'multiline':False]['text':'/   memory_key_padding_mask: `(N, S)`','line_number':58,'multiline':False]['text':'/','line_number':59,'multiline':False]['text':'/   Note:','line_number':60,'multiline':False]['text':'/     [src/tgt/memory]_mask ensures that position i is allowed to attend the','line_number':61,'multiline':False]['text':'/     unmasked positions. If a ByteTensor is provided, the non-zero','line_number':62,'multiline':False]['text':'/     positions are not allowed to attend while the zero positions will be','line_number':63,'multiline':False]['text':'/     unchanged. If a BoolTensor is provided, positions with `True` are not','line_number':64,'multiline':False]['text':'/     allowed to attend while `False` values will be unchanged. If a','line_number':65,'multiline':False]['text':'/     FloatTensor is provided, it will be added to the attention weight.','line_number':66,'multiline':False]['text':'/','line_number':67,'multiline':False]['text':'/     [src/tgt/memory]_key_padding_mask provides specified elements in the','line_number':68,'multiline':False]['text':'/     key to be ignored by the attention. If a ByteTensor is provided, the','line_number':69,'multiline':False]['text':'/     non-zero positions will be ignored while the zero positions will be','line_number':70,'multiline':False]['text':'/     unchanged. If a BoolTensor is provided, the positions with the value','line_number':71,'multiline':False]['text':'/     of `True` will be ignored while the position with the value of `False`','line_number':72,'multiline':False]['text':'/     will be unchanged.','line_number':73,'multiline':False]['text':'/','line_number':74,'multiline':False]['text':'/   output: `(T, N, E)`','line_number':75,'multiline':False]['text':'/','line_number':76,'multiline':False]['text':'/   Note:','line_number':77,'multiline':False]['text':'/     Due to the multi-head attention architecture in the transformer model,','line_number':78,'multiline':False]['text':'/     the output sequence length of a transformer is same as the input','line_number':79,'multiline':False]['text':'/     sequence (i.e. target) length of the decode.','line_number':80,'multiline':False]['text':'/','line_number':81,'multiline':False]['text':'/   where','line_number':82,'multiline':False]['text':'/   S is the source sequence length,','line_number':83,'multiline':False]['text':'/   T is the target sequence length,','line_number':84,'multiline':False]['text':'/   N is the batch size,','line_number':85,'multiline':False]['text':'/   E is the feature number.','line_number':86,'multiline':False]['text':'/ Generate a square mask for the sequence.','line_number':101,'multiline':False]['text':'/ The masked positions are filled with `-inf` in float type.','line_number':102,'multiline':False]['text':'/ Unmasked positions are filled with `0.0` in float type.','line_number':103,'multiline':False]['text':'/ Note:','line_number':104,'multiline':False]['text':'/   1. This function will always return a CPU tensor.','line_number':105,'multiline':False]['text':'/   2. This function requires the platform support IEEE754, since `-inf` is','line_number':106,'multiline':False]['text':'/   guaranteed to','line_number':107,'multiline':False]['text':'/      be valid only when IEEE754 is supported. If the platform doesn't','line_number':108,'multiline':False]['text':'/      support IEEE754, this function will fill the mask with the smallest','line_number':109,'multiline':False]['text':'/      float number instead of `-inf`, a one time warning will pop up as','line_number':110,'multiline':False]['text':'/      well.','line_number':111,'multiline':False]['text':'/ options with which this `Transformer` was constructed','line_number':124,'multiline':False]['text':'/ encoder module','line_number':127,'multiline':False]['text':'/ decoder module','line_number':130,'multiline':False]['text':'/ A `ModuleHolder` subclass for `TransformerImpl`.','line_number':134,'multiline':False]['text':'/ See the documentation for `TransformerImpl` class to learn what','line_number':135,'multiline':False]['text':'/ methods it provides, and examples of how to use `Transformer` with','line_number':136,'multiline':False]['text':'/ `torch::nn::TransformerOptions`.','line_number':137,'multiline':False]['text':'/ See the documentation for `ModuleHolder` to learn about PyTorch's','line_number':138,'multiline':False]['text':'/ module storage semantics.','line_number':139,'multiline':False]['text':' namespace nn','line_number':142,'multiline':False]['text':' namespace torch','line_number':143,'multiline':False]