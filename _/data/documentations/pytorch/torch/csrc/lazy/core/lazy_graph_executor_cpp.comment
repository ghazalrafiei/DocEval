['text':' PyTorch currently has an issue comparing tensors which have NaN values in','line_number':40,'multiline':False]['text':' it. The compare is not deterministic. So we do memory compare here until','line_number':41,'multiline':False]['text':' the PyTorch equal() API is fixed.','line_number':42,'multiline':False]['text':' Return true if any tensor in the list has an underlying IR (leaf or','line_number':51,'multiline':False]['text':' operation).','line_number':52,'multiline':False]['text':' namespace','line_number':63,'multiline':False]['text':' Keep the running seed as scalar as well, so we can return it directly','line_number':112,'multiline':False]['text':' without executing graphs.','line_number':113,'multiline':False]['text':' Compose new seeds from the root seed, to avoid creating too many','line_number':115,'multiline':False]['text':' computation parameters which might overflow the device capacity.','line_number':116,'multiline':False]['text':' Workaround since at::scalar_tensor doesn't support bfloat16 yet.','line_number':307,'multiline':False]['text':' If cache is disabled then always return a zero size cache','line_number':342,'multiline':False]['text':'sync_ltc_data=','line_number':408,'multiline':True]['text':' Move TrieCache's current pointer back to its root','line_number':431,'multiline':False]['text':' TODO: Remove the last use of Device(const std::string& device_spec).','line_number':443,'multiline':False]['text':' The LockDevices() API returns a vector of','line_number':447,'multiline':False]['text':' ExceptionCleanup object, which is going to be freed','line_number':448,'multiline':False]['text':' immediately, turning this operation into a lock barrier.','line_number':449,'multiline':False]['text':' NOLINTNEXTLINE','line_number':450,'multiline':False]['text':'tensor_id=','line_number':487,'multiline':True]['text':'read_only=','line_number':487,'multiline':True]['text':'tensor_id=','line_number':499,'multiline':True]['text':'read_only=','line_number':499,'multiline':True]['text':'is_scalar_expand=','line_number':530,'multiline':True]['text':' Accessing other Async members is safe only after MultiWait::Wait()','line_number':550,'multiline':False]['text':' completes.','line_number':551,'multiline':False]['text':' If we observe the status here, no need to let it propagate to the next','line_number':559,'multiline':False]['text':' device lock operation.','line_number':560,'multiline':False]['text':' The force_ltc_data controls aliasing compilation, so effectively the same','line_number':592,'multiline':False]['text':' graph with on/off force_ltc_data should not match, hash wise.','line_number':593,'multiline':False]['text':' Add only tensors which need to be synced.','line_number':606,'multiline':False]['text':' The tensor only has at::Tensor data. We need to queue it for a','line_number':611,'multiline':False]['text':' device upload.','line_number':612,'multiline':False]['text':' If we are here, it means that the IR Value for the tensor is not','line_number':626,'multiline':False]['text':' present. Also, we uploaded the at::Tensor data to the device, but such','line_number':627,'multiline':False]['text':' data is still valid so we leave it live on the lazy tensor (so that a','line_number':628,'multiline':False]['text':' following ToTensor() does not need to fetch it from device).','line_number':629,'multiline':False]['text':' If the config.force_ltc_data flag is true, the purpose of this tensor','line_number':681,'multiline':False]['text':' sync operation is to truncate the IR graph and materialize device data in','line_number':682,'multiline':False]['text':' place of IR graph, on selected tensors. But since operation will complete','line_number':683,'multiline':False]['text':' asynchronously, if a tensor does not already have device data, we need to','line_number':684,'multiline':False]['text':' install a placeholder. Since at this point we hold a lock on the device','line_number':685,'multiline':False]['text':' where the tensors reside (locks held within the coll structure, and moved','line_number':686,'multiline':False]['text':' into the async variable), any other operation trying to access the','line_number':687,'multiline':False]['text':' tensor's device data will have to wait until the asynchronous operation','line_number':688,'multiline':False]['text':' completes.','line_number':689,'multiline':False]['text':' Note: We are not using SetHandleData method here since that method','line_number':693,'multiline':False]['text':' resets the ir_value. We have already done the resetting as part','line_number':694,'multiline':False]['text':' of ExtractIRAndPrepareTensorData to overlap with previous execution.','line_number':695,'multiline':False]['text':' Acceptable race condition: HasValue may return false. This is OK
       * since the conditional barrier is a performance optimization. ','line_number':718,'multiline':True]['text':' If force_ltc_data is true it means that we did a proper sync and are','line_number':778,'multiline':False]['text':' inside a mark step. If GetTensors was called, force_ltc_data will','line_number':779,'multiline':False]['text':' be false meaning we are prematurely evaluating some value.','line_number':780,'multiline':False]['text':' TODO(whc) should computation be allowed null here? (because it is in one','line_number':790,'multiline':False]['text':' case)','line_number':791,'multiline':False]['text':'device=','line_number':798,'multiline':True]['text':'emitted_nodes=','line_number':799,'multiline':True]['text':'computation=','line_number':800,'multiline':True]['text':'parameters_data=','line_number':801,'multiline':True]['text':' Enure previous execution is complete before exiting this
     * function ','line_number':834,'multiline':True]['text':' There are two paths of discovery of an exception happening on an','line_number':917,'multiline':False]['text':' asynchronous task. One happens if the creator of the asynchronous task','line_number':918,'multiline':False]['text':' explicitly waits for completion, in which case the exception will be','line_number':919,'multiline':False]['text':' thrown from the Wait() API. Re-throwing the exception below makes sure','line_number':920,'multiline':False]['text':' this will be captured by the completer function created below, and','line_number':921,'multiline':False]['text':' surfaced by the Wait() API. But we also need to surface the exception','line_number':922,'multiline':False]['text':' even in case the caller does not wait, and that is accomplished by','line_number':923,'multiline':False]['text':' setting the unlockers status. In that case the exception will be','line_number':924,'multiline':False]['text':' surfaced when the user tries to acquire the device locks the next time.','line_number':925,'multiline':False]['text':' This gets tensors from the backend','line_number':973,'multiline':False]['text':' for TS backend, we'd ideally just cut through these layers and','line_number':974,'multiline':False]['text':' not need to copy the tensor, just move it','line_number':975,'multiline':False]['text':' for XLA backend, a copy is going to have to happen,','line_number':977,'multiline':False]['text':' could we replace the 'Data' object with an at::Tensor, which is 'undefined'','line_number':979,'multiline':False]['text':' unless a backend attaches a buffer to it?  That way we can have a','line_number':980,'multiline':False]['text':' 'PopulateTensor' method on backend, which can either attach an existing','line_number':981,'multiline':False]['text':' tensor buffer to the wrapper, or copy data?','line_number':982,'multiline':False]['text':' Current tensor is a duplicate of a previously processed tensor that had','line_number':1025,'multiline':False]['text':' an IR Node to sync. Get the data from the tensor_data_map.','line_number':1026,'multiline':False]['text':' If we are at the current index (it means that the tensor at index','line_number':1029,'multiline':False]['text':' 'i' had an IR node to sync), use the data held within the Async','line_number':1030,'multiline':False]['text':' object.','line_number':1031,'multiline':False]['text':' Temp solution to idetify unassigned devices ','line_number':1047,'multiline':True]['text':' namespace lazy','line_number':1078,'multiline':False]['text':' namespace torch','line_number':1079,'multiline':False]