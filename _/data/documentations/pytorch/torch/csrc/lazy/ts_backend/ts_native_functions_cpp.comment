['text':' namespace','line_number':50,'multiline':False]['text':' clone is special in LT because we make it a no-op.','line_number':52,'multiline':False]['text':' This should be safe to do, because every operator in the LT is functional.','line_number':53,'multiline':False]['text':' providing a new 'eager' value (self) for an existing lazy tensor (dst)','line_number':70,'multiline':False]['text':'sync=','line_number':73,'multiline':True]['text':' materializing a lazy tensor (self) and copying its value into eager','line_number':75,'multiline':False]['text':' tensor (dst) detached=false lets us skip a copy in `ToTensor`, which','line_number':76,'multiline':False]['text':' should be safe because we are only going to use the tensor for','line_number':77,'multiline':False]['text':' dst.copy_()','line_number':78,'multiline':False]['text':'detached=','line_number':80,'multiline':True]['text':'copy=','line_number':82,'multiline':True]['text':' Copying one lazy tensor to another','line_number':85,'multiline':False]['text':' if dest is not backed by IR (e.g. result of some lazy operation),','line_number':87,'multiline':False]['text':' then it should have at::Tensor data backing it instead','line_number':88,'multiline':False]['text':' both src/dst are simply backed by at::Tensor data, no IR- do a','line_number':93,'multiline':False]['text':' straightforward copy','line_number':94,'multiline':False]['text':' src needs to be materialized before its result can be used for a copy','line_number':97,'multiline':False]['text':' into dst since we use the src tensor only for making a copy, we don't','line_number':98,'multiline':False]['text':' need to detach it note: it would be even more efficient if we could','line_number':99,'multiline':False]['text':' cause ToTensor to materialize the value directly into dst's buffer','line_number':100,'multiline':False]['text':' (that would need to be detached though).','line_number':101,'multiline':False]['text':'detached=','line_number':102,'multiline':True]['text':'detached=','line_number':125,'multiline':True]['text':'copy=','line_number':127,'multiline':True]['text':' at this point we know dst is a lazy tensor','line_number':130,'multiline':False]['text':' I put each of these setters in a conditional instead of doing','line_number':155,'multiline':False]['text':' `self.options().dtype(dtype).layout(layout)... because calling','line_number':156,'multiline':False]['text':' .dtype(nullopt) on an options() that already has dtype appears to wipe it','line_number':157,'multiline':False]['text':' TODO(whc) can we honor 'pin_memory' in some/all cases?','line_number':167,'multiline':False]['text':' Case 1: eager->lazy (we create a new lazy tensor)','line_number':176,'multiline':False]['text':' See Note [Lazy Tensor Functionalization]','line_number':177,'multiline':False]['text':' Invariant: if the functionalization key is in the exclude set, then we're','line_number':178,'multiline':False]['text':' expected to return an ordinary tensor, which will be "lifted" into a','line_number':179,'multiline':False]['text':' functional wrapper later.','line_number':180,'multiline':False]['text':'non_blocking=','line_number':188,'multiline':True]['text':'functionalize_output=','line_number':189,'multiline':True]['text':' Case 2: lazy->eager (forces a graph break since we are materializing a','line_number':191,'multiline':False]['text':' tensor)','line_number':192,'multiline':False]['text':'detached=','line_number':195,'multiline':True]['text':'non_blocking=','line_number':198,'multiline':True]['text':'copy=','line_number':198,'multiline':True]['text':' Case 3: lazy:0 -> lazy:1','line_number':203,'multiline':False]['text':' TODO(whc) what do we actually want to do here?','line_number':205,'multiline':False]['text':'   option 1: materialize, move eager tensor, create new lazy tensor','line_number':206,'multiline':False]['text':'     - this should be our default, as it is what would happen before we','line_number':207,'multiline':False]['text':'     implemented _to_copy','line_number':208,'multiline':False]['text':'     - actually combines case 1 + case 2','line_number':209,'multiline':False]['text':'   option 2: support multiple devices inside one lazy/TS executor (case 4)','line_number':210,'multiline':False]['text':'     - but: we may have other assumptions that there is just one device','line_number':211,'multiline':False]['text':'     per executor? so don't take this lightly','line_number':212,'multiline':False]['text':'detached=','line_number':215,'multiline':True]['text':' we move the eager tensor to the 'eager' equivalent of our lazy device','line_number':216,'multiline':False]['text':' e.g. if our device is lazy:1, the backend maps that to cuda:1, which is','line_number':217,'multiline':False]['text':' what we use','line_number':218,'multiline':False]['text':'non_blocking=','line_number':223,'multiline':True]['text':'copy=','line_number':223,'multiline':True]['text':' Case 4: lazy->lazy (special case: keep the _to_copy INSIDE the lazy','line_number':230,'multiline':False]['text':' graph)','line_number':231,'multiline':False]['text':' Note: captured _to_copy will be executed with real eager tensors, not','line_number':233,'multiline':False]['text':' lazy tensors. We DO NOT want to burn 'lazy:0' as the device into this','line_number':234,'multiline':False]['text':' captured IR, or we will try to convert an eager tensor back to a lazy one','line_number':235,'multiline':False]['text':' inside the torchscript executor lazy:0 -> lazy:1 is handled in case3, so','line_number':236,'multiline':False]['text':' we can safely drop the device argument','line_number':237,'multiline':False]['text':' TODO: support this directly','line_number':278,'multiline':False]['text':' See Note [Lazy Tensor Functionalization]','line_number':288,'multiline':False]['text':' Invariant: if the functionalization key is in the exclude set, then we're','line_number':291,'multiline':False]['text':' expected to return an ordinary tensor, which will be "lifted" into a','line_number':292,'multiline':False]['text':' functional wrapper later.','line_number':293,'multiline':False]['text':'storage_offset=','line_number':313,'multiline':True]['text':' We need to explicitly override max pooling operators and just call the','line_number':336,'multiline':False]['text':' fallback for them because we've customized the autograd function for them','line_number':337,'multiline':False]['text':' (backward needs saved indices from forward).','line_number':338,'multiline':False]['text':' This is needed by the torch.tensor constructor.','line_number':382,'multiline':False]['text':' LazyTensor always opts into functionalization.','line_number':383,'multiline':False]['text':' "lifting" a tensor for functionalization means wrapping it in a','line_number':384,'multiline':False]['text':' FunctionalTensorWrapper object.','line_number':385,'multiline':False]['text':' All of the below ops correspond to CompositeExplicitAutograd kernels from','line_number':397,'multiline':False]['text':' core that call into view operators internally. These are all composite ops','line_number':398,'multiline':False]['text':' that LTC can technically re-use / get for free, but we need to','line_number':399,'multiline':False]['text':' "functionalize" them to remove the view ops before we can use them.','line_number':400,'multiline':False]['text':' functionalize_aten_op can't handle out= ops directly.','line_number':467,'multiline':False]['text':' Instead, we can call the composite kernel from core, and copy and mutations','line_number':468,'multiline':False]['text':' back to the inputs.','line_number':469,'multiline':False]['text':' directly call the composite kernel from core.','line_number':477,'multiline':False]['text':' Make sure to re-enable functionalization first.','line_number':478,'multiline':False]['text':' propagate mutations back to the inputs (including resizing)','line_number':488,'multiline':False]['text':' re-use the composite kernel from core, that way we don't need to provide a','line_number':524,'multiline':False]['text':' backwards formula for native_group_norm','line_number':525,'multiline':False]['text':' namespace lazy','line_number':539,'multiline':False]['text':' namespace torch','line_number':540,'multiline':False]