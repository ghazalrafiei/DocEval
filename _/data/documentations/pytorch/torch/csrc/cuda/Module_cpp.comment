['text':' True for children forked after cuda init','line_number':61,'multiline':False]['text':' Called in the forked child if cuda has already been initialized','line_number':64,'multiline':False]['text':' Should be called before the first cuda call.','line_number':71,'multiline':False]['text':' Note: This is distinct from initExtension because a stub cuda implementation','line_number':72,'multiline':False]['text':' has some working functions (e.g. device_count) but cannot fully initialize.','line_number':73,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':81,'multiline':False]['text':' CUDA management methods','line_number':82,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':83,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':134,'multiline':False]['text':' unused ','line_number':192,'multiline':True]['text':' unused ','line_number':215,'multiline':True]['text':' unused ','line_number':226,'multiline':True]['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)','line_number':257,'multiline':False]['text':' Unpack a PyObject to at::Scalar, throw an exception if it fails','line_number':323,'multiline':False]['text':' Zero-dim tensors are converted to Scalars as-is. Note this doesn't','line_number':325,'multiline':False]['text':' currently handle most NumPy scalar types except np.float64.','line_number':326,'multiline':False]['text':' Entrypoint for the callable created by torch.cuda.jiterator','line_number':345,'multiline':False]['text':' See jiterator.py for more details','line_number':346,'multiline':False]['text':' We need to ensure that as long as a thread will NEVER loose the GIL as long','line_number':480,'multiline':False]['text':' as it holds the CUDA mutex. Otherwise another thread might be scheduled and','line_number':481,'multiline':False]['text':' try to e.g. allocate a new tensor which will cause a deadlock. It's enough to','line_number':482,'multiline':False]['text':' have a single global, because it can be only set once (cudaMutex is not','line_number':483,'multiline':False]['text':' recursive) by the thread that owns the mutex (obviously there can be only one','line_number':484,'multiline':False]['text':' such thread).','line_number':485,'multiline':False]['text':' This has to be a busy loop because we **absolutely need to** hold the GIL','line_number':490,'multiline':False]['text':' or it's a recipe for a deadlock otherwise (if we let other Python threads','line_number':491,'multiline':False]['text':' run while we have the cudaMutex, but not the GIL, they might try to e.g.','line_number':492,'multiline':False]['text':' free a CUDA tensor and acquire the cudaMutex without giving up the GIL,','line_number':493,'multiline':False]['text':' because it happens deep within THC).','line_number':494,'multiline':False]['text':' we want the python objects to pickle easily so use an int to','line_number':695,'multiline':False]['text':' represent the stream rather than a torch.cuda.stream object','line_number':696,'multiline':False]['text':' without further compression frames can get really large on dump','line_number':776,'multiline':False]['text':' can't happen','line_number':853,'multiline':False]['text':' can't happen','line_number':871,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':876,'multiline':False]['text':' Cuda module initialization','line_number':877,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':878,'multiline':False]['text':' Add _cudaDevicePropertires class to torch._C','line_number':881,'multiline':False]['text':' NVIDA only property','line_number':896,'multiline':False]['text':' USE_ROCM','line_number':899,'multiline':False]['text':' HIP-only property; reuse name attribute for CUDA builds','line_number':900,'multiline':False]['text':' USE_ROCM','line_number':907,'multiline':False]['text':' USE_ROCM','line_number':915,'multiline':False]['text':' We choose to ignore certain blocks that are currently allocated','line_number':948,'multiline':False]['text':' when we set the pool to its checkpoint. For those blocks, we need','line_number':949,'multiline':False]['text':' to swap out the deleter function of their corresponding blocks','line_number':950,'multiline':False]['text':' so that a deallocation is not triggered when they die.','line_number':951,'multiline':False]['text':' iterate on std::vector for determinism','line_number':1204,'multiline':False]['text':' that block has already been freed,','line_number':1229,'multiline':False]['text':' so even those this will error, so too will the allocator','line_number':1230,'multiline':False]['text':' when the corresponding tensor dies because there is no','line_number':1231,'multiline':False]['text':' live tensor corresponding to it','line_number':1232,'multiline':False]['text':' Add method to torch.cuda','line_number':1249,'multiline':False]['text':' Callback for python part. Used for additional initialization of python','line_number':1259,'multiline':False]['text':' classes','line_number':1260,'multiline':False]['text':' Handled at python level','line_number':1270,'multiline':False]['text':' PyObject_SetAttrString doesn't steal reference. So no need to incref.','line_number':1281,'multiline':False]['text':' This reference is meant to be given away, so no need to incref here.','line_number':1295,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1309,'multiline':False]['text':' If there's no cuda context, at::cuda::currentStreamCaptureStatus returns','line_number':1344,'multiline':False]['text':' CaptureStatus::None without initializing a context.','line_number':1345,'multiline':False]['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,','line_number':1380,'multiline':False]['text':' cppcoreguidelines-avoid-non-const-global-variables,','line_number':1381,'multiline':False]['text':' cppcoreguidelines-avoid-c-arrays)','line_number':1382,'multiline':False]['text':' namespace shared','line_number':1536,'multiline':False]['text':' As weird as it seems, this file is also compiled for ROCm,','line_number':1540,'multiline':False]['text':' so this condition might not always be true...','line_number':1541,'multiline':False]['text':' namespace torch::cuda','line_number':1551,'multiline':False]