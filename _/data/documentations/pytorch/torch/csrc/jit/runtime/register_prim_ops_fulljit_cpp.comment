['text':' NOLINT','line_number':39,'multiline':False]['text':' NOLINT','line_number':48,'multiline':False]['text':' (...)  -> (..., bool) ','line_number':63,'multiline':True]['text':' if an rg property changes we assume a tensor does require','line_number':67,'multiline':False]['text':' gradients which is set in `guardDifferentiableGraph`','line_number':68,'multiline':False]['text':' Check every input's shape against profiled (expected) shape.','line_number':75,'multiline':False]['text':' NB: Chunk can sometimes return a smaller number of outputs.','line_number':107,'multiline':False]['text':' We know that the output is unused, so it's ok to push','line_number':125,'multiline':False]['text':' anything on the stack.','line_number':126,'multiline':False]['text':' This operator is generated inside the compiler for indexing into','line_number':175,'multiline':False]['text':' ModuleDict without a statically determinable key. Accordingly,','line_number':176,'multiline':False]['text':' self must be a ModuleType and the output must be an InterfaceType.','line_number':177,'multiline':False]['text':' (...)  -> (..., bool) ','line_number':188,'multiline':True]['text':' node ','line_number':189,'multiline':True]['text':' stack ','line_number':190,'multiline':True]['text':' NOLINT','line_number':191,'multiline':False]['text':' NOLINT','line_number':200,'multiline':False]['text':' stack ','line_number':210,'multiline':True]['text':' NOLINT','line_number':211,'multiline':False]['text':' TODO: today, we put a single bailout template at the front to','line_number':217,'multiline':False]['text':' carry the un-optimized graph for bailout nodes to use. Ideally','line_number':218,'multiline':False]['text':' this should never run, but we haven't written the code to remove','line_number':219,'multiline':False]['text':' it yet.','line_number':220,'multiline':False]['text':' TORCH_INTERNAL_ASSERT(false);','line_number':221,'multiline':False]['text':' Returns an int so that we have an easy way to do graph traversal','line_number':223,'multiline':False]['text':' NB: backward op might write to every input tensors in the graph and it's','line_number':263,'multiline':False]['text':' much more expensive to analyze the leaves and sometimes it might retain','line_number':264,'multiline':False]['text':' the whole gradients in every tensor of the Autograd graph with','line_number':265,'multiline':False]['text':' create_graph=True so we use aliasAnalysisConservative for these two OPs','line_number':266,'multiline':False]['text':' Pickle the tensor','line_number':294,'multiline':False]['text':' Write file','line_number':297,'multiline':False]['text':' TODO: remove this custom tracing code once the custom op bugfix','line_number':348,'multiline':False]['text':' lands','line_number':349,'multiline':False]['text':'num_outputs=','line_number':352,'multiline':True]['text':' TODO: remove this custom tracing code once the custom op bugfix','line_number':366,'multiline':False]['text':' lands','line_number':367,'multiline':False]['text':'num_outputs=','line_number':370,'multiline':True]['text':'allow_monotonic=','line_number':374,'multiline':True]['text':' We assume lists have homogenous types, use first element to determine','line_number':429,'multiline':False]['text':' best sorting methods. If in the future we need to support heterogenous','line_number':430,'multiline':False]['text':' types inside list, then sorting needs to have runtime sortable checks.','line_number':431,'multiline':False]['text':' Basic types like tensors/ints/floats/bools/strs are not checked in this','line_number':452,'multiline':False]['text':' method because they should have been schema matched to specialized','line_number':453,'multiline':False]['text':' aten::sort kernels using listSort<T>.','line_number':454,'multiline':False]['text':' NB: this must be registered after the other aten::sort operators','line_number':491,'multiline':False]['text':'has_reverse_arg','line_number':495,'multiline':True]['text':'copy_return_list','line_number':495,'multiline':True]['text':'has_reverse_arg','line_number':499,'multiline':True]['text':'copy_return_list','line_number':499,'multiline':True]['text':' reference: _output_size in torch/nn/functional.py','line_number':503,'multiline':False]['text':' size can be none, int or intlist','line_number':504,'multiline':False]['text':' scale_factors can be none, float, or floatlist','line_number':505,'multiline':False]['text':' return true if v is a real float','line_number':532,'multiline':False]['text':' and false if it is an integer','line_number':533,'multiline':False]['text':' reference: interpolate in torch/nn/functional.py','line_number':538,'multiline':False]['text':' size can be none, int or intlist','line_number':539,'multiline':False]['text':' scale_factors can be none, float, or floatlist','line_number':540,'multiline':False]['text':' only warn when the scales have floating values since','line_number':575,'multiline':False]['text':' the result for ints is the same with/without recompute_scale_factor','line_number':576,'multiline':False]['text':' only warn when the scales have floating values since','line_number':584,'multiline':False]['text':' the result for ints is the same with/without recompute_scale_factor','line_number':585,'multiline':False]['text':' interpolate takes in float & float[] for scale factor','line_number':736,'multiline':False]['text':' upsample takes in int & int[], so convert the ints to floats before','line_number':737,'multiline':False]['text':' passing on to the interpolate op','line_number':738,'multiline':False]['text':' These ops are no longer generated, but remain here for BC','line_number':798,'multiline':False]['text':' namespace','line_number':872,'multiline':False]['text':' namespace torch::jit','line_number':873,'multiline':False]