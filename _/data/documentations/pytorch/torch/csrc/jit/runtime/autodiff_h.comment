['text':' clang-format off','line_number':12,'multiline':False]['text':' Example showcasing how Gradient is constructed:','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':' Let's assume we have a function f, `m` and `n` do not require grad','line_number':15,'multiline':False]['text':' (`n` can depend only on `m`):','line_number':16,'multiline':False]['text':'   y, n = f(x, m)','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':' Now, let's assume that the reverse of f (called f') needs to use values of `x`, `t` and `y`.','line_number':19,'multiline':False]['text':' `t` is an intermediate value produced in the body of f, and let's assume that it requires','line_number':20,'multiline':False]['text':' grad too.','line_number':21,'multiline':False]['text':'','line_number':22,'multiline':False]['text':' In this case differentiate(f) will return this:','line_number':23,'multiline':False]['text':'   y, n, t = f(x, m)        // `t` is appended to the output list','line_number':24,'multiline':False]['text':'   dx = f'(dy, dt, x, t, y) // No `dm` or `dn` because they do not require gradient','line_number':25,'multiline':False]['text':'                            // All needed values from f are prepended to the input list','line_number':26,'multiline':False]['text':'','line_number':27,'multiline':False]['text':'   f_real_outputs = 2       // Only first two outputs were present in f originally','line_number':28,'multiline':False]['text':'   df_input_vjps = {0, 2}   // i.e. connect grad_fn of y and t variables produced by f,','line_number':29,'multiline':False]['text':'                    y  t    // with y's output_nr = 0 and t's output_nr = 1','line_number':30,'multiline':False]['text':'   df_input_captures = {I0, O2, O0} // Order matches the prefix of inputs to df','line_number':31,'multiline':False]['text':'                        x   t   y','line_number':32,'multiline':False]['text':'   df_output_vjps = {0}     // i.e. connect next_edge[0] of grad_fn to x's (grad_fn, output_nr).','line_number':33,'multiline':False]['text':'','line_number':34,'multiline':False]['text':' Terminology: vjp = vector-jacobian product','line_number':35,'multiline':False]['text':' clang-format on','line_number':36,'multiline':False]['text':' Describes how to construct outputs of f from what its graph will return.','line_number':45,'multiline':False]['text':' This is necessary because some trailing outputs are intermediates produced','line_number':46,'multiline':False]['text':' only to be saved for df (and should be ignored).','line_number':47,'multiline':False]['text':' initialized for safety.','line_number':48,'multiline':False]['text':' df inputs are split into two sections: vjps (aka grad_outputs) and','line_number':50,'multiline':False]['text':' captures. VJPs are "seeds" for the gradient computation given for each','line_number':51,'multiline':False]['text':' input capture of an Output kind. Captures are values the need to be saved','line_number':52,'multiline':False]['text':' when f is run. We handle inputs specially, because this allows us to avoid','line_number':53,'multiline':False]['text':' adding extra vjps as df inputs.','line_number':54,'multiline':False]['text':' Offsets into f's outputs.','line_number':56,'multiline':False]['text':' capture can come from inputs or outputs','line_number':57,'multiline':False]['text':' Offsets into f's inputs','line_number':58,'multiline':False]['text':' Offsets into f's outputs','line_number':59,'multiline':False]['text':' df will produce vjps for a subset of inputs of f that required grad.','line_number':61,'multiline':False]['text':' df_output_vjps[idx] == inp_idx means that idx-th output of df produces a','line_number':62,'multiline':False]['text':' vjp for inp_idx-th input of f.','line_number':63,'multiline':False]['text':' Offsets into f's inputs.','line_number':64,'multiline':False]['text':' How to use gradient to implement a differentiable autograd function:','line_number':66,'multiline':False]['text':' When running f:','line_number':67,'multiline':False]['text':'   - Unwrap input Variables','line_number':68,'multiline':False]['text':'   - Run f's graph','line_number':69,'multiline':False]['text':'   - Create grad_fn','line_number':70,'multiline':False]['text':'   - Wrap outputs in Variables (assume we have a tensor_outputs array):','line_number':71,'multiline':False]['text':'       outputs = map(Variable, tensor_output)','line_number':72,'multiline':False]['text':'       for i, offset in enumerate(df_input_vjps):','line_number':73,'multiline':False]['text':'         outputs[offset].set_grad_fn(grad_fn, output_nr=i)','line_number':74,'multiline':False]['text':'   - Use df_output_vjps to connect next_edges of grad_fn:','line_number':75,'multiline':False]['text':'       for idx in df_output_vjps:','line_number':76,'multiline':False]['text':'         grad_fn.add_next_edge(inputs[idx].gradient_edge())','line_number':77,'multiline':False]['text':'   - Save captures for df (care needs to be taken to use SavedVariables for','line_number':78,'multiline':False]['text':'                           inputs and outputs that we will actually return)','line_number':79,'multiline':False]['text':'   - Return outputs[:f_real_outputs]','line_number':80,'multiline':False]['text':'','line_number':81,'multiline':False]['text':' When running df:','line_number':82,'multiline':False]['text':'   - Concatenate received vjps and captured Variables','line_number':83,'multiline':False]['text':'   - Interpret df','line_number':84,'multiline':False]['text':'   - Wrap outputs of df into Variables (that don't require grad)','line_number':85,'multiline':False]['text':' can we take a derivative of this node symbolically?','line_number':89,'multiline':False]['text':' namespace torch::jit','line_number':94,'multiline':False]