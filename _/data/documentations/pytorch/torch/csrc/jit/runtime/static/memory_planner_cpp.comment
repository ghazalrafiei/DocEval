['text':' Heuristic and special case:','line_number':18,'multiline':False]['text':' If to_maybe_copy_out did not actually do anything in the','line_number':19,'multiline':False]['text':' first iteration, assume it will continue to not do anything','line_number':20,'multiline':False]['text':' and avoid managing its output.','line_number':21,'multiline':False]['text':' ival is allowed to be None in special cases, e.g. to_maybe_copy_out','line_number':41,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':49,'multiline':False]['text':' Don't change the size if it is already aligned, otherwise increase the size','line_number':57,'multiline':False]['text':' to make it aligned.','line_number':58,'multiline':False]['text':' Note: everything below is size_t','line_number':60,'multiline':False]['text':' namespace','line_number':69,'multiline':False]['text':' This set maps each Value* to its assigned storage group.','line_number':76,'multiline':False]['text':' On each iteration, this vector stores the set of storage groups that','line_number':78,'multiline':False]['text':' are available for re-use.','line_number':79,'multiline':False]['text':' Assign storage groups to outputs','line_number':104,'multiline':False]['text':' This node may be the last use of some managed tensors. If so, we','line_number':117,'multiline':False]['text':' can mark the corresponding storage groups as free.','line_number':118,'multiline':False]['text':' We need to check this here to handle special cases like','line_number':123,'multiline':False]['text':' to_maybe_copy_out. We don't know if the tensor value is managed until','line_number':124,'multiline':False]['text':' after the first iter, but `ranges` is initialized at load time!','line_number':125,'multiline':False]['text':' `size_` should already be 0 if not allocated, so double check it here','line_number':146,'multiline':False]['text':' namespace','line_number':201,'multiline':False]['text':' collect unmanaged output ivalues','line_number':213,'multiline':False]['text':' Manage output tensors might have been turned off, so we have to','line_number':226,'multiline':False]['text':' check the flag here','line_number':227,'multiline':False]['text':' Scalars do not need to be freed after each iteration.','line_number':236,'multiline':False]['text':' copy to unmanaged_ivalues_','line_number':257,'multiline':False]['text':' NOTE: Populating `ctx` enables clients to take the ownership of a','line_number':303,'multiline':False]['text':' tensor managed by Static Runtime. Some clients use "move" semantics to','line_number':304,'multiline':False]['text':' pass a Tensor object to another holding object (e.g., a thrift message)','line_number':305,'multiline':False]['text':' to avoid `memcpy`.','line_number':306,'multiline':False]['text':' `torch::distributed::detail::WireDumpOp::dumpTensorData is a concrete','line_number':307,'multiline':False]['text':' example of doing this (See `torch::distributed::detail::hasDeleter`).','line_number':308,'multiline':False]['text':' Since this output Tensor object is permanently owned by Static Runtime,','line_number':309,'multiline':False]['text':' this ownership passing does *not* have an intended effect of keeping the','line_number':310,'multiline':False]['text':' Tensor alive till the "owner" releases it: A premature call to','line_number':311,'multiline':False]['text':' `StaticRuntime::deallocateOutputTensors` can destruct such a Tensor','line_number':312,'multiline':False]['text':' object that a holding object believes to retain, causing it to read','line_number':313,'multiline':False]['text':' corrupted values from an already destructed Tensor object. Therefore, a','line_number':314,'multiline':False]['text':' client of receiving Static Runtime-managed Tensors needs to be very','line_number':315,'multiline':False]['text':' careful to call `StaticRuntime::deallocateOutputTensors` after these','line_number':316,'multiline':False]['text':' holding objects are gone.','line_number':317,'multiline':False]['text':'ctx=','line_number':319,'multiline':True]['text':' TODO: Improve this once D31357486 is landed.','line_number':327,'multiline':False]['text':' for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so','line_number':338,'multiline':False]['text':' that the objects pointed to by *iv may be reclaimed by reference counting','line_number':339,'multiline':False]['text':' It's important to call this function after all other owning refs','line_number':346,'multiline':False]['text':' of the managed StorageImpls are cleaned up. It can reset the','line_number':347,'multiline':False]['text':' the StorageImpl's refcount to (# tensors in storage group),','line_number':348,'multiline':False]['text':' so destructing any owning refs afterwards will bring the refcount','line_number':349,'multiline':False]['text':' lower than expected and trigger the debug assertion in','line_number':350,'multiline':False]['text':' ~intrusive_ptr_target.','line_number':351,'multiline':False]['text':' free memory used by outputs of ops in out variants','line_number':440,'multiline':False]['text':' but keep the TensorImpl and StorageImpl around.','line_number':441,'multiline':False]['text':' We don't have any guarantee that the model doesn't change the','line_number':443,'multiline':False]['text':' Storage for managed tensors out from under us during execution,','line_number':444,'multiline':False]['text':' so we have to check the Storages each time we deallocate.','line_number':445,'multiline':False]['text':' We want to manage StorageImpls' lifetimes ourselves, but TensorImpl','line_number':473,'multiline':False]['text':' expects to refcount them. unsafe_adapt_non_heap_allocated is our','line_number':474,'multiline':False]['text':' escape hatch: it sets the reference count for the StorageImpl to an','line_number':475,'multiline':False]['text':' impractically high value so that it will never get deallocated by','line_number':476,'multiline':False]['text':' intrusive_ptr, leaving us free to manage its lifetime as we see fit.','line_number':477,'multiline':False]['text':' (Note that allowing it to be deallocated by intrusive_ptr would be','line_number':478,'multiline':False]['text':' UB, because that would entail deleting an object that wasn't','line_number':479,'multiline':False]['text':' allocated with operator new.)','line_number':480,'multiline':False]['text':'','line_number':481,'multiline':False]['text':' For more information, see the doc comment for','line_number':482,'multiline':False]['text':' intrusive_ptr::unsafe_adapt_non_heap_allocated.','line_number':483,'multiline':False]['text':' If somehow the tensor got different storage, put it back to','line_number':490,'multiline':False]['text':' the shared impl for this group.','line_number':491,'multiline':False]['text':' Static runtime does not know the size of tensors statically, so we use','line_number':501,'multiline':False]['text':' the tensor size from the previous run to allocate tensors for the next','line_number':502,'multiline':False]['text':' run (following C2 tradition), exploiting the fact that tensor storage','line_number':503,'multiline':False]['text':' size does not have to match that of real tensor size. The following logic','line_number':504,'multiline':False]['text':' records the tensor storage size for the next run.','line_number':505,'multiline':False]['text':' namespace torch::jit','line_number':515,'multiline':False]