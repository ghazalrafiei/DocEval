['text':' TODO remove ifdef','line_number':93,'multiline':False]['text':' defer initial value so that we can load in gflags','line_number':100,'multiline':False]['text':' Initialize num_profiled_runs from command-line flag.','line_number':132,'multiline':False]['text':' Silence clang-tidy.','line_number':136,'multiline':False]['text':' Initialize bailout_depth from command-line flag.','line_number':141,'multiline':False]['text':' `prim::RequiresGradCheck` guarantees that requires_grad properties','line_number':173,'multiline':False]['text':' of input tensors will match the profiled, otherwise a fallback path','line_number':174,'multiline':False]['text':' will be triggered. This allow us to prune off gradients in backward','line_number':175,'multiline':False]['text':' graph for inputs that don't need gradients. We transfer requires_grad','line_number':176,'multiline':False]['text':' properties from inputs to the `prim::DifferentiableGraph` onto inputs to the','line_number':177,'multiline':False]['text':' differentiable graph. Autodiff will inspect these properties and prune','line_number':178,'multiline':False]['text':' off gradients that aren't required','line_number':179,'multiline':False]['text':' `requires_grad` properties from `dnode->outputs()` will also be transferred','line_number':180,'multiline':False]['text':' We also need to put requires_grad on outputs within subgraph, so autodiff','line_number':197,'multiline':False]['text':' can  set df_input_vjps and DifferentiableGraphOp can set `requires_grad=`','line_number':198,'multiline':False]['text':' properly','line_number':199,'multiline':False]['text':' Is it safe to not check other uses, because we are inside a','line_number':231,'multiline':False]['text':' DifferentiableGraph?','line_number':232,'multiline':False]['text':' The profiling node might have been absorbed in a preceding','line_number':260,'multiline':False]['text':' differentiable graph and thus not (not ideal for fusing either),','line_number':261,'multiline':False]['text':' see TestAutodiffSubgraphSlicing.test_does_not_create_cycles.','line_number':262,'multiline':False]['text':' Alternatives to this special casing could be specializing the types','line_number':263,'multiline':False]['text':' before autodiff or duplicating profile nodes for autodiff outputs','line_number':264,'multiline':False]['text':' but that should be done while creating subgraphs and would be','line_number':265,'multiline':False]['text':' a mess.','line_number':266,'multiline':False]['text':' XXX TODO: revisit the alternatives','line_number':267,'multiline':False]['text':' Propagate the requires_grad property to inputs','line_number':274,'multiline':False]['text':' A RequiresGrad check gets added (insertTypeGuard, below)','line_number':275,'multiline':False]['text':' so requires_grad is guaranteed to match for the inputs;','line_number':276,'multiline':False]['text':' but other properties are not guaranteed to match','line_number':277,'multiline':False]['text':' we check if the optional is defined','line_number':281,'multiline':False]['text':' we may have seen both true and false for requires_grad. In this case','line_number':286,'multiline':False]['text':' we guard with true here and the other case is in the fallback. This','line_number':287,'multiline':False]['text':' will give us trouble when we get "alternating patterns" of gradients','line_number':288,'multiline':False]['text':' of two inputs, but so it is. An alternative could be to look into','line_number':289,'multiline':False]['text':' the individual requires_grad seen in the profiling record.','line_number':290,'multiline':False]['text':' we inline the differentiable graph as a fallback','line_number':300,'multiline':False]['text':' ideally we would set this up for re-profiling','line_number':301,'multiline':False]['text':' runRequiredPasses','line_number':334,'multiline':False]['text':' runOptimization:','line_number':347,'multiline':False]['text':' run again with unrolled loops','line_number':367,'multiline':False]['text':' should never get here','line_number':395,'multiline':False]['text':' runNondiffOptimization','line_number':405,'multiline':False]['text':' Run custom passes that different backends can register.','line_number':407,'multiline':False]['text':' TupleConstruct / TupleUnpack pairs can still be present at this point','line_number':413,'multiline':False]['text':' and must be removed for fusion.','line_number':414,'multiline':False]['text':' Remove prim::profile nodes and embed the profile info directly in the','line_number':419,'multiline':False]['text':' IR in value types. We're doing such transformation as optimizations','line_number':420,'multiline':False]['text':' that try to merge/fuse nodes in the graph (e.g. BatchMM and GraphFuser)','line_number':421,'multiline':False]['text':' work worse in the presence of intermittent prim::profile nodes.','line_number':422,'multiline':False]['text':' Optimizations relying on the type info are also responsible for','line_number':423,'multiline':False]['text':' inserting proper type checks. Once we're done with these optimizations','line_number':424,'multiline':False]['text':' we will wipe the tensor type information from the IR, so that it's not','line_number':425,'multiline':False]['text':' accidentally used by any other pass.','line_number':426,'multiline':False]['text':' Rewrite subgraphs with many MMs into expressions that batch them.','line_number':431,'multiline':False]['text':' composed op','line_number':437,'multiline':True]['text':' Rewrite subgraphs with many MMs into expressions that batch them.','line_number':440,'multiline':False]['text':' Run custom post-fusion passes','line_number':448,'multiline':False]['text':' if we cannot guard (because of inputs without profiling information),','line_number':481,'multiline':False]['text':' we re-inline the subgraph and remove the differentiable node','line_number':482,'multiline':False]['text':' just like inside autograd.Functions, the forward of a differentiable','line_number':494,'multiline':False]['text':' graph is essentially in a torch.no_grad context.','line_number':495,'multiline':False]['text':' replaces fallback graphs inserted by TE Fuser','line_number':498,'multiline':False]['text':' TODO: maybe this can go later in pipeline / directly in autodiff forward','line_number':522,'multiline':False]['text':' creation','line_number':523,'multiline':False]['text':' clear any residual undefinedness','line_number':532,'multiline':False]['text':' as double backward graph inputs'','line_number':533,'multiline':False]['text':' may carry over undefinedness','line_number':534,'multiline':False]['text':' from profiled backward graphs','line_number':535,'multiline':False]['text':' runRequiredPasses','line_number':537,'multiline':False]['text':' Initialize bailout_depth from command-line flag.','line_number':586,'multiline':False]['text':' TODO: instantiate simple executor when getProfilingMode() is false','line_number':599,'multiline':False]['text':' no opt mode','line_number':600,'multiline':False]['text':' if tensorExprFuserEnabled() returns true we need to persist the very first','line_number':615,'multiline':False]['text':' time ProfilingGraphExecutorImpl is called, so we can update it correctly','line_number':616,'multiline':False]['text':' for fallback functions in ProfilingGraphExecutorImpl Else,','line_number':617,'multiline':False]['text':' getPlanFor(remaining_bailout_depth) is corrected and persisted by the Code','line_number':618,'multiline':False]['text':' object in interpreter.','line_number':619,'multiline':False]['text':' simple executor','line_number':628,'multiline':False]['text':' if a profiling graph hasn't been created yet','line_number':637,'multiline':False]['text':' `InsertProfileNodesForSpecializeAutogradZero` profiles a definition vs a','line_number':642,'multiline':False]['text':' use and it doesn't expect any profile nodes between a graph input and its','line_number':643,'multiline':False]['text':' consumer, `aten::_grad_sum_to_size`. This means we need to run it first,','line_number':644,'multiline':False]['text':' before any other pass that could insert `prim::iprofile_value` node on','line_number':645,'multiline':False]['text':' `aten::_grad_sum_to_size` input.','line_number':646,'multiline':False]['text':' fall-through','line_number':650,'multiline':False]['text':' profile until a graph is ready','line_number':653,'multiline':False]['text':' replaces a fallback graph inserted by','line_number':661,'multiline':False]['text':' specialize_autogradzero if one exists','line_number':662,'multiline':False]['text':' IMPORTANT: This is a hot path of calling a torchscript function. Try not to','line_number':676,'multiline':False]['text':' add any code above this.','line_number':677,'multiline':False]['text':' if depth is not set, use','line_number':681,'multiline':False]['text':' a GraphFunction call only have one output, so all the outputs','line_number':723,'multiline':False]['text':' need to be packed into a tuple','line_number':724,'multiline':False]['text':' namespace torch::jit','line_number':769,'multiline':False]