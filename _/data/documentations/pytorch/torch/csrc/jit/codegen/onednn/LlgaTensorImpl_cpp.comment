['text':' Non-default dnnl::graph::allocator needs an allocator.','line_number':12,'multiline':False]['text':' We would let it use c10::GetCPUAllocator's allocator,','line_number':13,'multiline':False]['text':' which uses posix_memalign with 64 byte alignment-size.','line_number':14,'multiline':False]['text':' Non-default dnnl::graph::allocator needs a deallocator.','line_number':20,'multiline':False]['text':' We would let it use c10::GetCPUAllocator's deallocator.','line_number':21,'multiline':False]['text':' Even if the default PyTorch CPU allocator would change, we'd still use the','line_number':28,'multiline':False]['text':' stale value. In practice, we don't expect users to change the CPU allocator','line_number':29,'multiline':False]['text':' dynamically anyway, as users preload jemalloc/tcmalloc at runtime, if they','line_number':30,'multiline':False]['text':' would like to. But this behavior might need to be changed, as some models','line_number':31,'multiline':False]['text':' work better with tcmalloc, while others work better with jemalloc, so','line_number':32,'multiline':False]['text':' switching the CPU allocator at runtime can be useful.','line_number':33,'multiline':False]['text':' device_id = ','line_number':37,'multiline':True]['text':'resizable=','line_number':81,'multiline':True]['text':' If a dtype is unsupported, oneDNN Graph will make that op a wildcard in','line_number':115,'multiline':False]['text':' the graph construction stage. Then when we would execute oneDNN Graph','line_number':116,'multiline':False]['text':' kernels pertaining to oneDNN Graph partitions, such an op would not be','line_number':117,'multiline':False]['text':' inside a oneDNN Graph partition, so we would not encounter inputs with','line_number':118,'multiline':False]['text':' unsupported dtypes at the time of executing compiled partitions.','line_number':119,'multiline':False]['text':' if input tensor is of mkldnn, it's originated from an upstream','line_number':126,'multiline':False]['text':' LLGA partition which carries opaque layout info','line_number':127,'multiline':False]['text':' if input tensor is not an mkldnn tensor, use default layout','line_number':130,'multiline':False]['text':' namespace onednn','line_number':155,'multiline':False]['text':' namespace fuser','line_number':156,'multiline':False]['text':' namespace jit','line_number':157,'multiline':False]['text':' namespace torch','line_number':158,'multiline':False]['text':' AT_MKLDNN_ENABLED()','line_number':160,'multiline':False]