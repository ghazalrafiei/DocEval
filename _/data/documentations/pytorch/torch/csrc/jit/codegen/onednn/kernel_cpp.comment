['text':' TODO: This is a workaround to recreate the partitions here.','line_number':21,'multiline':False]['text':' The ideal way is to use the partition serialization API (not available from','line_number':22,'multiline':False]['text':' LLGA now) to carry a serialized string representation from graph rewrite','line_number':23,'multiline':False]['text':' and deserialize it here.','line_number':24,'multiline':False]['text':' constantInputSpecs are placed after graphInputSpecs','line_number':138,'multiline':False]['text':' If the input tensor was between two partitions, it would've been','line_number':159,'multiline':False]['text':' wrapped with LlgaTensorImpl. But if it's being reused as the output','line_number':160,'multiline':False]['text':' tensor, which is not between two partitions, then we'd have to','line_number':161,'multiline':False]['text':' re-wrap it with a sub-class of TensorImpl, as it'd be fed into a','line_number':162,'multiline':False]['text':' PyTorch op.','line_number':163,'multiline':False]['text':' Wrap tensors between partitions with LlgaTensorImpl wrapper, so that we','line_number':189,'multiline':False]['text':' can bypass guard-check, as strides would be different than those','line_number':190,'multiline':False]['text':' expected.','line_number':191,'multiline':False]['text':' Since layouts of opaque outputs would be known after compilation,','line_number':217,'multiline':False]['text':' we need to query them out from compilation and update outputSpecs','line_number':218,'multiline':False]['text':' Build static mapping from output id to input offset','line_number':224,'multiline':False]['text':' in accordance with available inplace options','line_number':225,'multiline':False]['text':' Grab input values from stack','line_number':252,'multiline':False]['text':' Even in case of concurrent threads, the kernel would be initialized once.','line_number':260,'multiline':False]['text':' TODO: Try not using an atomic lock','line_number':261,'multiline':False]['text':' Update the stack.','line_number':288,'multiline':False]['text':' namespace onednn','line_number':297,'multiline':False]['text':' namespace fuser','line_number':298,'multiline':False]['text':' namespace jit','line_number':299,'multiline':False]['text':' namespace torch','line_number':300,'multiline':False]