['text':' We do not handle binary ops with two scalar inputs,','line_number':19,'multiline':False]['text':' and we assume scalar is always at the second place.','line_number':20,'multiline':False]['text':' If a scalar is added to be a tensor, we would assume that the','line_number':26,'multiline':False]['text':' scalar is of the same dtype as the tensor, as oneDNN graph','line_number':27,'multiline':False]['text':' currently requires inputs of binary ops to have the same dtype.','line_number':28,'multiline':False]['text':' We create a 1D tensor from the scalar input & "promote" its','line_number':29,'multiline':False]['text':' dtype to that of the first input. Doing so helps us satisfy PyTorch's','line_number':30,'multiline':False]['text':' type promotion rules.','line_number':31,'multiline':False]['text':' Although we convert the scalar to a tensor, we still need to promote','line_number':32,'multiline':False]['text':' types, as if the second input were still a scalar.','line_number':33,'multiline':False]['text':' The following sample code-snippet illustrates that converting a scalar','line_number':34,'multiline':False]['text':' input to a 1-D tensor may result in a different output dtype than would','line_number':35,'multiline':False]['text':' otherwise have been the case.','line_number':36,'multiline':False]['text':' clang-format off','line_number':37,'multiline':False]['text':'   >>> (1. + torch.rand([2]).half()).dtype','line_number':38,'multiline':False]['text':'       torch.float16','line_number':39,'multiline':False]['text':'   >>> (torch.tensor(1.).unsqueeze(0) + (torch.rand([2]).half())).dtype','line_number':40,'multiline':False]['text':'       torch.float32','line_number':41,'multiline':False]['text':' clang-format on','line_number':42,'multiline':False]['text':' 42 : Scalar  -->  tensor(42.0) : Float([])','line_number':47,'multiline':False]['text':' add dim & stride info to IR','line_number':49,'multiline':False]['text':' tensor(42.0) : Float([])  -->  tensor([42.0]) : Float([1])','line_number':56,'multiline':False]['text':' dtype might have changed, so needs to be updated in IR as well','line_number':61,'multiline':False]['text':' Here, both inputs are tensors, and we just wanna make sure that they','line_number':66,'multiline':False]['text':' are the same dtype, as oneDNN Graph requires both inputs to have the','line_number':67,'multiline':False]['text':' same dtype. We'll follow PyTorch's type-promotion rules here.','line_number':68,'multiline':False]['text':' dtype of the second tensor might not be available in the IR','line_number':73,'multiline':False]['text':' Type promotion is required','line_number':76,'multiline':False]['text':' dtype might have changed, so needs to be updated in IR as well','line_number':96,'multiline':False]['text':' both dtypes are same','line_number':101,'multiline':False]['text':' IR info of dtypes is missing sometimes in JIT IR,','line_number':102,'multiline':False]['text':' and we shouldn't treat those tensors as FP32 tensors by default.','line_number':103,'multiline':False]['text':' end inner if block','line_number':108,'multiline':False]['text':' end outer if block','line_number':109,'multiline':False]['text':' corner-case in BERT-mrpc that's not in line with','line_number':128,'multiline':False]['text':' native_functions.yaml','line_number':129,'multiline':False]['text':' ConvertScalarToTensor must be placed after EliminateIdentityMulAdd','line_number':178,'multiline':False]['text':' namespace onednn','line_number':182,'multiline':False]['text':' namespace fuser','line_number':183,'multiline':False]['text':' namespace jit','line_number':184,'multiline':False]['text':' namespace torch','line_number':185,'multiline':False]