['text':' Replace non-existent optional bias with const None','line_number':17,'multiline':False]['text':' PyTorch ops that can't otherwise be mapped to oneDNN Graph ops are mapped as','line_number':33,'multiline':False]['text':' Wildcards instead. They make the integration code with PyTorch simpler by','line_number':34,'multiline':False]['text':' passing every op to the oneDNN Graph library in the add_op call -','line_number':35,'multiline':False]['text':' no need to check beforehand whether the op is supported by oneDNN Graph or','line_number':36,'multiline':False]['text':' not oneDNN Graph ops separated by wildcards don't end up in the same','line_number':37,'multiline':False]['text':' partition.','line_number':38,'multiline':False]['text':' wildcard op contains only topology info','line_number':41,'multiline':False]['text':' If we don't meet a certain condition to map a PyTorch op to a oneDNN Graph','line_number':51,'multiline':False]['text':' op, then we create a wildcard op corresponding to that PyTorch op instead.','line_number':52,'multiline':False]['text':' Map a PyTorch op to its corresponding oneDNN Graph op.','line_number':70,'multiline':False]['text':' If mapping isn't possible, then create a wildcard op instead.','line_number':71,'multiline':False]['text':' The mapping is done as per oneDNN Graph op schema defined in','line_number':72,'multiline':False]['text':' third_party/ideep/mkl-dnn/src/interface/op_def.hpp.','line_number':73,'multiline':False]['text':' we're using an if-else clause instead of a switch staement','line_number':76,'multiline':False]['text':' because we would soon be adding custom ops with function schemas.','line_number':77,'multiline':False]['text':' We would have to use Symbol::fromQualString at that time anyway,','line_number':78,'multiline':False]['text':' but we are okay with this choice, since this code is not in the hot-path.','line_number':79,'multiline':False]['text':' cannot get training status in script mode','line_number':109,'multiline':False]['text':' PyTorch API already checks that both min & max are not None.','line_number':167,'multiline':False]['text':' But we can check it nevertheless.','line_number':168,'multiline':False]['text':' aten::cat needs a special handling since it takes a Tensor[] as input.','line_number':215,'multiline':False]['text':' We set the inputs of ListConstruct as the inputs of cat.','line_number':216,'multiline':False]['text':'','line_number':217,'multiline':False]['text':' Pytorch IR:                              LLGA sees:','line_number':218,'multiline':False]['text':'     %a    %b     %c          %dim              %a    %b    %c','line_number':219,'multiline':False]['text':'      \     |     /             |                \     |    /','line_number':220,'multiline':False]['text':'   prim::ListConstruct   prim::Constant     llga::Concat[axis=%dim]','line_number':221,'multiline':False]['text':'                    \      /','line_number':222,'multiline':False]['text':'                    aten::cat','line_number':223,'multiline':False]['text':' Currently, LLGA lacks support to create indices mask.','line_number':232,'multiline':False]['text':' Once it's supported, max_pool2d_with_indices should be mapped differently','line_number':233,'multiline':False]['text':' TODO: do we need add checks for all Constants?','line_number':249,'multiline':False]['text':' TODO: support all shape combinations','line_number':269,'multiline':False]['text':' fall through','line_number':276,'multiline':False]['text':' Contiguous should only be mapped to oneDNN Graph if the destination','line_number':299,'multiline':False]['text':' memory-layout is different than the source memory-format','line_number':300,'multiline':False]['text':' Strides would be different, but shape would be same','line_number':301,'multiline':False]['text':' Since prim::ListConstruct is not visible to the LLGA,','line_number':350,'multiline':False]['text':' it will not be in any partition returned from partfuseritioning results.','line_number':351,'multiline':False]['text':' We need rewrite opToOwningPartition to make the prim::ListConstruct to be','line_number':352,'multiline':False]['text':' 'virtually' in the same partition with the aten::cat, so that','line_number':353,'multiline':False]['text':' prim::ListConstruct can be fused into the fusion group by graph fuser.','line_number':354,'multiline':False]['text':' We emphasize on 'virtually' because get_num_ops() for cat's partition','line_number':355,'multiline':False]['text':' would still return 1.','line_number':356,'multiline':False]['text':' Verify that input tensors are compatible with oneDNN Graph.','line_number':364,'multiline':False]['text':' Scalars would be converted to 1-D tensors later anyway,','line_number':365,'multiline':False]['text':' but they shouldn't be complex-double','line_number':366,'multiline':False]['text':' If this check fails, convert op to wildcard','line_number':367,'multiline':False]['text':' We've allowed Long dtype here although oneDNN Graph does not support','line_number':380,'multiline':False]['text':' Long dtype because oneDNN Graph will end up not handling the op that','line_number':381,'multiline':False]['text':' has an input with Long dtype, so it'd be handled by PyTorch.','line_number':382,'multiline':False]['text':' TODO: select nodes in top-level block for now','line_number':412,'multiline':False]['text':' excluded unsupported Wildcard partitions','line_number':435,'multiline':False]['text':' Scanning the graph again for post processing','line_number':449,'multiline':False]['text':' Except for conv & GEMMs, which should always be handled by oneDNN Graph,','line_number':471,'multiline':False]['text':' only use single-op partitions for ops unsupported by NNC, or ops','line_number':472,'multiline':False]['text':' that oneDNN executes faster. prim::ListConstruct is an exception, since','line_number':473,'multiline':False]['text':' we simply want to fuse it with cat.','line_number':474,'multiline':False]['text':' multi-op partition','line_number':491,'multiline':False]['text':' this op isn't present in any partition','line_number':495,'multiline':False]['text':' if we're already in the process of merging','line_number':501,'multiline':False]['text':' TODO: count nodes in top-level block for now','line_number':568,'multiline':False]['text':' NOLINT','line_number':588,'multiline':False]['text':' NOLINT','line_number':603,'multiline':False]['text':' namespace onednn','line_number':619,'multiline':False]['text':' namespace fuser','line_number':620,'multiline':False]['text':' namespace jit','line_number':621,'multiline':False]['text':' namespace torch','line_number':622,'multiline':False]