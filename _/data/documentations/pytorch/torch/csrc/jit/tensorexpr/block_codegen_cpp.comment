['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':19,'multiline':False]['text':' Return Half for now','line_number':29,'multiline':False]['text':' For both Add, Mul we only print out the opening','line_number':86,'multiline':False]['text':' paranthesis. This behavior is to handle blocks add Op','line_number':87,'multiline':False]['text':' where c=a+b becomes add(a, b, c). The closing paran is','line_number':88,'multiline':False]['text':' added in the store statement.','line_number':89,'multiline':False]['text':' TODO: When handling fused ops d = a + b + c, the correct','line_number':90,'multiline':False]['text':' way would be to mutate the expression to Block version and print.','line_number':91,'multiline':False]['text':' print reverse reshape','line_number':133,'multiline':False]['text':' The dims for the multi-dim tensors','line_number':186,'multiline':False]['text':' The dimensions for the flattened tensors','line_number':192,'multiline':False]['text':' We are using a global counter here to make sure difference instances','line_number':319,'multiline':False]['text':' within BlockCodeGen have different names.','line_number':320,'multiline':False]['text':' Ensure all Bufs in reads/writes are in the map','line_number':336,'multiline':False]['text':' namespace torch::jit::tensorexpr','line_number':368,'multiline':False]