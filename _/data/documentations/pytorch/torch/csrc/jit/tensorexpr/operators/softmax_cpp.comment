['text':' Softmax is computed as follows:','line_number':14,'multiline':False]['text':'    softmax(vi) = exp(vi) / sum(exp(vi))','line_number':15,'multiline':False]['text':'','line_number':16,'multiline':False]['text':' In order to avoid overflow issues due to exp of a large number, we','line_number':17,'multiline':False]['text':' subtract the max of that dim before computing exp.','line_number':18,'multiline':False]['text':'    softmax(vi) = exp(vi - max(vi)) / sum(exp(vi - max(vi)))','line_number':19,'multiline':False]['text':'','line_number':20,'multiline':False]['text':' This is implemented as 4 loopnests:','line_number':21,'multiline':False]['text':'   - First loop computes the max over the softmax dim.','line_number':22,'multiline':False]['text':'   - Second loop computes exp for every element in v after subtracting','line_number':23,'multiline':False]['text':'     the max of the softmax dim it belongs to.','line_number':24,'multiline':False]['text':'   - Third loop computes the sum over the softmax dim.','line_number':25,'multiline':False]['text':'   - Final loop computes softmax for every element in v.','line_number':26,'multiline':False]['text':' LogSoftmax is computed as follows:','line_number':28,'multiline':False]['text':'    log_softmax(vi) = log(softmax(vi))','line_number':29,'multiline':False]['text':'                    = vi - log(sum(exp(vi)))','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' Using the same max trick as above:','line_number':32,'multiline':False]['text':'    log_softmax(vi) = vi - max(vi) - log(sum(exp(vi - max(vi))))','line_number':33,'multiline':False]['text':'','line_number':34,'multiline':False]['text':' This is implemented as 5 loopnests:','line_number':35,'multiline':False]['text':'   - First loop computes the max over the softmax dim.','line_number':36,'multiline':False]['text':'   - Second loop computes exp for every element in v after subtracting','line_number':37,'multiline':False]['text':'     the max of the softmax dim it belongs to.','line_number':38,'multiline':False]['text':'   - Third loop computes the sum over the softmax dim.','line_number':39,'multiline':False]['text':'   - Fourth loop computes log for every element in the sum.','line_number':40,'multiline':False]['text':'   - Final loop computes the log_softmax for every element in v.','line_number':41,'multiline':False]['text':' We do not handle None for dims (input 1) because that is supposed to','line_number':45,'multiline':False]['text':' be deprecated.','line_number':46,'multiline':False]['text':' Softmax implementation includes two reductions, one to find the max and','line_number':58,'multiline':False]['text':' the other to calculate the sum along the softmax dim. These reductions','line_number':59,'multiline':False]['text':' will have the softmax dimension as the inner most loop. So, the innermost','line_number':60,'multiline':False]['text':' index in the indices will refer to the softmax dimension.','line_number':61,'multiline':False]['text':' Update the indices by moving the softmax dimension index to the','line_number':63,'multiline':False]['text':' appropriate position.','line_number':64,'multiline':False]['text':' Remove the index corresponding to the softmax dimension.','line_number':77,'multiline':False]['text':' namespace tensorexpr','line_number':164,'multiline':False]['text':' namespace jit','line_number':165,'multiline':False]['text':' namespace torch','line_number':166,'multiline':False]