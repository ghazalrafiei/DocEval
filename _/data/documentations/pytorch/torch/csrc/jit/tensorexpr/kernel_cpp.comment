['text':' NOLINT','line_number':46,'multiline':False]['text':' NOLINT','line_number':47,'multiline':False]['text':' TODO: Remove this global var','line_number':112,'multiline':False]['text':' Ideally Block code gen should be decided','line_number':113,'multiline':False]['text':' based on device type in tensor.','line_number':114,'multiline':False]['text':' By default assume the device is CPU','line_number':174,'multiline':False]['text':' If v is a Tensor with concretely-known sizes and dtype, return them, else','line_number':180,'multiline':False]['text':' nullopt.','line_number':181,'multiline':False]['text':' TODO: ideally we should be strict here and return nullopt if the dtype is','line_number':194,'multiline':False]['text':' absent in the JIT IR. We're assuming a default Float dtype for now, until','line_number':195,'multiline':False]['text':' dtype propagation is implemented.','line_number':196,'multiline':False]['text':' Check dimension size first','line_number':227,'multiline':False]['text':' The fuser only supports conv2d with very specific properties:','line_number':251,'multiline':False]['text':' - Static shapes: 4-d input and filter, 1-d bias.','line_number':252,'multiline':False]['text':' - Constant strides/padding/dilation/groups','line_number':253,'multiline':False]['text':' - Equal padding and strides, dilation == 1.','line_number':254,'multiline':False]['text':' - Depthwise (groups == in_channels == out_channels)','line_number':255,'multiline':False]['text':' - 3x3 kernel','line_number':256,'multiline':False]['text':' Everything should be statically known.','line_number':267,'multiline':False]['text':' All inputs should be contiguous so no transposition is required.','line_number':273,'multiline':False]['text':' Everything should be statically known (bias could be NoneType =','line_number':300,'multiline':False]['text':' prim::Constant()).','line_number':301,'multiline':False]['text':' Weights and bias should be Constant when using mkldnn backend','line_number':307,'multiline':False]['text':' Input and weight should be NHWC contiguous.','line_number':315,'multiline':False]['text':' The fuser currently only supports matmul of 2D x 2D matrices','line_number':365,'multiline':False]['text':' Everything should be statically known.','line_number':370,'multiline':False]['text':' Proper ndim for tensor inputs.','line_number':376,'multiline':False]['text':' Inputs should be contiguous, or the TE will needlessly transpose them.','line_number':382,'multiline':False]['text':' namespace torch::jit::tensorexpr','line_number':391,'multiline':False]['text':' This is just a placeholder so we don't throw.  None-handling','line_number':407,'multiline':False]['text':' is operator-specific and should be handled properly in','line_number':408,'multiline':False]['text':' the operator-specific lowering code.','line_number':409,'multiline':False]['text':' Return arbitrarily typed vector','line_number':438,'multiline':False]['text':' This is just a placeholder so we don't throw.  None-handling','line_number':455,'multiline':False]['text':' is operator-specific and should be handled properly in','line_number':456,'multiline':False]['text':' the operator-specific lowering code.','line_number':457,'multiline':False]['text':' If the shape is present in the type info, just extract it from here. No','line_number':508,'multiline':False]['text':' need to infer it.','line_number':509,'multiline':False]['text':' Default','line_number':583,'multiline':False]['text':' handle optional bias','line_number':609,'multiline':False]['text':' True if all the loops in this vector have equal bounds.','line_number':648,'multiline':False]['text':' Recursively fuse all the loops with matching bounds in `st`.  Stops fusing','line_number':665,'multiline':False]['text':' at any level containing non-loops or non-matching bounds.  The restriction','line_number':666,'multiline':False]['text':' on matching bounds exists to avoid inserting conditionals on the loop','line_number':667,'multiline':False]['text':' indices where none would be needed, which would significantly complicate','line_number':668,'multiline':False]['text':' vectorization.','line_number':669,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':699,'multiline':False]['text':' Compute the trip count of a loop if it is a constant.','line_number':709,'multiline':False]['text':' Prune innermost loops until iterations satisfies a minimum grain size.','line_number':719,'multiline':False]['text':' Retain enough outermost loops to fill the number of threads.','line_number':735,'multiline':False]['text':' Flatten and parallelize outer loops, subject to a minimum number of elements','line_number':753,'multiline':False]['text':' in the inner loop, and a maximum level of thread-level parallelism in the','line_number':754,'multiline':False]['text':' outer loops.','line_number':755,'multiline':False]['text':' There are no loops to parallelize; give up.','line_number':763,'multiline':False]['text':' The loop nest contains a reduction; give up.','line_number':767,'multiline':False]['text':' The loop nest has loop carried dependences; give up.','line_number':772,'multiline':False]['text':' Try to flatten the outer loops and parallelize them if successful.','line_number':776,'multiline':False]['text':' For Block codegen we create a map of tensor dims before','line_number':804,'multiline':False]['text':' inlining. Like GPU codegen we need to inline. But the order','line_number':805,'multiline':False]['text':' where this analysis is run matters.','line_number':806,'multiline':False]['text':' Run Block analysis to get multi dim buffer info','line_number':809,'multiline':False]['text':' Inlining output & intermediate buffers can duplicate computation.','line_number':816,'multiline':False]['text':' Duplicating work can slow down the program if it's not ameliorated in some','line_number':817,'multiline':False]['text':' way, but we've empirically found that:','line_number':818,'multiline':False]['text':' - On CPU, LLVM's CSE does a good job as long as you horizontally fuse','line_number':819,'multiline':False]['text':'   output loops.','line_number':820,'multiline':False]['text':' - On GPU, there's enough compute to hide the extra work, and inlining','line_number':821,'multiline':False]['text':'   avoids synchronizing between kernels.','line_number':822,'multiline':False]['text':'allow_duplicated_work=','line_number':823,'multiline':True]['text':' Optimizing conditionals needs to be performed after inlining because','line_number':826,'multiline':False]['text':' inlining wouldn't work once the loops are split. Also, it has to be','line_number':827,'multiline':False]['text':' performed before loop fusion because loop fusion introduces cases where','line_number':828,'multiline':False]['text':' multiple conditionals are in the same loop and this optimization does not','line_number':829,'multiline':False]['text':' handle such cases yet.','line_number':830,'multiline':False]['text':' Fuse loops "horizontally".  This pass allows us to combine loops that','line_number':836,'multiline':False]['text':' write to different output buffers, as long as they have the same bounds.','line_number':837,'multiline':False]['text':' This happens when Buf is 0-dim','line_number':849,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':863,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':873,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':875,'multiline':False]['text':' TODO: change the number of microprocessors','line_number':877,'multiline':False]['text':' We only handle looplevels == 2 for now','line_number':898,'multiline':False]['text':' Arithmetic Simplification.','line_number':937,'multiline':False]['text':' we use the debug names in printing cuda code, they need to be removed','line_number':987,'multiline':False]['text':' of characters that can't be used in a variable identifier','line_number':988,'multiline':False]['text':' we could get fancier here, but name conflict is extremely unlikely','line_number':995,'multiline':False]['text':' first, generate non-dependent values','line_number':1068,'multiline':False]['text':' Contiguous and Transposed Contiguous depend on adjacent values','line_number':1082,'multiline':False]['text':' Get input size and strides','line_number':1149,'multiline':False]['text':' We don't need to copy the input if:','line_number':1153,'multiline':False]['text':'  1) it is not an output AND','line_number':1154,'multiline':False]['text':'  2) it is contiguous','line_number':1155,'multiline':False]['text':' if the input isn't contiguous or is an output,','line_number':1174,'multiline':False]['text':' write strided input into  contiguous buffer that is','line_number':1175,'multiline':False]['text':' then used in all further compute','line_number':1176,'multiline':False]['text':' initialize original index locations','line_number':1241,'multiline':False]['text':' We need to convert the output tensor so that its values are layed','line_number':1262,'multiline':False]['text':' so that when viewed from the output strides the values are correct.','line_number':1263,'multiline':False]['text':' A contiguous Tensor of size(2, 3) with values 0-5 is layed out as:','line_number':1264,'multiline':False]['text':' [0] [1] [2] [3] [4] [5]','line_number':1265,'multiline':False]['text':' The same valued tensor with strides (1, 2) would be layed out like','line_number':1266,'multiline':False]['text':' [0] [3] [1] [4] [2] [5]','line_number':1267,'multiline':False]['text':' When we are doing the re-ordering of values into the output tensor,','line_number':1268,'multiline':False]['text':' we are iterating per-element of the input, and we are fixed','line_number':1269,'multiline':False]['text':' in indexing in to the output tensor at [i, j] = val','line_number':1270,'multiline':False]['text':' `val` we want here is equal to the indices for the output','line_number':1271,'multiline':False]['text':' tensor that would have given the same position as the output','line_number':1272,'multiline':False]['text':' The position is equal to the sum of stride[i] * index[i],','line_number':1273,'multiline':False]['text':' and we can can calculate the equivalent indices in the','line_number':1274,'multiline':False]['text':' output tensor strides by iteratively computing the index of','line_number':1275,'multiline':False]['text':' the biggest stride:','line_number':1276,'multiline':False]['text':' absolute = ...','line_number':1277,'multiline':False]['text':' for stride in strides_from_largest_to_smallest:','line_number':1278,'multiline':False]['text':'     cur_idx = absolute // stride','line_number':1279,'multiline':False]['text':'     absolute = absolute % stride','line_number':1280,'multiline':False]['text':' XXX, in symbolic output ordering, we do not the arbitrary','line_number':1297,'multiline':False]['text':' ordering of strides as in usual output ordering, just','line_number':1298,'multiline':False]['text':' channels last, so even in the presence of size == 1','line_number':1299,'multiline':False]['text':' we produce correct output here','line_number':1300,'multiline':False]['text':' output is contiguous with specified memory format, no work to do','line_number':1325,'multiline':False]['text':' For a tensor with dimensions N C H W, channels last','line_number':1334,'multiline':False]['text':' format will is in format N H W C,','line_number':1335,'multiline':False]['text':' so the order largest to smallest will be N, H, W, C','line_number':1336,'multiline':False]['text':' See explanation in convertOutputToCorrectStrides','line_number':1340,'multiline':False]['text':' No shape info is present in the graph','line_number':1354,'multiline':False]['text':' All Tensors in NNC are layed out in default, contiguous layout.','line_number':1378,'multiline':False]['text':' If the output is also default contiguous we don't need to do anything','line_number':1379,'multiline':False]['text':' If the tensor is not dense or overlaps, we have','line_number':1383,'multiline':False]['text':' no way of matching the profiled striding','line_number':1384,'multiline':False]['text':' TODO: call into `convertOutputToCorrectStrides`. Currently this causes a','line_number':1393,'multiline':False]['text':' bug in IRSimplifier to occur. See explanation in','line_number':1394,'multiline':False]['text':' `convertOutputToCorrectStrides`','line_number':1395,'multiline':False]['text':' NOLINTNEXTLINE','line_number':1430,'multiline':False]['text':' Only Tensor constants need to be bound, scalar constants will be turned','line_number':1436,'multiline':False]['text':' into immediates in TE IR','line_number':1437,'multiline':False]['text':' Check if buf shape is static and compute its size if static.','line_number':1466,'multiline':False]['text':' Only allocate memory for static bufs.','line_number':1477,'multiline':False]['text':' The graph is supposed to have input params that represent the symbolic','line_number':1499,'multiline':False]['text':' dims at the end of the list of inputs. The number of such symbolic input','line_number':1500,'multiline':False]['text':' params is defined by the size of the `symbolic_shape_inputs_` vector.','line_number':1501,'multiline':False]['text':'','line_number':1502,'multiline':False]['text':' TODO: Check if the tensors with symbolic shapes are contiguous.','line_number':1503,'multiline':False]['text':' First, process the symbolic input params and create a new variable for','line_number':1508,'multiline':False]['text':' each of them.','line_number':1509,'multiline':False]['text':' NOTE: This has to be done before processing the tensor inputs, because','line_number':1510,'multiline':False]['text':' their symbolic sizes needs to be associated with these variables we','line_number':1511,'multiline':False]['text':' create for the symbolic input params.','line_number':1512,'multiline':False]['text':' For every shape symbol, store a map to the corresponding var.','line_number':1528,'multiline':False]['text':' Next, process symbolic input params and create an argument for symbolic','line_number':1534,'multiline':False]['text':' Block to collect the Stmts corresponding to all tensors.','line_number':1553,'multiline':False]['text':' Process the inputs before the symbolic input params.','line_number':1556,'multiline':False]['text':' Now, add all the variables corresponding to the symbolic input params.','line_number':1564,'multiline':False]['text':' Now, add all the variables corresponding to symbolic stride inputs','line_number':1570,'multiline':False]['text':' If the tensor is channels-last contiguous, the preferred memory layout','line_number':1580,'multiline':False]['text':' propagation policy is to use channels-last. Otherwise, the preferred policy','line_number':1581,'multiline':False]['text':' is to use contiguous.','line_number':1582,'multiline':False]['text':' Has symbolic stride information','line_number':1587,'multiline':False]['text':' No shape info is present in the graph','line_number':1596,'multiline':False]['text':' Filter out the tensor from the graph inputs and outputs to','line_number':1608,'multiline':False]['text':' deduce the memory layout propagation policy','line_number':1609,'multiline':False]['text':' std::all_of returns true if the range is empty. But we prefer to keep','line_number':1624,'multiline':False]['text':' the original memory layout propagation policy for this case. So we','line_number':1625,'multiline':False]['text':' check whether the range is empty.','line_number':1626,'multiline':False]['text':' If the memory layout of all the input and outputs is channels-last','line_number':1642,'multiline':False]['text':' contiguous, the propagated memory layout should be channels-last.','line_number':1643,'multiline':False]['text':' Otherwise, the propagated memory layout is contiguous which is as','line_number':1644,'multiline':False]['text':' same as current situation.','line_number':1645,'multiline':False]['text':' We may manipulate output pointers in graph manipulation. So we store the','line_number':1654,'multiline':False]['text':' original outputs for symbolic strides information synchronization','line_number':1655,'multiline':False]['text':' Get the graph device information first. The graph optimization','line_number':1658,'multiline':False]['text':' might be device specific.','line_number':1659,'multiline':False]['text':' Determine the propagated memory layout','line_number':1662,'multiline':False]['text':' Fuse Conv with Eltwise Op','line_number':1665,'multiline':False]['text':' Optimize the concatenation','line_number':1669,'multiline':False]['text':' Synchronize the symbolic strides information','line_number':1672,'multiline':False]['text':' Bind inputs to buffers.','line_number':1695,'multiline':False]['text':' Bind nodes to tensor compute expressions.','line_number':1698,'multiline':False]['text':' If there are for-loops before ExternalCall as follows,','line_number':1710,'multiline':False]['text':'   stmt1: for:','line_number':1711,'multiline':False]['text':'   stmt2    for:','line_number':1712,'multiline':False]['text':'   stmt3: ExternalCall','line_number':1713,'multiline':False]['text':' the for-loops would not be parallelized. So we mark the','line_number':1714,'multiline':False]['text':' buf args of ExternalCall as to be parallelized to make sure','line_number':1715,'multiline':False]['text':' its previous loop still could be parallelized.','line_number':1716,'multiline':False]['text':' Value is tensor','line_number':1725,'multiline':False]['text':' Value is scalar','line_number':1731,'multiline':False]['text':'','line_number':1732,'multiline':False]['text':' We represent scalar computations in TE with a pair of statements:','line_number':1733,'multiline':False]['text':'   Let val = <compute_expression>','line_number':1734,'multiline':False]['text':'   Store(buf_for_scalar[0], val)','line_number':1735,'multiline':False]['text':'','line_number':1736,'multiline':False]['text':' Subsequent computations will use val when they refer to the','line_number':1737,'multiline':False]['text':' given value, and the buffer will be used if we need to return','line_number':1738,'multiline':False]['text':' the computed value as an output of the kernel. If this is not an','line_number':1739,'multiline':False]['text':' output, the store will be removed later by DCE.','line_number':1740,'multiline':False]['text':'','line_number':1741,'multiline':False]['text':' NB: NNC's lowering functions return Tensor, which is a pair','line_number':1742,'multiline':False]['text':' <Buf, Stmt>, but here we also need Var. How can we obtain all of','line_number':1743,'multiline':False]['text':' Var, Buf, and Stmt?','line_number':1744,'multiline':False]['text':' We use the following trick: the lowering function creates the','line_number':1745,'multiline':False]['text':' Let-stmt and a "fake" buffer, whose only purpose is to hold the','line_number':1746,'multiline':False]['text':' Var. Then outside the lowering function (namely, right here) we','line_number':1747,'multiline':False]['text':' generate the store and the actual buffer.','line_number':1748,'multiline':False]['text':' Move output operands from `bufs_` to `bufOutputs_`','line_number':1768,'multiline':False]['text':' Scalar outputs are represented as 0-dim buffers.','line_number':1775,'multiline':False]['text':' The "strided" tensor will be incorrect if used in NNC,','line_number':1804,'multiline':False]['text':' since NNC views it as contiguous. Only convert it to the right','line_number':1805,'multiline':False]['text':' strides at the end of the kernel (if already contiguous it's a no-op)','line_number':1806,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)','line_number':1812,'multiline':False]['text':' If the tensor is not dense or overlaps, we have','line_number':1818,'multiline':False]['text':' no way of matching the profiled striding','line_number':1819,'multiline':False]['text':' Generate code.','line_number':1848,'multiline':False]['text':'= false','line_number':1867,'multiline':True]['text':' If there are symbolic shapes, then the output tensor size wouldn't have','line_number':1918,'multiline':False]['text':' been computed at compile time. That has to be done here by using the','line_number':1919,'multiline':False]['text':' symbolic shape input params passed in to this call.','line_number':1920,'multiline':False]['text':' TODO: preallocate `runArgs` during compilation and fill in values where','line_number':1962,'multiline':False]['text':' possible (e.g. for constant tensors)','line_number':1963,'multiline':False]['text':' add stride args','line_number':1985,'multiline':False]['text':' Set up arguments (inputs, then outputs) for kernel call.','line_number':2029,'multiline':False]['text':' Call the kernel.','line_number':2035,'multiline':False]['text':' Update the stack.','line_number':2038,'multiline':False]['text':' Scalar outputs are returned as 0-dim tensors, we need to extract the','line_number':2044,'multiline':False]['text':' scalar value from them','line_number':2045,'multiline':False]['text':' TODO: we can consider preallocating and pre-filling the args vector.','line_number':2060,'multiline':False]['text':' Call the kernel.','line_number':2065,'multiline':False]['text':' stack has inputs on the top and outputs right below them.','line_number':2076,'multiline':False]['text':' add stride args','line_number':2102,'multiline':False]['text':' This has only been tested on CPUs.','line_number':2115,'multiline':False]['text':' TODO: Test on GPUs.','line_number':2116,'multiline':False]['text':' Call the kernel.','line_number':2130,'multiline':False]['text':' Remove the inputs from the stack. The outputs are already below the inputs','line_number':2133,'multiline':False]['text':' in the stack.','line_number':2134,'multiline':False]