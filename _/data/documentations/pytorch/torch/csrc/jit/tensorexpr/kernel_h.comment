['text':' hashing input index and then dim index','line_number':19,'multiline':False]['text':' Returns true if the TE fuser supports this conv2d.','line_number':24,'multiline':False]['text':' Returns true if the TE fuser supports this conv2d with mkldnn prepacked conv.','line_number':26,'multiline':False]['text':' Returns true if the TE _convolution node is Conv2d.','line_number':28,'multiline':False]['text':' Returns true if the TE fuser supports this matmul.','line_number':30,'multiline':False]['text':' Get the dimensions of a value.','line_number':41,'multiline':False]['text':' If v is a tensor, broadcast it to match the shape of axes, or return','line_number':44,'multiline':False]['text':' directly if v is a constant.','line_number':45,'multiline':False]['text':' Only one of ptr and node is used at a time','line_number':103,'multiline':False]['text':' 1) ptr for the constant tensors','line_number':104,'multiline':False]['text':' 2) node for the constant custom class objects','line_number':105,'multiline':False]['text':' Constructor Params:','line_number':111,'multiline':False]['text':'  * subgraph','line_number':112,'multiline':False]['text':'      - the graph that needs to be compiled.','line_number':113,'multiline':False]['text':'  * kernel_func_name','line_number':114,'multiline':False]['text':'      - the name that should be used for the generated kernel.','line_number':115,'multiline':False]['text':'  * custom_lowerings','line_number':116,'multiline':False]['text':'      - map that represents custom lowering definitions for a set of ops.','line_number':117,'multiline':False]['text':'  * symbolic_shape_inputs','line_number':118,'multiline':False]['text':'      - a list of symbolic graph inputs that represent the symbolic dims of','line_number':119,'multiline':False]['text':'        the input tensors.','line_number':120,'multiline':False]['text':'  * pre_alloc','line_number':121,'multiline':False]['text':'      - a flag to control pre-allocation of buffers.','line_number':122,'multiline':False]['text':' Expected format of stack:','line_number':155,'multiline':False]['text':'  ... <outputs> <inputs>','line_number':156,'multiline':False]['text':' i.e., output IValues must be below the input IValues in the stack.','line_number':157,'multiline':False]['text':' These functions broadcast shape and also store a `hasBroadcast_` variable.','line_number':211,'multiline':False]['text':' Deduce the memory layout policy to be propagated within','line_number':242,'multiline':False]['text':' NNC fusion group. The memory layout policy could be `kContiguous`','line_number':243,'multiline':False]['text':' or `kChannelsLastNdContiguous`.','line_number':244,'multiline':False]['text':'    `kContiguous`: Always convert the non-contiguous input tensors and','line_number':245,'multiline':False]['text':'        internal buffers to contiguous.','line_number':246,'multiline':False]['text':'    `kChannelsLastNdContiguous`: Always convert the input tensors and','line_number':247,'multiline':False]['text':'        internal buffers to channels-last contiguous.','line_number':248,'multiline':False]['text':' Currently, the rule is simple.','line_number':249,'multiline':False]['text':'    If all the input and out tensors of NNC fusion group are channels-last','line_number':250,'multiline':False]['text':'    contiguous, the policy is `kChannelsLastNdContiguous`. Otherwise, it','line_number':251,'multiline':False]['text':'    is always `kContiguous`.','line_number':252,'multiline':False]['text':' Allocate memory for intermediate buffers at compile time.','line_number':269,'multiline':False]['text':' Specifically, we pre-allocate memory for intermediate buffers with static','line_number':270,'multiline':False]['text':' size and manage these buffers in the way we manage JIT constant tensors:','line_number':271,'multiline':False]['text':' push the buf args into the stack so NNC IR can access them at runtime.','line_number':272,'multiline':False]['text':' Apply the optimizations to the graph owned by the current fusion group,','line_number':301,'multiline':False]['text':' like concatenation optimization, post-op fusion, and some other graph-level','line_number':302,'multiline':False]['text':' optimizations.','line_number':303,'multiline':False]['text':' A map from ShapeSymbol.value() to the corresponding Var.','line_number':331,'multiline':False]['text':' List of values corresponding to the ShapeSymbols that are inputs to','line_number':334,'multiline':False]['text':' kernel being compiled. The order of these values correspond to the order','line_number':335,'multiline':False]['text':' of the symbolic inputs at the end of the list of inputs to the kernel.','line_number':336,'multiline':False]['text':' index of stack, stride index of tensor that will be appended as a codegen','line_number':348,'multiline':False]['text':' arg','line_number':349,'multiline':False]['text':' map from <input index, tensor dimension> to stride as arg VarHandle','line_number':351,'multiline':False]['text':' Memory layout to be propagated with fusion group','line_number':359,'multiline':False]['text':' namespace tensorexpr','line_number':380,'multiline':False]['text':' namespace jit','line_number':381,'multiline':False]['text':' namespace torch','line_number':382,'multiline':False]