['text':' dont handle transposed conv yet or not-constant transpose parameter','line_number':40,'multiline':False]['text':' check running_mean and running_var has value, if they are','line_number':68,'multiline':False]['text':' None(track_running_stats=False), skipping the folding path.','line_number':69,'multiline':False]['text':' implementation taken from torch/nn/utils/fusion.py','line_number':80,'multiline':False]['text':' If this is on GPU and bias is none and weight was half/bfloat, but','line_number':83,'multiline':False]['text':' bn_rm was float, then probably this was a case where autocasting','line_number':84,'multiline':False]['text':' casted inputs to conv. And since CUDA conv implementation requires','line_number':85,'multiline':False]['text':' all the inputs to have the same scalar dtype, we need to make this','line_number':86,'multiline':False]['text':' placeholder have the same type as conv_w.','line_number':87,'multiline':False]['text':' sub is equivalent to add','line_number':143,'multiline':False]['text':' In order to fuse add/sub/mul/div with conv, the dimensions of its','line_number':150,'multiline':False]['text':' constant tensor must satisfy the following:','line_number':151,'multiline':False]['text':' - with resizing, broadcast to w/ weight/bias tensor shape','line_number':152,'multiline':False]['text':' - broadcast to the conv output shape','line_number':153,'multiline':False]['text':' It needs to have a shape that can resize to weight/bias','line_number':154,'multiline':False]['text':' tensor shape because we need to run the op with the conv','line_number':155,'multiline':False]['text':' weights/bias without changing their sizes.','line_number':156,'multiline':False]['text':' It needs to broadcast to the conv output shape so that we do','line_number':157,'multiline':False]['text':' accidentally change the shape of op output by pre-fusing it','line_number':158,'multiline':False]['text':' compared to eager.','line_number':159,'multiline':False]['text':' The only dimension value shared by weight/bias/conv output','line_number':160,'multiline':False]['text':' is they all contain a dim with value = channels-out. In the','line_number':161,'multiline':False]['text':' conv output tensor, this is in the second dimension,','line_number':162,'multiline':False]['text':' so the pointwise op tensor may have a second dimension of','line_number':163,'multiline':False]['text':' value == channels-out, but all the other dimensions have to be 1','line_number':164,'multiline':False]['text':' channels-out dimension == weight_tensor.size(0)','line_number':170,'multiline':False]['text':' avoid fusing op that causes type promotion','line_number':193,'multiline':False]['text':' restricting to float avoids int/float difficulties with scalar overload','line_number':194,'multiline':False]['text':' expand errors if the shape input has less # dims than the tensor input','line_number':232,'multiline':False]['text':' DCE run after cleans up nodes','line_number':291,'multiline':False]['text':' div is equivalent to mul','line_number':301,'multiline':False]['text':' We've already verified that the second input has numel == 1 or','line_number':327,'multiline':False]['text':' channels-out resize it to the shape that will broadcast to','line_number':328,'multiline':False]['text':' weight_tensor when the op is run so we dont change weight size','line_number':329,'multiline':False]['text':' Suppress unused variable warning','line_number':332,'multiline':False]['text':' First fold with weight tensor','line_number':343,'multiline':False]['text':' now fold with bias tensor','line_number':361,'multiline':False]['text':' bias is of shape {channels_out}','line_number':364,'multiline':False]['text':' DCE run after cleans up nodes','line_number':385,'multiline':False]['text':' namespace','line_number':391,'multiline':False]['text':' namespace jit','line_number':411,'multiline':False]['text':' namespace torch','line_number':412,'multiline':False]