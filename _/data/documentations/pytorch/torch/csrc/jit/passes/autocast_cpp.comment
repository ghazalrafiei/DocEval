['text':' If we have an autocast instance, return it','line_number':48,'multiline':False]['text':'','line_number':49,'multiline':False]['text':' This is the pattern we're looking for (this is done after','line_number':50,'multiline':False]['text':'  autocast.__init__() has been inlined)','line_number':51,'multiline':False]['text':'','line_number':52,'multiline':False]['text':' %4 : bool = prim::Constant[value=1]()','line_number':53,'multiline':False]['text':' %5 : __torch__.torch.cuda.amp.autocast_mode.autocast = prim::CreateObject()','line_number':54,'multiline':False]['text':'  = prim::SetAttr[name="_enabled"](%5, %4)','line_number':55,'multiline':False]['text':'','line_number':56,'multiline':False]['text':' Notes:','line_number':57,'multiline':False]['text':'  1. There's no guarantee that the autocast instance is in the same block','line_number':58,'multiline':False]['text':'    as the prim::Enter() node','line_number':59,'multiline':False]['text':'  2. `prim::SetAttr` must follow `prim::CreateObject()` in the same block,','line_number':60,'multiline':False]['text':'    but there might be other nodes in between','line_number':61,'multiline':False]['text':'','line_number':62,'multiline':False]['text':' Not an autocast...','line_number':67,'multiline':False]['text':' TODO: support runtime flag','line_number':78,'multiline':False]['text':' Search for `prim::SetAttr[name="_enabled"]`','line_number':81,'multiline':False]['text':' Search for `prim::SetAttr[name="device"]`','line_number':89,'multiline':False]['text':' Search for `prim::SetAttr[name="fast_dtype"]`','line_number':97,'multiline':False]['text':' We only support simple and static autocast expressions. For example,','line_number':122,'multiline':False]['text':' the following should report an error (since the autocast would not','line_number':123,'multiline':False]['text':' work as expected)','line_number':124,'multiline':False]['text':'','line_number':125,'multiline':False]['text':'    autocast_on = autocast(enabled=True)','line_number':126,'multiline':False]['text':'    autocast_off = autocast(enabled=False)','line_number':127,'multiline':False]['text':'    with autocast_on if condition else autocast_off:','line_number':128,'multiline':False]['text':'        ...','line_number':129,'multiline':False]['text':'','line_number':130,'multiline':False]['text':' TODO: better error message','line_number':131,'multiline':False]['text':'','line_number':132,'multiline':False]['text':' need to also keep the inputs in order, otherwise tracing fails','line_number':150,'multiline':False]['text':' sanity checks because casting ops are inserted in random order','line_number':151,'multiline':False]['text':' TODO: update cast_op signature to take dynamic context flags','line_number':154,'multiline':False]['text':' Figure out the widest type','line_number':202,'multiline':False]['text':' (really, just looking for any float32 inputs)','line_number':203,'multiline':False]['text':'','line_number':204,'multiline':False]['text':' TODO: revisit this (do we need to consider float64 types?)','line_number':205,'multiline':False]['text':'','line_number':206,'multiline':False]['text':' Users can call torch.is_autocast_enabled() or is_autocast_cpu_enabled() to','line_number':218,'multiline':False]['text':' determine whether autocasting is enabled. With JIT-scripted functions, we','line_number':219,'multiline':False]['text':' actually need to return true if eager autocast OR jit autocast are enabled.','line_number':220,'multiline':False]['text':'','line_number':221,'multiline':False]['text':' In the case where JIT autocast is enabled, we replace','line_number':222,'multiline':False]['text':'    %x : bool = aten::is_autocast_enabled()','line_number':223,'multiline':False]['text':' with a constant "True".','line_number':224,'multiline':False]['text':'','line_number':225,'multiline':False]['text':' More context on eager vs JIT autocasting:','line_number':226,'multiline':False]['text':'','line_number':227,'multiline':False]['text':' Autocasting actually has two settings: eager autocasting, and JIT','line_number':228,'multiline':False]['text':' autocasting. Eager autocasting is the thread-local setting that turns on','line_number':229,'multiline':False]['text':' the relevant bit in the dispatcher settings. JIT autocasting is the pass','line_number':230,'multiline':False]['text':' implemented in this file, which makes changes to the graph to insert casting','line_number':231,'multiline':False]['text':' ops in order to achieve the same behavior as eager autocasting.','line_number':232,'multiline':False]['text':'','line_number':233,'multiline':False]['text':' If eager autocasting is enabled at the time when a JIT-scripted function is','line_number':234,'multiline':False]['text':' invoked, then autocasting will occur regardless of what the JIT-autocasting','line_number':235,'multiline':False]['text':' settings are.','line_number':236,'multiline':False]['text':' [Note: implicit type promotion in Autocast]','line_number':251,'multiline':False]['text':'','line_number':252,'multiline':False]['text':' Casting policy below mostly follows pytorch/aten/src/ATen/autocast.cpp, with','line_number':253,'multiline':False]['text':' a few exceptions, e.g. `aten::add`, which is needed to be put to promotion','line_number':254,'multiline':False]['text':' list for JIT autocast.','line_number':255,'multiline':False]['text':' The reason is that in eager amp, some binary ops promote inputs implicitly','line_number':256,'multiline':False]['text':' inside the operation, e.g. `aten::add` with fp16 & fp32 inputs would both be','line_number':257,'multiline':False]['text':' casted to fp32. In backward, autograd would cast dgrad to match their','line_number':258,'multiline':False]['text':' scalar_type in forward graph. So inputs with mismatched scalar_type would','line_number':259,'multiline':False]['text':' get the different dgrad.','line_number':260,'multiline':False]['text':' While in JIT, autodiff doesn't do this, so implicit cast is not visible to','line_number':261,'multiline':False]['text':' autodiff and backward dgrad for mismatched inputs would ended up with dgrads','line_number':262,'multiline':False]['text':' in the same scalar_type. This has caused downstream operations, which','line_number':263,'multiline':False]['text':' expects dgrad to be the same scalar type to throw mismatch error.','line_number':264,'multiline':False]['text':'','line_number':265,'multiline':False]['text':' TODO: Use the list from AMP eager directly','line_number':266,'multiline':False]['text':' The current autocast enabled/disabled state','line_number':272,'multiline':False]['text':' TODO: limit it only to amp related node;','line_number':281,'multiline':False]['text':' if the current autocasting state is the same as the global state,','line_number':283,'multiline':False]['text':' then autocasting will be done correctly on subsequent method and','line_number':284,'multiline':False]['text':' function calls','line_number':285,'multiline':False]['text':' TODO: limit it only to amp related node;','line_number':299,'multiline':False]['text':' if the current autocasting state is the same as the global state,','line_number':301,'multiline':False]['text':' then autocasting will be done correctly on subsequent method and','line_number':302,'multiline':False]['text':' function calls','line_number':303,'multiline':False]['text':' TODO: better error message','line_number':331,'multiline':False]['text':' CastPolicy::fp16 (cast all inputs to float16)','line_number':362,'multiline':False]['text':' CastPolicy::fp32 (cast all inputs to float32)','line_number':396,'multiline':False]['text':' CastPolicy::fp32_set_opt_dtype','line_number':445,'multiline':False]['text':' cast softmax to fp32 only on GPU','line_number':457,'multiline':False]['text':' CastPolicy::promote (promote inputs to the widest type)','line_number':466,'multiline':False]['text':' add, sub, mul, div were added to autocast jit, because aten implicit','line_number':478,'multiline':False]['text':' type promotion is not visible to JIT and could cause dtype mismatch on','line_number':479,'multiline':False]['text':' backward','line_number':480,'multiline':False]['text':' see [Note: implicit type promotion in Autocast]','line_number':481,'multiline':False]['text':' Banned in autocast, see binary_cross_entropy_banned()','line_number':491,'multiline':False]['text':' process sub-blocks, if any','line_number':498,'multiline':False]['text':' Sanity check: make sure there's no unbalanced transition','line_number':504,'multiline':False]['text':' namespace','line_number':508,'multiline':False]['text':' namespace jit','line_number':533,'multiline':False]['text':' namespace torch','line_number':534,'multiline':False]