['text':' For these operators, all inputs and outputs share the same scalar type.','line_number':45,'multiline':False]['text':' There is no operator-wise special case handling needed.','line_number':46,'multiline':False]['text':' For these operators, all inputs share the same scalar type.','line_number':62,'multiline':False]['text':' The output scalar type is always Bool.','line_number':63,'multiline':False]['text':' Type promotion between scalars and tensors','line_number':103,'multiline':False]['text':' per logic here','line_number':104,'multiline':False]['text':' https://pytorch.org/docs/master/tensor_attributes.html#tensor-attributes','line_number':105,'multiline':False]['text':'includeBool=','line_number':116,'multiline':True]['text':' Mimic PyTorch scalar type promotion logic','line_number':153,'multiline':False]['text':' from https://github.com/pytorch/pytorch/issues/9515','line_number':154,'multiline':False]['text':' Quoting:','line_number':155,'multiline':False]['text':'    A Tensor is a considered a "wrapped number" if it is','line_number':156,'multiline':False]['text':'    auto-wrapped from a C++ or Python number type. Integer types are','line_number':157,'multiline':False]['text':'    wrapped as 0-dim int64 tensors and floating-point types are','line_number':158,'multiline':False]['text':'    wrapped as 0-dim double tensors.','line_number':159,'multiline':False]['text':' floating-point numbers wrapped as float32/float64 tensors are','line_number':165,'multiline':False]['text':' considered to have default type, instead of double.','line_number':166,'multiline':False]['text':' bool and integer numbers remain the same type.','line_number':171,'multiline':False]['text':' other types are not from wrapped numbers,','line_number':175,'multiline':False]['text':' track them as types from tensors.','line_number':176,'multiline':False]['text':' This is a special pattern generated by code like `dim_size =','line_number':187,'multiline':False]['text':' x.size(0)`. It gets converted to the below ONNX IR graph','line_number':188,'multiline':False]['text':'    %1 : Long() = onnx::Constant[value={0}]()','line_number':189,'multiline':False]['text':'    %2 : Tensor = onnx::Shape(%x)','line_number':190,'multiline':False]['text':'    %dim_size : Long() = onnx::Gather(%2, %1)','line_number':191,'multiline':False]['text':' `dim_size` is treated in PyTorch as Scalar.','line_number':192,'multiline':False]['text':' However, in the ONNX IR graph, it is an output of onnx::Gather,','line_number':193,'multiline':False]['text':' which is by default considered as a tensor.','line_number':194,'multiline':False]['text':' get_scalar_type returns non-null value already guarantees','line_number':208,'multiline':False]['text':' that the input has a valid tensor_type.','line_number':209,'multiline':False]['text':' ONNX model track shape related computes that were done in pytorch','line_number':211,'multiline':False]['text':' by python numbers as tensor computes. This is the only way for ONNX','line_number':212,'multiline':False]['text':' to track them properly since ONNX only has tensor type, otherwise','line_number':213,'multiline':False]['text':' the computation result will be tracked statically as constant, and','line_number':214,'multiline':False]['text':' the model won't work for another input that differs in shape.','line_number':215,'multiline':False]['text':' Now for type promotion logic, scalars should be treated differently','line_number':217,'multiline':False]['text':' with tensors. More info regarding type promotion logic commented at','line_number':218,'multiline':False]['text':' `emplace_type_from_scalar`. Here we filter out rank 0 tensors and','line_number':219,'multiline':False]['text':' run it with `emplace_type_from_scalar` to determine if they are','line_number':220,'multiline':False]['text':' considered scalars for type promotion.','line_number':221,'multiline':False]['text':' NOTE that this might introduce regression that a REAL 0-rank tensor','line_number':223,'multiline':False]['text':' is now being recognized as scalar. The downside is the model will','line_number':224,'multiline':False]['text':' drop in accuracy for these cases as certain computations will','line_number':225,'multiline':False]['text':' happen in lower precision data types.','line_number':226,'multiline':False]['text':' For comparison ops, always promote scalar type to highest among inputs,','line_number':240,'multiline':False]['text':' regardless if that input is a tensor or scalar.','line_number':241,'multiline':False]['text':' If output scalar type is available, use that.','line_number':249,'multiline':False]['text':' PyTorch now does implicit type promotion regardless whether the inputs','line_number':252,'multiline':False]['text':' are tensors or scalars. (Previously only scalars support implicit','line_number':253,'multiline':False]['text':' casting).','line_number':254,'multiline':False]['text':' Per logic here','line_number':255,'multiline':False]['text':' https://pytorch.org/docs/master/tensor_attributes.html#tensor-attributes','line_number':256,'multiline':False]['text':' Some of standardOps do not support uint8\int8\int16 type for ONNX','line_number':267,'multiline':False]['text':' opset version < 14.','line_number':268,'multiline':False]['text':' Fix in this ONNX PR:','line_number':269,'multiline':False]['text':' https://github.com/onnx/onnx/pull/3334','line_number':270,'multiline':False]['text':' Fix up the scalar directly instead of inserting a cast operator.','line_number':301,'multiline':False]['text':' TODO: Keep only the else branch once constant_folding is enabled by','line_number':302,'multiline':False]['text':' default.','line_number':303,'multiline':False]['text':' This example error found when exports transfo_xl model using add op in uint8','line_number':349,'multiline':False]['text':' type, as below:','line_number':350,'multiline':False]['text':' if self.same_length:','line_number':351,'multiline':False]['text':'     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)','line_number':352,'multiline':False]['text':'     mask_len = klen - self.mem_len','line_number':353,'multiline':False]['text':'     if mask_len > 0:','line_number':354,'multiline':False]['text':'         mask_shift_len = qlen - mask_len','line_number':355,'multiline':False]['text':'     else:','line_number':356,'multiline':False]['text':'         mask_shift_len = qlen','line_number':357,'multiline':False]['text':'     dec_attn_mask = (torch.triu(all_ones, 1 + mlen) + torch.tril(all_ones,','line_number':358,'multiline':False]['text':'     -mask_shift_len))[:, :, None]  # -1','line_number':359,'multiline':False]['text':'','line_number':360,'multiline':False]['text':' `all_ones is` an uint8 tensor, But the calculation of `dec_attn_mask` using','line_number':361,'multiline':False]['text':' add(+) op to get the uint8 result. Reference Link:','line_number':362,'multiline':False]['text':' https://github.com/huggingface/transformers/blob/b020a736c374460af1b34267283f957988350630/src/transformers/models/transfo_xl/modeling_transfo_xl.py#L936','line_number':363,'multiline':False]['text':' skip LowPrecisionCast if op output type is null.','line_number':368,'multiline':False]['text':' skip LowPrecisionCast if any op input type node is null.','line_number':376,'multiline':False]['text':' The LowPrecision problem will be fixed in ONNX opset 14.','line_number':384,'multiline':False]['text':' If input type is changed, convert it to the original type.','line_number':390,'multiline':False]['text':' anonymous namespace','line_number':435,'multiline':False]['text':' namespace jit','line_number':453,'multiline':False]['text':' namespace torch','line_number':454,'multiline':False]