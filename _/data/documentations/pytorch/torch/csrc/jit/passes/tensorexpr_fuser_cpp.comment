['text':' NOLINTNEXTLINE','line_number':29,'multiline':False]['text':' clang-format off','line_number':80,'multiline':False]['text':' clang-format on','line_number':87,'multiline':False]['text':' For Block codegen we allow limited ops.','line_number':92,'multiline':False]['text':' clang-format on','line_number':107,'multiline':False]['text':' We only insert guards on Tensor types, so we rely on the output','line_number':114,'multiline':False]['text':' of a node being uniquely determined by its input types.','line_number':115,'multiline':False]['text':' bail if any non-Tensor input affects the output type','line_number':116,'multiline':False]['text':' and cannot be reasoned about statically','line_number':117,'multiline':False]['text':' Value is either an int or a float (can occur from .item())','line_number':119,'multiline':False]['text':' non-const dtype / device','line_number':126,'multiline':False]['text':' unschematized ops','line_number':142,'multiline':False]['text':' namespace tensorexpr','line_number':152,'multiline':False]['text':' A value can be profiled with differently typed uses.','line_number':214,'multiline':False]['text':' This can occur from:','line_number':215,'multiline':False]['text':' - having a use which is not executed, so the type will be','line_number':216,'multiline':False]['text':' TensorType::get()','line_number':217,'multiline':False]['text':' - control-flow that depends on tensor type:','line_number':218,'multiline':False]['text':'   if x.size() == 2 op(x) else op(x)','line_number':219,'multiline':False]['text':' - mutation of the value on a field represented in the tensor type','line_number':220,'multiline':False]['text':'   op(x); x.resize_([...]); op(x)','line_number':221,'multiline':False]['text':' The most common case today with num_profiles = 1 is from the first','line_number':223,'multiline':False]['text':' case. Here we can just ignore non-profiled uses, and choose any of the','line_number':224,'multiline':False]['text':' profiled uses. Because we guard all tensor types in the runtime, even','line_number':225,'multiline':False]['text':' if we set a Value to have a profiled type from one use and then execute','line_number':226,'multiline':False]['text':' a use with a different profiled type, we will still be correct.','line_number':227,'multiline':False]['text':' In the future we could consider unifying the types of uses, or adding a','line_number':228,'multiline':False]['text':' type refinement node so uses can have the correct corresponding type.','line_number':229,'multiline':False]['text':' If we encounter non-identical profiled types for the same value, merge','line_number':234,'multiline':False]['text':' them.  This situation can happen if, e.g., loop unrolling duplicates','line_number':235,'multiline':False]['text':' profiled types in a loop body in a manner that isn't logically','line_number':236,'multiline':False]['text':' consistent (see TestTEFuser.test_unrolled_cat).','line_number':237,'multiline':False]['text':' Constants & TensorExprGroup will always produce specialized tensor type,','line_number':263,'multiline':False]['text':' TypeCheck are inserted by this pass and only used by fusion groups that','line_number':264,'multiline':False]['text':' insert proper guards','line_number':265,'multiline':False]['text':' Fixup types of the subgraph inputs','line_number':308,'multiline':False]['text':' We only check inputs of the guarded nodes and expect user to infer','line_number':312,'multiline':False]['text':' intermediates and outputs shapes','line_number':313,'multiline':False]['text':' fusion outputs are already guarded','line_number':318,'multiline':False]['text':' Add prim::TypeCheck node','line_number':331,'multiline':False]['text':'','line_number':332,'multiline':False]['text':' TypeCheck nodes  look like the following:','line_number':333,'multiline':False]['text':'   %out1 : Float(2, 3), %out2 : Int(10, 30), %types_match : bool =','line_number':334,'multiline':False]['text':'   prim::TypeCheck(%inp1 : Tensor, %inp2 : Tensor)','line_number':335,'multiline':False]['text':'','line_number':336,'multiline':False]['text':' They have N inputs whose types we are going to check and N+1 outputs. The','line_number':337,'multiline':False]['text':' first N outputs specify expected types and N+1-th output holds the result','line_number':338,'multiline':False]['text':' of the check (bool).','line_number':339,'multiline':False]['text':' Fixup types of the typecheck node outputs, which are used by the op in','line_number':352,'multiline':False]['text':' execution','line_number':353,'multiline':False]['text':' Insert if','line_number':359,'multiline':False]['text':' Fill in the false block. It should contain the unoptimized','line_number':371,'multiline':False]['text':' copy of the fused subgraph.','line_number':372,'multiline':False]['text':' types get copied to the fallback graph, so remove specializations before','line_number':380,'multiline':False]['text':' replacing','line_number':381,'multiline':False]['text':' Fill in the true block. It has all inputs type-checked and its','line_number':385,'multiline':False]['text':' body should be the fusion group node.','line_number':386,'multiline':False]['text':' cant support non-constant pin_memory or pin_memory = True','line_number':401,'multiline':False]['text':' namespace','line_number':412,'multiline':False]['text':' Builds up expressions that compute shapes of all intermediates (and','line_number':428,'multiline':False]['text':' outputs) of the fusion group, based on the sizes of inputs. You should run','line_number':429,'multiline':False]['text':' DCE to remove those that you end up not using.','line_number':430,'multiline':False]['text':' When we have a guarantee that an output won't be removed, because it's','line_number':455,'multiline':False]['text':' used in expressions that don't involve size checks, we can use its size','line_number':456,'multiline':False]['text':' instead of computing a long chain of broadcasts, starting from the','line_number':457,'multiline':False]['text':' beginning of the kernel.','line_number':458,'multiline':False]['text':' we only support shape calculations for elementwise, some','line_number':508,'multiline':False]['text':' non-elementwise like batch_norm, conv, matmul, and','line_number':509,'multiline':False]['text':' a few exceptions (e.g. prim::ConstantChunk, etc) listed above','line_number':510,'multiline':False]['text':' XXX: Iterating in this order is not only good for performance reasons!','line_number':532,'multiline':False]['text':' It is also crucial for correctness (i has to reflect the current true','line_number':533,'multiline':False]['text':' index of outputs[i])!','line_number':534,'multiline':False]['text':' we maintain alias db correctness during initial fusion, but it is','line_number':557,'multiline':False]['text':' difficult to maintain correctness after inlining so inline only after','line_number':558,'multiline':False]['text':' fusion is done.','line_number':559,'multiline':False]['text':' Sort in reverse topological order','line_number':590,'multiline':False]['text':' Create a fusion group starting from the node N.','line_number':597,'multiline':False]['text':' We then try to pull inputs into the fusion group and repeat that process','line_number':598,'multiline':False]['text':' until there is nothing we can pull in.','line_number':599,'multiline':False]['text':' Allow single-node groups containing conv2d, since we'll only select','line_number':602,'multiline':False]['text':' those in cases where the tensorexpr implementation is faster than the','line_number':603,'multiline':False]['text':' aten implementation.','line_number':604,'multiline':False]['text':' we successfully merged, so the new group's `inputs` may have','line_number':616,'multiline':False]['text':' changed. So rescan the new group for more merging opportunities.','line_number':617,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.NonNullParamChecker)','line_number':627,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':629,'multiline':False]['text':' No Ops in eager shouldn't be outputs of Fusion Groups because it','line_number':635,'multiline':False]['text':' will degrade perf and change aliasing relationships','line_number':636,'multiline':False]['text':' There are some nodes that we can support, but we don't want to start a','line_number':654,'multiline':False]['text':' fusion group from - skip them.','line_number':655,'multiline':False]['text':' Merge fusible nodes into subgraphs in prim::TensorExprGroup nodes.','line_number':664,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':670,'multiline':False]['text':' Try to merge adjacent fusion groups together. Because we have only merged','line_number':683,'multiline':False]['text':' by looking at graph inputs, without this we would not attempt to merge','line_number':684,'multiline':False]['text':' adjacent fusion groups that don't have a dependency on each other','line_number':685,'multiline':False]['text':' Try merging the just created fusion group into the previous one.','line_number':698,'multiline':False]['text':' If it did not work, then put the previous fusion group into','line_number':699,'multiline':False]['text':' fusion_groups vector - we will not touch it anymore in this loop.','line_number':700,'multiline':False]['text':' If merging succeeded, save the merged group as the "previous" fusion','line_number':701,'multiline':False]['text':' group so that we can try to merge the next one into it.','line_number':702,'multiline':False]['text':' Don't count prim::Constants and prim::ListConstructs as these are nodes','line_number':724,'multiline':False]['text':' we only pull in along with another, "main", node. E.g. the','line_number':725,'multiline':False]['text':' ListConstruct nodes would also be pulled into a fusion group if they','line_number':726,'multiline':False]['text':' are inputs of an aten::cat node.','line_number':727,'multiline':False]['text':' Allow small subgraphs containing conv2d, since we'll only select those','line_number':754,'multiline':False]['text':' in cases where the tensorexpr implementation is faster than the aten','line_number':755,'multiline':False]['text':' implementation.','line_number':756,'multiline':False]['text':' Cleanup the subgraph from duplicated constants while we're at it.','line_number':762,'multiline':False]['text':' First, try to move all the nodes we want to fuse next to the fusion','line_number':795,'multiline':False]['text':' group.','line_number':796,'multiline':False]['text':' Now all the nodes that we're going to fuse are moved next to the fusion','line_number':807,'multiline':False]['text':' group, so we can safely merge them into the fusion group subgraph.','line_number':808,'multiline':False]['text':' TODO: Relax the checks to support dynamic shapes','line_number':829,'multiline':False]['text':' clang-format off','line_number':882,'multiline':False]['text':' breaks up the schema strings so they are no longer discoverable with ctrl-F','line_number':883,'multiline':False]['text':' On CPU, these are slower and less accurate than ATen kernels, because','line_number':902,'multiline':False]['text':' ATen is able to use MKL-VML, whereas the fuser currently can't.  The','line_number':903,'multiline':False]['text':' fuser uses sleef instead because sleef provides functions that operate','line_number':904,'multiline':False]['text':' on vectors, instead of large buffers.','line_number':905,'multiline':False]['text':' clang-format on','line_number':912,'multiline':False]['text':' Check types of input values.','line_number':914,'multiline':False]['text':' All tensors must be typed.','line_number':920,'multiline':False]['text':' Byte tensors introduce too many corner cases in type promotion.','line_number':925,'multiline':False]['text':' Better not to try to handle them.','line_number':926,'multiline':False]['text':' Float16 support has some issues (see e.g. #61336 and #61382), so for','line_number':931,'multiline':False]['text':' now it's disabled. There seem to be some problems in HalfRewriter,','line_number':932,'multiline':False]['text':' but on top of that Float16 has a few kinks on LLVM.  Thus, on CPU we','line_number':933,'multiline':False]['text':' additionally disable it until we either move to a more stable version','line_number':934,'multiline':False]['text':' or find workarounds.','line_number':935,'multiline':False]['text':' These operators only support floats, because integer divisors need to','line_number':946,'multiline':False]['text':' raise ZeroDivisionError.','line_number':947,'multiline':False]['text':' These operators have complicated casting rules for floats.','line_number':952,'multiline':False]['text':' Check scalar operands of float-only ops.','line_number':957,'multiline':False]['text':' aten::pow has special rules to avoid complicated integer cases.  We','line_number':968,'multiline':False]['text':' expect the first arg to be a floating point tensor, and if that's the','line_number':969,'multiline':False]['text':' case the type of the scalar exponent doesn't matter.','line_number':970,'multiline':False]['text':' Operator is only supported on CPU.','line_number':982,'multiline':False]['text':' Operator is only supported on GPU.','line_number':997,'multiline':False]['text':' only support same-device conversion','line_number':1009,'multiline':False]['text':' non_blocking only applies in cross-device conversion, which we bail on','line_number':1015,'multiline':False]['text':' copy arg only applies if op is a no-op, which we dont start fusion','line_number':1016,'multiline':False]['text':' group from memory format is separately handled in NNC output','line_number':1017,'multiline':False]['text':' all non-Tensor arguments must be constant','line_number':1019,'multiline':False]['text':' input tensor','line_number':1043,'multiline':False]['text':' Regarding CPU, aten would do nothing if the data type is','line_number':1059,'multiline':False]['text':' float. Then the aten performance is better than NNC. So NNC','line_number':1060,'multiline':False]['text':' does not pull it into its fusion group.','line_number':1061,'multiline':False]['text':' Regarding CPU, aten would do nothing if the data type is','line_number':1066,'multiline':False]['text':' BFloat16. Then the aten performance is better than NNC. So NNC','line_number':1067,'multiline':False]['text':' does not pull it into its fusion group.','line_number':1068,'multiline':False]['text':' `dim` argument must be a constant.','line_number':1079,'multiline':False]['text':' All tensor types should be known.','line_number':1121,'multiline':False]['text':' Only fuse aten::batch_norm when the parameter 'training' is false','line_number':1139,'multiline':False]['text':' A hook to optimizations limiter to allow bisecting the pass','line_number':1148,'multiline':False]['text':' Allow only if the node has a shape function defined.','line_number':1152,'multiline':False]['text':' ListConstruct node is an exception since that is needed to fuse','line_number':1153,'multiline':False]['text':' aten::cat, though it does not have a shape function.','line_number':1154,'multiline':False]['text':' Only fuse within a block','line_number':1165,'multiline':False]['text':' Symbolic checks','line_number':1168,'multiline':False]['text':' nvrtc has a limit on the number of arguments allowed in a CUDA kernel.','line_number':1173,'multiline':False]['text':' The specific limit is a function of constant memory size, amount','line_number':1174,'multiline':False]['text':' available to pass arguments, and some implementation dependence. Select a','line_number':1175,'multiline':False]['text':' safe limit here.','line_number':1176,'multiline':False]['text':' Device checks','line_number':1183,'multiline':False]['text':' aten::cat needs a special handling because it takes a Tensor[] as its','line_number':1185,'multiline':False]['text':' input We deal with that in the code below.','line_number':1186,'multiline':False]['text':' Alias checks','line_number':1194,'multiline':False]['text':' Ops that return aliases can only be folded if this is the only use.','line_number':1197,'multiline':False]['text':' Don't initiate a fusion group from prim::ListConstruct','line_number':1208,'multiline':False]['text':' Don't initiate a fusion group just for a constant operand','line_number':1214,'multiline':False]['text':' We're merging listconstruct->cat->consumer. cat is the producer here','line_number':1223,'multiline':False]['text':' and we cannot determine its device type - we should use device of the','line_number':1224,'multiline':False]['text':' listconstruct instead','line_number':1225,'multiline':False]['text':' We're merging listconstruct->cat. cat is the consumer and listconstruct','line_number':1241,'multiline':False]['text':' is the producer. cat doesn't have its device type and thus the only','line_number':1242,'multiline':False]['text':' thing we should check is that listconstruct's device is well defined','line_number':1243,'multiline':False]['text':' (e.g. all its inputs has the same device).','line_number':1244,'multiline':False]['text':' This function parses the option provided by the environment variable','line_number':1297,'multiline':False]['text':' "PYTORCH_TENSOREXPR_DONT_FUSE".','line_number':1298,'multiline':False]['text':' This variable allows users to disable fusion on a list of specified','line_number':1299,'multiline':False]['text':' operators that are separated by ':'. e.g.,','line_number':1300,'multiline':False]['text':' 'PYTORCH_TENSOREXPR_DONT_FUSE="clamp:mul:add"' disables fusion on','line_number':1301,'multiline':False]['text':' aten::clamp, aten::mul and aten::add.','line_number':1302,'multiline':False]['text':' Minimal size of a fusion group','line_number':1323,'multiline':False]['text':' compose Runtime Type Guard and Kernel in one op','line_number':1325,'multiline':False]['text':' generalize static shapes to dynamic shapes','line_number':1327,'multiline':False]['text':' Temporary change for Block code generation.','line_number':1338,'multiline':False]['text':' Get rid of dead code so that we don't waste effort fusing it.','line_number':1348,'multiline':False]['text':' Handle the case when dynamic shape fusion is enabled.','line_number':1374,'multiline':False]['text':' Striding Descriptor is serialized on the node as a vector of vector of','line_number':1390,'multiline':False]['text':' strings, translate back to StrideInput enum','line_number':1391,'multiline':False]['text':'pre_alloc','line_number':1424,'multiline':True]['text':' Stack contents:','line_number':1431,'multiline':False]['text':'   [<outputs>] <inputs>','line_number':1432,'multiline':False]['text':'','line_number':1433,'multiline':False]['text':' If the number of graph inputs is same as the stack size, then no','line_number':1434,'multiline':False]['text':' outputs are being passed in. Otherwise, output tensors are passed in','line_number':1435,'multiline':False]['text':' at the bottom of the stack. So, we call the appropriate run function','line_number':1436,'multiline':False]['text':' in TensorExprKernel.','line_number':1437,'multiline':False]['text':' namespace jit','line_number':1454,'multiline':False]['text':' namespace torch','line_number':1455,'multiline':False]