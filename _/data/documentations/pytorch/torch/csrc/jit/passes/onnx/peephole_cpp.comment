['text':' returns a vector `ret` such that transposing by `ret` is equivalent','line_number':47,'multiline':False]['text':' to transposing by `t1` and then by `t2`','line_number':48,'multiline':False]['text':'','line_number':49,'multiline':False]['text':' This fires in the case that we have transpose ops T1 -> T2. We are','line_number':50,'multiline':False]['text':' fusing the transpose op T1 into T2 and discarding T1. We assume the elements','line_number':51,'multiline':False]['text':' of the permutation in `t1` are raw indices into its input, since a previous','line_number':52,'multiline':False]['text':' iteration would have folded all the transposes up to that point. Thus,','line_number':53,'multiline':False]['text':' `ret[i] = t1[t2[i]]` says "the output of t2 at position i takes the value of','line_number':54,'multiline':False]['text':' the input tensor index contained in t1 at position `t2[i]``".','line_number':55,'multiline':False]['text':' Most of the element-wise ops in ONNX supports numpy broadcasting.','line_number':70,'multiline':False]['text':' Only GEMM supports one-directional broadcasting, which broadcasts the bias','line_number':71,'multiline':False]['text':' to the product.','line_number':72,'multiline':False]['text':' skip optional input if not provided','line_number':90,'multiline':False]['text':' Determine whether `from` can broadcast to `to`, and if so at which','line_number':101,'multiline':False]['text':' position. `from` must be a suffix of `to`, except that any','line_number':102,'multiline':False]['text':' occurrences of 1 in `from` are treated as wildcards.','line_number':103,'multiline':False]['text':' Fuses expand calls into ONNX operators, because it is','line_number':122,'multiline':False]['text':' easier for non-strided backends to more efficiently do broadcasts if this','line_number':123,'multiline':False]['text':' is local information. This optimization is not useful for PyTorch as','line_number':124,'multiline':False]['text':' 'expand' is free.','line_number':125,'multiline':False]['text':' Confirm it is expand node.','line_number':140,'multiline':False]['text':' We need to know what the type pre-expand is.  We should basically','line_number':149,'multiline':False]['text':' always have this information (because expands are only ever traced,','line_number':150,'multiline':False]['text':' not generated from symbolic), but if for some reason we don't','line_number':151,'multiline':False]['text':' have it, we need to skip.','line_number':152,'multiline':False]['text':' Not all broadcasts are supported by ONNX broadcast.','line_number':158,'multiline':False]['text':' from','line_number':164,'multiline':False]['text':' to','line_number':170,'multiline':False]['text':' Why this is here:','line_number':245,'multiline':False]['text':'','line_number':246,'multiline':False]['text':'   Pytorch has a "packed" representation of sequences, as well as a','line_number':247,'multiline':False]['text':'   "padded" representation. ONNX has only one representation,','line_number':248,'multiline':False]['text':'   corresponding to pytorch's "padded". Therefore, we need to remove','line_number':249,'multiline':False]['text':'   any use of packed sequences before exporting.','line_number':250,'multiline':False]['text':'','line_number':251,'multiline':False]['text':' What this does:','line_number':252,'multiline':False]['text':'','line_number':253,'multiline':False]['text':'   This code uses the observation that','line_number':254,'multiline':False]['text':'     RNN(PackPadded(x)) == PackPadded(RNN(x))','line_number':255,'multiline':False]['text':'   and converts the first form to the second whenever possible,','line_number':256,'multiline':False]['text':'   "pushing" the packing operation past the RNN operation. Then,','line_number':257,'multiline':False]['text':'   the removeNopPacking pass removes the packing operations','line_number':258,'multiline':False]['text':'   entirely by pairing them with their inverse PadPacked. If the','line_number':259,'multiline':False]['text':'   input graph does not pair the operations, export will fail.','line_number':260,'multiline':False]['text':' For now, only handle the case where there is one consumer.','line_number':272,'multiline':False]['text':' Packing only has an effect on a network when its outputs are actually','line_number':284,'multiline':False]['text':' used, so we can remove it here.','line_number':285,'multiline':False]['text':' The rnn is followed by a transpose and a reshape (if','line_number':294,'multiline':False]['text':' bidirectional), or by a squeeze (if unidirectional).','line_number':295,'multiline':False]['text':' remove PackPadded from in front of the RNN','line_number':306,'multiline':False]['text':' Make calculation of max_batch_size not depend on batch_sizes.','line_number':313,'multiline':False]['text':' This looks for a pattern generated by code such as','line_number':314,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/febff45/torch/nn/modules/rnn.py#L815-L815.','line_number':315,'multiline':False]['text':'','line_number':316,'multiline':False]['text':' Replace onnx::Gather[axis=0](batch_sizes, 0)','line_number':317,'multiline':False]['text':' with    onnx::Gather[axis=0](onnx::Shape(rnn_input), 1)','line_number':318,'multiline':False]['text':' We'll likely produce an invalid graph if this happens.','line_number':326,'multiline':False]['text':' New Constant node is needed, as it might be shared','line_number':335,'multiline':False]['text':' with a Constant node 0 from others.','line_number':336,'multiline':False]['text':' Make RNN not depend on batch_sizes.','line_number':343,'multiline':False]['text':' If there are other uses that are not:','line_number':347,'multiline':False]['text':' * PadPacked (which will be removed in removeNopPacking),','line_number':348,'multiline':False]['text':' * Dead code (which will be removed in dead code elimination),','line_number':349,'multiline':False]['text':' then we likely have produced an invalid graph, since there will be a','line_number':350,'multiline':False]['text':' use of the output of PackPadded, but the PackPadded (and that output)','line_number':351,'multiline':False]['text':' will be removed.','line_number':352,'multiline':False]['text':' and insert new PackPadded after the RNN','line_number':357,'multiline':False]['text':' make things consume from the new PackPadded','line_number':363,'multiline':False]['text':' set up the new PackPadded's inputs','line_number':367,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/9043 for a full','line_number':371,'multiline':False]['text':' description.  Since PackPadded is for now treated in an','line_number':372,'multiline':False]['text':' unhygenic way, Pytorch ends up propagating an incorrect type.','line_number':373,'multiline':False]['text':' Until a long-term cleanup comes around, we can fix this by','line_number':374,'multiline':False]['text':' resetting the size to the correct value.','line_number':375,'multiline':False]['text':' bidirection','line_number':382,'multiline':False]['text':' unidirection','line_number':385,'multiline':False]['text':' Despite the name, this actually removes the PadPacked node and leaves','line_number':397,'multiline':False]['text':' the PackPadded node. The PackPadded should become dead code which will','line_number':398,'multiline':False]['text':' be eliminated later.','line_number':399,'multiline':False]['text':' FIXME: the shape of the input to the fictional PadPacked node has','line_number':429,'multiline':False]['text':' incorrect shape. For now, just copy the shape of PadPacked to the shape','line_number':430,'multiline':False]['text':' of its input.','line_number':431,'multiline':False]['text':' The RNN code in pytorch accepts an optional hidden state.','line_number':453,'multiline':False]['text':' 1- When it is provided as an input, everything works great.','line_number':454,'multiline':False]['text':' 2- When it is not provided, it is default-initialized by constructing a new','line_number':455,'multiline':False]['text':' Variable, which gets','line_number':456,'multiline':False]['text':'    traced as a ConstantOfShape with the expected Shape.','line_number':457,'multiline':False]['text':' 3- When the batch size is fixed, everything works great as well.','line_number':458,'multiline':False]['text':' 4- When h0 and c0 are specified but are not inputs of the model (they are','line_number':459,'multiline':False]['text':'    Constants) and the batch size is variable, the model should be saved','line_number':460,'multiline':False]['text':'    with a batch size of 1 (or an error will occur), and we save the value','line_number':461,'multiline':False]['text':'    of h0 and c0 with a batch size of 1. When the model is then called with','line_number':462,'multiline':False]['text':'    a different batch size value, h0 and c0 are broadcasted to get the right','line_number':463,'multiline':False]['text':'    shape.','line_number':464,'multiline':False]['text':' Recognize that last pattern here (4) and fix the shape.','line_number':465,'multiline':False]['text':' Note that for multi-layer RNNs there will be a Slice operation between the','line_number':466,'multiline':False]['text':' Constant and the RNN.','line_number':467,'multiline':False]['text':' at::Scalar(n->i(attr::hidden_size)).toTensor());','line_number':503,'multiline':False]['text':' Hidden state is the sixth input for RNN, LSTM, GRU.','line_number':549,'multiline':False]['text':' See https://pytorch.org/docs/master/nn.html#torch.nn.RNN','line_number':550,'multiline':False]['text':' Cell state is the seventh input for LSTM.','line_number':568,'multiline':False]['text':' See https://pytorch.org/docs/master/nn.html#torch.nn.LSTM','line_number':569,'multiline':False]['text':' Moves ops outside of control flow blocks so that they are always executed,','line_number':581,'multiline':False]['text':' no matter the result of the control flow conditions.','line_number':582,'multiline':False]['text':' Needed only so that the split pass of the ONNX optimizer will put the ops','line_number':583,'multiline':False]['text':' into the init_net.','line_number':584,'multiline':False]['text':' TODO: Once the code in caffe2/python/onnx/backend.py no longer calls','line_number':585,'multiline':False]['text':' optimize_onnx, delete this function.','line_number':586,'multiline':False]['text':' note: increment first so that it is safe to move the node if needed','line_number':591,'multiline':False]['text':' XXX - only works for nodes with a single input','line_number':599,'multiline':False]['text':' move node n outside of the control flow it is nested in','line_number':600,'multiline':False]['text':' Skip if output of this node is part of block output.','line_number':605,'multiline':False]['text':' find the control flow node in the same block as node_input that contains','line_number':621,'multiline':False]['text':' Node n','line_number':622,'multiline':False]['text':' put the node right before this flow node','line_number':627,'multiline':False]['text':' make concat node output as new input, then ListConstruct should','line_number':660,'multiline':False]['text':' become dead','line_number':661,'multiline':False]['text':' TODO: Fix this pass/maybe get rid of this part.','line_number':688,'multiline':False]['text':' Tensor lists might be used for meshgrid and such ops as well.','line_number':689,'multiline':False]['text':' Replace prim::ListUnpack with onnx::SequenceAt.','line_number':702,'multiline':False]['text':' onnx::SequenceAt was introduced in onnx opset version 11','line_number':710,'multiline':False]['text':' From:','line_number':743,'multiline':False]['text':'   %list = ListConstruct(%x);','line_number':744,'multiline':False]['text':'   %unpacked = ListUnpack(%list);','line_number':745,'multiline':False]['text':'   do_something(%unpacked);','line_number':746,'multiline':False]['text':'','line_number':747,'multiline':False]['text':' To:','line_number':748,'multiline':False]['text':'   %list = ListConstruct(%x);','line_number':749,'multiline':False]['text':'   %unpacked = ListUnpack(%list);','line_number':750,'multiline':False]['text':'   do_something(%x)','line_number':751,'multiline':False]['text':'','line_number':752,'multiline':False]['text':' The ListConstruct and ListUnpack may now be dead code.','line_number':753,'multiline':False]['text':' https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter#quantized-model-export','line_number':769,'multiline':False]['text':' TupleConstruct is generated from the symbolics in quantized domain, and','line_number':773,'multiline':False]['text':' consumed by other quantized operators. The remained TupleConstruct should','line_number':774,'multiline':False]['text':' be at the output of the blocks.','line_number':775,'multiline':False]['text':' This optimization fuses LogSoftmax and NegativeLogLikelihoodLoss operators','line_number':809,'multiline':False]['text':' into one operator: SoftmaxCrossEntropyLoss, and depending on the dimensions','line_number':810,'multiline':False]['text':' of the input and different attributes there will be different subgraphs of','line_number':811,'multiline':False]['text':' LogSoftmax and NegativeLogLikelihoodLoss.','line_number':812,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':821,'multiline':False]['text':' Check for patterns especially in cases with autocasting enabled','line_number':824,'multiline':False]['text':' in which a cast node is inserted before the NegativeLogLikelihoodLoss','line_number':825,'multiline':False]['text':' node and this causes the patterns below not to be recognizable by the','line_number':826,'multiline':False]['text':' fuseLogSoftmaxNllLoss function','line_number':827,'multiline':False]['text':' For example if the input is 2D','line_number':828,'multiline':False]['text':' graph(%input : Half(3, 5),','line_number':829,'multiline':False]['text':' %target : Long(3)):','line_number':830,'multiline':False]['text':' %4 : Half(3, 5) = onnx::LogSoftmaxaxis=1','line_number':831,'multiline':False]['text':' %8 : Float = onnx::Cast[to=1](%4)','line_number':832,'multiline':False]['text':' %9 : Float(3) = onnx::NegativeLogLikelihoodLoss[reduction="none"]','line_number':833,'multiline':False]['text':' return (%8)','line_number':834,'multiline':False]['text':' if the input is 2D','line_number':842,'multiline':False]['text':' graph(%input : Float(3, 5),','line_number':843,'multiline':False]['text':' %target : Long(3)):','line_number':844,'multiline':False]['text':' %4 : Float(3, 5) = onnx::LogSoftmaxaxis=1','line_number':845,'multiline':False]['text':' %8 : Float(3) = onnx::NegativeLogLikelihoodLoss[reduction="none"]','line_number':846,'multiline':False]['text':' return (%8)','line_number':847,'multiline':False]['text':' if the input is 4D','line_number':852,'multiline':False]['text':' graph(%input : Float(3, 5, 2, 7),','line_number':853,'multiline':False]['text':' %target : Long(3, 2, 7)):','line_number':854,'multiline':False]['text':' %4 : Tensor = onnx::Transpose[perm=[0, 3, 2, 1]] (%input)','line_number':855,'multiline':False]['text':' %5 : Tensor = onnx::LogSoftmax[axis=3] (%4)','line_number':856,'multiline':False]['text':' %6 : Float(3, 5, 2, 7) = onnx::Transpose[perm=[0, 3, 2, 1]] (%5)','line_number':857,'multiline':False]['text':' %10 : Float(3, 2, 7) =','line_number':858,'multiline':False]['text':' onnx::NegativeLogLikelihoodLoss[reduction="none"](%6, %target) return','line_number':859,'multiline':False]['text':' (%10)','line_number':860,'multiline':False]['text':' if the input is 3D or > 4D','line_number':871,'multiline':False]['text':' graph(%input : Float(3, 5, 2),','line_number':872,'multiline':False]['text':' %target.1 : Long(3, 2)):','line_number':873,'multiline':False]['text':' %4 : Tensor = onnx::Transpose[perm=[0, 2, 1]] (%input)','line_number':874,'multiline':False]['text':' %5 : Tensor = onnx::LogSoftmax[axis=2] (%4)','line_number':875,'multiline':False]['text':' %6 : Float(3, 5, 2) = onnx::Transpose[perm=[0, 2, 1]] (%5)','line_number':876,'multiline':False]['text':' %8 : Tensor = onnx::Shape(%6)','line_number':877,'multiline':False]['text':' %10 : Tensor = onnx::Constantvalue={0}','line_number':878,'multiline':False]['text':' %11 : Long() = onnx::Gather[axis=0] (%8, %10)','line_number':879,'multiline':False]['text':' %13 : Tensor = onnx::Shape(%6)','line_number':880,'multiline':False]['text':' %15 Tensor = onnx::Constantvalue={1}','line_number':881,'multiline':False]['text':' %16 : Long() = onnx::Gather[axis=0] (%13, %15)','line_number':882,'multiline':False]['text':' ...','line_number':883,'multiline':False]['text':' %22 : Float(3, 5, 1, 2) = onnx::Reshape(%6, %21)','line_number':884,'multiline':False]['text':' ...','line_number':885,'multiline':False]['text':' %26 : Long(3, 1, 2) = onnx::Reshape(%target.1, %25)','line_number':886,'multiline':False]['text':' %30 : Float() = onnx::NegativeLogLikelihoodLoss[reduction="sum"](%22,','line_number':887,'multiline':False]['text':' %26) return (%30)','line_number':888,'multiline':False]['text':' when reduction=none a different graph is created and the graph','line_number':897,'multiline':False]['text':' doesn't end with node NegativeLogLikelihoodLoss like in all other','line_number':898,'multiline':False]['text':' cases.','line_number':899,'multiline':False]['text':' graph(%input : Float(3, 5, 2), %target.1 : Long(3, 2)):','line_number':900,'multiline':False]['text':' %4 : Tensor = onnx::Transposeperm=[0, 2, 1]','line_number':901,'multiline':False]['text':' %5 : Tensor = onnx::LogSoftmaxaxis=2','line_number':902,'multiline':False]['text':' %6 : Float(3, 5, 2) = onnx::Transposeperm=[0, 2, 1]','line_number':903,'multiline':False]['text':' ...','line_number':904,'multiline':False]['text':' %27 : Float(3, 5, 1, 2) = onnx::Reshape(%6, %26)','line_number':905,'multiline':False]['text':' %31 : Long(3, 1, 2) = onnx::Reshape(%target.1, %30)','line_number':906,'multiline':False]['text':' %35 : Float(3, 1, 2) =','line_number':907,'multiline':False]['text':' onnx::NegativeLogLikelihoodLoss[reduction="none"](%27, %31) %36 :','line_number':908,'multiline':False]['text':' int[] = prim::ListConstruct(%11, %21) %37 : Float(3, 2) =','line_number':909,'multiline':False]['text':' onnx::Reshape(%35, %36) return (%37)','line_number':910,'multiline':False]['text':' make output of reshape the output of nllloss','line_number':913,'multiline':False]['text':' If the pattern indeed consists of a cast node before the','line_number':921,'multiline':False]['text':' NegativeLogLikelihoodLoss node, place a cast node in the beginning','line_number':922,'multiline':False]['text':' of the pattern instead','line_number':923,'multiline':False]['text':' optional weight input is provided','line_number':946,'multiline':False]['text':' This optimization removes consecutive SplitToSequence and ConcatFromSequence','line_number':958,'multiline':False]['text':' operators. The optimization only happens when','line_number':959,'multiline':False]['text':'  1. Output of SplitToSequence is not used by any other nodes.','line_number':960,'multiline':False]['text':'  2. The attribute keepdims and axis of SplitToSequence match','line_number':961,'multiline':False]['text':'     attribute new_axis and axis of ConcatFromSequence.','line_number':962,'multiline':False]['text':' In that case, the two ops combined are no-op, and can be safely removed.','line_number':963,'multiline':False]['text':' Work around limitation from ONNX that the block input cannot be used directly','line_number':1006,'multiline':False]['text':' as block output. Inserts an Identity node inside the block, and have the','line_number':1007,'multiline':False]['text':' block return the output of the Identity.','line_number':1008,'multiline':False]['text':' This optimization does ONNX-specific peephole optimizations.','line_number':1028,'multiline':False]['text':'','line_number':1029,'multiline':False]['text':' Before you write an optimization here, ask yourself, "Could I do this','line_number':1030,'multiline':False]['text':' optimization on ATen operators"?  If so, you should seriously consider','line_number':1031,'multiline':False]['text':' writing your optimization in jit/passes/peephole.cpp rather than','line_number':1032,'multiline':False]['text':' here, as it will be generally applicable to the JIT as well.  The','line_number':1033,'multiline':False]['text':' optimizations here are ONLY applied on ONNX export.','line_number':1034,'multiline':False]['text':' TODO: decide on fixpoint strategy','line_number':1039,'multiline':False]['text':' TODO: make it easier not to do O(k) iterations over the graph, where','line_number':1040,'multiline':False]['text':' k is the number of distinct peephole optimizations','line_number':1041,'multiline':False]['text':' we only need to fix the size of hidden state and cell state if the batch','line_number':1045,'multiline':False]['text':' size is variable','line_number':1046,'multiline':False]['text':' namespace jit','line_number':1072,'multiline':False]['text':' namespace torch','line_number':1073,'multiline':False]