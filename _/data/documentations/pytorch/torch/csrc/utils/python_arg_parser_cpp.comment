['text':' Default arg name translations for compatibility with NumPy.','line_number':56,'multiline':False]['text':'','line_number':57,'multiline':False]['text':' Example:','line_number':58,'multiline':False]['text':' ```python','line_number':59,'multiline':False]['text':' t = torch.randn(10,10)','line_number':60,'multiline':False]['text':' torch.sum(a=t, axis=0, keepdim=True)','line_number':61,'multiline':False]['text':' ```','line_number':62,'multiline':False]['text':'','line_number':63,'multiline':False]['text':' A vector is necessary, because we might need to try multiple values.','line_number':64,'multiline':False]['text':' In particular, NumPy sometimes uses "x" and sometimes "a" for the main input','line_number':65,'multiline':False]['text':' tensor. Rather than annotate each function separately with whether it should','line_number':66,'multiline':False]['text':' take "x" or "a", just try both.','line_number':67,'multiline':False]['text':'','line_number':68,'multiline':False]['text':' TODO: Allow individual functions to specify non-default translations:','line_number':69,'multiline':False]['text':' For example, `torch.pow` should translate "exponent" to "x2".','line_number':70,'multiline':False]['text':' TODO: remove this. This is a temporary list of functions that allow Python','line_number':79,'multiline':False]['text':' numbers to bind to Tensors. Some binary ops have separate Tensor and Scalar','line_number':80,'multiline':False]['text':' overloads and binding to the Tensor overload with a number of a different','line_number':81,'multiline':False]['text':' type will trigger a type error.','line_number':82,'multiline':False]['text':'','line_number':83,'multiline':False]['text':' If you modify this, you will need to adjust the blocklist in','line_number':84,'multiline':False]['text':' tools/pyi/gen_pyi.py (and add hardcoded signatures for these','line_number':85,'multiline':False]['text':' functions.)','line_number':86,'multiline':False]['text':' alias of div','line_number':91,'multiline':False]['text':' alias of mul','line_number':93,'multiline':False]['text':' alias of sub','line_number':95,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':102,'multiline':False]['text':' Parse and remove brackets from type_str','line_number':122,'multiline':False]['text':' Combines self and args into one tuple.','line_number':199,'multiline':False]['text':' TODO: I'm not sure if I should call this __torch_function__ or','line_number':217,'multiline':False]['text':' torch_function.  The former makes it easier to take an existing','line_number':218,'multiline':False]['text':' Tensor-like __torch_function__ object and turn it into a mode;','line_number':219,'multiline':False]['text':' but in general modes don't have to be Tensor-like (and we will','line_number':220,'multiline':False]['text':' improperly accept mode objects as arguments when they shouldn't','line_number':221,'multiline':False]['text':' be passed around in this way).','line_number':222,'multiline':False]['text':' Note: [Overloaded args]','line_number':247,'multiline':False]['text':' An overloaded arg may be one of the following:','line_number':248,'multiline':False]['text':' - an instance of an object that has a __torch_function__ method','line_number':249,'multiline':False]['text':' - an instance of an object that has a __torch_dispatch__ classmethod','line_number':250,'multiline':False]['text':' - a class type that has a __torch_dispatch__ classmethod','line_number':251,'multiline':False]['text':'','line_number':252,'multiline':False]['text':' This function returns the type of the arg (if the arg is an instance),','line_number':253,'multiline':False]['text':' otherwise, it returns the arg.','line_number':254,'multiline':False]['text':' During __torch_dispatch__, don't dispatch on args with a disabled','line_number':280,'multiline':False]['text':' torch_dispatch. This code runs before infra modes, so we need to make','line_number':281,'multiline':False]['text':' sure that infra modes can run first. (In theory, maybe we can rearrange','line_number':282,'multiline':False]['text':' things so that infra modes are *always* attempted first, and just','line_number':283,'multiline':False]['text':' return NotImplemented when there are any user subclasses. Maybe that','line_number':284,'multiline':False]['text':' would fix this problem?)','line_number':285,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/63767','line_number':289,'multiline':False]['text':' Return the reference to the result. This also covers the case where','line_number':312,'multiline':False]['text':' ret is NULL and __torch_function__/__torch_dispatch raised an','line_number':313,'multiline':False]['text':' exception, which we throw below','line_number':314,'multiline':False]['text':' Disable mode on the inside; this makes for a more user-friendly','line_number':328,'multiline':False]['text':' experience if you try to, e.g., print your tensors.','line_number':329,'multiline':False]['text':' NB: We only really need keep the mode_obj live if the function call','line_number':333,'multiline':False]['text':' fails for error reporting, but whatever, Python refcounts are cheap','line_number':334,'multiline':False]['text':' Blegh.  This accidentally works in PyObject_CallFunctionObjArgs below','line_number':358,'multiline':False]['text':' because the nullptr terminates the argument list ick ick ick.','line_number':359,'multiline':False]['text':' See Note: [Overloaded args] for what they hold','line_number':385,'multiline':False]['text':' overloaded_args already all have unique types','line_number':405,'multiline':False]['text':' nb: modes don't go in the overloaded types list, as they are not','line_number':406,'multiline':False]['text':' necessarily types','line_number':407,'multiline':False]['text':' Step 1: Try to dispatch based on the mode stack, *ignoring* infra','line_number':418,'multiline':False]['text':' torch_dispatch modes.','line_number':419,'multiline':False]['text':' Check if any *user* torch_dispatch modes are active (not including','line_number':425,'multiline':False]['text':' fake and proxy modes, which are special)','line_number':426,'multiline':False]['text':' Note [__torch_dispatch__ dispatching order]','line_number':429,'multiline':False]['text':' The high-level idea motivating the dispatching','line_number':430,'multiline':False]['text':' order below is that: (1) modes get higher dispatch precedence over','line_number':431,'multiline':False]['text':' subclasses (2) "user" modes/subclasses get higher dispatch precedence over','line_number':432,'multiline':False]['text':' "infra" modes/subclasses.','line_number':433,'multiline':False]['text':'','line_number':434,'multiline':False]['text':' To give a complete example: let's say we are running torch.compile, with','line_number':435,'multiline':False]['text':' the following "user" modes and subclasses:','line_number':436,'multiline':False]['text':'   mode_stack: [ModeA]','line_number':437,'multiline':False]['text':'   user_args: [MyWrapperSubclassB(torchTensor)]','line_number':438,'multiline':False]['text':' During tracing in AOTAutograd tracing, we use some additional infra modes','line_number':440,'multiline':False]['text':' and subclasses to perform tracing:','line_number':441,'multiline':False]['text':'   FunctionalTensorMode, ProxyTorchDispatchMode, FakeTensorMode,','line_number':442,'multiline':False]['text':'   FunctionalTensor, FakeTensor','line_number':443,'multiline':False]['text':' The modified mode stack and tracing arguments will look like this:','line_number':444,'multiline':False]['text':'   mode_stack (user modes): [ModeA]','line_number':445,'multiline':False]['text':'   mode_stack (infra modes): [','line_number':446,'multiline':False]['text':'     FunctionalTensorMode, ProxyTorchDispatchMode, FakeTensorMode','line_number':447,'multiline':False]['text':'   ]','line_number':448,'multiline':False]['text':'   tracing_args: [','line_number':449,'multiline':False]['text':'     MyWrapperSubclassB(FunctionalTensor(_to_functional_tensor(FakeTensor)))','line_number':450,'multiline':False]['text':'   ]','line_number':451,'multiline':False]['text':' And the dispatching order that we want is as follows:','line_number':453,'multiline':False]['text':' (1) ModeA.__torch_dispatch__ (user modes highest)','line_number':454,'multiline':False]['text':' (2) MyWrapperSubclassB.__torch_dispatch__ (user subclasses next highest)','line_number':455,'multiline':False]['text':' (3) FunctionalTensorMode.__torch_dispatch__ (infra modes next highest)','line_number':456,'multiline':False]['text':' (4) ProxyTorchDispatchMode.__torch_dispatch__ (infra modes next highest)','line_number':457,'multiline':False]['text':' (5) FakeTensorMode.__torch_dispatch__ (infra modes next highest)','line_number':458,'multiline':False]['text':' (6) FakeTensor.__torch_fake_dispatch__ (infra subclasses next highest)','line_number':459,'multiline':False]['text':' Why does do FunctionalTensor and FakeTensor even need to be special-cased','line_number':461,'multiline':False]['text':' in the ordering?','line_number':462,'multiline':False]['text':' In theory we could remove their __torch_dispatch__, but both of these','line_number':463,'multiline':False]['text':' subclasses override sizes/strides metadata calls with __torch_dispatch__,','line_number':464,'multiline':False]['text':' which would mean a mode would be **required** to access their metadata.','line_number':465,'multiline':False]['text':' Step 1: Try to dispatch on any user TorchDispatchModes (including infra','line_number':468,'multiline':False]['text':' modes, which will always be at the bottom of the mode stack).','line_number':469,'multiline':False]['text':' Step 2: Try to dispatch based on any user subclasses,','line_number':481,'multiline':False]['text':' ignoring any subclasses that have a _mode_key field','line_number':482,'multiline':False]['text':' (corresponding to infra subclasses)','line_number':483,'multiline':False]['text':' Note: user subclasses should always run *before* infra modes like','line_number':484,'multiline':False]['text':' proxy/fake. This is handles by having proxy/fake modes return','line_number':485,'multiline':False]['text':' NotImplemented when they see a user subclass that they don't understand.','line_number':486,'multiline':False]['text':' if an exception occurred in a user's implementation of','line_number':502,'multiline':False]['text':' __torch_function__, throw it','line_number':503,'multiline':False]['text':' all __torch_function__ implementations in overloaded_args','line_number':506,'multiline':False]['text':' returned NotImplemented, so we raise a TypeError.','line_number':507,'multiline':False]['text':'
 *  obj has a __torch_function__ implementation and may either be a
 *  subclass of Tensor or a Tensor-like duck type. We may need to
 *  append this object to the overloaded_args vector, which tracks all
 *  of the arguments with distinct __torch_function__ implementations
 *  we've seen so far.
 *
 *  If this is the first argument we've seen with __torch_function__
 *  defined, we unconditionally add obj to the overloaded_args vector.
 *
 *  If we've already seen arguments with __torch_function__ defined,
 *  then we first need to check if obj is the same type as any of the
 *  entries in overloaded_args.  If so, we can ignore obj since we
 *  already have an entry in overloaded_args with the same
 *  __torch_function__ implementation.
 *
 *  If it's a different type, we then need to check if it's a subclass
 *  of one of the types we've already seen. If so, we need to insert an
 *  entry in overloaded_args for this type with higher precedence than
 *  the superclass.
 *
 *  See torch._overrides._get_overloaded_args for the equivalent
 *  function in the Python __torch_function__ implementation.
 *
 *  The precedence-determining algorithm implemented in this function is
 *  described in NEP-0018:
 *  https://numpy.org/neps/nep-0018-array-function-protocol.html
 *
 *  'overloaded_args' is a raw pointer to a vector of pybind11 handles
 *  that have distinct __torch_function__ implementations, in order of calling
 *  precedence.
 *
 *  'obj' is an object to check for a __torch_function__ implementation
 *
 * If changing this file in a way that can affect the __torch_function__
 * overhead, please report the benchmarks in 'benchmarks/overrides_benchmark'.
 * See the instructions in the 'README.md' in that directory.
 *
 ','line_number':603,'multiline':True]['text':' obj is the same type as another parameter we've seen in a prior','line_number':651,'multiline':False]['text':' iteration of the loop over parameters so we already have an entry','line_number':652,'multiline':False]['text':' with the proper __torch_function__ implementation to call, so skip','line_number':653,'multiline':False]['text':' this parameter','line_number':654,'multiline':False]['text':' obj is a subclass of another object we've seen already so its','line_number':664,'multiline':False]['text':' __torch_function__ should be called first, therefore we','line_number':665,'multiline':False]['text':' insert it into overloaded_args before the superclass','line_number':666,'multiline':False]['text':' add object to overloaded_args. If it's a subclass of another class','line_number':671,'multiline':False]['text':' we've already seen it will be inserted before the superclass,','line_number':672,'multiline':False]['text':' otherwise it will be inserted at the end of the array','line_number':673,'multiline':False]['text':'obj_is_type','line_number':682,'multiline':True]['text':'obj_is_type','line_number':688,'multiline':True]['text':' torch.Tensor instances (not subclasses, except for Parameter)','line_number':695,'multiline':False]['text':'ignore_mode','line_number':699,'multiline':True]['text':' tensor subclasses and unrelated objects with __torch_function__','line_number':700,'multiline':False]['text':' tensor subclasses without __torch_function__','line_number':704,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':716,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':737,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':762,'multiline':False]['text':' THPUtils_checkIndex may call __index__ or __int__','line_number':775,'multiline':False]['text':' which may have side effects if obj is a symint node','line_number':776,'multiline':False]['text':' so we do `is_symint` check first','line_number':777,'multiline':False]['text':' TODO: maybe we should be using checkLong here?','line_number':778,'multiline':False]['text':' FakeTensor(..., size=()) is qualified for SymInt param','line_number':787,'multiline':False]['text':'include_bool','line_number':791,'multiline':True]['text':' NOTE: JIT tracer allows arbitrary scalar tensors to act as ints','line_number':812,'multiline':False]['text':' in an intlist argument. Even float or complex scalar tensors.','line_number':813,'multiline':False]['text':' if a size is specified (e.g. IntArrayRef[2]) we also allow passing a single','line_number':823,'multiline':False]['text':' int','line_number':824,'multiline':False]['text':' argnum is needed for raising the TypeError, it's used in the error message.','line_number':828,'multiline':False]['text':' This will induce a guard','line_number':864,'multiline':False]['text':'includeBool=','line_number':875,'multiline':True]['text':' This will induce a guard','line_number':879,'multiline':False]['text':' if a size is specified (e.g. DimnameList[1]) we also allow passing a','line_number':890,'multiline':False]['text':' single Dimname','line_number':891,'multiline':False]['text':' throw_error ','line_number':896,'multiline':True]['text':' Allow SymInt where int is expected; we'll guard in this case','line_number':927,'multiline':False]['text':' WARNING: these strings are parsed invalid_arguments.cpp','line_number':938,'multiline':False]['text':' NB: SymInt is intentionally not mentioned here, as conventional user','line_number':946,'multiline':False]['text':' use will only know about ints','line_number':947,'multiline':False]['text':' *str_end == 0 if the entire string was parsed as an integer.','line_number':1000,'multiline':False]['text':'
Parse default value of IntArrayRef declared at native_functions.yaml

There are two kinds of default values:
1. IntArrayRef[2] x=1 (where size=2, value={1,1}
2. IntArrayRef x={1,2,3} (where size=3, value={1,2,3}, note that there cannot be
space after comma since native_parse.py uses ', ' to split args)
','line_number':1004,'multiline':True]['text':' case 1. s is an int (e.g., s=2)','line_number':1020,'multiline':False]['text':' case 2. s is a list of dims (e.g., s={1,2})','line_number':1026,'multiline':False]['text':' since already checked left brace '{' above, here only checks right brace','line_number':1028,'multiline':False]['text':' '}'','line_number':1029,'multiline':False]['text':' exclude '{' and '}'','line_number':1036,'multiline':False]['text':' Parse a string literal to remove quotes and escape sequences','line_number':1045,'multiline':False]['text':' Handle escape sequences','line_number':1068,'multiline':False]['text':' TODO: parse "x + xj"?','line_number':1124,'multiline':False]['text':' we sometimes rely on integer-vs-float values, e.g. with arange.','line_number':1128,'multiline':False]['text':' These types weren't handled here before. Adding a default error','line_number':1174,'multiline':False]['text':' led to a lot of test failures so adding this skip for now.','line_number':1175,'multiline':False]['text':' We should correctly handle these though because it might be causing','line_number':1176,'multiline':False]['text':' silent failures.','line_number':1177,'multiline':False]['text':' NOLINT','line_number':1178,'multiline':False]['text':' throw std::runtime_error("Invalid Tensor List");','line_number':1179,'multiline':False]['text':' NOLINT','line_number':1180,'multiline':False]['text':' throw std::runtime_error("ParameterType::GENERATOR");','line_number':1181,'multiline':False]['text':' NOLINT','line_number':1182,'multiline':False]['text':' throw std::runtime_error("ParameterType::PYOBJECT");','line_number':1183,'multiline':False]['text':' NOLINT','line_number':1184,'multiline':False]['text':' throw std::runtime_error("ParameterType::MEMORY_FORMAT");','line_number':1185,'multiline':False]['text':' NOLINT','line_number':1186,'multiline':False]['text':' throw std::runtime_error("ParameterType::DIMNAME");','line_number':1187,'multiline':False]['text':' NOLINT','line_number':1188,'multiline':False]['text':' throw std::runtime_error("ParameterType::DIMNAME_LIST");','line_number':1189,'multiline':False]['text':' NOLINT','line_number':1190,'multiline':False]['text':' throw std::runtime_error("ParameterType::SCALAR_LIST");','line_number':1191,'multiline':False]['text':' NOLINT','line_number':1192,'multiline':False]['text':' throw std::runtime_error("ParameterType::STORAGE");','line_number':1193,'multiline':False]['text':' NOLINT','line_number':1194,'multiline':False]['text':' throw std::runtime_error("ParameterType::QSCHEME");','line_number':1195,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':1201,'multiline':False]['text':' this 'if' happens for an empty parameter list, i.e. fn().','line_number':1227,'multiline':False]['text':' TODO: raise warning when parsing deprecated signatures','line_number':1252,'multiline':False]['text':' count the number of non-optional args','line_number':1260,'multiline':False]['text':' TODO: consider printing more proper schema strings with defaults,','line_number':1272,'multiline':False]['text':' optionals, etc.','line_number':1273,'multiline':False]['text':' this should never be hit','line_number':1384,'multiline':False]['text':' NOLINT','line_number':1392,'multiline':False]['text':' if there is a single positional IntArrayRef argument, i.e. expand(..),','line_number':1400,'multiline':False]['text':' view(...), allow a var-args style IntArrayRef, so expand(5,3) behaves as','line_number':1401,'multiline':False]['text':' expand((5,3))','line_number':1402,'multiline':False]['text':' foo() takes takes 2 positional arguments but 3 were given','line_number':1411,'multiline':False]['text':'ignore_mode','line_number':1418,'multiline':True]['text':' extra positional args given after single positional IntArrayRef arg','line_number':1425,'multiline':False]['text':' foo() missing 1 required positional argument: "b"','line_number':1450,'multiline':False]['text':' XXX: the Variable check is necessary because sizes become tensors when','line_number':1456,'multiline':False]['text':' tracer is enabled. This behavior easily leads to ambiguities, and we','line_number':1457,'multiline':False]['text':' should avoid having complex signatures that make use of it...','line_number':1458,'multiline':False]['text':' take all positional arguments as this parameter','line_number':1462,'multiline':False]['text':' e.g. permute(1, 2, 3) -> permute((1, 2, 3))','line_number':1463,'multiline':False]['text':' foo(): argument 'other' must be str, not int','line_number':1469,'multiline':False]['text':' foo(): argument 'other' (position 2) must be str, not int','line_number':1477,'multiline':False]['text':' foo() got an unexpected keyword argument "b"','line_number':1517,'multiline':False]['text':' Check deprecated signatures last','line_number':1543,'multiline':False]['text':' NOLINT','line_number':1575,'multiline':False]['text':' NOLINT','line_number':1602,'multiline':False]['text':' NB: we DO NOT put symbolic ints/floats into the Scalar itself,','line_number':1656,'multiline':False]['text':' because although Scalar supports SymInt/SymFloat, the subsequent','line_number':1657,'multiline':False]['text':' conversion to Tensor does not.  Instead, do it out of band.','line_number':1658,'multiline':False]['text':' This scalar value doesn't matter, it shouldn't ever actually','line_number':1661,'multiline':False]['text':' get read out.  Make it a big and weird looking number to help','line_number':1662,'multiline':False]['text':' people figure out if there's aproblem.','line_number':1663,'multiline':False]['text':' NB: Are you here because you passed None to a Variable method,','line_number':1672,'multiline':False]['text':' and you expected an undefined tensor to be returned?   Don't add','line_number':1673,'multiline':False]['text':' a test for Py_None here; instead, you need to mark the argument','line_number':1674,'multiline':False]['text':' as *allowing none*; you can do this by writing 'Tensor?' instead','line_number':1675,'multiline':False]['text':' of 'Tensor' in the ATen metadata.','line_number':1676,'multiline':False]['text':' TODO: remove','line_number':1680,'multiline':False]['text':' Zero-dim tensors are converted to Scalars as-is. Note this doesn't','line_number':1707,'multiline':False]['text':' currently handle most NumPy scalar types except np.float64.','line_number':1708,'multiline':False]['text':' Windows build fails with C2440: '<function-style-cast>'','line_number':1734,'multiline':False]['text':' when at:Scalar(py::cast<c10::SymBool>(arg))','line_number':1735,'multiline':False]['text':' namespace torch','line_number':1743,'multiline':False]