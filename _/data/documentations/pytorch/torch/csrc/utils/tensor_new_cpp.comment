['text':' NB: It appears there is some consistency invariant between options and','line_number':69,'multiline':False]['text':' device, where if device is non-empty, its type must be consistent with the','line_number':70,'multiline':False]['text':' device type in options.','line_number':71,'multiline':False]['text':' TODO: Refactor this so we just pass everything in via options','line_number':72,'multiline':False]['text':' Note that after the first iteration, obj is the only thing that keeps','line_number':96,'multiline':False]['text':' the seq raw pointer alive.','line_number':97,'multiline':False]['text':' This line uses seq so we must NOT override obj before this line','line_number':114,'multiline':False]['text':' this is always guaranteed to be a floating-point type, and makes it more','line_number':145,'multiline':False]['text':' convenient to write e.g. torch.tensor(0.) than torch.tensor(0.,','line_number':146,'multiline':False]['text':' dtype=torch.Tensor.dtype).','line_number':147,'multiline':False]['text':' match NumPy semantics, except use default tensor type instead of double.','line_number':178,'multiline':False]['text':' this won't change (unless we hit undefined, but that will fail','line_number':192,'multiline':False]['text':' later).','line_number':193,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':197,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':261,'multiline':False]['text':' TODO: use MaybeOwned','line_number':302,'multiline':False]['text':' infer the scalar type and device type; it's not expected to infer the','line_number':307,'multiline':False]['text':' layout since these constructors are defined per-layout-type (e.g. tensor','line_number':308,'multiline':False]['text':' vs sparse_coo_tensor).','line_number':309,'multiline':False]['text':'non_blocking=','line_number':318,'multiline':True]['text':'copy=','line_number':319,'multiline':True]['text':'non_blocking=','line_number':336,'multiline':True]['text':'copy=','line_number':337,'multiline':True]['text':'warn_if_not_writeable=','line_number':343,'multiline':True]['text':'non_blocking=','line_number':352,'multiline':True]['text':'copy=','line_number':353,'multiline':True]['text':' This exists to prevent us from tracing the call to empty().  The actual','line_number':363,'multiline':False]['text':' autograd code doesn't really matter, because requires_grad is always false','line_number':364,'multiline':False]['text':' here.','line_number':365,'multiline':False]['text':' What are the semantics of tensor_new()?','line_number':366,'multiline':False]['text':' We manually construct a tensor and place on it on the correct device with','line_number':367,'multiline':False]['text':' empty() and to(). We then have to "lift" the newly constructed tensor in','line_number':368,'multiline':False]['text':' some cases, like when we're performing a functorch transform or running','line_number':369,'multiline':False]['text':' functionalization. The exclude guards are all to ensure that extra logic','line_number':370,'multiline':False]['text':' doesn't run when we're constructing the raw tensor.','line_number':371,'multiline':False]['text':' functorch uses FuncTorchDynamicLayerBackMode as a mode key to wrap all','line_number':379,'multiline':False]['text':' tensors returned from operators in special TensorWrapper tensor extension','line_number':380,'multiline':False]['text':' We disable Fake and DeferredInit handlers for similar reasons as','line_number':385,'multiline':False]['text':' functorch.','line_number':386,'multiline':False]['text':' Note [Functionalization <> torch.Tensor constructor]','line_number':390,'multiline':False]['text':' Functionalization "lifts" the newly constructed tensor into a wrapper','line_number':391,'multiline':False]['text':' using aten::lift().','line_number':392,'multiline':False]['text':' Tracing should probably also use the "lift" operator to add the tensor','line_number':396,'multiline':False]['text':' to a trace, but it's technically BC-breaking to do that, since we','line_number':397,'multiline':False]['text':' currently trace .to() calls.','line_number':398,'multiline':False]['text':' If the device is Meta, take the shortcut. We don't want to allocate','line_number':425,'multiline':False]['text':' an empty CPU tensor which would break our contract for meta tensors.','line_number':426,'multiline':False]['text':' However, it is VERY important that we trace the to() call here (even','line_number':445,'multiline':False]['text':' though the reason this is important is a hack).  Without *some* factory','line_number':446,'multiline':False]['text':' function call that is traced at construction time, we will consider','line_number':447,'multiline':False]['text':' a tensor constant as originating from "outside" the trace, and if you','line_number':448,'multiline':False]['text':' try to return it directly we will fail with the error saying no','line_number':449,'multiline':False]['text':' "no observable data dependence".  In an ideal world, we wouldn't trace','line_number':450,'multiline':False]['text':' a to() call but I need to think harder about what exactly we should trace','line_number':451,'multiline':False]['text':' in this case.','line_number':452,'multiline':False]['text':'non_blocking=','line_number':454,'multiline':True]['text':'copy=','line_number':454,'multiline':True]['text':' torch.jit.trace will continue to trace out `.to()` instead of `.lift()`,','line_number':457,'multiline':False]['text':' since changing it is BC-breaking.','line_number':458,'multiline':False]['text':' lift has no autograd implementation, so we need to make sure we don't try','line_number':460,'multiline':False]['text':' to dispatch to it.','line_number':461,'multiline':False]['text':' TODO: arguably it should have an autograd implementation that noops','line_number':462,'multiline':False]['text':'copy_variables=','line_number':478,'multiline':True]['text':'copy_numpy=','line_number':479,'multiline':True]['text':'type_inference=','line_number':480,'multiline':True]['text':'copy_variables=','line_number':497,'multiline':True]['text':'copy_numpy=','line_number':498,'multiline':True]['text':'type_inference=','line_number':499,'multiline':True]['text':' "base" here refers to the Tensor type on which the function was invoked,','line_number':502,'multiline':False]['text':' e.g.: in x.new(y), 'x' is the base.','line_number':503,'multiline':False]['text':' TODO: Rewrite this using dispatchKeyToTensorOptions','line_number':504,'multiline':False]['text':' NOTE: no sparse XLA or Lazy','line_number':529,'multiline':False]['text':' TODO: Make this accept options instead of dispatch key','line_number':547,'multiline':False]['text':' NOLINTNEXTLINE(performance-no-int-to-ptr)','line_number':601,'multiline':False]['text':' Note: this signature doesn't have a dtype, even though it has a device;','line_number':610,'multiline':False]['text':' it probably shouldn't have a device (we should infer it).','line_number':611,'multiline':False]['text':' Note: this signature doesn't have a dtype, even though it has a device;','line_number':622,'multiline':False]['text':' it probably shouldn't have a device (we should infer it).','line_number':623,'multiline':False]['text':' new(sequence) binds to this signature but should be treated differently','line_number':634,'multiline':False]['text':' unless the sequences is a torch.Size','line_number':635,'multiline':False]['text':' NB: device_idx here is NOT a DeviceIndex, but index into PythonArgs','line_number':657,'multiline':False]['text':' TODO: This line doesn't seem to be exercised at all in tests','line_number':664,'multiline':False]['text':' namespace','line_number':670,'multiline':False]['text':' This constructor is no longer legacy, it will also be usable for','line_number':683,'multiline':False]['text':' subclass initialization','line_number':684,'multiline':False]['text':' prevent Tensor','line_number':686,'multiline':False]['text':' matching with','line_number':687,'multiline':False]['text':' IntArrayRef,','line_number':688,'multiline':False]['text':' PyObject*','line_number':689,'multiline':False]['text':' NOLINTNEXTLINE(performance-no-int-to-ptr)','line_number':724,'multiline':False]['text':' BASE_CTOR (aka torch.Tensor) is now relaxed to accept any','line_number':729,'multiline':False]['text':' dtype; previously it was "float" biased','line_number':730,'multiline':False]['text':' new(sequence) binds to this signature but should be treated differently','line_number':760,'multiline':False]['text':' unless the sequences is a torch.Size','line_number':761,'multiline':False]['text':' Handles ONLY torch.Tensor','line_number':776,'multiline':False]['text':' Unlike the legacy dtype/device specialized constructors, this one is','line_number':777,'multiline':False]['text':' relaxed to accept any device/dtype input tensor (even if it doesn't','line_number':778,'multiline':False]['text':' match the default)','line_number':779,'multiline':False]['text':' Handles calls like torch.DoubleTensor, torch.cuda.FloatTensor,','line_number':789,'multiline':False]['text':' torch.sparse.FloatTensor, etc.','line_number':790,'multiline':False]['text':' Handles tensor.new(...)','line_number':800,'multiline':False]['text':' Specific to tensor indexing, converts an indexing list to an','line_number':815,'multiline':False]['text':' indexing tensor (type Byte or Long)','line_number':816,'multiline':False]['text':'copy_variables=','line_number':825,'multiline':True]['text':'copy_numpy=','line_number':826,'multiline':True]['text':'type_inference=','line_number':827,'multiline':True]['text':'copy_variables=','line_number':834,'multiline':True]['text':'copy_numpy=','line_number':835,'multiline':True]['text':'type_inference=','line_number':836,'multiline':True]['text':' Clear error indicator if attribute does not exists.','line_number':886,'multiline':False]['text':' Otherwise subsequent Python C API calls might return bogus values.','line_number':887,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/58520 for more details','line_number':888,'multiline':False]['text':' Warning: a wrong attribute error may be suppressed here','line_number':894,'multiline':False]['text':' the global state of invariants check flag will be restored via','line_number':922,'multiline':False]['text':' CheckSparseTensorInvariantsContext destructor','line_number':923,'multiline':False]['text':'copy_variables=','line_number':932,'multiline':True]['text':'copy_numpy=','line_number':933,'multiline':True]['text':'type_inference=','line_number':934,'multiline':True]['text':'copy_variables=','line_number':940,'multiline':True]['text':'copy_numpy=','line_number':941,'multiline':True]['text':'type_inference=','line_number':942,'multiline':True]['text':'copy_variables=','line_number':948,'multiline':True]['text':'copy_numpy=','line_number':949,'multiline':True]['text':'type_inference=','line_number':950,'multiline':True]['text':' the global state of invariants check flag will be restored via','line_number':978,'multiline':False]['text':' CheckSparseTensorInvariantsContext destructor','line_number':979,'multiline':False]['text':'copy_variables=','line_number':988,'multiline':True]['text':'copy_numpy=','line_number':989,'multiline':True]['text':'type_inference=','line_number':990,'multiline':True]['text':'copy_variables=','line_number':996,'multiline':True]['text':'copy_numpy=','line_number':997,'multiline':True]['text':'type_inference=','line_number':998,'multiline':True]['text':'copy_variables=','line_number':1004,'multiline':True]['text':'copy_numpy=','line_number':1005,'multiline':True]['text':'type_inference=','line_number':1006,'multiline':True]['text':' Note [Ensuring sparse values and indices match devices]','line_number':1079,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1080,'multiline':False]['text':' In all places where we construct indices, we read out options from values','line_number':1081,'multiline':False]['text':' (rather than use inferred_options).  Why?  This handles the case when','line_number':1082,'multiline':False]['text':' values is a CUDA tensor, but indices is a non-Tensor value (and the device','line_number':1083,'multiline':False]['text':' argument is not set).  Example:','line_number':1084,'multiline':False]['text':'','line_number':1085,'multiline':False]['text':'  torch.sparse_coo_tensor(([0, 1],), self.empty(2, 0).cuda(), (4, 0))','line_number':1086,'multiline':False]['text':'','line_number':1087,'multiline':False]['text':' Sparse tensors require both indices and values to live on the same device.','line_number':1088,'multiline':False]['text':' If values lives on CUDA, we can infer where the indices should live, and','line_number':1089,'multiline':False]['text':' should accept even ordinary index sequences (and just make sure we write them','line_number':1090,'multiline':False]['text':' into the correct device).  values is the ONLY way we know that the index','line_number':1091,'multiline':False]['text':' tensor should go to CUDA, so we have to get the information in somehow.','line_number':1092,'multiline':False]['text':'','line_number':1093,'multiline':False]['text':' This code is kind of jank.  For one, the dtype in options is silently ignored','line_number':1094,'multiline':False]['text':' by internal_new_from_data.  Also, in classic janky code style, it used to','line_number':1095,'multiline':False]['text':' not work quite right: if values lives on "cuda:1", before all we said was','line_number':1096,'multiline':False]['text':' "this needs to be CUDA" and indices would be allocated on the wrong tensor.','line_number':1097,'multiline':False]['text':' Options is more right and gets this correct.','line_number':1098,'multiline':False]['text':' if no dtype provided, infer type based on value type.','line_number':1150,'multiline':False]['text':'copy_variables=','line_number':1156,'multiline':True]['text':'copy_numpy=','line_number':1157,'multiline':True]['text':'type_inference=','line_number':1158,'multiline':True]['text':' See Note [Ensuring sparse values and indices match devices]','line_number':1159,'multiline':False]['text':'copy_variables=','line_number':1165,'multiline':True]['text':'copy_numpy=','line_number':1166,'multiline':True]['text':'type_inference=','line_number':1167,'multiline':True]['text':'copy_variables=','line_number':1186,'multiline':True]['text':'copy_numpy=','line_number':1187,'multiline':True]['text':'type_inference=','line_number':1188,'multiline':True]['text':' See Note [Ensuring sparse values and indices match devices]','line_number':1189,'multiline':False]['text':'copy_variables=','line_number':1195,'multiline':True]['text':'copy_numpy=','line_number':1196,'multiline':True]['text':'type_inference=','line_number':1197,'multiline':True]['text':'copy_variables=','line_number':1239,'multiline':True]['text':'copy_numpy=','line_number':1240,'multiline':True]['text':'type_inference=','line_number':1241,'multiline':True]['text':' See Note [Ensuring sparse values and indices match devices]','line_number':1242,'multiline':False]['text':'copy_variables=','line_number':1248,'multiline':True]['text':'copy_numpy=','line_number':1249,'multiline':True]['text':'type_inference=','line_number':1250,'multiline':True]['text':'copy_variables=','line_number':1280,'multiline':True]['text':'copy_numpy=','line_number':1281,'multiline':True]['text':'type_inference=','line_number':1282,'multiline':True]['text':' See Note [Ensuring sparse values and indices match devices]','line_number':1283,'multiline':False]['text':'copy_variables=','line_number':1289,'multiline':True]['text':'copy_numpy=','line_number':1290,'multiline':True]['text':'type_inference=','line_number':1291,'multiline':True]['text':'copy_variables=','line_number':1297,'multiline':True]['text':'copy_numpy=','line_number':1298,'multiline':True]['text':'type_inference=','line_number':1299,'multiline':True]['text':'copy_variables=','line_number':1351,'multiline':True]['text':'copy_numpy=','line_number':1352,'multiline':True]['text':'type_inference=','line_number':1353,'multiline':True]['text':' See Note [Ensuring sparse values and indices match devices]','line_number':1354,'multiline':False]['text':'copy_variables=','line_number':1360,'multiline':True]['text':'copy_numpy=','line_number':1361,'multiline':True]['text':'type_inference=','line_number':1362,'multiline':True]['text':'copy_variables=','line_number':1368,'multiline':True]['text':'copy_numpy=','line_number':1369,'multiline':True]['text':'type_inference=','line_number':1370,'multiline':True]['text':'copy_variables=','line_number':1436,'multiline':True]['text':'copy_numpy=','line_number':1437,'multiline':True]['text':'type_inference=','line_number':1438,'multiline':True]['text':'validate_names=','line_number':1443,'multiline':True]['text':' ensure new_tensor a leaf node','line_number':1445,'multiline':False]['text':' TODO: add requires_grad once we decide on semantics for sharing data.','line_number':1456,'multiline':False]['text':'copy_variables=','line_number':1464,'multiline':True]['text':'copy_numpy=','line_number':1465,'multiline':True]['text':'type_inference=','line_number':1466,'multiline':True]['text':' ensure new_tensor a leaf node','line_number':1500,'multiline':False]['text':' atensor steals the ownership of the underlying storage. It also passes a','line_number':1615,'multiline':False]['text':' destructor function that will be called when the underlying storage goes','line_number':1616,'multiline':False]['text':' out of scope. When the destructor is called, the dlMTensor is destructed','line_number':1617,'multiline':False]['text':' too.','line_number':1618,'multiline':False]['text':' HACK: Ensure that we hold the GIL here just in case the','line_number':1619,'multiline':False]['text':' managed tensor originating from a buggy NumPy build.','line_number':1620,'multiline':False]['text':' Make sure this capsule will never be used again.','line_number':1625,'multiline':False]['text':' It is possible that the call to at::fromDLPack is the very first','line_number':1628,'multiline':False]['text':' call to create a Tensor in PyTorch. If so, then _lazy_init has','line_number':1629,'multiline':False]['text':' not been called, and the attempt to call createPyObject will fail','line_number':1630,'multiline':False]['text':' because cuda ATen types have not been registered in Python yet.','line_number':1631,'multiline':False]['text':' so if we have a cuda tensor, then we need to make sure','line_number':1632,'multiline':False]['text':' we have called _lazy_init here','line_number':1633,'multiline':False]['text':' Used when:','line_number':1652,'multiline':False]['text':' 1. 'obj' implements the buffer protocol and no type is given.','line_number':1653,'multiline':False]['text':' 2. creating a new tensor from a Python sequence.','line_number':1654,'multiline':False]['text':' Check whether 'obj' is a 'Tensor'','line_number':1658,'multiline':False]['text':' Check whether 'obj' is a NumPy Array or Scalar.','line_number':1665,'multiline':False]['text':' PyArray_CheckScalar is true for both scalars and 0-dim arrays, per','line_number':1673,'multiline':False]['text':' https://numpy.org/devdocs/reference/c-api/array.html#c.PyArray_CheckScalar','line_number':1674,'multiline':False]['text':' But for 0-dim arrays no `PyArray_FromScalar` call is needed','line_number':1675,'multiline':False]['text':'warn_if_not_writeable=','line_number':1686,'multiline':True]['text':' Uses a newly cloned storage, instead of the shared one.','line_number':1691,'multiline':False]['text':' The THPObjectPtr will delete the previous storage in the','line_number':1692,'multiline':False]['text':' end of the previous scope.','line_number':1693,'multiline':False]['text':' No need to clone again, later.','line_number':1696,'multiline':False]['text':' Check whether 'obj' is a 'DLPack' capsule','line_number':1703,'multiline':False]['text':' Check whether 'obj' implements the buffer protocol','line_number':1708,'multiline':False]['text':' Given an aliasable tensor, should we copy it?','line_number':1714,'multiline':False]['text':' Given a defined tensor, we copy it if either we have to (copy=True) or','line_number':1720,'multiline':False]['text':' if we need to (copy=None) because of mismatched device or dtype.','line_number':1721,'multiline':False]['text':'non_blocking=','line_number':1727,'multiline':True]['text':'copy=','line_number':1728,'multiline':True]['text':' If we are not copying, we have to check whther we have the tensor','line_number':1733,'multiline':False]['text':' in the right device, with the right dtype.','line_number':1734,'multiline':False]['text':' If tensor is a NumPy Array view, we warn the user about non-writeable','line_number':1749,'multiline':False]['text':' arrays if this is the case.','line_number':1750,'multiline':False]['text':' Setting 'requires_grad' when the tensor is not a leaf does not work.','line_number':1756,'multiline':False]['text':' Whenever that happens, we have to use 'detach'.','line_number':1757,'multiline':False]['text':' Undefined tensor means it does not implement neither DLPack nor','line_number':1764,'multiline':False]['text':' the buffer protocol. Last case is a sequence, in which case we must','line_number':1765,'multiline':False]['text':' copy (copy can't be false).','line_number':1766,'multiline':False]['text':' Make tensor from sequence, inferring its type, and then convert','line_number':1770,'multiline':False]['text':' it to the desired type.','line_number':1771,'multiline':False]['text':' Type inference is activated only if the dtype has not been specified.','line_number':1772,'multiline':False]['text':' Otherwise, we force the unwrapped dtype.','line_number':1773,'multiline':False]['text':' copy_variables = ','line_number':1779,'multiline':True]['text':' copy_numpy = ','line_number':1780,'multiline':True]['text':' type_inference = ','line_number':1781,'multiline':True]['text':' namespace utils','line_number':1788,'multiline':False]['text':' namespace torch','line_number':1789,'multiline':False]