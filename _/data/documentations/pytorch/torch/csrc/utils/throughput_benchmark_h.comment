['text':'*
 * The struct is used to provide results of a benchmark to the caller
 * In the future all additional statics should be added here.
 ','line_number':20,'multiline':True]['text':'*
 * Use this struct in order to configure a throughput benchmark run.
 * This struct should include parameters related to threading, batching, number
 * of iterations, warm-up, etc. More configs can be added as needed.
 * General rule here is that only things that c++ must(!) to be aware of should
 * be here. If we can keep other parts in python, we should keep them there.
 * This is typical for things that are not perf critical and don't affect
 * execution statistics benchmark returns.
 ','line_number':33,'multiline':True]['text':' Calling threads are those threads that are calling into a module in','line_number':44,'multiline':False]['text':' parallel.','line_number':45,'multiline':False]['text':' Worker threads are not supported yet. This is just an example that we plan','line_number':47,'multiline':False]['text':' to support some sort of multi-threaded forward calls. We may change this','line_number':48,'multiline':False]['text':' setting in the future to support different intra and inter op parallelism','line_number':49,'multiline':False]['text':' which is not available in PyTorch yet','line_number':50,'multiline':False]['text':' Warmup iters are used to make sure we run a module a few times before','line_number':52,'multiline':False]['text':' actually measuring things. This way we avoid cold caches and any other','line_number':53,'multiline':False]['text':' similar problems','line_number':54,'multiline':False]['text':' Number of iterations the benchmark should run with. This number is separate','line_number':56,'multiline':False]['text':' from the warmup iterations','line_number':57,'multiline':False]['text':' If set autograd profiler will be enabled. I.e. this variable would be','line_number':59,'multiline':False]['text':' created before the main benchmark loop (but after the warmup):','line_number':60,'multiline':False]['text':' RecordProfile guard(profiler_output_path);','line_number':61,'multiline':False]['text':'*
 * A helper class to abstract out different models we test throughput of
 ','line_number':67,'multiline':True]['text':' This method to be used in benchmark() method','line_number':77,'multiline':False]['text':' Note that there is no result. This way we don't have to call this under GIL','line_number':78,'multiline':False]['text':' even when running in the nn.Module mode. Otherwise destructor of the result','line_number':79,'multiline':False]['text':' would race with Python','line_number':80,'multiline':False]['text':' This method is to be used when calling from Python directly','line_number':82,'multiline':False]['text':' Aggregate input in the format Model expects in order to avoid further','line_number':84,'multiline':False]['text':' conversions at the benchmark time','line_number':85,'multiline':False]['text':' Destructor doesn't require the GIL because it is going to be executed on','line_number':94,'multiline':False]['text':' the PyThon thread','line_number':95,'multiline':False]['text':' namespace detail','line_number':156,'multiline':False]['text':'*
 * This class is a small c++ component responsible for executing a PyTorch
 * module under an inference server like load. It can emulate multiple calling
 * threads to a single module provided. In the future we plan to enhance this
 * component to support inter and intra-op parallelism as well as multiple
 * models running in a single process.
 *
 * For current available configurations refer to the BenchmarkConfig
 * documentation
 *
 * The class supports working with either nn.Module or ScriptModule.
 * Under the hood it just dispatches to corresponding specialization of
 * class BenchmarkHelper<Input, Output, Model>
 ','line_number':158,'multiline':True]['text':' Add one more input example. This input example should be in the exact','line_number':177,'multiline':False]['text':' format the module under test expects. It is responsibility of the module to','line_number':178,'multiline':False]['text':' perform any such format checks, the benchmark doesn't perform any','line_number':179,'multiline':False]['text':' validation of its own','line_number':180,'multiline':False]['text':' Equivalent to just running the model directly on the given input','line_number':183,'multiline':False]['text':' The main method of the class allows to perform a multi-threaded benchmark','line_number':186,'multiline':False]['text':' It returns BenchmarkExecutionStats object with a lot of useful statistics','line_number':187,'multiline':False]['text':' about runtime execution. We can enhance this class in the future to provide','line_number':188,'multiline':False]['text':' more information to the user','line_number':189,'multiline':False]['text':' namespace throughput_benchmark','line_number':196,'multiline':False]['text':' namespace torch','line_number':197,'multiline':False]