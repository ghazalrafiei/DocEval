['text':' The two imports below are not always available depending on the','line_number':4,'multiline':False]['text':' USE_DISTRIBUTED compile flag. Make sure they raise import error','line_number':5,'multiline':False]['text':' if we're trying to use them.','line_number':6,'multiline':False]['text':' torch.distributed makes all the calls in place','line_number':228,'multiline':False]['text':' we allocate new tensors to avoid this','line_number':229,'multiline':False]['text':' Need to create a list of tensors here to do the','line_number':247,'multiline':False]['text':' aggregation, get it from the group size','line_number':248,'multiline':False]['text':' tensor should be correctly sized for the method','line_number':249,'multiline':False]['text':' gathering','line_number':250,'multiline':False]['text':' Need contiguous tensors for collectives.','line_number':303,'multiline':False]['text':' Need contiguous tensors for collectives.','line_number':317,'multiline':False]['text':' As many backends doesn't support ReduceScatter, we use AlltoAll with .sum()','line_number':335,'multiline':False]['text':' to emulate the ReduceScatter behavior','line_number':336,'multiline':False]['text':' Implement it on means of scatter/gather, send/recv async operations have issues','line_number':375,'multiline':False]