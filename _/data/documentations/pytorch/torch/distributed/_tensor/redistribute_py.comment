['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' detect mis-aligned repeated shardings','line_number':44,'multiline':False]['text':' detect mis-aligned sharding','line_number':53,'multiline':False]['text':' decompose Shard(i) -> Shard(j) into Shard(i) -> Replicate() -> Shard(j)','line_number':66,'multiline':False]['text':' TODO: alltoall/permute reshuffling to change device_mesh if they are not the same','line_number':87,'multiline':False]['text':' if rank is not part of mesh, we simply return local_tensor,','line_number':105,'multiline':False]['text':' which should be an empty tensor','line_number':106,'multiline':False]['text':' short cut, just use the original local tensor','line_number':110,'multiline':False]['text':' Case 1: target is Replicate','line_number':115,'multiline':False]['text':' Case 2: target is Shard','line_number':131,'multiline':False]['text':' split the tensor and return the corresponding cloned local shard','line_number':139,'multiline':False]['text':' NOTE: this case shouldn't hit _decompose_sharding, decompose sharding should','line_number':148,'multiline':False]['text':' decompose Shard(0) -> Shard(1) into Shard(0) -> Replicate -> Shard(1)','line_number':149,'multiline':False]['text':' TODO: enable this with all_to_all','line_number':155,'multiline':False]['text':' For replicate -> partial, we perform division to num of chunks and generate','line_number':162,'multiline':False]['text':' parial, and recover it back when pending sum get cleared.','line_number':163,'multiline':False]['text':' type: ignore[override]','line_number':180,'multiline':False]['text':' pyre-fixme[2]: Parameter must be annotated.','line_number':181,'multiline':False]['text':' type: ignore[override]','line_number':207,'multiline':False]['text':' When we run backward pass of redistribute (i.e. manual redistribute from','line_number':209,'multiline':False]['text':' user code instead of torch_dispatch), we scan first and see if we need','line_number':210,'multiline':False]['text':' to change the target placement for one special case:','line_number':211,'multiline':False]['text':'   replicate -> partial.','line_number':212,'multiline':False]['text':' In this case we keep the grad as replicate, this is because we don't','line_number':213,'multiline':False]['text':' want to convert the replicated gradients back to partial, although','line_number':214,'multiline':False]['text':' that's logically conform with the same layout, converting the gradients','line_number':215,'multiline':False]['text':' back to partial is actually useless as you would have to do reduce later','line_number':216,'multiline':False]['text':' which would be more expensive than keeping it replicate! For this reason,','line_number':217,'multiline':False]['text':' we keep the replicate grad here.','line_number':218,'multiline':False]['text':' TODO: see if this make sense for all cases.','line_number':219,'multiline':False]['text':' keep target placement to replicate instead of partial in this case','line_number':225,'multiline':False]