['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' allgather the seed over the default PG','line_number':68,'multiline':False]['text':' instantiate a RNG tracker if haven't. By default DTensor uses an','line_number':77,'multiline':False]['text':' OffsetBasedRNGTracker to perform random operators.','line_number':78,'multiline':False]['text':' the current rank is in mesh','line_number':83,'multiline':False]['text':' synchronize RNG state using rank 0's current one','line_number':158,'multiline':False]['text':' check if the parallel rng state has been synchronized or not','line_number':168,'multiline':False]['text':' execute the region code','line_number':181,'multiline':False]['text':' update offset to synchronize among ranks','line_number':183,'multiline':False]['text':' Compute shard coordinate:','line_number':258,'multiline':False]['text':' The coordinate on each tensor dim is a tuple (idx, range)','line_number':259,'multiline':False]['text':' If a DTensor is partitioned on its dim i into n shards, and the current rank','line_number':260,'multiline':False]['text':' holds the j-th, then its shard coordinate will be (idx=j, range=n) on dim i','line_number':261,'multiline':False]['text':' compute shard linear index','line_number':271,'multiline':False]['text':' compute starting offset using the first shard's size','line_number':274,'multiline':False]['text':' get current RNG offset','line_number':291,'multiline':False]['text':' pytorch: offset must be multiple of 4','line_number':294,'multiline':False]['text':' source: aten/src/ATen/cuda/CUDAGeneratorImpl.cpp','line_number':295,'multiline':False]['text':' pytorch: offset must be multiple of 4','line_number':317,'multiline':False]['text':' source: aten/src/ATen/cuda/CUDAGeneratorImpl.cpp','line_number':318,'multiline':False]['text':' compute shard linear index','line_number':325,'multiline':False]['text':' copy the default RNG state','line_number':338,'multiline':False]['text':' this magic number 2718 comes from Megatron's code','line_number':350,'multiline':False]['text':' (https://github.com/NVIDIA/Megatron-LM/blob/060415572f4365a2e895f8036c4e37dad0efbdf5/megatron/core/tensor_parallel/random.py#L162-L163)','line_number':351,'multiline':False]['text':' check if the tensor parallel rng state has been synchronized or not','line_number':358,'multiline':False]