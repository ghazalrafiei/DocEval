['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' Rules that map each dimension of the output to dimensions of the input tensor','line_number':39,'multiline':False]['text':' repeating a singleton is the same as broadcasting it','line_number':95,'multiline':False]['text':' flattening a scalar leads to a singleton','line_number':113,'multiline':False]['text':' flattening a single dimension is no-op','line_number':116,'multiline':False]['text':' not really a group, just return the input dim back','line_number':141,'multiline':False]['text':' remove singletons from group','line_number':147,'multiline':False]['text':' group_mapping = [(new_index, (shape, old_index)) ...]','line_number':148,'multiline':False]['text':' 1. create padded input dimensions','line_number':181,'multiline':False]['text':' 2. check that input shapes are compatible','line_number':183,'multiline':False]['text':' type: ignore[redundant-cast]','line_number':205,'multiline':False]['text':' if any of the groups is singleton, great, we need to backtrack though','line_number':335,'multiline':False]['text':' produces ([1], [])','line_number':337,'multiline':False]['text':' produces ([], [1])','line_number':341,'multiline':False]['text':' produces ([1], [1]),  ([2], [2]), ([2,3], [6])','line_number':345,'multiline':False]['text':' FIXME: this is wrong when dim=None and one of the dimensions','line_number':389,'multiline':False]['text':' equals size of the mesh. For example squeeze(DTensor(tensor(4), Shard[0])) could','line_number':390,'multiline':False]['text':' end up as squeeze(tensor(1)) if we have 4 devices; this would lead to','line_number':391,'multiline':False]['text':' removal of a dimension that is not actually a singleton.','line_number':392,'multiline':False]['text':' for each input dim, for each mesh dim, provides a list of possible shardable dimensions','line_number':495,'multiline':False]['text':' in case an input dimension disappears (e.g. collapsing, reduction)','line_number':500,'multiline':False]['text':' we cannot shard in that dimension (we need a replication fall-back rule)','line_number':501,'multiline':False]['text':' we need to check that the input dimension is divisible','line_number':538,'multiline':False]['text':' by the size of the submesh we're sharding it on','line_number':539,'multiline':False]['text':' NOTE: it would be possible to shard the same input dimension','line_number':540,'multiline':False]['text':' on more than one mesh dimension. In that case, the dimension','line_number':541,'multiline':False]['text':' needs to be divisible by the product of mesh sizes.','line_number':542,'multiline':False]['text':' In order to keep the problem more tractable, we will not consider','line_number':543,'multiline':False]['text':' double resharding as a suggestion (e.g. [Shard(0), Shard(0) ])','line_number':544,'multiline':False]['text':' but we will allow it if that's the input and it's compatible','line_number':545,'multiline':False]['text':' 1. is this dimension shardable on each individual mesh dim?','line_number':547,'multiline':False]['text':' 2. here we special case things like [Shard(0), Shard(0)]','line_number':553,'multiline':False]['text':' we will only shard our first component of the split','line_number':562,'multiline':False]['text':' no reshard needed','line_number':632,'multiline':False]['text':' We only need the local shape to lower the call into the local op','line_number':635,'multiline':False]['text':' compute the local shape from the global shape, then return','line_number':639,'multiline':False]['text':' a resharding even if we don't really reshard, the only reason','line_number':640,'multiline':False]['text':' for this type of resharding is to lower the global shape to','line_number':641,'multiline':False]['text':' local shape','line_number':642,'multiline':False]['text':' TODO: optimize this. we shouldn't simply blindly replicate','line_number':663,'multiline':False]['text':'       unshardable dims ...','line_number':664,'multiline':False]['text':' FIXME: this can be wrong for situations where we have','line_number':665,'multiline':False]['text':'        [Shard(0), Shard(0)]','line_number':666,'multiline':False]