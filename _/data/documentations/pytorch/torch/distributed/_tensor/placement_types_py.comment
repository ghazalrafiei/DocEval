['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' base class Placement type','line_number':15,'multiline':False]['text':' convenient utils to check for placement types','line_number':17,'multiline':False]['text':' shard placement, shard on a dim','line_number':34,'multiline':False]['text':' chunk tensor over dimension `dim` into n slices with padding if necessary','line_number':58,'multiline':False]['text':' compute the chunk size inline with ``torch.chunk``','line_number':60,'multiline':False]['text':' Compute chunk size for each chunk for ``self.dim``','line_number':63,'multiline':False]['text':' Compute pad size on each chunk','line_number':68,'multiline':False]['text':' Reuse tensor to fill empty chunk with empty tensor','line_number':71,'multiline':False]['text':' Fill the empty tensor with zeroes with padding.','line_number':84,'multiline':False]['text':' Compute the chunk size inline with ``torch.chunk``','line_number':123,'multiline':False]['text':' Compute chunk size for each chunk on the dimension.','line_number':126,'multiline':False]['text':' Return global tensor dim size of current dimension if for empty shard','line_number':138,'multiline':False]['text':' to represent the end of the corresponding tensor dim.','line_number':139,'multiline':False]['text':' if rank is not part of mesh, we simply return an empty tensor','line_number':155,'multiline':False]['text':' Only unpad if the local_tensor was padded on the dimension.','line_number':165,'multiline':False]['text':' if rank is not part of mesh, we simply return local_tensor,','line_number':185,'multiline':False]['text':' which should be an empty tensor','line_number':186,'multiline':False]['text':' if rank is not part of mesh, we simply return local_tensor,','line_number':219,'multiline':False]['text':' which should be an empty tensor','line_number':220,'multiline':False]['text':' check if it needs to pad input tensor before all_gather','line_number':223,'multiline':False]['text':' Unpad the tensor if the input tensor was padded','line_number':247,'multiline':False]['text':' replicate placement','line_number':274,'multiline':False]['text':' every replicate placement is the same','line_number':281,'multiline':False]['text':' if rank is not part of mesh, we simply return an empty tensor','line_number':305,'multiline':False]['text':' This is a default partial placement with element-wise reduce op','line_number':315,'multiline':False]['text':' when doing reduction it follows the contract of `_to_replicate`','line_number':316,'multiline':False]['text':' and `_to_shard` to do the reduction and convert the local tensor','line_number':317,'multiline':False]['text':' to the corresponding state (replicate or shard)','line_number':318,'multiline':False]['text':'','line_number':319,'multiline':False]['text':' We can implement custom reductions as needed by subclassing this','line_number':320,'multiline':False]['text':' class and override those contracts.','line_number':321,'multiline':False]['text':' by default call reduce_shard_tensor of the shard_spec.','line_number':338,'multiline':False]['text':' simple named tuple to represent tensor metadata','line_number':364,'multiline':False]['text':' intentionally to stay simple only for sharding','line_number':365,'multiline':False]['text':' propagation purposes.','line_number':366,'multiline':False]['text':' used internally to propagate the placements','line_number':372,'multiline':False]['text':' tensor meta will only be set during sharding propagation','line_number':378,'multiline':False]['text':' Make sure to recompute the hash in case any of the hashed attributes','line_number':388,'multiline':False]['text':' change (though we do not expect `mesh` or `placements` to change)','line_number':389,'multiline':False]['text':' hashing and equality check for DTensorSpec are used to cache the sharding','line_number':394,'multiline':False]['text':' propagation results. We only need to consider the mesh, placements, shape','line_number':395,'multiline':False]['text':' dtype and stride.','line_number':396,'multiline':False]['text':' Caveat: we need to keep this in mind and sync hash and eq if we add more','line_number':397,'multiline':False]['text':' fields to them.','line_number':398,'multiline':False]['text':' We lazily cache the spec to avoid recomputing the hash upon each','line_number':412,'multiline':False]['text':' use, where we make sure to update the hash when the `tensor_meta`','line_number':413,'multiline':False]['text':' changes by overriding `__setattr__`. This must be lazy so that Dynamo','line_number':414,'multiline':False]['text':' does not try to hash non-singleton `SymInt`s for the stride.','line_number':415,'multiline':False]['text':' type: ignore[union-attr]','line_number':431,'multiline':False]['text':' type: ignore[union-attr]','line_number':432,'multiline':False]['text':' type: ignore[union-attr]','line_number':433,'multiline':False]['text':' simple aliasing for the mesh field, make some','line_number':474,'multiline':False]['text':' checks that mixes DTensor/DTensorSpec easier','line_number':475,'multiline':False]['text':' dims mapping of dist tensor sharding','line_number':499,'multiline':False]['text':' return size of tensor ndim, -1 represent replicate','line_number':500,'multiline':False]['text':' and int >=0 represent shard on that device mesh dim','line_number':501,'multiline':False]['text':' by default replicate on device mesh dims','line_number':550,'multiline':False]['text':' find all mesh dims that need pending reductions','line_number':553,'multiline':False]