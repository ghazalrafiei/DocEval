['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' type: ignore[no-redef]','line_number':32,'multiline':False]['text':' This flag is used internally to control whether we treat the torch.Tensor(non-DTensor)','line_number':91,'multiline':False]['text':' as implicitly replicated or we throw error to user.','line_number':92,'multiline':False]['text':' NOTE: It is EXTREMELY UNSAFE to turn this flag on by default so we intentionally leave','line_number':93,'multiline':False]['text':' it as False by default.','line_number':94,'multiline':False]['text':' operators that does not need to go through sharding propagation','line_number':106,'multiline':False]['text':' type: ignore[operator]','line_number':108,'multiline':False]['text':' extract local tensor and sharding infos to a OpInfo','line_number':110,'multiline':False]['text':' For a non-participating device, we do:','line_number':119,'multiline':False]['text':'   1. if the return type is scalar, set the local result to None.','line_number':120,'multiline':False]['text':'   The local results from all devices will then be all-gathered','line_number':121,'multiline':False]['text':'   and a reduce op will be performed on the list of results','line_number':122,'multiline':False]['text':'   with appropriate operators:','line_number':123,'multiline':False]['text':'       for bool type, we by default use AND to reduce;','line_number':124,'multiline':False]['text':'       we can extend for more ops if necessary.','line_number':125,'multiline':False]['text':'   2. if the return type is Tensor or List[Tensor], return empty','line_number':126,'multiline':False]['text':'   tensor(s) with correct dtype.','line_number':127,'multiline':False]['text':' For a scalar return type, the non-participating device has None','line_number':132,'multiline':False]['text':' as its local result','line_number':133,'multiline':False]['text':' scalar tensor','line_number':142,'multiline':False]['text':' non-scalar tensor','line_number':145,'multiline':False]['text':' return a Tensor value','line_number':151,'multiline':False]['text':' return a List[Tensor] value','line_number':154,'multiline':False]['text':' compute locally with redistribute first if needed','line_number':166,'multiline':False]['text':' run local op computation with potentially modified args/kwargs','line_number':180,'multiline':False]['text':' For DTensor random operator, run it within a distribute region','line_number':190,'multiline':False]['text':' communicate the result to all ranks for some operators that return scalar value','line_number':198,'multiline':False]['text':' perform reduce on the collection with AND op','line_number':204,'multiline':False]['text':' inplace op should return self instead of re-wrapping','line_number':208,'multiline':False]['text':' out variant could possibly have multiple out args (i.e. lu_unpack.out)','line_number':214,'multiline':False]['text':' NOTE: it's very rare that we need to reshard kwargs so we intentionally skip it','line_number':239,'multiline':False]['text':' TODO: the op schema should probably just remain flattened so that we can avoid this tree flatten','line_number':241,'multiline':False]['text':' Need to fix all the ops before doing this.','line_number':242,'multiline':False]['text':' get runtime schema to determine whether to use pytree to flatten inputs','line_number':273,'multiline':False]['text':' flatten args/kwargs when necessary','line_number':279,'multiline':False]['text':' scalar tensor can be safely treated as replicated','line_number':305,'multiline':False]['text':' NOTE: local results might return Optional Tensor from ATen op, so we need','line_number':388,'multiline':False]['text':' to handle that case and make sure we don't wrap None with DTensor.','line_number':389,'multiline':False]['text':' (i.e. native_layer_norm.backward)','line_number':390,'multiline':False]['text':' type: ignore[arg-type]','line_number':396,'multiline':False]['text':' if the res contains only non tensor values, we simply return it without rewrapping','line_number':400,'multiline':False]