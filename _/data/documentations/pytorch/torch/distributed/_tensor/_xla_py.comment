['text':' type:ignore[import]  # noqa: F401','line_number':16,'multiline':False]['text':' type:ignore[import]','line_number':17,'multiline':False]['text':' type:ignore[import]','line_number':18,'multiline':False]['text':' type:ignore[import]','line_number':21,'multiline':False]['text':' wrapper to check xla test requirements','line_number':32,'multiline':False]['text':' pyre-ignore[6]','line_number':36,'multiline':False]['text':' type: ignore[misc]','line_number':38,'multiline':False]['text':' TODO(yeounoh) replace this with xr.use_spmd() when we deprecate the flag.','line_number':41,'multiline':False]['text':' type: ignore[misc]','line_number':43,'multiline':False]['text':' per tensor dimension sharding','line_number':99,'multiline':False]['text':' type:ignore[truthy-function]','line_number':102,'multiline':False]['text':' mesh_idx to tensor_idx (spec.dim)','line_number':103,'multiline':False]['text':' type:ignore[attr-defined]','line_number':104,'multiline':False]['text':' type:ignore[call-overload]','line_number':105,'multiline':False]['text':' spec.dim is already set to None by default','line_number':107,'multiline':False]['text':' type:ignore[return-value]','line_number':111,'multiline':False]['text':' device_mesh is not optional in xla_distribute_tensor','line_number':147,'multiline':False]['text':' convert to XLA device mesh','line_number':151,'multiline':False]['text':' convert tensor to the corresponding device type if it's not in that device type','line_number':155,'multiline':False]['text':' set default placements to replicated if not specified','line_number':158,'multiline':False]['text':' convert placements to xla partition spec','line_number':165,'multiline':False]['text':' type:ignore[attr-defined]','line_number':179,'multiline':False]['text':' type:ignore[attr-defined]','line_number':184,'multiline':False]['text':' Annotates sharding and returns an XLAShardedTensor','line_number':187,'multiline':False]