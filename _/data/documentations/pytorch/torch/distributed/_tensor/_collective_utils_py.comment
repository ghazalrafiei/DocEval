['text':' TODO: we need to migrate these APIs to be functional collectives','line_number':24,'multiline':False]['text':' TODO: Ideally we should use the meta tensor way','line_number':51,'multiline':False]['text':' (to register a meta kernel for the collective op)','line_number':52,'multiline':False]['text':' so that it would avoid the communication. Need to','line_number':53,'multiline':False]['text':' remove the check below once that is done.','line_number':54,'multiline':False]['text':' src need to be global rank','line_number':59,'multiline':False]['text':' TODO: Ideally we should use the meta tensor way','line_number':107,'multiline':False]['text':' (to register a meta kernel for the collective op)','line_number':108,'multiline':False]['text':' so that it would avoid the communication. Need to','line_number':109,'multiline':False]['text':' remove the check below once that is done.','line_number':110,'multiline':False]['text':' src need to be global rank','line_number':115,'multiline':False]['text':' TODO: test uneven split on GLOO and NCCL','line_number':123,'multiline':False]['text':' no direct dist.all_to_all support on 'gloo' so we manually do scatters','line_number':135,'multiline':False]['text':' TODO: pull the handle of uneven case in #492','line_number':140,'multiline':False]['text':' src need to be global rank','line_number':143,'multiline':False]['text':' generate bandwidth factor for intra-host/inter-host communication pattern','line_number':171,'multiline':False]['text':' magic number for intra-host communication bandwidth factor','line_number':179,'multiline':False]['text':' TODO: see if we need to tweak this or offer a way for user','line_number':180,'multiline':False]['text':' to specify the bandwidths','line_number':181,'multiline':False]['text':' constant latency factor + bandwidth cost','line_number':190,'multiline':False]['text':' allreduce have 2x comm bytes compare to allgather/reduce_scatter','line_number':203,'multiline':False]['text':' constant latency factor + bandwidth cost','line_number':221,'multiline':False]['text':' make infinite cost if meshes are not same','line_number':245,'multiline':False]['text':' TODO: see if we want to support this once there's cross mesh communication','line_number':246,'multiline':False]['text':' short-cut:','line_number':250,'multiline':False]['text':' comm cost is 0 if current spec is already full replication','line_number':251,'multiline':False]['text':' Transformation that considered for redistribute cost:','line_number':257,'multiline':False]['text':' 1. allgather 2. alltoall','line_number':258,'multiline':False]['text':' 3. allreduce 4. reduce_scatter','line_number':259,'multiline':False]['text':' allgather gives larger comm bytes','line_number':266,'multiline':False]['text':' add up allgather comm cost','line_number':268,'multiline':False]['text':' should be alltoall comm, since we haven't implement it yet, add penalty','line_number':271,'multiline':False]['text':' to favor allgather instead','line_number':272,'multiline':False]['text':' add up allreduce comm cost','line_number':275,'multiline':False]['text':' add up reduce_scatter comm cost','line_number':278,'multiline':False]['text':' after reduce_scatter the comm bytes for further collectives halved.','line_number':280,'multiline':False]['text':' ban shard -> partial as it does not make sense to perform','line_number':283,'multiline':False]['text':' this redistribute','line_number':284,'multiline':False]