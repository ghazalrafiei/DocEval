['text':' type: ignore[no-redef]','line_number':18,'multiline':False]['text':' type: ignore[attr-defined]','line_number':141,'multiline':False]['text':' type: ignore[attr-defined]','line_number':176,'multiline':False]['text':' type: ignore[attr-defined]','line_number':204,'multiline':False]['text':' TODO this should be done inside AsyncCollectiveTensor to delay the wait() call','line_number':206,'multiline':False]['text':' torch.cat access the data so we already need to wait here, first do wait','line_number':208,'multiline':False]['text':' and then chunk + cat avoid us going through ACT dispatching logic again','line_number':209,'multiline':False]['text':' type: ignore[attr-defined]','line_number':211,'multiline':False]['text':' type: ignore[attr-defined]','line_number':246,'multiline':False]['text':' type: ignore[attr-defined]','line_number':271,'multiline':False]['text':' type: ignore[attr-defined]','line_number':295,'multiline':False]['text':' type: ignore[attr-defined]','line_number':331,'multiline':False]['text':' This is a bit unsafe: it checks if the first argument in the schema reports as a non-mutable alias.','line_number':336,'multiline':False]['text':' Today, this maps 1:1 with "aten ops that are views".','line_number':337,'multiline':False]['text':' check if op is a view','line_number':343,'multiline':False]['text':' type: ignore[attr-defined]','line_number':378,'multiline':False]['text':' type: ignore[attr-defined]','line_number':400,'multiline':False]['text':' Fast handle aten.view as a lot of view related op goes to aten.view','line_number':445,'multiline':False]['text':' eventually, this avoids pytree slowdown','line_number':446,'multiline':False]['text':' wait_tensor is idepotent and will do stream sync only once','line_number':455,'multiline':False]['text':' wait_tensor is idepotent and will do stream sync only once','line_number':461,'multiline':False]['text':' we don't wrap the result as it doesn't need to be waited on.','line_number':470,'multiline':False]['text':' View ops dont require a sync, so we should re-wrap the outputs.','line_number':473,'multiline':False]['text':' had to define this hack _inside_ expand_group to avoid','line_number':495,'multiline':False]['text':' graph_break [('torch.* op returned non-Tensor int','line_number':496,'multiline':False]['text':' caused by 'cast_*` functions being treated as 'torch.*' ops (iiuc)','line_number':497,'multiline':False]['text':' fake cast op for use at runtime since dynamo doesn't support real cast','line_number':507,'multiline':False]['text':' also, dynamo didn't like encountering 'typing' objects ()','line_number':508,'multiline':False]['text':' NotImplementedError: argument of type: <class 'typing._GenericAlias'>','line_number':509,'multiline':False]['text':' TODO: it should run collective in the whole mesh instead of dim 0','line_number':540,'multiline':False]['text':' If functionalization is turned on, we are almost definitely compiling/tracing.','line_number':566,'multiline':False]['text':' (In particular, AOTAutograd traces a model once with functionalization on','line_number':567,'multiline':False]['text':'  but proxy tracing turned of, so this is how we detect it).','line_number':568,'multiline':False]['text':' We now register meta kernels to deal with tracing','line_number':598,'multiline':False]['text':' NB: We often say all_to_all has dynamic output size, but this is not','line_number':645,'multiline':False]['text':' technically true: instead, what typically happens is you manually','line_number':646,'multiline':False]['text':' communicate the output_split_sizes ahead of time (which is dynamic),','line_number':647,'multiline':False]['text':' but then you pass those sizes explicitly, and the all to all itself','line_number':648,'multiline':False]['text':' isn't dynamic, it just follows the specified output splits','line_number':649,'multiline':False]['text':' noqa: B950','line_number':701,'multiline':False]['text':' Library MUST be defined at module scope or it doesn't work','line_number':715,'multiline':False]['text':' Creating a "DEF" Library always crashes torch::deploy so we create our Library instances here','line_number':716,'multiline':False]['text':'   guarded against running inside it','line_number':717,'multiline':False]['text':' TODO add a type,','line_number':764,'multiline':False]['text':' TODO type is actually c10d ReduceOp. is this ok?','line_number':778,'multiline':False]['text':' TODO add a type','line_number':779,'multiline':False]['text':' This dict should contain sets of functions that dynamo is allowed to remap.','line_number':795,'multiline':False]['text':' Functions in this set should accept the same args/kwargs 1:1 as their mapping.','line_number':796,'multiline':False]