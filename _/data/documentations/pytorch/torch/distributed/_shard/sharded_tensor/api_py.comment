['text':' type: ignore[attr-defined]','line_number':1,'multiline':False]['text':' Tracking for sharded tensor objects.','line_number':49,'multiline':False]['text':' Default sharded ops','line_number':54,'multiline':False]['text':' Customized user ops','line_number':57,'multiline':False]['text':' Use __new__ to construct a wrapper tensor, for recording tensor','line_number':78,'multiline':False]['text':' properties and logging purposes.','line_number':79,'multiline':False]['text':' check sharding spec and build sharded tensor metadata','line_number':82,'multiline':False]['text':' type: ignore[attr-defined]','line_number':102,'multiline':False]['text':' set sharding spec','line_number':110,'multiline':False]['text':' set metadata','line_number':112,'multiline':False]['text':' set local shards','line_number':114,'multiline':False]['text':' check if shards_metadata have overlap shards','line_number':171,'multiline':False]['text':' check if the shards_metadata is compatible with overall size of the sharded tensor.','line_number':174,'multiline':False]['text':' done validation, add local_shards','line_number':177,'multiline':False]['text':' prepare initialization, initialize fields like','line_number':252,'multiline':False]['text':' _process_group, _local_shards, etc.','line_number':253,'multiline':False]['text':' do post initialization (i.e. register sharded_tensor_id, initialize_rpc)','line_number':276,'multiline':False]['text':' Initialize RPC if available.','line_number':292,'multiline':False]['text':' Clean up the global map.','line_number':307,'multiline':False]['text':' type: ignore[call-overload]','line_number':314,'multiline':False]['text':' Validate PG and RPC ranks match.','line_number':317,'multiline':False]['text':' Gather all the sharded tensor ids.','line_number':329,'multiline':False]['text':' Share the local shards to the entire world.','line_number':340,'multiline':False]['text':' Skip self.','line_number':344,'multiline':False]['text':' Barrier for all RPCs to finish on all ranks.','line_number':358,'multiline':False]['text':' type: ignore[override]','line_number':370,'multiline':False]['text':' type: ignore[attr-defined]','line_number':397,'multiline':False]['text':' collect sizes','line_number':411,'multiline':False]['text':' enforce_dtype is deprecated.  Do it for backward compatibility.','line_number':424,'multiline':False]['text':' TODO make it as a view of out tensor','line_number':426,'multiline':False]['text':' enforce_dtype is deprecated.  Do it for backward compatibility.','line_number':433,'multiline':False]['text':' In _validate_output_tensor_for_gather, we raise if out == None and rank == dst','line_number':453,'multiline':False]['text':' TODO: make this a __torch_function__ op once ShardedTensor becomes a','line_number':491,'multiline':False]['text':' torch.Tensor subclass, see https://github.com/pytorch/pytorch/issues/75402','line_number':492,'multiline':False]['text':' type: ignore[union-attr]','line_number':499,'multiline':False]['text':' if every shard is already on CPU, return the original object','line_number':501,'multiline':False]['text':' if not, returns a copy of this object on CPU','line_number':505,'multiline':False]['text':' move all local shards to cpu, and change metadata','line_number':507,'multiline':False]['text':' type: ignore[call-arg]','line_number':509,'multiline':False]['text':' type: ignore[union-attr]','line_number':511,'multiline':False]['text':' type: ignore[union-attr]','line_number':518,'multiline':False]['text':' type: ignore[union-attr]','line_number':519,'multiline':False]['text':' returns a copy of ShardedTensor on CUDA current device','line_number':560,'multiline':False]['text':' move all local shards to current device, and change metadata','line_number':562,'multiline':False]['text':' if local shards already on the current device, there's no','line_number':563,'multiline':False]['text':' real data movement, only the metadata are copied.','line_number':564,'multiline':False]['text':' type: ignore[call-arg]','line_number':570,'multiline':False]['text':' type: ignore[union-attr]','line_number':572,'multiline':False]['text':' type: ignore[union-attr]','line_number':580,'multiline':False]['text':' type: ignore[union-attr]','line_number':581,'multiline':False]['text':' we need to use `init_from_local_shards` to communicate between ranks','line_number':584,'multiline':False]['text':' and update the sharding spec/shards metadata.','line_number':585,'multiline':False]['text':' if device_to set to cuda, set to current device even','line_number':626,'multiline':False]['text':' if user specify the device index.','line_number':627,'multiline':False]['text':' already have correct dtype and device, return itself','line_number':640,'multiline':False]['text':' returns a copy of ShardedTensor on CUDA current device','line_number':643,'multiline':False]['text':' type: ignore[call-overload]','line_number':647,'multiline':False]['text':' update metadata','line_number':659,'multiline':False]['text':' type: ignore[union-attr]','line_number':663,'multiline':False]['text':' we need to use `init_from_local_shards` to communicate between ranks','line_number':666,'multiline':False]['text':' and update the sharding spec/shards metadata.','line_number':667,'multiline':False]['text':' STEP 1: Validate the Shardmetadatas locally','line_number':685,'multiline':False]['text':' STEP 2. Validate metadata across ranks, and build a global sharded tensor','line_number':701,'multiline':False]['text':' metadata by gathering local ShardedTensorMetadata','line_number':702,'multiline':False]['text':' STEP 3: Validation done, create the actual ShardedTensor and populate fields','line_number':718,'multiline':False]['text':' prepare initialization','line_number':719,'multiline':False]['text':' attach local_shards to the ShardedTensor created','line_number':732,'multiline':False]['text':' run post initialization, i.e. map registration, rpc initialization','line_number':735,'multiline':False]['text':' TODO: figure out what the API should behave when some rank have no shard','line_number':834,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/7313','line_number':835,'multiline':False]['text':' type: ignore[override]','line_number':845,'multiline':False]['text':' collect local shard metadatas from the global sharded_tensor_metadata','line_number':872,'multiline':False]['text':' type: ignore[attr-defined]','line_number':873,'multiline':False]['text':' check if shards_metadata have overlap shards','line_number':971,'multiline':False]['text':' check if the shards_metadata is compatible with overall size of the sharded tensor.','line_number':974,'multiline':False]['text':' done validation, add local_shards','line_number':977,'multiline':False]['text':' run post initialization, i.e. map registration, rpc initialization','line_number':981,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1071,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1072,'multiline':False]['text':' type: ignore[arg-type]','line_number':1077,'multiline':False]['text':' type: ignore[arg-type]','line_number':1085,'multiline':False]['text':' Dispatch to custom user provided op first if it exists.','line_number':1109,'multiline':False]['text':' Dispatch to custom sharding spec op if it has one.','line_number':1113,'multiline':False]['text':' Find ShardedTensor instance to get process_group and sharding_spec.','line_number':1132,'multiline':False]['text':' type: ignore[override]','line_number':1150,'multiline':False]['text':' Setup process group','line_number':1208,'multiline':False]['text':' Validate process group.','line_number':1212,'multiline':False]