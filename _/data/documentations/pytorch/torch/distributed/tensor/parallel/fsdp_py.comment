['text':' This is where it gets tricky, we have to produce a ShardedTensor that has full coverage','line_number':76,'multiline':False]['text':' and yet has only one valid shard for the current rank.','line_number':77,'multiline':False]['text':' ignore memory_format and pin_memory as those are not supported by DT','line_number':107,'multiline':False]['text':' let's see if we need','line_number':132,'multiline':False]['text':' We do this differently here, we create a ST with no local shards then patch it','line_number':197,'multiline':False]['text':' We need to explicitly call .detach() to return a new tensor detached from the current graph.','line_number':242,'multiline':False]['text':' When a layer is not involved in TP, then the tensor will not be a DTensor.','line_number':245,'multiline':False]['text':' e.g. When a layer is not sppecified in the parallelize_plan, TP will have no effect on the layer.','line_number':246,'multiline':False]['text':' e.g. When you do PairwiseParallel on a 3 layer model, TP will have no effect on the third layer.','line_number':247,'multiline':False]['text':' For tensors, it is replicated across tp dimension and sharded across FSDP dimension.','line_number':250,'multiline':False]['text':' TP is the inner dimension and FSDP is the outer dimension.','line_number':251,'multiline':False]['text':' Therefore, shard placements for tensor is (Shard(0), Replicate()).','line_number':252,'multiline':False]['text':' type: ignore[call-overload]','line_number':255,'multiline':False]['text':' For DTensors, it is sharded across tp dimension first and then sharded across FSDP dimension.','line_number':270,'multiline':False]['text':' TP is the inner dimension and FSDP is the outer dimension.','line_number':271,'multiline':False]['text':' Therefore, shard placements for tensor is (Shard(0), tp_placement).','line_number':272,'multiline':False]['text':' type: ignore[call-overload]','line_number':274,'multiline':False]['text':' type: ignore[misc]','line_number':275,'multiline':False]['text':' type: ignore[call-overload]','line_number':276,'multiline':False]['text':' pyre-ignore[16]','line_number':292,'multiline':False]['text':' FSDP + TP: [Shard(0), tp_placement] -> [Replicate(), tp_placement]','line_number':306,'multiline':False]