['text':' Config','line_number':23,'multiline':False]['text':' embedding_sum(input, offsets)','line_number':65,'multiline':False]['text':' Make sure combined PS dimension is always bigger or equal than the FC input','line_number':69,'multiline':False]['text':' type: ignore[call-overload]','line_number':105,'multiline':False]['text':' Setup the model.','line_number':128,'multiline':False]['text':' Retrieve all model parameters as rrefs for DistributedOptimizer.','line_number':131,'multiline':False]['text':' Retrieve parameters from all embedding tables for the current trainer.','line_number':133,'multiline':False]['text':' model.parameters() only includes local parameters.','line_number':141,'multiline':False]['text':' Setup distributed optimizer','line_number':145,'multiline':False]['text':' Generate offsets.','line_number':155,'multiline':False]['text':' Include warm-up cycles during training','line_number':171,'multiline':False]['text':' create distributed autograd context','line_number':176,'multiline':False]['text':' Run distributed backward pass','line_number':184,'multiline':False]['text':' Run distributed optimizer. Gradients propagated all the way to the parameter servers','line_number':187,'multiline':False]['text':' Not necessary to zero grads as each iteration creates a different','line_number':190,'multiline':False]['text':' distributed autograd context which hosts different grads','line_number':191,'multiline':False]['text':' print("Training done for epoch {}".format(epoch))','line_number':194,'multiline':False]['text':' Throw away warm-up measurements','line_number':196,'multiline':False]['text':' Using different port numbers in TCP init_method for init_rpc and','line_number':205,'multiline':False]['text':' init_process_group to avoid port conflicts.','line_number':206,'multiline':False]['text':' Rank 16. Master','line_number':210,'multiline':False]['text':' type: ignore[attr-defined]','line_number':215,'multiline':False]['text':' Build the Embedding tables on the Parameter Servers.','line_number':220,'multiline':False]['text':' Run training loop on the trainers.','line_number':234,'multiline':False]['text':' Wait for all training to finish.','line_number':247,'multiline':False]['text':' Rank 0-7. Trainers','line_number':256,'multiline':False]['text':' Initialize process group for Distributed DataParallel on trainers.','line_number':259,'multiline':False]['text':' Initialize RPC. Trainer just waits for RPCs from master.','line_number':267,'multiline':False]['text':' Rank 8-15. Parameter Servers','line_number':276,'multiline':False]['text':' type: ignore[attr-defined]','line_number':283,'multiline':False]['text':' parameter server do nothing','line_number':286,'multiline':False]['text':' block until all rpcs finish','line_number':289,'multiline':False]['text':' Cmd arguments to enable automated runs (e.g. Chronos, SSH, etc).','line_number':311,'multiline':False]['text':' Defaults:','line_number':359,'multiline':False]['text':'  8 trainers (rank 0-7),','line_number':360,'multiline':False]['text':'  8 parameter servers (rank 8-15),','line_number':361,'multiline':False]['text':'  1 master (rank 16).','line_number':362,'multiline':False]['text':' Trainers + PS + Master','line_number':363,'multiline':False]