['text':' Define a TorchScript compatible Functional Adagrad Optimizer','line_number':10,'multiline':False]['text':' where we use these optimizer in a functional way.','line_number':11,'multiline':False]['text':' Instead of using the `param.grad` when updating parameters,','line_number':12,'multiline':False]['text':' we explicitly let the user pass gradients to the `step` function','line_number':13,'multiline':False]['text':' this is so that we could separate the gradients and parameters','line_number':14,'multiline':False]['text':' and allow multithreaded trainer to update the parameters','line_number':15,'multiline':False]['text':' without data traces on accumulating to the same .grad.','line_number':16,'multiline':False]['text':' NOTE: This should be only used by distributed optimizer internals','line_number':17,'multiline':False]['text':' and not meant to expose to the user.','line_number':18,'multiline':False]['text':' NOTE: we only have one param_group and don't allow user to add additional','line_number':53,'multiline':False]['text':' param group as it's not a common use case.','line_number':54,'multiline':False]['text':' TODO: no union or any types in TorchScript, make step a scalar tensor instead','line_number':57,'multiline':False]['text':' This is also needed by if we want to share_memory on the step across processes','line_number':58,'multiline':False]