['text':' Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' This source code is licensed under the BSD license found in the','line_number':3,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':4,'multiline':False]['text':' Credits:  classy_vision/generic/distributed_util.py','line_number':28,'multiline':False]['text':' Send the object','line_number':93,'multiline':False]['text':' Receive the object','line_number':102,'multiline':False]['text':' DDP guarantees all parameters in the bucket have the same device','line_number':165,'multiline':False]['text':' Modified per bucket reconstruction','line_number':239,'multiline':False]['text':' Group Ranks','line_number':243,'multiline':False]['text':' Modified per iteration','line_number':248,'multiline':False]['text':' Used by `hook_with_zero_step()`','line_number':251,'multiline':False]['text':' Perform type and assumption checks on the input parameters','line_number':380,'multiline':False]['text':' NOTE: The parent constructor uses `add_param_group()` which is','line_number':384,'multiline':False]['text':' partially overloaded in ZeroRedundancyOptimizer, so we use the','line_number':385,'multiline':False]['text':' `initialized` flag to dissociate the behaviour of `add_param_group()`','line_number':386,'multiline':False]['text':' between the parent and child.','line_number':387,'multiline':False]['text':' Now, all parameters are held in both `self._all_params` and','line_number':392,'multiline':False]['text':' `self.param_groups`','line_number':393,'multiline':False]['text':' Internal data structures (`_cache` indicates lazily evaluated)','line_number':395,'multiline':False]['text':' Default device for collective communication and buckets','line_number':408,'multiline':False]['text':' If `overlap_with_ddp=True`, local optimizer initialization is delayed','line_number':424,'multiline':False]['text':' to run time after the necessary information has been collected','line_number':425,'multiline':False]['text':' `self._buckets` is used if `parameters_as_bucket_view=True`, in','line_number':437,'multiline':False]['text':' which case parameter data is flattened into contiguous bucket tensors','line_number':438,'multiline':False]['text':' Optional consolidated optimizer state, only populated if this rank','line_number':443,'multiline':False]['text':' is the target in `consolidate_state_dict()`','line_number':444,'multiline':False]['text':' NOTE: The rest of the method assumes that the call to the parent's','line_number':483,'multiline':False]['text':' `add_param_group()` appends the new parameter group and preserves','line_number':484,'multiline':False]['text':' the previous parameter-group ordering','line_number':485,'multiline':False]['text':' Force a re-partitioning of the parameters','line_number':488,'multiline':False]['text':' NOTE: All parameters in the old parameter groups should be','line_number':491,'multiline':False]['text':' assigned to the same ranks so that the local optimizers do not','line_number':492,'multiline':False]['text':' need to be reinitialized','line_number':493,'multiline':False]['text':' Add the parameters assigned to this rank from the new parameter','line_number':495,'multiline':False]['text':' group to the local optimizer, if any','line_number':496,'multiline':False]['text':' Update the bucketing strategy accordingly','line_number':500,'multiline':False]['text':' Sync the exposed `param_groups` attributes to the local optimizer in','line_number':522,'multiline':False]['text':' case they have been updated','line_number':523,'multiline':False]['text':' Pull the sharded state from all ranks and store them in rank order','line_number':526,'multiline':False]['text':' NOTE: We wastefully use `broadcast()` (e.g. instead of `gather()`)','line_number':531,'multiline':False]['text':' due to compatibility issues with NCCL backend; a possible follow-up','line_number':532,'multiline':False]['text':' is to move all sharded state management to RPC RRef','line_number':533,'multiline':False]['text':' Consolidate all local `state_dict`s on this rank, storing on','line_number':540,'multiline':False]['text':' CPU to save GPU memory','line_number':541,'multiline':False]['text':' Directly append own optimizer state','line_number':543,'multiline':False]['text':' Receive the optimizer state from the source rank','line_number':552,'multiline':False]['text':' Send the optimizer state to the target rank','line_number':568,'multiline':False]['text':' Discard the received object; `broadcast()` is used for','line_number':576,'multiline':False]['text':' compatibility reasons','line_number':577,'multiline':False]['text':' Partition the parameters optimizing for uniformity','line_number':670,'multiline':False]['text':' Sort the parameters by size (largest first)','line_number':678,'multiline':False]['text':' Greedily add the parameter to rank with smallest size so far','line_number':683,'multiline':False]['text':' Apply the constructed partition of the parameter group','line_number':687,'multiline':False]['text':' Partition the parameters according to `params_per_rank`','line_number':694,'multiline':False]['text':' Apply the passed-in partition of the parameter group','line_number':706,'multiline':False]['text':' Define the assignment threshold to approximate uniformity','line_number':941,'multiline':False]['text':' type: ignore[operator]','line_number':943,'multiline':False]['text':' Assign each DDP bucket entirely to a single rank','line_number':950,'multiline':False]['text':' Assign each DDP bucket to possibly multiple ranks','line_number':962,'multiline':False]['text':' Specifically, sort the DDP buckets by increasing size, and for','line_number':963,'multiline':False]['text':' each bucket, iteratively assign the maximal unassigned subset','line_number':964,'multiline':False]['text':' with size less than `threshold` to the rank with the least total','line_number':965,'multiline':False]['text':' size so far -- each such assignment is represented by a','line_number':966,'multiline':False]['text':' `_DDPBucketAssignment` instance and only contains parameters from','line_number':967,'multiline':False]['text':' a single DDP bucket','line_number':968,'multiline':False]['text':' Include up to but not including the parameter that','line_number':985,'multiline':False]['text':' exceeded the threshold','line_number':986,'multiline':False]['text':' Assign the remainder of the bucket so that no assignment','line_number':998,'multiline':False]['text':' spans across two buckets','line_number':999,'multiline':False]['text':' Check if the model trainability has changed','line_number':1044,'multiline':False]['text':' Sync the exposed `param_groups` attributes to the local optimizer in','line_number':1061,'multiline':False]['text':' case they have been updated','line_number':1062,'multiline':False]['text':' Run the optimizer step on this shard only','line_number':1065,'multiline':False]['text':' Sync any updated attributes in the local optimizer to the exposed','line_number':1082,'multiline':False]['text':' `param_groups`','line_number':1083,'multiline':False]['text':' Perform the local optimizer step','line_number':1111,'multiline':False]['text':' Sync all of the updated parameter shards across the ranks','line_number':1114,'multiline':False]['text':' Clear any state irrelevant to this rank','line_number':1169,'multiline':False]['text':' Load the parameter state to the local optimizer','line_number':1172,'multiline':False]['text':' Force zero-dimensional tensors (like Adam "step") on CPU','line_number':1176,'multiline':False]['text':' Sync the input state with the exposed and local optimizer states','line_number':1183,'multiline':False]['text':' Get the possibly-stale global optimizer state that uses global','line_number':1213,'multiline':False]['text':' parameter indexing','line_number':1214,'multiline':False]['text':' Update the global optimizer state with local state information,','line_number':1217,'multiline':False]['text':' factoring in the translation from local to global indexing','line_number':1218,'multiline':False]['text':' `local_param_group` stores local indices, while','line_number':1229,'multiline':False]['text':' `global_param_group` stores the tensors directly','line_number':1230,'multiline':False]['text':' Update the global parameter state, if any','line_number':1240,'multiline':False]['text':' Sort the parameters in the state','line_number':1247,'multiline':False]['text':' Sync all attributes except the parameters','line_number':1273,'multiline':False]['text':' `self._buckets[i][j]` are the parameters stored on device i and','line_number':1302,'multiline':False]['text':' assigned to rank j','line_number':1303,'multiline':False]['text':' type: ignore[assignment]','line_number':1305,'multiline':False]['text':' Clone in case the parameter was previously part of','line_number':1316,'multiline':False]['text':' a bucket to avoid the data from being destroyed','line_number':1317,'multiline':False]['text':' assumes all same dtype','line_number':1322,'multiline':False]['text':' Create a dummy bucket if there are no parameters','line_number':1325,'multiline':False]['text':' Construct the bucket (assuming all dense and same dtype)','line_number':1328,'multiline':False]['text':' type: ignore[arg-type]','line_number':1336,'multiline':False]['text':' assumes all same dtype','line_number':1364,'multiline':False]['text':' Construct the bucket tensor (assuming all dense and same dtype)','line_number':1367,'multiline':False]['text':' Ensure that `self._all_params` contains a list of all parameters','line_number':1423,'multiline':False]['text':' `all_params` contains parameter groups (not parameters)','line_number':1428,'multiline':False]['text':' `overlap_with_ddp=True` requires a local functional optimizer','line_number':1485,'multiline':False]['text':' Functional optimizers only support a single parameter group and','line_number':1487,'multiline':False]['text':' require passing in the parameters as a list','line_number':1488,'multiline':False]['text':' Try to pass `_allow_empty_param_list=True` to avoid erroring','line_number':1494,'multiline':False]['text':' type: ignore[no-redef]','line_number':1509,'multiline':False]['text':' Log information about the DDP and ZeRO bucketing','line_number':1511,'multiline':False]['text':' NOTE: Passing `param_groups` into the local optimizer constructor','line_number':1531,'multiline':False]['text':' bypasses the empty parameter list check','line_number':1532,'multiline':False]['text':' type: ignore[no-redef]','line_number':1533,'multiline':False]['text':' TODO: Manually add `self.param_groups` if using a functional','line_number':1535,'multiline':False]['text':' optimizer; remove this if/when the functional optimizers support','line_number':1536,'multiline':False]['text':' multiple parameter groups','line_number':1537,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1543,'multiline':False]['text':' Using a functional optimizer is only supported when','line_number':1623,'multiline':False]['text':' `overlap_with_ddp=True`','line_number':1624,'multiline':False]['text':' Already a functional optimizer','line_number':1633,'multiline':False]['text':' Translate the passed-in optimizer class to its functional','line_number':1636,'multiline':False]['text':' equivalent if `overlap_with_ddp=True`','line_number':1637,'multiline':False]