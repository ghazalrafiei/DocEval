['text':' Here we ensure the return dtype is the same as the outputs dtype.','line_number':132,'multiline':False]['text':' For the FSDP + Mixed Precision use case, the loss output is in the Mixed Precision','line_number':133,'multiline':False]['text':' format (fp16, bf16) and so the scaled loss should be of the same dtype.','line_number':134,'multiline':False]['text':' Here we ensure the return dtype is the same as the outputs dtype.','line_number':148,'multiline':False]['text':' For the FSDP + Mixed Precision use case, the loss output is in the Mixed Precision','line_number':149,'multiline':False]['text':' format (fp16, bf16) and so the scaled loss should be of the same dtype.','line_number':150,'multiline':False]['text':' To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.','line_number':201,'multiline':False]['text':' There could be thousands of grads, so we'd like to iterate through them just once.','line_number':202,'multiline':False]['text':' However, we don't know their devices or dtypes in advance.','line_number':203,'multiline':False]['text':' https://stackoverflow.com/questions/5029934/defaultdict-of-defaultdict','line_number':205,'multiline':False]['text':' Google says mypy struggles with defaultdicts type annotations.','line_number':206,'multiline':False]['text':' type: ignore[var-annotated]','line_number':207,'multiline':False]['text':' is_coalesced() == False means the sparse grad has values with duplicate indices.','line_number':216,'multiline':False]['text':' coalesce() deduplicates indices and adds all values that have the same index.','line_number':217,'multiline':False]['text':' For scaled fp16 values, there's a good chance coalescing will cause overflow,','line_number':218,'multiline':False]['text':' so we should check the coalesced _values().','line_number':219,'multiline':False]['text':' coalesce is not supported in torch.float16','line_number':221,'multiline':False]['text':' There exist contexts (e.g. w/ `use_orig_params=True`) wherein some','line_number':246,'multiline':False]['text':' ranks may have no (non-zero sized) parameter shards, necessitating the','line_number':247,'multiline':False]['text':' initialization of `per_device_found_inf._per_device_tensors` here','line_number':248,'multiline':False]['text':' FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.','line_number':269,'multiline':False]['text':' Synchronize the detected inf across the ranks','line_number':281,'multiline':False]['text':' Make sure that the calls are done before moving out.','line_number':301,'multiline':False]['text':' type: ignore[var-annotated]','line_number':343,'multiline':False]['text':' Accept a new user-defined scale.','line_number':346,'multiline':False]['text':' type: ignore[union-attr]','line_number':348,'multiline':False]['text':' type: ignore[attr-defined]','line_number':351,'multiline':False]['text':' type: ignore[union-attr]','line_number':354,'multiline':False]['text':' Consume shared inf/nan data collected from optimizers to update the scale.','line_number':356,'multiline':False]['text':' If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.','line_number':357,'multiline':False]['text':' type: ignore[arg-type]','line_number':375,'multiline':False]['text':' type: ignore[arg-type]','line_number':376,'multiline':False]['text':' type: ignore[arg-type]','line_number':378,'multiline':False]['text':' type: ignore[arg-type]','line_number':379,'multiline':False]['text':' type: ignore[arg-type]','line_number':380,'multiline':False]['text':' To prepare for next iteration, clear the data collected from optimizers this iteration.','line_number':383,'multiline':False]