['text':' We can't use .items() below cuz we'd run into a concurrent modification error','line_number':171,'multiline':False]['text':' Positive-dimension tensor state: communicate across ranks','line_number':219,'multiline':False]['text':' If the parameter is not sharded, then neither is the','line_number':221,'multiline':False]['text':' positive-dimension tensor state, so no need to communicate it --','line_number':222,'multiline':False]['text':' we take the target rank's value','line_number':223,'multiline':False]['text':' Assume that positive-dimension tensor optimizer state','line_number':235,'multiline':False]['text':' has the same shape as the sharded flat parameter','line_number':236,'multiline':False]['text':' type: ignore[attr-defined]','line_number':237,'multiline':False]['text':' Zero-dimension tensor state and non-tensor state: take this rank's','line_number':247,'multiline':False]['text':' value directly','line_number':248,'multiline':False]['text':' Add positive-dimension tensor state: unflatten with views','line_number':293,'multiline':False]['text':' Add zero-dimension tensor state: take the target rank's value','line_number':324,'multiline':False]['text':' Add non-tensor state: take the target rank's value','line_number':327,'multiline':False]['text':' type: ignore[union-attr]','line_number':343,'multiline':False]['text':' type: ignore[attr-defined]','line_number':390,'multiline':False]['text':' Flatten and shard the state.','line_number':396,'multiline':False]['text':' type: ignore[operator]','line_number':406,'multiline':False]['text':' Broadcast unflat_osd without non-scalar tensor if rank0_only is True.','line_number':455,'multiline':False]['text':' Construct the "state" part','line_number':459,'multiline':False]['text':' Only include non-empty states since as expected by','line_number':495,'multiline':False]['text':' `torch.optim.Optimizer` s unless the optimizer is KeyedOptimizer','line_number':496,'multiline':False]['text':' or NamedOptimizer.','line_number':497,'multiline':False]['text':' NamedOptimizer or KeyedOptimizer case.','line_number':504,'multiline':False]['text':' type: ignore[call-overload]','line_number':505,'multiline':False]['text':' do not flatten non-FSDP parameters' states','line_number':518,'multiline':False]['text':' Deference the tensor so that PyTorch can collect the memory.','line_number':529,'multiline':False]['text':' Move the tensor in the original osd back to CPU to make the','line_number':532,'multiline':False]['text':' original osd unaffected.','line_number':533,'multiline':False]['text':' Handle user-defined state, states that are not associated with parameters.','line_number':538,'multiline':False]['text':' Construct the "param_groups" part -- copy as is since it will be','line_number':546,'multiline':False]['text':' rekeyed later according to the target rank's optimizer','line_number':547,'multiline':False]['text':' Only copy param_groups if it exists in unflat_osd','line_number':548,'multiline':False]['text':' Check if these unflattened parameters have any optimizer state','line_number':593,'multiline':False]['text':' If none of the unflattened parameters comprising this flat parameter have','line_number':598,'multiline':False]['text':' any state, then we do not want an entry in the optimizer state dict','line_number':599,'multiline':False]['text':' no need to flatten any state','line_number':601,'multiline':False]['text':' There may still be some unflattened parameters with state and some','line_number':602,'multiline':False]['text':' without','line_number':603,'multiline':False]['text':' Check that the unflattened parameters have the same state names','line_number':614,'multiline':False]['text':' Flatten the state','line_number':629,'multiline':False]['text':' If all ranks have None, this is a None value','line_number':637,'multiline':False]['text':' Shard the flattened tensor immediately to minimize max memory','line_number':663,'multiline':False]['text':' usage','line_number':664,'multiline':False]['text':' Check that all are tensors with the same dtype','line_number':734,'multiline':False]['text':' Check that each tensor state matches its parameter's shape','line_number':744,'multiline':False]['text':' Flatten the tensor states: we do not need to add any right-hand-side','line_number':753,'multiline':False]['text':' padding since the flat optimizer state tensor is sharded via','line_number':754,'multiline':False]['text':' `_get_shard()`, which pads the shard as needed (just like for the flat','line_number':755,'multiline':False]['text':' parameter)','line_number':756,'multiline':False]['text':' type: ignore[attr-defined]','line_number':771,'multiline':False]['text':' Enforce that all have the same value and dtype','line_number':813,'multiline':False]['text':' Enforce that all have the same value (same type already checked)','line_number':859,'multiline':False]['text':' All parameter keys in `param_to_param_key` should be in','line_number':903,'multiline':False]['text':' `param_to_fqns` -- strict inequality follows when not all parameters are','line_number':904,'multiline':False]['text':' passed to the optimizer','line_number':905,'multiline':False]['text':' for "state"','line_number':910,'multiline':False]['text':' for "param_groups"','line_number':913,'multiline':False]['text':' This parameter was not passed to the optimizer','line_number':916,'multiline':False]['text':' Only process param_groups if it exists in sharded_osd','line_number':934,'multiline':False]['text':' Assume the standard case of passing `model.parameters()` to the optimizer','line_number':989,'multiline':False]['text':' if `optim_input` is not specified','line_number':990,'multiline':False]['text':' Check if the optimizer input represents tensors or parameter groups','line_number':1003,'multiline':False]['text':' type: ignore[operator]','line_number':1016,'multiline':False]['text':' type: ignore[index]','line_number':1021,'multiline':False]['text':' Implicitly map `flat_param_id` (current length of the list) to','line_number':1022,'multiline':False]['text':' `param`','line_number':1023,'multiline':False]['text':' FlatParameter case','line_number':1094,'multiline':False]['text':' use_orig_params case','line_number':1098,'multiline':False]['text':' Ensure that all ranks have at least the optimizer states needed by','line_number':1154,'multiline':False]['text':' rank 0's optimizer','line_number':1155,'multiline':False]['text':' A parameter from rank 0's optimizer does not exist for this','line_number':1159,'multiline':False]['text':' rank's optimizer','line_number':1160,'multiline':False]['text':' We cannot use FSDPState.compute_device as this API is a global view.','line_number':1168,'multiline':False]['text':' local','line_number':1205,'multiline':False]['text':' Do not include parameters without state to avoid empty mappings','line_number':1209,'multiline':False]['text':' just like in normal `torch.optim.Optimizer.state_dict()`','line_number':1210,'multiline':False]['text':' flatten the list of lists','line_number':1274,'multiline':False]['text':' If we cannot find a state, assume it is not NamedOptimizer as','line_number':1288,'multiline':False]['text':' NamedOptimizer has eager initialization.','line_number':1289,'multiline':False]['text':' The key of these dictionaries are the state name, e.g., `exp_avg`.','line_number':1300,'multiline':False]['text':' Allgather the scalar tensor state, non-tensor states and tensors metadata.','line_number':1321,'multiline':False]['text':' Ensure that `step` is on CPU.','line_number':1326,'multiline':False]['text':' First check all the non-scalar states and get the information of','line_number':1365,'multiline':False]['text':' states on each rank.','line_number':1366,'multiline':False]['text':' N.B. We need to move the state to compute_device. The reason is','line_number':1389,'multiline':False]['text':' not yet clear and we need to figure out why the state may be on a','line_number':1390,'multiline':False]['text':' different device.','line_number':1391,'multiline':False]['text':' Restoring the scalar and non-tensor states. If the corresponding','line_number':1396,'multiline':False]['text':' non-scalar states do not exist on the rank, we also skip the scalar','line_number':1397,'multiline':False]['text':' non-tensor states on that rank.','line_number':1398,'multiline':False]['text':' TODO: This solution is not general and only apply to PTD TP solution.','line_number':1449,'multiline':False]['text':' If gathered state is a DTensor and its TP placement is not Replicate(), we need to','line_number':1452,'multiline':False]['text':' gather the tensor on its TP dimension before chunking them into DTensor again.','line_number':1453,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1455,'multiline':False]['text':' If gathered state is a replicate DTensor, we directly reshape it.','line_number':1461,'multiline':False]['text':' If gathered state is a tensor, we directly reshape it into unflatten state.','line_number':1465,'multiline':False]['text':' Loop through the ``state_buffers`` and construct the flattened, concatenated,','line_number':1531,'multiline':False]['text':' sharded states. The size of the constructed state will be the same size as','line_number':1532,'multiline':False]['text':' flat_param (also sharded).','line_number':1533,'multiline':False]['text':' Then we perform an allgather_into_tensor to get the full flat_param state.','line_number':1534,'multiline':False]['text':' The full flat_param state is the result of concatenation of multiple states','line_number':1535,'multiline':False]['text':' the order of of flat_param._fqns.','line_number':1536,'multiline':False]['text':' The final step is to split the flat_param state into original param states','line_number':1537,'multiline':False]['text':' and return the result.','line_number':1538,'multiline':False]['text':' Synchronize can be slow but this will be easier for us to debug.','line_number':1544,'multiline':False]['text':' End is inclusive.','line_number':1549,'multiline':False]['text':' param_idx corresponds to the parameter index in the FlatParameter.','line_number':1551,'multiline':False]['text':' This memory range is a padding or the param is frozen and does','line_number':1562,'multiline':False]['text':' not require gradient. For the later case, we treat it as a','line_number':1563,'multiline':False]['text':' padding and add empty values to the local_buffers.','line_number':1564,'multiline':False]['text':' The range is an align padding before the first parameter in','line_number':1568,'multiline':False]['text':' the shard. The shard includes parts of this align padding.','line_number':1569,'multiline':False]['text':' The range is an align padding after the last parameter in','line_number':1576,'multiline':False]['text':' the shard. The shard includes parts of this align padding.','line_number':1577,'multiline':False]['text':' The range is an align padding that is completely in the','line_number':1584,'multiline':False]['text':' shard.','line_number':1585,'multiline':False]['text':' This memory range is a parameter in FlatParameter. So there','line_number':1593,'multiline':False]['text':' should be an corresponding state in the optimizer unless the','line_number':1594,'multiline':False]['text':' parameter is frozen, which we treat it as a padding above.','line_number':1595,'multiline':False]['text':' We need to check if this rank owns the buffer. If this is None:','line_number':1597,'multiline':False]['text':' 1.) the rank does not own any part of the original parameter.','line_number':1598,'multiline':False]['text':'     As a result, there is no corresponding optimizer state on','line_number':1599,'multiline':False]['text':'     the rank as well.','line_number':1600,'multiline':False]['text':' 2.) the parameter is frozen AND no optimizer state for the','line_number':1601,'multiline':False]['text':'     parameter. If a parameter is frozen, there can still be','line_number':1602,'multiline':False]['text':'     optimizer state if the parameter is not frozen in the','line_number':1603,'multiline':False]['text':'     previous steps.','line_number':1604,'multiline':False]['text':' Add right-handed padding.','line_number':1624,'multiline':False]['text':' Synchronize can be slow but this will be easier for us to debug.','line_number':1638,'multiline':False]['text':' This variable is used to deduplicate the FSDPParamInfo as one FSDPParamInfo','line_number':1726,'multiline':False]['text':' usually corresponds to multiple parameters. We could not use FSDPParamInfo','line_number':1727,'multiline':False]['text':' as the key because FSDPParamInfo is not hashable. As a result, we fall back','line_number':1728,'multiline':False]['text':' to `id(FSDPParamInfo)`, which the type is an integer.','line_number':1729,'multiline':False]['text':' Iterate in rank 0's flat parameter ID order to ensure aligned all-gathers','line_number':1731,'multiline':False]['text':' across ranks','line_number':1732,'multiline':False]['text':' This can happen if the not all FSDP instances have all the','line_number':1745,'multiline':False]['text':' parameters. This can happen with FSDP + some MPMD style','line_number':1746,'multiline':False]['text':' parallelism.','line_number':1747,'multiline':False]['text':' TODO: it is unclear if we need to do the same check with','line_number':1749,'multiline':False]['text':' non-FSDP managed keys.','line_number':1750,'multiline':False]['text':' Instead of gathering the state of each parameter individually, we perform','line_number':1773,'multiline':False]['text':' the gathering  all at once to speed up the process.','line_number':1774,'multiline':False]['text':' Iterate in rank 0's flat parameter ID order to ensure aligned all-gathers','line_number':1817,'multiline':False]['text':' across ranks','line_number':1818,'multiline':False]['text':' If there are multiple unflat_param_names (not use_orig_params),','line_number':1830,'multiline':False]['text':' they share the same FSDPParamInfo. So the first unflat_param_name','line_number':1831,'multiline':False]['text':' is sufficient to fetch the FSDPParamInfo.','line_number':1832,'multiline':False]['text':' At this point, communication is complete and ranks can return early if nothing','line_number':1982,'multiline':False]['text':' will be saved on that rank.','line_number':1983,'multiline':False]['text':' This key is not recognized by FSDP. It may be a user-defined state','line_number':1997,'multiline':False]['text':' or some parameters state that FSDP is unable to map from','line_number':1998,'multiline':False]['text':' ``optim.param_groups``.','line_number':1999,'multiline':False]['text':' NOTE: `idx` indexes into the data structures *without* padding','line_number':2041,'multiline':False]['text':' elements','line_number':2042,'multiline':False]['text':' FlatParameter._fqns stores the local fqn, starting from the root of the','line_number':2058,'multiline':False]['text':' FSDP. Using _apply_to_modules() with model (may not be the FSDP root','line_number':2059,'multiline':False]['text':' module) allows us to construct the global fqn.','line_number':2060,'multiline':False]['text':' If device_mesh is passed in when initalizing FSDP, we automatically turn the','line_number':2075,'multiline':False]['text':' _use_dtensor flag to be true for ShardedOptimStateDictConfig() if state_dict_type','line_number':2076,'multiline':False]['text':' has to be set to SHARDED_STATE_DICT.','line_number':2077,'multiline':False]['text':' noqa: G004','line_number':2088,'multiline':False]