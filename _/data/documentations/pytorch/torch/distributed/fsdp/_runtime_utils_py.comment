['text':' Do not include "process_group" to enable hybrid shard and MoE cases','line_number':43,'multiline':False]['text':' NOTE: This function assumes that `module.modules()` proceeds top-down.','line_number':74,'multiline':False]['text':' Force a lazy initialization to determine the FSDP root','line_number':101,'multiline':False]['text':' mypy','line_number':103,'multiline':False]['text':' TODO: Change this to handle's sharding strategy if we deprecate','line_number':118,'multiline':False]['text':' `ShardingStrategy` internally.','line_number':119,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/90857','line_number':120,'multiline':False]['text':' No instances use a hybrid sharding strategy','line_number':125,'multiline':False]['text':' no-op: already lazily initialized','line_number':155,'multiline':False]['text':' Allow the FSDP constructor to run even without CUDA but check this','line_number':157,'multiline':False]['text':' once we start real execution','line_number':158,'multiline':False]['text':' The following logic is only run on the root FSDP instance since it will','line_number':160,'multiline':False]['text':' set `_is_root=False` for the non-root instances','line_number':161,'multiline':False]['text':' share reference','line_number':215,'multiline':False]['text':' Update _has_optim_in_backward for each handle.','line_number':216,'multiline':False]['text':' Relax the assert for non-root FSDP instances in case the nested','line_number':237,'multiline':False]['text':' initialized module is wrapped again in FSDP later (e.g. after','line_number':238,'multiline':False]['text':' training to run inference)','line_number':239,'multiline':False]['text':' Prioritize all-gathers/reduce-scatters over async all-reduce for HSDP and','line_number':277,'multiline':False]['text':' preserve the default priority of 0 otherwise','line_number':278,'multiline':False]['text':' Default stream for computation','line_number':280,'multiline':False]['text':' Stream for unshard logic, including allocating the all-gather destination','line_number':282,'multiline':False]['text':' tensors and the all-gathers themselves','line_number':283,'multiline':False]['text':' Stream for overlapping gradient reduction with the backward pass gradient','line_number':285,'multiline':False]['text':' computation','line_number':286,'multiline':False]['text':' Stream for pre-unshard logic, namely allocations and writes for CPU','line_number':288,'multiline':False]['text':' offloading (H2D copy) and mixed precision (low precision cast)','line_number':289,'multiline':False]['text':' Stream to run HSDP's all-reduce as async (if using HSDP)','line_number':291,'multiline':False]['text':' We don't run a even queue for freeing under torch compile atm','line_number':343,'multiline':False]['text':' But maybe we need to? TODO(voz): Look into this','line_number':344,'multiline':False]['text':' Flat parameter freed or not, we always have to "unshard" the parameter','line_number':349,'multiline':False]['text':' upon next access to get its shape correct.','line_number':350,'multiline':False]['text':' For `fully_shard` + `checkpoint`, skip pre-forward logic in the','line_number':394,'multiline':False]['text':' recomputed forward','line_number':395,'multiline':False]['text':' For both checkpoint implementations, we do not need to re-cast','line_number':397,'multiline':False]['text':' inputs here since they will be checkpointed in the low precision','line_number':398,'multiline':False]['text':' either by AC or normally by autograd as long as the AC region is','line_number':399,'multiline':False]['text':' nested within FSDP','line_number':400,'multiline':False]['text':' Register post-backward hooks to reshard the parameters and reduce-scatter','line_number':408,'multiline':False]['text':' their gradients. They must be re-registered every forward pass in case','line_number':409,'multiline':False]['text':' the `grad_fn` is mutated.','line_number':410,'multiline':False]['text':' We have to reallocate the _cpu_grad if optimizer overlap','line_number':412,'multiline':False]['text':' set the grad to None in the backward pass.','line_number':413,'multiline':False]['text':' Recursively convert args and kwargs to specified precision.','line_number':424,'multiline':False]['text':' If the handles have been prefetched, then there is no need to call','line_number':439,'multiline':False]['text':' `_unshard()` again','line_number':440,'multiline':False]['text':' For `fully_shard` + `checkpoint`, skip post-forward logic in the','line_number':482,'multiline':False]['text':' recomputed forward','line_number':483,'multiline':False]['text':' Register pre-backward hooks to unshard the flat parameters for the','line_number':490,'multiline':False]['text':' gradient computation (if needed)','line_number':491,'multiline':False]['text':' Do not free the root's parameters in the post-forward for `FULL_SHARD`','line_number':507,'multiline':False]['text':' with the intention that they are immediately used for backward','line_number':508,'multiline':False]['text':' computation (though this may not be true)','line_number':509,'multiline':False]['text':' Always cast forward inputs in the root of this local FSDP unit for mixed','line_number':538,'multiline':False]['text':' precision, as this is where mixed precision could be configed.','line_number':539,'multiline':False]['text':' This is more useful for auto wrapping that is recommended in composable path.','line_number':540,'multiline':False]['text':' For manual wrapping, cast forward inputs on each local FSDP unit root will','line_number':541,'multiline':False]['text':' increase some overhead, so not turned on for model wrapper path right now where','line_number':542,'multiline':False]['text':' manual wrapping is more broadly used.','line_number':543,'multiline':False]['text':' We cast buffers back to full precision if we're forcing full precision. Disjointly, we check if buffers','line_number':548,'multiline':False]['text':' are in full precision and if we should cast them back to lower precision, which happens when','line_number':549,'multiline':False]['text':' exiting eval() mode.','line_number':550,'multiline':False]['text':' This flag is only set when we cast buffers to full precision, to avoid the','line_number':563,'multiline':False]['text':' CPU overhead that can stem from retrieving all buffers and their types in the','line_number':564,'multiline':False]['text':' following else branch.','line_number':565,'multiline':False]['text':' Check if buffers are in full precision and we need to cast them','line_number':568,'multiline':False]['text':' back down.','line_number':569,'multiline':False]['text':' Assume we have to cast everything if there is one mismatch','line_number':581,'multiline':False]['text':' We don't have to check this again until we cast buffers to full precision again.','line_number':585,'multiline':False]['text':' Prepares the forward inputs by moving them to ``compute_device``','line_number':603,'multiline':False]['text':' TODO: Do not use the side stream for tensor copies for now; investigate','line_number':604,'multiline':False]['text':' the perf with/without it.','line_number':605,'multiline':False]['text':' Only run the pre-backward hook once per group of handles involved in the','line_number':650,'multiline':False]['text':' same module forward computation','line_number':651,'multiline':False]['text':' Queue the post-backward callback once for the root FSDP instance to','line_number':656,'multiline':False]['text':' attach it to the outermost backward graph task so that it is called','line_number':657,'multiline':False]['text':' after all backward calls complete','line_number':658,'multiline':False]['text':' Queueing the post-backward callback is the only logic that is not','line_number':668,'multiline':False]['text':' per-handle in the pre-backward hook, so we can return early here if','line_number':669,'multiline':False]['text':' there are no handles.','line_number':670,'multiline':False]['text':' If the handles have been prefetched, then there is no need to','line_number':676,'multiline':False]['text':' call `_unshard()` again','line_number':677,'multiline':False]['text':' Set this to `False` to ensure that a mistargeted prefetch does not','line_number':687,'multiline':False]['text':' actually unshard these handles','line_number':688,'multiline':False]['text':' For multiple applications of reentrant AC across submodules sharing','line_number':724,'multiline':False]['text':' the same `FlatParameter`, the post-backward hook may run multiple','line_number':725,'multiline':False]['text':' times in one backward, in which case we permit the state to already','line_number':726,'multiline':False]['text':' be in `BACKWARD_POST`.','line_number':727,'multiline':False]['text':' Wait for all ops in the current stream (e.g. gradient computation) to','line_number':746,'multiline':False]['text':' finish before reduce-scattering the gradient','line_number':747,'multiline':False]['text':' If we are forcing full precision but communicating grads','line_number':755,'multiline':False]['text':' (i.e. model.eval() + full precision in eval was configured), don't downcast gradient.','line_number':756,'multiline':False]['text':' Since the unsharded gradient is produced in the computation','line_number':764,'multiline':False]['text':' stream and consumed in the post-backward stream, inform the','line_number':765,'multiline':False]['text':' caching allocator (before it goes out of scope)','line_number':766,'multiline':False]['text':' TODO: Post-backward prefetching does not support the multiple handles','line_number':780,'multiline':False]['text':' per module case since the post-backward hook runs per handle, not per','line_number':781,'multiline':False]['text':' group of handles.','line_number':782,'multiline':False]['text':' If not syncing gradients, then we do not free for strategies that do not','line_number':800,'multiline':False]['text':' reshard after forward as a *heuristic* to tradeoff higher memory for','line_number':801,'multiline':False]['text':' higher throughput.','line_number':802,'multiline':False]['text':' We clear `.grad` to permit multiple backwards. This avoids a race where','line_number':820,'multiline':False]['text':' the second backward pass computation precedes ahead of the first backward','line_number':821,'multiline':False]['text':' pass reduction, which is possible since the reduction is issued in a','line_number':822,'multiline':False]['text':' separate stream and is async and would result in reducing the wrong','line_number':823,'multiline':False]['text':' gradient.','line_number':824,'multiline':False]['text':' default path','line_number':830,'multiline':False]['text':' Since the new sharded gradient is produced in the post-','line_number':845,'multiline':False]['text':' backward stream and consumed in the all-reduce stream,','line_number':846,'multiline':False]['text':' inform the caching allocator','line_number':847,'multiline':False]['text':' NOTE: HSDP variants do not support communication hook.','line_number':861,'multiline':False]['text':' padded','line_number':878,'multiline':False]['text':' Save the sharded gradient in `_saved_grad_shard` to support gradient','line_number':895,'multiline':False]['text':' accumulation -- for multiple backwards, the gradient reductions may','line_number':896,'multiline':False]['text':' happen in arbitrary order','line_number':897,'multiline':False]['text':' default path','line_number':915,'multiline':False]['text':' For `NO_SHARD`, we can keep the low precision gradients by simply','line_number':921,'multiline':False]['text':' omitting the cast altogether','line_number':922,'multiline':False]['text':' Additional arguments needed for the callback logic','line_number':933,'multiline':False]['text':' Offload the gradient to CPU to ensure parameters and gradients are on the','line_number':953,'multiline':False]['text':' same device as required by the optimizer','line_number':954,'multiline':False]['text':' TODO: Investigate why `NO_SHARD` breaks correctness when using','line_number':955,'multiline':False]['text':' `non_blocking=True` here.','line_number':956,'multiline':False]['text':' TODO (rohan-varma): When CPU offload and optimizer overlap,','line_number':957,'multiline':False]['text':' non_blocking=True won't work since the copy may have not finished before','line_number':958,'multiline':False]['text':' the optimizer step executes on CPU. If we want to use non-blocking=True','line_number':959,'multiline':False]['text':' here, we'll have to synchronize before using result on CPU.','line_number':960,'multiline':False]['text':' synchronized in the post-backward callback','line_number':964,'multiline':False]['text':' Since the gradient being offloaded may have been produced in the','line_number':965,'multiline':False]['text':' computation stream and is being consumed here in the post-backward','line_number':966,'multiline':False]['text':' stream, inform the caching allocator','line_number':967,'multiline':False]['text':' Since the handle's `FlatParameter` completed its gradient computation, we','line_number':975,'multiline':False]['text':' should reset the gradient noneness mask','line_number':976,'multiline':False]['text':' Delay using sharded gradient views until after the reduce-scatter instead','line_number':978,'multiline':False]['text':' of immediately after resharding','line_number':979,'multiline':False]['text':' Check for `None` gradient to filter parameters not in the rank','line_number':984,'multiline':False]['text':' TODO (rohan-varma): For CPU offload, this unfortunately','line_number':988,'multiline':False]['text':' operates on CPU because the parameters and gradients have','line_number':989,'multiline':False]['text':' already been offloaded. We should run this on GPU after','line_number':990,'multiline':False]['text':' refactoring.','line_number':991,'multiline':False]['text':' Since for `NO_SHARD`, the gradient is produced in the computation','line_number':1026,'multiline':False]['text':' stream and consumed here in the post-backward stream, inform the','line_number':1027,'multiline':False]['text':' caching allocator; for the sharded strategies, the gradient is','line_number':1028,'multiline':False]['text':' produced in the post-backward stream, so this `record_stream()`','line_number':1029,'multiline':False]['text':' should be a no-op','line_number':1030,'multiline':False]['text':' TODO (rohan-varma): this also waits for the overlapped optimizer step to finish','line_number':1078,'multiline':False]['text':' since it currently runs in the post-backward stream. That can be','line_number':1079,'multiline':False]['text':' pushed to the next forward if run in a different stream','line_number':1080,'multiline':False]['text':' uses HSDP','line_number':1082,'multiline':False]['text':' Wait for non-blocking GPU -> CPU sharded gradient copies from the','line_number':1085,'multiline':False]['text':' post-backward hooks to finish explicitly since CPU gradients do','line_number':1086,'multiline':False]['text':' not automatically synchronize with the GPU','line_number':1087,'multiline':False]['text':' Reset for cases like one forward and multiple backwards','line_number':1102,'multiline':False]['text':' Wrap with a try-except to provide a more informative traceback if an','line_number':1117,'multiline':False]['text':' error is raised','line_number':1118,'multiline':False]['text':' TODO: This already-resharded check is brittle:','line_number':1121,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/83956','line_number':1122,'multiline':False]['text':' If FSDP skipped using sharded views, then the flat parameter','line_number':1126,'multiline':False]['text':' still points to the sharded data, so we need to reshard to','line_number':1127,'multiline':False]['text':' use sharded views','line_number':1128,'multiline':False]['text':' Preserve the gradient accumulation state if not synchronizing','line_number':1170,'multiline':False]['text':' gradients: `.grad` remains the unsharded gradient  from prior','line_number':1171,'multiline':False]['text':' `no_sync()` iterations, and `_saved_grad_shard` remains the','line_number':1172,'multiline':False]['text':' sharded gradient from the last synchronized iteration','line_number':1173,'multiline':False]['text':' Temporarily emulate the training state while calling `_unshard` to','line_number':1199,'multiline':False]['text':' ensure the correct `as_params` for `_use_unsharded_views()`','line_number':1200,'multiline':False]['text':' Prefetch the next set of handles without synchronizing to allow','line_number':1208,'multiline':False]['text':' the sync to happen as late as possible to maximize overlap','line_number':1209,'multiline':False]['text':' If there is no gradient computation, then there is no need for','line_number':1366,'multiline':False]['text':' pre-backward logic','line_number':1367,'multiline':False]['text':' only defined on the root','line_number':1371,'multiline':False]['text':' Since these handles' `FlatParameter`s participated in a forward, we','line_number':1375,'multiline':False]['text':' conservatively assume that they will be used in the backward','line_number':1376,'multiline':False]['text':' If there is no gradient computation, then there is no need for','line_number':1413,'multiline':False]['text':' post-backward logic','line_number':1414,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1427,'multiline':False]['text':' Get the `AccumulateGrad` object','line_number':1432,'multiline':False]['text':' type: ignore[union-attr]','line_number':1439,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1444,'multiline':False]['text':' If there is no gradient computation, then there is no need for','line_number':1459,'multiline':False]['text':' post-backward logic','line_number':1460,'multiline':False]['text':' Construct `inp_tensors` lazily to avoid CPU overhead in typical case','line_number':1463,'multiline':False]['text':' where each flat parameter requires gradient','line_number':1464,'multiline':False]['text':' mypy','line_number':1482,'multiline':False]['text':' type: ignore[attr-defined, assignment]','line_number':1487,'multiline':False]['text':' type: ignore[attr-defined, assignment]','line_number':1489,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1524,'multiline':False]['text':' Having the pre-all-gather stream wait for the current stream even if we','line_number':1525,'multiline':False]['text':' do not leverage the pre-all-gather stream is tolerable since this only','line_number':1526,'multiline':False]['text':' runs once per iteration','line_number':1527,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1528,'multiline':False]['text':' Traverse the FSDP states bottom-up so that we prefer the owning FSDP','line_number':1561,'multiline':False]['text':' instance's mixed precision setting for each buffer','line_number':1562,'multiline':False]