['text':' `CheckpointWrapper` adds a prefix that has to be removed as well.','line_number':64,'multiline':False]['text':' TODO: need to check if this is always correct for composable FSDP.','line_number':136,'multiline':False]['text':' For composable `fully_shard`, it does not need to unshard parameters for `NO_SHARD` cases.','line_number':152,'multiline':False]['text':' Return early for trivial cases','line_number':181,'multiline':False]['text':' If a rank does not have unsharded parameters(when `rank0_only=True`','line_number':190,'multiline':False]['text':' and `rank != 0`), then the rank only needed to participate in the','line_number':191,'multiline':False]['text':' all-gather and does not need to save the # state dict. We simply check','line_number':192,'multiline':False]['text':' rank0_only to ensure this issue.','line_number':193,'multiline':False]['text':' no_fsdp_return means the state_dict returned by this rank should contain','line_number':198,'multiline':False]['text':' only non-FSDP controlled parameters and buffers.','line_number':199,'multiline':False]['text':' This is a hack to support activation checkpoint.','line_number':203,'multiline':False]['text':' Non-zero ranks have flat_param key when rank0_only=True, because rank0_only=True is','line_number':208,'multiline':False]['text':' passed in to unshard context, but nonzero ranks reshard early, causing this flat_param','line_number':209,'multiline':False]['text':' to appear in state_dict.','line_number':210,'multiline':False]['text':' Loop only the parameters saved in this instance's wrapped module to','line_number':215,'multiline':False]['text':' avoid processing buffers.','line_number':216,'multiline':False]['text':' This is a hack to support activation checkpoint.','line_number':241,'multiline':False]['text':' A buffer can be registered as non-persistent.','line_number':245,'multiline':False]['text':' skip upcasting for ignored buffers','line_number':256,'multiline':False]['text':' Strip prefix out of key if needed as buffer names and param names','line_number':331,'multiline':False]['text':' do not have prefix considered as they are not computed in `state_dict`','line_number':332,'multiline':False]['text':' call.','line_number':333,'multiline':False]['text':' Clone parameters before exiting the `_unshard_fsdp_state_params()` context.','line_number':337,'multiline':False]['text':' type: ignore[attr-defined]','line_number':341,'multiline':False]['text':' Add FSDP_PREFIX only for wrapper-based FSDP.','line_number':369,'multiline':False]['text':' state_dict[f"{prefix}{FLAT_PARAM}"] exists and has the same tensor','line_number':424,'multiline':False]['text':' value as the flat_param but it is a pure Tensor because','line_number':425,'multiline':False]['text':' nn.Module.state_dict() will detach the parameter. Therefore, we need','line_number':426,'multiline':False]['text':' to get flat_param to get the metadata.','line_number':427,'multiline':False]['text':' Constructs a ShardedTensor from the flat_param "without" padding.','line_number':430,'multiline':False]['text':' Removing the padding allows users to change the number of ranks','line_number':431,'multiline':False]['text':' when loading the local_state_dict.','line_number':432,'multiline':False]['text':' type: ignore[attr-defined]','line_number':433,'multiline':False]['text':' If FlatParameter is returned, FlatParameter._local_shard cause a','line_number':437,'multiline':False]['text':' pickling issue (can be torch.save but not torch.load). Since there','line_number':438,'multiline':False]['text':' is no benefit for state_dict to return the actual FlatParameter class,','line_number':439,'multiline':False]['text':' a view (which is a tensor) of the FlatParameter will be returned.','line_number':440,'multiline':False]['text':' type: ignore[assignment]','line_number':449,'multiline':False]['text':' TODO: Add DTensor state_dict support for LOCAL_STATE_DICT.','line_number':450,'multiline':False]['text':' Convert the ShardedTensor to a Tensor.','line_number':488,'multiline':False]['text':' Get the metadata of the flat_param to decide whether to pad the loaded','line_number':497,'multiline':False]['text':' tensor.','line_number':498,'multiline':False]['text':' TODO: Add DTensor state_dict support for LOCAL_STATE_DICT.','line_number':507,'multiline':False]['text':' Setting offload_to_cpu here does not work even if offload_to_cpu is True.','line_number':530,'multiline':False]['text':' We have to create ShardedTensor first then move it to CPU.','line_number':531,'multiline':False]['text':' noqa: G004','line_number':624,'multiline':False]['text':' TODO: Improve unittesting for state_dict finetuning','line_number':627,'multiline':False]['text':' cases: https://github.com/pytorch/pytorch/issues/109134','line_number':628,'multiline':False]['text':' All-gather the param (ShardedTensor)','line_number':631,'multiline':False]['text':' If device_mesh is passed in when initalizing FSDP, we automatically turn the','line_number':797,'multiline':False]['text':' _use_dtensor flag to be true for ShardedStateDictConfig().','line_number':798,'multiline':False]['text':' noqa: G004','line_number':809,'multiline':False]['text':' Code that is common for all state_dict impls','line_number':850,'multiline':False]['text':' Dispatch into state_dict specific implementation of pre-hook.','line_number':853,'multiline':False]['text':' Code that is common for all state_dict impls','line_number':881,'multiline':False]['text':' Dispatch into state_dict type specific implementation of post-hook for','line_number':882,'multiline':False]['text':' loading state_dict.','line_number':883,'multiline':False]