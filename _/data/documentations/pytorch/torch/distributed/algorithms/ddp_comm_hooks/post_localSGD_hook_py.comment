['text':' The group used for all-reducing gradients globally.','line_number':42,'multiline':False]['text':' The group used for all-reducing gradients locally.','line_number':44,'multiline':False]['text':' Allreduce gradients locally since iteration `start_localSGD_iter`.','line_number':47,'multiline':False]['text':' This may help with the convergence efficiency at the cost of relatively cheap intra-subgroup communication.','line_number':48,'multiline':False]['text':' Iteration/step in the training loop.','line_number':50,'multiline':False]['text':' Since bucket 0 is the last bucket to allreduce in an iteration.','line_number':54,'multiline':False]['text':' Only increase `iter` when bucket 0 is processed.','line_number':55,'multiline':False]['text':' The input tensor is a flattened 1D tensor.','line_number':96,'multiline':False]['text':' Run allreduce using `global_group_to_use` in the first `start_localSGD_iter` iterations.','line_number':99,'multiline':False]['text':' If `post_local_gradient_allreduce` is not set,','line_number':104,'multiline':False]['text':' then no gradient synchronization after the first `start_localSGD_iter` iterations.','line_number':105,'multiline':False]['text':' Run allreduce using `subgroup` after the first `start_localSGD_iter` iterations.','line_number':111,'multiline':False]['text':' Note that by default, a separate subgroup for each node is created which','line_number':112,'multiline':False]['text':' causes an intra-node allreduce to be done at each training step.','line_number':113,'multiline':False]['text':' From this moment, model averaging should run after the optimizer step,','line_number':114,'multiline':False]['text':' to globally allreduce all the parameters.','line_number':115,'multiline':False]