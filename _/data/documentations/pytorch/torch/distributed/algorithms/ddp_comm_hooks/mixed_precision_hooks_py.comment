['text':' Cast bucket if different than param_dtype.','line_number':32,'multiline':False]['text':' Cast bucket tensor to reduce_dtype','line_number':36,'multiline':False]['text':' Upcast parameters and gradients so optimizer step can run in fp32.','line_number':46,'multiline':False]['text':' free storage for mp param as it will be allocated again in next','line_number':50,'multiline':False]['text':' forward pass.','line_number':51,'multiline':False]['text':' enqueue a callback to wait for this stream at end of backward','line_number':55,'multiline':False]['text':' Remove post-backward hooks since they are re-installed in next','line_number':58,'multiline':False]['text':' iteration, similar to FSDP.','line_number':59,'multiline':False]['text':' Parameters that don't require grad still needed to be casted since','line_number':60,'multiline':False]['text':' they may participate in computation. However, they would not be recast','line_number':61,'multiline':False]['text':' by hook above as they don't have a grad hook installed, so cast them','line_number':62,'multiline':False]['text':' back here.','line_number':63,'multiline':False]['text':' reset for next backward pass','line_number':71,'multiline':False]['text':' mark that the callback is enqueued','line_number':78,'multiline':False]