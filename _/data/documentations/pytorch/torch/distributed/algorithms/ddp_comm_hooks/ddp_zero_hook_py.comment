['text':' Functional optimizers require passing a list of gradients to their `step()`','line_number':14,'multiline':False]['text':' method, and ZeRO requires a functional optimizer to overlap with DDP','line_number':15,'multiline':False]['text':' Passing a `None` instead of an actual gradient indicates to the optimizer','line_number':16,'multiline':False]['text':' to not update the corresponding parameter','line_number':17,'multiline':False]['text':' Construct the `gradients` input for the local optimizer step, which','line_number':44,'multiline':False]['text':' expects `None` in a list position to indicate that the corresponding','line_number':45,'multiline':False]['text':' parameter should not be updated','line_number':46,'multiline':False]['text':' Sort to ensure the same ordering across ranks','line_number':79,'multiline':False]['text':' Save the parameters in the bucket','line_number':116,'multiline':False]['text':' Additionally save the bucket size for the assignment heuristic to use','line_number':119,'multiline':False]['text':' Proceed as normal until the DDP buckets have been rebuilt','line_number':146,'multiline':False]['text':' type: ignore[union-attr]','line_number':147,'multiline':False]['text':' This corresponds to the first bucket of the backward pass','line_number':158,'multiline':False]['text':' immediately after all information has been saved, so we','line_number':159,'multiline':False]['text':' can perform the delayed ZeRO initialization','line_number':160,'multiline':False]['text':' Once DDP buckets have been rebuilt but ZeRO has not been','line_number':163,'multiline':False]['text':' properly initialized yet, save the information needed','line_number':164,'multiline':False]['text':' NOTE: Gloo may hang with this overlapping approach, so we require','line_number':229,'multiline':False]['text':' NCCL/HCCL backend for now; see https://github.com/pytorch/pytorch/issues/62300','line_number':230,'multiline':False]['text':' type: ignore[union-attr]','line_number':231,'multiline':False]['text':' Save the bucket reference and all-reduce future for the final bucket','line_number':274,'multiline':False]['text':' Check that buckets are indexed incrementally starting from 0 in the','line_number':279,'multiline':False]['text':' order of their autograd hooks firing','line_number':280,'multiline':False]['text':' Directly return the future without any optimizer computation if this','line_number':288,'multiline':False]['text':' is not the last bucket','line_number':289,'multiline':False]['text':' Perform partial optimizer step on all buckets after the final','line_number':295,'multiline':False]['text':' bucket has been computed','line_number':296,'multiline':False]['text':' NOTE: This should not be chained as a callback to the last bucket's','line_number':297,'multiline':False]['text':' all-reduce future since that would add synchronization that delays','line_number':298,'multiline':False]['text':' all optimizer computation to wait for that last all-reduce','line_number':299,'multiline':False]['text':' Wait on the bucket's all-reduce future to ensure correct','line_number':303,'multiline':False]['text':' gradients','line_number':304,'multiline':False]['text':' Perform the partial optimizer step','line_number':311,'multiline':False]['text':' Ensure that all parameter updates are finished before the','line_number':317,'multiline':False]['text':' next forward pass','line_number':318,'multiline':False]['text':' NOTE: Gloo may hang with this overlapping approach, so we require','line_number':389,'multiline':False]['text':' NCCL/HCCL backend for now; see https://github.com/pytorch/pytorch/issues/62300','line_number':390,'multiline':False]['text':' type: ignore[union-attr]','line_number':391,'multiline':False]['text':' Ensure that all parameter updates are finished before the','line_number':443,'multiline':False]['text':' next forward pass','line_number':444,'multiline':False]