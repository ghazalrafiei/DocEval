['text':' value that the SymInt evaluates to','line_number':57,'multiline':False]['text':' vaue that this SymInt evaluates to on the local shard','line_number':58,'multiline':False]['text':' device mesh of the DTensor where this SymInt is retrieved from','line_number':59,'multiline':False]['text':' type: ignore[index]','line_number':136,'multiline':False]['text':' type: ignore[operator]','line_number':137,'multiline':False]['text':' TODO: this is broken because it won't redistributed potential tensors on the kwargs','line_number':141,'multiline':False]['text':' Figure out how to specify a type spec for the return specs value','line_number':145,'multiline':False]['text':' without the entire structure.','line_number':146,'multiline':False]['text':' pyre-fixme','line_number':147,'multiline':False]['text':' Code adapted from pack_args_kwargs_with_local_tensor','line_number':149,'multiline':False]['text':' When no tensor redistribution is required, we only need to update non-tensor args','line_number':177,'multiline':False]['text':' of the node according to op_schema and avoid building a GraphModule just for the','line_number':178,'multiline':False]['text':' node.','line_number':179,'multiline':False]['text':' This is a shared arg, already has a tracer from previous','line_number':208,'multiline':False]['text':' tracing. Delete the tracer.','line_number':209,'multiline':False]['text':' extract sharded dimensions in the size list, the output DTensor should','line_number':243,'multiline':False]['text':' follow these placements.','line_number':244,'multiline':False]['text':' set node args to real int sizes.','line_number':247,'multiline':False]['text':' Dispatch override for view and factory ops that consume SymInt arguments,','line_number':336,'multiline':False]['text':' where the output spec should follow dimension placement where the SymInt comes','line_number':337,'multiline':False]['text':' from.','line_number':338,'multiline':False]['text':' Dispatch override for factory ops, as DTensor cannot propogate sharding spec','line_number':353,'multiline':False]['text':' without DTensor inputs.','line_number':354,'multiline':False]['text':' Args should be a list of objects post remapping.','line_number':370,'multiline':False]['text':' HACK: this is a hack to get around with the fact that some','line_number':401,'multiline':False]['text':' view operations on a "global" tensor is invalid usage','line_number':402,'multiline':False]['text':' but somehow the view operation on the batch input might hit it','line_number':403,'multiline':False]['text':' so we convert the view op to reshape before calling DTensor','line_number':404,'multiline':False]['text':' DSymInt args are not sharded on any dimension, local value and global','line_number':407,'multiline':False]['text':' value should be the same','line_number':408,'multiline':False]['text':' Don't pass factory ops to DTensor dispatch, as DTensor cannot','line_number':415,'multiline':False]['text':' propagate sharding spec without DTensor inputs.','line_number':416,'multiline':False]['text':' FIXME(@wanchaol, @mrshenli): the above seems to accidentally captured','line_number':430,'multiline':False]['text':' DeviceMesh tensor ops when handling inplace operators? The ``_to_copy`` is','line_number':431,'multiline':False]['text':' not connected to graph output. So, using DCE to get rid of it, but this','line_number':432,'multiline':False]['text':' doesn't look correct.','line_number':433,'multiline':False]['text':'','line_number':434,'multiline':False]['text':' The following operators appear in the captured graph, where the dtype is','line_number':435,'multiline':False]['text':' torch.int64.','line_number':436,'multiline':False]['text':'','line_number':437,'multiline':False]['text':' get_attr       _tensor_constant0  _tensor_constant0         ()','line_number':438,'multiline':False]['text':' call_function  transpose          aten.transpose.int        (_tensor_constant0, -1, 0)','line_number':439,'multiline':False]['text':' call_function  view               aten.view.default         (transpose, [-1, 2])','line_number':440,'multiline':False]['text':' call_function  view_1             aten.view.default         (view, [2])','line_number':441,'multiline':False]['text':' call_function  _to_copy           aten._to_copy.default     (view_1,)','line_number':442,'multiline':False]['text':' TODO(anj): This depends on the call function node -> actual DTensor output','line_number':480,'multiline':False]['text':' mapping that we want to avoid for SPMD expansion','line_number':481,'multiline':False]['text':' type: ignore[union-attr]','line_number':492,'multiline':False]['text':' we know it's a dtensor from is partial DT check...','line_number':505,'multiline':False]['text':' remove add node and replace it with wait node','line_number':518,'multiline':False]['text':' also update the actual DTensor corresponding to the node','line_number':521,'multiline':False]['text':' TODO(anj): We require mapping of the final DTensor output to the wait','line_number':522,'multiline':False]['text':' comm node.','line_number':523,'multiline':False]['text':' do nothing, ignore placeholders, as it has','line_number':529,'multiline':False]['text':' already been prepared in value_remap','line_number':530,'multiline':False]['text':' the concrete DTensor value of output was added when creating the','line_number':537,'multiline':False]['text':' inner graph (in _build_dummy_add_graph). Just add it to the final','line_number':538,'multiline':False]['text':' output node so that we can report the final output specs correctly.','line_number':539,'multiline':False]['text':' TODO(anj): We are depending on the concrete DTensor output of the dummy add.','line_number':540,'multiline':False]['text':' replace nodes in local traced graph with DTensor's dispatch graph','line_number':563,'multiline':False]['text':' Map DT's dispatch graph input placeholder nodes to the ones in','line_number':569,'multiline':False]['text':' local traced graph. It uses index-based accessing, which is','line_number':570,'multiline':False]['text':' brittle, just for testing purpose.','line_number':571,'multiline':False]['text':' insert DT's dispatch graph to traced local graph.','line_number':579,'multiline':False]['text':' do nothing, ignore placeholders, as it has already','line_number':583,'multiline':False]['text':' been prepared in value_remap','line_number':584,'multiline':False]['text':' we currently support two very specific types of output','line_number':591,'multiline':False]['text':' 1. single output','line_number':592,'multiline':False]['text':' 2. multiple outputs resulting from getitem of all elements of tuple','line_number':593,'multiline':False]['text':' for single output, we replace the node with the single node','line_number':595,'multiline':False]['text':' for multiple outputs, we check that these outputs correspond','line_number':598,'multiline':False]['text':' to all elements of a tuple. In that case, we replace','line_number':599,'multiline':False]['text':' uses of the output directly with the original tuple','line_number':600,'multiline':False]['text':' we allow None outputs for certain items in the tuple','line_number':603,'multiline':False]['text':' FIXME(@mrshenli): This is a temporary solution enable','line_number':626,'multiline':False]['text':' foreach ops. The problem is that foreach ops returns','line_number':627,'multiline':False]['text':' List[Tensor], but make_fx will flatten that before','line_number':628,'multiline':False]['text':' passing those tensors to output node, which will','line_number':629,'multiline':False]['text':' introduce additional getitem nodes. These redundant','line_number':630,'multiline':False]['text':' getitem nodes breaks graph correctness as we cannot do','line_number':631,'multiline':False]['text':' getitem(getitem(foreach_out, 0), 0). This temporary','line_number':632,'multiline':False]['text':' solution skips getitem nodes in DTensor expanded','line_number':633,'multiline':False]['text':' subgraphs.','line_number':634,'multiline':False]['text':' explicitly erase node instead of relying on DCE, as DCE does not','line_number':637,'multiline':False]['text':' remove inplace copy_ correctly.','line_number':638,'multiline':False]['text':' Run through reverse nodes and record the first instance of a use','line_number':648,'multiline':False]['text':' of a given node. This represents the *last* use of the node in the','line_number':649,'multiline':False]['text':' execution order of the program, which we will use to free unused','line_number':650,'multiline':False]['text':' values','line_number':651,'multiline':False]['text':' map local op node in traced_f to its corresponding subgraph of','line_number':690,'multiline':False]['text':' DTensor ops.','line_number':691,'multiline':False]['text':' our example inputs are local shards. Create DTensors from them.','line_number':705,'multiline':False]['text':' use clone to avoid modifications from inplace ops','line_number':707,'multiline':False]['text':' prevent running this collective in backwards pass','line_number':710,'multiline':False]['text':' Returns an expanded dummy add node that ensures','line_number':724,'multiline':False]['text':' that the partial output tensor has been converted','line_number':725,'multiline':False]['text':' to a replicated tensor.','line_number':726,'multiline':False]['text':' Save output sharding for the inputs to backward pass.','line_number':729,'multiline':False]['text':' TODO(anj): Pipe the output schema for the BW pass','line_number':730,'multiline':False]['text':' instead of requiring the full output DTensor to be','line_number':731,'multiline':False]['text':' materialized.','line_number':732,'multiline':False]['text':' type: ignore[arg-type]','line_number':738,'multiline':False]['text':' Save memory by deleting objs that wont be used anymore.','line_number':777,'multiline':False]