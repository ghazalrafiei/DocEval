['text':' NOTE [CustomOp autograd kernel indirection]','line_number':7,'multiline':False]['text':' We register `inner` as the autograd kernel for this custom_op.','line_number':8,'multiline':False]['text':' `inner` either calls the autograd formula registered by the user,','line_number':9,'multiline':False]['text':' or goes into an `autograd_not_implemented` kernel.','line_number':10,'multiline':False]['text':'','line_number':11,'multiline':False]['text':' The reason why this indirection exists is','line_number':12,'multiline':False]['text':' so that we can swap out the autograd kernel (the PyTorch dispatcher','line_number':13,'multiline':False]['text':' doesn't actually allow us to do this). By default, we want','line_number':14,'multiline':False]['text':' the `autograd_not_implemented` behavior, but then the user may come','line_number':15,'multiline':False]['text':' and register something that is actually a backward formula','line_number':16,'multiline':False]['text':' As explained in NOTE ["backward", "save_for_backward", and "autograd"],','line_number':24,'multiline':False]['text':' after the user gives us "backward" and "save_for_backward", we generate','line_number':25,'multiline':False]['text':' the "autograd" impl. If the user only provided one, then we tell','line_number':26,'multiline':False]['text':' the user they've done something wrong.','line_number':27,'multiline':False]['text':' TODO(#101191): Use the actual C++ autograd not implemented fallback,','line_number':46,'multiline':False]['text':' or change the default autograd fallback to the autograd not implemented fallback.','line_number':47,'multiline':False]['text':' Output types are restricted to be:','line_number':60,'multiline':False]['text':' - Tensor','line_number':61,'multiline':False]['text':' - Tensor[]','line_number':62,'multiline':False]['text':' - int, bool, Scalar, float','line_number':63,'multiline':False]['text':' See _check_can_register_backward','line_number':64,'multiline':False]['text':' type: ignore[assignment]','line_number':69,'multiline':False]['text':' We use the info about args to give better error messages in backward','line_number':109,'multiline':False]['text':' There is nothing on the ctx object for now, it is just there so','line_number':127,'multiline':False]['text':' that we can add additional things in the future.','line_number':128,'multiline':False]['text':' Massage the grad_inputs_dict to a form acceptable by','line_number':134,'multiline':False]['text':' autograd.Function.','line_number':135,'multiline':False]['text':' mypy doesn't support dynamic namedtuple name','line_number':164,'multiline':False]['text':' type: ignore[misc]','line_number':165,'multiline':False]['text':' Saves "stuff" (a pytree) onto the ctx object. Use unpack_saved to unpack it.','line_number':244,'multiline':False]['text':' autograd.Function prefers that users use ctx.save_for_backward to','line_number':245,'multiline':False]['text':' save Tensors (to avoid reference cycles) and for non-Tensors to go onto the','line_number':246,'multiline':False]['text':' ctx object.','line_number':247,'multiline':False]['text':' Inverse operation to save_pytree_for_backward','line_number':266,'multiline':False]