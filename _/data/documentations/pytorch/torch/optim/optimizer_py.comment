['text':' Note on graph break below:','line_number':62,'multiline':False]['text':' we need to graph break to ensure that aot respects the no_grad annotation.','line_number':63,'multiline':False]['text':' This is important for perf because without this, functionalization will generate an epilogue','line_number':64,'multiline':False]['text':' which updates the mutated parameters of the optimizer which is *not* visible to inductor, as a result,','line_number':65,'multiline':False]['text':' inductor will allocate for every parameter in the model, which is horrible.','line_number':66,'multiline':False]['text':' With this, aot correctly sees that this is an inference graph, and functionalization will generate','line_number':67,'multiline':False]['text':' an epilogue which is appended to the graph, which *is* visible to inductor, as a result, inductor sees that','line_number':68,'multiline':False]['text':' step is in place and is able to avoid the extra allocation.','line_number':69,'multiline':False]['text':' In the future, we will either 1) continue to graph break on backward, so this graph break does not matter','line_number':70,'multiline':False]['text':' or 2) have a fully fused forward and backward graph, which will have no_grad by default, and we can remove this','line_number':71,'multiline':False]['text':' graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.','line_number':72,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/104053','line_number':73,'multiline':False]['text':' item is significantly faster than a cpu tensor in eager mode','line_number':85,'multiline':False]['text':' float annotation is needed because of torchscript type inference','line_number':97,'multiline':False]['text':' For any optimizer with a faster implementation, we attempt to default to the','line_number':103,'multiline':False]['text':' fastest + stablest whenever possible. For foreach, the requirements are to have','line_number':104,'multiline':False]['text':' native params all on CUDA. For fused, there's currently the additional requirement','line_number':105,'multiline':False]['text':' that the tensors' dtypes must be floating point. Neither alternative supports','line_number':106,'multiline':False]['text':' torch.jit.script nor differentiable, so we fall back to the single tensor','line_number':107,'multiline':False]['text':' implementation in those cases.','line_number':108,'multiline':False]['text':' Common doc strings among optimizers','line_number':135,'multiline':False]['text':' type: ignore[misc]','line_number':235,'multiline':False]['text':' type: ignore[misc]','line_number':236,'multiline':False]['text':' Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,','line_number':280,'multiline':False]['text':' which I don't think exists','line_number':281,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/72948','line_number':282,'multiline':False]['text':' To support multiprocessing pickle/unpickle','line_number':306,'multiline':False]['text':' Currently needed by Adam and AdamW','line_number':320,'multiline':False]['text':' Note [torch.compile x capturable]','line_number':322,'multiline':False]['text':' If we are compiling, we try to take the capturable path automatically by','line_number':323,'multiline':False]['text':' setting the flag to True during tracing. Due to this, we skip all the checks','line_number':324,'multiline':False]['text':' normally required for determining whether we can use CUDA graphs and','line_number':325,'multiline':False]['text':' shunt the responsibility to torch.inductor. This saves time during tracing','line_number':326,'multiline':False]['text':' since the checks are slow without sacrificing UX since inductor will warn','line_number':327,'multiline':False]['text':' later if CUDA graphs cannot be enabled, e.g.,','line_number':328,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/d3ba8901d8640eb16f88b2bfef9df7fa383d4b47/torch/_inductor/compile_fx.py#L390.','line_number':329,'multiline':False]['text':' Thus, when compiling, inductor will determine if cudagraphs','line_number':330,'multiline':False]['text':' can be enabled based on whether there is input mutation or CPU tensors.','line_number':331,'multiline':False]['text':' call optimizer step pre hooks','line_number':374,'multiline':False]['text':' type: ignore[assignment]','line_number':379,'multiline':False]['text':' call optimizer step post hooks','line_number':388,'multiline':False]['text':' type: ignore[assignment]','line_number':415,'multiline':False]['text':' type: ignore[attr-defined]','line_number':416,'multiline':False]['text':' Save order indices instead of Tensors','line_number':583,'multiline':False]['text':' Remap state to use order indices as keys','line_number':596,'multiline':False]['text':' Floating-point types are a bit special here. They are the only ones','line_number':619,'multiline':False]['text':' that are assumed to always match the type of params.','line_number':620,'multiline':False]['text':' Make sure state['step'] is not casted https://github.com/pytorch/pytorch/issues/74424','line_number':621,'multiline':False]['text':' UNLESS fused or capturable, see note [special device hosting for step]','line_number':622,'multiline':False]['text':' type: ignore[attr-defined]','line_number':715,'multiline':False]['text':' shallow copy, to be consistent with module API','line_number':727,'multiline':False]['text':' Validate the state_dict','line_number':735,'multiline':False]['text':' Deepcopy as we write into saved_groups later to update state','line_number':738,'multiline':False]['text':' Update the state','line_number':750,'multiline':False]['text':' type: ignore[call-arg]','line_number':761,'multiline':False]['text':' Copy state assigned to params (and cast tensors to appropriate types).','line_number':765,'multiline':False]['text':' State that is not assigned to params is copied as is (needed for','line_number':766,'multiline':False]['text':' backward compatibility).','line_number':767,'multiline':False]['text':' Update parameter groups, setting their 'params' value','line_number':776,'multiline':False]