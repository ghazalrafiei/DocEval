['text':' TODO(crcrpar): [low prec params & their higher prec copy]','line_number':58,'multiline':False]['text':' Suppor AMP with FP16/BF16 model params which would need','line_number':59,'multiline':False]['text':' higher prec copy of params to do update math in higher prec to','line_number':60,'multiline':False]['text':' alleviate the loss of information.','line_number':61,'multiline':False]['text':' State initialization','line_number':113,'multiline':False]['text':' note(crcrpar): Deliberately host `step` on CPU if both capturable and fused are off.','line_number':115,'multiline':False]['text':' This is because kernel launches are costly on CUDA and XLA.','line_number':116,'multiline':False]['text':' Exponential moving average of gradient values','line_number':122,'multiline':False]['text':' Exponential moving average of squared gradient values','line_number':126,'multiline':False]['text':' Maintains max of all exp. moving avg. of sq. grad. values','line_number':131,'multiline':False]['text':' Foreach without capturable does not support a tensor lr','line_number':144,'multiline':False]['text':' kwonly args with defaults are not supported by functions compiled with torchscript issue #70627','line_number':286,'multiline':False]['text':' setting this as kwarg for now as functional API is compiled by torch/distributed/optim','line_number':287,'multiline':False]['text':' Respect when the user inputs False/True for foreach or fused. We only want to change','line_number':313,'multiline':False]['text':' the default when neither have been user-specified. Note that we default to foreach','line_number':314,'multiline':False]['text':' and pass False to use_fused. This is not a mistake--we want to give the fused impl','line_number':315,'multiline':False]['text':' bake-in time before making it the default, even if it is typically faster.','line_number':316,'multiline':False]['text':' Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.','line_number':319,'multiline':False]['text':' this assert is due to JIT being dumb and not realizing that the ops below','line_number':386,'multiline':False]['text':' have overloads to handle both float and Tensor lrs, so we just assert it's','line_number':387,'multiline':False]['text':' a float since most people using JIT are using floats','line_number':388,'multiline':False]['text':' If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]','line_number':397,'multiline':False]['text':' update step','line_number':411,'multiline':False]['text':' Perform stepweight decay','line_number':414,'multiline':False]['text':' Decay the first and second moment running average coefficient','line_number':417,'multiline':False]['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':433,'multiline':False]['text':' Uses the max. for normalizing running avg. of gradient','line_number':441,'multiline':False]['text':' Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write','line_number':442,'multiline':False]['text':' (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)','line_number':443,'multiline':False]['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':464,'multiline':False]['text':' Use the max. for normalizing running avg. of gradient','line_number':467,'multiline':False]['text':' Lastly, switch back to complex view','line_number':474,'multiline':False]['text':' If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]','line_number':506,'multiline':False]['text':' Update steps','line_number':535,'multiline':False]['text':' If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over','line_number':536,'multiline':False]['text':' and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just','line_number':537,'multiline':False]['text':' wrapped it once now. The alpha is required to assure we go to the right overload.','line_number':538,'multiline':False]['text':' Perform stepweight decay','line_number':544,'multiline':False]['text':' Decay the first and second moment running average coefficient','line_number':548,'multiline':False]['text':' Delete the local intermediate since it won't be used anymore to save on peak memory','line_number':554,'multiline':False]['text':' foreach_sub doesn't allow a scalar as the first arg','line_number':560,'multiline':False]['text':' we do not negate bias_correction1 as it'll need to be negated later anyway','line_number':563,'multiline':False]['text':' foreach_div doesn't allow a scalar as the first arg','line_number':566,'multiline':False]['text':' Re-assign for clarity as we maintain minimal intermediates: we'll have','line_number':572,'multiline':False]['text':' step_size = - lr / (1 - beta1 ^ t) where t = num_steps','line_number':573,'multiline':False]['text':' bias_correction2_sqrt = sqrt(1 - beta2 ^ t)','line_number':574,'multiline':False]['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':579,'multiline':False]['text':' Use the max. for normalizing running avg. of gradient','line_number':582,'multiline':False]['text':' at this point, exp_avg_sq_sqrt = - (1 - beta^t) * [sqrt(exp_avg_sq / (1 - beta2^t)) + eps] / lr','line_number':591,'multiline':False]['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':602,'multiline':False]['text':' Use the max. for normalizing running avg. of gradient','line_number':605,'multiline':False]['text':' Needed for consistency.','line_number':632,'multiline':False]['text':' We only shuffle around the lr when it is a Tensor and on CUDA, otherwise, we prefer','line_number':644,'multiline':False]['text':' treating it as a scalar.','line_number':645,'multiline':False]