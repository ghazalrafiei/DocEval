['text':' State initialization','line_number':90,'multiline':False]['text':' kwonly args with defaults are not supported by functions compiled with torchscript issue #70627','line_number':181,'multiline':False]['text':' setting this as kwarg for now as functional API is compiled by torch/distributed/optim','line_number':182,'multiline':False]['text':' update step','line_number':262,'multiline':False]['text':' decay term','line_number':270,'multiline':False]['text':' update parameter','line_number':273,'multiline':False]['text':' averaging','line_number':276,'multiline':False]['text':' Update steps','line_number':322,'multiline':False]['text':' If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over','line_number':323,'multiline':False]['text':' and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just','line_number':324,'multiline':False]['text':' wrapped it once now. The alpha is required to assure we go to the right overload.','line_number':325,'multiline':False]['text':' intermediate = grad + param * lambd','line_number':331,'multiline':False]['text':' update param','line_number':343,'multiline':False]['text':' param * (1 - lambd * eta) - eta * grad','line_number':344,'multiline':False]['text':' => param - param * lambd * eta - eta * grad','line_number':345,'multiline':False]['text':' => param - eta * intermediate','line_number':346,'multiline':False]['text':' update grouped_axs','line_number':350,'multiline':False]['text':' averaging: ax = ax + mu * (param - ax)','line_number':351,'multiline':False]['text':' Note (mlazos): We can't use lerp here since it requires weight to be float64','line_number':352,'multiline':False]['text':' and our grouping code requires dtypes to match for all tensors in a group (and it should, since','line_number':353,'multiline':False]['text':' we use the mus in other places)','line_number':354,'multiline':False]['text':' all dtypes need to match, so we could introduce a cast in a loop','line_number':355,'multiline':False]['text':' but since this only adds one additional kernel launch, this looks like the cleaner','line_number':356,'multiline':False]['text':' and faster solution','line_number':357,'multiline':False]['text':' update grouped_mus','line_number':363,'multiline':False]['text':' update eta = lr / (1 + lambd * lr * step^alpha)','line_number':370,'multiline':False]