['text':' list[list[T]] -> list[T]','line_number':10,'multiline':False]['text':' NB: lstm.all_weights format:','line_number':228,'multiline':False]['text':' wih, whh, bih, bhh = lstm.all_weights[layer]','line_number':229,'multiline':False]['text':' XXX: It's more efficient to store the output in its padded form,','line_number':251,'multiline':False]['text':' but that might not be conducive to loss computation.','line_number':252,'multiline':False]['text':' Un-padding the output also makes the backward pass 2x slower...','line_number':253,'multiline':False]['text':' return [padded[:lengths[i], i, :] for i in range(lengths.size(0))]','line_number':254,'multiline':False]['text':' List of: (output, hx, cx)','line_number':278,'multiline':False]['text':' cudnn_layernorm_lstm: since cudnn does not have Layernorm LSTM, we cannot benchmark','line_number':318,'multiline':False]['text':' the lowerbound directly. Instead, we only benchmark the forward pass by mimicing the','line_number':319,'multiline':False]['text':' computation of a cudnn lstm + seq_len * 3 layernorm computation. This should serve','line_number':320,'multiline':False]['text':' as a perf lowerbound for the Layernorm LSTM forward pass(given that Layernorm itself','line_number':321,'multiline':False]['text':' is invariant), the lowerbound of backward pass is hard to get since we lose the','line_number':322,'multiline':False]['text':' intermediate results, we can still optimize the layernorm implementation to make','line_number':323,'multiline':False]['text':' a faster forward lowerbound though.','line_number':324,'multiline':False]['text':' plus (seq_len * three laynorm cell computation) to mimic the lower bound of','line_number':336,'multiline':False]['text':' Layernorm cudnn LSTM in the forward pass','line_number':337,'multiline':False]['text':' input: lstm.all_weights format (wih, whh, bih, bhh = lstm.all_weights[layer])','line_number':356,'multiline':False]['text':' output: packed_weights with format','line_number':357,'multiline':False]['text':' packed_weights[0] is wih with size (layer, 4*hiddenSize, inputSize)','line_number':358,'multiline':False]['text':' packed_weights[1] is whh with size (layer, 4*hiddenSize, hiddenSize)','line_number':359,'multiline':False]['text':' packed_weights[2] is bih with size (layer, 4*hiddenSize)','line_number':360,'multiline':False]['text':' packed_weights[3] is bhh with size (layer, 4*hiddenSize)','line_number':361,'multiline':False]['text':' XXX: script fns have problems indexing multidim lists, so we try to','line_number':370,'multiline':False]['text':' avoid them by stacking tensors','line_number':371,'multiline':False]['text':' returns: x, (hx, cx), all_weights, lstm module with all_weights as params','line_number':377,'multiline':False]['text':' NB: lstm.all_weights format:','line_number':401,'multiline':False]['text':' wih, whh, bih, bhh = lstm.all_weights[layer]','line_number':402,'multiline':False]['text':' premul: we're going to premultiply the inputs & weights','line_number':431,'multiline':False]['text':' premul: we're going to premultiply the inputs & weights, and add bias','line_number':457,'multiline':False]['text':' add bias for all timesteps instead of going step-by-step, results in a single reduction kernel in the backward','line_number':470,'multiline':False]['text':' FIXME matmul(x,y) + bias currently goes through jit AD, and backward formula in AD is not optimized for this','line_number':471,'multiline':False]['text':' case. Workaround with mm and views.','line_number':472,'multiline':False]['text':' simple: flat inputs (no tuples), no list to accumulate outputs','line_number':489,'multiline':False]['text':'         useful mostly for benchmarking older JIT versions','line_number':490,'multiline':False]['text':' for scoping','line_number':493,'multiline':False]['text':' for scoping','line_number':494,'multiline':False]['text':' NB: this assumes that biases are there','line_number':511,'multiline':False]['text':' for scoping...','line_number':513,'multiline':False]