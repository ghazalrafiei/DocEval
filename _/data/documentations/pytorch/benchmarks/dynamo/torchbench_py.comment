['text':'!/usr/bin/env python3','line_number':1,'multiline':False]['text':' We are primarily interested in tf32 datatype','line_number':21,'multiline':False]['text':' avoids some spam','line_number':28,'multiline':False]['text':' Some models have large dataset that doesn't fit in memory. Lower the batch','line_number':49,'multiline':False]['text':' size to test the accuracy.','line_number':50,'multiline':False]['text':' reduced from 16 due to cudagraphs OOM in TorchInductor dashboard','line_number':59,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/101','line_number':79,'multiline':False]['text':' https://github.com/pytorch/torchdynamo/issues/145','line_number':81,'multiline':False]['text':' TIMEOUT, https://github.com/pytorch/pytorch/issues/98467','line_number':83,'multiline':False]['text':' Error: RelaxedUnspecConstraint(L['input_ids'].size()[0]) - inferred constant (4)','line_number':85,'multiline':False]['text':' Error: RelaxedUnspecConstraint(L['input_ids'].size()[0]) - inferred constant (4)','line_number':86,'multiline':False]['text':' takes too long, extreme slowdown (< .001)','line_number':87,'multiline':False]['text':' Failing in eager mode','line_number':89,'multiline':False]['text':' multi gpu not always available in benchmark runners','line_number':91,'multiline':False]['text':' OOMs','line_number':107,'multiline':False]['text':' model is CUDA only','line_number':108,'multiline':False]['text':' timeout','line_number':109,'multiline':False]['text':' timeout','line_number':110,'multiline':False]['text':' model is CUDA only','line_number':111,'multiline':False]['text':' flaky','line_number':112,'multiline':False]['text':' requires FBGEMM, CUDA only','line_number':113,'multiline':False]['text':' works on cuda, accuracy failure on cpu','line_number':115,'multiline':False]['text':' only works on CPU','line_number':120,'multiline':False]['text':' only works on CPU','line_number':121,'multiline':False]['text':' only works on CPU','line_number':122,'multiline':False]['text':' Additional models that are skipped in training','line_number':125,'multiline':False]['text':' not designed for training','line_number':127,'multiline':False]['text':' doesnt fit in memory','line_number':135,'multiline':False]['text':' These models support only train mode. So accuracy checking can't be done in','line_number':140,'multiline':False]['text':' eval mode.','line_number':141,'multiline':False]['text':' Need lower tolerance on GPU. GPU kernels have non deterministic kernels for these models.','line_number':152,'multiline':False]['text':' These models need >1e-3 tolerance','line_number':164,'multiline':False]['text':' Just keeping it here even though its empty, if we need this in future.','line_number':184,'multiline':False]['text':' non-deterministic output / cant check correctness','line_number':187,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/98355','line_number':189,'multiline':False]['text':' These benchmarks took >600s on an i9-11900K CPU','line_number':193,'multiline':False]['text':' 3339s','line_number':195,'multiline':False]['text':' 3062s','line_number':196,'multiline':False]['text':' 930s','line_number':197,'multiline':False]['text':' These benchmarks took >60s on an i9-11900K CPU','line_number':200,'multiline':False]['text':' 137s','line_number':203,'multiline':False]['text':' 116s','line_number':204,'multiline':False]['text':' 242s','line_number':205,'multiline':False]['text':' 221s','line_number':206,'multiline':False]['text':' 400s','line_number':207,'multiline':False]['text':' 334s','line_number':208,'multiline':False]['text':' 187s','line_number':209,'multiline':False]['text':' 470s','line_number':210,'multiline':False]['text':' 141s','line_number':211,'multiline':False]['text':' 317s','line_number':212,'multiline':False]['text':' 99s','line_number':213,'multiline':False]['text':' https://github.com/pytorch/benchmark/pull/1656','line_number':232,'multiline':False]['text':' Models too large to have eager, dynamo and fp64_numbers simultaneosuly','line_number':237,'multiline':False]['text':' even for 40 GB machine. We have tested accuracy for smaller version of','line_number':238,'multiline':False]['text':' these models','line_number':239,'multiline':False]['text':' accuracy https://github.com/pytorch/pytorch/issues/93847','line_number':243,'multiline':False]['text':' Models that deterministic algorithms can not be turned on for eager mode.','line_number':250,'multiline':False]['text':' models in canary_models that we should run anyway','line_number':272,'multiline':False]['text':' torchbench removed torchtext dependency','line_number':275,'multiline':False]['text':' Models that should only run in --multiprocess mode','line_number':279,'multiline':False]['text':' Control the memory footprint for few models','line_number':401,'multiline':False]['text':' workaround "RuntimeError: not allowed to set torch.backends.cudnn flags"','line_number':405,'multiline':False]['text':' Output of vision_maskrcnn model is a list of bounding boxes,','line_number':413,'multiline':False]['text':' sorted on the basis of their scores. This makes accuracy','line_number':414,'multiline':False]['text':' comparison hard with torch.compile. torch.compile can cause minor','line_number':415,'multiline':False]['text':' divergences in the output because of how fusion works for amp in','line_number':416,'multiline':False]['text':' TorchInductor compared to eager.  Therefore, instead of looking at','line_number':417,'multiline':False]['text':' all the bounding boxes, we compare only top 4.','line_number':418,'multiline':False]['text':' Models that must be in train mode while training','line_number':443,'multiline':False]['text':' Torchbench has quite different setup for yolov3, so directly passing','line_number':451,'multiline':False]['text':' the right example_inputs','line_number':452,'multiline':False]['text':' See https://github.com/pytorch/benchmark/issues/1561','line_number':455,'multiline':False]['text':' global current_name, current_device','line_number':461,'multiline':False]['text':' current_device = device','line_number':462,'multiline':False]['text':' current_name = benchmark.name','line_number':463,'multiline':False]['text':' work around for: https://github.com/pytorch/xla/issues/4174','line_number':466,'multiline':False]['text':' noqa: F401','line_number':467,'multiline':False]['text':' Increase the tolerance for torch allclose','line_number':507,'multiline':False]