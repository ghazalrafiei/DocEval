['text':'!/usr/bin/env python3','line_number':1,'multiline':False]['text':' This is to woraround the backward issue https://github.com/pytorch/xla/issues/4174','line_number':83,'multiline':False]['text':' ignore the error if torch_xla is not installed','line_number':86,'multiline':False]['text':' We are primarily interested in TF32','line_number':91,'multiline':False]['text':' Suppress torch.profiler spam','line_number':94,'multiline':False]['text':' aot_eager or inductor','line_number':107,'multiline':False]['text':' TIMM','line_number':114,'multiline':False]['text':' accuracy','line_number':115,'multiline':False]['text':' Stack issue in fx','line_number':116,'multiline':False]['text':' HF','line_number':117,'multiline':False]['text':' Stack issue in fx','line_number':118,'multiline':False]['text':' Stack issue in fx','line_number':119,'multiline':False]['text':' Stack issue in fx','line_number':120,'multiline':False]['text':' OOM','line_number':121,'multiline':False]['text':' See https://github.com/mindee/doctr/blob/f2114758d529ed8d3d0030581638f0520b6b98d8/doctr/models/detection/core.py#L89','line_number':126,'multiline':False]['text':' It iterates over the batch, which is dynamic, and dynamo chokes','line_number':127,'multiline':False]['text':' We should be able to graphbreak there.','line_number':128,'multiline':False]['text':' if prior results failed the header might not be filled in yet','line_number':177,'multiline':False]['text':' Specialize for multi tensor sgd as reason is not identical','line_number':227,'multiline':False]['text':' Drop all sgd rows','line_number':232,'multiline':False]['text':' Add back a single row','line_number':235,'multiline':False]['text':' This happens for failed runs','line_number':246,'multiline':False]['text':' Dont collect outputs to correctly measure timing','line_number':313,'multiline':False]['text':' Put this call inside the loop to reset the seed for each iteration.','line_number':315,'multiline':False]['text':' Don't include reset_rng_state() to correctly measure timing','line_number':316,'multiline':False]['text':' instead of calling sync on result_list, we should call mark_step.','line_number':321,'multiline':False]['text':' In training case, result_list may be empty, but we want to','line_number':322,'multiline':False]['text':' send all the pending graphs for compilation.','line_number':323,'multiline':False]['text':' For the model running on regular torchxla (baseline), we need the','line_number':325,'multiline':False]['text':' mark step to send the accumulated graph for compilation.','line_number':326,'multiline':False]['text':'','line_number':327,'multiline':False]['text':' For the model running with dynamo/torchxla bridge, in training case,','line_number':328,'multiline':False]['text':' we need the mark step to send the optimizer graph out for','line_number':329,'multiline':False]['text':' compilation.','line_number':330,'multiline':False]['text':' NOTE(bowbao): For huggingface benchmark, example_inputs are formatted as dictionary,','line_number':345,'multiline':False]['text':' and consumed like `model(**example_inputs)`.','line_number':346,'multiline':False]['text':' For other benchmarks, example_inputs are formatted as tuple and consumed','line_number':347,'multiline':False]['text':' like `model(*example_inputs)`.','line_number':348,'multiline':False]['text':' NOTE(angelayi): For huggingface benchmark, some example outputs are','line_number':356,'multiline':False]['text':' formatted as a dataclass which pytree cannot consume. So we want','line_number':357,'multiline':False]['text':' to register the pytree implementation here','line_number':358,'multiline':False]['text':' Note: we can not simply tune integer tensors as follows','line_number':462,'multiline':False]['text':'   `return torch.randint_like(inputs, high=inputs.max().item())`','line_number':463,'multiline':False]['text':' This may break some invariants between tensors.','line_number':464,'multiline':False]['text':' E.g. in embedding lookup case, one tensor is the length','line_number':465,'multiline':False]['text':' and another is an indices tensor.','line_number':466,'multiline':False]['text':' if args.dynamic_shapes:','line_number':489,'multiline':False]['text':'     return speedup_experiment_ds(args, model_iter_fn, model, example_inputs)','line_number':490,'multiline':False]['text':' if we randomize the input, we should also check the result is correct','line_number':493,'multiline':False]['text':' Use higher tolerance for XLA since XLA cause numerical unstability when','line_number':512,'multiline':False]['text':' graph size changes','line_number':513,'multiline':False]['text':' need call mark_step to perform the computation','line_number':531,'multiline':False]['text':' on randomize_input. Otherwise the first call using the','line_number':532,'multiline':False]['text':' inputs will incur high penalty then the next one.','line_number':533,'multiline':False]['text':' interleave the runs to handle frequency scaling and load changes','line_number':536,'multiline':False]['text':' call mark_step between the 2 calls to make the comparison fair.','line_number':547,'multiline':False]['text':' Start each rep fresh, e.g. only warmup on example 0','line_number':650,'multiline':False]['text':' interleave the runs to handle frequency scaling and load changes','line_number':657,'multiline':False]['text':' different from regular speedup_experiment, we _DO_ want to allow recompilation','line_number':661,'multiline':False]['text':' TODO this x[0] is not going to work in general but bert only has 1 input','line_number':671,'multiline':False]['text':' Goal is to move the iobinding creation outside of the timer function.','line_number':732,'multiline':False]['text':' NOTE: Making perf comparison fair by moving out the i/o adapting part.','line_number':743,'multiline':False]['text':' 1. Pre-adapt `pt_inputs` to `onnx_inputs` here.','line_number':744,'multiline':False]['text':' 2. Drop `onnx_outputs` to `pt_outputs` adapting. Output comparison is not part of perf measurement.','line_number':745,'multiline':False]['text':' Insert ONNX warm-up','line_number':769,'multiline':False]['text':' warmup','line_number':923,'multiline':False]['text':' Register the output dataclass to pytree','line_number':964,'multiline':False]['text':' copy.deepcopy is required to prevent any surprising side-effect,','line_number':967,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/113029','line_number':968,'multiline':False]['text':' noqa: TRY200','line_number':1048,'multiline':False]['text':' NOTE(bowbao): Reduce OOM by running ORT on another gpu.','line_number':1124,'multiline':False]['text':' TODO(bowbao): This works to avoid OOM, but performance is surprisingly very bad.','line_number':1125,'multiline':False]['text':' Error','line_number':1131,'multiline':False]['text':' NOTE: Run ORT on another cuda device to reduce OOM.','line_number':1157,'multiline':False]['text':' 'outputs' are torch empty tensors binded to 'iobinding'.','line_number':1186,'multiline':False]['text':' NOTE: For CUDA performance testing, use `run_with_iobinding` to exclude memory','line_number':1217,'multiline':False]['text':' copying overhead for inputs/outputs between cpu and gpu.','line_number':1218,'multiline':False]['text':' Otherwise perf number is inaccurate.','line_number':1219,'multiline':False]['text':' Hack for huggingface models (kwargs only).','line_number':1255,'multiline':False]['text':' NOTE(bowbao): For huggingface benchmark, pt_inputs are formatted as dictionary,','line_number':1277,'multiline':False]['text':' and consumed like `model(**pt_inputs)`.','line_number':1278,'multiline':False]['text':' For other benchmarks, pt_inputs are formatted as tuple and consumed','line_number':1279,'multiline':False]['text':' like `model(*pt_inputs)`.','line_number':1280,'multiline':False]['text':' Hack for huggingface model outputs','line_number':1293,'multiline':False]['text':' Clear the model proto to save memory.','line_number':1320,'multiline':False]['text':' The model proto is saved to disk and no longer needed from `onnx_program`.','line_number':1321,'multiline':False]['text':' `onnx_program` is kept for i/o adapter usage.','line_number':1322,'multiline':False]['text':' Apply AOT inline post export.','line_number':1359,'multiline':False]['text':' Requires onnx >= 1.15','line_number':1360,'multiline':False]['text':' Workaround for inliner not supporting with models larger than 2GB.','line_number':1364,'multiline':False]['text':' Save model to disk first separating out external data,','line_number':1365,'multiline':False]['text':' and load back without external data for inliner to work on.','line_number':1366,'multiline':False]['text':' Copied from `same` function in `torch._dynamo.utils`','line_number':1409,'multiline':False]['text':' Flatten nested tuple of tensors, i.e. past_key_values','line_number':1425,'multiline':False]['text':' Hack to put results from different runs on same device.','line_number':1427,'multiline':False]['text':' This is needed for ONNX CPU fallback benchmark, where PyTorch eager is run on GPU.','line_number':1428,'multiline':False]['text':' Assuming outputs from a single run are always on same device!','line_number':1429,'multiline':False]['text':' NOTE(bowbao): This function creates and returns the onnx version of 'run_n_iterations',','line_number':1527,'multiline':False]['text':' which does the following:','line_number':1528,'multiline':False]['text':'   1. Export and cache model.','line_number':1529,'multiline':False]['text':'   2. Create iobinding for ORT.','line_number':1530,'multiline':False]['text':'   3. Run ORT for n iterations.','line_number':1531,'multiline':False]['text':' The cached model is stored in 'context' under the returned callable.','line_number':1532,'multiline':False]['text':' NOTE(bowbao): Capture all export & ort errors and diagnostics.','line_number':1540,'multiline':False]['text':' Serialize to csv, to be parsed and summarized later by '._onnx/reporter.py'.','line_number':1541,'multiline':False]['text':' TODO: Accuracy mismatch is not reported here in csv.','line_number':1542,'multiline':False]['text':' Serializes inputs and outputs to .pb files for further offline analysis.','line_number':1562,'multiline':False]['text':' Due to this, this function is not and should not be used for perf measurement.','line_number':1563,'multiline':False]['text':' `torch.onnx.dynamo_export` raises error that encloses diagnostics.','line_number':1570,'multiline':False]['text':' Check also the raw exception that caused export failure.','line_number':1577,'multiline':False]['text':' Skip if it is already analyzed by diagnostics.','line_number':1578,'multiline':False]['text':' `torch.onnx.export` errors.','line_number':1589,'multiline':False]['text':' ORT errors.','line_number':1590,'multiline':False]['text':' cast model and inputs to fp16','line_number':1664,'multiline':False]['text':' TODO: consider deepcopy'ing the entire counters struct and','line_number':1701,'multiline':False]['text':' adding a helper to do subtraction on it','line_number':1702,'multiline':False]['text':' NB: The plus removes zero counts','line_number':1708,'multiline':False]['text':' AMP training can lead to small loss values which can undeflow','line_number':1773,'multiline':False]['text':' gradient values returning in zero gradients. To solve this','line_number':1774,'multiline':False]['text':' problem, PyTorch introduces GradScaler. GradScaler is a stateful','line_number':1775,'multiline':False]['text':' structure, that scales the loss values to prevent underflow. Loss','line_number':1776,'multiline':False]['text':' values are big at the beginning of training (therefore not','line_number':1777,'multiline':False]['text':' requiring scaling), while loss value tends to be small as network','line_number':1778,'multiline':False]['text':' starts getting better (requiring scaling). GradScaler manages all','line_number':1779,'multiline':False]['text':' of this fine tuning, checking the gradients are turning to inf,','line_number':1780,'multiline':False]['text':' discarding such batches.','line_number':1781,'multiline':False]['text':' Since we are not running a long iteration, default value of','line_number':1783,'multiline':False]['text':' init_scale 65536 is going to turn all gradients to inf. Therefore,','line_number':1784,'multiline':False]['text':' we just use a init_scale of 2.0 for benchmarking purpose.','line_number':1785,'multiline':False]['text':' Disabling Gradscaler because','line_number':1787,'multiline':False]['text':'  1) Benchmark setup runs 2 iterations of fwd-bwd. So, not useful.','line_number':1788,'multiline':False]['text':'  2) Current setup shares grad_scaler for eager and dynamo model,','line_number':1789,'multiline':False]['text':'  which is bad as Gradscaler has state and can adjust the scaling','line_number':1790,'multiline':False]['text':'  factor between eager and dynamo run, making accuracy check','line_number':1791,'multiline':False]['text':'  harder.','line_number':1792,'multiline':False]['text':' self.grad_scaler = torch.cuda.amp.GradScaler(init_scale=2.0)','line_number':1793,'multiline':False]['text':' bad benchmark implementation','line_number':1903,'multiline':False]['text':' handcrafted wrap policy','line_number':2023,'multiline':False]['text':' default to using wrap policy based on module size','line_number':2035,'multiline':False]['text':' Gradient communication precision.','line_number':2069,'multiline':False]['text':' Buffer precision.','line_number':2071,'multiline':False]['text':' Collect the fp64 reference outputs to be used later for accuracy checking.','line_number':2132,'multiline':False]['text':' Cast the model to float16/float32 as necessary','line_number':2163,'multiline':False]['text':' Get results of native pytorch','line_number':2167,'multiline':False]['text':' Rerun native pytorch','line_number':2188,'multiline':False]['text':' Two eager runs should have exactly same result','line_number':2209,'multiline':False]['text':' Sometimes torch.allclose may throw RuntimeError','line_number':2225,'multiline':False]['text':' Run with Dynamo','line_number':2234,'multiline':False]['text':' apply export on module directly','line_number':2242,'multiline':False]['text':' no need for n iterations','line_number':2243,'multiline':False]['text':' the logic should be the same to self.model_iter_fn (forward_pass)','line_number':2244,'multiline':False]['text':' Workaround for ONNX for non-tensor outputs','line_number':2274,'multiline':False]['text':' Relax tolerance for ONNX cuda','line_number':2282,'multiline':False]['text':' TODO: store correct_result into the dumped file for offline onnx model validation.','line_number':2286,'multiline':False]['text':' The downside and potential problem, is that the output formats may be different.','line_number':2287,'multiline':False]['text':' E.g., the output order might not match, None might be part of output, etc.','line_number':2288,'multiline':False]['text':' Sometimes torch.allclose may throw RuntimeError','line_number':2301,'multiline':False]['text':' Cast the model to float16/float32 as necessary','line_number':2323,'multiline':False]['text':' Get results of native pytorch','line_number':2327,'multiline':False]['text':' Run with Dynamo','line_number':2338,'multiline':False]['text':' Sometime CI fails with random triton compilation failure which will be skipped for now','line_number':2339,'multiline':False]['text':' TODO: revisit this after switching to new Triton runtime','line_number':2340,'multiline':False]['text':' Cast the model to float16/float32 as necessary','line_number':2418,'multiline':False]['text':' Use distributed wrapping as necessary','line_number':2421,'multiline':False]['text':' run with torch._dynamo few times to populate the cache','line_number':2469,'multiline':False]['text':' should be 0','line_number':2472,'multiline':False]['text':' should be 0','line_number':2475,'multiline':False]['text':' Delimiter because reason could have comma','line_number':2589,'multiline':False]['text':' --nvfuser is now the default, keep the option to not break scripts','line_number':2968,'multiline':False]['text':' We do this here so we error out earlier if there's an issue','line_number':3132,'multiline':False]['text':' NB: Do NOT query device count before CUDA initialization; we're','line_number':3146,'multiline':False]['text':' going to overwrite CUDA_VISIBLE_DEVICES and this will result in','line_number':3147,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/107300','line_number':3148,'multiline':False]['text':' multiprocess path','line_number':3154,'multiline':False]['text':' single process path just uses the main process','line_number':3158,'multiline':False]['text':' Pass the parsed args object to benchmark runner object','line_number':3182,'multiline':False]['text':' Run fewer iterations when checking accuracy','line_number':3202,'multiline':False]['text':' Set translation validation on by default on CI accuracy runs.','line_number':3205,'multiline':False]['text':' TODO: we could also hook DDP bench up to --speedup bench, _not_ for mgpu e2e perf,','line_number':3212,'multiline':False]['text':' but just to measure impact on singlenode of performing graph-breaks.','line_number':3213,'multiline':False]['text':' Left it as a follow up to keep this PR isolated.','line_number':3214,'multiline':False]['text':' TODO(whc) after enabling DDPOptimizer by default this could be removed or assert','line_number':3222,'multiline':False]['text':' Use small batch size. We use >1 batch size to ensure we test','line_number':3230,'multiline':False]['text':' batch_norm type of operators that work on batch dims.','line_number':3231,'multiline':False]['text':' TODO - Go through the failures for batch size = 2','line_number':3232,'multiline':False]['text':' Larger batch size of TIMM models to have stable batch_norm','line_number':3239,'multiline':False]['text':' Remove sources of randomness','line_number':3243,'multiline':False]['text':' TODO - Using train mode for timm_models and HF models. Move to train mode for Torchbench as well.','line_number':3245,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96724','line_number':3255,'multiline':False]['text':' some of the models do not support use_deterministic_algorithms','line_number':3260,'multiline':False]['text':' Remove randomeness when torch manual seed is called','line_number':3268,'multiline':False]['text':' Some models e.g. yolov3 assert batch size on n_gpus','line_number':3271,'multiline':False]['text':' Stricter check to disable fallbacks','line_number':3275,'multiline':False]['text':' Ensure that we test on real scenarios','line_number':3285,'multiline':False]['text':' OOM errors on an RTX 3090 with 24gb RAM','line_number':3307,'multiline':False]['text':' torchbench','line_number':3310,'multiline':False]['text':' AOTInductor doesn't support control flow yet','line_number':3448,'multiline':False]['text':' Overwrite 'translation_validation' config, if specified.','line_number':3512,'multiline':False]['text':' Adding diff-branch again to the args will override previous value','line_number':3523,'multiline':False]['text':' Run for main branch','line_number':3527,'multiline':False]['text':' Run for comparison branch','line_number':3529,'multiline':False]['text':' Go back to main branch','line_number':3533,'multiline':False]['text':' Always load model on cpu for fsdp','line_number':3579,'multiline':False]['text':' When initializing FSDP, we will use the cuda device if args.cuda is set','line_number':3580,'multiline':False]['text':' bad benchmark implementation','line_number':3618,'multiline':False]['text':' Look for stuff that looks like batch size, and mark it dynamic.','line_number':3632,'multiline':False]['text':' Better integration would integrate directly with benchmark suite','line_number':3633,'multiline':False]['text':' but cannot conveniently do this','line_number':3634,'multiline':False]['text':' NB: This must be done late enough so that we don't do more','line_number':3635,'multiline':False]['text':' conversions on the inputs','line_number':3636,'multiline':False]['text':' NB: Assumes only the first batch-y like dimension is the batch','line_number':3637,'multiline':False]['text':' TODO - add option for coalescing inputs over multiple runs','line_number':3726,'multiline':False]