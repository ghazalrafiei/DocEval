['text':' This is a copy of rnn_attention from MLPerf, with some common sizes hardcoded','line_number':1,'multiline':False]['text':' for benchmarking and some control flow stripped out.','line_number':2,'multiline':False]['text':' https://github.com/mlperf/training/blob/master/rnn_translator/pytorch/seq2seq/models/attention.py','line_number':3,'multiline':False]['text':' If matmul is not fused, must write and then read `sum_qk`.','line_number':78,'multiline':False]