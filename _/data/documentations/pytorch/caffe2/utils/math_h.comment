['text':' This is a simple translation from the old Caffe math interfaces. We aim to','line_number':3,'multiline':False]['text':' still keep it simple, so all platforms would be able to support it fairly','line_number':4,'multiline':False]['text':' easily.','line_number':5,'multiline':False]['text':' We include the cblas header here so that we can obtain the macros from cblas.','line_number':7,'multiline':False]['text':' CAFFE2_USE_ACCELERATE','line_number':14,'multiline':False]['text':' TODO: Change dims related arguments to int64_t?','line_number':26,'multiline':False]['text':' An empty class as a placeholder for a math function that has no specific','line_number':29,'multiline':False]['text':' engine specified.','line_number':30,'multiline':False]['text':' Broadcasts X with X_dims to Y with Y_dims.','line_number':119,'multiline':False]['text':' Computes inv_std from variance.','line_number':132,'multiline':False]['text':' Adds batch sub-tensors elementwise to output. Stripe is the stripe length','line_number':141,'multiline':False]['text':' and N is the number of elements to add (size of Y).','line_number':142,'multiline':False]['text':' Compute the row-wise max of a N*D matrix X, and write it to a N','line_number':152,'multiline':False]['text':' dimensional vector y.','line_number':153,'multiline':False]['text':' Compute the column-wise max of a N*D matrix X, and write it to a D','line_number':158,'multiline':False]['text':' dimensional vector y.','line_number':159,'multiline':False]['text':' Elemwise maximum of vector x and scalar alpha. y[i] = max(x[i], alpha)','line_number':164,'multiline':False]['text':' Decaf gemm provides a simpler interface to the gemm functions, with the','line_number':169,'multiline':False]['text':' limitation that the data has to be contiguous in memory.','line_number':170,'multiline':False]['text':' We also provide a gemm that has explicit lda, ldb and ldc specified.','line_number':186,'multiline':False]['text':' In most cases you probably want to use the function above, though.','line_number':187,'multiline':False]['text':' GemmBatched provides a simple abstraction into library routines','line_number':205,'multiline':False]['text':' Gemv always takes in a M*N matrix A, and depending on whether we set TransA','line_number':241,'multiline':False]['text':' to Trans, the output is:','line_number':242,'multiline':False]['text':' CblasNoTrans: x is an N dim vector and y is an M dim vector.','line_number':243,'multiline':False]['text':' CblasTrans:   x is an M dim vector and y is an N dim vector.','line_number':244,'multiline':False]['text':' Generate n values that sum up to a fixed sum','line_number':262,'multiline':False]['text':' and subject to a restriction a <= x <= b for each x generated','line_number':263,'multiline':False]['text':' Generate n values from synthetic data distribution,','line_number':283,'multiline':False]['text':' define by unique accesses and stack distances','line_number':284,'multiline':False]['text':' Dot matrix of vector a and b, and writes the result to a single value y.','line_number':293,'multiline':False]['text':' Sum of vector x, and writes the result to a single value y.','line_number':298,'multiline':False]['text':' Sum of squares of vector x, and writes the result to a single value y.','line_number':307,'multiline':False]['text':' Select does index selection of the rows a N*D matrix x, and gives the N','line_number':316,'multiline':False]['text':' dimensional vector y that contains the selected data.','line_number':317,'multiline':False]['text':' groups must be 1 for GPU','line_number':327,'multiline':False]['text':' For NHWC order with groups > 1, the result will be layout in','line_number':328,'multiline':False]['text':' NHW G RS C/G order to make data within the same group to be contiguous.','line_number':329,'multiline':False]['text':' For NCHW order, groups doesn't make any difference because we're doing Im2Col','line_number':330,'multiline':False]['text':' for each N and C is the slowest moving dimension among CHW.','line_number':331,'multiline':False]['text':' groups must be 1 for GPU','line_number':352,'multiline':False]['text':' groups must be 1 for GPU','line_number':369,'multiline':False]['text':' For NHWC order with groups > 1, the result will be layout in','line_number':370,'multiline':False]['text':' NHW G RS C/G order to make data within the same group to be contiguous.','line_number':371,'multiline':False]['text':' For NCHW order, groups doesn't make any difference because we're doing Im2Col','line_number':372,'multiline':False]['text':' for each N and C is the slowest moving dimension among CHW.','line_number':373,'multiline':False]['text':' groups must be 1 for GPU','line_number':394,'multiline':False]['text':' For NHWC order with groups > 1, the result will be layout in','line_number':395,'multiline':False]['text':' NHW G RS C/G order to make data within the same group to be contiguous.','line_number':396,'multiline':False]['text':' For NCHW order, groups doesn't make any difference because we're doing Im2Col','line_number':397,'multiline':False]['text':' for each N and C is the slowest moving dimension among CHW.','line_number':398,'multiline':False]['text':' Applies a per-channel bias value to each channel of the input','line_number':415,'multiline':False]['text':' image. image_size is H * W','line_number':416,'multiline':False]['text':' namespace math','line_number':463,'multiline':False]['text':' namespace caffe2','line_number':464,'multiline':False]['text':' CAFFE2_UTILS_MATH_H_','line_number':467,'multiline':False]