['text':' Needed to be included first to check the CAFFE2_USE_CUDNN macros.','line_number':12,'multiline':False]['text':' CAFFE2_USE_CUDNN','line_number':18,'multiline':False]['text':' For description of CUB caching allocator configuration, see','line_number':32,'multiline':False]['text':' https://nvlabs.github.io/cub/structcub_1_1_caching_device_allocator.html','line_number':33,'multiline':False]['text':' namespace at','line_number':73,'multiline':False]['text':' Generic implementation - CUDA will handle the right function to call for us','line_number':77,'multiline':False]['text':' TODO: verify that the CUDA handles copy from device to device correctly','line_number':84,'multiline':False]['text':' even without SetDevice()','line_number':85,'multiline':False]['text':' TODO: verify whether source or dest device should be a priority in picking','line_number':86,'multiline':False]['text':' the stream','line_number':87,'multiline':False]['text':' NB: right now the cross-device copy logic is invoked only in the contexts','line_number':88,'multiline':False]['text':' when surrounding code explicitly manages data dependencies and sets up','line_number':89,'multiline':False]['text':' events, so it's fine.  In order to make it a standalone function proper','line_number':90,'multiline':False]['text':' synchronization between stream is required','line_number':91,'multiline':False]['text':' This emulates Caffe2 original behavior where sync copy doesn't change the','line_number':114,'multiline':False]['text':' device. It's probably better for clarity to switch to the target device','line_number':115,'multiline':False]['text':' explicitly here, but in the worst case CUDA would sync for us.','line_number':116,'multiline':False]['text':' TODO: change it to CUDAGuard','line_number':117,'multiline':False]['text':' take current device','line_number':118,'multiline':False]['text':' destructor of context synchronizes','line_number':121,'multiline':False]['text':' For the CPU context, we also allow a (probably expensive) function','line_number':124,'multiline':False]['text':' to copy the data from a cuda context. Inside the function, we create','line_number':125,'multiline':False]['text':' a temporary CUDAContext object to carry out the copy. From the caller's','line_number':126,'multiline':False]['text':' side, these functions are synchronous with respect to the host, similar','line_number':127,'multiline':False]['text':' to a normal CPUContext::CopyBytes<CPUContext, CPUContext> call.','line_number':128,'multiline':False]['text':' namespace caffe2','line_number':146,'multiline':False]['text':' TODO(jiayq): these variables shouldn't be currently accessed during static','line_number':155,'multiline':False]['text':' initialization. We should consider moving them to a Mayer's singleton to','line_number':156,'multiline':False]['text':' be totally safe against SIOF.','line_number':157,'multiline':False]['text':' Static global variables for setting up the memory pool.','line_number':159,'multiline':False]['text':' an unordered map that holds the map from the cuda memory pointer to the','line_number':164,'multiline':False]['text':' device id that it is allocated from. This is used in the cuda memory pool','line_number':165,'multiline':False]['text':' cases, where we need the device id to carry out the deletion.','line_number':166,'multiline':False]['text':' Note(jiayq): an alternate approach is to use cudaGetPointerAttributes, but','line_number':167,'multiline':False]['text':' that is usually quite slow. We might want to benchmark the speed difference','line_number':168,'multiline':False]['text':' though.','line_number':169,'multiline':False]['text':' Note(jiayq): another alternate approach is to augment the Tensor class that','line_number':170,'multiline':False]['text':' would allow one to record the device id. However, this does not address any','line_number':171,'multiline':False]['text':' non-tensor allocation and deallocation.','line_number':172,'multiline':False]['text':' Ideally, a memory pool should already have the device id information, as','line_number':173,'multiline':False]['text':' long as we are using UVA (as of CUDA 5 and later) so the addresses are','line_number':174,'multiline':False]['text':' unique.','line_number':175,'multiline':False]['text':' Data structures for optional memory tracking. Access to these structures','line_number':178,'multiline':False]['text':' is guarded by the CUDAContext::mutex.','line_number':179,'multiline':False]['text':'/////////////////////////////////////////////////////////////////////////////','line_number':191,'multiline':False]['text':' A wrapper to allow us to lazily initialize all cuda environments that Caffe','line_number':192,'multiline':False]['text':' uses. This gets done the first time a caffe2::CUDAContext::New() gets called','line_number':193,'multiline':False]['text':' which is probably the decisive indication that this caffe2 run is going to','line_number':194,'multiline':False]['text':' use GPUs. We avoid cuda initialization with core/init.h functionalities so','line_number':195,'multiline':False]['text':' that we have minimal resource impact in case we will need to run multiple','line_number':196,'multiline':False]['text':' caffe2 instances on a GPU machine.','line_number':197,'multiline':False]['text':'/////////////////////////////////////////////////////////////////////////////','line_number':198,'multiline':False]['text':' If the current run does not have any cuda devices, do nothing.','line_number':201,'multiline':False]['text':' Check if the number of GPUs matches the expected compile-time max number','line_number':207,'multiline':False]['text':' of GPUs.','line_number':208,'multiline':False]['text':' Enable peer access.','line_number':219,'multiline':False]['text':' Note: just for future reference, the 0 here is not a gpu id, it is','line_number':234,'multiline':False]['text':' a reserved flag for cudaDeviceEnablePeerAccess that should always be','line_number':235,'multiline':False]['text':' zero currently.','line_number':236,'multiline':False]['text':' It is ok if peer access is already enabled...','line_number':237,'multiline':False]['text':' reset cuda error code','line_number':243,'multiline':False]['text':' Check the versions of cuDNN that were compiled and linked with are compatible','line_number':249,'multiline':False]['text':' CAFFE2_USE_CUDNN','line_number':251,'multiline':False]['text':' Sets up the cub memory pool','line_number':256,'multiline':False]['text':' Sets up cub.','line_number':279,'multiline':False]['text':' Initialize caching allocator','line_number':284,'multiline':False]['text':'*
 * An allocator that does the CPU memory allocation with pinned memory.
 *
 * This is needed because if we want to do any asynchronous cuda memcpy,
 * the underlying CPU memory also needs to be allocated into pinned memory
 * space. As a result, whenever Caffe2 is built with GPU and there is
 * GPU present during runtime, at global initialization time we will set
 * the CPU memory allocator to allocate pinned memory.
 *
 * NB: This behavior is probably too aggressive. We should consider asking users
 * to do on-demand memory pinning (like exposed in PyTorch APIs) instead.
 ','line_number':292,'multiline':True]['text':' replicate c10::alloc_cpu behavior - return nullptr','line_number':311,'multiline':False]['text':' Caffe2 uses a lazy way to figure out if one is actually going to use GPUs','line_number':344,'multiline':False]['text':' or not. If a CUDAContext::New() call is made, inside the CUDAContext','line_number':345,'multiline':False]['text':' function we will switch the cpu side allocator to a PinnedCPUAllocator.','line_number':346,'multiline':False]['text':' But, if one calls CPUContext::New() before any cuda allocations,','line_number':347,'multiline':False]['text':' PinnedCPUAllocator can still delete the corresponding memory.','line_number':348,'multiline':False]['text':' Calling cudaGetLastError will reset the cuda error.','line_number':358,'multiline':False]['text':' For all other errors, still do a cuda check.','line_number':361,'multiline':False]['text':' An initialization function that sets the CPU side to use pinned cpu','line_number':372,'multiline':False]['text':' allocator.','line_number':373,'multiline':False]['text':' Note(jiayq): for more details, see','line_number':376,'multiline':False]['text':'     https://github.com/google/sanitizers/issues/629','line_number':377,'multiline':False]['text':' If CUDA is enabled, using CPU allocators other than PinnedCPUAllocator','line_number':390,'multiline':False]['text':' will cause memory corruptions. Therefore, we need to set the priority','line_number':391,'multiline':False]['text':' to highest to avoid being overwritten.','line_number':392,'multiline':False]['text':' priority ','line_number':395,'multiline':True]['text':' Caffe2CudaInitializerHelper is a minimal struct whose sole purpose is to','line_number':399,'multiline':False]['text':' detect the first hint that this Caffe2 run is going to use GPU: either','line_number':400,'multiline':False]['text':' CUDAContext is initialized or CUDAContext::New is called. It then runs','line_number':401,'multiline':False]['text':' all the related cuda initialization functions.','line_number':402,'multiline':False]['text':' We cannot use bool because nvcc changes bool to __nv_bool which does','line_number':406,'multiline':False]['text':' not have a std::atomic instantiation.','line_number':407,'multiline':False]['text':' namespace','line_number':416,'multiline':False]['text':'*
 * A utility function to rectify the gpu id. If the context specifies the
 * gpu id to be -1, it means that we will just use the current gpu id when
 * the function is being called.
 ','line_number':418,'multiline':True]['text':' CUDAContext is used in 2 cases now:','line_number':448,'multiline':False]['text':' - long-lived instance inside OperatorBase in which case what happens in','line_number':449,'multiline':False]['text':'   destructor doesn't really matter','line_number':450,'multiline':False]['text':' - short-lived on-the-fly instances that are utilized as CUDAGuard - in','line_number':451,'multiline':False]['text':'   this case there's only one stream id (passed to SwitchToDevice) and','line_number':452,'multiline':False]['text':'   it's preferrable to synchronize in the destructor','line_number':453,'multiline':False]['text':' shared mutex to lock out alloc / free during NCCL launches','line_number':460,'multiline':False]['text':' Lock the mutex','line_number':513,'multiline':False]['text':' A one-time caffe2 cuda initializer.','line_number':515,'multiline':False]['text':' The reason we have this stream guard here is to preserve','line_number':545,'multiline':False]['text':' the historical behavior of the 'thc' allocator in Caffe2,','line_number':546,'multiline':False]['text':' which is to put all allocations on the same (default)','line_number':547,'multiline':False]['text':' stream.  This behavior is morally wrong (since passing','line_number':548,'multiline':False]['text':' allocations between streams allows for the possibility','line_number':549,'multiline':False]['text':' of you handing out some memory that an old stream','line_number':550,'multiline':False]['text':' is still working on), but it doesn't seem to cause issues','line_number':551,'multiline':False]['text':' in Caffe2 today.  Our hypothesis for why this is the case','line_number':552,'multiline':False]['text':' is that Caffe2 doesn't really do very many allocations','line_number':553,'multiline':False]['text':' on the fly; instead they allocate once and then reuse','line_number':554,'multiline':False]['text':' the allocations for the whole program.  In this case,','line_number':555,'multiline':False]['text':' the hazard is avoided.','line_number':556,'multiline':False]['text':'','line_number':557,'multiline':False]['text':' We intend to remove this stream guard, but the benefit','line_number':558,'multiline':False]['text':' to putting all allocations on the same stream is it','line_number':559,'multiline':False]['text':' reduces per-stream fragmentation, and this helps','line_number':560,'multiline':False]['text':' some models that are currently running with the thc','line_number':561,'multiline':False]['text':' allocator fit in memory.  We will need to find some','line_number':562,'multiline':False]['text':' way of resolving this problem.','line_number':563,'multiline':False]['text':' lock the mutex','line_number':586,'multiline':False]['text':' If memory pool is not set up, use simple cudaFree.','line_number':600,'multiline':False]['text':' For some reason, in Python runtime we sometimes delete a data pointer','line_number':602,'multiline':False]['text':' after the cuda runtime exits - this is odd but is probably caused by','line_number':603,'multiline':False]['text':' a static workspace that pycaffe2 uses, and the destruction got','line_number':604,'multiline':False]['text':' entangled in some race condition. Anyway, since cuda runtime is','line_number':605,'multiline':False]['text':' exiting anyway, we will not need to worry about memory leak, so we','line_number':606,'multiline':False]['text':' basically ignore it. This is definitely not ideal but works for now.','line_number':607,'multiline':False]['text':' namespace caffe2','line_number':641,'multiline':False]['text':' namespace at','line_number':661,'multiline':False]