['text':' Copyright 2004-present Facebook. All Rights Reserved.','line_number':1,'multiline':False]['text':' Note [What is CuDNNWrapper good for?]','line_number':9,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':10,'multiline':False]['text':' Suppose you are writing a kernel that calls into CuDNN, and','line_number':11,'multiline':False]['text':' you need a cudnnHandle_t to pass to the kernel call.  How should','line_number':12,'multiline':False]['text':' you go about getting one of those handles?  You'd prefer not','line_number':13,'multiline':False]['text':' to make a new cudnnHandle_t every call; this can be somewhat','line_number':14,'multiline':False]['text':' expensive (1-2%, according to some measurements in TensorFlow.)','line_number':15,'multiline':False]['text':' But cudnnHandle_t is not thread-safe, so we can't just have','line_number':16,'multiline':False]['text':' a single global cudnnHandle_t that everyone uses.','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':' Thus, the most common method in Caffe2 for getting a CuDNN handle','line_number':19,'multiline':False]['text':' is to get a per-thread, per-stream CuDNN handle from CUDAContext','line_number':20,'multiline':False]['text':' (which knows what the current thread and stream are).  The idiomatic','line_number':21,'multiline':False]['text':' way to do this in Caffe2 today is to make a CuDNNWrapper and then call','line_number':22,'multiline':False]['text':' inline_cudnn_handle(), although you didn't really need the','line_number':23,'multiline':False]['text':' CuDNNWrapper at all (you could have gotten it directly from','line_number':24,'multiline':False]['text':' CUDAContext.)','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' So, what's all this business about CuDNNWrapper?  In theory, it was','line_number':27,'multiline':False]['text':' designed with a more specialized use-case in mind, where you need to','line_number':28,'multiline':False]['text':' make multiple calls to CuDNN in parallel; e.g., when manually','line_number':29,'multiline':False]['text':' computing group convolution.  By using with_cudnn_state(), you can','line_number':30,'multiline':False]['text':' get separate cudnnHandle_t and CUDA stream per parallel thread of','line_number':31,'multiline':False]['text':' execution, and run all of the cuDNN calls in parallel.  CuDNNWrapper','line_number':32,'multiline':False]['text':' handles the business of synchronizing with the stream prior to this','line_number':33,'multiline':False]['text':' call.','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' (By the way, this is why no such CUBLASWrapper exists; there isn't','line_number':36,'multiline':False]['text':' ever any reason you need to call cublas in parallel, since most','line_number':37,'multiline':False]['text':' cublas operations have batched variants.)','line_number':38,'multiline':False]['text':'','line_number':39,'multiline':False]['text':' Now, that's the theory... in practice, this is only ever used when','line_number':40,'multiline':False]['text':' multiple operators are run in parallel, and not to actually','line_number':41,'multiline':False]['text':' parallelize multiple CuDNN calls (for example, group convolution is','line_number':42,'multiline':False]['text':' now supported natively in CuDNN.)  So... while the kit provided here','line_number':43,'multiline':False]['text':' might be useful for someone else in the future, it's not really used','line_number':44,'multiline':False]['text':' now.  So we might consider deleting it, or unifying this mechanism','line_number':45,'multiline':False]['text':' with PyTorch's own CuDNN handle pool.  (which is it's own thing.)','line_number':46,'multiline':False]['text':'*
 * CuDNNWorkspace is a wrapper around a raw cuda pointer that holds the cudnn
 * scratch space. This struct is meant to be only used in CuDNNWrapper to
 * provide a program-wide scratch space for CuDNN. The reason behind it is that
 * cudnn function calls are usually very efficient, hence one probably does not
 * want to run multiple cudnn calls at the same time. As a result, one should
 * not need more than one cudnn workspace per device.
 ','line_number':52,'multiline':True]['text':' CuDNNState is the owner of the CuDNNWorkspace, and serializes all','line_number':83,'multiline':False]['text':' executions of operations that use the state onto it's own stream','line_number':84,'multiline':False]['text':' (so multiple Net workers can reuse the same workspace from','line_number':85,'multiline':False]['text':' different threads and CUDA streams).','line_number':86,'multiline':False]['text':'*
 * CuDNNWrapper is a class that wraps the cudnn handles and cudnn workspaces.
 *
 * The wrapper ensures that for each thread and each gpu, there is one
 * identical cudnn handle, which is also associated with the thread-local
 * per-device cuda stream. The wrapper also hosts the device-specific cudnn
 * workspace (scratch space for some cudnn functions).
 *
 ','line_number':133,'multiline':True]['text':'*
   * Creates a cudnn wrapper associated with a CUDAContext object. Note that
   * the CUDAContext object should outlive the CuDNNWrapper.
   ','line_number':144,'multiline':True]['text':'*
   * Returns the inline cudnn handle that executes on the current
   * thread's cuda_stream.
   ','line_number':150,'multiline':True]['text':' Executes the closure F on the CuDNNState associated with state_idx','line_number':158,'multiline':False]['text':' We need to serialize execution on the CuDNNState as we can't','line_number':167,'multiline':False]['text':' allow multiple threads to race through the cudaEventRecord','line_number':168,'multiline':False]['text':' calls (so a worker thread might wait on another worker thread's','line_number':169,'multiline':False]['text':' execution)','line_number':170,'multiline':False]['text':' Pointer to an external cuda context that the cudnn wrapper will use.','line_number':179,'multiline':False]['text':' namespace caffe2','line_number':197,'multiline':False]