['text':' __GNUC__','line_number':14,'multiline':False]['text':' USE_ROCM','line_number':15,'multiline':False]['text':' CAFFE2_CUDA_API gets translated to CAFFE2_HIP_API in hipify script, which','line_number':30,'multiline':False]['text':' causes a marco redefinition issue with the later definition of','line_number':31,'multiline':False]['text':' CAFFE2_HIP_API, so we exclude this definition when HIP is specified','line_number':32,'multiline':False]['text':' USE_ROCM','line_number':35,'multiline':False]['text':'TODO: [ROCm] Need to remove this after CUDA->HIP mapping is updated.','line_number':37,'multiline':False]['text':' This is a macro defined for cuda fp16 support. In default, cuda fp16 is','line_number':41,'multiline':False]['text':' supported by NVCC 7.5, but it is also included in the Tegra X1 platform with','line_number':42,'multiline':False]['text':' a (custom?) NVCC 7.0. As a result, we would normally just check the cuda','line_number':43,'multiline':False]['text':' version here, but would also allow a use to pass in the flag','line_number':44,'multiline':False]['text':' CAFFE_HAS_CUDA_FP16 manually.','line_number':45,'multiline':False]['text':' CAFFE_HAS_CUDA_FP16','line_number':49,'multiline':False]['text':' cuda major revision number below which fp16 compute is not supoorted','line_number':55,'multiline':False]['text':' Re-enable strict aliasing diagnostic if it was disabled.','line_number':62,'multiline':False]['text':' __GNUC__','line_number':68,'multiline':False]['text':' USE_ROCM','line_number':69,'multiline':False]['text':'*
 * The maximum number of peers that each gpu can have when doing p2p setup.
 * Currently, according to NVidia documentation, each device can support a
 * system-wide maximum of eight peer connections.
 * When Caffe2 sets up peer access resources, if we have more than 8 gpus,
 * we will enable peer access in groups of 8.
 ','line_number':71,'multiline':True]['text':'*
 * Empty class to identify TensorCore-based math
 ','line_number':83,'multiline':True]['text':' USE_ROCM','line_number':87,'multiline':False]['text':'*
 * A runtime function to report the cuda version that Caffe2 is built with.
 ','line_number':95,'multiline':True]['text':'*
 * Returns the number of devices.
 ','line_number':106,'multiline':True]['text':'*
 * Check if the current running session has a cuda gpu present.
 *
 * Note that this is different from having caffe2 built with cuda. Building
 * Caffe2 with cuda only guarantees that this function exists. If there are no
 * cuda gpus present in the machine, or there are hardware configuration
 * problems like an insufficient driver, this function will still return false,
 * meaning that there is no usable GPU present.
 *
 * In the open source build, it is possible that Caffe2's GPU code is
 * dynamically loaded, and as a result a library could be only linked to the
 * CPU code, but want to test if cuda is later available or not. In this case,
 * one should use HasCudaRuntime() from common.h.
 ','line_number':111,'multiline':True]['text':'*
 * Gets the current GPU id. This is a simple wrapper around cudaGetDevice().
 ','line_number':129,'multiline':True]['text':'*
 * Gets the current GPU id. This is a simple wrapper around cudaGetDevice().
 ','line_number':134,'multiline':True]['text':'*
 * Gets the GPU id that the current pointer is located at.
 ','line_number':139,'multiline':True]['text':'*
 * Gets the device property for the given device. This function is thread safe.
 * The initial run on this function is ~1ms/device; however, the results are
 * cached so subsequent runs should be much faster.
 ','line_number':144,'multiline':True]['text':'*
 * Runs a device query function and prints out the results to LOG(INFO).
 ','line_number':151,'multiline':True]['text':'*
 * Return a peer access pattern by returning a matrix (in the format of a
 * nested vector) of boolean values specifying whether peer access is possible.
 *
 * This function returns false if anything wrong happens during the query of
 * the GPU access pattern.
 ','line_number':156,'multiline':True]['text':'*
 * Return the availability of TensorCores for math
 ','line_number':165,'multiline':True]['text':'*
 * Return a human readable cublas error string.
 ','line_number':170,'multiline':True]['text':'*
 * Return a human readable curand error string.
 ','line_number':175,'multiline':True]['text':' CUDA: various checks for different function calls.','line_number':180,'multiline':False]['text':' The following helper functions are here so that you can write a kernel call','line_number':271,'multiline':False]['text':' when you are not particularly interested in maxing out the kernels'','line_number':272,'multiline':False]['text':' performance. Usually, this will give you a reasonable speed, but if you','line_number':273,'multiline':False]['text':' really want to find the best performance, it is advised that you tune the','line_number':274,'multiline':False]['text':' size of the blocks and grids more reasonably.','line_number':275,'multiline':False]['text':' A legacy note: this is derived from the old good Caffe days, when I simply','line_number':276,'multiline':False]['text':' hard-coded the number of threads and wanted to keep backward compatibility','line_number':277,'multiline':False]['text':' for different computation capabilities.','line_number':278,'multiline':False]['text':' For more info on CUDA compute capabilities, visit the NVidia website at:','line_number':279,'multiline':False]['text':'    http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities','line_number':280,'multiline':False]['text':' The number of cuda threads to use. Since work is assigned to SMs at the','line_number':282,'multiline':False]['text':' granularity of a block, 128 is chosen to allow utilizing more SMs for','line_number':283,'multiline':False]['text':' smaller input sizes.','line_number':284,'multiline':False]['text':' 1D grid','line_number':285,'multiline':False]['text':' 2D grid','line_number':287,'multiline':False]['text':' The maximum number of blocks to use in the default kernel call. We set it to','line_number':291,'multiline':False]['text':' 4096 which would work for compute capability 2.x (where 65536 is the limit).','line_number':292,'multiline':False]['text':' This number is very carelessly chosen. Ideally, one would like to look at','line_number':293,'multiline':False]['text':' the hardware at runtime, and pick the number of blocks that makes most','line_number':294,'multiline':False]['text':' sense for the specific runtime environment. This is a todo item.','line_number':295,'multiline':False]['text':' 1D grid','line_number':296,'multiline':False]['text':' 2D grid','line_number':298,'multiline':False]['text':'*
 * @brief Compute the number of blocks needed to run N threads.
 ','line_number':306,'multiline':True]['text':' Use at least 1 block, since CUDA does not allow empty block','line_number':314,'multiline':False]['text':'*
 * @brief Compute the number of blocks needed to run N threads for a 2D grid
 ','line_number':318,'multiline':True]['text':' M ','line_number':321,'multiline':True]['text':' Not calling the 1D version for each dim to keep all constants as literals','line_number':323,'multiline':False]['text':' Use at least 1 block, since CUDA does not allow empty block','line_number':330,'multiline':False]['text':' Use at least 1 block, since CUDA does not allow empty block','line_number':338,'multiline':False]['text':' namespace caffe2','line_number':479,'multiline':False]['text':' CAFFE2_CORE_COMMON_GPU_H_','line_number':481,'multiline':False]