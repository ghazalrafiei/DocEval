['text':'/ Provides slicing info for the outputs. All the vector members should be of','line_number':22,'multiline':False]['text':'/ the same size as number of outputs of the Onnxifi op.','line_number':23,'multiline':False]['text':' namespace details','line_number':43,'multiline':False]['text':' Setup input/output descriptor templates','line_number':80,'multiline':False]['text':' For output, we try to get its output size hint','line_number':115,'multiline':False]['text':' Populate output_shapes_per_bs_','line_number':141,'multiline':False]['text':' Get output resizing hints','line_number':165,'multiline':False]['text':' Encode arguments starting with "custom_" to backend','line_number':169,'multiline':False]['text':' Initialize the backend if it has not been already created. When we','line_number':175,'multiline':False]['text':' initialized the backend, we will get the weights (initializers) from the','line_number':176,'multiline':False]['text':' workspace and offload onto the backend. This should be done only once.','line_number':177,'multiline':False]['text':' Subsequent call of this function with the same model id should find a','line_number':178,'multiline':False]['text':' cached backend and therefore there is no need to repeat the above','line_number':179,'multiline':False]['text':' process.','line_number':180,'multiline':False]['text':' Second argument is a cache vector to avoid repeated reallocation.','line_number':204,'multiline':False]['text':' The existence of this is not ideal, which is purely due to the fact that','line_number':205,'multiline':False]['text':' we use int64_t for c2::tensor dim but uint64_t for onnxDesciptor dim.','line_number':206,'multiline':False]['text':' Maybe we should just use int64_t.','line_number':207,'multiline':False]['text':' unused ','line_number':213,'multiline':True]['text':' unused ','line_number':215,'multiline':True]['text':' unused ','line_number':216,'multiline':True]['text':' Build the Onnxifi engine','line_number':230,'multiline':False]['text':' If using Glow AOT, override the backend_id to 1, since it uses a custom','line_number':233,'multiline':False]['text':' ONNX format, and that's the id we use for the ONNX backend.','line_number':234,'multiline':False]['text':' Release unused backend ids.','line_number':270,'multiline':False]['text':' Get weights','line_number':278,'multiline':False]['text':' Extra weight shapes','line_number':289,'multiline':False]['text':'/ Set up function pointer if onnxifi_ext is enabled','line_number':344,'multiline':False]['text':'/ Helper method for extractOutputBatchSizes(), used to deduplicate code of populating output reshape infos','line_number':385,'multiline':False]['text':'/ Helper method for updating output reshape info using provided output shape hints.','line_number':393,'multiline':False]['text':'/ Extract output batch size. If the output batch size is going to be at','line_number':396,'multiline':False]['text':'/ max_batch_size_, return true indicating that no output shape adjustment is','line_number':397,'multiline':False]['text':'/ needed. Otherwise, return false.','line_number':398,'multiline':False]['text':'/ Adjust output tensor shape based on the current input batch size.','line_number':401,'multiline':False]['text':'/ If the output shape is conditioned on first dim (batch size), we have a','line_number':402,'multiline':False]['text':'/ fast path to shrink the tensor shape by just manipulating the meta data.','line_number':403,'multiline':False]['text':'/ Otherwise, we have to slice it in the middle of the dimension with copy','line_number':404,'multiline':False]['text':'/ invoked. This is a slow path and we don't expect it to happen very often.','line_number':405,'multiline':False]['text':'/ We can already omit this step by setting "adjust_output_batch_" to false','line_number':406,'multiline':False]['text':'/ initialize an OutputReshapeInfo object','line_number':417,'multiline':False]['text':' pointer to loaded onnxifi library','line_number':420,'multiline':False]['text':' input/output descriptors','line_number':430,'multiline':False]['text':' Output reshape info','line_number':434,'multiline':False]['text':' It is a map keyed on batch size and the value OutputReshapeInfo for the','line_number':435,'multiline':False]['text':' batch size.','line_number':436,'multiline':False]['text':' onnxifi extension mode function pointer','line_number':440,'multiline':False]['text':' ONNX model or not','line_number':464,'multiline':False]['text':' Glow AOT model or not','line_number':467,'multiline':False]['text':' max batch size','line_number':470,'multiline':False]['text':' max sequence lookup size','line_number':473,'multiline':False]['text':' Inference timeout limits. Default 0 means no timeout.','line_number':476,'multiline':False]['text':' index of the input whose first dimension represents the batch size','line_number':479,'multiline':False]['text':' We bind the op input/output by position while ONNXIFI binds input/output by','line_number':482,'multiline':False]['text':' names. In addition, op input/output names can be written by, for example,','line_number':483,'multiline':False]['text':' memonger. We cache the original input/output name of ONNX object here and','line_number':484,'multiline':False]['text':' bind them by position.','line_number':485,'multiline':False]['text':' NetDef of the onnxifi subgraph for shape inference','line_number':489,'multiline':False]['text':' Mapping of batch sizes to output shapes','line_number':495,'multiline':False]['text':' Indicate if i-th output is a quantized tensor','line_number':498,'multiline':False]['text':' This is for multi group quantization info','line_number':501,'multiline':False]['text':' output shape hints','line_number':505,'multiline':False]['text':' input shape info. Used by shape inference when inputs are not at','line_number':508,'multiline':False]['text':' max_batch_size','line_number':509,'multiline':False]['text':' Whether we should use passed output shape hints or do shape inference','line_number':512,'multiline':False]['text':' Whether we need to resize outputs or not','line_number':515,'multiline':False]['text':' Whether we enable tracing in one run of inference','line_number':518,'multiline':False]['text':' Adjust the quantized offset to compensate mismatch of certain backend','line_number':521,'multiline':False]['text':' Whether we should read batch size value from Onnxifi request data','line_number':524,'multiline':False]['text':' namespace caffe2','line_number':528,'multiline':False]