['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':136,'multiline':False]['text':' Memory type','line_number':145,'multiline':False]['text':' We only allow weights to be CPU tensor or int8tensor for now','line_number':146,'multiline':False]['text':' Data type','line_number':159,'multiline':False]['text':' Set dims','line_number':163,'multiline':False]['text':' Data type','line_number':176,'multiline':False]['text':' Set dims','line_number':179,'multiline':False]['text':' namespace','line_number':209,'multiline':False]['text':' namespace details','line_number':242,'multiline':False]['text':' Since onnxTensorDescriptorV1.name will point into the memory in','line_number':256,'multiline':False]['text':' weight_names, we need to prevent weight_names from reallocating by','line_number':257,'multiline':False]['text':' reserving enough memory ahead of time','line_number':258,'multiline':False]['text':' Initialize the tensors used to slice the output','line_number':291,'multiline':False]['text':' Get the real batch size from nominal input. If it's equal to','line_number':422,'multiline':False]['text':' max_batch_size, mark that we don't need to adjust batch size and return.','line_number':423,'multiline':False]['text':' Otherwise, do a pass of shape inference to get the real shapes of the','line_number':424,'multiline':False]['text':' outputs.','line_number':425,'multiline':False]['text':' We still need to adjust output size but we can skip the shape inference as','line_number':447,'multiline':False]['text':' it was done before.','line_number':448,'multiline':False]['text':' We need to use generic Slice','line_number':477,'multiline':False]['text':' Setup the output C2 tensor','line_number':517,'multiline':False]['text':' Normal Tensor','line_number':519,'multiline':False]['text':' single quantizer, output Int8Tensor','line_number':527,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':642,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':645,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':652,'multiline':False]['text':'*
   * If onnxifi extension mode is enabled,
   * and onnxSetIOAndRunGraph is supported in backend,
   * then we run through this workflow;
   * Else we fallback to non-onnxifi-extension workflow.
   *','line_number':676,'multiline':True]['text':' Check if we should rely on Onnxifi to provide current batch size','line_number':713,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':729,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':731,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':743,'multiline':False]['text':' Call the async run on backend, signal event on input fence and wait for','line_number':784,'multiline':False]['text':' the event on output fence','line_number':785,'multiline':False]['text':' Destroy the event objects','line_number':795,'multiline':False]['text':' namespace caffe2','line_number':835,'multiline':False]