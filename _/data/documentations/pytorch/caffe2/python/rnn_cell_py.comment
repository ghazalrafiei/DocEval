['text':'# @package rnn_cell','line_number':1,'multiline':False]['text':' Module caffe2.python.rnn_cell','line_number':2,'multiline':False]['text':' states_for_all_steps consists of combination of','line_number':120,'multiline':False]['text':' states gather for all steps and final states. It looks like this:','line_number':121,'multiline':False]['text':' (state_1_all, state_1_final, state_2_all, state_2_final, ...)','line_number':122,'multiline':False]['text':' based on https://pytorch.org/docs/master/nn.html#torch.nn.RNNCell','line_number':292,'multiline':False]['text':' TODO If this codepath becomes popular, it may be worth','line_number':347,'multiline':False]['text':' taking a look at optimizing it - for now a simple','line_number':348,'multiline':False]['text':' implementation is used to round out compatibility with','line_number':349,'multiline':False]['text':' ONNX.','line_number':350,'multiline':False]['text':' brew.layer_norm call is only difference from LSTMCell','line_number':556,'multiline':False]['text':' defining initializers for MI parameters','line_number':638,'multiline':False]['text':' alpha * input_t + beta_h','line_number':660,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':661,'multiline':False]['text':' (alpha * input_t + beta_h) * prev_t =','line_number':667,'multiline':False]['text':' alpha * input_t * prev_t + beta_h * prev_t','line_number':668,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':669,'multiline':False]['text':' beta_i * input_t + b','line_number':674,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':675,'multiline':False]['text':' alpha * input_t * prev_t + beta_h * prev_t + beta_i * input_t + b','line_number':681,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':682,'multiline':False]['text':' defining initializers for MI parameters','line_number':738,'multiline':False]['text':' alpha * input_t + beta_h','line_number':760,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':761,'multiline':False]['text':' (alpha * input_t + beta_h) * prev_t =','line_number':767,'multiline':False]['text':' alpha * input_t * prev_t + beta_h * prev_t','line_number':768,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':769,'multiline':False]['text':' beta_i * input_t + b','line_number':774,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':775,'multiline':False]['text':' alpha * input_t * prev_t + beta_h * prev_t + beta_i * input_t + b','line_number':781,'multiline':False]['text':' Shape: [1, batch_size, 4 * hidden_size]','line_number':782,'multiline':False]['text':' brew.layer_norm call is only difference from MILSTMCell._apply','line_number':788,'multiline':False]['text':' # If cells don't have different names we still','line_number':1006,'multiline':False]['text':' take care of scoping','line_number':1007,'multiline':False]['text':' Since we're using here non-public method _apply,','line_number':1027,'multiline':False]['text':' instead of apply, we have to manually extract output','line_number':1028,'multiline':False]['text':' from states','line_number':1029,'multiline':False]['text':' [batch_size, encoder_length, 1]','line_number':1257,'multiline':False]['text':' [encoder_length, batch_size, encoder_output_dim]','line_number':1295,'multiline':False]['text':' total attention weight applied across decoding steps_per_checkpoint','line_number':1318,'multiline':False]['text':' shape: [encoder_length]','line_number':1319,'multiline':False]['text':' outputs_with_grads argument indexes into final layer','line_number':1559,'multiline':False]['text':' Now they are blob references - outputs of splitting the input sequence','line_number':1601,'multiline':False]['text':' Parameters of all timesteps are shared','line_number':1614,'multiline':False]['text':' Interleave the state values similar to','line_number':1642,'multiline':False]['text':'','line_number':1643,'multiline':False]['text':'   x = [1, 3, 5]','line_number':1644,'multiline':False]['text':'   y = [2, 4, 6]','line_number':1645,'multiline':False]['text':'   z = [val for pair in zip(x, y) for val in pair]','line_number':1646,'multiline':False]['text':'   # z is [1, 2, 3, 4, 5, 6]','line_number':1647,'multiline':False]['text':'','line_number':1648,'multiline':False]['text':' and returns it as outputs','line_number':1649,'multiline':False]['text':' Multiply by 4 since we have 4 gates per LSTM unit','line_number':1759,'multiline':False]['text':' TODO','line_number':1777,'multiline':False]['text':' TODO','line_number':1778,'multiline':False]['text':' TODO','line_number':1779,'multiline':False]['text':' Populate the weights-blob from blobs containing parameters for','line_number':1788,'multiline':False]['text':' the individual components of the LSTM, such as forget/input gate','line_number':1789,'multiline':False]['text':' weights and bises. Also, create a special param_extract_net that','line_number':1790,'multiline':False]['text':' can be used to grab those individual params from the black-box','line_number':1791,'multiline':False]['text':' weights blob. These results can be then fed to InitFromLSTMParams()','line_number':1792,'multiline':False]['text':' TODO: dropout seed','line_number':1827,'multiline':False]['text':' leave it as a first line to grab all params','line_number':1954,'multiline':False]