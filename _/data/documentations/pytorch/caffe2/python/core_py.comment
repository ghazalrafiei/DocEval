['text':'# @package core','line_number':1,'multiline':False]['text':' Module caffe2.python.core','line_number':2,'multiline':False]['text':' Mac os specific message','line_number':28,'multiline':False]['text':' Convenience redirections to functions inside scope.','line_number':35,'multiline':False]['text':' Bring datatype enums to the main namespace','line_number':40,'multiline':False]['text':' Verify that the DataType values defined above match the ones defined in','line_number':60,'multiline':False]['text':' the caffe2.proto file','line_number':61,'multiline':False]['text':' At least one option is for CPU, check if both are for CPU.','line_number':145,'multiline':False]['text':' TODO: T18892922, use device annotations','line_number':160,'multiline':False]['text':' meta allows helper functions to put whatever metainformation needed','line_number':225,'multiline':False]['text':' there.','line_number':226,'multiline':False]['text':' add self to the input list.','line_number':277,'multiline':False]['text':' If blobs is a single string, prepend scope.CurrentNameScope()','line_number':335,'multiline':False]['text':' and put it as a list.','line_number':336,'multiline':False]['text':' TODO(jiayq): enforce using BlobReference instead of raw strings.','line_number':337,'multiline':False]['text':' If blob is a BlobReference, simply put it as a list.','line_number':340,'multiline':False]['text':' If blob is a list, we go through it and type check.','line_number':343,'multiline':False]['text':' Add rectified inputs and outputs','line_number':385,'multiline':False]['text':' Set device option:','line_number':393,'multiline':False]['text':' (1) If device_option is explicitly set, use device_option.','line_number':394,'multiline':False]['text':' (2) If not, but scope.CurrentDeviceScope() is set,','line_number':395,'multiline':False]['text':'     then we use scope.CurrentDeviceScope().','line_number':396,'multiline':False]['text':' (3) Otherwise, do not set device option.','line_number':397,'multiline':False]['text':' random seed is defined in the device option, so we need to do special','line_number':406,'multiline':False]['text':' care.','line_number':407,'multiline':False]['text':' Add given arguments that do not need parsing','line_number':412,'multiline':False]['text':' Add all other arguments','line_number':415,'multiline':False]['text':' The IR class holds multiple metadata from the forward pass:','line_number':495,'multiline':False]['text':' a) ssa: a list of [op, in_versions, out_versions] recording the','line_number':496,'multiline':False]['text':'    input and the output version of each operator, similar','line_number':497,'multiline':False]['text':'    to a normal SSA form.','line_number':498,'multiline':False]['text':' b) input_usages: a dictionary specifying for each blob and','line_number':499,'multiline':False]['text':'    each of its version, how many times it is used as input for another','line_number':500,'multiline':False]['text':'    op.','line_number':501,'multiline':False]['text':' c) frontier: maintaining the current versions of the blobs','line_number':502,'multiline':False]['text':'    we are having in the workspace, after the execution of all the ops','line_number':503,'multiline':False]['text':'    added to the IR so far. This is useful because if a gradient is','line_number':504,'multiline':False]['text':'    trying to access an earlier version of a blob, we can sanity check','line_number':505,'multiline':False]['text':'    that it is no longer there, and thus throw an error.','line_number':506,'multiline':False]['text':' d) gradient_frontier: maps the names of blobs to its version that the','line_number':507,'multiline':False]['text':'    gradient corresponds to.','line_number':508,'multiline':False]['text':' e) gradient_generators: for each blob and each of its version, maps to','line_number':509,'multiline':False]['text':'    a list of operators that generates its gradient together with the','line_number':510,'multiline':False]['text':'    gradient name.','line_number':511,'multiline':False]['text':' Validate StopGradient usage by checking that StopGradient's output','line_number':526,'multiline':False]['text':' is actually passed forward','line_number':527,'multiline':False]['text':' For input, they are the current version in the dict.','line_number':539,'multiline':False]['text':' For output, they are the current version plus one. If this is a','line_number':545,'multiline':False]['text':' newly created blob, its version starts with zero.','line_number':546,'multiline':False]['text':' Add to SSA for bookkeeping.','line_number':553,'multiline':False]['text':' Functions to generate debug help for version-mismatches','line_number':562,'multiline':False]['text':' If it is a dense or sparse gradient name, it should match the','line_number':581,'multiline':False]['text':' version of the corresponding output.','line_number':582,'multiline':False]['text':' If it is an output name, the current version should match the','line_number':595,'multiline':False]['text':' version when the operator was run.','line_number':596,'multiline':False]['text':' If it is an input name, the current version should match the','line_number':606,'multiline':False]['text':' version when the operator was run.','line_number':607,'multiline':False]['text':' If it is none of the above, it should be a blob that is','line_number':617,'multiline':False]['text':' generated locally by one of the previous gradient operators.','line_number':618,'multiline':False]['text':' merge indices and values generators for sparse gradients','line_number':628,'multiline':False]['text':' either indices or values are generated (but not both)','line_number':632,'multiline':False]['text':' both indices and values are generated','line_number':635,'multiline':False]['text':' NOQA','line_number':654,'multiline':False]['text':' (1) check that inputs are valid','line_number':662,'multiline':False]['text':' (2) add outputs to the locally generated blobs','line_number':667,'multiline':False]['text':' If an output corresponds to the gradient of an input, we also','line_number':668,'multiline':False]['text':' record it to gradient_generators','line_number':669,'multiline':False]['text':' the output corresponds either to the indices or the','line_number':678,'multiline':False]['text':' values of the sparse gradient. In either case we','line_number':679,'multiline':False]['text':' create a (partial) SparseGradGenMeta. If necessary,','line_number':680,'multiline':False]['text':' we'll merge indices and values generators','line_number':681,'multiline':False]['text':' corresponding to the same gradient in step (3)','line_number':682,'multiline':False]['text':' (3) merge indices and values generators for sparse gradients, and','line_number':696,'multiline':False]['text':' add them to gradient_generators','line_number':697,'multiline':False]['text':' (4) for ops (e.g., Add, Sum, Sub) which have gradient outputs directly','line_number':700,'multiline':False]['text':' passed from inputs (not computed from gradient ops), we create an','line_number':701,'multiline':False]['text':' GradGenMeta with None grad_op and idx so that the gradient_generators','line_number':702,'multiline':False]['text':' knows where the gradients are coming from. This is needed for creating','line_number':703,'multiline':False]['text':' Sum op to accumulate the gradients from multiple parents.','line_number':704,'multiline':False]['text':' Finally, for the gradients specified in g_input, we update the','line_number':720,'multiline':False]['text':' gradient frontier to reflect the input versions that the gradients','line_number':721,'multiline':False]['text':' correspond to.','line_number':722,'multiline':False]['text':' we already checked that device options are consistent so we can just','line_number':756,'multiline':False]['text':' break after finding the first clear_info request','line_number':757,'multiline':False]['text':' if we find that device_option in the generator that','line_number':764,'multiline':False]['text':' requires clear the extra info for the auto gen sum','line_number':765,'multiline':False]['text':' Then we will try to clear them and only leave the','line_number':766,'multiline':False]['text':' IS_AUTO_GEN_SUM_OPS_TAG','line_number':767,'multiline':False]['text':' we already checked that device options are consistent so we can just','line_number':773,'multiline':False]['text':' use the first one we find','line_number':774,'multiline':False]['text':' TODO not sure what this message really means','line_number':794,'multiline':False]['text':' Sum inplace mode works only for the first input','line_number':822,'multiline':False]['text':' So we do a swap','line_number':823,'multiline':False]['text':' Sum the given sparse representations by simply concatenating the','line_number':860,'multiline':False]['text':' indices (resp. values) tensors together. We don't do any deduplication','line_number':861,'multiline':False]['text':' of indices at this point. This will be done as needed before the','line_number':862,'multiline':False]['text':' optimizer is called','line_number':863,'multiline':False]['text':' (1) check if all gradients are of the same type. Aggregating a mix of','line_number':900,'multiline':False]['text':' sparse and dense gradients is not supported yet','line_number':901,'multiline':False]['text':' If for all the operators that used the operator, none or only one','line_number':907,'multiline':False]['text':' produced the gradient, then no additional sum needs to be carried','line_number':908,'multiline':False]['text':' out.','line_number':909,'multiline':False]['text':' Check if all grad op device options are the same.','line_number':926,'multiline':False]['text':' We do not need to do gradient accumulation yet.','line_number':960,'multiline':False]['text':' Finally, let's create the sum operator.','line_number':974,'multiline':False]['text':' Gradient here is not sparse  as it was generated by','line_number':981,'multiline':False]['text':' a ConstantFill operator. Autogeneration for sparse gradients is','line_number':982,'multiline':False]['text':' not supported','line_number':983,'multiline':False]['text':' Since the C++ gradient registry does not have notion of','line_number':1005,'multiline':False]['text':' NameScopes, we will convert all references to strings.','line_number':1006,'multiline':False]['text':' Autogenerated gradients are assumed to be provided for the last','line_number':1010,'multiline':False]['text':' input version','line_number':1011,'multiline':False]['text':' Check if the gradient operators are legal, and update','line_number':1029,'multiline':False]['text':' gradient_generators and gradient_frontier','line_number':1030,'multiline':False]['text':' Record the gradient map to all_input_to_grad.','line_number':1033,'multiline':False]['text':' Do not overwrite an existing gradient with a None','line_number':1035,'multiline':False]['text':' unless the input is also an output of the op, since','line_number':1036,'multiline':False]['text':' we update the blob version when blob is output of an','line_number':1037,'multiline':False]['text':' operator.','line_number':1038,'multiline':False]['text':' Set the gradient frontier with the initialized external','line_number':1062,'multiline':False]['text':' gradients.','line_number':1063,'multiline':False]['text':' (2) Now, after having the virtual play above, we now play the ops','line_number':1071,'multiline':False]['text':' backwards, creating the gradients along the path. Note that although','line_number':1072,'multiline':False]['text':' we are playing it backwards, we cannot refer to variables that are','line_number':1073,'multiline':False]['text':' at a version older than current_versions because it is already been','line_number':1074,'multiline':False]['text':' overwritten.','line_number':1075,'multiline':False]['text':' If there are multiple use blobs, do gradient accumulation.','line_number':1082,'multiline':False]['text':' This line is so that if in an accumulation some of the operators','line_number':1085,'multiline':False]['text':' have not produced gradients, they still do not overwrite the','line_number':1086,'multiline':False]['text':' general all_input_to_grad map.','line_number':1087,'multiline':False]['text':' (3) Post-processing.','line_number':1091,'multiline':False]['text':' After we have done computation for each op, we now have the gradient','line_number':1092,'multiline':False]['text':' operators ready. For the output map, we will convert everything to','line_number':1093,'multiline':False]['text':' BlobReferences for easier handling in python.','line_number':1094,'multiline':False]['text':' TODO(tulloch) - Propagate GradientWrapper up through the stack.','line_number':1123,'multiline':False]['text':' Not supported in C++; will try python registration next.','line_number':1167,'multiline':False]['text':' argument is a proto','line_number':1323,'multiline':False]['text':' TODO(azzolini): improve schema type checking','line_number':1394,'multiline':False]['text':' Register blobs so that it's guaranteed that different calls to','line_number':1488,'multiline':False]['text':' NextBlob/NextScopedBlob always return blobs with different names','line_number':1489,'multiline':False]['text':' We are initializing a network by a NetDef. In this case, we will','line_number':1497,'multiline':False]['text':' initialize our network with the given netdef.','line_number':1498,'multiline':False]['text':' Set the next name index properly.','line_number':1509,'multiline':False]['text':' make sure that this net name hasn't been used before','line_number':1537,'multiline':False]['text':' a map between prefix and ID for fast generation of blob names','line_number':1540,'multiline':False]['text':' by default we want to put RecurrentNetworkOp and','line_number':1718,'multiline':False]['text':' RecurrentNetworkGradientOp into remap_funcs, as these two operators','line_number':1719,'multiline':False]['text':' also take blobs and proto into the arguments.','line_number':1720,'multiline':False]['text':' external input list','line_number':1782,'multiline':False]['text':' external outputs','line_number':1795,'multiline':False]['text':' assuming that the net has no inplace blob','line_number':1902,'multiline':False]['text':' tensor should not be modified yet.','line_number':1908,'multiline':False]['text':' the place to inject new_producer is not just determined by tensor','line_number':1916,'multiline':False]['text':' modify external outputs','line_number':1920,'multiline':False]['text':' modify consumers','line_number':1926,'multiline':False]['text':' this is not necessarily true','line_number':1930,'multiline':False]['text':' Check if in immediate mode: the grad_ops are actually being produced','line_number':2050,'multiline':False]['text':' by C++ and bypasses the CreateOperator() call, so in immediate mode','line_number':2051,'multiline':False]['text':' we will have to explicitly run them.','line_number':2052,'multiline':False]['text':' This returns a reference to the observer','line_number':2101,'multiline':False]['text':' Move RecurrentNetwork operators on GPU as well','line_number':2214,'multiline':False]['text':' If we do not specify an output, we will assume that this op','line_number':2249,'multiline':False]['text':' produces one output in this case.','line_number':2250,'multiline':False]['text':' In this case, we will auto-fill the given number of outputs','line_number':2253,'multiline':False]['text':' with auto-generated names.','line_number':2254,'multiline':False]['text':' remapping of input blobs for each op.','line_number':2489,'multiline':False]['text':' if external_inputs have device remappings generated by previous nets,','line_number':2495,'multiline':False]['text':' then add those remappings as external inputs as well.','line_number':2496,'multiline':False]['text':' Get where inputs and outputs should be. If it is a Placeholder','line_number':2507,'multiline':False]['text':' (i.e. fake) op, then set op's device as blob's devices.','line_number':2508,'multiline':False]['text':' reuse already moved input','line_number':2529,'multiline':False]['text':' need to make input on correct device.','line_number':2534,'multiline':False]['text':' Enforcing no reuse blob between operators. In-place blob usage in an','line_number':2561,'multiline':False]['text':' op is allowed. This is based on the assumption that in-place op has','line_number':2562,'multiline':False]['text':' same device info','line_number':2563,'multiline':False]['text':' keep inplace blobs inplace','line_number':2583,'multiline':False]['text':' A holistic blob to device mapping.','line_number':2617,'multiline':False]['text':' DEPRECATED','line_number':2858,'multiline':False]['text':' DEPRECATED','line_number':2859,'multiline':False]['text':' FIXME(azzolini): This is actually wrong. Report nets should be','line_number':2875,'multiline':False]['text':' instantiated first since they may run before any substep is run.','line_number':2876,'multiline':False]['text':' However, curerntly, Reporter depends on this behavior.','line_number':2877,'multiline':False]['text':' A list of ExecutionStep','line_number':2887,'multiline':False]['text':' nets need to be added to the plan in order of usage','line_number':2918,'multiline':False]['text':' Ignore top 3 layers of stack: this function, _CreateAndAddToSelf, and','line_number':3062,'multiline':False]['text':' whatever calls _CreateAndAddToSelf (either __getattr__ or Python)','line_number':3063,'multiline':False]['text':' We just go down the frame stack in a loop','line_number':3065,'multiline':False]['text':' Its important to extract information from the frame here','line_number':3067,'multiline':False]['text':' as frame's current line most probably will change later.','line_number':3068,'multiline':False]