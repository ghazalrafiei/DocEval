['text':' @package layer_model_helper','line_number':1,'multiline':False]['text':' Module caffe2.python.layer_model_helper','line_number':2,'multiline':False]['text':' seed default','line_number':55,'multiline':False]['text':' optimizer bookkeeping','line_number':59,'multiline':False]['text':' breakdown map; breakdown features are categorical (like dense) but not','line_number':71,'multiline':False]['text':' necessarily used to represent data for training','line_number':72,'multiline':False]['text':' Connect Schema to self.net. That particular instance of schmea will be','line_number':75,'multiline':False]['text':' use for generation of the Layers across the network and would be used','line_number':76,'multiline':False]['text':' for connection with Readers.','line_number':77,'multiline':False]['text':' additional (hard-coded) diagnose_options to report based on the model','line_number':96,'multiline':False]['text':' TODO(xlwang): it's hack!','line_number':97,'multiline':False]['text':' an empty white_set will skip everything','line_number':115,'multiline':False]['text':' to add a global constant to model, one first need to get the','line_number':135,'multiline':False]['text':' initializer','line_number':136,'multiline':False]['text':' TODO: make GivenTensor generic','line_number':145,'multiline':False]['text':' This is global namescope for constants. They will be created in all','line_number':175,'multiline':False]['text':' init_nets and there should be very few of them.','line_number':176,'multiline':False]['text':' To ad hoc add new global constants without duplication','line_number':191,'multiline':False]['text':' if the name was already registered in global_constants, it will not be','line_number':192,'multiline':False]['text':' added even if the intended value is different from its original value','line_number':193,'multiline':False]['text':' check if the original initializer is the same as the one intended','line_number':201,'multiline':False]['text':' now','line_number':202,'multiline':False]['text':' there are three possible values for optim:','line_number':247,'multiline':False]['text':' 1) None (which will use self._default_optimizer after this layer is instantiated)','line_number':248,'multiline':False]['text':' 2) self.NoOptim','line_number':249,'multiline':False]['text':' 3) an instance of Optimizer class such as AdagradOptimizer','line_number':250,'multiline':False]['text':' this implies this parameter is not shared with any other parameter so far','line_number':252,'multiline':False]['text':' Parameter name will be equal to current Namescope that got','line_number':311,'multiline':False]['text':' resolved with the respect of parameter sharing of the scopes.','line_number':312,'multiline':False]['text':' TODO:','line_number':378,'multiline':False]['text':' Currently, LSTM and RNNcells, which use ModelHelper instead of','line_number':379,'multiline':False]['text':' LayerModelHelper as super class, are called in pooling_methods','line_number':380,'multiline':False]['text':' In ModelHelper, regularization is not supported in create_param','line_number':381,'multiline':False]['text':' We will unify the way of create_param of ModelHelper and','line_number':382,'multiline':False]['text':' LayerModelHelper in the future.','line_number':383,'multiline':False]['text':' The primary value of adding everything to self.net - generation of the','line_number':391,'multiline':False]['text':' operators right away, i.e. if error happens it'll be detected','line_number':392,'multiline':False]['text':' immediately. Other than this - create_x_net should be called.','line_number':393,'multiline':False]['text':' Store seed config that will be applied to each op in the net.','line_number':428,'multiline':False]['text':' If sequence_seed is True, the i-th op has rand_seed=`seed + i`','line_number':430,'multiline':False]['text':' loss could've been set through model.loss directly which could be','line_number':523,'multiline':False]['text':' a scalar','line_number':524,'multiline':False]['text':' be the first field','line_number':544,'multiline':False]['text':' merge with other fields','line_number':546,'multiline':False]['text':' TODO(amalevich): Add add support for ifbpy inline documentation','line_number':578,'multiline':False]['text':' TODO(xlwang): Desginated layer shadows the usage of an op as a','line_number':589,'multiline':False]['text':' single layer. To enforce using an op (e.g. Split) as functional','line_number':590,'multiline':False]['text':' layer, one can call 'model.FunctionalLayerSplit'','line_number':591,'multiline':False]['text':' TODO(amalevich): Switch to net.operator as soon as it gets','line_number':596,'multiline':False]['text':' landed','line_number':597,'multiline':False]['text':' this needs to be an AttributeError to fit hasattr semantics','line_number':619,'multiline':False]['text':' if given, blob_to_device is a map from blob to device_option','line_number':657,'multiline':False]['text':' if given, blob_to_device is a map from blob to device_option','line_number':713,'multiline':False]['text':' note that not all params has gradient and thus we sent None if','line_number':718,'multiline':False]['text':' gradient does not exists','line_number':719,'multiline':False]['text':' extra info is not applicable for optimizers','line_number':727,'multiline':False]['text':' An optimizer which allows us to do NO optimization','line_number':737,'multiline':False]['text':' TODO(xlwang): provide more rich feature information in breakdown_map;','line_number':747,'multiline':False]['text':' and change the assertion accordingly','line_number':748,'multiline':False]