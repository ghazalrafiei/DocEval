['text':' #define DNNLOWP_MEASURE_TIME_BREAKDOWN','line_number':3,'multiline':False]['text':' Create shared buffer mutex in the constructor','line_number':50,'multiline':False]['text':' to avoid race-condition in DAGNet.','line_number':51,'multiline':False]['text':' NOLINTNEXTLINE(modernize-use-equals-default)','line_number':65,'multiline':False]['text':' FIXME : code duplication with','line_number':80,'multiline':False]['text':' ConvDNNLowPPackWeightOp::TakeDepthWise3x3FastPath_','line_number':81,'multiline':False]['text':' FIXME : code duplication with','line_number':93,'multiline':False]['text':' ConvDNNLowPPackWeightOp::TakeDepthWise3x3x3FastPath_','line_number':94,'multiline':False]['text':' FIXME : code duplication with','line_number':160,'multiline':False]['text':' ConvDNNLowPPackWeightOp::TakeGConvFastPath_','line_number':161,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':181,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':186,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':188,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':199,'multiline':False]['text':' im2col fusion','line_number':240,'multiline':False]['text':' If input tensor doesn't use dynamic quantization, we fold column_offsets_','line_number':251,'multiline':False]['text':' into bias.','line_number':252,'multiline':False]['text':' Pre-compute row_offset / column_offset','line_number':260,'multiline':False]['text':' Quantize bias','line_number':288,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':300,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':318,'multiline':False]['text':' signed ','line_number':328,'multiline':True]['text':' If column_offsets_ is empty even when we need column_offsets (asymmetric','line_number':339,'multiline':False]['text':' quantization in input), it means we need to fuse column_offsets to bias.','line_number':340,'multiline':False]['text':' When b_quantized_data_ is from pre-packed bias or Int8TensorCPU,','line_number':344,'multiline':False]['text':' we can't inplace modify so copy to internal b_quantized_ vector.','line_number':345,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':349,'multiline':False]['text':' no bias but create one filling with column offset values','line_number':376,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':380,'multiline':False]['text':' Quantize W if not done already','line_number':407,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':467,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':478,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':490,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':511,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':519,'multiline':False]['text':' fast path using fbgemm','line_number':533,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':534,'multiline':False]['text':' ld','line_number':540,'multiline':False]['text':' pmat','line_number':541,'multiline':False]['text':'*
 * @return false if something goes wrong
 ','line_number':571,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':579,'multiline':False]['text':' It's actually fused with Relu not followed by but setting this to make','line_number':589,'multiline':False]['text':' sure quantization error is correctly measured in','line_number':590,'multiline':False]['text':' this->MeasureQuantizationError_','line_number':591,'multiline':False]['text':' Choose quantization for X','line_number':598,'multiline':False]['text':' From here, W_quantized_ is not used anymore when we have Wq_packed_','line_number':607,'multiline':False]['text':' If quantization parameters are not chosen beforehand, run reference','line_number':615,'multiline':False]['text':' Conv op in fp32 to choose quantization for Y.','line_number':616,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':623,'multiline':False]['text':' to measure quantization error, run ref impl.','line_number':633,'multiline':False]['text':' See batch_matmul_dnnlowp_op.cc to why we compute column_offsets,','line_number':655,'multiline':False]['text':' row_offset, and const_offset in this way.','line_number':656,'multiline':False]['text':' Get quantization parameters','line_number':693,'multiline':False]['text':' The dimension of each kernel','line_number':725,'multiline':False]['text':' The offset corresponding to a single input image, and a single output','line_number':744,'multiline':False]['text':' image.','line_number':745,'multiline':False]['text':' The col buffer is stored in CHW order as well - kernel_dim, and the','line_number':748,'multiline':False]['text':' height and width.','line_number':749,'multiline':False]['text':' We must not call mutable_data inside omp region','line_number':752,'multiline':False]['text':' Im2Col, followed by gemm.','line_number':764,'multiline':False]['text':' quantize col_buffer','line_number':807,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':819,'multiline':False]['text':' j','line_number':825,'multiline':False]['text':' i','line_number':826,'multiline':False]['text':' for each group','line_number':834,'multiline':False]['text':' for each image_id','line_number':835,'multiline':False]['text':' f','line_number':839,'multiline':False]['text':' RunOnDeviceWithOrderNCHW','line_number':844,'multiline':False]['text':' Adjust with bias and zero_point and then requantize','line_number':858,'multiline':False]['text':' See batch_matmul_dnnlowp_op.cc to why we compute column_offsets,','line_number':859,'multiline':False]['text':' row_offset, and const_offset in this way.','line_number':860,'multiline':False]['text':' for each group','line_number':901,'multiline':False]['text':' for each row i','line_number':902,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':931,'multiline':False]['text':' for each group','line_number':962,'multiline':False]['text':' for each row i','line_number':963,'multiline':False]['text':' static if','line_number':990,'multiline':False]['text':' for each group','line_number':995,'multiline':False]['text':' for each row i','line_number':996,'multiline':False]['text':' !__AVX2__','line_number':997,'multiline':False]['text':' Make sure i_per_thread is a multiple of 32 because','line_number':1012,'multiline':False]['text':' cblas_gemm_compute_u8s8s32_acc16 performs the best when M is a multiple','line_number':1013,'multiline':False]['text':' of 32.','line_number':1014,'multiline':False]['text':' The offset corresponding to a single input image, and a single output','line_number':1035,'multiline':False]['text':' image.','line_number':1036,'multiline':False]['text':' num_frames','line_number':1080,'multiline':False]['text':' H','line_number':1081,'multiline':False]['text':' W','line_number':1082,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':1126,'multiline':False]['text':' This function is called within an OpenMP region','line_number':1142,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1155,'multiline':False]['text':' Dump input activation','line_number':1193,'multiline':False]['text':' Dump weight','line_number':1205,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1238,'multiline':False]['text':' need zero_point for padding','line_number':1239,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1247,'multiline':False]['text':'act_times_w_scale','line_number':1251,'multiline':True]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1257,'multiline':False]['text':' need zero_point for padding','line_number':1258,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1266,'multiline':False]['text':'act_times_w_scale','line_number':1270,'multiline':True]['text':' omp parallel','line_number':1274,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1296,'multiline':False]['text':' need zero_point for padding','line_number':1297,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1305,'multiline':False]['text':'act_times_w_scale','line_number':1309,'multiline':True]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1321,'multiline':False]['text':' need zero_point for padding','line_number':1322,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1330,'multiline':False]['text':'act_times_w_scale','line_number':1334,'multiline':True]['text':' omp parallel','line_number':1338,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1346,'multiline':False]['text':' TODO: add parallelization once fbgemmGroupwiseConv supports multi-threading','line_number':1357,'multiline':False]['text':' #pragma omp parallel','line_number':1358,'multiline':False]['text':' dnnlowp_get_thread_num();','line_number':1361,'multiline':False]['text':' dnnlowp_get_num_threads();','line_number':1362,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1370,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1383,'multiline':False]['text':' need zero_point for padding','line_number':1384,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1398,'multiline':False]['text':' need zero_point for padding','line_number':1399,'multiline':False]['text':' column_offsets_ empty means column_offsets_ are folded into bias','line_number':1414,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1429,'multiline':False]['text':' need zero_point for padding','line_number':1430,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1445,'multiline':False]['text':' need zero_point for padding','line_number':1446,'multiline':False]['text':' omp parallel','line_number':1459,'multiline':False]['text':' Normal path for non-special (e.g., no depth-wise) convolutions.','line_number':1464,'multiline':False]['text':' fast path to use fbgemm','line_number':1496,'multiline':False]['text':' buffer for packed matrix','line_number':1517,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1519,'multiline':False]['text':' need zero_point for padding','line_number':1520,'multiline':False]['text':' 3D','line_number':1534,'multiline':False]['text':' buffer for packed matrix','line_number':1553,'multiline':False]['text':' Shouldn't pass 0 if column_offsets_ is empty here because we','line_number':1555,'multiline':False]['text':' need zero_point for padding','line_number':1556,'multiline':False]['text':' 3D','line_number':1569,'multiline':False]['text':' no im2col fusion','line_number':1571,'multiline':False]['text':' buffer for packed matrix','line_number':1578,'multiline':False]['text':' no im2col fusion','line_number':1585,'multiline':False]['text':' buffer for packed matrix','line_number':1592,'multiline':False]['text':' no im2col fusion','line_number':1606,'multiline':False]['text':' Wq_packed_.empty()','line_number':1609,'multiline':False]['text':'if (VLOG_IS_ON(3))','line_number':1635,'multiline':True]['text':' Get quantization parameters','line_number':1641,'multiline':False]['text':'if (VLOG_IS_ON(3))','line_number':1647,'multiline':True]['text':' The col buffer is stored in HWC order as well - kernel_dim, and the height','line_number':1675,'multiline':False]['text':' and width.','line_number':1676,'multiline':False]['text':'if (VLOG_IS_ON(3)) ','line_number':1679,'multiline':True]['text':' Im2col, followed by gemm.','line_number':1688,'multiline':False]['text':'if (VLOG_IS_ON(3)) ','line_number':1693,'multiline':True]['text':'if (VLOG_IS_ON(3)) ','line_number':1702,'multiline':True]['text':'if (VLOG_IS_ON(3)) ','line_number':1714,'multiline':True]['text':' In fast path with fbgemm except when','line_number':1724,'multiline':False]['text':' rescaling quantized numbers should've been already done.','line_number':1725,'multiline':False]['text':' f','line_number':1730,'multiline':False]['text':'if (VLOG_IS_ON(3)) ','line_number':1735,'multiline':True]['text':' The dimension of each kernel','line_number':1737,'multiline':False]['text':' The output image size is the spatial size of the output.','line_number':1739,'multiline':False]['text':' namespace caffe2','line_number':1796,'multiline':False]