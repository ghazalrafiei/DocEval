['text':' Helper functions','line_number':19,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':48,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':53,'multiline':False]['text':'weight','line_number':55,'multiline':True]['text':' qparams[g] is computed for unsigned type.','line_number':57,'multiline':False]['text':' Adjust for the fact that weight will actually use signed.','line_number':58,'multiline':False]['text':' TODO reuse col_offsets_with_zero_pt_s8acc32_ref in fbgemm','line_number':86,'multiline':False]['text':' RefImplementations.cc . We can't do this now because W_quantized is','line_number':87,'multiline':False]['text':' not transposed here.','line_number':88,'multiline':False]['text':' for each group','line_number':180,'multiline':False]['text':' FIXME: code duplication with ConvDNNLowPOp::QuantizeBias_','line_number':187,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':199,'multiline':False]['text':' FLAGS_caffe2_fbgemm_fake_fp16_clamp ','line_number':217,'multiline':True]['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':221,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':223,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':225,'multiline':False]['text':' signed ','line_number':234,'multiline':True]['text':' FullyConnectedDNNLowPPackWeightOp','line_number':244,'multiline':False]['text':' Create tensor with the same shape but this new tensor shouldn't actually','line_number':272,'multiline':False]['text':' allocate memory for the tensor.','line_number':273,'multiline':False]['text':' This is just a convenient way to pass tensor shape information','line_number':274,'multiline':False]['text':' Pre-compute column offsets','line_number':300,'multiline':False]['text':' This should happen before ExtractOutlierMatrix because W_quantized is','line_number':301,'multiline':False]['text':' changed in ExtractOutlierMatrix.','line_number':302,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':303,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':321,'multiline':False]['text':' pmat','line_number':328,'multiline':False]['text':' group','line_number':329,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':331,'multiline':False]['text':' pmat','line_number':338,'multiline':False]['text':' group','line_number':339,'multiline':False]['text':' Quantize bias','line_number':342,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':344,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':348,'multiline':False]['text':' Output quantized bias if we specify a second output. This output is meant','line_number':355,'multiline':False]['text':' to be consumed by accelerator instead of CPU ops.','line_number':356,'multiline':False]['text':' The reason we don't support this is basically due to limitation of','line_number':359,'multiline':False]['text':' Int8TensorCPU only support single scale and zero_point. If we choose to','line_number':360,'multiline':False]['text':' output bias as Int8FCDNNLowPPackedWeightBlob with original layout,','line_number':361,'multiline':False]['text':' everything should still work for accelerator.','line_number':362,'multiline':False]['text':' ConvDNNLowPPackWeightOp','line_number':385,'multiline':False]['text':' The number of output channels','line_number':403,'multiline':False]['text':' The number of input channels per group','line_number':405,'multiline':False]['text':' The number of output channels','line_number':417,'multiline':False]['text':' The number of input channels per group','line_number':419,'multiline':False]['text':' NOLINTNEXTLINE(modernize-use-transparent-functors)','line_number':430,'multiline':False]['text':' dummy','line_number':443,'multiline':False]['text':' dummy','line_number':447,'multiline':False]['text':' dummy','line_number':461,'multiline':False]['text':' dummy','line_number':466,'multiline':False]['text':' Create tensor with the same shape but this new tensor shouldn't actually','line_number':496,'multiline':False]['text':' allocate memory for the tensor.','line_number':497,'multiline':False]['text':' This is just a convenient way to pass tensor shape information','line_number':498,'multiline':False]['text':' Assume KRSC layout','line_number':501,'multiline':False]['text':' The number of output channels','line_number':502,'multiline':False]['text':' The number of input channels per group','line_number':504,'multiline':False]['text':' Pre-compute column offsets','line_number':540,'multiline':False]['text':' This should happen before ExtractOutlierMatrix because W_quantized is','line_number':541,'multiline':False]['text':' changed in ExtractOutlierMatrix.','line_number':542,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':543,'multiline':False]['text':' Check if we should fallback to 32-bit accumulation.','line_number':548,'multiline':False]['text':' This check is only meaningful when engine is DNNLOWP_ACC16.','line_number':549,'multiline':False]['text':' In Skylake, acc16 is not faster when N or K is smaller than 128','line_number':556,'multiline':False]['text':' FIXME : code duplication with conv_dnnlowp_acc16_op.cc','line_number':557,'multiline':False]['text':' When nbits_in_non_outlier == 0, we fall back to acc32','line_number':583,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':609,'multiline':False]['text':' pmat','line_number':616,'multiline':False]['text':' acc32','line_number':628,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':630,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':634,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':639,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':646,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':654,'multiline':False]['text':' pmat','line_number':661,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':667,'multiline':False]['text':' NOLINTNEXTLINE(modernize-make-shared)','line_number':671,'multiline':False]['text':' const Int8FCDNNLowPPackedWeightBlob* int8_tensor =','line_number':701,'multiline':False]['text':'     reinterpret_cast<const Int8FCDNNLowPPackedWeightBlob*>(c);','line_number':702,'multiline':False]['text':' We forced the output type to be uint8_t since we know it always is.','line_number':703,'multiline':False]['text':' If it is going to be implemented elsewhere, we might need to change here.','line_number':704,'multiline':False]['text':' return (int8_tensor->original_tensor).dtype();','line_number':705,'multiline':False]['text':' const Int8ConvDNNLowPPackedWeightBlob* int8_tensor =','line_number':711,'multiline':False]['text':'     reinterpret_cast<const Int8ConvDNNLowPPackedWeightBlob*>(c);','line_number':712,'multiline':False]['text':' return (int8_tensor->original_tensor).dtype();','line_number':713,'multiline':False]['text':' Set up dim and shape','line_number':809,'multiline':False]['text':' not an offline tensor','line_number':815,'multiline':False]['text':' Set up dim and shape','line_number':858,'multiline':False]['text':' not an offline tensor','line_number':864,'multiline':False]['text':' Explicitly register TypeMeta','line_number':868,'multiline':False]['text':' Register DNNLOWP Type in caffe2 core','line_number':872,'multiline':False]['text':' namespace caffe2','line_number':981,'multiline':False]