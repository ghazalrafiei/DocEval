['text':' dilated convolution supported by some algorithms in cuDNN v6','line_number':40,'multiline':False]['text':' dilated grouped convolution supported in cuDNN v7.1','line_number':46,'multiline':False]['text':' verify TensorCore math is supported','line_number':58,'multiline':False]['text':' A helper function to set up the tensor Nd descriptor, depending on the order','line_number':100,'multiline':False]['text':' the group and the type given.','line_number':101,'multiline':False]['text':' top desc for bias add in case we do group convolution','line_number':420,'multiline':False]['text':' stored as FWD, dFILTER, dDATA','line_number':428,'multiline':False]['text':' Input: X, W, b','line_number':449,'multiline':False]['text':' Output: Y','line_number':450,'multiline':False]['text':' input: X, W, dY','line_number':496,'multiline':False]['text':' output: dW, db, and optionally dX','line_number':497,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':502,'multiline':False]['text':' Implementations','line_number':503,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':504,'multiline':False]['text':' Figure out the output shape','line_number':518,'multiline':False]['text':' Set up the cudnn algorithms & workspace if necessary','line_number':583,'multiline':False]['text':' We only need to divide dims by group_ when CUDNN version < 7.0','line_number':611,'multiline':False]['text':' see CUDA group convolution doc: https://fburl.com/dgj6dvpd','line_number':612,'multiline':False]['text':' Set the output','line_number':646,'multiline':False]['text':' Set the output with descriptor useful for bias addition in one run.','line_number':649,'multiline':False]['text':' enable cuDNN conv groups','line_number':683,'multiline':False]['text':' Even when FP16 compute is supported and requested, try FP32','line_number':692,'multiline':False]['text':' because it may be faster. However, if FP32 compute is specified,','line_number':693,'multiline':False]['text':' FP16 is not a suitable alternative - early out from the loop.','line_number':694,'multiline':False]['text':' When we do an exhaustive search, we will ignore the workspace','line_number':703,'multiline':False]['text':' size limit and simply go for the fastest algorithm. If you','line_number':704,'multiline':False]['text':' happen to run out of memory later, you will be on your own...','line_number':705,'multiline':False]['text':' no need to clean up workspace,','line_number':710,'multiline':False]['text':' Actually run the search.','line_number':713,'multiline':False]['text':' When set to fp32 compute, don't try fp16','line_number':736,'multiline':False]['text':' For FP32 compute, just use the best FP32 algorithm','line_number':743,'multiline':False]['text':' For FP16 compute, choose algo with fastest execution','line_number':746,'multiline':False]['text':' Get the convolution algorithm based on the workspace limit.','line_number':755,'multiline':False]['text':' Now, actually run the computation.','line_number':809,'multiline':False]['text':' Run directly through cuDNN if possible','line_number':810,'multiline':False]['text':' otherwise manually run through groups','line_number':829,'multiline':False]['text':' Bias','line_number':849,'multiline':False]['text':' Done.','line_number':865,'multiline':False]['text':' X','line_number':872,'multiline':False]['text':' W','line_number':873,'multiline':False]['text':' B','line_number':874,'multiline':False]['text':' Y','line_number':875,'multiline':False]['text':' X','line_number':878,'multiline':False]['text':' W','line_number':879,'multiline':False]['text':' B','line_number':880,'multiline':False]['text':' Y','line_number':881,'multiline':False]['text':' Set up the cudnn algorithms & workspace if necessary','line_number':994,'multiline':False]['text':' We only need to divide dims by group_ when CUDNN version < 7.0','line_number':1022,'multiline':False]['text':' see CUDA group convolution doc: https://fburl.com/dgj6dvpd','line_number':1023,'multiline':False]['text':' Set the output','line_number':1058,'multiline':False]['text':' Set the output with descriptor useful for bias addition in one run.','line_number':1061,'multiline':False]['text':' set cuDNN groups if appropriate','line_number':1102,'multiline':False]['text':' Choose dW algorithm','line_number':1107,'multiline':False]['text':' Even when FP16 compute is supported and requested, try FP32','line_number':1114,'multiline':False]['text':' because it may be faster. However, if FP32 compute is specified,','line_number':1115,'multiline':False]['text':' FP16 is not a suitable alternative - early out from the loop.','line_number':1116,'multiline':False]['text':' When we do an exhaustive search, we will ignore the workspace','line_number':1125,'multiline':False]['text':' size limit and simply go for the fastest algorithm. If you','line_number':1126,'multiline':False]['text':' happen to run out of memory later, you will be on your own...','line_number':1127,'multiline':False]['text':' We clean up the current workspace memory so that the forward','line_number':1129,'multiline':False]['text':' algorithm is free to allocate memory.','line_number':1130,'multiline':False]['text':' Actually run the search.','line_number':1131,'multiline':False]['text':' When set to fp32 compute, don't try fp16','line_number':1163,'multiline':False]['text':' For FP32 compute, just use the best FP32 algorithm','line_number':1170,'multiline':False]['text':' For FP16 compute, choose algo with fastest execution','line_number':1173,'multiline':False]['text':' choose backward algorithm for filter','line_number':1183,'multiline':False]['text':' Pick dX algo if needed','line_number':1207,'multiline':False]['text':' Even when FP16 compute is supported and requested, try FP32','line_number':1214,'multiline':False]['text':' because it may be faster. However, if FP32 compute is specified,','line_number':1215,'multiline':False]['text':' FP16 is not a suitable alternative - early out from the loop.','line_number':1216,'multiline':False]['text':' When set to fp32 compute, don't try fp16','line_number':1265,'multiline':False]['text':' For FP32 compute, just use the best FP32 algorithm','line_number':1272,'multiline':False]['text':' For FP16 compute, choose algo with fastest execution','line_number':1275,'multiline':False]['text':' get workspace size for backwards filter algorithm','line_number':1310,'multiline':False]['text':' get workspace size for backwards data algorithm','line_number':1341,'multiline':False]['text':' Now, actually run the computation.','line_number':1378,'multiline':False]['text':' Compute the gradient w.r.t. the input.','line_number':1408,'multiline':False]['text':' Compute the gradient w.r.t. the input.','line_number':1448,'multiline':False]['text':' TODO(Yangqing): a lot of the function contents are very similar. Consider','line_number':1472,'multiline':False]['text':' consolidating them.','line_number':1473,'multiline':False]['text':'  X','line_number':1477,'multiline':False]['text':' dY','line_number':1478,'multiline':False]['text':'  W','line_number':1479,'multiline':False]['text':'  b','line_number':1480,'multiline':False]['text':' dX','line_number':1481,'multiline':False]['text':' dW','line_number':1482,'multiline':False]['text':' db','line_number':1483,'multiline':False]['text':'  X','line_number':1486,'multiline':False]['text':' dY','line_number':1487,'multiline':False]['text':'  W','line_number':1488,'multiline':False]['text':'  b','line_number':1489,'multiline':False]['text':' dX','line_number':1490,'multiline':False]['text':' dW','line_number':1491,'multiline':False]['text':' db','line_number':1492,'multiline':False]['text':' namespace caffe2','line_number':1511,'multiline':False]