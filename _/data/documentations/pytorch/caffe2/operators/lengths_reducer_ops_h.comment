['text':' A templated class that implements SparseLengths[Sum,WeightedSum,Mean].','line_number':15,'multiline':False]['text':' output type','line_number':17,'multiline':False]['text':' supported input types, such as TensorTypes<float>','line_number':18,'multiline':False]['text':' Whether it is SparseLengthsWeightedSum','line_number':19,'multiline':False]['text':' Whether this is SparseLengthsMean','line_number':20,'multiline':False]['text':' USE_WEIGHT = true and USE_POSITIONAL_WEIGHT = true','line_number':22,'multiline':False]['text':' -> SparseLengthsPositionalWeightedSum','line_number':23,'multiline':False]['text':' Currently, we support float and at::Half inputs for input data type, and','line_number':37,'multiline':False]['text':' int32_t and int64_t for the index type.','line_number':38,'multiline':False]['text':' static if','line_number':82,'multiline':False]['text':' If this is the first call or block size has changed (should never','line_number':95,'multiline':False]['text':' happen actually), generate a kernel.','line_number':96,'multiline':False]['text':'prefetch distance','line_number':106,'multiline':True]['text':'use_offsets','line_number':108,'multiline':True]['text':'prefetch distance','line_number':116,'multiline':True]['text':'use_offsets','line_number':118,'multiline':True]['text':'prefetch distance','line_number':128,'multiline':True]['text':'use_offsets','line_number':130,'multiline':True]['text':'prefetch distance','line_number':138,'multiline':True]['text':'use_offsets','line_number':140,'multiline':True]['text':' delegate work to perfkernel that branches based on architecture','line_number':227,'multiline':False]['text':' scale_bias field is only used in SparseLengths8BitsRowwiseOp','line_number':237,'multiline':False]['text':' Data input.','line_number':244,'multiline':False]['text':' Weight input used in SparseLengthsWeightedSum','line_number':245,'multiline':False]['text':' 1 in SparseLengths[Sum,Mean] and','line_number':246,'multiline':False]['text':' 2 in SparseLengthsWeightedSum','line_number':247,'multiline':False]['text':' 2 in SparseLengths[Sum, Mean],','line_number':248,'multiline':False]['text':' 3 in SparseLengthsWeightedSum','line_number':249,'multiline':False]['text':' cumprod of i, used for index slice','line_number':283,'multiline':False]['text':' TODO: vectorization','line_number':293,'multiline':False]['text':' implement the functionality index_select(core, 1, ind_slice)','line_number':310,'multiline':False]['text':' ind: it stores the index to each tensor core','line_number':321,'multiline':False]['text':' bs: the number of indices','line_number':322,'multiline':False]['text':' GatherAllRows uses two steps to calculate the lengthsum functionality: 1) it uses tensor train','line_number':323,'multiline':False]['text':' to calculate the embedding for each index. 2) it sums the embedding for each bag.','line_number':324,'multiline':False]['text':' In Step 1), it batches all the indices together. Specifically, for every index, it uses the pre-computed','line_number':325,'multiline':False]['text':' ind of each tensor core to extract the corresponding slice of the core. Then it does gemm operation','line_number':326,'multiline':False]['text':' sequentially on the slices to produce the embedding result for each index.','line_number':327,'multiline':False]['text':' In Step 2), it takes the embedding computed in step 1) and apply the sum operation for each bag.','line_number':328,'multiline':False]['text':' compute the largest memory consumption of intermediate result','line_number':337,'multiline':False]['text':' TODO: dynamic allocation size: cur_rows*factor_j[i]*ranks[i+1]','line_number':338,'multiline':False]['text':' and also explore the contiguous memory storage for res and int_res','line_number':339,'multiline':False]['text':' Store the matrix A','line_number':345,'multiline':False]['text':' Store the intermediate result in each layer','line_number':347,'multiline':False]['text':' slice cur','line_number':358,'multiline':False]['text':' save the intermediate output for backward path','line_number':392,'multiline':False]['text':' shape for the core','line_number':393,'multiline':False]['text':' reduction and store back to output','line_number':407,'multiline':False]['text':' store the tmp_sum into output','line_number':418,'multiline':False]['text':' Store the factor index for backward path','line_number':466,'multiline':False]['text':' Store the factorized index for each core','line_number':471,'multiline':False]['text':' implement the gradient op for TTLengthSumGradient op','line_number':498,'multiline':False]['text':' restore the arguments from shape','line_number':513,'multiline':False]['text':' size of each batch','line_number':561,'multiline':False]['text':' construct the ranks','line_number':564,'multiline':False]['text':' expand the gradient into all indices','line_number':565,'multiline':False]['text':' =======================================================','line_number':579,'multiline':False]['text':' Calculate dCore2_data:','line_number':580,'multiline':False]['text':' 1) Transpose core1_out and multiply iwth core2_out_grad','line_number':581,'multiline':False]['text':' 2)  add to dCore2_data','line_number':582,'multiline':False]['text':' const T* core1_out_p[bs];','line_number':586,'multiline':False]['text':' M','line_number':597,'multiline':False]['text':' N','line_number':598,'multiline':False]['text':' K','line_number':599,'multiline':False]['text':' update the corresponding slice','line_number':607,'multiline':False]['text':' Calculate core1_out_grad','line_number':624,'multiline':False]['text':' M','line_number':638,'multiline':False]['text':' N','line_number':639,'multiline':False]['text':' K','line_number':640,'multiline':False]['text':' =======================================================','line_number':648,'multiline':False]['text':' Calculate dCore1_data:','line_number':649,'multiline':False]['text':' 1) Transpose core1_out_grad and multiply with core0_out','line_number':650,'multiline':False]['text':' 2) Transpose the result and then add to dCore1_data','line_number':651,'multiline':False]['text':' M','line_number':665,'multiline':False]['text':' N','line_number':666,'multiline':False]['text':' K','line_number':667,'multiline':False]['text':' update the corresponding slice','line_number':675,'multiline':False]['text':' Calculate core0_out_grad','line_number':691,'multiline':False]['text':' M','line_number':705,'multiline':False]['text':' N','line_number':706,'multiline':False]['text':' K','line_number':707,'multiline':False]['text':' namespace caffe2','line_number':725,'multiline':False]