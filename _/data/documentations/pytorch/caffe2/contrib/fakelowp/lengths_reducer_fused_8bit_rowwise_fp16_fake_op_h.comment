['text':' Subtract 8 from the #columns of data for the 4 bytes for scale and 4','line_number':60,'multiline':False]['text':' bytes for bias that we use in the fused representation (per row).','line_number':61,'multiline':False]['text':' Copied from Fused8BitRowwiseEmbeddingLookupGenericSlow in','line_number':65,'multiline':False]['text':' fused_8bit_rowwise_embedding_lookup.cc','line_number':66,'multiline':False]['text':' block_size is the number of elements and fused_block_size is the size of','line_number':82,'multiline':False]['text':' an entire row, including scale and bias.','line_number':83,'multiline':False]['text':' Only do double buffer accumulation when block size is even','line_number':100,'multiline':False]['text':' Fake fp16 rounding of weight','line_number':116,'multiline':False]['text':' Vendor might store scale as s' = 1 / s which implies b' = b / s','line_number':124,'multiline':False]['text':' We do      x = x_q * s + b','line_number':125,'multiline':False]['text':' Vendor does x = (x_q + b') / s'','line_number':126,'multiline':False]['text':' Solving these equations yields to the results above','line_number':127,'multiline':False]['text':' Fake fp16 rounding of scale and bias','line_number':142,'multiline':False]['text':' Fake fp16 rounding of scale and bias','line_number':148,'multiline':False]['text':' Fake fp16 rounding of input/ it is already ints','line_number':153,'multiline':False]['text':' bias *= weight;','line_number':161,'multiline':False]['text':' Fake fp16 rounding of scale x input + bias','line_number':171,'multiline':False]['text':' Accumulate w x (scale x input + bias) to output','line_number':179,'multiline':False]['text':' Fake fp16 rounding of w x scale x input','line_number':211,'multiline':False]['text':' Fake fp16 rounding of w x scale x input + w x bias','line_number':221,'multiline':False]['text':' Accumulate w x scale x input + w x bias to output','line_number':228,'multiline':False]['text':' Fake fp16 rounding of out + (w x scale x input + w x bias)','line_number':235,'multiline':False]['text':' Fake fp16 rounding of scale and out','line_number':284,'multiline':False]['text':' hack: context is not really used','line_number':294,'multiline':False]['text':' namespace caffe2','line_number':312,'multiline':False]