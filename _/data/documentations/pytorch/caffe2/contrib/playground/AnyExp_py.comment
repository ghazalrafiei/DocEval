['text':' instantiate logger outside of distributed operators may trigger error','line_number':16,'multiline':False]['text':' logger need to be created in each idividual operator instead.','line_number':17,'multiline':False]['text':' at this point, the intance of this class becomes many instances','line_number':120,'multiline':False]['text':' running on different machines.  Most of their attributes are same,','line_number':121,'multiline':False]['text':' but the shard_ids are different.','line_number':122,'multiline':False]['text':' log.info('metric.meter_kargs {}'.format(metric.meter_kargs))','line_number':146,'multiline':False]['text':' log.info('type meter_kargs {}'.format(type(metric.meter_kargs)))','line_number':147,'multiline':False]['text':' @abstractmethod','line_number':222,'multiline':False]['text':' def gen_checkpoint_path(self, is_checkpoint, epoch):','line_number':223,'multiline':False]['text':'     pass','line_number':224,'multiline':False]['text':' @abstractmethod','line_number':282,'multiline':False]['text':' "shared model" will only keep model parameters for cpu_0 or gpu_0','line_number':325,'multiline':False]['text':' will cause issue when initialize each gpu_0, gpu_1, gpu_2 ...','line_number':326,'multiline':False]['text':' shared_model=(self.opts['distributed']['device'] == 'cpu'),','line_number':327,'multiline':False]['text':' log.info("Current blobs in workspace: {}".format(workspace.Blobs()))','line_number':332,'multiline':False]['text':' for op in model.net.Proto().op:','line_number':337,'multiline':False]['text':'     log.info('op type engine {} {}'.format(op.type, op.engine))','line_number':338,'multiline':False]['text':' for op in model.net.Proto().op:','line_number':344,'multiline':False]['text':'     log.info('after CreateNet op type engine {} {}'.','line_number':345,'multiline':False]['text':'         format(op.type, op.engine))','line_number':346,'multiline':False]