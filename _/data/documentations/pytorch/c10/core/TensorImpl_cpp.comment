['text':' Yes, I know this looks really weird.  But I don't really have a choice as','line_number':47,'multiline':False]['text':' long as this function returns a const reference to Tensor.  I'm not','line_number':48,'multiline':False]['text':' really sure how I would have designed this API differently, but it','line_number':49,'multiline':False]['text':' is not so easy to fix right now because the mutable counterpart of','line_number':50,'multiline':False]['text':' this function must keep working so that "x.grad() = ..." keeps working','line_number':51,'multiline':False]['text':' (part of public API).','line_number':52,'multiline':False]['text':' See TensorImpl::grad() above for explanation about the line below','line_number':61,'multiline':False]['text':' Use std::forward to suppress static analyzer false positive.','line_number':83,'multiline':False]['text':' [Note: Python key removal]','line_number':90,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':91,'multiline':False]['text':' In most constructors for TensorImpl, you will see Python and','line_number':92,'multiline':False]['text':' PythonTLSSnapshot keys are removed from the passed in DispatchKeySet.  Why?','line_number':93,'multiline':False]['text':'','line_number':94,'multiline':False]['text':' INVARIANT: Python and PythonTLSSnapshot dispatch keys are set iff PyObject','line_number':95,'multiline':False]['text':' for the Tensor has a nontrivial __torch_dispatch__ implementation.','line_number':96,'multiline':False]['text':'','line_number':97,'multiline':False]['text':' When a fresh TensorImpl is created, there is *no* PyObject (this only gets','line_number':98,'multiline':False]['text':' initialized lazily at the first point in time the Tensor passes into Python).','line_number':99,'multiline':False]['text':' So we would violate the invariant.','line_number':100,'multiline':False]['text':'','line_number':101,'multiline':False]['text':' In practice, what will happen shortly afterwards is that the TensorImpl','line_number':102,'multiline':False]['text':' will get its PyObject initialized by Tensor._make_subclass; at this point','line_number':103,'multiline':False]['text':' the Python and PythonTLSSnapshot dispatch keys will be set and all is well.','line_number':104,'multiline':False]['text':' The point is to delay the dispatch key setting until that point.','line_number':105,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':107,'multiline':False]['text':' See [Note: Python key removal]','line_number':118,'multiline':False]['text':' Inference tensor doesn't have version counter.','line_number':120,'multiline':False]['text':'version=','line_number':122,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':126,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':133,'multiline':False]['text':' UndefinedTensorImpl is a singleton, so we skip logging it','line_number':149,'multiline':False]['text':' XXX: if updating keyset logic here also update','line_number':153,'multiline':False]['text':' _change_backend_component_keys','line_number':154,'multiline':False]['text':' TODO: be more explicit about the full key set at call sites so we','line_number':157,'multiline':False]['text':' don't have to keep recomputing it here','line_number':158,'multiline':False]['text':' See [Note: Python key removal]','line_number':163,'multiline':False]['text':' Inference tensor doesn't have autograd related keys.','line_number':166,'multiline':False]['text':' See Note [Expected TLS state in InferenceMode] for why we exclude','line_number':168,'multiline':False]['text':' Autograd & ADInplaceOrView keys. Normally key_set only contains backend','line_number':169,'multiline':False]['text':' keys but we do the substraction here to make sure.','line_number':170,'multiline':False]['text':' TODO: Ideally we only add AutogradBackend key when the tensor requires','line_number':173,'multiline':False]['text':' grad.','line_number':174,'multiline':False]['text':'       See Note [Dream: skip VariableType kernel when requires_grad=false]','line_number':175,'multiline':False]['text':' Inference tensor doesn't have version counter.','line_number':179,'multiline':False]['text':'version=','line_number':181,'multiline':True]['text':' we would also like to check that non-cpu devices have an index, but some','line_number':183,'multiline':False]['text':' Caffe2 operators create Storages with default devices.','line_number':184,'multiline':False]['text':' following logic TensorImpl::TensorImpl, update the BackendComponent related','line_number':191,'multiline':False]['text':' keys to correspond to device','line_number':192,'multiline':False]['text':' TODO: Autocoast should be a per-backend functionality key, once that change','line_number':194,'multiline':False]['text':' is made this key swap will not be necessary.','line_number':195,'multiline':False]['text':' See note [Removing keys from DispatchKeySet Only Affects Functionality','line_number':200,'multiline':False]['text':' Keys]','line_number':201,'multiline':False]['text':' If needed, we will free the data. the next mutable_data() call','line_number':207,'multiline':False]['text':' will create the data storage.','line_number':208,'multiline':False]['text':' If tensor is reserved then don't claim its memory unless nbytes()','line_number':211,'multiline':False]['text':' is smaller than new size','line_number':212,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':304,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':313,'multiline':False]['text':' TODO: fix this','line_number':408,'multiline':False]['text':' return layout_default();','line_number':411,'multiline':False]['text':' TODO: fix this','line_number':416,'multiline':False]['text':' Setting requires_grad to true on inference tensor outside InferenceMode','line_number':450,'multiline':False]['text':' is forbidden.  Ideally it would also be illegal inside InferenceMode.','line_number':451,'multiline':False]['text':' But there's no way that we can directly allocate a tensor to have','line_number':452,'multiline':False]['text':' requires_grad = true in C++ constructor so set_requires_grad is widely','line_number':453,'multiline':False]['text':' used in C++ frontend. Forbidding it inside InferenceMode will force users','line_number':454,'multiline':False]['text':' to delete these setter code in their code which is not ideal.','line_number':455,'multiline':False]['text':' NB: In principle, setting requires_grad to false could result in','line_number':464,'multiline':False]['text':' the AutogradMeta becoming equal to a default constructed state,','line_number':465,'multiline':False]['text':' in which case we could apply the nullptr AutogradMeta optimization','line_number':466,'multiline':False]['text':' (see autograd_meta_ docs).  But we don't do this right now.  Note','line_number':467,'multiline':False]['text':' that it is unsound to unconditionally set AutogradMeta to false','line_number':468,'multiline':False]['text':' when you set requires_grad to False, as there may be nontrivial','line_number':469,'multiline':False]['text':' information content in the other fields; for example, we may','line_number':470,'multiline':False]['text':' have set the string name for a Variable, or there may be hooks','line_number':471,'multiline':False]['text':' registered for it.','line_number':472,'multiline':False]['text':' NB: autograd_meta may be null!  That just means it's the default','line_number':484,'multiline':False]['text':' constructor','line_number':485,'multiline':False]['text':' NB: Might return null!','line_number':490,'multiline':False]['text':' TODO: do we have to exclude after Python dispatch key set?','line_number':500,'multiline':False]['text':' otherwise just copy the TensorImpl and not the PyObject.  Since','line_number':516,'multiline':False]['text':' the interpreter is dead no one can call us out on it','line_number':517,'multiline':False]['text':' No need to populate Storage; copy_tensor_metadata will do it for us.','line_number':519,'multiline':False]['text':'src_impl=','line_number':524,'multiline':True]['text':'dest_impl=','line_number':525,'multiline':True]['text':'version_counter=','line_number':526,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':527,'multiline':True]['text':' This function copies all of the metadata from the src tensor except for:','line_number':545,'multiline':False]['text':' - key_set_','line_number':546,'multiline':False]['text':' - storage_','line_number':547,'multiline':False]['text':' - storage_access_should_throw_','line_number':548,'multiline':False]['text':' - sizes_strides_policy_','line_number':549,'multiline':False]['text':' - version_counter_','line_number':550,'multiline':False]['text':' - allow_tensor_metadata_change_','line_number':551,'multiline':False]['text':' The idea is that if we have a "wrapper tensor" (like in functionalization),','line_number':552,'multiline':False]['text':' all of the above are properties that the wrapper will want to customize,','line_number':553,'multiline':False]['text':' while everything else should be mirrored between the wrapper and the inner','line_number':554,'multiline':False]['text':' tensor.','line_number':555,'multiline':False]['text':' NB: symbolic sizes and strides are copied as is custom policy, but python','line_number':582,'multiline':False]['text':' policy is NOT (you have no Python object to dispatch to!)','line_number':583,'multiline':False]['text':' NB: subclass relevant policy doesn't have to be copied; the','line_number':584,'multiline':False]['text':' constructor sets this up','line_number':585,'multiline':False]['text':' First call the generic copy function','line_number':596,'multiline':False]['text':' Then copy everything else (see the comment at copy_generic_tensor_metadata','line_number':598,'multiline':False]['text':' for the list of metadata that it does not directly copy).','line_number':599,'multiline':False]['text':' Copying tensor metadata doesn't change the PyObject (maybe','line_number':601,'multiline':False]['text':' it should), which means that we have to preserve whatever the','line_number':602,'multiline':False]['text':' original Python keyset was (as it's associated with the PyObject','line_number':603,'multiline':False]['text':' being a tensor subclass or not)','line_number':604,'multiline':False]['text':' TODO: In the ideal end state, it's okay to set disabled version_counter','line_number':619,'multiline':False]['text':' on inference tensor since it's a no-op. This requires refactor on call','line_number':620,'multiline':False]['text':' sites.','line_number':621,'multiline':False]['text':' Legacy Caffe2 operations','line_number':639,'multiline':False]['text':' The following copy uses the current (thread local) stream for copying','line_number':680,'multiline':False]['text':' and also takes the GPU id from the device() field passed in.','line_number':681,'multiline':False]['text':'','line_number':682,'multiline':False]['text':' TODO: Potentially more enforcements are necessary to avoid accidental','line_number':683,'multiline':False]['text':' switch to sync copy if the currently set device is wrong.','line_number':684,'multiline':False]['text':'','line_number':685,'multiline':False]['text':' Specifically, we might need to switch to a different context device','line_number':686,'multiline':False]['text':' here explicitly to avoid relying on user synchronizing things','line_number':687,'multiline':False]['text':' properly.','line_number':688,'multiline':False]['text':' non-blocking','line_number':695,'multiline':False]['text':' TODO: eliminate newCapacity.','line_number':711,'multiline':False]['text':' Old data is discarded','line_number':720,'multiline':False]['text':' Allocate new memory but don't copy over the data','line_number':726,'multiline':False]['text':' TODO(jiayq): remove the following warning after pending diffs','line_number':750,'multiline':False]['text':' stabilize.','line_number':751,'multiline':False]['text':' We'll detach from the old Storage and create a new one','line_number':760,'multiline':False]['text':' Right now, we are assuming the device_type are the same, since it is','line_number':771,'multiline':False]['text':' inherently the same in the non-templatized code. We should probably add','line_number':772,'multiline':False]['text':' an assert here which might affect perf a little bit.','line_number':773,'multiline':False]['text':' It is possible that the source tensor hasn't called mutable_data() yet,','line_number':777,'multiline':False]['text':' in which case ShareData() doesn't make much sense since we don't really','line_number':778,'multiline':False]['text':' know what to share yet.','line_number':779,'multiline':False]['text':' TODO: Add the assert after all uninitialized states are eliminated','line_number':780,'multiline':False]['text':' TORCH_CHECK(src.dtype_initialized(),','line_number':781,'multiline':False]['text':'            "Source tensor don't have a data type (did you call','line_number':782,'multiline':False]['text':'            mutable_data<T> on the tensor?)");','line_number':783,'multiline':False]['text':' Finally, do sharing.','line_number':791,'multiline':False]['text':' Since we create new Storage whenever we need to change data_type/nbytes
   * this still keeps the original semantics
   ','line_number':792,'multiline':True]['text':' Create a new Storage','line_number':821,'multiline':False]['text':'allocator=','line_number':826,'multiline':True]['text':'resizable=','line_number':827,'multiline':True]['text':' NB: this doesn't check that the sizes/strides/offset are in bound for the','line_number':842,'multiline':False]['text':' storage, and furthermore, it CANNOT do so as in some cases we temporarily','line_number':843,'multiline':False]['text':' violate invariants by first setting sizes/strides, and then updating the','line_number':844,'multiline':False]['text':' storage','line_number':845,'multiline':False]['text':' NB: storage_offset guaranteed to be positive','line_number':853,'multiline':False]['text':' calls refresh_contiguous()','line_number':912,'multiline':False]['text':' TODO: figure out if the non-symint version can also devirtualize;','line_number':920,'multiline':False]['text':' the last time we tried it was probably a narrowing problem','line_number':921,'multiline':False]['text':' Cleaning warning messages, no need to break as TORCH_CHECK(false)','line_number':950,'multiline':False]['text':' terminates flow.','line_number':951,'multiline':False]['text':' break;','line_number':952,'multiline':False]['text':' recompute contiguous flag, as currently NHWC/NCHW flags are not mutually','line_number':956,'multiline':False]['text':' exclusive see #24090','line_number':957,'multiline':False]['text':' hard code some known true settings, for unbacked case','line_number':959,'multiline':False]['text':' TODO: avoid chundering into the guards for computing these','line_number':960,'multiline':False]['text':' namespace','line_number':988,'multiline':False]['text':' namespace impl','line_number':1000,'multiline':False]['text':' namespace c10','line_number':1002,'multiline':False]