['text':' A global boolean variable to control whether we free memory when a Tensor','line_number':32,'multiline':False]['text':' is shrunk to a smaller size. As a result, a Tensor is always going to','line_number':33,'multiline':False]['text':' keep the memory allocated for its maximum capacity reshaped to so far.','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' This parameter is respected "upper-case" methods which call Resize()','line_number':36,'multiline':False]['text':' (e.g., CopyFrom, ResizeLike); it is NOT respected by Tensor::resize_','line_number':37,'multiline':False]['text':' or ShrinkTo, both of which guarantee to never to free memory.','line_number':38,'multiline':False]['text':' Since we can have high variance in blob memory allocated across different','line_number':41,'multiline':False]['text':' inputs in the same run, we will shrink the blob only if the memory gain','line_number':42,'multiline':False]['text':' is larger than this flag in bytes.  This only applies to functions which','line_number':43,'multiline':False]['text':' respect caffe2_keep_on_shrink.','line_number':44,'multiline':False]['text':' namespace at','line_number':55,'multiline':False]['text':'*
 * A utility function to convert vector<int> to vector<int64_t>.
 ','line_number':59,'multiline':True]['text':'*
 * Return product of all dimensions starting from k
 ','line_number':66,'multiline':True]['text':' Product of all dims up to k (not including dims[k])','line_number':77,'multiline':False]['text':' Product of all dims between k and l (not including dims[k] and dims[l])','line_number':87,'multiline':False]['text':' Wrap around axis_index if it is negative, s.t., -1 is the last dim','line_number':103,'multiline':False]['text':'
 * A Context that will call extra placement deleter during
 * deconstruction.
 *
 * Accept a already constructed DataPtr and store it as member
 * during destruction, we'll call extra deleter on the underlying
 * data pointer before the DataPtr is destructed.
 * `data_ptr_` owns the memory.
 ','line_number':115,'multiline':True]['text':' original memory will be freed when data_ptr_ is destructed','line_number':142,'multiline':False]['text':' Unfortunately, the definition of AutogradMeta lives in a separate','line_number':165,'multiline':False]['text':' compilation unit than TensorImpl (libtorch.so versus libc10.so)','line_number':166,'multiline':False]['text':' which means that we cannot construct an AutogradMeta from TensorImpl,','line_number':167,'multiline':False]['text':' not even from the cpp file.  So we have to indirect it through a factory','line_number':168,'multiline':False]['text':' function which will be initialized when we load libtorch.so.','line_number':169,'multiline':False]['text':' This method is the dumbest method.  But I don't have access','line_number':174,'multiline':False]['text':' to Tensor (not TensorImpl) which is undefined in this header.','line_number':175,'multiline':False]['text':' namespace impl','line_number':188,'multiline':False]['text':' For ease of copy pasting','line_number':202,'multiline':False]['text':'*
 * This structure is intended to hold additional metadata of the specific device
 * backend.
 *','line_number':212,'multiline':True]['text':' NOTE [ Version Counter Sharing ]','line_number':268,'multiline':False]['text':'','line_number':269,'multiline':False]['text':' Every Tensor has a version counter. Version counters are incremented whenever','line_number':270,'multiline':False]['text':' the data or size of a tensor changes through in-place Variable operations.','line_number':271,'multiline':False]['text':' Version counters are used to detect modifications to saved variables which','line_number':272,'multiline':False]['text':' would result in incorrect gradient calculations. Version counters may be','line_number':273,'multiline':False]['text':' shared between Variables:','line_number':274,'multiline':False]['text':'','line_number':275,'multiline':False]['text':' 1. A view shares the version counter of the base Variable,','line_number':276,'multiline':False]['text':' 2. `x.detach()` shares the version counter of `x`,','line_number':277,'multiline':False]['text':' 3. Unpacked saved variables share the version counter of the source.','line_number':278,'multiline':False]['text':'','line_number':279,'multiline':False]['text':' Version counters are not shared in these scenarios:','line_number':280,'multiline':False]['text':'','line_number':281,'multiline':False]['text':' 1. When we replace a `Variable`'s underlying `Tensor` by calling','line_number':282,'multiline':False]['text':' `set_data(...)`,','line_number':283,'multiline':False]['text':' 2. `x.data` does not share the version counter of `x`. (See discussion at','line_number':284,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/5396)','line_number':285,'multiline':False]['text':'','line_number':286,'multiline':False]['text':' Question: Why do we put the version counter in TensorImpl instead of','line_number':287,'multiline':False]['text':' AutogradMeta?','line_number':288,'multiline':False]['text':'','line_number':289,'multiline':False]['text':' Answer: After the Variable/Tensor merge, a tensor will not have AutogradMeta','line_number':290,'multiline':False]['text':' when its `requires_grad_` is false, but when we use this tensor in the','line_number':291,'multiline':False]['text':' forward pass of a function that requires saving this tensor for backward, we','line_number':292,'multiline':False]['text':' need to keep track of this tensor's version to make sure it's always valid in','line_number':293,'multiline':False]['text':' the autograd graph.','line_number':294,'multiline':False]['text':'','line_number':295,'multiline':False]['text':' To achieve this goal, we put the version counter in TensorImpl instead of','line_number':296,'multiline':False]['text':' AutogradMeta, and have it always be available. This allows us to have the','line_number':297,'multiline':False]['text':' optimization of not carrying AutogradMeta when a tensor doesn't require','line_number':298,'multiline':False]['text':' gradient.','line_number':299,'multiline':False]['text':'','line_number':300,'multiline':False]['text':' A hypothetical alternative way to achieve this goal is to initialize','line_number':301,'multiline':False]['text':' AutogradMeta and create the version counter for the non-requires-grad tensor','line_number':302,'multiline':False]['text':' only when it's saved for backward. However, since saving a tensor for','line_number':303,'multiline':False]['text':' backward happens in the forward pass, and our invariant is that forward pass','line_number':304,'multiline':False]['text':' needs to be thread-safe, lazy-initializing AutogradMeta when saving a tensor','line_number':305,'multiline':False]['text':' can introduce race conditions when we are running the forward pass in','line_number':306,'multiline':False]['text':' multi-thread scenarios, thus making the forward pass not thread-safe anymore,','line_number':307,'multiline':False]['text':' which breaks the invariant.','line_number':308,'multiline':False]['text':' Note [Disabled VariableVersion]','line_number':318,'multiline':False]['text':' VariableVersion struct has an intrusive_ptr pointing VersionCounter struct','line_number':319,'multiline':False]['text':' with an atomic variable. Thus `VariableVersion(/*version=*/0)` is not as','line_number':320,'multiline':False]['text':' cheap as we expected. In some cases constructing a VariableVersion with','line_number':321,'multiline':False]['text':' version 0 is not necessary so we add a cheap constructor which','line_number':322,'multiline':False]['text':' doesn't allocate the intrusive_ptr.','line_number':323,'multiline':False]['text':' Example use cases are:','line_number':324,'multiline':False]['text':'  - Inference tensors don't track version counter, so they'll just always','line_number':325,'multiline':False]['text':'    have disabled VariableVersion.','line_number':326,'multiline':False]['text':'  - In SavedVariable class we override version_counter_ inside its','line_number':327,'multiline':False]['text':'  constructor','line_number':328,'multiline':False]['text':'    so that we can use the cheap constructor there.','line_number':329,'multiline':False]['text':' It's okay to return true even for inference tensor which','line_number':331,'multiline':False]['text':' doesn't have version counter enabled.','line_number':332,'multiline':False]['text':' We want to be permissive here since in many cases (e.g. make_variable)','line_number':333,'multiline':False]['text':' we can std::move a TensorImpl if there's no other uses which saves us','line_number':334,'multiline':False]['text':' an additional TensorImpl allocation.','line_number':335,'multiline':False]['text':' NOTE: As of C++11 and 14, default-constructing a std::atomic variable','line_number':339,'multiline':False]['text':' leaves it in a persistently undefined state. See','line_number':340,'multiline':False]['text':' https://cplusplus.github.io/LWG/issue2334.','line_number':341,'multiline':False]['text':' Note [Inplace update inference tensor]','line_number':350,'multiline':False]['text':' 1. Inplace update to inference tensor is forbidden in normal mode.','line_number':351,'multiline':False]['text':'   For example:','line_number':352,'multiline':False]['text':'     inference_tensor.copy_(normal_tensor_requires_grad)','line_number':353,'multiline':False]['text':'   This inplace makes inference_tensor have requires_grad=True and','line_number':354,'multiline':False]['text':'   have a grad_fn.  This is bad because views of `inference_tensor`','line_number':355,'multiline':False]['text':'   created in InferenceMode won't be able to know the grad_fn since','line_number':356,'multiline':False]['text':'   their ViewMeta were not recorded. To match NoGradMode behavior','line_number':357,'multiline':False]['text':'   that "inplace update to a view created in NoGradMode raise an error",','line_number':358,'multiline':False]['text':'   we just ban inplace update to inference tensor since we can't tell','line_number':359,'multiline':False]['text':'   if an inference tensor is a view created in InferenceMode.','line_number':360,'multiline':False]['text':'','line_number':361,'multiline':False]['text':'   Note that views of normal tensor created in InferenceMode has proper','line_number':362,'multiline':False]['text':'   ViewMeta so that they're aware of the grad_fn correctly.','line_number':363,'multiline':False]['text':'','line_number':364,'multiline':False]['text':' 2. Inplace update to inference tensor in inference tensor doesn't bump','line_number':365,'multiline':False]['text':'    version counter.','line_number':366,'multiline':False]['text':'    * It either doesn't call bump() by skipping ADInplaceOrView kernel,','line_number':367,'multiline':False]['text':'      - e.g. inference_tensor.add_(1)','line_number':368,'multiline':False]['text':'    * or bump() is a no-op for inference tensor.','line_number':369,'multiline':False]['text':'      - e.g. inference_tensor.add_(normal_tensor)','line_number':370,'multiline':False]['text':' TODO: Replace the link to the documentation once it's available.','line_number':372,'multiline':False]['text':' Inference tensor doesn't have version counter so it shouldn't be','line_number':392,'multiline':False]['text':' accessed.','line_number':393,'multiline':False]['text':' Forward declaration of TensorImpl needed for forward declaration of','line_number':401,'multiline':False]['text':' C10_TensorImpl_Size_Check_Dummy_Class','line_number':402,'multiline':False]['text':'*
 * NOTE: Some TensorImpl methods are small and not overridden in the
 * PyTorch codebase itself, but may theoretically need to be
 * overridden by third-party TensorImpl subclasses. This macro allows
 * users that need maximum performance and don't need these extension
 * points to disable them with a build-time flag. (In particular,
 * XLA's XLATensorImpl currently overrides these methods, so we can't
 * enable this flag by default.)
 ','line_number':405,'multiline':True]['text':'*
 * The low-level representation of a tensor, which contains a pointer
 * to a storage (which contains the actual data) and metadata (e.g., sizes and
 * strides) describing this particular view of the data as a tensor.
 *
 * Some basic characteristics about our in-memory representation of
 * tensors:
 *
 *  - It contains a pointer to a storage struct (Storage/StorageImpl)
 *    which contains the pointer to the actual data and records the
 *    data type and device of the view.  This allows multiple tensors
 *    to alias the same underlying data, which allows to efficiently
 *    implement differing *views* on a tensor.
 *
 *  - The tensor struct itself records view-specific metadata about
 *    the tensor, e.g., sizes, strides and offset into storage.
 *    Each view of a storage can have a different size or offset.
 *
 *  - This class is intrusively refcounted.  It is refcounted so that
 *    we can support prompt deallocation of large tensors; it is
 *    intrusively refcounted so that we can still perform reference
 *    counted operations on raw pointers, which is often more convenient
 *    when passing tensors across language boundaries.
 *
 *  - For backwards-compatibility reasons, a tensor may be in an
 *    uninitialized state.  A tensor may be uninitialized in the following
 *    two ways:
 *
 *      - A tensor may be DTYPE UNINITIALIZED.  A tensor of this
 *        form has an uninitialized dtype.  This situation most
 *        frequently arises when a user writes Tensor x(CPU).  The dtype
 *        is subsequently initialized when mutable_data<T>() is
 *        invoked for the first time.
 *
 *      - A tensor may be STORAGE UNINITIALIZED.  A tensor of this form
 *        has non-zero size, but has a storage with a null data pointer.
 *        This situation most frequently arises when a user calls
 *        Resize() or FreeMemory().  This is because Caffe2 historically
 *        does lazy allocation: allocation of data doesn't occur until
 *        mutable_data<T>() is invoked.  A tensor with zero size is
 *        always storage initialized, because no allocation is necessary
 *        in this case.
 *
 *    All combinations of these two uninitialized states are possible.
 *    Consider the following transcript in idiomatic Caffe2 API:
 *
 *      Tensor x(CPU); // x is storage-initialized, dtype-UNINITIALIZED
 *      x.Resize(4); // x is storage-UNINITIALIZED, dtype-UNINITIALIZED
 *      x.mutable_data<float>(); // x is storage-initialized, dtype-initialized
 *      x.FreeMemory(); // x is storage-UNINITIALIZED, dtype-initialized.
 *
 *    All other fields on tensor are always initialized.  In particular,
 *    size is always valid. (Historically, a tensor declared as Tensor x(CPU)
 *    also had uninitialized size, encoded as numel == -1, but we have now
 *    decided to default to zero size, resulting in numel == 0).
 *
 *    Uninitialized storages MUST be uniquely owned, to keep our model
 *    simple.  Thus, we will reject operations which could cause an
 *    uninitialized storage to become shared (or a shared storage to
 *    become uninitialized, e.g., from FreeMemory).
 *
 *    In practice, tensors which are storage-UNINITIALIZED and
 *    dtype-UNINITIALIZED are *extremely* ephemeral: essentially,
 *    after you do a Resize(), you basically always call mutable_data()
 *    immediately afterwards.  Most functions are not designed to
 *    work if given a storage-UNINITIALIZED, dtype-UNINITIALIZED tensor.
 *
 *    We intend to eliminate all uninitialized states, so that every
 *    tensor is fully initialized in all fields.  Please do not write new code
 *    that depends on these uninitialized states.
 ','line_number':420,'multiline':True]['text':' Note [Enum ImplType]','line_number':494,'multiline':False]['text':' This enum is temporary. In the followup refactor we should','line_number':495,'multiline':False]['text':' think about how to specialize TensorImpl creation for view','line_number':496,'multiline':False]['text':' tensors. Currently we only special case its key_set_ but','line_number':497,'multiline':False]['text':' there's also potential to share version_counter_ directly','line_number':498,'multiline':False]['text':' without creating first and then override in as_view.','line_number':499,'multiline':False]['text':'*
   * Construct a 1-dim 0-size tensor backed by the given storage.
   ','line_number':502,'multiline':True]['text':' See Note [Enum ImplType]','line_number':510,'multiline':False]['text':'*
   * Construct a 1-dim 0 size tensor that doesn't have a storage.
   ','line_number':517,'multiline':True]['text':' Legacy constructors so I don't have to go update call sites.','line_number':525,'multiline':False]['text':' TODO: When Variable is added, delete these constructors','line_number':526,'multiline':False]['text':' This constructor is private, because the data_type is redundant with','line_number':542,'multiline':False]['text':' storage.  Still, we pass it in separately because it's easier to write','line_number':543,'multiline':False]['text':' the initializer list if we're not worried about storage being moved out','line_number':544,'multiline':False]['text':' from under us.','line_number':545,'multiline':False]['text':'*
   * Release (decref) storage, and any other external allocations.  This
   * override is for `intrusive_ptr_target` and is used to implement weak
   * tensors.
   ','line_number':558,'multiline':True]['text':'*
   * Return the DispatchKeySet corresponding to this Tensor, specifying
   * all of the DispatchKeys that this Tensor identifies as.  This is the
   * information used to dispatch operations on this tensor.
   ','line_number':566,'multiline':True]['text':' NOTE: The general recipe for customizable methods is that the fastpath','line_number':578,'multiline':False]['text':' function (e.g., sizes()) does an unlikely policy test, and if doesn't','line_number':579,'multiline':False]['text':' trigger, it does the fast path implementation with no checks and going','line_number':580,'multiline':False]['text':' directly to on-TensorImpl fields.  In particular, you never need to','line_number':581,'multiline':False]['text':' check ExtraMeta if the policy doesn't trigger, as non-trivial ExtraMeta','line_number':582,'multiline':False]['text':' implies the policy will always match.','line_number':583,'multiline':False]['text':'','line_number':584,'multiline':False]['text':' The default implementations of methods are "safe": they do extra tests','line_number':585,'multiline':False]['text':' to make sure the internal state is consistent no matter if you are','line_number':586,'multiline':False]['text':' doing symbolic shapes or not.  If you don't want the tests, directly','line_number':587,'multiline':False]['text':' override the custom method (e.g., custom_sizes()) to do your preferred','line_number':588,'multiline':False]['text':' behavior.','line_number':589,'multiline':False]['text':'*
   * Return a reference to the sizes of this tensor.  This reference remains
   * valid as long as the tensor is live and not resized.
   ','line_number':592,'multiline':True]['text':' Sizes guaranteed to be non-negative, so unchecked cast is OK','line_number':607,'multiline':False]['text':' Sizes guaranteed to be non-negative, so unchecked cast is OK','line_number':623,'multiline':False]['text':' From https://stackoverflow.com/a/3057522/23845','line_number':628,'multiline':False]['text':' TODO: does C++14 have a stdlib template for this?','line_number':629,'multiline':False]['text':'*
   * The number of elements in a tensor.
   *
   * WARNING: Previously, if you were using the Caffe2 API, you could
   * test numel() == -1 to see if a tensor was uninitialized.  This
   * is no longer true; numel always accurately reports the product
   * of sizes of a tensor.
   ','line_number':671,'multiline':True]['text':'*
   * Return the number of dimensions of this tensor.  Note that 0-dimension
   * represents a Tensor that is a Scalar, e.g., one that has a single element.
   ','line_number':708,'multiline':True]['text':'*
   * Return the offset in number of elements into the storage that this
   * tensor points to.  Most tensors have storage_offset() == 0, but,
   * for example, an index into a tensor will have a non-zero storage_offset().
   *
   * WARNING: This is NOT computed in bytes.
   ','line_number':727,'multiline':True]['text':' TODO: maybe this should be toggled by strides','line_number':735,'multiline':False]['text':'*
   * Return a reference to the strides of this tensor.  This reference remains
   * valid as long as the tensor is live and not restrided.
   ','line_number':764,'multiline':True]['text':'*
   * Whether or not a tensor is laid out in contiguous memory.
   *
   * Tensors with non-trivial strides are not contiguous.  See
   * compute_contiguous() for the exact definition of whether or not
   * a tensor is contiguous or not.
   ','line_number':797,'multiline':True]['text':' These are factored into separate functions in case subclasses','line_number':812,'multiline':False]['text':' want to use them','line_number':813,'multiline':False]['text':' NB: these dim accessor functions don't have _default(), as you can use','line_number':867,'multiline':False]['text':' sizes_default/strides_default','line_number':868,'multiline':False]['text':'*
   * Return the size of a tensor at some dimension, wrapping the dimension if
   * necessary.
   *
   * NOTE: if you know wrapping is unnecessary, do sizes()[d] instead; it will
   * be faster
   ','line_number':869,'multiline':True]['text':'wrap_scalar=','line_number':880,'multiline':True]['text':'wrap_scalar=','line_number':888,'multiline':True]['text':'*
   * Return the stride of a tensor at some dimension, wrapping the dimension
   * if necessary.
   *
   * NOTE: if you know wrapping is unnecessary, do sizes()[d] instead; it will
   * be faster
   ','line_number':893,'multiline':True]['text':' TODO: provide stride_custom, symmetrically with size_custom.','line_number':903,'multiline':False]['text':' There is presently no user for it; only NestedTensor is using','line_number':904,'multiline':False]['text':' size_custom overrideability','line_number':905,'multiline':False]['text':' unchecked (maybe_wrap_dim enforces bounds)','line_number':906,'multiline':False]['text':' Intentionally don't call default, which also handles symbolic','line_number':908,'multiline':False]['text':' Default behavior, e.g., dense tensor.','line_number':913,'multiline':False]['text':'','line_number':914,'multiline':False]['text':' Can override: nothing','line_number':915,'multiline':False]['text':' Customizable strides behavior, e.g., sparse tensor,','line_number':917,'multiline':False]['text':' mkldnn tensor.','line_number':918,'multiline':False]['text':'','line_number':919,'multiline':False]['text':' Can override: strides(), is_contiguous()','line_number':920,'multiline':False]['text':' Customizable sizes behavior, e.g., nested tensor','line_number':922,'multiline':False]['text':'','line_number':923,'multiline':False]['text':' Can override: strides(), is_contiguous(), sizes(), dim(), numel()','line_number':924,'multiline':False]['text':'*
   * Customization points for the functions above.  sizes_strides_policy_
   * must be set to enable these.
   *
   * NB: dim is overrideable separately from sizes because it is possible
   * for a tensor to have rank, but not well defined sizes.
   ','line_number':945,'multiline':True]['text':' sizes_strides_policy_ >= CustomStrides','line_number':952,'multiline':False]['text':' sizes_strides_policy_ >= CustomSizes','line_number':956,'multiline':False]['text':' Currently this method only exists to be overwritten by subclasses such as','line_number':957,'multiline':False]['text':' NestedTensorImpl.','line_number':958,'multiline':False]['text':' TODO: We could add support to Python dispatch here.','line_number':960,'multiline':False]['text':' TODO: We could call into aten::size.int instead of','line_number':961,'multiline':False]['text':' sizes_custom()[d] and enable use of the dispatcher.','line_number':962,'multiline':False]['text':'wrap_scalar=','line_number':963,'multiline':True]['text':' unchecked (maybe_wrap_dim enforces bounds)','line_number':964,'multiline':False]['text':' TODO: We could add support to Python dispatch here.','line_number':968,'multiline':False]['text':' TODO: We could call into aten::size.int instead of','line_number':969,'multiline':False]['text':' sym_sizes_custom()[d] and enable use of the dispatcher.','line_number':970,'multiline':False]['text':'wrap_scalar=','line_number':971,'multiline':True]['text':' unchecked (maybe_wrap_dim enforces bounds)','line_number':972,'multiline':False]['text':'*
   * True if this tensor has storage. See storage() for details.
   ','line_number':989,'multiline':True]['text':' Allow subclasses to check that their storage_ is never getting set in debug','line_number':993,'multiline':False]['text':' builds.','line_number':994,'multiline':False]['text':' NOTE: we devirtualize this because it arguably shouldn't be an','line_number':1001,'multiline':False]['text':' error just to ask subclasses if they have storage.','line_number':1002,'multiline':False]['text':' This used to throw for most subclasses, but OpaqueTensorImpl','line_number':1003,'multiline':False]['text':' wanted it to successfully return false, so we went ahead and made','line_number':1004,'multiline':False]['text':' it a non-error.','line_number':1005,'multiline':False]['text':'*
   * Return the underlying storage of a Tensor.  Multiple tensors may share
   * a single storage.  A Storage is an impoverished, Tensor-like class
   * which supports far less operations than Tensor.
   *
   * Avoid using this method if possible; try to use only Tensor APIs to perform
   * operations.
   ','line_number':1014,'multiline':True]['text':'*
   * Return the underlying storage, unsafely assuming this is a basic strided
   * tensor. In cases where `storage` access would throw, this returns a
   * default-constructed Storage.
   ','line_number':1029,'multiline':True]['text':' Whether a tensor is sparse COO or not.','line_number':1049,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1051,'multiline':False]['text':' reasons.','line_number':1052,'multiline':False]['text':' Whether a tensor is sparse CSR or not.','line_number':1056,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1062,'multiline':False]['text':' reasons.','line_number':1063,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1069,'multiline':False]['text':' reasons.','line_number':1070,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1078,'multiline':False]['text':' reasons.','line_number':1079,'multiline':False]['text':' Note: we cannot rely on dispatch keys to determine the device type','line_number':1083,'multiline':False]['text':' of a tensor, because "wrapper" tensors (like FunctionalTensorWrapper)','line_number':1084,'multiline':False]['text':' don't include backend dispatch keys.','line_number':1085,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1090,'multiline':False]['text':' reasons.','line_number':1091,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1099,'multiline':False]['text':' reasons.','line_number':1100,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1143,'multiline':False]['text':' reasons.','line_number':1144,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1152,'multiline':False]['text':' reasons.','line_number':1153,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for performance','line_number':1161,'multiline':False]['text':' reasons.','line_number':1162,'multiline':False]['text':' TODO: remove this once we don't automatically enabled Autograd dispatch','line_number':1205,'multiline':False]['text':' keys','line_number':1206,'multiline':False]['text':'       in TensorImpl constructor.','line_number':1207,'multiline':False]['text':' DON'T USE THIS API!! It's only created for testing purpose in','line_number':1208,'multiline':False]['text':' file aten/src/ATen/core/boxing/impl/test_helpers.h','line_number':1209,'multiline':False]['text':' Inference tensor doesn't have autograd or ADInplaceOrView key.','line_number':1214,'multiline':False]['text':' Invariant:','line_number':1215,'multiline':False]['text':'   Inference tensor has version_counter_.enabled() == false','line_number':1216,'multiline':False]['text':' See NOTE [c10::optional operator usage in CUDA]','line_number':1243,'multiline':False]['text':' NB: This method is not virtual and avoid dispatches for perf.','line_number':1253,'multiline':False]['text':' strided is also the most common layout type, so we check for','line_number':1254,'multiline':False]['text':' strided case first.','line_number':1255,'multiline':False]['text':' This keyset must also be kept in sync with the logic in','line_number':1256,'multiline':False]['text':' is_sparse() / is_sparse_csr() / is_mkldnn()','line_number':1257,'multiline':False]['text':' Typically, the tensor dispatch keys define the tensor layout','line_number':1265,'multiline':False]['text':' uniquely. This allows using non-virtual layout method for','line_number':1266,'multiline':False]['text':' better performance. However, when tensor's layout depends,','line_number':1267,'multiline':False]['text':' say, on tensor attributes, one must use this execution path','line_number':1268,'multiline':False]['text':' where the corresponding tensor impl class overwrites virtual','line_number':1269,'multiline':False]['text':' layout_impl() method.','line_number':1270,'multiline':False]['text':'','line_number':1271,'multiline':False]['text':' TODO: implement layout() as native function/method so that','line_number':1272,'multiline':False]['text':' __torch_dispatch__ users will be able to redefine the','line_number':1273,'multiline':False]['text':' layout() method.','line_number':1274,'multiline':False]['text':'*
   * True if a tensor was auto-wrapped from a C++ or Python number.
   * For example, when you write 't + 2', 2 is auto-wrapped into a Tensor
   * with `is_wrapped_number_` set to true.
   *
   * Wrapped numbers do not participate in the result type computation for
   * mixed-type operations if there are any Tensors that are not wrapped
   * numbers.  This is useful, because we want 't + 2' to work with
   * any type of tensor, not just LongTensor (which is what integers
   * in Python represent).
   *
   * Otherwise, they behave like their non-wrapped equivalents.
   * See [Result type computation] in TensorIterator.h.
   *
   * Why did we opt for wrapped numbers, as opposed to just having
   * an extra function add(Tensor, Scalar)?  This helps greatly reduce
   * the amount of code we have to write for add, when actually
   * a Tensor-Scalar addition is really just a Tensor-Tensor
   * addition when the RHS is 0-dim (except for promotion behavior.)
   ','line_number':1283,'multiline':True]['text':'*
   * Set whether or not a tensor was auto-wrapped from a C++ or Python
   * number.  You probably don't want to call this, unless you are
   * writing binding code.
   ','line_number':1307,'multiline':True]['text':'*
   * Returns true if Tensor supports as_strided and as_strided_backward.
   * This is used in autograd to perform inplace update on view Tensors.
   * See Note [View + Inplace update for base tensor] and
   * [View + Inplace update for view tensor] for details.
   * Note this method only returns true for XLA backend, where it
   * simulates strided Tensor to support most view ops, but it cannot
   * fully support general `as_strided` case.
   * It can be expanded as needed in the future, e.g sparse Tensor.
   ','line_number':1317,'multiline':True]['text':' ~~~~~ Autograd API ~~~~~','line_number':1337,'multiline':False]['text':' Some methods below are defined in TensorImpl.cpp because Tensor is an','line_number':1338,'multiline':False]['text':' incomplete type.','line_number':1339,'multiline':False]['text':'*
   * Set whether or not a tensor requires gradient.
   ','line_number':1341,'multiline':True]['text':'*
   * True if a tensor requires gradient.  Tensors which require gradient
   * have history tracked for any operations performed on them, so that
   * we can automatically differentiate back to them.  A tensor that
   * requires gradient and has no history is a "leaf" tensor, which we
   * accumulate gradients into.
   ','line_number':1346,'multiline':True]['text':'*
   * Return a mutable reference to the gradient.  This is conventionally
   * used as `t.grad() = x` to set a gradient to a completely new tensor.
   ','line_number':1355,'multiline':True]['text':'*
   * Return the accumulated gradient of a tensor.  This gradient is written
   * into when performing backwards, when this tensor is a leaf tensor.
   ','line_number':1361,'multiline':True]['text':'*
   * Whether or not the imaginary part of the tensor should be negated
   ','line_number':1367,'multiline':True]['text':'*
   * Set whether or not to take the conjugate of the tensor (flip the imaginary
   * bit).
   ','line_number':1375,'multiline':True]['text':'*
   * XXX: do not use, private api!
   * Update the backend component related keys to the backend component
   * corresponding to this device.
   ','line_number':1388,'multiline':True]['text':'*
   * Whether or not the tensor is a zerotensor
   ','line_number':1395,'multiline':True]['text':'*
   Set whether or not the tensor is a zero tensor
  ','line_number':1403,'multiline':True]['text':'*
   * Whether or not the tensor should be negated
   ','line_number':1416,'multiline':True]['text':'*
   * Set whether or not to take the conjugate of the tensor (flip the imaginary
   * bit).
   ','line_number':1424,'multiline':True]['text':'*
   * Return the accumulated gradient of a tensor. This gradient is computed
   * using forward mode AD.
   *
   * This is an internal API that should never be used by end users.
   *
   * The API is as follows:
   *   - "level" allows to specify the level of forward AD nesting for which the
   *     gradient should be returned. Note that since levels are not fully
   *     supported yet, this argument should be 0. See documentation for
   *     torch::autograd::enter_dual_level for more details about forward AD
   * nesting.
   *   - "self" should represent the Tensor whose forward grad is accessed. It
   * is required when dealing with view.
   ','line_number':1436,'multiline':True]['text':'*
   * Sets the forward gradient for this Tensor.
   * The given Tensor might not be used directly and its content will be copied.
   *
   * This is an internal API that should never be used by end users.
   *
   * The API is as follows:
   *   - "new_grad" is a Tensor containing the new value of the gradient that
   * should be set
   *   - "self" should represent the Tensor whose forward grad is accessed. It
   * is required when dealing with view.
   *   - "level" allows to specify the level of forward AD nesting for which the
   *     gradient should be set. Note that since levels are not fully supported
   *     yet, this argument should be 0. See documentation for
   * torch::autograd::enter_dual_level for more details about forward AD
   * nesting.
   *   - "is_inplace_op" is a boolean flag that tells if this gradient was
   * generated by an inplace operation or an out of place one. This allows
   * better error checking.
   ','line_number':1453,'multiline':True]['text':'*
   * Return a typed data pointer to the actual data which this tensor refers to.
   * This checks that the requested type (from the template parameter) matches
   * the internal type of the tensor.
   *
   * It is invalid to call data() on a dtype-uninitialized tensor, even if
   * the size is 0.
   *
   * WARNING: If a tensor is not contiguous, you MUST use strides when
   * performing index calculations to determine the location of elements in
   * the tensor.  We recommend using 'TensorAccessor' to handle this computation
   * for you; this class is available from 'Tensor'.
   ','line_number':1479,'multiline':True]['text':'*
   * Return a mutable typed data pointer to the actual data which this
   * tensor refers to. This checks that the requested type (from the
   * template parameter) matches the internal type of the tensor.
   *
   * It is invalid to call data() on a dtype-uninitialized tensor, even if
   * the size is 0.
   *
   * WARNING: If a tensor is not contiguous, you MUST use strides when
   * performing index calculations to determine the location of elements in
   * the tensor.  We recommend using 'TensorAccessor' to handle this computation
   * for you; this class is available from 'Tensor'.
   ','line_number':1498,'multiline':True]['text':' Shared implementation of data_dtype_initialized() and','line_number':1518,'multiline':False]['text':' mutable_data_dtype_initialized().','line_number':1519,'multiline':False]['text':'*
   * More efficient helper for Tensor::data_ptr(). Like data<T>(), but
   * does not do a type check. Unlike the untemplated data(), does
   * check has_storage() and storage_initialized().
   ','line_number':1533,'multiline':True]['text':'*
   * More efficient helper for Tensor::data_ptr(). Like data<T>(), but
   * does not do a type check. Unlike the untemplated data(), does
   * check has_storage() and storage_initialized().
   ','line_number':1544,'multiline':True]['text':' Shared implementation of mutable_data_ptr_impl() and the future','line_number':1556,'multiline':False]['text':' mutable_data_ptr_impl().','line_number':1557,'multiline':False]['text':' Caller does the type check.','line_number':1569,'multiline':False]['text':' Note: storage_offset_ can be non-null even for zero-elements tensors','line_number':1570,'multiline':False]['text':' (for example if created as `torch.empty(5)[10:]`) that triggers','line_number':1571,'multiline':False]['text':' applying non-zero offset to null pointer in UBSan','line_number':1572,'multiline':False]['text':'*
   * Return a const void* data pointer to the actual data which this
   * tensor refers to.
   *
   * It is invalid to call data() on a dtype-uninitialized tensor, even if the
   * size is 0.
   *
   * WARNING: The data pointed to by this tensor may not contiguous; do NOT
   * assume that itemsize() * numel() is sufficient to compute the bytes that
   * can be validly read from this tensor.
   ','line_number':1577,'multiline':True]['text':'*
   * Return a void* data pointer to the actual data which this tensor refers to.
   *
   * It is invalid to call mutable_data() on a dtype-uninitialized
   * tensor, even if the size is 0.
   *
   * WARNING: The data pointed to by this tensor may not contiguous; do NOT
   * assume that itemsize() * numel() is sufficient to compute the bytes that
   * can be validly read from this tensor.
   ','line_number':1593,'multiline':True]['text':'/ Shared implementation of data() and mutable_data().','line_number':1609,'multiline':False]['text':'/','line_number':1610,'multiline':False]['text':'/ get_data must return a byte-addressed pointer, e.g. char*,','line_number':1611,'multiline':False]['text':'/ std::byte const*, etc.','line_number':1612,'multiline':False]['text':' Computing an offset into an empty tensor would be UB, since an empty','line_number':1625,'multiline':False]['text':' tensor's storage will be nullptr, and adding a nonzero offset to nullptr','line_number':1626,'multiline':False]['text':' is UB.  So we skip the offset computation in this case.','line_number':1627,'multiline':False]['text':'*
   * Returns the TypeMeta of a tensor, which describes what data type
   * it is (e.g., int, float, ...)
   ','line_number':1635,'multiline':True]['text':'*
   * Return the size of a single element of this tensor in bytes.
   ','line_number':1643,'multiline':True]['text':'*
   * Returns the human-readable name of the actual type of this object (e.g.,
   * TensorImpl, BatchedTensorImpl, etc.). Used for error messages.
   ','line_number':1681,'multiline':True]['text':'*
   * True if a tensor has no elements (e.g., numel() == 0).
   ','line_number':1711,'multiline':True]['text':' if we are going to use sym sizes, we should be setting sym strides at the','line_number':1718,'multiline':False]['text':' same time, otherwise it's very easy to misuse this API','line_number':1719,'multiline':False]['text':' This is renamed to avoid breaking overload BC','line_number':1724,'multiline':False]['text':'*
   * Change the size at some dimension.  This DOES NOT update strides;
   * thus, most changes to size will not preserve contiguity.  You probably
   * also want to call set_stride() when you call this.
   *
   * TODO: This should be jettisoned in favor of `set_sizes_and_strides`,
   * which is harder to misuse.
   ','line_number':1730,'multiline':True]['text':'*
   * Change the stride at some dimension.
   *
   * TODO: This should be jettisoned in favor of `set_sizes_and_strides`,
   * which is harder to misuse.
   ','line_number':1751,'multiline':True]['text':'*
   * Set the offset into the storage of this tensor.
   *
   * WARNING: This does NOT check if the tensor is in bounds for the new
   * location at the storage; the caller is responsible for checking this
   * (and resizing if necessary.)
   ','line_number':1769,'multiline':True]['text':' TODO: this should probably consult policy','line_number':1781,'multiline':False]['text':'*
   * Like set_sizes_and_strides but assumes contiguous strides.
   *
   * WARNING: This function does not check if the requested
   * sizes/strides are in bounds for the storage that is allocated;
   * this is the responsibility of the caller
   ','line_number':1788,'multiline':True]['text':' calls refresh_contiguous()','line_number':1807,'multiline':False]['text':'*
   * Set the sizes and strides of a tensor.
   *
   * WARNING: This function does not check if the requested
   * sizes/strides are in bounds for the storage that is allocated;
   * this is the responsibility of the caller
   ','line_number':1810,'multiline':True]['text':' XXX: This behavior is surprising and may need to be removed to','line_number':1844,'multiline':False]['text':' support negative strides. Some pytorch functions rely on it:','line_number':1845,'multiline':False]['text':' for example, torch.cat (run TestTorch.test_cat_empty).','line_number':1846,'multiline':False]['text':' Keep stride monotonically increasing to match NumPy.','line_number':1850,'multiline':False]['text':'*
   * Set whether a tensor allows changes to its metadata (e.g. sizes / strides /
   * storage / storage_offset). See NOTE [ Metadata Change for a Detached Tensor
   * ] for details.
   ','line_number':1872,'multiline':True]['text':' TODO: at some point, we should kill this field completely.','line_number':1878,'multiline':False]['text':'*
   * True if a tensor allows changes to its metadata (e.g. sizes / strides /
   * storage / storage_offset). See NOTE [ Metadata Change for a Detached Tensor
   * ] for details.
   ','line_number':1882,'multiline':True]['text':'*
   * Set the pointer to autograd metadata.
   ','line_number':1891,'multiline':True]['text':'*
   * Return the pointer to autograd metadata.  May return nullptr if the
   * tensor does not track gradients.
   ','line_number':1897,'multiline':True]['text':'*
   * Set the pointer to named tensor metadata.
   ','line_number':1903,'multiline':True]['text':'*
   * Return the pointer to named tensor metadata.
   ','line_number':1940,'multiline':True]['text':' NOTE [ TensorImpl Shallow-Copying ]','line_number':1964,'multiline':False]['text':'','line_number':1965,'multiline':False]['text':' TensorImpl shallow-copying is used when we want to have two Variables share','line_number':1966,'multiline':False]['text':' the same tensor metadata (e.g. sizes / strides / storage pointer /','line_number':1967,'multiline':False]['text':' storage_offset), but each with a different autograd history. Example call','line_number':1968,'multiline':False]['text':' sites:','line_number':1969,'multiline':False]['text':'','line_number':1970,'multiline':False]['text':' 1. `var_detached = var.detach()` uses `shallow_copy_and_detach()` to create','line_number':1971,'multiline':False]['text':' `var_detached` that shares the same tensor metadata with `var`, but with a','line_number':1972,'multiline':False]['text':' completely new autograd history.','line_number':1973,'multiline':False]['text':' 2. `var.set_data(tensor)` uses `shallow_copy_from()` to copy tensor','line_number':1974,'multiline':False]['text':' metadata from `tensor` into `var`, while keeping `var`'s original','line_number':1975,'multiline':False]['text':' AutogradMeta.','line_number':1976,'multiline':False]['text':'','line_number':1977,'multiline':False]['text':' Functions that shallow-copy a TensorImpl (such as','line_number':1978,'multiline':False]['text':' `shallow_copy_and_detach()` / `shallow_copy_from()` /','line_number':1979,'multiline':False]['text':' `copy_tensor_metadata()`) copy the tensor metadata fields (e.g. sizes /','line_number':1980,'multiline':False]['text':' strides / storage pointer / storage_offset) by value. However, the','line_number':1981,'multiline':False]['text':' following fields are not copied:','line_number':1982,'multiline':False]['text':'','line_number':1983,'multiline':False]['text':' 1. the AutogradMeta pointer, because it is unique for each Variable.','line_number':1984,'multiline':False]['text':' 2. the version counter, because the destination TensorImpl's version','line_number':1985,'multiline':False]['text':' counter is either set to the passed-in `version_counter` (in','line_number':1986,'multiline':False]['text':' `shallow_copy_and_detach()` and `copy_tensor_metadata()`), or it is kept','line_number':1987,'multiline':False]['text':' intact (in `shallow_copy_from()`). See NOTE [ Version Counter Sharing ] for','line_number':1988,'multiline':False]['text':' details.','line_number':1989,'multiline':False]['text':'','line_number':1990,'multiline':False]['text':' In `shallow_copy_and_detach()` and `copy_tensor_metadata()`, the passed-in','line_number':1991,'multiline':False]['text':' `allow_tensor_metadata_change` determines whether the TensorImpl','line_number':1992,'multiline':False]['text':' shallow-copy allows changes to its metadata (e.g. sizes / strides / storage','line_number':1993,'multiline':False]['text':' / storage_offset). See NOTE [ Metadata Change for a Detached Tensor ] for','line_number':1994,'multiline':False]['text':' details.','line_number':1995,'multiline':False]['text':'','line_number':1996,'multiline':False]['text':' In `shallow_copy_from()`, we don't check the destination TensorImpl's','line_number':1997,'multiline':False]['text':' `allow_tensor_metadata_change_`, because `shallow_copy_from()` is used for','line_number':1998,'multiline':False]['text':' implementing functions such as `var.set_data(tensor)`, which changes','line_number':1999,'multiline':False]['text':' `var`'s tensor metadata and expects its `allow_tensor_metadata_change_` to','line_number':2000,'multiline':False]['text':' be ignored.','line_number':2001,'multiline':False]['text':'*
   * One TensorImpl can be copied to another TensorImpl if they have the same
   * DispatchKeySet. The only two special cases (for legacy reason) are:
   * CPU is compatible with CUDA and SparseCPU is
   * compatible with SparseCUDA.
   ','line_number':2003,'multiline':True]['text':'*
   * Return a TensorImpl that is a shallow-copy of this TensorImpl.
   *
   * For usage of `version_counter` and `allow_tensor_metadata_change`,
   * see NOTE [ TensorImpl Shallow-Copying ].
   ','line_number':2041,'multiline':True]['text':'*
   * Return a TensorImpl that is a shallow-copy of this TensorImpl.
   *
   * For usage of `version_counter` and `allow_tensor_metadata_change`,
   * see NOTE [ TensorImpl Shallow-Copying ].
   ','line_number':2051,'multiline':True]['text':'*
   * Shallow-copies data from another TensorImpl into this TensorImpl.
   *
   * For why this function doesn't check this TensorImpl's
   * `allow_tensor_metadata_change_`, see NOTE [ TensorImpl Shallow-Copying ].
   ','line_number':2061,'multiline':True]['text':'src_impl=','line_number':2069,'multiline':True]['text':'dest_impl=','line_number':2070,'multiline':True]['text':'version_counter=','line_number':2071,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':2072,'multiline':True]['text':' Inference tensor doesn't have version counter,','line_number':2075,'multiline':False]['text':' set_version_counter is no-op for them.','line_number':2076,'multiline':False]['text':' See NOTE [c10::optional operator usage in CUDA]','line_number':2108,'multiline':False]['text':' We probably don't want to expose this publicly until','line_number':2109,'multiline':False]['text':' the note is addressed.','line_number':2110,'multiline':False]['text':'*
   * The device type of a Tensor, e.g., DeviceType::CPU or DeviceType::CUDA.
   ','line_number':2116,'multiline':True]['text':' TODO: A useful internal assert would be to show that device_opt_ is null','line_number':2120,'multiline':False]['text':' only if you are an undefined tensor','line_number':2121,'multiline':False]['text':' See NOTE [c10::optional operator usage in CUDA]','line_number':2125,'multiline':False]['text':'*
   * @brief Extends the outer-most dimension of this tensor by num elements,
   * preserving the existing data.
   *
   * The underlying data may be reallocated in order to accommodate the new
   * elements, in which case this tensors' capacity is grown at a factor of
   * growthPct. This ensures that Extend runs on an amortized O(1) time
   * complexity.
   *
   * This op is auto-asynchronous if the underlying device (CUDA) supports it.
   ','line_number':2129,'multiline':True]['text':'*
   * @brief Reserve space for the underlying tensor.
   *
   * This must be called after Resize(), since we only specify the first
   * dimension This does not copy over the old data to the newly allocated space
   ','line_number':2142,'multiline':True]['text':'*
   * @brief Resizes a tensor.
   *
   * Resize takes in a vector of ints specifying the dimensions of the tensor.
   * You can pass in an empty vector to specify that it is a scalar (i.e.
   * containing one single item).
   *
   * The underlying storage may be deleted after calling Resize: if the new
   * shape leads to a different number of items in the tensor, the old memory
   * is deleted and new memory will be allocated next time you call
   * mutable_data(). However, if the shape is different but the total number of
   * items is the same, the underlying storage is kept.
   *
   * This method respects caffe2_keep_on_shrink.  Consult the internal logic
   * of this method to see exactly under what circumstances this flag matters.
   ','line_number':2150,'multiline':True]['text':'*
   * Resizes the tensor without touching underlying storage.
   * This requires the total size of the tensor to remains constant.
   ','line_number':2179,'multiline':True]['text':'*
   * Release whatever memory the tensor was holding but keep size and type
   * information. Subsequent call to mutable_data will trigger new memory
   * allocation.
   ','line_number':2185,'multiline':True]['text':'*
   * @brief Shares the data with another tensor.
   *
   * To share data between two tensors, the sizes of the two tensors must be
   * equal already. The reason we do not implicitly do a Resize to make the two
   * tensors have the same shape is that we want to allow tensors of different
   * shapes but the same number of items to still be able to share data. This
   * allows one to e.g. have a n-dimensional Tensor and a flattened version
   * sharing the same underlying storage.
   *
   * The source tensor should already have its data allocated.
   ','line_number':2192,'multiline':True]['text':' To be deprecated','line_number':2204,'multiline':False]['text':'*
   * Returns a mutable raw pointer of the underlying storage. Since we will need
   * to know the type of the data for allocation, a TypeMeta object is passed in
   * to specify the necessary information. This is conceptually equivalent of
   * calling mutable_data<T>() where the TypeMeta parameter meta is derived from
   * the type T. This function differs from mutable_data<T>() in the sense that
   * the type T can be specified during runtime via the TypeMeta object.
   *
   * If the existing data does not match the desired type, it will be deleted
   * and a new storage will be created.
   ','line_number':2212,'multiline':True]['text':' For 0-size tensors it's fine to return any pointer (including nullptr)','line_number':2224,'multiline':False]['text':' NB: device is not changed','line_number':2233,'multiline':False]['text':' We can reuse the existing buffer if the current data does not have','line_number':2235,'multiline':False]['text':' a special destructor and the new data doesn't have a special','line_number':2236,'multiline':False]['text':' constructor.','line_number':2237,'multiline':False]['text':' because we just reallocated','line_number':2242,'multiline':False]['text':' Storage might have nullptr allocator in rare cases, for example, if','line_number':2246,'multiline':False]['text':' an external memory segment has been wrapped with Tensor and we don't','line_number':2247,'multiline':False]['text':' know how to reallocate it. However, in order to preserve legacy C2','line_number':2248,'multiline':False]['text':' behavior, we allow reallocating the memory using default allocator.','line_number':2249,'multiline':False]['text':' For types that need placement new, we will call it, as well as','line_number':2254,'multiline':False]['text':' making sure that when the data is freed, it calls the right','line_number':2255,'multiline':False]['text':' destruction procedure.','line_number':2256,'multiline':False]['text':' For fundamental type, new and delete is easier.','line_number':2264,'multiline':False]['text':' because we just reallocated','line_number':2270,'multiline':False]['text':'*
   * Returns a typed pointer of the underlying storage.
   *
   * For fundamental types, we reuse possible existing storage if there
   * is sufficient capacity.
   ','line_number':2276,'multiline':True]['text':' Check it here statically - otherwise TypeMeta would throw the runtime','line_number':2287,'multiline':False]['text':' error in attempt to invoke TypeMeta::ctor()','line_number':2288,'multiline':False]['text':'*
   * True if a tensor is storage initialized.  A tensor may become
   * storage UNINITIALIZED after a Resize() or FreeMemory()
   ','line_number':2295,'multiline':True]['text':'*
   * True if a tensor is dtype initialized.  A tensor allocated with
   * Caffe2-style constructors is dtype uninitialized until the
   * first time mutable_data<T>() is called.
   ','line_number':2306,'multiline':True]['text':'*
   * Set the strides of the tensor to match memory_format
   *
   * WARNING: This function doesn't rearrange data and assumes tensor is a
   * memory contiguous
   ','line_number':2333,'multiline':True]['text':' dim_ is a virtual call, don't repeat it','line_number':2352,'multiline':False]['text':' Cleaning warning messages, no need to break as TORCH_CHECK(false)','line_number':2385,'multiline':False]['text':' terminates flow.','line_number':2386,'multiline':False]['text':' break;','line_number':2387,'multiline':False]['text':' recompute contiguous flag, as currently NHWC/NCHW flags are not mutually','line_number':2391,'multiline':False]['text':' exclusive see #24090','line_number':2392,'multiline':False]['text':' The Caffe2 Resize() method supports being called both as Resize({2,2}) as','line_number':2425,'multiline':False]['text':' well as variadic with Resize(2, 2).  These overloads provide all of the','line_number':2426,'multiline':False]['text':' supported calling configurations, while being overloads (and not templates)','line_number':2427,'multiline':False]['text':' so that implicit conversions still work.','line_number':2428,'multiline':False]['text':'','line_number':2429,'multiline':False]['text':' SetDims on ArrayRef is internally implemented as a template, so we can','line_number':2430,'multiline':False]['text':' handle both ArrayRefs of different types (there are some uses of','line_number':2431,'multiline':False]['text':' Resize in Caffe2 which pass in int, not int64_t.)','line_number':2432,'multiline':False]['text':'*
   * Compute the number of elements based on the sizes of a tensor.
   ','line_number':2490,'multiline':True]['text':' NB: This is ONLY called when sizes_and_strides_ is used directly; if','line_number':2493,'multiline':False]['text':' we are virtualizing, then numel calls are virtualized as well, and this','line_number':2494,'multiline':False]['text':' should never get called','line_number':2495,'multiline':False]['text':' Use overflow checks if supported by the compiler','line_number':2499,'multiline':False]['text':'*
   * Compute the number of elements based on the sizes of a
   * tensor. Catches integer overflow that may occur when a tensor
   * using a sparse layout has multiple dimensions with large sizes.
   ','line_number':2506,'multiline':True]['text':'*
   * Compute whether or not a tensor is contiguous based on the sizes and
   * strides of a tensor.
   ','line_number':2525,'multiline':True]['text':'*
   * Recompute the cached numel of a tensor.  Call this if you modify
   * sizes.
   *
   * For tensors with sparse layouts, use safe_refresh_numel() instead
   * because it will catch integer overflow that may occur for tensors
   * with sparse layouts and large dimensions.
   *
   * NB: We may uselessly recompute cached numel even in situations where
   * it is completely never used (e.g., if CustomSizes for Python).  However,
   * we still must keep it up to date in case the Python overload
   * returns None (in which case we will consult the field here).  This also
   * implies that sizes/strides will never be complete garbage; in the
   * very worst case scenario, it will reflect a 1-dim zero size tensor.
   ','line_number':2542,'multiline':True]['text':'*
   * Recompute the cached numel of a tensor.  Call this if you modify
   * sizes. Use only for tensors with sparse layouts because only
   * sparse tensor are likely to have sizes that may lead to integer
   * overflow when computing numel.
   ','line_number':2565,'multiline':True]['text':' NB: sym numel is done with symbolic integers, which handle overflow','line_number':2573,'multiline':False]['text':' checking','line_number':2574,'multiline':False]['text':' NB: the TypeId argument prevents confusion where you pass a true/false','line_number':2582,'multiline':False]['text':' literal and pick the wrong overload','line_number':2583,'multiline':False]['text':' These are little wrappers over the real compute_ functions that','line_number':2609,'multiline':False]['text':' can make use of other contiguity fields to short circuit.','line_number':2610,'multiline':False]['text':' Note:','line_number':2644,'multiline':False]['text':' Dim 0, 1, 2 will never be a channels last 2d/3d format','line_number':2645,'multiline':False]['text':' Dim 3+ is possibly be a channels last 2d format (Dim 4 only at this','line_number':2646,'multiline':False]['text':' point) Dim 4+ is possibly be a channels last 3d format (Dim 5 only at','line_number':2647,'multiline':False]['text':' this point)','line_number':2648,'multiline':False]['text':' is_channels_last_ and is_channels_last_3d_ are suggested','line_number':2676,'multiline':False]['text':' memory_format. Being channels_last_contiguous doesn't necessarily','line_number':2677,'multiline':False]['text':' mean the tensor is strided like channels_last: for strides on channel','line_number':2678,'multiline':False]['text':' dimension could suggest desired memory_layout, but it doesn't affect','line_number':2679,'multiline':False]['text':' memory storage','line_number':2680,'multiline':False]['text':'*
   * Recompute the cached contiguity of a tensor.  Call this if you modify sizes
   * or strides.
   ','line_number':2693,'multiline':True]['text':'*
   * Copy the tensor metadata fields (e.g. sizes / strides / storage pointer /
   * storage_offset) from one TensorImpl to another TensorImpl.
   *
   * For usage of `version_counter` and `allow_tensor_metadata_change`, see NOTE
   * [ TensorImpl Shallow-Copying ].
   ','line_number':2705,'multiline':True]['text':'*
   * Copy the tensor metadata fields (e.g. sizes / strides / storage pointer /
   * storage_offset) from one TensorImpl to another TensorImpl.
   *
   * For usage of `version_counter` and `allow_tensor_metadata_change`, see NOTE
   * [ TensorImpl Shallow-Copying ].
   ','line_number':2718,'multiline':True]['text':' Error message to show when the user tries to change tensor metadata on','line_number':2738,'multiline':False]['text':' Tensor created from .data or .detach().','line_number':2739,'multiline':False]['text':'','line_number':2740,'multiline':False]['text':' See NOTE [ Metadata Change for a Detached Tensor ] for details.','line_number':2741,'multiline':False]['text':' This pointer points to an AutogradMeta struct that stores autograd-specific','line_number':2807,'multiline':False]['text':' fields (such as grad_ / grad_fn_ / grad_accumulator_). This pointer always','line_number':2808,'multiline':False]['text':' has unique ownership (meaning only one TensorImpl can own it at a time).','line_number':2809,'multiline':False]['text':'','line_number':2810,'multiline':False]['text':' autograd_meta_ can be nullptr, as an optimization.  When this occurs, it is','line_number':2811,'multiline':False]['text':' equivalent to having an autograd_meta_ pointing to a default constructed','line_number':2812,'multiline':False]['text':' AutogradMeta; intuitively, tensors which don't require grad will have this','line_number':2813,'multiline':False]['text':' field set to null.','line_number':2814,'multiline':False]['text':'','line_number':2815,'multiline':False]['text':' This means accessors on autograd_meta_ have to be careful to test if they','line_number':2816,'multiline':False]['text':' got a nullptr, and handle default behavior appropriately in that case.','line_number':2817,'multiline':False]['text':'','line_number':2818,'multiline':False]['text':' Note that we don't enforce the invariant that if the AutogradMeta is','line_number':2819,'multiline':False]['text':' default constructed, it is nullptr (to do this, we'd have to continuously','line_number':2820,'multiline':False]['text':' check if an AutogradMeta became, by mutation, equal to the default','line_number':2821,'multiline':False]['text':' constructed form.  (This might be useful, but it seems rare enough that','line_number':2822,'multiline':False]['text':' a requires_grad=True variable will turn back into the requires_grad=False','line_number':2823,'multiline':False]['text':' version.)  So there are three representable states:','line_number':2824,'multiline':False]['text':'','line_number':2825,'multiline':False]['text':'    1. autograd_meta_ == nullptr','line_number':2826,'multiline':False]['text':'    2. autograd_meta_ is default constructed (semantically, same as (1))','line_number':2827,'multiline':False]['text':'    3. autograd_meta_ has nontrivial information content','line_number':2828,'multiline':False]['text':'','line_number':2829,'multiline':False]['text':' If sizes and strides are empty, the numel is 1!!  However, most of the','line_number':2842,'multiline':False]['text':' time, we will immediately set sizes to {0} and reset numel to 0.','line_number':2843,'multiline':False]['text':' (Can't do that in the default initializers, because there's no way to','line_number':2844,'multiline':False]['text':' spell "allocate a one-element array" for strides_).','line_number':2845,'multiline':False]['text':' INVARIANT: When storage is non-null, this type meta must','line_number':2848,'multiline':False]['text':' agree with the type meta in storage','line_number':2849,'multiline':False]['text':' NOTE [c10::optional operator usage in CUDA]','line_number':2852,'multiline':False]['text':' Our optional definition doesn't compile in .cu file if `value()` or','line_number':2853,'multiline':False]['text':' `operator->` are used.  Instead, we always use `operator*`.','line_number':2854,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/18496 for more info.','line_number':2855,'multiline':False]['text':' If this is too burdensome to maintain, we can just','line_number':2856,'multiline':False]['text':' manually implement this with an additional bool.','line_number':2857,'multiline':False]['text':' INVARIANT: When storage is non-null, this Device must','line_number':2859,'multiline':False]['text':' agree with the type meta in storage.','line_number':2860,'multiline':False]['text':'','line_number':2861,'multiline':False]['text':' INVARIANT: device_opt_ is only nullopt for undefined tensors','line_number':2862,'multiline':False]['text':' (which do not have a device.)','line_number':2863,'multiline':False]['text':' default member initializers for bit-fields only available with -std=c++2a','line_number':2866,'multiline':False]['text':' or -std=gnu++2a','line_number':2867,'multiline':False]['text':' Tensor is contiguous','line_number':2892,'multiline':False]['text':' Tensor is a subclass that does not permit storage access.','line_number':2895,'multiline':False]['text':' Tensor is stored in the channels last 2d memory format, when dimensions','line_number':2898,'multiline':False]['text':' order is (N)CHW and C-strides < W-strides < H-strides (< N-strides)','line_number':2899,'multiline':False]['text':' (If size of any dimension is equal to 1, this dimension strides value','line_number':2900,'multiline':False]['text':' is not taken into account).','line_number':2901,'multiline':False]['text':' Channels last contiguous tensor is channel last tensor which occupies','line_number':2904,'multiline':False]['text':' contiguous memory block.','line_number':2905,'multiline':False]['text':' Tensor is stored in the channels last 3d memory format, when dimensions','line_number':2908,'multiline':False]['text':' order is (N)CDHW and C-strides < W-strides < H-strides < D - strides (<','line_number':2909,'multiline':False]['text':' N-strides) (If size of any dimension is equal to 1, this dimension strides','line_number':2910,'multiline':False]['text':' value is not taken into account).','line_number':2911,'multiline':False]['text':' Channels last 3d contiguous tensor is channel last 3d tensor which occupies','line_number':2914,'multiline':False]['text':' contiguous memory block.','line_number':2915,'multiline':False]['text':' Dense tensor is the tensor that store values in a contiguous block of','line_number':2918,'multiline':False]['text':' memory. Non-overlapping tensor is the tensor in which elements occupy','line_number':2919,'multiline':False]['text':' individual non-repetitive memory.','line_number':2920,'multiline':False]['text':' NOTE [ Metadata Change for a Detached Tensor ]','line_number':2925,'multiline':False]['text':'','line_number':2926,'multiline':False]['text':' Normally, a user is allowed to change the tensor metadata','line_number':2927,'multiline':False]['text':' (e.g. sizes / strides / storage / storage_offset) of a tensor.','line_number':2928,'multiline':False]['text':' However, if the tensor is created by `t1_detached = t1.data` in Python','line_number':2929,'multiline':False]['text':' or `t1_detached = t1.detach()` in Python/C++, those changes to the','line_number':2930,'multiline':False]['text':' tensor metadata of `t1_detached` will not be propagated back to the','line_number':2931,'multiline':False]['text':' original tensor `t1`. In order to make such changes explicitly illegal,','line_number':2932,'multiline':False]['text':' we created the `allow_tensor_metadata_change_` flag, to prevent users','line_number':2933,'multiline':False]['text':' from changing metadata of the detached tensor and expecting the original','line_number':2934,'multiline':False]['text':' tensor to also be updated.','line_number':2935,'multiline':False]['text':'','line_number':2936,'multiline':False]['text':' NOTE: For a full list of tensor metadata fields, please see','line_number':2937,'multiline':False]['text':' `copy_tensor_metadata()` in TensorImpl and its subclasses to find','line_number':2938,'multiline':False]['text':' which fields are copied by value.','line_number':2939,'multiline':False]['text':' we decide to keep reserved_ and it will','line_number':2942,'multiline':False]['text':' live in Tensor after the split','line_number':2943,'multiline':False]['text':' The logic is that if Extend() or ReserveSpace() were ever called,','line_number':2944,'multiline':False]['text':' then subsequent Resize()s will not free up Storage.','line_number':2945,'multiline':False]['text':' Call _custom() virtual methods for','line_number':2948,'multiline':False]['text':' strides()/is_contiguous()/sizes()/dim()/numel()','line_number':2949,'multiline':False]['text':' This is a combination of sizes_strides_custom_dispatch_','line_number':2950,'multiline':False]['text':' and has_symbolic_sizes_strides_','line_number':2951,'multiline':False]['text':' Whether or not sizes_and_strides_ contains a symbolic value.','line_number':2954,'multiline':False]['text':' Call _custom() virtual method for','line_number':2957,'multiline':False]['text':' strides()/is_contiguous()/sizes()/dim()/numel()','line_number':2958,'multiline':False]['text':' Combo of custom_ and python_custom_','line_number':2961,'multiline':False]['text':' Call _custom() virtual method for device()','line_number':2965,'multiline':False]['text':' Call _custom() virtual method for layout()','line_number':2968,'multiline':False]['text':' Call into Python for','line_number':2971,'multiline':False]['text':' strides()/is_contiguous()/sizes()/dim()/numel()','line_number':2972,'multiline':False]['text':' Call into Python for device()','line_number':2975,'multiline':False]['text':' Call into Python for layout()','line_number':2978,'multiline':False]['text':' The set of DispatchKeys which describe this tensor.  NB: this','line_number':2981,'multiline':False]['text':' does NOT include Autograd (historically, it did, but','line_number':2982,'multiline':False]['text':' not anymore!)','line_number':2983,'multiline':False]['text':'','line_number':2984,'multiline':False]['text':' INVARIANT: extra_meta_->named_tensor_meta_ != nullptr  <==>','line_number':2985,'multiline':False]['text':' key_set_.has(DispatchKey::Named)','line_number':2986,'multiline':False]['text':' C10_TensorImpl_Size_Check_Dummy_Class needs to be friends with','line_number':2990,'multiline':False]['text':' TensorImpl so it can inspect the size of private fields','line_number':2991,'multiline':False]['text':' Note [TensorImpl size constraints]','line_number':3004,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':3005,'multiline':False]['text':' Changed the size of TensorImpl?  If the size went down, good for','line_number':3006,'multiline':False]['text':' you!  Adjust the documentation below and the expected size.','line_number':3007,'multiline':False]['text':' Did it go up?  Read on...','line_number':3008,'multiline':False]['text':'','line_number':3009,'multiline':False]['text':' Struct size matters.  In some production systems at Facebook, we have','line_number':3010,'multiline':False]['text':' 400M live tensors during a training run.  Do the math: every 64-bit','line_number':3011,'multiline':False]['text':' word you add to Tensor is an extra 3.2 gigabytes in RAM.','line_number':3012,'multiline':False]['text':'','line_number':3013,'multiline':False]['text':' If you are a Facebook employee, you can check if the run in question','line_number':3014,'multiline':False]['text':' has tipped you over the point using the command here:','line_number':3015,'multiline':False]['text':' https://fburl.com/q5enpv98','line_number':3016,'multiline':False]['text':'','line_number':3017,'multiline':False]['text':' For reference, we OOMed at 160 bytes (20 words) per TensorImpl.','line_number':3018,'multiline':False]['text':' This is not counting overhead from strides out-of-line allocation and','line_number':3019,'multiline':False]['text':' StorageImpl space and this is from before we inlined sizes and strides','line_number':3020,'multiline':False]['text':' directly into TensorImpl as SmallVectors.','line_number':3021,'multiline':False]['text':'','line_number':3022,'multiline':False]['text':' Our memory usage on 32-bit systems is suboptimal, but we're not checking','line_number':3023,'multiline':False]['text':' for it at the moment (to help avoid rage inducing cycles when the','line_number':3024,'multiline':False]['text':' 32-bit number is wrong).','line_number':3025,'multiline':False]['text':'','line_number':3026,'multiline':False]['text':' Current breakdown:','line_number':3027,'multiline':False]['text':'','line_number':3028,'multiline':False]['text':'    vtable pointer','line_number':3029,'multiline':False]['text':'    strong refcount           TODO: pack these into one word','line_number':3030,'multiline':False]['text':'    weak refcount','line_number':3031,'multiline':False]['text':'    storage pointer','line_number':3032,'multiline':False]['text':'    autograd metadata pointer','line_number':3033,'multiline':False]['text':'    named tensor metadata pointer','line_number':3034,'multiline':False]['text':'    version counter pointer','line_number':3035,'multiline':False]['text':'    PyObjectSlot','line_number':3036,'multiline':False]['text':'    SizesAndStrides size/pointer','line_number':3037,'multiline':False]['text':'    SizesAndStrides sizes (pre-allocated 0)','line_number':3038,'multiline':False]['text':'    SizesAndStrides sizes (pre-allocated 1)','line_number':3039,'multiline':False]['text':'    SizesAndStrides sizes (pre-allocated 2)','line_number':3040,'multiline':False]['text':'    SizesAndStrides sizes (pre-allocated 3)','line_number':3041,'multiline':False]['text':'    SizesAndStrides sizes (pre-allocated 4)','line_number':3042,'multiline':False]['text':'    SizesAndStrides strides (pre-allocated 0)','line_number':3043,'multiline':False]['text':'    SizesAndStrides strides (pre-allocated 1)','line_number':3044,'multiline':False]['text':'    SizesAndStrides strides (pre-allocated 2)','line_number':3045,'multiline':False]['text':'    SizesAndStrides strides (pre-allocated 3)','line_number':3046,'multiline':False]['text':'    SizesAndStrides strides (pre-allocated 4)','line_number':3047,'multiline':False]['text':'    storage offset','line_number':3048,'multiline':False]['text':'    numel','line_number':3049,'multiline':False]['text':'    data type, device, is_contiguous, storage_access_should_throw_, bitfields','line_number':3050,'multiline':False]['text':'    DispatchKeySet','line_number':3051,'multiline':False]['text':'','line_number':3052,'multiline':False]['text':' Various preprocessor macros we use to check that the','line_number':3054,'multiline':False]['text':' TensorImpl size hasn't changed unexpectedly. We undef','line_number':3055,'multiline':False]['text':' these later.','line_number':3056,'multiline':False]['text':' We use a templatized class to both contain the logic of checking the sizes','line_number':3093,'multiline':False]['text':' as well as to provide compile-time information that might be useful in','line_number':3094,'multiline':False]['text':' figuring out why sizes may have changed.','line_number':3095,'multiline':False]['text':' All the compile time information is given by the template fields that are','line_number':3096,'multiline':False]['text':' always printed by the compiler when the static_assert fails.','line_number':3097,'multiline':False]['text':' Names of (non-bitfield) fields in TensorImpl; used to provide','line_number':3108,'multiline':False]['text':' compile-time info about fields whose size changes unexpectedly.','line_number':3109,'multiline':False]['text':' Provides compile-time equality check that reveals what numbers','line_number':3125,'multiline':False]['text':' were used and on which quantity','line_number':3126,'multiline':False]['text':' Provides compile-time <= check that reveals what numbers','line_number':3135,'multiline':False]['text':' were used and on which quantity','line_number':3136,'multiline':False]['text':' Compile-time check that TensorImpl field sizes are as expected','line_number':3146,'multiline':False]['text':'','line_number':3147,'multiline':False]['text':' Observed total sizes and associated versions','line_number':3148,'multiline':False]['text':' If you find a flag that predicts when unique_ptr has 16 bytes','line_number':3149,'multiline':False]['text':' on 64-bit systems or when sizes_and_strides_ is 84 vs 88 bytes','line_number':3150,'multiline':False]['text':' on 32-bit systems you get a cookie!','line_number':3151,'multiline':False]['text':' Length | LLVM | GCC  |    C++ |  CUDA','line_number':3152,'multiline':False]['text':'    192 |    ? | 11.2 | 201703 | 11040','line_number':3153,'multiline':False]['text':'    208 |    ? | 11.2 | 201703 | 11040','line_number':3154,'multiline':False]['text':'    208 |    ? | 11.2 | 201402 | 11040','line_number':3155,'multiline':False]['text':'    192 |    ? | 11.2 | 201402 | 11040','line_number':3156,'multiline':False]['text':'    160 |   12 |  4.2 | 201703 |     0','line_number':3157,'multiline':False]['text':'','line_number':3158,'multiline':False]['text':' To keep things clean, we split on systems here.','line_number':3159,'multiline':False]['text':' This is a 32-bit system','line_number':3162,'multiline':False]['text':' clang-format off','line_number':3166,'multiline':False]['text':' clang-format on','line_number':3179,'multiline':False]['text':' This is a 64-bit system','line_number':3184,'multiline':False]['text':' clang-format off','line_number':3188,'multiline':False]['text':' On some systems involving NVCC the size of unique_ptr is 16 bytes. We haven't','line_number':3190,'multiline':False]['text':' figured out how to detect those via macro preprocessors yet, so we use <=','line_number':3191,'multiline':False]['text':' comparisons for the relevant fields.','line_number':3192,'multiline':False]['text':' clang-format on','line_number':3204,'multiline':False]['text':' We use a class to encapsulate size-checking logic with','line_number':3211,'multiline':False]['text':' templates to capture sizes and flags. We call this within','line_number':3212,'multiline':False]['text':' a static assert to prove there is no run-time behaviour.','line_number':3213,'multiline':False]['text':' Since the methods we call return either true or fail their','line_number':3214,'multiline':False]['text':' own static_asserts, we should never see the error messages','line_number':3215,'multiline':False]['text':' below. We have to provide it though for c++ <17.','line_number':3216,'multiline':False]['text':' Clean up after ourselves','line_number':3221,'multiline':False]['text':' namespace c10','line_number':3229,'multiline':False]