['text':' backend_dispatch_keyset includes all dispatch keys that map to backends.','line_number':6,'multiline':False]['text':' Alias key DispatchKey::CompositeExplicitAutograd maps to','line_number':7,'multiline':False]['text':' backend_dispatch_keyset','line_number':8,'multiline':False]['text':' See Note [CompositeExplicitAutogradNonFunctional Key]','line_number':12,'multiline':False]['text':' We have several types of decompositions in aten, that each have their own','line_number':13,'multiline':False]['text':' alias key. You should register your decomposition to the','line_number':14,'multiline':False]['text':' `CompositeExplicitAutogradNonFunctional key` if: (1) It's an out-of-place op','line_number':15,'multiline':False]['text':' (2) It decomposes into one more mutation ops','line_number':16,'multiline':False]['text':' (3) It has a derivative formula','line_number':17,'multiline':False]['text':'     (In theory we could also have a separate key for','line_number':18,'multiline':False]['text':'     "CompositeImplicitAutogradNonFunctional", but there isn't much of a use','line_number':19,'multiline':False]['text':'     case for it currently).','line_number':20,'multiline':False]['text':' This key is important for "functional" backends like LazyTensor / XLA.','line_number':21,'multiline':False]['text':' If you're a backend that only expects to deal with "functional ops",','line_number':22,'multiline':False]['text':' then you don't want to decompose a functional op into an op that causes','line_number':23,'multiline':False]['text':' aliasing. You should just directly write a kernel for that functional op','line_number':24,'multiline':False]['text':' instead!','line_number':25,'multiline':False]['text':' XLA and LazyTensor are currently the only 2 backends in core','line_number':28,'multiline':False]['text':' that use functionalization pass in eager mode.','line_number':29,'multiline':False]['text':' See Note [No Alias Keys in DispatchKeySet]','line_number':36,'multiline':False]['text':' Note [NestedTensor Not Included in Backend Keys]','line_number':38,'multiline':False]['text':' NestedTensor has been explicitly removed from the "backend keyset" due','line_number':39,'multiline':False]['text':' to incompatibility with some kernels, so we don't want it to be','line_number':40,'multiline':False]['text':' included in CompositeExplicitAutograd kernels.','line_number':41,'multiline':False]['text':' math_dispatch_keyset contains all keys in backend_dispatch_keyset and','line_number':45,'multiline':False]['text':' autograd_dispatch_keyset Alias key DispatchKey::CompositeImplicitAutograd','line_number':46,'multiline':False]['text':' maps to [math_dispatch_keyset x full_backend_mask]','line_number':47,'multiline':False]['text':' See Note [NestedTensor Not Included in Backend Keys]','line_number':50,'multiline':False]['text':' The caveat to that note is that nested_tensor is a special case','line_number':51,'multiline':False]['text':' where we would like to support composite implicit kernels but not','line_number':52,'multiline':False]['text':' explicit kernels therefore we manually add the key to the','line_number':53,'multiline':False]['text':' math_dispatch_keyset','line_number':54,'multiline':False]['text':' Functionalize should always re-use CompositeImplicit decomps.','line_number':56,'multiline':False]['text':' See Note [autograd_dispatch_keyset Does Not Include Backend Bits]','line_number':68,'multiline':False]['text':' That's why we OR it with a mask of the backend bits here.','line_number':69,'multiline':False]['text':' getRuntimeDispatchKeySet() expects to return a keyset of runtime','line_number':70,'multiline':False]['text':' dispatch keys, like AutogradCPU, but that requires having backend bits.','line_number':71,'multiline':False]['text':' See Note [NestedTensor Not Included in Backend Keys]','line_number':93,'multiline':False]['text':' See Note [NestedTensor Not Included in Backend Keys]','line_number':96,'multiline':False]['text':' See Note [NestedTensor Not Included in Backend Keys]','line_number':99,'multiline':False]['text':' See Note [NestedTensor Not Included in Backend Keys]','line_number':102,'multiline':False]['text':' for a given autograd key, return the (guaranteed nonempty) set of associated','line_number':112,'multiline':False]['text':' backend keys. for a non-autograd key, return the empty keyset.','line_number':113,'multiline':False]['text':' Create a masked version of the set representation to ignore previous','line_number':182,'multiline':False]['text':' keys that we've iterated through.','line_number':183,'multiline':False]['text':' If there are no keys, set to end iterator value','line_number':194,'multiline':False]['text':' Set up state to be the same as end()','line_number':197,'multiline':False]['text':' The +1 is because of DispatchKey::Undefined and','line_number':205,'multiline':False]['text':' BackendComponent::InvalidBit','line_number':206,'multiline':False]['text':' and the -num_backends is because the first <num_backends> bits in the','line_number':209,'multiline':False]['text':' keyset are not Dispatch Keys.','line_number':210,'multiline':False]['text':' If the current functionality bit is a per-backend bit, we need special','line_number':213,'multiline':False]['text':' handling','line_number':214,'multiline':False]['text':' case 1: if the current backend is undefined, then there is no valid','line_number':217,'multiline':False]['text':' backend instance of this functionality key so we can skip it.','line_number':218,'multiline':False]['text':' increment the functionality mask so we skip the current functionality','line_number':220,'multiline':False]['text':' bit on the next increment.','line_number':221,'multiline':False]['text':' Otherwise, at this point we know what the current backend and','line_number':227,'multiline':False]['text':' functionality bits are.','line_number':228,'multiline':False]['text':' Next, we need to set up the masks for the next increment.','line_number':232,'multiline':False]['text':' case 2: the current backend is valid, but there is not another backend','line_number':239,'multiline':False]['text':' in the keyset. In this case, we need to bump the functionality mask and','line_number':240,'multiline':False]['text':' reset the backend mask for the next increment','line_number':241,'multiline':False]['text':' case 3: we have another backend to iterate over. We want to iterate','line_number':245,'multiline':False]['text':' over the same functionality bit next time, but a different backend bit.','line_number':246,'multiline':False]['text':' Functionality bits that aren't per backend are simpler to handle. We can','line_number':250,'multiline':False]['text':' ignore the backend bits.','line_number':251,'multiline':False]['text':' manually set the first entry, which corresponds to Undefined.','line_number':263,'multiline':False]['text':' loop through every functionality key (aside from Undefined).','line_number':265,'multiline':False]['text':' functionality_idx should be Dense -> 1, ...','line_number':267,'multiline':False]['text':' If the previous functionality was not per-backend, then we can just','line_number':271,'multiline':False]['text':' increment the previous offset. Otherwise, the next offset =','line_number':272,'multiline':False]['text':' previous_offset + num_backends.','line_number':273,'multiline':False]['text':' the mask is used in the runtime index calculation to find the offset of','line_number':276,'multiline':False]['text':' the backend. For non-per-backend functionalities, this offset should','line_number':277,'multiline':False]['text':' always be 0. Otherwise, we need to get the index of the backend (which we','line_number':278,'multiline':False]['text':' can do using a backend mask).','line_number':279,'multiline':False]['text':' Sanity check that the computed offset index of the last functionality key','line_number':284,'multiline':False]['text':' is correct. This assumes that the highest priority functionality key is not','line_number':285,'multiline':False]['text':' per backend.','line_number':286,'multiline':False]['text':' namespace c10','line_number':297,'multiline':False]