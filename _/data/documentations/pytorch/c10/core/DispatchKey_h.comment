['text':' Semantically, each value of BackendComponent identifies a "backend" for our','line_number':11,'multiline':False]['text':' dispatch. Some functionalities that we may dispatch to are allowed to','line_number':12,'multiline':False]['text':' register different handlers for each backend. The BackendComponent is then','line_number':13,'multiline':False]['text':' used to figure out which backend implementation to dispatch to.','line_number':14,'multiline':False]['text':' In implementation terms, the backend component identifies a specific "bit" in','line_number':16,'multiline':False]['text':' a DispatchKeySet. The bits in the DispatchKeySet are split between the bottom','line_number':17,'multiline':False]['text':' ~12 "BackendComponent" bits, while the remaining upper bits are assigned to','line_number':18,'multiline':False]['text':' functionalities. When we encounter a functionality bit that is known to be','line_number':19,'multiline':False]['text':' customizable per-backend, then we also look at the lower BackendComponent','line_number':20,'multiline':False]['text':' bits and take the highest bit to determine which backend's implementation to','line_number':21,'multiline':False]['text':' use.','line_number':22,'multiline':False]['text':' WARNING!  If you add a new backend component to the end of this list,','line_number':24,'multiline':False]['text':' make sure you register it before Meta.','line_number':25,'multiline':False]['text':' Meta must be at the end so that meta key in tls triggers meta kernels.','line_number':26,'multiline':False]['text':' (But you shouldn't: private use keys should have higher precedence than all','line_number':27,'multiline':False]['text':' built-in keys)','line_number':28,'multiline':False]['text':' If you add a new (non-privateuse) backend here,','line_number':30,'multiline':False]['text':' make sure to add an Autograd<Backend> fallthrough kernel','line_number':31,'multiline':False]['text':' in aten/src/ATen/core/VariableFallbackKernel.cpp','line_number':32,'multiline':False]['text':' WARNING!  If we add a new per-backend functionality key that has higher','line_number':51,'multiline':False]['text':' priority than Autograd, then make sure you update EndOfRuntimeBackendKeys','line_number':52,'multiline':False]['text':' A "backend" is colloquially used to refer to handlers for dispatch','line_number':63,'multiline':False]['text':' which actually implement the numerics of an operation in question.','line_number':64,'multiline':False]['text':'','line_number':65,'multiline':False]['text':' Due to the nature of the enum, these backends are specified in','line_number':66,'multiline':False]['text':' an ordered way, but for most backends this order is not semantically','line_number':67,'multiline':False]['text':' meaningful (e.g., it's valid to reorder these backends without changing','line_number':68,'multiline':False]['text':' semantics).  The only situation when backend ordering is meaningful','line_number':69,'multiline':False]['text':' is when the backend participates in multiple dispatch with another','line_number':70,'multiline':False]['text':' backend; e.g., CPU and CUDA (cuda must have higher priority).','line_number':71,'multiline':False]['text':' These keys don't correspond to individual kernels.','line_number':73,'multiline':False]['text':' Instead, they represent the backends that are allowed to override specific','line_number':74,'multiline':False]['text':' pieces of functionality:','line_number':75,'multiline':False]['text':' - dense kernels (e.g. DispatchKey::CPU)','line_number':76,'multiline':False]['text':' - sparse kernels (e.g. DispatchKey::SparseCPU)','line_number':77,'multiline':False]['text':' - quantized kernels (e.g. DispatchKey::QuantizedCPU)','line_number':78,'multiline':False]['text':' - autograd kernels (e.g. DispatchKey::AutogradCPU)','line_number':79,'multiline':False]['text':' We reserve space in the runtime operator table for this full cross product','line_number':80,'multiline':False]['text':' of','line_number':81,'multiline':False]['text':' [backends in this enum] x [keys below that are explicitly marked as having','line_number':82,'multiline':False]['text':' per-backend functionality]','line_number':83,'multiline':False]['text':'','line_number':84,'multiline':False]['text':' A meta tensor is a tensor without any data associated with it.  (They','line_number':85,'multiline':False]['text':' have also colloquially been referred to as tensors on the "null" device).','line_number':86,'multiline':False]['text':' A meta tensor can be used to dry run operators without actually doing any','line_number':87,'multiline':False]['text':' computation, e.g., add on two meta tensors would give you another meta','line_number':88,'multiline':False]['text':' tensor with the output shape and dtype, but wouldn't actually add anything.','line_number':89,'multiline':False]['text':' Define an alias to represent end of backend dispatch keys.','line_number':96,'multiline':False]['text':' If you add new backend keys after PrivateUse3, please also update it here.','line_number':97,'multiline':False]['text':' Semantically, a dispatch key identifies a possible "level" in our','line_number':101,'multiline':False]['text':' dispatch, for which a handler may be registered. Each handler corresponds','line_number':102,'multiline':False]['text':' to a type of functionality.','line_number':103,'multiline':False]['text':'','line_number':104,'multiline':False]['text':' In implementation terms, the dispatch key identifies a specific "bit" in a','line_number':105,'multiline':False]['text':' DispatchKeySet.  Higher bit indexes get handled by dispatching first (because','line_number':106,'multiline':False]['text':' we "count leading zeros" when we extract the highest priority dispatch','line_number':107,'multiline':False]['text':' key.)','line_number':108,'multiline':False]['text':'','line_number':109,'multiline':False]['text':' Note [DispatchKey Classification]','line_number':110,'multiline':False]['text':' This enum actually contains several types of keys, which are explained','line_number':111,'multiline':False]['text':' in more detail further down:','line_number':112,'multiline':False]['text':' (1) non-customizable backends (e.g. FPGA)','line_number':113,'multiline':False]['text':' (2) non-customizable functionalities (e.g. Functionalize)','line_number':114,'multiline':False]['text':' (3) functionalized that are customizable per backend (e.g. Dense, Sparse,','line_number':115,'multiline':False]['text':' AutogradFunctionality) (4) per-backend instances of customizable','line_number':116,'multiline':False]['text':' functionalities (e.g. CPU, SparseCPU, AutogradCPU) (5) alias keys (e.g.','line_number':117,'multiline':False]['text':' CompositeImplicitAutograd)','line_number':118,'multiline':False]['text':'','line_number':119,'multiline':False]['text':' Of the categories above, it's important to note:','line_number':120,'multiline':False]['text':' (a) which keys are assigned individual bits in a DispatchKeySet','line_number':121,'multiline':False]['text':' (b) which keys are assigned individual slots in the runtime operator table','line_number':122,'multiline':False]['text':' ("Runtime keys")','line_number':123,'multiline':False]['text':'','line_number':124,'multiline':False]['text':' (1), (2) and (3) all get their own dedicated bits in the DispatchKeySet.','line_number':125,'multiline':False]['text':' (1), (2) and (4) all get their own dedicated slots in the runtime operator','line_number':126,'multiline':False]['text':' table.','line_number':127,'multiline':False]['text':' See Note [DispatchKeySet Internal Representation] for more details.','line_number':129,'multiline':False]['text':'','line_number':130,'multiline':False]['text':' NOTE: Keep the list in sync with `DispatchKey` in torchgen/model.py','line_number':131,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~ UNDEFINED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //','line_number':134,'multiline':False]['text':' This is not a "real" functionality, but it exists to give us a "nullopt"','line_number':135,'multiline':False]['text':' element we can return for cases when a DispatchKeySet contains no elements.','line_number':136,'multiline':False]['text':' You can think a more semantically accurate definition of DispatchKey is:','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':'    using DispatchKey = optional<RealDispatchKey>','line_number':139,'multiline':False]['text':'','line_number':140,'multiline':False]['text':' and Undefined == nullopt.  We didn't actually represent','line_number':141,'multiline':False]['text':' it this way because optional<RealDispatchKey> would take two','line_number':142,'multiline':False]['text':' words, when DispatchKey fits in eight bits.','line_number':143,'multiline':False]['text':' Define an alias for Undefined to represent CatchAll (long term','line_number':147,'multiline':False]['text':' this will get eliminated, but for now it's convenient)','line_number':148,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~ Functionality Keys ~~~~~~~~~~~~~~~~~~~~~~ //','line_number':151,'multiline':False]['text':' Every value in the enum (up to EndOfFunctionalityKeys)','line_number':152,'multiline':False]['text':' corresponds to an individual "functionality" that can be dispatched to.','line_number':153,'multiline':False]['text':' This is represented in the DispatchKeySet by assigning each of these enum','line_number':154,'multiline':False]['text':' values','line_number':155,'multiline':False]['text':' to each of the remaining (64 - len(BackendComponent)) bits.','line_number':156,'multiline':False]['text':'','line_number':157,'multiline':False]['text':' Most of these functionalities have a single handler assigned to them,','line_number':158,'multiline':False]['text':' making them "runtime keys".','line_number':159,'multiline':False]['text':' That map to a single slot in the runtime operator table.','line_number':160,'multiline':False]['text':'','line_number':161,'multiline':False]['text':' A few functionalities are allowed to be customizable per backend.','line_number':162,'multiline':False]['text':' See [Note: Per-Backend Functionality Dispatch Keys] for details.','line_number':163,'multiline':False]['text':' See [Note: Per-Backend Functionality Dispatch Keys]','line_number':165,'multiline':False]['text':' Below are non-extensible backends.','line_number':168,'multiline':False]['text':' These are backends that currently don't have their own overrides for','line_number':169,'multiline':False]['text':' Autograd/Sparse/Quantized kernels,','line_number':170,'multiline':False]['text':' and we therefore don't waste space in the runtime operator table allocating','line_number':171,'multiline':False]['text':' space for them.','line_number':172,'multiline':False]['text':' If any of these backends ever need to customize, e.g., Autograd, then we'll','line_number':173,'multiline':False]['text':' need to add a DispatchKey::*Bit for them.','line_number':174,'multiline':False]['text':' TODO: put this in BackendComponents','line_number':176,'multiline':False]['text':' Xilinx support lives out of tree at','line_number':177,'multiline':False]['text':' https://gitlab.com/pytorch-complex/vitis_kernels','line_number':178,'multiline':False]['text':' TODO: put this in BackendComponents','line_number':180,'multiline':False]['text':' ONNX Runtime, lives out of tree at https://github.com/pytorch/ort and','line_number':181,'multiline':False]['text':' https://github.com/microsoft/onnxruntime, and is also used to test general','line_number':182,'multiline':False]['text':' backend/extension machinery in the core. cf:','line_number':183,'multiline':False]['text':' - test/cpp_extensions/ort_extension.cpp','line_number':184,'multiline':False]['text':' - test/test_torch.py','line_number':185,'multiline':False]['text':' - aten/src/ATen/test/extension_backend_test.cpp','line_number':186,'multiline':False]['text':' TODO: put this in BackendComponents','line_number':189,'multiline':False]['text':' TODO: put this in BackendComponents','line_number':190,'multiline':False]['text':' See [Note: Per-Backend Functionality Dispatch Keys]','line_number':192,'multiline':False]['text':' This backend is to support custom RNGs; it lets you go','line_number':195,'multiline':False]['text':' to a different kernel if you pass in a generator that is not a','line_number':196,'multiline':False]['text':' traditional CPUGeneratorImpl/CUDAGeneratorImpl.  To make use of this','line_number':197,'multiline':False]['text':' key:','line_number':198,'multiline':False]['text':'  1) set it as a second parameter of at::Generator constructor call in','line_number':199,'multiline':False]['text':'     the user-defined PRNG class.','line_number':200,'multiline':False]['text':'  2) use it as a dispatch key while registering custom kernels','line_number':201,'multiline':False]['text':'     (templatized kernels specialized for user-defined PRNG class)','line_number':202,'multiline':False]['text':' intended for out of tree use; tested by aten/src/ATen/test/rng_test.cpp','line_number':203,'multiline':False]['text':' TODO: Make Mkldnn a functionality key, so we can give it Meta','line_number':206,'multiline':False]['text':' support','line_number':207,'multiline':False]['text':' Here are backends which specify more specialized operators','line_number':208,'multiline':False]['text':' based on the layout of the tensor.  Note that the sparse backends','line_number':209,'multiline':False]['text':' are one case where ordering matters: sparse multi-dispatches with','line_number':210,'multiline':False]['text':' the corresponding dense tensors, and must be handled before them.','line_number':211,'multiline':False]['text':' registered at build/aten/src/ATen/RegisterMkldnnCPU.cpp','line_number':212,'multiline':False]['text':' NB: not to be confused with MKLDNN, which is Caffe2 only','line_number':213,'multiline':False]['text':' See [Note: Per-Backend Functionality Dispatch Keys]','line_number':215,'multiline':False]['text':' TODO: Make SparseCsr a functionality key','line_number':218,'multiline':False]['text':' In some situations, it is not immediately obvious what the correct','line_number':224,'multiline':False]['text':' backend for function is, because the function in question doesn't','line_number':225,'multiline':False]['text':' have any "tensor" arguments.  In this case, a BackendSelect function','line_number':226,'multiline':False]['text':' can be registered to implement the custom determination of the','line_number':227,'multiline':False]['text':' correct backend.','line_number':228,'multiline':False]['text':' Out-of-core key for Fake Tensor in torchdistx.','line_number':233,'multiline':False]['text':' See https://pytorch.org/torchdistx/latest/fake_tensor.html','line_number':234,'multiline':False]['text':' TODO: delete this in favor of Python-implemented fake tensor','line_number':235,'multiline':False]['text':' See Note [Out-of-tree vmap+grad prototype]. The purpose of this key','line_number':237,'multiline':False]['text':' is to insert code after the "autograd subsystem" runs, so this key should','line_number':238,'multiline':False]['text':' be directly after ADInplaceOrView and all of the autograd keys.','line_number':239,'multiline':False]['text':' Alias and mutation removal.','line_number':242,'multiline':False]['text':' If some backends want to opt into only alias removal or only mutation','line_number':243,'multiline':False]['text':' removal,','line_number':244,'multiline':False]['text':' we can consider adding separate keys dedicated to those individual passes.','line_number':245,'multiline':False]['text':' See Note [Functionalization Pass In Core] for details.','line_number':246,'multiline':False]['text':' The named dispatch key is set for any tensors with named dimensions.','line_number':249,'multiline':False]['text':' Although we have a dispatch key for named tensors, for historical reasons,','line_number':250,'multiline':False]['text':' this dispatch key doesn't do any of the substantive functionality for named','line_number':251,'multiline':False]['text':' tensor (though, hypothetically, it could!)  At the moment, it's just','line_number':252,'multiline':False]['text':' responsible for letting us give good error messages when operations','line_number':253,'multiline':False]['text':' don't support named tensors.','line_number':254,'multiline':False]['text':'','line_number':255,'multiline':False]['text':' NB: If you ever consider moving named tensor functionality into','line_number':256,'multiline':False]['text':' this dispatch key, note that it might be necessary add another dispatch','line_number':257,'multiline':False]['text':' key that triggers before composite operators, in case a composite operator','line_number':258,'multiline':False]['text':' has named dimension propagation that doesn't match that of its','line_number':259,'multiline':False]['text':' constituent parts.','line_number':260,'multiline':False]['text':' TODO: delete this once torchdim lands in functorch','line_number':261,'multiline':False]['text':' The Conjugate dispatch key is set for any tensors that need to perform','line_number':264,'multiline':False]['text':' conjugation','line_number':265,'multiline':False]['text':' This is implemented at a dispatch level right before any backends run','line_number':266,'multiline':False]['text':' The Negative dispatch key is set for any tensors that need to perform','line_number':269,'multiline':False]['text':' negation','line_number':270,'multiline':False]['text':' This is implemented at a dispatch level right before any backends run','line_number':271,'multiline':False]['text':' registered at build/aten/src/ATen/RegisterZeroTensor.cpp','line_number':274,'multiline':False]['text':' Note [ADInplaceOrView key]','line_number':276,'multiline':False]['text':' ADInplaceOrView key is used by inplace or view ops to register a kernel','line_number':277,'multiline':False]['text':' that does additional setup for future autograd computation.','line_number':278,'multiline':False]['text':'','line_number':279,'multiline':False]['text':' 1. For inplace ops this kernel does version bump','line_number':280,'multiline':False]['text':' 2. For view ops this kernel does `as_view` setup where we properly setup','line_number':281,'multiline':False]['text':'    DifferentiableViewMeta on the view tensors.','line_number':282,'multiline':False]['text':'','line_number':283,'multiline':False]['text':' For other ops it's fallthrough kernel since there's no extra','line_number':284,'multiline':False]['text':' work to do.','line_number':285,'multiline':False]['text':'','line_number':286,'multiline':False]['text':' Note [Dream: skip VariableType kernel when requires_grad=false]','line_number':287,'multiline':False]['text':'','line_number':288,'multiline':False]['text':' In an ideal world where we can skip VariableType kernel for inputs','line_number':289,'multiline':False]['text':' with requires_grad=false, instead of a fallthrough kernel, we'll','line_number':290,'multiline':False]['text':' register a kernel shown below to all functional ops as well:','line_number':291,'multiline':False]['text':' torch::Tensor my_functional_op(...) {','line_number':292,'multiline':False]['text':'   {','line_number':293,'multiline':False]['text':'     // Note for every op in VariableType, you need to go through','line_number':294,'multiline':False]['text':'     // `AutoDispatchBelowADInplaceOrView` guard exactly once to add the','line_number':295,'multiline':False]['text':'     // key to TLS excluded set. If you don't go through it at all,','line_number':296,'multiline':False]['text':'     // inplace/view ops called through `at::` inside your backend','line_number':297,'multiline':False]['text':'     // kernel will dispatch to ADInplaceOrView kernels and do a lot','line_number':298,'multiline':False]['text':'     // of extra work.','line_number':299,'multiline':False]['text':'     at::AutoDispatchBelowADInplaceOrView guard;','line_number':300,'multiline':False]['text':'     at::redispatch::my_functional_op(...);','line_number':301,'multiline':False]['text':'   }','line_number':302,'multiline':False]['text':' }','line_number':303,'multiline':False]['text':' But this work is currently blocked since it adds an extra dispatch','line_number':304,'multiline':False]['text':' for all ops and it's non-trivial overhead at model level(a few percents).','line_number':305,'multiline':False]['text':' Thus our current approach takes advantage of the fact every kernel go','line_number':306,'multiline':False]['text':' through VariableType kernel first and pulls the','line_number':307,'multiline':False]['text':' `at::AutoDispatchBelowADInplaceOrView` guard of functional ops','line_number':308,'multiline':False]['text':' up to the `VariableType` kernel. Thus we only add the extra dispatch','line_number':309,'multiline':False]['text':' to view/inplace ops to minimize its perf impact to real models.','line_number':310,'multiline':False]['text':' Note [Alias Dispatch Key : Autograd]','line_number':312,'multiline':False]['text':' All backends are oblivious to autograd; autograd is handled as a','line_number':313,'multiline':False]['text':' layer which happens on top of all backends. It inspects the autograd','line_number':314,'multiline':False]['text':' metadata of all inputs, determines what autograd metadata should be','line_number':315,'multiline':False]['text':' constructed by the output, and otherwise defers to the backend to','line_number':316,'multiline':False]['text':' actually do the numeric computation.  Autograd contains','line_number':317,'multiline':False]['text':' the bulk of this logic.','line_number':318,'multiline':False]['text':' Autograd is now an alias dispatch key which by default maps to all','line_number':320,'multiline':False]['text':' backend-specific autograd keys.','line_number':321,'multiline':False]['text':' Backend-specific allow backends to override the default kernel registered','line_number':322,'multiline':False]['text':' to Autograd key as needed.','line_number':323,'multiline':False]['text':' For example, XLA wants to define autograd for einsum directly.','line_number':324,'multiline':False]['text':' Registering a custom autograd implementation at the XLA key won't work','line_number':325,'multiline':False]['text':' because we process Autograd before XLA.  This key has higher priority and','line_number':326,'multiline':False]['text':' gets processed first.  You generally should NOT redispatch after handling','line_number':327,'multiline':False]['text':' autograd here (since that would result in execution of the Autograd','line_number':328,'multiline':False]['text':' operator, which you're trying to skip).  In AutogradXLA implementations,','line_number':329,'multiline':False]['text':' you are responsible for handling autograd yourself, or deferring to other','line_number':330,'multiline':False]['text':' operators which support autograd.','line_number':331,'multiline':False]['text':' Currently we only have backend-specific autograd keys for CPU/CUDA/XLA and','line_number':333,'multiline':False]['text':' reserved user-defined backends. All other in-tree backends share the','line_number':334,'multiline':False]['text':' AutogradOther key. We can add specific autograd key for those backends','line_number':335,'multiline':False]['text':' upon request.','line_number':336,'multiline':False]['text':' See [Note: Per-Backend Functionality Dispatch Keys]','line_number':339,'multiline':False]['text':' NestedTensor is an example of something that isn't a "real backend"','line_number':342,'multiline':False]['text':' (because it mostly consists of redispatching kernels)','line_number':343,'multiline':False]['text':' but it would like to override autograd functionality in C++.','line_number':344,'multiline':False]['text':' We can handle cases like this by adding an extra functionality key','line_number':345,'multiline':False]['text':' exclusively for handling autograd for NestedTensor.','line_number':346,'multiline':False]['text':' lives out of tree at','line_number':347,'multiline':False]['text':' https://github.com/pytorch/nestedtensor','line_number':348,'multiline':False]['text':' TODO: make Autocast a functionality key','line_number':353,'multiline':False]['text':' Autocasting precedes VariableTypeId, to ensure casts are autograd-exposed','line_number':354,'multiline':False]['text':' and inputs are saved for backward in the post-autocast type.','line_number':355,'multiline':False]['text':' AutocastXLA is only being used for TPUs. XLA GPUs continue to use','line_number':361,'multiline':False]['text':' AutocastCUDA.','line_number':362,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~ WRAPPERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //','line_number':366,'multiline':False]['text':' There are a number of alternative modes which may want to handle before','line_number':367,'multiline':False]['text':' autograd; for example, error checking, tracing, profiling or vmap.  They','line_number':368,'multiline':False]['text':' go here.','line_number':369,'multiline':False]['text':' See Note [Out-of-tree vmap+grad prototype]','line_number':371,'multiline':False]['text':' Dispatch key for BatchedTensorImpl wrapping a nested tensor.','line_number':373,'multiline':False]['text':' See Note [Out-of-tree vmap+grad prototype]','line_number':376,'multiline':False]['text':' This is the dispatch key for BatchedTensorImpl, which is used to implement','line_number':378,'multiline':False]['text':' batching rules for vmap.','line_number':379,'multiline':False]['text':' When we are inside a vmap, all tensors dispatch on this key.','line_number':382,'multiline':False]['text':' See Note: [DispatchKey::VmapMode usage] for more details.','line_number':383,'multiline':False]['text':' See Note [Out-of-tree vmap+grad prototype]','line_number':386,'multiline':False]['text':' Out-of-core key for Deferred Module Initialization in torchdistx.','line_number':388,'multiline':False]['text':' See https://pytorch.org/torchdistx/latest/deferred_init.html','line_number':389,'multiline':False]['text':' Used by Python key logic to know the set of tls on entry to the dispatcher','line_number':392,'multiline':False]['text':' This kernel assumes it is the top-most non-functorch-related DispatchKey.','line_number':393,'multiline':False]['text':' If you add a key above, make sure to update the fallback implementation for','line_number':394,'multiline':False]['text':' this.','line_number':395,'multiline':False]['text':' This key should be at the very top of the dispatcher','line_number':398,'multiline':False]['text':' See Note [Out-of-tree vmap+grad prototype]','line_number':399,'multiline':False]['text':' TESTING: This is intended to be a generic testing tensor type id.','line_number':401,'multiline':False]['text':' Don't use it for anything real; its only acceptable use is within a single','line_number':402,'multiline':False]['text':' process test.  Use it by creating a TensorImpl with this DispatchKey, and','line_number':403,'multiline':False]['text':' then registering operators to operate on this type id.  See','line_number':404,'multiline':False]['text':' aten/src/ATen/core/dispatch/backend_fallback_test.cpp for a usage example.','line_number':405,'multiline':False]['text':' TESTING: This is intended to be a generic testing tensor type id.','line_number':408,'multiline':False]['text':' Don't use it for anything real; its only acceptable use is within a ingle','line_number':409,'multiline':False]['text':' process test.  Use it by toggling the mode on and off via','line_number':410,'multiline':False]['text':' TESTING_ONLY_tls_generic_mode_set_enabled and then registering operators','line_number':411,'multiline':False]['text':' to operate on this type id.  See','line_number':412,'multiline':False]['text':' aten/src/ATen/core/dispatch/backend_fallback_test.cpp','line_number':413,'multiline':False]['text':' for a usage example','line_number':414,'multiline':False]['text':' This key is used for pre-dispatch tracing in make_fx.','line_number':417,'multiline':False]['text':' It has lower priority than the PythonDispatcher key','line_number':418,'multiline':False]['text':' because we use the PythonDispatcher to intercept the key from python,','line_number':419,'multiline':False]['text':' and avoid having to implement it in C++.','line_number':420,'multiline':False]['text':' This is a bypass that allows you to skip running the C++ dispatcher','line_number':423,'multiline':False]['text':' entirely','line_number':424,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //','line_number':427,'multiline':False]['text':' End of functionality keys.','line_number':428,'multiline':False]['text':' ~~~~~~~~~~~~~~ "Dense" Per-Backend Dispatch keys ~~~~~~~~~~~~~~~~~~~~ //','line_number':430,'multiline':False]['text':' Here are backends which you think of as traditionally specifying','line_number':431,'multiline':False]['text':' how to implement operations on some device.','line_number':432,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~ Alias Dispatch Keys ~~~~~~~~~~~~~~~~~~~~~~~~~~ //','line_number':449,'multiline':False]['text':' Note [Alias Dispatch Keys]','line_number':450,'multiline':False]['text':' Alias dispatch keys are synthetic dispatch keys which map to multiple','line_number':451,'multiline':False]['text':' runtime dispatch keys. Alisa keys have precedence, but they are always','line_number':452,'multiline':False]['text':' lower precedence than runtime keys. You can register a kernel to an','line_number':453,'multiline':False]['text':' alias key, the kernel might be populated to the mapped runtime keys','line_number':454,'multiline':False]['text':' during dispatch table computation.','line_number':455,'multiline':False]['text':' If a runtime dispatch key has multiple kernels from alias keys, which','line_number':456,'multiline':False]['text':' kernel wins is done based on the precedence of alias keys (but runtime','line_number':457,'multiline':False]['text':' keys always have precedence over alias keys).','line_number':458,'multiline':False]['text':' Alias keys won't be directly called during runtime.','line_number':459,'multiline':False]['text':' See Note [Alias Dispatch Key : Autograd]','line_number':461,'multiline':False]['text':' registered at','line_number':463,'multiline':False]['text':' build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp','line_number':464,'multiline':False]['text':' Note: The alias keyset for FuncTorchBatchedDecomposition is disjoint from','line_number':466,'multiline':False]['text':' all','line_number':467,'multiline':False]['text':' other alias keysets','line_number':468,'multiline':False]['text':' and so precedence order doesn't matter','line_number':469,'multiline':False]['text':' registered at','line_number':470,'multiline':False]['text':' build/aten/src/ATen/RegisterFuncTorchBatchedDecomposition.cpp','line_number':471,'multiline':False]['text':' Note: The alias keyset for CompositeImplicitAutogradNestedTensor is','line_number':472,'multiline':False]['text':' disjoint from all other alias keysets','line_number':473,'multiline':False]['text':' registered at','line_number':474,'multiline':False]['text':' build/aten/src/ATen/RegisterCompositeImplicitAutogradNestedTensor.cpp','line_number':475,'multiline':False]['text':' registered at','line_number':476,'multiline':False]['text':' build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp','line_number':477,'multiline':False]['text':' See Note [CompositeExplicitAutogradNonFunctional Key]','line_number':478,'multiline':False]['text':' registered at','line_number':479,'multiline':False]['text':' build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp','line_number':480,'multiline':False]['text':' Define an alias key to represent end of alias dispatch keys.','line_number':482,'multiline':False]['text':' If you add new alias keys after Autograd, please also update it here.','line_number':483,'multiline':False]['text':'','line_number':485,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~ BC ALIASES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //','line_number':487,'multiline':False]['text':' The aliases exist for backwards compatibility reasons, they shouldn't','line_number':488,'multiline':False]['text':' be used','line_number':489,'multiline':False]['text':' Note [Private use DispatchKey]','line_number':499,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':500,'multiline':False]['text':' Private use tensor IDs are preallocated tensor type IDs for use in user','line_number':501,'multiline':False]['text':' applications.  Similar to private use fields in HTTP, they can be used','line_number':502,'multiline':False]['text':' by end users for experimental or private applications, without needing','line_number':503,'multiline':False]['text':' to "standardize" the tensor ID (which would be done by submitting a PR','line_number':504,'multiline':False]['text':' to PyTorch to add your type ID).','line_number':505,'multiline':False]['text':'','line_number':506,'multiline':False]['text':' Private use tensor IDs are appropriate to use if you want to experiment','line_number':507,'multiline':False]['text':' with adding a new tensor type (without having to patch PyTorch first) or','line_number':508,'multiline':False]['text':' have a private, non-distributed application that needs to make use of a','line_number':509,'multiline':False]['text':' new tensor type.  Private use tensor IDs are NOT appropriate to use for','line_number':510,'multiline':False]['text':' libraries intended to be distributed to further users: please contact','line_number':511,'multiline':False]['text':' the PyTorch developers to get a type ID registered in this case.','line_number':512,'multiline':False]['text':'','line_number':513,'multiline':False]['text':' We provide two classes of private user tensor id: regular DispatchKeys','line_number':514,'multiline':False]['text':' and Autograd DispatchKeys.  DispatchKeys serve the role of ordinary "backend"','line_number':515,'multiline':False]['text':' DispatchKeys; if you were adding support for a new type of accelerator, you','line_number':516,'multiline':False]['text':' would use a backend DispatchKey, and ideally automatically reuse','line_number':517,'multiline':False]['text':' AutogradOther definitions already defined in PyTorch.  AutogradPrivateUse','line_number':518,'multiline':False]['text':' DispatchKeys serve as "wrapper" DispatchKeys: they are only necessary for','line_number':519,'multiline':False]['text':' tensors that compose multiple internal tensors, and for cases when the','line_number':520,'multiline':False]['text':' built-in autograd formulas for operators are not appropriate.','line_number':521,'multiline':False]['text':' Check if a DispatchKey is an alias mapping to other runtime keys.','line_number':530,'multiline':False]['text':' [Note: Per-Backend Functionality Dispatch Keys]','line_number':535,'multiline':False]['text':' Check if a DispatchKey is a per-backend functionality key','line_number':536,'multiline':False]['text':' Any functionalities that can be customized per-backend should be added here.','line_number':537,'multiline':False]['text':' These keys correspond to functionalities that can be customized individually','line_number':538,'multiline':False]['text':' per backend. While they only take up one bit in the `DispatchKeySet` bitset,','line_number':539,'multiline':False]['text':' they map to (# backends) slots in the operator table.','line_number':540,'multiline':False]['text':' Each of these keys also has a separate set of "runtime keys" in the dispatch','line_number':541,'multiline':False]['text':' key enum, per backend, which *do* map to the individual operator table slots.','line_number':542,'multiline':False]['text':' For example, the "Sparse" key maps to an individual bit in the','line_number':543,'multiline':False]['text':' DispatchKeySet, while `SparseCPU`, `SparseCUDA`, etc all map to individual','line_number':544,'multiline':False]['text':' slots in the runtime operator table.','line_number':545,'multiline':False]['text':' Note that this includes Undefined in the total count.','line_number':557,'multiline':False]['text':' BUT EndOfFunctionalityKeys is its own (placeholder) key.','line_number':558,'multiline':False]['text':' e.g. Undefined=0, Dense=1, Sparse=2, EndOfFunctionalityKeys=3.','line_number':559,'multiline':False]['text':' In the above example, there are 3 total functionality keys.','line_number':560,'multiline':False]['text':' Note [No More Than 16 Backends]','line_number':567,'multiline':False]['text':' Search for this note to find places in the code where the "no more than 16','line_number':568,'multiline':False]['text':' backends" invariant is baked in.','line_number':569,'multiline':False]['text':' See [Note: Trimmed Mobile Dispatch Keys]','line_number':585,'multiline':False]['text':' See Note [No More Than 16 Backends]','line_number':592,'multiline':False]['text':' Parses a string into a dispatch key.','line_number':603,'multiline':False]['text':' If the string cannot be correctly parsed, throws an exception.','line_number':604,'multiline':False]['text':' These are some convenience identifiers for dispatch keys which are','line_number':607,'multiline':False]['text':' shorter to type than their long counterparts.  Note that some of these','line_number':608,'multiline':False]['text':' dispatch keys directly correspond to DeviceType; and most APIs that','line_number':609,'multiline':False]['text':' accept DispatchKey also accept DeviceType; e.g.,','line_number':610,'multiline':False]['text':' torch::dispatch(torch::kCPU, ...) is also valid.','line_number':611,'multiline':False]['text':' See Note [The Ordering of Per-Backend Dispatch Keys Matters!]','line_number':614,'multiline':False]['text':' This function relies on the invariant that the dispatch keys between','line_number':615,'multiline':False]['text':' StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend','line_number':616,'multiline':False]['text':' in the same order as `BackendComponent`.','line_number':617,'multiline':False]['text':' Given (DispatchKey::Dense, BackendComponent::CUDABit), returns','line_number':674,'multiline':False]['text':' DispatchKey::CUDA.','line_number':675,'multiline':False]['text':' See Note [The Ordering of Per-Backend Dispatch Keys Matters!]','line_number':676,'multiline':False]['text':' This function relies on the invariant that the dispatch keys between','line_number':677,'multiline':False]['text':' StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend','line_number':678,'multiline':False]['text':' in the same order as `BackendComponent`.','line_number':679,'multiline':False]['text':' namespace c10','line_number':712,'multiline':False]['text':' Expose the constant, but not the TYPE (DispatchKey is an implementation','line_number':715,'multiline':False]['text':' detail!)','line_number':716,'multiline':False]['text':' namespace torch','line_number':718,'multiline':False]['text':' NB: You really shouldn't use this instance; this enum is guaranteed','line_number':720,'multiline':False]['text':' to be pretty small so a regular array should be acceptable.','line_number':721,'multiline':False]['text':' namespace std','line_number':732,'multiline':False]