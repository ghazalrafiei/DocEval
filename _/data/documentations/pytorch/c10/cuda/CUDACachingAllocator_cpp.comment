['text':' Included here as this is externally used in CUDAAllocatorConfig','line_number':49,'multiline':False]['text':' "large" allocations may be packed in 20 MiB blocks','line_number':51,'multiline':False]['text':'','line_number':55,'multiline':False]['text':' Yet another caching allocator for CUDA device allocations.','line_number':56,'multiline':False]['text':'','line_number':57,'multiline':False]['text':' - Allocations are associated with a stream. Once freed, blocks can be','line_number':58,'multiline':False]['text':'   re-allocated on the same stream, but not on any other stream.','line_number':59,'multiline':False]['text':' - The allocator attempts to find the smallest cached block that will fit the','line_number':60,'multiline':False]['text':'   requested size. If the block is larger than the requested size, it may be','line_number':61,'multiline':False]['text':'   split. If no block is found, the allocator will delegate to cudaMalloc.','line_number':62,'multiline':False]['text':' - If the cudaMalloc fails, the allocator will attempt to free one cached','line_number':63,'multiline':False]['text':'   block of sufficient size that is not split and retry the allocation.','line_number':64,'multiline':False]['text':'   If this also fails, the allocator will attempt to free all cached blocks','line_number':65,'multiline':False]['text':'   that are not split and retry the allocation.','line_number':66,'multiline':False]['text':' - Large (>1MB) and small allocations are stored in separate pools.','line_number':67,'multiline':False]['text':'   Small requests are packed into 2MB buffers. Large requests will use the','line_number':68,'multiline':False]['text':'   smallest available free block or allocate a new block using cudaMalloc.','line_number':69,'multiline':False]['text':' - To reduce fragmentation, requests between 1MB and 10MB will allocate and','line_number':70,'multiline':False]['text':'   split a 20MB block, if no free block of sufficient size is available.','line_number':71,'multiline':False]['text':' - To further reduce fragmentation, blocks >= 200MB are not allowed to be','line_number':72,'multiline':False]['text':'   split. These oversize cached blocks will still satisfy requests within','line_number':73,'multiline':False]['text':'   20MB of the oversize cached block size.','line_number':74,'multiline':False]['text':'','line_number':75,'multiline':False]['text':' With this allocator, allocations and frees should logically be considered','line_number':76,'multiline':False]['text':' "usages" of the memory segment associated with streams, just like kernel','line_number':77,'multiline':False]['text':' launches. The programmer must insert the proper synchronization if memory','line_number':78,'multiline':False]['text':' segments are used from multiple streams.','line_number':79,'multiline':False]['text':'','line_number':80,'multiline':False]['text':' The library provides a recordStream() function to help insert the correct','line_number':81,'multiline':False]['text':' synchronization when allocations are used on multiple streams. This will','line_number':82,'multiline':False]['text':' ensure that the block is not reused before each recorded stream completes','line_number':83,'multiline':False]['text':' work.','line_number':84,'multiline':False]['text':'','line_number':85,'multiline':False]['text':'*
 * Note [Interaction with CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Graph capture performs a dry run of a region of execution, freezing all CUDA
 * work (and virtual addresses used during that work) into a "graph." The graph
 * may be "replayed" like a single giant kernel, with greatly reduced CPU
 * overhead as well as modestly improved GPU performance.
 *
 * Because capture bakes in memory addresses, the memory used during capture
 * must be available for the graph to use during replay. DeviceCachingAllocator
 * assigns and frees memory eagerly and dynamically, so if we're not careful
 * about managing graphs' memory, at replay time those memory addresses could be
 * used by other tensors.
 *
 * To guarantee a graph's baked in addresses are safe to reuse in replay,
 * DeviceAllocator satisfies allocations from a graph-private memory pool during
 * capture, and doesn't begin cudaFreeing those addresses until the graph is
 * destroyed.
 *
 * Within the private pool, allocations are freed and reassigned as usual during
 * capture. Memory regions will be used in a consistent order during replay. So
 * a private pool doesn't use memory more wastefully than the default pools
 * during capture, but it does reserve its high-water mark of used memory away
 * from the default pools as long as the capture(s) it served survive
 * (regardless whether those captures are idle or replaying).
 *
 * CUDAGraph's requests for private pools are mediated by
 * DeviceAllocator::notifyCaptureBegin,
 *                  notifyCaptureAboutToEnd,
 *                  notifyCaptureEnded,
 *                  notifyCaptureDestroy.
 ','line_number':87,'multiline':True]['text':' all sizes are rounded to at least 512 bytes','line_number':121,'multiline':False]['text':' largest "small" allocation is 1 MiB','line_number':122,'multiline':False]['text':' "small" allocations are packed in 2 MiB blocks','line_number':124,'multiline':False]['text':' allocations between 1 and 10 MiB may use kLargeBuffer','line_number':126,'multiline':False]['text':' round up large allocations to 2 MiB','line_number':127,'multiline':False]['text':' gpu','line_number':200,'multiline':False]['text':' allocation stream','line_number':201,'multiline':False]['text':' streams on which the block was used','line_number':202,'multiline':False]['text':' block size in bytes','line_number':203,'multiline':False]['text':' memory originally requested','line_number':204,'multiline':False]['text':' owning memory pool','line_number':205,'multiline':False]['text':' memory address','line_number':206,'multiline':False]['text':' in-use flag','line_number':207,'multiline':False]['text':' is the virtual address range this Block references','line_number':208,'multiline':False]['text':' backed by physical pages. Always true when','line_number':209,'multiline':False]['text':' expandable_segment_ is null. When false','line_number':210,'multiline':False]['text':' This Block will be aligned to the segment size','line_number':211,'multiline':False]['text':' of its expandable_segment_.','line_number':212,'multiline':False]['text':' prev block if split from a larger allocation','line_number':213,'multiline':False]['text':' next block if split from a larger allocation','line_number':214,'multiline':False]['text':' number of outstanding CUDA events','line_number':215,'multiline':False]['text':' counter for prioritizing older / less useful blocks for','line_number':216,'multiline':False]['text':' garbage collection','line_number':217,'multiline':False]['text':' only set for the first block in the segment (when prev == null)','line_number':219,'multiline':False]['text':' this records the frame information when cudaMalloc was called','line_number':220,'multiline':False]['text':' whereas context_when_allocated records the last time we handed this','line_number':221,'multiline':False]['text':' memory out from our cache.','line_number':222,'multiline':False]['text':' constructor for search key','line_number':241,'multiline':False]['text':'
Note [Expandable Segments]

Rationale

For large (>2MB) allocations, the allocator calls cudaMalloc to get allocations
that are the same size as what the user requests. In the future, parts of these
allocations can be reused for other requests if they are free. This works well
when the program makes many requests of exactly the same size or of sizes that
even multiples of that size. Many deep learning models follow this behavior.
However, one common exception is when the batch size changes slightly from one
iteration to the next, e.g. in batched inference. When the program runs
initially with batch size N, it will make allocations appropriate for that size.
If in the future, it runs at size N - 1, the existing allocations will still be
big enough. However, if it runs at size N + 1, then it will have to make new
allocations that are slightly larger. Not all the tensors are the same size.
Some might be (N + 1)*A and others (N + 1)*A*B where A and B are some non-batch
dimensions in the model. Because the allocator reuses existing allocations when
they are big enough, some number of (N + 1)*A allocations will actually fit in
the already existing N*B*A segments, though not perfectly. As the model runs it
will partially fill up all of these segments leaving unusable free slices of
memory at the end of these segments. The allocator at some point will need to
cudaMalloc a new (N + 1)*A*B segment. If there is not enough memory, there is
now no way to recover the slices of memory that are free at the end of existing
segments. With models 50+ layers deep, this pattern might repeat 50+ times
creating many slivers.

Approach

Expandable segments allows the allocator to create a segment initially and then
expand its size later when more memory is needed. Instead of making one segment
per allocation, it tries to make one segment (per stream) that grows as
necessary. Now when the N + 1 case runs, the allocations will tile nicely into
the one large segment until it fills up. Then more memory is requested and
appended to the end of the segment. This process does not create as many slivers
of unusable memory, so it is more likely to succeed at finding this memory.

Implementation

The expandable_segments:True option is used to enable/disable this behavior. We
use cuda's low-level memory APIs, which are similar to mmap, to extend the
memory segments. These APIs separate the allocation of physical memory
(cuMemCreate) from the allocation of virtual address space (cuMemAddressReserve)
and the associate between them cuMemMap/cuMemSetAccess.

When we allocate a new segment, we allocate enough address space to map
basically the entire physical memory of the GPU (there is 256TiB of address
space), but we only map enough physical memory to handle the current amount of
memory needed by the program. As more is requested, we add more physical memory
to the segment. This can work at the granularity of GPU pages which are 2MiB
currently.

If we end up out of memory, we can unmap all the memory in our segment
corresponding to empty physical pages, and return it to CUDA for use at another
address in the segment or in a segment for a different stream.

A current limitation of CUDA's API is that physical memory
(CUmemGenericAllocationHandle) cannot be split up after it is mapped even if the
handle holds multiple GPU pages. The cost to map/unmap memory is proportional to
the number of physical memory chunks that were allocated (mapping 10 separately
allocated 2MiB pages takes 10x time compared to mapping one 20MiB physical
allocation of 10 pages).  Changing memory mappings also appears to involve at
least some synchronous actions with the GPU and so should be considered an
expensive operation. To limit overhead, we use 2MiB pages for our small pool and
20MiB pages for our large pool. Initially allocation using expandable_blocks
will be slower than cudaMalloc, though still in the milliseconds range for
mapping the entire memory.

When mapping new memory to expand the segment, we look for the lowest address at
which we can fit a new allocation by adding new pages. Normally this will be at
the end of the block. But if have previously unmapped blocks earlier in the
segment during an OOM, it will first try to fill in those gaps to keep the
segment as a single block. By allocating at the lowest address we encourage
the split up parts of the block to merge into a single block again, reducing
fragmentation potential.

Allocation of blocks in the segment uses the same best-fit heuristics of the
rest of the allocator.

Expandable blocks can be enabled/disabled throughout the run of a program. When
disabled, the allocator will not put new allocations in an expandable block.

Limitations

* Slightly slower initial memory allocation speed.
* IPC of cuda tensors (e.g. for multiprocess dataloaders) is not supported.
However, it is possible to temporarily disable (expandable_segments:False) the
bevhavior for allocator tensors that need to be used cross-process.
* CUDA runtime APIs related to sharing memory across process
(cudaDeviceEnablePeerAccess) do not work for memory allocated with cuMemMap.
Instead these mapping have to be done manually. The allocator now has an
`enablePeerAccess` method to do this.
','line_number':274,'multiline':True]['text':' 2MB for small pool, 20MB for large pool','line_number':377,'multiline':False]['text':' we allocate enough address space for 1 1/8 the total memory on the GPU.','line_number':382,'multiline':False]['text':' This allows for some cases where we have to unmap pages earlier in the','line_number':383,'multiline':False]['text':' segment to put them at the end.','line_number':384,'multiline':False]['text':' begin must be aligned to segment_size_.','line_number':389,'multiline':False]['text':' returns the actual range mapped, which may be','line_number':390,'multiline':False]['text':' greater than requested if size is not aligned to segment_size_.','line_number':391,'multiline':False]['text':' return size of 0 indicates OOM','line_number':392,'multiline':False]['text':' unmaps all the completely empty segment_size_ segments between','line_number':440,'multiline':False]['text':' [begin, begin + size), returns the offset where the range begin,','line_number':441,'multiline':False]['text':' and the actual size unmapped (multiple of segment_size_)','line_number':442,'multiline':False]['text':' note: unlike cudaFree, MemUnmap and MemRelease do','line_number':484,'multiline':False]['text':' not appear to synchronize in all cases, so we have to wait for the','line_number':485,'multiline':False]['text':' stream to finish before this memory is truly free.','line_number':486,'multiline':False]['text':' cannot call c10::cuda::stream_synchronize because','line_number':488,'multiline':False]['text':' it might grab the GIL which can lead to a deadlock','line_number':489,'multiline':False]['text':' Locking order must be GIL -> Allocator Lock','line_number':490,'multiline':False]['text':' devices on which this memory should be mapped in addition','line_number':538,'multiline':False]['text':' to the device where the physical memory lives (device_).','line_number':539,'multiline':False]['text':' BlockState, BlockPoolState, and PrivatePoolState contain the information','line_number':567,'multiline':False]['text':' needed to reconstruct a private pool to a previous state. See note','line_number':568,'multiline':False]['text':' [Checkpointing PrivatePoolState]','line_number':569,'multiline':False]['text':' maintain invariant that event_count == 0 ;','line_number':578,'multiline':False]['text':' history will be left alone in checkpoint','line_number':579,'multiline':False]['text':' omitting use_count, and cudaMalloc_count as they remain the same','line_number':592,'multiline':False]['text':' Note: cudaEventCreate when concurrently invoked from multiple threads can be','line_number':655,'multiline':False]['text':' very expensive (at least on certain device/driver combinations). Thus, we a)','line_number':656,'multiline':False]['text':' serialize event creation at a per-device level, and b) pool the events to','line_number':657,'multiline':False]['text':' avoid constantly calling cudaEventCreate/cudaEventDestroy. This results in','line_number':658,'multiline':False]['text':' significant improvements in multithreaded workloads with high allocation','line_number':659,'multiline':False]['text':' rates.','line_number':660,'multiline':False]['text':' TODO: Explicit device count','line_number':664,'multiline':False]['text':' Try to acquire an event from the per-device pool.','line_number':676,'multiline':False]['text':' otherwise, allocate a new event that will be returned to the pool on','line_number':685,'multiline':False]['text':' destruction.','line_number':686,'multiline':False]['text':' CUDA graphs helper','line_number':709,'multiline':False]['text':'small=','line_number':714,'multiline':True]['text':'small=','line_number':715,'multiline':True]['text':' Number of live graphs using this pool','line_number':719,'multiline':False]['text':' Number of unfreed cudaMallocs made for this pool. When use_count and','line_number':721,'multiline':False]['text':' cudaMalloc_count drop to zero, we can delete this PrivatePool from','line_number':722,'multiline':False]['text':' graph_pools.','line_number':723,'multiline':False]['text':' Instead of maintaining private BlockPools here, I could stuff all blocks','line_number':725,'multiline':False]['text':' (private or no) into the top-level large_blocks and small_blocks, and','line_number':726,'multiline':False]['text':' distinguish private blocks by adding a "pool id" check above the stream','line_number':727,'multiline':False]['text':' check in BlockComparator. BlockComparator is performance- critical though,','line_number':728,'multiline':False]['text':' I'd rather not add more logic to it.','line_number':729,'multiline':False]['text':' It's ok to capture cudaMallocs, as long as we never cudaFree those','line_number':778,'multiline':False]['text':' addresses before replay.','line_number':779,'multiline':False]['text':' Capturing cudaMalloc behaves nicely: it gives the graph new VA,','line_number':780,'multiline':False]['text':' but is ignored (won't leakily allocate new memory) in replays.','line_number':781,'multiline':False]['text':' anonymous namespace','line_number':788,'multiline':False]['text':' namespace Native','line_number':789,'multiline':False]['text':' lock around all operations','line_number':851,'multiline':False]['text':' device statistics','line_number':854,'multiline':False]['text':' unallocated cached blocks larger than 1 MB','line_number':857,'multiline':False]['text':' unallocated cached blocks 1 MB or smaller','line_number':860,'multiline':False]['text':' allocated or in use by a stream. Holds all active allocations,','line_number':863,'multiline':False]['text':' whether they came from graph_pools or one of the BlockPools above.','line_number':864,'multiline':False]['text':' captures_underway tracks if a capture might be underway on any stream.','line_number':867,'multiline':False]['text':' Most of the time it's zero, in which case malloc can avoid calling','line_number':868,'multiline':False]['text':' cudaStreamGetCaptureInfo in the hot path.','line_number':869,'multiline':False]['text':' See free() for this thing's purpose','line_number':871,'multiline':False]['text':' outstanding cuda events','line_number':873,'multiline':False]['text':' record used memory.','line_number':879,'multiline':False]['text':' all live expandable segments','line_number':884,'multiline':False]['text':' pointer because we need to intentionally leak this on','line_number':897,'multiline':False]['text':' deallocation it can hold references to Python state which','line_number':898,'multiline':False]['text':' will already be destroyed when we are in exit handlers','line_number':899,'multiline':False]['text':' Members specific to CUDA graphs','line_number':901,'multiline':False]['text':' Private pools for CUDA graphs','line_number':903,'multiline':False]['text':' Pools no longer referenced by any graph. Their BlockPools are eligible for','line_number':906,'multiline':False]['text':' free_blocks. Can't be a vector or deque because we might erase entries in','line_number':907,'multiline':False]['text':' any order. Could be an std::list, but we don't care much, access and','line_number':908,'multiline':False]['text':' insert/erase are rare.','line_number':909,'multiline':False]['text':' Indicates that a current stream should be allocated to a pool','line_number':913,'multiline':False]['text':' rather than the global memory.','line_number':914,'multiline':False]['text':' XXX - maybe we should generalize and have multiple events','line_number':917,'multiline':False]['text':'small=','line_number':924,'multiline':True]['text':'small=','line_number':925,'multiline':True]['text':' Must be called outside of `mutex` or deadlocks are possible with Python','line_number':984,'multiline':False]['text':' All public methods (except the above) acquire the allocator mutex.','line_number':992,'multiline':False]['text':' Thus, do not call a public method from another public method.','line_number':993,'multiline':False]['text':' done outside the lock because we don't know what locks the recorder needs','line_number':996,'multiline':False]['text':' to have...','line_number':997,'multiline':False]['text':' Processes end-of-life events for outstanding allocations used on','line_number':1003,'multiline':False]['text':' multiple streams (checks if their GPU-side uses are complete and','line_number':1004,'multiline':False]['text':' recycles their memory if so)','line_number':1005,'multiline':False]['text':'','line_number':1006,'multiline':False]['text':' Q. Why skip process_events if a capture might be underway?','line_number':1007,'multiline':False]['text':' A. process_events involves cudaEventQueries, illegal during CUDA graph','line_number':1008,'multiline':False]['text':'    capture.','line_number':1009,'multiline':False]['text':'    Dumb simple solution: defer reclaiming these allocations until after','line_number':1010,'multiline':False]['text':'    capture. Cross-stream memory use is uncommon, so the deferral's','line_number':1011,'multiline':False]['text':'    effect on memory use during capture should be small.','line_number':1012,'multiline':False]['text':' First, try to get a block from the existing pool.','line_number':1021,'multiline':False]['text':' Search pool','line_number':1023,'multiline':False]['text':' Trigger callbacks and retry search','line_number':1025,'multiline':False]['text':' Can't reuse an existing block; try to get a new one.','line_number':1028,'multiline':False]['text':' Do garbage collection if the flag is set.','line_number':1030,'multiline':False]['text':' Attempt allocate','line_number':1036,'multiline':False]['text':' WARNING: alloc_block may release the allocator lock when calling','line_number':1037,'multiline':False]['text':' cudaMalloc. So far this function has not modified allocator state, but','line_number':1038,'multiline':False]['text':' keep in mind that any observed allocator state may change across calls','line_number':1039,'multiline':False]['text':' to alloc_block since it may release the lock.','line_number':1040,'multiline':False]['text':' Free enough available cached blocks to satisfy alloc and retry','line_number':1042,'multiline':False]['text':' alloc.','line_number':1043,'multiline':False]['text':' Free all non-split cached blocks and retry alloc.','line_number':1046,'multiline':False]['text':' For any error code other than cudaErrorMemoryAllocation,','line_number':1053,'multiline':False]['text':' alloc_block should have thrown an exception already.','line_number':1054,'multiline':False]['text':' Make sure we do not have the device lock before calling our','line_number':1093,'multiline':False]['text':' observers which might need hold the GIL','line_number':1094,'multiline':False]['text':' It is safe to release at this point because will no longer','line_number':1095,'multiline':False]['text':' be reading any allocator state.','line_number':1096,'multiline':False]['text':' "total capacity": total global memory on GPU','line_number':1107,'multiline':False]['text':' "allowed": memory is allowed to use, which set by fraction.','line_number':1108,'multiline':False]['text':' "already allocated": memory allocated by the program using the','line_number':1109,'multiline':False]['text':'                      caching allocator','line_number':1110,'multiline':False]['text':' "free": free memory as reported by the CUDA API','line_number':1111,'multiline':False]['text':' "cached": memory held by the allocator but not used by the program','line_number':1112,'multiline':False]['text':'','line_number':1113,'multiline':False]['text':' The "allocated" amount  does not include memory allocated outside','line_number':1114,'multiline':False]['text':' of the caching allocator, such as memory allocated by other programs','line_number':1115,'multiline':False]['text':' or memory held by the driver.','line_number':1116,'multiline':False]['text':'','line_number':1117,'multiline':False]['text':' The sum of "allocated" + "free" + "cached" may be less than the','line_number':1118,'multiline':False]['text':' total capacity due to memory held by the driver and usage by other','line_number':1119,'multiline':False]['text':' programs.','line_number':1120,'multiline':False]['text':'','line_number':1121,'multiline':False]['text':' Note that at this point free_cached_blocks has already returned all','line_number':1122,'multiline':False]['text':' possible "cached" memory to the driver. The only remaining "cached"','line_number':1123,'multiline':False]['text':' memory is split from a larger block that is partially in-use.','line_number':1124,'multiline':False]['text':' An already-split inactive block is being shrunk by size bytes.','line_number':1189,'multiline':False]['text':' A new split inactive block is being created from a previously unsplit','line_number':1195,'multiline':False]['text':' block, size remaining->size bytes.','line_number':1196,'multiline':False]['text':' An already-split block is becoming active','line_number':1206,'multiline':False]['text':' following logic might modifying underlaying Block, causing the size','line_number':1263,'multiline':False]['text':' changed. We store ahead for reporting','line_number':1264,'multiline':False]['text':' It's forbidden to cudaEventQuery an event recorded during CUDA graph','line_number':1289,'multiline':False]['text':' capture. We conservatively defer recording end-of-life events until','line_number':1290,'multiline':False]['text':' the next call to process_events() (which won't happen until no','line_number':1291,'multiline':False]['text':' captures are underway)','line_number':1292,'multiline':False]['text':' ignore uses on the allocation stream, since those don't require any','line_number':1332,'multiline':False]['text':' special synchronization','line_number':1333,'multiline':False]['text':'* set memory fraction to limit maximum allocated memory *','line_number':1339,'multiline':True]['text':'* returns cached blocks to the system allocator *','line_number':1348,'multiline':True]['text':'* Retrieves size of largest unused block held by the memory cache *','line_number':1355,'multiline':True]['text':' make an initial guess if a zero *largest is passed in','line_number':1359,'multiline':False]['text':' Use free memory as an optimistic initial guess of *largest','line_number':1362,'multiline':False]['text':'* Returns a copy of the memory allocator stats *','line_number':1373,'multiline':True]['text':'* Resets the historical accumulation stats for the device *','line_number':1379,'multiline':True]['text':'* Resets the historical peak stats for the device *','line_number':1402,'multiline':True]['text':' Checkpoint the state of a private pool necessary to return it to its
   * current state ','line_number':1422,'multiline':True]['text':' When we free a block, its pointer should never change','line_number':1455,'multiline':False]['text':' only its adjacent blocks, so free, then look at pointer','line_number':1456,'multiline':False]['text':' checkpoint the state of an allocation that may have been','line_number':1478,'multiline':False]['text':' split into multiple blocks','line_number':1479,'multiline':False]['text':' allocate all blocks in the segment','line_number':1492,'multiline':False]['text':' splitting a block depends on `max_split_size`, which may have changed','line_number':1506,'multiline':False]['text':' between whe checkpoint was taken and now, so we make sure to recreate','line_number':1507,'multiline':False]['text':' the behavior from the checkpoint.','line_number':1508,'multiline':False]['text':' curr_block will become next pointer if it is split, so reassign with','line_number':1511,'multiline':False]['text':' the returned value','line_number':1512,'multiline':False]['text':' free blocks that are not allocated in the checkpoint','line_number':1529,'multiline':False]['text':'*
   * Note [Checkpointing PrivatePoolState]
   *
   * Refer above to Note [Interaction with CUDA graph capture]. Allocations made
   * during graph capture are made from a separate private pool. During graph
   * capture allocations behave as usual. During graph replay the allocator
   * state does not change even as new tensors are created. The private pool
   * will not free its blocks to the main caching allocator until cuda graph use
   * is finished to prevent an allocation from eager clobbering the memory from
   * a live but unaccounted for tensor that was created during replay.
   *
   * `make_graphed_callables`, a series of separate callables chained in
   * successive cuda graphs, can share a memory pool because after a cuda graph
   * recording the allocations in the shared private pool exactly reflect the
   * tensors that are allocated.
   *
   * We would like to extend callable chaining to support a graphed callable
   * tree. In this scenario, we have a tree of callable chains which will be
   * captured with cuda graphs. In the diagram below, we have a tree with four
   * callables, A, B, C, and D. Suppose we have captured, and subsequently
   * replayed, A, B, and C. Then on a new invocation, we replay A and B, but
   * would now like to record D. At this point the private pool will not reflect
   * any of the live tensors created during graph replay. Allocations made
   * during a new recording with the pool could overwrite those live tensors.
   *
   * In order to record a new graph capture after replaying prior callables in
   * the tree, we need the allocator to reflect the state of the live tensors.
   * We checkpoint the state of the private pool after each recording, and then
   * reapply it when we are starting a new recording chain. Additionally, we
   * must free the allocations for any tensors that died between the end of our
   * previous graph replaying and our new recording. All of the allocated
   * segments that existed in the checkpointed state must still exist in the
   * pool. There may also exist new allocated blocks.
   * (TODO : link note [live tensors between iterations] when it exists). For
   * every block that is currently allocated but no allocated in the snapshot,
   * we will return a pointer to their block.
   *.
   *
   *
   *  ---------------> A ---------------> B ---------------> C
   *                                      |
   *                                      |
   *                                      |
   *                                      |
   *                                      ╰ ---------------> D
   ','line_number':1549,'multiline':True]['text':' To reset the caching allocator state we will','line_number':1596,'multiline':False]['text':' - Free all the blocks currently allocated to the pool (see [live tensors','line_number':1597,'multiline':False]['text':' between iterations])','line_number':1598,'multiline':False]['text':' - Allocate all the blocks in a checkpointed segment, whether they are','line_number':1599,'multiline':False]['text':' live or not','line_number':1600,'multiline':False]['text':' - Free the blocks in a checkpointed segment which are not live','line_number':1601,'multiline':False]['text':' This could be optimized, but it nicely reuses exiting apis, and this','line_number':1602,'multiline':False]['text':' is not on the hot path.','line_number':1603,'multiline':False]['text':' following `done outside the lock because we don't know what locks the','line_number':1605,'multiline':False]['text':' recorder needs to have...`','line_number':1606,'multiline':False]['text':' at this point, all of the blocks should be free, so they will all be in','line_number':1627,'multiline':False]['text':' the block set','line_number':1628,'multiline':False]['text':'* Dump a complete snapshot of the memory held by the allocator. Potentially
   * VERY expensive. *','line_number':1646,'multiline':True]['text':' For expandable segments, we report one segment for each contiguous','line_number':1665,'multiline':False]['text':' mapped range of memory','line_number':1666,'multiline':False]['text':' Convert all the timestamps from tsc to epoch time in microseconds.','line_number':1734,'multiline':False]['text':' This function takes the size and number of divisions argument and rounds','line_number':1741,'multiline':False]['text':' up the size argument for the nearest power-of-2 division.','line_number':1742,'multiline':False]['text':' For example, if we need to round-up 1200 and number of divisions is 4,','line_number':1743,'multiline':False]['text':' the size 1200 lies between 1024 and 2048 and if we do 4 divisions between','line_number':1744,'multiline':False]['text':' them, the values are 1024, 1280, 1536, and 1792. So the function will','line_number':1745,'multiline':False]['text':' return 1280 as the nearest ceiling of power-2 divison.','line_number':1746,'multiline':False]['text':' divide the space between these 2's power into equal divisions','line_number':1755,'multiline':False]['text':' If division is zero, return the power-of-2 ceiling.','line_number':1756,'multiline':False]['text':' See Note [Interaction with CUDA graph capture]','line_number':1781,'multiline':False]['text':' Called by CUDAGraph::capture_begin','line_number':1783,'multiline':False]['text':' mempool_id does not reference an existing pool. Make a new pool for','line_number':1789,'multiline':False]['text':' this capture.','line_number':1790,'multiline':False]['text':' mempool_id references an existing pool, which the current capture will','line_number':1793,'multiline':False]['text':' share. Check this pool is live (at least one other capture already','line_number':1794,'multiline':False]['text':' references it).','line_number':1795,'multiline':False]['text':' Maps this stream to mempool_id and makes sure this graph_id wasn't','line_number':1800,'multiline':False]['text':' somehow assigned a mempool_id already. Keeps essential effect (insert)','line_number':1801,'multiline':False]['text':' out of macro.','line_number':1802,'multiline':False]['text':' Called by CUDAGraph::capture_end','line_number':1807,'multiline':False]['text':' Called by CUDAGraph::reset','line_number':1816,'multiline':False]['text':' The instantiated cudaGraphExec_t has been destroyed. We can't blindly','line_number':1819,'multiline':False]['text':' delete and cudaFree the mempool its capture used, because','line_number':1820,'multiline':False]['text':'  1. other graph(s) might share the same pool','line_number':1821,'multiline':False]['text':'  2. the user might still hold references to output tensors allocated','line_number':1822,'multiline':False]['text':'  during capture.','line_number':1823,'multiline':False]['text':' To handle 1 and 2, we track the number of graphs using this particular','line_number':1824,'multiline':False]['text':' mempool. When the count reaches 0, we tell free_cached_blocks it may now','line_number':1825,'multiline':False]['text':' cudaFree blocks from this graph's pool when it discovers they're unused','line_number':1826,'multiline':False]['text':' (unsplit).','line_number':1827,'multiline':False]['text':' Allows free_cached_blocks to begin cudaFreeing this pool's memory,','line_number':1833,'multiline':False]['text':' and makes sure this pool wasn't somehow made freeable already.','line_number':1834,'multiline':False]['text':' All private methods do not acquire the allocator mutex.','line_number':1859,'multiline':False]['text':' returns the smallest possible address in any segment','line_number':1904,'multiline':False]['text':' where there is enough free address space to fit size','line_number':1905,'multiline':False]['text':' may be composed of free and unmapped segments','line_number':1906,'multiline':False]['text':' we found the lowest address of an unmapped segment','line_number':1930,'multiline':False]['text':' but there might be a free segment we can also use','line_number':1931,'multiline':False]['text':' right before it','line_number':1932,'multiline':False]['text':' unmapped blocks should not keep','line_number':1958,'multiline':False]['text':' history','line_number':1959,'multiline':False]['text':' failed to map the memory','line_number':1962,'multiline':False]['text':' to_map -> remaining -> to_map->next(?)','line_number':1974,'multiline':False]['text':' update statistics','line_number':1993,'multiline':False]['text':' Candidate is now a list free/unmapped blocks with at least size room:','line_number':2021,'multiline':False]['text':' unmapped -> null','line_number':2022,'multiline':False]['text':' unmapped -> free -> *','line_number':2023,'multiline':False]['text':' free -> unmapped -> *','line_number':2024,'multiline':False]['text':' invariant: free -> unmapped -> *','line_number':2033,'multiline':False]['text':' map_block will map some of unmapped and merge with free','line_number':2034,'multiline':False]['text':'* moves a block into a pool of cached free blocks ','line_number':2047,'multiline':True]['text':' Makes sure the Block* isn't already present in the pool we're freeing it','line_number':2082,'multiline':False]['text':' back into.','line_number':2083,'multiline':False]['text':' inactive_split tries to capture the idea that blocks','line_number':2095,'multiline':False]['text':' cannot be freed when requested, but fully free pages','line_number':2096,'multiline':False]['text':' of expandable blocks can always be freed.','line_number':2097,'multiline':False]['text':' The logic to track this as statistic is pretty involved,','line_number':2098,'multiline':False]['text':' so we simply just exclude expandable segments from','line_number':2099,'multiline':False]['text':' inactive_split','line_number':2100,'multiline':False]['text':'* combine previously split blocks. returns the size of the subsumed block,
   * or 0 on failure. ','line_number':2118,'multiline':True]['text':' [src dst]','line_number':2128,'multiline':False]['text':' [dest src]','line_number':2136,'multiline':False]['text':' captures_underway is a conservative guess that the current stream may be','line_number':2154,'multiline':False]['text':' capturing. It's only > 0 if some thread has begun and not yet ended a','line_number':2155,'multiline':False]['text':' capture, so it's usually 0, and we can short-circuit','line_number':2156,'multiline':False]['text':' cudaStreamCaptureStatus (which does a TLS lookup).','line_number':2157,'multiline':False]['text':' Track block reuse interval only when garbage collection is enabled.','line_number':2212,'multiline':False]['text':' if we are allocated to the part of the block that is expandable','line_number':2223,'multiline':False]['text':' for the purposes of "best fit" we consider its size to be the size it','line_number':2224,'multiline':False]['text':' can expand to, not the size it currently is. This means that we','line_number':2225,'multiline':False]['text':' sometimes have to search for blocks with bigger 'size' before','line_number':2226,'multiline':False]['text':' choosing this segment.','line_number':2227,'multiline':False]['text':' Rarely expandable segments has been turned off after we have','line_number':2239,'multiline':False]['text':' already allocated some blocks as expandable. For instance,','line_number':2240,'multiline':False]['text':' since we cannot share expandable memory via IPC, someone might','line_number':2241,'multiline':False]['text':' temporarily disable it. In this case we need to honor this request','line_number':2242,'multiline':False]['text':' by only finding non-expandable blocks','line_number':2243,'multiline':False]['text':' Do not return an oversized block for a large request','line_number':2254,'multiline':False]['text':' Allow oversized block size to be rounded up but within a limit','line_number':2258,'multiline':False]['text':' Denote this block has been used','line_number':2263,'multiline':False]['text':' Free unused cached blocks to reclaim GPU memory.','line_number':2278,'multiline':False]['text':' Unlike release_cached_blocks(), this does not enforce synchronization and','line_number':2279,'multiline':False]['text':' therefore should be of less overheads.','line_number':2280,'multiline':False]['text':' No need to trigger GC yet','line_number':2285,'multiline':False]['text':' Calculate the total age of the free-able blocks. We'll use it later to','line_number':2292,'multiline':False]['text':' get "avg age" threshold.','line_number':2293,'multiline':False]['text':' No free-able blocks?','line_number':2302,'multiline':False]['text':' Repeat GC until we reach reclaim > target size.','line_number':2307,'multiline':False]['text':' Free blocks exceeding this age threshold first.','line_number':2311,'multiline':False]['text':' Stop iteration if we can no longer free a block.','line_number':2313,'multiline':False]['text':' Free blocks of > avg age. Don't stop upon reaching the target_size,','line_number':2316,'multiline':False]['text':' we don't want this GC to be triggered frequently.','line_number':2317,'multiline':False]['text':' Decrement the age','line_number':2325,'multiline':False]['text':' One less block that can be freed','line_number':2326,'multiline':False]['text':' This function assumes that global lock has been taken whle calling into','line_number':2333,'multiline':False]['text':' this function. We do cudaMalloc sync call in this function which','line_number':2334,'multiline':False]['text':' can be expensive while holding the lock. Hence, we pass-in the lock to the','line_number':2335,'multiline':False]['text':' function to temporarily release the lock before cudaMalloc call and acquire','line_number':2336,'multiline':False]['text':' it back again after the call so that other threads dont get blocked.','line_number':2337,'multiline':False]['text':' Defensively checks for preexisting CUDA error state.','line_number':2343,'multiline':False]['text':' our checkpointing logic for private pools doesn't support','line_number':2359,'multiline':False]['text':' the expandable_segments_ structure yet','line_number':2360,'multiline':False]['text':' At scope exit, acquire the lock again. This provides safety against','line_number':2372,'multiline':False]['text':' any potential exceptions in the cudaMallocMaybeCapturing function.','line_number':2373,'multiline':False]['text':' If this is the first attempt (!isRetry), we can forgive and clear','line_number':2387,'multiline':False]['text':' CUDA's internal error state.','line_number':2388,'multiline':False]['text':'','line_number':2389,'multiline':False]['text':' If this is the second attempt (isRetry), malloc's TORCH_CHECK_WITH','line_number':2390,'multiline':False]['text':' will take over to throw a helpful exception. The user can choose','line_number':2391,'multiline':False]['text':' to catch the exception, free some stuff in their script, and','line_number':2392,'multiline':False]['text':' attempt the allocation again. In this case, we can also forgive and','line_number':2393,'multiline':False]['text':' clear CUDA's internal error state.','line_number':2394,'multiline':False]['text':' If the error's unrelated to memory allocation, we should throw','line_number':2397,'multiline':False]['text':' immediately.','line_number':2398,'multiline':False]['text':' The block is for a CUDA graph's PrivatePool.','line_number':2406,'multiline':False]['text':' p.block came from new, not cudaMalloc. It should not be nullptr here.','line_number':2419,'multiline':False]['text':'* Free one or more oversize blocks to the system allocator.  But only enough
   * *','line_number':2432,'multiline':True]['text':'* to satisfy the target size *','line_number':2434,'multiline':True]['text':' because of std::unique_ptr, block cannot be trivially copied','line_number':2441,'multiline':False]['text':' No single block is large enough; free multiple oversize blocks,','line_number':2453,'multiline':False]['text':' starting with the largest','line_number':2454,'multiline':False]['text':' Back up one item.  Now on the largest block for the correct','line_number':2458,'multiline':False]['text':' stream','line_number':2459,'multiline':False]['text':' First ensure that all blocks that can't currently be allocated due to','line_number':2482,'multiline':False]['text':' outstanding events are returned to the pool.','line_number':2483,'multiline':False]['text':' Free all non-split cached blocks to system allocator','line_number':2486,'multiline':False]['text':' See notifyCaptureDestroy for the strategy here.','line_number':2492,'multiline':False]['text':' The cudaFreed block belonged to a CUDA graph's PrivatePool.','line_number':2539,'multiline':False]['text':' prev? -> before_free -> block','line_number':2569,'multiline':False]['text':' block -> after_free -> next?','line_number':2579,'multiline':False]['text':' update statistics','line_number':2599,'multiline':False]['text':' Frees all non-split blocks','line_number':2616,'multiline':False]['text':' unmapping will mutate the free pool','line_number':2622,'multiline':False]['text':' so just gather what needs to be freed','line_number':2623,'multiline':False]['text':' to avoid invalidating the iterator','line_number':2624,'multiline':False]['text':' Leak the event pool to avoid shutdown issues.','line_number':2639,'multiline':False]['text':' Synchronize on outstanding events and then free associated blocks.','line_number':2646,'multiline':False]['text':' This function syncs, so capture should not be underway. Might as well','line_number':2648,'multiline':False]['text':' make sure capture-deferred end of life events get processed too.','line_number':2649,'multiline':False]['text':' Process outstanding cudaEvents. Events that are completed are','line_number':2703,'multiline':False]['text':' removed from the queue, and the 'event_count' for the','line_number':2704,'multiline':False]['text':' corresponding allocation is decremented. We maintain a separate','line_number':2705,'multiline':False]['text':' list of events per stream to avoid head-of-line delays if one','line_number':2706,'multiline':False]['text':' or more streams has long-running operations.','line_number':2707,'multiline':False]['text':' Iterate over different streams.','line_number':2709,'multiline':False]['text':' Iterate over this stream's (event, block) pairs.','line_number':2711,'multiline':False]['text':' ignore and clear the error if not ready','line_number':2719,'multiline':False]['text':' Return the ownership of the Event (unique ptr)','line_number':2721,'multiline':False]['text':' Iterates over sizes of all memory blocks for given device in given pool','line_number':2743,'multiline':False]['text':' Callbacks should not include any Pytorch call','line_number':2772,'multiline':False]['text':' Returns whether to force all allocations to bypass the caching allocator and','line_number':2790,'multiline':False]['text':' go straight to cudaMalloc.  This setting is useful when debugging GPU memory','line_number':2791,'multiline':False]['text':' errors, since the caching allocator foils cuda-memcheck.','line_number':2792,'multiline':False]['text':' allocated blocks by device pointer','line_number':2817,'multiline':False]['text':'* allocates a block which is safe to use from the provided stream ','line_number':2857,'multiline':True]['text':' remove ','line_number':2878,'multiline':True]['text':' Empty tensor's storage().data() might be a null ptr. As there is no','line_number':2956,'multiline':False]['text':' blocks associated with those tensors, it is fine to do nothing here.','line_number':2957,'multiline':False]['text':' If a tensor is not allocated by this instance, simply skip','line_number':2962,'multiline':False]['text':' This usually happens when CUDA tensors are shared across processes,','line_number':2963,'multiline':False]['text':' we have implemented reference counting based sharing mechanism to','line_number':2964,'multiline':False]['text':' guarantee tensors won't be accidentally freed by one process while','line_number':2965,'multiline':False]['text':' they are still being used in another','line_number':2966,'multiline':False]['text':' block must not be null reaching here','line_number':2971,'multiline':False]['text':' Set-up converter to convert timestamps from tsc to microseconds.','line_number':2977,'multiline':False]['text':'*
   * @brief Checkpoint the private pool state identified in `as` to its prior
   * state
   *
   * @param device - device of the pool to manipulate
   * @param as - allocator state
   * @param stale_live_storages - storages of tensors which are currently
   * allocated but which will be not be allocated after the checkpoint is set.
   * For these storages we will remove their deleter function.
   * @return CheckpointDelta - Freed Pointers and DataPtrs that contain deleter
   * functions for all allocated blocks in the new checkpoint state.
   ','line_number':2997,'multiline':True]['text':'remove','line_number':3021,'multiline':True]['text':' Deliberately don't use cudaMallocMaybeCapturing here, to force an error','line_number':3051,'multiline':False]['text':' if someone tries to use forceUncachedAllocator while capturing.','line_number':3052,'multiline':False]['text':' Allocator declars allocate const!?','line_number':3061,'multiline':False]['text':' CUDAGraph interactions','line_number':3106,'multiline':False]['text':' ignore and clear the error if access was already enabled','line_number':3152,'multiline':False]['text':' memcpy ok because memory is mapped in both devices','line_number':3168,'multiline':False]['text':' memcpy ok on a single device','line_number':3169,'multiline':False]['text':' memcpy ok because both dst and src must have come from cudaMalloc','line_number':3170,'multiline':False]['text':' when p2p is not enabled, only cudaMemcpyPeerAsync correctly handles','line_number':3175,'multiline':False]['text':' memory not allocated via cudaMalloc','line_number':3176,'multiline':False]['text':' In CUDA IPC, sender sends a tensor to receiver, getIpcDevPtr','line_number':3184,'multiline':False]['text':' is called by the receiving process to map the CUDA memory from the sending','line_number':3185,'multiline':False]['text':' process into its own address space.','line_number':3186,'multiline':False]['text':'','line_number':3187,'multiline':False]['text':' CUDA IPC only allows sharing a big memory block associated with a','line_number':3188,'multiline':False]['text':' cudaIpcMemHandle_t and it can be opened only **once** per context per','line_number':3189,'multiline':False]['text':' process. There can be multiple types of storage in the same IPC mem block,','line_number':3190,'multiline':False]['text':' so we must cache the device ptr to construct typed storage as it comes.','line_number':3191,'multiline':False]['text':'','line_number':3192,'multiline':False]['text':' ipcMemHandle_to_devptr maps a cudaIpcMemHandle_t to a device pointer in the','line_number':3193,'multiline':False]['text':' process that can be used to access the memory block in the sender process.','line_number':3194,'multiline':False]['text':' It only saves a weak_ptr of the device pointer in the map, the shared_ptr','line_number':3195,'multiline':False]['text':' will be used to reconstruct all storages in this CudaMalloc allocation. And','line_number':3196,'multiline':False]['text':' it will deleted in cudaIpcCloseMemHandle when its reference count is 0.','line_number':3197,'multiline':False]['text':'','line_number':3198,'multiline':False]['text':' This ipcMemHandle hasn't been opened, or already expired, open it to','line_number':3210,'multiline':False]['text':' enable IPC access to that mem block.','line_number':3211,'multiline':False]['text':' devPtr has to be deleted in same device when created.','line_number':3217,'multiline':False]['text':' To eliminate an additional search, we can use insert().','line_number':3228,'multiline':False]['text':' It doesn't overwrite when key already exists(ptr expired).','line_number':3229,'multiline':False]['text':' But in the deleter for sp we erased the entry,','line_number':3230,'multiline':False]['text':' this should be safe to do now.','line_number':3231,'multiline':False]['text':' namespace Native','line_number':3251,'multiline':False]['text':' Size pretty-printer','line_number':3252,'multiline':False]['text':' If this is put in its own header file, it gets incorrectly renamed in HIPify.','line_number':3273,'multiline':False]['text':' namespace CudaMallocAsync','line_number':3276,'multiline':False]['text':' Parses env for backend at load time, duplicating some logic from','line_number':3279,'multiline':False]['text':' CUDAAllocatorConfig. CUDAAllocatorConfig double-checks it later (at','line_number':3280,'multiline':False]['text':' runtime). Defers verbose exceptions and error checks, including Cuda','line_number':3281,'multiline':False]['text':' version checks, to CUDAAllocatorConfig's runtime doublecheck. If this','line_number':3282,'multiline':False]['text':' works, maybe we should move all of CUDAAllocatorConfig here?','line_number':3283,'multiline':False]['text':' namespace CUDACachingAllocator','line_number':3321,'multiline':False]['text':' namespace cuda','line_number':3322,'multiline':False]['text':' namespace c10','line_number':3323,'multiline':False]