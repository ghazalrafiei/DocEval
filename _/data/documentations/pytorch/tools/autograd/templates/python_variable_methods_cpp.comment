['text':' ${generated_comment}','line_number':2,'multiline':False]['text':' Undefine the copysign macro so that at::copysign works as intended with MSVC','line_number':6,'multiline':False]['text':' https://github.com/python/cpython/blob/c60394c7fc9cc09b16e9675a3eeb5844b6d8523f/PC/pyconfig.h#L196','line_number':7,'multiline':False]['text':' _MSC_VER','line_number':10,'multiline':False]['text':' implemented on the python object bc no support for first-class functions in native_functions.yaml','line_number':80,'multiline':False]['text':' See: ATen/native/README.md for more context','line_number':81,'multiline':False]['text':' will error out if a tensor has symints','line_number':118,'multiline':False]['text':' yes, this is called strides in ATen.','line_number':152,'multiline':False]['text':' we can't do the normal wrapping here because IntArrayRef maps to both','line_number':154,'multiline':False]['text':' torch.Size and tuple in python','line_number':155,'multiline':False]['text':' TODO: consider factoring this out','line_number':156,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':172,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':195,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':207,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':219,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':231,'multiline':False]['text':' avoids touching the GIL or current device if self is already contiguous','line_number':268,'multiline':False]['text':' NOTE: this logic is duplicated from VariableType.cpp. Since we need to','line_number':270,'multiline':False]['text':' record this call to contiguous() in the trace regardless of whether','line_number':271,'multiline':False]['text':' we actually call contiguous here, we need to record this information','line_number':272,'multiline':False]['text':' manually.','line_number':273,'multiline':False]['text':'num_outputs=','line_number':277,'multiline':True]['text':' we can't dispatch to item<int64_t> here because we want to avoid ATen overflow checks;','line_number':356,'multiline':False]['text':' the python integral type (long in python2) can't overflow.','line_number':357,'multiline':False]['text':' This is the __index__ function in Python which is similar to __int__, but','line_number':365,'multiline':False]['text':' called when used as a slice.','line_number':366,'multiline':False]['text':' TODO: change the condition to `self_.dim() != 0` once we expose scalars','line_number':373,'multiline':False]['text':' in PyTorch.','line_number':374,'multiline':False]['text':'includeBool=','line_number':375,'multiline':True]['text':'includeBool=','line_number':394,'multiline':True]['text':' NOTE: this is where we record aten::to in the graph during tracing. However, the behavior of aten::to','line_number':403,'multiline':False]['text':' is different with respect to TensorOptions fields that are not present: aten::to inherits fields that','line_number':404,'multiline':False]['text':' are missing from the self argument while the tracer assumes that they should be populated with the','line_number':405,'multiline':False]['text':' default values (eg. float for scalar type). By explicitly copying over the tensor options here we fully','line_number':406,'multiline':False]['text':' specify all tensor options and thus record the proper trace','line_number':407,'multiline':False]['text':' TODO: Make this call the TensorOptions version, maybe?','line_number':418,'multiline':False]['text':' TODO: Make this call the TensorOptions version, maybe?','line_number':424,'multiline':False]['text':' implemented on the python object bc PyObjects not declarable in native_functions.yaml','line_number':771,'multiline':False]['text':' See: ATen/native/README.md for more context','line_number':772,'multiline':False]['text':' temporary hack to improve functorch UX.','line_number':806,'multiline':False]['text':' should we throw if requires_grad is true?  var.requires_grad = True throws here','line_number':813,'multiline':False]['text':' but it's nice to let this be a no-op.','line_number':814,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':830,'multiline':False]['text':' implemented on the python object to avoid dispatch overhead','line_number':850,'multiline':False]['text':' implemented on the python object bc no support for first class functions in native_functions.yaml','line_number':867,'multiline':False]['text':' See: ATen/native/README.md for more context','line_number':868,'multiline':False]['text':' implemented on the python object bc no support for first class functions in native_functions.yaml','line_number':895,'multiline':False]['text':' See: ATen/native/README.md for more context','line_number':896,'multiline':False]['text':'allow_copy','line_number':971,'multiline':True]['text':' implemented on the python object b/c arbitrarily nested list not declarable in native_functions.yaml','line_number':998,'multiline':False]['text':' See: ATen/native/README.md for more context','line_number':999,'multiline':False]['text':'non_blocking=','line_number':1051,'multiline':True]['text':'copy=','line_number':1051,'multiline':True]['text':'non_blocking=','line_number':1062,'multiline':True]['text':'copy=','line_number':1062,'multiline':True]['text':' generated methods start here','line_number':1066,'multiline':False]['text':' Wrapper converts a raised TypeError into returning NotImplemented','line_number':1080,'multiline':False]['text':' Used to implement binary arithmetic operators','line_number':1081,'multiline':False]['text':' set_ has to be defined in the template because the c10::Storage object','line_number':1094,'multiline':False]['text':' does not have a type, and we need to make sure the Python storage object's','line_number':1095,'multiline':False]['text':' type matches the tensor's type','line_number':1096,'multiline':False]['text':'traceable=','line_number':1111,'multiline':True]['text':' aten::set_(Tensor(a!) self) -> Tensor(a!)','line_number':1118,'multiline':False]['text':' aten::set_.source_Storage(Tensor(a!) self, Storage source) ->','line_number':1126,'multiline':False]['text':' Tensor(a!)','line_number':1127,'multiline':False]['text':' aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage','line_number':1142,'multiline':False]['text':' source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)','line_number':1143,'multiline':False]['text':' aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)','line_number':1163,'multiline':False]['text':' aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor','line_number':1172,'multiline':False]['text':' source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)','line_number':1173,'multiline':False]['text':' XXX: ops that are bound here are not exposed to the C++ api nor the JIT.','line_number':1191,'multiline':False]['text':' Any new ops added here should be accompanied with a comment why they are not','line_number':1192,'multiline':False]['text':' being registered through native_functions.yaml, and be tagged cpp / JIT','line_number':1193,'multiline':False]['text':' These magic methods are all implemented on python object to wrap NotImplementedError','line_number':1195,'multiline':False]['text':' namespace torch::autograd','line_number':1278,'multiline':False]