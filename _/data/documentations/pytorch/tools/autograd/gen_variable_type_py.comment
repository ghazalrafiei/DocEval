['text':' Generates VariableType.h/cpp','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' **If any changes are being made to the VariableType codegen please also check','line_number':3,'multiline':False]['text':' if updates are needed in torch/csrc/autograd/autograd_not_implemented_fallback.cpp','line_number':4,'multiline':False]['text':'','line_number':5,'multiline':False]['text':' VariableType is a subclass of at::Type that provides the binding code','line_number':6,'multiline':False]['text':' necessary to provide a differentiable version of ATen operators. There are a','line_number':7,'multiline':False]['text':' number of different things we could mean:','line_number':8,'multiline':False]['text':'','line_number':9,'multiline':False]['text':'   - Given a non-differentiable forward implementation, we might','line_number':10,'multiline':False]['text':'     directly associate it with a backward implementation to make','line_number':11,'multiline':False]['text':'     it differentiable.  This is the common case.','line_number':12,'multiline':False]['text':'','line_number':13,'multiline':False]['text':'   - Some functions don't need a backwards implementation, because','line_number':14,'multiline':False]['text':'     backpropagation will never propagate beyond them.  There are a','line_number':15,'multiline':False]['text':'     number of different reasons why this may be the case:','line_number':16,'multiline':False]['text':'','line_number':17,'multiline':False]['text':'       - The function has no differentiable inputs','line_number':18,'multiline':False]['text':'       - The function's output is not differentiable','line_number':19,'multiline':False]['text':'       - The function has no data dependency on its input','line_number':20,'multiline':False]['text':'','line_number':21,'multiline':False]['text':'   - Some function don't need a backwards implementation because they','line_number':22,'multiline':False]['text':'     are implemented as a composition of other (differentiable) ATen','line_number':23,'multiline':False]['text':'     functions.  These are dispatched directly to the Type superclass,','line_number':24,'multiline':False]['text':'     which will in turn dispatch back to VariableType for its','line_number':25,'multiline':False]['text':'     differentiable subcomponents.','line_number':26,'multiline':False]['text':'','line_number':27,'multiline':False]['text':' We don't set or modify grad_fn on these methods. Generally, they return','line_number':107,'multiline':False]['text':' tensors that have requires_grad=False. In-place functions listed here will','line_number':108,'multiline':False]['text':' not examine or modify requires_grad or grad_fn.','line_number':109,'multiline':False]['text':' NB: this does NOT include overload name','line_number':110,'multiline':False]['text':' These only depend on the input Tensor's shape and device, not the data','line_number':112,'multiline':False]['text':' These are only implemented on integral types','line_number':124,'multiline':False]['text':' These work on integral data types, and hence don't require derivative','line_number':135,'multiline':False]['text':' This is an unsafe method that is meant to be out of reach of autograd.','line_number':140,'multiline':False]['text':' Quantize functions should not record gradients','line_number':142,'multiline':False]['text':' Functions that return integers should not have output that require gradients','line_number':145,'multiline':False]['text':' Functions that return booleans are not differentiable','line_number':151,'multiline':False]['text':' Functions return none are not differentiable','line_number':159,'multiline':False]['text':' These functions are not differentiable','line_number':161,'multiline':False]['text':' This function returns nested_tensor shape as a tensor that is non-differentiable','line_number':166,'multiline':False]['text':' The C -> R functions at the time of adding this are still being audited and tested','line_number':172,'multiline':False]['text':' but will not error out.','line_number':173,'multiline':False]['text':' C -> C, R -> C functions for which backward is correctly implemented and tested','line_number':174,'multiline':False]['text':' Some operators invalidate the grad_accumulator. Let's reset it.','line_number':392,'multiline':False]['text':' NOTE [ TensorImpl and Storage Pointer Sanity Checks ]','line_number':395,'multiline':False]['text':'','line_number':396,'multiline':False]['text':' We check the following properties:','line_number':397,'multiline':False]['text':'   1) A function should never change the input tensors' underlying c10::TensorImpl','line_number':398,'multiline':False]['text':'      pointers or c10::Storage pointers, even if it modifies its input tensors (via','line_number':399,'multiline':False]['text':'      inplace or out-variants)','line_number':400,'multiline':False]['text':' If the function does not modify its arguments, we also check the following properties','line_number':401,'multiline':False]['text':' pertaining to its output:','line_number':402,'multiline':False]['text':'   2) Its TensorImpl has use_count of 1','line_number':403,'multiline':False]['text':'   3) If the function is a view function, it has the same StorageImpl as that of','line_number':404,'multiline':False]['text':'      the input it is aliased with. Otherwise, its StorageImpl has use_count of 1','line_number':405,'multiline':False]['text':'','line_number':406,'multiline':False]['text':' The following code templates implement the checks for this invariant:','line_number':407,'multiline':False]['text':' If tensor_name == out_tensor_name, used to enforce (1), otherwise used for (2)','line_number':416,'multiline':False]['text':' The following list contains functions that we don't enforce the invariant on.','line_number':529,'multiline':False]['text':' These functions are expected to change impl or storage of input tensors','line_number':531,'multiline':False]['text':' These non-inplace, non-out functions return tensors with use_count > 1','line_number':536,'multiline':False]['text':' Therefore, they MAY (but not necessarily) return one of its inputs as-is','line_number':537,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/60426 for more information','line_number':538,'multiline':False]['text':' The below failed StorageImpl use_count check but we skip tensor_impl check','line_number':545,'multiline':False]['text':' just in case','line_number':546,'multiline':False]['text':' lift() should never actually be called with a requires_grad=True tensor,','line_number':549,'multiline':False]['text':' Nested Tensors related functions','line_number':553,'multiline':False]['text':' _nested_tensor_size() should never actually be called with requires_grad=True tensor','line_number':554,'multiline':False]['text':' These non-view functions return tensors with storage use_count != 1','line_number':561,'multiline':False]['text':' If an input is returned as-is in output, we cannot guarantee its storage_impl','line_number':565,'multiline':False]['text':' use count to be 1 either.','line_number':566,'multiline':False]['text':' END CHECKS FOR [ TensorImpl and Storage Pointer Sanity Checks ]','line_number':569,'multiline':False]['text':' note(crcrpar): `compute_requires_grad` in the template below is supplied with arguments indexed with `i`','line_number':613,'multiline':False]['text':' while the `SETUP_ANY_REQUIRES_GRAD` above takes whole tensors and scalars.','line_number':614,'multiline':False]['text':' If the non-variable operation has return values, we use the `tmp` variable to hold the','line_number':637,'multiline':False]['text':' values temporarily and pass the values to the return variables outside of the','line_number':638,'multiline':False]['text':' `at::AutoDispatchBelowAutograd` guard block.','line_number':639,'multiline':False]['text':' helper that generates a TORCH_LIBRARY_IMPL macro for each','line_number':853,'multiline':False]['text':' dispatch key that appears in derivatives.yaml','line_number':854,'multiline':False]['text':' Generate a new template from VariableType.cpp which replaces ${wrapper_registrations}','line_number':871,'multiline':False]['text':' with per key TORCH_LIBRARY_IMPL macros for each key that appears in derivatives.yaml','line_number':872,'multiline':False]['text':' Generate final VariableType_*.cpp files from the generated template','line_number':889,'multiline':False]['text':' NOTE: see Note [Sharded File] at the top of the VariableType.cpp','line_number':896,'multiline':False]['text':' template regarding sharding of the generated files.','line_number':897,'multiline':False]['text':' NOTE: [ Registering AutogradNotImplemented boxed kernel ]','line_number':939,'multiline':False]['text':'','line_number':940,'multiline':False]['text':' When there is no derivatives.yaml entry, we register a generic boxed','line_number':941,'multiline':False]['text':' NotImplemented kernel to set grad_fn to be NotImplemented, so that forward','line_number':942,'multiline':False]['text':' proceeds as usual but an error is properly produced on backward.','line_number':943,'multiline':False]['text':' TODO: it would be nice to not have these special cases','line_number':944,'multiline':False]['text':'','line_number':945,'multiline':False]['text':' There are several cases where still let codegen handle it:','line_number':946,'multiline':False]['text':' 1) ops that need to reset grad accumulator (we let codegen handle this case','line_number':947,'multiline':False]['text':'     because) the list is (currently) only accessible in Python.','line_number':948,'multiline':False]['text':' 2) User explicitly specifies DONT_REQUIRE_DERIVATIVE. This basically makes','line_number':949,'multiline':False]['text':'    autograd a fallthrough with NDEBUG checks. This can be useful for when all','line_number':950,'multiline':False]['text':'    outputs are integral.','line_number':951,'multiline':False]['text':' 3) When there are no differentiable outputs. This is similar to (2).','line_number':952,'multiline':False]['text':' 4) There are certain ops where we skip certain NDEBUG checks. this is similar','line_number':953,'multiline':False]['text':'    to (1).','line_number':954,'multiline':False]['text':' See Note [Manual Backend kernels]','line_number':988,'multiline':False]['text':' If you want to register a kernel to Autograd, you must make the op abstract.','line_number':990,'multiline':False]['text':' In other words, this op must have dispatch section in native_functions.yaml.','line_number':991,'multiline':False]['text':' No reference backward available due to the lack of `{maximum, minimum}(tensor, scalar)`.','line_number':1008,'multiline':False]['text':' No reference backward available as addcdiv/addcmul don't support Tensor as scaling factor.','line_number':1013,'multiline':False]['text':' These ops lack `alpha` of scaling factor to applied to the right hand side argument.','line_number':1020,'multiline':False]['text':' TODO: `cpp_type` is only to keep it byte-for-byte compatible with the old codegen, should remove.','line_number':1080,'multiline':False]['text':' NB: This is not a clone of cpp.argument() - TensorOptionsArguments / faithful / binds are','line_number':1081,'multiline':False]['text':' not handled properly as they are irrelevant for this codegen.','line_number':1082,'multiline':False]['text':' note(crcrpar): From what I understand, what matters is only the name.','line_number':1099,'multiline':False]['text':' Thus originally I only replace argument only when the names are different.','line_number':1100,'multiline':False]['text':' TODO(crcrpar): Make it simpler.','line_number':1101,'multiline':False]['text':' note(crcrpar): In-place foreach functions are a void function.','line_number':1139,'multiline':False]['text':' out= ops are allowed to have zero returns which cause requires_derivative to be False','line_number':1148,'multiline':False]['text':' we shouldn't error out though (out= ops for autograd just redispatch)','line_number':1149,'multiline':False]['text':' note(crcrpar): In-place foreach functions do not support forward AD','line_number':1156,'multiline':False]['text':' We don't want to save tensors if we know that they will never be used','line_number':1184,'multiline':False]['text':' when computing the derivative, so we add guards to those statements','line_number':1185,'multiline':False]['text':' It's hard to determine the edge offset if we have TensorLists','line_number':1189,'multiline':False]['text':' NOTE(crcrpar): in-place foreach functions' arguments include tensorlist','line_number':1190,'multiline':False]['text':' but their derivatives don't use it, so let them bypass this check.','line_number':1191,'multiline':False]['text':' Empirical evaluation of the cases where we insert those guards in','line_number':1195,'multiline':False]['text':' backward show that they are somewhat useless. E.g. there's no need','line_number':1196,'multiline':False]['text':' to guard on some values captured from forward, because they had to','line_number':1197,'multiline':False]['text':' require_grad if the backward function even gets executed. I don't','line_number':1198,'multiline':False]['text':' have any good ideas for detecting those cases, so I simply disabled the','line_number':1199,'multiline':False]['text':' checks.','line_number':1200,'multiline':False]['text':' If there's a single derivative we could compute, we already have','line_number':1204,'multiline':False]['text':' a requires_grad check that is sufficient','line_number':1205,'multiline':False]['text':' We really only care about trimming down the amount of tensors we save','line_number':1209,'multiline':False]['text':' We want to emit simple guards, so we only allow that if checking one','line_number':1213,'multiline':False]['text':' input is enough to determine whether we need that value','line_number':1214,'multiline':False]['text':' Case with multioutput formulas','line_number':1221,'multiline':False]['text':' TODO: process all derivative formulas!!!','line_number':1222,'multiline':False]['text':' Condition is between 'wrap_opt_if(var_name,' and ')'.','line_number':1236,'multiline':False]['text':' replace 'grad_input_mask[num]' with 'grad_fn->should_compute_output(num)'','line_number':1241,'multiline':False]['text':' Figure out the offset of the edge that uses this variable','line_number':1249,'multiline':False]['text':' For out functions, ensure that no input or output requires grad','line_number':1276,'multiline':False]['text':' note(crcrpar): Assuming in-place foreach function's self_arg is always TensorList.','line_number':1305,'multiline':False]['text':' TODO: should be `arg.type.is_tensor_like()`?','line_number':1346,'multiline':False]['text':' Double-backwards definitions sometimes take in 'input' and','line_number':1364,'multiline':False]['text':' 'output', but only define the derivative for input.','line_number':1365,'multiline':False]['text':' assign the saved variables to the generated grad_fn','line_number':1411,'multiline':False]['text':' todo(crcrpar): See if we can add some check e.g. `assert foreacharg is not None`.','line_number':1425,'multiline':False]['text':' for now the example assert would fail.','line_number':1426,'multiline':False]['text':' note(crcrpar): Here `expr` is generated from scratch, `arg.expr` is ignored.','line_number':1447,'multiline':False]['text':' See Note [nuanced return type of out-of-place foreach functions]','line_number':1479,'multiline':False]['text':' Generates a Dispatcher::redispatch() call into the dispatcher. We do this mainly for performance reasons:','line_number':1510,'multiline':False]['text':'  - Pre-compute the full DispatchKeySet. This saves the dispatcher from having to read from TLS.','line_number':1511,'multiline':False]['text':'  - redispatch() avoids a redundant call to RecordFunction, which was already called right before','line_number':1512,'multiline':False]['text':'    we entered this autograd kernel.','line_number':1513,'multiline':False]['text':' code-generated autograd kernels plumb and recompute dispatch keys directly through the kernel for performance.','line_number':1521,'multiline':False]['text':' Ops also always have a function variant of the redispatch API.','line_number':1522,'multiline':False]['text':' See Note [Plumbing Keys Through The Dispatcher] for details.','line_number':1523,'multiline':False]['text':' See NOTE [ TensorImpl and Storage Pointer Sanity Checks ]','line_number':1553,'multiline':False]['text':' Check properties of inputs (enforce (1))','line_number':1560,'multiline':False]['text':' Check properties of outputs (enforce (2), (3))','line_number':1604,'multiline':False]['text':' TODO: should be str(f.func.name.name)?','line_number':1606,'multiline':False]['text':' Currently we don't have any functions that return the following types, but','line_number':1642,'multiline':False]['text':' we should update the checks once we do','line_number':1643,'multiline':False]['text':' We only care about adding `at::AutoDispatchBelowAutograd` guard for non-variable dispatch','line_number':1664,'multiline':False]['text':' (which corresponds to 'use_derived' strategy). The purpose of this guard is to make sure','line_number':1665,'multiline':False]['text':' the baseType operations still dispatch to non-Variable type, even if the arguments passed','line_number':1666,'multiline':False]['text':' in are now Variables.','line_number':1667,'multiline':False]['text':' See NOTE [ Treating Variables as non-Variables in type dispatch ] for details.','line_number':1668,'multiline':False]['text':' TODO: flatten allocates a std::vector, which could be expensive','line_number':1730,'multiline':False]['text':' out functions don't currently support differentiation','line_number':1747,'multiline':False]['text':' TODO update this when inplace namings are unified','line_number':1858,'multiline':False]['text':' The gradient wasn't already cloned, do it if grad mode is enabled','line_number':1907,'multiline':False]['text':' Is there a way to get from BaseType to BaseCType','line_number':1923,'multiline':False]['text':' TODO(crcrpar): Should this (= the foreach specific logic) be refactored somehow?','line_number':1968,'multiline':False]['text':' Only out-place foreach functions that have entries in `tools/autograd/derivatives.yaml`','line_number':1969,'multiline':False]['text':' can reach here.','line_number':1970,'multiline':False]['text':' View ops create fw_grad that already is a view of the base's fw_grad so just use that','line_number':1982,'multiline':False]['text':' note(crcrpar): Assuming `self` is TensorList.','line_number':1993,'multiline':False]['text':' note(crcrpar): Massage only Scalar and ArrayRef<Scalar> here.','line_number':2002,'multiline':False]['text':' Set all the grads at the end to avoid: https://github.com/pytorch/pytorch/issues/67367','line_number':2037,'multiline':False]['text':'','line_number':2042,'multiline':False]['text':' Produces a condition string (e.g, "isFwGradDefined(grad_output) || isFwGradDefined(output)")','line_number':2043,'multiline':False]['text':'','line_number':2044,'multiline':False]['text':' (1) If a derivative is NOT provided, cond will check fw_grad of ALL differentiable inputs','line_number':2046,'multiline':False]['text':' - Used in the out_fn case when we want to forbid fw derivatives','line_number':2047,'multiline':False]['text':' - Used in the case where the fw_derivative is not defined, but we want','line_number':2048,'multiline':False]['text':'   To check if there is a decomposition registered for jvp','line_number':2049,'multiline':False]['text':' type: ignore[operator]','line_number':2054,'multiline':False]['text':' (2) If derivative is provided, use that information to determine which inputs','line_number':2073,'multiline':False]['text':'     to check fw_grad for','line_number':2074,'multiline':False]['text':' Handle functions like stack','line_number':2078,'multiline':False]['text':' For these, we don't unpack anything and always call the user function','line_number':2079,'multiline':False]['text':' set_flags has to appear after version_counter, because rebase_history','line_number':2137,'multiline':False]['text':' requires that the counter is incremented before it is called','line_number':2138,'multiline':False]['text':' Save only after the forward AD has been set up','line_number':2152,'multiline':False]['text':' `inplace` implies that there is exactly one output named `self`,','line_number':2156,'multiline':False]['text':' so we can keep the generated code easy. If you need to','line_number':2157,'multiline':False]['text':' `reset_grad_accumulator` in an operator that's not `inplace`, you can','line_number':2158,'multiline':False]['text':' remove this assert but the code generation will get more elaborate','line_number':2159,'multiline':False]