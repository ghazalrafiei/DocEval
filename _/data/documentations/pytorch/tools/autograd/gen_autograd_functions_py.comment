['text':' Generates C++ autograd functions for the derivatives of ATen operations','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' This writes two files:','line_number':3,'multiline':False]['text':'  Functions.h/cpp: subclasses of autograd::Node','line_number':4,'multiline':False]['text':'  python_functions.h/cpp: Python bindings for the above classes','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':' note(crcrpar): `self` argument and other optional positional argument','line_number':119,'multiline':False]['text':' of foreach functions are basically a list of n `Tensor`s thus iterating over','line_number':120,'multiline':False]['text':' `grads` in order to utilize and apply the existing derivative definitions','line_number':121,'multiline':False]['text':' to each `Tensor`(s) of `self`, and the others.','line_number':122,'multiline':False]['text':' Generates python bindings','line_number':158,'multiline':False]['text':'','line_number':159,'multiline':False]['text':' This generates the definitions for:','line_number':160,'multiline':False]['text':'   (1) The PyTypeObject for each backward grad_fn subclassing Node','line_number':161,'multiline':False]['text':'   (2) The entry for PyTypeObject's tp_getset slot (an array of PyGetSetDef structs)','line_number':162,'multiline':False]['text':'       We generate one PyGetSetDef struct for each of grad_fn's saved inputs and outputs','line_number':163,'multiline':False]['text':'       Each PyGetSetDef has a function ptr to a getter, also defined here (3).','line_number':164,'multiline':False]['text':'   (3) Getters for each of grad_fn's saved inputs and outputs.','line_number':165,'multiline':False]['text':'','line_number':166,'multiline':False]['text':' Getter templates','line_number':197,'multiline':False]['text':' Getter body','line_number':293,'multiline':False]['text':' These functions have backwards which cannot be traced, and so must have','line_number':436,'multiline':False]['text':' their backward functions traced opaquely.','line_number':437,'multiline':False]['text':' VIEW_FUNCTIONS are not traceable because they use as_strided, which','line_number':438,'multiline':False]['text':' has an untraceable backwards, see','line_number':439,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/4250','line_number':440,'multiline':False]['text':' TODO: This is probably not exhaustive, but it's a start','line_number':441,'multiline':False]['text':' get a 1D list of diffinfos, we do not need them to be per FunctionSchema/DispatchKey here','line_number':468,'multiline':False]['text':' infos with the diff dispatchkeys but the same name will still be in the same shard.','line_number':469,'multiline':False]['text':' get a 1D list of diffinfos, we do not need them to be per FunctionSchema/DispatchKey here','line_number':514,'multiline':False]['text':' infos with the diff dispatchkeys but the same name will still be in the same shard.','line_number':515,'multiline':False]['text':' note(crcrpar): [nuanced return type of out-of-place foreach functions]','line_number':593,'multiline':False]['text':' When an out-of-place foreach function whose return signature is `Tensor[]`','line_number':594,'multiline':False]['text':' spells out its backward definitions in `derivatives.yaml`, and some of them depend on','line_number':595,'multiline':False]['text':' `result`, `result`'s type is interpreted and treated as `std::vector<Tensor>`.','line_number':596,'multiline':False]['text':' An out-of-place foreach whose backwards rely on their output doesn't suffer from this','line_number':597,'multiline':False]['text':' difference if the definitions are codegen'ed.','line_number':598,'multiline':False]['text':' This special case is needed for `_foreach_pow.List` and `_foreach_pow.ScalarAndTensor`','line_number':599,'multiline':False]['text':' as of https://github.com/pytorch/pytorch/pull/105504.','line_number':600,'multiline':False]['text':' Just clear() is sufficient, we don't need to loop and clear each variable.','line_number':607,'multiline':False]['text':' Because the SavedVariable owns a tensor and a grad_fn, removing the SavedVariable makes them go away as well.','line_number':608,'multiline':False]['text':' Just clear() is sufficient, we don't need to loop and clear each variable.','line_number':629,'multiline':False]['text':' Because the SavedVariable owns a tensor and a grad_fn, removing the SavedVariable makes them go away as well.','line_number':630,'multiline':False]['text':' Just clear() is sufficient, we don't need to loop and clear each variable.','line_number':729,'multiline':False]['text':' Because the SavedVariable owns a tensor and a grad_fn, removing the SavedVariable makes them go away as well.','line_number':730,'multiline':False]['text':' release_variables.append(f"{name}_released_ = true;")','line_number':732,'multiline':False]['text':' unpack.append(f"auto {name} = unpack_list({name}_);")','line_number':733,'multiline':False]['text':' asserts.append(f"TORCH_CHECK(!{name}_released_, ERR_BACKWARD_TWICE);")','line_number':734,'multiline':False]['text':' Check for indicators that you're putting a non-owning reference','line_number':757,'multiline':False]['text':' into the saved variable field.  If this is spuriously firing,','line_number':758,'multiline':False]['text':' edit this field.  Otherwise, you probably need to add a case','line_number':759,'multiline':False]['text':' above.','line_number':760,'multiline':False]['text':' Types we don't expose python bindings to yet:','line_number':775,'multiline':False]['text':'   TypeAndSize, at::ScalarType, TensorOptions, TensorGeometry,','line_number':776,'multiline':False]['text':'   std::vector<std::vector<int64_t>>, std::vector<at::ScalarType>','line_number':777,'multiline':False]['text':' lock the mutex when we release variables and in Node::apply to protect thread safety','line_number':798,'multiline':False]['text':' see Note [Thread Safety on Autograd Node]','line_number':799,'multiline':False]['text':' Generate aliases for gradients named for returned values.','line_number':815,'multiline':False]['text':' We can add undefined grad support if the input variable is a Tensor','line_number':834,'multiline':False]['text':' Since single-output derivative formulas need to check if grads are','line_number':878,'multiline':False]['text':' defined, only perform the check once, before all the formulas','line_number':879,'multiline':False]