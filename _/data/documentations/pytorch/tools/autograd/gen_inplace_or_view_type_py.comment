['text':' Generates ADInplaceOrViewType.h/cpp','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' NOTE: If any changes are being made to the ADInplaceOrView codegen please also check','line_number':3,'multiline':False]['text':' if updates are needed in torch/csrc/autograd/autograd_not_implemented_fallback.cpp','line_number':4,'multiline':False]['text':' The fallback is expected to mimick this codegen, so we should keep the two in sync.','line_number':5,'multiline':False]['text':' See Note [Nested Arg Types]','line_number':27,'multiline':False]['text':' See NOTE [ Autograd View Variables ] in variable.h for details.','line_number':49,'multiline':False]['text':' If you update list VIEW_FUNCTIONS or RETURNS_VIEWS_OF_INPUT,','line_number':50,'multiline':False]['text':' you **MUST** also update the public list of view ops accordingly in','line_number':51,'multiline':False]['text':' docs/source/tensor_view.rst. Note not all ATen functions are exposed to public,','line_number':52,'multiline':False]['text':' e.g alias & sparse_coo_tensor_with_dims_and_tensors.','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' A map: function name => name of the argument that all outputs are view of','line_number':55,'multiline':False]['text':' sparse_coo ctor output should really be views of both indices and values,','line_number':92,'multiline':False]['text':' but we only supports making as view of a single variable, and indices is','line_number':93,'multiline':False]['text':' discrete anyways.','line_number':94,'multiline':False]['text':' FIXME: clone indices on construction.','line_number':95,'multiline':False]['text':' note: some VIEW_FUNCTIONS are just compositions of the view functions above','line_number':104,'multiline':False]['text':' this list contains both the root view functions and any that are purely composed','line_number':105,'multiline':False]['text':' of viewing functions, and is used by the JIT to determine when an operator','line_number':106,'multiline':False]['text':' may return a view of its inputs; however they may sometimes return a copy.','line_number':107,'multiline':False]['text':' (e.g. `contiguous`)','line_number':108,'multiline':False]['text':' These are the functions we consider views for the purposes of validating','line_number':132,'multiline':False]['text':' StorageImpl and TensorImpl in gen_variable_type.','line_number':133,'multiline':False]['text':' `_unsafe_view` is not included in VIEW_FUNCTIONS above because it is not a','line_number':134,'multiline':False]['text':' view for the purposes of ADInplaceOrView kernel, we do not want to call as_view','line_number':135,'multiline':False]['text':' See NOTE [Unsafe View] for more info.','line_number':136,'multiline':False]['text':' FIXME: Ideally these functions should be methods on Type class, but we have a','line_number':226,'multiline':False]['text':'        comment in codegen/model.py there saying these concepts are not well defined.','line_number':227,'multiline':False]['text':'        Thus we put a version that commonly used by autograd codegen here.','line_number':228,'multiline':False]['text':' TODO: Should handle optional here?','line_number':230,'multiline':False]['text':' TODO: Should handle optional here?','line_number':235,'multiline':False]['text':' TODO: should be str(f.func.name.name)?','line_number':301,'multiline':False]['text':' For view replay calls, we generate an ordinary Dispatcher::call() instead, because:','line_number':312,'multiline':False]['text':'  - We want to replay the entire call into the op, including any previously-set dispatch keys (including autograd!).','line_number':313,'multiline':False]['text':'  - The view replay call also is not part of the hot path.','line_number':314,'multiline':False]['text':' View replay functions use the standard Dispatcher::call API.','line_number':318,'multiline':False]['text':' It's not safe to close over IntArrayRef by value, since this is a','line_number':357,'multiline':False]['text':' reference type, so materialize a vector to close over by value','line_number':358,'multiline':False]['text':' Materialize int64_t? to int64_t','line_number':363,'multiline':False]['text':' [NOTE] [Nested Arg Types]','line_number':372,'multiline':False]['text':' This is temporary. Nested tensors will be migrating to use SymInts and','line_number':373,'multiline':False]['text':' nested_size and nested_strides will no longer be tensors.','line_number':374,'multiline':False]['text':' See NOTE [ Autograd View Variables ] in variable.h for details.','line_number':397,'multiline':False]['text':' no output is differentiable (.indices() for SparseTensors for example)','line_number':409,'multiline':False]['text':' Single differentiable output (Tensor or Tensor[])','line_number':415,'multiline':False]['text':' We only support simple Tensor or a TensorList for functions that return views','line_number':417,'multiline':False]['text':' See Note [ View + Inplace detection]','line_number':425,'multiline':False]['text':' Only allow rebasing of the history if we return a single Tensor','line_number':430,'multiline':False]['text':' If we are in a no grad block, raise a warning','line_number':431,'multiline':False]['text':' See NOTE [ View + Inplace detection ] for more details about this logic','line_number':432,'multiline':False]['text':' This could be supported but we don't need it at the moment, so keeping things simple.','line_number':451,'multiline':False]['text':' code-generated ADInplaceOrView kernels plumb and recompute dispatch keys directly through the kernel for performance.','line_number':471,'multiline':False]['text':' See Note [Plumbing Keys Through The Dispatcher] for details.','line_number':472,'multiline':False]['text':' Note that this calls the slow, dispatching variants of manual_cpp_binding ops.','line_number':476,'multiline':False]['text':' We could probably work harder to ensure that the fast variants are called instead, but the perf benefit would be minimal.','line_number':477,'multiline':False]['text':' inplace op','line_number':478,'multiline':False]['text':' code-generated autograd kernels plumb and recompute dispatch keys directly through the kernel for performance.','line_number':512,'multiline':False]['text':' See Note [Plumbing Keys Through The Dispatcher] for details.','line_number':513,'multiline':False]['text':' For functions that modify their inputs but don't return them,','line_number':528,'multiline':False]['text':' we can't give them autograd support.','line_number':529,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/53796','line_number':530,'multiline':False]['text':' NOTE: see Note [Sharded File] at the top of the VariableType.cpp','line_number':593,'multiline':False]['text':' template regarding sharding of the generated files.','line_number':594,'multiline':False]