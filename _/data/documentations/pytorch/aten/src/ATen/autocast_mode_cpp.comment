['text':' Imitate Apex and cache some of the casts to streamline parameter reuse.','line_number':66,'multiline':False]['text':' Our heuristic is to cache lower_precision_fp casts of fp32 model weights (see cached_cast below).','line_number':67,'multiline':False]['text':'','line_number':68,'multiline':False]['text':' After discussion with @ezyang, the cache uses the following structure:','line_number':69,'multiline':False]['text':' The key is the fp32 source tensor's TensorImpl*, a proxy for a Tensor uuid that's','line_number':70,'multiline':False]['text':' unchanged across shallow copies.','line_number':71,'multiline':False]['text':' The value is a tuple with a weakref to the source tensor's TensorImpl as the first','line_number':72,'multiline':False]['text':' element and the casted tensor as the second element.','line_number':73,'multiline':False]['text':'','line_number':74,'multiline':False]['text':' The weakref keeps the source's TensorImpl from being deleted.  We need to because we're','line_number':75,'multiline':False]['text':' using the source TensorImpl* as the key.  If it were deleted, another random Tensor could','line_number':76,'multiline':False]['text':' be allocated whose TensorImpl* happened to have the same value.  This TensorImpl* would','line_number':77,'multiline':False]['text':' then mistakenly hit in cache:  a rare, intermittent, unpredictable bug.','line_number':78,'multiline':False]['text':'','line_number':79,'multiline':False]['text':' I'm not using the weak_intrusive_ptr as the key because it's more difficult to compare','line_number':80,'multiline':False]['text':' directly against incoming TensorImpl*s.','line_number':81,'multiline':False]['text':' nesting tracks the nesting depth of the Python-side context manager.','line_number':88,'multiline':False]['text':' When the autocast context manager exits to a nesting level that's outside','line_number':89,'multiline':False]['text':' any instance of autocast (which should occur at the end of each forward pass)','line_number':90,'multiline':False]['text':' it calls clear_cache() to ensure cached Tensors don't leak outside the autocasting region.','line_number':91,'multiline':False]['text':' autocast_cpu_dtype is the lower_precision_fp used by AutocastCPU.','line_number':94,'multiline':False]['text':' autocast_xpu_dtype is the lower_precision_fp used by AutocastXPU.','line_number':97,'multiline':False]['text':' autocast_ipu_dtype is the lower_precision_fp used by AutocastIPU.','line_number':100,'multiline':False]['text':' autocast_hpu_dtype is the lower_precision_fp used by AutocastHPU.','line_number':103,'multiline':False]['text':' autocast_xla_dtype is the lower_precision_fp used by AutocastXLA.','line_number':106,'multiline':False]['text':' should we enabled the cache inside autocast.','line_number':109,'multiline':False]['text':' autocast_gpu_dtype is the lower_precision_fp used by AutocastGPU.','line_number':112,'multiline':False]['text':' autocast_privateuseone_dtype is the lower_precision_fp used by AutocastPrivateUse1.','line_number':115,'multiline':False]['text':' Overload to catch Tensor args','line_number':196,'multiline':False]['text':' TODO (possible optimization):','line_number':197,'multiline':False]['text':' Move cast_cache to an inline function in a header with cached_casts declared as','line_number':198,'multiline':False]['text':' extern thread_local in the header.','line_number':199,'multiline':False]['text':' Heuristic:  Do what Apex does, and cache lower_precision_fp casts of fp32 model weights (leaves).','line_number':202,'multiline':False]['text':' See cached_casts declaration above for detailed strategy.','line_number':203,'multiline':False]['text':'******************************
Banned functions
******************************','line_number':227,'multiline':True]['text':'****************************************
Explicit registration for out-of-place ops
****************************************','line_number':241,'multiline':True]['text':' lower_precision_fp','line_number':249,'multiline':False]['text':' fp32','line_number':286,'multiline':False]['text':' fp32_set_opt_dtype','line_number':333,'multiline':False]['text':' commenting these out because they accept an explicit (not-optional) dtype, and we shouldn't try to flip that even','line_number':348,'multiline':False]['text':' when autocasting.','line_number':349,'multiline':False]['text':' KERNEL_CUDA2(norm, ScalarOpt_dtype, fp32_set_opt_dtype)','line_number':350,'multiline':False]['text':' KERNEL_CUDA2(norm, ScalarOpt_dim_dtype, fp32_set_opt_dtype)','line_number':351,'multiline':False]['text':' KERNEL_CUDA2(norm, names_ScalarOpt_dim_dtype, fp32_set_opt_dtype)','line_number':352,'multiline':False]['text':' fp32_append_dtype','line_number':356,'multiline':False]['text':' The fp32_append_dtype wrapper overrides implicit promotion behavior.','line_number':357,'multiline':False]['text':' norm does not implicitly promote, but be aware when adding new ops to this policy.','line_number':358,'multiline':False]['text':' promote','line_number':362,'multiline':False]['text':' lower_precision_fp cast policy','line_number':384,'multiline':False]['text':' fp32 cast policy','line_number':409,'multiline':False]['text':' promote','line_number':512,'multiline':False]['text':' namespace','line_number':520,'multiline':False]['text':' namespace at::autocast','line_number':521,'multiline':False]