['text':' defined in header so that getNonVariableType has ability to inline','line_number':124,'multiline':False]['text':' call_once check. getNonVariableType is called fairly frequently','line_number':125,'multiline':False]['text':' NB: This method is *purely* whether or not a user requested','line_number':138,'multiline':False]['text':' that CuDNN was enabled, it doesn't actually say anything about','line_number':139,'multiline':False]['text':' whether or not CuDNN is actually usable.  Use cudnn_is_acceptable','line_number':140,'multiline':False]['text':' to test this instead','line_number':141,'multiline':False]['text':' Note [Disabling Fused SDP Kernels]','line_number':153,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':154,'multiline':False]['text':' Flash and Memory Efficient SDP kernels are enabled by default.','line_number':155,'multiline':False]['text':' However, they can be disabled by setting','line_number':156,'multiline':False]['text':' at::globalContext().setUserEnabledFlashSDP(false) flag.','line_number':157,'multiline':False]['text':' This is useful for debugging purposes. For example, if you want to','line_number':158,'multiline':False]['text':' compare the performance of the flash SDP kernels with the unfused','line_number':159,'multiline':False]['text':' kernel, you can disable the flash SDP kernels. By disabling','line_number':160,'multiline':False]['text':' the math SDP kernel, you can force your code to use flash kernels.','line_number':161,'multiline':False]['text':' The math SDP kernel can be disabled by setting','line_number':162,'multiline':False]['text':' at::globalContext().setUserEnabledMathSDP(false) flag.','line_number':163,'multiline':False]['text':' Note [Enabling Deterministic Operations]','line_number':176,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':177,'multiline':False]['text':' Operations in PyTorch that normally act nondeterministically, but have an','line_number':178,'multiline':False]['text':' alternate deterministic implementation, should satisfy the following','line_number':179,'multiline':False]['text':' requirements:','line_number':180,'multiline':False]['text':'','line_number':181,'multiline':False]['text':' * Include this comment: "See Note [Enabling Deterministic Operations]"','line_number':182,'multiline':False]['text':'','line_number':183,'multiline':False]['text':' * Check the value of `at::globalContext().deterministicAlgorithms()` to','line_number':184,'multiline':False]['text':' toggle','line_number':185,'multiline':False]['text':'   between nondeterministic and deterministic implementations.','line_number':186,'multiline':False]['text':'','line_number':187,'multiline':False]['text':' * Have an entry in the list of PyTorch operations that toggle between','line_number':188,'multiline':False]['text':' nondeterministic','line_number':189,'multiline':False]['text':'   and deterministic implementations, in the docstring of','line_number':190,'multiline':False]['text':'   `use_deterministic_algorithms()` in torch/__init__.py','line_number':191,'multiline':False]['text':'','line_number':192,'multiline':False]['text':' `example_func()` below shows an example of toggling between','line_number':193,'multiline':False]['text':' nondeterministic and deterministic implementations:','line_number':194,'multiline':False]['text':'','line_number':195,'multiline':False]['text':'    void example_func() {','line_number':196,'multiline':False]['text':'      // See Note [Enabling Deterministic Operations]','line_number':197,'multiline':False]['text':'      if (at::globalContext().deterministicAlgorithms()) {','line_number':198,'multiline':False]['text':'        example_func_deterministic();','line_number':199,'multiline':False]['text':'      } else {','line_number':200,'multiline':False]['text':'        example_func_nondeterministic();','line_number':201,'multiline':False]['text':'      }','line_number':202,'multiline':False]['text':'    }','line_number':203,'multiline':False]['text':' Note [Writing Nondeterministic Operations]','line_number':211,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':212,'multiline':False]['text':' Operations in PyTorch that act nondeterministically and do not have an','line_number':213,'multiline':False]['text':' alternate deterministic implementation should satisfy the following','line_number':214,'multiline':False]['text':' requirements:','line_number':215,'multiline':False]['text':'','line_number':216,'multiline':False]['text':' * Include this comment: "See Note [Writing Nondeterministic Operations]"','line_number':217,'multiline':False]['text':'','line_number':218,'multiline':False]['text':' * Include a comment explaining why the operation is nondeterministic.','line_number':219,'multiline':False]['text':'','line_number':220,'multiline':False]['text':' * Throw an error when `Context::deterministicAlgorithms()` is true. Most','line_number':221,'multiline':False]['text':'   of the time, this should be accomplished by calling','line_number':222,'multiline':False]['text':'   `at::globalContext().alertNotDeterminstic()`.  However, if the','line_number':223,'multiline':False]['text':'   nondeterministic behavior is caused by the CuBLAS workspace','line_number':224,'multiline':False]['text':'   configuration in CUDA >= 10.2,','line_number':225,'multiline':False]['text':'   `at::globalContext().alertCuBLASConfigNotDeterministic()` should be','line_number':226,'multiline':False]['text':'   called instead (in this case, a comment explaining why the operation is','line_number':227,'multiline':False]['text':'   nondeterministic is not necessary). See below for details on these','line_number':228,'multiline':False]['text':'   methods.','line_number':229,'multiline':False]['text':'','line_number':230,'multiline':False]['text':' * Have an entry in the list of nondeterministic PyTorch operations in the','line_number':231,'multiline':False]['text':'   docstring of `use_deterministic_algorithms()` in torch/__init__.py','line_number':232,'multiline':False]['text':'','line_number':233,'multiline':False]['text':' * Have a test function in `test/test_torch.py` whose name begins with','line_number':234,'multiline':False]['text':'   `test_nondeterministic_alert_`. Alternatively, if CuBLAS workspace','line_number':235,'multiline':False]['text':'   configuration is the reason for nondeterminism, the operation should be','line_number':236,'multiline':False]['text':'   included in the `test_cublas_config_nondeterministic_alert` test. Any new','line_number':237,'multiline':False]['text':'   tests should ideally follow a pattern similar to the existing ones.','line_number':238,'multiline':False]['text':'','line_number':239,'multiline':False]['text':' `example_func()` below shows an example of the comments and error-throwing','line_number':240,'multiline':False]['text':' code for a nondeterministic operation:','line_number':241,'multiline':False]['text':'','line_number':242,'multiline':False]['text':'    void example_func() {','line_number':243,'multiline':False]['text':'      // See Note [Writing Nondeterministic Operations]','line_number':244,'multiline':False]['text':'      // Nondeterministic because <reason>','line_number':245,'multiline':False]['text':'      at::globalContext().alertNondeterministic("example_func");','line_number':246,'multiline':False]['text':'      ...','line_number':247,'multiline':False]['text':'    }','line_number':248,'multiline':False]['text':' Throws an error if `Context::deterministicAlgorithms()` is true','line_number':250,'multiline':False]['text':' Throws an error if `Context::deterministicAlgorithms()` is true, CUDA','line_number':253,'multiline':False]['text':' >= 10.2, and CUBLAS_WORKSPACE_CONFIG is not set to either ":16:8" or','line_number':254,'multiline':False]['text':' ":4096:8". For more details:','line_number':255,'multiline':False]['text':' https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility','line_number':256,'multiline':False]['text':' This method is used to release the original weight after pre-packing.','line_number':276,'multiline':False]['text':' It should be called once before loading/running the model.','line_number':277,'multiline':False]['text':' NB: By default it is set to true for mobile builds.','line_number':278,'multiline':False]['text':' Despite its name, this function returns the number of *CUDA* GPUs.','line_number':407,'multiline':False]['text':' WARNING: DO NOT ADD LOGIC TO HANDLE OTHER DEVICE TYPES TO THIS','line_number':409,'multiline':False]['text':' FUNCTION.  If you are interested in interrogating the number of','line_number':410,'multiline':False]['text':' devices for a specific device type, add that function to the','line_number':411,'multiline':False]['text':' relevant library (e.g., similar to at::cuda::device_count())','line_number':412,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':450,'multiline':False]['text':' NB: Sometimes we build with CUDA, but we don't have any GPUs','line_number':454,'multiline':False]['text':' available. In that case, we must not seed CUDA; it will fail!','line_number':455,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':462,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':475,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':484,'multiline':False]['text':' When the global flag `allow_tf32` is set to true, cuBLAS handles are','line_number':490,'multiline':False]['text':' automatically configured to use math mode CUBLAS_TF32_TENSOR_OP_MATH.','line_number':491,'multiline':False]['text':' For some operators, such as addmv, TF32 offers no performance improvement','line_number':492,'multiline':False]['text':' but causes precision loss. To help this case, this class implements','line_number':493,'multiline':False]['text':' a RAII guard that can be used to quickly disable TF32 within its scope.','line_number':494,'multiline':False]['text':'','line_number':495,'multiline':False]['text':' Usage:','line_number':496,'multiline':False]['text':'     NoTF32Guard disable_tf32;','line_number':497,'multiline':False]['text':' namespace at','line_number':518,'multiline':False]