['text':' Indexing tensors by by tensors','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' This corresponds to "advanced indexing" in NumPy. The two operations are:','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':'  index(Tensor self, indices) -> Tensor','line_number':5,'multiline':False]['text':'  index_put_(Tensor self, indices, value, accumulate=false)','line_number':6,'multiline':False]['text':'','line_number':7,'multiline':False]['text':' The index is a TensorList containing kLong, kBool or kByte tensors or nulls. Byte','line_number':8,'multiline':False]['text':' tensors (boolean masks) are expanded to long tensors via nonzero(). Null','line_number':9,'multiline':False]['text':' tensors signify that the dimension is not indexed.','line_number':10,'multiline':False]['text':'','line_number':11,'multiline':False]['text':' All indexes are broadcast together and iterated as *one*. From NumPy:','line_number':12,'multiline':False]['text':'','line_number':13,'multiline':False]['text':' result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],','line_number':14,'multiline':False]['text':'                           ..., ind_N[i_1, ..., i_M]]','line_number':15,'multiline':False]['text':'','line_number':16,'multiline':False]['text':' Note 1: ByteTensors expand to index as many dimensions as there are in the','line_number':17,'multiline':False]['text':' mask.','line_number':18,'multiline':False]['text':'','line_number':19,'multiline':False]['text':' Note 2: The behavior is more complicated when the index tensors are not all','line_number':20,'multiline':False]['text':' adjacent (e.g. x[[0, 1], :, [2, 3]]). In this case, self and the index','line_number':21,'multiline':False]['text':' tensors are transposed to the front: x.transpose(1, 2)[[0, 1], [2, 3]]','line_number':22,'multiline':False]['text':'','line_number':23,'multiline':False]['text':' The code contains two implementations of indexing. The more efficient','line_number':24,'multiline':False]['text':' implementation treats indexing like an elementwise operation over the','line_number':25,'multiline':False]['text':' tensors `result`, `x`, `ind_1`, `ind_2`, etc. This implementation does','line_number':26,'multiline':False]['text':' not work for index_put_ with accumulate=True. The other implementation','line_number':27,'multiline':False]['text':' combines the indexed tensors into a single linear index that is used','line_number':28,'multiline':False]['text':' with Tensor.put_. This is used for index_put_ with accumulate=True.','line_number':29,'multiline':False]['text':'','line_number':30,'multiline':False]['text':' The more efficient implementation takes the following steps for the','line_number':31,'multiline':False]['text':' above operation:','line_number':32,'multiline':False]['text':'','line_number':33,'multiline':False]['text':' 1) Broadcast ind_1, ind_2, ind_3 together to a common shape','line_number':34,'multiline':False]['text':' 2) Record x.stride(i) for each indexed dimension `i`','line_number':35,'multiline':False]['text':' 3) Replace the indexed subspace of `x` with the shape of the corresponding','line_number':36,'multiline':False]['text':'    subspace of `result` but with stride 0','line_number':37,'multiline':False]['text':' 4) Add dimensions of size 1 to the index tensors (ind_1, ind_2, etc.) so','line_number':38,'multiline':False]['text':'    that their shape is compatible with the result shape','line_number':39,'multiline':False]['text':'','line_number':40,'multiline':False]['text':' The CPU or CUDA kernel then computes element-wise over the broadcasted','line_number':41,'multiline':False]['text':' and restrided result, x, ind_1,  ind_2, etc.:','line_number':42,'multiline':False]['text':'','line_number':43,'multiline':False]['text':'   result[...] = *(&x[...] +','line_number':44,'multiline':False]['text':'                   ind_1[...] * x.stride(1) +','line_number':45,'multiline':False]['text':'                   ind_2[...] * x.stride(2) +','line_number':46,'multiline':False]['text':'                   ...)','line_number':47,'multiline':False]['text':'','line_number':48,'multiline':False]['text':' where & and * represent the C-style address-of and indirection operations.','line_number':49,'multiline':False]['text':' #define TORCH_ASSERT_ONLY_METHOD_OPERATORS','line_number':50,'multiline':False]['text':' namespace native','line_number':155,'multiline':False]['text':' Memory overlap checks need to be done after resizing (if required) is done.','line_number':164,'multiline':False]['text':' But it only makes sense to do these checks when result was defined, hence','line_number':165,'multiline':False]['text':' the boolean variable `check_result` here.','line_number':166,'multiline':False]['text':' For more details, see: https://github.com/pytorch/pytorch/pull/63312#discussion_r694794832','line_number':167,'multiline':False]['text':' and https://github.com/pytorch/pytorch/issues/63837','line_number':168,'multiline':False]['text':' Check if we have a valid reduce operator.','line_number':211,'multiline':False]['text':'use_new_options=','line_number':261,'multiline':True]['text':' Memory overlap checks need to be done after resizing (if required) is done.','line_number':270,'multiline':False]['text':' But it only makes sense to do these checks when result was defined, hence','line_number':271,'multiline':False]['text':' the boolean variable `check_result` here.','line_number':272,'multiline':False]['text':' For more details, see: https://github.com/pytorch/pytorch/pull/63312#discussion_r694794832','line_number':273,'multiline':False]['text':' and https://github.com/pytorch/pytorch/issues/63837','line_number':274,'multiline':False]['text':' Check that source and destination slices have the same size','line_number':299,'multiline':False]['text':' A hack to run TensorIterator checks in the meta function.','line_number':369,'multiline':False]['text':' See comment: https://github.com/pytorch/pytorch/pull/65993#discussion_r760307417','line_number':370,'multiline':False]['text':' TODO: (@krshrimali) Try inheriting from TensorIteratorBase instead.','line_number':371,'multiline':False]['text':' 'TensorIterator' needs to own the things comming from 'info', since','line_number':405,'multiline':False]['text':' 'info' will be destroyed after the META function.','line_number':406,'multiline':False]['text':' info.src is a restrided view of result','line_number':408,'multiline':False]['text':' Only allow: `dev_tensor[{cpu,dev}_tensor]`.','line_number':445,'multiline':False]['text':' See: https://github.com/pytorch/pytorch/pull/69607','line_number':446,'multiline':False]['text':' namespace meta','line_number':471,'multiline':False]['text':' Replace indexed dimensions in src with stride 0 and the size of the result tensor.','line_number':527,'multiline':False]['text':' The offset in these dimensions is computed by the kernel using the index tensor's','line_number':528,'multiline':False]['text':' values and the stride of src. The new shape is not meaningful. It's used to make','line_number':529,'multiline':False]['text':' the shape compatible with the result tensor.','line_number':530,'multiline':False]['text':' Add dimensions of size 1 to an index tensor so that it can be broadcast to the result','line_number':543,'multiline':False]['text':' shape and iterated over element-wise like the result tensor and the restrided src.','line_number':544,'multiline':False]['text':' Check if the indexed subspace contains a dim of size 0, but the replacement','line_number':574,'multiline':False]['text':' shape does not. This implies that an index is out of bounds, because there','line_number':575,'multiline':False]['text':' is no number that's a valid index for an empty tensor. Normally, out of','line_number':576,'multiline':False]['text':' bounds is handled in the indexing kernel, but this case fails earlier in','line_number':577,'multiline':False]['text':' restride_src with an unhelpful error message.','line_number':578,'multiline':False]['text':' For CUDA/MPS tensors, force all index tensors to have the same striding to','line_number':594,'multiline':False]['text':' simplify the CUDA/MPS kernel.','line_number':595,'multiline':False]['text':' info.src is restrided by restride_src with 0 strided dimensions','line_number':613,'multiline':False]['text':' For now, this is a naive implementation which does dq -> index -> q.','line_number':639,'multiline':False]['text':' TODO(future PR): improve performance by removing the copies.','line_number':640,'multiline':False]['text':' Disallow boolean indexing since it leads to dynamic output shapes','line_number':648,'multiline':False]['text':' See note [Writing Nondeterministic Operations]','line_number':661,'multiline':False]['text':' Nondeterministic when index contains duplicate entries and we do not accumulate','line_number':662,'multiline':False]['text':' If we accumulate on GPU, we use atomicGPUAdd, which is non-deterministic','line_number':663,'multiline':False]['text':' Type and device checks','line_number':668,'multiline':False]['text':' index checks','line_number':675,'multiline':False]['text':' Early return','line_number':683,'multiline':False]['text':' Do not iterate over self, we will compute the offsets manually','line_number':689,'multiline':False]['text':' NOLINTNEXTLINE(performance-implicit-conversion-in-loop)','line_number':733,'multiline':False]['text':' Type and device checks','line_number':753,'multiline':False]['text':' index checks','line_number':760,'multiline':False]['text':' Do not iterate over self, we will compute the offsets manually','line_number':767,'multiline':False]['text':' out is resized inside tensor_iterator','line_number':768,'multiline':False]['text':' Early return after out has been resized','line_number':776,'multiline':False]['text':'unsafe=','line_number':793,'multiline':True]['text':' See Note [Enabling Deterministic Operations]','line_number':800,'multiline':False]['text':' Handle the case when self / source is 0-dim','line_number':813,'multiline':False]['text':' The only difference between the following  tensor iterator and that of index_fill_ is that','line_number':817,'multiline':False]['text':' this one has also source as an input. We should refactor it when if constexpr is available (C++17)','line_number':818,'multiline':False]['text':' Prepare `index` for TensorIterator.','line_number':820,'multiline':False]['text':' It is restrided to be broadcastable over `self` in TensorIterator.','line_number':821,'multiline':False]['text':' `index` is 1d or scalar','line_number':825,'multiline':False]['text':' Prepare `result` for TensorIterator.','line_number':829,'multiline':False]['text':' Restride `result` to not advance in dimension `dim`.','line_number':830,'multiline':False]['text':' We do not use squash_dim here because `index` will','line_number':831,'multiline':False]['text':' need to advance in this dimension.','line_number':832,'multiline':False]['text':' Note that self_sizes[dim] is set to index.numel().','line_number':833,'multiline':False]['text':' This is done so that self_sizes[dim] and index_sizes[dim]','line_number':834,'multiline':False]['text':' match as required by TensorIterator (input shape should','line_number':835,'multiline':False]['text':' strictly broadcast over output shape, i.e.','line_number':836,'multiline':False]['text':' output.shape[i] >= input.shape[i] for i in range(dims)).','line_number':837,'multiline':False]['text':' We do not check for overlap because `result` is restrided','line_number':845,'multiline':False]['text':' with zero stride. Zero strides trigger memory overlap assert','line_number':846,'multiline':False]['text':' within TensorIterator.','line_number':847,'multiline':False]['text':' Not calling into index_reduce_func_impl because of a different dtype dispatch','line_number':866,'multiline':False]['text':' Equivalent to:','line_number':877,'multiline':False]['text':'   for (const auto i : c10::irange(numel)) {','line_number':878,'multiline':False]['text':'     auto selfSlice = self.select(dim, index_data[i]);','line_number':879,'multiline':False]['text':'     auto sourceSlice = source.select(dim, i);','line_number':880,'multiline':False]['text':'     selfSlice.add_(sourceSlice);','line_number':881,'multiline':False]['text':'   }','line_number':882,'multiline':False]['text':' But much faster as this reuses the iterator from add_','line_number':883,'multiline':False]['text':' When the slice of source or result is noncontiguous,','line_number':890,'multiline':False]['text':' original index_add is slow as it uses add for the sliced tensor,','line_number':891,'multiline':False]['text':' which is serial on index and parallel on sliced tensor to avoid write conflict.','line_number':892,'multiline':False]['text':' Doing parallel on the sliced tensor is not optimal as the size of sliced tensor','line_number':893,'multiline':False]['text':' may be not big enough to parallel and also causes multiple parallelizations.','line_number':894,'multiline':False]['text':' scatter_add is used to speedup for this case as scatter_add parallels on','line_number':895,'multiline':False]['text':' the outer dimension of input and is serial on the inner dimension to','line_number':896,'multiline':False]['text':' avoid write conflict. scatter_add only need one parallel and the size of','line_number':897,'multiline':False]['text':' outer dimensions is bigger to do parallel.','line_number':898,'multiline':False]['text':' Data type of index should be long and alpha should be 1 to use scatter_add.','line_number':901,'multiline':False]['text':' scatter_add does not support ComplexHalf','line_number':903,'multiline':False]['text':' Check whether result and source are matched apart from the dimension dim.','line_number':909,'multiline':False]['text':' Note that the broadcast case:','line_number':910,'multiline':False]['text':' source.select(dim, i) is broadcast for result.select(dim, index_data[i])','line_number':911,'multiline':False]['text':' The broadcast case is not applicable for scatter_add','line_number':912,'multiline':False]['text':' explicitly capture all required variables to work around windows build','line_number':962,'multiline':False]['text':' TODO: fix this when windows can correctly capture variables in nested lambda','line_number':963,'multiline':False]['text':' TODO: Maybe TensorAccessor can be used here?','line_number':969,'multiline':False]['text':' index_fill_ requires index to be a LongTensor','line_number':1016,'multiline':False]['text':' Equivalent to:','line_number':1026,'multiline':False]['text':'   for (const auto i : c10::irange(numel)) {','line_number':1027,'multiline':False]['text':'     auto selfSlice = self.select(dim, index_data[i]);','line_number':1028,'multiline':False]['text':'     auto sourceSlice = source.select(dim, i);','line_number':1029,'multiline':False]['text':'     selfSlice.op_(sourceSlice);','line_number':1030,'multiline':False]['text':'   }','line_number':1031,'multiline':False]['text':' But much faster as this reuses the iterator from the binary op','line_number':1032,'multiline':False]['text':' explicitly capture all required variables to work around windows build','line_number':1085,'multiline':False]['text':' TODO: fix this when windows can correctly capture variables in nested lambda','line_number':1086,'multiline':False]['text':' TODO: Maybe TensorAccessor can be used here?','line_number':1092,'multiline':False]['text':' Check that indices fall within dimension array size','line_number':1152,'multiline':False]['text':' Avoid redispatch call to min/max','line_number':1153,'multiline':False]['text':' Special-case single-float copy for efficiency','line_number':1198,'multiline':False]['text':' outer_dims_product specifies how many times we repeat inner dimensions,','line_number':1211,'multiline':False]['text':' so we just iterate over it to cover all outer dimensions.','line_number':1212,'multiline':False]['text':' fast pass','line_number':1267,'multiline':False]['text':' explicitly capture all required variables to work around windows build','line_number':1289,'multiline':False]['text':' TODO: fix this when windows can correctly capture variables in nested lambda','line_number':1290,'multiline':False]['text':' parallel on inner loop in case the slice is large enough;','line_number':1310,'multiline':False]['text':' otherwise parallel on outer loop','line_number':1311,'multiline':False]['text':' use a fast loop when self and result are contiguous and of the same data type','line_number':1315,'multiline':False]['text':' explicitly capture all required variables to work around windows build','line_number':1318,'multiline':False]['text':' TODO: fix this when windows can correctly capture variables in nested lambda','line_number':1319,'multiline':False]['text':' explicitly capture all required variables to work around windows build','line_number':1342,'multiline':False]['text':' TODO: fix this when windows can correctly capture variables in nested lambda','line_number':1343,'multiline':False]['text':' for composite compliance, use out-of-place variant of','line_number':1401,'multiline':False]['text':' `index_add` if index tensor is a Tensor Subclass.','line_number':1402,'multiline':False]['text':' Handle the case when `self` is 0-dim','line_number':1428,'multiline':False]['text':' Prepare `index` for TensorIterator.','line_number':1434,'multiline':False]['text':' It is restrided to be broadcastable over `self` in TensorIterator.','line_number':1435,'multiline':False]['text':' `index` is 1d or scalar','line_number':1439,'multiline':False]['text':' Prepare `self` for TensorIterator.','line_number':1443,'multiline':False]['text':' Restride `self` to not advance in dimension `dim`.','line_number':1444,'multiline':False]['text':' We do not use squash_dim here because `index` will','line_number':1445,'multiline':False]['text':' need to advance in this dimension.','line_number':1446,'multiline':False]['text':' Note that self_sizes[dim] is set to index.numel().','line_number':1447,'multiline':False]['text':' This is done so that self_sizes[dim] and index_sizes[dim]','line_number':1448,'multiline':False]['text':' match as required by TensorIterator (input shape should','line_number':1449,'multiline':False]['text':' strictly broadcast over output shape, i.e.','line_number':1450,'multiline':False]['text':' output.shape[i] >= input.shape[i] for i in range(dims)).','line_number':1451,'multiline':False]['text':' We do not check for overlap because `self` is restrided','line_number':1459,'multiline':False]['text':' with zero stride. Zero strides trigger memory overlap assert','line_number':1460,'multiline':False]['text':' within TensorIterator.','line_number':1461,'multiline':False]['text':' fast paths for GNN usage','line_number':1496,'multiline':False]['text':' skip when having empty tensor','line_number':1520,'multiline':False]['text':' skip when having scalar tensor','line_number':1525,'multiline':False]['text':' allow only different size on dim 0 for src and index','line_number':1530,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/99595','line_number':1531,'multiline':False]['text':' using `spmm` for scatter would require sorting on index,','line_number':1539,'multiline':False]['text':' this is only perf beneficial when the inner dimension, aka, `channels`','line_number':1540,'multiline':False]['text':' is big enough.','line_number':1541,'multiline':False]['text':' usually the expanded index has stride on the first dimension to be 1,','line_number':1548,'multiline':False]['text':' and strides on other dims to be 0 or 1, e.g.','line_number':1549,'multiline':False]['text':'   shape [108365, 16]; strides [1, 0]','line_number':1550,'multiline':False]['text':'   shape [13264, 1, 7]; strides [1, 1, 0]','line_number':1551,'multiline':False]['text':' index is expanded','line_number':1558,'multiline':False]['text':' gather_out_cpu_cuda','line_number':1562,'multiline':False]['text':'is_scatter_like=','line_number':1567,'multiline':True]['text':' for composite, vmap and inductor compliance, use out-of-place variant of','line_number':1579,'multiline':False]['text':' `scatter_add` if index or grad tensors is a Tensor Subclass.','line_number':1580,'multiline':False]['text':' Copy mut_out_contig's strides into a tensor','line_number':1680,'multiline':False]['text':' TODO: Is there a utility function that already does this?','line_number':1681,'multiline':False]['text':' `index_flat` contains the 1-D indices corresponding with the','line_number':1692,'multiline':False]['text':' flattened `mut_out`','line_number':1693,'multiline':False]['text':' scatter inits for reduction to appropriate indices (used by scatter_reduce.two)','line_number':1741,'multiline':False]['text':' _scatter_via_index_put can only handle sum and mean reduction type','line_number':1744,'multiline':False]['text':' Scalar src should already be deterministic','line_number':1748,'multiline':False]['text':' both runtime and compile check are required','line_number':1750,'multiline':False]['text':' See Note [Enabling Deterministic Operations]','line_number':1828,'multiline':False]['text':' Avoid gpuAtomicAdd for CUDA if deterministic mode is turned on','line_number':1829,'multiline':False]['text':'accumulate','line_number':1831,'multiline':True]['text':'is_scatter_like','line_number':1833,'multiline':True]['text':'is_scatter_like','line_number':1858,'multiline':True]['text':'use_new_options=','line_number':1863,'multiline':True]['text':' because mask_selected returns a 1-d tensor with size of masked elements','line_number':1900,'multiline':False]['text':' that are 1, we need to fill out the rest with zeros then reshape back to','line_number':1901,'multiline':False]['text':' tensor2's size.','line_number':1902,'multiline':False]['text':' deprecated, but not a hard error','line_number':1924,'multiline':False]['text':' Create strided view of result before feeding into TensorIterator','line_number':2003,'multiline':False]['text':' serial kernel','line_number':2008,'multiline':False]['text':' serial kernel requires that src is traversed in its logical order. However, TensorIterator might','line_number':2009,'multiline':False]['text':' have reordered dimensions so that src would be traversed in its physical order, producing wrong','line_number':2010,'multiline':False]['text':' answers. A sufficient condition that no reorder happened is that both _self and _mask is contiguous.','line_number':2011,'multiline':False]['text':' If it is not satisfied, use parallel kernel that handles permutations correctly','line_number':2012,'multiline':False]['text':' result is intentionally zero-strided above','line_number':2017,'multiline':False]['text':' Use a prefix sum to record the output locations of the masked elements,','line_number':2029,'multiline':False]['text':' so as to parallel with TensorIterator.','line_number':2030,'multiline':False]['text':' TODO: Here can only use std::partial_sum for C++14,','line_number':2035,'multiline':False]['text':' use std::exclusive_scan when PyTorch upgrades to C++17, which have better performance.','line_number':2036,'multiline':False]['text':' std::exclusive_scan(mask_long_data, mask_long_data + mask_long.numel(), mask_prefix_sum_data, 0);','line_number':2037,'multiline':False]['text':' result is intentionally zero-strided above','line_number':2041,'multiline':False]['text':' The following could just be written as `zeros_like(input).masked_scatter(mask, grad)`.','line_number':2065,'multiline':False]['text':' However, as an optimization, we call the in-place variant of masked_scatter.','line_number':2066,'multiline':False]['text':' Unfortunately, that doesn't allow for the broadcasting of the LHS, so we need','line_number':2067,'multiline':False]['text':' to explicitly broadcast here (the out-of-place variant of masked_scatter','line_number':2068,'multiline':False]['text':' implicitly handles broadcasting).','line_number':2069,'multiline':False]['text':' for composite compliance, use out-of-place variant','line_number':2073,'multiline':False]['text':' of `masked_scatter`.','line_number':2074,'multiline':False]['text':' update number of elements at dim as per indices','line_number':2099,'multiline':False]['text':' update number of elements at dim as per self','line_number':2105,'multiline':False]['text':' anonymous namespace','line_number':2129,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':2134,'multiline':False]['text':' similar to `take`, but `take` doesn't support the same dtypes as `gather`.','line_number':2142,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':2149,'multiline':False]['text':' similar to `take`, but `take` doesn't support the same dtypes as `gather`.','line_number':2157,'multiline':False]['text':' special case scalar input and/or index','line_number':2162,'multiline':False]['text':' Optimized all-reduce','line_number':2230,'multiline':False]['text':' Pass 1: Count nonzero element per-thread','line_number':2280,'multiline':False]['text':' Convert thread-local counts to cumulative sum','line_number':2290,'multiline':False]['text':' Default to fortran-contiguous output (see gh-46224)','line_number':2299,'multiline':False]['text':' Pass 2: Write indexes','line_number':2307,'multiline':False]['text':' Work needs to be distributed the same on both passes','line_number':2312,'multiline':False]['text':' +1 faster than additional condition check inside loop','line_number':2315,'multiline':False]['text':' Copy into local variables to improve compiler alias analysis','line_number':2331,'multiline':False]['text':' If nonzero, write index','line_number':2344,'multiline':False]['text':' Advance current index','line_number':2354,'multiline':False]['text':' Check if `size` is not negative','line_number':2384,'multiline':False]['text':' Verify that the output tensor is resized to expected size=(size, ndim)','line_number':2397,'multiline':False]['text':' Return earlier if either dim is 0','line_number':2417,'multiline':False]['text':' Delegate call to regular nonzero to get a data-dependent output','line_number':2422,'multiline':False]['text':' Copy the dynamic result to the fixed-size tensor','line_number':2426,'multiline':False]['text':' Pad result with `fill_value`','line_number':2429,'multiline':False]['text':' Check if `size` is not negative','line_number':2439,'multiline':False]['text':' Allocate fixed-size out tensor','line_number':2442,'multiline':False]['text':' special case scalar for compatibility with numpy:','line_number':2452,'multiline':False]['text':'','line_number':2453,'multiline':False]['text':' >>> np.array(5).nonzero()','line_number':2454,'multiline':False]['text':' (array([0]),)','line_number':2455,'multiline':False]['text':' >>> np.array(0).nonzero()','line_number':2456,'multiline':False]['text':' (array([], dtype=int64),)','line_number':2457,'multiline':False]['text':' order of indexing matters','line_number':2496,'multiline':False]['text':' at::native','line_number':2506,'multiline':False]