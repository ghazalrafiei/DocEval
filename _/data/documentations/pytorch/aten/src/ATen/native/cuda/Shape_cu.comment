['text':' X dim of grid for cat array cooperates on a single tensor in the cat.','line_number':42,'multiline':False]['text':' Given half of the GPU, full utilization will always occur.','line_number':43,'multiline':False]['text':' This will have cating two tensors fill the entire grid, but prevent','line_number':45,'multiline':False]['text':' many threads from needlessly load meta data if their sizes is small.','line_number':46,'multiline':False]['text':' Limit the number of thread blocks to prevent too many threads to load the metadata','line_number':65,'multiline':False]['text':' if they operate on very small tensors.','line_number':66,'multiline':False]['text':' Similar to any other IndexToOffset calculation for copying along a given','line_number':77,'multiline':False]['text':' dimension.','line_number':78,'multiline':False]['text':' linearIndex is not really linear index, but instead the offset in','line_number':87,'multiline':False]['text':' input tensor. If the input tensor is contiguous, then this offset','line_number':88,'multiline':False]['text':' is the linear index, but if the input tensor is channels last, then','line_number':89,'multiline':False]['text':' it is the linear index of the permuted contiguous tensor','line_number':90,'multiline':False]['text':'*
  * Kernel used to concatenated grimDim.y tensors into an output tensor. Uses a
  * grid-stride loop based off of the blockIdx.x, threadIdx.x for each input to
  * copy each element from each input tensor into the output.
  *
  * output: base pointer to the storage associated with the output tensor
  * inputs: GPU-allocated array of input metadata for each input to concatenate
  *         in the kernel
  * os: the size/stride vectors for the output tensor
  * concatDim: dimension along which we are concatenating
  * dimStride: the stride of the output tensor at the concatDim
  *
  * The most important assumption made is that the input tensors are contiguous.
  ','line_number':113,'multiline':True]['text':' pass meta data directly through kernel argument instead of pin memory','line_number':129,'multiline':False]['text':' In contiguous case, we will not need stride_size, setting it as 1 as placeholder','line_number':130,'multiline':False]['text':' to pass compile.','line_number':131,'multiline':False]['text':'
  Specialized implementation of the CatArrayBatchedCopy written to generate wide memory loads
  to improve memory bandwidth throughput.
','line_number':178,'multiline':True]['text':' This kernel tries to use 128 bit loads','line_number':191,'multiline':False]['text':' Handle remaining tail in case nElements does not divide','line_number':226,'multiline':False]['text':' exactly to kILP','line_number':227,'multiline':False]['text':' First, let's set up our kernel parameters. We start with a raw pointer to','line_number':240,'multiline':False]['text':' the storage for the output Tensor.','line_number':241,'multiline':False]['text':' Next, let's initialize the size, stride arrays for the output Tensor.','line_number':246,'multiline':False]['text':' permute the semantics of dims from NCHW to NHWC so that the input','line_number':253,'multiline':False]['text':' tensor is now contiguous','line_number':254,'multiline':False]['text':' If all batches are contiguous we can call a specialized implementation','line_number':269,'multiline':False]['text':' which requires the input tensor addresses to be aligned to a','line_number':270,'multiline':False]['text':' 16 Byte boundary.','line_number':271,'multiline':False]['text':' Now we loop','line_number':277,'multiline':False]['text':' There is a legacy case where a 1-D empty tensor can be concat with','line_number':286,'multiline':False]['text':' high-dimensional tensor','line_number':287,'multiline':False]['text':' If at least one of the inputs is not aligned, we can't call the','line_number':297,'multiline':False]['text':' CatArrayBatchedCopy_aligned16_contig','line_number':298,'multiline':False]['text':' Update offset','line_number':314,'multiline':False]['text':' We need max elements per tensor to compute grid parameters','line_number':317,'multiline':False]['text':' Skip if the tensor is empty. Otherwise, the grid dim is invalid','line_number':322,'multiline':False]['text':' Template Declarations for dim = 1, 2, 3, 4','line_number':349,'multiline':False]['text':' The kernels are templated on an opaque, self-aligned type of the correct','line_number':378,'multiline':False]['text':' size to avoid redundant kernels for different types of the same size.','line_number':379,'multiline':False]['text':' namespace','line_number':382,'multiline':False]['text':' We parallelize the copy if all 6 conditions pass:','line_number':399,'multiline':False]['text':'','line_number':400,'multiline':False]['text':' 1. There is more than one input tensor','line_number':401,'multiline':False]['text':' 2. The out tensor is 32-bit indexable','line_number':402,'multiline':False]['text':' 3. The number of dimensions is <= 4','line_number':403,'multiline':False]['text':' 4. All input tensors are contiguous (output tensor may be non-contig)','line_number':404,'multiline':False]['text':' 5. All input tensors can use 32-bit indexing','line_number':405,'multiline':False]['text':' We support the contiguous inputs and non-contiguous input (<=4 dims) in different ways','line_number':414,'multiline':False]['text':' For contiguous input, we don't need to pass stride meta data to cuda kernel through constant','line_number':415,'multiline':False]['text':' memory. Therefore, we could pass more inputs to cuda threads.','line_number':416,'multiline':False]['text':' For non-contiguous, we reduce the number of inputs passed to cuda kernel due to the limitation','line_number':417,'multiline':False]['text':' of constant memory.','line_number':418,'multiline':False]['text':' namespace at::native','line_number':473,'multiline':False]