['text':' The code below is only valid for the CUDA implementation. It's "okay"','line_number':96,'multiline':False]['text':' to put it here because half-to-float conversion is not supported by','line_number':97,'multiline':False]['text':' the CPU implementation of _softmax. There is a TORCH_CHECK in the CUDA','line_number':98,'multiline':False]['text':' implementation that should ideally go here as well, but there is at least','line_number':99,'multiline':False]['text':' one test in which the grad and input dtypes do not match for the CPU','line_number':100,'multiline':False]['text':' implementation of this kernel and it is not true that the grad type is','line_number':101,'multiline':False]['text':' float and the input dtype is half (see #63057).','line_number':102,'multiline':False]['text':' The code below is only valid for the CUDA implementation. It's "okay"','line_number':128,'multiline':False]['text':' to put it here because half-to-float conversion is not supported by','line_number':129,'multiline':False]['text':' the CPU implementation of _softmax. There is a TORCH_CHECK in the CUDA','line_number':130,'multiline':False]['text':' implementation that should ideally go here as well, but there is at least','line_number':131,'multiline':False]['text':' one test in which the grad and input dtypes do not match for the CPU','line_number':132,'multiline':False]['text':' implementation of this kernel and it is not true that the grad type is','line_number':133,'multiline':False]['text':' float and the input dtype is half (see #63057).','line_number':134,'multiline':False]['text':' If mask_type == 2, then mask_.sizes() must equal input_.sizes()','line_number':164,'multiline':False]['text':' Process mask differently depending on the type:','line_number':195,'multiline':False]['text':' For a generic mask of mask_type == 2, mask shape is the same as the input shape,','line_number':196,'multiline':False]['text':' so indexing is the same.','line_number':197,'multiline':False]['text':' Optimized case: attention mask of shape LxL','line_number':200,'multiline':False]['text':' outer_idx goes over BxHxL, mask_outer_idx goes over L.','line_number':201,'multiline':False]['text':' Optimized case: padding mask of shape BxL','line_number':204,'multiline':False]['text':' outer_idx goes over BxHxL, mask_outer_idx goes over B.','line_number':205,'multiline':False]['text':' Calc max in softmax dim','line_number':212,'multiline':False]['text':' Calc sum in softmax dim','line_number':230,'multiline':False]['text':' update output','line_number':253,'multiline':False]['text':' LogSoftMax and MaskedSoftMax should not both be true','line_number':255,'multiline':False]['text':' namespace','line_number':334,'multiline':False]['text':' special_softmax, alias for softmax','line_number':504,'multiline':False]['text':' Mask type might get transformed below','line_number':594,'multiline':False]['text':' Mask types 0 and 1 are only allowed for 2D masks and 4D inputs','line_number':601,'multiline':False]['text':' Padding mask of shape (B, L)','line_number':609,'multiline':False]['text':' We only process padding mask in the optimized way if softmax is applied along the last dimesion,','line_number':613,'multiline':False]['text':' otherwise we need to expand the mask into a generic 4D one','line_number':614,'multiline':False]['text':' Attention mask of shape (L, L)','line_number':620,'multiline':False]['text':' We only process attention mask in a optimized way if softmax is applied along the last dimesion,','line_number':624,'multiline':False]['text':' otherwise we need to expand the mask into a generic 4D one','line_number':625,'multiline':False]['text':' LogSoftMax ','line_number':645,'multiline':True]['text':' MaskedSoftMax ','line_number':646,'multiline':True]['text':' LogSoftMax ','line_number':678,'multiline':True]['text':' MaskedSoftmax ','line_number':679,'multiline':True]