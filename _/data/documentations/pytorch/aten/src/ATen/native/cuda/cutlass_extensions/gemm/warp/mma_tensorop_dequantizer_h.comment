['text':'**************************************************************************************************
 * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *************************************************************************************************','line_number':1,'multiline':True]['text':'! \file
  \brief Defines iterators used by warp-level matrix multiply operations targeting Tensor Cores.
','line_number':31,'multiline':True]['text':'#include <src/fastertransformer/utils/cuda_bf16_wrapper.h>','line_number':55,'multiline':False]['text':'#ifdef ENABLE_BF16','line_number':56,'multiline':False]['text':'#endif','line_number':58,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':60,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':66,'multiline':False]['text':'/ Matrix multiply operator','line_number':69,'multiline':False]['text':'/ Size of the matrix to load (concept: MatrixShape)','line_number':71,'multiline':False]['text':'/ Operand identity','line_number':73,'multiline':False]['text':'/ Data type of Scale elements','line_number':75,'multiline':False]['text':'/ Layout of operand','line_number':77,'multiline':False]['text':'/ Number of threads participating in one matrix operation','line_number':79,'multiline':False]['text':'/','line_number':81,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':85,'multiline':False]['text':' Bfloat specialization for Ampere','line_number':86,'multiline':False]['text':'/ Underlying matrix multiply operator (concept: MmaTensorOp)','line_number':88,'multiline':False]['text':'/ Shape of the warp level matrix multiply (concept: GemmShape)','line_number':90,'multiline':False]['text':'/ Mma Operator','line_number':104,'multiline':False]['text':' The architecture specific mma ooperator being used','line_number':107,'multiline':False]['text':' Mma Instruction Shape','line_number':110,'multiline':False]['text':' This is the ratio of the load instruction vs the compute instruction.','line_number':113,'multiline':False]['text':'/ Type of the scales','line_number':116,'multiline':False]['text':'/ Fragment to hold B data before Mma','line_number':119,'multiline':False]['text':' Fragment to hold scale data to apply to B before mma','line_number':122,'multiline':False]['text':' We need 1 fp16 per matrix iteration in the N dimension','line_number':123,'multiline':False]['text':'/ Warp mma shape','line_number':127,'multiline':False]['text':'/ Layout of the scales in shared memory','line_number':130,'multiline':False]['text':'/ TensorRef type for loading element from a tensor','line_number':133,'multiline':False]['text':'#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && defined(ENABLE_BF16))','line_number':158,'multiline':False]['text':' Slow path not implemented here on purpose. If we need to do HMMA on older arch, scale conversion should','line_number':181,'multiline':False]['text':' happen before scales are stored to shared memory and we should use the fp16 dequantizer. This will avoid','line_number':182,'multiline':False]['text':' numerous conversion instructions in GEMM main loop.','line_number':183,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':192,'multiline':False]['text':' Specialization for Turing & Ampere','line_number':194,'multiline':False]['text':'/ Underlying matrix multiply operator (concept: MmaTensorOp)','line_number':196,'multiline':False]['text':'/ Shape of the warp level matrix multiply (concept: GemmShape)','line_number':198,'multiline':False]['text':'/ Mma Operator','line_number':212,'multiline':False]['text':' The architecture specific mma ooperator being used','line_number':215,'multiline':False]['text':' Mma Instruction Shape','line_number':218,'multiline':False]['text':' This is the ratio of the load instruction vs the compute instruction.','line_number':221,'multiline':False]['text':'/ Type of the scales','line_number':224,'multiline':False]['text':'/ Fragment to hold B data before Mma','line_number':227,'multiline':False]['text':' Fragment to hold scale data to apply to B before mma','line_number':230,'multiline':False]['text':' We need 1 fp16 per matrix iteration in the N dimension','line_number':231,'multiline':False]['text':'/ Warp mma shape','line_number':235,'multiline':False]['text':'/ Layout of the scales in shared memory','line_number':238,'multiline':False]['text':'/ TensorRef type for loading element from a tensor','line_number':241,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':285,'multiline':False]['text':' Specialization for Volta A x RowMajor B tensorOp, for 32x32x4 interleaved gemm','line_number':287,'multiline':False]['text':'/ Underlying matrix multiply operator (concept: MmaTensorOp)','line_number':289,'multiline':False]['text':'/ Shape of the warp level matrix multiply (concept: GemmShape)','line_number':291,'multiline':False]['text':'/ Mma Operator','line_number':307,'multiline':False]['text':' The architecture specific mma ooperator being used','line_number':310,'multiline':False]['text':' Mma Instruction Shape','line_number':313,'multiline':False]['text':'/ Type of the scales','line_number':316,'multiline':False]['text':'/ Fragment to hold B data before Mma','line_number':319,'multiline':False]['text':'/ Warp mma shape','line_number':322,'multiline':False]['text':' Fragment to hold scale data to apply to B before mma','line_number':325,'multiline':False]['text':' Each 32x32x4 matmul uses 8 elements from B.','line_number':326,'multiline':False]['text':'/ Layout of the scales in shared memory','line_number':332,'multiline':False]['text':'/ TensorRef type for loading element from a tensor','line_number':335,'multiline':False]['text':' We jump by 32 here since volta does <32x32x4> super mmas inside a warp.','line_number':354,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':372,'multiline':False]['text':' Specialization for Volta A x ColumnMajor B tensorOp, for 32x32x4 interleaved gemm','line_number':374,'multiline':False]['text':'/ Underlying matrix multiply operator (concept: MmaTensorOp)','line_number':376,'multiline':False]['text':'/ Shape of the warp level matrix multiply (concept: GemmShape)','line_number':378,'multiline':False]['text':'/ Mma Operator','line_number':394,'multiline':False]['text':' The architecture specific mma ooperator being used','line_number':397,'multiline':False]['text':' Mma Instruction Shape','line_number':400,'multiline':False]['text':'/ Type of the scales','line_number':403,'multiline':False]['text':'/ Fragment to hold B data before Mma','line_number':406,'multiline':False]['text':'/ Warp mma shape','line_number':409,'multiline':False]['text':' Fragment to hold scale data to apply to B before mma','line_number':412,'multiline':False]['text':' Each 32x32x4 matmul uses 8 elements from B.','line_number':413,'multiline':False]['text':'/ Layout of the scales in shared memory','line_number':418,'multiline':False]['text':'/ TensorRef type for loading element from a tensor','line_number':421,'multiline':False]['text':' We jump by 32 here since volta does <32x32x4> super mmas inside a warp.','line_number':438,'multiline':False]['text':' For col major B, each thread will jump 4 cols to get its next value inside','line_number':439,'multiline':False]['text':' of the super mma.','line_number':440,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':468,'multiline':False]['text':' namespace warp','line_number':470,'multiline':False]['text':' namespace gemm','line_number':471,'multiline':False]['text':' namespace cutlass','line_number':472,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':474,'multiline':False]