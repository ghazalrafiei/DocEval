['text':' When using Jiterator, addcmul and addcdiv kernels get stuck during a','line_number':18,'multiline':False]['text':' promotion test on CUDA 11.3, so only enable that from CUDA 11.5:','line_number':19,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/74234#issuecomment-1100932209','line_number':20,'multiline':False]['text':'name=','line_number':27,'multiline':True]['text':'return_dtype=','line_number':28,'multiline':True]['text':'common_dtype=','line_number':29,'multiline':True]['text':'arity=','line_number':30,'multiline':True]['text':'scalar_pos=','line_number':33,'multiline':True]['text':'scalar_val=','line_number':34,'multiline':True]['text':'extra_args=','line_number':35,'multiline':True]['text':' note(mkozuki): If scalar_t is fp16 or bfloat16, cast scalar to float','line_number':47,'multiline':False]['text':' and do math in fp32 for better accuracy.','line_number':48,'multiline':False]['text':' return a + alpha * (b / static_cast<accscalar_t>(c));','line_number':58,'multiline':False]['text':' When using Jiterator, addcmul and addcdiv kernels get stuck during a','line_number':63,'multiline':False]['text':' promotion test on CUDA 11.3, so only enable that from CUDA 11.5:','line_number':64,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/74234#issuecomment-1100932209','line_number':65,'multiline':False]['text':'name=','line_number':73,'multiline':True]['text':'return_dtype=','line_number':74,'multiline':True]['text':'common_dtype=','line_number':75,'multiline':True]['text':'arity=','line_number':76,'multiline':True]['text':'scalar_pos=','line_number':79,'multiline':True]['text':'scalar_val=','line_number':80,'multiline':True]['text':'extra_args=','line_number':81,'multiline':True]['text':' note(mkozuki): If scalar_t is fp16 or bfloat16, cast scalar to float','line_number':93,'multiline':False]['text':' and do math in fp32 for better accuracy.','line_number':94,'multiline':False]['text':' namespace at::native','line_number':151,'multiline':False]