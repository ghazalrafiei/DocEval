['text':' Calculate the column offsets.','line_number':34,'multiline':False]['text':' Note this includes the sum of the columns as well as the scalar term','line_number':35,'multiline':False]['text':' B_zero_point * K, whereas the row_offsets created by','line_number':36,'multiline':False]['text':' PackAWithQuantRowOffset is only the sum of the A rows.','line_number':37,'multiline':False]['text':' namespace','line_number':57,'multiline':False]['text':' TODO: contiguous is called for further JIT optimizations.','line_number':70,'multiline':False]['text':'K=','line_number':98,'multiline':True]['text':'N=','line_number':99,'multiline':True]['text':'Bint8=','line_number':100,'multiline':True]['text':'B_zero_point=','line_number':101,'multiline':True]['text':'col_offsets=','line_number':102,'multiline':True]['text':'qtype=','line_number':103,'multiline':True]['text':'trans=','line_number':116,'multiline':True]['text':'nRow=','line_number':117,'multiline':True]['text':'nCol=','line_number':118,'multiline':True]['text':'smat=','line_number':119,'multiline':True]['text':'ld=','line_number':120,'multiline':True]['text':'pmat=','line_number':121,'multiline':True]['text':' PackBMatrix manages ownership of pmat','line_number':121,'multiline':False]['text':'groups=','line_number':122,'multiline':True]['text':' USE_FBGEMM','line_number':130,'multiline':False]['text':' We set the pre-packed linear weights to nullptr below as we call pre-pack','line_number':167,'multiline':False]['text':' during the first invocation of operator run. Refer to Linear.cpp for more','line_number':168,'multiline':False]['text':' details. TODO Update to actually call pre-pack here once bias is removed','line_number':169,'multiline':False]['text':' from pre-packing step.','line_number':170,'multiline':False]['text':' int8_t weight ','line_number':173,'multiline':True]['text':' fp32 bias ','line_number':174,'multiline':True]['text':' input_scale ','line_number':175,'multiline':True]['text':' USE_PYTORCH_QNNPACK','line_number':180,'multiline':False]['text':' TODO(mingzhe09088):','line_number':195,'multiline':False]['text':' Consider using a functor here in PackedGemmMatrixFP16','line_number':196,'multiline':False]['text':' Comments from (XQ): Not entirely sure this make_unique is safe.','line_number':197,'multiline':False]['text':' make_unique is created with regular "new", and freed through','line_number':198,'multiline':False]['text':' TypeMetaData::deleteFn in this function. This is perfectly fine if the','line_number':199,'multiline':False]['text':' tensors are created and freed within this translation unit. It might be','line_number':200,'multiline':False]['text':' very problematic if that tensor flows across dll boundaries.','line_number':201,'multiline':False]['text':' USE_FBGEMM','line_number':208,'multiline':False]['text':' Weight','line_number':218,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':230,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':240,'multiline':False]['text':' Prepack weight','line_number':246,'multiline':False]['text':' ONEDNN requires transposed weight','line_number':249,'multiline':False]['text':' Unknown when prepacking','line_number':250,'multiline':False]['text':' Bias','line_number':261,'multiline':False]['text':' oneDNN requires transposed weight','line_number':291,'multiline':False]['text':' #if AT_MKLDNN_ENABLED()','line_number':306,'multiline':False]['text':' #if AT_MKLDNN_ENABLED()','line_number':343,'multiline':False]['text':' temporarily convert weight back to fp32, needs to be fixed','line_number':358,'multiline':False]['text':' after fbgemm fixes the interface for their prepacking op (take fp16 input0','line_number':359,'multiline':False]['text':' USE_FBGEMM','line_number':366,'multiline':False]['text':' USE_PYTORCH_QNNPACK','line_number':374,'multiline':False]['text':' #if AT_MKLDNN_ENABLED()','line_number':382,'multiline':False]['text':' Not QTensor','line_number':411,'multiline':False]['text':' namespace','line_number':448,'multiline':False]['text':' namespace native','line_number':449,'multiline':False]['text':' namespace at','line_number':450,'multiline':False]