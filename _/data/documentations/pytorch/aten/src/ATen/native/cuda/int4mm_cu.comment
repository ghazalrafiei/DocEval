['text':' Overflow safe variant of (a + b - 1) / b','line_number':25,'multiline':False]['text':' Returns the increment needed to aligned the pointer to the next highest','line_number':119,'multiline':False]['text':' aligned address','line_number':120,'multiline':False]['text':' f16 vector types','line_number':131,'multiline':False]['text':' bf16 vector types','line_number':148,'multiline':False]['text':' bf162 vector types','line_number':165,'multiline':False]['text':' from','line_number':195,'multiline':False]['text':' https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h','line_number':196,'multiline':False]['text':' First, we extract the i4s and construct an intermediate fp16 number.','line_number':204,'multiline':False]['text':' We don't have enough mantissa to remove as much shift overhead as FP16, so','line_number':209,'multiline':False]['text':' we must loop. No shift needed for first item.','line_number':210,'multiline':False]['text':' or is it 8?','line_number':217,'multiline':False]['text':' (i4s & 0x000f000f) | 0x43004300','line_number':218,'multiline':False]['text':' This is the BF16 {-136, -136} represented as an integer.','line_number':225,'multiline':False]['text':' Finally, we construct the output numbers.','line_number':229,'multiline':False]['text':' Since this section is for Ampere+, we use bf16 fma to do the bias','line_number':232,'multiline':False]['text':' subtraction','line_number':233,'multiline':False]['text':' No k-reduction is needed between blocks as the number of k-tiles processed','line_number':245,'multiline':False]['text':' per block are exact and we can directly write the output','line_number':246,'multiline':False]['text':' Loads the A matrix in 16-bit standard m x k row major layout, and writes','line_number':250,'multiline':False]['text':' the C matrix in 16-bit standard m x n row major layout:','line_number':251,'multiline':False]['text':'','line_number':252,'multiline':False]['text':' size [m][k]','line_number':253,'multiline':False]['text':' access','line_number':274,'multiline':False]['text':' [mTile * kMTileSize + (laneId / 4)]','line_number':275,'multiline':False]['text':' [kTileStart * kKTileSize + (laneId % 4) * 2]','line_number':276,'multiline':False]['text':' sum.x / sum.y are written at','line_number':315,'multiline':False]['text':' [laneId / 4], [(laneId % 4) * 2, (laneId % 4) * 2 + 1]','line_number':316,'multiline':False]['text':' sum.z / sum.w are written at','line_number':317,'multiline':False]['text':' [8 + (laneId / 4)], [(laneId % 4) * 2, (laneId % 4) * 2 + 1]','line_number':318,'multiline':False]['text':' i.e., same columns, different row.','line_number':319,'multiline':False]['text':' Pointer where sum.x / sum.y is written','line_number':323,'multiline':False]['text':' sum.z, sum.w at +8 rows from cPtr','line_number':333,'multiline':False]['text':' type uint32, size [n / 8][k / (InnerKTiles * 16)][32][InnerKTiles / 2]','line_number':350,'multiline':False]['text':' n / 8: n-tiles (n8)','line_number':351,'multiline':False]['text':' k / (InnerKTiles * 16): TC size per k-tile is 16 (m16n8k16)','line_number':352,'multiline':False]['text':' 32: value per warp lane','line_number':353,'multiline':False]['text':' (InnerKTiles / 2): B layout has 4 values per lane (16 bits) per k-tile.','line_number':354,'multiline':False]['text':' 2 k-tiles packed is a uint32 (hence InnerKTiles == 2 is our smallest','line_number':355,'multiline':False]['text':' value) 4 k-tiles packed is a uint32x2 (64 bits) 8 k-tiles packed is a','line_number':356,'multiline':False]['text':' uint32x4 (128 bits)','line_number':357,'multiline':False]['text':' size [k / qGroupSize][n][2]','line_number':359,'multiline':False]['text':' Contains the scale and zero point of each of the quantized int4 values','line_number':360,'multiline':False]['text':' within B','line_number':361,'multiline':False]['text':' v_reconstructed = (bf16(B_int4_val) * scale) - zero','line_number':362,'multiline':False]['text':' offset [nTile][kTileStart / InnerKTiles][laneId][0]','line_number':372,'multiline':False]['text':' asm volatile("ld.global.cs.v2.u32 {%0, %1}, [%2];\n"','line_number':390,'multiline':False]['text':'              : "=r"(b_int4[i][0]), "=r"(b_int4[i][1])','line_number':391,'multiline':False]['text':'              : "l"(bPtrCur));','line_number':392,'multiline':False]['text':' asm volatile("ld.global.cs.v4.u32 {%0, %1, %2, %3}, [%4];\n"','line_number':400,'multiline':False]['text':'              : "=r"(b_int4[i][0]), "=r"(b_int4[i][1]),','line_number':401,'multiline':False]['text':'              "=r"(b_int4[i][2]), "=r"(b_int4[i][3]) : "l"(bPtrCur));','line_number':402,'multiline':False]['text':' Load needed info for dequantization','line_number':412,'multiline':False]['text':' smallest quantization group size is 32 (2 k-tiles are packed in an int32)','line_number':416,'multiline':False]['text':' a q-group could be larger than what we are handling in a single warp','line_number':419,'multiline':False]['text':' offset [qScale_kGroup][qScale_n][0]','line_number':431,'multiline':False]['text':'','line_number':442,'multiline':False]['text':' De-quantize int4 values to bf16. Values are dequantized as truly int4','line_number':443,'multiline':False]['text':' [-8, 7] range; dequant = (bf16(int4_value) * bf16_scale) + bf16_zero','line_number':444,'multiline':False]['text':'','line_number':445,'multiline':False]['text':' FIXME: does this negatively affect register counts, or will nvcc','line_number':447,'multiline':False]['text':' move this expansion (and data loads above) closer to the point of use?','line_number':448,'multiline':False]['text':' The dequantized values in `v` for a given lane have the same n','line_number':467,'multiline':False]['text':' dimension (the B tensor core layout has all values in the same','line_number':468,'multiline':False]['text':' thread along the same n) but different k dimension, but all are','line_number':469,'multiline':False]['text':' guaranteed to occur within the same quantization group, so we need','line_number':470,'multiline':False]['text':' only load a single scale + zero to cover what this lane has','line_number':471,'multiline':False]['text':' type pun, the __nv_bfloat162 value in bf16x2x4 is a struct and','line_number':477,'multiline':False]['text':' can't be used as a 32-bit asm register argument for `mma`','line_number':478,'multiline':False]['text':' Data for the A matrix, loaded as per ALayout','line_number':495,'multiline':False]['text':' Data for the B matrix, loaded as per BLayout','line_number':498,'multiline':False]['text':' Optional quantization data for dequantizing B, loaded as per BLayout','line_number':501,'multiline':False]['text':' Output data for the C matrix, stored as per CLayout','line_number':504,'multiline':False]['text':' The size of the matrix multiplication','line_number':507,'multiline':False]['text':' The size of the matrix multiplication, in multiples of our TC tile size','line_number':512,'multiline':False]['text':' 2/4/8 inner k-tiles correspond to 4, 8 and 16 byte innermost loads','line_number':537,'multiline':False]['text':' We always process at least kInnerKTiles k-tiles back to back in a warp','line_number':541,'multiline':False]['text':' First, handle whole multiples of KTilesPerIteration','line_number':555,'multiline':False]['text':' Each warp handles a set of KTilesPerIteration under the above limit','line_number':558,'multiline':False]['text':'','line_number':562,'multiline':False]['text':' Load data from A','line_number':563,'multiline':False]['text':'','line_number':564,'multiline':False]['text':'','line_number':569,'multiline':False]['text':' Load data from B and de-quantize as needed','line_number':570,'multiline':False]['text':' Each k-tile is bf16x2x2','line_number':571,'multiline':False]['text':'','line_number':572,'multiline':False]['text':'','line_number':586,'multiline':False]['text':' Now, perform the matrix multiplication','line_number':587,'multiline':False]['text':'','line_number':588,'multiline':False]['text':' We accumulate across k-tiles here','line_number':590,'multiline':False]['text':' We don't simply accumulate into `c` as this creates a too-strong','line_number':596,'multiline':False]['text':' execution dependency. Instead, we only periodically accumulate into','line_number':597,'multiline':False]['text':' `c`','line_number':598,'multiline':False]['text':' for all tiles under kTilesLimit','line_number':636,'multiline':False]['text':' Now, there could be a remainder of 1 to KTilesPerIteration - 1 k-tiles','line_number':638,'multiline':False]['text':' remaining. We guarantee that the number of warps is >= KTilesPerIteration /','line_number':639,'multiline':False]['text':' kInnerKTiles, so that each warp can simply load kInnerKTiles and do its','line_number':640,'multiline':False]['text':' thing without needing more warps','line_number':641,'multiline':False]['text':' If we have any remainder k-tiles, some warps will handle them, processing','line_number':646,'multiline':False]['text':' kInnerKTiles k-tiles at a time','line_number':647,'multiline':False]['text':' We don't simply accumulate into `c` as this creates a too-strong','line_number':668,'multiline':False]['text':' execution dependency. Instead, we only periodically accumulate into','line_number':669,'multiline':False]['text':' `c`','line_number':670,'multiline':False]['text':'','line_number':706,'multiline':False]['text':' Reduce independent k-tiles (same m/n) across warps','line_number':707,'multiline':False]['text':'','line_number':708,'multiline':False]['text':' FIXME: this likely doesn't need to be a true reduction tree, can just be a','line_number':711,'multiline':False]['text':' serial sum, maybe (unless nvcc/ptxas goes back to its old ways)','line_number':712,'multiline':False]['text':' smem_sum[warpId][laneId] = TreeReduce4<KTilesPerIteration>::reduce(c);','line_number':713,'multiline':False]['text':' Reduce across the block in the first warp','line_number':721,'multiline':False]['text':' Write the reduced result (in the first warp) into the output','line_number':730,'multiline':False]['text':' n for C output becomes k for A input, so for m16n8k16,','line_number':737,'multiline':False]['text':' we need to halve the tiles','line_number':738,'multiline':False]['text':' optional ','line_number':756,'multiline':True]['text':' The chunking kernel requires that kTiles is a multiple of kInnerKTiles','line_number':765,'multiline':False]['text':' After intra-block reduction across the k dimension, we are left with this','line_number':774,'multiline':False]['text':' many tiles','line_number':775,'multiline':False]['text':'  int32_t postKernelKTiles = kTiles / (Warps * KTilesPerWarp);','line_number':776,'multiline':False]['text':' we loop','line_number':777,'multiline':False]['text':' FIXME: parallelize better, smem staging etc?','line_number':804,'multiline':False]['text':' size [n][k]','line_number':807,'multiline':False]['text':' size [ceil(n / 8)][ceil(k / (InnerKTiles * 16))][32][InnerKTiles / 2]','line_number':809,'multiline':False]['text':' int4 values are packed into int32 values, which require at least 8. Given','line_number':811,'multiline':False]['text':' m16n8k16 B layout requires 4 scalar values/lane, the minimum number of','line_number':812,'multiline':False]['text':' innermost k-tiles that we can use is 2.','line_number':813,'multiline':False]['text':' gridDim.x corresponds to the number of k-tiles divided by InnerKTiles','line_number':819,'multiline':False]['text':' Two k-tiles are packed into an int32 at a time','line_number':824,'multiline':False]['text':' n dimension that this lane loads from','line_number':827,'multiline':False]['text':' inner k-tiles pack two at a time','line_number':857,'multiline':False]['text':' row major layout','line_number':880,'multiline':False]['text':' tensor core layout','line_number':884,'multiline':False]['text':' row major layout','line_number':888,'multiline':False]['text':' The number of inner k tiles is the innermost dimension of  times 2','line_number':892,'multiline':False]['text':' 2 k-tiles (4 values per lane per tile, 8 values total) quantized to int4','line_number':893,'multiline':False]['text':' packed into 1 int32 for int4 B','line_number':894,'multiline':False]['text':' A is standard row major','line_number':898,'multiline':False]['text':' B has B_innerKTiles k-tiles in the innermost dimension','line_number':903,'multiline':False]['text':' Validate the scale and zero point tensor for dequantization','line_number':910,'multiline':False]['text':' These are the only versions handled at the moment','line_number':911,'multiline':False]['text':' Output is a standard row-major matrix','line_number':924,'multiline':False]['text':' input is [n][k] (int32 dtype)','line_number':1038,'multiline':False]['text':' output is [n / 8][k / (InnerKTiles * 16)][32][innerKTiles / 2]','line_number':1039,'multiline':False]['text':' At least 2 k-tiles need to be packed back to back in the innermost','line_number':1050,'multiline':False]['text':' dimension, as the m16n8k16 tensor core tile presents 4 scalar values for','line_number':1051,'multiline':False]['text':' the B matrix, but the minimum word size for the packed format is 4 bytes','line_number':1052,'multiline':False]['text':' (int32). 4 inner K-tiles = 8 byte load, 8 inner k-tiles = 16 byte load','line_number':1053,'multiline':False]['text':' which is the maximum vectorized load/store size','line_number':1054,'multiline':False]['text':' k-tiles are packed back to back in the innermost dimension in order to','line_number':1062,'multiline':False]['text':' allow for 4/8/16 byte loads','line_number':1063,'multiline':False]['text':' kSuperTiles is the number of k-tiles assuming k is innerKTiles * kKTileSize','line_number':1065,'multiline':False]['text':' each block handles `innerKTiles` k-tiles.','line_number':1068,'multiline':False]['text':' 2 k-tiles are a single int32','line_number':1069,'multiline':False]['text':' namespace at::native','line_number':1098,'multiline':False]