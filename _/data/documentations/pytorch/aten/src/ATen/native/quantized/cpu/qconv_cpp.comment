['text':' To have a sanity check for maximum matrix size.','line_number':41,'multiline':False]['text':' namespace','line_number':43,'multiline':False]['text':' Batch size','line_number':119,'multiline':False]['text':' Output channels','line_number':120,'multiline':False]['text':' USE_FBGEMM','line_number':175,'multiline':False]['text':'D','line_number':186,'multiline':True]['text':' USE_PYTORCH_QNNPACK','line_number':194,'multiline':False]['text':' Quantized kernels are all written with NHWC (channels last) layout in','line_number':258,'multiline':False]['text':' mind. Ideally, we'd be compatible with conv2d behavior and preserve the','line_number':259,'multiline':False]['text':' inputs layout as is (doing necessary upconversions).','line_number':260,'multiline':False]['text':'','line_number':261,'multiline':False]['text':' However, to be more robust, for now we just force output layout to always','line_number':262,'multiline':False]['text':' be NHWC (channels last), thus opportunistically improving perf.','line_number':263,'multiline':False]['text':'','line_number':264,'multiline':False]['text':' This might change when full memory format support lands','line_number':265,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/23403','line_number':266,'multiline':False]['text':' Batch size','line_number':372,'multiline':False]['text':' Number of input channels','line_number':373,'multiline':False]['text':' Number of output channels','line_number':374,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':393,'multiline':False]['text':' if use direct convolution implementation, compute the col_offsets','line_number':420,'multiline':False]['text':' of the weight matrix at model initialization stage.','line_number':421,'multiline':False]['text':' We need to know the shape of output matrix','line_number':422,'multiline':False]['text':' to compute col_offsets for direct convolution.','line_number':423,'multiline':False]['text':' Hence it cannot be called from inside weight packing function','line_number':424,'multiline':False]['text':' like other quantized conv implementation','line_number':425,'multiline':False]['text':' row offset buffer ','line_number':482,'multiline':True]['text':' thread_id','line_number':495,'multiline':True]['text':' num_threads ','line_number':496,'multiline':True]['text':' row offset buffer ','line_number':508,'multiline':True]['text':' thread_id','line_number':522,'multiline':True]['text':' num_threads ','line_number':523,'multiline':True]['text':' USE_FBGEMM','line_number':561,'multiline':False]['text':'
   * NB:
   * [de]conv_prepack prepares weights (values, scale, and zero_points) ahead of
   * time during prepack() call assuming the activation will be uint8_t. But it
   * may not always be the case. A solution may involve making prepack routine
   * aware of the input qdtype. But currently all the pieces are not ready to
   * pass that model level info to the prepack function. So, for now, here in
   * this function we have to massage weights if we learn the input qdtype is
   * not uint8_t. This involves copying and converting uint8_t to int8_t
   * whenever necessary. To add to that, since XNNPACK, as of writing this,
   * doesn't support per_channel weights for quint8_t, we add following assert
   * makes sure we don't run into that case. Also take shortcuts when processing
   * weights, which means we have to revisit and fix some weight massging logic
   * when we enable the missing feature in XNNPACK.
   *
   * Table below summarizes how the weights are handled,
   *
   * .-------------------------------------------------------------------------.
   * | input_qdtype |              uint8_t            |            int8_t      |
   * | per_channel  |       yes       |       no      |      yes     |    no   |
   * |-------------------------------------------------------------------------|
   * | zero_points  | at::zeros()*    | orig_zp + 128 | at:zeros()** | orig_zp |
   * | scale        |            dtype = float, no changes needed              |
   * | values       |        always processed before passing to XNNPACK        |
   * .-------------------------------------------------------------------------.
   *
   * Notes: * - zero_points for uint8_t + per_channel: no support in xnnpack, need
   * to fix when support is added. ** - zero_points for int8_t: symmetric
   * quantization means XNNPACK will ignore kernel zero point(s).
   ','line_number':581,'multiline':True]['text':' More checks','line_number':618,'multiline':False]['text':' Create an operator iff necessary','line_number':639,'multiline':False]['text':' Update the input scale so we may cache the op','line_number':644,'multiline':False]['text':' create an empty tensor for packing the weights','line_number':647,'multiline':False]['text':' layout ','line_number':662,'multiline':True]['text':' pin_memory ','line_number':664,'multiline':True]['text':' per_channel ','line_number':668,'multiline':True]['text':' see comment above about w_zp ','line_number':672,'multiline':True]['text':' layout ','line_number':675,'multiline':True]['text':' pin_memory ','line_number':677,'multiline':True]['text':' copy from the original weight and take care of dtype change if necessary','line_number':681,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':689,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':693,'multiline':False]['text':' Original bias was float, so we requantize it here.','line_number':698,'multiline':False]['text':' will be ignored for Q[SC]8, see comment
                above about w_zp','line_number':719,'multiline':True]['text':' Allocate output Tensor and a buffer for XNNPACK to use','line_number':762,'multiline':False]['text':' layout ','line_number':766,'multiline':True]['text':' pin_memory ','line_number':768,'multiline':True]['text':' Setup the operator','line_number':773,'multiline':False]['text':' Run the operator','line_number':794,'multiline':False]['text':' xnn_operator_t op ','line_number':796,'multiline':True]['text':' pthreadpool_t threadpool ','line_number':797,'multiline':True]['text':' USE_XNNPACK','line_number':809,'multiline':False]['text':' QNNPack is not thread safe','line_number':817,'multiline':False]['text':' TODO Can be replaced with packB->getOutputChannels() when update pre-pack','line_number':837,'multiline':False]['text':' to actually do the packing.','line_number':838,'multiline':False]['text':' inputs are in semantic NCHW format','line_number':841,'multiline':False]['text':' output channels','line_number':847,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':855,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':860,'multiline':False]['text':' Re-quantizing the bias based on input scale and weight scale.','line_number':867,'multiline':False]['text':' Get the original weight and adjust it to uint8 from int8','line_number':874,'multiline':False]['text':' We calculate requant scale here as the vector holding the requant scale','line_number':881,'multiline':False]['text':' is owned by this module. The pointer is then passed to qnnpack backend.','line_number':882,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':884,'multiline':False]['text':' TODO Kimish, we are allocating affine_quantized regardless of per channel or not.','line_number':887,'multiline':False]['text':' This allocation is actually used only for packing weight and thus will be freed.','line_number':888,'multiline':False]['text':' Still we should be consistent. Fix this.','line_number':889,'multiline':False]['text':' Original bias was float, so we requantize it here.','line_number':901,'multiline':False]['text':' Update the input scale to not pack again.','line_number':904,'multiline':False]['text':' On mobile, we release the original weight by resetting the intrusive_ptr.','line_number':914,'multiline':False]['text':' Calling unpack after this will throw an assertion.','line_number':915,'multiline':False]['text':' Set padding buffer to zero point. This can only be done if we want','line_number':919,'multiline':False]['text':' to do it only once.','line_number':920,'multiline':False]['text':' Allocate output Tensor and a buffer for QNNPACK to use','line_number':959,'multiline':False]['text':' layout ','line_number':963,'multiline':True]['text':' pin_memory ','line_number':965,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':970,'multiline':False]['text':' No support for 3d convolution ','line_number':1025,'multiline':True]['text':' int8_t deconv does not support per-channel ','line_number':1027,'multiline':True]['text':' don't want this to fall through to QNNPACK ','line_number':1029,'multiline':True]['text':' USE_XNNPACK','line_number':1039,'multiline':False]['text':' fall through for unsupported types, configs, or shapes ','line_number':1050,'multiline':True]['text':' USE_XNNPACK','line_number':1051,'multiline':False]['text':' fall through for unsupported types, configs, or shapes ','line_number':1064,'multiline':True]['text':' USE_XNNPACK','line_number':1065,'multiline':False]['text':' USE_PYTORCH_QNNPACK','line_number':1099,'multiline':False]['text':' has_accum: extra input besides the conv to do conv add fusion.','line_number':1151,'multiline':False]['text':' src','line_number':1171,'multiline':False]['text':' weights & bias','line_number':1178,'multiline':False]['text':' dst','line_number':1182,'multiline':False]['text':' Prepacked weight format: [o, i, ...]','line_number':1186,'multiline':False]['text':' batch size','line_number':1187,'multiline':False]['text':' input channels','line_number':1188,'multiline':False]['text':' output channels','line_number':1189,'multiline':False]['text':' input depth','line_number':1190,'multiline':False]['text':' input height','line_number':1191,'multiline':False]['text':' input width','line_number':1192,'multiline':False]['text':' kernel height','line_number':1193,'multiline':False]['text':' kernel width','line_number':1194,'multiline':False]['text':' kernel depth','line_number':1195,'multiline':False]['text':' weight: [o, i, ...]','line_number':1196,'multiline':False]['text':' When fused with sum, the dst tensor will share the data ptr as the accum tensor.','line_number':1233,'multiline':False]['text':' Parameters','line_number':1240,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':1247,'multiline':False]['text':' Just tells we have these post op, the actual value such as scale and zero point will be setted later.','line_number':1258,'multiline':False]['text':' Set the dst scale and zero point with the value of accum.','line_number':1262,'multiline':False]['text':' The true scale and zero point is stored in ideep::scale_t(scale_size, inv_output_scale) and dst_zero_points.','line_number':1263,'multiline':False]['text':' Bias might be modified outside (e.g. by quantization bias correction).','line_number':1270,'multiline':False]['text':' If so, update the prepacked bias as well.','line_number':1271,'multiline':False]['text':' Primitive cache is initialized when called for the first time','line_number':1278,'multiline':False]['text':' and won't be updated afterwards.','line_number':1279,'multiline':False]['text':' not transposed','line_number':1311,'multiline':False]['text':' If hit, use cached data. If miss, fall back to normal path.','line_number':1328,'multiline':False]['text':' When fused with sum, the accum tensor share the data ptr as dst tensor as the output.','line_number':1344,'multiline':False]['text':' Reset output's scale and zero point into accum_contig.','line_number':1345,'multiline':False]['text':' contains quantized values but not QTensor','line_number':1375,'multiline':False]['text':' MKLDNN tensor with quantized values','line_number':1378,'multiline':False]['text':' Bias is not packed into MKLDNN tensor','line_number':1381,'multiline':False]['text':' inv_output_scale is the reciprocal of scale in fake quant','line_number':1387,'multiline':False]['text':' accum to fused with conv add','line_number':1389,'multiline':False]['text':'*******************************','line_number':1398,'multiline':True]['text':'          Checks               ','line_number':1399,'multiline':True]['text':'*******************************','line_number':1400,'multiline':True]['text':' Due the constant folding inside Inductor freeze,','line_number':1401,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/b99d605a3070de35677cc43f0196c2f2e807b822/torch/ao/quantization/fx/_decomposed.py#L62-L63','line_number':1402,'multiline':False]['text':' inv_scale = 1.0 / scale will be folded.','line_number':1403,'multiline':False]['text':' So, we can only get inv_scale from quant node which is used as','line_number':1404,'multiline':False]['text':' output_scale of this op.','line_number':1405,'multiline':False]['text':' When fp32 or bf16 output, oneDNN expects op_attr doesn't set_scales and set_zero_points.','line_number':1409,'multiline':False]['text':' So, we will use default inv_output_scale as 1.0 and output_zero_point as 0, since','line_number':1410,'multiline':False]['text':' when inv_output_scale is 1.0, we will skip invoking of op_attr.set_scales in ideep;','line_number':1411,'multiline':False]['text':' when output_zero_point is 0, we will skip invoking of op_attr.set_zero_points in ideep.','line_number':1412,'multiline':False]['text':' has_accum_postop_sum: extra input besides the conv to do conv add fusion with post op sum.','line_number':1422,'multiline':False]['text':' N, C, L -> N, C, 1, L','line_number':1454,'multiline':False]['text':' Parameters','line_number':1482,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':1483,'multiline':False]['text':' 1. If the weight scale generated by observer should with dtype float32','line_number':1487,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/d2c24eca8a60c56b31ca967a44d5cc4522802aa6/torch/ao/quantization/observer.py#L323','line_number':1488,'multiline':False]['text':' 2. If the weight scale got from the quantized tensor, like did in the UT. It's with dtype of double.','line_number':1489,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/d2fa3f608b5e4f582a8aaf752f10efe4ca72a7d0/aten/src/ATen/quantized/Quantizer.cpp#L69','line_number':1490,'multiline':False]['text':' For case 2, we will convert it from double to float, since ideep::scale_t is alias of std::vector<float>','line_number':1495,'multiline':False]['text':' TODO (leslie): optimize the performance here:','line_number':1504,'multiline':False]['text':' 1. Remove the reciprocal of weight scale, we have done the reciprocal of weight scale back in Ideep:','line_number':1505,'multiline':False]['text':' https://github.com/intel/ideep/blob/3c90e365526e19c110371d23831678a7e9d4353d/include/ideep/operators/conv.hpp#L163-L168','line_number':1506,'multiline':False]['text':' 2. Remove 2 memory copies of weight_scales:','line_number':1507,'multiline':False]['text':'   2.1 Input of weights_scales is PyTorch Dense tensor, we convert it to vector<float>','line_number':1508,'multiline':False]['text':'   2.2 OneDNN stream submit convert weights_scales from vector to ideep::tensor','line_number':1509,'multiline':False]['text':'   https://github.com/intel/ideep/blob/3c90e365526e19c110371d23831678a7e9d4353d/include/ideep/operators/conv.hpp#L1855-L1860','line_number':1510,'multiline':False]['text':' We should be able to directly convert weights_scales from PyTorch Dense Tensor to IDeep Tensor which can share same data ptr.','line_number':1511,'multiline':False]['text':' Weight is quant per tensor, then weight_scales will be a scalar Tensor','line_number':1514,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':1515,'multiline':False]['text':' Weight is quant per channel','line_number':1517,'multiline':False]['text':' Weight','line_number':1529,'multiline':False]['text':' Bias','line_number':1532,'multiline':False]['text':' For int8-mixed-bf16, we will also use float32 bias','line_number':1539,'multiline':False]['text':'*******************************','line_number':1551,'multiline':True]['text':'        Computation            ','line_number':1552,'multiline':True]['text':'*******************************','line_number':1553,'multiline':True]['text':' src','line_number':1554,'multiline':False]['text':' dst','line_number':1564,'multiline':False]['text':' Output is not a quantized tensor but data type is uint8','line_number':1570,'multiline':False]['text':' accum_contig is KFloat32 and we expect a kBFloat16 output','line_number':1604,'multiline':False]['text':' or accum_contig is kBFloat16 and we expect a KFloat32 output','line_number':1605,'multiline':False]['text':' When fused with sum, the dst tensor will share the data ptr as the accum tensor.','line_number':1610,'multiline':False]['text':' Conv without add: int8-in, fp32-output','line_number':1614,'multiline':False]['text':' attr','line_number':1623,'multiline':False]['text':' Set the dst scale and zero point with the value of accum.','line_number':1628,'multiline':False]['text':' The true scale and zero point is stored in ideep::scale_t(scale_size, inv_output_scale) and dst_zero_points.','line_number':1629,'multiline':False]['text':' Weight Reorder','line_number':1652,'multiline':False]['text':' Computation','line_number':1665,'multiline':False]['text':' #if AT_MKLDNN_ENABLED()','line_number':1679,'multiline':False]['text':'
 * FBGEMM uses vpmaddubsw instruction to multiply activations (uint8_t) and
 * weights (int8_t).
 *
 * https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_maddubs_epi16&expand=3284,3530
 *
 * vpmaddubsw operates on a vector of activations and a vector of
 * weights. If these vectors are
 *
 *    A (uint8_t) = a0, a1, a2, a3 ...
 *
 * and
 *
 *    B (int8_t)  = b0, b1, b2, b3 ...
 *
 * the result of this instruction is an int16_t vector with values
 *
 *    C (int16_t) = a0*b0 + a1*b1, a2*b2 + a3*b3 ...
 *
 * For large values of A and/or B the result (a0*b0 + a1*b1) might not fit into
 * an int16_t number. So the instruction saturates them to max (or min) possible
 * value of an int16_t number. Such behavior is expected for the
 * implementation below.
 *
 * For example, a0 = 255, a1 = 255, b0 = 127 and b1 = 127 the actual result
 * 64770 overflows for an int16_t number (-32768, 32767) so the returned result
 * is 32767.
 *
 ','line_number':1684,'multiline':True]['text':' N, C, L -> N, C, 1, L','line_number':1766,'multiline':False]['text':' N, C, 1, L -> N, C, L','line_number':1773,'multiline':False]['text':' kernel for maintaining backward compatibility','line_number':1778,'multiline':False]['text':'stride','line_number':1785,'multiline':True]['text':'padding','line_number':1786,'multiline':True]['text':'dilation','line_number':1787,'multiline':True]['text':'groups','line_number':1788,'multiline':True]['text':' contains quantized values but not QTensor','line_number':1810,'multiline':False]['text':' contains quantized values but not QTensor','line_number':1813,'multiline':False]['text':' inv_output_scale is the reciprocal of scale in fake quant','line_number':1821,'multiline':False]['text':' Conv1D/3D post op check','line_number':1829,'multiline':False]['text':' Conv2D post op check','line_number':1838,'multiline':False]['text':'transposed','line_number':1848,'multiline':True]['text':'accum','line_number':1850,'multiline':True]['text':'accum_scale','line_number':1850,'multiline':True]['text':'accum_zero_point','line_number':1850,'multiline':True]['text':'output_dtype','line_number':1851,'multiline':True]['text':'binary_attr','line_number':1851,'multiline':True]['text':'binary_alpha','line_number':1851,'multiline':True]['text':'unary_attr','line_number':1852,'multiline':True]['text':'unary_scalars','line_number':1852,'multiline':True]['text':'unary_algorithm','line_number':1852,'multiline':True]['text':' contains quantized values but not QTensor','line_number':1859,'multiline':False]['text':' contains quantized values but not QTensor','line_number':1862,'multiline':False]['text':' contains quantized values but not QTensor','line_number':1865,'multiline':False]['text':' inv_output_scale is the reciprocal of scale in fake quant','line_number':1873,'multiline':False]['text':' Conv2D post op check','line_number':1882,'multiline':False]['text':'transposed','line_number':1900,'multiline':True]['text':'output_dtype','line_number':1903,'multiline':True]['text':' for backward compatibility','line_number':1921,'multiline':False]['text':' transpose','line_number':1927,'multiline':False]['text':' transpose','line_number':1939,'multiline':False]['text':' Conv1D/2D/3D with unary postop','line_number':1945,'multiline':False]['text':' Conv2D with binary postop','line_number':1950,'multiline':False]['text':' namespace','line_number':1954,'multiline':False]['text':' namespace at::native','line_number':1955,'multiline':False]