['text':' #define START_IND(a,b,c) a * c / b','line_number':34,'multiline':False]['text':' #define END_IND(a,b,c)  (a + 1) * c / b + ((a + 1) * c % b > 0)?1:0','line_number':35,'multiline':False]['text':' this is safe, in reality 256 is our limit','line_number':37,'multiline':False]['text':' increasing block_stride to lower # of blocks launched','line_number':38,'multiline':False]['text':' 4d tensor B x D x H x W','line_number':44,'multiline':False]['text':' All kernels view batch dim B and feature dim D as collapsed.','line_number':45,'multiline':False]['text':'
   * Description:
   *    this function adaptively average pools an input 4D tensor along dimensions 2 and 3
   *    4D input, 4D output
   ','line_number':47,'multiline':True]['text':' iterators on output pixels','line_number':59,'multiline':False]['text':' select input/output plane based on thread/block ID','line_number':62,'multiline':False]['text':' For all output pixels...','line_number':77,'multiline':False]['text':' Compute the average pooling over corresponding input pixels','line_number':90,'multiline':False]['text':' next input line','line_number':100,'multiline':False]['text':' Update output','line_number':102,'multiline':False]['text':'
   * Description:
   *    this function computes the gradInput from gradOutput
   ','line_number':108,'multiline':True]['text':' iterators on input pixels','line_number':118,'multiline':False]['text':' select input/output plane based on thread/block ID','line_number':121,'multiline':False]['text':' compute gradInput','line_number':136,'multiline':False]['text':' Compute the gradients over corresponding output pixels','line_number':147,'multiline':False]['text':'
   * Description:
   *    this function computes the gradInput from gradOutput
   *    (uses atomic add)
   ','line_number':163,'multiline':True]['text':' iterators on output indices','line_number':174,'multiline':False]['text':' select input/output plane based on thread/block ID','line_number':177,'multiline':False]['text':' For all output pixels...','line_number':192,'multiline':False]['text':' Compute the gradients for over corresponding input pixels','line_number':205,'multiline':False]['text':' atomic add since different threads could update same variable','line_number':213,'multiline':False]['text':' next input line','line_number':216,'multiline':False]['text':'
   * Description:
   *    this function adaptively average pools an input 4D tensor along dimensions 2 and 3
   *    NHWC layout for both input and output tensor
   *    4D input, 4D output
   ','line_number':222,'multiline':True]['text':' flattening cta for pre-computation & smem initialization;','line_number':242,'multiline':False]['text':' use shared memory to store temporary output value. This is simply to','line_number':246,'multiline':False]['text':' reduce register usage.','line_number':247,'multiline':False]['text':' each CTA handles a portion of a single slice on batch dimension;','line_number':254,'multiline':False]['text':' each CTA handles a single slice on batch dimension;','line_number':259,'multiline':False]['text':' We use gridDim.x to handle striding on C as well.','line_number':260,'multiline':False]['text':' split out_cached and exclusively it assigned to each thread;','line_number':264,'multiline':False]['text':' iterate on output H & W.','line_number':267,'multiline':False]['text':' Each CTA handles a consecutive H & W section (TILE); Do NOT stride CTA on','line_number':268,'multiline':False]['text':' tile so there's a better chance to hit L1 cache.','line_number':269,'multiline':False]['text':' Stride for threads, each warp can reuse L1 as they go. So theoretically','line_number':277,'multiline':False]['text':' better chance to survive cache eviction.','line_number':278,'multiline':False]['text':' loop on input: hierarchy h->w->c, use shared memory here hopefully','line_number':287,'multiline':False]['text':' would not stall global memory read;','line_number':288,'multiline':False]['text':' write accumulated output to global memory;','line_number':304,'multiline':False]['text':' This causes numerical issueptr when unit test with NCHW kernel;','line_number':308,'multiline':False]['text':' switch to could verify the correctness;','line_number':309,'multiline':False]['text':' output[c] = out_cached[c] / (iendH-istartH) / (iendW-istartW);','line_number':310,'multiline':False]['text':' no need to __syncthreads() since out_cached is not shared.','line_number':315,'multiline':False]['text':'
   * Description:
   *    this function computes the gradInput from gradOutput
   *    NHWC layout for both input and output tensor
   *    4D input, 4D output
   ','line_number':320,'multiline':True]['text':' be careful with alignment, in case scalar_t is fp16, we want to assign','line_number':340,'multiline':False]['text':' int pointers first.','line_number':341,'multiline':False]['text':' flattening cta for pre-computation & smem initialization;','line_number':346,'multiline':False]['text':' Precompute output start/end index per input index on width dimension;','line_number':350,'multiline':False]['text':' Not doing this for height dimension, as that's our out-most loop.','line_number':351,'multiline':False]['text':' Precompute pooling height/weight factor for each output element;','line_number':357,'multiline':False]['text':' This is used to weight output gradient when accumulate them on input','line_number':358,'multiline':False]['text':' gradient.','line_number':359,'multiline':False]['text':' Technically we don't have to compute it for the whole `osizeH`, since','line_number':360,'multiline':False]['text':' each cta only covers a consecutive portion of the entire output. But it's','line_number':361,'multiline':False]['text':' not going to save us from code divergence, and shared memory save is not','line_number':362,'multiline':False]['text':' an issue neither, so just leave it as is for now.','line_number':363,'multiline':False]['text':' each CTA handles a portion of a single slice on batch dimension;','line_number':371,'multiline':False]['text':' use shared memory to store temporary output value. This is simply to','line_number':376,'multiline':False]['text':' reduce register usage.','line_number':377,'multiline':False]['text':' each CTA handles a portion of a single slice on batch dimension;','line_number':384,'multiline':False]['text':' We use gridDim.x to handle striding on C as well.','line_number':385,'multiline':False]['text':' split out_cached and exclusively it assigned to each thread;','line_number':389,'multiline':False]['text':' iterate on input H & W.','line_number':392,'multiline':False]['text':' Each CTA handles a consecutive H & W section (TILE); Do NOT stride CTA on','line_number':393,'multiline':False]['text':' tile so there's a better chance to hit L1 cache.','line_number':394,'multiline':False]['text':' Stride for threads, each warp can reuse L1 as they go. So theoretically','line_number':402,'multiline':False]['text':' better chance to survive cache eviction.','line_number':403,'multiline':False]['text':' loop on output: hierarchy h->w->c, so we could reuse weight factor f','line_number':408,'multiline':False]['text':' because it remains the same for given oh & ow','line_number':409,'multiline':False]['text':' write accumulated gradIput to global memory;','line_number':425,'multiline':False]['text':' no need to __syncthreads() since out_cached is not shared.','line_number':433,'multiline':False]['text':' 4d tensor B x D x H x W','line_number':438,'multiline':False]['text':' special case for tensor memory format in channels_last','line_number':463,'multiline':False]['text':' preserve channels_last stride on output tensor;','line_number':480,'multiline':False]['text':' TODO: modify this after resize_ added `memory_format` tag','line_number':482,'multiline':False]['text':' Launch kernel on output tensor elements. Logic behind launch config:','line_number':496,'multiline':False]['text':' output tensor size NCHW, strides NHWC;','line_number':497,'multiline':False]['text':' Launch on:','line_number':498,'multiline':False]['text':' N -> grid.x','line_number':499,'multiline':False]['text':' H -> grid.z * block.z','line_number':500,'multiline':False]['text':' W -> grid.y * block.y','line_number':501,'multiline':False]['text':' C -> block.x','line_number':502,'multiline':False]['text':' encourage larger block_y & block_z for better cache hit while maintain','line_number':503,'multiline':False]['text':' reasonable block_x for coalesced memory access;','line_number':504,'multiline':False]['text':' Do NOT clip grid_x, striding on Batch dimension is not in the kernel,','line_number':517,'multiline':False]['text':' although it could be easily implemented given current kernel.','line_number':518,'multiline':False]['text':' it's OK to clip grid_y & grid_z, as we block the two dimensions in the kernel;','line_number':520,'multiline':False]['text':' we are dealing with packed tensor here. max index is the same as numel.','line_number':528,'multiline':False]['text':' TODO: to really support input tensor large enought to go beyond int32,','line_number':529,'multiline':False]['text':' we will need to restrict out shared memory usage and adjust the launch','line_number':530,'multiline':False]['text':' config;','line_number':531,'multiline':False]['text':' cuda blocks & threads:','line_number':579,'multiline':False]['text':' run averagepool kernel','line_number':584,'multiline':False]['text':' special case for tensor memory format in channels_last','line_number':616,'multiline':False]['text':' preserve channels_last stride on input tensor;','line_number':635,'multiline':False]['text':' Launch kernel on input tensor elements. Logic behind launch config:','line_number':648,'multiline':False]['text':' input tensor size NCHW, strides NHWC;','line_number':649,'multiline':False]['text':' Launch on:','line_number':650,'multiline':False]['text':' N(C) -> grid.x (striding on C to reduce sh_mem usage)','line_number':651,'multiline':False]['text':' H    -> grid.z * block.z','line_number':652,'multiline':False]['text':' W    -> grid.y * block.y','line_number':653,'multiline':False]['text':' C    -> block.x','line_number':654,'multiline':False]['text':' encourage larger block_y & block_z for better cache hit while maintain','line_number':655,'multiline':False]['text':' reasonable block_x for coalesced memory access;','line_number':656,'multiline':False]['text':' Do NOT clip grid_x, striding on Batch dimension is not in the kernel,','line_number':669,'multiline':False]['text':' although it could be easily implemented given current kernel.','line_number':670,'multiline':False]['text':' it's OK to clip grid_y & grid_z, as we block the two dimensions in the kernel;','line_number':672,'multiline':False]['text':' we are dealing with packed tensor here. max index is the same as numel.','line_number':679,'multiline':False]['text':' TODO: to really support input tensor large enought to go beyond int32,','line_number':680,'multiline':False]['text':' we will need to restrict out shared memory usage and adjust the launch','line_number':681,'multiline':False]['text':' config;','line_number':682,'multiline':False]['text':' suboptimal, but without atomic it doesn't pass the tests','line_number':700,'multiline':False]['text':'bool atomic = (isizeW%osizeW != 0) || (isizeH%osizeH != 0);','line_number':714,'multiline':False]['text':' cuda blocks & threads:','line_number':720,'multiline':False]['text':' run updateGradInput kernel, accumulate gradients atomically','line_number':727,'multiline':False]['text':' run updateGradInput kernel','line_number':735,'multiline':False]['text':' namespace','line_number':753,'multiline':False]['text':' See Note [Writing Nondeterministic Operations]','line_number':780,'multiline':False]['text':' Nondeterministic because of atomicAdd usage','line_number':781,'multiline':False]['text':' See Note [Writing Nondeterministic Operations]','line_number':795,'multiline':False]['text':' Nondeterministic because of atomicAdd usage','line_number':796,'multiline':False]['text':' namespace at::native','line_number':806,'multiline':False]