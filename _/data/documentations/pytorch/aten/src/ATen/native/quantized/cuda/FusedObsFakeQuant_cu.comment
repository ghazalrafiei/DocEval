['text':' We extend the [min, max] interval to ensure that it contains 0.','line_number':49,'multiline':False]['text':' Otherwise, we would not meet the requirement that 0 be an exactly','line_number':50,'multiline':False]['text':' representable value.','line_number':51,'multiline':False]['text':' Moving this check outside this function would result in extra Device to','line_number':56,'multiline':False]['text':' Host copy of the min and max val which would result in a perf hit.','line_number':57,'multiline':False]['text':' Note: preserve_sparsity here means symmetric quantization.','line_number':73,'multiline':False]['text':' for symmetric quantization, we force zero_point','line_number':74,'multiline':False]['text':' to be a middle value between qmin and qmax.','line_number':75,'multiline':False]['text':' If either min or max is 0, then we just use 0 as zero_point.','line_number':76,'multiline':False]['text':' Now we need to nudge the zero point to be an integer','line_number':80,'multiline':False]['text':' (our zero points are integer, and this is motivated by the','line_number':81,'multiline':False]['text':' requirement to be able to represent the real value "0" exactly as a','line_number':82,'multiline':False]['text':' quantized value, which is required in multiple places, for example in','line_number':83,'multiline':False]['text':' Im2col with zero padding).','line_number':84,'multiline':False]['text':' CUDA kernel to compute Moving Average Min/Max of the tensor.','line_number':97,'multiline':False]['text':' It uses the running_min and running_max along with averaging const, c.','line_number':98,'multiline':False]['text':' The formula used to compute the new min/max is as follows','line_number':99,'multiline':False]['text':'','line_number':100,'multiline':False]['text':' running_min = (1 - c) * running_min + c * x_min, if running_min != inf','line_number':101,'multiline':False]['text':' running_min = x_min, if running_min == inf','line_number':102,'multiline':False]['text':' Moving Average Min/Max observer for activations','line_number':157,'multiline':False]['text':' Moving Average Min/Max observer for activations','line_number':171,'multiline':False]['text':'size','line_number':179,'multiline':True]['text':' size','line_number':226,'multiline':False]['text':' preserve_sparsity','line_number':227,'multiline':False]['text':' namespace','line_number':234,'multiline':False]['text':' Calculate the size of the dimension we need to quantize over,','line_number':252,'multiline':False]['text':' For per-channel quant we default to axis 0, since it is only for','line_number':253,'multiline':False]['text':' weight quantization currently.','line_number':254,'multiline':False]['text':' namespace native','line_number':323,'multiline':False]['text':' namespace at','line_number':324,'multiline':False]