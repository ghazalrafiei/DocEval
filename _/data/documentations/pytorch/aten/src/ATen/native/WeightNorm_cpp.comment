['text':' Staying faithful to the Python for now for clarity, look for optimizations later','line_number':29,'multiline':False]['text':' (e.g., single return statement for RVO)','line_number':30,'multiline':False]['text':' I assume tensor.contiguous(), view(), norm(), etc. here will dispatch through VariableType.','line_number':33,'multiline':False]['text':' To consider: at::native::norm_except_dim is probably fine as well,','line_number':45,'multiline':False]['text':' and would avoid an additional dynamic dispatch.','line_number':46,'multiline':False]['text':' optimize?','line_number':47,'multiline':False]['text':' align with cuda behavior, keep norm in 'Float' when g is 'BFloat16'','line_number':57,'multiline':False]['text':' weight_norm does not have a derivative defined for it, so this will route back through','line_number':103,'multiline':False]['text':' VariableType.cpp, and construct a WeightNormFusedBackward object in the autograd graph.','line_number':104,'multiline':False]['text':' Double-differentiable primitive ops','line_number':107,'multiline':False]['text':' at::native::norm_except_dim would probably be fine as well.','line_number':108,'multiline':False]['text':' Differentiable backward path, an alternative to weight_norm_backward, to be used','line_number':113,'multiline':False]['text':' when backward is itself creating a graph.','line_number':114,'multiline':False]['text':' The GradMode::is_enabled() check must be performed within Functions.cpp; that's why we','line_number':115,'multiline':False]['text':' define a separate function here, instead of inlining it in weight_norm_cuda_backward.','line_number':116,'multiline':False]['text':' In Functions.cpp, the HardshrinkBackward object supplies "grad.contiguous()"','line_number':124,'multiline':False]['text':' as the first argument, so grad_w should be contiguous here.','line_number':125,'multiline':False]['text':' All these checks should succeed:','line_number':126,'multiline':False]['text':' Like weight_norm_fused_backward, weight_norm_differentiable_backward should only ever be called','line_number':135,'multiline':False]['text':' through a WeightNormFusedBackward object, so we expect that dim == 0 || dim == saved_v.size(-1)','line_number':136,'multiline':False]['text':' saved_g and saved_norms are already shaped to broadcast over the correct dimensions','line_number':139,'multiline':False]['text':' ...but saved_norms might be Float when saved_g and saved_v are half.','line_number':141,'multiline':False]['text':' To consider:  saved_norms.to(..., True /*non_blocking*/);','line_number':142,'multiline':False]['text':' Analytic backward path using differentiable primitive ops','line_number':147,'multiline':False]['text':' dim == last_dim','line_number':154,'multiline':False]['text':' namespace native','line_number':163,'multiline':False]['text':' namespace at','line_number':164,'multiline':False]