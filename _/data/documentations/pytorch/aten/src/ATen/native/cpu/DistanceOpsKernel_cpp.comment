['text':' Depending on the value of the pnorm, there are specific implementations','line_number':20,'multiline':False]['text':' that are much faster than std::pow(std::abs(a - b), p), but have the same','line_number':21,'multiline':False]['text':' standard loop code for how to process the input vector. To reuse the main','line_number':22,'multiline':False]['text':' outside loop while still guaranteeing that the compiler inlines every','line_number':23,'multiline':False]['text':' different function on p, we break the inner norm logic into structs with','line_number':24,'multiline':False]['text':' static functions that represent what's done differently, and template the','line_number':25,'multiline':False]['text':' outer loop on those structs.','line_number':26,'multiline':False]['text':'','line_number':27,'multiline':False]['text':' The four functions are:','line_number':28,'multiline':False]['text':'     map :      This tells how to modify (a - b) to form the component that','line_number':29,'multiline':False]['text':'                gets summed.','line_number':30,'multiline':False]['text':'     red :      This tells how to sum the result of map up. This is','line_number':31,'multiline':False]['text':'                separate because the inf norm actually uses max instead of','line_number':32,'multiline':False]['text':'                sum.','line_number':33,'multiline':False]['text':'     finish :   This tells what to do with the aggregated value to compute','line_number':34,'multiline':False]['text':'                the norm. Generally this is the result of val ^ (1 / p).','line_number':35,'multiline':False]['text':'     backward : This is the gradient for that norm. Arguments are pretty','line_number':36,'multiline':False]['text':'                self explanitory.','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' There are a few cases where these aren't used. The 0 norm has no backward,','line_number':39,'multiline':False]['text':' because it's always 0, so that's shortcircuited earlier. There's a special','line_number':40,'multiline':False]['text':' implementation of the general backward pass when p is less than two, so','line_number':41,'multiline':False]['text':' there's a struct with only a backward pass for this case.','line_number':42,'multiline':False]['text':' TODO This is an inefficient way to compite sign, and can be much faster','line_number':44,'multiline':False]['text':' using native SSE instructions that should be added to Vectorized.','line_number':45,'multiline':False]['text':' Zero norm','line_number':91,'multiline':False]['text':'p','line_number':96,'multiline':True]['text':' One norm','line_number':99,'multiline':False]['text':'p','line_number':104,'multiline':True]['text':'dist','line_number':105,'multiline':True]['text':'p','line_number':105,'multiline':True]['text':' Special general pnorm derivative if p is less than two','line_number':108,'multiline':False]['text':' Two norm','line_number':117,'multiline':False]['text':' TODO This can probably use fused add multiply to get better perf','line_number':120,'multiline':False]['text':' General p norm','line_number':127,'multiline':False]['text':' Inf norm','line_number':136,'multiline':False]['text':' TODO This backward pass uses a very complext expression to compute (diff','line_number':142,'multiline':False]['text':' == dist) that could be much faster if using SSE instructions.','line_number':143,'multiline':False]['text':' n * (n - 1) / 2','line_number':155,'multiline':False]['text':' We conceptually iterate over tuples of (i, j, k) where i is the first','line_number':157,'multiline':False]['text':' vector from the input, j is the second, and k is the result index. This','line_number':158,'multiline':False]['text':' parallelizes over the range of k and infers what i and j are from the','line_number':159,'multiline':False]['text':' value of k.','line_number':160,'multiline':False]['text':' The -1 accounts for floating point truncation issues','line_number':164,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':165,'multiline':False]['text':' Assumes self is nonempty, contiguous, and 2D','line_number':189,'multiline':False]['text':' This does a backward pass down a Vec column of the input','line_number':268,'multiline':False]['text':' The only way to parallelize and avoid locking requires parallelizing','line_number':304,'multiline':False]['text':' over the columns of the input, i.e. we compute the gradient for the','line_number':305,'multiline':False]['text':' first section of each vector independentaly of the second section, etc.','line_number':306,'multiline':False]['text':' Assumes self is nonempty, contiguous, and 2D and dist is also contiguous','line_number':323,'multiline':False]['text':'current implementation supports only tensor that can be collapsed to 1D. However, to avoid checking if grad satisfies this assumption,','line_number':365,'multiline':False]['text':'we call .contiguous() on grad before backward, thus stride is guaranteed to be 1','line_number':366,'multiline':False]['text':'don't use grad.stride(-1), because if last dimension is 1, stride can be bogus.','line_number':367,'multiline':False]['text':' anonymous namespace','line_number':444,'multiline':False]['text':' namespace at::native','line_number':451,'multiline':False]