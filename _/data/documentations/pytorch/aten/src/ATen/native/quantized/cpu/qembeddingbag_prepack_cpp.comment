['text':'
 * Prepack function for embedding_bag weights.
 * This function expects a per-row quantized weight tensor
 * with a floating point scale and zero_point value.
 * zero point is set to be (-Xmin/scale)
 * To prepack the weights we store the scale and bias (where bias is Xmin)
 * for each row along with the quantized weights.
 ','line_number':28,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':50,'multiline':False]['text':' extra 8 bytes to store FP scale and bias per row.','line_number':56,'multiline':False]['text':' extra 4 bytes to store at::Half scale and bias per row.','line_number':60,'multiline':False]['text':' extra bytes to store scale and bias per row.','line_number':89,'multiline':False]['text':' Allocate output packed weights.','line_number':92,'multiline':False]['text':' don't use float* to avoid unaligned address access','line_number':106,'multiline':False]['text':' Re-calculate the number of embedding_cols, to account for values packed','line_number':119,'multiline':False]['text':' in a byte.','line_number':120,'multiline':False]['text':' don't use at::Half* to avoid unaligned address access','line_number':131,'multiline':False]['text':' The weight values have already been packed, so here we just','line_number':139,'multiline':False]['text':' store it in the output tensor.','line_number':140,'multiline':False]['text':' Note - This is a temporary pack function for embedding bag which quantizes','line_number':161,'multiline':False]['text':' and packs the float weight tensor. In the next step it will be replaced by a','line_number':162,'multiline':False]['text':' quantize and pack function once we support FP scale and FP zero_point','line_number':163,'multiline':False]['text':'','line_number':164,'multiline':False]['text':' Python example examining a packed 8bit zero_point and scale:','line_number':165,'multiline':False]['text':'','line_number':166,'multiline':False]['text':' >> x = torch.from_numpy(np.array([[[10, 20], [30, 40]],[[50, 60], [70, 80]]],','line_number':167,'multiline':False]['text':' dtype=np.float32))','line_number':168,'multiline':False]['text':' >> x_packed = torch.ops.quantized.embedding_bag_byte_prepack(x)','line_number':169,'multiline':False]['text':'','line_number':170,'multiline':False]['text':' # Pull out and examine packed scales, zero_points and values','line_number':171,'multiline':False]['text':' >> zero_points = x_packed[:,:,-4:].numpy()','line_number':172,'multiline':False]['text':' >> scales = x_packed[:,:,-8:-4].numpy()','line_number':173,'multiline':False]['text':' >> values = x_packed[:,:,:-8].numpy()','line_number':174,'multiline':False]['text':'','line_number':175,'multiline':False]['text':' >> zero_points','line_number':176,'multiline':False]['text':' array([[[  0,   0,  32,  65],','line_number':177,'multiline':False]['text':'        [  0,   0, 240,  65]],','line_number':178,'multiline':False]['text':'','line_number':179,'multiline':False]['text':'       [[  0,   0,  72,  66],','line_number':180,'multiline':False]['text':'        [  0,   0, 140,  66]]], dtype=uint8)','line_number':181,'multiline':False]['text':'','line_number':182,'multiline':False]['text':' >> scales','line_number':183,'multiline':False]['text':' array([[[161, 160,  32,  61],','line_number':184,'multiline':False]['text':'        [161, 160,  32,  61]],','line_number':185,'multiline':False]['text':'','line_number':186,'multiline':False]['text':'       [[161, 160,  32,  61],','line_number':187,'multiline':False]['text':'        [161, 160,  32,  61]]], dtype=uint8)','line_number':188,'multiline':False]['text':' >> values','line_number':189,'multiline':False]['text':' array([[[  0, 255],','line_number':190,'multiline':False]['text':'        [  0, 255]],','line_number':191,'multiline':False]['text':'','line_number':192,'multiline':False]['text':'       [[  0, 255],','line_number':193,'multiline':False]['text':'        [  0, 255]]], dtype=uint8)','line_number':194,'multiline':False]['text':'','line_number':195,'multiline':False]['text':' # Convert 4 byte packed scales and zero_points to float','line_number':196,'multiline':False]['text':' # and apply against values in order to recover unquantized values.','line_number':197,'multiline':False]['text':' def bytes2float(arr):','line_number':198,'multiline':False]['text':'    packed_hex = bytearray(arr)','line_number':199,'multiline':False]['text':'    return struct.unpack('f', packed_hex)','line_number':200,'multiline':False]['text':'','line_number':201,'multiline':False]['text':' >> float_zero_points = np.apply_along_axis(bytes2float, 2, zero_points)','line_number':202,'multiline':False]['text':' >> float_zero_points','line_number':203,'multiline':False]['text':' array([[[10.],','line_number':204,'multiline':False]['text':'         [30.]],','line_number':205,'multiline':False]['text':'','line_number':206,'multiline':False]['text':'        [[50.],','line_number':207,'multiline':False]['text':'         [70.]]])','line_number':208,'multiline':False]['text':' >> float_scales = np.apply_along_axis(bytes2float, 2, scales)','line_number':209,'multiline':False]['text':' >> float_scales','line_number':210,'multiline':False]['text':' array([[[0.03921569],','line_number':211,'multiline':False]['text':'        [0.03921569]],','line_number':212,'multiline':False]['text':'','line_number':213,'multiline':False]['text':'       [[0.03921569],','line_number':214,'multiline':False]['text':'        [0.03921569]]])','line_number':215,'multiline':False]['text':' >> values *  float_scales + float_zero_points','line_number':216,'multiline':False]['text':' array([[[10.        , 20.00000035],','line_number':217,'multiline':False]['text':'         [30.        , 40.00000035]],','line_number':218,'multiline':False]['text':'','line_number':219,'multiline':False]['text':'        [[50.        , 60.00000035],','line_number':220,'multiline':False]['text':'         [70.        , 80.00000035]]])','line_number':221,'multiline':False]['text':' The "last" dimension of an N-Dimensioned batch of embedding bags is','line_number':223,'multiline':False]['text':' quantization channel. E.g. for a 2D embedding bag, this has','line_number':224,'multiline':False]['text':' [ row, col ] dimensions, for batched of embedding bags, dimensions might be','line_number':225,'multiline':False]['text':' [ batch, row, col ].','line_number':226,'multiline':False]['text':'','line_number':227,'multiline':False]['text':' Python Batched Embedding Example:','line_number':228,'multiline':False]['text':' weights = torch.from_numpy((np.random.random_sample((','line_number':229,'multiline':False]['text':'          2, 10, 3)).squeeze() + 1).astype(np.float32))','line_number':230,'multiline':False]['text':' assert(weights.size() == torch.Size([2, 10, 3]))','line_number':231,'multiline':False]['text':' # NOTE: 8 bytes (columns) are added due to fp32 zero_point and scales','line_number':232,'multiline':False]['text':' packed_weights = torch.ops.quantized.embedding_bag_byte_prepack(weights)','line_number':233,'multiline':False]['text':' assert(packed_weights.size() == torch.Size([2, 10, 11]))','line_number':234,'multiline':False]['text':' Add 8 bytes per column to store FP32 scale and zero_point per row.','line_number':245,'multiline':False]['text':' Adjust output dimensions to account for FP32 scale and zero_points.','line_number':250,'multiline':False]['text':' embedding_cols','line_number':306,'multiline':False]['text':' embedding_rows','line_number':307,'multiline':False]['text':' USE_FBGEMM','line_number':308,'multiline':False]['text':' Add 8 bytes per column to store FP32 scale and zero_point per row.','line_number':337,'multiline':False]['text':' Adjust output dimensions to account for FP32 scale and zero_points.','line_number':340,'multiline':False]['text':' TODO: Extend support to N-D batched embeddings, similar to','line_number':350,'multiline':False]['text':' qembeddingbag_byte_prepack','line_number':351,'multiline':False]['text':' The "fused" representation stores the scale and bias with the','line_number':380,'multiline':False]['text':' row-wise quantized data in one tensor.','line_number':381,'multiline':False]['text':' Since we represent the scale and bias in 16-bit float, we'll use the','line_number':382,'multiline':False]['text':' last 4 bytes of each row for scale (2 bytes) and bias (2 bytes).','line_number':383,'multiline':False]['text':' | ... quantized data ... | scale | bias |','line_number':384,'multiline':False]['text':' |    number_of_columns   |  2B   |  2B  |','line_number':385,'multiline':False]['text':' USE_FBGEMM','line_number':425,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':436,'multiline':False]['text':' Set scale to 1.0f for the corner case of Xmax == Xmin .','line_number':453,'multiline':False]['text':' Any non-zero scale would work because during quantization','line_number':454,'multiline':False]['text':' (X - Xmin) / scale will be 0 for all X unless scale is 0.','line_number':455,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':456,'multiline':False]['text':' Corner case handling when Xmax == Xmin','line_number':460,'multiline':False]['text':' Any scale would work because X - Xmin will be 0 for all X','line_number':461,'multiline':False]['text':' Update the scale and zero_point of each row.','line_number':465,'multiline':False]['text':' Pack the weight values.','line_number':473,'multiline':False]['text':' We pack 2 4-bit values in a byte. Index 0 is packed in the lower','line_number':480,'multiline':False]['text':' 4-bits and index 1 is packed in the upper 4-bits.','line_number':481,'multiline':False]['text':' embedding_cols','line_number':488,'multiline':False]['text':' embedding_rows','line_number':489,'multiline':False]['text':' USE_FBGEMM','line_number':492,'multiline':False]['text':' Applies 4-bit row-wise quantization by determining the range','line_number':497,'multiline':False]['text':' (maximum - minimum) and bias (minimum value) of each row in the input','line_number':498,'multiline':False]['text':' matrix, and then scaling each element to an 2-bit number between 0 and','line_number':499,'multiline':False]['text':' 15.','line_number':500,'multiline':False]['text':' To later de-quantize values, the scale (range / 15) and zero_point','line_number':501,'multiline':False]['text':' are stored alongside the data. More precisely, each row first has quantized','line_number':502,'multiline':False]['text':' values, and then 2-byte fp16 scale and 2-byte zero_offset.','line_number':503,'multiline':False]['text':'bit_width','line_number':510,'multiline':True]['text':' Applies 2-bit row-wise quantization by determining the range','line_number':513,'multiline':False]['text':' (maximum - minimum) and bias (minimum value) of each row in the input','line_number':514,'multiline':False]['text':' matrix, and then scaling each element to an 2-bit number between 0 and','line_number':515,'multiline':False]['text':' 3.','line_number':516,'multiline':False]['text':' To later de-quantize values, the scale (range / 3) and zero_point','line_number':517,'multiline':False]['text':' are stored alongside the data. More precisely, each row first has quantized','line_number':518,'multiline':False]['text':' values, and then 2-byte fp16 scale and 2-byte zero_offset.','line_number':519,'multiline':False]['text':' TODO() - Add 2Bit Embedding Lookup operator.','line_number':520,'multiline':False]['text':'bit_width','line_number':527,'multiline':True]['text':' namespace','line_number':562,'multiline':False]['text':' namespace native','line_number':563,'multiline':False]['text':' namespace at','line_number':564,'multiline':False]