['text':' [Note AVX-SSE transitions] In general we avoid calls into cmath for code','line_number':20,'multiline':False]['text':' compiled with AVX/AVX2 This is because of SSE-AVX transitions and a bug in','line_number':21,'multiline':False]['text':' Glibc2.23 See https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1663280','line_number':22,'multiline':False]['text':'','line_number':23,'multiline':False]['text':' On grainsize: The grainsize is chosen to roughly get GRAIN_SIZE number of','line_number':24,'multiline':False]['text':' computations per task. Each task works across dim_size elements. 16 should be','line_number':25,'multiline':False]['text':' a very rough approximation of the number of computations per dim_size element','line_number':26,'multiline':False]['text':' by counting simple computations (*, +, -) as 1 and exp or log as 4.','line_number':27,'multiline':False]['text':'','line_number':28,'multiline':False]['text':' We use a chunk size such that it'd fit in L1D.','line_number':29,'multiline':False]['text':' Coincidentally, at::internal::GRAIN_SIZE is 32768, which is equal to the','line_number':41,'multiline':False]['text':' size of L1D cache on many processors. Some processors have 48 KB L1D cache','line_number':42,'multiline':False]['text':' nowadays, so maybe in the future, we can leverage the knowledge of a','line_number':43,'multiline':False]['text':' machine's L1D cache size.','line_number':44,'multiline':False]['text':' MSVC requires such a declaration of dynamic arrays','line_number':52,'multiline':False]['text':' Source: https://stackoverflow.com/a/33423538','line_number':53,'multiline':False]['text':' See [Note AVX-SSE transitions] for why this should call the','line_number':78,'multiline':False]['text':' vectorized version (aside from perf improvements).','line_number':79,'multiline':False]['text':' It's necessary to keep the order of the operations below.','line_number':92,'multiline':False]['text':' In some cases that input is large digits and the difference','line_number':93,'multiline':False]['text':' is small, if we compute `max_input` plus `tmp_sum` before,','line_number':94,'multiline':False]['text':' there would be a numerical problem. See an example in','line_number':95,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/11752#issuecomment-422883379','line_number':96,'multiline':False]['text':' thread local temp buffer.','line_number':154,'multiline':False]['text':' reduce to max and cache float input data','line_number':161,'multiline':False]['text':' map (x - max).exp() and reduce to sum','line_number':179,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':230,'multiline':False]['text':' thread local temp buffer that holds vertical sum result','line_number':281,'multiline':False]['text':' init','line_number':291,'multiline':False]['text':' compute sum of grad_output * output','line_number':301,'multiline':False]['text':' compute output * (grad_output - sum)','line_number':321,'multiline':False]['text':' thread local temp buffer that holds vertical sum result','line_number':365,'multiline':False]['text':' thread local buffer that holds grad_output and output data in float32','line_number':369,'multiline':False]['text':' init','line_number':382,'multiline':False]['text':' compute sum of grad_output * output','line_number':393,'multiline':False]['text':' cache the 'converted' float grad_output and output','line_number':419,'multiline':False]['text':' compute output * (grad_output - sum)','line_number':435,'multiline':False]['text':' thread local temp buffer that holds vertical sum result','line_number':490,'multiline':False]['text':' init','line_number':500,'multiline':False]['text':' compute sum of grad_output','line_number':510,'multiline':False]['text':' compute grad_output - output.exp() * sum','line_number':528,'multiline':False]['text':' thread local temp buffer that holds vertical sum result','line_number':573,'multiline':False]['text':' thread local buffer that holds grad_output data in float32','line_number':577,'multiline':False]['text':' init','line_number':587,'multiline':False]['text':' compute sum of grad_output','line_number':598,'multiline':False]['text':' cache the 'converted' float grad_output','line_number':618,'multiline':False]['text':' compute grad_output - output.exp() * sum','line_number':630,'multiline':False]['text':' Currently, we only support BFloat16/Half in this special implementation','line_number':698,'multiline':False]['text':' Vectorization','line_number':710,'multiline':False]['text':' Step 1: Get max Score','line_number':715,'multiline':False]['text':' Step2: Calculate sum','line_number':730,'multiline':False]['text':' Step3: Unify','line_number':744,'multiline':False]['text':' Tail case(Scalar): it is exactly same logic as host_softmax','line_number':755,'multiline':False]['text':' inside aten/src/ATen/native/SoftMax.cpp. There are 2 kind of','line_number':756,'multiline':False]['text':' cases which will fall through this part:','line_number':757,'multiline':False]['text':' Case 1: For the idx at the end of total chunk for each thread, there are not enough numbers for parallization.','line_number':758,'multiline':False]['text':' Case 2: For the idx at the end of each inner_size inside thread, there are not enough numbers for parallization.','line_number':759,'multiline':False]['text':'Case1','line_number':760,'multiline':True]['text':'Case2','line_number':760,'multiline':True]['text':' Step1: Get max score','line_number':768,'multiline':False]['text':' Step2: Calculate the Sum','line_number':773,'multiline':False]['text':' Step3: Unify','line_number':781,'multiline':False]['text':' Vectorization','line_number':813,'multiline':False]['text':' Step 1: Get max Score','line_number':818,'multiline':False]['text':' Step2: Calculate sum','line_number':824,'multiline':False]['text':' Step3: Unify','line_number':832,'multiline':False]['text':' Tail case(Scalar): it is exactly same logic as host_softmax','line_number':840,'multiline':False]['text':' inside aten/src/ATen/native/SoftMax.cpp. There are 2 kind of','line_number':841,'multiline':False]['text':' cases which will fall through this part:','line_number':842,'multiline':False]['text':' Case 1: For the idx at the end of total chunk for each thread, there are not enough numbers for parallization.','line_number':843,'multiline':False]['text':' Case 2: For the idx at the end of each inner_size inside thread, there are not enough numbers for parallization.','line_number':844,'multiline':False]['text':'Case1','line_number':845,'multiline':True]['text':'Case2','line_number':845,'multiline':True]['text':' Step1: Get max score','line_number':853,'multiline':False]['text':' Step2: Calculate the Sum','line_number':858,'multiline':False]['text':' Step3: Unify','line_number':865,'multiline':False]['text':' NB: fast kernel for log_softmax when dim != -1','line_number':877,'multiline':False]['text':' input shape is normalized to {outer_size, dim_size, inner_size}','line_number':878,'multiline':False]['text':'','line_number':879,'multiline':False]['text':' The algorithm requires to load input tensor 3 times, to increase parallelsim','line_number':880,'multiline':False]['text':' and cache hit rate, inner_size is blocked as:','line_number':881,'multiline':False]['text':'   inner_size: {CHUNK_SIZE, CHUNK_SIZE, ..., Remainder}','line_number':882,'multiline':False]['text':'','line_number':883,'multiline':False]['text':' Parallel on {outer_size, num_chunks} and do vertical reduction on each block of','line_number':884,'multiline':False]['text':' {dim_size, CHUNK_SIZE}, block size (128KB) selected to be L2 hit.','line_number':885,'multiline':False]['text':'','line_number':886,'multiline':False]['text':' thread local temp buffer which holds vertical reduction result: max and sum.','line_number':903,'multiline':False]['text':' init','line_number':914,'multiline':False]['text':' compute max','line_number':927,'multiline':False]['text':' compute sum of (x - max).exp()','line_number':946,'multiline':False]['text':' apply log','line_number':966,'multiline':False]['text':' compute x - max - sum','line_number':969,'multiline':False]['text':' thread local buffer that holds input data in float32 to save next 2 dtype conversion','line_number':1012,'multiline':False]['text':' init','line_number':1016,'multiline':False]['text':' compute max','line_number':1037,'multiline':False]['text':' cache the 'converted' float input','line_number':1054,'multiline':False]['text':' compute sum of (x - max).exp()','line_number':1066,'multiline':False]['text':' apply log','line_number':1090,'multiline':False]['text':' compute x - max - sum','line_number':1093,'multiline':False]['text':' anonymous namespace','line_number':1286,'multiline':False]['text':' namespace at::native','line_number':1303,'multiline':False]