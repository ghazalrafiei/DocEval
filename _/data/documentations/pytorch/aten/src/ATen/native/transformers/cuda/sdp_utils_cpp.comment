['text':'*
* Note [SDPA Runtime Dispatch]
* SDPA relies on a runtime dispatch mechanism to select the appropriate
* kernel. This file contains exposes this through the `select_sdp_backend`
* The basic structure of this function is to call `priority_order` to get a
* list of backends to try, and then iterate through them until one succeeds.
* Each backend defines a use_<backend> function that returns true if the
* backend can be run with the given SDP parameters. The use_<backend> function
* will iterate over a list of "filters" that check for specific properties of
* the SDP parameters. If all filters pass, the backend can be used and use_<backend>
* returns true. If any filter fails, then use_<backend> returns false.
*
* In order to aid in debugging, each filter takes sdp_params and a debug flag.
* If the debug flag is set, the filter will print a warning message if it fails.
* The behavior of select_sdp_backend is to return the first backend that
* succeeds. If no backend is viable then it will run each use_<backend> function
* with debug=true and return SDPBackend::error.
','line_number':24,'multiline':True]['text':' flash_attention V2 is universally faster than efficient_attention and Math','line_number':45,'multiline':False]['text':' All head_dim sizes must be equal and less than 256','line_number':80,'multiline':False]['text':'*
 * Checks if the current CUDA device architecture is inclusively within the specified range.
 *
 * @param lower_bound The lower bound of the CUDA device architecture range.
 * @param upper_bound The upper bound of the CUDA device architecture range.
 * @param params The parameters for the current operation.
 * @return True if the current CUDA device architecture is within the specified range, false otherwise.
 ','line_number':161,'multiline':True]['text':' Check that the gpu is capable of running flash attention','line_number':195,'multiline':False]['text':' Mem Efficient attention supports hardware in the range [sm_50, sm_90]','line_number':232,'multiline':False]['text':' Flash Attention will raise an error in the backward pass if the head_dim','line_number':253,'multiline':False]['text':' size is greater than 192 And the device is between in the range [sm86, sm89]','line_number':254,'multiline':False]['text':' FlashAttention 2 updated the default mask meaning for causal in this PR:','line_number':275,'multiline':False]['text':' 9e5e8bc91e it is now aligned to lower_right which would be a BC break','line_number':276,'multiline':False]['text':' for non-square masks. We will not support non-square masks for causal w/ FAV2','line_number':277,'multiline':False]['text':' Check that all tensors are on the GPU device','line_number':293,'multiline':False]['text':' This should be handled by the stub dispatch, but whe call can_use_*_attention','line_number':294,'multiline':False]['text':' directly from python we need to ensure that the tensors are on cuda','line_number':295,'multiline':False]['text':' namespace','line_number':311,'multiline':False]['text':' Define gate functions that determine if a flash kernel can be ran','line_number':318,'multiline':False]['text':' Replace with std::to_array when we migrate to c++20','line_number':319,'multiline':False]['text':' Constraints specific to mem efficient attention','line_number':374,'multiline':False]['text':'  Define gate functions that determine if a mem efficient kernel can be ran','line_number':380,'multiline':False]['text':' This function defines the priority order of the different sdp backends','line_number':424,'multiline':False]['text':' 1. Flash Attention','line_number':425,'multiline':False]['text':' 2. Mem Efficient Attention','line_number':426,'multiline':False]['text':' 3. Math fallback','line_number':427,'multiline':False]['text':' Get ideal kernel ordering','line_number':433,'multiline':False]['text':' Because TORCHCHECK checks if condition is true we negate debug so that','line_number':436,'multiline':False]['text':' The statements will be printed when debug is true','line_number':437,'multiline':False]['text':' If we have gotten to this point then two things have happened:','line_number':460,'multiline':False]['text':' 1. use_flash_attention or use_mem_efficient did not satisfy the','line_number':461,'multiline':False]['text':' constraints to be ran','line_number':462,'multiline':False]['text':' 2. The user has explicitly disabled the math kernel','line_number':463,'multiline':False]['text':' We then re-run the kernel checks with debug enabled to print out the','line_number':464,'multiline':False]['text':' reason why the kernel was not selected','line_number':465,'multiline':False]['text':' When this function is called we are assured that the nt is dim==4','line_number':477,'multiline':False]['text':' This is being called inside sdp with shape [batch, heads, {seq_len}, dim]','line_number':489,'multiline':False]['text':' namespace sdp','line_number':503,'multiline':False]