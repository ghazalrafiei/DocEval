['text':'/ Contains the implementation of parallel reductions in TensorIterator.','line_number':14,'multiline':False]['text':' Fill with the identity','line_number':49,'multiline':False]['text':' Bump output ptr so each thread has its own ouput slice','line_number':62,'multiline':False]['text':'/ Chooses a dimension over which to parallelize. Prefers the outer-most','line_number':74,'multiline':False]['text':'/ dimension thats larger than the number of available threads.','line_number':75,'multiline':False]['text':' start with the outer-most dimension','line_number':80,'multiline':False]['text':' only round the 'end' column down if it's not the final column','line_number':98,'multiline':False]['text':'arg=','line_number':108,'multiline':True]['text':' round columns to multiples of 128 bytes if adjacent columns are','line_number':113,'multiline':False]['text':' contiguous in memory.','line_number':114,'multiline':False]['text':' On some broken setups, `#ifdef _OPENMP` is true,','line_number':167,'multiline':False]['text':' and `get_num_threads` returns > 1, but','line_number':168,'multiline':False]['text':' `#pragma omp parallel` is ignored.','line_number':169,'multiline':False]['text':' There is no API to check for this, so we need to explicitly','line_number':170,'multiline':False]['text':' stop trying to parallelize if we've already gotten here.','line_number':171,'multiline':False]['text':'','line_number':172,'multiline':False]['text':' (If we are on one of those broken setups, we will','line_number':173,'multiline':False]['text':'  only have one thread here, and end - begin == cols.)','line_number':174,'multiline':False]['text':' namespace at','line_number':180,'multiline':False]