['text':'**************************************************************************************************
 * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
 *reserved. SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *POSSIBILITY OF SUCH DAMAGE.
 *
 *************************************************************************************************','line_number':1,'multiline':True]['text':'! \file
    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
','line_number':32,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':47,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':53,'multiline':False]['text':'/ Structure to compute the matrix product targeting CUDA cores and SIMT math','line_number':55,'multiline':False]['text':'/ instructions.','line_number':56,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':58,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':60,'multiline':False]['text':'/ Number of stages,','line_number':62,'multiline':False]['text':'/ Used for partial specialization','line_number':64,'multiline':False]['text':'/< Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':68,'multiline':False]['text':'/< Policy describing tuning details','line_number':71,'multiline':False]['text':'','line_number':74,'multiline':False]['text':' Dependent types','line_number':75,'multiline':False]['text':'','line_number':76,'multiline':False]['text':'/ Warp-level Mma','line_number':78,'multiline':False]['text':'/ Shape describing the overall GEMM computed from shared memory','line_number':81,'multiline':False]['text':'/ by each warp.','line_number':82,'multiline':False]['text':'/ Shape describing the number of warps filling the CTA','line_number':85,'multiline':False]['text':'/ Number of warp-level GEMM oeprations','line_number':91,'multiline':False]['text':'/ Number of stages','line_number':95,'multiline':False]['text':'','line_number':98,'multiline':False]['text':' Nested structs','line_number':99,'multiline':False]['text':'','line_number':100,'multiline':False]['text':'/ Shared storage object needed by threadblock-scoped GEMM','line_number':102,'multiline':False]['text':'/ Returns a TensorRef to the operand','line_number':113,'multiline':False]['text':'/ Shape of the A matrix operand in shared memory','line_number':120,'multiline':False]['text':'/ Shape of the B matrix operand in shared memory','line_number':125,'multiline':False]['text':'/ Buffer for A operand','line_number':142,'multiline':False]['text':'/ Buffer for B operand','line_number':145,'multiline':False]['text':'','line_number':150,'multiline':False]['text':' Data members','line_number':151,'multiline':False]['text':'','line_number':152,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of A operand from shared memory','line_number':154,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of B operand from shared memory','line_number':157,'multiline':False]['text':'/ Construct from tensor references','line_number':161,'multiline':False]['text':'/< Shared storage needed for internal use by threadblock-scoped GEMM','line_number':164,'multiline':False]['text':'/< ID within the threadblock','line_number':167,'multiline':False]['text':'/< ID of warp','line_number':169,'multiline':False]['text':'/< ID of each thread within a warp','line_number':171,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':177,'multiline':False]['text':' namespace threadblock','line_number':179,'multiline':False]['text':' namespace gemm','line_number':180,'multiline':False]['text':' namespace cutlass','line_number':181,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':183,'multiline':False]