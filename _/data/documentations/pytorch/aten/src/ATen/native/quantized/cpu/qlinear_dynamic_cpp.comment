['text':' fp32 * int8 -> fp32 (with quantization on activation, and dequantization','line_number':35,'multiline':False]['text':' on the result).','line_number':36,'multiline':False]['text':' We make a strong guarantee that models using these operators will have','line_number':38,'multiline':False]['text':' the same numerics across different machines. Therefore, we do not provide','line_number':39,'multiline':False]['text':' a fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':40,'multiline':False]['text':' TODO: contiguous is called for further jit optimizations.','line_number':44,'multiline':False]['text':' C(output) = A(input) x B(weight), where C, A, B are M x N, M x K, K x N','line_number':51,'multiline':False]['text':' matrices, respectively.','line_number':52,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':53,'multiline':False]['text':' Calculate statistics for quantization of the input Tensor','line_number':65,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':66,'multiline':False]['text':'m=','line_number':69,'multiline':True]['text':'min=','line_number':70,'multiline':True]['text':'max=','line_number':71,'multiline':True]['text':'len=','line_number':72,'multiline':True]['text':' Input tensor is quantized as 8-bit unsigned values','line_number':74,'multiline':False]['text':' Calculate scale and zero point for quantization of input tensor','line_number':78,'multiline':False]['text':'min=','line_number':80,'multiline':True]['text':'max=','line_number':81,'multiline':True]['text':'qmin=','line_number':82,'multiline':True]['text':'qmax=','line_number':83,'multiline':True]['text':'preserve_sparsity=','line_number':85,'multiline':True]['text':'force_scale_power_of_two=','line_number':86,'multiline':True]['text':'reduce_range=','line_number':87,'multiline':True]['text':' ReQuantizeForFloat requires pointers to the zero point values,','line_number':91,'multiline':False]['text':' since in the case of rowwise quantization these will be arrays rather','line_number':92,'multiline':False]['text':' than scalars. But in this case, we're doing whole-tensor quantization so','line_number':93,'multiline':False]['text':' we just pass a pointer to the scale values (and internally','line_number':94,'multiline':False]['text':' ReQuantizeForFloat won't index past 0.','line_number':95,'multiline':False]['text':' TODO: contiguous is called for further jit optimizations.','line_number':105,'multiline':False]['text':' The resulting matrix here is 2-D, let's view it with the original','line_number':109,'multiline':False]['text':' left hand dimensions of the input. Here are two examples:','line_number':110,'multiline':False]['text':' 1. If the input tensor is {M, K}, the output tensor is {M, N}.','line_number':111,'multiline':False]['text':' 2. If the input tensor is {b, M, K}, the output tensor is {b, M, N}.','line_number':112,'multiline':False]['text':' Allocate output Tensor and a buffer for fbgemmPacked to use','line_number':115,'multiline':False]['text':' This operation does the following:','line_number':124,'multiline':False]['text':' 1) Quantizes the input matrix given the statistics we've calculated','line_number':125,'multiline':False]['text':' above','line_number':126,'multiline':False]['text':' 2) Creates a "row buffer" vector with offset values that must be','line_number':127,'multiline':False]['text':' added','line_number':128,'multiline':False]['text':'    to the integer matrix multiplication operation to ensure','line_number':129,'multiline':False]['text':'    correctness. This "row buffer" is also called the row offset, and it','line_number':130,'multiline':False]['text':'    is needed when we use affine quantization for weights.','line_number':131,'multiline':False]['text':' 3) Packs the resulting quantized matrix into vector-register and cache','line_number':132,'multiline':False]['text':'    friendly tiles.','line_number':133,'multiline':False]['text':'','line_number':134,'multiline':False]['text':'  Note this is not executed eagerly, but rather within the fbgemmPacked','line_number':135,'multiline':False]['text':'  call below.','line_number':136,'multiline':False]['text':'trans=','line_number':139,'multiline':True]['text':'nRow=','line_number':140,'multiline':True]['text':'nCol=','line_number':141,'multiline':True]['text':'smat=','line_number':142,'multiline':True]['text':'ld=','line_number':143,'multiline':True]['text':'pmat=','line_number':144,'multiline':True]['text':' Currently, packA manages ownership of `pmat`.','line_number':144,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':145,'multiline':False]['text':'scale=','line_number':146,'multiline':True]['text':'zero_pt=','line_number':147,'multiline':True]['text':' TODO: Consider a way to pre-allocate and reuse','line_number':148,'multiline':False]['text':' pmat buffer.','line_number':149,'multiline':False]['text':' This is the end of the pipeline, pass the resulting matrix through.','line_number':151,'multiline':False]['text':' Process the per tensor quantization.','line_number':156,'multiline':False]['text':'','line_number':157,'multiline':False]['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':158,'multiline':False]['text':' operation does:','line_number':159,'multiline':False]['text':'  1) Add in row and column offsets to the rows and columns,','line_number':160,'multiline':False]['text':'  respectively.','line_number':161,'multiline':False]['text':'  2) Dequantize the results into floating point.','line_number':162,'multiline':False]['text':'  3) Add in the bias term.','line_number':163,'multiline':False]['text':'nextop=','line_number':165,'multiline':True]['text':'Aq_scale=','line_number':166,'multiline':True]['text':'Bq_scale=','line_number':167,'multiline':True]['text':'Aq_zero_point=','line_number':168,'multiline':True]['text':'Bq_zero_point=','line_number':169,'multiline':True]['text':'row_offsets=','line_number':170,'multiline':True]['text':'col_offsets=','line_number':171,'multiline':True]['text':'bias=','line_number':172,'multiline':True]['text':'nCol=','line_number':173,'multiline':True]['text':' Do the GEMM','line_number':175,'multiline':False]['text':'packA=','line_number':177,'multiline':True]['text':'packB=','line_number':178,'multiline':True]['text':'C=','line_number':179,'multiline':True]['text':'C_buffer=','line_number':180,'multiline':True]['text':'ldc=','line_number':181,'multiline':True]['text':'outProcess=','line_number':182,'multiline':True]['text':'thread_id=','line_number':183,'multiline':True]['text':'num_threads=','line_number':184,'multiline':True]['text':' Process the per channel quantization.','line_number':187,'multiline':False]['text':'','line_number':188,'multiline':False]['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':189,'multiline':False]['text':' operation does:','line_number':190,'multiline':False]['text':'  1) Add in row and column offsets to the rows and columns,','line_number':191,'multiline':False]['text':'  respectively.','line_number':192,'multiline':False]['text':'  2) Dequantize the results into floating point.','line_number':193,'multiline':False]['text':'  3) Add in the bias term.','line_number':194,'multiline':False]['text':'nextop=','line_number':199,'multiline':True]['text':'Aq_scale=','line_number':200,'multiline':True]['text':'Bq_scale=','line_number':201,'multiline':True]['text':'Aq_zero_point=','line_number':202,'multiline':True]['text':'Bq_zero_point=','line_number':203,'multiline':True]['text':'row_offsets=','line_number':204,'multiline':True]['text':'col_offsets=','line_number':205,'multiline':True]['text':'bias=','line_number':206,'multiline':True]['text':'nCol=','line_number':207,'multiline':True]['text':' Do the GEMM','line_number':209,'multiline':False]['text':'packA=','line_number':211,'multiline':True]['text':'packB=','line_number':212,'multiline':True]['text':'C=','line_number':213,'multiline':True]['text':'C_buffer=','line_number':214,'multiline':True]['text':'ldc=','line_number':215,'multiline':True]['text':'outProcess=','line_number':216,'multiline':True]['text':'thread_id=','line_number':217,'multiline':True]['text':'num_threads=','line_number':218,'multiline':True]['text':'ReluFused=','line_number':229,'multiline':True]['text':'ReluFused=','line_number':236,'multiline':True]['text':' USE_FBGEMM','line_number':239,'multiline':False]['text':' C(output) = A(input) x B(weight), where C, A, B are M x N, M x K, K x N','line_number':255,'multiline':False]['text':' matrices, respectively.','line_number':256,'multiline':False]['text':' Weight packing is not thread safe','line_number':258,'multiline':False]['text':' Calculate statistics for quantization of input Tensor','line_number':271,'multiline':False]['text':' TODO: optimized kernel','line_number':272,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':273,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':275,'multiline':False]['text':' On empty input, no output data will be generated,','line_number':281,'multiline':False]['text':' so use arbitrary qparams.','line_number':282,'multiline':False]['text':'min=','line_number':288,'multiline':True]['text':'max=','line_number':289,'multiline':True]['text':'qmin=','line_number':290,'multiline':True]['text':'qmax=','line_number':291,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':296,'multiline':False]['text':' Get the original weight and adjust it to uint8 from int8','line_number':304,'multiline':False]['text':' TODO(kimishpatel), we are allocating affine_quantized regardless of per','line_number':307,'multiline':False]['text':' channel or not. This allocation is actually used only for packing weight','line_number':308,'multiline':False]['text':' and thus will be freed. Still we should be consistent. Fix this.','line_number':309,'multiline':False]['text':' Pass in nullptr for bias, as we pass FP32 bias to run function.','line_number':322,'multiline':False]['text':' input_channels ','line_number':325,'multiline':True]['text':' output_channels ','line_number':326,'multiline':True]['text':' On mobile, we release the original weight by resetting the','line_number':333,'multiline':False]['text':' intrusive_ptr. Calling unpack after this will throw an assertion.','line_number':334,'multiline':False]['text':' Update the input scale to not pack weights again.','line_number':339,'multiline':False]['text':' as well as to avoid repopulating requant scale if scale has not changed.','line_number':340,'multiline':False]['text':' Quantize input','line_number':343,'multiline':False]['text':' The resulting matrix here is 2-D, let's view it with the original','line_number':347,'multiline':False]['text':' left hand dimensions of the input. Here are two examples:','line_number':348,'multiline':False]['text':' 1. If the input tensor is {M, K}, the output tensor is {M, N}.','line_number':349,'multiline':False]['text':' 2. If the input tensor is {b, M, K}, the output tensor is {b, M, N}.','line_number':350,'multiline':False]['text':' batch_size ','line_number':362,'multiline':True]['text':' input_channels ','line_number':363,'multiline':True]['text':' output_channels ','line_number':364,'multiline':True]['text':' for dynamic should really be called dequant scale ','line_number':367,'multiline':True]['text':' input_stride ','line_number':370,'multiline':True]['text':' output_stride ','line_number':374,'multiline':True]['text':' threadpool ','line_number':375,'multiline':True]['text':' Call the relu operator here until qlinear dynamic in QNNPACK','line_number':381,'multiline':False]['text':' supports it natively.','line_number':382,'multiline':False]['text':'ReluFused=','line_number':392,'multiline':True]['text':'ReluFused=','line_number':398,'multiline':True]['text':' USE_PYTORCH_QNNPACK','line_number':401,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':417,'multiline':False]['text':' Resize output Tensor','line_number':423,'multiline':False]['text':' Call the fp16 gemm interface','line_number':429,'multiline':False]['text':'transa=','line_number':431,'multiline':True]['text':'m=','line_number':432,'multiline':True]['text':'A=','line_number':433,'multiline':True]['text':'Bp=','line_number':434,'multiline':True]['text':'beta=','line_number':435,'multiline':True]['text':'C=','line_number':436,'multiline':True]['text':'thread_id=','line_number':437,'multiline':True]['text':'num_threads=','line_number':438,'multiline':True]['text':' Add bias term','line_number':442,'multiline':False]['text':' reduce_range ','line_number':453,'multiline':True]['text':'ReluFused=','line_number':455,'multiline':True]['text':' reduce_range ','line_number':460,'multiline':True]['text':'ReluFused=','line_number':462,'multiline':True]['text':' reduce_range ','line_number':468,'multiline':True]['text':' reduce_range ','line_number':476,'multiline':True]['text':' USE_FBGEMM','line_number':485,'multiline':False]['text':' Dynamic: fp32 * int8 -> fp32','line_number':492,'multiline':False]['text':' Input -> uint8','line_number':501,'multiline':False]['text':' Find quantization parameters','line_number':512,'multiline':False]['text':' Use FBGEMM's FindMinMax if available since it's faster','line_number':515,'multiline':False]['text':'m=','line_number':517,'multiline':True]['text':'min=','line_number':518,'multiline':True]['text':'max=','line_number':519,'multiline':True]['text':'len=','line_number':520,'multiline':True]['text':'min=','line_number':531,'multiline':True]['text':'max=','line_number':532,'multiline':True]['text':'qmin=','line_number':533,'multiline':True]['text':'qmax=','line_number':534,'multiline':True]['text':'preserve_sparsity=','line_number':535,'multiline':True]['text':'force_scale_power_of_two=','line_number':536,'multiline':True]['text':'reduce_range=','line_number':537,'multiline':True]['text':' weights, dst','line_number':539,'multiline':False]['text':' Compute -> f32','line_number':544,'multiline':False]['text':' Use ideep::matmul_forward instead of ideep::inner_product_forward,','line_number':545,'multiline':False]['text':' since the latter does not support asymmetric quantization','line_number':546,'multiline':False]['text':' Allocate output Tensor','line_number':547,'multiline':False]['text':' Bias might be modified outside (e.g. by quantization bias correction).','line_number':555,'multiline':False]['text':' If so, update the prepacked bias as well.','line_number':556,'multiline':False]['text':' Primitive cache is initialized when called for the first time','line_number':562,'multiline':False]['text':' and won't be updated afterwards.','line_number':563,'multiline':False]['text':'accum scale','line_number':566,'multiline':True]['text':'accum zero point','line_number':566,'multiline':True]['text':'is_dynamic=','line_number':569,'multiline':True]['text':'ReluFused=','line_number':595,'multiline':True]['text':'ReluFused=','line_number':602,'multiline':True]['text':' #if AT_MKLDNN_ENABLED()','line_number':606,'multiline':False]['text':' We make a strong guarantee that models using these operators will have','line_number':634,'multiline':False]['text':' the same numerics across different machines. Therefore, we do not provide','line_number':635,'multiline':False]['text':' a fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':636,'multiline':False]['text':' Call the relu operator here until fp16 linear dynamic in FBGEMM','line_number':642,'multiline':False]['text':' supports it natively.','line_number':643,'multiline':False]['text':' USE_FBGEMM','line_number':649,'multiline':False]['text':' input ','line_number':651,'multiline':True]['text':' packed_weight ','line_number':652,'multiline':True]['text':' We make a strong guarantee that models using these operators will have','line_number':653,'multiline':False]['text':' the same numerics across different machines. Therefore, we do not provide','line_number':654,'multiline':False]['text':' a fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':655,'multiline':False]['text':' USE_FBGEMM','line_number':659,'multiline':False]['text':' namespace','line_number':685,'multiline':False]['text':' namespace native','line_number':686,'multiline':False]['text':' namespace at','line_number':687,'multiline':False]