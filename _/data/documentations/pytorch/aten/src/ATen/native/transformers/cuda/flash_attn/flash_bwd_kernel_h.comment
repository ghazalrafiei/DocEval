['text':'**************************************************************************************************
 * Copyright (c) 2023, Tri Dao.
 *****************************************************************************','line_number':1,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':27,'multiline':False]['text':' Divide by 2 because right now we always use 2 for the ValLayout','line_number':39,'multiline':False]['text':' This gives the correct layout, idk why.','line_number':42,'multiline':False]['text':' auto t = make_tile(Layout<Shape<Shape<_8, _2>, _2>,','line_number':43,'multiline':False]['text':'                           Stride<Stride<_1, _64>, _8> >{},','line_number':44,'multiline':False]['text':' auto t = make_tile(Layout<Shape<_8, _2, _2>,','line_number':45,'multiline':False]['text':'                           Stride<_1, _64, _8> >{},','line_number':46,'multiline':False]['text':' (8, 2, 2) or (8, 4, 2)','line_number':47,'multiline':False]['text':' (1, 64, 8) or (1, 32, 8)','line_number':48,'multiline':False]['text':' if (cute::thread0()) {printf("make_tiled_copy_B_warpcontiguousN "); print(t); printf("\n");  }','line_number':50,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':54,'multiline':False]['text':' Divide by 2 because right now we always use 2 for the ValLayout','line_number':66,'multiline':False]['text':' (8, 2, 2) or (8, 4, 2)','line_number':70,'multiline':False]['text':' (1, 64, 8) or (1, 32, 8)','line_number':71,'multiline':False]['text':' if (cute::thread0()) {printf("make_tiled_copy_C_warpcontiguousN "); print(t); printf("\n");  }','line_number':72,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':76,'multiline':False]['text':' Reshape do_ and o from (8, kBlockM / 32, kHeadDim / 64) to (kBlockM / 32, 8 * kHeadDim / 64)','line_number':84,'multiline':False]['text':' The last coordinate is the "page".','line_number':85,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':107,'multiline':False]['text':' Just compute dot(do, o) and write the result (softmax_d) to global memory as a separate kernel.','line_number':109,'multiline':False]['text':' This is used in the case where we want to parallelize the backward across seqlen_k.','line_number':110,'multiline':False]['text':' The block index for the batch.','line_number':118,'multiline':False]['text':' The block index for the head.','line_number':120,'multiline':False]['text':' The thread index.','line_number':122,'multiline':False]['text':' TODO: careful, we're zeroing out dQaccum with type float4, but when','line_number':153,'multiline':False]['text':' we do atomicAdds, we use type float. The layouts are different. Check this.','line_number':154,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':162,'multiline':False]['text':' Allocate predicate tensors for k','line_number':165,'multiline':False]['text':' Set predicates for k bounds','line_number':167,'multiline':False]['text':'Is_even_MN=','line_number':173,'multiline':True]['text':'Is_even_K=','line_number':173,'multiline':True]['text':'Clear_OOB_MN=','line_number':173,'multiline':True]['text':'Is_even_MN=','line_number':176,'multiline':True]['text':'Is_even_K=','line_number':176,'multiline':True]['text':'Clear_OOB_MN=','line_number':176,'multiline':True]['text':' By right we need to scale dP up by 1/p_dropout, but instead we don't and only scale the final','line_number':179,'multiline':False]['text':' results (dQ and dK) by 1/p_dropout. So we need to keep dP_sum scaled down by p_dropout here,','line_number':180,'multiline':False]['text':' so that (dP - dP_sum) is on the same scale.','line_number':181,'multiline':False]['text':' We're actually not zero'ing out all of dQaccum, but only the part that we're going to','line_number':185,'multiline':False]['text':' do atomicAdds on.','line_number':186,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':193,'multiline':False]['text':' The block index for the batch.','line_number':201,'multiline':False]['text':' The block index for the head.','line_number':203,'multiline':False]['text':' The thread index.','line_number':205,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':231,'multiline':False]['text':' Convert dQ from dQaccum (in float) to fp16/bf16.','line_number':233,'multiline':False]['text':' This is used in the case where we want to parallelize the backward across seqlen_k.','line_number':234,'multiline':False]['text':' Shared memory.','line_number':241,'multiline':False]['text':' The block index for the batch.','line_number':245,'multiline':False]['text':' The block index for the head.','line_number':247,'multiline':False]['text':' The thread index.','line_number':249,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':281,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':283,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':287,'multiline':False]['text':' Convert acc_dq from fp32 to fp16','line_number':296,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':298,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':304,'multiline':False]['text':' Clear_OOB_K must be false since we don't want to write zeros to gmem','line_number':309,'multiline':False]['text':'Is_even_MN=','line_number':310,'multiline':True]['text':'Is_even_K=','line_number':310,'multiline':True]['text':'Clear_OOB_MN=','line_number':310,'multiline':True]['text':'Clear_OOB_K=','line_number':310,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':315,'multiline':False]['text':' Convert dK and dV from dKaccum and dVaccum (in float) to fp16/bf16.','line_number':317,'multiline':False]['text':' This is used in the case where we want to parallelize the backward across seqlen_q.','line_number':318,'multiline':False]['text':' Shared memory.','line_number':325,'multiline':False]['text':' The block index for the batch.','line_number':329,'multiline':False]['text':' The block index for the head.','line_number':331,'multiline':False]['text':' The thread index.','line_number':333,'multiline':False]['text':' (SMEM_N, SMEM_K)','line_number':364,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':374,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':375,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':377,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':379,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':384,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':385,'multiline':False]['text':' Convert acc_dk from fp32 to fp16','line_number':401,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':404,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':405,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':414,'multiline':False]['text':' Clear_OOB_K must be false since we don't want to write zeros to gmem','line_number':419,'multiline':False]['text':'Is_even_MN=','line_number':420,'multiline':True]['text':'Is_even_K=','line_number':420,'multiline':True]['text':'Clear_OOB_MN=','line_number':420,'multiline':True]['text':'Clear_OOB_K=','line_number':420,'multiline':True]['text':'Is_even_MN=','line_number':423,'multiline':True]['text':'Is_even_K=','line_number':423,'multiline':True]['text':'Clear_OOB_MN=','line_number':423,'multiline':True]['text':'Clear_OOB_K=','line_number':423,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':428,'multiline':False]['text':' Shared memory.','line_number':437,'multiline':False]['text':' The thread index.','line_number':440,'multiline':False]['text':' constexpr int kNWarps = Kernel_traits::kNWarps;','line_number':446,'multiline':False]['text':'Varlen=','line_number':451,'multiline':True]['text':' Double buffer for sQ','line_number':508,'multiline':False]['text':' sP and sdQ share the same memory so be careful','line_number':524,'multiline':False]['text':' (KCPY, KCPY_N, KCPY_K)','line_number':551,'multiline':False]['text':' (VCPY, VCPY_N, VCPY_K)','line_number':553,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':555,'multiline':False]['text':' if (cute::thread0()) { print(tdQgdQaccum.layout()); printf("\n"); }','line_number':558,'multiline':False]['text':' __syncthreads();','line_number':559,'multiline':False]['text':' if (blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0 && tidx < 64) {','line_number':560,'multiline':False]['text':'     printf("tidx = %d, tdQgdQaccum = 0x%p\n", tidx, tdQgdQaccum.data());','line_number':561,'multiline':False]['text':' }','line_number':562,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':566,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':567,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':568,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':569,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':573,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':574,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':575,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':576,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':580,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':581,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':583,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':584,'multiline':False]['text':'','line_number':586,'multiline':False]['text':' Copy Atom retiling','line_number':587,'multiline':False]['text':'','line_number':588,'multiline':False]['text':' auto smem_thr_copy_KV = make_tiled_copy_B(typename Kernel_traits::SmemCopyAtom{}, tiled_mma_sdp).get_thread_slice(tidx);','line_number':595,'multiline':False]['text':' if (cute::thread(0, 0) && n_block == 0) { printf("sK layout: "); print(sK.layout()); printf("\n"); }','line_number':599,'multiline':False]['text':' if (cute::thread(0, 0) && n_block == 0) { print(tSsK.layout()); printf("\n"); }','line_number':600,'multiline':False]['text':' Partition sP and sdS to match the accumulator partitioning','line_number':603,'multiline':False]['text':' This has to be tiled_mma_sdp, not tiled_mma_dkv','line_number':604,'multiline':False]['text':' auto smem_thr_copy_PdS = make_tiled_copy_C(typename Kernel_traits::SmemCopyAtomPdS{}, tiled_mma_sdp).get_thread_slice(tidx);','line_number':605,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':608,'multiline':False]['text':' if (cute::thread(0, 0) && n_block == 0) { printf("sP layout: "); print(sP.layout()); printf("\n"); }','line_number':609,'multiline':False]['text':' if (cute::thread(0, 0) && n_block == 0) { print(tPsP.layout()); printf("\n"); }','line_number':610,'multiline':False]['text':' if (n_block == 0 && blockIdx.x == 0 && blockIdx.y == 0 && tidx < 64) {','line_number':611,'multiline':False]['text':'     printf("tidx=%d, tPsP = 0x%p\n", tidx, tPsP.data());','line_number':612,'multiline':False]['text':' }','line_number':613,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':614,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':636,'multiline':False]['text':'','line_number':638,'multiline':False]['text':' PREDICATES','line_number':639,'multiline':False]['text':'','line_number':640,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':642,'multiline':False]['text':' (BLK_N,BLK_K) -> (blk_n,blk_k)','line_number':643,'multiline':False]['text':' Allocate predicate tensors for k','line_number':647,'multiline':False]['text':' Set predicates for k bounds','line_number':651,'multiline':False]['text':' Prologue','line_number':659,'multiline':False]['text':' We'll advance gdQ and gdQaccum before the 1st read/write.','line_number':661,'multiline':False]['text':' If not local, we're guaranteed that m_block_min <= m_block:','line_number':669,'multiline':False]['text':' We checked earlier that n_block * kBlockN < actual_seqlen_k, so in the causal case,','line_number':670,'multiline':False]['text':' n_block * kBlockN + binfo.actual_seqlen_q - binfo.actual_seqlen_k < actual_seqlen_q.','line_number':671,'multiline':False]['text':' So m_block_min <= (actual_seqlen_q - 1) / kBlockM.','line_number':672,'multiline':False]['text':' Recall that m_block_max = cute::ceil_div(binfo.actual_seqlen_q, kBlockM) = (actual_seqlen_q + kBlockM - 1) / kBlockM.','line_number':673,'multiline':False]['text':' So m_block_m - 1 = (actual_seqlen_q - 1) / kBlockM.','line_number':674,'multiline':False]['text':' We conclude that m_block_min <= m_block, so we will always have at least 1 iteration of the for loop.','line_number':675,'multiline':False]['text':' However, if local, then this possible to have some blocks of K & V not attending to any query.','line_number':676,'multiline':False]['text':' We might need to exit early and write 0 to dK and dV for those blocks.','line_number':677,'multiline':False]['text':' Otherwise we get wrong result for the case where we don't enter the for loop.','line_number':678,'multiline':False]['text':' And we might read OOB elements from gQ and gdO.','line_number':679,'multiline':False]['text':' This also covers the case where actual_seqlen_q == 0','line_number':680,'multiline':False]['text':' (BLK_N,BLK_K) -> (blk_n,blk_k)','line_number':700,'multiline':False]['text':' Clear_OOB_K must be false since we don't want to write zeros to gmem','line_number':705,'multiline':False]['text':'Clear_OOB_MN=','line_number':706,'multiline':True]['text':'Clear_OOB_K=','line_number':706,'multiline':True]['text':'Clear_OOB_MN=','line_number':709,'multiline':True]['text':'Clear_OOB_K=','line_number':709,'multiline':True]['text':' Double buffer for sQ','line_number':715,'multiline':False]['text':' Clear the smem tiles to account for predicated off loads','line_number':724,'multiline':False]['text':'Clear_OOB_MN=','line_number':725,'multiline':True]['text':' Clear the smem tiles to account for predicated off loads','line_number':734,'multiline':False]['text':'Clear_OOB_MN=','line_number':735,'multiline':True]['text':'Clear_OOB_MN=','line_number':739,'multiline':True]['text':'Clear_OOB_MN=','line_number':742,'multiline':True]['text':'Clear_OOB_MN=','line_number':746,'multiline':True]['text':' (BLK_M,BLK_N) -> (blk_m,blk_n)','line_number':750,'multiline':False]['text':' (MMA,MMA_N,MMA_N)','line_number':751,'multiline':False]['text':' Convert to ((2, 2), MMA_N, MMA_N) then take only the row indices.','line_number':753,'multiline':False]['text':' Tensor tKrK = make_fragment_like(tKsK);','line_number':762,'multiline':False]['text':' // cute::copy(gmem_tiled_copy_QKV, tKgK(_, _, _, 0), tKrK);','line_number':763,'multiline':False]['text':' cute::copy(gmem_tiled_copy_QKV, tKgK, tKrK);','line_number':764,'multiline':False]['text':' // if (cute::thread(1, 0)) { print(tKrK); }','line_number':765,'multiline':False]['text':'Clear_OOB_MN=','line_number':767,'multiline':True]['text':'Clear_OOB_MN=','line_number':771,'multiline':True]['text':' if (cute::thread0()) { print(tdOgdO.layout()); printf("\n"); print(tdOrdO); print(tdOrO); }','line_number':777,'multiline':False]['text':' M','line_number':788,'multiline':False]['text':' (MMA=4, MMA_N, MMA_N)','line_number':800,'multiline':False]['text':' if (cute::thread0()) { print(sK); }','line_number':809,'multiline':False]['text':' Tensor tSrK_copy_view = smem_thr_copy_KV.retile_D(tSrK);','line_number':810,'multiline':False]['text':' #pragma unroll','line_number':811,'multiline':False]['text':' for (int k = 0; k < size<2>(tSrK_copy_view); ++k) {','line_number':812,'multiline':False]['text':'     cute::copy(smem_tiled_copy_KV, tSsK(_, _, k), tSrK_copy_view(_, _, k));','line_number':813,'multiline':False]['text':' }','line_number':814,'multiline':False]['text':' if (cute::thread0()) { print(tSrK); }','line_number':815,'multiline':False]['text':' Reshape acc_s from (MMA=4, MMA_N, MMA_N) to (col=(2, MMA_N), row=(2, MMA_N))','line_number':819,'multiline':False]['text':' if (cute::thread(32, 0)) { print(scores); }','line_number':821,'multiline':False]['text':' TD [2023-07-29]: I was thinking that we don't need to mask out the elements beyond','line_number':822,'multiline':False]['text':' actual_seqlen_k, because acc_s would be some finite value for those indices.','line_number':823,'multiline':False]['text':' In the end when we multiply with K to get dQ, the corresponding values of K would be 0,','line_number':824,'multiline':False]['text':' so the result would still be correct.','line_number':825,'multiline':False]['text':' However, it's possible that the values in acc_s are so large that they overflow','line_number':826,'multiline':False]['text':' when we multiply with dP and convert to fp16, resulting in Inf in dS and NaNs in dQ.','line_number':827,'multiline':False]['text':' So we need to mask out the elements beyond actual_seqlen_k.','line_number':828,'multiline':False]['text':' Putting this causal masking right after acc_s is *much* slower for some reason.','line_number':835,'multiline':False]['text':' TD [2023-08-16]: We need the 2nd condition because if seqlen_q is long and seqlen_k is short','line_number':836,'multiline':False]['text':' (e.g., 256 and 2), the 2nd block of seqlen_q (from 128 to 255), we're not doing causal masking.','line_number':837,'multiline':False]['text':' But we still want to mask out elements beyond actual_seqlen_k.','line_number':838,'multiline':False]['text':' binfo.actual_seqlen_k, m_block * kBlockM + (tidx / 32) % AtomLayoutMS * 16 + (tidx % 32) / 4,','line_number':844,'multiline':False]['text':' if (cute::thread(32, 0)) { print(scores); }','line_number':858,'multiline':False]['text':' Compute the exponential value.','line_number':859,'multiline':False]['text':'scale_max=','line_number':860,'multiline':True]['text':' Need col to be multiples of 32, since we're doing dropout with block of 16 x 32','line_number':864,'multiline':False]['text':'encode_dropout_in_sign_bit=','line_number':868,'multiline':True]['text':' Convert scores from fp32 to fp16/bf16','line_number':873,'multiline':False]['text':' Reshape rP from (nrow=(2, MMA_N), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_N, MMA_N / 2)','line_number':877,'multiline':False]['text':' if using m16n8k16 or ((2, 2, 1), MMA_N, MMA_N) if using m16n8k8.','line_number':878,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':880,'multiline':False]['text':' if (cute::thread0()) { print(tPaP); }','line_number':882,'multiline':False]['text':' __syncthreads();','line_number':883,'multiline':False]['text':' if (cute::thread0()) { print(sP); }','line_number':884,'multiline':False]['text':' (MMA=4, MMA_N, MMA_N)','line_number':886,'multiline':False]['text':' MMA','line_number':887,'multiline':False]['text':' MMA','line_number':888,'multiline':False]['text':' MMA','line_number':889,'multiline':False]['text':' Tensor acc_dp_reshaped = make_tensor(acc_dp.data(), flash::convert_layout_acc_rowcol(acc_dp.layout()));','line_number':892,'multiline':False]['text':' #pragma unroll','line_number':893,'multiline':False]['text':' for (int mi = 0; mi < size<0>(acc_dp_reshaped); ++mi) {','line_number':894,'multiline':False]['text':'     #pragma unroll','line_number':895,'multiline':False]['text':'     for (int ni = 0; ni < size<1>(acc_dp_reshaped); ++ni) {','line_number':896,'multiline':False]['text':'         acc_dp_reshaped(mi, ni) = -dP_sum(mi);','line_number':897,'multiline':False]['text':'     }','line_number':898,'multiline':False]['text':' }','line_number':899,'multiline':False]['text':' if (cute::thread0()) { print(dP_sum); }','line_number':901,'multiline':False]['text':'A_in_regs=','line_number':903,'multiline':True]['text':'B_in_regs=','line_number':903,'multiline':True]['text':' Reshape acc_dp from (MMA=4, MMA_N, MMA_N) to (col=(2, MMA_N), row=(2, MMA_N))','line_number':908,'multiline':False]['text':' if (cute::thread0()) { print(dS); }','line_number':920,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':922,'multiline':False]['text':' Reshape acc_dq from (4, 1, 2) to (4, 2, 1) to write to gdQaccum','line_number':927,'multiline':False]['text':' Double buffer for sQ','line_number':936,'multiline':False]['text':' Advance gQ','line_number':940,'multiline':False]['text':'Is_even_MN=','line_number':942,'multiline':True]['text':' Convert dS from fp32 to fp16','line_number':947,'multiline':False]['text':' if (cute::thread0()) { print(tPrP); }','line_number':949,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':950,'multiline':False]['text':' Layout p_l = tPrP.layout();','line_number':954,'multiline':False]['text':' Tensor tdVrPt = make_tensor(tPrP.data(), make_layout(get<0>(p_l), get<2>(p_l), get<1>(p_l)));','line_number':955,'multiline':False]['text':' flash::gemm_A_in_regs(acc_dv, tdVrPt, tdVrdO, tdVsdOt, tiled_mma_dkv, smem_thr_copy_QdOt);','line_number':956,'multiline':False]['text':' Tensor tdKrdSt = make_tensor(tdSrdS.data(), tdVrPt.layout());','line_number':957,'multiline':False]['text':' flash::gemm_A_in_regs(acc_dk, tdKrdSt, tdKrQt, tdKsQt, tiled_mma_dkv, smem_thr_copy_QdOt);','line_number':958,'multiline':False]['text':' if (cute::thread0() && n_block == 0 && m_block == 0) { print(tdVrPt); }','line_number':961,'multiline':False]['text':' if (cute::thread0()) { print(acc_dv); }','line_number':962,'multiline':False]['text':' Need syncthreads since we're writing to the same sdO location','line_number':964,'multiline':False]['text':' Advance gdO','line_number':967,'multiline':False]['text':'Is_even_MN=','line_number':971,'multiline':True]['text':'Is_even_MN=','line_number':972,'multiline':True]['text':'Is_even_MN=','line_number':974,'multiline':True]['text':' if (cute::thread0()) { print(acc_dq); }','line_number':981,'multiline':False]['text':' Reshape acc_dq from (4, 1, 2) to (4, 2, 1) to write to gdQaccum','line_number':991,'multiline':False]['text':' if (cute::thread0()) { print(acc_dq.layout()); printf("\n"); print(acc_dq_reshaped.layout()); printf("\n"); print(tdQgdQaccum.layout()); printf("\n"); }','line_number':999,'multiline':False]['text':' Convert acc_dq from fp32 to fp16','line_number':1007,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1009,'multiline':False]['text':' if (cute::thread0()) { print(acc_dk); }','line_number':1015,'multiline':False]['text':' Double buffer for sQ','line_number':1016,'multiline':False]['text':' Advance gQ','line_number':1021,'multiline':False]['text':'Is_even_MN=','line_number':1023,'multiline':True]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':1038,'multiline':False]['text':' Epilogue','line_number':1050,'multiline':False]['text':' Convert acc_dv from fp32 to fp16','line_number':1059,'multiline':False]['text':' (SMEM_N, SMEM_K)','line_number':1063,'multiline':False]['text':' (SMEM_N, SMEM_K)','line_number':1064,'multiline':False]['text':' Partition sdV and sdK to match the accumulator partitioning','line_number':1066,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1069,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':1070,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1071,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':1072,'multiline':False]['text':' We need syncthreads here since we're writing to the same location as sK and sV.','line_number':1074,'multiline':False]['text':' Without syncthreads, some thread might modify the location of sK while another thread','line_number':1075,'multiline':False]['text':' is reading it for dQ gemm, leading to a race condition.','line_number':1076,'multiline':False]['text':' If Is_last, there's already a __syncthreads() at the end of the loop.','line_number':1077,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':1096,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':1098,'multiline':False]['text':' (BLK_N,BLK_K) -> (blk_n,blk_k)','line_number':1106,'multiline':False]['text':' Clear_OOB_K must be false since we don't want to write zeros to gmem','line_number':1111,'multiline':False]['text':'Clear_OOB_MN=','line_number':1112,'multiline':True]['text':'Clear_OOB_K=','line_number':1112,'multiline':True]['text':'Clear_OOB_MN=','line_number':1115,'multiline':True]['text':'Clear_OOB_K=','line_number':1115,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':1121,'multiline':False]['text':' Shared memory.','line_number':1130,'multiline':False]['text':' The thread index.','line_number':1133,'multiline':False]['text':' constexpr int kNWarps = Kernel_traits::kNWarps;','line_number':1139,'multiline':False]['text':'Varlen=','line_number':1143,'multiline':True]['text':' We iterate over the blocks in reverse order. This is because the last block is the only one','line_number':1151,'multiline':False]['text':' that needs masking when we read K and V from global memory. Moreover, iterating in reverse','line_number':1152,'multiline':False]['text':' might save us 1 register (we just need n_block instead of both n_block and n_block_max).','line_number':1153,'multiline':False]['text':' We move K and V to the last block.','line_number':1157,'multiline':False]['text':' We'll advance gdKaccum and gdVaccum before the first write.','line_number':1166,'multiline':False]['text':' We assume that params.d == kHeadDim for now','line_number':1171,'multiline':False]['text':' Double buffer for sK','line_number':1205,'multiline':False]['text':' (KCPY, KCPY_N, KCPY_K)','line_number':1230,'multiline':False]['text':' (VCPY, VCPY_N, VCPY_K)','line_number':1232,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':1239,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':1240,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':1241,'multiline':False]['text':' (MMA,MMA_N,MMA_K)','line_number':1242,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':1246,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':1247,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':1248,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':1249,'multiline':False]['text':' (MMA, MMA_N, MMA_N)','line_number':1253,'multiline':False]['text':' (MMA, MMA_K, MMA_N)','line_number':1254,'multiline':False]['text':' MMA, MMA_M_SdP, MMA_K','line_number':1256,'multiline':False]['text':'','line_number':1258,'multiline':False]['text':' Copy Atom retiling','line_number':1259,'multiline':False]['text':'','line_number':1260,'multiline':False]['text':' Partition sP and sdS to match the accumulator partitioning','line_number':1272,'multiline':False]['text':' This has to be tiled_mma_sdp, not tiled_mma_dkv','line_number':1273,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':1276,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':1277,'multiline':False]['text':'','line_number':1297,'multiline':False]['text':' PREDICATES','line_number':1298,'multiline':False]['text':'','line_number':1299,'multiline':False]['text':' Construct identity layout for sQ and sK','line_number':1301,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':1302,'multiline':False]['text':' (BLK_N,BLK_K) -> (blk_n,blk_k)','line_number':1303,'multiline':False]['text':' Repeat the partitioning with identity layouts','line_number':1304,'multiline':False]['text':' (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)','line_number':1305,'multiline':False]['text':' (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)','line_number':1306,'multiline':False]['text':' Allocate predicate tensors for k','line_number':1308,'multiline':False]['text':' Set predicates for k bounds','line_number':1312,'multiline':False]['text':' Prologue','line_number':1320,'multiline':False]['text':' TODO: Might need to exit early and write 0 to gdQ.','line_number':1325,'multiline':False]['text':'Is_even_MN=','line_number':1327,'multiline':True]['text':'Clear_OOB_MN=','line_number':1327,'multiline':True]['text':'Is_even_MN=','line_number':1330,'multiline':True]['text':'Clear_OOB_MN=','line_number':1330,'multiline':True]['text':'Is_even_MN=','line_number':1335,'multiline':True]['text':'Clear_OOB_MN=','line_number':1335,'multiline':True]['text':'Clear_OOB_MN=','line_number':1346,'multiline':True]['text':'Clear_OOB_MN=','line_number':1349,'multiline':True]['text':' (BLK_M,BLK_N) -> (blk_m,blk_n)','line_number':1353,'multiline':False]['text':' (MMA,MMA_N,MMA_N)','line_number':1354,'multiline':False]['text':' Convert to ((2, 2), MMA_N, MMA_N) then take only the row indices.','line_number':1356,'multiline':False]['text':' (MMA=4, MMA_M_SdP, MMA_N)','line_number':1384,'multiline':False]['text':' Reshape acc_s from (MMA=4, MMA_N, MMA_N) to (col=(2, MMA_N), row=(2, MMA_N))','line_number':1392,'multiline':False]['text':' We don't need to mask out the elements beyond actual_seqlen_k, because acc_s would','line_number':1394,'multiline':False]['text':' be some finite value for those indices. In the end when we multiply with K to get dQ,','line_number':1395,'multiline':False]['text':' the corresponding values of K would be 0, so the result would still be correct.','line_number':1396,'multiline':False]['text':' binfo.actual_seqlen_k, m_block * kBlockM + (tidx / 32) % AtomLayoutMS * 16 + (tidx % 32) / 4,','line_number':1400,'multiline':False]['text':' Compute the exponential value.','line_number':1404,'multiline':False]['text':'scale_max=','line_number':1405,'multiline':True]['text':' Need col to be multiples of 32, since we're doing dropout with block of 16 x 32','line_number':1409,'multiline':False]['text':'encode_dropout_in_sign_bit=','line_number':1413,'multiline':True]['text':' Convert scores from fp32 to fp16/bf16','line_number':1418,'multiline':False]['text':' Reshape rP from (nrow=(2, MMA_N), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_N, MMA_N / 2)','line_number':1422,'multiline':False]['text':' if using m16n8k16 or ((2, 2, 1), MMA_N, MMA_N) if using m16n8k8.','line_number':1423,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1425,'multiline':False]['text':' (MMA=4, MMA_N, MMA_N)','line_number':1428,'multiline':False]['text':' MMA','line_number':1429,'multiline':False]['text':' MMA','line_number':1430,'multiline':False]['text':' MMA','line_number':1431,'multiline':False]['text':' Reshape acc_dp from (MMA=4, MMA_N, MMA_N) to (col=(2, MMA_N), row=(2, MMA_N))','line_number':1437,'multiline':False]['text':' Convert dS from fp32 to fp16','line_number':1451,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1453,'multiline':False]['text':' Double buffer for sK','line_number':1458,'multiline':False]['text':' Advance gK, gV','line_number':1462,'multiline':False]['text':'Is_even_MN=','line_number':1465,'multiline':True]['text':'Is_even_MN=','line_number':1466,'multiline':True]['text':' This cp_async_fence needs to be in the if block, otherwise the synchronization','line_number':1467,'multiline':False]['text':' isn't right and we get race conditions.','line_number':1468,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':1472,'multiline':False]['text':' if (threadIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) { print(acc_dv); }','line_number':1476,'multiline':False]['text':' MMA, MMA_N, MMA_K','line_number':1482,'multiline':False]['text':' Double buffer for sK','line_number':1492,'multiline':False]['text':' Epilogue','line_number':1497,'multiline':False]['text':' Convert acc_dq from fp32 to fp16','line_number':1501,'multiline':False]['text':' Partition sdV and sdK to match the accumulator partitioning','line_number':1506,'multiline':False]['text':' ((Atom,AtomNum), MMA_N, MMA_N)','line_number':1509,'multiline':False]['text':' ((Atom,AtomNum),PIPE_M,PIPE_N)','line_number':1510,'multiline':False]['text':' ((Atom,AtomNum),ATOM_M,ATOM_N)','line_number':1523,'multiline':False]['text':' (BLK_M,BLK_K) -> (blk_m,blk_k)','line_number':1531,'multiline':False]['text':' Clear_OOB_K must be false since we don't want to write zeros to gmem','line_number':1538,'multiline':False]['text':'Is_even_MN=','line_number':1539,'multiline':True]['text':'Clear_OOB_MN=','line_number':1539,'multiline':True]['text':'Clear_OOB_K=','line_number':1539,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':1544,'multiline':False]['text':' The block index for the batch.','line_number':1549,'multiline':False]['text':' const int bidb = blockIdx.y;','line_number':1551,'multiline':False]['text':' The block index for the head.','line_number':1552,'multiline':False]['text':' const int bidh = blockIdx.z;','line_number':1554,'multiline':False]['text':' The thread index.','line_number':1555,'multiline':False]['text':' Iterating backward from n_block_max - 1 to 0 might save 1 register','line_number':1562,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':1571,'multiline':False]['text':' The block index for the batch.','line_number':1577,'multiline':False]['text':' The block index for the head.','line_number':1579,'multiline':False]['text':'Seq_parallel=','line_number':1582,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':1585,'multiline':False]['text':' The block index for the batch.','line_number':1591,'multiline':False]['text':' The block index for the head.','line_number':1593,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':1599,'multiline':False]['text':' namespace pytorch_flash','line_number':1600,'multiline':False]