['text':' uint8 * int8 -> uint8 (no quantization/dequantization)','line_number':30,'multiline':False]['text':' We make a strong guarantee that models using these operators will have','line_number':32,'multiline':False]['text':' the same numerics across different machines. Therefore, we do not provide','line_number':33,'multiline':False]['text':' a fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':34,'multiline':False]['text':' TODO: contiguous is called for further jit optimizations.','line_number':38,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':46,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':58,'multiline':False]['text':' Process the per tensor quantization.','line_number':68,'multiline':False]['text':' Process the per channel quantization.','line_number':73,'multiline':False]['text':' The resulting matrix here is 2-D, let's view it with the original','line_number':97,'multiline':False]['text':' left hand dimensions of the input. Here are two examples:','line_number':98,'multiline':False]['text':' 1. If the input tensor is {batch_size, K}, the output tensor is','line_number':99,'multiline':False]['text':' {batch_size, out_channels}.','line_number':100,'multiline':False]['text':' 2. If the input tensor is {x, batch_size, K}, the output tensor is {x,','line_number':101,'multiline':False]['text':' batch_size, out_channels}.','line_number':102,'multiline':False]['text':' NOLINT','line_number':104,'multiline':False]['text':' Allocate output Tensor and a buffer for fbgemmPacked to use','line_number':105,'multiline':False]['text':' fbgemm kernel computes the following:','line_number':119,'multiline':False]['text':' C(output) = A(weight) x B(input), where C, A, B are out_channels x','line_number':120,'multiline':False]['text':' batch_size, out_channels x K, K x batch_size matrices, respectively.','line_number':121,'multiline':False]['text':' Therefore we need to transpose input','line_number':122,'multiline':False]['text':' TODO: Activation transpose before and after the kernel can be removed if we','line_number':131,'multiline':False]['text':' keep activation tensor always tranposed.','line_number':132,'multiline':False]['text':'activation offsets','line_number':145,'multiline':True]['text':' Process the per tensor quantization.','line_number':150,'multiline':False]['text':'','line_number':151,'multiline':False]['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':152,'multiline':False]['text':' operation does:','line_number':153,'multiline':False]['text':'  1) Add in row and column offsets to the rows and columns,','line_number':154,'multiline':False]['text':'  respectively.','line_number':155,'multiline':False]['text':'  2) Add in the bias term.','line_number':156,'multiline':False]['text':' Do the GEMM','line_number':158,'multiline':False]['text':'ldb=','line_number':165,'multiline':True]['text':'C_i32=','line_number':166,'multiline':True]['text':'C_u8=','line_number':167,'multiline':True]['text':'ldc=','line_number':169,'multiline':True]['text':'rParams=','line_number':170,'multiline':True]['text':'accum=','line_number':171,'multiline':True]['text':'thread_id=','line_number':172,'multiline':True]['text':'num_threads=','line_number':173,'multiline':True]['text':' Process the per channel quantization.','line_number':175,'multiline':False]['text':'','line_number':176,'multiline':False]['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':177,'multiline':False]['text':' operation does:','line_number':178,'multiline':False]['text':'  1) Add in row and column offsets to the rows and columns,','line_number':179,'multiline':False]['text':'  respectively.','line_number':180,'multiline':False]['text':'  2) Add in the bias term.','line_number':181,'multiline':False]['text':' Do the GEMM','line_number':183,'multiline':False]['text':'ldb=','line_number':190,'multiline':True]['text':'C_i32=','line_number':191,'multiline':True]['text':'C_u8=','line_number':192,'multiline':True]['text':'ldc=','line_number':194,'multiline':True]['text':'rParams=','line_number':195,'multiline':True]['text':'accum','line_number':196,'multiline':True]['text':'thread_id=','line_number':197,'multiline':True]['text':'num_threads=','line_number':198,'multiline':True]['text':' transpose output_tr back to batch_size x out_channels','line_number':203,'multiline':False]['text':' USE_FBGEMM','line_number':229,'multiline':False]['text':' namespace','line_number':259,'multiline':False]['text':' namespace ao::sparse','line_number':260,'multiline':False]