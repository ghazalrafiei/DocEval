['text':'*****************************************************************************
 * Copyright (c) 2023, Tri Dao.
 *****************************************************************************','line_number':1,'multiline':True]['text':' If Share_Q_K_smem is true, that forces Is_Q_in_regs to be true','line_number':52,'multiline':False]['text':' The number of threads.','line_number':66,'multiline':False]['text':' 4x1x1 or 8x1x1 thread group','line_number':80,'multiline':False]['text':' 1x2x1 or 1x2x2 value group for 16x16x16 MMA and LDSM','line_number':81,'multiline':False]['text':' This has to be kBlockKSmem, using kHeadDim gives wrong results for d=128','line_number':85,'multiline':False]['text':' This has to be kBlockN and not 8, otherwise we get wrong results for d=128','line_number':96,'multiline':False]['text':' Maybe the VtransposeNoSwizzle just needs to have the right shape','line_number':104,'multiline':False]['text':' And the strides don't matter?','line_number':105,'multiline':False]['text':' using SmemLayoutVtransposedNoSwizzle = decltype(SmemLayoutVtransposed{}.layout_fn());','line_number':109,'multiline':False]['text':' Using kBlockKSmem here is 6-10% faster than kBlockKGmem for d=128 because of bank conflicts.','line_number':129,'multiline':False]['text':' For example, for d=128, smem is split into 2 "pages", each page takes care of columns','line_number':130,'multiline':False]['text':' 0-63 and 64-127. If we have 16 threads per row for gmem read, when we write to smem,','line_number':131,'multiline':False]['text':' thread 0 - 7 will write to the first page and thread 8 - 15 will write to the second page,','line_number':132,'multiline':False]['text':' to the same banks.','line_number':133,'multiline':False]['text':' We use CACHEGLOBAL instead of CACHEALWAYS for both Q and K/V, since we won't be reading','line_number':139,'multiline':False]['text':' from the same address by the same threadblock. This is slightly faster.','line_number':140,'multiline':False]['text':' Val layout, 8 vals per read','line_number':149,'multiline':False]['text':' Val layout, 8 vals per store','line_number':153,'multiline':False]['text':' Val layout, 8 vals per store','line_number':162,'multiline':False]['text':' Thread layout, 8 threads per row','line_number':166,'multiline':False]['text':' Thread layout, 16 threads per row','line_number':168,'multiline':False]['text':' Val layout, 4 vals per store','line_number':174,'multiline':False]['text':' Val layout, 4 vals per load','line_number':179,'multiline':False]['text':' Val layout, 8 vals per load','line_number':183,'multiline':False]['text':' Is_V_in_regs is an option to reduce smem usage, but will increase register pressue.','line_number':186,'multiline':False]['text':' No_double_buffer is another option to reduce smem usage, but will slow things down.','line_number':187,'multiline':False]['text':' The number of threads.','line_number':203,'multiline':False]['text':' 1x2x1 or 1x2x2 value group for 16x16x16 MMA and LDSM','line_number':223,'multiline':False]['text':' 1x2x1 or 1x2x2 value group for 16x16x16 MMA and LDSM','line_number':228,'multiline':False]['text':' 2x4x1 or 4x2x1 thread group','line_number':232,'multiline':False]['text':' 1x2x1 or 1x2x2 value group for 16x16x16 MMA and LDSM','line_number':233,'multiline':False]['text':' SmemLayoutAtomQdO{},','line_number':248,'multiline':False]['text':' Maybe the KtransposeNoSwizzle just needs to have the right shape','line_number':259,'multiline':False]['text':' And the strides don't matter?','line_number':260,'multiline':False]['text':' using SmemLayoutKtransposedNoSwizzle = decltype(SmemLayoutKtransposed{}.layout_fn());','line_number':264,'multiline':False]['text':' TODO: generalize to other values of kBlockN','line_number':266,'multiline':False]['text':' TODO: what should be the Swizzle here? 3 is faster than 1, and 1 is faster than 2','line_number':267,'multiline':False]['text':' static constexpr int kPBlockN = kBlockN;','line_number':268,'multiline':False]['text':' TD [2023-03-19]: Idk why kPBlockN = 16 and kSwizzlePdS=3 is the fastest.','line_number':270,'multiline':False]['text':' static constexpr int kSwizzlePdS = kPBlockN == 16 ? 1 : (kPBlockN == 32 ? 2 : 3);','line_number':273,'multiline':False]['text':' using SmemLayoutPdStransposedNoSwizzle = decltype(SmemLayoutPdStransposed{}.layout_fn());','line_number':292,'multiline':False]['text':' using SmemLayoutQdOtransposedNoSwizzle = decltype(SmemLayoutQdOtransposed{}.layout_fn());','line_number':305,'multiline':False]['text':' Double buffer for sQ','line_number':325,'multiline':False]['text':' Using kBlockKSmem instead of kHeadDim here to avoid bank conflicts, but doesn't seem','line_number':349,'multiline':False]['text':' to affect speed in practice.','line_number':350,'multiline':False]['text':' We use CACHEGLOBAL instead of CACHEALWAYS for both Q and K/V, since we won't be reading','line_number':356,'multiline':False]['text':' from the same address by the same threadblock. This is slightly faster.','line_number':357,'multiline':False]['text':' Val layout, 8 vals per read','line_number':366,'multiline':False]['text':' Val layout, 8 vals per store','line_number':370,'multiline':False]['text':' Val layout, 8 vals per store','line_number':374,'multiline':False]['text':' Val layout, 8 vals per store','line_number':378,'multiline':False]['text':' Thread layout, 8 threads per row','line_number':381,'multiline':False]['text':' Thread layout, 16 threads per row','line_number':383,'multiline':False]['text':' Val layout, 4 vals per store','line_number':389,'multiline':False]['text':' Thread layout, 8 threads per row','line_number':393,'multiline':False]['text':' Val layout, 1 val per store','line_number':395,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':399,'multiline':False]['text':' namespace pytorch_flash','line_number':400,'multiline':False]