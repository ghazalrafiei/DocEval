['text':' XXX: gradOutput that we get here is really gradOutput * output','line_number':82,'multiline':False]['text':' Look for cmul in SoftMax_updateGradInput','line_number':83,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':94,'multiline':False]['text':' Spatial kernel (fast with large inner_size and small dim_size)','line_number':95,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':96,'multiline':False]['text':' Let's assume that our input has been flattened to have only three dimension:','line_number':97,'multiline':False]['text':'     outer x dim x inner','line_number':98,'multiline':False]['text':' The spatial algorithm tries to parallelize along all of them.','line_number':99,'multiline':False]['text':' Within a 2d block threadIdx.y parallelizes over dim slices, and threads that','line_number':100,'multiline':False]['text':' share it will speed up reductions over dim (along axis x).','line_number':101,'multiline':False]['text':' The 2d grid is used to parallelize inner dimension over y axis and outer over x.','line_number':102,'multiline':False]['text':' First, tile as many blocks as we can over the y axis','line_number':106,'multiline':False]['text':' Fill the x axis with as many blocks as we can fit (a little more is ok too)','line_number':110,'multiline':False]['text':' HIP function signature is not compatible yet.','line_number':143,'multiline':False]['text':' In the vectorized case we want to trade off allowing more of the buffers to be accessed','line_number':160,'multiline':False]['text':' in a vectorized way against wanting a larger block size to get better utilisation.','line_number':161,'multiline':False]['text':' In general with ILP you can have (ILP-1)/ILP of the buffer accessed vectorised, at the risk','line_number':162,'multiline':False]['text':' of having a very small block size. We choose to keep >= 1/2 of the buffer vectorised while','line_number':163,'multiline':False]['text':' allowing a larger block size.','line_number':164,'multiline':False]['text':' Launch at least a single warp - the kernel assumes that.','line_number':170,'multiline':False]['text':' Note that it's not a complete block-wide reduction.','line_number':189,'multiline':False]['text':' Only threads that share threadIdx.y reduce values.','line_number':190,'multiline':False]['text':' NOTE: loop starts with __syncthreads()','line_number':201,'multiline':False]['text':'//////////////////////////////////////////////////////////','line_number':229,'multiline':False]['text':' These two blocks are really equivalent, but specializing on','line_number':230,'multiline':False]['text':' blockDim.x == 1 makes the kernel faster when it's unused.','line_number':231,'multiline':False]['text':' I didn't want to thread an extra template parameter, and nvcc','line_number':232,'multiline':False]['text':' seems to be smart enough to hoist the if outside of the loops.','line_number':233,'multiline':False]['text':'//////////////////////////////////////////////////////////','line_number':234,'multiline':False]['text':' See the comment in forward kernel','line_number':287,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':317,'multiline':False]['text':' Regular kernel (fast when dim_size is large; requires inner_size == 1)','line_number':318,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':319,'multiline':False]['text':' To avoid RaW races from chaining blockReduce calls together, we need a sync here','line_number':357,'multiline':False]['text':' First warp will perform per-warp reductions for the remaining warps','line_number':366,'multiline':False]['text':' First thread will perform a reduction of the above per-warp reductions','line_number':384,'multiline':False]['text':' Sync and broadcast','line_number':394,'multiline':False]['text':' shift and do 1','line_number':411,'multiline':False]['text':' Epilogue','line_number':436,'multiline':False]['text':'*
 * This will apply the Epilogue with vectorized reads & writes when input & output have the same shift
 ','line_number':443,'multiline':True]['text':' if unaligned, do one value / thread and move on, guaranteeing aligned reads/writes later','line_number':459,'multiline':False]['text':' handle the tail','line_number':493,'multiline':False]['text':' if unaligned, do one value / thread and move on, guaranteeing aligned reads/writes later','line_number':513,'multiline':False]['text':'*
 * This will apply the Epilogue with non-vectrorized reads & writes for the general case
 ','line_number':558,'multiline':True]['text':' Main bulk of loop with ILP','line_number':572,'multiline':False]['text':' Remainder - no ILP','line_number':586,'multiline':False]['text':' Remainder - no ILP','line_number':621,'multiline':False]['text':' forward pointers to batch[blockIdx.x]','line_number':637,'multiline':False]['text':' each block handles a sample in the mini-batch','line_number':638,'multiline':False]['text':' find the max','line_number':645,'multiline':False]['text':' reduce all values','line_number':651,'multiline':False]['text':' This kernel spawns a block per each element in the batch.','line_number':736,'multiline':False]['text':' XXX: it assumes that inner_size == 1','line_number':737,'multiline':False]['text':' not masked ','line_number':751,'multiline':True]['text':' not masked ','line_number':772,'multiline':True]['text':' This kernel runs in a 2D grid, where each application along y dimension has a fixed','line_number':787,'multiline':False]['text':' outer_size, and runs in parallel over inner_size. Dimension x is parallel over outer_size.','line_number':788,'multiline':False]['text':' Reductions over dim are done in a single-threaded manner.','line_number':789,'multiline':False]['text':' See descriptions of kernels above.','line_number':843,'multiline':False]['text':' masked_softmax ','line_number':857,'multiline':True]['text':' masked_softmax ','line_number':881,'multiline':True]['text':' If input is [B, H, T, T] and mask is [B, T]','line_number':991,'multiline':False]['text':' we have special fast kernel','line_number':992,'multiline':False]['text':' mask_type == 1 => mask_ is a src_key_padding_mask','line_number':993,'multiline':False]['text':' If input is [B, H, T, T] and mask is [T, T]','line_number':996,'multiline':False]['text':' expand mask to [B, H, T, T] and treat it like regular mask','line_number':997,'multiline':False]['text':' TODO We should have special fast kernel for TxT mask as well','line_number':998,'multiline':False]['text':' mask_type == 0 => mask_ is a src_mask','line_number':999,'multiline':False]['text':' If mask_type == 2, then mask_.sizes() must equal input_.sizes()','line_number':1001,'multiline':False]['text':' Persistent softmax is only supported when all of the conditions are held:','line_number':1012,'multiline':False]['text':'     1) softmax_elements <= 1024','line_number':1013,'multiline':False]['text':'     2) softmax_elements * input.element_size() <= 4096','line_number':1014,'multiline':False]['text':'     3) mask.is_contiguous()','line_number':1015,'multiline':False]['text':'     4) dim == input.dim() - 1','line_number':1016,'multiline':False]['text':' Otherwise, we fallback to vanilla softmax (where we do not support transformer_mask since converting the mask is expensive)','line_number':1017,'multiline':False]['text':' Only support when num_heads is even in transformer','line_number':1035,'multiline':False]['text':' is_log_softmax ','line_number':1044,'multiline':True]['text':' is_masked ','line_number':1044,'multiline':True]['text':' dst','line_number':1045,'multiline':False]['text':' src','line_number':1046,'multiline':False]['text':' is_transformer_mask','line_number':1052,'multiline':False]['text':' is_log_softmax ','line_number':1064,'multiline':True]['text':' is_masked ','line_number':1064,'multiline':True]['text':' dst','line_number':1065,'multiline':False]['text':' src','line_number':1066,'multiline':False]['text':' masked_softmax ','line_number':1126,'multiline':True]['text':' gI_ptr','line_number':1127,'multiline':False]['text':' grad_ptr','line_number':1128,'multiline':False]['text':' output_ptr','line_number':1129,'multiline':False]['text':' softmax_elements','line_number':1130,'multiline':False]['text':' softmax_elements_stride','line_number':1131,'multiline':False]['text':' batch_count','line_number':1132,'multiline':False]['text':' not masked ','line_number':1133,'multiline':True]['text':' namespace at::native','line_number':1140,'multiline':False]