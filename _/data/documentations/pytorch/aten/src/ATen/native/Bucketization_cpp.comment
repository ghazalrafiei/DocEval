['text':' Implement a numpy like searchsorted and a TF like bucketize function running on cpu
 *
 * - torch.searchsorted(sorted_sequence, values, right=False, side='left', out_int32=False, sorter=None)
 *   sorted_sequence - N*D or 1D (apply to all values) tensor containing sorted sequences in last dimension
 *   values          - N*D tensor or a Scalar (when sorted_sequence is 1D) containing the search values
 *   right           - corresponding to lower bound if False and upper bound if True
 *   side            - (preferred to right) corresponding to lower bound if 'left' and upper bound if 'right'
 *   out_int32       - the output tensor is int64_t type if False and int(32bit normally) type if True.
 *   sorter          - if provided, sorted_sequence may not be sorted and the sorted order is given by this tensor
 *
 * - torch.bucketize(values, boundaries, right=False, out_int32=False)
 *   values     - N*D tensor or a Scalar containing the search value
 *   boundaries - 1D tensor containing a sorted sequences
 *   right      - corresponding to lower bound if False and upper bound if True
 *   out_int32  - the output tensor is int64_t type if False and int(32bit normally) type if True.
 *
 * - Restrictions are defined in searchsorted_pre_check()
 ','line_number':17,'multiline':True]['text':' minimal size for searchsorted_cpu_contiguous to run parallel (multithread)','line_number':41,'multiline':False]['text':' customized lower_bound func to ensure the low bound of 'nan', 'inf' etc. be the end of boundary','line_number':44,'multiline':False]['text':' and we can properly handle a sorter argument','line_number':45,'multiline':False]['text':' std::lower_bound can not be used here since its customized comparator need strict weak ordering','line_number':46,'multiline':False]['text':' and the customized comparators require both arguments to have the same type, which wouldn't','line_number':47,'multiline':False]['text':' happen when comparing val of input_t to an indexer value from sorter of int64','line_number':48,'multiline':False]['text':' sorter gives relative ordering for ND tensors, so we need to save and add the non-updated start as an offset','line_number':51,'multiline':False]['text':' i.e. the second row of a 3x3 tensors starts at element 3 but sorter's second row only contains 0, 1, or 2','line_number':52,'multiline':False]['text':' customized upper_bound func to ensure we can properly handle a sorter argument','line_number':67,'multiline':False]['text':' std::upper_bound can not be used here since its customized comparator requires both arguments to have the','line_number':68,'multiline':False]['text':' same type, which wouldn't happen when comparing val of input_t to an indexer value from sorter of int64','line_number':69,'multiline':False]['text':' sorter gives relative ordering for ND tensors, so we need to save and add the non-updated start as an offset','line_number':72,'multiline':False]['text':' i.e. the second row of a 3x3 tensors starts at element 3 but sorter's second row only contains 0, 1, or 2','line_number':73,'multiline':False]['text':' inner most dim size of input and boundaries','line_number':92,'multiline':False]['text':' If boundaries tensor is 1d, we always search the entire boundary tensor','line_number':104,'multiline':False]['text':' type conversion might happen here','line_number':112,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':153,'multiline':False]['text':' we have two inputs to set right, pre_check checks that they aren't set to opposites','line_number':159,'multiline':False]['text':' for non-contiguous result tensors, we write the output to a contiguous copy so we can later copy back, maintaing the original result tensor','line_number':166,'multiline':False]['text':' if result is non-contiguous, we wrote the answer to a copied version, so we copy back to the original result tensor','line_number':185,'multiline':False]['text':' namespace at::native','line_number':247,'multiline':False]