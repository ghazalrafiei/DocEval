['text':' Copyright (c) 2022, Tri Dao.','line_number':1,'multiline':False]['text':' If Is_local is true, Is_causal should be false','line_number':31,'multiline':False]['text':' We want to specialize to is_even_MN and not just is_even_M, since in the case where N is not','line_number':63,'multiline':False]['text':' a multiple of kBlockN, we'll need to apply mask in the loop.','line_number':64,'multiline':False]['text':' printf("smem_size_dq_dk_dv = %d\n", smem_size_dq_dk_dv);','line_number':68,'multiline':False]['text':' If not IsEvenKConst, we also set IsEvenMNConst to false to reduce number of templates.','line_number':73,'multiline':False]['text':' If head dim > 128, set IsEvenMNConst to false to reduce number of templates','line_number':74,'multiline':False]['text':' If Is_local, set Is_causal to false','line_number':75,'multiline':False]['text':' auto kernel = &flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits, Is_dropout, Is_causal, IsEvenMNConst, true>;','line_number':77,'multiline':False]['text':' We also use is_even_N to set Unpadded in the BlockInfo constructor, so we need to check','line_number':107,'multiline':False]['text':' for cu_seqlens_k as well.','line_number':108,'multiline':False]['text':' printf("smem_size_dq_dk_dv = %d\n", smem_size_dq_dk_dv);','line_number':112,'multiline':False]['text':' If not IsEvenKConst, we also set IsEvenMNConst to false to reduce number of templates.','line_number':116,'multiline':False]['text':' auto kernel = &flash_bwd_dq_dk_dv_loop_seqq_parallel_kernel<Kernel_traits, false, false, IsEvenNConst, IsEvenKConst>;','line_number':118,'multiline':False]['text':' 104 KB','line_number':156,'multiline':False]['text':' We can afford more registers to keep V in registers','line_number':157,'multiline':False]['text':' 96 KB','line_number':162,'multiline':False]['text':' printf("max_smem_per_block = %d\n", max_smem_per_block);','line_number':179,'multiline':False]['text':' Changing AtomLayoutMdQ from 2 to 4 takes the same time','line_number':181,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 4, 2, false, false, T>>(params, stream, configure);','line_number':182,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 4, 2, true, false, T>>(params, stream, configure);','line_number':183,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 128, 8, 2, 4, 4, false, false, T>>(params, stream, configure);','line_number':184,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':185,'multiline':False]['text':' This is slightly faster. We want to split M more so we need fewer registers to store LSE.','line_number':186,'multiline':False]['text':' This has a lot of register spilling','line_number':189,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 128, 8, 4, 4, 4, true, false, T>, Is_dropout>(params, stream, configure);','line_number':190,'multiline':False]['text':' if (params.h == params.h_k) {','line_number':192,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':193,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':195,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 4, true, false, T>, Is_dropout>(params, stream, configure);','line_number':196,'multiline':False]['text':' } else {','line_number':197,'multiline':False]['text':'     run_flash_bwd_seqq_parallel<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':198,'multiline':False]['text':' }','line_number':199,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 4, true, false, T>>(params, stream, configure);','line_number':202,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 4, 2, 2, 2, true, false, T>>(params, stream, configure);','line_number':203,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 32, 128, 4, 1, 4, 1, false, false, T>>(params, stream, configure);','line_number':204,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 16, 128, 4, 1, 4, 1, false, false, T>>(params, stream, configure);','line_number':205,'multiline':False]['text':' M=128, N=64 is quite slow, I think because we need to read/write dQaccum twice as many times','line_number':206,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 2, 2, 2, false, T>>(params, stream, configure);','line_number':207,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, false, T>>(params, stream, configure);','line_number':208,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 4, false, T>>(params, stream, configure);','line_number':209,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 4, 4, 2, 4, false, false, T>>(params, stream, configure);','line_number':211,'multiline':False]['text':' printf("max_smem_per_block = %d\n", max_smem_per_block);','line_number':225,'multiline':False]['text':' if (params.h == params.h_k) {','line_number':227,'multiline':False]['text':' 92KB','line_number':229,'multiline':False]['text':' 116 KB','line_number':231,'multiline':False]['text':' This is faster for dropout since we don't have many registers to spare','line_number':232,'multiline':False]['text':' } else {','line_number':238,'multiline':False]['text':' run_flash_bwd_seqq_parallel<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 4, 4, false, false, T>>(params, stream, configure);','line_number':239,'multiline':False]['text':' }','line_number':240,'multiline':False]['text':' printf("max_smem_per_block = %d\n", max_smem_per_block);','line_number':255,'multiline':False]['text':' if (params.h == params.h_k) {','line_number':257,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 32, 128, 8, 2, 2, 2, false, false, T>>(params, stream, configure);','line_number':258,'multiline':False]['text':' This is faster, in the case of sequence-parallel bwd (where we need fewer registers).','line_number':259,'multiline':False]['text':' Out of these three, the 2nd one is slightly faster (2% faster than the first). Idk why.','line_number':260,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 2, 2, false, false, T>>(params, stream, configure);','line_number':261,'multiline':False]['text':' run_flash_bwd_seqk_parallel<Flash_bwd_kernel_traits<Headdim, 128, 128, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':264,'multiline':False]['text':' run_flash_bwd_seqk_parallel<Flash_bwd_kernel_traits<Headdim, 128, 128, 8, 4, 4, 4, false, true, T>, Is_dropout>(params, stream, configure);','line_number':265,'multiline':False]['text':' run_flash_bwd_seqq_parallel<Flash_bwd_kernel_traits<Headdim, 128, 128, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);','line_number':266,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 4, 2, true, false, T>, Is_dropout>(params, stream, configure);','line_number':267,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);','line_number':268,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 2, 2, true, false, T>, Is_dropout>(params, stream, configure);','line_number':269,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);','line_number':271,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 128, 8, 2, 4, 4, false, false, T>>(params, stream, configure);','line_number':274,'multiline':False]['text':' run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 4, 4, false, false, T>>(params, stream, configure);','line_number':276,'multiline':False]['text':' } else {','line_number':277,'multiline':False]['text':' run_flash_bwd_seqq_parallel<Flash_bwd_kernel_traits<Headdim, 128, 64, 8, 4, 4, 4, false, false, T>>(params, stream, configure);','line_number':278,'multiline':False]['text':' }','line_number':279,'multiline':False]['text':' H100','line_number':343,'multiline':False]['text':' A100, we don't do double buffering to save smem','line_number':345,'multiline':False]['text':' namespace pytorch_fmha','line_number':352,'multiline':False]