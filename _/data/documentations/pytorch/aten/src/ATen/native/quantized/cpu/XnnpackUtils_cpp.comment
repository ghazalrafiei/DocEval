['text':' NCHW -> NHWC','line_number':17,'multiline':False]['text':' 0123 -> 0231','line_number':18,'multiline':False]['text':' H ','line_number':19,'multiline':True]['text':' W ','line_number':20,'multiline':True]['text':' C ','line_number':21,'multiline':True]['text':' NCDHW -> NDHWC','line_number':23,'multiline':False]['text':' 01234 -> 02341','line_number':24,'multiline':False]['text':' D ','line_number':25,'multiline':True]['text':' H ','line_number':26,'multiline':True]['text':' W ','line_number':27,'multiline':True]['text':' C ','line_number':28,'multiline':True]['text':'
 * Stolen from fbgemm_utils::ConvertConvWeightsToChannelLastTensor to avoid
 * dependence on USE_FBGEMM. Reorder weights to the format xnnpack expects.
 * TODO: add a 3d variant.
 ','line_number':59,'multiline':True]['text':' 2D conv transpose weight transform','line_number':70,'multiline':False]['text':' IC OC/G KH KW -> G OC/G KH KW IC/G','line_number':71,'multiline':False]['text':' 2d conv weight transform','line_number':82,'multiline':False]['text':' namespace xnnp_utils','line_number':85,'multiline':False]['text':' namespace native','line_number':86,'multiline':False]['text':' namespace at','line_number':87,'multiline':False]['text':' USE_XNNPACK','line_number':89,'multiline':False]