['text':' FlashAttention Specific Imports','line_number':66,'multiline':False]['text':' MemoryEfficient Attention Specific Imports','line_number':70,'multiline':False]['text':' [B, T, 3 * D]','line_number':87,'multiline':False]['text':' [3 * D]','line_number':89,'multiline':False]['text':' [3, B, NH, T, DH]','line_number':91,'multiline':False]['text':' warp per DH.','line_number':94,'multiline':False]['text':' so launch B * NH * T warps.','line_number':95,'multiline':False]['text':' Here we require D % VEC == 0 for these vectorized loads.','line_number':119,'multiline':False]['text':' TODO: specialize for float2half2/half2float2?','line_number':135,'multiline':False]['text':' Here we require DH % VEC == 0 for these vectorized stores.','line_number':149,'multiline':False]['text':' Same as above, but we can't vectorize memory access.','line_number':158,'multiline':False]['text':' [B, T, 3 * D], but it's a NestedTensor buffer','line_number':188,'multiline':False]['text':' [3 * D]','line_number':190,'multiline':False]['text':' [3, B, NH, T, DH]','line_number':194,'multiline':False]['text':' warp per DH.','line_number':197,'multiline':False]['text':' so launch B * NH * T warps.','line_number':198,'multiline':False]['text':' Here we require D % VEC == 0 for these vectorized loads.','line_number':231,'multiline':False]['text':' TODO: specialize for float2half2/half2float2?','line_number':248,'multiline':False]['text':' Here we require DH % VEC == 0 for these vectorized stores.','line_number':308,'multiline':False]['text':' namespace','line_number':362,'multiline':False]['text':' compute q = (q + q_bias) / sqrt(dim_per_head), k = k + k_bias, v = v + v_bias','line_number':363,'multiline':False]['text':' TODO: calculate this without the std::vector -- NestedTensor_to_mask wants','line_number':371,'multiline':False]['text':' this too','line_number':372,'multiline':False]['text':' Don't mess with non-nested case for now since it's not set up to fiddle','line_number':377,'multiline':False]['text':' with mask size.','line_number':378,'multiline':False]['text':' Round T up to next multiple of 8 so as to be able to utilize Tensor','line_number':380,'multiline':False]['text':' cores. Otherwise, sometimes with padding, *no* row will have the maximum','line_number':381,'multiline':False]['text':' sequence length and so we'll have a non-divisible-by-8 dimension even if','line_number':382,'multiline':False]['text':' the model author chose a multiple of 8.','line_number':383,'multiline':False]['text':' query shape: [B, T, D]','line_number':481,'multiline':False]['text':' qkv_weight shape: [3 * D, D]','line_number':482,'multiline':False]['text':' We have not done linear projection yet but the input for SDP','line_number':544,'multiline':False]['text':' Is expected to be 4 dimensional. We "cheaply" create view tensors','line_number':545,'multiline':False]['text':' That will then be used for checking hot path conditions with select_sd_backend','line_number':546,'multiline':False]['text':' strides from packed projection for nested tensors when seq_len is 1 will be','line_number':553,'multiline':False]['text':' and will trigger a contiguous call in the kernel, so we prevent this','line_number':554,'multiline':False]['text':' The API for transfomer_encoder is a mask of shape (Batch_Size, Seq_len_q)','line_number':556,'multiline':False]['text':' For mem-eff attention this will cause the expand call to error','line_number':557,'multiline':False]['text':' For now I am going to turn of that path not have to deal with all the annoying','line_number':558,'multiline':False]['text':' Mask type shape grossness','line_number':559,'multiline':False]['text':' Returned math or error lets not use it','line_number':579,'multiline':False]['text':' shape: [B, T, 3 x D]','line_number':582,'multiline':False]['text':' shape: 3 x [B, num_head, T, dim_per_head]','line_number':606,'multiline':False]['text':' Not used any more, allow free','line_number':608,'multiline':False]['text':' shape: [B, num_head, T, T]','line_number':623,'multiline':False]['text':' q & k are dead but cannot be freed because they were packed with v','line_number':625,'multiline':False]['text':' shape: [B, num_head, T, T]','line_number':633,'multiline':False]['text':' TODO: long-term, have a kernel that works with','line_number':634,'multiline':False]['text':' NestedTensor directly if there is no mask passed','line_number':635,'multiline':False]['text':' shape: [B, num_head, T, dim_per_head]','line_number':641,'multiline':False]['text':' reuse storage for q; we're done with it','line_number':642,'multiline':False]['text':' qkv is not dead; we just reused storage for q!','line_number':644,'multiline':False]['text':' shape: [B, T, D]','line_number':655,'multiline':False]['text':' Fuse transform_0213 inside','line_number':656,'multiline':False]['text':' weights are not needed for full transformer, so don't worry too','line_number':663,'multiline':False]['text':' much about performance -- we implement this just to make use','line_number':664,'multiline':False]['text':' cases that don't disable need_weights still get some speedup.','line_number':665,'multiline':False]['text':' Used for tracking usage statistics','line_number':679,'multiline':False]['text':' Query (Batch x Num_heads x Q_seq_len  x Dim_per_head)','line_number':681,'multiline':False]['text':' Key   (Batch x Num_heads x KV_seq_len x Dim_per_head)','line_number':682,'multiline':False]['text':' Value (Batch x Num_heads x KV_seq_len x Dim_per_head)','line_number':683,'multiline':False]['text':' Query -> Query(Batch x Q_seq_len  x Num_heads x Dim_per_head)','line_number':692,'multiline':False]['text':' Key   -> Key  (Batch x KV_seq_len x Num_heads x Dim_per_head)','line_number':693,'multiline':False]['text':' Value -> Value(Batch x KV_seq_len x Num_heads x Dim_per_head)','line_number':694,'multiline':False]['text':' Reshape output to convert nnz to batch_size and seq_len','line_number':717,'multiline':False]['text':' Used for tracking usage statistics','line_number':732,'multiline':False]['text':' Query -> Query(Batch x Q_seq_len x Num_heads x Dim_per_head)','line_number':734,'multiline':False]['text':' Key   -> Key(Batch x KV_seq_len x Num_heads x Dim_per_head)','line_number':735,'multiline':False]['text':' Value -> Value(Batch x KV_seq_len x  Num_heads x Dim_per_head)','line_number':736,'multiline':False]['text':' This can be used when your sequence length k is not the full extent','line_number':793,'multiline':False]['text':' of the tensor. This is useful for kv cache scenarios but for now','line_number':794,'multiline':False]['text':' we will not support in this PR.','line_number':795,'multiline':False]['text':' We are going to have two paths:','line_number':798,'multiline':False]['text':' 1. The standard MHA path for dense tensors','line_number':799,'multiline':False]['text':' 2. The Varseqlen path','line_number':800,'multiline':False]['text':'seqused_k','line_number':824,'multiline':True]['text':'zero_tensors','line_number':829,'multiline':True]['text':'window_size_left','line_number':831,'multiline':True]['text':'window_size_right','line_number':832,'multiline':True]['text':'gen_','line_number':834,'multiline':True]['text':'window_size_left','line_number':853,'multiline':True]['text':'window_size_right','line_number':854,'multiline':True]['text':'return_softmax (this is used for testing)','line_number':855,'multiline':True]['text':' [b, seqlen, num_heads, K]','line_number':878,'multiline':False]['text':' [b, seqlen, num_heads, K]','line_number':879,'multiline':False]['text':' [b, seqlen, num_heads, Kv]','line_number':880,'multiline':False]['text':' [b, num_heads, seqlen, seqlen]','line_number':881,'multiline':False]['text':' (Mode 1MHK only) [b+1]: cu_seqlens_q[b] contains the','line_number':882,'multiline':False]['text':' position of the first query token for batch $b','line_number':883,'multiline':False]['text':' (Mode 1MHK only) [b+1]: cu_seqlen_k[b] contains the','line_number':885,'multiline':False]['text':' position of the first key token for batch $b','line_number':886,'multiline':False]['text':' (Mode 1MHK only) Maximum sequence length across batches','line_number':888,'multiline':False]['text':' attention matrix dropout probability','line_number':891,'multiline':False]['text':' TODO In theory it is possible to compile with _CUDA_ARCH < 5.0 and run on a','line_number':898,'multiline':False]['text':' machine that is >= 5.0. In practice, this is not a problem but since','line_number':899,'multiline':False]['text':' this would avoid runtime architecture checks, we should look into it','line_number':900,'multiline':False]['text':' Batch sizes','line_number':906,'multiline':False]['text':' Sequence length','line_number':910,'multiline':False]['text':' Num heads','line_number':913,'multiline':False]['text':' Embedding per head','line_number':917,'multiline':False]['text':' TODO: is this actually being set inside the kernel anywhere?','line_number':932,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/115590s','line_number':933,'multiline':False]['text':' Note [Seed and Offset Device]','line_number':959,'multiline':False]['text':' If we are currently in graph capture mode, we need to create the seed and offset tensors on the device.','line_number':960,'multiline':False]['text':' This is necessary for CUDA graph-safe random number generation, which requires the seed and offset tensors','line_number':961,'multiline':False]['text':' to be single element tensors on device. During graph capture, when the seed and offset tensors are passed','line_number':962,'multiline':False]['text':' the pointers act as scratch space for storing the RNG state for the backwards pass.','line_number':963,'multiline':False]['text':' When calling backwards, we either construct a PhiloxState with the pointers or the actual values.','line_number':964,'multiline':False]['text':' For more information on CUDA graph-safe RNG states, see Note [CUDA Graph-safe RNG states].','line_number':965,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':975,'multiline':False]['text':' if using dropout, we produce 1 random number for each element of the','line_number':977,'multiline':False]['text':' attention tensor','line_number':978,'multiline':False]['text':' The seed and offset will be populated by the kernel','line_number':982,'multiline':False]['text':' Not using dropout','line_number':993,'multiline':False]['text':' Check if this kernel is compatible','line_number':1012,'multiline':False]['text':' Alignment','line_number':1023,'multiline':False]['text':' Uses too much shmem','line_number':1029,'multiline':False]['text':' NOTE: Should be aligned (by padding) in case M is','line_number':1041,'multiline':False]['text':' not a good number for loading during backward','line_number':1042,'multiline':False]['text':' Dispatch to the right kernel','line_number':1158,'multiline':False]['text':' TODO: why isn't this being set in the kernel?','line_number':1171,'multiline':False]['text':'*
 * simple kernel that populates a tensor with rand uniform values.
 * currently only used for testing purposes, not much attention
 * is paid to performance.
 *
 * problem is partitioned as follows:
 * - (batch, head) is given by block coordinates
 * - each thread handles a row for a given (batch, head)
 ','line_number':1187,'multiline':True]['text':' namespace','line_number':1234,'multiline':False]['text':'*
 * fill tensor with random uniform values. only used for testing, not much
 * attention is paid to performance
 ','line_number':1236,'multiline':True]['text':' namespace native','line_number':1273,'multiline':False]['text':' namespace at','line_number':1274,'multiline':False]