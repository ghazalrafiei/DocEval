['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':67,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':69,'multiline':False]['text':' mini-batch doesn't have any impact on how we pack weights','line_number':75,'multiline':False]['text':' so we pass it as 1','line_number':76,'multiline':False]['text':' Input image height/width also don't have any impact on how we pack','line_number':77,'multiline':False]['text':' weights so we can pass any values','line_number':78,'multiline':False]['text':' dummy batch size','line_number':81,'multiline':False]['text':' dummy image size','line_number':84,'multiline':False]['text':' FBGEMM expects weights to be in channels last','line_number':111,'multiline':False]['text':' TODO: Change this when ChannelsLast3d is ready.','line_number':112,'multiline':False]['text':' FBGEMM needs G OC/G kDim0 ... kDimN IC/G','line_number':113,'multiline':False]['text':' for both conv and conv transpose','line_number':114,'multiline':False]['text':' but PyTorch lays them out as {out_c, in_c/groups, kH, kW}','line_number':115,'multiline':False]['text':' (or for ConvTranspose {in_c, out_c/groups, kH, kW})','line_number':116,'multiline':False]['text':' compute column offsets (Similar to','line_number':122,'multiline':False]['text':' fbgemm::col_offsets_with_zero_pt_s8acc32_ref) please note that offsets','line_number':123,'multiline':False]['text':' include the sum of columns as well as the scalar term weight_zero_point *','line_number':124,'multiline':False]['text':' KDim','line_number':125,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':126,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':128,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':134,'multiline':False]['text':' USE_FBGEMM','line_number':191,'multiline':False]['text':' 1D is packed as 2d, hence we don't need other checks','line_number':207,'multiline':False]['text':' QNNPACK expects weights to be of the format {out_c, kH, kW, in_c/groups},','line_number':234,'multiline':False]['text':' but PyTorch lays them out as {out_c, in_c/groups, kH, kW}','line_number':235,'multiline':False]['text':' (or for ConvTranspose {in_c, out_c/groups, kH, kW})','line_number':236,'multiline':False]['text':' We set the pre-packed conv weights to nullptr below as we call pre-pack','line_number':290,'multiline':False]['text':' during the first invocation of operator run. Refer to qconv.cpp for more','line_number':291,'multiline':False]['text':' details. TODO Update to actually call pre-pack here once bias is removed','line_number':292,'multiline':False]['text':' from pre-packing step.','line_number':293,'multiline':False]['text':' PrePackConvWeights ','line_number':295,'multiline':True]['text':' int8_t weight ','line_number':296,'multiline':True]['text':' fp32 bias ','line_number':297,'multiline':True]['text':' input_scale ','line_number':304,'multiline':True]['text':' USE_PYTORCH_QNNPACK','line_number':325,'multiline':False]['text':' Weight','line_number':363,'multiline':False]['text':' Format: [OC IC//group KH KW] for conv; [IC OC//group KH KW] for deconv','line_number':364,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':385,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':404,'multiline':False]['text':' Set runtime src zero point','line_number':413,'multiline':False]['text':' zero_points_mask= ','line_number':414,'multiline':True]['text':' template args: <(src/dst) is_channels_last, transposed>','line_number':421,'multiline':False]['text':' convolution_transpose_forward::expected_weights_desc() gives format [i, o, ...],','line_number':427,'multiline':False]['text':' but ONEDNN requires [o, i, ...] for computation','line_number':428,'multiline':False]['text':' for permutation of weight','line_number':431,'multiline':False]['text':'is_channels_last=','line_number':440,'multiline':True]['text':' Scales are needed for feed_from().','line_number':453,'multiline':False]['text':' Also for feed_from()','line_number':456,'multiline':False]['text':' expect wgt to be in [OC IC KH KW] format','line_number':457,'multiline':False]['text':' Bias','line_number':462,'multiline':False]['text':' Return the packed weight as Mkldnn Tensor','line_number':494,'multiline':False]['text':' from CPU backend instead of QuantizedCPU','line_number':496,'multiline':False]['text':' Weight zero points must be 0 for onednn','line_number':497,'multiline':False]['text':' N, C, L -> N, C, 1, L','line_number':526,'multiline':False]['text':' Weight is quant per tensor, then weight_scales will be a scalar Tensor','line_number':547,'multiline':False]['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':554,'multiline':False]['text':' Weight is quant per channel','line_number':559,'multiline':False]['text':' src_scales_mask= ','line_number':572,'multiline':True]['text':' src_zero_points_mask= ','line_number':575,'multiline':True]['text':'is_channels_last=','line_number':587,'multiline':True]['text':' Note: Weight in Conv1D will unsqueeze into Conv2D in previous step','line_number':589,'multiline':False]['text':' Scales are needed for feed_from().','line_number':603,'multiline':False]['text':' Also for feed_from()','line_number':607,'multiline':False]['text':'transposed','line_number':608,'multiline':True]['text':' expect wgt to be in [OC IC KH KW] format','line_number':608,'multiline':False]['text':' #if AT_MKLDNN_ENABLED()','line_number':618,'multiline':False]['text':'transpose=','line_number':640,'multiline':True]['text':'transpose=','line_number':652,'multiline':True]['text':' x86','line_number':678,'multiline':False]['text':' defined(USE_FBGEMM) || AT_MKLDNN_ENABLED()','line_number':679,'multiline':False]['text':'transpose=','line_number':725,'multiline':True]['text':'transpose=','line_number':737,'multiline':True]['text':' x86','line_number':774,'multiline':False]['text':' from CPU backend instead of QuantizedCPU','line_number':811,'multiline':False]['text':' Weight zero points must be 0s for onednn','line_number':812,'multiline':False]['text':' Conv','line_number':831,'multiline':False]['text':' conv_prepack is deprecated, please use conv2d_prepack for 2D conv.','line_number':832,'multiline':False]['text':' ConvTranspose','line_number':837,'multiline':False]['text':' Conv','line_number':844,'multiline':False]['text':' ConvTranspose','line_number':847,'multiline':False]['text':' New OP definition for Quantization in PyTorch 2.0 Export','line_number':854,'multiline':False]['text':' Conv Prepack','line_number':855,'multiline':False]['text':' namespace','line_number':859,'multiline':False]['text':' namespace native','line_number':860,'multiline':False]['text':' namespace at','line_number':861,'multiline':False]