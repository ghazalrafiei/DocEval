['text':'correction=','line_number':44,'multiline':True]['text':'take_sqrt=','line_number':44,'multiline':True]['text':' There will be a warning if we declare a __shared__ WelfordType array.','line_number':53,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/13967','line_number':54,'multiline':False]['text':'identity_element=','line_number':62,'multiline':True]['text':' Accumulate each 32 cols into a 32 * 32 tile.','line_number':206,'multiline':False]['text':' Since the blockDim is (32, 16), accumulate twice for 1st and 2nd 16 rows','line_number':207,'multiline':False]['text':' of a 32 contiguous elements.','line_number':208,'multiline':False]['text':' Write accumulated tile to shared memory.','line_number':235,'multiline':False]['text':' Do warp reduce for the 1st 16 cols in the tile.','line_number':242,'multiline':False]['text':' Do warp reduce for the 2nd 16 cols in the tile.','line_number':259,'multiline':False]['text':' Accumulate each 32 cols into a 32 * 32 tile.','line_number':414,'multiline':False]['text':' Since the blockDim is (32, 16), accumulate twice for 1st and 2nd 16 rows','line_number':415,'multiline':False]['text':' of a 32 contiguous elements.','line_number':416,'multiline':False]['text':' Write accumulated tile to shared memory.','line_number':439,'multiline':False]['text':' Do warp reduce for the 1st 16 cols in the tile.','line_number':446,'multiline':False]['text':' Do warp reduce for the 2st 16 cols in the tile.','line_number':463,'multiline':False]['text':' TODO: Since there is some issues in gpu_kernel_multiple_outputs, we are','line_number':613,'multiline':False]['text':' using maunal kernel here. Make it using gpu_kernel_multiple_outputs once','line_number':614,'multiline':False]['text':' the issue fixed.','line_number':615,'multiline':False]['text':' The algorithm for colwise reduction here is to accumulate each 32 cols','line_number':774,'multiline':False]['text':' to a 32 * 32 tile and write the tile to shared memmory. Then do warp','line_number':775,'multiline':False]['text':' reduce for each col in the tile. So here the blockDim must be (32, 16).','line_number':776,'multiline':False]['text':' For small batch size, do colwise reduce directly.','line_number':931,'multiline':False]['text':' The algorithm for colwise reduction here is to accumulate each 32 cols','line_number':946,'multiline':False]['text':' to a 32 * 32 tile and write the tile to shared memmory. Then do warp','line_number':947,'multiline':False]['text':' reduce for each col in the tile. So here the blockDim must be (32, 16).','line_number':948,'multiline':False]['text':' namespace','line_number':991,'multiline':False]['text':' namespace at::native','line_number':996,'multiline':False]