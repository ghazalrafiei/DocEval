['text':'','line_number':29,'multiline':False]['text':' Convolution type classification','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':'','line_number':74,'multiline':False]['text':' Rearrangement functions for pre-packing','line_number':75,'multiline':False]['text':'','line_number':76,'multiline':False]['text':'
 * Rearranges a convolution weight tensor to a layout that can be used by
 * convolution compute shaders. The goal of this packing is to arrange the data
 * such that data access in the compute shader is as linear as possible. The
 * reasoning behind the packing pattern will be described in the shader kernel
 * code.
 *
 * To understand the transformations performed by this function, consider an
 * example input of size {11, 1, 3, 3}. The following transformations will
 * applied to this weight tensor:
 *
 * 1. First, apply padding to the N dims so that it is a multiple of 4.
 * In this case, 1 batch is added, producing a tensor of size {12,1,3,3}.
 *
 * 2. Next, flatten the last two dims of the tensor. This is done by reshaping
 * the tensor to size {12,1,9}.
 *
 * 3. Finally, we want to "fold" the batch dim into the channel dim. We start by
 * splitting the tensor along the N dim so that each split has 4 batches. This
 * is done by reshaping the tensor to size {3,4,1,9}.
 *
 * 4. Normally, we would be done, but we want to stack each back vertically.
 * This is done by permuting the N and C dims and reshaping the tensor to size
 * {4,3,9}.
 ','line_number':78,'multiline':True]['text':' Add padding to the N dimension so that it's a multiple of 4','line_number':113,'multiline':False]['text':' Flatten so the H and W dim are on one row','line_number':118,'multiline':False]['text':' Split batch dim to make groups of 4','line_number':121,'multiline':False]['text':' Permute the groups of 4 so they are arranged along the channel dim, then','line_number':125,'multiline':False]['text':' reshape to stack the resulting batches vertically','line_number':126,'multiline':False]['text':'
 * Rearranges a convolution weight tensor to a layout that can be used by
 * convolution compute shaders. The goal of this packing is to arrange the data
 * such that data access in the compute shader is as linear as possible. The
 * reasoning behind the packing pattern will be described in the shader kernel
 * code.
 *
 * To understand the transformations performed by this function, consider an
 * example input of size {10, 7, 3, 3}. The following transformations will
 * applied to this weight tensor:
 *
 * 1. First, apply padding to the N and C dims so that both are a multiple of 4.
 * In this case, 2 batches and 1 channel of padding are added, producing a
 * tensor of size {12,8,3,3}.
 *
 * 2. Next, split the tensor along the C dim so that each split has 4 channels.
 * This is done by reshaping the channel to have the size {12,2,(4,3,3)}. ()
 * brackets denote the size of the split.
 *
 * 3. For each split, we want to "fold" the C dim into the W dim. So suppose the
 * first rows at H=0 of the split has values
 *
 *    0,1,2 | 10,11,12 | 20,21,22 | 30,31,32
 *
 *    where | denotes a channel boundary, then the goal is to combine those rows
 * into one row with the values
 *
 *    0, 10, 20, 30, 1, 11, 21, 31, 2, 12, 22, 32
 *
 *    This is done in code by permuting and reshaping the tensor, producing a
 * tensor of size {12,2,(3,12)}.
 *
 * 4. Next, we want to stack the splits belonging to the same batch horizontally
 * which is done by swapping the C and H dims of the intermediate tensor and
 * reshaping to produce a tensor of size {12,3,24}.
 *
 * 5. Now we will repeat a similar process of "folding" the N dim into the C
 * dim. We start by splitting along the N dim so that each split has 4 batches.
 * To do this the tensor is reshaped to {3,4,3,24}.
 *
 * 6. Normally, we would be done but we also want to stack each batch on each
 * other vertically. Therefore final step is another permute swapping the N and
 * C dims and reshaping to the output shape of {4, 9, 24}.
 *
 * For transposed convolutions, there are some slight differences to reflect the
 * data access pattern in the shader. The first major difference is that the
 * weight tensor is flipped along the H and W dims. The second major difference
 * is that steps 3 and 4 are slightly different so that the splits are
 * interleaved.
 ','line_number':132,'multiline':True]['text':' Flip values along the H and W axes for transposed convolutions','line_number':185,'multiline':False]['text':' Add padding to the N and C dimensions so that it's a multiple of 4','line_number':198,'multiline':False]['text':' Split the C dim into groups of 4','line_number':207,'multiline':False]['text':' Collapse each group of 4 channels onto the width axis','line_number':212,'multiline':False]['text':' Next collapse each group of four onto the width axis','line_number':214,'multiline':False]['text':' For tconv, do the same thing as above but we want to interleave batches','line_number':218,'multiline':False]['text':' of 4 from each of the channels','line_number':219,'multiline':False]['text':' Next reshape to combine the last two dims into a single row','line_number':221,'multiline':False]['text':' Split the N dim into groups of 4','line_number':225,'multiline':False]['text':' Collapse the outermost dim so that each group of 4 is stacked vertically','line_number':229,'multiline':False]['text':'
 * Rearranges a convolution weight tensor to a layout that can be used by
 * convolution compute shaders. The goal of this packing is to arrange the data
 * such that data access in the compute shader is as linear as possible. The
 * reasoning behind the packing pattern will be described in the shader kernel
 * code.
 *
 * The rearrangement structure is quite straightforward. Essentially we are
 * taking each texel and arranging them along the x axis.
 ','line_number':235,'multiline':True]['text':' If optional is empty, just return zeros','line_number':249,'multiline':False]['text':' Bias should just be a 1D tensor','line_number':261,'multiline':False]['text':' Add padding so that the length is a multiple of 4','line_number':266,'multiline':False]['text':' Reshape + permute to group every 4 consecutive elements along the same','line_number':270,'multiline':False]['text':' channel','line_number':271,'multiline':False]['text':'','line_number':279,'multiline':False]['text':' Shader and Workgroup size determination','line_number':280,'multiline':False]['text':'','line_number':281,'multiline':False]['text':' todo fail for quantized transposed conv','line_number':308,'multiline':False]['text':' 1x1 refers to the output tile size','line_number':326,'multiline':False]['text':' 1x1 refers to the output tile size','line_number':331,'multiline':False]['text':'','line_number':342,'multiline':False]['text':' Op Recording','line_number':343,'multiline':False]['text':'','line_number':344,'multiline':False]['text':'reverse=','line_number':385,'multiline':True]['text':'reverse=','line_number':387,'multiline':True]['text':'reverse=','line_number':388,'multiline':True]['text':'reverse=','line_number':389,'multiline':True]['text':' shader descriptor','line_number':395,'multiline':False]['text':' pipeline barrier','line_number':397,'multiline':False]['text':' global work group size','line_number':399,'multiline':False]['text':' local work group size','line_number':401,'multiline':False]['text':' fence handle','line_number':403,'multiline':False]['text':' shader arguments','line_number':405,'multiline':False]['text':' params buffer','line_number':413,'multiline':False]['text':'reverse=','line_number':470,'multiline':True]['text':'reverse=','line_number':472,'multiline':True]['text':'reverse=','line_number':473,'multiline':True]['text':'reverse=','line_number':474,'multiline':True]['text':' shader descriptor','line_number':480,'multiline':False]['text':' pipeline barrier','line_number':482,'multiline':False]['text':' global work group size','line_number':484,'multiline':False]['text':' local work group size','line_number':486,'multiline':False]['text':' fence handle','line_number':488,'multiline':False]['text':' shader arguments','line_number':490,'multiline':False]['text':' params buffer','line_number':498,'multiline':False]['text':' namespace conv2d','line_number':502,'multiline':False]['text':'
 * Computes the size of the overlay region when computing a convolution output.
 ','line_number':574,'multiline':True]['text':' output_padding ','line_number':669,'multiline':True]['text':' namespace','line_number':839,'multiline':False]['text':' This implementation only cover a special case. It only supports','line_number':843,'multiline':False]['text':' input = (n=1, input_channel, lengths)','line_number':844,'multiline':False]['text':' ouput = (n=1, output_channel, lengths - kernel_size + 1)','line_number':845,'multiline':False]['text':' stride=1, padding=0, dilation=1, groups=input_channels=output_channels','line_number':846,'multiline':False]['text':'','line_number':847,'multiline':False]['text':' Hence:','line_number':848,'multiline':False]['text':' weight's shape should be (output_channel, 1, kernel_size)','line_number':849,'multiline':False]['text':' bias's shape (if applicable) should be (output_channel,)','line_number':850,'multiline':False]['text':'','line_number':851,'multiline':False]['text':' In this implementation, it reduces to running a 1d convolution for reach','line_number':852,'multiline':False]['text':' channel.','line_number':853,'multiline':False]['text':' There are multiple perf improvement opportunities: e.g. width-packing','line_number':854,'multiline':False]['text':' input and weight tensors, batch reading when groups is low.','line_number':855,'multiline':False]['text':' shader descriptor','line_number':929,'multiline':False]['text':' pipeline barrier','line_number':931,'multiline':False]['text':' global work group size','line_number':933,'multiline':False]['text':' local work group size','line_number':935,'multiline':False]['text':' fence handle','line_number':937,'multiline':False]['text':' shader arguments','line_number':939,'multiline':False]['text':' params buffer','line_number':947,'multiline':False]['text':' namespace conv1d','line_number':953,'multiline':False]['text':' transposed = ','line_number':1066,'multiline':True]['text':' quantized = ','line_number':1067,'multiline':True]['text':' output_padding_arg = ','line_number':1068,'multiline':True]['text':' transposed = ','line_number':1090,'multiline':True]['text':' quantized = ','line_number':1091,'multiline':True]['text':' transposed = ','line_number':1113,'multiline':True]['text':' quantized = ','line_number':1114,'multiline':True]['text':' output_padding_arg = ','line_number':1115,'multiline':True]['text':' Validate input tensor is a Vulkan tensor, then convert to vTensor','line_number':1128,'multiline':False]['text':' Extract everything from the PackedContext','line_number':1132,'multiline':False]['text':' Backwards compatibility ','line_number':1287,'multiline':True]['text':' quantized = ','line_number':1309,'multiline':True]['text':' transposed = ','line_number':1352,'multiline':True]['text':' output_padding = ','line_number':1353,'multiline':True]['text':' namespace ops','line_number':1370,'multiline':False]['text':' namespace vulkan','line_number':1371,'multiline':False]['text':' namespace native','line_number':1372,'multiline':False]['text':' namespace at','line_number':1373,'multiline':False]