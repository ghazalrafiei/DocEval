['text':'**************************************************************************************************
 * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
 *reserved. SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *POSSIBILITY OF SUCH DAMAGE.
 *
 *************************************************************************************************','line_number':1,'multiline':True]['text':'! \file
    \brief Inspired from
   "cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h" Loads tiles of GEMM
   operands from a RowMajor shared-memory layout into registers to use by A100
   TensorCores.

    The difference with "mma_tensor_op_tile_access_iterator.h" is that:
    (1) We use "ldmatrix" to load tiles, rather than manual loads (slightly
   faster) (2) We support to transpose the operand (eg read `A.transpose()` when
   the shared memory holds `A`)

    This is only implemented for the specific shapes.
','line_number':32,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':49,'multiline':False]['text':'/ Operand identity','line_number':55,'multiline':False]['text':'/ Data type of A elements','line_number':57,'multiline':False]['text':'/ Shape of tile to load (concept: MatrixShape)','line_number':63,'multiline':False]['text':'/ Operand tag','line_number':66,'multiline':False]['text':'/ Basic check','line_number':72,'multiline':False]['text':'/ Element type','line_number':77,'multiline':False]['text':'/ Layout of source tile','line_number':81,'multiline':False]['text':'/ Shape of one matrix product operation (concept: MatrixShape)','line_number':84,'multiline':False]['text':'/ Delta between *MMA operations (in units of *MMA operations, concept:','line_number':91,'multiline':False]['text':'/ MatrixShape)','line_number':92,'multiline':False]['text':'/ Number of participating threads','line_number':95,'multiline':False]['text':'/ TensorRef type for loading element from a tensor','line_number':98,'multiline':False]['text':'/ Index type','line_number':101,'multiline':False]['text':'/ Long Index type','line_number':104,'multiline':False]['text':'/ Coordinate for an element in the tensor','line_number':107,'multiline':False]['text':'/ Number of elements accessed per Shared Memory load','line_number':110,'multiline':False]['text':'','line_number':124,'multiline':False]['text':' Derived quantities','line_number':125,'multiline':False]['text':'','line_number':126,'multiline':False]['text':'/ Fragment object holding a thread's part of a tile','line_number':128,'multiline':False]['text':'/ Memory access type','line_number':135,'multiline':False]['text':' using AccessType = AlignedArray<Element, kElementsPerAccess>;','line_number':136,'multiline':False]['text':' Number of 32bits tiles to load per `ldmatrix`','line_number':144,'multiline':False]['text':'/ Underlying tensor reference','line_number':149,'multiline':False]['text':'/ Origin','line_number':152,'multiline':False]['text':'/ Iterations in a tile','line_number':155,'multiline':False]['text':'/ Constructor from TensorRef','line_number':159,'multiline':False]['text':' See also:','line_number':166,'multiline':False]['text':' https://docs.nvidia.com/cuda/archive/11.7.1/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-1688','line_number':167,'multiline':False]['text':' 16x8x8: kAccessesInner = 1 (1 ldmatrix.x4)','line_number':168,'multiline':False]['text':' 16x8x16: kAccessesInner = 2 (2 ldmatrix.x4)','line_number':169,'multiline':False]['text':' XXX: This is not tested or used','line_number':187,'multiline':False]['text':'/ Advances an iterator along logical dimensions of matrix in units of whole','line_number':213,'multiline':False]['text':'/ tiles','line_number':214,'multiline':False]['text':'/ Advances the iterator along the advance dimension','line_number':229,'multiline':False]['text':'/ increase iterations in a tile','line_number':241,'multiline':False]['text':'/ Loads a fragment from memory at the location pointed to by the iterator.','line_number':252,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':279,'multiline':False]['text':' namespace warp','line_number':281,'multiline':False]['text':' namespace gemm','line_number':282,'multiline':False]['text':' namespace cutlass','line_number':283,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':284,'multiline':False]