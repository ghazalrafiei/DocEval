['text':' this is safe, in reality 256 is our limit','line_number':31,'multiline':False]['text':' increasing block_stride to lower # of blocks launched','line_number':33,'multiline':False]['text':' kernels borrowed from Caffe','line_number':43,'multiline':False]['text':' -Infinity','line_number':65,'multiline':False]['text':' flattening cta for pre-computation & smem initialization;','line_number':98,'multiline':False]['text':' use shared memory to store temporary output value. This is simply to','line_number':102,'multiline':False]['text':' reduce register usage.','line_number':103,'multiline':False]['text':' namespace','line_number':292,'multiline':False]['text':' The backward kernel is launched on input instead output.','line_number':522,'multiline':False]['text':' If it is launched on output layer, atomic_add would not provide much benefit on FP16.','line_number':523,'multiline':False]['text':' Please check comments at https://github.com/pytorch/pytorch/pull/34519.','line_number':524,'multiline':False]['text':' at::native','line_number':567,'multiline':False]