['text':'calculate n,c,h,w indices, replacing modulos by divide and multiply add,','line_number':54,'multiline':False]['text':'result is same as would be in the code below','line_number':55,'multiline':False]['text':'const int n = linearIndex / batchStride; //batchStride = outputChannels * outputHeight * outputWidth','line_number':56,'multiline':False]['text':'const int c = (linearIndex / channelStride) % outputChannels; //channelStride = outputHeight * outputWidth','line_number':57,'multiline':False]['text':'const int h = (linearIndex / outputWidth) % outputHeight;','line_number':58,'multiline':False]['text':'const int w = linearIndex % outputWidth;','line_number':59,'multiline':False]['text':' Each Block is responsible for accumulating over a permutation of','line_number':193,'multiline':False]['text':' (channels x kH x kW), use blockIdx to determine which one','line_number':194,'multiline':False]['text':' Need to calculate which input channel is associated with this filter','line_number':200,'multiline':False]['text':' channel','line_number':201,'multiline':False]['text':' Use warp per item.  In the original kernel, a threadblock was used to sum over NHW.','line_number':210,'multiline':False]['text':' Here, we use a warp to sum values over HW dimension, and if batchSize is larger than the','line_number':211,'multiline':False]['text':' number of warps, a warp would loop over remaining batch items (e.g. if there are 8 warps,','line_number':212,'multiline':False]['text':' warp 0 would go over 0-8-16 etc image, warp 1 over 1-9-17 etc). Later in blockReduce,','line_number':213,'multiline':False]['text':' all the warps will be reduced anyway, thus the full reduction will be over NHW, like it','line_number':214,'multiline':False]['text':' should be. That allows to get rid of one modulo operation inside the loop (because n/batchIdx','line_number':215,'multiline':False]['text':' now does not have to be computed through modulo, you are just looping over it), and','line_number':216,'multiline':False]['text':' bring a nice speed-up.','line_number':217,'multiline':False]['text':' Warp-stride loop over elements in a batch item','line_number':219,'multiline':False]['text':' Need to calculate the following: batch position, and offset into the grad_output','line_number':221,'multiline':False]['text':' in height, and width. We can intuit the corresponding position in the input from','line_number':222,'multiline':False]['text':' the other parameters we have','line_number':223,'multiline':False]['text':' At this point each thread in the block has a local gradient, which we need to','line_number':239,'multiline':False]['text':' accumulate prior to writing the global value','line_number':240,'multiline':False]['text':' After reduction, first thread in the block has the gradient, so its responsible','line_number':245,'multiline':False]['text':' for writing it to grad_weight','line_number':246,'multiline':False]['text':' Only handle 4D Input Tensors for now','line_number':262,'multiline':False]['text':' We assume that the input and weight Tensors are shaped properly by','line_number':270,'multiline':False]['text':' the caller, so we verify that here to some extent','line_number':271,'multiline':False]['text':' Weight Tensor is shape (output_channels, 1, kH, kW)','line_number':273,'multiline':False]['text':' Input Tensor is shape (N, input_channels, H, W)','line_number':276,'multiline':False]['text':' We verify that the # of output_channels is a multiple of input_channels','line_number':277,'multiline':False]['text':' Bias has same # of channels as output','line_number':280,'multiline':False]['text':' Following the behavior of other THCUNN functions, we shape the output','line_number':284,'multiline':False]['text':' Tensor ourselves','line_number':285,'multiline':False]['text':' One thread per output value','line_number':299,'multiline':False]['text':' Create PackedTensorAccessor','line_number':309,'multiline':False]['text':' Kernel currently relies upon all the Tensors to be contiguous, but we made','line_number':310,'multiline':False]['text':' them contiguous above','line_number':311,'multiline':False]['text':' Only handle 4D Input Tensors for now','line_number':349,'multiline':False]['text':' Minimal shape checking, as above','line_number':354,'multiline':False]['text':' Same # of elements in batch','line_number':355,'multiline':False]['text':' Same # of filters as outputChannels','line_number':357,'multiline':False]['text':' Resize Grainput_a','line_number':360,'multiline':False]['text':' Kernel currently relies upon all the Tensors to be contiguous','line_number':375,'multiline':False]['text':' One thread per grainput_a value','line_number':380,'multiline':False]['text':' Crude benchmarks suggest 256 is better than 512 and 1024','line_number':447,'multiline':False]['text':' TODO: Autotune/use better heuristics, improve speed more.','line_number':448,'multiline':False]['text':'warp per item in a batch, up to a maximum','line_number':450,'multiline':False]['text':' Only handle 4D Input Tensors for now','line_number':463,'multiline':False]['text':' Minimal shape checking as above','line_number':467,'multiline':False]['text':' Same # of elements in batch','line_number':468,'multiline':False]['text':' Kernel currently relies upon all the Tensors to be contiguous','line_number':486,'multiline':False]['text':' We parallelize so that each block computes a single value in grad_weight','line_number':491,'multiline':False]['text':' Make sure we have enough threads to perform the reduction, and use this number','line_number':496,'multiline':False]['text':' to create the shared memory size for the reduction','line_number':497,'multiline':False]['text':' namespace (anonymous)','line_number':518,'multiline':False]['text':' namespace at::native','line_number':635,'multiline':False]