['text':' Note on naming: it is unconventional.','line_number':28,'multiline':False]['text':' grad_in does not mean that it is a gradient wrt to input,','line_number':29,'multiline':False]['text':' grad_in/grad_out is just an input/output of unfold_backward kernel.','line_number':30,'multiline':False]['text':' last dim stores the folds','line_number':40,'multiline':False]['text':' dictates the number of elements to iterate over','line_number':44,'multiline':False]['text':' in dimension `dim`','line_number':45,'multiline':False]['text':' prepare grad_out for TensorIterator { ','line_number':51,'multiline':True]['text':' } ','line_number':58,'multiline':True]['text':' prepare grad_in for TensorIterator { ','line_number':60,'multiline':True]['text':' set strides for dim to 0','line_number':64,'multiline':False]['text':' and size to 1 because','line_number':65,'multiline':False]['text':' this dimension is indexed inside the kernel','line_number':66,'multiline':False]['text':' } ','line_number':76,'multiline':True]['text':' During the TensorIterator iteration we have to know','line_number':78,'multiline':False]['text':' i_dim in grad_out[i_1,...,i_dim,...i_n],','line_number':79,'multiline':False]['text':' idx_dim stores this information','line_number':80,'multiline':False]['text':' prepare idx_dim for TensorIterator { ','line_number':81,'multiline':True]['text':' idx_dim size will broadcast over determined by grad_out sizes in TensorIterator','line_number':94,'multiline':False]['text':' } ','line_number':96,'multiline':True]['text':' namespace at::native','line_number':112,'multiline':False]