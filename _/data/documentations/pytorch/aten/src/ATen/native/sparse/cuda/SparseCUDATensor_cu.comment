['text':' NOTE: Since `coalesce` is not an in-place operation when `is_coalesced` is false,','line_number':46,'multiline':False]['text':' we should keep the original tensor intact and do coalesce on a copy of the tensor','line_number':47,'multiline':False]['text':' Replace instances with','line_number':57,'multiline':False]['text':' For indices, a simple sort + unique suffices','line_number':59,'multiline':False]['text':' For values, we use a custom kernel for segmented reduction (can't use Thrust due to indirection).','line_number':60,'multiline':False]['text':' indices will be modified by Thrust, so we have to clone or use new storage','line_number':66,'multiline':False]['text':' here.','line_number':67,'multiline':False]['text':' Fill sortedOrigIndices with sequential indices','line_number':79,'multiline':False]['text':' this forces device-host synchronization!','line_number':91,'multiline':False]['text':' If there is no values to copy, save running the kernel.','line_number':103,'multiline':False]['text':' is_cuda ','line_number':114,'multiline':True]['text':' this grid-strided version is slower but probably more flexible','line_number':128,'multiline':False]['text':' to different sizes','line_number':129,'multiline':False]['text':' int64_t blockX = min(stride, (int64_t) 512);','line_number':130,'multiline':False]['text':' dim3 block(blockX, 512 / blockX);','line_number':131,'multiline':False]['text':' int64_t grid = min((int64_t) 1024, ceil_div((int64_t) newNnz * stride, (int64_t) block.x * block.y));','line_number':132,'multiline':False]['text':' THCSTensor_coalesceValuesKernel_gridStrided<real, accreal><<<grid, block, 0, stream> >>(','line_number':133,'multiline':False]['text':'   THCIndexTensor_(data)(state, uniqueOffsets),','line_number':134,'multiline':False]['text':'   THCIndexTensor_(data)(state, origIndices),','line_number':135,'multiline':False]['text':'   THCTensor_(data)(state, values),','line_number':136,'multiline':False]['text':'   THCTensor_(data)(state, newValues),','line_number':137,'multiline':False]['text':'   nnz,','line_number':138,'multiline':False]['text':'   newNnz,','line_number':139,'multiline':False]['text':'   stride','line_number':140,'multiline':False]['text':' );','line_number':141,'multiline':False]['text':' C10_CUDA_KERNEL_LAUNCH_CHECK();','line_number':142,'multiline':False]['text':'//////////////////////////////////////////////////////////','line_number':144,'multiline':False]['text':' unflatten indices if necessary','line_number':145,'multiline':False]['text':' NB: Not a select, so I can preserve the outer dimension','line_number':152,'multiline':False]['text':'//////////////////////////////////////////////////////////','line_number':159,'multiline':False]['text':' We can use unsafe sparse tensor constructor because the indices do not','line_number':160,'multiline':False]['text':' need to be revalidated as we do not add or change indices, just remove','line_number':161,'multiline':False]['text':' duplicates.','line_number':162,'multiline':False]['text':' namespace at::native','line_number':169,'multiline':False]