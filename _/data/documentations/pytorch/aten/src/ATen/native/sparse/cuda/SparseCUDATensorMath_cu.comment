['text':' --------------------------------------------------------------------','line_number':60,'multiline':False]['text':' Utility functions','line_number':61,'multiline':False]['text':' --------------------------------------------------------------------','line_number':62,'multiline':False]['text':' NB: Deleted spaddcmul (aka addcmul_, but not actually wired up), spaddcdiv (not','line_number':74,'multiline':False]['text':' wired at all)','line_number':75,'multiline':False]['text':' --------------------------------------------------------------------','line_number':86,'multiline':False]['text':' addmm(Tensor, SparseTensor, Tensor, Scalar, Scalar)  [broadcasts]','line_number':87,'multiline':False]['text':' --------------------------------------------------------------------','line_number':88,'multiline':False]['text':' no need to check dense_dim because dense_dim + sparse_dim = dim','line_number':100,'multiline':False]['text':' mxk * kxn = mxn','line_number':102,'multiline':False]['text':' NB: Purposely no broadcasting version of addmm inplace','line_number':174,'multiline':False]['text':' Deleted sspaddmm (sparse, dense) -> sparse','line_number':176,'multiline':False]['text':' --------------------------------------------------------------------','line_number':178,'multiline':False]['text':' hspmm(SparseTensor mat1, Tensor mat2)','line_number':179,'multiline':False]['text':' --------------------------------------------------------------------','line_number':180,'multiline':False]['text':' , const Scalar& alpha ','line_number':186,'multiline':True]['text':' create values in column-major format to avoid copying in spaddmm','line_number':218,'multiline':False]['text':' why does sparse need to be cloned? If this is really necessary maybe we','line_number':222,'multiline':False]['text':' need to fuse this with newCoalesce','line_number':223,'multiline':False]['text':' Save destination indices to output hybrid tensor','line_number':227,'multiline':False]['text':' Replace destination indices with 0, 1, 2, 3, ... and compute output values','line_number':229,'multiline':False]['text':' tensor with sparse * dense multiplication','line_number':230,'multiline':False]['text':'alpha','line_number':238,'multiline':True]['text':' --------------------------------------------------------------------','line_number':250,'multiline':False]['text':' add(Tensor, SparseTensor, Scalar)','line_number':251,'multiline':False]['text':'    formerly known as spcadd','line_number':252,'multiline':False]['text':' --------------------------------------------------------------------','line_number':253,'multiline':False]['text':' TODO benchmark to decide whether to remove this special case','line_number':315,'multiline':False]['text':' sparseElementwiseKernel needs values to be contiguous too','line_number':336,'multiline':False]['text':' --------------------------------------------------------------------','line_number':372,'multiline':False]['text':' add(SparseTensor, SparseTensor, Scalar)  [broadcasts]','line_number':373,'multiline':False]['text':' --------------------------------------------------------------------','line_number':374,'multiline':False]['text':' TODO: This test seems a bit goofy','line_number':383,'multiline':False]['text':' We deliberately choose to simply concat the indices and values tensors','line_number':406,'multiline':False]['text':' rather than merging them. This removes the need to synchronously fetch nnz','line_number':407,'multiline':False]['text':' at the end of the operation, at the cost of having a non-coalesced result.','line_number':408,'multiline':False]['text':' This trade-off is preferable for the common use-case of gradient accumulation.','line_number':409,'multiline':False]['text':' performs the addition under the common dtype.','line_number':429,'multiline':False]['text':' Prevent unbounded growth of nnz','line_number':439,'multiline':False]['text':' TODO: Improved heuristic on when to coalesce or remove need to coalesce','line_number':440,'multiline':False]['text':' --------------------------------------------------------------------','line_number':449,'multiline':False]['text':' mul(SparseTensor, SparseTensor)  [broadcasts]','line_number':450,'multiline':False]['text':' --------------------------------------------------------------------','line_number':451,'multiline':False]['text':' case mul(sparse, dense)','line_number':467,'multiline':False]['text':' case mul(dense, sparse)','line_number':471,'multiline':False]['text':' case mul(sparse, sparse) with a 0-dim input.','line_number':476,'multiline':False]['text':' mul(sparse, sparse)','line_number':488,'multiline':False]['text':' Short circuit when there is zero nnz.','line_number':490,'multiline':False]['text':' Not strictly necessary, but there are tests checking whether','line_number':491,'multiline':False]['text':' resize in mul fails if run on tensors coming from .data/.detach.','line_number':492,'multiline':False]['text':' --------------------------------------------------------------------','line_number':500,'multiline':False]['text':' sparse.sum() backward','line_number':501,'multiline':False]['text':'','line_number':502,'multiline':False]['text':' see NOTE [ sparse.sum() backward ]','line_number':503,'multiline':False]['text':' --------------------------------------------------------------------','line_number':504,'multiline':False]['text':' Short circuit if grad is either zero or empty','line_number':546,'multiline':False]['text':' remove nnz dim','line_number':587,'multiline':False]['text':' -1 since grad has no nnz dim','line_number':588,'multiline':False]['text':' convert to grad dtype','line_number':592,'multiline':False]['text':' update nnz','line_number':605,'multiline':False]['text':' get 1D indices','line_number':625,'multiline':False]['text':' flatten indices on all sparse_dim of grad, output indices is coalesced and sorted','line_number':629,'multiline':False]['text':' store lower_bound of input indices at grad indices','line_number':634,'multiline':False]['text':' config to run cuda kernel','line_number':642,'multiline':False]['text':' Search through a 1D tensor of sorted sparse matrix','line_number':717,'multiline':False]['text':' indices to find the end index for each matrix','line_number':718,'multiline':False]['text':' linux cuda >= 10.1 or windows cuda >= 11.0','line_number':760,'multiline':False]['text':' If the result tensor is contiguous, we can just write results directly to it.','line_number':784,'multiline':False]['text':' Otherwise, we'll need to write results to a temp buffer and then copy.','line_number':785,'multiline':False]['text':' Dense matrices have to be contiguous for cusparseSpMM to work','line_number':794,'multiline':False]['text':' First need to coalesce to get all of the first dimension indices','line_number':798,'multiline':False]['text':' in order since we'll be sending each matrix into the MM operation','line_number':799,'multiline':False]['text':' Need to convert dim1 and dim2 indices to 32-bit since cusparseSpMM','line_number':808,'multiline':False]['text':' only supports 32-bit indices','line_number':809,'multiline':False]['text':' Need a pointer to an array to access within a lambda','line_number':828,'multiline':False]['text':' See Note [Enabling Deterministic Operations]','line_number':840,'multiline':False]['text':' Iterate through each set of 2D matrices within the 3D','line_number':844,'multiline':False]['text':' tensor inputs, performing a matrix multiply with each','line_number':845,'multiline':False]['text':' Create tensors to view just the current set of matrices','line_number':865,'multiline':False]['text':' Need to transpose the result matrices since cusparse stores','line_number':953,'multiline':False]['text':' them in column-major order in memory','line_number':954,'multiline':False]['text':' namespace at::native','line_number':964,'multiline':False]