['text':' TODO: support noncontiguous case','line_number':26,'multiline':False]['text':' error out for now','line_number':27,'multiline':False]['text':' We check check the second dimension for linear because it transposes before matrix multiply','line_number':40,'multiline':False]['text':' namespace','line_number':57,'multiline':False]['text':' Now the last entry in every row of new_sizes should be weight_size_1.','line_number':71,'multiline':False]['text':' Now the last entry in every row of new_sizes should be other_size_1.','line_number':85,'multiline':False]['text':' Interesting case: alpha * NT * T + beta * T','line_number':97,'multiline':False]['text':'non-blocking=','line_number':190,'multiline':True]['text':'mask type ','line_number':191,'multiline':True]['text':' NestedTensor_to_mask produces a BxT mask','line_number':191,'multiline':False]['text':' TODO: port optimization for 1x1 tensors from','line_number':225,'multiline':False]['text':' pytorch/nestedtensor's version.','line_number':226,'multiline':False]['text':' Shape: # of tensors in our NestedTensor by max size along first dim','line_number':232,'multiline':False]['text':' TODO: calculate this without allocating a std::vector.','line_number':233,'multiline':False]['text':' namespace native','line_number':249,'multiline':False]['text':' namespace at','line_number':250,'multiline':False]