['text':' ForwardIt: only legacy random access iterator is supported.','line_number':33,'multiline':False]['text':' NOTE: std::distance(first, last) compiles but produces wrong results on CUDA,','line_number':39,'multiline':False]['text':' so only legacy random access iterators are safe in this code.','line_number':40,'multiline':False]['text':' avoiding std::advance(it, step),','line_number':46,'multiline':False]['text':' although it does work unlike std::distance on CUDA.','line_number':47,'multiline':False]['text':' The decision which separates finding a lower bound vs an upper bound.','line_number':49,'multiline':False]['text':' Note that a lower bound is a value at *it with the smallest index','line_number':50,'multiline':False]['text':' such that *it >= value if such value exists, or last if does not.','line_number':51,'multiline':False]['text':' Similarly, an upper bound is a value at *it with the smallest index','line_number':52,'multiline':False]['text':' such that *it > value if such value exists, or last if does not.','line_number':53,'multiline':False]['text':' Let is_lower = true and *it < value, then we know that *it and values','line_number':54,'multiline':False]['text':' preceeding *it cannot contain a lower bound, so we adjust initial iterator range','line_number':55,'multiline':False]['text':' from [first, first + count] to [first + step + 1, first + count - (step + 1)],','line_number':56,'multiline':False]['text':' where +1 skips the element at which we have just evaluated *it < value.','line_number':57,'multiline':False]['text':' Samilar logic holds when is_lower = false.','line_number':58,'multiline':False]['text':' keep nnz dim','line_number':86,'multiline':False]['text':' remove nnz dim for smooth broadcasting','line_number':88,'multiline':False]['text':' update nnz dim to be the lenght of an index','line_number':90,'multiline':False]['text':' The common dtype check is relevant when op is done in-place.','line_number':142,'multiline':False]['text':' This is because binary_of_t produces new values and it could be that','line_number':143,'multiline':False]['text':' new_values.dtype != res.dtype. In such a case we should error out','line_number':144,'multiline':False]['text':' as soon as possible to avoid redundant kernel runs.','line_number':145,'multiline':False]['text':' If the op and sum are not distributive, coalesce is required.','line_number':154,'multiline':False]['text':' No need to coalesce in such a case.','line_number':156,'multiline':False]['text':' Otherwise coalesce and force hash recompute.','line_number':160,'multiline':False]['text':' Given sparse tensors x and y we decide which one is source, and which one','line_number':170,'multiline':False]['text':' is probably_coalesced. The indices of both source and probably_coalesced are','line_number':171,'multiline':False]['text':' hashed and then the hash values of the source's indices are binary-searched','line_number':172,'multiline':False]['text':' into the hash values of the probably_coalesced's indices.','line_number':173,'multiline':False]['text':' If probably_coalesce is coalesced, by the property of the hashing method','line_number':174,'multiline':False]['text':' (see below), the hash values are already sorted and we can avoid any','line_number':175,'multiline':False]['text':' explicit sorting routines.','line_number':176,'multiline':False]['text':' Case 1: either x or y is coalesced.','line_number':180,'multiline':False]['text':' Case 2: Both x and y are either coalesced or non-coalesced.','line_number':186,'multiline':False]['text':' If both are coalesced, search into the larger tensor is faster.','line_number':187,'multiline':False]['text':' Same holds when both are non-coalesced.','line_number':188,'multiline':False]['text':' If under a uniform distribution it is likely to hit many elements in larger,','line_number':198,'multiline':False]['text':' it is best to coalesce it for better performance.','line_number':199,'multiline':False]['text':' If nnz > prod(larger.shape[:sparse_dim]), by the pidgeonhole principle,','line_number':206,'multiline':False]['text':' there is at least one bucket with nnz / prod(larger.shape[:sparse_dim]) elements.','line_number':207,'multiline':False]['text':' It provides a lower bound for the max count in the intersection.','line_number':208,'multiline':False]['text':' This condition is very conservative as we do not check whether such an event','line_number':209,'multiline':False]['text':' actually occurred, although it is very likely under a uniform distribution,','line_number':210,'multiline':False]['text':' the distribution with the highest uncertainty (maximizes entropy).','line_number':211,'multiline':False]['text':' coalesce invalidates hash values, so force-recompute','line_number':215,'multiline':False]['text':' The employed hash function maps a d-dim index to a linear offset','line_number':221,'multiline':False]['text':' into a contiguous memory that is sufficient to fit a dense tensor','line_number':222,'multiline':False]['text':' of shape broadcasted_shape(x.shape, y.shape), i.e.','line_number':223,'multiline':False]['text':' idx -> \sum_{i = 0}^d idx[i] * hash_coeffs[i], where','line_number':224,'multiline':False]['text':' hash_coeffs are the strides of a contiguous tensor of shape','line_number':225,'multiline':False]['text':' broadcasted_shape(x.shape, y.shape).','line_number':226,'multiline':False]['text':' Assuming the following order on the dimensions, i.e. the right-most dim is the','line_number':227,'multiline':False]['text':' fastest-changing dim, and the left-most is the slowest-changing dim,','line_number':228,'multiline':False]['text':' which is implicit in the definition of hash_coeffs,','line_number':229,'multiline':False]['text':' it could be shown that the hash function is actually bijective and, hence,','line_number':230,'multiline':False]['text':' is a perfect hash function (no collisions ever).','line_number':231,'multiline':False]['text':' Need owning storage in case of the Tensor class.','line_number':233,'multiline':False]['text':' non-const because of gcc-5/clang-5 issues','line_number':250,'multiline':False]['text':' Apply the hash function to probably_coalesced.indices','line_number':253,'multiline':False]['text':' probably_coalesced is coalesced and hash provided? Reuse it!','line_number':255,'multiline':False]['text':' non-const because of gcc-5/clang-5 issues','line_number':261,'multiline':False]['text':' NOTE: capture by value required by CUDA','line_number':277,'multiline':False]['text':' Now that we have hash values of probably_coalesced.indices,','line_number':293,'multiline':False]['text':' we need to decide whether they need to get sorted.','line_number':294,'multiline':False]['text':' The sort is not requires if probably_coalesced is coalesced.','line_number':295,'multiline':False]['text':' NOTE: argsort.dtype == nnz_arange.dtype','line_number':299,'multiline':False]['text':' NOTE: we want argsort.dtype == nnz_arange.dtype,','line_number':303,'multiline':False]['text':' but sort() produces indices of type int64_t,','line_number':304,'multiline':False]['text':' so we convert to nnz_arange.dtype to avoid issues','line_number':305,'multiline':False]['text':' with pointer types in the kernels below.','line_number':306,'multiline':False]['text':' Perform hash intersection.','line_number':313,'multiline':False]['text':' Let  s_hash = hash(source.indices),','line_number':314,'multiline':False]['text':'     pc_hash = hash(probably_coalesced.indices), then','line_number':315,'multiline':False]['text':' for i = 0, ..., len(s_hash) - 1:','line_number':316,'multiline':False]['text':'     lb = <index of a value in pc_hash[argsort_hash] which is a lower bound for s_hash[i]>,','line_number':317,'multiline':False]['text':'     up = <index of a value in pc_hash[argsort_hash] which is an upper bound for s_hash[i]>,','line_number':318,'multiline':False]['text':'     intersection_count[i] = up - lb','line_number':319,'multiline':False]['text':'     intersection_first_idx[i] = lb.','line_number':320,'multiline':False]['text':'','line_number':321,'multiline':False]['text':' intersection_count and intersection_first_idx are used to form indices at which','line_number':322,'multiline':False]['text':' intersection values are selected.','line_number':323,'multiline':False]['text':' non-const because of gcc-5/clang-5 issues','line_number':333,'multiline':False]['text':' Fusing hash computation with hash intersection.','line_number':358,'multiline':False]['text':' NOTE: capture by value required by CUDA','line_number':360,'multiline':False]['text':' Compute hash value','line_number':366,'multiline':False]['text':' Perform hash values intersection','line_number':375,'multiline':False]['text':'is_lower=','line_number':376,'multiline':True]['text':'is_lower=','line_number':382,'multiline':True]['text':' If op distributes with the sum, the arguments are processed as is,','line_number':429,'multiline':False]['text':' without the calls to coalesce().','line_number':430,'multiline':False]['text':' 8 sparse dims should be more than enough?','line_number':460,'multiline':False]['text':' COO indices are only 64-bit integers for now.','line_number':463,'multiline':False]['text':' For some reason MSVC complaints about passing constexpr max_sparse_dims','line_number':468,'multiline':False]['text':' as a template parameter claiming as if it is not know at compile time.','line_number':469,'multiline':False]['text':' anonymous namespace','line_number':479,'multiline':False]['text':' at::native','line_number':481,'multiline':False]