['text':'*****************************************************************************
 * Copyright (c) 2023, Tri Dao.
 *****************************************************************************','line_number':1,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':9,'multiline':False]['text':' If is_seqlens_k_cumulative, then seqlen_k is cu_seqlens_k[bidb + 1] - cu_seqlens_k[bidb].','line_number':19,'multiline':False]['text':' Otherwise it's cu_seqlens_k[bidb], i.e., we use cu_seqlens_k to store the sequence lengths of K.','line_number':20,'multiline':False]['text':' We have to have seqlen_k_cache declared before actual_seqlen_k, otherwise actual_seqlen_k is set to 0.','line_number':39,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////////////////////////','line_number':44,'multiline':False]['text':' namespace pytorch_flash','line_number':46,'multiline':False]