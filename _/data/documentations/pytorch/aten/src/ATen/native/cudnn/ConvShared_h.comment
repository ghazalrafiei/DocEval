['text':' ---------------------------------------------------------------------','line_number':15,'multiline':False]['text':'','line_number':16,'multiline':False]['text':' Helper classes','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':19,'multiline':False]['text':' This POD struct is used to let us easily compute hashes of the','line_number':21,'multiline':False]['text':' parameters','line_number':22,'multiline':False]['text':' NB: transposed purposely omitted: transposed just swaps','line_number':37,'multiline':False]['text':' forward and backward, so you can reuse the benchmark entry,','line_number':38,'multiline':False]['text':' NB: This can't be a constructor, because then ConvolutionParams','line_number':43,'multiline':False]['text':' would not be a POD anymore.','line_number':44,'multiline':False]['text':' TODO: Use TensorGeometry here instead of the entire Tensor, which we','line_number':45,'multiline':False]['text':' don't actually need.  (OTOH: We can always pass in','line_number':46,'multiline':False]['text':' grad_input/grad_output, so this is not very pressing)','line_number':47,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':57,'multiline':False]['text':'','line_number':58,'multiline':False]['text':' Raw functions','line_number':59,'multiline':False]['text':'','line_number':60,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':61,'multiline':False]['text':' v7 functions are preserved here to allow for runtime switching to v7','line_number':115,'multiline':False]['text':' (e.g., TORCH_CUDNN_V8_API_DISABLED=1).','line_number':116,'multiline':False]['text':' Note that v7 forward/backward out can have different behavior from the v8','line_number':117,'multiline':False]['text':' versions, as v7 explicitly splits large tensors as a 32-bit indexing','line_number':118,'multiline':False]['text':' workaround whereas v8 expects cuDNN to handle large tensors.','line_number':119,'multiline':False]