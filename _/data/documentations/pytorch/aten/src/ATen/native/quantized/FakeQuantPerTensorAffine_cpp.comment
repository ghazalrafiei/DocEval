['text':' FakeQuantize Op for PerTensorAffine quantization scheme.','line_number':8,'multiline':False]['text':' Use REGISTER_DISPATCH to run CPU and CUDA backend.','line_number':12,'multiline':False]['text':' Fake-quantizes the 'inputs' tensor.

Args:
  self: Forward input tensor.
  dY: Backward input tensor (_backward op only).
  scale: scale of per tensor affine quantization
  zero_point: zero_point of per tensor affine quantization
  quant_min: minimum quantized value
  quant_max: maximum quantized value

Returns:
  Quantized tensor (double dtype).

','line_number':17,'multiline':True]['text':' Fake-quantizes the 'inputs' tensor, saving a mask for the backward pass.

This is numerically equivalent to `fake_quantize_per_tensor_affine`,
but has a lower memory overhead in the backward pass.

Args:
  self: Forward input tensor.
  scale: scale of per tensor affine quantization
  zero_point: zero_point of per tensor affine quantization
  quant_min: minimum quantized value
  quant_max: maximum quantized value

Returns:
  Quantized tensor (double dtype).
  Mask (bool dtype).
','line_number':53,'multiline':True]['text':' TODO(future, optional): look into packing the mask further (BoolTensor uses','line_number':87,'multiline':False]['text':'   1 byte per element, we only need 1 bit per element).','line_number':88,'multiline':False]['text':' TODO(future, optional): look into packing the mask further (BoolTensor uses','line_number':107,'multiline':False]['text':'   1 byte per element, we only need 1 bit per element).','line_number':108,'multiline':False]['text':' Backward path to fake-quantize the 'inputs' tensor, with mask.

Args:
  dY: output grad.
  mask: mask tensor from the forward pass.

Returns:
  dX (input grad).
','line_number':112,'multiline':True]['text':' Note: no additional kernels needed, since mask is pre-computed','line_number':131,'multiline':False]['text':' and we can use the existing tensor multiplication kernels.','line_number':132,'multiline':False]['text':' The gradients for scale and zero point are calculated as below:
     Let Xfq be the fake quantized version of X.
     Let Xq be the quantized version of X (clamped at qmin and qmax).
     Let Delta and z be the scale and the zero point.
     :math:
      \frac{d\Delta }{dx} =
        \begin{cases}
          q_{\min} - z& \text{ if } X_q= q_{\min} \\
          q_{\max} - z& \text{ if } X_q= q_{\max} \\
          (X_{fq} - X) / \Delta & \text{ else }
        \end{cases}

      \frac{dz }{dx} =
        \begin{cases}
          -\Delta& \text{ if } X_q= q_{\min} \text{ or } X_q = q_{\max} \\
          0 & \text{ else }
        \end{cases}
  ','line_number':169,'multiline':True]['text':' The total sums over the scale and zero point gradient vectors are what will be returned in the end.','line_number':222,'multiline':False]['text':' namespace native','line_number':229,'multiline':False]['text':' namespace at','line_number':230,'multiline':False]