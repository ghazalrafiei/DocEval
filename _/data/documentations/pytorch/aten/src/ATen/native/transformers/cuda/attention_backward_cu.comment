['text':' FlashAttention Specific Imports','line_number':34,'multiline':False]['text':' MemoryEfficient Attention Specific Imports','line_number':38,'multiline':False]['text':'  CUDA code assumes that dout is contiguous','line_number':66,'multiline':False]['text':'  The kernel computes irregardless we will drop for this functions return','line_number':74,'multiline':False]['text':' We check the whether the cumulative_sequence_length_q is defined','line_number':77,'multiline':False]['text':' in order to determine whether we are using varlen or dense forward','line_number':78,'multiline':False]['text':' Varlen forward','line_number':80,'multiline':False]['text':'zero_tensors','line_number':97,'multiline':True]['text':'window_size_left','line_number':99,'multiline':True]['text':'window_size_right','line_number':100,'multiline':True]['text':' Dense forward','line_number':105,'multiline':False]['text':'window_size_left','line_number':119,'multiline':True]['text':'window_size_right','line_number':120,'multiline':True]['text':' additive attention bias','line_number':137,'multiline':False]['text':' (Mode 1MHK only) [b+1]: cu_seqlens_q[b] contains the','line_number':139,'multiline':False]['text':' position of the first query token for batch $b','line_number':140,'multiline':False]['text':' (Mode 1MHK only) [b+1]: cu_seqlens_k[b] contains the','line_number':142,'multiline':False]['text':' position of the first key token for batch $b','line_number':143,'multiline':False]['text':' (Mode 1MHK only) Maximum sequence length across batches','line_number':145,'multiline':False]['text':' (Mode 1MHK only) Maximum sequence length across batches','line_number':147,'multiline':False]['text':' dropout probability','line_number':150,'multiline':False]['text':' seed using for generating random numbers for dropout','line_number':151,'multiline':False]['text':' offset into random number sequence','line_number':152,'multiline':False]['text':' This path is used when we directly call _efficient_attention_forward','line_number':161,'multiline':False]['text':' from python.','line_number':162,'multiline':False]['text':' This is needed because SaveVariable automatically converts','line_number':163,'multiline':False]['text':' c10::optional to undefined tensor','line_number':164,'multiline':False]['text':' ndim','line_number':170,'multiline':False]['text':' batch size','line_number':176,'multiline':False]['text':' seqlen','line_number':181,'multiline':False]['text':' Num heads','line_number':185,'multiline':False]['text':' Embedding per head','line_number':190,'multiline':False]['text':' handle potentially non-contiguous grad_out through a copy','line_number':194,'multiline':False]['text':' force alignment for the last dim','line_number':241,'multiline':False]['text':'dim=','line_number':247,'multiline':True]['text':'start=','line_number':247,'multiline':True]['text':'end=','line_number':247,'multiline':True]['text':' See Note [Seed and Offset Device]','line_number':253,'multiline':False]['text':' dropout + capture','line_number':261,'multiline':False]['text':' Check if this kernel is compatible','line_number':284,'multiline':False]['text':' Dropout must be supported if we need it','line_number':288,'multiline':False]['text':' Alignment','line_number':297,'multiline':False]['text':' Uses too much shmem','line_number':303,'multiline':False]['text':' TODO: Fuse this into a kernel?','line_number':311,'multiline':False]['text':' This is a bottleneck for smaller sequences (M <= 128)','line_number':312,'multiline':False]['text':' We removed the chunk/cat optimization and the multiplier is always 1','line_number':362,'multiline':False]['text':' Heuristic for finding optimal number of splits','line_number':422,'multiline':False]['text':' Skip heuristic, if user provided an explicit value','line_number':426,'multiline':False]['text':' If we already have enough parallelism, split-keys can help','line_number':428,'multiline':False]['text':' better use L2 cache.','line_number':429,'multiline':False]['text':' This is negligible when the seqlen is too small tho','line_number':430,'multiline':False]['text':' Increasing `split_keys` leads to using more gmem for temporary storage','line_number':435,'multiline':False]['text':' when we need a staging area for gK/gV. let's avoid that','line_number':436,'multiline':False]['text':' https://docs.nvidia.com/cuda/cuda-c-programming-guide/#features-and-technical-specifications-technical-specifications-per-compute-capability','line_number':471,'multiline':False]['text':' second syntax resulted in the error below on windows','line_number':482,'multiline':False]['text':' error C3495: 'kernel_fn': a simple capture must be a variable','line_number':483,'multiline':False]['text':' with automatic storage duration declared','line_number':484,'multiline':False]['text':' in the reaching scope of the lambda','line_number':485,'multiline':False]['text':' This is needed because SaveVarible automatically converts','line_number':595,'multiline':False]['text':' c10::optional to undefined tensor','line_number':596,'multiline':False]['text':' Will add with signauter changes for dropout and bias','line_number':601,'multiline':False]['text':' We are only handiling Dense inputs, but this should be passed','line_number':602,'multiline':False]['text':' from forward to backward','line_number':603,'multiline':False]['text':' num_split_keys','line_number':629,'multiline':False]['text':' namespace native','line_number':634,'multiline':False]['text':' namespace at','line_number':635,'multiline':False]