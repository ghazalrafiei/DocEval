['text':'
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 ','line_number':1,'multiline':True]['text':'
    Helper functions to efficient store/load RF to gmem

    GEMM accumulators have a particular format on A100, and
    it takes some compute/shared-memory to rearrange them to
    a RowMajor or ColumnMajor format in global memory through
    an Epilogue. The same complexity goes for loading into RF.

    This class loads/stores RF as they are, and can be used for
    efficient accumulation across gemms for instance:

    ```
    GmemTile tile;
    for (int i = 0; i < N; ++i) {
      // ...

      Fragment accum;
      if (i == 0) {
        accum.clear();
      } else {
        tile.load(accum);
      }
      mma(accum, ...);
      if (i < N-1) {
        // Store for next GEMM
        tile.store(accum);
      } else {
        // Store in tensor (eg RowMajor)
        epilogue(accum);
      }

      // ...
    }
    ```
  ','line_number':68,'multiline':True]['text':' 128bits per thread','line_number':104,'multiline':False]['text':'cmp','line_number':170,'multiline':True]['text':'setval','line_number':170,'multiline':True]['text':' namespace','line_number':200,'multiline':False]['text':' which arch we target (eg `cutlass::arch::Sm80`)','line_number':203,'multiline':False]['text':' input/output type','line_number':205,'multiline':False]['text':' run optimized kernel because memory accesses will be aligned','line_number':207,'multiline':False]['text':' use dropout if enabled','line_number':209,'multiline':False]['text':' when doing a GEMM, preload the next one (uses more shmem)','line_number':211,'multiline':False]['text':' block dimensions','line_number':213,'multiline':False]['text':' upperbound on `max(value.shape[-1], query.shape[-1])`','line_number':216,'multiline':False]['text':' assumes that `cu_seqlen` is None, and','line_number':218,'multiline':False]['text':' (1) `num_queries % kBlockSizeI == 0`','line_number':219,'multiline':False]['text':' (2) `num_keys % kBlockSizeJ == 0`','line_number':220,'multiline':False]['text':' If this is true, we store and accumulate dK/dV in RF','line_number':246,'multiline':False]['text':' rather than going back to gmem everytime','line_number':247,'multiline':False]['text':' Compute delta for the f16 kernels','line_number':263,'multiline':False]['text':' TODO: Figure out why it's slower on the f32 kernels','line_number':264,'multiline':False]['text':' (something due to RF pressure?)','line_number':265,'multiline':False]['text':' TODO: Remove condition on `kOutputInRF` - this is needed to work','line_number':266,'multiline':False]['text':' around a compiler bug on V100, not exactly sure why but I spent','line_number':267,'multiline':False]['text':' too much time on this already. Reproducible with','line_number':268,'multiline':False]['text':' (B, Mq, Mkv, K) = (1, 1, 1, 136) for instance','line_number':269,'multiline':False]['text':' Launch bounds','line_number':273,'multiline':False]['text':' ElementC','line_number':285,'multiline':False]['text':' ElementAccumulator','line_number':286,'multiline':False]['text':'
    attn_T = k_j @ q_i.transpose(-2, -1) # matmul
    attn_T = (attn_T - logsumexp[i_start:i_end].unsqueeze(1).transpose(-2,
    -1)).exp() # epilogue

    with attn_T.shape = (kBlockSizeJ, kBlockSizeI)
    ','line_number':294,'multiline':True]['text':' ElementA','line_number':305,'multiline':False]['text':' LayoutA','line_number':306,'multiline':False]['text':' ElementB','line_number':308,'multiline':False]['text':' LayoutB','line_number':309,'multiline':False]['text':' ElementC','line_number':311,'multiline':False]['text':' LayoutC','line_number':312,'multiline':False]['text':' AccumulatorsInRowMajor = false,','line_number':320,'multiline':False]['text':' used for efficient load of bias tile (Bij) from global memory to shared','line_number':326,'multiline':False]['text':' memory','line_number':327,'multiline':False]['text':' Bij is applied to transposed attn matrix tile (Pij.T). Bij is loaded','line_number':330,'multiline':False]['text':' row-major but needs to have transposed shape so we get the same','line_number':331,'multiline':False]['text':' elements.','line_number':332,'multiline':False]['text':' input restriction: kv_len has to be a multiple of this value','line_number':335,'multiline':False]['text':' Epilogue to store to shared-memory in a format that we can use later for','line_number':338,'multiline':False]['text':' the second matmul','line_number':339,'multiline':False]['text':'
    grad_v[j_start:j_end] += attn_T @ do_i # matmul

    Dimensions: (kBlockSizeJ * kNumWarpsPerBlock, kBlockSizeI, K)
    (we might need to iterate multiple times on K)
    ','line_number':354,'multiline':True]['text':' ElementA,','line_number':366,'multiline':False]['text':' LayoutA,','line_number':367,'multiline':False]['text':' ElementB,','line_number':369,'multiline':False]['text':' LayoutB,','line_number':370,'multiline':False]['text':' LayoutC,','line_number':373,'multiline':False]['text':' ThreadblockSwizzle - not used','line_number':381,'multiline':False]['text':' SplitKSerial','line_number':383,'multiline':False]['text':' if dropout:','line_number':386,'multiline':False]['text':'   for computing dVj += (Pij.T * Zij) @ dOi','line_number':387,'multiline':False]['text':'   Pij_dropped.T = Pij.T * Zij is computed on the fly as fragments of','line_number':388,'multiline':False]['text':'   Pij.T are loaded in. The reason we do it this way is because Pij.T and','line_number':389,'multiline':False]['text':'   Zij are reused in later steps, while Pij_dropped.T is only needed in','line_number':390,'multiline':False]['text':'   this step. computing Pij_dropped.T on the fly allows us to avoid','line_number':391,'multiline':False]['text':'   keeping all 3 of Pij_dropped.T, Pij.T, and Zij in shared memory at the','line_number':392,'multiline':False]['text':'   same time.','line_number':393,'multiline':False]['text':' if no dropout:','line_number':394,'multiline':False]['text':'   for computing dVj += Pij.T @ dOi','line_number':395,'multiline':False]['text':' WarpShape','line_number':398,'multiline':False]['text':' InstructionShape','line_number':400,'multiline':False]['text':' RegularWarpIterator','line_number':402,'multiline':False]['text':' Policy','line_number':403,'multiline':False]['text':' kScaleOperandA','line_number':410,'multiline':False]['text':' Epilogue','line_number':416,'multiline':False]['text':'
    doi_t_vj = do_i @ v_j.transpose(-2, -1) # matmul
    tmp = (doi_t_vj - Di.unsqueeze(1)) * attn # inplace / epilogue?
    ','line_number':426,'multiline':True]['text':' no-op output op - epilogue just stores result to global memory','line_number':437,'multiline':False]['text':' ElementA','line_number':447,'multiline':False]['text':' LayoutA','line_number':448,'multiline':False]['text':' ElementB','line_number':450,'multiline':False]['text':' LayoutB','line_number':451,'multiline':False]['text':' ElementC','line_number':453,'multiline':False]['text':' LayoutC','line_number':454,'multiline':False]['text':' ElementAccumulator','line_number':455,'multiline':False]['text':' EpilogueOutputOp','line_number':461,'multiline':False]['text':' ThreadblockSwizzle (not used)','line_number':462,'multiline':False]['text':' multiple preloads, dropout Zij tile, and 3 stages push us over shared','line_number':463,'multiline':False]['text':' memory capacity on A100. set a ceiling on number of stages to save','line_number':464,'multiline':False]['text':' shared memory if dropout is in use.','line_number':465,'multiline':False]['text':' Stages','line_number':468,'multiline':False]['text':' SplitKSerial','line_number':469,'multiline':False]['text':' epilogue used to write bias gradient, which is just the output of this','line_number':478,'multiline':False]['text':' matmul with some operations applied to the fragment','line_number':479,'multiline':False]['text':' Epilogue to store to shared-memory in a format that we can use later for','line_number':482,'multiline':False]['text':' the second matmul','line_number':483,'multiline':False]['text':' grad_q <- tmp @ k_j','line_number':494,'multiline':False]['text':' ElementA,','line_number':501,'multiline':False]['text':' LayoutA,','line_number':502,'multiline':False]['text':' ElementB,','line_number':504,'multiline':False]['text':' LayoutB,','line_number':505,'multiline':False]['text':' LayoutC,','line_number':508,'multiline':False]['text':' ThreadblockSwizzle - not used','line_number':516,'multiline':False]['text':' SplitKSerial','line_number':518,'multiline':False]['text':' kScaleOperandA','line_number':532,'multiline':False]['text':' Epilogue','line_number':537,'multiline':False]['text':' grad_k <- tmp.transpose(-2, -1) @ q_i','line_number':546,'multiline':False]['text':' ElementA,','line_number':553,'multiline':False]['text':' LayoutA,','line_number':554,'multiline':False]['text':' ElementB,','line_number':556,'multiline':False]['text':' LayoutB,','line_number':557,'multiline':False]['text':' LayoutC,','line_number':560,'multiline':False]['text':' ThreadblockSwizzle - not used','line_number':568,'multiline':False]['text':' SplitKSerial','line_number':570,'multiline':False]['text':' kMaxK','line_number':582,'multiline':False]['text':' kScaleOperandA','line_number':584,'multiline':False]['text':' kMaxK','line_number':588,'multiline':False]['text':' kScaleOperandA','line_number':590,'multiline':False]['text':' kTransposeA','line_number':591,'multiline':False]['text':' Epilogue','line_number':600,'multiline':False]['text':' pad to 128bits','line_number':621,'multiline':False]['text':' Input tensors','line_number':626,'multiline':False]['text':' [Mq, nH, K]','line_number':627,'multiline':False]['text':' [Mk, nH, K]','line_number':628,'multiline':False]['text':' [Mk, nH, Kv]','line_number':629,'multiline':False]['text':' [nH, Mq]','line_number':631,'multiline':False]['text':' [Mq, nH, Kv]','line_number':632,'multiline':False]['text':' [Mq, nH, Kv]','line_number':633,'multiline':False]['text':' [nH, Mq]','line_number':634,'multiline':False]['text':' Output tensors','line_number':638,'multiline':False]['text':'  [Mq, nH, K]','line_number':639,'multiline':False]['text':'    [Mk, nH, K]','line_number':640,'multiline':False]['text':'  [Mk, nH, Kv]','line_number':641,'multiline':False]['text':' Accumulators','line_number':644,'multiline':False]['text':' [Mq, Kq] + [Mkv, Kq] + [Mkv, Kv]','line_number':645,'multiline':False]['text':' (will be calculated by the kernel)','line_number':647,'multiline':False]['text':' (will be calculated by the kernel)','line_number':649,'multiline':False]['text':' Scale','line_number':651,'multiline':False]['text':' Dimensions/strides','line_number':654,'multiline':False]['text':' 3 for packed, 1 otherwise','line_number':668,'multiline':False]['text':' RNG sequence offset based on batch_id and head_id','line_number':672,'multiline':False]['text':' Everything below is only used in `advance_to_block`','line_number':689,'multiline':False]['text':' and shouldn't use registers','line_number':690,'multiline':False]['text':' We use `gridDim.x` inside kernel','line_number':706,'multiline':False]['text':' Advance pointers that depend on the total concatenated','line_number':748,'multiline':False]['text':' number of queries, as `num_queries` is modified in the block','line_number':749,'multiline':False]['text':' below','line_number':750,'multiline':False]['text':' Jump manually','line_number':769,'multiline':False]['text':' Some values are modified above','line_number':803,'multiline':False]['text':' Signal to the compiler that they are the same in all threads','line_number':804,'multiline':False]['text':' and can be stored in warp-uniform registers (Sm75+)','line_number':805,'multiline':False]['text':' Aligned on 128bits','line_number':865,'multiline':False]['text':' Returns size of buffer we need to run this kernel','line_number':872,'multiline':False]['text':' shared storage for keeping Zij matrix. not needed if we aren't using','line_number':880,'multiline':False]['text':' dropout, in which case we use an empty array to save shared memory','line_number':881,'multiline':False]['text':' dummy shared storage object that takes up no space.','line_number':885,'multiline':False]['text':' windows builds throw the error:','line_number':888,'multiline':False]['text':' "type containing an unknown-size array is not allowed"','line_number':889,'multiline':False]['text':' if we try to make Zij shared storage zero-sized.','line_number':890,'multiline':False]['text':' To get around this just make it sized 1 on windows.','line_number':891,'multiline':False]['text':' (do_i * o_i).sum(-1)','line_number':902,'multiline':False]['text':' part1 - after Q.K / dV / dO.V','line_number':907,'multiline':False]['text':' 1. efficient load of bias tile Bij, which is then applied to Pij','line_number':909,'multiline':False]['text':' 4. store Pij. it is needed:','line_number':911,'multiline':False]['text':' - in dVj += (Pij.T * Zij) @ dOi','line_number':912,'multiline':False]['text':' - in dSij = Pij * (dPij - Di)','line_number':913,'multiline':False]['text':' 6. dVj += (Pij.T * Zij) @ dOi','line_number':914,'multiline':False]['text':' 10. write to fragment','line_number':915,'multiline':False]['text':' 5. store Zij. it is needed in dVj += (Pij.T * Zij) @ dOi','line_number':918,'multiline':False]['text':' 2. prologue for dVj','line_number':922,'multiline':False]['text':' 6. workspace for dVj += (Pij.T * Zij) @ dOi','line_number':923,'multiline':False]['text':' 7. dVj epilogue','line_number':925,'multiline':False]['text':' 3. prologue for dPij_dropped','line_number':929,'multiline':False]['text':' 8. used in dPij_dropped = dOi @ Vj.T','line_number':930,'multiline':False]['text':' part2 - dQ','line_number':935,'multiline':False]['text':' (from part1)','line_number':938,'multiline':False]['text':' (preload)','line_number':941,'multiline':False]['text':' (preload)','line_number':942,'multiline':False]['text':' store dB = dSij to global memory','line_number':944,'multiline':False]['text':' part3 - after last iteration on dQ's epilogue / dK','line_number':952,'multiline':False]['text':' (from part1)','line_number':955,'multiline':False]['text':' (preload)','line_number':958,'multiline':False]['text':' part4 - after last iteration on dK's epilogue / preload next K.Q_t','line_number':966,'multiline':False]['text':' If we reach end of current key, dump RF->gmem with "final" epilogues','line_number':969,'multiline':False]['text':' Field size','line_number':977,'multiline':False]['text':' ===========================================','line_number':1006,'multiline':False]['text':' (do_i * o_i).sum(-1)','line_number':1035,'multiline':False]['text':' part1 - Q.K matmul','line_number':1039,'multiline':False]['text':' part2 - compute gradV','line_number':1045,'multiline':False]['text':' 1. efficient load of bias tile Bij, which is then applied to Pij','line_number':1047,'multiline':False]['text':' 2. store Pij to shared memory. it is needed:','line_number':1049,'multiline':False]['text':' - in this step, where it is used in dVj += (Pij.T * Zij) @ dOi','line_number':1050,'multiline':False]['text':' - in next step where it is used in dSij = Pij * (dPij - Di)','line_number':1051,'multiline':False]['text':' 3. store Zij. it is needed in this step, where it is used','line_number':1054,'multiline':False]['text':' to compute Pij_dropped = Pij * Zij on the fly as fragments of Pij are','line_number':1055,'multiline':False]['text':' loaded for the computation of dVj.','line_number':1056,'multiline':False]['text':' part3 - DO.V matmul','line_number':1066,'multiline':False]['text':' first compute dPij = (dOi @ Vj.T) * Zij','line_number':1068,'multiline':False]['text':' and dSij = Pij * (dPij - Di)','line_number':1069,'multiline':False]['text':' (from part2) - Pij for computing dSij = Pij * (dPij - Di)','line_number':1071,'multiline':False]['text':' matmul to compute dOiVj','line_number':1073,'multiline':False]['text':' then store dB = dSij to global memory','line_number':1076,'multiline':False]['text':' part4 - compute gradQ','line_number':1082,'multiline':False]['text':' (from part2)','line_number':1084,'multiline':False]['text':' part5 - compute gradK','line_number':1095,'multiline':False]['text':' (from part2)','line_number':1097,'multiline':False]['text':' part6 - store RF accumulated into gmem','line_number':1106,'multiline':False]['text':' ===========================================','line_number':1124,'multiline':False]['text':' Computes (dO*out).sum(-1) and writes it to `p.delta_ptr`','line_number':1295,'multiline':False]['text':' See Note [Seed and Offset Device]','line_number':1318,'multiline':False]['text':' each element of the attention matrix P with shape','line_number':1320,'multiline':False]['text':' (batch_sz, n_heads, n_queries, n_keys) is associated with a single','line_number':1321,'multiline':False]['text':' offset in RNG sequence. we initialize the RNG state with offset that','line_number':1322,'multiline':False]['text':' starts at the beginning of a (n_queries, n_keys) matrix for this','line_number':1323,'multiline':False]['text':' block's batch_id and head_id','line_number':1324,'multiline':False]['text':' initializing rng state is very expensive, so we run once per kernel,','line_number':1325,'multiline':False]['text':' rather than once per iteration. each iteration takes a copy of the','line_number':1326,'multiline':False]['text':' initialized RNG state and offsets it as needed.','line_number':1327,'multiline':False]['text':' This line here','line_number':1343,'multiline':False]['text':' vvvvvvvvvvvvvv','line_number':1344,'multiline':False]['text':' ^^^^^^^^^^^^^^','line_number':1346,'multiline':False]['text':' ... makes everything use less RF and be 10% faster. Why?','line_number':1347,'multiline':False]['text':' I don't know. My theory is that it forces `nvcc` to','line_number':1348,'multiline':False]['text':' re-compute indices, offsets etc... and not keep them','line_number':1349,'multiline':False]['text':' from the previous iteration, which prevents MASSIVE','line_number':1350,'multiline':False]['text':' register spilling.','line_number':1351,'multiline':False]['text':' This function is not really optimized, but should rarely be used','line_number':1388,'multiline':False]['text':' It's only used when some keys are "useless" and don't attend to','line_number':1389,'multiline':False]['text':' any query, due to causal masking','line_number':1390,'multiline':False]['text':' Prevents `nvcc` from keeping values deduced from','line_number':1434,'multiline':False]['text':' `thread_id`, `warp_id`, ... in RF - to reduce register pressure','line_number':1435,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1520,'multiline':False]['text':' MatmulQK','line_number':1521,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1522,'multiline':False]['text':' k','line_number':1529,'multiline':False]['text':' k_j','line_number':1532,'multiline':False]['text':' q_i.transpose(-2, -1)','line_number':1540,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':1562,'multiline':False]['text':' Epilogue: add LSE + exp and store that to our shared memory buffer','line_number':1568,'multiline':False]['text':' shmem <- (matmul_result -','line_number':1569,'multiline':False]['text':' logsumexp[i_start:i_end].unsqueeze(1)).exp()','line_number':1570,'multiline':False]['text':' apply bias if applicable','line_number':1577,'multiline':False]['text':' load bias tile Bij into shared memory','line_number':1579,'multiline':False]['text':' Pij += Bij, where Pij is in register fragment and Bij is in shmem','line_number':1592,'multiline':False]['text':' remember we are transposed','line_number':1599,'multiline':False]['text':' Apply mask','line_number':1605,'multiline':False]['text':' current_key = key_start + accum_m','line_number':1614,'multiline':False]['text':' current_query = query_start + accum_n','line_number':1615,'multiline':False]['text':' mask if: `current_key > current_query`','line_number':1616,'multiline':False]['text':' if we are using dropout, compute Zij, writing it to shared memory.','line_number':1651,'multiline':False]['text':' each element of Zij is:','line_number':1652,'multiline':False]['text':' - 0 with probability dropout_p','line_number':1653,'multiline':False]['text':' - 1 / (1 - dropout_p) with probability 1 - dropout_p','line_number':1654,'multiline':False]['text':' each thread generates a contiguous sequence of elements in Zij, all','line_number':1657,'multiline':False]['text':' in the same row. the reason they have to come from the same row is','line_number':1658,'multiline':False]['text':' that sampling random numbers from a contiguous random number sequence','line_number':1659,'multiline':False]['text':' is much more efficient than jumping around, and the linear offset of','line_number':1660,'multiline':False]['text':' each element of Z (the global matrix) maps to an offset in a random','line_number':1661,'multiline':False]['text':' number sequence. for Z, the end of a row and the beginning of the','line_number':1662,'multiline':False]['text':' next have adjacent offsets, but for Zij (tile of global matrix), this','line_number':1663,'multiline':False]['text':' is not necessarily the case.','line_number':1664,'multiline':False]['text':' We must fill the entire `zij` shmem with values (even out of bounds','line_number':1665,'multiline':False]['text':' on the K-dimension) otherwise we can get NaNs during the GEMM','line_number':1666,'multiline':False]['text':' generate elements of Zij, 4 elements at a time','line_number':1684,'multiline':False]['text':' we'll write Zij transposed since attention is also transposed','line_number':1693,'multiline':False]['text':' during the matmul to compute dV.','line_number':1694,'multiline':False]['text':'k','line_number':1695,'multiline':True]['text':'q','line_number':1695,'multiline':True]['text':' Save mask for later DOIVJ matmul','line_number':1708,'multiline':False]['text':'q','line_number':1721,'multiline':True]['text':'k','line_number':1721,'multiline':True]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1732,'multiline':False]['text':' GradV matmul','line_number':1733,'multiline':False]['text':'','line_number':1734,'multiline':False]['text':' grad_v[j_start:j_end] += attn_T @ do_i','line_number':1735,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1736,'multiline':False]['text':' if dropout: dVj += (Pij.T * Zij) @ dOi','line_number':1760,'multiline':False]['text':' otherwise:  dVj += Pij.T @ dOi','line_number':1761,'multiline':False]['text':' operand A: Pij.T','line_number':1763,'multiline':False]['text':' operand A_scale Zij.T:','line_number':1765,'multiline':False]['text':' if we're using dropout, operand A is Pij_dropped.T = Pij.T * Zij.T','line_number':1766,'multiline':False]['text':' which is computed on the fly as fragments of Pij.T are loaded in','line_number':1767,'multiline':False]['text':' operand B: dOi - which was loaded into shared memory previously','line_number':1769,'multiline':False]['text':' when we computed dVj','line_number':1770,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':1791,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1820,'multiline':False]['text':' MatmulDOIVJ','line_number':1821,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1822,'multiline':False]['text':' do_i','line_number':1825,'multiline':False]['text':' v_j.transpose(-2, -1)','line_number':1833,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':1852,'multiline':False]['text':' TODO: This must be terribly inefficient. There must be a better way','line_number':1867,'multiline':False]['text':' tmp [RF] <- (accum [RF] - Di [smem] ) * attn_T.T [smem]','line_number':1868,'multiline':False]['text':' attn_shared_storage  [smem] <- tmp.T','line_number':1869,'multiline':False]['text':' tmp_shared_storage [smem] <- tmp','line_number':1870,'multiline':False]['text':' if dropout was used, compute dPij = dPij_dropped * Zij','line_number':1875,'multiline':False]['text':' dSij = (dPij - Di) * Pij','line_number':1897,'multiline':False]['text':' TODO: Otherwise we can get nans as we','line_number':1902,'multiline':False]['text':' might have infs here (only seen on f16 tho)','line_number':1903,'multiline':False]['text':' store bias gradient tile dBij to global memory,','line_number':1917,'multiline':False]['text':' where dBij = dSij = Pij * (dPij - Di)','line_number':1918,'multiline':False]['text':' grad_bias_ptr is offset to point at beginning of','line_number':1924,'multiline':False]['text':' matrix of shape (queries, keys) for a given','line_number':1925,'multiline':False]['text':' (batch_id, head_id) the pointer arithmetic here produces','line_number':1926,'multiline':False]['text':' a pointer to the start of the current tile within that','line_number':1927,'multiline':False]['text':' matrix','line_number':1928,'multiline':False]['text':' no-op epilogue operator - just casting and storing contents of','line_number':1933,'multiline':False]['text':' accum to global memory','line_number':1934,'multiline':False]['text':' attn <- attn_T.T','line_number':1951,'multiline':False]['text':' Force `nvcc` to recompute values that depend on the variables just below','line_number':1969,'multiline':False]['text':' to use less RF and prevent some spilling','line_number':1970,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1975,'multiline':False]['text':' GradQ matmul','line_number':1976,'multiline':False]['text':'','line_number':1977,'multiline':False]['text':' grad_q[i_start:i_end] += tmp @ k_j','line_number':1978,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1979,'multiline':False]['text':' Skip the loop & associated branches if we know at compile time the number','line_number':1980,'multiline':False]['text':' of iterations','line_number':1981,'multiline':False]['text':' k_j','line_number':1994,'multiline':False]['text':' operand A: dSij','line_number':2004,'multiline':False]['text':' operand B: Kj','line_number':2006,'multiline':False]['text':' Make sure we can see other block's output','line_number':2025,'multiline':False]['text':' if we know we are the first to access it, we know it's only zeros.','line_number':2032,'multiline':False]['text':' Avoids a load from gmem (and gmem init as well)','line_number':2033,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':2042,'multiline':False]['text':' Output results','line_number':2063,'multiline':False]['text':' `atomicAdd` returns the old value','line_number':2068,'multiline':False]['text':' Make sure everyone wrote before we release the lock','line_number':2077,'multiline':False]['text':' NOTE: We're not releasing the lock because no one is expected','line_number':2083,'multiline':False]['text':' to come after us (we're the last one to write)','line_number':2084,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':2102,'multiline':False]['text':' GradK matmul','line_number':2103,'multiline':False]['text':'','line_number':2104,'multiline':False]['text':' grad_k[i_start:i_end] += tmp.transpose(-2, -1) @ q_i','line_number':2105,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':2106,'multiline':False]['text':' q_i','line_number':2129,'multiline':False]['text':' this is basically:','line_number':2139,'multiline':False]['text':' opA = kIsTransposedA ? getTmp() : getTmpT();','line_number':2140,'multiline':False]['text':' operand A: dSij.T','line_number':2148,'multiline':False]['text':' operand B: Qi','line_number':2150,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':2171,'multiline':False]['text':' Output results','line_number':2195,'multiline':False]['text':' Iteration order logic','line_number':2220,'multiline':False]['text':' Returns how many kernel blocks will write to a given block in `grad_query`','line_number':2241,'multiline':False]['text':' This is usually equal to the number of key splits, but can be different','line_number':2242,'multiline':False]['text':' for instance in the causal case, or varying seqlen','line_number':2243,'multiline':False]['text':' Returns the next block to process','line_number':2260,'multiline':False]['text':' Wrap around','line_number':2270,'multiline':False]['text':' jump to next key','line_number':2276,'multiline':False]['text':' jump to next key','line_number':2284,'multiline':False]['text':' Next key','line_number':2286,'multiline':False]['text':' IterationsUnroll','line_number':2406,'multiline':False]['text':' Each thread computes one value for Delta','line_number':2420,'multiline':False]['text':' Depending on warp configuration, we might have multiple','line_number':2421,'multiline':False]['text':' threads of the same warp working on the same row','line_number':2422,'multiline':False]['text':' on windows, previous syntax __restrict__ AccessType*','line_number':2433,'multiline':False]['text':' resulted in error: "restrict" is not allowed','line_number':2434,'multiline':False]['text':' Load for next iter','line_number':2471,'multiline':False]['text':' If we have a small lower-bound for K, we can unroll the loop','line_number':2485,'multiline':False]['text':' Reduce between workers','line_number':2500,'multiline':False]['text':' Store in gmem','line_number':2510,'multiline':False]['text':' namespace PyTorchMemEffAttention','line_number':2530,'multiline':False]