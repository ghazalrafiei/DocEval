['text':'
 * This function formats an input tensor in NCHW layout to NC4HW layout such
 * that the buffer of the formatted tensor can be directly copied into a GPU
 * texture. Conceptually, the formatting can be achieved via the following
 * steps:
 *
 * 1. Given that the src tensor has size {N,C,H,W}
 *
 * 2. Combine the batch and channel dims by reshaping to {N*C, H, W}
 *
 * 3. Determine the amount of padding to add: determine how many channels to add
 *    in order to align N*C to the next multiple of 4
 *
 * 4. Add padding to the tensor so that the batch-channel dimension is a
 *    multiple of four; the shape of the tensor is now {NC_aligned, H, W}
 *
 * 5. Split the batch-channel dimension into groups of 4 by reshaping the tensor
 *    to size {NC_aligned/4, 4, H, W}
 *
 * 6. The groups of 4 channels (dim 1) should be contiguous. Therefore, permute
 *    the dims of the tensor in the order {0, 2, 3, 1}
 *
 * 7. Finally, return a contiguous version of the tensor. The final shape of the
 *    tensor would be {NC_aligned/4, H, W, 4}
 ','line_number':22,'multiline':True]['text':' Add padding to the tensor so that the channel dim is a multiple of 4','line_number':56,'multiline':False]['text':' Reshape to group channels into groups of 4 and permute so that the groups','line_number':59,'multiline':False]['text':' are in the first dimension so that they are contiguous','line_number':60,'multiline':False]['text':' Return a contiguous version of the tensor','line_number':63,'multiline':False]['text':'
 * Creates a staging tensor into which texture data, which will be in NC4HW
 * format, can be copied directly. The shape of the staging tensor will be the
 * same as the tensor produced by a call to format_src_tensor().
 ','line_number':67,'multiline':True]['text':' Note that the dtype corresponding with the texture format of the vTensor is','line_number':80,'multiline':False]['text':' used instead of options().dtype(). This is to ensure the number of bytes in','line_number':81,'multiline':False]['text':' the staging tensor matches the number of bytes in the image texture. Refer','line_number':82,'multiline':False]['text':' to comments for api::vk_format()','line_number':83,'multiline':False]['text':'
 * After copying texture data, which will be in NC4HW format, to a staging
 * tensor created in create_staging_tensor(), this function reformats the tensor
 * to NCHW format. It essentially reverses the transformations made by
 * format_src_tensor().
 *
 * Note that the sizes of the original tensor must be passed in to fully restore
 * the properties of the original tensor.
 ','line_number':88,'multiline':True]['text':' Undo the permute step and channel grouping step','line_number':105,'multiline':False]['text':' Remove the padding channels','line_number':107,'multiline':False]['text':'dim=','line_number':109,'multiline':True]['text':'start','line_number':109,'multiline':True]['text':'end','line_number':109,'multiline':True]['text':' Reshape to original sizing and dtype and return a contiguous Tensor','line_number':111,'multiline':False]['text':' pipeline barrier','line_number':127,'multiline':False]['text':' resources','line_number':129,'multiline':False]['text':' copy details','line_number':135,'multiline':False]['text':' fence handle','line_number':139,'multiline':False]['text':' pipeline barrier','line_number':151,'multiline':False]['text':' resources','line_number':153,'multiline':False]['text':' copy details','line_number':156,'multiline':False]['text':' fence handle','line_number':160,'multiline':False]['text':' pipeline barrier','line_number':177,'multiline':False]['text':' resources','line_number':179,'multiline':False]['text':' copy details','line_number':185,'multiline':False]['text':' fence handle','line_number':189,'multiline':False]['text':'
 * Broadcasting Utils
 ','line_number':241,'multiline':True]['text':' check if two tensors are broadcastable','line_number':245,'multiline':False]['text':' check if the shapes of input tensors are broadcastable','line_number':251,'multiline':False]['text':' see https://pytorch.org/docs/stable/notes/broadcasting.html','line_number':252,'multiline':False]['text':' for broadcasting semantics','line_number':253,'multiline':False]['text':' compute the output shape by broadcasting the shapes of t1 and t2','line_number':282,'multiline':False]['text':' x, y, z, w all using a single element tensor. We intend to pull','line_number':328,'multiline':False]['text':' (0, 0, 0).x from each tensor. This allows us to isolate the effect','line_number':329,'multiline':False]['text':' of most packing mechanism.','line_number':330,'multiline':False]['text':' namespace utils','line_number':379,'multiline':False]['text':' namespace ops','line_number':380,'multiline':False]['text':' namespace vulkan','line_number':381,'multiline':False]['text':' namespace native','line_number':382,'multiline':False]['text':' namespace at','line_number':383,'multiline':False]