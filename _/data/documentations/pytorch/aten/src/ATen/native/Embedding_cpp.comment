['text':' TODO: use tensor.index() after improving perf','line_number':43,'multiline':False]['text':' TODO: if we teach sparse tensor how to propagate symints, the guard','line_number':60,'multiline':False]['text':' here is not strictly necessary.  However, we think it is fine as is','line_number':61,'multiline':False]['text':' because num weights is derived from a parameter and therefore','line_number':62,'multiline':False]['text':' typically not varying.','line_number':63,'multiline':False]['text':' TODO: implement scale_grad_by_freq','line_number':82,'multiline':False]['text':' check if all our grad come from padding_idx','line_number':100,'multiline':False]['text':'squash_dims=','line_number':129,'multiline':True]['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)','line_number':140,'multiline':False]['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)','line_number':160,'multiline':False]['text':' grad_weight[k].add_(grad[i], scale);','line_number':164,'multiline':False]['text':' Note that we cannot use at::parallel_for here because we perform operations on','line_number':196,'multiline':False]['text':' Tensor inside the loop. See github.com/pytorch/pytorch/issues/28370 for more details.','line_number':197,'multiline':False]['text':' namespace at::native','line_number':215,'multiline':False]