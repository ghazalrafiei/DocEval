['text':' Check if pytorch is compiled with MIOpen.','line_number':68,'multiline':False]['text':' Those could have been function pointers, but MSVC chokes on function pointers as template parameters','line_number':106,'multiline':False]['text':' Simple type for __getstate__/__setstate__ serialization','line_number':124,'multiline':False]['text':'','line_number':125,'multiline':False]['text':' Element 0 is a string key to say what kind of CellParam this is. It','line_number':126,'multiline':False]['text':' should be a valid key into cell_params_deserializers','line_number':127,'multiline':False]['text':' Element 1 is the Tensors contained within the CellParams instance','line_number':128,'multiline':False]['text':' Element 2 is the doubles (if any) contained in the CellParams instance','line_number':129,'multiline':False]['text':' Element 3 is the longs (if any) contained within the CellParams instance','line_number':130,'multiline':False]['text':' Base class so we can polymorphically handle these','line_number':138,'multiline':False]['text':' by default doing nothing. CellParams will override this','line_number':142,'multiline':False]['text':' to define correct behavior for LSTMs with projections.','line_number':143,'multiline':False]['text':' This function is not pure virtual, because it's useful to','line_number':144,'multiline':False]['text':' provide this default implementation, so that all cell params','line_number':145,'multiline':False]['text':' that don't support projections work correctly (e.g. QuantizedCellParams variations)','line_number':146,'multiline':False]['text':' Pretty much all cells we support take the same set of arguments, but threading those','line_number':159,'multiline':False]['text':' 4 arguments manually is really annoying. Their lifetime is externally managed, so we only','line_number':160,'multiline':False]['text':' pass this struct of references around. LSTMs with projections have 5th argument w_hr, for all','line_number':161,'multiline':False]['text':' other models it's always going to be undefined.','line_number':162,'multiline':False]['text':' optional ','line_number':174,'multiline':True]['text':' optional ','line_number':175,'multiline':True]['text':' only defined for LSTMs with projections ','line_number':176,'multiline':True]['text':'w_ih=','line_number':313,'multiline':True]['text':'w_hh=','line_number':314,'multiline':True]['text':'b_ih_=','line_number':315,'multiline':True]['text':'b_hh_=','line_number':316,'multiline':True]['text':'packed_ih=','line_number':317,'multiline':True]['text':'packed_hh=','line_number':318,'multiline':True]['text':'col_offsets_ih=','line_number':319,'multiline':True]['text':'col_offsets_hh=','line_number':320,'multiline':True]['text':'scale_ih=','line_number':321,'multiline':True]['text':'scale_hh=','line_number':322,'multiline':True]['text':'zero_point_ih=','line_number':323,'multiline':True]['text':'zero_point_hh=','line_number':324,'multiline':True]['text':'qw_ih=','line_number':350,'multiline':True]['text':'qw_hh=','line_number':351,'multiline':True]['text':'b_ih=','line_number':352,'multiline':True]['text':'b_hh=','line_number':353,'multiline':True]['text':'packed_ih=','line_number':354,'multiline':True]['text':'packed_hh=','line_number':355,'multiline':True]['text':'col_offsets_ih=','line_number':356,'multiline':True]['text':'col_offsets_hh=','line_number':357,'multiline':True]['text':'scale_ih=','line_number':358,'multiline':True]['text':'scale_hh=','line_number':359,'multiline':True]['text':'zero_point_ih=','line_number':360,'multiline':True]['text':'zero_point_hh=','line_number':361,'multiline':True]['text':' QuantizedCellParams vs. QuantizedCellParamsDynamic','line_number':364,'multiline':False]['text':'','line_number':365,'multiline':False]['text':' QuantizedCellParams uses the legacy','line_number':366,'multiline':False]['text':' fbgemm_linear_int8_weight_fp32_activation API, which requires the explicit','line_number':367,'multiline':False]['text':' scale and zero point parameters for the weight. QuantizedCellParamsDynamic','line_number':368,'multiline':False]['text':' uses the new fbgemm_linear_dynamic API, which doesn't require the explicit','line_number':369,'multiline':False]['text':' scale and zero point parameters. These quantization parameters are','line_number':370,'multiline':False]['text':' encapsulated in the `PackedLinearWeight` struct in','line_number':371,'multiline':False]['text':' aten/src/ATen/native/quantized/cpu/fbgemm_utils.h.','line_number':372,'multiline':False]['text':' Prepacked Weight Tensor ','line_number':384,'multiline':True]['text':' Prepacked Weight Tensor ','line_number':386,'multiline':True]['text':' float Bias Tensor ','line_number':387,'multiline':True]['text':' float Bias Tensor ','line_number':388,'multiline':True]['text':' Use reduced range for activation tensors ','line_number':389,'multiline':True]['text':'b_ih=','line_number':424,'multiline':True]['text':'b_hh=','line_number':425,'multiline':True]['text':' reduce_range parameter is serialized along with the int field values.','line_number':431,'multiline':False]['text':'w_ih_packed=','line_number':451,'multiline':True]['text':'w_hh_packed=','line_number':452,'multiline':True]['text':'bias_ih=','line_number':453,'multiline':True]['text':'bias_hh=','line_number':454,'multiline':True]['text':'reduce_range=','line_number':455,'multiline':True]['text':'_packed_w_ih=','line_number':467,'multiline':True]['text':'_packed_w_hh=','line_number':468,'multiline':True]['text':'_b_ih=','line_number':469,'multiline':True]['text':'_b_hh=','line_number':470,'multiline':True]['text':'_reduce_range=','line_number':471,'multiline':True]['text':' unused ','line_number':489,'multiline':True]['text':' unused ','line_number':492,'multiline':True]['text':'w_ih_packed=','line_number':523,'multiline':True]['text':'w_hh_packed=','line_number':524,'multiline':True]['text':' Stupid wrapper to convert from -> to .','line_number':543,'multiline':False]['text':' Gathers every two elements of a vector in a vector of pairs','line_number':573,'multiline':False]['text':' Flattens a vector of pairs','line_number':585,'multiline':False]['text':' Parses a flat list of parameter tensors into a list of CellParams','line_number':597,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':629,'multiline':False]['text':' HIDDEN STATE FUNCTIONS','line_number':630,'multiline':False]['text':'','line_number':631,'multiline':False]['text':' Functions implemented below are implemented as templates based on hidden type,','line_number':632,'multiline':False]['text':' because they need to work both with simple RNNs and GRU (which use a single Tensor),','line_number':633,'multiline':False]['text':' as well as with LSTM (or possibly more complicated architectures in the future).','line_number':634,'multiline':False]['text':' Still, there are some operations that need to be performed on the hidden states','line_number':635,'multiline':False]['text':' alone, and for this purpose we provide an overloaded set of functions below.','line_number':636,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':664,'multiline':False]['text':' CELL IMPLEMENTATIONS','line_number':665,'multiline':False]['text':'','line_number':666,'multiline':False]['text':' Cell is a basic component of an RNN, representing a single application of the','line_number':667,'multiline':False]['text':' recurrent function. You can think of it as a function of signature','line_number':668,'multiline':False]['text':'','line_number':669,'multiline':False]['text':' (Tensor input, hidden_type hidden, CellParams) -> hidden_type','line_number':670,'multiline':False]['text':'','line_number':671,'multiline':False]['text':' which means that it consumes an input tensor, and updates the previous hidden state.','line_number':672,'multiline':False]['text':' It's a struct only because functional programming in C++ is a pain, and it's easier','line_number':673,'multiline':False]['text':' to pass around "vtable pointers" than actual function pointers.','line_number':674,'multiline':False]['text':' This is really dumb, but enables projects with','line_number':697,'multiline':False]['text':' -Wnon-virtual-dtor to compile...','line_number':698,'multiline':False]['text':' TODO: can use inplace ops?','line_number':720,'multiline':False]['text':' applying projections if w_hr is defined','line_number':739,'multiline':False]['text':' Slice off the workspace argument (it's needed only for AD).','line_number':741,'multiline':False]['text':' Slice off the workspace argument (it's needed only for AD).','line_number':775,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':792,'multiline':False]['text':' LAYER IMPLEMENTATIONS','line_number':793,'multiline':False]['text':'','line_number':794,'multiline':False]['text':' Layers are scan-like higher-order functions, which take in cells, and','line_number':795,'multiline':False]['text':' transform them to functions of signature','line_number':796,'multiline':False]['text':'','line_number':797,'multiline':False]['text':' (io_type input, hidden_type hidden, param_type params) -> (io_type, hidden_type)','line_number':798,'multiline':False]['text':'','line_number':799,'multiline':False]['text':' which can apply the cell over a sequence of inputs, and produce both a new set','line_number':800,'multiline':False]['text':' of hidden states, as well as a concatenated output of each step.','line_number':801,'multiline':False]['text':' This is really dumb, but enables projects with','line_number':813,'multiline':False]['text':' -Wnon-virtual-dtor to compile...','line_number':814,'multiline':False]['text':' Batch sizes is a sequence of decreasing lengths, which are offsets','line_number':948,'multiline':False]['text':' into a 1D list of inputs. At every step we slice out batch_size elements,','line_number':949,'multiline':False]['text':' and possibly account for the decrease in the batch size since the last step,','line_number':950,'multiline':False]['text':' which requires us to slice the hidden state (since some sequences','line_number':951,'multiline':False]['text':' are completed now). The sliced parts are also saved, because we will need','line_number':952,'multiline':False]['text':' to return a tensor of final hidden state.','line_number':953,'multiline':False]['text':' Here the situation is similar to that above, except we start out with','line_number':1007,'multiline':False]['text':' the smallest batch size (and a small set of hidden states we actually use),','line_number':1008,'multiline':False]['text':' and progressively expand the hidden states, as we move backwards over the','line_number':1009,'multiline':False]['text':' 1D list of inputs.','line_number':1010,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1062,'multiline':False]['text':' apply_layer_stack','line_number':1063,'multiline':False]['text':'','line_number':1064,'multiline':False]['text':' layers are convenient, but in reality we often want to stack them. this little','line_number':1065,'multiline':False]['text':' helper manages slicing of all inputs and parameters, and repeatedly feeds them','line_number':1066,'multiline':False]['text':' into the given layer. returns the last layer's outputs, and a vector of final','line_number':1067,'multiline':False]['text':' hidden states produced at each level.','line_number':1068,'multiline':False]['text':'train=','line_number':1071,'multiline':True]['text':'train=','line_number':1075,'multiline':True]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1103,'multiline':False]['text':' HELPERS SIMPLIFYING DISPATCH TO FUNCTIONS ABOVE','line_number':1104,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1105,'multiline':False]['text':' It's much more useful for us to work on lists of pairs of hx and cx for each layer, so we need','line_number':1139,'multiline':False]['text':' to transpose a pair of those tensors.','line_number':1140,'multiline':False]['text':' Now, we need to reverse the transposed we performed above.','line_number':1152,'multiline':False]['text':' anonymous namespace','line_number':1163,'multiline':False]['text':' NB: This a (composite) wrapper for _thnn_fused_lstm_cell_backward_impl.','line_number':1169,'multiline':False]['text':'     It duplicates the outputs of this function so the non-composite verison doesn't have to.','line_number':1170,'multiline':False]['text':'     The point is so that we avoid triggering TensorImpl use count asserts in debug mode','line_number':1171,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1181,'multiline':False]['text':' PUBLIC FUNCTIONS','line_number':1182,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1183,'multiline':False]['text':' BC wrappers for quantized_gru','line_number':1379,'multiline':False]['text':' if cells are of different size, that means projections are used','line_number':1453,'multiline':False]['text':' if cells are of different size, that means projections are used','line_number':1506,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1533,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1553,'multiline':False]['text':'keepdim=','line_number':1598,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1607,'multiline':False]['text':'keepdim=','line_number':1638,'multiline':True]['text':'keepdim=','line_number':1639,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1647,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1661,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1675,'multiline':False]['text':' Quantized implementations','line_number':1686,'multiline':False]['text':'','line_number':1687,'multiline':False]['text':' These implementations use FBGEMM to do the i2h and h2h linear layers with','line_number':1688,'multiline':False]['text':' an int8 or float16 quantized weight. This is advantageous in small-batch-size','line_number':1689,'multiline':False]['text':' scenarios where runtime is dominated by memory fetches of the weight matrix.','line_number':1690,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':1722,'multiline':False]['text':' BC wrappers for quantized_lstm','line_number':1744,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':1791,'multiline':False]['text':' Set reduced range to be True for all RNN Cells by default. This flag is used only for FBGEMM kernels','line_number':1863,'multiline':False]['text':' QNNPACK does not reduce range for activations','line_number':1864,'multiline':False]['text':' Quantized LSTM cell','line_number':1884,'multiline':False]['text':' Quantized LSTM cell','line_number':1891,'multiline':False]['text':' Helpers for simpler cells','line_number':1898,'multiline':False]['text':' Quantized GRU cell','line_number':1904,'multiline':False]['text':' Quantized RNN w/ ReLU cell','line_number':1912,'multiline':False]['text':' Quantized RNN w/ tanh cell','line_number':1918,'multiline':False]['text':' namespace','line_number':1993,'multiline':False]['text':' namespace at::native','line_number':1994,'multiline':False]