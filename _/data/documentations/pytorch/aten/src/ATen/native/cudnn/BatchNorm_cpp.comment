['text':' See Note [ATen preprocessor philosophy]','line_number':10,'multiline':False]['text':' namespace at::native','line_number':24,'multiline':False]['text':' AT_CUDNN_ENABLED','line_number':26,'multiline':False]['text':' CUDNN_VERSION >= 7400','line_number':66,'multiline':False]['text':' CUDNN_VERSION >= 8100','line_number':74,'multiline':False]['text':' TODO: The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was','line_number':77,'multiline':False]['text':' introduced in CuDNN 7 for performance optimization, but it results in','line_number':78,'multiline':False]['text':' accuracy losses in convolution models such as ResNeXt-101 and','line_number':79,'multiline':False]['text':' video R(2+1)D. We will fall back to the normal CUDNN_BATCHNORM_SPATIAL','line_number':80,'multiline':False]['text':' namespace','line_number':85,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':91,'multiline':False]['text':' TODO: is weight required to be contiguous?','line_number':115,'multiline':False]['text':' TODO: TensorArg check should start handle memory format','line_number':117,'multiline':False]['text':' exclusive ','line_number':120,'multiline':True]['text':' input descriptor','line_number':140,'multiline':False]['text':' descriptor for weight, bias, running_mean, etc.','line_number':141,'multiline':False]['text':' get the reserved size and allocate as tensor','line_number':170,'multiline':False]['text':' z descriptor for BN-Add-Relu','line_number':189,'multiline':False]['text':' z for BN-Add-ReLU','line_number':190,'multiline':False]['text':' CUDNN_VERSION >= 7400','line_number':222,'multiline':False]['text':' This keeps a consistent output with native_batch_norm','line_number':225,'multiline':False]['text':' save_mean and save_var can be undefined','line_number':240,'multiline':False]['text':' If this causes problems, we can initialize them to empty tensors','line_number':241,'multiline':False]['text':' of the correct type','line_number':242,'multiline':False]['text':' NB: CuDNN only implements the backward algorithm for batchnorm','line_number':246,'multiline':False]['text':' in training mode (evaluation mode batchnorm has a different algorithm),','line_number':247,'multiline':False]['text':' which is why this doesn't accept a 'training' parameter.','line_number':248,'multiline':False]['text':' Unused: but we require them to be passed so that double backwards','line_number':253,'multiline':False]['text':' has access','line_number':254,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':261,'multiline':False]['text':' TODO: Is it worth it to have a contiguous call or maybe we should go with','line_number':267,'multiline':False]['text':' whatever format is given here.','line_number':268,'multiline':False]['text':' TODO: is weight required to be contiguous?','line_number':288,'multiline':False]['text':' TODO: TensorArg check should start handle memory format','line_number':290,'multiline':False]['text':' exclusive ','line_number':293,'multiline':True]['text':' training','line_number':301,'multiline':False]['text':' input, grad_output descriptor','line_number':313,'multiline':False]['text':' input, grad_output descriptor','line_number':314,'multiline':False]['text':' descriptor for weight, save_mean, etc.','line_number':315,'multiline':False]['text':' CUDNN_VERSION >= 7400','line_number':369,'multiline':False]['text':' namespace native','line_number':374,'multiline':False]