['text':' for the definition of AT_CUDNN_ENABLED','line_number':2,'multiline':False]['text':' we currently assume both inputs are given as the same size (i.e., no broadcasting)','line_number':38,'multiline':False]['text':' FIXME: make this thread-safe by reusing the benchmark cache in Conv_v7.cpp','line_number':64,'multiline':False]['text':' we currently set the maximum number of input dimensions to 5','line_number':65,'multiline':False]['text':' this can be increased, if necessary','line_number':66,'multiline':False]['text':' TODO: this is also in BinaryOps.cpp and some other cpp files in quantized/cpu/. I think we should','line_number':69,'multiline':False]['text':' move everything into a utilities file in quantized/ directory later.','line_number':70,'multiline':False]['text':' currently we only support int8 symmetric (zero_point = 0 for inputs and output) quantized add','line_number':83,'multiline':False]['text':' We implement relu ( (a_int8 + b_int8 * ( b_scale/a_scale) ) ) * ( a_scale / out_scale )','line_number':84,'multiline':False]['text':' which requires 4 cudnn ops (2 multiplication, 1 addition, and 1 relu ops)','line_number':85,'multiline':False]['text':' Multiplication ops: rhs_mult_op, requant_op','line_number':86,'multiline':False]['text':' Addition op: add_op','line_number':87,'multiline':False]['text':' Relu op: relu_op','line_number':88,'multiline':False]['text':' TODO: add shape checking when broadcasted add is supported. For now we assume the input tensors are the same shape','line_number':94,'multiline':False]['text':' cudnn expects tensors to be at least 3D. So we will prepend dummy dimensions if the input tensors are not at least 3D','line_number':99,'multiline':False]['text':' cudnn expects leading dimensions to be the dummy dimensions','line_number':103,'multiline':False]['text':' memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are','line_number':126,'multiline':False]['text':' used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two','line_number':127,'multiline':False]['text':' CacheKey objects have the same user defined parameters, but','line_number':128,'multiline':False]['text':' different padded values, resulting in different hash outputs.','line_number':129,'multiline':False]['text':' computes qb_int8 * ( qb_scale/qa_scale )','line_number':171,'multiline':False]['text':' add_op computes (qa_int8 + qb_int8 * ( qb_scale/qa_scale ) )','line_number':179,'multiline':False]['text':' add_output is a fp32 tensor for accumulation purposes','line_number':180,'multiline':False]['text':' relu_op computes','line_number':188,'multiline':False]['text':' relu( (qa_int8 + qb_int8 * ( qb_scale/qa_scale ) )  )','line_number':189,'multiline':False]['text':' output is a fp32 tensor','line_number':190,'multiline':False]['text':' we use inplace operation here where the output is assigned to the input','line_number':193,'multiline':False]['text':' requant_op computes','line_number':201,'multiline':False]['text':' (a_int8 + b_int8 * ( b_scale/a_scale) ) * a_scale / out_scale','line_number':202,'multiline':False]['text':' std::cout << "opGraph: " << opGraph.describe() << std::endl;','line_number':220,'multiline':False]['text':'ReLUFused=','line_number':254,'multiline':True]['text':'ReLUFused=','line_number':255,'multiline':True]['text':' namespace','line_number':258,'multiline':False]['text':' namespace native','line_number':259,'multiline':False]['text':' namespace at','line_number':260,'multiline':False]['text':' HAS_CUDNN_V8','line_number':262,'multiline':False]['text':' AT_CUDNN_ENABLED','line_number':263,'multiline':False]['text':' USE_CUDA','line_number':264,'multiline':False]