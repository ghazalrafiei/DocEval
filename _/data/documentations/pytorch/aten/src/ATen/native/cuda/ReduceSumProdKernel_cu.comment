['text':' jiterated specialization for `complex<Half>`','line_number':23,'multiline':False]['text':' jiterator reduction fails on windows','line_number':27,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/77305','line_number':28,'multiline':False]['text':' jiterator reduction fails on windows','line_number':85,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/77305','line_number':86,'multiline':False]['text':' Workaround for the error: '*' in boolean context, suggest '&&' instead [-Werror=int-in-bool-context]','line_number':107,'multiline':False]['text':' jiterated specialization for `complex<Half>`','line_number':118,'multiline':False]['text':' jiterator reduction fails on windows','line_number':121,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/77305','line_number':122,'multiline':False]['text':' The function `reduce_dispatch` below dispatches to the kernel based','line_number':143,'multiline':False]['text':' on the type of `iter`. It takes care of the common logic','line_number':144,'multiline':False]['text':' for handling Half-Precision floating types.','line_number':145,'multiline':False]['text':' Otherwise the functor `op` is called to dispatch to the kernel','line_number':146,'multiline':False]['text':' of relevant type.','line_number':147,'multiline':False]['text':'','line_number':148,'multiline':False]['text':' Note: Functor `op` should take care of all the types to be supported','line_number':149,'multiline':False]['text':'       except for `at::Half` and `at::BFloat16`.','line_number':150,'multiline':False]['text':' type promotion that does cast and reduction in a single kernel','line_number':162,'multiline':False]['text':' type promotion that does cast and reduction in a single kernel','line_number':167,'multiline':False]['text':' namespace at::native','line_number':215,'multiline':False]