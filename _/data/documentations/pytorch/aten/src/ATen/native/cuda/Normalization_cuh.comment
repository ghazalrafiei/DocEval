['text':' The maximum number of threads in a block','line_number':24,'multiline':False]['text':' Number of threads in a block given an input size up to MAX_BLOCK_SIZE','line_number':33,'multiline':False]['text':' Returns the index of the most significant 1 bit in `val`.','line_number':48,'multiline':False]['text':' Sum across (batch, x/y/z) applying Op() pointwise','line_number':104,'multiline':False]['text':' this works by first having each thread sum it's part','line_number':105,'multiline':False]['text':' of the data. Then there is a double-shuffling reduction.','line_number':106,'multiline':False]['text':' First each warp (of C10_WARP_SIZE threads) uses warpSum to reduce its','line_number':107,'multiline':False]['text':' data to the "warp leader", who writes its value into shared memory.','line_number':108,'multiline':False]['text':' Then a single warp reads the remaining (at most C10_WARP_SIZE) items','line_number':109,'multiline':False]['text':' and reduces them using another warpSum.','line_number':110,'multiline':False]['text':' The implicit assumption is that there are no more','line_number':111,'multiline':False]['text':' than C10_WARP_SIZE**2 threads.','line_number':112,'multiline':False]['text':' first the reductions each thread does separately','line_number':115,'multiline':False]['text':' Everyone picks it up, should be broadcast into the whole grad_input','line_number':129,'multiline':False]['text':' enables concurrency within each thread to hide latency','line_number':133,'multiline':False]['text':' it's not worth having a grid reduction if the reduction dimension is not big enough','line_number':154,'multiline':False]['text':' merge mean/m2n among threadIdx.y within block','line_number':180,'multiline':False]['text':' write to shared memory','line_number':188,'multiline':False]['text':' read shared memory back to register for reduction','line_number':201,'multiline':False]['text':' Compute the mean and variance across (batch, x/y/z)','line_number':282,'multiline':False]['text':' this uses the Welford (in the for loop)/parallel algorithm (to sum across the block)','line_number':283,'multiline':False]['text':' https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm','line_number':284,'multiline':False]['text':' and the parallel algorithm on the same page.','line_number':285,'multiline':False]['text':' We use two shuffles to reduce across the entire block.','line_number':286,'multiline':False]['text':' https://devblogs.nvidia.com/faster-parallel-reductions-kepler/ has a description.','line_number':287,'multiline':False]['text':' first the reductions each thread does separately','line_number':290,'multiline':False]['text':' first warpSum to get one value per thread to','line_number':304,'multiline':False]['text':' one value per warp','line_number':305,'multiline':False]['text':' this writes each warps  item into shared memory','line_number':315,'multiline':False]['text':' there are at most C10_WARP_SIZE items left because','line_number':316,'multiline':False]['text':' there are at most C10_WARP_SIZE**2 threads at the beginning','line_number':317,'multiline':False]['text':' now have a second warpSum to reduce the intermediate values','line_number':325,'multiline':False]['text':' from shared memory to a single number. The very first','line_number':326,'multiline':False]['text':' thread writes it to shared memory.','line_number':327,'multiline':False]['text':' Save the mean, variance, and moving averages','line_number':343,'multiline':False]['text':' Compute two values across (batch, x/y/z) in one pass:','line_number':385,'multiline':False]['text':' 1. Sum(grad_output)','line_number':386,'multiline':False]['text':' 2. DotProduct(input - mean, grad_output)','line_number':387,'multiline':False]['text':' first the reductions each thread does separately','line_number':444,'multiline':False]['text':' internally we merge the feature dimensions','line_number':668,'multiline':False]['text':' internally we merge the feature dimensions','line_number':699,'multiline':False]['text':' NOTE: We use transform_input_kernel in training mode, which ignores epsilon','line_number':716,'multiline':False]['text':' The input_transform kernel is pointwise, but we need to balance reading parameters (save_var/mean,','line_number':719,'multiline':False]['text':' weight/bias) - which we only do once and have a for loop afterwards - with having many threads and blocks','line_number':720,'multiline':False]['text':' and good occupancy. Quiet likely, we could go with even more blocks than 1024.','line_number':721,'multiline':False]['text':' The various planes are independent, so we use blocks for them.','line_number':722,'multiline':False]['text':' internally we merge the feature dimensions','line_number':788,'multiline':False]['text':' We want block_x to be at least a warp width','line_number':825,'multiline':False]['text':' internally we merge the feature dimensions','line_number':844,'multiline':False]['text':' The kernel is pointwise, but we need to balance reading parameters (save_var/mean,','line_number':867,'multiline':False]['text':' weight/bias) - which we only do once and have a for loop afterwards - with having many threads and blocks','line_number':868,'multiline':False]['text':' and good occupancy. Quiet likely, we could go with even more blocks than 1024.','line_number':869,'multiline':False]['text':' The various planes are independent, so we use blocks for them.','line_number':870,'multiline':False]['text':' internally we merge the feature dimensions','line_number':895,'multiline':False]['text':' The kernel is pointwise, but we need to balance reading parameters (save_var/mean,','line_number':918,'multiline':False]['text':' weight/bias) - which we only do once and have a for loop afterwards - with having many threads and blocks','line_number':919,'multiline':False]['text':' and good occupancy. Quiet likely, we could go with even more blocks than 1024.','line_number':920,'multiline':False]['text':' The various planes are independent, so we use blocks for them.','line_number':921,'multiline':False]['text':' welford kernel for c last tensor calculating mean/biased_variance/unbiased_variance','line_number':936,'multiline':False]['text':' original apex name: welford_kernel_c_last','line_number':937,'multiline':False]['text':' hide latency with concurrency','line_number':953,'multiline':False]['text':' tensor dimension (m,c)','line_number':964,'multiline':False]['text':' loop along m dimension','line_number':966,'multiline':False]['text':' offset along m dimension','line_number':969,'multiline':False]['text':' load multiple data in','line_number':982,'multiline':False]['text':' calculate mean/m2n with welford','line_number':999,'multiline':False]['text':' thread reduction to accumulate mean/m_2_n/count between PARALLEL_LOADS','line_number':1009,'multiline':False]['text':' release x_mean / m_2_n','line_number':1015,'multiline':False]['text':' block-wise reduction with shared memory (since reduction cannot be done within a warp)','line_number':1020,'multiline':False]['text':' write data to staging_data;','line_number':1033,'multiline':False]['text':' ensuring writes to staging_ is visible to all blocks','line_number':1041,'multiline':False]['text':' mark block done','line_number':1044,'multiline':False]['text':' check that all data is now available in global memory','line_number':1052,'multiline':False]['text':' elementwise BN kernel','line_number':1081,'multiline':False]['text':' original apex name: batchnorm_forward_c_last_kernel','line_number':1082,'multiline':False]['text':' tensor dimension (m,c)','line_number':1099,'multiline':False]['text':' loop along m dimension','line_number':1100,'multiline':False]['text':' offset along m dimension','line_number':1103,'multiline':False]['text':' write to shared memory','line_number':1141,'multiline':False]['text':' batchnorm backward kernel for c last tensor','line_number':1160,'multiline':False]['text':' original apex name: reduce_bn_c_last_kernel','line_number':1161,'multiline':False]['text':' hide latency with concurrency','line_number':1181,'multiline':False]['text':' tensor dimension (m,c)','line_number':1190,'multiline':False]['text':' loop along m dimension','line_number':1192,'multiline':False]['text':' offset along m dimension','line_number':1195,'multiline':False]['text':' load multiple data in','line_number':1214,'multiline':False]['text':' calculate sum_dy / sum_dy_xmu','line_number':1228,'multiline':False]['text':' thread reduction to accumulate sum_dy / sum_dy_xmu between PARALLEL_LOADS','line_number':1236,'multiline':False]['text':' release array of registers','line_number':1243,'multiline':False]['text':' block-wise reduction with shared memory (since reduction cannot be done within a warp)','line_number':1247,'multiline':False]['text':' write data to staging_data;','line_number':1258,'multiline':False]['text':' ensuring writes to staging_ is visible to all blocks','line_number':1265,'multiline':False]['text':' mark block done','line_number':1268,'multiline':False]['text':' check that all data is now available in global memory','line_number':1276,'multiline':False]['text':'mean_dy[c_offset] = sum_dy_th / reduction_size;','line_number':1295,'multiline':False]['text':'mean_dy_xmu[c_offset] = sum_dy_xmu_th / reduction_size;','line_number':1296,'multiline':False]['text':'mean_dy[c_offset] = sum_dy_th / reduction_size;','line_number':1309,'multiline':False]['text':'mean_dy_xmu[c_offset] = sum_dy_xmu_th / reduction_size;','line_number':1310,'multiline':False]['text':' elementwise BN kernel','line_number':1317,'multiline':False]['text':' original apex name: batchnorm_backward_c_last_kernel','line_number':1318,'multiline':False]['text':' tensor dimension (m,c)','line_number':1336,'multiline':False]['text':' loop along m dimension','line_number':1337,'multiline':False]['text':' offset along m dimension','line_number':1340,'multiline':False]['text':' bias of BN','line_number':1470,'multiline':False]['text':' bias after BN','line_number':1473,'multiline':False]['text':' because I cannot return an uninitialized at::Tensor','line_number':1546,'multiline':False]['text':' Input is guarunteed to be channels-last compatible','line_number':1626,'multiline':False]['text':' Input is guarunteed to be channels-last compatible','line_number':1694,'multiline':False]['text':' namespace at::native','line_number':1742,'multiline':False]