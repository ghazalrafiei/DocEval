['text':'
 * Copyright (c) Facebook, Inc. and its affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 ','line_number':1,'multiline':True]['text':' KERNEL_NAME and W_INDEX_DTYPE macros are defined in','line_number':30,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4c1x4-dq-packedA-sse2.c','line_number':31,'multiline':False]['text':' Packed A format.','line_number':49,'multiline':False]['text':' 8kx4m blocks for alls blocks given 4 rows (4m) are placed in contiguous memory.','line_number':50,'multiline':False]['text':' Original A','line_number':51,'multiline':False]['text':' --------- K -----------          -- (K + 4 - 1) / 4 --','line_number':52,'multiline':False]['text':' |                     |          |                   |','line_number':53,'multiline':False]['text':' |                     |        (M + 8 - 1)/8         |','line_number':54,'multiline':False]['text':' |                     | Packed   |                   |','line_number':55,'multiline':False]['text':' M                     |  =>      |-------------------|','line_number':56,'multiline':False]['text':' |                     |        Thus Packed A has (K + 4 - 1)/4 * (M + 8 -1)/8 blocks','line_number':57,'multiline':False]['text':' |                     |','line_number':58,'multiline':False]['text':' |---------------------|','line_number':59,'multiline':False]['text':'','line_number':60,'multiline':False]['text':' Each 8 x 4 blocks is transposed and stored.','line_number':61,'multiline':False]['text':' Each of the (K + 4 - 1)/4 blocks for a given group of 8 m blocks','line_number':62,'multiline':False]['text':' are stored adjacent in memory','line_number':63,'multiline':False]['text':' Thus, each block:','line_number':64,'multiline':False]['text':' |----8m-----|----8m-----|','line_number':65,'multiline':False]['text':' 4k          |           | .....','line_number':66,'multiline':False]['text':' |-----------|-----------|','line_number':67,'multiline':False]['text':' This locality helps in loading 8kx8m blocks of activations','line_number':68,'multiline':False]['text':' Note when M is not multiple of 8, the rest can contain arbitrary','line_number':69,'multiline':False]['text':' data in packed A as we will not be writing those out.','line_number':70,'multiline':False]['text':' This wil be taken care by just copying the appropriate valid data','line_number':71,'multiline':False]['text':' Offset into compressed values.','line_number':85,'multiline':False]['text':' w_row_ptr[0] is the block offset in the compressed values.','line_number':86,'multiline':False]['text':' Where the corresponding row of the weight matrix starts.','line_number':87,'multiline':False]['text':' Similarly w_row_ptr[0] is also the block offset where','line_number':89,'multiline':False]['text':' corresponding row's block column ids start.','line_number':90,'multiline':False]['text':' Per row # of block column ids = # of block values','line_number':91,'multiline':False]['text':' Load two 1x4 uint8 blocks 2 ints','line_number':94,'multiline':False]['text':' This is not perf optimal since this will result in','line_number':96,'multiline':False]['text':' register spills. We probably should work with output block','line_number':97,'multiline':False]['text':' of 1x4 instead of 1x8','line_number':98,'multiline':False]['text':' But doing is this way because mostly this how we will','line_number':99,'multiline':False]['text':' do it for ARM and this reference code helps establish','line_number':100,'multiline':False]['text':' the baseline for functional correctness.','line_number':101,'multiline':False]['text':' Now we will load 8kx1(broadcast 8) weight values','line_number':110,'multiline':False]['text':' Load activation blocks. In this kernel we assume','line_number':120,'multiline':False]['text':' a mat is already transposed. K x M','line_number':121,'multiline':False]['text':' 1. Load 8 1x8 registers = 8k x 8m','line_number':122,'multiline':False]['text':' Load column id of the first 1x4 block','line_number':124,'multiline':False]['text':' Load column id of the second 1x4 block','line_number':126,'multiline':False]['text':' acc += a0 * b0;','line_number':170,'multiline':False]['text':' acc += a1 * b1;','line_number':177,'multiline':False]['text':' acc += a2 * b2;','line_number':184,'multiline':False]['text':' acc += a3 * b3;','line_number':191,'multiline':False]['text':' acc += a4 * b4;','line_number':198,'multiline':False]['text':' acc += a5 * b5;','line_number':205,'multiline':False]['text':' acc += a6 * b6;','line_number':212,'multiline':False]['text':' acc += a7 * b7;','line_number':219,'multiline':False]['text':' Now we have 1x8 m acculated 32 bit values in vacc_low[n](4) and vacc_high[n](4)','line_number':227,'multiline':False]['text':' Load two 1x4 uint8 blocks 2 ints','line_number':234,'multiline':False]['text':' Now we will load 8kx1(broadcast 8) weight values','line_number':240,'multiline':False]['text':' Then load transformed weight blocks','line_number':246,'multiline':False]['text':' 1. Load 4 1x8 registers = 4k x 8m','line_number':247,'multiline':False]['text':' Thus have 4x8 (4k x 8m) activations a0, a1, a2, a3','line_number':248,'multiline':False]['text':' Each a containing 8 m values.','line_number':249,'multiline':False]['text':' Load column id of the first 1x4 block','line_number':251,'multiline':False]['text':' acc += a0 * b0;','line_number':274,'multiline':False]['text':' acc += a1 * b1;','line_number':281,'multiline':False]['text':' acc += a2 * b2;','line_number':288,'multiline':False]['text':' acc += a3 * b3;','line_number':295,'multiline':False]['text':' Now we have 1x8 m acculated 32 bit values in vacc_low[n](4) and vacc_high[n](4)','line_number':303,'multiline':False]['text':' Transform low half of 4x8 result','line_number':310,'multiline':False]['text':' That is 4x4 block (4n x 4m)','line_number':311,'multiline':False]['text':' Convert to FP and transpose: 4m x 4n','line_number':312,'multiline':False]