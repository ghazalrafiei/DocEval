['text':' computes `result = self <= threshold ? value : other`','line_number':86,'multiline':False]['text':' other is `self` in threshold() and `grad` in threshold_backward()','line_number':87,'multiline':False]['text':' threshold is idempotent, so overlap is okay','line_number':91,'multiline':False]['text':' other','line_number':94,'multiline':False]['text':' computes `result = self <= threshold ? value : other`','line_number':100,'multiline':False]['text':' other is `self` in threshold() and `grad` in threshold_backward()','line_number':101,'multiline':False]['text':' threshold is idempotent, so overlap is okay','line_number':105,'multiline':False]['text':' other','line_number':108,'multiline':False]['text':' Note: leakyReLu backward calculation doesn't support in-place call with negative slope.','line_number':173,'multiline':False]['text':' The reason is that for in-place forward call, the forward result will be saved into autograd','line_number':174,'multiline':False]['text':' node instead of the input itself, when calculating backward gradient, there is no way to know','line_number':175,'multiline':False]['text':' whether the original input for current node is positive or not if the input slope is','line_number':176,'multiline':False]['text':' negative. eg. forward is 2, slope is -0.2, the original input for this node could be','line_number':177,'multiline':False]['text':' either 2, or -10, so no way to get a correct backward gradient in this case.','line_number':178,'multiline':False]['text':' namespace meta','line_number':241,'multiline':False]['text':' input is mkldnn Tensor','line_number':384,'multiline':False]['text':' input is dense layout and bfloat16/float32','line_number':387,'multiline':False]['text':'alpha','line_number':400,'multiline':True]['text':'alpha','line_number':419,'multiline':True]['text':'preserve legacy behavior of boundaries not causing type promotion','line_number':436,'multiline':False]['text':'include_bool','line_number':438,'multiline':True]['text':'min_val=','line_number':521,'multiline':True]['text':'max_val=','line_number':521,'multiline':True]['text':'min_val=','line_number':529,'multiline':True]['text':'max_val=','line_number':529,'multiline':True]['text':' channel_size default to 1','line_number':692,'multiline':False]['text':' Adjust weight to broadcast over self and have weight.ndim == self.ndim','line_number':701,'multiline':False]['text':' This will always be a view in CPU/CUDA, but some backends','line_number':708,'multiline':False]['text':' like MKLDNN do not support views','line_number':709,'multiline':False]['text':' Weight broadcasts over self and they have the same dtype','line_number':717,'multiline':False]['text':' FIXME: do these actually need to be zeros_like or can they be empty_like?','line_number':752,'multiline':False]['text':' NOTE: buffer is only used by CPU dispatch, we just ignore it here','line_number':782,'multiline':False]['text':' namespace at::native','line_number':832,'multiline':False]