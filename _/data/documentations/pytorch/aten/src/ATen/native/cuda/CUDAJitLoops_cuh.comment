['text':' Jiterator functions are guarded behind this macro','line_number':4,'multiline':False]['text':' warning : unused parameter when tuple is empty.','line_number':34,'multiline':False]['text':' Helper function convert tuple to std::array<void*, N>','line_number':38,'multiline':False]['text':' for passing the arguments to CUDA Kernel','line_number':39,'multiline':False]['text':' NOTE: We capture tuple by reference,','line_number':40,'multiline':False]['text':' so the pointers in returned array are only valid','line_number':41,'multiline':False]['text':' till tuple is alive.','line_number':42,'multiline':False]['text':' Different kernels are compiled depending on what we're vectorizing up to (1, 2 or 4 elements)','line_number':50,'multiline':False]['text':'casting result to int is always safe, intermediate is int64 and won't overflow','line_number':93,'multiline':False]['text':' N is still int64_t for the computation, but it's always safe to cast result to int','line_number':119,'multiline':False]['text':' Different kernels are compiled depending on what we're vectorizing up to (1, 2 or 4 elements)','line_number':124,'multiline':False]['text':'   fn_ptr is set to the appropriate function based on the vec size and GPU used','line_number':125,'multiline':False]['text':' cache miss!','line_number':141,'multiline':False]['text':' Generates program','line_number':143,'multiline':False]['text':'contiguous=','line_number':145,'multiline':True]['text':'dynamic_casting=','line_number':145,'multiline':True]['text':' Acquires the program','line_number':149,'multiline':False]['text':' NVCC complains about unused variables l and s.','line_number':159,'multiline':False]['text':' It should be false positive in most cases, so we suppress the warnings.','line_number':160,'multiline':False]['text':' Decides which of 4 kernel types to launch','line_number':199,'multiline':False]['text':' Variations are:','line_number':200,'multiline':False]['text':'   - Case 1: no dynamic casting and contiguous','line_number':201,'multiline':False]['text':'   - Case 2: no dynamic casting and noncontiguous','line_number':202,'multiline':False]['text':'   - Case 3: dynamic casting and contiguous','line_number':203,'multiline':False]['text':'   - Case 4: dynamic casting and noncontiguous','line_number':204,'multiline':False]['text':' These cases align with the non-jitted CUDALoops.cuh cases in gpu_kernel_impl','line_number':205,'multiline':False]['text':' Case 1: no dynamic casting and contiguous','line_number':209,'multiline':False]['text':' Case 2: no dynamic casting and noncontiguous','line_number':216,'multiline':False]['text':' Cases 3 and 4 are handled below','line_number':228,'multiline':False]['text':' Both require construction of a storer (this asserts 1 output) and one or more loaders','line_number':229,'multiline':False]['text':' Creates store cast to output (the zeroth tensor in TensorIterator)','line_number':231,'multiline':False]['text':' Creates load casts from inputs (note offset indexing into the iterators 1...n tensors)','line_number':234,'multiline':False]['text':' Case 3: dynamic casting and contiguous','line_number':238,'multiline':False]['text':' Case 4: dynamic casting and noncontiguous','line_number':247,'multiline':False]['text':' NOTE: static to reduce chances of name collision.','line_number':255,'multiline':False]['text':' TODO: Memory use can probably be optimized by re-using kernels across GPUs with','line_number':271,'multiline':False]['text':'   the same compute capability','line_number':272,'multiline':False]['text':' TODO: Support more than 1 output','line_number':277,'multiline':False]['text':' at::native','line_number':295,'multiline':False]['text':' AT_USE_JITERATOR()','line_number':297,'multiline':False]