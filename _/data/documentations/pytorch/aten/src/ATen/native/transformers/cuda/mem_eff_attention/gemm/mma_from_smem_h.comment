['text':'**************************************************************************************************
 * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights
 *reserved. SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *POSSIBILITY OF SUCH DAMAGE.
 *
 *************************************************************************************************','line_number':1,'multiline':True]['text':'! \file
    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
','line_number':32,'multiline':True]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':71,'multiline':False]['text':'/ Shared storage object needed by accumulator','line_number':77,'multiline':False]['text':'/ From 13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h','line_number':78,'multiline':False]['text':'','line_number':86,'multiline':False]['text':' Type definitions','line_number':87,'multiline':False]['text':'','line_number':88,'multiline':False]['text':'/ Tensor reference to the accumulator','line_number':94,'multiline':False]['text':'/ Shape of the accumulator matrix in shared memory','line_number':97,'multiline':False]['text':'','line_number':102,'multiline':False]['text':' Data members','line_number':103,'multiline':False]['text':'','line_number':104,'multiline':False]['text':'/ Buffer for accumulator','line_number':106,'multiline':False]['text':'','line_number':110,'multiline':False]['text':' Methods','line_number':111,'multiline':False]['text':'','line_number':112,'multiline':False]['text':'/ Returns a layout object for the Accum matrix','line_number':114,'multiline':False]['text':'/ Returns a TensorRef to the Accumulator','line_number':120,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':127,'multiline':False]['text':' Taken from','line_number':128,'multiline':False]['text':' https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h','line_number':129,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':130,'multiline':False]['text':'/ Structure to compute the matrix product targeting CUDA cores and SIMT math','line_number':132,'multiline':False]['text':'/ instructions.','line_number':133,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':135,'multiline':False]['text':' Maximum K dimension - also the dimension of the shared-memory','line_number':137,'multiline':False]['text':' holding `OperandA`','line_number':138,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':140,'multiline':False]['text':'/ Number of stages,','line_number':142,'multiline':False]['text':'/ Layout in shared-memory of operand A','line_number':144,'multiline':False]['text':'/ Used for partial specialization','line_number':146,'multiline':False]['text':'/< Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':150,'multiline':False]['text':'/< Policy describing tuning details','line_number':154,'multiline':False]['text':'','line_number':157,'multiline':False]['text':' Dependent types','line_number':158,'multiline':False]['text':'','line_number':159,'multiline':False]['text':'/ Warp-level Mma','line_number':161,'multiline':False]['text':'/ Shape describing the overall GEMM computed from shared memory','line_number':164,'multiline':False]['text':'/ by each warp.','line_number':165,'multiline':False]['text':'/ Shape describing the number of warps filling the CTA','line_number':168,'multiline':False]['text':'/ Number of warp-level GEMM oeprations','line_number':175,'multiline':False]['text':'/ Number of stages','line_number':180,'multiline':False]['text':'/ If this is true, we fill the entire shmem buffer at start','line_number':183,'multiline':False]['text':'/ and don't need to iterate through it in a circular fashion','line_number':184,'multiline':False]['text':'/ Tensor reference to the A operand','line_number':187,'multiline':False]['text':'/ Tensor reference to the B operand','line_number':190,'multiline':False]['text':'','line_number':194,'multiline':False]['text':' Nested structs','line_number':195,'multiline':False]['text':'','line_number':196,'multiline':False]['text':'/ Shared storage object needed by threadblock-scoped GEMM','line_number':198,'multiline':False]['text':'','line_number':201,'multiline':False]['text':' Type definitions','line_number':202,'multiline':False]['text':'','line_number':203,'multiline':False]['text':'/ Shape of the B matrix operand in shared memory','line_number':205,'multiline':False]['text':'','line_number':211,'multiline':False]['text':' Data members','line_number':212,'multiline':False]['text':'','line_number':213,'multiline':False]['text':'/ Buffer for B operand','line_number':215,'multiline':False]['text':'','line_number':219,'multiline':False]['text':' Methods','line_number':220,'multiline':False]['text':'','line_number':221,'multiline':False]['text':'/ Returns a layout object for the B matrix','line_number':223,'multiline':False]['text':'/ Returns a TensorRef to the B operand','line_number':229,'multiline':False]['text':'','line_number':237,'multiline':False]['text':' Data members','line_number':238,'multiline':False]['text':'','line_number':239,'multiline':False]['text':' /// Iterator to load a warp-scoped tile of A operand from shared memory','line_number':241,'multiline':False]['text':' typename Operator::IteratorA warp_tile_iterator_A_;','line_number':242,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of B operand from shared memory','line_number':244,'multiline':False]['text':'/ Construct from tensor references','line_number':248,'multiline':False]['text':'/< Shared storage needed for internal use by threadblock-scoped GEMM','line_number':251,'multiline':False]['text':'/< ID within the threadblock','line_number':253,'multiline':False]['text':'/< ID of warp','line_number':255,'multiline':False]['text':'/< ID of each thread within a warp','line_number':257,'multiline':False]['text':' has necessary trait compliance with WarpIteratorFromSmem but doesn't do','line_number':264,'multiline':False]['text':' anything, can be default initialized, and uses fragment that takes up','line_number':265,'multiline':False]['text':' (almost) no space. this warp iterator is selected at compile time when','line_number':266,'multiline':False]['text':' elementwise on-the-fly scaling for operand A is disabled, in which case','line_number':267,'multiline':False]['text':' operations related to loading scale factors for operand A get wiped out by','line_number':268,'multiline':False]['text':' the compiler.','line_number':269,'multiline':False]['text':' in pipelined+multistage MMA implementations we keep an array of fragments.','line_number':273,'multiline':False]['text':' if we aren't using scaling we don't want to waste registers on fragments','line_number':274,'multiline':False]['text':' of scale elements, so ideally this would be sized 0.','line_number':275,'multiline':False]['text':' Since arrays of zero-sized objects are not allowed, using size as 1.','line_number':276,'multiline':False]['text':' The compiler will most likely wipe it out anyways.','line_number':277,'multiline':False]['text':' if scaling is enabled, performs fragment elementwise multiplication between','line_number':301,'multiline':False]['text':' fragment and its scaling factor.','line_number':302,'multiline':False]['text':' specialization for scaling being enabled.','line_number':306,'multiline':False]['text':' cast scale_frag to correct type then apply elementwise to fragment','line_number':310,'multiline':False]['text':' specialization for scaling being disabled. doesn't do anything and should','line_number':321,'multiline':False]['text':' just get wiped out by the compiler.','line_number':322,'multiline':False]['text':' namespace','line_number':331,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':333,'multiline':False]['text':' Taken from','line_number':334,'multiline':False]['text':' https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h','line_number':335,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':336,'multiline':False]['text':'/ Structure to compute the matrix product targeting CUDA cores and SIMT math','line_number':338,'multiline':False]['text':'/ instructions.','line_number':339,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':341,'multiline':False]['text':' BEGIN smem','line_number':343,'multiline':False]['text':'/ Iterates over the intermediate accumulator tile in shared memory','line_number':344,'multiline':False]['text':'/ whether or not to perform elementwise multiplication of A','line_number':346,'multiline':False]['text':'  by another matrix (A_scale) that is also kept in shared memory prior','line_number':347,'multiline':False]['text':'  to matmul A @ B','line_number':348,'multiline':False]['text':'/ Max GEMM problem size in K dimension','line_number':350,'multiline':False]['text':'/ Iterates over tiles of B operand in global memory','line_number':352,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':353,'multiline':False]['text':'  MaskedTileIterator)','line_number':354,'multiline':False]['text':'/ Iterates over tiles of B operand in shared memory','line_number':356,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':357,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':359,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':361,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':363,'multiline':False]['text':'/ Transformation applied to B operand','line_number':365,'multiline':False]['text':'/ Used for partial specialization','line_number':370,'multiline':False]['text':'/< Base class','line_number':379,'multiline':False]['text':'/< Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':388,'multiline':False]['text':'/< loads fragments of A_scale from shared memory if operand A scaling is','line_number':392,'multiline':False]['text':'/< enabled. otherwise no-op.','line_number':393,'multiline':False]['text':'/< Iterates over tiles of B operand in global memory','line_number':400,'multiline':False]['text':'/< Data type of accumulator matrix','line_number':401,'multiline':False]['text':'/< Layout of accumulator matrix','line_number':402,'multiline':False]['text':'/< Policy describing tuning details','line_number':403,'multiline':False]['text':'','line_number':409,'multiline':False]['text':' Dependent types','line_number':410,'multiline':False]['text':'','line_number':411,'multiline':False]['text':'/ Fragment of operand B loaded from global memory','line_number':413,'multiline':False]['text':'/ Fragment of accumulator tile','line_number':416,'multiline':False]['text':'/ Warp-level Mma','line_number':419,'multiline':False]['text':'/ Obtain the arch tag from the warp-level operator','line_number':422,'multiline':False]['text':'/ Complex transform on B operand','line_number':425,'multiline':False]['text':' staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)','line_number':428,'multiline':False]['text':'/ fragment type of OperandA elementwise scaling matrix. (almost) empty','line_number':436,'multiline':False]['text':'/ if operand A scaling is disabled.','line_number':437,'multiline':False]['text':'/ applies scaling factor to operand A fragment if operand A scaling is','line_number':442,'multiline':False]['text':'/ enabled. otherwise no-op.','line_number':443,'multiline':False]['text':' /// Iterator to write threadblock-scoped tile of A operand to shared memory','line_number':450,'multiline':False]['text':' SmemIteratorA smem_iterator_A_;','line_number':451,'multiline':False]['text':'/ Iterator to write threadblock-scoped tile of B operand to shared memory','line_number':453,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of A operand from intermediate','line_number':456,'multiline':False]['text':'/ accumulator tile','line_number':457,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of A_scale from intermediate','line_number':460,'multiline':False]['text':'/ accumulator tile (only used if ScaleOperandA_ is true)','line_number':461,'multiline':False]['text':'/ constructor for MMA with operand A scaling enabled.','line_number':465,'multiline':False]['text':' Operand A in shared memory','line_number':468,'multiline':False]['text':' Operand A_scale in shared memory','line_number':469,'multiline':False]['text':' staging memory for loading tiles of B','line_number':471,'multiline':False]['text':' Compute warp location within threadblock tile by mapping the warp_id to','line_number':479,'multiline':False]['text':' three coordinates:','line_number':480,'multiline':False]['text':'   _m: the warp's position within the threadblock along the M dimension','line_number':481,'multiline':False]['text':'   _n: the warp's position within the threadblock along the N dimension','line_number':482,'multiline':False]['text':'   _k: the warp's position within the threadblock along the K dimension','line_number':483,'multiline':False]['text':' Add per-warp offsets in units of warp-level tiles','line_number':489,'multiline':False]['text':'/ Construct from tensor references','line_number':498,'multiline':False]['text':'/< Operand A in shared memory','line_number':501,'multiline':False]['text':'/< staging memory for loading B','line_number':502,'multiline':False]['text':'/< ID within the threadblock','line_number':503,'multiline':False]['text':'/< ID of warp','line_number':504,'multiline':False]['text':'/< ID of each thread within a warp','line_number':505,'multiline':False]['text':' Compute warp location within threadblock tile by mapping the warp_id to','line_number':509,'multiline':False]['text':' three coordinates:','line_number':510,'multiline':False]['text':'   _m: the warp's position within the threadblock along the M dimension','line_number':511,'multiline':False]['text':'   _n: the warp's position within the threadblock along the N dimension','line_number':512,'multiline':False]['text':'   _k: the warp's position within the threadblock along the K dimension','line_number':513,'multiline':False]['text':' Add per-warp offsets in units of warp-level tiles','line_number':521,'multiline':False]['text':' For API compatibility with MmaMultistageFromSharedMemory','line_number':528,'multiline':False]['text':' but not supported as it worsens perf: older gpus < sm80 don't','line_number':529,'multiline':False]['text':' support async tranfers and have to waste registers','line_number':530,'multiline':False]['text':'/ Perform a threadblock-scoped matrix multiply-accumulate','line_number':540,'multiline':False]['text':'/< number of iterations of the mainloop','line_number':543,'multiline':False]['text':'/< destination accumulator tile','line_number':544,'multiline':False]['text':' IteratorA iterator_A,                             ///< iterator over A','line_number':545,'multiline':False]['text':' operand in global memory','line_number':546,'multiline':False]['text':'/< iterator over B operand in global memory','line_number':547,'multiline':False]['text':'/< source accumulator tile','line_number':548,'multiline':False]['text':' TransformA transform_A = TransformA(),            ///< transformation','line_number':549,'multiline':False]['text':' applied to A fragment','line_number':550,'multiline':False]['text':'/< transformation applied to B fragment','line_number':552,'multiline':False]['text':'','line_number':554,'multiline':False]['text':' Prologue','line_number':555,'multiline':False]['text':'','line_number':556,'multiline':False]['text':' Perform accumulation in the 'd' output operand','line_number':558,'multiline':False]['text':' The last kblock is loaded in the prolog','line_number':565,'multiline':False]['text':' remember that WarpFragmentAScale and WarpIteratorAScale are empty/no-op','line_number':577,'multiline':False]['text':' if scaling is disabled.','line_number':578,'multiline':False]['text':' Pair of fragments used to overlap shared memory loads and math','line_number':580,'multiline':False]['text':' instructions','line_number':581,'multiline':False]['text':' Avoid reading out of bounds','line_number':603,'multiline':False]['text':' Issue loads during the first warp-level matrix multiply-add *AFTER*','line_number':607,'multiline':False]['text':' issuing shared memory loads (which have the tightest latency','line_number':608,'multiline':False]['text':' requirement).','line_number':609,'multiline':False]['text':'','line_number':611,'multiline':False]['text':' Mainloop','line_number':612,'multiline':False]['text':'','line_number':613,'multiline':False]['text':' Note: The main loop does not support Base::kWarpGemmIterations == 2.','line_number':615,'multiline':False]['text':'','line_number':618,'multiline':False]['text':' Loop over GEMM K dimension','line_number':619,'multiline':False]['text':'','line_number':620,'multiline':False]['text':' Load warp-level tiles from shared memory, wrapping to k offset if','line_number':625,'multiline':False]['text':' this is the last group as the case may be.','line_number':626,'multiline':False]['text':' Write fragments to shared memory','line_number':631,'multiline':False]['text':' Add negative offsets to return iterators to the 'start' of the','line_number':639,'multiline':False]['text':' circular buffer in shared memory SMEM: Don't reset iterator A, as','line_number':640,'multiline':False]['text':' we are continuing our iteration at this point','line_number':641,'multiline':False]['text':' Only read the next if we need to','line_number':655,'multiline':False]['text':' Avoid reading out of bounds if this was the last loop iteration','line_number':674,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':691,'multiline':False]['text':' Taken from','line_number':692,'multiline':False]['text':' https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h','line_number':693,'multiline':False]['text':'//////////////////////////////////////////////////////////////////////////////','line_number':694,'multiline':False]['text':'/ Structure to compute the matrix product targeting CUDA cores and SIMT math','line_number':696,'multiline':False]['text':'/ instructions.','line_number':697,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':699,'multiline':False]['text':'/ Iterates over the intermediate accumulator tile in shared memory','line_number':701,'multiline':False]['text':'/ whether or not to perform elementwise multiplication of A','line_number':703,'multiline':False]['text':'  by another matrix (A_scale) that is also kept in shared memory prior','line_number':704,'multiline':False]['text':'  to matmul A @ B','line_number':705,'multiline':False]['text':'/ Iterates over tiles of B operand in global memory','line_number':707,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':708,'multiline':False]['text':'  MaskedTileIterator)','line_number':709,'multiline':False]['text':'/ Iterates over tiles of B operand in shared memory','line_number':711,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':712,'multiline':False]['text':'/ Cache operation for operand B','line_number':714,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':716,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':718,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':720,'multiline':False]['text':'/ Number of stages,','line_number':722,'multiline':False]['text':'/ Used for partial specialization','line_number':725,'multiline':False]['text':'/< Base class','line_number':734,'multiline':False]['text':'/< Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':742,'multiline':False]['text':'/< Iterates over tiles of B operand in global memory','line_number':744,'multiline':False]['text':'/< Policy describing tuning details','line_number':747,'multiline':False]['text':'/< Iterates over the intermediate','line_number':751,'multiline':False]['text':'/< accumulator tile in shared memory','line_number':752,'multiline':False]['text':'/< warp level iterator over A_scale matrix tile kept in shared memory.','line_number':755,'multiline':False]['text':'/< if elementwise A scaling is disabled then everything this does is no-op.','line_number':756,'multiline':False]['text':'/< Data type of accumulator matrix','line_number':761,'multiline':False]['text':'/< Layout of accumulator matrix','line_number':763,'multiline':False]['text':'','line_number':769,'multiline':False]['text':' Dependent types','line_number':770,'multiline':False]['text':'','line_number':771,'multiline':False]['text':'/ Fragment of accumulator tile','line_number':773,'multiline':False]['text':'/ Warp-level Mma','line_number':777,'multiline':False]['text':'/ Minimum architecture is Sm80 to support cp.async','line_number':780,'multiline':False]['text':'/ Complex transform on B operand','line_number':783,'multiline':False]['text':'/ Internal structure exposed for introspection.','line_number':786,'multiline':False]['text':'/ Number of cp.async instructions to load one stage of operand B','line_number':793,'multiline':False]['text':'/ Number of cp.async instructions to load on group of operand B','line_number':797,'multiline':False]['text':'/ fragment of OperandA scale matrix. if operand A scaling is disabled this','line_number':808,'multiline':False]['text':'/ is (almost) empty.','line_number':809,'multiline':False]['text':'/ applies elementwise scaling to fragment of A. if operand A scaling is','line_number':815,'multiline':False]['text':'/ disabled this is a no-op.','line_number':816,'multiline':False]['text':'','line_number':823,'multiline':False]['text':' Data members','line_number':824,'multiline':False]['text':'','line_number':825,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of A1 operand from intermediate','line_number':827,'multiline':False]['text':'/ accumulator tile','line_number':828,'multiline':False]['text':'/ Iterator to load a warp-scoped tile of A1_scale operand from shared memory','line_number':831,'multiline':False]['text':'/ if operand A scaling is disabled everything this does is a no-op.','line_number':832,'multiline':False]['text':'/ Iterator to write threadblock-scoped tile of B operand to shared memory','line_number':835,'multiline':False]['text':'/ constructor for MMA with operand A scaling enabled.','line_number':841,'multiline':False]['text':' Compute warp location within threadblock tile by mapping the warp_id to','line_number':855,'multiline':False]['text':' three coordinates:','line_number':856,'multiline':False]['text':'   _m: the warp's position within the threadblock along the M dimension','line_number':857,'multiline':False]['text':'   _n: the warp's position within the threadblock along the N dimension','line_number':858,'multiline':False]['text':'   _k: the warp's position within the threadblock along the K dimension','line_number':859,'multiline':False]['text':' Add per-warp offsets in units of warp-level tiles','line_number':866,'multiline':False]['text':'/ Construct from tensor references','line_number':875,'multiline':False]['text':'/< ID within the threadblock','line_number':880,'multiline':False]['text':'/< ID of warp','line_number':882,'multiline':False]['text':'/< ID of each thread within a warp','line_number':884,'multiline':False]['text':' Compute warp location within threadblock tile by mapping the warp_id to','line_number':890,'multiline':False]['text':' three coordinates:','line_number':891,'multiline':False]['text':'   _m: the warp's position within the threadblock along the M dimension','line_number':892,'multiline':False]['text':'   _n: the warp's position within the threadblock along the N dimension','line_number':893,'multiline':False]['text':'   _k: the warp's position within the threadblock along the K dimension','line_number':894,'multiline':False]['text':' Add per-warp offsets in units of warp-level tiles','line_number':903,'multiline':False]['text':' Load for operand B','line_number':936,'multiline':False]['text':' Issue several complete stages','line_number':967,'multiline':False]['text':' Load for operand B','line_number':977,'multiline':False]['text':' Move to the next stage','line_number':1000,'multiline':False]['text':' Defines the boundary of a stage of cp.async.','line_number':1005,'multiline':False]['text':'/ Perform a threadblock-scoped matrix multiply-accumulate','line_number':1012,'multiline':False]['text':'/< problem size of GEMM','line_number':1015,'multiline':False]['text':'/< destination accumulator tile','line_number':1017,'multiline':False]['text':'/< iterator over B1 operand in global memory','line_number':1019,'multiline':False]['text':'/< initial value of accumulator','line_number':1021,'multiline':False]['text':' 2nd Gemm','line_number':1023,'multiline':False]['text':'','line_number':1025,'multiline':False]['text':' Prologue','line_number':1026,'multiline':False]['text':'','line_number':1027,'multiline':False]['text':' Perform accumulation in the 'd' output operand','line_number':1028,'multiline':False]['text':' Restore the iterators increments','line_number':1034,'multiline':False]['text':' Issue several complete stages','line_number':1037,'multiline':False]['text':' Load for operand B','line_number':1044,'multiline':False]['text':' DEPBAR+SYNC','line_number':1060,'multiline':False]['text':' remember that WarpFragmentAScale and WarpIteratorAScale are no-op/empty','line_number':1064,'multiline':False]['text':' if scaling is disabled.','line_number':1065,'multiline':False]['text':' Pair of fragments used to overlap shared memory loads and math','line_number':1067,'multiline':False]['text':' instructions','line_number':1068,'multiline':False]['text':' tf32x3 kernels use staging accumulation. warp_mma uses a temporary','line_number':1097,'multiline':False]['text':' accumulator and this temporary accumulator is added to the final','line_number':1098,'multiline':False]['text':' accumulator once in every mainloop iteration.','line_number':1099,'multiline':False]['text':'','line_number':1113,'multiline':False]['text':' Mainloop','line_number':1114,'multiline':False]['text':'','line_number':1115,'multiline':False]['text':'','line_number':1121,'multiline':False]['text':' Loop over GEMM K dimension','line_number':1122,'multiline':False]['text':'','line_number':1123,'multiline':False]['text':' Computes a warp-level GEMM on data held in shared memory','line_number':1125,'multiline':False]['text':' Each "warp_mma_k" refers to a warp-level matrix multiply-accumulate','line_number':1126,'multiline':False]['text':' Load warp-level tile from accumulator fragment (A)','line_number':1130,'multiline':False]['text':' or shared memory (operand B)','line_number':1131,'multiline':False]['text':' skip warp tile loading for the last kgroup (we are out of the buf)','line_number':1134,'multiline':False]['text':' Issue global->shared copies for the this stage','line_number':1181,'multiline':False]['text':' Inserts a memory fence between stages of cp.async instructions.','line_number':1201,'multiline':False]['text':' Waits until kStages-2 stages have committed.','line_number':1204,'multiline':False]['text':' Move to the next stage','line_number':1208,'multiline':False]['text':' Add negative offsets to return iterators to the 'start' of the','line_number':1213,'multiline':False]['text':' circular buffer in shared memory','line_number':1214,'multiline':False]['text':' Do any conversions feeding the first stage at the end of the loop so','line_number':1238,'multiline':False]['text':' we can start right away on mma instructions','line_number':1239,'multiline':False]['text':' Converts a "regular" Mma into their counterpart from shared memory','line_number':1262,'multiline':False]['text':'/ whether or not to apply elementwise multiplication of operand A by','line_number':1267,'multiline':False]['text':'/ another matrix in shared memory before usage in A @ B','line_number':1268,'multiline':False]['text':' Mma pipelined','line_number':1273,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':1275,'multiline':False]['text':'/ Iterates over tiles of A operand in global memory','line_number':1277,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':1278,'multiline':False]['text':'  MaskedTileIterator)','line_number':1279,'multiline':False]['text':'/ Iterates over tiles of A operand in shared memory','line_number':1281,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':1282,'multiline':False]['text':'/ Iterates over tiles of B operand in global memory','line_number':1285,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':1286,'multiline':False]['text':'  MaskedTileIterator)','line_number':1287,'multiline':False]['text':'/ Iterates over tiles of B operand in shared memory','line_number':1289,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':1290,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':1292,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':1294,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':1296,'multiline':False]['text':'/ Transformation applied to A operand','line_number':1298,'multiline':False]['text':'/ Transformation applied to B operand','line_number':1300,'multiline':False]['text':' Max MMA problem size K','line_number':1302,'multiline':False]['text':'/ whether or not to apply elementwise multiplication of operand A by','line_number':1304,'multiline':False]['text':'/ another matrix in shared memory before usage in A @ B','line_number':1305,'multiline':False]['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':1359,'multiline':False]['text':'/ Iterates over tiles of A operand in global memory','line_number':1361,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':1362,'multiline':False]['text':'  MaskedTileIterator)','line_number':1363,'multiline':False]['text':'/ Iterates over tiles of A operand in shared memory','line_number':1365,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':1366,'multiline':False]['text':'/ Cache operation for operand A','line_number':1369,'multiline':False]['text':'/ Iterates over tiles of B operand in global memory','line_number':1371,'multiline':False]['text':'  (concept: ReadableTileIterator | ForwardTileIterator |','line_number':1372,'multiline':False]['text':'  MaskedTileIterator)','line_number':1373,'multiline':False]['text':'/ Iterates over tiles of B operand in shared memory','line_number':1375,'multiline':False]['text':'/ (concept: WriteableTileIterator | RandomAccessTileIterator)','line_number':1376,'multiline':False]['text':'/ Cache operation for operand B','line_number':1378,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':1380,'multiline':False]['text':'/ Data type of accumulator matrix','line_number':1382,'multiline':False]['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':1384,'multiline':False]['text':'/ Number of stages,','line_number':1386,'multiline':False]['text':'/ Use zfill or predicate for out-of-bound cp.async','line_number':1388,'multiline':False]['text':'/ whether or not to apply elementwise multiplication of operand A by','line_number':1391,'multiline':False]['text':'/ another matrix in shared memory before usage in A @ B','line_number':1392,'multiline':False]['text':' Reduce the number of stages if we don't need that many','line_number':1437,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1460,'multiline':False]['text':' Tensor Cores >= Sm75 specialization (Ampere ...)','line_number':1470,'multiline':False]['text':'/ Size of the matrix to load (concept: MatrixShape)','line_number':1471,'multiline':False]['text':'/ Element type','line_number':1473,'multiline':False]['text':'/ Layout of operand in memory','line_number':1475,'multiline':False]['text':'/ Shape of one matrix product operation (concept: MatrixShape)','line_number':1477,'multiline':False]['text':'/ Interval between adjacent *MMA instructions (in units of MMA','line_number':1479,'multiline':False]['text':'/ instructions, concept: MatrixShape)','line_number':1480,'multiline':False]['text':' Iterator to load accumulators (results of matmul in registers)','line_number':1513,'multiline':False]['text':' Iterator to store to shared-memory','line_number':1522,'multiline':False]['text':' accum_t,','line_number':1526,'multiline':False]['text':' We need to provide an operation for the epilogue. Let's create an','line_number':1534,'multiline':False]['text':' operation that does nothing (ScaleType::Nothing), just converts','line_number':1535,'multiline':False]['text':' from accum_t (float) -> scalar_t (can be half)','line_number':1536,'multiline':False]['text':' ElementOutput','line_number':1538,'multiline':False]['text':' ElementAccumulator','line_number':1540,'multiline':False]['text':' ElementCompute','line_number':1541,'multiline':False]['text':' ScaleBiasIterator - not used','line_number':1546,'multiline':False]['text':' Epilogue 2: with LSE (for backwards pass)','line_number':1549,'multiline':False]['text':' TODO: Why 2?','line_number':1550,'multiline':False]['text':' Shape','line_number':1554,'multiline':False]['text':' WarpShape','line_number':1556,'multiline':False]['text':' ElementOutput_','line_number':1562,'multiline':False]['text':' ElementLSE_','line_number':1563,'multiline':False]['text':' ElementAccumulator_','line_number':1564,'multiline':False]['text':' ElementCompute_','line_number':1565,'multiline':False]['text':' FragmentIteratorAccumulator::Fragment::kElements','line_number':1567,'multiline':False]['text':' InstructionShape::kM * InstructionShape::kN / 32','line_number':1568,'multiline':False]['text':' offset','line_number':1607,'multiline':False]['text':' scale - unused','line_number':1622,'multiline':False]['text':' bias','line_number':1624,'multiline':False]['text':' Volta Specialization','line_number':1629,'multiline':False]['text':' only supported for f16','line_number':1630,'multiline':False]['text':' Storage in shared-memory for Q.Kt','line_number':1657,'multiline':False]['text':' Padding','line_number':1665,'multiline':False]['text':' Those are MmaVoltaTensorOpAccumulatorTileIterator private fields','line_number':1670,'multiline':False]['text':' Let's copy their values','line_number':1671,'multiline':False]['text':' ctor - from MmaVoltaTensorOpAccumulatorTileIterator','line_number':1686,'multiline':False]['text':' (quad[2],quad[0])+lane_in_quad[0]','line_number':1693,'multiline':False]['text':' (quad[1])+lane_in_quad[1]','line_number':1695,'multiline':False]['text':' (quad[2],quad[0])','line_number':1701,'multiline':False]['text':' Tile offset','line_number':1706,'multiline':False]['text':' store - from MmaVoltaTensorOpAccumulatorTileIterator','line_number':1714,'multiline':False]['text':' Non-optimized way to apply LSE to registers','line_number':1771,'multiline':False]['text':' NOTE: accum is attn.T','line_number':1772,'multiline':False]['text':' TODO: Optimize for each architecture','line_number':1773,'multiline':False]['text':' Simt Specialization','line_number':1805,'multiline':False]['text':' for f32 on Sm70-Sm75 and f16/f32 below','line_number':1806,'multiline':False]['text':' Storage in shared-memory for Q.Kt','line_number':1841,'multiline':False]['text':' Padding','line_number':1847,'multiline':False]['text':' ctor - MmaSimtTileIterator','line_number':1861,'multiline':False]['text':' compute offset based on thread ID and lane layout','line_number':1862,'multiline':False]['text':' Tile offset','line_number':1870,'multiline':False]['text':' store - MmaSimtTileIterator','line_number':1876,'multiline':False]['text':' Non-optimized way to apply LSE to registers','line_number':1910,'multiline':False]['text':' NOTE: accum is attn.T','line_number':1911,'multiline':False]['text':' TODO: Optimize for each architecture','line_number':1912,'multiline':False]['text':' namespace threadblock','line_number':1944,'multiline':False]['text':' namespace gemm','line_number':1945,'multiline':False]['text':' namespace cutlass','line_number':1946,'multiline':False]['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':1948,'multiline':False]