['text':' convenience helper for converting tensors to cpu','line_number':22,'multiline':False]['text':' We can't just call at::to_cpu() on the entire list of Tensors','line_number':25,'multiline':False]['text':' Because it will break on undefined tensors. Separate out undefined tensors first.','line_number':26,'multiline':False]['text':' Explicitly handling undefined tensors here instead of letting `at::_to_cpu` handle it.','line_number':32,'multiline':False]['text':' Otherwise, we'd need to require all backends with their own implementation of _to_cpu','line_number':33,'multiline':False]['text':' to properly handle undefined tensors.','line_number':34,'multiline':False]['text':' Decide what device to move the output tensor(s) to.','line_number':52,'multiline':False]['text':' The current convention is that we use the first tensor arg to pick the device','line_number':53,'multiline':False]['text':' Barring that, we take the first tensor from a TensorList arg.','line_number':54,'multiline':False]['text':' We need to loop through all of the (potentially multiple) TensorList arguments','line_number':58,'multiline':False]['text':' In case, e.g. the first one is empty but the second is not.','line_number':59,'multiline':False]['text':' save converted cpu tensor for TensorList','line_number':92,'multiline':False]['text':' Step 1: Convert all non-CPU tensor inputs into CPU tensors','line_number':95,'multiline':False]['text':' and put them on the stack at the correct indices.','line_number':96,'multiline':False]['text':' Note: we copy each TensorList argument to CPU individually out of convenience,','line_number':103,'multiline':False]['text':' but XLA would benefit from materializing all tensor and TensorList args onto the CPU at the same time.','line_number':104,'multiline':False]['text':' We can improve this if we need better perf for XLA's CPU fallbacks.','line_number':105,'multiline':False]['text':' XLA requires all of the tensor arguments to be gathered up and converted to CPU together.','line_number':113,'multiline':False]['text':' Step 2: Call the underlying CPU implementation of the operator','line_number':121,'multiline':False]['text':' Step 3: We need to take special care to handle mutable aliases properly:','line_number':124,'multiline':False]['text':' If any input tensors are mutable aliases, we need to','line_number':125,'multiline':False]['text':' directly copy the updated data on the CPU tensors back to the original inputs.','line_number':126,'multiline':False]['text':' We also need to explicit reapply input mutations to inputs that are lists','line_number':135,'multiline':False]['text':' of tensors','line_number':136,'multiline':False]['text':' Step 4: Convert any CPU output tensors back to the original input device.','line_number':148,'multiline':False]['text':' For mutable alias'd outputs, we also need to take special care','line_number':149,'multiline':False]['text':' to move the ORIGINAL input tensor back onto the stack, in place of','line_number':150,'multiline':False]['text':' the temporary CPU output tensor that we created.','line_number':151,'multiline':False]['text':'','line_number':152,'multiline':False]['text':' Note [CPU Fallback Does Not Handle View Operators]','line_number':153,'multiline':False]['text':' Also note that we are incapable of handling immutable alises properly.','line_number':154,'multiline':False]['text':' Why?','line_number':155,'multiline':False]['text':' Schemas with an immutable alias'd tensor outputs correspond to view operators.','line_number':156,'multiline':False]['text':' For example, the `view_as` schema from native_functions.yaml:','line_number':157,'multiline':False]['text':' `view_as(Tensor(a) self, Tensor other) -> Tensor(a)`','line_number':158,'multiline':False]['text':' We can't handle these ops properly, because view ops are supposed to return','line_number':159,'multiline':False]['text':' a NEW tensor that shares the SAME storage as the original tensor.','line_number':160,'multiline':False]['text':' However, the new tensor that we created cannot share the same storage,','line_number':161,'multiline':False]['text':' since it lives on CPU and the original tensor lives on a different device.','line_number':162,'multiline':False]['text':' Because of that, we warn if someone attempts to call the','line_number':163,'multiline':False]['text':' CPU fallback on a view operator (this is to maintain BC for view ops for XLA','line_number':164,'multiline':False]['text':' that fall back to CPU).','line_number':165,'multiline':False]['text':' Case (1): mutable alias case.','line_number':177,'multiline':False]['text':' Move the input ivalue directly onto the stack in place of','line_number':178,'multiline':False]['text':' the existing cpu output tensor.','line_number':179,'multiline':False]['text':' We could store some extra metadata on the function schema to avoid','line_number':182,'multiline':False]['text':' the loop here if we need to improve perf.','line_number':183,'multiline':False]['text':' Checked above; adding assert to guard against breakage of the below','line_number':189,'multiline':False]['text':' condition due to changing the above if test.','line_number':190,'multiline':False]['text':' We've found the original input tensor that aliases with the','line_number':196,'multiline':False]['text':' current output. Wrap it in an IValue and put it directly on the','line_number':197,'multiline':False]['text':' stack.','line_number':198,'multiline':False]['text':' Checked above; adding assert to guard against breakage of the below','line_number':211,'multiline':False]['text':' condition due to changing the above if test.','line_number':212,'multiline':False]['text':' We've found the original input tensor that aliases with the','line_number':218,'multiline':False]['text':' current output. Wrap it in an IValue and put it directly on the','line_number':219,'multiline':False]['text':' stack.','line_number':220,'multiline':False]['text':' Case (3): immutable alias (view) case.','line_number':236,'multiline':False]['text':' Warn here, since we're copying and not creating a view.','line_number':237,'multiline':False]['text':' If this operator is needed, the backend should provide a kernel for','line_number':238,'multiline':False]['text':' it. See Note [CPU Fallback Does Not Handle View Operators]','line_number':239,'multiline':False]['text':' Case (2): copy case.','line_number':268,'multiline':False]['text':' Copy the cpu output tensor to the original device.','line_number':269,'multiline':False]['text':' We technically  might not have a target device, e.g. if you call','line_number':271,'multiline':False]['text':' torch.cat() with an empty list In that case, we shouldn't have any','line_number':272,'multiline':False]['text':' tensors to schlep across devices anyway.','line_number':273,'multiline':False]['text':' namespace native','line_number':296,'multiline':False]['text':' namespace at','line_number':297,'multiline':False]