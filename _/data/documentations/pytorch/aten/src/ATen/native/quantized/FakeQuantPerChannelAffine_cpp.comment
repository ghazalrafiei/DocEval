['text':' FakeQuantize Op for PerChannelAffine quantization scheme.','line_number':10,'multiline':False]['text':' Use REGISTER_DISPATCH to run CPU and CUDA backend.','line_number':14,'multiline':False]['text':' Per channel fake-quantizes the 'inputs' tensor.
Args:
  X: Forward input tensor.
  dY: Backward input tensor (_backward op only).
  scale: scale of per channel affine quantization
  zero_point: zero_point of per channel affine quantization
  axis: int specifying the axis to be quantized
  quant_min: minimum quantized value
  quant_max: maximum quantized value
Returns:
  Fake quantized tensor (double dtype).

','line_number':18,'multiline':True]['text':' TODO(future, optional): read once, write twice.  Not done at the moment','line_number':91,'multiline':False]['text':'   for simplicity, as we do not expect this to be a bottleneck.','line_number':92,'multiline':False]['text':' TODO(future, optional): look into packing the mask further (BoolTensor uses','line_number':101,'multiline':False]['text':'   1 byte per element, we only need 1 bit per element).','line_number':102,'multiline':False]['text':' Backward path to fake-quantize the 'inputs' tensor per channel, with mask.

Args:
  dY: output grad.
  mask: mask tensor from the forward pass.

Returns:
  dX (input grad).
','line_number':107,'multiline':True]['text':' Note: no additional kernels needed, since mask is pre-computed','line_number':126,'multiline':False]['text':' and we can use the existing tensor multiplication kernels.','line_number':127,'multiline':False]['text':' This assumes the per channel zero point vector is single-dimensioned.','line_number':135,'multiline':False]['text':' The gradients for scale and zero point are calculated as below:
     Let Xfq be the fake quantized version of X.
     Let Xq be the quantized version of X (clamped at qmin and qmax).
     Let Delta and z be the scale and the zero point.
     :math:
      \frac{d\Delta }{dx} =
        \begin{cases}
          q_{\min} - z& \text{ if } X_q= q_{\min} \\
          q_{\max} - z& \text{ if } X_q= q_{\max} \\
          (X_{fq} - X) / \Delta & \text{ else }
        \end{cases}

      \frac{dz }{dx} =
        \begin{cases}
          -\Delta& \text{ if } X_q= q_{\min} \text{ or } X_q = q_{\max} \\
          0 & \text{ else }
        \end{cases}
  ','line_number':161,'multiline':True]['text':' Create an axis mask for vectorizing and reshaping the scale and zero point tensors','line_number':217,'multiline':False]['text':' into the same shapes as X along the channel axis.','line_number':218,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-no-malloc)','line_number':219,'multiline':False]['text':' Create a collection of axes that include all but the channel axis for','line_number':243,'multiline':False]['text':' reduction when summing over the dScale and dZeroPoint tensors.','line_number':244,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-no-malloc)','line_number':245,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-no-malloc)','line_number':257,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-no-malloc)','line_number':259,'multiline':False]['text':' namespace native','line_number':263,'multiline':False]['text':' namespace at','line_number':264,'multiline':False]