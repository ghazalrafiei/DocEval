['text':' Supports NHWC and NCHW FP32 convolutions with any valid','line_number':19,'multiline':False]['text':'  - kernel size','line_number':20,'multiline':False]['text':'  - padding','line_number':21,'multiline':False]['text':'  - stride','line_number':22,'multiline':False]['text':'  - dilation','line_number':23,'multiline':False]['text':'  - grouping','line_number':24,'multiline':False]['text':' TODO: Decouple and improve error handling and messages.','line_number':26,'multiline':False]['text':' XNNPACK','line_number':37,'multiline':False]['text':' Weight','line_number':39,'multiline':False]['text':' Bias','line_number':45,'multiline':False]['text':' Padding','line_number':51,'multiline':False]['text':' Stride','line_number':54,'multiline':False]['text':' Dilation','line_number':57,'multiline':False]['text':' Groups','line_number':60,'multiline':False]['text':' Input','line_number':62,'multiline':False]['text':' Output','line_number':64,'multiline':False]['text':' Output - Groups','line_number':66,'multiline':False]['text':' Output Min / Max','line_number':68,'multiline':False]['text':' TODO: Decouple and improve error handling and messages.','line_number':73,'multiline':False]['text':' Input','line_number':75,'multiline':False]['text':' XNNPack's deconvolution operator expects weights to be indexed in the following order:','line_number':113,'multiline':False]['text':'   * Groups','line_number':114,'multiline':False]['text':'   * Group Output Channels','line_number':115,'multiline':False]['text':'   * Kernel Height','line_number':116,'multiline':False]['text':'   * Kernel Width','line_number':117,'multiline':False]['text':'   * Group Input Channels','line_number':118,'multiline':False]['text':'','line_number':119,'multiline':False]['text':' (ref: https://github.com/google/XNNPACK/blob/ecd8311c8fd3d9ab47edbc3df5f2b5de7dabe75f/test/deconvolution-operator-tester.h#L678)','line_number':120,'multiline':False]['text':'','line_number':121,'multiline':False]['text':' This function takes in a contiguous NHWC pytorch tensor (e.g. MemoryFormat == ChannelsLast) and rearranges the weights in preparation for use with xnnpack.','line_number':122,'multiline':False]['text':' By default, for pytorch, transpose conv2d weights are {input_channels, output_Channels_per_group, kernel_height, kernel_width}.','line_number':123,'multiline':False]['text':' In addition, it condenses the tensor from 5 to 4 dimensions as expected by the rest of the pytorch framework by combining the groups and input_channels dimension.','line_number':124,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':130,'multiline':False]['text':' namespace','line_number':169,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':205,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':207,'multiline':False]['text':' output_padding_top','line_number':216,'multiline':False]['text':' output_padding_right','line_number':217,'multiline':False]['text':' output_padding_bottom','line_number':218,'multiline':False]['text':' output_padding_left','line_number':219,'multiline':False]['text':' kernel_height','line_number':220,'multiline':False]['text':' kernel_width','line_number':221,'multiline':False]['text':' subsampling_height','line_number':222,'multiline':False]['text':' subsampling_width','line_number':223,'multiline':False]['text':' dilation_height','line_number':224,'multiline':False]['text':' dilation_width','line_number':225,'multiline':False]['text':' groups','line_number':226,'multiline':False]['text':' group_input_channels','line_number':227,'multiline':False]['text':' group_output_channels','line_number':228,'multiline':False]['text':' input_pixel_stride','line_number':229,'multiline':False]['text':' output_pixel_stride','line_number':230,'multiline':False]['text':' kernel','line_number':231,'multiline':False]['text':' bias','line_number':234,'multiline':False]['text':' output_min','line_number':235,'multiline':False]['text':' output_max','line_number':236,'multiline':False]['text':' flags','line_number':237,'multiline':False]['text':' xnn_caches_t','line_number':238,'multiline':False]['text':' operator','line_number':239,'multiline':False]['text':' input_padding_top','line_number':245,'multiline':False]['text':' input_padding_right','line_number':246,'multiline':False]['text':' input_padding_bottom','line_number':247,'multiline':False]['text':' input_padding_left','line_number':248,'multiline':False]['text':' kernel_height','line_number':249,'multiline':False]['text':' kernel_width','line_number':250,'multiline':False]['text':' subsampling_height','line_number':251,'multiline':False]['text':' subsampling_width','line_number':252,'multiline':False]['text':' dilation_height','line_number':253,'multiline':False]['text':' dilation_width','line_number':254,'multiline':False]['text':' groups','line_number':255,'multiline':False]['text':' group_input_channels','line_number':256,'multiline':False]['text':' group_output_channels','line_number':257,'multiline':False]['text':' input_pixel_stride','line_number':258,'multiline':False]['text':' output_pixel_stride','line_number':259,'multiline':False]['text':' kernel','line_number':260,'multiline':False]['text':' bias','line_number':263,'multiline':False]['text':' output_min','line_number':264,'multiline':False]['text':' output_max','line_number':265,'multiline':False]['text':' flags','line_number':266,'multiline':False]['text':' xnn_caches_t','line_number':267,'multiline':False]['text':' operator','line_number':268,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':326,'multiline':False]['text':'
   * Input Pointer Caching:
   * Previously, we cached the input/output pointers and dimension parameters
   * so that if the same pointers and parameters are used, this setup could be
   * skipped.
   * However, XNNPack has integrated offsets with its indirection buffer, so the
   * buffer does not need to be recalculated even if activation tensor pointer
   * changes as long as tensor dimensions are the same. Thus, the aforementioned
   * manual caching is not needed here.
   ','line_number':329,'multiline':True]['text':' operator','line_number':342,'multiline':False]['text':' batch_size','line_number':343,'multiline':False]['text':' input_height','line_number':344,'multiline':False]['text':' input_width','line_number':345,'multiline':False]['text':' adjustment_height','line_number':346,'multiline':False]['text':' adjustment_width','line_number':347,'multiline':False]['text':' input','line_number':348,'multiline':False]['text':' output','line_number':349,'multiline':False]['text':' threadpool','line_number':350,'multiline':False]['text':' operator','line_number':354,'multiline':False]['text':' batch_size','line_number':355,'multiline':False]['text':' input_height','line_number':356,'multiline':False]['text':' input_width','line_number':357,'multiline':False]['text':' input','line_number':358,'multiline':False]['text':' output','line_number':359,'multiline':False]['text':' operator','line_number':369,'multiline':False]['text':' threadpool','line_number':370,'multiline':False]['text':' Op is registered to have Any argument as we plan to reuse it for prepacked conv2d of other backends','line_number':429,'multiline':False]['text':' namespace convolution2d','line_number':450,'multiline':False]['text':' namespace internal','line_number':451,'multiline':False]['text':' output_padding','line_number':488,'multiline':False]['text':' transposed','line_number':492,'multiline':False]['text':' namespace at::native::xnnpack','line_number':497,'multiline':False]['text':' USE_XNNPACK ','line_number':499,'multiline':True]