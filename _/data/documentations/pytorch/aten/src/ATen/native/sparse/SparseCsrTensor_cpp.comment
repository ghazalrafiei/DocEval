['text':' Basic functions on sparse tensors','line_number':1,'multiline':False]['text':'
    This function solves the equation

      input == arange(start, end, step)

    for integers start, end, and step, if possible. If the solution
    exists, returns true.
  ','line_number':66,'multiline':True]['text':' a trivial solution','line_number':76,'multiline':False]['text':' a simple solution','line_number':80,'multiline':False]['text':' a special solution','line_number':89,'multiline':False]['text':' detect if general solution exists','line_number':94,'multiline':False]['text':' no solution','line_number':102,'multiline':False]['text':' end anonymous namespace','line_number':110,'multiline':False]['text':'
  Validate the arguments to sparse compressed (CSR, CSC, BSR, and BSC)
  tensor factory functions.

  The CSR and BSR invariants for PyTorch are outlined in

    https://pearu.github.io/csr_tensor_invariants.html
    https://pearu.github.io/bsr_tensor_invariants.html

  that in what follows are generalized for all sparse compressed
  formats with support to batched and dense dimensions.
','line_number':112,'multiline':True]['text':' Layout must be Sparse Compressed, 2.4','line_number':126,'multiline':False]['text':'upper=','line_number':129,'multiline':True]['text':' Layout Invariants','line_number':135,'multiline':False]['text':' Re 3.5 and 3.6: in the case of compressed/plain indices tensors,','line_number':137,'multiline':False]['text':' we require contiguity per-patch basis, that is, the last stride','line_number':138,'multiline':False]['text':' of these indices must be 1. The reasoning for this is that','line_number':139,'multiline':False]['text':' indices tensors within a patch are "atomic" in the sense that','line_number':140,'multiline':False]['text':' sliced compressed/plain indices would not represent the indices','line_number':141,'multiline':False]['text':' of any sparse compressed tensor as the slicing would break the','line_number':142,'multiline':False]['text':' description of the tensor index structure.','line_number':143,'multiline':False]['text':' 2.1','line_number':145,'multiline':False]['text':' 2.2','line_number':149,'multiline':False]['text':' corresponds to compressed and plain indices','line_number':153,'multiline':False]['text':' 2.3','line_number':160,'multiline':False]['text':' 3.7 is dropped, that is, values tensor does not need to be','line_number':164,'multiline':False]['text':' contiguous, in general. Particular algorithms on sparse','line_number':165,'multiline':False]['text':' compressed tensors may require contiguity though.','line_number':166,'multiline':False]['text':' Shape and Strides invariants','line_number':168,'multiline':False]['text':' 3.2','line_number':170,'multiline':False]['text':' 3.3','line_number':175,'multiline':False]['text':' 3.4','line_number':181,'multiline':False]['text':' 3.5','line_number':187,'multiline':False]['text':' 3.6','line_number':191,'multiline':False]['text':' 3.1','line_number':195,'multiline':False]['text':' For CSR/CSC formats, we define blocksize=(1, 1) so that checking','line_number':201,'multiline':False]['text':' the sparse compressed tensor invariants can be unified with the','line_number':202,'multiline':False]['text':' BSR/BSC invariants.','line_number':203,'multiline':False]['text':' 3.10','line_number':204,'multiline':False]['text':' All batch sizes must be the same and consistent with tensor batchsize, 3.1, 3.8, 3.9, 3.10','line_number':211,'multiline':False]['text':' A tensor constitutes of full blocks, 3.1','line_number':225,'multiline':False]['text':' 3.8','line_number':238,'multiline':False]['text':' 3.9, 3.10','line_number':243,'multiline':False]['text':' Type Invariants','line_number':248,'multiline':False]['text':' 1.1, 1.2, 1.3','line_number':251,'multiline':False]['text':' Indices invariants','line_number':261,'multiline':False]['text':'is_crow = ','line_number':264,'multiline':True]['text':' Device Invariants','line_number':273,'multiline':False]['text':' 4.1','line_number':274,'multiline':False]['text':' 4.2, 4.3, 4.4','line_number':280,'multiline':False]['text':' Autograd Invariants','line_number':296,'multiline':False]['text':'','line_number':297,'multiline':False]['text':' These are internal asserts because users should not be able to','line_number':298,'multiline':False]['text':' create non-floating point dtype tensors with requires_grad flag','line_number':299,'multiline':False]['text':' set to true.','line_number':300,'multiline':False]['text':' Construction of CSR, CSC, BSR, and BSC tensors.','line_number':325,'multiline':False]['text':' Note: The usage of "Csr" in names like SparseCsrTensor,','line_number':327,'multiline':False]['text':' SparseCsrCPU, SparseCsrCUDA, and SparseCsrTensorImpl exists because','line_number':328,'multiline':False]['text':' of historical reasons (that ought to be removed in future) and does','line_number':329,'multiline':False]['text':' not mean that the corresponding functionality would be CSR layout','line_number':330,'multiline':False]['text':' only specific.','line_number':331,'multiline':False]['text':' TODO: remove this comment after enabling autograd support for CSR tensor','line_number':333,'multiline':False]['text':' constructor.','line_number':334,'multiline':False]['text':' TORCH_INTERNAL_ASSERT(impl::variable_excluded_from_dispatch());','line_number':335,'multiline':False]['text':' corresponds to compressed and plain indices','line_number':418,'multiline':False]['text':' TODO: This constructor should probably use an ATen abstract method in order','line_number':468,'multiline':False]['text':' to make autograd dispatch available for the CSR constructor. See the relevant','line_number':469,'multiline':False]['text':' note in native_functions.yaml.','line_number':470,'multiline':False]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':487,'multiline':False]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':518,'multiline':False]['text':' Strided is the default layout for torch.empty.','line_number':576,'multiline':False]['text':' torch.empty cannot be used to create blocked tensors because its','line_number':579,'multiline':False]['text':' API lacks a method to specify the block size.','line_number':580,'multiline':False]['text':' actually, values copy allows different shapes as long as operands are broadcastable','line_number':627,'multiline':False]['text':' Access members of CSR tensors.','line_number':670,'multiline':False]['text':' Note: The impl method does all required checking to see if resize/data copy','line_number':736,'multiline':False]['text':' on member tensors is required.','line_number':737,'multiline':False]['text':' Selecting batch dimension','line_number':885,'multiline':False]['text':' Selecting sparse dimension','line_number':896,'multiline':False]['text':' select is always a view operation','line_number':915,'multiline':False]['text':' Notice that dim_indices is a sorted sequence of non-negative','line_number':926,'multiline':False]['text':' distinct integers. Below we'll try to solve `dim_indices ==','line_number':927,'multiline':False]['text':' arange(start, stop, step)`. If the solution exists then the','line_number':928,'multiline':False]['text':' select will be a view operation also for the `dim !=','line_number':929,'multiline':False]['text':' fast_dim` case.','line_number':930,'multiline':False]['text':' select will be a copy operation due to index_select!','line_number':937,'multiline':False]['text':'
            The formula for select indices and values below are best
            explained by an example. Consider a BSR tensor with a
            block size (2, 3) having four blocks (the other two blocks
            contain all zeros and hence will not be specified):

              [ 1  2  3] | [ 7  8  9]
              [ 4  5  6] | [10 11 12]
              ---------------------
              [13 14 15] | [ 0  0  0]
              [16 17 18] | [ 0  0  0]
              -----------------------
              [ 0  0  0] | [19 20 21]
              [ 0  0  0] | [22 23 24]

            that represents a 6 x 6 tensor:

              [  1  2  3  7  8  9 ]
              [  4  5  6 10 11 12 ]
              [ 13 14 15  0  0  0 ]
              [ 16 17 18  0  0  0 ]
              [  0  0  0 19 20 21 ]
              [  0  0  0 22 23 24 ]

            The corresponding data for the BSR representation is:

              crow_indices = [0 2 3 4]
              col_indices =  [0 1 0 1]
              values = [ [[1 2 3], [4 5 6]], [[7 8 9], [10 11 12]], [[13 14 15], [16 17 18]], [[19 20 21], [22 23 24]] ]
              shape = (6, 6)

            From crow_indices, we can find that

              row_indices = [0 0 1 2]

            In the following, we'll illustrate the details of
            computing the result of torch.select_copy(input, dim,
            index) where dim is 0 or 1, and index is in
            range(shape[dim]).

            Select a row of a BSR tensor
            ----------------------------

            We will consider first the dim=0 case that corresponds to
            selecting a index-th row of the tensor. For instance, for
            dim=0 and index=1, the expected result would represent a
            1D tensor:

              [  4  5  6 10 11 12 ]

            that is a concatenated tensor of certain slices from the
            first and the second block that is computed as follows:

              values[dim_indices].select(1 + dim, index % blocksize[dim]).flatten(0, 1)
              -> values[[0, 1]][:, 1 % 2].flatten(0, 1)
              -> [ [[1 2 3], [4 5 6]], [[7 8 9], [10 11 12]] ][:, 1].flatten(0, 1)
              -> [ [4 5 6], [10 11 12]].flatten(0, 1)
              -> [ 4 5 6 10 11 12]

            where dim_indices is found as

              where(row_indices == index//blocksize[dim])
              -> where([0 0 1 2] == 1//2)
              -> [0 1]

            The corresponding column indices are computed as

              (col_indices[dim_indices].mul(blocksize[other_dim]).unsqueeze(1) + arange(blocksize[other_dim]).unsqueeze(0)).flatten(0, 1)

            where other_dim is 1 if dim is 0, and 0 if dim is 1. Let's
            expand the above expression with the data in the example:

              -> (col_indices[[0, 1]].mul(3).unsqueeze(1) + arange(3).unsqueeze(0)).flatten(0, 1)
              -> ([[0 1].mul(3).unsqueeze(1) + [[0 1 2]]).flatten(0, 1)
              -> ([[[0], [3]] + [[0 1 2]]).flatten(0, 1)     <- here addition will use broadcasting rules!
              -> ([[[0 1 2], [3 4 5]]).flatten(0, 1)
              -> [0 1 2 3 4 5]

            Finally, the select(dim=0, index=1) op on the given sparse
            compressed tensors will return a COO tensor:

              sparse_coo_tensor([0 1 2 3 4 5].unsqueeze(0), [4 5 6 10 11 12], (6,))

            that represents the expected result: [ 4 5 6 10 11 12 ]

            Select a column of a BSR tensor
            -------------------------------

            Next, we'll consider the dim=1 case that corresponds to
            selecting the index-th column of the tensor. For instance,
            for dim=1 and index=4, the expected result would represent
            a 1D tensor:

              [  8 11 0  0 20 23]

            that is a concatenated tensor of certain slices from the
            second and the last block:

              values[dim_indices].select(1 + dim, index % blocksize[dim]).flatten(0, 1)
              -> values[[1, 3]][:, :, 4 % 3 ].flatten(0, 1)
              -> [ [[7 8 9], [10 11 12]], [[19 20 21], [22 23 24]] ][:, 1, 1].flatten(0, 1)
              -> [ [8 11], [20 23]].flatten(0, 1)
              -> [ 8 11 20 23 ]

            The corresponding row indices are computed as

              (row_indices[dim_indices].mul(blocksize[other_dim]).unsqueeze(1) + arange(blocksize[other_dim]).unsqueeze(0)).flatten(0, 1)

            where dim_indices is

              where(col_indices == index//blocksize[dim])
              -> where([0 1 0 1] == 4//3)
              -> [1 3]

            and we have

              (row_indices[dim_indices].mul(blocksize[other_dim]).unsqueeze(1) + arange(blocksize[other_dim]).unsqueeze(0)).flatten(0, 1)
              -> (row_indices[[1 3]].mul(2).unsqueeze(1) + arange(2).unsqueeze(0)).flatten(0, 1)
              -> ([0 4].unsqueeze(1) + [0 1].unsqueeze(0)).flatten(0, 1)
              -> ([[0], [4]] + [[0 1]]).flatten(0, 1)     <- here addition will use broadcasting rules!
              -> ([[0 1], [4 5]]).flatten(0, 1)
              -> [ 0 1 4 5 ]

            Finally, the select(dim=1, index=4) op on the given sparse
            compressed tensors will return a COO tensor:

              sparse_coo_tensor([0 1 4 5].unsqueeze(0), [8 11 20 23], (6,))

            that represents the expected result: [ 8 11 0 0 20 23 ]

           ','line_number':945,'multiline':True]['text':' flatten(0, 1) can be a view or a copy operation. If view','line_number':1079,'multiline':False]['text':' is required, it will be checked below via is_alias_of,','line_number':1080,'multiline':False]['text':' otherwise, we'll check if copy is made here to avoid','line_number':1081,'multiline':False]['text':' unnecessary clone below:','line_number':1082,'multiline':False]['text':' Selecting dense dimension','line_number':1099,'multiline':False]['text':' Non blocked layout (2 sparse dims become 1 nnz dim in values, so dim','line_number':1103,'multiline':False]['text':' is found one position to the left)','line_number':1104,'multiline':False]['text':' Block layout (2 sparse dims become 1 nnz dim + 2 block-shape dims in','line_number':1106,'multiline':False]['text':' values, so dim is found 1 position to the right)','line_number':1107,'multiline':False]['text':' namespace at::native','line_number':1129,'multiline':False]