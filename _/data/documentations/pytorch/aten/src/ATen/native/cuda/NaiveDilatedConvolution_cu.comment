['text':' hyper-volume to column, CUDA','line_number':29,'multiline':False]['text':' column to hyper-volume, CUDA','line_number':88,'multiline':False]['text':'
   check tensor data locations
','line_number':147,'multiline':True]['text':' checking data locations of user-provided tensor arguments','line_number':156,'multiline':False]['text':' we are not checking the data locations of other tensor','line_number':166,'multiline':False]['text':' arguments such as output, grad_input, etc because of these are','line_number':167,'multiline':False]['text':' allocated based on input options and hence these tensors always','line_number':168,'multiline':False]['text':' have the same data location as of input tensor.','line_number':169,'multiline':False]['text':'
  slow_conv_dilated_all_cuda_template

  Main worker. Computes tensors output, grad_input, grad_weight,
  and/or grad_bias if defined, respectively.
 ','line_number':172,'multiline':True]['text':' The rear part of input tensor sizes:','line_number':196,'multiline':False]['text':' The rear part of output tensor sizes:','line_number':198,'multiline':False]['text':' Temporary buffers:','line_number':204,'multiline':False]['text':' Initialize','line_number':211,'multiline':False]['text':' When using ROCm, the sum evaluation is inaccurate for double
     tensors. The reason is currently unknown. Hence, we use gemv for
     computing `grad_output_n.sum(dims)` until the ROCm-sum issue is
     resolved. ','line_number':223,'multiline':True]['text':' MSVC does not like #ifdef-s inside the CPP macro
     AT_DISPATCH_FLOATING_TYPES_AND_HALF. So, we define the code
     branching outside the CPP macro: ','line_number':232,'multiline':True]['text':'trans=','line_number':237,'multiline':True]['text':'    m=','line_number':238,'multiline':True]['text':'    n=','line_number':239,'multiline':True]['text':'alpha=','line_number':240,'multiline':True]['text':'    A=','line_number':241,'multiline':True]['text':'  lda=','line_number':242,'multiline':True]['text':'    x=','line_number':243,'multiline':True]['text':' incx=','line_number':244,'multiline':True]['text':' beta=','line_number':245,'multiline':True]['text':'    y=','line_number':246,'multiline':True]['text':' incy=','line_number':247,'multiline':True]['text':' Helpers','line_number':252,'multiline':False]['text':' For each elt in batch, do:','line_number':259,'multiline':False]['text':' Matrix multiply per output:','line_number':261,'multiline':False]['text':' Output','line_number':264,'multiline':False]['text':' For gemm argument derivation, see
                 slow_conv_dilated_all_cuda_template in
                 ATen/native/DilatedConvolution.cpp ','line_number':268,'multiline':True]['text':' Extract columns:','line_number':275,'multiline':False]['text':' For gemm argument derivation, see
               slow_conv_dilated_all_cuda_template in
               ATen/native/DilatedConvolution.cpp ','line_number':287,'multiline':True]['text':'transa=','line_number':291,'multiline':True]['text':'transb=','line_number':292,'multiline':True]['text':'     m=','line_number':293,'multiline':True]['text':'     n=','line_number':294,'multiline':True]['text':'     k=','line_number':295,'multiline':True]['text':' alpha=','line_number':296,'multiline':True]['text':'     A=','line_number':297,'multiline':True]['text':'   lda=','line_number':298,'multiline':True]['text':'     B=','line_number':299,'multiline':True]['text':'   ldb=','line_number':300,'multiline':True]['text':'  beta=','line_number':301,'multiline':True]['text':'     C=','line_number':302,'multiline':True]['text':'   ldc=','line_number':303,'multiline':True]['text':' All gradients','line_number':306,'multiline':False]['text':' Gradient of input:','line_number':310,'multiline':False]['text':' For gemm argument derivation, see
               slow_conv_dilated_all_cuda_template in
               ATen/native/DilatedConvolution.cpp ','line_number':312,'multiline':True]['text':'transa=','line_number':316,'multiline':True]['text':'transb=','line_number':317,'multiline':True]['text':'     m=','line_number':318,'multiline':True]['text':'     n=','line_number':319,'multiline':True]['text':'     k=','line_number':320,'multiline':True]['text':' alpha=','line_number':321,'multiline':True]['text':'     A=','line_number':322,'multiline':True]['text':'   lda=','line_number':323,'multiline':True]['text':'     B=','line_number':324,'multiline':True]['text':'   ldb=','line_number':325,'multiline':True]['text':'  beta=','line_number':326,'multiline':True]['text':'     C=','line_number':327,'multiline':True]['text':'   ldc=','line_number':328,'multiline':True]['text':' Unpack columns back into input:','line_number':329,'multiline':False]['text':' Gradient of weight:','line_number':345,'multiline':False]['text':' Extract columns:','line_number':347,'multiline':False]['text':' TODO: expose as argument?','line_number':360,'multiline':False]['text':' For gemm argument derivation, see
               slow_conv_dilated_all_cuda_template in
               ATen/native/DilatedConvolution.cpp ','line_number':361,'multiline':True]['text':'transa=','line_number':365,'multiline':True]['text':'transb=','line_number':366,'multiline':True]['text':'     m=','line_number':367,'multiline':True]['text':'     n=','line_number':368,'multiline':True]['text':'     k=','line_number':369,'multiline':True]['text':' alpha=','line_number':370,'multiline':True]['text':'     A=','line_number':371,'multiline':True]['text':'   lda=','line_number':372,'multiline':True]['text':'     B=','line_number':373,'multiline':True]['text':'   ldb=','line_number':374,'multiline':True]['text':'  beta=','line_number':375,'multiline':True]['text':'     C=','line_number':376,'multiline':True]['text':'   ldc=','line_number':377,'multiline':True]['text':' Gradient of bias:','line_number':380,'multiline':False]['text':' For gemv argument derivation, see
               slow_conv_dilated_all_cpu_template in
               ATen/native/DilatedConvolution.cpp ','line_number':382,'multiline':True]['text':' MSVC does not like #ifdef-s
                                    inside the CPP macros, see above. ','line_number':385,'multiline':True]['text':'
              TODO: when scale != 1 is introduced then use:
                grad_bias += scale * grad_output_n.sum(dims);
             ','line_number':387,'multiline':True]['text':' slow_conv_dilated_all_cuda_template','line_number':395,'multiline':False]['text':' namespace','line_number':397,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':406,'multiline':False]['text':' calculate output tensor size','line_number':422,'multiline':False]['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':425,'multiline':False]['text':' insert batch dimension without affecting the original tensor.','line_number':426,'multiline':False]['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':471,'multiline':False]['text':' insert batch dimension without affecting the original tensor.','line_number':472,'multiline':False]['text':' compute only gradients for which the corresponding output_mask is true:','line_number':479,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':512,'multiline':False]['text':' calculate output tensor size','line_number':528,'multiline':False]['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':531,'multiline':False]['text':' insert batch dimension without affecting the original tensor.','line_number':532,'multiline':False]['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':577,'multiline':False]['text':' insert batch dimension without affecting the original tensor.','line_number':578,'multiline':False]['text':' compute only gradients for which the corresponding output_mask is true:','line_number':585,'multiline':False]['text':' namespace at::native','line_number':614,'multiline':False]