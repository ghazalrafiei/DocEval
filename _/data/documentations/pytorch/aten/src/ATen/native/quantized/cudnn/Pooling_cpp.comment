['text':' for the definition of AT_CUDNN_ENABLED','line_number':3,'multiline':False]['text':' AT_CUDNN_ENABLED','line_number':11,'multiline':False]['text':' USE_CUDA','line_number':12,'multiline':False]['text':' TODO: This function is the same as that of Pooling.cpp. We should refactor this into quantized directory','line_number':27,'multiline':False]['text':' so that we don't need to duplicate the function','line_number':28,'multiline':False]['text':' The current implementation of quantized cuda adaptive average pooling uses the following:','line_number':45,'multiline':False]['text':' dequant -> fp32 adaptive average pooling -> quant. This is the same numerically as','line_number':46,'multiline':False]['text':' quantized adaptive average pooling. This is not the ideal implementation, as we desire to','line_number':47,'multiline':False]['text':' operate on the quantized values directly.','line_number':48,'multiline':False]['text':' However, we are currently blocked on this as we are waiting for cudnn's 8.5.0 release, which is anticipated','line_number':49,'multiline':False]['text':' to support adaptive average pooling. When that support is made available, we will use it directly. TODO','line_number':50,'multiline':False]['text':' TODO: renable these cudnn preprocessors like quantized_max_pool2d_cudnn below when we implement this function with cudnn','line_number':54,'multiline':False]['text':' #if AT_CUDNN_ENABLED()','line_number':56,'multiline':False]['text':' #if HAS_CUDNN_V8()','line_number':57,'multiline':False]['text':' TODO: limit this to per tensor quantized tensors for now, though should be easy to adapt','line_number':58,'multiline':False]['text':' to per channel quantized tensors','line_number':59,'multiline':False]['text':' USE_CUDA','line_number':64,'multiline':False]['text':' never reached, placates the compiler','line_number':66,'multiline':False]['text':' Currently we support 4D and 3D input (qx) tensors, the latter of which is supported for','line_number':70,'multiline':False]['text':' legacy reasons. The first dimension of a 4D input tensor is the batch size.','line_number':71,'multiline':False]['text':' For a 3D tensor, there is no batch size dimension -- it can be viewed as a single batch.','line_number':72,'multiline':False]['text':' cudnn's 2D pooling operation requires the input and output to be 4D tensors, so we must cast','line_number':73,'multiline':False]['text':' any 3D tensors to 4D prior to using cudnn','line_number':74,'multiline':False]['text':' This implementation currently uses the v7 cudnn APIs as v8 cudnn APIs are not yet available for','line_number':75,'multiline':False]['text':' pooling operations.','line_number':76,'multiline':False]['text':' Consult https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward for','line_number':77,'multiline':False]['text':' documentation on the APIs','line_number':78,'multiline':False]['text':' Currently, it appears there is no cudnn support for dilated pooling -- we will','line_number':79,'multiline':False]['text':' submit a feature request for this with cudnn','line_number':80,'multiline':False]['text':' TODO: ideally, we would like to use structured kernel support here so we do not have to repeat','line_number':81,'multiline':False]['text':' the input checks, however, that would require us to implement max_pool2d_with_indices_out_quantized_cuda','line_number':82,'multiline':False]['text':' based on how the dispatch table is currently constructed in native_functions.yaml. currently,','line_number':83,'multiline':False]['text':' there is no support for producing indices with cudnn max pooling, so until that becomes available, this cannot be done.','line_number':84,'multiline':False]['text':' 3D','line_number':130,'multiline':False]['text':' Check output dimensions.','line_number':138,'multiline':False]['text':' cudnn requires 4D input and output for 2D pooling, so we prepend a dummy dimension','line_number':165,'multiline':False]['text':' whose size represents the batch size (1)','line_number':166,'multiline':False]['text':' kernel height','line_number':185,'multiline':False]['text':' kernel width','line_number':186,'multiline':False]['text':' vertical padding','line_number':187,'multiline':False]['text':' horizontal padding','line_number':188,'multiline':False]['text':' vertical stride','line_number':189,'multiline':False]['text':' horizontal stride','line_number':190,'multiline':False]['text':' recall we casted our input and output to 4D if qx was 3D, so we recast it back to 3D prior to returning','line_number':208,'multiline':False]['text':' HAS_CUDNN_V8()','line_number':210,'multiline':False]['text':' never reached, placates the compiler','line_number':212,'multiline':False]['text':' HAS_CUDNN_V8()','line_number':213,'multiline':False]['text':' AT_CUDNN_ENABLED()','line_number':214,'multiline':False]['text':' never reached, placates the compiler','line_number':216,'multiline':False]['text':' AT_CUDNN_ENABLED()','line_number':217,'multiline':False]['text':' USE_CUDA','line_number':218,'multiline':False]['text':' never reached, placates the compiler','line_number':220,'multiline':False]['text':' Keep the registry in the anonymous namespace.','line_number':224,'multiline':False]['text':' namespace','line_number':246,'multiline':False]['text':' namespace native','line_number':247,'multiline':False]['text':' namespace at','line_number':248,'multiline':False]