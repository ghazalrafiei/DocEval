['text':' ****************** HEY YOU! YES YOU! Read this! ********************','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' Please read the README.md in this directory before editing this file','line_number':55,'multiline':False]['text':' Prefix sum of input channels for fast indexing','line_number':66,'multiline':False]['text':' NOLINTNEXTLINE(performance-implicit-conversion-in-loop)','line_number':73,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':111,'multiline':False]['text':' N, H, and W are explicitly captured here because there's a bug in GCC5','line_number':121,'multiline':False]['text':' and clang5 which causes an internal compiler error if they're not','line_number':122,'multiline':False]['text':' loop over input tensors','line_number':127,'multiline':False]['text':' Vectorized loop','line_number':149,'multiline':False]['text':' Vectorized loop for channel between 8 and 32 (avx2)','line_number':173,'multiline':False]['text':' Scalar loop','line_number':201,'multiline':False]['text':' for c','line_number':213,'multiline':False]['text':' for tidx','line_number':214,'multiline':False]['text':' for i','line_number':215,'multiline':False]['text':' horizontal sum over a range of uint8_t','line_number':222,'multiline':False]['text':' vectorized','line_number':231,'multiline':False]['text':' first argument is unsigned, second is signed','line_number':237,'multiline':False]['text':' vectorized','line_number':252,'multiline':False]['text':' first argument is unsigned, second is signed','line_number':258,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':269,'multiline':False]['text':' scalar','line_number':271,'multiline':False]['text':' horizontal sum over a range of int8_t','line_number':279,'multiline':False]['text':' vectorized','line_number':288,'multiline':False]['text':' first argument is unsigned, second is signed','line_number':294,'multiline':False]['text':' vectorized','line_number':309,'multiline':False]['text':' first argument is unsigned, second is signed','line_number':315,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':326,'multiline':False]['text':' scalar','line_number':328,'multiline':False]['text':' horizontal sum over a range of int32_t','line_number':336,'multiline':False]['text':' vectorized','line_number':343,'multiline':False]['text':' widen','line_number':346,'multiline':False]['text':' add','line_number':351,'multiline':False]['text':' vectorized','line_number':363,'multiline':False]['text':' widen','line_number':366,'multiline':False]['text':' add','line_number':371,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':381,'multiline':False]['text':' scalar','line_number':383,'multiline':False]['text':' horizontal sum of squares over a range of uint8_t','line_number':391,'multiline':False]['text':' vectorized','line_number':397,'multiline':False]['text':' 2147483647(max of int32)/(256*256)*8 = 262144','line_number':400,'multiline':False]['text':' (i15, ..., i0)','line_number':404,'multiline':False]['text':' (i15 ^ 2, ..., i0 ^ 2)','line_number':407,'multiline':False]['text':' (i7 ^ 2, ..., i0 ^ 2)','line_number':409,'multiline':False]['text':' (i15 ^ 2, ..., i8 ^ 2)','line_number':411,'multiline':False]['text':' widen to epu32','line_number':413,'multiline':False]['text':' add to running sum','line_number':416,'multiline':False]['text':' 2147483647(max of int32)/(512*512)*8 = 262144','line_number':429,'multiline':False]['text':' (i31, ..., i0)','line_number':433,'multiline':False]['text':' (i31 ^ 2, ..., i0 ^ 2)','line_number':436,'multiline':False]['text':' (i15 ^ 2, ..., i0 ^ 2)','line_number':438,'multiline':False]['text':' (i31 ^ 2, ..., i16 ^ 2)','line_number':440,'multiline':False]['text':' widen to epu32','line_number':442,'multiline':False]['text':' add to running sum','line_number':445,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':455,'multiline':False]['text':' scalar','line_number':457,'multiline':False]['text':' horizontal sum of squares over a range of int8_t','line_number':465,'multiline':False]['text':' vectorized','line_number':471,'multiline':False]['text':'2147483647/(128*128)*8 = 1048576','line_number':475,'multiline':False]['text':' (i15, ..., i0)','line_number':480,'multiline':False]['text':' (i15 ^ 2, ..., i0 ^ 2)','line_number':483,'multiline':False]['text':' (i7 ^ 2, ..., i0 ^ 2)','line_number':485,'multiline':False]['text':' (i15 ^ 2, ..., i8 ^ 2)','line_number':487,'multiline':False]['text':' widen to epi32','line_number':489,'multiline':False]['text':' add to running sum','line_number':492,'multiline':False]['text':' vectorized','line_number':504,'multiline':False]['text':'2147483647/(256*256)*8 = 1048576','line_number':508,'multiline':False]['text':' (i31, ..., i0)','line_number':513,'multiline':False]['text':' (i31 ^ 2, ..., i0 ^ 2)','line_number':516,'multiline':False]['text':' (i15 ^ 2, ..., i0 ^ 2)','line_number':518,'multiline':False]['text':' (i31 ^ 2, ..., i16 ^ 2)','line_number':520,'multiline':False]['text':' widen to epi32','line_number':522,'multiline':False]['text':' add to running sum','line_number':525,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':536,'multiline':False]['text':' scalar','line_number':538,'multiline':False]['text':' horizontal sum os squares over a range of int32_t','line_number':546,'multiline':False]['text':' floats throughout are necessary to prevent overflow','line_number':547,'multiline':False]['text':' vectorized','line_number':554,'multiline':False]['text':' vectorized','line_number':568,'multiline':False]['text':' CPU_CAPABILITY_AVX2 or CPU_CAPABILITY_AVX512','line_number':580,'multiline':False]['text':' scalar','line_number':582,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':596,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':616,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':620,'multiline':False]['text':' Naive implementation uses dequant/quant loop.','line_number':627,'multiline':False]['text':' Vectorized implementation creates a multiplicand vector, which has
           * "alpha" for all negative dx values and ones-vector for all
           * positive values of dx. The multiplicand then is multiplied by the
           * input.
           ','line_number':648,'multiline':True]['text':' This logic is present in at::prelu and repeated here, as this path can be','line_number':683,'multiline':False]['text':' hit via quantized::prelu, which is registered under quantized/cpu/qprelu.cpu','line_number':684,'multiline':False]['text':' This will always be a view in CPU/CUDA, but some backends','line_number':691,'multiline':False]['text':' like MKLDNN do not support views','line_number':692,'multiline':False]['text':' Quantized one as weight','line_number':707,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':743,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':761,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':800,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':835,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':845,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':878,'multiline':False]['text':' - Output scale is set to 1.0 / 2^(BIT_NUM)','line_number':886,'multiline':False]['text':' 1.0 / 2^8','line_number':887,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.NullDereference)','line_number':888,'multiline':False]['text':' 1.0 / 2^32','line_number':890,'multiline':False]['text':' The default zero-point is zero.  As a one-off optimization for','line_number':894,'multiline':False]['text':' kQInt8, we set the zero-point to -128 to maximize precision in the','line_number':895,'multiline':False]['text':' [0, 1] output range. kQInt32 can be handled in a future PR if needed.','line_number':896,'multiline':False]['text':' Naive implemenentation: uses dequantize/execute/quantize routine','line_number':916,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':949,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':983,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':1009,'multiline':False]['text':' TODO: For future tasks, since output quantization parameters are set equal to','line_number':1031,'multiline':False]['text':' the input ones, it might make sense to implement this completely in the','line_number':1032,'multiline':False]['text':' quantized domain.','line_number':1033,'multiline':False]['text':' defines input and output scales and zero_points','line_number':1039,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1041,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1044,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1046,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':1052,'multiline':False]['text':' vectorized','line_number':1058,'multiline':False]['text':' defines the iterator','line_number':1061,'multiline':False]['text':' defines the vectorized versions','line_number':1063,'multiline':False]['text':' defines the floating-point versions of threshold and value','line_number':1067,'multiline':False]['text':' Naive implemenentation: uses dequantize/execute/quantize routine','line_number':1073,'multiline':False]['text':' dequantize','line_number':1077,'multiline':False]['text':' Applies the Threshold operation','line_number':1079,'multiline':False]['text':' quantize','line_number':1081,'multiline':False]['text':' dequantize','line_number':1085,'multiline':False]['text':' check if any elements are below threshold','line_number':1089,'multiline':False]['text':' blend','line_number':1092,'multiline':False]['text':' quantize','line_number':1096,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1109,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1148,'multiline':False]['text':' Naive implemenentation: uses dequantize/execute/quantize routine','line_number':1155,'multiline':False]['text':' - Output scale is set to 2.0 / 2^(BIT_NUM)','line_number':1156,'multiline':False]['text':' - For signed types output zero point is set to 0','line_number':1157,'multiline':False]['text':' - For unsigned types output zero point is set to (qmax + qmin) / 2.0','line_number':1158,'multiline':False]['text':' 2.0 / 512','line_number':1159,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.NullDereference)','line_number':1161,'multiline':False]['text':' 2.0 / 2^32','line_number':1163,'multiline':False]['text':' scale and input_scale arguments refer to a generalized ELU formula','line_number':1205,'multiline':False]['text':' if x >= 0, ELU(x) = x * scale','line_number':1206,'multiline':False]['text':' if x <= 0, ELU(x) = (exp(x * input_scale) - 1) * scale','line_number':1207,'multiline':False]['text':' in the normal ELU formula, both are equal to 1','line_number':1208,'multiline':False]['text':' they are NOT related to the quantization scale term','line_number':1209,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1212,'multiline':False]['text':' In a future PR, we can improve on output scale and zero_point','line_number':1215,'multiline':False]['text':' selection.','line_number':1216,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1218,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1220,'multiline':False]['text':' vectorized','line_number':1231,'multiline':False]['text':' dequantize','line_number':1247,'multiline':False]['text':' ELU','line_number':1249,'multiline':False]['text':' quantize','line_number':1254,'multiline':False]['text':' dequantize','line_number':1258,'multiline':False]['text':' quickly check if any elements are below zero','line_number':1262,'multiline':False]['text':' calculate the negative part of ELU on the copy','line_number':1268,'multiline':False]['text':' blend','line_number':1273,'multiline':False]['text':' quantize','line_number':1280,'multiline':False]['text':' Note: out is assumed to be the same size as self and other.','line_number':1288,'multiline':False]['text':' Note: Addition is only supported when self and out are of the same dtype.','line_number':1289,'multiline':False]['text':' Note: other is already assumed to be in int32, i.e., it's','line_number':1290,'multiline':False]['text':' round(float/self_scale)','line_number':1291,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1295,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1299,'multiline':False]['text':' Note: out is assumed to be the same size as self and other.','line_number':1337,'multiline':False]['text':' Note: Addition is only supported when self, other, out are of the same dtype.','line_number':1338,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1342,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1346,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1349,'multiline':False]['text':' Broadcast out the parameters here to amortize out that cost across','line_number':1352,'multiline':False]['text':' loop iterations.','line_number':1353,'multiline':False]['text':' TODO: we can optimize dequantization by doing a premultiplication','line_number':1354,'multiline':False]['text':' of the zero point by scale and doing FMA on scale*x_q - (scale*zero_point)','line_number':1355,'multiline':False]['text':' TODO: fbgemm::Quantize doesn't support taking in the','line_number':1394,'multiline':False]['text':' pre-broadcasted parameters. We might be able to save some cycles by','line_number':1395,'multiline':False]['text':' enabling that in the API.','line_number':1396,'multiline':False]['text':' TODO: specialize fbgemm::Quantize for a single vector and make it','line_number':1397,'multiline':False]['text':' inlineable. This could help with interleaving as suggested by the','line_number':1398,'multiline':False]['text':' TensorIterator implementations','line_number':1399,'multiline':False]['text':' Note: out is assumed to be the same size as self and other.','line_number':1406,'multiline':False]['text':' Note: Multiplication is only supported when self, other, out are of the same','line_number':1407,'multiline':False]['text':' dtype.','line_number':1408,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1412,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1416,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1419,'multiline':False]['text':' input/output channels','line_number':1464,'multiline':False]['text':' input sizes','line_number':1466,'multiline':False]['text':' output sizes','line_number':1468,'multiline':False]['text':' kernel size','line_number':1470,'multiline':False]['text':' strides','line_number':1472,'multiline':False]['text':' padding','line_number':1474,'multiline':False]['text':' dilation','line_number':1476,'multiline':False]['text':' Loop over reduction block','line_number':1490,'multiline':False]['text':' Interleaved vector loop 4x','line_number':1502,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)','line_number':1507,'multiline':False]['text':' for x','line_number':1519,'multiline':False]['text':' for y','line_number':1520,'multiline':False]['text':' for c','line_number':1524,'multiline':False]['text':' Vector loop','line_number':1526,'multiline':False]['text':' for x','line_number':1537,'multiline':False]['text':' for y','line_number':1538,'multiline':False]['text':' for c','line_number':1540,'multiline':False]['text':' for x','line_number':1551,'multiline':False]['text':' for y','line_number':1552,'multiline':False]['text':' for c','line_number':1555,'multiline':False]['text':' input/output channels','line_number':1564,'multiline':False]['text':' input sizes','line_number':1566,'multiline':False]['text':' output sizes','line_number':1568,'multiline':False]['text':' kernel size','line_number':1570,'multiline':False]['text':' strides','line_number':1572,'multiline':False]['text':' padding','line_number':1574,'multiline':False]['text':' dilation','line_number':1576,'multiline':False]['text':' input/output channels','line_number':1591,'multiline':False]['text':' input sizes','line_number':1594,'multiline':False]['text':' output sizes','line_number':1597,'multiline':False]['text':' kernel size','line_number':1600,'multiline':False]['text':' strides','line_number':1603,'multiline':False]['text':' padding','line_number':1606,'multiline':False]['text':' dilation','line_number':1609,'multiline':False]['text':' Loop over reduction block','line_number':1624,'multiline':False]['text':' Vector loop','line_number':1640,'multiline':False]['text':' for x','line_number':1652,'multiline':False]['text':' for y','line_number':1653,'multiline':False]['text':' for t','line_number':1654,'multiline':False]['text':' for c','line_number':1656,'multiline':False]['text':' for x','line_number':1668,'multiline':False]['text':' for y','line_number':1669,'multiline':False]['text':' for t','line_number':1670,'multiline':False]['text':' for c','line_number':1672,'multiline':False]['text':' buffer for channel accumulator, used to interchange channel-loop','line_number':1700,'multiline':False]['text':' to inner-most, so that memory access of the input tensor data is','line_number':1701,'multiline':False]['text':' continuous.','line_number':1702,'multiline':False]['text':' initialize loop','line_number':1720,'multiline':False]['text':' compute loop','line_number':1724,'multiline':False]['text':' convert int32 accumulative to fp32','line_number':1740,'multiline':False]['text':' first quantize using AVX2 or AVX512 using 32 lanes, then 8, finally falls','line_number':1743,'multiline':False]['text':' back to single','line_number':1744,'multiline':False]['text':' Set to 1 for 2d','line_number':1828,'multiline':False]['text':' Set to 1 for 2d','line_number':1831,'multiline':False]['text':' Set to 1 for 2d','line_number':1836,'multiline':False]['text':' For int8 or uint8quantization, we implicitly use int32 as','line_number':1875,'multiline':False]['text':' accumulation Or else, it will go to the slow path','line_number':1876,'multiline':False]['text':' TODO: support 16bit, 32bit, and etc.','line_number':1877,'multiline':False]['text':' Note: If AVX is not available, `do_avg_pool_on_AVX_n is a noop.','line_number':1883,'multiline':False]['text':'       In that case, the following loop takes over','line_number':1884,'multiline':False]['text':' TODO: more vectorization with loop interleaving','line_number':1885,'multiline':False]['text':' 1) The following loop handles the remaining channels','line_number':1905,'multiline':False]['text':' 2) It also handles the Non-AVX2 path','line_number':1906,'multiline':False]['text':' clamp','line_number':1921,'multiline':False]['text':' c','line_number':1925,'multiline':False]['text':' oh','line_number':1926,'multiline':False]['text':' ow','line_number':1927,'multiline':False]['text':' od','line_number':1928,'multiline':False]['text':'isizeD=','line_number':1952,'multiline':True]['text':'osizeD=','line_number':1955,'multiline':True]['text':'istrideD=','line_number':1960,'multiline':True]['text':' lift these operations outside the loop to reduce access overheads','line_number':2035,'multiline':False]['text':' For int8 quantization, we implicitly use int32 as accumulation','line_number':2075,'multiline':False]['text':' Or else, it will go to the slow path','line_number':2076,'multiline':False]['text':' TODO: support 16bit, 32bit, and etc.','line_number':2077,'multiline':False]['text':' 1) The following loop handles the remaining channels','line_number':2096,'multiline':False]['text':' 2) It also handles the Non-AVX2 path','line_number':2097,'multiline':False]['text':' clamp','line_number':2111,'multiline':False]['text':' c','line_number':2115,'multiline':False]['text':'cubic=','line_number':2315,'multiline':True]['text':'cubic=','line_number':2323,'multiline':True]['text':' We use float32 to do the computation','line_number':2331,'multiline':False]['text':' We have to isolate this function out because the VS does not','line_number':2336,'multiline':False]['text':' expand the macro correctly.','line_number':2337,'multiline':False]['text':' 1) The following loop handles the remaining channels','line_number':2355,'multiline':False]['text':' 2) It also handles the Non-AVX2 path','line_number':2356,'multiline':False]['text':' c','line_number':2370,'multiline':False]['text':'squash_dims=','line_number':2389,'multiline':True]['text':'grain_size=','line_number':2409,'multiline':True]['text':' Fake scale of 1.0 here, should not affect performance (FMA in place of sub)','line_number':2430,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':2437,'multiline':False]['text':'output_scale=','line_number':2438,'multiline':True]['text':'inv_output_scale=','line_number':2438,'multiline':True]['text':' Fake scale again','line_number':2439,'multiline':False]['text':' Hoisted variables','line_number':2470,'multiline':False]['text':' for channel between 8 and 32, still use 32 width for performance','line_number':2499,'multiline':False]['text':' Benchmark shows it is faster than doing 8 channels each time','line_number':2500,'multiline':False]['text':' 3 cycles','line_number':2505,'multiline':False]['text':' for channels less than 8','line_number':2522,'multiline':False]['text':' static if','line_number':2527,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2608,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2610,'multiline':False]['text':'  When a for_each call is made on a TensorIterator with multiple inputs and outputs,
        the order they are accessed follows the order they are built within the iterator.
        For example, if an iterator is built in the following order:
        auto iter = TensorIteratorConfig().
          .add_output(firstOutput)
          .add_output(secondOutput)
          .add_input(firstInput)
          .add_input(secondInput)
          .build()
        data will contain 4 pointers to pointers to values in the following order:
        firstOutput, secondOutput, firstInput, secondInput.
        Proper pointer referencing and dereferencing, along with the usage of strides
        (to move onto different elements), can allow accessing of the input and assignment
        to the right output.
    ','line_number':2613,'multiline':True]['text':' Calculate gradients for X.','line_number':2634,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2636,'multiline':False]['text':' Calculate gradients for scale and zero point.','line_number':2638,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2639,'multiline':False]['text':' Calculate gradients according to the gradient of the clamp function.','line_number':2641,'multiline':False]['text':' When zero_point is float, quantize mirroring affine quantizer equation','line_number':2663,'multiline':False]['text':' Xq = Round(Xf * inv_scale + zero_point)','line_number':2664,'multiline':False]['text':' where zero_point is in float.','line_number':2665,'multiline':False]['text':' write mask','line_number':2667,'multiline':False]['text':' write fake_quant','line_number':2674,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2677,'multiline':False]['text':' write mask','line_number':2689,'multiline':False]['text':' write fake_quant','line_number':2696,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2699,'multiline':False]['text':' TODO(future, optional): read once, write twice.  Not done at the moment','line_number':2719,'multiline':False]['text':'   for simplicity, as we do not expect this to be a bottleneck.','line_number':2720,'multiline':False]['text':'  To see how the input and outputs are referenced and assigned,
        please see the implemenetation of
        fake_quantize_learnable_tensor_grad_kernel_cpu.
    ','line_number':2734,'multiline':True]['text':' Calculate gradients for X.','line_number':2750,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':2752,'multiline':False]['text':' Calculate gradients for scale and zero point.','line_number':2754,'multiline':False]['text':' Assumes X is composed of M groups of N elements. Normalizes each of the','line_number':2767,'multiline':False]['text':' groups and optionally applies affine scaling. Useful for LayerNorm,','line_number':2768,'multiline':False]['text':' GroupNorm, InstanceNorm.','line_number':2769,'multiline':False]['text':' input tensor','line_number':2771,'multiline':False]['text':' weight (optional)','line_number':2772,'multiline':False]['text':' bias (optional)','line_number':2773,'multiline':False]['text':' scaling applied elementwise if false, per channel if true','line_number':2774,'multiline':False]['text':' only used if affine_per_channel is set','line_number':2775,'multiline':False]['text':' only used if affine_per_channel is set','line_number':2776,'multiline':False]['text':' number of groups','line_number':2777,'multiline':False]['text':' number of elements in each group','line_number':2778,'multiline':False]['text':' First pass: calculate mean and variance.','line_number':2830,'multiline':False]['text':' mean(dqX) / scale_x','line_number':2836,'multiline':False]['text':' var(dqX) / scale_x^2','line_number':2838,'multiline':False]['text':' scale_x / sqrt(var(dqX) + eps)','line_number':2842,'multiline':False]['text':' Second pass: normalize','line_number':2848,'multiline':False]['text':' TODO replace with TensorIterator implementation once #33166 is fixed.','line_number':2850,'multiline':False]['text':' if scaling per channel, scaling parameters can be pre-multiplied','line_number':2853,'multiline':False]['text':' with normalization parameters','line_number':2854,'multiline':False]['text':' scale_x / layer_std * gamma','line_number':2858,'multiline':False]['text':' Remainder','line_number':2881,'multiline':False]['text':' chIdx','line_number':2897,'multiline':False]['text':' parallel_for','line_number':2932,'multiline':False]['text':' 'opt_dtype' should be none or equal to that of input','line_number':2943,'multiline':False]['text':' Num of groups','line_number':2949,'multiline':False]['text':' Num of elements to take average of in each group','line_number':2950,'multiline':False]['text':' Num of groups','line_number':2999,'multiline':False]['text':' Num of elements to take std of in each group','line_number':3000,'multiline':False]['text':' Denominator when computing mean and deviation','line_number':3013,'multiline':False]['text':' Use double for intermediate variables to avoid accuracy issue','line_number':3035,'multiline':False]['text':' Mean with zero point','line_number':3036,'multiline':False]['text':' variance / x_scale^2','line_number':3039,'multiline':False]['text':' For group norm of channels_last input','line_number':3053,'multiline':False]['text':' input tensor','line_number':3055,'multiline':False]['text':' weight (optional)','line_number':3056,'multiline':False]['text':' bias (optional)','line_number':3057,'multiline':False]['text':' must be true for group/instance norm','line_number':3058,'multiline':False]['text':' only used if affine_per_channel is set','line_number':3059,'multiline':False]['text':' only used if affine_per_channel is set','line_number':3060,'multiline':False]['text':' number of groups = Bs * G','line_number':3061,'multiline':False]['text':' number of elements in each group = C * H * W / G','line_number':3062,'multiline':False]['text':' Buffer for x and x^2','line_number':3112,'multiline':False]['text':' We can parallel in the following 2 impls:','line_number':3116,'multiline':False]['text':'','line_number':3117,'multiline':False]['text':' impl-1: parallel on N * G. Only need one omp session but memory access','line_number':3118,'multiline':False]['text':'   per thread is non-contiguous.','line_number':3119,'multiline':False]['text':'','line_number':3120,'multiline':False]['text':' impl-2: parallel on N * HxW. Memory access per thread is contiguous,','line_number':3121,'multiline':False]['text':'   but requires help of extra temp buffer of size {T, N, 2C}.','line_number':3122,'multiline':False]['text':'','line_number':3123,'multiline':False]['text':' Generally impl-2 has better performance when HxW is large enough','line_number':3124,'multiline':False]['text':' The threshold is found by tests.','line_number':3125,'multiline':False]['text':' Impl-1: Parallel for each group','line_number':3128,'multiline':False]['text':'','line_number':3129,'multiline':False]['text':' Parallel for each group, M = Bs * G','line_number':3130,'multiline':False]['text':' batch index ','line_number':3132,'multiline':True]['text':' group index in each batch ','line_number':3132,'multiline':True]['text':' For each group','line_number':3134,'multiline':False]['text':' Step 1: calculate mean and variance.','line_number':3136,'multiline':False]['text':' mean(dqX) / scale_x + x_zp','line_number':3146,'multiline':False]['text':' mean(dqX) / scale_x','line_number':3148,'multiline':False]['text':' var(dqX) / scale_x^2','line_number':3150,'multiline':False]['text':' scale_x / sqrt(var(dqX) + eps)','line_number':3154,'multiline':False]['text':' Step 2: calculate scale and bias','line_number':3158,'multiline':False]['text':' Step 3: applying scale and bias','line_number':3167,'multiline':False]['text':' vectorized','line_number':3171,'multiline':False]['text':' Remaining scalar','line_number':3185,'multiline':False]['text':' loop over HxW','line_number':3194,'multiline':False]['text':' for each group','line_number':3197,'multiline':False]['text':' parallel_for','line_number':3198,'multiline':False]['text':' HxW > feature_map_threshold','line_number':3199,'multiline':False]['text':' impl-2: parallel on Bs * HxW.','line_number':3200,'multiline':False]['text':'','line_number':3201,'multiline':False]['text':' Buffer for x and x^2','line_number':3202,'multiline':False]['text':' To avoid thread conflict, we use a temp buffer of {T, Bs, 2*C}','line_number':3203,'multiline':False]['text':' Step 1: Accumulate on C dimension','line_number':3212,'multiline':False]['text':' batch index ','line_number':3217,'multiline':True]['text':' HxW index ','line_number':3217,'multiline':True]['text':' Step 2: Calculate mean and rstd','line_number':3233,'multiline':False]['text':' for d','line_number':3242,'multiline':False]['text':' for t','line_number':3243,'multiline':False]['text':' mean / scale_x + x_zp','line_number':3245,'multiline':False]['text':' mean / scale_x','line_number':3247,'multiline':False]['text':' var / scale_x^2','line_number':3249,'multiline':False]['text':' scale_x / sqrt(var + eps)','line_number':3253,'multiline':False]['text':' for g','line_number':3259,'multiline':False]['text':' for n','line_number':3260,'multiline':False]['text':' Step 3: Calculate scale and bias','line_number':3262,'multiline':False]['text':'','line_number':3263,'multiline':False]['text':' We could fuse step 3 and 4 into a single session but this way is better:','line_number':3264,'multiline':False]['text':'   a. D might be too small for vectorization;','line_number':3265,'multiline':False]['text':'   b. Avoid duplicate caculation of scale/bias, each HxW plain share the same scale/bias','line_number':3266,'multiline':False]['text':'','line_number':3267,'multiline':False]['text':' for d','line_number':3278,'multiline':False]['text':' for g','line_number':3279,'multiline':False]['text':' for n','line_number':3280,'multiline':False]['text':' step-4: apply scale and bias','line_number':3282,'multiline':False]['text':'','line_number':3283,'multiline':False]['text':' Parallel on all the outer dimensions of Bs and HxW','line_number':3284,'multiline':False]['text':' and vectorize on C.','line_number':3285,'multiline':False]['text':'','line_number':3286,'multiline':False]['text':' Vectorized','line_number':3295,'multiline':False]['text':' Remaining scalar','line_number':3309,'multiline':False]['text':' for idx on nhw','line_number':3320,'multiline':False]['text':' parallel_for on nhw','line_number':3321,'multiline':False]['text':' if HxW > feature_map_threshold','line_number':3323,'multiline':False]['text':' AT_DISPATCH_QINT_TYPES','line_number':3325,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':3339,'multiline':False]['text':'LEGACY','line_number':3347,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3348,'multiline':False]['text':'src=','line_number':3349,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3350,'multiline':False]['text':'dst=','line_number':3351,'multiline':True]['text':'len','line_number':3352,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3353,'multiline':False]['text':'qparams=','line_number':3354,'multiline':True]['text':'thread_id','line_number':3355,'multiline':True]['text':'num_threads','line_number':3356,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':3372,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3382,'multiline':False]['text':'src=','line_number':3383,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3384,'multiline':False]['text':'dst=','line_number':3385,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3386,'multiline':False]['text':'len=','line_number':3387,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':3388,'multiline':False]['text':'qparams=','line_number':3389,'multiline':True]['text':'thread_id','line_number':3390,'multiline':True]['text':'num_threads','line_number':3391,'multiline':True]['text':' USE_FBGEMM','line_number':3396,'multiline':False]['text':' Generic template defaults to naive quantize implementation','line_number':3402,'multiline':False]['text':' namespace quantize_tensor_arm_intrinsics','line_number':3441,'multiline':False]['text':' Specialized implementation from caffe2::Int8Quantize.','line_number':3443,'multiline':False]['text':' There may be slight accuracy difference between this and implementation of','line_number':3444,'multiline':False]['text':' quantize_val','line_number':3445,'multiline':False]['text':' TODO Update quantize_tensor_arm implementation to follow quantize_val,','line_number':3446,'multiline':False]['text':' i.e. f = Round(value/scale + zero_point)','line_number':3447,'multiline':False]['text':' TODO Make quantize_tensor_arm work for int32 datatype too.','line_number':3448,'multiline':False]['text':' magic float and magic int to take care of rounding','line_number':3461,'multiline':False]['text':' int magic_round(float f): interpret_int32(f + 12582912.0f) - 0x4B400000','line_number':3462,'multiline':False]['text':' Some detail:','line_number':3463,'multiline':False]['text':' 12582912.0f is 2**23 + 2**22. The trick is based on the fact that when you','line_number':3464,'multiline':False]['text':' add a small number to a large number, the result rounds to the precision of','line_number':3465,'multiline':False]['text':' the least significant bit of the large number. For IEEE-754','line_number':3466,'multiline':False]['text':' single-precision number mantissa has 23 bits, and adding 2**23 would cause','line_number':3467,'multiline':False]['text':' rounding to the nearest even integer. The we cast to int and subtract the','line_number':3468,'multiline':False]['text':' same number (0x4B400000 is the integer representation of 12582912.0f) to','line_number':3469,'multiline':False]['text':' get only the mantissa. This works if -2**22 < x < 2**22, but preserves the','line_number':3470,'multiline':False]['text':' sign for negative numbers.','line_number':3471,'multiline':False]['text':' vmovl_high intrinsic not supported','line_number':3551,'multiline':False]['text':' Generic template defaults to naive dequantize implementation','line_number':3558,'multiline':False]['text':' Zero point is restricted to be in bounds of a signed 8 bit integer','line_number':3581,'multiline':False]['text':' Extract upper or lower values to int16x8 and subtract zero point','line_number':3588,'multiline':False]['text':' Each input element and the zero point are restricted to be in bounds of','line_number':3589,'multiline':False]['text':' a signed 8 bit integer, so the difference will fit in a signed 16 bit','line_number':3590,'multiline':False]['text':' integer','line_number':3591,'multiline':False]['text':' 0 ... 7','line_number':3592,'multiline':False]['text':' 8 ... 15','line_number':3593,'multiline':False]['text':' 0 ... 3','line_number':3595,'multiline':False]['text':' 4 ... 7','line_number':3596,'multiline':False]['text':' 8 ... 11','line_number':3597,'multiline':False]['text':' 12 ... 15','line_number':3598,'multiline':False]['text':' Store            * scale   int32->fp32','line_number':3600,'multiline':False]['text':' use default dequantize for remaining vals','line_number':3611,'multiline':False]['text':' Zero point is restricted to be in bounds of an unsigned 8 bit integer','line_number':3626,'multiline':False]['text':' Extract upper or lower values to uint16x8 and subtract zero point','line_number':3633,'multiline':False]['text':' Each input element and the zero point are restricted to be in bounds of','line_number':3634,'multiline':False]['text':' an unsigned 8 bit integer, so the difference will fit in a signed 16 bit','line_number':3635,'multiline':False]['text':' integer','line_number':3636,'multiline':False]['text':' 0 ... 7','line_number':3637,'multiline':False]['text':' 8 ... 15','line_number':3638,'multiline':False]['text':' 0 ... 3','line_number':3640,'multiline':False]['text':' 4 ... 7','line_number':3641,'multiline':False]['text':' 8 ... 11','line_number':3642,'multiline':False]['text':' 12 ... 15','line_number':3643,'multiline':False]['text':' Store            * scale   int32->fp32','line_number':3645,'multiline':False]['text':' use default dequantize for remaining vals','line_number':3656,'multiline':False]['text':' defined(__ARM_NEON__) || defined(__aarch64__)','line_number':3661,'multiline':False]['text':' Fallback path','line_number':3686,'multiline':False]['text':' defined(__ARM_NEON__) || defined(__aarch64__)','line_number':3694,'multiline':False]['text':' Fallback path','line_number':3720,'multiline':False]['text':' defined(__ARM_NEON__) || defined(__aarch64__)','line_number':3728,'multiline':False]['text':' USE_FBGEMM','line_number':3730,'multiline':False]['text':' TODO: add fbgemm for per channel','line_number':3732,'multiline':False]['text':' Generic template defaults to naive quantize implementation','line_number':3733,'multiline':False]['text':' TODO: channels last kernel can be made faster.','line_number':3741,'multiline':False]['text':' For contiguous tensors, e.g. NCHW, arbitrary axis can be used.','line_number':3742,'multiline':False]['text':' For channels_last/3d however axis == 0 or 1.','line_number':3743,'multiline':False]['text':' Since current implemntation on channels_last format does not','line_number':3744,'multiline':False]['text':' cover per channel quant with arbitrary axis value, it is better','line_number':3745,'multiline':False]['text':' to check and fail.','line_number':3746,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':3748,'multiline':False]['text':' This code handles per channel quant when axis = 1 and','line_number':3758,'multiline':False]['text':' channels_last contig.','line_number':3759,'multiline':False]['text':' If axis = 0 and channels_last contig, implementation for channels','line_number':3760,'multiline':False]['text':' first (NCHW) works.','line_number':3761,'multiline':False]['text':' Specialized implementation from caffe2::Int8Quantize.','line_number':3786,'multiline':False]['text':' There may be slight accuracy difference between this and implementation of','line_number':3787,'multiline':False]['text':' quantize_val','line_number':3788,'multiline':False]['text':' TODO Update quantize_tensor_per_channel_impl implementation to follow','line_number':3789,'multiline':False]['text':' quantize_val, i.e. f = Round(value/scale + zero_point)','line_number':3790,'multiline':False]['text':' TODO Make quantize_tensor_per_channel_impl work for other datatypes too','line_number':3791,'multiline':False]['text':' (int8, int32).','line_number':3792,'multiline':False]['text':' magic float and magic int to take care of rounding','line_number':3808,'multiline':False]['text':' int magic_round(float f): interpret_int32(f + 12582912.0f) - 0x4B400000','line_number':3809,'multiline':False]['text':' Some detail:','line_number':3810,'multiline':False]['text':' 12582912.0f is 2**23 + 2**22. The trick is based on the fact that when you','line_number':3811,'multiline':False]['text':' add a small number to a large number, the result rounds to the precision of','line_number':3812,'multiline':False]['text':' the least significant bit of the large number. For IEEE-754','line_number':3813,'multiline':False]['text':' single-precision number mantissa has 23 bits, and adding 2**23 would cause','line_number':3814,'multiline':False]['text':' rounding to the nearest even integer. The we cast to int and subtract the','line_number':3815,'multiline':False]['text':' same number (0x4B400000 is the integer representation of 12582912.0f) to','line_number':3816,'multiline':False]['text':' get only the mantissa. This works if -2**22 < x < 2**22, but preserves the','line_number':3817,'multiline':False]['text':' sign for negative numbers.','line_number':3818,'multiline':False]['text':' Copy reciprocal of scales (double) into float array','line_number':3820,'multiline':False]['text':' Copy zero_points with magic int (int64_t) into int32_t array','line_number':3821,'multiline':False]['text':' This code handles per channel quant when axis = 1 and','line_number':3831,'multiline':False]['text':' channels_last contig.','line_number':3832,'multiline':False]['text':' If axis = 0 and channels_last contig, implementation for channels','line_number':3833,'multiline':False]['text':' first (NCHW) works.','line_number':3834,'multiline':False]['text':' defined(__ARM_NEON__)','line_number':3901,'multiline':False]['text':' Copy scales (double) into float array','line_number':3902,'multiline':False]['text':' Copy zero_points (int64_t) into int16_t array','line_number':3903,'multiline':False]['text':' This code handles per channel quant when axis = 1 and','line_number':3913,'multiline':False]['text':' channels_last contig.','line_number':3914,'multiline':False]['text':' If axis = 0 and channels_last contig, implementation for channels','line_number':3915,'multiline':False]['text':' first (NCHW) works.','line_number':3916,'multiline':False]['text':' defined(__ARM_NEON__)','line_number':3976,'multiline':False]['text':' defined(__ARM_NEON__) || defined(__aarch64__)','line_number':3978,'multiline':False]['text':' For contiguous tensors, e.g. NCHW, arbitrary axis can be used.','line_number':4007,'multiline':False]['text':' For channels_last/3d however axis == 0 or 1.','line_number':4008,'multiline':False]['text':' Since current implemntation on channels_last format does not','line_number':4009,'multiline':False]['text':' cover per channel quant with arbitrary axis value, it is better','line_number':4010,'multiline':False]['text':' to check and fail.','line_number':4011,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':4017,'multiline':False]['text':' We need to convert the qint8 value to float to ensure the','line_number':4032,'multiline':False]['text':' subtraction subexpression returns a float','line_number':4033,'multiline':False]['text':' We need to convert the qint8 value to float to ensure the','line_number':4049,'multiline':False]['text':' subtraction subexpression returns a float','line_number':4050,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.DivideZero)','line_number':4051,'multiline':False]['text':' quantize stubs for floating point scale and zero_point.','line_number':4076,'multiline':False]['text':' For contiguous tensors, e.g. NCHW, arbitrary axis can be used.','line_number':4083,'multiline':False]['text':' For channels_last/3d however axis == 0 or 1.','line_number':4084,'multiline':False]['text':' Since current implemntation on channels_last format does not','line_number':4085,'multiline':False]['text':' cover per channel quant with arbitrary axis value, it is better','line_number':4086,'multiline':False]['text':' to check and fail.','line_number':4087,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.DivideZero)','line_number':4112,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.DivideZero)','line_number':4129,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-unused-variable)','line_number':4148,'multiline':False]['text':' TODO Use fbgemm kernel to pack values','line_number':4160,'multiline':False]['text':' We pack sub_byte values and align them to a byte.','line_number':4173,'multiline':False]['text':' Eg. for 4-bits Index 0 is packed in the lower 4-bits','line_number':4174,'multiline':False]['text':' and index 1 is packed in the upper 4-bits.','line_number':4175,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.DivideZero)','line_number':4176,'multiline':False]['text':' for numel','line_number':4182,'multiline':False]['text':' TODO Use fbgemm kernel to pack values','line_number':4191,'multiline':False]['text':' NOLINTNEXTLINE(clang-diagnostic-unused-variable)','line_number':4192,'multiline':False]['text':' NOLINTNEXTLINE(clang-analyzer-core.DivideZero)','line_number':4202,'multiline':False]['text':' This function expects quantized_val input to already be quantized','line_number':4211,'multiline':False]['text':' currently, we do not support accumulate=True for quantized tensors. We throw an exception in _index_put_impl_quantized_cpu_','line_number':4239,'multiline':False]['text':' NOTE: duplicate indices are only supported if accumulate is true.','line_number':4241,'multiline':False]['text':' See Note [Enabling Deterministic Operations]','line_number':4243,'multiline':False]['text':' Parallel cpu_index_kernel with accumulation is nondeterministic, so we','line_number':4244,'multiline':False]['text':' must enable serial execution if deterministic algorithms are enabled.','line_number':4245,'multiline':False]['text':'serial_execution=','line_number':4249,'multiline':True]['text':' anonymous namespace','line_number':4252,'multiline':False]['text':' Some quantization tests are flaky on Windows with AVX512. If --continue-through-error','line_number':4254,'multiline':False]['text':' is used, only one fails. But if the failing test is skipped, another one fails.','line_number':4255,'multiline':False]['text':' If the second test is also skipped, a third one fails.','line_number':4256,'multiline':False]['text':' So, until Quantization support for Windows is fixed for AVX512,','line_number':4257,'multiline':False]['text':' AVX2 kernels would be used instead. Ref: GH 56992.','line_number':4258,'multiline':False]['text':' These kernels are dispatched to AVX512','line_number':4269,'multiline':False]['text':' CPU_CAPABILITY_AVX512 && _WIN32','line_number':4278,'multiline':False]['text':' The kernels below are dispatched to AVX2 because they don't perform as well','line_number':4280,'multiline':False]['text':' with AVX512. We might revisit this decision in the near future.','line_number':4281,'multiline':False]['text':' namespace native','line_number':4350,'multiline':False]['text':' namespace at','line_number':4351,'multiline':False]