['text':' CUDA < 11.0 doesn't support row-major layout, return column-major in this case','line_number':46,'multiline':False]['text':' This function is used for old CUDA Toolkit versions that doesn't support new cuSPARSE Generic API','line_number':75,'multiline':False]['text':' cusparse bsrsv2 and bsrsm2 have a synchronization issue that may cause illegal memory access in cuda <= 11.6.x','line_number':126,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/71297','line_number':127,'multiline':False]['text':' else: do nothing!','line_number':130,'multiline':False]['text':' values is expected to be a blocks of sparse matrix','line_number':148,'multiline':False]['text':' blocks are expected to be square','line_number':150,'multiline':False]['text':' only block of size > 1 is supported in cuSPARSE','line_number':152,'multiline':False]['text':' blocks are expected to be in row- or column-major order','line_number':154,'multiline':False]['text':' cuSPARSE can't work with empty sparse matrices','line_number':159,'multiline':False]['text':' values is expected to be a blocks of sparse matrix','line_number':276,'multiline':False]['text':' blocks are expected to be square','line_number':278,'multiline':False]['text':' only block of size > 1 is supported in cuSPARSE','line_number':280,'multiline':False]['text':' blocks are expected to be in row- or column-major order','line_number':282,'multiline':False]['text':' cuSPARSE can't work with empty sparse matrices','line_number':287,'multiline':False]['text':' values is expected to be a blocks of sparse matrix','line_number':409,'multiline':False]['text':' blocks are expected to be square','line_number':411,'multiline':False]['text':' only block of size > 1 is supported in cuSPARSE','line_number':413,'multiline':False]['text':' blocks are expected to be in row- or column-major order','line_number':415,'multiline':False]['text':' values is expected to be a blocks of sparse matrix','line_number':474,'multiline':False]['text':' blocks are expected to be square','line_number':476,'multiline':False]['text':' only block of size > 1 is supported in cuSPARSE','line_number':478,'multiline':False]['text':' blocks are expected to be in row- or column-major order','line_number':480,'multiline':False]['text':' NOTE: the code below allows arbitrary block sizes','line_number':485,'multiline':False]['text':' and might be potentially faster than cuSPARSE implementation','line_number':486,'multiline':False]['text':' especially for not very sparse inputs.','line_number':487,'multiline':False]['text':'beta=','line_number':495,'multiline':True]['text':'alpha=','line_number':496,'multiline':True]['text':' @nikitaved: not sure whether `const Tensor& result` makes sense,','line_number':497,'multiline':False]['text':' but let's keep the interface intact, hence the const cast.','line_number':498,'multiline':False]['text':' cuSPARSE expects column-major strides for result and we can't manipulate','line_number':513,'multiline':False]['text':' transpose flag of mat1','line_number':514,'multiline':False]['text':' according to cuSPARSE documentation, opA can only be NON_TRANSPOSE','line_number':536,'multiline':False]['text':' Here subscript "c" stands for column-major, substript "r" stands for','line_number':595,'multiline':False]['text':' row-major order Both orders are supported by cuSPARSE. For mixed input we','line_number':596,'multiline':False]['text':' need to cast 'mat2' to order of 'result'. We compute','line_number':597,'multiline':False]['text':' result = mat1 @ op(mat2) + result.','line_number':598,'multiline':False]['text':' If order of 'mat2' and 'result' matches, the op is','line_number':599,'multiline':False]['text':' identity; op(mat2) == mat2. If 'result' is column-major and 'mat2' is','line_number':600,'multiline':False]['text':' row-major we pass 'mat2' as column-major and compute','line_number':601,'multiline':False]['text':' result_c = mat1 @ transpose(mat2_c) + result_c; mat2_r==transpose(mat2_c)','line_number':602,'multiline':False]['text':' if 'result' is row-major and 'mat2' is column-major we pass 'mat2'','line_number':603,'multiline':False]['text':' as row-major and compute','line_number':604,'multiline':False]['text':' result_r = mat1 @ transpose(mat2_r) + result_r; mat2_c==transpose(mat2_r)','line_number':605,'multiline':False]['text':' CUDA < 11.0 doesn't support 64-bit indices and doesn't raise an error about this','line_number':620,'multiline':False]['text':' silently returning incorrect results','line_number':621,'multiline':False]['text':' TODO: update this to support COO sparse layout','line_number':634,'multiline':False]['text':' output','line_number':667,'multiline':False]['text':' !(AT_USE_CUSPARSE_GENERIC_API() || AT_USE_HIPSPARSE_GENERIC_API())','line_number':690,'multiline':False]['text':' older versions of cusparse on Windows segfault for complex128 dtype','line_number':706,'multiline':False]['text':' Only 32-bit indices are supported','line_number':720,'multiline':False]['text':' Modify C tensor in-place to swap indices tensors with 32-bit variants','line_number':724,'multiline':False]['text':' It's required to call workEstimation twice','line_number':746,'multiline':False]['text':' It's required to call compute twice','line_number':781,'multiline':False]['text':' Get how many specified elements are there in C','line_number':815,'multiline':False]['text':' Resize result using nnz information from cusparse','line_number':822,'multiline':False]['text':' Update matC with the new pointers','line_number':825,'multiline':False]['text':' Copy the data into C','line_number':828,'multiline':False]['text':' anonymous namespace','line_number':845,'multiline':False]['text':' Layout checks are nested mat1, mat2, result','line_number':859,'multiline':False]['text':' Conditions are ordered strided, csr, csc, bsr, bsc.','line_number':860,'multiline':False]['text':' Valid combinations terminate in a return','line_number':861,'multiline':False]['text':' Invalid combinations are omitted and will fall though to the TORCH check','line_number':862,'multiline':False]['text':' generating an informative error message','line_number':863,'multiline':False]['text':' mm functions that copy input to result when needed (e.g. mm','line_number':865,'multiline':False]['text':' triton kernels do not require result being initialized with','line_number':866,'multiline':False]['text':' input):','line_number':867,'multiline':False]['text':' copy input to result:','line_number':891,'multiline':False]['text':' mm functions that assume that result contains input:','line_number':896,'multiline':False]['text':' TODO: Add native CSC support via cuSPARSE if supported.','line_number':900,'multiline':False]['text':' TODO: Add native CSC support via cuSPARSE if supported.','line_number':933,'multiline':False]['text':' CSR @ CSC kernel would be very fast due to format alignment','line_number':934,'multiline':False]['text':' TODO: Add native CSC support via cuSPARSE if supported.','line_number':942,'multiline':False]['text':' TODO: Add native CSC support via cuSPARSE if supported.','line_number':948,'multiline':False]['text':' TODO: Add native CSC support via cuSPARSE if supported.','line_number':953,'multiline':False]['text':'
  Computes a sparse matrix-dense vector product defined as
  y <- alpha*op(A)*x + beta*y

  Args:
  * `mat` - Tensor storing sparse m x n matrix A.
  * `vec` - Tensor storing dense vector x of size n.
  * `result` - [in] Tensor storing dense vector y of size m.
               [out] result of the operation.
','line_number':977,'multiline':True]['text':' TODO: update this to support COO sparse layout','line_number':1008,'multiline':False]['text':' cusparseSpMVAlg_t was updated in cuda 11.2.1 (cusparse 11.4.0)','line_number':1013,'multiline':False]['text':' SpMV doesn't support uniform precision computation','line_number':1020,'multiline':False]['text':' For float16/bfloat16 inputs compute_type must be CUDA_R_32F','line_number':1021,'multiline':False]['text':' and type of alpha, beta must be float','line_number':1022,'multiline':False]['text':' output','line_number':1049,'multiline':False]['text':' !(AT_USE_CUSPARSE_GENERIC_API() || AT_USE_HIPSPARSE_GENERIC_API())','line_number':1070,'multiline':False]['text':'
  Computes C = alpha * A + beta * B

  Args:
  * `A` - [in] sparse Tensor of size m × n.
  * `B` - [in] sparse Tensor of size m × n.
  * `C` - [out] sparse Tensor of size m × n.
','line_number':1073,'multiline':True]['text':' Only 32-bit indices are supported','line_number':1094,'multiline':False]['text':' Modify C tensor in-place to swap indices tensors with 32-bit variants','line_number':1113,'multiline':False]['text':' no-op with 32-bit indices','line_number':1116,'multiline':False]['text':' Windows compilers don't support nested macros','line_number':1137,'multiline':False]['text':' so we need this lambda outside of the','line_number':1138,'multiline':False]['text':' AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES','line_number':1139,'multiline':False]['text':' For some reason POINTER_MODE_HOST is not working here','line_number':1146,'multiline':False]['text':' Let's extract manually the nnz from the C_crow_indices','line_number':1147,'multiline':False]['text':' output','line_number':1191,'multiline':False]['text':' Resize result using nnz information from cusparse','line_number':1217,'multiline':False]['text':'
  Solves a system of linear equations whose coefficients are represented in a sparse triangular matrix A:
  op(A) X = B.

  Args:
  * `A` - sparse Tensor of size m × m.
  * `B` - dense Tensor of size m × nrhs.
  * `X` - dense Tensor of size m × nrhs.
  * `upper` - controls whether upper or lower triangular part of A is considered in computations.
  * `transpose` - if true then op(A) = A^T.
  * `unitriangular` - if true then the diagonal elements of A are assumed to be one.
','line_number':1257,'multiline':True]['text':' If A has no nnz, then A is singular and we can't solve.','line_number':1277,'multiline':False]['text':' It should be possible to use mixed memory format','line_number':1296,'multiline':False]['text':' but there is a bug in CUDA 11.3.1 version:','line_number':1297,'multiline':False]['text':' strides of matrix B are used to write result to matrix X.','line_number':1298,'multiline':False]['text':' As a workaround we need to convert matrices to have the same strides.','line_number':1299,'multiline':False]['text':' TODO: update this to support COO sparse layout','line_number':1302,'multiline':False]['text':' output','line_number':1330,'multiline':False]['text':' output','line_number':1389,'multiline':False]['text':' !AT_USE_CUSPARSE_GENERIC_SPSM()','line_number':1420,'multiline':False]['text':' !AT_USE_CUSPARSE_GENERIC_SPSV()','line_number':1425,'multiline':False]['text':' CUDA 11.6 doesn't support batched inputs, it raises an error:','line_number':1458,'multiline':False]['text':' ** On entry to cusparseSDDMM_bufferSize(): batched SDDMM is not supported','line_number':1459,'multiline':False]['text':' So we need to resort to the for loop','line_number':1460,'multiline':False]['text':'batch_offset=','line_number':1462,'multiline':True]['text':'batch_offset=','line_number':1463,'multiline':True]['text':'batch_offset=','line_number':1464,'multiline':True]['text':' output','line_number':1482,'multiline':False]['text':' namespace cuda','line_number':1518,'multiline':False]['text':' namespace impl','line_number':1519,'multiline':False]['text':' namespace sparse','line_number':1520,'multiline':False]['text':' namespace native','line_number':1521,'multiline':False]['text':' namespace at','line_number':1522,'multiline':False]