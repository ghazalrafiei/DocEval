['text':' Columns are just a view on the input for this special case.','line_number':67,'multiline':False]['text':' Allow for empty batch size but not other dimensions','line_number':164,'multiline':False]['text':' to support grouped conv we need to check if input.size(dim_planes)','line_number':230,'multiline':False]['text':' is multiple of weight.size(dim_planes)','line_number':231,'multiline':False]['text':' Compute out = weight * input','line_number':290,'multiline':False]['text':' Note gemm expects fortran order, so all 3 matrices are transposed.','line_number':291,'multiline':False]['text':' Swapping argument order cancels this, since C == AB <=> T(C) == T(B)T(A)','line_number':292,'multiline':False]['text':' Compute fgrad_input = weight.T * grad_output.reshape({grad_output.shape(0), -1})','line_number':328,'multiline':False]['text':' Note gemm expects fortran order, so all 3 matrices are transposed.','line_number':329,'multiline':False]['text':' Swapping argument order cancels this, since C == AB <=> T(C) == T(B)T(A)','line_number':330,'multiline':False]['text':' Compute grad_weight += grad_output.reshape({grad_output.shape(0), -1}) * finput.T','line_number':471,'multiline':False]['text':' Note gemm expects fortran order, so all 3 matrices are transposed.','line_number':472,'multiline':False]['text':' Swapping argument order cancels this, since C == AB <=> T(C) == T(B)T(A)','line_number':473,'multiline':False]['text':' namespace','line_number':553,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':561,'multiline':False]['text':' TODO: hacky way of deciding the groups','line_number':575,'multiline':False]['text':' Assuming the group size is checked in upstream functions','line_number':576,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':675,'multiline':False]['text':' TODO: hacky way of determine the group size','line_number':700,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':779,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':799,'multiline':False]['text':' namespace native','line_number':806,'multiline':False]['text':' namespace at','line_number':807,'multiline':False]