['text':' for the definition of AT_CUDNN_ENABLED','line_number':2,'multiline':False]['text':' Note [behavior of cudnnFind and cudnnGet]','line_number':40,'multiline':False]['text':' You'll notice that by default, in the ConvolutionDescriptor, we do the following:','line_number':41,'multiline':False]['text':'','line_number':42,'multiline':False]['text':'     AT_CUDNN_CHECK(cudnnSetConvolutionMathType(mut_desc(), CUDNN_DEFAULT_MATH));','line_number':43,'multiline':False]['text':'     if(dataType == CUDNN_DATA_HALF)','line_number':44,'multiline':False]['text':'       AT_CUDNN_CHECK(cudnnSetConvolutionMathType(mut_desc(), CUDNN_TENSOR_OP_MATH));','line_number':45,'multiline':False]['text':'','line_number':46,'multiline':False]['text':'     Update: AT_CUDNN_CHECK is updated with AT_CUDNN_CHECK_WITH_SHAPES, which','line_number':47,'multiline':False]['text':'        automatically prints tensor shapes and convolution parameters if there is','line_number':48,'multiline':False]['text':'        a cuDNN exception thrown.','line_number':49,'multiline':False]['text':'','line_number':50,'multiline':False]['text':' When cudnnSetConvolutionMathType is called before cudnnGet/cudnnFind, it informs','line_number':51,'multiline':False]['text':' cudnnGet/cudnnFind to iterate/take into account both tensor core and non-tensor-core algos.','line_number':52,'multiline':False]['text':' If you don't call cudnnSetConvolutionMathType before calling cudnnGet/cudnnFind,','line_number':53,'multiline':False]['text':' cudnnGet/cudnnFind may not pick tensor core algos.','line_number':54,'multiline':False]['text':'','line_number':55,'multiline':False]['text':' Now after its run, cudnnGet/cudnnFind comes up with the best pair of algo+mathType','line_number':56,'multiline':False]['text':' with all the initial knowledge its given. It then becomes the user's responsibility','line_number':57,'multiline':False]['text':' to update mathType of the convolution descriptor and call the subsequent cudnn calls with','line_number':58,'multiline':False]['text':' the best algo and the updated descriptor. If we don't update the descriptor but just run','line_number':59,'multiline':False]['text':' with the best algo, under the hood, cudnn will run with the slower kernel','line_number':60,'multiline':False]['text':' since it sees fastest algorithm combination with a sub optimal mathType.','line_number':61,'multiline':False]['text':' Note [blocklist fft algorithms for strided dgrad]','line_number':63,'multiline':False]['text':' This is a workaround for a CuDNN bug that gave wrong results in certain strided convolution','line_number':64,'multiline':False]['text':' gradient setups. Check Issue #16610 for bug details. Bug is there for CUDNN version < 7.5 .','line_number':65,'multiline':False]['text':' Convenience struct for passing around descriptors and data','line_number':73,'multiline':False]['text':' pointers','line_number':74,'multiline':False]['text':' already has a trailing newline','line_number':88,'multiline':False]['text':' already has a trailing newline','line_number':89,'multiline':False]['text':' already has a trailing newline','line_number':90,'multiline':False]['text':' already has a trailing newline','line_number':91,'multiline':False]['text':' already has a trailing newline','line_number':92,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':101,'multiline':False]['text':'','line_number':102,'multiline':False]['text':' Benchmarking','line_number':103,'multiline':False]['text':'','line_number':104,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':105,'multiline':False]['text':' TODO: Use something less heavy duty than a big honking mutex','line_number':107,'multiline':False]['text':' TODO: Stop manually allocating CUDA memory; allocate an ATen byte','line_number':133,'multiline':False]['text':' tensor instead.','line_number':134,'multiline':False]['text':' Sometimes cuDNN returns a workspace size > 2^63, this could makes the allocation of','line_number':137,'multiline':False]['text':' workspace fail with some 64bit indexing error instead of an OOM error. In such case,','line_number':138,'multiline':False]['text':' we manually fail with OOM.','line_number':139,'multiline':False]['text':' For the native allocator, retrieves the size of the largest unused block.','line_number':210,'multiline':False]['text':' For cudaMallocAsync, see c10/cuda/CUDAMallocAsync.cpp:cacheInfo for details.','line_number':211,'multiline':False]['text':' See Note [blocklist fft algorithms for strided dgrad]','line_number':228,'multiline':False]['text':' TODO: Shouldn't all returned results be successful?','line_number':242,'multiline':False]['text':' Double check documentation for cudnnFindConvolutionForwardAlgorithmEx','line_number':243,'multiline':False]['text':' See Note [blocklist fft algorithms for strided dgrad]','line_number':247,'multiline':False]['text':' Free the cached blocks in our caching allocator. They are','line_number':315,'multiline':False]['text':' needed here because the above benchmarking uses a huge amount of memory,','line_number':316,'multiline':False]['text':' e.g. a few GBs.','line_number':317,'multiline':False]['text':' Free the cached blocks in our caching allocator. They are','line_number':386,'multiline':False]['text':' needed here because the above benchmarking uses a huge amount of memory,','line_number':387,'multiline':False]['text':' e.g. a few GBs.','line_number':388,'multiline':False]['text':' NOTE: - 1 because ALGO_WINOGRAD is not implemented','line_number':427,'multiline':False]['text':' Free the cached blocks in our caching allocator. They are','line_number':459,'multiline':False]['text':' needed here because the above benchmarking uses a huge amount of memory,','line_number':460,'multiline':False]['text':' e.g. a few GBs.','line_number':461,'multiline':False]['text':' clear CUDA error','line_number':516,'multiline':False]['text':' clear CUDA error','line_number':527,'multiline':False]['text':' clear CUDA error','line_number':529,'multiline':False]['text':' Sometimes cuDNN returns a workspace size > 2^63, this could makes the allocation of','line_number':537,'multiline':False]['text':' workspace fail with some 64bit indexing error instead of an OOM error. In such case,','line_number':538,'multiline':False]['text':' we manually fail with OOM.','line_number':539,'multiline':False]['text':' NOTE [ raw_cudnn_convolution_forward_out ]','line_number':544,'multiline':False]['text':'','line_number':545,'multiline':False]['text':'    - raw_cudnn_convolution_forward_out (Tensor)','line_number':546,'multiline':False]['text':'      Functiont that handles tensors that are too large to use 32bit indexing.','line_number':547,'multiline':False]['text':'      It just split the tensor and dispatches to `raw_cudnn_convolution_forward_out_32bit`.','line_number':548,'multiline':False]['text':'','line_number':549,'multiline':False]['text':'    - raw_cudnn_convolution_forward_out_32bit (Tensor)','line_number':550,'multiline':False]['text':'      Low level function which invokes CuDNN, and takes an output','line_number':551,'multiline':False]['text':'      tensor which is directly written to (thus _out).','line_number':552,'multiline':False]['text':'','line_number':553,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':556,'multiline':False]['text':'','line_number':557,'multiline':False]['text':' Splitting to 32bit','line_number':558,'multiline':False]['text':'','line_number':559,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':560,'multiline':False]['text':' Assume the shape of the tensor is (N, C, D1, D2, ...)','line_number':573,'multiline':False]['text':' if N * C * D1 * D2 * ... <= int_max, then no need to split at all','line_number':574,'multiline':False]['text':' else, if C * D1 * D2 * ... <= int_max, then we just need to split across the N dimension','line_number':579,'multiline':False]['text':'','line_number':580,'multiline':False]['text':' Here we use a simple heuristics to determine the size of each split','line_number':581,'multiline':False]['text':' We don't max out the 2^31 address space because this number is super','line_number':582,'multiline':False]['text':' large and very likely to get an OOM.','line_number':583,'multiline':False]['text':' If control flow reaches here, this means even splitting N is not enough, then things starts to become complicated:','line_number':598,'multiline':False]['text':' For example, for conv2d, there following questions needs to be considered.','line_number':599,'multiline':False]['text':' - Is the memory layout NCHW or NHWC ?','line_number':600,'multiline':False]['text':' - If the conv is NCHW -> NC'H'W', then should we','line_number':601,'multiline':False]['text':'   - split only NC?','line_number':602,'multiline':False]['text':'   - split only N'C'?','line_number':603,'multiline':False]['text':'   - split both?','line_number':604,'multiline':False]['text':' - If the conv is NHWC, then we need to split across H, we need to be very careful about the boundary condition','line_number':605,'multiline':False]['text':'   to make sure that the boundary is handled correctly.','line_number':606,'multiline':False]['text':' - If we decide to make these splits, is the memory contiguous? Do we need to copy the memory?','line_number':607,'multiline':False]['text':' Considering the complexity of this issue, it is better not to use cuDNN for this case','line_number':608,'multiline':False]['text':' CUDNN_VERSION >= 8000','line_number':620,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':623,'multiline':False]['text':'','line_number':624,'multiline':False]['text':' Convolution forward / Transposed convolution backward','line_number':625,'multiline':False]['text':'','line_number':626,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':627,'multiline':False]['text':' TODO: when we do legacy group convolution support, we'll repeatedly','line_number':645,'multiline':False]['text':' reinitialize the workspace for each convolution we do.  This is','line_number':646,'multiline':False]['text':' wasteful; we'd rather reuse the workspace.  OTOH, legacy group','line_number':647,'multiline':False]['text':' convolution support is already pretty slow, so this might not','line_number':648,'multiline':False]['text':' matter.  (This applies to raw_cudnn_convolution_backward_input as well.)','line_number':649,'multiline':False]['text':' update convDesc mathType since cudnn 7.4+ now requires both algo + mathType to figure out','line_number':654,'multiline':False]['text':' whether to use Tensor core kernels or not','line_number':655,'multiline':False]['text':' See Note [behavior of cudnnFind and cudnnGet]','line_number':656,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':686,'multiline':False]['text':'','line_number':687,'multiline':False]['text':' Convolution backward / Transposed convolution forward','line_number':688,'multiline':False]['text':'','line_number':689,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':690,'multiline':False]['text':' update convDesc mathType since cudnn 7.4+ now requires both algo + mathType to figure out','line_number':713,'multiline':False]['text':' whether to use Tensor core kernels or not','line_number':714,'multiline':False]['text':' See Note [behavior of cudnnFind and cudnnGet]','line_number':715,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':750,'multiline':False]['text':'','line_number':751,'multiline':False]['text':' Convolution backward (weight)','line_number':752,'multiline':False]['text':'','line_number':753,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':754,'multiline':False]['text':' update convDesc mathType since cudnn 7.4+ now requires both algo + mathType to figure out','line_number':776,'multiline':False]['text':' whether to use Tensor core kernels or not','line_number':777,'multiline':False]['text':' See Note [behavior of cudnnFind and cudnnGet]','line_number':778,'multiline':False]['text':' Assume the shape of the tensor is (N, C, D1, D2, ...)','line_number':811,'multiline':False]['text':' if N * C * D1 * D2 * ... <= int_max, then no need to split at all','line_number':812,'multiline':False]['text':' else, if C * D1 * D2 * ... <= int_max, then we just need to split across the N dimension','line_number':817,'multiline':False]['text':'','line_number':818,'multiline':False]['text':' Here we use a simple heuristics to determine the size of each split','line_number':819,'multiline':False]['text':' We don't max out the 2^31 address space because this number is super','line_number':820,'multiline':False]['text':' large and very likely to get an OOM.','line_number':821,'multiline':False]['text':' If control flow reaches here, this means even splitting N is not enough, then things starts to become complicated:','line_number':842,'multiline':False]['text':' For example, for conv2d, there following questions needs to be considered.','line_number':843,'multiline':False]['text':' - Is the memory layout NCHW or NHWC ?','line_number':844,'multiline':False]['text':' - If the conv is NCHW -> NC'H'W', then should we','line_number':845,'multiline':False]['text':'   - split only NC?','line_number':846,'multiline':False]['text':'   - split only N'C'?','line_number':847,'multiline':False]['text':'   - split both?','line_number':848,'multiline':False]['text':' - If the conv is NHWC, then we need to split across H, we need to be very careful about the boundary condition','line_number':849,'multiline':False]['text':'   to make sure that the boundary is handled correctly.','line_number':850,'multiline':False]['text':' - If we decide to make these splits, is the memory contiguous? Do we need to copy the memory?','line_number':851,'multiline':False]['text':' Considering the complexity of this issue, it is better not to use cuDNN for this case','line_number':852,'multiline':False]['text':' update convDesc mathType since cudnn 7.4+ now requires both algo +','line_number':915,'multiline':False]['text':' mathType to figure out whether to use Tensor core kernels or not See','line_number':916,'multiline':False]['text':' Note [behavior of cudnnFind and cudnnGet]','line_number':917,'multiline':False]['text':' cuDNN Conv-Bias-Activation:','line_number':971,'multiline':False]['text':' y = act ( alpha1 * conv(x) + alpha2 * z + bias )','line_number':972,'multiline':False]['text':' In pytorch function `raw_cudnn_convolution_add_relu_out`: alpha1 is 1, alpha 2 is `float alpha`','line_number':973,'multiline':False]['text':' namespace at::native','line_number':981,'multiline':False]