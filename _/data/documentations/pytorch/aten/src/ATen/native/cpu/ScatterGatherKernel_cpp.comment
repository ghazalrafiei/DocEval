['text':' Implement as functors since lambdas don't get optimized.','line_number':31,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':110,'multiline':False]['text':' loop optimization in clang-7','line_number':111,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':137,'multiline':False]['text':' loop optimization in clang-7','line_number':138,'multiline':False]['text':' `dim` is traversed in the kernel,','line_number':176,'multiline':False]['text':' that is why index.stride(dim) = 0 and index.size(dim) = 1.','line_number':177,'multiline':False]['text':' Also, index.size(dim) = 1 makes sure that TensorIterator.DimCounter','line_number':178,'multiline':False]['text':' has the following form : (i_1,..., i_{dim-1}, 0, i_{dim+1},...,i_n).','line_number':179,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':186,'multiline':False]['text':'squash_dim=','line_number':187,'multiline':True]['text':' since the index dimension is squashed, need to alter the grain size according','line_number':200,'multiline':False]['text':' to keep equal granularity in parallelism.','line_number':201,'multiline':False]['text':' we change the order of TensorIterator-dim loop','line_number':214,'multiline':False]['text':' vs dim-TensorIterator loop order depending on','line_number':215,'multiline':False]['text':' whether dim is the last dimension','line_number':216,'multiline':False]['text':' dim loop is a separate code block','line_number':219,'multiline':False]['text':' for better performance','line_number':220,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':237,'multiline':False]['text':' loop optimization in clang-7','line_number':238,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':273,'multiline':False]['text':'squash_dim=','line_number':274,'multiline':True]['text':' we change the order of TensorIterator-dim loop','line_number':305,'multiline':False]['text':' vs dim-TensorIterator loop order depending on','line_number':306,'multiline':False]['text':' whether dim is the last dimension','line_number':307,'multiline':False]['text':' dim loop is a separate code block','line_number':310,'multiline':False]['text':' for better performance','line_number':311,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':332,'multiline':False]['text':' loop optimization in clang-7','line_number':333,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':369,'multiline':False]['text':'squash_dim=','line_number':370,'multiline':True]['text':' we change the order of TensorIterator-dim loop','line_number':401,'multiline':False]['text':' vs dim-TensorIterator loop order depending on','line_number':402,'multiline':False]['text':' whether dim is the last dimension','line_number':403,'multiline':False]['text':' dim loop is a separate code block','line_number':406,'multiline':False]['text':' for better performance','line_number':407,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':428,'multiline':False]['text':' loop optimization in clang-7','line_number':429,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':464,'multiline':False]['text':'squash_dim=','line_number':465,'multiline':True]['text':' we change the order of TensorIterator-dim loop','line_number':496,'multiline':False]['text':' vs dim-TensorIterator loop order depending on','line_number':497,'multiline':False]['text':' whether dim is the last dimension','line_number':498,'multiline':False]['text':' dim loop is a separate code block','line_number':501,'multiline':False]['text':' for better performance','line_number':502,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':523,'multiline':False]['text':' loop optimization in clang-7','line_number':524,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':560,'multiline':False]['text':'squash_dim=','line_number':561,'multiline':True]['text':' we change the order of TensorIterator-dim loop','line_number':592,'multiline':False]['text':' vs dim-TensorIterator loop order depending on','line_number':593,'multiline':False]['text':' whether dim is the last dimension','line_number':594,'multiline':False]['text':' dim loop is a separate code block','line_number':597,'multiline':False]['text':' for better performance','line_number':598,'multiline':False]['text':' we are not putting idx_dim in the error message because it disables','line_number':619,'multiline':False]['text':' loop optimization in clang-7','line_number':620,'multiline':False]['text':' Note [scatter reduce optimization]','line_number':664,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':665,'multiline':False]['text':'','line_number':666,'multiline':False]['text':' 1. initiative: optimize `scatter_reduce` on classic PyG use-case:','line_number':667,'multiline':False]['text':'   `scatter_reduce` is extensively used on 'message passing' when','line_number':668,'multiline':False]['text':'   aggregating info.','line_number':669,'multiline':False]['text':'','line_number':670,'multiline':False]['text':'   Typically, `self` will 2D tensor and `index` is a 1D extended/broadcasted','line_number':671,'multiline':False]['text':'   tensor, which means that the aggregation is on rowwise and we can vectorize','line_number':672,'multiline':False]['text':'   on the inner dimensions.','line_number':673,'multiline':False]['text':'','line_number':674,'multiline':False]['text':' 2. implementation: map `scatter_reduce` to `spmm` reduce','line_number':675,'multiline':False]['text':'   in the shape of `[M, N]` * `[N, K]`, where:','line_number':676,'multiline':False]['text':'','line_number':677,'multiline':False]['text':'   M: self_dim_size','line_number':678,'multiline':False]['text':'   nnz: index_dim_size','line_number':679,'multiline':False]['text':'   K: index.numel() / index_dim_size;','line_number':680,'multiline':False]['text':'','line_number':681,'multiline':False]['text':'   step 1: convert input index to CSR format (use radix_sort to','line_number':682,'multiline':False]['text':'     solve write addr conflicts on `self` tensor)','line_number':683,'multiline':False]['text':'','line_number':684,'multiline':False]['text':'   step 2: spmm reduce, parallel on M and vectorize on K','line_number':685,'multiline':False]['text':'','line_number':686,'multiline':False]['text':' in case some rows are not written into, num_nonzero_rows will be smaller than M','line_number':741,'multiline':False]['text':'is_cuda=','line_number':770,'multiline':True]['text':' TODO: do blocking on col dimension to reduce WR bandwidth','line_number':775,'multiline':False]['text':' step 1: reinit rows in `self` if needed','line_number':793,'multiline':False]['text':' step 2: reduce','line_number':796,'multiline':False]['text':' step 3: finalize','line_number':805,'multiline':False]['text':'include_self','line_number':854,'multiline':True]['text':'is_scatter_like=','line_number':877,'multiline':True]['text':' anonymous namespace','line_number':956,'multiline':False]['text':' fast paths for GNN usage','line_number':966,'multiline':False]['text':' namespace at::native','line_number':971,'multiline':False]