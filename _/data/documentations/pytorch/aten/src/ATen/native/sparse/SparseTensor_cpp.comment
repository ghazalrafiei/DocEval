['text':' Basic functions on sparse tensors','line_number':1,'multiline':False]['text':'*****************************************************************************
 * access methods
 *****************************************************************************','line_number':74,'multiline':True]['text':' Why are there so many methods to get indices and value?','line_number':99,'multiline':False]['text':' See Note [ Sparse: different methods to get indices and values ] in','line_number':100,'multiline':False]['text':' native_functions.yaml','line_number':101,'multiline':False]['text':'*****************************************************************************
 * creation methods
 * See NOTE [ Sparse: autograd and API ] for details
 *****************************************************************************','line_number':138,'multiline':True]['text':'** Helper methods **','line_number':143,'multiline':True]['text':'* Actual dispatched creation methods **','line_number':167,'multiline':True]['text':' NOTE: There is no guarantee that `indices` and `values` don't contain','line_number':196,'multiline':False]['text':' AutogradMeta. However, we want to maintain the invariant that `indices_`','line_number':197,'multiline':False]['text':' and `values_` of a sparse tensor don't contain AutogradMeta, and to achieve','line_number':198,'multiline':False]['text':' that we shallow-copy `indices` and `values` here.','line_number':199,'multiline':False]['text':'version_counter=','line_number':202,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':203,'multiline':True]['text':'version_counter=','line_number':206,'multiline':True]['text':'allow_tensor_metadata_change=','line_number':207,'multiline':True]['text':' alias_into_sparse overrides coalesced flag, so resetting the flag to','line_number':209,'multiline':False]['text':' the desired state here:','line_number':210,'multiline':False]['text':' TODO: alias_into_sparse sets the coalesce flag to','line_number':214,'multiline':False]['text':' `self._values().shape[0] < 2`. There exist methods (e.g. permute','line_number':215,'multiline':False]['text':' on COO tensors when `dims[0] != 0` holds) that force coalesced','line_number':216,'multiline':False]['text':' flag to false even when nnz is less that 2. Here we cannot','line_number':217,'multiline':False]['text':' determine if this is the intention of such methods but it is','line_number':218,'multiline':False]['text':' likely that these methods are overly restrictive when estimating','line_number':219,'multiline':False]['text':' is_coalesced state. The condition `!is_coalesced && self._nnz() <','line_number':220,'multiline':False]['text':' 2` provides a way to detect and optimize such methods with','line_number':221,'multiline':False]['text':' respect to estimating the is_coalesced state.','line_number':222,'multiline':False]['text':'* Public creation API that dispatch to methods above *','line_number':226,'multiline':True]['text':'* Empty init *','line_number':228,'multiline':True]['text':' Shape init ','line_number':243,'multiline':True]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':249,'multiline':False]['text':' Pointer-copy init ','line_number':255,'multiline':True]['text':' helper','line_number':257,'multiline':False]['text':' expand','line_number':260,'multiline':False]['text':' Mimic Numpy behavior here and treat it as a 1D tensor','line_number':262,'multiline':False]['text':' namespace','line_number':268,'multiline':False]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':276,'multiline':False]['text':' arg checking','line_number':281,'multiline':False]['text':' the following checks are redundant because they are also checked in','line_number':286,'multiline':False]['text':' SparseTensorImpl::set_indices_and_values_unsafe but we need to ensure them','line_number':287,'multiline':False]['text':' in order to infer the shape.','line_number':288,'multiline':False]['text':' If sizes are not given, it is inferred as max index of each dim.','line_number':298,'multiline':False]['text':' If the indices has elements in it, we infer the minimum sparse dimension','line_number':304,'multiline':False]['text':' sizes as the max value of each dim in indices. NB: It used to keepdim. I','line_number':305,'multiline':False]['text':' think that was wrong.','line_number':306,'multiline':False]['text':' values ','line_number':308,'multiline':True]['text':' dim ','line_number':308,'multiline':True]['text':' keepdim ','line_number':308,'multiline':True]['text':' values ','line_number':310,'multiline':True]['text':' dim ','line_number':310,'multiline':True]['text':' keepdim ','line_number':310,'multiline':True]['text':' len = max_index + 1','line_number':311,'multiline':False]['text':' If the indices doesn't have elements in it, there is not enough','line_number':330,'multiline':False]['text':' information to know what the minimum sparse dimension sizes should be,','line_number':331,'multiline':False]['text':' and in this case we set them to 0','line_number':332,'multiline':False]['text':' the following checks are redundant because they are also checked in','line_number':359,'multiline':False]['text':' SparseTensorImpl::set_indices_and_values_unsafe but we need to ensure them','line_number':360,'multiline':False]['text':' in order to infer the shape.','line_number':361,'multiline':False]['text':' Check to make sure all indices are within the boundaries of `size`','line_number':381,'multiline':False]['text':' values ','line_number':384,'multiline':True]['text':' dim ','line_number':384,'multiline':True]['text':' keepdim ','line_number':384,'multiline':True]['text':' values ','line_number':386,'multiline':True]['text':' dim ','line_number':386,'multiline':True]['text':' keepdim ','line_number':386,'multiline':True]['text':' NB: This used to sync ndim times to access each entry; now we copy','line_number':398,'multiline':False]['text':' everything to CPU first and then access it.','line_number':399,'multiline':False]['text':' NB: Got rid of the sizes == NULL case','line_number':426,'multiline':False]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':433,'multiline':False]['text':' arg checking','line_number':435,'multiline':False]['text':' NOTE: _sparse_coo_tensor_unsafe() differs from sparse_coo_tensor()','line_number':463,'multiline':False]['text':' in that we don't check whether any indices are out of boundaries of `size`, thus avoiding a','line_number':464,'multiline':False]['text':' copy from CUDA to CPU. However, this function should ONLY be used where we know that the indices','line_number':465,'multiline':False]['text':' are guaranteed to be within bounds or if the caller is going to call','line_number':466,'multiline':False]['text':' _validate_sparse_coo_tensor_args before using the tensor.','line_number':467,'multiline':False]['text':' NB: Got rid of the size == NULL case','line_number':468,'multiline':False]['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':475,'multiline':False]['text':' This guard is intentional: we don't support dynamic shapes along the','line_number':479,'multiline':False]['text':' indices dimension because that implies variable dimensionality','line_number':480,'multiline':False]['text':' NB: Deleted newWithSizeNd variants','line_number':494,'multiline':False]['text':'*****************************************************************************
 * reshaping methods
 *****************************************************************************','line_number':515,'multiline':True]['text':' namespace','line_number':544,'multiline':False]['text':' Invoked from native/Resize.cpp (no dynamic dispatch necessary)','line_number':546,'multiline':False]['text':' NB: Dropped the resizeNd variants','line_number':554,'multiline':False]['text':' TODO: Once copy_ is fully migrated to use dispatcher, handle named','line_number':560,'multiline':False]['text':' inference using dispatcher instead of doing it everywhere','line_number':561,'multiline':False]['text':' See NOTE: [ coalesce autograd ]','line_number':592,'multiline':False]['text':' NOTE: Since `coalesce` is not an in-place operation when `is_coalesced` is false,','line_number':605,'multiline':False]['text':' we should keep the original tensor intact and do coalesce on a copy of the tensor','line_number':606,'multiline':False]['text':' TODO: is there a more idiomatic way to do this?','line_number':627,'multiline':False]['text':' NB: The accessor accesses here rely on self._nnz() > 0 (tested earlier in','line_number':635,'multiline':False]['text':' this function)','line_number':636,'multiline':False]['text':' if values is an empty tensor, there are no elements to copy','line_number':655,'multiline':False]['text':' if values is an empty tensor, there are no elements to copy','line_number':670,'multiline':False]['text':' This is a helper function for operations that implement "sparse_mask"-like','line_number':698,'multiline':False]['text':' functionality, namely, projection of values of one tensor onto the other.','line_number':699,'multiline':False]['text':' These operations mostly rely on COO intersection primitives that heavily','line_number':700,'multiline':False]['text':' exploit coalesced inputs to avoid any syncs and calls to sort. The problem','line_number':701,'multiline':False]['text':' is that these primitives might project first argument onto second one or','line_number':702,'multiline':False]['text':' the other way around depending on which arguments are coalesced and which are','line_number':703,'multiline':False]['text':' larger. This function prepares inputs for `sparse_mask` such that `t` is','line_number':704,'multiline':False]['text':' projected onto `mask` by sorting `t` if uncoalesced and artifically marking it','line_number':705,'multiline':False]['text':' as coalesced all while `mask` is set to uncoalesced.','line_number':706,'multiline':False]['text':' The result of this projectionk is going to be uncoalesced, so it is up to the','line_number':707,'multiline':False]['text':' user to set the corresponding flag correctly with respect to the operations'','line_number':708,'multiline':False]['text':' semantics.','line_number':709,'multiline':False]['text':' We already assume that t.sizes() == mask.sizes()','line_number':711,'multiline':False]['text':' Probably worth having a dedicated kernel for.','line_number':742,'multiline':False]['text':' NOTE: res is not necessariy coalesced, but it is sorted.','line_number':746,'multiline':False]['text':' We mark it as "coalesced" to skip sorting in the intersection kernel.','line_number':747,'multiline':False]['text':' namespace at::native','line_number':859,'multiline':False]