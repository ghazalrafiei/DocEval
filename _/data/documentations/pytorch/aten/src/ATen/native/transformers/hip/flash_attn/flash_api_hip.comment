['text':'*****************************************************************************
 * Copyright (c) 2023, Advanced Micro Devices, Inc.
 * Copyright (c) 2022, Tri Dao.
 * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *****************************************************************************','line_number':1,'multiline':True]['text':' OORT headers','line_number':62,'multiline':False]['text':' batch_size x seqlen_q x num_heads x head_size','line_number':105,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':106,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':107,'multiline':False]['text':' batch_size x seqlen_q x num_heads x head_size','line_number':108,'multiline':False]['text':' FIXME: ROCM probably does not need this','line_number':126,'multiline':False]['text':' causal=true is the same as causal=false in this case','line_number':144,'multiline':False]['text':' Otherwise the kernel will be launched from cuda:0 device','line_number':174,'multiline':False]['text':' Cast to char to avoid compiler warning about narrowing','line_number':175,'multiline':False]['text':' We want to checkpoint and save the RNG state for backward if dropout','line_number':178,'multiline':False]['text':' We get the default generator and return the seed and offset which will','line_number':179,'multiline':False]['text':' be used in the backward function','line_number':180,'multiline':False]['text':' number of times random will be generated per thread, to offset philox counter in thc random','line_number':185,'multiline':False]['text':' state','line_number':186,'multiline':False]['text':' We use a custom RNG that increases the offset by batch_size * nheads * 32.','line_number':187,'multiline':False]['text':' See Note [Acquire lock when using random generators]','line_number':189,'multiline':False]['text':'reorder tensors and make contiguous','line_number':212,'multiline':False]['text':' aka softmax_lse','line_number':218,'multiline':False]['text':' compiled triton kernel intrinsic','line_number':226,'multiline':False]['text':' TODO: Error handling','line_number':231,'multiline':False]['text':' TODO: Ugly but works','line_number':248,'multiline':False]['text':'undo reorder tensors','line_number':296,'multiline':False]['text':' total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i','line_number':307,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':308,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':309,'multiline':False]['text':' total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':310,'multiline':False]['text':' b+1','line_number':311,'multiline':False]['text':' b+1','line_number':312,'multiline':False]['text':' b. If given, only this many elements of each batch element's keys are used.','line_number':313,'multiline':False]['text':' batch_size x seqlen_q x num_heads, x head_size_og','line_number':337,'multiline':False]['text':' batch_size x seqlen_q x num_heads x head_size','line_number':338,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':339,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':340,'multiline':False]['text':' batch_size x seqlen_q x num_heads x head_size','line_number':341,'multiline':False]['text':' b x h x seqlen_q','line_number':342,'multiline':False]['text':' batch_size x seqlen_q x num_heads x head_size','line_number':343,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':344,'multiline':False]['text':' batch_size x seqlen_k x num_heads_k x head_size','line_number':345,'multiline':False]['text':' probability to drop','line_number':346,'multiline':False]['text':' const at::Tensor& dout_padded = dout;','line_number':437,'multiline':False]['text':' bool loop = seqlen_k > blocksize_c;','line_number':439,'multiline':False]['text':' TODO: change later, for now set to true for simplicity','line_number':440,'multiline':False]['text':' Otherwise the kernel will be launched from cuda:0 device','line_number':443,'multiline':False]['text':' Cast to char to avoid compiler warning about narrowing','line_number':444,'multiline':False]['text':' dk_accum = at::empty({batch_size, num_heads_k, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));','line_number':453,'multiline':False]['text':' dv_accum = at::empty({batch_size, num_heads_k, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));','line_number':454,'multiline':False]['text':' MQA / GQA','line_number':458,'multiline':False]['text':' dropout + capture','line_number':472,'multiline':False]['text':'JCG TODO WE GO IN HERE TODO backwards','line_number':478,'multiline':False]['text':'reorder tensors and make contiguous','line_number':479,'multiline':False]['text':'reorder tensors and make contiguous','line_number':485,'multiline':False]['text':' TODO: Error handling','line_number':497,'multiline':False]['text':'undo reorder tensors for returns','line_number':606,'multiline':False]['text':' For MQA/GQA we need to sum dK and dV across the groups','line_number':611,'multiline':False]['text':' total_q x num_heads, x head_size','line_number':622,'multiline':False]['text':' total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i','line_number':623,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':624,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':625,'multiline':False]['text':' total_q x num_heads x head_size','line_number':626,'multiline':False]['text':' b x h x s   softmax logsumexp','line_number':627,'multiline':False]['text':' total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i','line_number':628,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':629,'multiline':False]['text':' total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i','line_number':630,'multiline':False]['text':' b+1','line_number':631,'multiline':False]['text':' b+1','line_number':632,'multiline':False]['text':' max sequence length to choose the kernel','line_number':634,'multiline':False]['text':' probability to drop','line_number':635,'multiline':False]['text':' namespace pytorch_fmha','line_number':649,'multiline':False]