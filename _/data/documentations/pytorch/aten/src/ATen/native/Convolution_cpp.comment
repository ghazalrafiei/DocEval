['text':' Check workload to activate fast depthwise FP16 cudnn conv kernels','line_number':94,'multiline':False]['text':' same as h','line_number':97,'multiline':False]['text':' All batch sizes and nb_channels','line_number':102,'multiline':False]['text':' large nb_channels','line_number':107,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':109,'multiline':False]['text':' batch_size specific','line_number':117,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':119,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':130,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':137,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':149,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':156,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':171,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':180,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':187,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':194,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-branch-clone,cppcoreguidelines-avoid-magic-numbers)','line_number':201,'multiline':False]['text':' simplified version for cudnn 8.2 and above','line_number':217,'multiline':False]['text':' 1D conv','line_number':220,'multiline':False]['text':' 2d conv','line_number':225,'multiline':False]['text':' only square filters','line_number':226,'multiline':False]['text':' only 1/3/5 filter','line_number':229,'multiline':False]['text':' we don't enforce square input but only check width to reduce heuristic space','line_number':231,'multiline':False]['text':' min width 7','line_number':232,'multiline':False]['text':' only 1/2 stride, use cudnn for all stride 1','line_number':234,'multiline':False]['text':' special case since bs1 show good perf in lots of cases','line_number':240,'multiline':False]['text':' Never use xnnpack for symbolic tracing','line_number':276,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':281,'multiline':False]['text':' This struct is templated so that we can run backend selection in a dynamic','line_number':282,'multiline':False]['text':' shapes context; all of the real kernel selection in eager mode runs with','line_number':283,'multiline':False]['text':' int64_t','line_number':284,'multiline':False]['text':' Currently only 3x3 depthwise convolutions on tensors of float are supported.','line_number':373,'multiline':False]['text':' empty input','line_number':399,'multiline':False]['text':' input size can not be reduced to the range of int by splitting the batch dim','line_number':403,'multiline':False]['text':' output size can not be reduced to the range of int by splitting the batch dim','line_number':408,'multiline':False]['text':' Note [Mobile check segfaults]','line_number':421,'multiline':False]['text':' cudnn and miopen are guaranteed not to be on mobile, and T102591915 / T110194934 suggest','line_number':422,'multiline':False]['text':' that maybe the compiledWithCuDNN() check sometimes segfaults (though I can't imagine how)','line_number':423,'multiline':False]['text':' bypass dilation checks for channels_last convolution','line_number':440,'multiline':False]['text':' cudnn doesn't support deterministic dilated convolution fully yet','line_number':442,'multiline':False]['text':' Use cudnn for FP16 depthwise convolutions','line_number':455,'multiline':False]['text':' always use cudnn_depthwise for channels_last format','line_number':458,'multiline':False]['text':' only for FP16','line_number':465,'multiline':False]['text':' TODO: 5-D contiguous depthwise is not supported yet, need benchmarks','line_number':468,'multiline':False]['text':' no dilation supported','line_number':469,'multiline':False]['text':' square or 1d','line_number':470,'multiline':False]['text':' min 32 channels supported)','line_number':471,'multiline':False]['text':' keep (7600 <= cudnn < 8200) code unchanged','line_number':476,'multiline':False]['text':' only for FP16','line_number':479,'multiline':False]['text':' TODO: 5-D contiguous depthwise is not supported yet, need benchmarks','line_number':482,'multiline':False]['text':' only square kernels','line_number':483,'multiline':False]['text':' min width/height 7','line_number':484,'multiline':False]['text':' no dilation supported','line_number':485,'multiline':False]['text':' equal strides','line_number':486,'multiline':False]['text':' min 32 channels supported)','line_number':488,'multiline':False]['text':' MIOpen currently does not support dilation with groups of size > 1','line_number':507,'multiline':False]['text':' input is mkldnn Tensor','line_number':524,'multiline':False]['text':' only on CPU Float Tensors','line_number':526,'multiline':False]['text':' For 1x1 filters, MKLDNN is faster than THNN when multi-threaded,','line_number':527,'multiline':False]['text':' but THNN is faster when single-threaded.','line_number':528,'multiline':False]['text':' for some case, native is faster','line_number':534,'multiline':False]['text':' only on CPU Float Tensors','line_number':544,'multiline':False]['text':' or dilation','line_number':545,'multiline':False]['text':' or transposed tensors','line_number':546,'multiline':False]['text':' must be in NCHW format','line_number':547,'multiline':False]['text':' NNPACK only supports kernels up to 16x16','line_number':549,'multiline':False]['text':' NNPACK only supports padding < kernel_size. See https://github.com/pytorch/pytorch/issues/90142.','line_number':550,'multiline':False]['text':' ensure large enough batch size to ensure perf, tuneable','line_number':552,'multiline':False]['text':' NB: for the call here, it MATTERS that we are templated. If you','line_number':562,'multiline':False]['text':' untemplate this to always use SymInt, the function','line_number':563,'multiline':False]['text':' xnnpack_use_convolution2d will always return false','line_number':564,'multiline':False]['text':' These checks need to be expanded. Currently we have very limited set of','line_number':581,'multiline':False]['text':' checks for MPS.','line_number':582,'multiline':False]['text':' We currently only have depthwise support for the case where groups ==','line_number':596,'multiline':False]['text':' nInputPlane and nInputPlane == nOutputPlane (the latter due to the lack of','line_number':597,'multiline':False]['text':' a depthwise multiplier)','line_number':598,'multiline':False]['text':' no point if there is only a single group','line_number':604,'multiline':False]['text':' output channels must be a multiple of input channels','line_number':605,'multiline':False]['text':' log new kernel size considering dilation','line_number':695,'multiline':False]['text':' If kernel size is incorrect','line_number':705,'multiline':False]['text':' transposed','line_number':719,'multiline':False]['text':'bias=','line_number':736,'multiline':True]['text':' Given an input tensor and an expected number of spatial dimensions, checks that the','line_number':739,'multiline':False]['text':' input is a valid shape and returns the batched form of the input.','line_number':740,'multiline':False]['text':'','line_number':741,'multiline':False]['text':' Args:','line_number':742,'multiline':False]['text':'     input (Tensor): Input tensor','line_number':743,'multiline':False]['text':'     num_spatial_dims (int): Number of spatial dimensions expected for the input','line_number':744,'multiline':False]['text':'     func_name (string): Function name to produce a nice error message for invalid input','line_number':745,'multiline':False]['text':'','line_number':746,'multiline':False]['text':' Returns a std::tuple containing:','line_number':747,'multiline':False]['text':'     batched_input (Tensor): Input with a batch dimension','line_number':748,'multiline':False]['text':'     is_batched (bool): Indicates whether the original input was already batched','line_number':749,'multiline':False]['text':' assume NTs are always batched','line_number':754,'multiline':False]['text':'bias=','line_number':782,'multiline':True]['text':' [NOTE] Complex Convolution','line_number':854,'multiline':False]['text':' conv(W, x, b) = conv(Wr, xr, br) - conv(Wi, xi, 0) + i(conv(Wi, xr, bi) + conv(Wr, xi, 0))','line_number':855,'multiline':False]['text':' where W, x and b are all complex inputs.','line_number':856,'multiline':False]['text':' With Gauss Trick:','line_number':857,'multiline':False]['text':' a = conv(Wr, xr, br),','line_number':858,'multiline':False]['text':' b = conv(Wi, xi, 0),','line_number':859,'multiline':False]['text':' c = conv(Wr + Wi, xr + xi, bi + br)','line_number':860,'multiline':False]['text':' conv(W, x, b) = a - b + i(c - a - b)','line_number':861,'multiline':False]['text':' See [NOTE] Complex Convolution','line_number':893,'multiline':False]['text':' namespace','line_number':911,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':916,'multiline':False]['text':'num_spatial_dims=','line_number':930,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':943,'multiline':False]['text':'num_spatial_dims=','line_number':957,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':970,'multiline':False]['text':'num_spatial_dims=','line_number':984,'multiline':True]['text':' Calculate the correct padding','line_number':1018,'multiline':False]['text':' All backends handle symmetric padding natively','line_number':1034,'multiline':False]['text':' Apply padding by the difference, leaving only a symmetric padding','line_number':1044,'multiline':False]['text':' F.pad goes from last dim to first','line_number':1046,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1064,'multiline':False]['text':'num_spatial_dims=','line_number':1084,'multiline':True]['text':'num_spatial_dims=','line_number':1100,'multiline':True]['text':'num_spatial_dims=','line_number':1116,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1129,'multiline':False]['text':'num_spatial_dims=','line_number':1135,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1150,'multiline':False]['text':'num_spatial_dims=','line_number':1156,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1171,'multiline':False]['text':'num_spatial_dims=','line_number':1177,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1193,'multiline':False]['text':' See Note [Enabling Deterministic Operations]','line_number':1198,'multiline':False]['text':' Function to select the convolution backend based on the inputs and params.','line_number':1212,'multiline':False]['text':' This overload is used within the convolution internals but not exposed to python.','line_number':1213,'multiline':False]['text':' NB: The forward pass provides a bias tensor while the backward pass provides','line_number':1214,'multiline':False]['text':' a bool indicating whether the bias is defined. This is done to save memory by','line_number':1215,'multiline':False]['text':' avoiding saving the full bias tensor for backward.','line_number':1216,'multiline':False]['text':' don't send empty inputs through backends','line_number':1226,'multiline':False]['text':' unsupported','line_number':1244,'multiline':False]['text':' Using prepacked conv is preferred, but XNNPACK is still the fastest','line_number':1266,'multiline':False]['text':' option for NHWC.','line_number':1267,'multiline':False]['text':' 3x3 depthwith convolutions implementation is inference only','line_number':1269,'multiline':False]['text':' fast path for grouped conv3d','line_number':1276,'multiline':False]['text':' backends without support for groups','line_number':1279,'multiline':False]['text':' unsupported','line_number':1286,'multiline':False]['text':' Not transposed ','line_number':1288,'multiline':True]['text':' dim == 4, non-dilated ','line_number':1292,'multiline':True]['text':' CPU implementation has specialized MM kernels
               for non-dilated case here ','line_number':1296,'multiline':True]['text':' dim == 5, CPU, non-dilated ','line_number':1303,'multiline':True]['text':' CPU implementation has specialized MM kernels
           for non-dilated case here ','line_number':1304,'multiline':True]['text':' unsupported','line_number':1308,'multiline':False]['text':' Only reach here when input is backend with out-of-source implementation.','line_number':1318,'multiline':False]['text':' Error out if no suitable backend was found.','line_number':1322,'multiline':False]['text':' Selects a backend for convolution based on the inputs and params.','line_number':1326,'multiline':False]['text':' Expand 1d -> 2d.','line_number':1353,'multiline':False]['text':' This is only done for backends that don't natively support 1d spatial input.','line_number':1354,'multiline':False]['text':' avoid accidentally going through NHWC for permuted 3d input.','line_number':1356,'multiline':False]['text':' For BC reasons, have a copy that does not require bias_opt','line_number':1369,'multiline':False]['text':' Handle empty # of channels.','line_number':1421,'multiline':False]['text':' See Note [Mobile check segfaults]','line_number':1435,'multiline':False]['text':'at::MemoryFormat::ChannelsLast3d','line_number':1449,'multiline':True]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1489,'multiline':False]['text':' Expand 1d -> 2d.','line_number':1517,'multiline':False]['text':' This is only done for backends that don't natively support 1d spatial input.','line_number':1518,'multiline':False]['text':' avoid accidentally going through NHWC for permuted 3d input.','line_number':1520,'multiline':False]['text':' Select appropriate backend to use.','line_number':1527,'multiline':False]['text':' Call the backend.','line_number':1534,'multiline':False]['text':' Use permute and clone to avoid at::_unsafe_view(weight, -1) failure for non-contiguous cases where','line_number':1567,'multiline':False]['text':' view size is not compatible with input tensor's size and stride.','line_number':1568,'multiline':False]['text':' need to ensure contiguous for non-mkldnn tensors','line_number':1607,'multiline':False]['text':' need to ensure contiguous for non-mkldnn tensors','line_number':1622,'multiline':False]['text':' Handle backends that don't natively support groups > 1.','line_number':1658,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1730,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1759,'multiline':False]['text':' TODO: hacky way of inferring the groups number for grouped Conv3D','line_number':1777,'multiline':False]['text':' See: https://github.com/pytorch/pytorch/pull/36355','line_number':1778,'multiline':False]['text':' Avoid undefined behavior when num channels == 0; params are unused for that case.','line_number':1780,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':1781,'multiline':False]['text':' Compute ggO = conv(ggI, w) + conv(i, ggW) + ggb','line_number':1787,'multiline':False]['text':' View as (1, ggb.size(0), 1, 1...)','line_number':1811,'multiline':False]['text':' Expand','line_number':1813,'multiline':False]['text':' Expand','line_number':1819,'multiline':False]['text':' Compute gW = conv(ggI, gO)','line_number':1829,'multiline':False]['text':' Modified params with correct padding','line_number':1833,'multiline':False]['text':' Disable groups as they are handled separately','line_number':1836,'multiline':False]['text':' Transpose gO and ggI to accumulate over batch','line_number':1841,'multiline':False]['text':' Compute conv','line_number':1846,'multiline':False]['text':' Compute conv','line_number':1853,'multiline':False]['text':' Compute conv','line_number':1869,'multiline':False]['text':' Transpose gW to match chan_in and chan_out','line_number':1881,'multiline':False]['text':' narrow gW to only relevant portion','line_number':1884,'multiline':False]['text':' we do it this way instead of narrowing the input itself because','line_number':1885,'multiline':False]['text':' the ConvForward kernels don't support asymmetric padding.','line_number':1886,'multiline':False]['text':' Compute gI = convT(gO, ggW) if !transposed','line_number':1898,'multiline':False]['text':'         gI = conv(gO, ggw)  if transposed','line_number':1899,'multiline':False]['text':' narrow gI to only relevant portion','line_number':1912,'multiline':False]['text':' we do it this way because negative output_padding is not supported','line_number':1913,'multiline':False]['text':' TODO: figure out if we can narrow gO and save some compute,','line_number':1914,'multiline':False]['text':' rather than narrowing the computed gI','line_number':1915,'multiline':False]['text':' calculate output_padding','line_number':1925,'multiline':False]['text':' TODO: figure out why this needs to be computed...','line_number':1926,'multiline':False]['text':' Check if whole input has been used or not','line_number':1932,'multiline':False]['text':' NB: nnpack backward does not support strided convolutions; use slow impl instead','line_number':1965,'multiline':False]['text':' Backward pass for convolution. Computes gradients for input, weight, and bias depending on the','line_number':1988,'multiline':False]['text':' output_mask setting. This function supports 1D, 2D, or 3D spatial convolution and currently requires','line_number':1989,'multiline':False]['text':' a single batch dimension to be present.','line_number':1990,'multiline':False]['text':'','line_number':1991,'multiline':False]['text':' Args:','line_number':1992,'multiline':False]['text':'   grad_output_: tensor of shape (N, C_out, L_out), (N, C_out, H_out, W_out), or (N, C_out, D_out, H_out, W_out)','line_number':1993,'multiline':False]['text':'   input_: tensor of shape (N, C_in, L_in), (N, C_in, H_in, W_in), or (N, C_in, D_in, H_in, W_in)','line_number':1994,'multiline':False]['text':'   weight_: tensor of shape (C_out, C_in // groups, *kernel_size); dimension of kernel_size must match the number','line_number':1995,'multiline':False]['text':'       of input spatial dimensions','line_number':1996,'multiline':False]['text':'   bias_sizes_opt: if specified, indicates that a bias was used in the forward pass and contains the shape','line_number':1997,'multiline':False]['text':'       of the bias. While the bias shape can be computed from other inputs, it is provided to this function for','line_number':1998,'multiline':False]['text':'       ease of use. The bias shape is (weight.shape[0]) for normal convolution and (weight.shape[1] * groups)','line_number':1999,'multiline':False]['text':'       for transposed convolution.','line_number':2000,'multiline':False]['text':'   stride: single value or an array with dimension matching the number of input spatial dimensions','line_number':2001,'multiline':False]['text':'   padding: single value or an array with dimension matching the number of input spatial dimensions','line_number':2002,'multiline':False]['text':'   dilation: single value or an array with dimension matching the number of input spatial dimensions','line_number':2003,'multiline':False]['text':'   transposed: boolean indicating whether the convolution is transposed','line_number':2004,'multiline':False]['text':'   output_padding: single value or dimension == number of input spatial dimensions; only supported when','line_number':2005,'multiline':False]['text':'       transposed is true','line_number':2006,'multiline':False]['text':'   groups: number of groups for grouped convolution','line_number':2007,'multiline':False]['text':'   output_mask: 3-dim boolean array specifying which gradients to compute in input, weight, bias order','line_number':2008,'multiline':False]['text':' Validate inputs.','line_number':2036,'multiline':False]['text':' output_padding is only supported for transposed convolutions','line_number':2042,'multiline':False]['text':' Expand 1d -> 2d.','line_number':2050,'multiline':False]['text':' This is only done for backends that don't natively support 1d spatial input.','line_number':2051,'multiline':False]['text':' avoid accidentally going through NHWC for permuted 3d input.','line_number':2053,'multiline':False]['text':' Select appropriate backend to use.','line_number':2061,'multiline':False]['text':'need_backward=','line_number':2062,'multiline':True]['text':' Call the backend.','line_number':2065,'multiline':False]['text':' Only make input contiguous when it is necessary for the backwards computation','line_number':2090,'multiline':False]['text':' Only make input contiguous when it is necessary for the backwards computation','line_number':2115,'multiline':False]['text':' Only make input contiguous when it is necessary for the backwards computation','line_number':2130,'multiline':False]['text':' mkldnn weight is not supported during training by the mkldnn backend','line_number':2160,'multiline':False]['text':' mkldnn bias is not supported during training by the mkldnn backend','line_number':2164,'multiline':False]['text':' Only reach here when input is backend with out-of-source implementation.','line_number':2217,'multiline':False]['text':' Note that no CUDA implementation of this kernel exists currently.','line_number':2223,'multiline':False]['text':' Handle backends that don't natively support groups > 1.','line_number':2229,'multiline':False]['text':' Backward is not supported for these backends.','line_number':2267,'multiline':False]['text':' Convert 2D inputs back to 1D for backends that don't natively support 1D','line_number':2276,'multiline':False]['text':' spatial inputs.','line_number':2277,'multiline':False]['text':' Calculate bias gradients outside of the backend for those that don't support it.','line_number':2290,'multiline':False]['text':' at::native','line_number':2308,'multiline':False]