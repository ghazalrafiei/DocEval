['text':'numel is total number of flattened indices, not expanded to dimensions that are not indexed.','line_number':51,'multiline':False]['text':'stride is the cumulative size of the not-indexed last dimensions','line_number':52,'multiline':False]['text':'stride_before is the stride of the dimension immediately preceding first indexed dimension','line_number':53,'multiline':False]['text':'if indexing starts from the 0th dimension, stride_before does not matter because blockIdx.z will be 0 in this case','line_number':54,'multiline':False]['text':'outer_dim is number of elements in the first unindexed dimensions','line_number':55,'multiline':False]['text':' Each warp is responsible for an input into the LookupTable.','line_number':58,'multiline':False]['text':' If the preceding input has the same destination index as this input, then the warp','line_number':59,'multiline':False]['text':' exits immediately. The warp also processes subsequent inputs with the','line_number':60,'multiline':False]['text':' same value.','line_number':61,'multiline':False]['text':'','line_number':62,'multiline':False]['text':' Input Warp','line_number':63,'multiline':False]['text':' 1     <warp 1>','line_number':64,'multiline':False]['text':' 1     <warp 1> (<warp 2> exits without doing any work)','line_number':65,'multiline':False]['text':' 5     <warp 3>','line_number':66,'multiline':False]['text':' 8     <warp 4>','line_number':67,'multiline':False]['text':' Number of values processed by each thread (grain size)','line_number':69,'multiline':False]['text':' if not accumulate, we only keep the last duplicate index so skip those before it','line_number':76,'multiline':False]['text':' Number of values processed by each thread (grain size)','line_number':131,'multiline':False]['text':' Determine the number of duplicates in advance','line_number':139,'multiline':False]['text':' Continue computing weights','line_number':145,'multiline':False]['text':' Number of values processed by each thread (grain size)','line_number':187,'multiline':False]['text':' Determine the number of duplicates in advance','line_number':197,'multiline':False]['text':' Continue computing weights','line_number':203,'multiline':False]['text':' This implementation is adopted from indexing_backward_kernel above.','line_number':231,'multiline':False]['text':' we only keep the last duplicate index so skip those before it','line_number':239,'multiline':False]['text':' we do quantization here','line_number':269,'multiline':False]['text':' suppress unused warning','line_number':296,'multiline':False]['text':' suppress unused warning','line_number':315,'multiline':False]['text':' suppress unused warning','line_number':325,'multiline':False]['text':'we don't need to check range in backward - if there were out of bounds indices forward should already have errored out','line_number':334,'multiline':False]['text':' computes the stride as if tensor were contiguous','line_number':349,'multiline':False]['text':' Compute the linear index by multiplying the indexing tensors by the','line_number':365,'multiline':False]['text':' stride and summing them. All the indexing tensors have the same shape at','line_number':366,'multiline':False]['text':' this point. We also compute the number of dimensions before and after that','line_number':367,'multiline':False]['text':' are not being index.','line_number':368,'multiline':False]['text':' Cast index to the longType matching src's device','line_number':373,'multiline':False]['text':' This allows us to support ie indexing a cuda tensor with a cpu tensor','line_number':374,'multiline':False]['text':' stride after undefined dimensions','line_number':381,'multiline':False]['text':'allow_int','line_number':396,'multiline':True]['text':' first expand BoolTensor (masks) or ByteTensor (masks) into 1 or more LongTensors','line_number':397,'multiline':False]['text':' next broadcast all index tensors together','line_number':404,'multiline':False]['text':' add missing null Tensors so that it matches self.dim()','line_number':406,'multiline':False]['text':' if the non-null indices are not all adjacent, transpose self and indices','line_number':410,'multiline':False]['text':' together so that they're adjacent at the front','line_number':411,'multiline':False]['text':' cub on CUDA <= 11.2 have a bug that for small sizes','line_number':476,'multiline':False]['text':' cub's sort can be much slower than thrust's merge sort','line_number':477,'multiline':False]['text':' this bug is fixed in CUDA 11.3','line_number':478,'multiline':False]['text':' Sort the inputs into sorted with the corresponding indices','line_number':485,'multiline':False]['text':' linearIndex can not be negative, and we take advantage of this','line_number':487,'multiline':False]['text':' fact to sort on less bits for better performance.','line_number':488,'multiline':False]['text':' This implementation is faster with high amounts of duplicates but could overflow','line_number':510,'multiline':False]['text':' if FP16 / BF16 is used','line_number':511,'multiline':False]['text':' cub on CUDA <= 11.2 have a bug that for small sizes','line_number':606,'multiline':False]['text':' cub's sort can be much slower than thrust's merge sort','line_number':607,'multiline':False]['text':' this bug is fixed in CUDA 11.3','line_number':608,'multiline':False]['text':' Sort the inputs into sorted with the corresponding indices','line_number':615,'multiline':False]['text':' linearIndex can not be negative, and we take advantage of this','line_number':617,'multiline':False]['text':' fact to sort on less bits for better performance.','line_number':618,'multiline':False]['text':'anonymous','line_number':669,'multiline':False]['text':' Check tensor dimensions for index operations, and return the slice size.','line_number':672,'multiline':False]['text':' We prefer this kernel to avoid reloading index points if the number','line_number':720,'multiline':False]['text':' of indices is a small number.','line_number':721,'multiline':False]['text':' This kernel in fact works for all choices of problem size, but if','line_number':722,'multiline':False]['text':' the number of indices chosen is large, then the','line_number':723,'multiline':False]['text':' indexFuncLargeIndex kernel is a better choice to increase','line_number':724,'multiline':False]['text':' parallelism.','line_number':725,'multiline':False]['text':' In order to avoid reloading the index that we are copying, load','line_number':738,'multiline':False]['text':' it once to handle all of the points that are being selected, so','line_number':739,'multiline':False]['text':' it can be reused as much as possible. This kernel is chosen when','line_number':740,'multiline':False]['text':' this is a good choice (small number of chosen indices), since','line_number':741,'multiline':False]['text':' re-accessing indices in addition to src elements can be slow.','line_number':742,'multiline':False]['text':' Lua indices begin at 1','line_number':744,'multiline':False]['text':' We stride over the output ignoring the indexed dimension','line_number':749,'multiline':False]['text':' (innerSize), whose offset calculation is handled differently','line_number':750,'multiline':False]['text':' We prefer this kernel to balance parallelism across index points,','line_number':769,'multiline':False]['text':' if there are a large number of indices.','line_number':770,'multiline':False]['text':' This kernel in fact works for all choices of problem size, but if','line_number':771,'multiline':False]['text':' the number of indices chosen is small, then the','line_number':772,'multiline':False]['text':' indexFuncSmallIndex kernel is a better choice to reduce memory','line_number':773,'multiline':False]['text':' accesses.','line_number':774,'multiline':False]['text':' We stride over the output including the indexed dimension','line_number':788,'multiline':False]['text':' (totalSize), and calculate the destination index point based on that','line_number':789,'multiline':False]['text':' Lua indices begin at 1','line_number':803,'multiline':False]['text':' Compare the stride between adjacent slices (sliceStride) with strides in the','line_number':821,'multiline':False]['text':' other dimensions (i.e., strides *inside* each slice).','line_number':822,'multiline':False]['text':'','line_number':823,'multiline':False]['text':' - Returns true if some dimension inside the slice has lower stride than','line_number':824,'multiline':False]['text':'   sliceStride.  The simplest example is a 2-D contiguous tensor with sliceDim','line_number':825,'multiline':False]['text':'   == 0 (that is, each slice is a row).','line_number':826,'multiline':False]['text':'','line_number':827,'multiline':False]['text':'   In this case, we choose the CUDA kernel that processes the data in','line_number':828,'multiline':False]['text':'   "index-major order".  For example, if thread count equals slice size, then','line_number':829,'multiline':False]['text':'   all threads process slice #0 in lockstep, and then slice #1, and so on.','line_number':830,'multiline':False]['text':'','line_number':831,'multiline':False]['text':' - Otherwise (i.e., sliceStride has the lowest value), this function returns','line_number':832,'multiline':False]['text':'   false.  The simplest example is a 2-D contiguous tensor with sliceDim == 1','line_number':833,'multiline':False]['text':'   (each slice is a column).','line_number':834,'multiline':False]['text':'','line_number':835,'multiline':False]['text':'   In this case, we choose the CUDA kernel that processes the data in','line_number':836,'multiline':False]['text':'   "elementInSlice-major order".  For example, each thread can process element','line_number':837,'multiline':False]['text':'   #0 of every slice, and then element #1 of every slice, and so on.','line_number':838,'multiline':False]['text':' The stride between adjacent slices (e.g., between element #0 of slice #100','line_number':843,'multiline':False]['text':' and element #0 of slice #101).','line_number':844,'multiline':False]['text':' Scalars are treated as 1-d tensor','line_number':861,'multiline':False]['text':' The `source` is partitioned into two parts:','line_number':880,'multiline':False]['text':' -the size of each slice we are indexing, which is the','line_number':881,'multiline':False]['text':' total size of the tensor ignoring dimension `dim`;','line_number':882,'multiline':False]['text':' -the number of index we are choosing, which is the total size','line_number':883,'multiline':False]['text':' of the tensor `index`.','line_number':884,'multiline':False]['text':' A reasonable choice for when to have each thread iterate over','line_number':943,'multiline':False]['text':' index to choose','line_number':944,'multiline':False]['text':' Scalars are treated as 1-d tensor','line_number':1019,'multiline':False]['text':' index_fill_ requires index to be a LongTensor','line_number':1048,'multiline':False]['text':' The `source` is partitioned into two parts:','line_number':1053,'multiline':False]['text':' -the size of each slice we are indexing, which is the','line_number':1054,'multiline':False]['text':' total size of the tensor ignoring dimension `dim`;','line_number':1055,'multiline':False]['text':' -the number of index we are choosing, which is the total size','line_number':1056,'multiline':False]['text':' of the tensor `index`.','line_number':1057,'multiline':False]['text':' A reasonable choice for when to have each thread iterate over','line_number':1116,'multiline':False]['text':' index to choose','line_number':1117,'multiline':False]['text':' We prefer this kernel to avoid reloading index points if the number','line_number':1215,'multiline':False]['text':' of indices is a small number.','line_number':1216,'multiline':False]['text':' This kernel in fact works for all choices of problem size, but if','line_number':1217,'multiline':False]['text':' the number of indices chosen is large, then the','line_number':1218,'multiline':False]['text':' indexSelectLargeIndex kernel is a better choice to increase','line_number':1219,'multiline':False]['text':' parallelism.','line_number':1220,'multiline':False]['text':' In order to avoid reloading the index that we are copying, load','line_number':1229,'multiline':False]['text':' it once to handle all of the points that are being selected, so','line_number':1230,'multiline':False]['text':' it can be reused as much as possible. This kernel is chosen when','line_number':1231,'multiline':False]['text':' this is a good choice (small number of chosen indices), since','line_number':1232,'multiline':False]['text':' re-accessing indices in addition to src elements can be slow.','line_number':1233,'multiline':False]['text':' We stride over the output ignoring the indexed dimension','line_number':1239,'multiline':False]['text':' (innerSize), whose offset calculation is handled differently','line_number':1240,'multiline':False]['text':' We prefer this kernel to balance parallelism across index points,','line_number':1257,'multiline':False]['text':' if there are a large number of indices.','line_number':1258,'multiline':False]['text':' This kernel in fact works for all choices of problem size, but if','line_number':1259,'multiline':False]['text':' the number of indices chosen is small, then the','line_number':1260,'multiline':False]['text':' indexSelectSmallIndex kernel is a better choice to reduce memory','line_number':1261,'multiline':False]['text':' accesses.','line_number':1262,'multiline':False]['text':' We stride over the output including the indexed dimension','line_number':1273,'multiline':False]['text':' (totalSize), and calculate the destination index point based on that','line_number':1274,'multiline':False]['text':' When using a 0-dim scalar tensor, we need the legacy (THC) semantics of','line_number':1306,'multiline':False]['text':' TensorInfo: Pretend that the scalar tensor is in fact a one-element vector.','line_number':1307,'multiline':False]['text':' The `self` is partitioned into two parts:','line_number':1356,'multiline':False]['text':' -the size of each slice we are indexing, which is the','line_number':1357,'multiline':False]['text':' total size of the tensor ignoring dimension `dim`;','line_number':1358,'multiline':False]['text':' -the number of indices we are choosing, which is the total size','line_number':1359,'multiline':False]['text':' of the tensor `indices`.','line_number':1360,'multiline':False]['text':' A reasonable choice for when to have each thread iterate over','line_number':1405,'multiline':False]['text':' indices to choose','line_number':1406,'multiline':False]['text':' anonymous namespace','line_number':1457,'multiline':False]['text':' anonymous namespace','line_number':1553,'multiline':False]['text':' We hit this function if either of the input tensor lives on CUDA.','line_number':1588,'multiline':False]['text':' It is ok, if `value` is `CPU` tensor but we should not allow `self` or','line_number':1589,'multiline':False]['text':' `mask` to be CPU tensor. Check for `self` and `mask` being on same device','line_number':1590,'multiline':False]['text':' exists in `masked_fill__cuda` (Scalar version).','line_number':1591,'multiline':False]['text':' ForwardIt: only legacy random access iterator is supported.','line_number':1598,'multiline':False]['text':' NOTE: std::distance(first, last) compiles but produces wrong results here,','line_number':1604,'multiline':False]['text':' so only legacy random access iterators are safe in this code.','line_number':1605,'multiline':False]['text':' avoiding std::advance(it, step),','line_number':1611,'multiline':False]['text':' although it does work unlike std::distance','line_number':1612,'multiline':False]['text':' If indexing into sparse dimensions','line_number':1644,'multiline':False]['text':' short-circuit if index is empty','line_number':1660,'multiline':False]['text':' Unavoidable sync since the shape of the result is not known in advance','line_number':1738,'multiline':False]['text':' Short-circuit if empty intersection','line_number':1740,'multiline':False]['text':' Need to have output as TensorIterator does not allow having void lambdas.','line_number':1753,'multiline':False]['text':' All iterations map to a single element in dummy_output by design,','line_number':1757,'multiline':False]['text':' hence removed output memory overlap check.','line_number':1758,'multiline':False]['text':' A dummy return scalar for a dummy output','line_number':1783,'multiline':False]['text':' If indexing into dense dimensions','line_number':1794,'multiline':False]['text':' It is sufficient to just perform `index_select` on values','line_number':1796,'multiline':False]['text':' if `dim` refers to dense dimensions.','line_number':1797,'multiline':False]['text':' at::native','line_number':1806,'multiline':False]