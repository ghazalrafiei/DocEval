['text':' TODO(NS): Investigate why FP8 conversion intrisncs end up being slower','line_number':22,'multiline':False]['text':' !defined(USE_ROCM) ','line_number':53,'multiline':True]['text':' !defined(USE_ROCM) ','line_number':91,'multiline':True]['text':' !defined(USE_ROCM) ','line_number':116,'multiline':True]['text':' device-to-device copy, does type conversion','line_number':133,'multiline':False]['text':' We can memcpy the memory if both tensors have the same type AND both','line_number':139,'multiline':False]['text':' tensors are contiguous after dimension coalescing and reordering.','line_number':140,'multiline':False]['text':' We always perform the copy on the source device, using the current stream','line_number':151,'multiline':False]['text':' on the source device, and we fully synchronize on both src and dst's','line_number':152,'multiline':False]['text':' current streams for completion of the copy. We have to explicitly do this','line_number':153,'multiline':False]['text':' for non-contig copies. This mimics the behavior of cross-device','line_number':154,'multiline':False]['text':' cudaMemcpyAsync on the default stream.','line_number':155,'multiline':False]['text':' This is a cross-device copy on the src current stream and dst current','line_number':158,'multiline':False]['text':' stream. We perform a two-way barrier between both devices' streams','line_number':159,'multiline':False]['text':' before the copy. This ensures that any write-after-write and','line_number':160,'multiline':False]['text':' write-after-read dependencies on the destination side are handled, so','line_number':161,'multiline':False]['text':' that no one is operating on the dst memory when we perform the copy.','line_number':162,'multiline':False]['text':' src waits on dst barrier (src already waits on src)','line_number':163,'multiline':False]['text':' Due to bizarre cuda driver intricacies, copies of','line_number':177,'multiline':False]['text':' cudaMallocAsynced memory between devices that aren't','line_number':178,'multiline':False]['text':' peer-to-peer-capable need "cudaMemcpyPeerAsync".','line_number':179,'multiline':False]['text':' So we let the allocator implement the correct call','line_number':180,'multiline':False]['text':' (either cudaMemcpyAsync or cudaMemcpyPeerAsync)','line_number':181,'multiline':False]['text':' dst waits on src barrier (dst already waits on dst). We cannot','line_number':204,'multiline':False]['text':' operate on dst's copy until the copy is complete.','line_number':205,'multiline':False]['text':' Still on src_device, record stream event','line_number':207,'multiline':False]['text':' We never require temporaries for copies on the same GPU.','line_number':223,'multiline':False]['text':' Contiguous same-dtype copies can always use cudaMemcpyAsync','line_number':230,'multiline':False]['text':' Copies between GPUs can use the copy kernel if P2P is supported','line_number':233,'multiline':False]['text':' The remaining cases require temporaries. For example, this includes','line_number':236,'multiline':False]['text':' non-contiguous copies between CPU and GPU.','line_number':237,'multiline':False]['text':' Enable p2p access between devices. (No-op if it involves the CPU)','line_number':255,'multiline':False]['text':' NB: this involves recursive calls to copy. Be careful that those copies','line_number':259,'multiline':False]['text':' don't require temporaries or you will cause an infinite recursion!','line_number':260,'multiline':False]['text':' If non_blocking is true - type conversions are performed on the GPU','line_number':265,'multiline':False]['text':' for CPU-GPU copies, otherwise type conversions are performed on the CPU.','line_number':266,'multiline':False]['text':' Type conversions are performed on the src device for GPU-GPU copies.','line_number':267,'multiline':False]['text':' propagate the correct conjugate bit','line_number':277,'multiline':False]['text':' perform a same-dtype copy on contiguous tensors','line_number':284,'multiline':False]['text':' if necessary, copy back into dst','line_number':289,'multiline':False]['text':' Copy on GPU (or between GPUs)','line_number':297,'multiline':False]['text':' Copy between CPU and GPU','line_number':303,'multiline':False]['text':' we use both the storage context and the tensor data pointer as the key','line_number':323,'multiline':False]['text':' for the caching host allocator. This allows us to better attribute the','line_number':324,'multiline':False]['text':' events to the original tensor allocation correctly. The cases we seek to','line_number':325,'multiline':False]['text':' handle are:','line_number':326,'multiline':False]['text':' 1: a user can pass a pinned memory tensor with an alternative','line_number':328,'multiline':False]['text':' context, for example if allocating memory directly from the pinned memory','line_number':329,'multiline':False]['text':' allocator and constructing a tensor with torch::from_blob.','line_number':330,'multiline':False]['text':' 2: a user can pass a tensor with a different base pointer to the original','line_number':332,'multiline':False]['text':' allocation (via slicing).','line_number':333,'multiline':False]['text':' TODO: warn on the return value.','line_number':339,'multiline':False]['text':' namespace at::native','line_number':356,'multiline':False]