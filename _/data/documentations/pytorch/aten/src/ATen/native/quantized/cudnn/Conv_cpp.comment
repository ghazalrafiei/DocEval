['text':' for the definition of AT_CUDNN_ENABLED','line_number':2,'multiline':False]['text':' TODO: there is a table from input dtype and weight dtype to operator qdtype,','line_number':28,'multiline':False]['text':' we can derive the operator dtype based on input dtype','line_number':29,'multiline':False]['text':' FIXME: make this thread-safe by reusing the benchmark cache in Conv_v7.cpp','line_number':43,'multiline':False]['text':' default to -1 when no bias','line_number':50,'multiline':False]['text':' anonymous namespace','line_number':55,'multiline':False]['text':' TODO: we can use cudnn_frontend::ExecutionPlanCache when it supports caching','line_number':56,'multiline':False]['text':' multiple operators','line_number':57,'multiline':False]['text':' reference: https://github.com/NVIDIA/cudnn-frontend/blob/main/samples/conv_sample.cpp#L293','line_number':58,'multiline':False]['text':'static cudnn_frontend::ExecutionPlanCache plan_cache("sample_cache");','line_number':59,'multiline':False]['text':' the parameter quantized_output is a quantized tensor','line_number':61,'multiline':False]['text':' the input bias is a 1-D tensor whose size is the same as the size of the second dimension of quantized_output.','line_number':73,'multiline':False]['text':' we need to add trailing dimensions in order to properly broadcast bias, otherwise broadcast_to will fail.','line_number':74,'multiline':False]['text':' the number of trailling dimensions is quantized_output.dim() - 2, so the new size of the broadcast_bias','line_number':75,'multiline':False]['text':' becomes quantized_output.dim() - 2 + 1. nothing needs to be done for the leading dimensions','line_number':76,'multiline':False]['text':' memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are','line_number':89,'multiline':False]['text':' used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two','line_number':90,'multiline':False]['text':' CacheKey objects have the same user defined parameters, but','line_number':91,'multiline':False]['text':' different padded values, resulting in different hash outputs.','line_number':92,'multiline':False]['text':' operator datatype needs to be int32 for int8 convolution, but we can','line_number':101,'multiline':False]['text':' set the datatype for output tensor to int32 or fp32','line_number':102,'multiline':False]['text':' conv_op computes act_fp32 * w_fp32 (matrix multiplication)','line_number':142,'multiline':False]['text':' where act_fp32 and w_fp32 are the input and weight variables, resp.','line_number':143,'multiline':False]['text':' output is a fp32 tensor','line_number':144,'multiline':False]['text':' for virtual tensors, the alignment is not used, so we can just put an arbitrary value here, e.g., key.output_alignment','line_number':147,'multiline':False]['text':' std::cout << "operator:" << conv_op.describe() << std::endl;','line_number':152,'multiline':False]['text':' we can't directly assign bias_mult_op becauase operator= is deleted for cudnn_frontend::Operation;','line_number':157,'multiline':False]['text':' alternatively, I think we can use std::unique_ptr and dynamically allocate these builder ops','line_number':158,'multiline':False]['text':' but here, we chose to do it statically. c10::optional<T>::emplace() enables this approach','line_number':159,'multiline':False]['text':' bias_mult_op computes bias_fp32 / (act_scale * w_scale) or bias_fp32 * (1 / (act_scale * w_scale))','line_number':161,'multiline':False]['text':' where bias_multiplier = (1 / (act_scale * w_scale))','line_number':162,'multiline':False]['text':' output is a fp32 tensor','line_number':163,'multiline':False]['text':' we use inplace operation here where the output is assigned to the input','line_number':164,'multiline':False]['text':' computes (act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)])','line_number':172,'multiline':False]['text':' where the 1st and 2nd summands is output of conv_op and broadcasted_bias, resp.','line_number':173,'multiline':False]['text':' output is a fp32 tensor','line_number':174,'multiline':False]['text':' we use inplace operation here where the output is assigned to the input','line_number':175,'multiline':False]['text':' for virtual tensors, the alignment is not used, so we can just put an arbitrary value here, e.g., key.output_alignment','line_number':179,'multiline':False]['text':' relu_op computes relu(act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]','line_number':185,'multiline':False]['text':' or relu(act_int8 * w_int8) if bias is not present.','line_number':186,'multiline':False]['text':' output is a fp32 tensor','line_number':187,'multiline':False]['text':' we use inplace operation here where the output is assigned to the input','line_number':191,'multiline':False]['text':' for virtual tensors, the alignment is not used, so we can just put an arbitrary value here, e.g., key.output_alignment','line_number':194,'multiline':False]['text':' relu_op computes relu(act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]) / (out_scale / (act_scale * w_scale))','line_number':200,'multiline':False]['text':' or relu(act_int8 * w_int8) / (out_scale / (act_scale * w_scale))) if bias is not present.','line_number':201,'multiline':False]['text':' output is a fp32 tensor','line_number':202,'multiline':False]['text':' std::cout << "operator:" << requant_op.describe() << std::endl;','line_number':209,'multiline':False]['text':' std::cout << "opGraph: " << opGraph.describe() << std::endl;','line_number':225,'multiline':False]['text':'','line_number':258,'multiline':False]['text':' output Tensor will be a clampped int8 Tensor','line_number':259,'multiline':False]['text':' both act and weight will be int8 Tensor','line_number':260,'multiline':False]['text':'
Numerics:
out_fp32 = conv_fp32(act_fp32, w_fp32, â€¦)
                    = act_fp32 * w_fp32 + bias_fp32
act_int8 = act_fp32 / act_scale + act_zero_point
w_int8 = w_fp32 / w_scale + w_zero_point
out_int8 = out_fp32 / out_scale + out_zero_point
out_int8 = (act_fp32 * w_fp32 + [bias_fp32]) / out_scale + out_zero_point
              = (act_int8 - act_zero_point) * act_scale * (w_int8 - w_zero_point) * w_scale / out_scale + out_zero_point + [bias_fp32 / out_scale]
             = (act_int8 * w_int8 - act_int8 * w_zero_point - act_zero_point * w_int8 + act_zero_point * w_zero_point) * act_scale * w_scale / out_scale + out_zero_point + [bias_fp32 / out_scale]
             = (if both act and weight are symmetrically quantized, int8, then act_zero_point = w_zero_point = 0)
             = (act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]) * act_scale * w_scale / out_scale
             = (act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]) / (out_scale / (act_scale * w_scale))
             = requantize((act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]), out_scale / (act_scale * w_scale))
','line_number':261,'multiline':True]['text':' output channels','line_number':286,'multiline':False]['text':' cudnn v8.4.0 expects conv2d's int8 activation tensor's input channels to be a multiple of 4. if it is not','line_number':297,'multiline':False]['text':' we need to explicitly pad it to a multiple of 4 ourselves as cudnn does not currently support padding.','line_number':298,'multiline':False]['text':' TODO: when and if cudnn enables padding in their operators, we can remove padding on our end;','line_number':299,'multiline':False]['text':' currently, limit padding support to groups=1 (ungrouped conv)','line_number':300,'multiline':False]['text':' TODO: implement this for groups > 1; should be straightforward since we're only padding a single dimension','line_number':301,'multiline':False]['text':' number of slices we need to pad','line_number':304,'multiline':False]['text':' need to return sliced tensor if output_channels was padded','line_number':310,'multiline':False]['text':' we currently use conv2d kernel for conv1d by making the input and weight tensors','line_number':355,'multiline':False]['text':' 4D rather than 3D. we add a dummy width dimension of size 1','line_number':356,'multiline':False]['text':' N, C, L -> N, C, 1, L','line_number':357,'multiline':False]['text':' N, C, 1, L -> N, C, L','line_number':364,'multiline':False]['text':' TODO: check all zero_points are zero/all tensors are symmetrically quantized','line_number':379,'multiline':False]['text':' the cpu conv1d doesn't use the quantized::conv1d*.new variant for packed weights. instead it just uses','line_number':389,'multiline':False]['text':' quantized::conv1d for packed weights (see quantized/library.cpp).','line_number':390,'multiline':False]['text':' this is inconsistent with what has been done for conv2d where new variants use packed weights, and','line_number':391,'multiline':False]['text':' old variant does not. we adopt this inconsistency for now to be consistent with QuantizedCPU's conv1d','line_number':392,'multiline':False]['text':' and will eventually deprecate the old variants','line_number':393,'multiline':False]['text':' anonyous namespace','line_number':400,'multiline':False]['text':' namespace at::native','line_number':401,'multiline':False]['text':' HAS_CUDNN_V8','line_number':404,'multiline':False]['text':' AT_CUDNN_ENABLED','line_number':405,'multiline':False]['text':' USE_CUDA','line_number':406,'multiline':False]