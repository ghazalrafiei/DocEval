['text':'
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 ','line_number':1,'multiline':True]['text':' source: https://stackoverflow.com/a/51549250','line_number':67,'multiline':False]['text':' namespace','line_number':72,'multiline':False]['text':' The datatype of Q/K/V','line_number':75,'multiline':False]['text':' Architecture we are targeting (eg `cutlass::arch::Sm80`)','line_number':77,'multiline':False]['text':' If Q/K/V are correctly aligned in memory and we can run a fast kernel','line_number':79,'multiline':False]['text':' upperbound on `max(value.shape[-1], query.shape[-1])`','line_number':83,'multiline':False]['text':' This is quite slower on V100 for some reason','line_number':85,'multiline':False]['text':' Set to false if you know at compile-time you will never need dropout','line_number':86,'multiline':False]['text':' Accumulator between 2 iterations','line_number':101,'multiline':False]['text':' Using `accum_t` improves perf on f16 at the cost of','line_number':102,'multiline':False]['text':' numerical errors','line_number':103,'multiline':False]['text':' block size of backward','line_number':112,'multiline':False]['text':' Launch bounds','line_number':126,'multiline':False]['text':' Input tensors','line_number':132,'multiline':False]['text':' [num_queries, num_heads, head_dim]','line_number':133,'multiline':False]['text':' [num_keys, num_heads, head_dim]','line_number':134,'multiline':False]['text':' [num_keys, num_heads, head_dim_value]','line_number':135,'multiline':False]['text':' [num_heads, num_queries, num_keys]','line_number':136,'multiline':False]['text':' Output tensors','line_number':144,'multiline':False]['text':' [num_queries, num_heads, head_dim_value]','line_number':145,'multiline':False]['text':' [num_queries, num_heads, head_dim_value]','line_number':146,'multiline':False]['text':' [num_heads, num_queries] - can be null','line_number':148,'multiline':False]['text':' Scale','line_number':151,'multiline':False]['text':' Dimensions/strides','line_number':154,'multiline':False]['text':' Everything below is only used in `advance_to_block`','line_number':170,'multiline':False]['text':' and shouldn't use registers','line_number':171,'multiline':False]['text':' dropout','line_number':185,'multiline':False]['text':' Moves pointers to what we should process','line_number':193,'multiline':False]['text':' Returns "false" if there is no work to do','line_number':194,'multiline':False]['text':' Advance to current batch - in case of different sequence lengths','line_number':209,'multiline':False]['text':' Advance to the current batch / head / query_start','line_number':246,'multiline':False]['text':' Accumulate directly in the destination buffer (eg for f32)','line_number':262,'multiline':False]['text':' lse[batch_id, head_id, query_start]','line_number':267,'multiline':False]['text':' Custom masking','line_number':272,'multiline':False]['text':' We use num_keys_absolute to index into the rng_state','line_number':279,'multiline':False]['text':' We need this index to match between forward and backwards','line_number':280,'multiline':False]['text':' the bottom row of the current block is query_start + kQueriesPerBlock','line_number':284,'multiline':False]['text':' the last active key is then query_start + causal_diagonal_offset +','line_number':285,'multiline':False]['text':' kQueriesPerBlock so num_keys is the min between actual num_keys and','line_number':286,'multiline':False]['text':' this to avoid extra computations','line_number':287,'multiline':False]['text':' no longer used after','line_number':294,'multiline':False]['text':' If num_queries == 1, and there is only one key head we're wasting','line_number':296,'multiline':False]['text':' 15/16th of tensor core compute In that case :','line_number':297,'multiline':False]['text':'  - we only launch kernels for head_id % kQueriesPerBlock == 0','line_number':298,'multiline':False]['text':'  - we iterate over heads instead of queries (strideM = strideH)','line_number':299,'multiline':False]['text':' unused but here for intent','line_number':307,'multiline':False]['text':' remove causal since n_query = 1','line_number':308,'multiline':False]['text':' otherwise, offset would change with head !','line_number':309,'multiline':False]['text':' Make sure the compiler knows these variables are the same on all','line_number':314,'multiline':False]['text':' the threads of the warp.','line_number':315,'multiline':False]['text':'
      In this first matmul, we compute a block of `Q @ K.T`.
      While the calculation result is still hot in registers, we update
      `mi`, `m_prime`, `s_prime` in shared-memory, and then store this value
      into a shared-memory ("AccumulatorSharedStorage") that is used later as
      operand A for the second matmul (see MM1)
    ','line_number':348,'multiline':True]['text':' ElementC','line_number':364,'multiline':False]['text':' ElementAccumulator','line_number':365,'multiline':False]['text':' ElementA,','line_number':375,'multiline':False]['text':' LayoutA,','line_number':376,'multiline':False]['text':' ElementB,','line_number':378,'multiline':False]['text':' LayoutB,','line_number':379,'multiline':False]['text':' LayoutC,','line_number':382,'multiline':False]['text':' ArchTag','line_number':384,'multiline':False]['text':' ThreadblockShape','line_number':385,'multiline':False]['text':' WarpShape','line_number':386,'multiline':False]['text':' InstructionShape','line_number':387,'multiline':False]['text':' Operator','line_number':391,'multiline':False]['text':' used for efficient load of bias tile Bij from global to shared memory','line_number':411,'multiline':False]['text':' input restriction: kv_len has to be a multiple of this value','line_number':416,'multiline':False]['text':' Epilogue to store to shared-memory in a format that we can use later for','line_number':419,'multiline':False]['text':' the second matmul','line_number':420,'multiline':False]['text':'*
      Second matmul: perform `attn @ V` where `attn` is the attention (not
      normalized) and stored in shared memory
    ','line_number':431,'multiline':True]['text':' ElementC','line_number':444,'multiline':False]['text':' ElementAccumulator','line_number':445,'multiline':False]['text':' from smem','line_number':447,'multiline':False]['text':' ElementA,','line_number':457,'multiline':False]['text':' LayoutA,','line_number':458,'multiline':False]['text':' ElementB,','line_number':460,'multiline':False]['text':' LayoutB,','line_number':461,'multiline':False]['text':' LayoutC,','line_number':464,'multiline':False]['text':' ThreadblockSwizzle - not used','line_number':472,'multiline':False]['text':' SplitKSerial','line_number':476,'multiline':False]['text':' WarpShape','line_number':481,'multiline':False]['text':' kMaxK','line_number':488,'multiline':False]['text':' kScaleOperandA','line_number':490,'multiline':False]['text':' Shared storage - depends on kernel params','line_number':513,'multiline':False]['text':' Everything here might be overwritten during MM0','line_number':525,'multiline':False]['text':' Everything here might be overwritten during MM0','line_number':547,'multiline':False]['text':' In this block, we will only ever:','line_number':622,'multiline':False]['text':' - read query[query_start:query_end, :]','line_number':623,'multiline':False]['text':' - write to output[query_start:query_end, :]','line_number':624,'multiline':False]['text':' See Note [Seed and Offset Device]','line_number':673,'multiline':False]['text':' When we are in cuda graph capture mode the seed and offset are stored','line_number':674,'multiline':False]['text':' on device We pass in int64_t* seed, and int64_t* offset to act as','line_number':675,'multiline':False]['text':' scratch space for storing the rng state during the forward pass and','line_number':676,'multiline':False]['text':' saving for backwards.','line_number':677,'multiline':False]['text':' each element of the attention matrix P with shape','line_number':682,'multiline':False]['text':' (batch_sz, n_heads, n_queries, n_keys) is associated with a single','line_number':683,'multiline':False]['text':' offset in RNG sequence. we initialize the RNG state with offset that','line_number':684,'multiline':False]['text':' starts at the beginning of a (n_queries, n_keys) matrix for this','line_number':685,'multiline':False]['text':' block's batch_id and head_id','line_number':686,'multiline':False]['text':' initializing rng state is very expensive, so we run once per kernel,','line_number':687,'multiline':False]['text':' rather than once per iteration. each iteration takes a copy of the','line_number':688,'multiline':False]['text':' initialized RNG state and offsets it as needed.','line_number':689,'multiline':False]['text':' Iterate through keys','line_number':697,'multiline':False]['text':' Need to have shared memory initialized, and `m_prime`','line_number':722,'multiline':False]['text':' updated from end of prev iter','line_number':723,'multiline':False]['text':'','line_number':724,'multiline':False]['text':' MATMUL: Q.K_t','line_number':725,'multiline':False]['text':'','line_number':726,'multiline':False]['text':' Computes the block-matrix product of:','line_number':727,'multiline':False]['text':' (a) query[query_start:query_end, :]','line_number':728,'multiline':False]['text':' with','line_number':729,'multiline':False]['text':' (b) key[iter_key_start:iter_key_start + kKeysPerBlock]','line_number':730,'multiline':False]['text':' and stores that into `shared_storage.si`','line_number':731,'multiline':False]['text':'','line_number':732,'multiline':False]['text':' Compute threadblock location','line_number':734,'multiline':False]['text':' Construct iterators to A and B operands','line_number':743,'multiline':False]['text':' Construct thread-scoped matrix multiply','line_number':763,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add','line_number':774,'multiline':False]['text':' multiply by scaling factor','line_number':789,'multiline':False]['text':' apply attention bias if applicable','line_number':795,'multiline':False]['text':' load bias tile Bij into shared memory','line_number':797,'multiline':False]['text':' attn_bias_pointer points to matrix of size (n_queries, n_keys)','line_number':800,'multiline':False]['text':' for the relevant batch_id and head_id','line_number':801,'multiline':False]['text':' Pij += Bij, Pij is in register fragment and Bij is in shared memory','line_number':812,'multiline':False]['text':' Mask out last if causal','line_number':826,'multiline':False]['text':' This is only needed if upper-right corner of current query / key block','line_number':827,'multiline':False]['text':' intersects the mask Coordinates of upper-right corner of current block','line_number':828,'multiline':False]['text':' is y=query_start x=min(iter_key_start + kKeysPerBlock, num_keys)) The','line_number':829,'multiline':False]['text':' first masked element is x = y + offset -> query_start + offset There is','line_number':830,'multiline':False]['text':' intersection (and we need to mask) if min(iter_key_start +','line_number':831,'multiline':False]['text':' kKeysPerBlock, num_keys)) >= query_start + offset','line_number':832,'multiline':False]['text':' last absolute col is (last absolute query + offset)','line_number':843,'multiline':False]['text':' last local col is (last absolute query + offset -','line_number':844,'multiline':False]['text':' iter_key_start)','line_number':845,'multiline':False]['text':' Update `mi` from accum stored in registers','line_number':857,'multiline':False]['text':' Also does accum[i] <- exp(accum[i] - mi)','line_number':858,'multiline':False]['text':' Output results to shared-memory','line_number':875,'multiline':False]['text':' apply dropout (if applicable) after we've written Pij to smem.','line_number':887,'multiline':False]['text':' dropout is applied by multiplying each element of Pij by:','line_number':888,'multiline':False]['text':' - 0 with probability dropout_p','line_number':889,'multiline':False]['text':' - 1 / (1 - dropout_p) with probability 1 - dropout_p','line_number':890,'multiline':False]['text':'','line_number':891,'multiline':False]['text':' for backward purposes we want to be able to map each element of the','line_number':892,'multiline':False]['text':' attention matrix to the same random uniform number as the one we used','line_number':893,'multiline':False]['text':' in forward, without needing to use the same iteration order or having','line_number':894,'multiline':False]['text':' to store the dropout matrix. its possible to do this in registers but','line_number':895,'multiline':False]['text':' it ends up being very slow because each thread having noncontiguous','line_number':896,'multiline':False]['text':' strips of the Pij tile means we have to skip around a lot, and also','line_number':897,'multiline':False]['text':' have to generate a single random number at a time','line_number':898,'multiline':False]['text':' each thread handles a contiguous sequence of elements from Sij, all','line_number':901,'multiline':False]['text':' coming from the same row. the reason they have to come from the same','line_number':902,'multiline':False]['text':' row is that the sampling random numbers from a contiguous random','line_number':903,'multiline':False]['text':' number sequence is much more efficient than jumping around, and the','line_number':904,'multiline':False]['text':' linear offset of each element of S (the global matrix) maps to an','line_number':905,'multiline':False]['text':' offset in a random number sequence. for S, the end of a row and the','line_number':906,'multiline':False]['text':' beginning of the next have adjacent offsets, but for Sij, this is not','line_number':907,'multiline':False]['text':' necessarily the case.','line_number':908,'multiline':False]['text':' apply dropout scaling to elements this thread is responsible for,','line_number':928,'multiline':False]['text':' in chunks of 4','line_number':929,'multiline':False]['text':' p.use_dropout should have same value kernel-wide','line_number':945,'multiline':False]['text':'','line_number':948,'multiline':False]['text':' MATMUL: Attn . V','line_number':949,'multiline':False]['text':' Run the matmul `attn @ V` for a block of attn and V.','line_number':950,'multiline':False]['text':' `attn` is read from shared memory (in `shared_storage_si`)','line_number':951,'multiline':False]['text':' `V` is read from global memory (with iterator_B)','line_number':952,'multiline':False]['text':'','line_number':953,'multiline':False]['text':' Compute threadblock-scoped matrix multiply-add and store it in accum','line_number':963,'multiline':False]['text':' (in registers)','line_number':964,'multiline':False]['text':' we share shmem between mma and epilogue','line_number':966,'multiline':False]['text':' operand A: Pij_dropped in shared memory','line_number':976,'multiline':False]['text':' operand B: shared memory staging area for Vj, which is loaded','line_number':978,'multiline':False]['text':' from global memory','line_number':979,'multiline':False]['text':' IterationsUnroll','line_number':1035,'multiline':False]['text':' Read','line_number':1036,'multiline':False]['text':' iterator','line_number':1037,'multiline':False]['text':' we modify `m_prime` after','line_number':1061,'multiline':False]['text':' output','line_number':1072,'multiline':False]['text':' source','line_number':1073,'multiline':False]['text':' accum','line_number':1075,'multiline':False]['text':' compute','line_number':1076,'multiline':False]['text':' destination','line_number':1085,'multiline':False]['text':' IterationsUnroll','line_number':1092,'multiline':False]['text':' source tile','line_number':1093,'multiline':False]['text':' 7. Calculate logsumexp','line_number':1105,'multiline':False]['text':' To make the backward easier, we pad logsumexp with `inf`','line_number':1106,'multiline':False]['text':' this avoids a few bound checks, and is not more expensive during fwd','line_number':1107,'multiline':False]['text':' log_2(e) = M_LOG2E','line_number':1111,'multiline':False]['text':' output so far','line_number':1124,'multiline':False]['text':' Iterates on the accumulator and corresponding position on result matrix

    (1) Update `mi[r]` to the max value of the row `r`
    (2) In a second iteration do the following:
        (a) accum   <- exp(accum - mi)
        (b) m_prime <- exp(m_prime - mi)
        (c) s_prime <- s_prime * m_prime + sum(accum)

    All of this is done on registers, before we store all of this
    on shared memory for the next matmul with Value.
    ','line_number':1139,'multiline':True]['text':' Convert to `accum_t` (rather than double)','line_number':1155,'multiline':False]['text':' log_2(e) = M_LOG2E','line_number':1156,'multiline':False]['text':' First update `mi` to the max per-row','line_number':1166,'multiline':False]['text':' Having 4x atomicMax seems faster than reduce within warp','line_number':1180,'multiline':False]['text':' first...','line_number':1181,'multiline':False]['text':' Make sure we all share the update values for `mi`','line_number':1186,'multiline':False]['text':' Doing this `exp` is quite expensive. Let's','line_number':1189,'multiline':False]['text':' split it across the warps','line_number':1190,'multiline':False]['text':' `false` if both are -inf','line_number':1196,'multiline':False]['text':' Only when bias is enabled, it's possible that all the first values','line_number':1202,'multiline':False]['text':' of attention are masked to `-inf`. In that case we want to avoid','line_number':1203,'multiline':False]['text':' `nan = exp2f(-inf - (-inf))` so we temporarily set `mi` to 0','line_number':1204,'multiline':False]['text':' Update output fragments','line_number':1213,'multiline':False]['text':' Update accum_m, accum_n, ...','line_number':1224,'multiline':False]['text':' NOTE: we could atomically add `total_row` to `s_prime`, but','line_number':1244,'multiline':False]['text':' it's faster (and deterministic) to avoid atomics here','line_number':1245,'multiline':False]['text':' Restore `mi`, see above when we set `restore_mi_to_minus_inf=true`','line_number':1257,'multiline':False]['text':' namespace PyTorchMemEffAttention','line_number':1294,'multiline':False]