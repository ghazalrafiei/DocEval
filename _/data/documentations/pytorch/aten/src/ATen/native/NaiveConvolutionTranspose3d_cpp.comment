['text':' number of input & output planes and kernel size is indirectly defined by','line_number':97,'multiline':False]['text':' the weight tensor','line_number':98,'multiline':False]['text':' TODO: TORCH_CHECK just have 2 args: condition and message ','line_number':100,'multiline':True]['text':' 4D or 5D (batch) tensor','line_number':179,'multiline':False]['text':' weight tensor (n_input_plane x n_output_plane x','line_number':180,'multiline':False]['text':' kernel_depth x kernel_height x kernel_width)','line_number':181,'multiline':False]['text':' Force batch','line_number':260,'multiline':False]['text':' Batch size + input planes','line_number':280,'multiline':False]['text':' Resize output','line_number':283,'multiline':False]['text':' Create temporary columns','line_number':287,'multiline':False]['text':' Define a buffer of ones, for bias accumulation','line_number':291,'multiline':False]['text':' Helpers','line_number':296,'multiline':False]['text':' For each elt in batch, do:','line_number':301,'multiline':False]['text':' Matrix mulitply per output:','line_number':303,'multiline':False]['text':' M,N,K are dims of matrix A and B','line_number':307,'multiline':False]['text':' (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)','line_number':308,'multiline':False]['text':' Do GEMM (note: this is a bit confusing because gemm assumes','line_number':314,'multiline':False]['text':' column-major matrices)','line_number':315,'multiline':False]['text':' Unpack columns back into input:','line_number':331,'multiline':False]['text':' Do Bias after:','line_number':355,'multiline':False]['text':' M,N,K are dims of matrix A and B','line_number':356,'multiline':False]['text':' (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)','line_number':357,'multiline':False]['text':' Do GEMM (note: this is a bit confusing because gemm assumes','line_number':362,'multiline':False]['text':' column-major matrices)','line_number':363,'multiline':False]['text':' Resize output','line_number':382,'multiline':False]['text':' number of input & output planes and kernel size is indirectly defined by','line_number':443,'multiline':False]['text':' the weight tensor','line_number':444,'multiline':False]['text':' Force batch','line_number':476,'multiline':False]['text':' Batch size + input planes','line_number':501,'multiline':False]['text':' Resize output','line_number':504,'multiline':False]['text':' Create temporary columns','line_number':509,'multiline':False]['text':' Helpers','line_number':520,'multiline':False]['text':' For each elt in batch, do:','line_number':525,'multiline':False]['text':' Matrix mulitply per sample:','line_number':527,'multiline':False]['text':' Extract columns:','line_number':532,'multiline':False]['text':' M,N,K are dims of matrix A and B','line_number':557,'multiline':False]['text':' (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)','line_number':558,'multiline':False]['text':' Do GEMM (note: this is a bit confusing because gemm assumes','line_number':564,'multiline':False]['text':' column-major matrices)','line_number':565,'multiline':False]['text':' Resize output','line_number':584,'multiline':False]['text':' number of input & output planes and kernel size is indirectly defined by','line_number':648,'multiline':False]['text':' the grad_weight tensor','line_number':649,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':672,'multiline':False]['text':' Force batch','line_number':694,'multiline':False]['text':' Batch size + input planes','line_number':719,'multiline':False]['text':' Create temporary columns','line_number':722,'multiline':False]['text':' Helpers','line_number':735,'multiline':False]['text':' For each elt in batch, do:','line_number':742,'multiline':False]['text':' Matrix mulitply per output:','line_number':744,'multiline':False]['text':' Do Weight:','line_number':747,'multiline':False]['text':' Matrix mulitply per output:','line_number':749,'multiline':False]['text':' Extract columns:','line_number':753,'multiline':False]['text':' M,N,K are dims of matrix A and B','line_number':778,'multiline':False]['text':' (see http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemm)','line_number':779,'multiline':False]['text':' n_input_plane','line_number':781,'multiline':False]['text':' Do GEMM (note: this is a bit confusing because gemm assumes','line_number':784,'multiline':False]['text':' column-major matrices)','line_number':785,'multiline':False]['text':' Resize','line_number':809,'multiline':False]['text':' namespace','line_number':819,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':829,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':855,'multiline':False]['text':' namespace native','line_number':1001,'multiline':False]['text':' namespace at','line_number':1002,'multiline':False]