['text':' NOTE: all the checks but the very last one are designed','line_number':32,'multiline':False]['text':' to work with vectors.','line_number':33,'multiline':False]['text':' To enable vectorization one would need to write a conversion','line_number':34,'multiline':False]['text':' Vec -> bool and make kernel launchers call into vectorized','line_number':35,'multiline':False]['text':' execution paths.','line_number':36,'multiline':False]['text':' All the invariants are described in','line_number':38,'multiline':False]['text':' https://pearu.github.io/bsr_tensor_invariants.html NOTE: in the code we also','line_number':39,'multiline':False]['text':' use `cidx/idx` to refer to `compressed_indices/plain_indices` respectively.','line_number':40,'multiline':False]['text':' Invariant 5.1','line_number':53,'multiline':False]['text':' compressed_index[..., 0] == 0.','line_number':54,'multiline':False]['text':' Invariant 5.2','line_number':67,'multiline':False]['text':' compressed_index[..., -1] == nnz.','line_number':68,'multiline':False]['text':' Invariant 5.3','line_number':81,'multiline':False]['text':' 0 <= compressed_indices[..., 1:] - compressed_indices[..., :-1] <= plain_dim.','line_number':82,'multiline':False]['text':' Invariants 5.4 and 5.5','line_number':102,'multiline':False]['text':' 0 <= plain_index < plain_dim.','line_number':103,'multiline':False]['text':' Invariant 5.6','line_number':117,'multiline':False]['text':' plain_indices[..., compressed_indices[..., i - 1]:compressed_indices[..., i]]','line_number':118,'multiline':False]['text':' for all i = 1, ..., compressed_dim','line_number':119,'multiline':False]['text':' are sorted and distinct along the last dimension values.','line_number':120,'multiline':False]['text':' Note that ptr_idx_batch = &idx[batch_idx] and is contiguous.','line_number':126,'multiline':False]['text':' For TensorIterator's output: no void lambdas.','line_number':240,'multiline':False]['text':' Catch integer overflow from large dimensions. Otherwise, the','line_number':243,'multiline':False]['text':' invariant checks may fail with bogus exceptions or succeed with','line_number':244,'multiline':False]['text':' false-positive results when int64_t typed dimensions are cast to','line_number':245,'multiline':False]['text':' index dtype that corresponds to smaller interger type such as','line_number':246,'multiline':False]['text':' int32_t.','line_number':247,'multiline':False]['text':' Invariants 5.4 and 5.5','line_number':266,'multiline':False]['text':' Invariants 5.1, 5.2, 5.3, 5.6','line_number':283,'multiline':False]['text':' Invariant 5.1','line_number':326,'multiline':False]['text':' Invariant 5.2','line_number':328,'multiline':False]['text':' Invariant 5.3','line_number':330,'multiline':False]['text':' Invariant 5.6','line_number':334,'multiline':False]['text':' NOTE: the implementation below is sync-less, but,','line_number':335,'multiline':False]['text':' unfortunately, work is not guaranteed to be well-balanced','line_number':336,'multiline':False]['text':' between different threads.','line_number':337,'multiline':False]['text':' assuming idx contiguity per batch:','line_number':339,'multiline':False]['text':' up to 7-dim batch.','line_number':369,'multiline':False]['text':' namespace','line_number':407,'multiline':False]['text':' namespace native','line_number':409,'multiline':False]['text':' namespace at','line_number':410,'multiline':False]