['text':' namespace','line_number':26,'multiline':False]['text':' Check whether input is in BFloat16/Half data type and output is in float','line_number':34,'multiline':False]['text':' data type, or input is in float data type and output is in BFloat16/Half','line_number':35,'multiline':False]['text':' data type. In addition, input and output need contiguous parts to utilize','line_number':36,'multiline':False]['text':' vectorization.','line_number':37,'multiline':False]['text':' Always at least 2d strides to support 2d for_each loops','line_number':60,'multiline':False]['text':' TODO: we don't actually need separate instantiations per dtype;','line_number':225,'multiline':False]['text':' we only need a separate instantiation per dtype size. This would','line_number':226,'multiline':False]['text':' probably save us a little bit of code size here','line_number':227,'multiline':False]['text':' TODO: not sure if optimizer is able to compile two levels of','line_number':228,'multiline':False]['text':' conditionals into a single jump table.  We should have a','line_number':229,'multiline':False]['text':' single jump table here; might be worth just writing out the','line_number':230,'multiline':False]['text':' dispatch statement by hand instead of using AT_DISPATCH','line_number':231,'multiline':False]['text':' fused a = b.neg().conj_physical()','line_number':259,'multiline':False]['text':' This case should never actually happen since currently there's no way to get a complex tensor','line_number':270,'multiline':False]['text':' with negative bit.','line_number':271,'multiline':False]['text':'non_blocking','line_number':286,'multiline':True]['text':' This inplace "copy" will perform any missing neg or conj operations','line_number':318,'multiline':False]['text':' namespace CPU_CAPABILITY','line_number':326,'multiline':False]['text':' namespace at::native','line_number':330,'multiline':False]