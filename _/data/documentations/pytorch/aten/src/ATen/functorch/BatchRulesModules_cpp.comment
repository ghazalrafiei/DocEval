['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' [batch_size, 1, 1, 1, ..., 1]','line_number':16,'multiline':False]['text':' B*, ED -> B*D','line_number':28,'multiline':False]['text':' *, BED -> *, E(BD) -> *(BD) -> *BD','line_number':32,'multiline':False]['text':'embedding_dim','line_number':34,'multiline':True]['text':' B*, BED -> B*, (BE)D -> B*D','line_number':40,'multiline':False]['text':' We'll need to do something extra: add (0, E, 2*E, ...) to the indices.','line_number':41,'multiline':False]['text':' Fill in the padding. We can't do it in the embedding_dense_backward call','line_number':76,'multiline':False]['text':' because we need to fill in multiple rows!','line_number':77,'multiline':False]['text':'*
 * grid sample batch rule breaks down into 3 cases:
 *   case 1 (input is batched, grid is not):
 *     batch input along first dimension, unpack along first dimension
 *     2d:
 *       input: N(BC)H_{in}W_{in}, grid: NH_{out}W_{out}2
 *       output: N(BC)H_{out}W_{out}
 *     3d:
 *       input: N(BC)D_{in}H_{in}W_{in}, grid: ND_{out}H_{out}W_{out}3
 *       output: N(BC)D_{out}H_{out}W_{out}
 *   case 2 (input is not batched, grid is batched):
 *     batch grid along second dimension, unpack along second dimension
 *     2d:
 *       input: NCH_{in}W_{in}, grid: N(BH_{out})W_{out}2
 *       output: NC(BH_{out})W_{out}
 *     3d:
 *       input: NCD_{in}H_{in}W_{in}, grid: N(BD_{out})H_{out}W_{out}3
 *       output: NC(BD_{out})H_{out}W_{out}
 *   case 3 (input and grid are both batched):
 *     batch grid and input along 0th dimension, unpack along 0th dimension
 *     2d:
 *       input: (BN)CH_{in}W_{in}, grid: (BN)H_{out}W_{out}2
 *       output: (BN)CH_{out}W_{out}
 *     3d:
 *       input: (BN)CD_{in}H_{in}W_{in}, grid: (BN)D_{out}H_{out}W_{out}3
 *       output: (BN)CD_{out}H_{out}W_{out}
 ','line_number':84,'multiline':True]['text':' grid of N(BH)W2 -> NC(BH)W or grid of N(BD)HBW3 -> NC(BD)HW','line_number':121,'multiline':False]['text':' TODO: replace with targetable functionalization','line_number':218,'multiline':False]['text':' empty tensor could be converted to one hot representation,','line_number':223,'multiline':False]['text':' but shape inference is not possible.','line_number':224,'multiline':False]['text':' Disabling all of the following checks. This is OK because scatter has checks too.','line_number':237,'multiline':False]['text':' Maybe one_hot should be a primitive wrt autograd so we don't have to deal with this.','line_number':238,'multiline':False]['text':' // non-empty tensor','line_number':239,'multiline':False]['text':' if (self.device().type() != at::kCUDA) {','line_number':240,'multiline':False]['text':'   //for cuda, rely on device assert thrown by scatter','line_number':241,'multiline':False]['text':'   TORCH_CHECK(self.min().item().toLong() >= 0, "Class values must be non-negative.");','line_number':242,'multiline':False]['text':' }','line_number':243,'multiline':False]['text':' if (self.device().type() != at::kCUDA) {','line_number':244,'multiline':False]['text':'   //rely on device asserts from scatter to avoid sync here','line_number':245,'multiline':False]['text':'   TORCH_CHECK(num_classes > self.max().item().toLong(), "Class values must be smaller than num_classes.");','line_number':246,'multiline':False]['text':' }','line_number':247,'multiline':False]['text':' input_size is wrong so we correct it','line_number':266,'multiline':False]