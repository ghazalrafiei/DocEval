['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' not taken care of with binary batch rule, which assumes at least one input is batched','line_number':78,'multiline':False]['text':' avoids unnecessary checks and batch rule assuming output is batched','line_number':83,'multiline':False]['text':' compute max logical rank','line_number':108,'multiline':False]['text':' If the dimensions aren't aligned, we need to line them up.','line_number':116,'multiline':False]['text':' Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]','line_number':117,'multiline':False]['text':' Note that only tensors that have a batch dim need to be modified.','line_number':118,'multiline':False]['text':' Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed','line_number':119,'multiline':False]['text':' compute max logical rank','line_number':130,'multiline':False]['text':' If the dimensions aren't aligned, we need to line them up.','line_number':138,'multiline':False]['text':' Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]','line_number':139,'multiline':False]['text':' Note that only tensors that have a batch dim need to be modified.','line_number':140,'multiline':False]['text':' Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed','line_number':141,'multiline':False]['text':' repeat the preprocessing from _binary_pointwise_batch_rule','line_number':171,'multiline':False]['text':' gelu_backward doesn't broadcast well so we need to insist all inputs have a bdim','line_number':176,'multiline':False]['text':' masked_select returns a 1D tensor, so we have to reshape it into 2D','line_number':198,'multiline':False]['text':' We need to make sure that x1 has batch dim if cdist has one','line_number':237,'multiline':False]['text':' otherwise, we get','line_number':238,'multiline':False]['text':' RuntimeError: Function CdistBackward0 returned an invalid gradient at index 1 - got [5]','line_number':239,'multiline':False]['text':' but expected shape compatible with [4, 5]','line_number':240,'multiline':False]['text':' We need to apply the same preprocessing on x1 and x2 as in the forward pass','line_number':247,'multiline':False]['text':' _binary_pointwise_batch_rule','line_number':248,'multiline':False]['text':' We need to make sure that grad has batch dim if x1 or x2 have one','line_number':255,'multiline':False]['text':' Probably, there is an assumption on the strides.','line_number':256,'multiline':False]['text':' Otherwise grad input contains thrash values, e.g. -7.0816e+29, 7.0816e+29','line_number':257,'multiline':False]['text':' Optimization: fill_ is faster than the other path which does','line_number':279,'multiline':False]['text':' reshaping + copy_','line_number':280,'multiline':False]['text':'do_type_promotion','line_number':288,'multiline':True]['text':' NB: This emulates handle_pointwise_ops except we ignore the last argument, buffer','line_number':296,'multiline':False]['text':' when any of the inputs are on cuda.','line_number':297,'multiline':False]['text':' We do this because on cuda, buffer is a dummy tensor always of logical rank 1 and','line_number':298,'multiline':False]['text':' it becomes an issue when the rest of the inputs are scalar','line_number':299,'multiline':False]['text':' Bug in PyTorch, prob shouldn't need to be contiguous','line_number':311,'multiline':False]['text':' For all 3 combinations of Tensor x Tensor, Tensor x Scalar, Scalar x Tensor','line_number':340,'multiline':False]['text':' Batching rule registrations start','line_number':351,'multiline':False]['text':' Implementation note: _binary_pointwise_helper performs a dtype promotion if args are scalars,','line_number':393,'multiline':False]['text':' but cdist can't work with scalars, at least 2d tensors.','line_number':394,'multiline':False]['text':' just testing','line_number':468,'multiline':False]