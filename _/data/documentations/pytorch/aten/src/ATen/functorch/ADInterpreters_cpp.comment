['text':' TensorWrapper creation may call dispatcher ops (e.g. aten.sym_storage_offset).','line_number':35,'multiline':False]['text':' We need to ensure that they pass through the functorch stack properly.','line_number':36,'multiline':False]['text':' In order to do that, we want to call those dispatcher ops at the next layer,','line_number':37,'multiline':False]['text':' hence we disable DynamicLayerFrontMode so the call to the op automatically','line_number':38,'multiline':False]['text':' goes to DynamicLayerBackMode which will then send it to the next layer.','line_number':39,'multiline':False]['text':'is_immutable=','line_number':43,'multiline':True]['text':'is_immutable=','line_number':50,'multiline':True]['text':' if is a grad transform, and the operation is in-place, and the mutated','line_number':66,'multiline':False]['text':' argument is not currently wrapped in a TensorWrapper, then we need to','line_number':67,'multiline':False]['text':' error out otherwise the result is silently incorrect','line_number':68,'multiline':False]['text':' materialize live GradWrappers','line_number':71,'multiline':False]['text':' if (c10::show_dispatch_trace_enabled()) {','line_number':116,'multiline':False]['text':'   std::cout << "wrap " << current_level << std::endl;','line_number':117,'multiline':False]['text':' }','line_number':118,'multiline':False]['text':' TODO: we only need to do the following (marked with !) on in-place functions','line_number':122,'multiline':False]['text':' that modify sizes or strides. There aren't many of them.','line_number':123,'multiline':False]['text':' If autograd dispatch key:','line_number':124,'multiline':False]['text':' 1. (!) Put a copy of all of the args onto the stack','line_number':125,'multiline':False]['text':' 2. Unwrap all the args in the copy set','line_number':126,'multiline':False]['text':' 3. Call the operator','line_number':127,'multiline':False]['text':' 4. Wrap the output','line_number':128,'multiline':False]['text':' 5. (!) refreshMetadata for all the args in the original set','line_number':129,'multiline':False]['text':' 6. (!) Pop those args off.','line_number':130,'multiline':False]['text':' Step 1 & 2','line_number':132,'multiline':False]['text':' Step 1','line_number':135,'multiline':False]['text':' set = 1 for all bits','line_number':141,'multiline':False]['text':' only input that can be aliased is a tensor, not a tensor list (expect in ops without returns)','line_number':146,'multiline':False]['text':' if the input is immutable, we find if it aliases anything, noting that','line_number':151,'multiline':False]['text':' args are in reverse order on stack, so the last arg is at the top of the stack','line_number':152,'multiline':False]['text':' each output aliases at most one input, so we can only hit this once','line_number':156,'multiline':False]['text':' Step 2','line_number':162,'multiline':False]['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':165,'multiline':False]['text':' Re-dispatch','line_number':176,'multiline':False]['text':' Step 4, 5, 6','line_number':181,'multiline':False]['text':' Step 4','line_number':185,'multiline':False]['text':' Step 5','line_number':188,'multiline':False]['text':' Step 6','line_number':202,'multiline':False]['text':' namespace at::functorch','line_number':242,'multiline':False]