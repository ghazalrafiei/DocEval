['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' This files contains the legacy (now-deprecated) batching rule API.','line_number':14,'multiline':False]['text':' Please try to use the new-style batching rule API (see writing_batch_rules.md)','line_number':15,'multiline':False]['text':' This file contains abstractions used for transforming *logical* vmap arguments','line_number':17,'multiline':False]['text':' into *physical* arguments. (Keep reading for definitions of these terms).','line_number':18,'multiline':False]['text':' NOTE: [Logical vs physical args]','line_number':20,'multiline':False]['text':' Consider the following vmap.','line_number':21,'multiline':False]['text':'   vmap(vmap(func, in_dims=(2,)), in_dims=(0,))(torch.ones(2, 3, 4))','line_number':22,'multiline':False]['text':' This would produce a BatchedTensor wrapping a Tensor of size [2, 3, 4],','line_number':23,'multiline':False]['text':' with batch dims 0 and 2:','line_number':24,'multiline':False]['text':'   BatchedTensor(ones(2, 3, 4), bdims=[(lvl=1,dim=0),(lvl=2,dim=2)])','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' We say the *logical* view of the tensor has size [3] -- tensors inside','line_number':27,'multiline':False]['text':' `func` appear to have size [3].','line_number':28,'multiline':False]['text':' However, the *physical* underlying tensor (the one passed to vmap) has size','line_number':29,'multiline':False]['text':' [2, 3, 4].','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' This notion of logical vs physical also extends to non-tensor arguments.','line_number':32,'multiline':False]['text':' Consider the previous tensor; let's assume the user called','line_number':33,'multiline':False]['text':' `torch.sum(tensor, dim=0)` inside of `func`. Then the logical','line_number':34,'multiline':False]['text':' dimension they are reducing over is dim 0 but the physical dim is dim 1','line_number':35,'multiline':False]['text':' (the first non-batch dimension)','line_number':36,'multiline':False]['text':' Forward declared; see NOTE: [What is a VmapPhysicalView?]','line_number':38,'multiline':False]['text':' Most PyTorch operators take 4 or fewer inputs.','line_number':41,'multiline':False]['text':' Pytorch generally advertises good performance for <= 5 dims.','line_number':45,'multiline':False]['text':' (see ATen/core/DimVector.h). We add a few extra dims (~3) for vmap','line_number':46,'multiline':False]['text':' dimensions to get 8. Adjust this number as necessary','line_number':47,'multiline':False]['text':' NOTE: [What is an VmapTransform?]','line_number':52,'multiline':False]['text':' An *VmapTransform* converts logical views of tensors to physical views.','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' Batching rules use VmapTransforms to convert logical arguments to','line_number':55,'multiline':False]['text':' physical arguments, then call one or more at:: operator that handles the','line_number':56,'multiline':False]['text':' physical arguments, and then converts the physical result back to a logical','line_number':57,'multiline':False]['text':' argument.','line_number':58,'multiline':False]['text':' VmapTransform for operators that take tensors with multiple batch dims.','line_number':60,'multiline':False]['text':' Given one or more logical views on Tensors, `logicalToPhysical`','line_number':61,'multiline':False]['text':' permutes all of the batch dims to the front of the tensor, aligns','line_number':62,'multiline':False]['text':' and expands the batch dims to match each other (according to their `level`),','line_number':63,'multiline':False]['text':' and returns a VmapPhysicalView on the tensor(s).','line_number':64,'multiline':False]['text':' VmapTransform for operators that broadcast all inputs.','line_number':70,'multiline':False]['text':' Given some logical views on Tensors, `logicalToPhysical`:','line_number':71,'multiline':False]['text':' - permutes all of the batch dims to the front of the tensors','line_number':72,'multiline':False]['text':' - aligns all the batch dims to the collective levels of all of the tensors.','line_number':73,'multiline':False]['text':'   If a tensor does not have a batch dim for a vmap level, then it receives','line_number':74,'multiline':False]['text':'   a size-one dimension for said level.','line_number':75,'multiline':False]['text':' - aligns the non-batch dims to have the same dimensionality, adding extra','line_number':76,'multiline':False]['text':'   size-1 dimensions in between the batch dimensions and the non-batch dimensions','line_number':77,'multiline':False]['text':'   so that the batch dimensions are lined up from the right.','line_number':78,'multiline':False]['text':'','line_number':79,'multiline':False]['text':' For example: given inputs of size (B, 2) and (B, 3, 2) where B is the batch','line_number':80,'multiline':False]['text':' dimension, BroadcastingVmapTransform returns VmapPhysicalViews that wrap tensors','line_number':81,'multiline':False]['text':' of size (B, 1, 2) and (B, 3, 2).','line_number':82,'multiline':False]['text':'','line_number':83,'multiline':False]['text':' Given inputs of size (B, 2) and (2,), BroadcastingVmapTransform returns','line_number':84,'multiline':False]['text':' VmapPhysicalViews wrapping tensors of size (B, 2) and (1, 2). We don't','line_number':85,'multiline':False]['text':' actually *need* to return a tensor of size (1, 2) for the second tensor','line_number':86,'multiline':False]['text':' because the broadcasting operation takes care of that for us, but we do','line_number':87,'multiline':False]['text':' it anyways to keep things simple.','line_number':88,'multiline':False]['text':' Forward declared, if you're reading this file head to toe, don't worry about','line_number':93,'multiline':False]['text':' it yet.','line_number':94,'multiline':False]['text':' NOTE: [What is a VmapPhysicalView?]','line_number':97,'multiline':False]['text':' VmapPhysicalView represents a physical view on a Tensor.','line_number':98,'multiline':False]['text':'','line_number':99,'multiline':False]['text':' One can use it to further convert logical dimension indices, logical shapes,','line_number':100,'multiline':False]['text':' and more to their physical variants, or convert a new (physical) tensor into','line_number':101,'multiline':False]['text':' a logical BatchedTensor. (TODO(rzou): some of these are not yet implemented).','line_number':102,'multiline':False]['text':'','line_number':103,'multiline':False]['text':' VmapPhysicalView stores a physical tensor with all of its batch dimensions at','line_number':104,'multiline':False]['text':' the front and some levels that correspond to said batch dimensions.','line_number':105,'multiline':False]['text':'','line_number':106,'multiline':False]['text':' The levels bitset specifies which vmap levels correspond to the batch','line_number':107,'multiline':False]['text':' dimensions at the front of the tensor. In particular, the number of set bits','line_number':108,'multiline':False]['text':' corresponds to the number of batch dimensions on `tensor` and the rightmost','line_number':109,'multiline':False]['text':' bit of `levels` specifies the maximum number of nested vmaps we are in at','line_number':110,'multiline':False]['text':' this point in time.','line_number':111,'multiline':False]['text':' For example, given:','line_number':112,'multiline':False]['text':'   physical_view = VmapPhysicalView(tensor=ones(2, 3, 4, 5, 6), levels={1, 3})','line_number':113,'multiline':False]['text':'','line_number':114,'multiline':False]['text':' Rightmost bit of `levels` is 3 indicating the number of nested vmaps less','line_number':115,'multiline':False]['text':' than or equal to 3.','line_number':116,'multiline':False]['text':'   bitset: 010100','line_number':117,'multiline':False]['text':'              ^','line_number':118,'multiline':False]['text':'              |','line_number':119,'multiline':False]['text':'   levels: 012345','line_number':120,'multiline':False]['text':' TORCH_INTERNAL_ASSERT(!isBatchedTensor(tensor));','line_number':124,'multiline':False]['text':' Maps logical dim indices to physical dim indices. Also does dim wrapping.','line_number':130,'multiline':False]['text':'','line_number':131,'multiline':False]['text':' For example, given:','line_number':132,'multiline':False]['text':'   physical_view = VmapPhysicalView(tensor=ones(2, 3, 4, 5), levels={1, 3})','line_number':133,'multiline':False]['text':'','line_number':134,'multiline':False]['text':' Then physical_view.getPhysicalDims({0, 1}) returns {2, 3}.','line_number':135,'multiline':False]['text':' This is because the size of levels tell us that the first two dimensions','line_number':136,'multiline':False]['text':' of `tensor_` are batch dimensions, so a logical dim of `n` is actually','line_number':137,'multiline':False]['text':' a physical dim of `n + 2`.','line_number':138,'multiline':False]['text':' Returns a VmapPhysicalToLogicalMap object. This can be used for','line_number':142,'multiline':False]['text':' mapping a physical tensor to a new logical tensor (BatchedTensor)','line_number':143,'multiline':False]['text':' Maps a logical shape to a physical shape by pre-pending the batch','line_number':146,'multiline':False]['text':' sizes to the logical shape.','line_number':147,'multiline':False]['text':' Convenience struct used for mapping a physical tensor (a non-BatchedTensor)','line_number':160,'multiline':False]['text':' to a logical one (BatchedTensor). It holds some levels that are used to do the','line_number':161,'multiline':False]['text':' mapping and assumes that the batch dimensions in the physical tensor all','line_number':162,'multiline':False]['text':' occur at the front of the tensor.','line_number':163,'multiline':False]['text':' Maps a physical tensor to a new logical tensor (BatchedTensor).','line_number':167,'multiline':False]['text':' Assumes that all of the "batch dimensions" are at the front','line_number':168,'multiline':False]['text':' of the physical tensor. For example, given:','line_number':169,'multiline':False]['text':' - x = rank-4 Tensor with size 2, 3, 5, 7','line_number':170,'multiline':False]['text':' - levels = (2, 4)','line_number':171,'multiline':False]['text':' Returns:','line_number':172,'multiline':False]['text':' - BatchedTensor(x, bdims=[(dim=0,lvl=2), (dim=1, lvl=4)])','line_number':173,'multiline':False]['text':' Given a vector of physical tensors,','line_number':176,'multiline':False]['text':' 1. maps each tensor to a new logical tensor. Assumes that all of the','line_number':177,'multiline':False]['text':'    "batch dimensions" are at the front of the physical tensors.','line_number':178,'multiline':False]['text':' 2. stores the new logical tensors back into the passed-in vector. This is','line_number':179,'multiline':False]['text':'    to avoid additional dynamic allocations.','line_number':180,'multiline':False]['text':' namespace at::functorch','line_number':187,'multiline':False]