['text':' NOTE: [functorch's PyTorch Operator Hacks]','line_number':16,'multiline':False]['text':'','line_number':17,'multiline':False]['text':' This file contains hacks for composite PyTorch operators that are problematic.','line_number':18,'multiline':False]['text':' For example, the composite op might have in-place operations,','line_number':19,'multiline':False]['text':' or call data_ptr. We have some idea of how to fix these things in the long term','line_number':20,'multiline':False]['text':' e.g., upstream the changes to PyTorch.','line_number':21,'multiline':False]['text':'','line_number':22,'multiline':False]['text':' TODO: all of these should be fixed in a more blessed way. In particular,','line_number':23,'multiline':False]['text':' it is bad if any of these go out-of-sync with the implementations in','line_number':24,'multiline':False]['text':' pytorch/pytorch.','line_number':25,'multiline':False]['text':' TODO: upstream into core','line_number':27,'multiline':False]['text':' TODO: generalize this to more transforms','line_number':50,'multiline':False]['text':' If b has any wrapper that a does not, then we cannot do a.inplace_(b)','line_number':64,'multiline':False]['text':' TODO: linear is pretty important for performance, but I'm not sure how to work','line_number':74,'multiline':False]['text':' around the in-place.','line_number':75,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':77,'multiline':False]['text':' Fused op is marginally faster.','line_number':91,'multiline':False]['text':' Also hit the fused path for contiguous 3D input.','line_number':95,'multiline':False]['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':129,'multiline':False]['text':' pos_weight need to be broadcasted, thus mul(target) is not inplace.','line_number':137,'multiline':False]['text':' Workaround using index_put instead of yet unsupported index_fill_','line_number':157,'multiline':False]['text':' dropout hack','line_number':163,'multiline':False]['text':' TODO: make the following changes in pytorch/pytorch','line_number':164,'multiline':False]['text':' NB: THIS WAS CHANGED FROM THE ORIGINAL','line_number':182,'multiline':False]['text':' NB: sure, we could have used different overloads here, but I would feel insecure','line_number':190,'multiline':False]['text':' knowing that this dispatch depends only on the constness of the references','line_number':191,'multiline':False]['text':' used for alpha_dropout only','line_number':215,'multiline':False]['text':' NB: THIS WAS CHANGED FROM THE ORIGINAL','line_number':217,'multiline':False]['text':' NB: it is important that this is at::empty and not at::empty_like','line_number':223,'multiline':False]['text':' dropout_hack','line_number':296,'multiline':False]