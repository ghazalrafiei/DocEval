['text':' We always want to call the functionalization kernels if functionalize() is on the layer stack.','line_number':18,'multiline':False]['text':' It's the responsibility of the functionalization kernel to no-op and redispatch','line_number':19,'multiline':False]['text':' if none of the input tensors are functional.','line_number':20,'multiline':False]['text':' We have some side-car TLS that we can set to toggle the functionaliation behavior.','line_number':23,'multiline':False]['text':' If set, then we functionalization will only remove mutations, instead of','line_number':24,'multiline':False]['text':' removing both mutations AND view operators.','line_number':25,'multiline':False]['text':' Functorch is responsible for setting the level on the wrapper, since we don't','line_number':35,'multiline':False]['text':' have that info available in core (for now).','line_number':36,'multiline':False]['text':' We could just "propagate" the level from the input tensors inside of the functionalize kernels,','line_number':37,'multiline':False]['text':' but unfortunately we can't do that for factory operators.','line_number':38,'multiline':False]['text':' For now, we don't support nested functionalization calls.','line_number':50,'multiline':False]['text':' This check just enforces that - after the functionalize kernel runs','line_number':51,'multiline':False]['text':' and we hit the BackModeFallback, we'll have unwrapped our FunctionalTensors','line_number':52,'multiline':False]['text':' so we can check that the unwrapped thing is not another (nested) FunctionalTensor.','line_number':53,'multiline':False]['text':' Re-dispatch','line_number':57,'multiline':False]['text':' namespace at::functorch','line_number':67,'multiline':False]