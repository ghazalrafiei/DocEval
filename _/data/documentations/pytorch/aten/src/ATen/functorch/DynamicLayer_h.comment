['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' Forward declared','line_number':18,'multiline':False]['text':' This file contains the implementation of functorch's interpreter stack.','line_number':23,'multiline':False]['text':' See NOTE: [functorch interpreter stack] first before reading on.','line_number':24,'multiline':False]['text':'','line_number':25,'multiline':False]['text':' NB: the functorch interpreter stack is also referred to as:','line_number':26,'multiline':False]['text':' - the "dynamic layer stack" -- an older name for "interpreter" was','line_number':27,'multiline':False]['text':'   "dynamic layer".','line_number':28,'multiline':False]['text':' - the "functorch mode stack". You can think of each functorch transform as a','line_number':29,'multiline':False]['text':'   "mode" (in the same sense as torch_dispatch mode or torch_function mode),','line_number':30,'multiline':False]['text':'   and functorch being an implementation of a "mode stack" where the modes','line_number':31,'multiline':False]['text':'   may be arbitrary composed.','line_number':32,'multiline':False]['text':' DynamicLayer is basically the same thing as an Interpreter.','line_number':34,'multiline':False]['text':' It represents a functorch transform and it holds an Interpreter,','line_number':35,'multiline':False]['text':' which contains metadata related to the transform and instructions on','line_number':36,'multiline':False]['text':' how to perform the transform.','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' TODO: we can excise DynamicLayer in favor of Interpreter,','line_number':39,'multiline':False]['text':' But I am going to leave it for now as a compatiblity shim to avoid','line_number':40,'multiline':False]['text':' needing to refactor a lot of callsites...','line_number':41,'multiline':False]['text':' Only valid for vmap','line_number':58,'multiline':False]['text':' NOTE: [Life handles and lexically scoped transforms]','line_number':79,'multiline':False]['text':' functorch transforms are lexically scoped.','line_number':80,'multiline':False]['text':' Given a level, we store a "life handle" that is a boolean that tells us if the','line_number':81,'multiline':False]['text':' transform with that level is active or not.','line_number':82,'multiline':False]['text':'','line_number':83,'multiline':False]['text':' functorch's TensorWrapper (for grad transforms) stores a life handle.','line_number':84,'multiline':False]['text':' If a TensorWrapper escapes from the scope of the transform, then somehow','line_number':85,'multiline':False]['text':' it must know it escaped; it can tell by querying the life handle.','line_number':86,'multiline':False]['text':' Returns if an operator is in-place. An operator is inplace if:','line_number':89,'multiline':False]['text':' 1. The first argument is a Tensor and it is being written to','line_number':90,'multiline':False]['text':' 2. The first argument is being returned','line_number':91,'multiline':False]['text':' 3. No other arguments are aliased','line_number':92,'multiline':False]['text':' Here is an example of an in-place operator:','line_number':93,'multiline':False]['text':' add_(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)','line_number':94,'multiline':False]['text':' Given the indices of unwrapped inputs and the schema, this returns the indices of any outputs that should remain unwrapped','line_number':97,'multiline':False]['text':' Pretty printers','line_number':103,'multiline':False]['text':' While a functorch transform is active, torch.autograd.function._SingleLevelFunction','line_number':107,'multiline':False]['text':' is disabled by default. The following two APIs are APIs for enabling','line_number':108,'multiline':False]['text':' it. These are not user-facing APIs. We can delete this in the future, but','line_number':109,'multiline':False]['text':' it is useful for debugging when something goes wrong with the','line_number':110,'multiline':False]['text':' autograd.Function <> functorch interaction, which uses _SingleLevelFunction,','line_number':111,'multiline':False]['text':' because it leads to loud errors if something is incorrect.','line_number':112,'multiline':False]['text':' While a functorch grad transform is active, Tensor.requires_grad_() gets','line_number':116,'multiline':False]['text':' disabled. These two functions are the mechanism to controlling that.','line_number':117,'multiline':False]['text':' namespace at::functorch','line_number':124,'multiline':False]