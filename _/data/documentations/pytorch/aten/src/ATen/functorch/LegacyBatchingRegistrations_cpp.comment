['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' NOTE: [What is a batching rule?]','line_number':26,'multiline':False]['text':'','line_number':27,'multiline':False]['text':' NB: the following description only applies to this file and is about','line_number':28,'multiline':False]['text':' the legacy (deprecated) batching rule API. Please see writing_batch_rules.md','line_number':29,'multiline':False]['text':' for how to write new-style batching rules.','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' This files contains batching rules written with the legacy (now-deprecated)','line_number':32,'multiline':False]['text':' batching rule API.','line_number':33,'multiline':False]['text':' Please try to use the new-style batching rule API (see writing_batch_rules.md)','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' A *batching rule* implements the logic of how to call an operator on inputs','line_number':36,'multiline':False]['text':' that have zero or more additional batch dimensions. When one does a vmap, the','line_number':37,'multiline':False]['text':' dimension(s) being vmap'ed over get recorded as batch dimensions.','line_number':38,'multiline':False]['text':'','line_number':39,'multiline':False]['text':' For example, vmap(torch.add)(x, y)','line_number':40,'multiline':False]['text':' 1. wraps `x` into batched_x = BatchedTensor(x, bdims=[(lvl=1, dim=0)];','line_number':41,'multiline':False]['text':' 2. wraps `y` into batched_y = BatchedTensor(y, bdims=[(lvl=1, dim=0)];','line_number':42,'multiline':False]['text':' 3. and then runs `torch.add(batched_x, batched_y)`.','line_number':43,'multiline':False]['text':' NOTE: [When should I add a batching rule?]','line_number':45,'multiline':False]['text':' When you are adding a new operator, you'll need to add a batching rule so','line_number':46,'multiline':False]['text':' that vmap can work efficiently with said operator. If you do not, we'll attempt','line_number':47,'multiline':False]['text':' to generate a slow fallback for the batching rule.','line_number':48,'multiline':False]['text':' NOTE: [How to write batching rules?]','line_number':50,'multiline':False]['text':' The signature of a batching rule should look like exactly like the C++ signature','line_number':51,'multiline':False]['text':' of its operator.','line_number':52,'multiline':False]['text':'','line_number':53,'multiline':False]['text':' First, see NOTE: [Logical vs physical args] in VmapTransforms.h for terminology.','line_number':54,'multiline':False]['text':'','line_number':55,'multiline':False]['text':' At a high level, what a batching rule does is the following:','line_number':56,'multiline':False]['text':' 1. Converts (logical) BatchedTensors to views on physical tensors.','line_number':57,'multiline':False]['text':' 2. Converts logical arguments (e.g. dimension indexes, shapes) to physical','line_number':58,'multiline':False]['text':'    arguments that correspond to the physical tensors.','line_number':59,'multiline':False]['text':' 3. Calls at:: operations on the physical tensors and arguments to produce','line_number':60,'multiline':False]['text':'    some physical results.','line_number':61,'multiline':False]['text':' 4. Converts physical results back to BatchedTensors.','line_number':62,'multiline':False]['text':'','line_number':63,'multiline':False]['text':' Steps 1, 2, and 4 differ for operators with different batching behaviors. When','line_number':64,'multiline':False]['text':' writing a new batching rule, please select a VmapTransform that matches the','line_number':65,'multiline':False]['text':' batching behavior of your operation. The VmapTransform provides helper functions','line_number':66,'multiline':False]['text':' to do steps (1), (2), and (4).','line_number':67,'multiline':False]['text':' (see NOTE: [What is an VmapTransform?] in VmapTransforms.h)','line_number':68,'multiline':False]['text':' PyTorch allows operations to specify dim 0 and dim -1 on a scalar tensor.','line_number':71,'multiline':False]['text':' This check should probably go into the dispatcher...','line_number':82,'multiline':False]['text':' Adjust any dimensions higher than the batch dimension','line_number':119,'multiline':False]['text':' A column before batch dimension will be dropped so adjust accordingly.','line_number':127,'multiline':False]['text':' Since dimension to be squeezed is after the batch dimension adjust by one to account','line_number':131,'multiline':False]['text':' for the original batch dimension. In this case batch dimension won't move.','line_number':132,'multiline':False]['text':' Need to find out how many dimensions of size 1 are before the bdim','line_number':156,'multiline':False]['text':' if bdim is not 1, can just call squeeze_()','line_number':171,'multiline':False]['text':' otherwise, squeeze_() is going to get rid of the bdim too.','line_number':174,'multiline':False]['text':' We "fix it up" by calling unsqueeze_.','line_number':175,'multiline':False]['text':' Refresh metadata','line_number':180,'multiline':False]['text':' Also need to change some metadata...','line_number':201,'multiline':False]['text':' PyTorch has a special case where scalar_tensor.transpose(dim0, dim1) works','line_number':214,'multiline':False]['text':' for dim0, dim1 in {0, -1} and returns the scalar tensor. If the following happens:','line_number':215,'multiline':False]['text':' >>> x = torch.randn(B0)  # the per-examples are all scalars','line_number':216,'multiline':False]['text':' >>> vmap(lambda x: x.transpose_(0, -1), x)','line_number':217,'multiline':False]['text':' then we replicate this behavior.','line_number':218,'multiline':False]['text':' No transposing happened :P','line_number':222,'multiline':False]['text':' Also need to change some metadata...','line_number':233,'multiline':False]['text':' given (sizes, strides, storage_offset) returns the maximum location that','line_number':274,'multiline':False]['text':' can be indexed (or nullopt if such a location doesn't exist, e.g., tensors','line_number':275,'multiline':False]['text':' with zero-size dims).','line_number':276,'multiline':False]['text':' Let x be the "first slice" of physical_tensor.','line_number':286,'multiline':False]['text':' This checks that the range of possible memory locations accessible by','line_number':287,'multiline':False]['text':' x.as_strided(sizes, strides, maybe_storage_offset)','line_number':288,'multiline':False]['text':' are within the bounds of possible memory locations accessible by x.','line_number':289,'multiline':False]['text':' What are the semantics of as_strided inside of vmap?','line_number':327,'multiline':False]['text':' y = vmap(lambda x: x.as_strided(sizes, strides, offset))(xs)','line_number':328,'multiline':False]['text':' This returns a view on `x`, `y`, such that each y[i] has:','line_number':329,'multiline':False]['text':' - sizes: `sizes`','line_number':330,'multiline':False]['text':' - strides: `strides`','line_number':331,'multiline':False]['text':' - storage_offset: offset + i * x.stride(batch_dim)','line_number':332,'multiline':False]['text':'','line_number':333,'multiline':False]['text':' In other words, it is as if we had treated each x[i] as having storage','line_number':334,'multiline':False]['text':' offset equal to xs.offset() and called as_strided(sizes, sizes, offset).','line_number':335,'multiline':False]['text':' (that is equivalent to x[i].as_strided(','line_number':336,'multiline':False]['text':'    sizes, sizes, offset + x[i].storage_offset() - xs.offset()) for all i)','line_number':337,'multiline':False]['text':'','line_number':338,'multiline':False]['text':' Note that this *may* be different from actually running as_strided','line_number':339,'multiline':False]['text':' in a for-loop. This is due to how as_strided takes in `offset` to be','line_number':340,'multiline':False]['text':' an *absolute* offset. As an example, consider:','line_number':341,'multiline':False]['text':' >>> x = torch.tensor([0., 1., 2., 3., 4.]).as_strided([4], [1], 1)','line_number':342,'multiline':False]['text':' >>> z = [x[i].as_strided([1], [1], 1) for i in range(4)]','line_number':343,'multiline':False]['text':' Each z[i] is actually the same view on x (z[i] == torch.tensor([1.]))!','line_number':344,'multiline':False]['text':' However, we consider the above for-loop comprehension to be a user error:','line_number':345,'multiline':False]['text':' a user should have written the following if they wanted to use as_strided','line_number':346,'multiline':False]['text':' in a per-sample way:','line_number':347,'multiline':False]['text':' >>> z = [x[i].as_strided([1], [1], 1 + x[i].storage_offset() - 1) for i in range(4)]','line_number':348,'multiline':False]['text':' We can't rely on the physical as_strided call to do this for us because','line_number':363,'multiline':False]['text':' we do some sanity checks on the size/strides before calling into as_strided.','line_number':364,'multiline':False]['text':' Sanity checks:','line_number':369,'multiline':False]['text':' 1. as_strided(sizes, strides, storage_offset + tensor[i].offset() - tensor.offset())','line_number':370,'multiline':False]['text':' is valid for a slice of the input tensor.','line_number':371,'multiline':False]['text':' See Note: [When will the as_strided batching rule fail?] for details.','line_number':372,'multiline':False]['text':' physical_strides = physical tensor's batch strides + (logical) strides','line_number':376,'multiline':False]['text':' If zi = xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':385,'multiline':False]['text':' is valid for all i, then it turns out that','line_number':386,'multiline':False]['text':' xs.as_strided(physical_sizes, physical_strides, offset) always succeeds','line_number':387,'multiline':False]['text':' and creates a tensor y such that each y[i] references the same memory','line_number':388,'multiline':False]['text':' locations as zi. See NOTE: [When will the as_strided batching rule fail?]','line_number':389,'multiline':False]['text':' NOTE: [When will the as_strided batching rule fail?]','line_number':395,'multiline':False]['text':' If zi = xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':396,'multiline':False]['text':' is valid for all i, then it turns out that','line_number':397,'multiline':False]['text':' xs.as_strided(physical_sizes, physical_strides, offset) always succeeds and','line_number':398,'multiline':False]['text':' creates a tensor y such that each y[i] refers to the same memory as zi.','line_number':399,'multiline':False]['text':'','line_number':400,'multiline':False]['text':' Let's say we have xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset()).','line_number':401,'multiline':False]['text':' Furthermore, let's say that as a part of being "valid" this as_strided call','line_number':402,'multiline':False]['text':' does not return a result that can index memory not indexable by xs[i].','line_number':403,'multiline':False]['text':'','line_number':404,'multiline':False]['text':' WLOG, assume that there's only one batch dim and it is at the front of the','line_number':405,'multiline':False]['text':' `xs` tensor. Let B be the batch size and S be the stride of the batch dim.','line_number':406,'multiline':False]['text':' - If the batch dim isn't at the front of the tensor, then we can just move it','line_number':407,'multiline':False]['text':' to the front with movedim/permute. This is always valid because it just swaps','line_number':408,'multiline':False]['text':' some strides around.','line_number':409,'multiline':False]['text':' - This proof also works for tensors with multiple batch dims. We just have to','line_number':410,'multiline':False]['text':' do a little accounting:','line_number':411,'multiline':False]['text':'   - instead of [B], we'd have [B0, B1, ..., Bk].','line_number':412,'multiline':False]['text':'   - instead of [S], we'd have [S0, S1, ..., Sk].','line_number':413,'multiline':False]['text':'   - instead of i, we'd have a list of indices [I0, I1, ..., Ik]','line_number':414,'multiline':False]['text':'   - instead of S * I, we'd have \sum_{i=0}^k S_i * I_i','line_number':415,'multiline':False]['text':'','line_number':416,'multiline':False]['text':' [Equation 1]','line_number':417,'multiline':False]['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset()) has:','line_number':418,'multiline':False]['text':' - sizes: sizes','line_number':419,'multiline':False]['text':' - strides: strides','line_number':420,'multiline':False]['text':' - offset: offset + S * i','line_number':421,'multiline':False]['text':'','line_number':422,'multiline':False]['text':' x.as_strided itself checks that:','line_number':423,'multiline':False]['text':' - (sizes, strides, offset) are in bounds for `x`'s storage.','line_number':424,'multiline':False]['text':' - strides are positive','line_number':425,'multiline':False]['text':' - offset is positive','line_number':426,'multiline':False]['text':'','line_number':427,'multiline':False]['text':' Claim 1: if xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':428,'multiline':False]['text':' is valid, then','line_number':429,'multiline':False]['text':' ([B] + sizes, [S] + strides, offset + xs.offset()) are in bounds for `xs`'s storage.','line_number':430,'multiline':False]['text':'','line_number':431,'multiline':False]['text':' If we have the claim, then xs.as_strided([B] + sizes, [S] + strides, offset)','line_number':432,'multiline':False]['text':' won't error out. So all we need to check is that the memory locations are','line_number':433,'multiline':False]['text':' what we expected. See [Hand-wavy proof of Claim 1] for proof (it's not very important)','line_number':434,'multiline':False]['text':'','line_number':435,'multiline':False]['text':' xs.as_strided(physical_sizes, physical_strides, offset) is equivalent to','line_number':436,'multiline':False]['text':' xs.as_strided([B] + sizes, [S] + strides, offset)','line_number':437,'multiline':False]['text':'','line_number':438,'multiline':False]['text':' xs.as_strided([B] + sizes, [S] + strides, offset) has:','line_number':439,'multiline':False]['text':' - sizes: [B] + sizes','line_number':440,'multiline':False]['text':' - strides: [S] + strides','line_number':441,'multiline':False]['text':' - offset: offset','line_number':442,'multiline':False]['text':'','line_number':443,'multiline':False]['text':' xs.as_strided([B] + sizes, [S] + strides, offset)[i] has:','line_number':444,'multiline':False]['text':' - sizes: sizes','line_number':445,'multiline':False]['text':' - strides: strides','line_number':446,'multiline':False]['text':' - offset: offset + S * i','line_number':447,'multiline':False]['text':' These memory locations are exactly the same as what we got for [Equation 1],','line_number':448,'multiline':False]['text':' so the xs.as_strided([B] + sizes, [S] + strides, offset) is valid.','line_number':449,'multiline':False]['text':'','line_number':450,'multiline':False]['text':' [Hand-wavy proof of Claim 1]','line_number':451,'multiline':False]['text':' Part of our definition of being valid is that xs[i].as_strided(...)','line_number':452,'multiline':False]['text':' must return a tensor that only uses memory indexable by xs[i].','line_number':453,'multiline':False]['text':' This means that (sizes, strides, offset + xs[i].offset() - xs.offset()) satisfies:','line_number':454,'multiline':False]['text':'    offset + xs[i].offset() - xs.offset() + 1 + \sum_j (sizes[j] - 1) * strides[j]','line_number':455,'multiline':False]['text':'    <= xs[i].offset() + 1 + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':456,'multiline':False]['text':' (the largest-index memory location of xs[i].as_strided(...) must be \leq','line_number':457,'multiline':False]['text':' the largest-index memory location of xs[i])','line_number':458,'multiline':False]['text':'','line_number':459,'multiline':False]['text':' Fiddling that inequality gives us:','line_number':460,'multiline':False]['text':'    offset - xs.offset() + 1 + \sum_j (sizes[j] - 1) * strides[j]','line_number':461,'multiline':False]['text':'    <= 1 + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':462,'multiline':False]['text':'','line_number':463,'multiline':False]['text':'    offset - xs.offset() + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':464,'multiline':False]['text':'    <= 1 + (B-1)*S + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':465,'multiline':False]['text':'','line_number':466,'multiline':False]['text':'    offset - xs.offset() + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':467,'multiline':False]['text':'    <= 1 + \sum_j (xs.size(j) - 1) * xs.stride(j)','line_number':468,'multiline':False]['text':'','line_number':469,'multiline':False]['text':'    offset + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':470,'multiline':False]['text':'    <= xs.offset() + 1 + \sum_j (xs.size(j) - 1) * xs.stride(j)','line_number':471,'multiline':False]['text':' (the largest-index memory location of xs.as_strided(size, stride, offset)','line_number':472,'multiline':False]['text':' is \leq than the largest-index memory location of xs)','line_number':473,'multiline':False]['text':' Under the assumptions we've made, the lower bound (lowest indexed memory)','line_number':474,'multiline':False]['text':' is trivially within the storage.','line_number':475,'multiline':False]['text':'','line_number':476,'multiline':False]['text':' Therefore ([B] + sizes, [S] + strides, offset) are in bounds for','line_number':477,'multiline':False]['text':' `xs`'s storage.','line_number':478,'multiline':False]['text':' guard against the user passing in a batch of scalar tensors with batch','line_number':486,'multiline':False]['text':' NB: Probably bad for perf that we're allocating std::vectors for each level, but','line_number':511,'multiline':False]['text':' what can you do.','line_number':512,'multiline':False]['text':' Strategy:','line_number':516,'multiline':False]['text':' we're going to unwrap tensors, move their batch dims to the front,','line_number':517,'multiline':False]['text':' and put them into `tensors_to_cat`. Tensors that don't have a batch dim','line_number':518,'multiline':False]['text':' will get one forced onto them.','line_number':519,'multiline':False]['text':'','line_number':520,'multiline':False]['text':' Then, we'll do at::cat(tensors_to_cat, ...).','line_number':521,'multiline':False]['text':'','line_number':522,'multiline':False]['text':' There's a special case where at::cat ignores tensors that have logical shape','line_number':523,'multiline':False]['text':' [0]. If we see a Tensor that has logical shape [0] (but physical shape [B, 0]),','line_number':524,'multiline':False]['text':' we'll just slice the tensor to get a Tensor of shape [0] to pass to at::cat.','line_number':525,'multiline':False]['text':' find the bdim size. Might not exist if all BatchedTensors should be skipped','line_number':530,'multiline':False]['text':' by cat's special case.','line_number':531,'multiline':False]['text':' unwrap batchedtensors; expand out bdims','line_number':544,'multiline':False]['text':'has_bdim','line_number':551,'multiline':True]['text':' Special case: slice the tensor to get something of shape [0] to pass to cat','line_number':556,'multiline':False]['text':' We slice instead of allocate a new tensor to propagate requires_gradness...','line_number':557,'multiline':False]['text':'dim=','line_number':558,'multiline':True]['text':'index=','line_number':558,'multiline':True]['text':' Implementing this as a dummy for loop for now, since I'm not sure how to do it any better.','line_number':580,'multiline':False]['text':' I'm probably not accounting for potentially multiple batched dimensions?','line_number':581,'multiline':False]['text':' NB: stack wraps the dimensionality to (logical dim + 1), so we have to','line_number':608,'multiline':False]['text':' manually handle that here.','line_number':609,'multiline':False]['text':'logical','line_number':611,'multiline':True]['text':' Let [B0, B1, B2] be the shape of the batch dims. We're going to create','line_number':636,'multiline':False]['text':' the batch dimensions at the front of the tensor (in memory layout),','line_number':637,'multiline':False]['text':' irrespective of whether or not they are actually at the front (in memory layout)','line_number':638,'multiline':False]['text':' in the original `self` tensor. This is because when a user calls','line_number':639,'multiline':False]['text':' `new_empty_strided` in general, the `strides` they provide are for a new','line_number':640,'multiline':False]['text':' tensor and have no relation to the strides of the original tensor.','line_number':641,'multiline':False]['text':'','line_number':642,'multiline':False]['text':' So, the physical shape of the result should be ([B0, B1, B2] + size),','line_number':643,'multiline':False]['text':' but what about the physical strides?','line_number':644,'multiline':False]['text':'','line_number':645,'multiline':False]['text':' We're actually free to pick whatever stride we want:','line_number':646,'multiline':False]['text':' e.g., for size=[5, 3], stride=[0, 1], we could decide to','line_number':647,'multiline':False]['text':' use','line_number':648,'multiline':False]['text':' - physical size: [B0, B1, B2, 5, 3]','line_number':649,'multiline':False]['text':' - physical stride: [9999*B1*B2, 9999*B2, 9999, 0, 1]','line_number':650,'multiline':False]['text':'','line_number':651,'multiline':False]['text':' Let's select some reasonable strides such that:','line_number':652,'multiline':False]['text':' - The batch dims are "contiguous" with respect to each other','line_number':653,'multiline':False]['text':' - if empty_strided(size, stride) would have created a contiguous Tensor,','line_number':654,'multiline':False]['text':' then this new physical Tensor (with batch dims) is also contiguous','line_number':655,'multiline':False]['text':'','line_number':656,'multiline':False]['text':' Let S be the size of the storage if one were to construct a tensor','line_number':657,'multiline':False]['text':' with `size` and `stride` via empty_strided(size, stride).','line_number':658,'multiline':False]['text':' Then the physical sizes/strides should be:','line_number':659,'multiline':False]['text':' - physical size: [B0, B1, B2, 5, 3]','line_number':660,'multiline':False]['text':' - physical stride: [B1 * B2 * S, B2 * S, S, 0, 1]','line_number':661,'multiline':False]['text':' physical_strides = [B1 * B2 * S, B2 * S, S]','line_number':665,'multiline':False]['text':' physical_strides = [B1 * B2 * S, B2 * S, S] + strides','line_number':676,'multiline':False]['text':' Do a cat for each set of zipped unbound components','line_number':702,'multiline':False]['text':' NB: NTs only support batching over dim 0','line_number':713,'multiline':False]['text':' still legacy b/c teturns multiple tensors','line_number':725,'multiline':False]['text':' still legacy b/c needs special inplace rules','line_number':733,'multiline':False]['text':' still legacy because these are ridiculously complicated','line_number':740,'multiline':False]['text':' TODO: Move this somewhere better?','line_number':750,'multiline':False]['text':' namespace functorch','line_number':754,'multiline':False]['text':' namespace at','line_number':755,'multiline':False]