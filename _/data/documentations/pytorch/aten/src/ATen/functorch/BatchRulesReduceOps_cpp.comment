['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' Macros and templates have a difficult time dealing with enums,','line_number':80,'multiline':False]['text':' so we didn't turn this into an enum.','line_number':81,'multiline':False]['text':' See NOTE: [keepdim cases] for explanation of what these are.','line_number':82,'multiline':False]['text':' dim_arg_pos allows us to specify the location of the dim/dim array argument.','line_number':87,'multiline':False]['text':' For most PyTorch ops, this is equal to 1.','line_number':88,'multiline':False]['text':'','line_number':89,'multiline':False]['text':' NOTE: [keepdim cases]','line_number':90,'multiline':False]['text':' The operator in question either:','line_number':91,'multiline':False]['text':' - has a keepdim argument (KeepdimCase.Variable)','line_number':92,'multiline':False]['text':'   In this case, `maybe_keepdim_arg_pos` says where the index of the keepdim arg is.','line_number':93,'multiline':False]['text':'   example: sum(tensor, dim, keepdim)','line_number':94,'multiline':False]['text':' - always does a reduction with no keepdim (KeepdimCase.False)','line_number':95,'multiline':False]['text':'   that is, the rank of the output tensor is less than the rank of the input tensor.','line_number':96,'multiline':False]['text':' - always does a reduction with keepdim=True semantics (KeepdimCase.True)','line_number':97,'multiline':False]['text':'   That is, the rank of the output tensor is always the same as that of the input.','line_number':98,'multiline':False]['text':'   examples: log_softmax(tensor, dim), cumsum(tensor, dim)','line_number':99,'multiline':False]['text':' optional cannot be used in a template, otherwise we would use it here.','line_number':103,'multiline':False]['text':' NOTE: [boxed_reduction_batch_rule scalar tensor handling]','line_number':176,'multiline':False]['text':' Reduction operations in PyTorch have an edge case where they allow','line_number':177,'multiline':False]['text':' dim=0 and dim=-1 if the tensor has shape [].','line_number':178,'multiline':False]['text':'','line_number':179,'multiline':False]['text':' This can come up if we do something like','line_number':180,'multiline':False]['text':' vmap(lambda x: x.sum(0))(torch.tensor([10.])),','line_number':181,'multiline':False]['text':'','line_number':182,'multiline':False]['text':' In order to handle this edge case, we unsqueeze a dimension on the Tensor,','line_number':183,'multiline':False]['text':' run the operation (with dim=1 instead), and then process the output tensor.','line_number':184,'multiline':False]['text':' There are two cases:','line_number':185,'multiline':False]['text':' - keepdim = True','line_number':186,'multiline':False]['text':'     unsqueeze   op      squeeze','line_number':187,'multiline':False]['text':'   [B] -> [B, 1] -> [B, 1] -> [B]','line_number':188,'multiline':False]['text':' - keepdim = False','line_number':189,'multiline':False]['text':'     unsqueeze   op     no need to squeeze','line_number':190,'multiline':False]['text':'   [B] -> [B, 1] -> [B]','line_number':191,'multiline':False]['text':' if keepdim is True, then we need to squeeze the dimension of size 1.','line_number':192,'multiline':False]['text':' Determine the value of keepdim','line_number':194,'multiline':False]['text':' see NOTE: [boxed_reduction_batch_rule scalar tensor handling]','line_number':225,'multiline':False]['text':' squeeze(-1) is a no-op if the shape of the dim is not 1.','line_number':227,'multiline':False]['text':' To make it safer, we internal assert here.','line_number':228,'multiline':False]['text':' Skipping all/any since they don't have opinfo tests right now :P','line_number':239,'multiline':False]['text':' softmax_backward's decomposition is y * gy - y * (y * gy).sum(dim, keepdim=True)','line_number':266,'multiline':False]['text':' NB: the CUDA kernel handles strides so we can just expand','line_number':267,'multiline':False]['text':' all of the tensors and call it a day. The CPU kernel is not as good but','line_number':268,'multiline':False]['text':' idk if the perf on that really matters','line_number':269,'multiline':False]['text':' Expand out that extra dimension for everyone','line_number':273,'multiline':False]['text':' Scalar tensor case. softmax turns into the identity when this happens.','line_number':278,'multiline':False]['text':' I don't know why the output is zeros, though, but that's what softmax tells me...','line_number':279,'multiline':False]['text':'has_batch_dim','line_number':284,'multiline':True]['text':' Not sure why output_ needs to be marked as .contiguous(). Someting must','line_number':286,'multiline':False]['text':' have changed in PyTorch (and output of softmax is probably always contiguous)','line_number':287,'multiline':False]['text':' NB: It turns out that expanding + calling log_softmax_backward is generally','line_number':296,'multiline':False]['text':' faster than the decomposition.','line_number':297,'multiline':False]['text':' Benchmark here: https://gist.github.com/zou3519/ae3b33b5730a84aae8a80a05c89e078a','line_number':298,'multiline':False]['text':' Decomposition is (grad_output - grad_output.sum(dim, keepdim=True) * result.exp())','line_number':299,'multiline':False]['text':' We can squeeze out a last mile of performance by writing custom kernels.','line_number':300,'multiline':False]['text':' Expand out that extra dimension for everyone','line_number':304,'multiline':False]['text':' Scalar tensor case. log_softmax returns zeros when this happens','line_number':309,'multiline':False]['text':'has_batch_dim','line_number':314,'multiline':True]['text':' Preprocess sorter and sorted_sequence.','line_number':332,'multiline':False]['text':' If they both exist, and only one has a bdim, then we need to make sure both do.','line_number':333,'multiline':False]['text':' After this step, we can forget about sorter for a bit.','line_number':334,'multiline':False]['text':' Two cases: buckets_logical_rank is 1, or it is greater than 1.','line_number':355,'multiline':False]['text':' searchsorted is basically two operators with different semantics jammed','line_number':356,'multiline':False]['text':' into one','line_number':357,'multiline':False]['text':' B<...>D, B<...>V -> no change','line_number':359,'multiline':False]['text':' B<...>D, <...>V -> B<...>D, B<...>V','line_number':365,'multiline':False]['text':' <...>D, B<...>V -> <...>D, <...>(BV)','line_number':372,'multiline':False]['text':' buckets_logical_rank == 1 case.','line_number':382,'multiline':False]['text':' BD, B* -> BD, B flat(*)','line_number':383,'multiline':False]['text':' BD, * -> BD, flat(*) -> BD, B flat(*)','line_number':391,'multiline':False]['text':' D, B* -> no change','line_number':400,'multiline':False]['text':' checking logical rank','line_number':413,'multiline':False]['text':' checking logical rank','line_number':423,'multiline':False]['text':' Use when the other macros don't work out.','line_number':428,'multiline':False]['text':' - dim_pos: index of the dim argument','line_number':429,'multiline':False]['text':' - keepdim_case: either True, False, or Variable.','line_number':430,'multiline':False]['text':'   See NOTE: [keepdim cases] for more details.','line_number':431,'multiline':False]['text':' - maybe_keepdim_pos. The index of the keepdim argument,','line_number':432,'multiline':False]['text':'   if exists. Otherwise, the value is ignored.','line_number':433,'multiline':False]['text':' Provided for your convenience; most operators that have a keepdim arg','line_number':438,'multiline':False]['text':' will work with this macro.','line_number':439,'multiline':False]['text':' Assumes the dim arg is at position 1 and the keepdim arg is at pos 2.','line_number':440,'multiline':False]['text':' Provided for your convenience; most operators that do not have a keepdim','line_number':444,'multiline':False]['text':' arg will work with this macro.','line_number':445,'multiline':False]['text':' Assumes the dim arg is at position 1 and the operation always returns','line_number':446,'multiline':False]['text':' a tensor of the same rank (instead of a smaller rank).','line_number':447,'multiline':False]