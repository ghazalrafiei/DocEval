['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' convolution_batch_rule translated from jax with modifications:','line_number':13,'multiline':False]['text':' https://github.com/google/jax/blob/master/jax/_src/lax/lax.py#L3143','line_number':14,'multiline':False]['text':' PyTorch's convolution is different from JAX's conv_general_dilated:','line_number':16,'multiline':False]['text':' we do not support batch_group_count (which is needed for convolution backwards).','line_number':17,'multiline':False]['text':' Instead, there's a convolution_backward op that needs a batching rule.','line_number':18,'multiline':False]['text':' If we have a batched bias or weight, we need to perform the computation separately.','line_number':30,'multiline':False]['text':' conv_transpose with groups is normally NIHW, IOHW -> N(GO)HW','line_number':56,'multiline':False]['text':' With RHS batched, we do the following:','line_number':57,'multiline':False]['text':' NIHW, BIOHW -> NIHW, I(BO)HW -> N(GBO)HW -> BN(GO)HW','line_number':58,'multiline':False]['text':' NB: the following isn't written using rhs_spec','line_number':59,'multiline':False]['text':' (PyTorch convs have a fixed dimension order)','line_number':60,'multiline':False]['text':' BIOHW -> I(BO)HW','line_number':62,'multiline':False]['text':' NIHW, I(BO)HW -> N(GBO)HW','line_number':64,'multiline':False]['text':' N(GBO)HW -> NG(BO)HW','line_number':66,'multiline':False]['text':' NG(BO)HW -> NGBOHW','line_number':68,'multiline':False]['text':' NGBOHW -> NB(GO)HW','line_number':70,'multiline':False]['text':' conv with groups is normally N(GI)HW, (GO)IHW -> N(GO)HW','line_number':74,'multiline':False]['text':' With RHS batched, we do the following:','line_number':75,'multiline':False]['text':' N(GI)HW, B(GO)IHW -> N(GI)HW, (GBO)IHW -> N(GBO)HW -> BN(GO)HW','line_number':76,'multiline':False]['text':' NB: the following isn't written using rhs_spec','line_number':77,'multiline':False]['text':' (PyTorch convs have a fixed dimension order)','line_number':78,'multiline':False]['text':' B(GO)IHW -> BGOIHW','line_number':80,'multiline':False]['text':' BGOIHW -> G(BO)IHW','line_number':82,'multiline':False]['text':' G(BO)IHW -> (GBO)IHW','line_number':84,'multiline':False]['text':' N(GI)HW, (GBO)IHW -> N(GBO)HW','line_number':86,'multiline':False]['text':' N(GBO)HW -> NG(BO)HW','line_number':88,'multiline':False]['text':' NG(BO)HW -> NGBOHW','line_number':90,'multiline':False]['text':' NGBOHW -> NB(GO)HW','line_number':92,'multiline':False]['text':' Ignore everything. If the user called this in the normal way,','line_number':131,'multiline':False]['text':' then they should be fine.','line_number':132,'multiline':False]['text':' TODO: delete the following after confirming performance','line_number':141,'multiline':False]['text':' bool first_dim_has_size_1(const Tensor& value, int64_t bdim) {','line_number':142,'multiline':False]['text':'   if (bdim == 0) {','line_number':143,'multiline':False]['text':'     return value.size(1) == 1;','line_number':144,'multiline':False]['text':'   }','line_number':145,'multiline':False]['text':'   return value.size(0) == 1;','line_number':146,'multiline':False]['text':' }','line_number':147,'multiline':False]['text':'','line_number':148,'multiline':False]['text':' std::tuple<Tensor,int64_t,Tensor,int64_t> cudnn_conv_per_sample_grad_rule(','line_number':149,'multiline':False]['text':'     const Tensor& self, optional<int64_t> self_bdim,','line_number':150,'multiline':False]['text':'     const Tensor& grad_output, optional<int64_t> grad_output_bdim,','line_number':151,'multiline':False]['text':'     const Tensor& weight, optional<int64_t> weight_bdim,','line_number':152,'multiline':False]['text':'     IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark,','line_number':153,'multiline':False]['text':'     bool deterministic, bool allow_tf32, std::array<bool, 2> output_mask) {','line_number':154,'multiline':False]['text':'   TORCH_INTERNAL_ASSERT(self_bdim && grad_output_bdim && !weight_bdim);','line_number':155,'multiline':False]['text':'   // TODO: No clue if this works if the first non-batch dim isn't size 1','line_number':156,'multiline':False]['text':'   TORCH_INTERNAL_ASSERT(first_dim_has_size_1(self, *self_bdim));','line_number':157,'multiline':False]['text':'   TORCH_INTERNAL_ASSERT(self.dim() == 5);','line_number':158,'multiline':False]['text':'','line_number':159,'multiline':False]['text':'   auto bdim_size = self.size(*self_bdim);','line_number':160,'multiline':False]['text':'   auto self_ = reshape_dim_into(*self_bdim, 0, self);','line_number':161,'multiline':False]['text':'   auto in_channels = self_.size(1);','line_number':162,'multiline':False]['text':'   auto grad_output_ = reshape_dim_into(*grad_output_bdim, 0, grad_output);','line_number':163,'multiline':False]['text':'','line_number':164,'multiline':False]['text':'   auto grad_self = at::cudnn_convolution_backward_input(','line_number':165,'multiline':False]['text':'       self_.sizes(), grad_output_, weight,','line_number':166,'multiline':False]['text':'       padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);','line_number':167,'multiline':False]['text':'   grad_self = reshape_dim_outof(0, bdim_size, grad_self);','line_number':168,'multiline':False]['text':'','line_number':169,'multiline':False]['text':'   // Copied from https://github.com/pytorch/opacus/blob/master/opacus/grad_sample/conv.py','line_number':170,'multiline':False]['text':'   auto A = at::im2col(self_, {weight.size(2), weight.size(3)}, dilation, padding, stride);','line_number':171,'multiline':False]['text':'   auto B = grad_output_.reshape({bdim_size, -1, A.size(-1)});','line_number':172,'multiline':False]['text':'   auto grad_sample = at::einsum("noq,npq->nop", {B, A});','line_number':173,'multiline':False]['text':'   grad_sample = grad_sample.view({','line_number':174,'multiline':False]['text':'       bdim_size, groups, -1, groups, in_channels / groups,','line_number':175,'multiline':False]['text':'       weight.size(2) * weight.size(3) });','line_number':176,'multiline':False]['text':'   grad_sample = at::einsum("ngrg...->ngr...", {grad_sample});','line_number':177,'multiline':False]['text':'   grad_sample = grad_sample.reshape(','line_number':178,'multiline':False]['text':'       {bdim_size, weight.size(0), weight.size(1), weight.size(2), weight.size(3)});','line_number':179,'multiline':False]['text':'','line_number':180,'multiline':False]['text':'   return std::make_tuple(grad_self, 0, grad_sample, 0);','line_number':181,'multiline':False]['text':' }','line_number':182,'multiline':False]['text':'','line_number':183,'multiline':False]['text':' std::tuple<Tensor,Tensor> cudnn_convolution_backward_plumbing(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, std::array<bool, 2> output_mask) {','line_number':184,'multiline':False]['text':'   auto maybe_layer = maybeCurrentDynamicLayer();','line_number':185,'multiline':False]['text':'   TORCH_INTERNAL_ASSERT(maybe_layer.has_value());','line_number':186,'multiline':False]['text':'   int64_t cur_level = maybe_layer->layerId();','line_number':187,'multiline':False]['text':'','line_number':188,'multiline':False]['text':'   Tensor self_value;','line_number':189,'multiline':False]['text':'   optional<int64_t> self_bdim;','line_number':190,'multiline':False]['text':'   std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);','line_number':191,'multiline':False]['text':'   Tensor grad_output_value;','line_number':192,'multiline':False]['text':'   optional<int64_t> grad_output_bdim;','line_number':193,'multiline':False]['text':'   std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);','line_number':194,'multiline':False]['text':'   Tensor weight_value;','line_number':195,'multiline':False]['text':'   optional<int64_t> weight_bdim;','line_number':196,'multiline':False]['text':'   std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);','line_number':197,'multiline':False]['text':'','line_number':198,'multiline':False]['text':'   if (self_bdim.has_value() && self_value.dim() == 5 && first_dim_has_size_1(self_value, *self_bdim) && grad_output_bdim.has_value() && !weight_bdim.has_value()) {','line_number':199,'multiline':False]['text':'     c10::impl::ExcludeDispatchKeyGuard guard(DispatchKey::FuncTorchBatched);','line_number':200,'multiline':False]['text':'     auto result = cudnn_conv_per_sample_grad_rule(','line_number':201,'multiline':False]['text':'         self_value, self_bdim,','line_number':202,'multiline':False]['text':'         grad_output_value, grad_output_bdim,','line_number':203,'multiline':False]['text':'         weight_value, weight_bdim,','line_number':204,'multiline':False]['text':'         padding, stride, dilation, groups,','line_number':205,'multiline':False]['text':'         benchmark, deterministic, allow_tf32, output_mask);','line_number':206,'multiline':False]['text':'     return std::make_tuple(','line_number':207,'multiline':False]['text':'         makeBatched(std::get<0>(result), std::get<1>(result), cur_level),','line_number':208,'multiline':False]['text':'         makeBatched(std::get<2>(result), std::get<3>(result), cur_level));','line_number':209,'multiline':False]['text':'   }','line_number':210,'multiline':False]['text':'','line_number':211,'multiline':False]['text':'   static auto op = c10::Dispatcher::singleton()','line_number':212,'multiline':False]['text':'     .findSchemaOrThrow("aten::cudnn_convolution_backward", "");','line_number':213,'multiline':False]['text':'   return slow_fallback<Tensor,Tensor>(op, { self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask });','line_number':214,'multiline':False]['text':' reshapes the batch_size into dim','line_number':228,'multiline':False]['text':' regular: BNO, BOI -> N(BO), (BO)I -> N(BI)','line_number':251,'multiline':False]['text':' transposed: BNO, BIO -> N(BO), (BI)O -> N(BI)','line_number':252,'multiline':False]['text':' BNO, OI -> (BN)O, OI -> (BN)I','line_number':263,'multiline':False]['text':' transposed is the same.','line_number':264,'multiline':False]['text':' regular: NO, BOI -> NO, O(BI) -> N(BI)','line_number':276,'multiline':False]['text':' transposed: NO, BIO -> NO, (BI)O -> N(BI)','line_number':277,'multiline':False]['text':' N(GO), B(GO)I -> N(GO), (GO)(BI) -> N(GBI)','line_number':289,'multiline':False]['text':' N(GBI)','line_number':295,'multiline':False]['text':' N(GO), B(GI)O -> N(GO), (GBI)O -> N(GBI)','line_number':297,'multiline':False]['text':' B(GI)O','line_number':298,'multiline':False]['text':' BGIO','line_number':299,'multiline':False]['text':' GBIO','line_number':300,'multiline':False]['text':' (GBI)O','line_number':301,'multiline':False]['text':' N(GBI)','line_number':306,'multiline':False]['text':' N(GBI) -> NG(BI) -> NGBI -> NBGI -> NB(GI)','line_number':308,'multiline':False]['text':' BNO, BNI -> N(BO), N(BI) -> (BO)I (regular) (BI)O (transposed)','line_number':332,'multiline':False]['text':' regular: BNO, NI -> N(BO), NI -> (BO)I','line_number':346,'multiline':False]['text':' transposed: BNO, NI -> N(BO), NI -> I(BO)','line_number':347,'multiline':False]['text':' BN(GO)','line_number':358,'multiline':False]['text':' BNGO','line_number':359,'multiline':False]['text':' NGBO','line_number':360,'multiline':False]['text':' N(GBO)','line_number':361,'multiline':False]['text':' BN(GO), N(GI) -> N(GBO), N(GI) -> (GBO)I','line_number':363,'multiline':False]['text':' GBOI','line_number':369,'multiline':False]['text':' BGOI','line_number':370,'multiline':False]['text':' B(GO)I','line_number':371,'multiline':False]['text':' BN(GO), N(GI) -> N(GBO), N(GI) -> (GI)(BO)','line_number':374,'multiline':False]['text':' regular: NO, BNI -> NO, N(BI) -> O(BI)','line_number':387,'multiline':False]['text':' transposed: NO, BNI -> NO, N(BI) -> (BI)O','line_number':388,'multiline':False]['text':' BN(GI)','line_number':399,'multiline':False]['text':' BNGI','line_number':400,'multiline':False]['text':' NGBI','line_number':401,'multiline':False]['text':' N(GBI)','line_number':402,'multiline':False]['text':' regular: N(GO), BN(GI) -> N(GO), N(GBI) -> (GO)(BI)','line_number':404,'multiline':False]['text':' transposed: N(GO), BN(GI) -> N(GO), N(GBI) -> (GBI)O','line_number':413,'multiline':False]['text':' GBIO','line_number':419,'multiline':False]['text':' BGIO','line_number':420,'multiline':False]['text':' B(GI)O','line_number':421,'multiline':False]['text':' TODO: A little bird says that unfold + matmul is actually faster than','line_number':465,'multiline':False]['text':' group convolution in many cases. We should benchmark some of','line_number':466,'multiline':False]['text':' the common cases and replace things with unfold + matmul as necessary.','line_number':467,'multiline':False]['text':' Notation:','line_number':469,'multiline':False]['text':' B - a batch dimension','line_number':470,'multiline':False]['text':' G - groups (sometimes omitted because it doesn't matter)','line_number':471,'multiline':False]['text':' NO - grad_output','line_number':472,'multiline':False]['text':' NI - input','line_number':473,'multiline':False]['text':' OI - weight','line_number':474,'multiline':False]['text':' "(BO)I" - we don't actually care about the values of this Tensor,','line_number':475,'multiline':False]['text':'           we just need to create a tensor on the same device with the','line_number':476,'multiline':False]['text':'           correct shape and pray that the implementation is smart enough','line_number':477,'multiline':False]['text':'           to not do anything with it.','line_number':478,'multiline':False]['text':' BNO, BNI, BOI','line_number':480,'multiline':False]['text':' AKA one of the model ensembling case','line_number':481,'multiline':False]['text':' BNO, BNI, BOI -> N(BO), N(BI), (BO)I','line_number':486,'multiline':False]['text':' N(BI), (BO)I -> NBI, BOI','line_number':493,'multiline':False]['text':' Someone's definitely going to find a problem with this batching rule so','line_number':527,'multiline':False]['text':' I'm leaving the following fallback if we need it back.','line_number':528,'multiline':False]['text':' static auto op = c10::Dispatcher::singleton()','line_number':529,'multiline':False]['text':'   .findSchemaOrThrow("aten::convolution_backward", "");','line_number':530,'multiline':False]['text':' auto result = slow_fallback<Tensor,Tensor,Tensor>(op, {','line_number':531,'multiline':False]['text':'   grad_output_, input_, weight_, bias_sizes_opt,','line_number':532,'multiline':False]['text':'   stride, padding, dilation, transposed, output_padding, groups, output_mask','line_number':533,'multiline':False]['text':' });','line_number':534,'multiline':False]['text':' return std::make_tuple(grad_input, std::get<1>(result), grad_bias);','line_number':535,'multiline':False]['text':' namespace at;:functorch','line_number':545,'multiline':False]