['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]['text':' All rights reserved.','line_number':2,'multiline':False]['text':'','line_number':3,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]['text':' We assume this in a few other places in the codebase,','line_number':20,'multiline':False]['text':' but there isn't a centralized definition.','line_number':21,'multiline':False]['text':' The valid vmap levels range from [0, 64). This effectively means that we','line_number':24,'multiline':False]['text':' support a maximum of 64 nested vmaps.','line_number':25,'multiline':False]['text':' Store this number of elements of BatchDims on the stack. Most people will','line_number':28,'multiline':False]['text':' probably use <= 5 nested vmaps, but adjust this number as necessary.','line_number':29,'multiline':False]['text':' A BatchedTensorImpl holds an underlying Tensor and a single batch dim','line_number':32,'multiline':False]['text':' NB: We use the term "BatchedTensor" to mean a Tensor that is backed with a','line_number':33,'multiline':False]['text':' BatchedTensorImpl.','line_number':34,'multiline':False]['text':'','line_number':35,'multiline':False]['text':' The batch dimensions are treated as being "private"; they are not user-visible.','line_number':36,'multiline':False]['text':' For example, in the following Tensor,','line_number':37,'multiline':False]['text':'    bt = BatchedTensorImpl(ones(2, 3, 5, 7), lvl=1, dim=0)','line_number':38,'multiline':False]['text':' dimension 0 is batch dimension.','line_number':39,'multiline':False]['text':'','line_number':40,'multiline':False]['text':' bt.sizes() returns (5, 7); bt.sum(0) performs a reduction over the (public)','line_number':41,'multiline':False]['text':' dim 0, which is equivalent to dim 3 in the underlying ones(2, 3, 5, 7) tensor.','line_number':42,'multiline':False]['text':' Returns batch dimension of this tensor','line_number':46,'multiline':False]['text':' Returns batch dimension of this tensor','line_number':49,'multiline':False]['text':' BatchedTensorImpl wraps a Tensor','line_number':52,'multiline':False]['text':' Given a public dimension index, return the dimension index in the underlying','line_number':55,'multiline':False]['text':' value() tensor.','line_number':56,'multiline':False]['text':' For example, if we have','line_number':57,'multiline':False]['text':'    bt = BatchedTensorImpl(ones(2, 3, 5, 7), lvl=1, dim=0)','line_number':58,'multiline':False]['text':' bt.actualDim(0) -> 1','line_number':59,'multiline':False]['text':' bt.actualDim(1) -> 2','line_number':60,'multiline':False]['text':' bt.actualDim(2) -> 3','line_number':61,'multiline':False]['text':' bt.actualDim(3) -> Error','line_number':62,'multiline':False]['text':' We have to override this because we opted into CustomStrides','line_number':69,'multiline':False]['text':' Override a bunch of methods inherited from TensorImpl to return error messages.','line_number':72,'multiline':False]['text':' Used in torchdim. torchdim uses non-lexical BatchedTensor; the way it','line_number':89,'multiline':False]['text':' accomplishes this is a hack where it is able to modify the levels of','line_number':90,'multiline':False]['text':' BatchedTensor to match the level of the current vmap transform.','line_number':91,'multiline':False]['text':' Used in batching rule for in-place view operations that can change','line_number':96,'multiline':False]['text':' the index of the bdim (think squeeze_, unsqueeze_)','line_number':97,'multiline':False]['text':' NB: you MUST call refreshTensorMetadata after doing this.','line_number':99,'multiline':False]['text':' see NOTE: [BatchedTensorImpl levels invariant]','line_number':103,'multiline':False]['text':' NB: We use the term "BatchedTensor" to mean a Tensor that is backed with a','line_number':113,'multiline':False]['text':' BatchedTensorImpl.','line_number':114,'multiline':False]['text':' It is unsafe to call this on a Tensor that is not backed by a','line_number':120,'multiline':False]['text':' BatchedTensorImpl. Please use `maybeGetBatchedImpl` whenever possible.','line_number':121,'multiline':False]['text':' Returns a bitset. If bit i is set, then that means dim i is a batchdim.','line_number':133,'multiline':False]['text':' Creates a bitset for the given level','line_number':140,'multiline':False]['text':' Use this to construct a BatchedTensor from a regular Tensor','line_number':147,'multiline':False]['text':' Adds a batch dim to `tensor`, returning a BatchedTensor','line_number':150,'multiline':False]['text':' Certain dispatch keys must be propagated to the BatchedTensor (or, in general,','line_number':153,'multiline':False]['text':' any wrapper Tensor subclasses). This is because there are methods on Tensor','line_number':154,'multiline':False]['text':' that skip dispatch and check for the presence of a dispatch key (e.g. is_cpu()).','line_number':155,'multiline':False]['text':' TODO: should probably contain more (or all?) backend keys','line_number':156,'multiline':False]['text':' namespace at::functorch','line_number':170,'multiline':False]