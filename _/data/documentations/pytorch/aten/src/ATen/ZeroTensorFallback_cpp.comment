['text':' TODO: add a note explaining the design decisions','line_number':12,'multiline':False]['text':' ZeroTensors are designed to be immutable. Thus, we error out when an in-place operation is performed on ZeroTensors','line_number':13,'multiline':False]['text':' We assume that view operators automatically handle the ZeroTensor bit','line_number':37,'multiline':False]['text':' correctly by propagating the dispatch key in key_set.','line_number':38,'multiline':False]['text':' This is not necessarily always right, so you should test these cases.','line_number':39,'multiline':False]['text':' Was already tested by is_write loop above','line_number':53,'multiline':False]['text':' TODO: assert requires_grad=False','line_number':71,'multiline':False]['text':'_like should not propagate zerotensor dispatch key','line_number':72,'multiline':False]['text':' The functions in the list below have a specific registeration in native_functions.yaml and','line_number':98,'multiline':False]['text':' do not use the fallback.','line_number':99,'multiline':False]['text':' m.impl("mul.Tensor", torch::CppFunction::makeFallthrough());','line_number':100,'multiline':False]['text':' m.impl("add.Tensor", torch::CppFunction::makeFallthrough());','line_number':101,'multiline':False]['text':' m.impl("linalg_cross", torch::CppFunction::makeFallthrough());','line_number':102,'multiline':False]['text':' namespace at','line_number':107,'multiline':False]