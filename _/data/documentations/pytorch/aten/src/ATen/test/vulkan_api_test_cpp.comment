['text':' TODO: These functions should move to a common place.','line_number':10,'multiline':False]['text':' For values near the clamp threshold, results may be ambiguous.','line_number':60,'multiline':False]['text':' For values near the clamp threshold, results may be ambiguous.','line_number':99,'multiline':False]['text':' generate all permutations of a given dims','line_number':147,'multiline':False]['text':' generate all subsets of set {0,...,n - 1} through backtracking','line_number':164,'multiline':False]['text':' Arrange','line_number':181,'multiline':False]['text':' Act','line_number':185,'multiline':False]['text':' Assert','line_number':189,'multiline':False]['text':' i.e., 4D tensor's equivalent indexing = [:,:,:,10:30:1]','line_number':200,'multiline':False]['text':' i.e., 4D tensor's equivalent indexing = [:,:,:,10:30:7]','line_number':201,'multiline':False]['text':' i.e., 4D tensor's equivalent indexing = [:,:,:,10:50:2] with end=out of range','line_number':202,'multiline':False]['text':' i.e., 4D tensor's equivalent indexing = [:,:,:,-60:60:2] with start/end=out of range','line_number':203,'multiline':False]['text':' i.e., 4D tensor's equivalent indexing = [:,:,:,-30:-10:1] with negative start/end','line_number':204,'multiline':False]['text':' i.e., 4D 's equivalent indexing = [:,:,:,0:9223372036854775807:1] with end=INT64_MAX','line_number':205,'multiline':False]['text':' i.e., 4D 's equivalent indexing = [:,:,:,-10:9223372036854775807:1] with negative start and end=INT64_MAX','line_number':206,'multiline':False]['text':' This triggers a SymInt assert since [-2^63, -2^62-1] range is reserved for packed symints','line_number':207,'multiline':False]['text':'slice_test(dim2size.second, dim2size.first, INT64_MIN, INT64_MAX, 1); // i.e., 4D 's equivalent indexing = [:,:,:,-9223372036854775808:9223372036854775807:1] with start=INT64_MIN and end=INT64_MAX','line_number':208,'multiline':False]['text':' i.e., 4D 's equivalent indexing = [:,:,:,::1] with empty start/end','line_number':209,'multiline':False]['text':' Arrange','line_number':214,'multiline':False]['text':' Act','line_number':218,'multiline':False]['text':' Assert','line_number':222,'multiline':False]['text':' namespace','line_number':256,'multiline':False]['text':' Force the zero-dim tensor to a non-zero constant v.','line_number':328,'multiline':False]['text':' 4D','line_number':337,'multiline':False]['text':' 3D','line_number':339,'multiline':False]['text':' 2D','line_number':341,'multiline':False]['text':' 1D','line_number':343,'multiline':False]['text':' Uses the shader: image_to_nchw_quantized_mul4 ((H * W) % 4 == 0)','line_number':377,'multiline':False]['text':' ch % 4 != 0,  ch < 4','line_number':378,'multiline':False]['text':' ch % 4 != 0, ch > 5','line_number':383,'multiline':False]['text':' 3d, 2d, 1d','line_number':388,'multiline':False]['text':' Uses the shader: image_to_nchw_quantized_mul4 ((H * W) % 4 == 0)','line_number':395,'multiline':False]['text':' ch % 4 == 0','line_number':396,'multiline':False]['text':' Uses the shader: image_to_nchw_uint ((H * W) % 4 != 0)','line_number':403,'multiline':False]['text':' Incompatible dimensions for broadcasting for binary elementwise op','line_number':448,'multiline':False]['text':' mismatched bias size (should be 1-dim or {17, 9})','line_number':768,'multiline':False]['text':' Act: Vulkan batchnorm only supports evaluation mode','line_number':792,'multiline':False]['text':' Act: Vulkan batchnorm expects 4-dim input','line_number':806,'multiline':False]['text':' Act: Vulkan batchnorm expects 4-dim input','line_number':820,'multiline':False]['text':' Act: Vulkan batchnorm expects channel dim to be multiple of 4','line_number':834,'multiline':False]['text':' Act: weight tensor contains incorrect number of elements','line_number':848,'multiline':False]['text':' Act: bias tensor contains incorrect number of elements','line_number':862,'multiline':False]['text':' Act: running mean tensor contains incorrect number of elements','line_number':876,'multiline':False]['text':' Act: running var tensor contains incorrect number of elements','line_number':890,'multiline':False]['text':' mismatched dimensions of batch sizes.','line_number':1073,'multiline':False]['text':' This will call at::bmm. Will crash for unknow reason.','line_number':1233,'multiline':False]['text':' This will call at::bmm. Will crash for unknow reason.','line_number':1242,'multiline':False]['text':' This will call at::mm','line_number':1251,'multiline':False]['text':' This will call at::mm','line_number':1258,'multiline':False]['text':' mismatched dimensions of batch sizes.','line_number':1333,'multiline':False]['text':' This is a simple case using arange for input, ones for weights, and arange','line_number':1379,'multiline':False]['text':' for bias. This makes debugging easiser.','line_number':1380,'multiline':False]['text':' cpu','line_number':1479,'multiline':False]['text':' vulkan','line_number':1483,'multiline':False]['text':' check','line_number':1497,'multiline':False]['text':' cpu','line_number':1520,'multiline':False]['text':' vulkan','line_number':1524,'multiline':False]['text':' check','line_number':1538,'multiline':False]['text':' cpu','line_number':1562,'multiline':False]['text':' vulkan','line_number':1566,'multiline':False]['text':' check','line_number':1580,'multiline':False]['text':'TODO: Support conv2d with dilation != 1','line_number':1593,'multiline':False]['text':' input_shape','line_number':1660,'multiline':False]['text':' weight_shape','line_number':1661,'multiline':False]['text':' bias_shape','line_number':1662,'multiline':False]['text':' stride','line_number':1663,'multiline':False]['text':' padding','line_number':1664,'multiline':False]['text':' dilation','line_number':1665,'multiline':False]['text':' groups','line_number':1666,'multiline':False]['text':' input_shape','line_number':1671,'multiline':False]['text':' weight_shape','line_number':1672,'multiline':False]['text':' bias_shape','line_number':1673,'multiline':False]['text':' stride','line_number':1674,'multiline':False]['text':' padding','line_number':1675,'multiline':False]['text':' dilation','line_number':1676,'multiline':False]['text':' groups','line_number':1677,'multiline':False]['text':' input_shape','line_number':1880,'multiline':False]['text':' weight_shape','line_number':1881,'multiline':False]['text':' bias_shape','line_number':1882,'multiline':False]['text':' stride','line_number':1883,'multiline':False]['text':' padding','line_number':1884,'multiline':False]['text':' dilation','line_number':1885,'multiline':False]['text':' groups','line_number':1886,'multiline':False]['text':' input_shape','line_number':1891,'multiline':False]['text':' weight_shape','line_number':1892,'multiline':False]['text':' bias_shape','line_number':1893,'multiline':False]['text':' stride','line_number':1894,'multiline':False]['text':' padding','line_number':1895,'multiline':False]['text':' dilation','line_number':1896,'multiline':False]['text':' groups','line_number':1897,'multiline':False]['text':' input_shape','line_number':1974,'multiline':False]['text':' weight_shape','line_number':1975,'multiline':False]['text':' bias_shape','line_number':1976,'multiline':False]['text':' stride','line_number':1977,'multiline':False]['text':' padding','line_number':1978,'multiline':False]['text':' dilation','line_number':1979,'multiline':False]['text':' groups','line_number':1980,'multiline':False]['text':' input_shape','line_number':1989,'multiline':False]['text':' weight_shape','line_number':1990,'multiline':False]['text':' bias_shape','line_number':1991,'multiline':False]['text':' stride','line_number':1992,'multiline':False]['text':' padding','line_number':1993,'multiline':False]['text':' dilation','line_number':1994,'multiline':False]['text':' groups','line_number':1995,'multiline':False]['text':' The followin 2 tests failed on Meta's CI when all tests are executed.  Output','line_number':1998,'multiline':False]['text':' has lots of nan. Cause unknown.','line_number':1999,'multiline':False]['text':' When this test is run alone (with gtest_filter), it passes.','line_number':2000,'multiline':False]['text':' The test also passes with smaller planes, see "conv2d_pw_prepack_medium".','line_number':2001,'multiline':False]['text':' input_shape','line_number':2004,'multiline':False]['text':' weight_shape','line_number':2005,'multiline':False]['text':' bias_shape','line_number':2006,'multiline':False]['text':' stride','line_number':2007,'multiline':False]['text':' padding','line_number':2008,'multiline':False]['text':' dilation','line_number':2009,'multiline':False]['text':' groups','line_number':2010,'multiline':False]['text':' input_shape','line_number':2015,'multiline':False]['text':' weight_shape','line_number':2016,'multiline':False]['text':' bias_shape','line_number':2017,'multiline':False]['text':' stride','line_number':2018,'multiline':False]['text':' padding','line_number':2019,'multiline':False]['text':' dilation','line_number':2020,'multiline':False]['text':' groups','line_number':2021,'multiline':False]['text':' Arrange','line_number':2025,'multiline':False]['text':'TODO: Support conv_transpose2d with dilation != 1','line_number':2030,'multiline':False]['text':' Act','line_number':2069,'multiline':False]['text':' Assert','line_number':2090,'multiline':False]['text':' input_shape','line_number':2101,'multiline':False]['text':' weight_shape','line_number':2102,'multiline':False]['text':' bias_shape','line_number':2103,'multiline':False]['text':' stride','line_number':2104,'multiline':False]['text':' padding','line_number':2105,'multiline':False]['text':' output_padding','line_number':2106,'multiline':False]['text':' dilation','line_number':2107,'multiline':False]['text':' groups','line_number':2108,'multiline':False]['text':' cpu','line_number':2126,'multiline':False]['text':' vulkan','line_number':2137,'multiline':False]['text':' check','line_number':2150,'multiline':False]['text':' 0 do nothing','line_number':2175,'multiline':False]['text':' 1 frame','line_number':2176,'multiline':False]['text':' not implemented','line_number':2177,'multiline':False]['text':' 2 height','line_number':2179,'multiline':False]['text':' 3 width','line_number':2188,'multiline':False]['text':' Vulkan expand supports input dims <= 4','line_number':2425,'multiline':False]['text':' Vulkan expand supports output_size <= 4','line_number':2429,'multiline':False]['text':' Vulkan expand expects output size >= input','line_number':2433,'multiline':False]['text':' Non-singleton dimensions must match','line_number':2437,'multiline':False]['text':' -1 not allowed in leading, non-existing dimension','line_number':2441,'multiline':False]['text':' 1d->2d','line_number':2449,'multiline':False]['text':' 1d->3d','line_number':2450,'multiline':False]['text':' 1d->4d','line_number':2451,'multiline':False]['text':' W','line_number':2455,'multiline':False]['text':' H','line_number':2456,'multiline':False]['text':' 2d->3d','line_number':2458,'multiline':False]['text':' 2d->4d','line_number':2459,'multiline':False]['text':' W','line_number':2463,'multiline':False]['text':' H','line_number':2464,'multiline':False]['text':' C','line_number':2465,'multiline':False]['text':' 3d->4d','line_number':2467,'multiline':False]['text':' W','line_number':2471,'multiline':False]['text':' H','line_number':2472,'multiline':False]['text':' C','line_number':2473,'multiline':False]['text':' N','line_number':2474,'multiline':False]['text':' expand_as calls into expand, without negative sizes, those tests should be sufficient.','line_number':2478,'multiline':False]['text':' Re-enable once glu_channel shader is fixed','line_number':2615,'multiline':False]['text':' Re-enable once glu_channel shader is fixed','line_number':2620,'multiline':False]['text':' Generate values between -10 and +10','line_number':2657,'multiline':False]['text':' Generate values between -10 and +10','line_number':2670,'multiline':False]['text':' Act: incorrect normalized shape','line_number':2775,'multiline':False]['text':' Act: incorrect weight dimensions','line_number':2786,'multiline':False]['text':' Act: incorrect bias dimensions','line_number':2797,'multiline':False]['text':' Act: input has too many dimensions','line_number':2808,'multiline':False]['text':' Arrange: Vulkan masked_fill expects inputs of dim <= 4','line_number':3191,'multiline':False]['text':' Act','line_number':3198,'multiline':False]['text':' Arrange: Vulkan masked_fill expects mask of dim <= 4','line_number':3208,'multiline':False]['text':' Act','line_number':3215,'multiline':False]['text':' Arrange: shapes of input tensor and mask tensor should be broadcastable','line_number':3225,'multiline':False]['text':' Act','line_number':3232,'multiline':False]['text':' Arrange: value should be a 0-dimensional value tensor or a scalar','line_number':3242,'multiline':False]['text':' Act','line_number':3249,'multiline':False]['text':'*
   * We test masked_fill by considering all possible broadcasting cases of
   * input_shape and mask_shape. The given input_shape and mask_shape are
   * identical, e.g. both are equal to [3, 5, 2, 3]. First we truncate all
   * possible proceeding dimensions of input_shape and mask_shape respectively.
   * Denote the results as curr_input_shape and curr_mask_shape, e.g.
   * curr_input_shape = [5, 2, 3] and curr_mask_shape = [2, 3]. Then for both
   * curr_input_shape and curr_mask_shape we generate all possible subsets of
   * the indices and set the corresponding elements to 1 for each subset. For
   * example, for curr_input_shape = [5, 2, 3], a possible input_idx_subset =
   * [0, 2]. We set the 0th and 2nd elements of curr_input_shape to be 1, then
   * curr_input_shape = [1, 2, 1]. Similarly for curr_mask_shape = [2, 3], a
   * possible mask_idx_subset = [0], then the updated curr_mask_shape = [1, 3].
   * In the end, we test masked_fill with the combinations of curr_input_shape
   * and curr_mask_shape. In the example above, an output tensor of shape [1, 2,
   * 3] will be generated.
   ','line_number':3271,'multiline':True]['text':' truncate input_shape by the proceeding dimensitions','line_number':3292,'multiline':False]['text':' generate all possible subsets of numbers between 0 and input_dim -','line_number':3296,'multiline':False]['text':' input_shape_id - 1 (inclusive)','line_number':3297,'multiline':False]['text':' set the elements at indices of the subset of curr_input_shape to 1','line_number':3307,'multiline':False]['text':' truncate amsk_shape by the proceeding dimensitions','line_number':3315,'multiline':False]['text':' generate all possible subsets of numbers between 0 and mask_dim -','line_number':3319,'multiline':False]['text':' mask_shape_id - 1 (inclusive)','line_number':3320,'multiline':False]['text':' set the elements at indices of the subset of curr_mask_shape to 1','line_number':3330,'multiline':False]['text':' Act: input dimension too large','line_number':3420,'multiline':False]['text':' Act: dimension out of range','line_number':3426,'multiline':False]['text':' Act: dimension out of range','line_number':3432,'multiline':False]['text':' Act: repeated dimensions','line_number':3438,'multiline':False]['text':' Act: repeated dimensions','line_number':3444,'multiline':False]['text':' mismatched dimensions of m1 and m2.','line_number':3599,'multiline':False]['text':' broadcast input','line_number':3840,'multiline':False]['text':' mul4ch','line_number':3844,'multiline':False]['text':' broadcast other','line_number':3847,'multiline':False]['text':' mul4ch','line_number':3852,'multiline':False]['text':' broadcast both','line_number':3854,'multiline':False]['text':' mul4ch','line_number':3859,'multiline':False]['text':' 1d','line_number':3918,'multiline':False]['text':' 2d','line_number':3919,'multiline':False]['text':' 3d','line_number':3920,'multiline':False]['text':' 4d','line_number':3921,'multiline':False]['text':' Make sure inputs are not 0, cannot compare','line_number':3925,'multiline':False]['text':' 1d','line_number':3943,'multiline':False]['text':' 2d','line_number':3944,'multiline':False]['text':' 3d','line_number':3945,'multiline':False]['text':' 4d','line_number':3946,'multiline':False]['text':' 1d','line_number':3967,'multiline':False]['text':' 2d','line_number':3968,'multiline':False]['text':' 3d','line_number':3969,'multiline':False]['text':' 4d','line_number':3970,'multiline':False]['text':' max tolerance is 1.0 due to floor.','line_number':3983,'multiline':False]['text':' may consider adding extra check on number of violation. it should be rare.','line_number':3984,'multiline':False]['text':' max tolerance is 1.0 due to floor.','line_number':4021,'multiline':False]['text':' may consider adding extra check on number of violation. it should be rare.','line_number':4022,'multiline':False]['text':' max tolerance is 1.0 due to floor.','line_number':4065,'multiline':False]['text':' may consider adding extra check on number of violation. it should be rare.','line_number':4066,'multiline':False]['text':' "other" is at least 0.5 to avoid rounding error causes by very small','line_number':4085,'multiline':False]['text':' values.','line_number':4086,'multiline':False]['text':' max tolerance is 1.0 due to floor.','line_number':4096,'multiline':False]['text':' may consider adding extra check on number of violation. it should be rare.','line_number':4097,'multiline':False]['text':' "other" is at least 0.5 to avoid rounding error causes by very small','line_number':4115,'multiline':False]['text':' values.','line_number':4116,'multiline':False]['text':' max tolerance is 1.0 due to floor.','line_number':4126,'multiline':False]['text':' may consider adding extra check on number of violation. it should be rare.','line_number':4127,'multiline':False]['text':' Arrange: Vulkan repeat only supports input of dims <= 4','line_number':4185,'multiline':False]['text':' Act','line_number':4191,'multiline':False]['text':' Arrange: Number of dimensions of repeat dims can not be smaller than','line_number':4197,'multiline':False]['text':' number of dimensions of tensor','line_number':4198,'multiline':False]['text':' Act','line_number':4204,'multiline':False]['text':' Arrange: Vulkan repeat only supports output of dims <= 4','line_number':4210,'multiline':False]['text':' Act','line_number':4216,'multiline':False]['text':' Cast to signed to test negative index for dim','line_number':4463,'multiline':False]['text':' Test on all dim','line_number':4466,'multiline':False]['text':' TODO: Currently the op is not working correctly. Add it back when it is fixed.','line_number':4521,'multiline':False]['text':' Act: input dimension too large','line_number':4821,'multiline':False]['text':' Act: dimension out of range','line_number':4827,'multiline':False]['text':' Act: dimension out of range','line_number':4833,'multiline':False]['text':' Act: repeated dimensions','line_number':4839,'multiline':False]['text':' Act: repeated dimensions','line_number':4845,'multiline':False]['text':' Verify range, also perform a loose check with on histogram distribution.','line_number':5003,'multiline':False]['text':' Very relaxed definition of uniform. Pass if all bins are within 5% of','line_number':5019,'multiline':False]['text':' expected.','line_number':5020,'multiline':False]['text':' verify that the input are still all zeros (not in-place)','line_number':5041,'multiline':False]['text':' Verify the distribution is normal. The difference between given mean vs generated mean should be within 5% of standard deviation, and the same for standard deviation itself.','line_number':5047,'multiline':False]['text':' verify that the input are still all zeros (not in-place)','line_number':5090,'multiline':False]['text':' Test Unary Ops','line_number':5260,'multiline':False]['text':' Need to add a very small constant to avoid 0.','line_number':5359,'multiline':False]['text':' Need to add a very small constant to avoid 0.','line_number':5385,'multiline':False]['text':' inpu.dim() == dim_list.size(), only keepdim == true is supported','line_number':5619,'multiline':False]['text':' inpu.dim() == dim_list.size(), only keepdim == true is supported','line_number':5627,'multiline':False]['text':' Act: only one dimension can be inferred','line_number':5742,'multiline':False]['text':' Act: invalid shape dimension','line_number':5748,'multiline':False]['text':' Act: incompatible shape','line_number':5754,'multiline':False]['text':' Arrange: Vulkan cat inputs must have matching sizes except concatenated dimension','line_number':5762,'multiline':False]['text':' Act','line_number':5768,'multiline':False]['text':' Arrange: Vulkan cat expects 4 dimensional inputs','line_number':5774,'multiline':False]['text':' Act','line_number':5780,'multiline':False]['text':' Arrange','line_number':5788,'multiline':False]['text':' Act','line_number':5793,'multiline':False]['text':' dim=batch','line_number':5795,'multiline':False]['text':' Assert','line_number':5797,'multiline':False]['text':' Arrange','line_number':5807,'multiline':False]['text':' Act','line_number':5812,'multiline':False]['text':' dim=batch','line_number':5814,'multiline':False]['text':' Assert','line_number':5816,'multiline':False]['text':' Arrange: batch x channel (1x1) = single depth texture','line_number':5826,'multiline':False]['text':' Act','line_number':5831,'multiline':False]['text':' dim=batch','line_number':5833,'multiline':False]['text':' Assert','line_number':5835,'multiline':False]['text':' Arrange: single input tensor','line_number':5845,'multiline':False]['text':' Act','line_number':5848,'multiline':False]['text':' dim=batch','line_number':5850,'multiline':False]['text':' Assert','line_number':5852,'multiline':False]['text':' Arrange: two input tensors','line_number':5862,'multiline':False]['text':' Act','line_number':5866,'multiline':False]['text':' dim=batch','line_number':5868,'multiline':False]['text':' Assert','line_number':5870,'multiline':False]['text':' Arrange','line_number':5880,'multiline':False]['text':' Act','line_number':5885,'multiline':False]['text':' Assert','line_number':5889,'multiline':False]['text':' Arrange','line_number':5899,'multiline':False]['text':' Act','line_number':5904,'multiline':False]['text':' Assert','line_number':5908,'multiline':False]['text':' Arrange','line_number':5918,'multiline':False]['text':' Act','line_number':5923,'multiline':False]['text':' Assert','line_number':5927,'multiline':False]['text':' Arrange','line_number':5937,'multiline':False]['text':' Act','line_number':5942,'multiline':False]['text':' Assert','line_number':5946,'multiline':False]['text':' Arrange','line_number':5957,'multiline':False]['text':' Act','line_number':5962,'multiline':False]['text':' dim=feature(channel)','line_number':5964,'multiline':False]['text':' Assert','line_number':5966,'multiline':False]['text':' Arrange','line_number':5976,'multiline':False]['text':' Act','line_number':5981,'multiline':False]['text':' dim=feature(channel)','line_number':5983,'multiline':False]['text':' Assert','line_number':5985,'multiline':False]['text':' Arrange: 2D Texture (VK_IMAGE_VIEW_TYPE_2D)','line_number':5995,'multiline':False]['text':' Act','line_number':6000,'multiline':False]['text':' dim=feature(channel)','line_number':6002,'multiline':False]['text':' Assert','line_number':6004,'multiline':False]['text':' !defined(__APPLE__) ','line_number':6012,'multiline':True]['text':' Arrange: batch x channel (1x1) = single depth texture','line_number':6015,'multiline':False]['text':' Act','line_number':6020,'multiline':False]['text':' dim=feature(channel)','line_number':6022,'multiline':False]['text':' Assert','line_number':6024,'multiline':False]['text':' Arrange: single input tensor','line_number':6034,'multiline':False]['text':' Act','line_number':6037,'multiline':False]['text':' dim=feature(channel)','line_number':6039,'multiline':False]['text':' Assert','line_number':6041,'multiline':False]['text':' Arrange: two input tensors','line_number':6051,'multiline':False]['text':' Act','line_number':6055,'multiline':False]['text':' dim=feature(channel)','line_number':6057,'multiline':False]['text':' Assert','line_number':6059,'multiline':False]['text':' Arrange: batch=1 and channel (a multiple of 4 <-> channel %4 == 0)','line_number':6069,'multiline':False]['text':' Act','line_number':6074,'multiline':False]['text':' dim=feature(channel)','line_number':6076,'multiline':False]['text':' Assert','line_number':6078,'multiline':False]['text':' Arrange: batch=2 and channel (a multiple of 4 <-> channel %4 == 0)','line_number':6088,'multiline':False]['text':' Act','line_number':6093,'multiline':False]['text':' dim=feature(channel)','line_number':6095,'multiline':False]['text':' Assert','line_number':6097,'multiline':False]['text':' Arrange: batch=1 and channel (different multiples of 4 <-> channel %4 == 0)','line_number':6107,'multiline':False]['text':' Act','line_number':6112,'multiline':False]['text':' dim=feature(channel)','line_number':6114,'multiline':False]['text':' Assert','line_number':6116,'multiline':False]['text':' Arrange: batch=1 and channel (a mixed set of multiples and non-multiples of 4)','line_number':6126,'multiline':False]['text':' Act','line_number':6132,'multiline':False]['text':' dim=feature(channel)','line_number':6134,'multiline':False]['text':' Assert','line_number':6136,'multiline':False]['text':' Arrange','line_number':6146,'multiline':False]['text':' Act','line_number':6151,'multiline':False]['text':' Assert','line_number':6155,'multiline':False]['text':' Arrange','line_number':6165,'multiline':False]['text':' Act','line_number':6170,'multiline':False]['text':' Assert','line_number':6174,'multiline':False]['text':' Arrange: batch x channel (1x1) = single depth texture','line_number':6184,'multiline':False]['text':' Act','line_number':6189,'multiline':False]['text':' Assert','line_number':6193,'multiline':False]['text':' Arrange: Vulkan cat inputs must have matching sizes except concatenated dimension','line_number':6203,'multiline':False]['text':' Act','line_number':6209,'multiline':False]['text':' Arrange: Vulkan cat expects inputs of same dimensions','line_number':6215,'multiline':False]['text':' Act','line_number':6221,'multiline':False]['text':' Arrange: Vulkan cat inputs must have matching sizes except concatenated dimension','line_number':6229,'multiline':False]['text':' Act','line_number':6235,'multiline':False]['text':' Arrange: Vulkan cat expects 4 dimensional inputs','line_number':6241,'multiline':False]['text':' Act','line_number':6247,'multiline':False]['text':' Arrange','line_number':6255,'multiline':False]['text':' Act','line_number':6260,'multiline':False]['text':' Assert','line_number':6264,'multiline':False]['text':' Arrange','line_number':6274,'multiline':False]['text':' Act','line_number':6279,'multiline':False]['text':' Assert','line_number':6283,'multiline':False]['text':' Arrange','line_number':6293,'multiline':False]['text':' Act','line_number':6298,'multiline':False]['text':' Assert','line_number':6302,'multiline':False]['text':' Arrange','line_number':6312,'multiline':False]['text':' Act','line_number':6317,'multiline':False]['text':' Assert','line_number':6321,'multiline':False]['text':' Arrange','line_number':6331,'multiline':False]['text':' Act','line_number':6336,'multiline':False]['text':' Assert','line_number':6340,'multiline':False]['text':' Arrange','line_number':6350,'multiline':False]['text':' Act','line_number':6355,'multiline':False]['text':' Assert','line_number':6359,'multiline':False]['text':' Arrange','line_number':6369,'multiline':False]['text':' Act','line_number':6374,'multiline':False]['text':' Assert','line_number':6378,'multiline':False]['text':' Arrange','line_number':6388,'multiline':False]['text':' Act','line_number':6393,'multiline':False]['text':' Assert','line_number':6397,'multiline':False]['text':' Arrange','line_number':6407,'multiline':False]['text':' Act','line_number':6412,'multiline':False]['text':' Assert','line_number':6416,'multiline':False]['text':' Arrange','line_number':6426,'multiline':False]['text':' Act','line_number':6431,'multiline':False]['text':' Assert','line_number':6435,'multiline':False]['text':' Arrange','line_number':6445,'multiline':False]['text':' Act','line_number':6450,'multiline':False]['text':' Assert','line_number':6454,'multiline':False]['text':' Arrange','line_number':6464,'multiline':False]['text':' Act','line_number':6469,'multiline':False]['text':' Assert','line_number':6473,'multiline':False]['text':' Arrange','line_number':6483,'multiline':False]['text':' Act','line_number':6488,'multiline':False]['text':' Assert','line_number':6492,'multiline':False]['text':' Arrange','line_number':6502,'multiline':False]['text':' Act','line_number':6507,'multiline':False]['text':' Assert','line_number':6511,'multiline':False]['text':' Arrange','line_number':6521,'multiline':False]['text':' Act','line_number':6526,'multiline':False]['text':' Assert','line_number':6530,'multiline':False]['text':' Arrange','line_number':6540,'multiline':False]['text':' Act','line_number':6545,'multiline':False]['text':' Assert','line_number':6549,'multiline':False]['text':' Arrange','line_number':6559,'multiline':False]['text':' Act','line_number':6564,'multiline':False]['text':' Assert','line_number':6568,'multiline':False]['text':' Arrange','line_number':6578,'multiline':False]['text':' Act','line_number':6583,'multiline':False]['text':' Assert','line_number':6587,'multiline':False]['text':' Arrange','line_number':6597,'multiline':False]['text':' Act','line_number':6602,'multiline':False]['text':' Assert','line_number':6606,'multiline':False]['text':' Arrange','line_number':6616,'multiline':False]['text':' Act','line_number':6621,'multiline':False]['text':' Assert','line_number':6625,'multiline':False]['text':' Arrange','line_number':6635,'multiline':False]['text':' Act','line_number':6640,'multiline':False]['text':' Assert','line_number':6644,'multiline':False]['text':' Arrange','line_number':6654,'multiline':False]['text':' Act','line_number':6657,'multiline':False]['text':' Assert','line_number':6661,'multiline':False]['text':' Arrange','line_number':6671,'multiline':False]['text':' Act','line_number':6680,'multiline':False]['text':' Assert','line_number':6684,'multiline':False]['text':' Arrange','line_number':6695,'multiline':False]['text':' Act','line_number':6704,'multiline':False]['text':' Assert','line_number':6708,'multiline':False]['text':' Arrange: McLaren Model usage','line_number':6719,'multiline':False]['text':' Act','line_number':6722,'multiline':False]['text':' Assert','line_number':6726,'multiline':False]['text':' Arrange','line_number':6736,'multiline':False]['text':' Act','line_number':6744,'multiline':False]['text':' Assert','line_number':6748,'multiline':False]['text':' Arrange','line_number':6759,'multiline':False]['text':' Act: {-1,-2,-3,0} is equivalent to {3,2,1,0}','line_number':6762,'multiline':False]['text':' Assert','line_number':6766,'multiline':False]['text':' Arrange','line_number':6776,'multiline':False]['text':' Act: Repeated dim','line_number':6779,'multiline':False]['text':' Act: Number of dims don't match','line_number':6789,'multiline':False]['text':' Act: Dim out of range','line_number':6808,'multiline':False]['text':' Act: Input tensor size > 4D','line_number':6818,'multiline':False]['text':' Arrange','line_number':6831,'multiline':False]['text':' 4D tensors with dim=width','line_number':6833,'multiline':False]['text':' 3D tensors with dim=width','line_number':6834,'multiline':False]['text':' 2D tensors with dim=width','line_number':6835,'multiline':False]['text':' 1D tensors with dim=width','line_number':6836,'multiline':False]['text':' Act/Assert','line_number':6839,'multiline':False]['text':' Arrange','line_number':6844,'multiline':False]['text':' 4D tensors with dim=height','line_number':6846,'multiline':False]['text':' 3D tensors with dim=height','line_number':6847,'multiline':False]['text':' 2D tensors with dim=height','line_number':6848,'multiline':False]['text':' 1D tesnors don't have height dim for test','line_number':6849,'multiline':False]['text':' Act/Assert','line_number':6852,'multiline':False]['text':' Arrange','line_number':6857,'multiline':False]['text':' 4D tensors with dim=feature(channel)','line_number':6859,'multiline':False]['text':' 3D tensors with dim=feature(channel)','line_number':6860,'multiline':False]['text':' 1D and 2D tesnors don't have feature(channel) dim for test','line_number':6861,'multiline':False]['text':' Act/Assert','line_number':6864,'multiline':False]['text':' Arrange','line_number':6869,'multiline':False]['text':' 4D tensors with dim=batch','line_number':6871,'multiline':False]['text':' 1D, 2D and 3D tesnors don't have batch dim for test','line_number':6872,'multiline':False]['text':' Act/Assert','line_number':6875,'multiline':False]['text':' When start == end','line_number':6880,'multiline':False]['text':' When start > end','line_number':6882,'multiline':False]['text':' Act: slice step must be positive','line_number':6887,'multiline':False]['text':' Act: Vulkan stack expects at least one tensor','line_number':6894,'multiline':False]['text':' Act: Vulkan stack inputs must have matching sizes','line_number':6899,'multiline':False]['text':' Arrange: Vulkan tile only supports input of dims <= 4','line_number':6959,'multiline':False]['text':' Act','line_number':6965,'multiline':False]['text':' Arrange: Vulkan tile only supports output of dims <= 4','line_number':6973,'multiline':False]['text':' Act','line_number':6979,'multiline':False]['text':' Arrange','line_number':7068,'multiline':False]['text':' 4D tensors with MemoryFormat::Preserve','line_number':7070,'multiline':False]['text':' 4D tensors with MemoryFormat::Contiguous','line_number':7071,'multiline':False]['text':' 4D tensors with null','line_number':7072,'multiline':False]['text':' 3D tensors with MemoryFormat::Preserve','line_number':7073,'multiline':False]['text':' 3D tensors with MemoryFormat::Contiguous','line_number':7074,'multiline':False]['text':' 3D tensors with null','line_number':7075,'multiline':False]['text':' 2D tensors with MemoryFormat::Preserve','line_number':7076,'multiline':False]['text':' 2D tensors with MemoryFormat::Contiguous','line_number':7077,'multiline':False]['text':' 2D tensors with null','line_number':7078,'multiline':False]['text':' 1D tensors with MemoryFormat::Preserve','line_number':7079,'multiline':False]['text':' 1D tensors with MemoryFormat::Contiguous','line_number':7080,'multiline':False]['text':' 1D tensors with null','line_number':7081,'multiline':False]['text':' Act/Assert','line_number':7084,'multiline':False]['text':' Act: Vulkan supports Preserve and Contiguous memory foramts','line_number':7091,'multiline':False]['text':' Act: Vulkan supports Preserve and Contiguous memory foramts','line_number':7096,'multiline':False]['text':' Arrange','line_number':7355,'multiline':False]['text':' input_size','line_number':7356,'multiline':False]['text':' hidden_size','line_number':7357,'multiline':False]['text':' shape (3 * hidden_size, input_size)','line_number':7369,'multiline':False]['text':' shape (3 * hidden_size, hidden_size)','line_number':7370,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7371,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7372,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7384,'multiline':False]['text':' to avoid the following error:','line_number':7385,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7386,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7387,'multiline':False]['text':' Act','line_number':7390,'multiline':False]['text':' weights/biases should be always on CPU.','line_number':7397,'multiline':False]['text':' Assert','line_number':7409,'multiline':False]['text':' Arrange','line_number':7424,'multiline':False]['text':' input_size','line_number':7425,'multiline':False]['text':' hidden_size','line_number':7426,'multiline':False]['text':' shape (3 * hidden_size, input_size)','line_number':7438,'multiline':False]['text':' shape (3 * hidden_size, hidden_size)','line_number':7439,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7440,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7441,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7453,'multiline':False]['text':' to avoid the following error:','line_number':7454,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7455,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7456,'multiline':False]['text':' Act','line_number':7459,'multiline':False]['text':' weights/biases should be always on CPU.','line_number':7464,'multiline':False]['text':' Assert','line_number':7474,'multiline':False]['text':' Arrange','line_number':7489,'multiline':False]['text':' input_size','line_number':7490,'multiline':False]['text':' hidden_size','line_number':7491,'multiline':False]['text':' shape (3 * hidden_size, input_size)','line_number':7503,'multiline':False]['text':' shape (3 * hidden_size, hidden_size)','line_number':7504,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7505,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7506,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7518,'multiline':False]['text':' to avoid the following error:','line_number':7519,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7520,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7521,'multiline':False]['text':' Act: incorrect # of weights/biases','line_number':7524,'multiline':False]['text':' Act: non-3D input tensor','line_number':7531,'multiline':False]['text':' Act: non-3D hidden tensor','line_number':7539,'multiline':False]['text':' Act: has_biases should be true','line_number':7547,'multiline':False]['text':' Act: train should be false','line_number':7554,'multiline':False]['text':' Act: bidirectional should be false','line_number':7561,'multiline':False]['text':' Act: batch_first should be true','line_number':7568,'multiline':False]['text':' Act: dropout should be 0.0','line_number':7575,'multiline':False]['text':' Arrange','line_number':7584,'multiline':False]['text':' input_size','line_number':7585,'multiline':False]['text':' hidden_size','line_number':7586,'multiline':False]['text':' shape (3 * hidden_size, input_size)','line_number':7598,'multiline':False]['text':' shape (3 * hidden_size, hidden_size)','line_number':7599,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7600,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7601,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7613,'multiline':False]['text':' to avoid the following error:','line_number':7614,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7615,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7616,'multiline':False]['text':' Act','line_number':7619,'multiline':False]['text':' Assert','line_number':7640,'multiline':False]['text':' Arrange','line_number':7655,'multiline':False]['text':' input_size','line_number':7656,'multiline':False]['text':' hidden_size','line_number':7657,'multiline':False]['text':' shape (3 * hidden_size, input_size)','line_number':7669,'multiline':False]['text':' shape (3 * hidden_size, hidden_size)','line_number':7670,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7671,'multiline':False]['text':' shape (3 * hidden_size)','line_number':7672,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7684,'multiline':False]['text':' to avoid the following error:','line_number':7685,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7686,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7687,'multiline':False]['text':' Act: incorrect # of weights/biases','line_number':7690,'multiline':False]['text':' Act: non-3D input tensor','line_number':7700,'multiline':False]['text':' Act: non-3D hidden tensor','line_number':7715,'multiline':False]['text':' Act: has_biases should be true','line_number':7730,'multiline':False]['text':' Act: train should be false','line_number':7740,'multiline':False]['text':' Act: bidirectional should be false','line_number':7750,'multiline':False]['text':' Act: batch_first should be true','line_number':7760,'multiline':False]['text':' Act: dropout should be 0.0','line_number':7774,'multiline':False]['text':' Arrange','line_number':7854,'multiline':False]['text':' shape (4 * hidden_size, input_size)','line_number':7869,'multiline':False]['text':' shape (4 * hidden_size, hidden_size)','line_number':7870,'multiline':False]['text':' shape (4 * hidden_size)','line_number':7871,'multiline':False]['text':' shape (4 * hidden_size)','line_number':7872,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7884,'multiline':False]['text':' to avoid the following error:','line_number':7885,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7886,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7887,'multiline':False]['text':' Act','line_number':7890,'multiline':False]['text':' weights/biases should be always on CPU.','line_number':7898,'multiline':False]['text':' Assert','line_number':7913,'multiline':False]['text':' Arrange','line_number':7934,'multiline':False]['text':' shape (4 * hidden_size, input_size)','line_number':7949,'multiline':False]['text':' shape (4 * hidden_size, hidden_size)','line_number':7950,'multiline':False]['text':' shape (4 * hidden_size)','line_number':7951,'multiline':False]['text':' shape (4 * hidden_size)','line_number':7952,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':7964,'multiline':False]['text':' to avoid the following error:','line_number':7965,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':7966,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':7967,'multiline':False]['text':' Act','line_number':7970,'multiline':False]['text':' weights/biases should be always on CPU.','line_number':7976,'multiline':False]['text':' Assert','line_number':7989,'multiline':False]['text':' Arrange','line_number':8010,'multiline':False]['text':' shape (4 * hidden_size, l == 0 ? input_size : hidden_size)','line_number':8025,'multiline':False]['text':' shape (4 * hidden_size, hidden_size)','line_number':8026,'multiline':False]['text':' shape (4 * hidden_size)','line_number':8027,'multiline':False]['text':' shape (4 * hidden_size)','line_number':8028,'multiline':False]['text':' put this guard here to run inference inststead of training','line_number':8040,'multiline':False]['text':' to avoid the following error:','line_number':8041,'multiline':False]['text':'     C++ exception with description "0INTERNAL ASSERT FAILED at "xplat/caffe2/aten/src/ATen/core/boxing/KernelFunction.cpp":31, please report a bug to PyTorch. aten::gru.input has kernels registered to both CompositeImplicitAutograd and a backend mapped to AutogradOther. This makes the backend kernel unreachable; the dispatcher will always prefer the CompositeImplicitAutograd lowering (see Note [Ambiguity in AutogradOther kernel]). If you want to override CompositeImplicitAutograd, please open an issue to request a dedicated Autograd dispatch key for the backend.','line_number':8042,'multiline':False]['text':'     If you only want to run inference instead of training, add `c10::InferenceMode mode;` before model.forward(). Note this guard is only available in C++ but not Python at present.','line_number':8043,'multiline':False]['text':' Act','line_number':8046,'multiline':False]['text':' Assert','line_number':8071,'multiline':False]['text':'
    The most recent shaders should be
    (-12) vulkan.nchw_to_image
    (-11) vulkan.nchw_to_image
    (-10) vulkan.add
    (-9)  vulkan.image_to_nchw

    (-8)  vulkan.nchw_to_image
    (-7)  vulkan.nchw_to_image
    (-6)  vulkan.sub
    (-5)  vulkan.image_to_nchw

    (-4)  vulkan.nchw_to_image
    (-3)  vulkan.nchw_to_image
    (-2)  vulkan.mul
    (-1)  vulkan.image_to_nchw
  ','line_number':8134,'multiline':True]['text':' namespace','line_number':8181,'multiline':False]['text':' USE_VULKAN_API ','line_number':8183,'multiline':True]