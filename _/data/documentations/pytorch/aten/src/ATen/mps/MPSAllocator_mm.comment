['text':'  Copyright Â© 2022 Apple Inc.','line_number':1,'multiline':False]['text':' debug verbosity flags (see DebugVerbosity enum)','line_number':25,'multiline':False]['text':' using a container for pools to simplify iterating over them','line_number':43,'multiline':False]['text':' Pool of large buffers with private storage mode','line_number':44,'multiline':False]['text':' Pool of large buffers with shared storage mode','line_number':47,'multiline':False]['text':' Pool of small buffers with private storage mode','line_number':50,'multiline':False]['text':' Pool of small buffers with shared storage mode','line_number':53,'multiline':False]['text':' Pool of small buffers with shared storage mode used to allocate and copy Scalars','line_number':56,'multiline':False]['text':' from CPU to Metal buffers (see allocScalarBufferWithValue()).','line_number':57,'multiline':False]['text':' no Hazard Tracking required for the Scalar pool (synchronized manually).','line_number':58,'multiline':False]['text':' used for comparison with lower_watermark_ratio','line_number':95,'multiline':False]['text':' we use this to detect if there's memory pressure','line_number':99,'multiline':False]['text':' remove and re-insert heap in the set later after a buffer is created.','line_number':128,'multiline':False]['text':' this ensures updating the order of heaps based on their new available sizes','line_number':129,'multiline':False]['text':' this will cause releasing pool buffers to free up memory','line_number':142,'multiline':False]['text':' this should never happen as the backing memory (i.e., heap) was allocated successfully.','line_number':147,'multiline':False]['text':' insert heap after a buffer was created on it to update the order of heap's set','line_number':149,'multiline':False]['text':' this helps to monitor "implicit" allocations from MPS backend and to prevent OOM and system failure.','line_number':170,'multiline':False]['text':' track buffer reuse intervals only on large pool when low watermark limit is enabled.','line_number':175,'multiline':False]['text':' the logic in here is simple: keep reusing existing heaps capacity as long as possible (by splitting','line_number':185,'multiline':False]['text':' or releasing oversize buffers, if required), and avoid 'new' heap allocations as much as possible.','line_number':186,'multiline':False]['text':' return the existing buffer if it already fits the requested size (i.e., not oversize)','line_number':188,'multiline':False]['text':' if there's an 'existing' heap with enough capacity, then don't','line_number':192,'multiline':False]['text':' return the oversize buffer and sub-allocate from that existing heap.','line_number':193,'multiline':False]['text':' otherwise if buffer is releasable immediately, we make room by releasing the','line_number':197,'multiline':False]['text':' buffer and reuse the new space within its heap container for the new smaller buffer allocation','line_number':198,'multiline':False]['text':' this will skip unnecessary garbage collection as we'll reuse the newly released space','line_number':200,'multiline':False]['text':' the oversized buffer is busy and not reusable at the moment. So release it (and potentially its heap','line_number':203,'multiline':False]['text':' container) in allocator, and ARC will later free up its backing memory when the busy command buffer finishes.','line_number':204,'multiline':False]['text':' only if there's no memory pressure, we'll reuse the oversized buffer','line_number':207,'multiline':False]['text':' this will make allocator to allocate a new buffer','line_number':214,'multiline':False]['text':' we care about memory pressure if only we're allocating large buffers when the','line_number':239,'multiline':False]['text':' low watermark limit has been reached','line_number':240,'multiline':False]['text':' first, try to get a block from the existing pool.','line_number':244,'multiline':False]['text':' do garbage collection if memory pressure is high and there's enough memory in pool','line_number':247,'multiline':False]['text':' Attempt allocate','line_number':253,'multiline':False]['text':' Callbacks might release more memory (eg. by forcing a GC in the host language) thus','line_number':255,'multiline':False]['text':' we can retry getting a free buffer in the pool, before trying to alloc again.','line_number':256,'multiline':False]['text':' Free enough available cached blocks to satisfy alloc and retry alloc.','line_number':259,'multiline':False]['text':' Free all cached buffers and retry alloc.','line_number':261,'multiline':False]['text':' the OOM could be triggered if:','line_number':267,'multiline':False]['text':'   1- the High Watermark limit has been reached (if enabled)','line_number':268,'multiline':False]['text':'   2- ran out of device memory, or the memory fragmentation is so high that a contiguous','line_number':269,'multiline':False]['text':'      chunk of requested size couldn't be found.','line_number':270,'multiline':False]['text':' Makes sure the BufferBlock* isn't already present in the pool we're freeing it back into.','line_number':310,'multiline':False]['text':' reset shape','line_number':313,'multiline':False]['text':' returns the MPSEvent back to MPSEventPool','line_number':317,'multiline':False]['text':' will re-insert later to keep the heaps list sorted based on heap's new available size (if heap not empty)','line_number':339,'multiline':False]['text':' if heap wasn't released and its released buffer is still busy in command buffer, the available','line_number':365,'multiline':False]['text':' size of the heap cannot be updated and we should defer updating until command buffer finishes.','line_number':366,'multiline':False]['text':' check if the heap block still exists','line_number':372,'multiline':False]['text':' before releasing the buffers make sure the command buffer has finished.','line_number':440,'multiline':False]['text':' we need to release the lock temporarily as synchronizing may cause deadlock with completion handlers.','line_number':441,'multiline':False]['text':' Free all cached blocks to system allocator','line_number':448,'multiline':False]['text':' skip garbage collection if memory pressure has already relieved','line_number':457,'multiline':False]['text':' attempt to collect garbage until we reach below low watermark limit','line_number':461,'multiline':False]['text':' calculate the total age of the free-able blocks. We'll use it later to get the average age threshold.','line_number':464,'multiline':False]['text':' repeat GC until we reach reclaim > target size.','line_number':478,'multiline':False]['text':' free blocks exceeding this age threshold first.','line_number':481,'multiline':False]['text':' stop iteration if we can no longer free a block.','line_number':483,'multiline':False]['text':' free blocks of > avg age. Stop garbage collection if we reach below the','line_number':485,'multiline':False]['text':' low watermark limit since re-allocation or fragmentation could be costly.','line_number':486,'multiline':False]['text':' public interface to MPSAllocator','line_number':508,'multiline':False]['text':' it's OK for the buffer_block to not exist yet','line_number':520,'multiline':False]['text':' buffer is out of the pool, so no mutex lock is needed','line_number':537,'multiline':False]['text':' return if buffer was not allocated on MPSAllocator or isn't a Shared buffer','line_number':546,'multiline':False]['text':' return if buffer was not allocated on MPSAllocator or isn't a Shared buffer','line_number':562,'multiline':False]['text':'needsLock','line_number':568,'multiline':True]['text':' wait on event if "shared" buffer was allocated on MPSAllocator and','line_number':581,'multiline':False]['text':' or actually needs waiting (based on retainCount)','line_number':582,'multiline':False]['text':' check for retain count again as the previous wait might have released the buffer','line_number':592,'multiline':False]['text':' after waiting, it's a good time to free some pending inactive buffers','line_number':596,'multiline':False]['text':' even if one of the buffers weren't recorded beforehand, we return','line_number':600,'multiline':False]['text':' without continuing with other buffers since retainCount > 1','line_number':601,'multiline':False]['text':' -1 indicates the passed buffer pointer wasn't found','line_number':624,'multiline':False]['text':' note that the IntArrayRef doesn't own the underlying data, and the backing','line_number':633,'multiline':False]['text':' memory for shape data must persist as long as the buffer is in use.','line_number':634,'multiline':False]['text':' So we need to copy to vector.','line_number':635,'multiline':False]['text':' we sync the scalar pool manually with completion handler at the time buffer is','line_number':662,'multiline':False]['text':' freed when the MPSScalar instance goes our of scope','line_number':663,'multiline':False]['text':' check if low watermark limit is disabled','line_number':695,'multiline':False]['text':' current_allocated_size could exceed m_low_watermark_limit (e.g., when swapping to disk)','line_number':699,'multiline':False]['text':' namespace HeapAllocator','line_number':719,'multiline':False]['text':' Use "at::mps::GetMPSAllocator()" to acquire a handle to MPS Allocator','line_number':721,'multiline':False]['text':' MPS allocator struct to be registered with Pytorch','line_number':729,'multiline':False]['text':' implementation of IMPSAllocator interface','line_number':756,'multiline':False]['text':' anonymous namespace','line_number':843,'multiline':False]['text':' namespace at::mps','line_number':856,'multiline':False]['text':' torch.is_pinned() implementation','line_number':860,'multiline':False]['text':' Pinned memory will be helpful on Apple Silicon Macs with Unified memory as we','line_number':861,'multiline':False]['text':' will be able to use SharedStorageMode for MTLBuffer allocations. This will','line_number':862,'multiline':False]['text':' avoid extra copies on DataLoading operations.','line_number':863,'multiline':False]['text':' torch.pin_memory() implementation','line_number':869,'multiline':False]['text':' namespace at::native','line_number':883,'multiline':False]