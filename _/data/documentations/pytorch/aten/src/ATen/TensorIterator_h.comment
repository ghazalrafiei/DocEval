['text':' namespace at','line_number':21,'multiline':False]['text':' TensorIterator is a helper class for element-wise operations, such as','line_number':23,'multiline':False]['text':' arithmetic, comparisons, and trigonometric functions. It handles','line_number':24,'multiline':False]['text':' broadcasting and type conversions of operands.','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' This is inspired by NumPy's Array Iterator API (NpyIter).','line_number':27,'multiline':False]['text':'','line_number':28,'multiline':False]['text':' The files Loops.h and Loops.cuh provide functions to build kernels that','line_number':29,'multiline':False]['text':' use TensorIterator.','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' Example:','line_number':32,'multiline':False]['text':'','line_number':33,'multiline':False]['text':'   auto iter = TensorIteratorConfig()','line_number':34,'multiline':False]['text':'     .add_output(output)','line_number':35,'multiline':False]['text':'     .add_input(input)','line_number':36,'multiline':False]['text':'     .build()','line_number':37,'multiline':False]['text':'','line_number':38,'multiline':False]['text':' [MyKernel.cpp / MyKernel.cu]','line_number':39,'multiline':False]['text':'   cpu_kernel(iter, [](float a, float b) {','line_number':40,'multiline':False]['text':'     return a + b;','line_number':41,'multiline':False]['text':'   });','line_number':42,'multiline':False]['text':'','line_number':43,'multiline':False]['text':'   gpu_kernel(iter, []GPU_LAMBDA(float a, float b) -> float {','line_number':44,'multiline':False]['text':'     return a + b;','line_number':45,'multiline':False]['text':'   });','line_number':46,'multiline':False]['text':'','line_number':47,'multiline':False]['text':' Note [Order of Construction]','line_number':48,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':49,'multiline':False]['text':' When setting up the tensor iterator configuration, the output Tensors','line_number':50,'multiline':False]['text':' have to be added first via','line_number':51,'multiline':False]['text':' TensorIteratorConfig::add_owned_output(at::Tensor). After adding all outputs,','line_number':52,'multiline':False]['text':' the inputs can be added via','line_number':53,'multiline':False]['text':' TensorIteratorConfig::add_owned_input(at::Tensor).','line_number':54,'multiline':False]['text':' Adding another output after inputs have been added will rise an exception.','line_number':55,'multiline':False]['text':'','line_number':56,'multiline':False]['text':' Note [Common Dtype Computation]','line_number':57,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':58,'multiline':False]['text':' Some operations have a natural notion of a "common dtype" or','line_number':59,'multiline':False]['text':'   "computation dtype" where all inputs are cast to one dtype, the','line_number':60,'multiline':False]['text':'   operation is performed, and then the results are cast to all outputs.','line_number':61,'multiline':False]['text':'','line_number':62,'multiline':False]['text':' TensorIterator infers a common dtype if all inputs have the same dtype,','line_number':63,'multiline':False]['text':'   and it computes one using type promotion rules on its inputs if','line_number':64,'multiline':False]['text':'   promote_inputs_to_common_dtype_ is true. Attempting to query','line_number':65,'multiline':False]['text':'   a common dtype otherwise will throw an exception.','line_number':66,'multiline':False]['text':'','line_number':67,'multiline':False]['text':' Note that the outputs are not considered when computing a common dtype.','line_number':68,'multiline':False]['text':' This parameter is heuristically chosen to determine the minimum number of','line_number':73,'multiline':False]['text':' work that warrants parallelism. For example, when summing an array, it is','line_number':74,'multiline':False]['text':' deemed inefficient to parallelise over arrays shorter than 32768. Further,','line_number':75,'multiline':False]['text':' no parallel algorithm (such as parallel_reduce) should split work into','line_number':76,'multiline':False]['text':' smaller than GRAIN_SIZE chunks.','line_number':77,'multiline':False]['text':' Storage for a non-owning Tensor, without needing to include Tensor.h','line_number':80,'multiline':False]['text':' namespace internal','line_number':115,'multiline':False]['text':'/ The data pointer. This may be different from tensor->data_ptr() if the','line_number':136,'multiline':False]['text':'/ iterator is split.','line_number':137,'multiline':False]['text':'/ Stride after broadcasting. The stride is in bytes, not number of elements.','line_number':140,'multiline':False]['text':'/ The desired device and type for the operand. For inputs, this specifies','line_number':143,'multiline':False]['text':'/ that the input should be converted to this type if necessary. For outputs,','line_number':144,'multiline':False]['text':'/ this specifies which type to allocate. target_dtype and device are','line_number':145,'multiline':False]['text':'/ initialized with the dtype and device of the tensor but during type','line_number':146,'multiline':False]['text':'/ promotion target_dtype value can become different from tensor's dtype','line_number':147,'multiline':False]['text':'/ also, during type promotion target_dtype and device can be set for an','line_number':148,'multiline':False]['text':'/ undefined tensor so that tensor can be properly constructed later.','line_number':149,'multiline':False]['text':' Caches dtype of the tensor, because scalar_type is an expensive operation','line_number':152,'multiline':False]['text':' If dtype of the tensor is changed (e.g. as a result of type promotion or in','line_number':153,'multiline':False]['text':' allocate_outputs), this','line_number':154,'multiline':False]['text':' value should be changed too.','line_number':155,'multiline':False]['text':'/ The tensor operand. Note that the strides, data pointer, and','line_number':181,'multiline':False]['text':'/ other attributes may differ due to dimension reordering and','line_number':182,'multiline':False]['text':'/ coalescing.','line_number':183,'multiline':False]['text':' Save the original tensor operand in cases when an output is modified','line_number':192,'multiline':False]['text':' (e.g. if dtype is changed)','line_number':193,'multiline':False]['text':' Set tensor to a new value, and store the old tensor value in','line_number':201,'multiline':False]['text':' original_tensor Should only ever be called once for the lifetime of an','line_number':202,'multiline':False]['text':' operand','line_number':203,'multiline':False]['text':' Move original_tensor back into tensor, exchange_tensor must have been','line_number':206,'multiline':False]['text':' called before','line_number':207,'multiline':False]['text':' We store TensorBase visibly in the header to allow inline access.','line_number':215,'multiline':False]['text':' However, we sometimes need a genuine `const Tensor &` for the','line_number':216,'multiline':False]['text':' TensorIterator API. So, we also store a non-owning `Tensor`','line_number':217,'multiline':False]['text':' object in these `_storage_` variables.','line_number':218,'multiline':False]['text':' The inner-loop function operates on the fastest moving dimension. It','line_number':243,'multiline':False]['text':' implements element-wise operations in terms of 1-d strided tensors.','line_number':244,'multiline':False]['text':'','line_number':245,'multiline':False]['text':' Arguments:','line_number':246,'multiline':False]['text':'  data: data pointers for each operand (length `ntensors`)','line_number':247,'multiline':False]['text':'  strides: stride for each operand (length `ntensors`)','line_number':248,'multiline':False]['text':'  size: size of inner loop','line_number':249,'multiline':False]['text':'','line_number':250,'multiline':False]['text':' The `size` often matches shape[0], but may be smaller due to','line_number':251,'multiline':False]['text':' parallelization of the inner loop.','line_number':252,'multiline':False]['text':'/ number of elements in the output operand. this is the same as numel() for','line_number':280,'multiline':False]['text':'/ operations that are not reductions.','line_number':281,'multiline':False]['text':'/ number of reduced dimensions in a reduction operation','line_number':284,'multiline':False]['text':'/ 1-dimensional iteration and no buffering or type conversion','line_number':287,'multiline':False]['text':'/ Reducible to 1-dimensional and all operands are contiguous','line_number':289,'multiline':False]['text':'/ Accessors for each operand','line_number':293,'multiline':False]['text':' Copies from temporary outputs back to the original outputs','line_number':348,'multiline':False]['text':' NOTE: only used on CPU','line_number':349,'multiline':False]['text':'/ Removes an operand from this iterator','line_number':352,'multiline':False]['text':'/ Shrinks an iterated dimension','line_number':354,'multiline':False]['text':'/ Narrows every dim after and including `start_dim` to size one.','line_number':356,'multiline':False]['text':'/ Replaces the data pointer for the operand at index `arg`.','line_number':358,'multiline':False]['text':'/ The new pointer should have the same sizes, strides and dtype as the','line_number':359,'multiline':False]['text':'/ original','line_number':360,'multiline':False]['text':'/ Splits this TensorIterator into two iterators. Together they iterate over','line_number':363,'multiline':False]['text':'/ the entire operation. Used by `with_32bit_indexing()`.','line_number':364,'multiline':False]['text':'/ Returns the dimension with the largest extent: (size[dim]-1) * stride[dim]','line_number':367,'multiline':False]['text':'/ Return scalar value from original_tensor_base if it is defined. When','line_number':376,'multiline':False]['text':'/ common_dtype is Half, casting scalar input to common_dtype might overflow.','line_number':377,'multiline':False]['text':'/ If the scalar is aleady given in the type of Half, then return scalar','line_number':378,'multiline':False]['text':'/ value from tensor_base.','line_number':379,'multiline':False]['text':'/ Create a strides array for a Tensor with shape of this iterator. The','line_number':443,'multiline':False]['text':'/ parameter `element_size` specifies the size of Tensor's data type in','line_number':444,'multiline':False]['text':'/ bytes (e.g. `4` for `float`)','line_number':445,'multiline':False]['text':'/ Inverts the re-ordering done by reorder_dimensions. This can only be','line_number':448,'multiline':False]['text':'/ called *before* coalesce_dimensions() is called.','line_number':449,'multiline':False]['text':'/ Reapply same re-ordering as it is done by reorder_dimensions. This can','line_number':452,'multiline':False]['text':'/ only be called *before* coalesce_dimensions() is called.','line_number':453,'multiline':False]['text':'/ Helper functions for CPU iteration','line_number':456,'multiline':False]['text':' Helper functions for advanced stride manipulations (e.g. torch.flip)','line_number':464,'multiline':False]['text':'/ true if the stride computation can use 32-bit arithmetic. Used by GPU','line_number':472,'multiline':False]['text':'/ kernels','line_number':473,'multiline':False]['text':'/ An "iteratable" object that recursively splits this iterator into','line_number':476,'multiline':False]['text':'/ sub-iterators that can use 32-bit indexing.','line_number':477,'multiline':False]['text':'/ If the kernel should accumulate into the output. Only relevant for CUDA','line_number':480,'multiline':False]['text':'/ reductions.','line_number':481,'multiline':False]['text':'/ Whether this iterator produces the actual output,','line_number':486,'multiline':False]['text':'/ as opposed to something that will be accumulated further. Only relevant','line_number':487,'multiline':False]['text':'/ for CUDA reductions.','line_number':488,'multiline':False]['text':' Odd special case needed for pow. Has to borrow the output because','line_number':557,'multiline':False]['text':' it's a structured kernel, but the argument is potentially a copy.','line_number':558,'multiline':False]['text':' Another special case: we need to own the second argument for comparison','line_number':577,'multiline':False]['text':' ops.','line_number':578,'multiline':False]['text':' Mutable reference as it moves tensors out of TensorIteratorConfig','line_number':591,'multiline':False]['text':'/ Records the "computation" shape of the output tensor. The computation','line_number':610,'multiline':False]['text':'/ shape is different from the regular shape in a few ways:','line_number':611,'multiline':False]['text':'/','line_number':612,'multiline':False]['text':'/   - The shape may be permuted (via permute_dimensions) so that we','line_number':613,'multiline':False]['text':'/     process the dimensions in the most computationally efficient order','line_number':614,'multiline':False]['text':'/     (rather than the logical order given to us by the users.)','line_number':615,'multiline':False]['text':'/   - The shape may have adjacent dimensions collapsed (via','line_number':616,'multiline':False]['text':'/     coalesce_dimensions) so that we minimize the number of','line_number':617,'multiline':False]['text':'/     dimensions we have to explicitly iterate over.  For example,','line_number':618,'multiline':False]['text':'/     a pointwise operation on a contiguous tensor "computationally"','line_number':619,'multiline':False]['text':'/     consists of only a single dimension.','line_number':620,'multiline':False]['text':'/','line_number':621,'multiline':False]['text':'/ In other words, the computation shape is the output shape as it','line_number':622,'multiline':False]['text':'/ actually matters for implementing the kernel, but not necessarily the','line_number':623,'multiline':False]['text':'/ output shape that the user will see in the end.','line_number':624,'multiline':False]['text':'/','line_number':625,'multiline':False]['text':'/ The lifecycle of mutations to shape_ in TensorIterator:','line_number':626,'multiline':False]['text':'/   - declare_static_shape() sets an initial shape explicitly','line_number':627,'multiline':False]['text':'/     provided by user, otherwise','line_number':628,'multiline':False]['text':'/   - compute_shape() computes the true (non-computational) shape','line_number':629,'multiline':False]['text':'/     specified by the user.','line_number':630,'multiline':False]['text':'/   - reorder_dimensions() reorders dimensions to improve coalescing.','line_number':631,'multiline':False]['text':'/   - coalesce_dimensions() then coalesces adjacent dimensions when','line_number':632,'multiline':False]['text':'/     possible.','line_number':633,'multiline':False]['text':'/','line_number':634,'multiline':False]['text':'/ The shape may also be further modified if we create sub-TensorIterators,','line_number':635,'multiline':False]['text':'/ e.g., via narrow or select_all_keeping_dim.','line_number':636,'multiline':False]['text':'/ Temporarily records the permutation computed by reorder_dimensions.','line_number':639,'multiline':False]['text':'/ This permutation maps the computation output dimension (dim) to','line_number':640,'multiline':False]['text':'/ the original true output dimension (perm_[dim]).  It is used by','line_number':641,'multiline':False]['text':'/ invert_perm to undo the permutation.  After coalesce_dimensions is','line_number':642,'multiline':False]['text':'/ called, the permutation is no longer valid (as, in general, there','line_number':643,'multiline':False]['text':'/ is no permutation that will make computation dimensions to','line_number':644,'multiline':False]['text':'/ output dimensions); methods that manipulate perm_ are obligated','line_number':645,'multiline':False]['text':'/ to test that !has_coalesced_dimensions','line_number':646,'multiline':False]['text':'/ Has coalesce_dimensions() (or any moral equivalent, e.g., fast_build())','line_number':649,'multiline':False]['text':'/ been called?  This is SOLELY used to check validity of perm_.','line_number':650,'multiline':False]['text':'/ Whether iteration must be fixed. This disables dimension permuting and','line_number':653,'multiline':False]['text':'/ also changes how for_each divides work among threads.','line_number':654,'multiline':False]['text':'/ The index offsets into the original tensors for each dimension.','line_number':657,'multiline':False]['text':'/ This is only non-zero when you narrow() a TensorIterator (e.g.,','line_number':658,'multiline':False]['text':'/ when you make sub-TensorIterators).','line_number':659,'multiline':False]['text':'/ The computed names of the output tensor.  Computed by compute_names()','line_number':662,'multiline':False]['text':'/ The operands of the TensorIterator: both the inputs and outputs.  The','line_number':665,'multiline':False]['text':'/ outputs MUST come first in the operands_ list.  There is always an','line_number':666,'multiline':False]['text':'/ operand for each output of the TensorIterator, even if TensorIterator','line_number':667,'multiline':False]['text':'/ will ultimately be responsible for allocating the output; in those','line_number':668,'multiline':False]['text':'/ cases, tensor is simply undefined (and will be populated later','line_number':669,'multiline':False]['text':'/ during build()).','line_number':670,'multiline':False]['text':'/','line_number':671,'multiline':False]['text':'/ This list is initially populated prior to build(), but build() mutates','line_number':672,'multiline':False]['text':'/ OperandInfo to populate more information.','line_number':673,'multiline':False]['text':'/ Number of outputs in operands_ (the length of the outputs prefix','line_number':676,'multiline':False]['text':'/ in operands_).','line_number':677,'multiline':False]['text':'/ Whether or not all operands have the same shape and are 1d+. Having all','line_number':680,'multiline':False]['text':'/ the same shape affects whether or not the iterator is eligible for fast','line_number':681,'multiline':False]['text':'/ setup.','line_number':682,'multiline':False]['text':'/ Whether or not all operands are 0d, this affects type promotion','line_number':684,'multiline':False]['text':'/ The "computation" dtype of TensorIterator, specifying what the dtype','line_number':687,'multiline':False]['text':'/ we will do the internal computation in TensorIterator.  Typically,','line_number':688,'multiline':False]['text':'/ this matches the dtype of the output tensors, but not always!','line_number':689,'multiline':False]['text':'/ This is currently defined as kCPU, or the device of the first non-CPU','line_number':692,'multiline':False]['text':'/ tensor argument. See TensorIteratorBase::compute_types for details.','line_number':693,'multiline':False]['text':'/ Set by split(), see should_accumulate() and is_final_output()','line_number':696,'multiline':False]['text':' From TensorIteratorConfig','line_number':700,'multiline':False]['text':'/ Set by populate_operands(), says if we're handling meta tensors','line_number':703,'multiline':False]['text':' Slicing is OK, TensorIterator guaranteed NOT to have any fields','line_number':709,'multiline':False]['text':'/ Construction','line_number':763,'multiline':False]['text':' Stores input/output Tensors without incrementing the reference count.','line_number':764,'multiline':False]['text':' Important: the outputs have to be added before the inputs.','line_number':765,'multiline':False]['text':' Borrowing from temporaries is unlikely to go well.','line_number':773,'multiline':False]['text':' Stores input/output Tensors while incrementing the reference count.','line_number':777,'multiline':False]['text':' Note that add_{in,out}put are nearly always what you','line_number':778,'multiline':False]['text':' want, and the exception (adding an unnamed temporary) won't','line_number':779,'multiline':False]['text':' compile.','line_number':780,'multiline':False]['text':' Advanced API: stores input/output Tensors without incrementing','line_number':784,'multiline':False]['text':' the reference count. The caller must ensure that these Tensors','line_number':785,'multiline':False]['text':' live at least as long as this TensorIteratorConfig and any','line_number':786,'multiline':False]['text':' TensorIteratorBase built from this TensorIteratorConfig.','line_number':787,'multiline':False]['text':' Important: the outputs have to be added before the inputs.','line_number':788,'multiline':False]['text':' Borrowing from temporaries is unlikely to go well.','line_number':792,'multiline':False]['text':' Sets the check_mem_overlap_ flag, which is true by default.','line_number':796,'multiline':False]['text':' If true, inputs are checked for partial overlap with the outputs and','line_number':797,'multiline':False]['text':' outputs are checked for internal overlap (e.g. broadcasted views). An error','line_number':798,'multiline':False]['text':' is raised if unacceptable overlap is detected.','line_number':799,'multiline':False]['text':' If you're migrating an existing operator to using TensorIterator, please','line_number':800,'multiline':False]['text':' consider if the previous implementation checked memory overlap. If it did','line_number':801,'multiline':False]['text':' not, and if the operator is idempotent (for example, Tensor.fill_(0)), then','line_number':802,'multiline':False]['text':' checking memory overlap is BC-breaking. Please don't check memory overlap','line_number':803,'multiline':False]['text':' in that case.','line_number':804,'multiline':False]['text':' Sets the check_all_same_dtype_ flag, which is true by default','line_number':810,'multiline':False]['text':' If true, checks that all inputs and defined outputs have the same dtype','line_number':811,'multiline':False]['text':' Setting either of promote_inputs_to_common_dtype_','line_number':812,'multiline':False]['text':'   or cast_common_dtype_to_outputs_ to true will set','line_number':813,'multiline':False]['text':'   check_all_same_dtype_ to false.','line_number':814,'multiline':False]['text':' Sets the check_all_same_device_ flag, which is true by default','line_number':820,'multiline':False]['text':' If true, all operands must be on the same device, with the possible','line_number':821,'multiline':False]['text':'   exception of CPU scalars, which can be passed to some CUDA kernels','line_number':822,'multiline':False]['text':'   as kernel arguments.','line_number':823,'multiline':False]['text':' Sets the enforce_safe_casting_to_output_ flag, which is false by default','line_number':830,'multiline':False]['text':' If true, the iterator's "common dtype" must be computable','line_number':831,'multiline':False]['text':'   (see the [Common Dtype Computation] note) and','line_number':832,'multiline':False]['text':'   canCast(common dtype, output dtype) must be true for all outputs.','line_number':833,'multiline':False]['text':' Sets the enforce_linear_iteration_ flag, which is false by default.','line_number':840,'multiline':False]['text':' If true, iteration goes in the same order as a C-contiguous tensor','line_number':841,'multiline':False]['text':' is layed out in memory. i.e. last dimension iterates fastest.','line_number':842,'multiline':False]['text':'','line_number':843,'multiline':False]['text':' This iteration order can be less efficient and may even prevent','line_number':844,'multiline':False]['text':' vectorization. So only use if the correctness of your kernel depends on it.','line_number':845,'multiline':False]['text':' Sets the promote_inputs_to_common_dtype_ flag, which is false by default','line_number':852,'multiline':False]['text':' If true, the iterator's "common dtype" is always computed (see the','line_number':853,'multiline':False]['text':'   [Common Dtype Computation] note) and, on the CPU, temporary copies of','line_number':854,'multiline':False]['text':'   the inputs in the common dtype are passed as the actual inputs to','line_number':855,'multiline':False]['text':'   the operation.','line_number':856,'multiline':False]['text':' Setting this flag to true sets check_all_same_dtype_ to false.','line_number':857,'multiline':False]['text':' Sets the promote_integer_inputs_to_float_ flag, which is false by default','line_number':867,'multiline':False]['text':' NOTE: If set to true, the promote_inputs_to_common_dtype_ must also be','line_number':868,'multiline':False]['text':' true. If true, if the iterator's "common dtype" is an integral type','line_number':869,'multiline':False]['text':' (including bool)','line_number':870,'multiline':False]['text':'   then it is changed to the default float scalar type.','line_number':871,'multiline':False]['text':' Sets the cast_common_dtype_to_outputs_ flag, which is false by default','line_number':890,'multiline':False]['text':' If true, the iterator's "common dtype" must be computatable','line_number':891,'multiline':False]['text':'   (see the [Common Dtype Computation] note) and, on the CPU, temporary','line_number':892,'multiline':False]['text':'   copies of the outputs are passed as the actual output to the operation.','line_number':893,'multiline':False]['text':'   These temporaries are then copied to the original outputs after','line_number':894,'multiline':False]['text':'   the operation is performed (see cast_outputs()).','line_number':895,'multiline':False]['text':' Setting this flag to true sets check_all_same_dtype_ to false.','line_number':896,'multiline':False]['text':' Bypass output dtype/device computation and fix the dtype/device as','line_number':911,'multiline':False]['text':' specified here.','line_number':912,'multiline':False]['text':' It would be better if this was && qualified, but this would be at the cost','line_number':923,'multiline':False]['text':' of a lot of boilerplate above','line_number':924,'multiline':False]['text':'/ A container-like struct that acts as if it contains splits of a','line_number':952,'multiline':False]['text':'/ TensorIterator that can use 32-bit indexing. Taken together the splits cover','line_number':953,'multiline':False]['text':'/ the original TensorIterator.','line_number':954,'multiline':False]['text':' Guaranteed to be a TensorIterator proper!','line_number':961,'multiline':False]['text':' two iterators are equal if they are the same object or they're both','line_number':965,'multiline':False]['text':' empty','line_number':966,'multiline':False]['text':' needed for C++11 range-based for loop','line_number':969,'multiline':False]['text':'/ stack of TensorIterators to be split','line_number':974,'multiline':False]['text':' namespace at','line_number':987,'multiline':False]