['text':' uuid count starts at 1. 0 is reserved to mean "wasn't set by graph_pool_handle".','line_number':18,'multiline':False]['text':' Sets just the second value, to distinguish it from MempoolId_ts created from','line_number':20,'multiline':False]['text':' cudaStreamGetCaptureInfo id_s in capture_begin.','line_number':21,'multiline':False]['text':' Get the expected id of a capture sequence so that we can call beginAllocateStreamToPool','line_number':30,'multiline':False]['text':' before starting a graph capture','line_number':31,'multiline':False]['text':' id starts at 1:','line_number':33,'multiline':False]['text':' Ensures uuid count starts at 1. 0 is reserved to mean "not set by cudaStreamGetCaptureInfo".','line_number':34,'multiline':False]['text':' (But how do we know GetCaptureInfo never sets id_ to 0? Because that's the current behavior,','line_number':35,'multiline':False]['text':' and I asked cuda devs to keep it that way, and they agreed.)','line_number':36,'multiline':False]['text':'*
 * Note [CUDA Graph Wrapper Class]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Q: Why do we need graph capture and launch bindings in Pytorch?
 *    Why can't they live in a user extension, for example?
 *
 * A1: Convenience.
 * A2: To ensure valid numerics on replay, some native CUDA ops (like RNG ops with
 *     CPU statefulness) need cooperation from the capture and replay bindings
 *     (see Note [CUDA Graph-safe RNG states] in CUDAGeneratorImpl.h).
 *
 *     We can't expect users to know about this cooperation.  If users write capture
 *     bindings naively in an extension, they likely won't interact with the native
 *     ops properly.  Their graphs would yield invalid numerics on replay.
 ','line_number':41,'multiline':True]['text':'*
 * Note [Interaction with CUDA graph capture] in CUDACachingAllocator.cpp
 * describes memory management for captures.
 ','line_number':57,'multiline':True]['text':' Track any outstanding event queries that could happen e.g., in a NCCL watchdog so that they','line_number':64,'multiline':False]['text':' can be resolved before the capture begins. Note that event queries are not allowed during a','line_number':65,'multiline':False]['text':' graph capture in the default capture mode.','line_number':66,'multiline':False]['text':' CUDAStreams may not be default-constructed.','line_number':82,'multiline':False]['text':'=0','line_number':89,'multiline':True]['text':' For now, a CUDAGraph instance only accommodates the default generator on the device that's','line_number':95,'multiline':False]['text':' current when capture begins. If any op in the captured region uses a non-default generator,','line_number':96,'multiline':False]['text':' or a generator on another device, the offending generator will throw an error.','line_number':97,'multiline':False]['text':' These restrictions simplify CUDAGraph, but could be relaxed in the future:','line_number':98,'multiline':False]['text':' in principle, the underlying Cuda calls do permit cross-device ops to be captured.','line_number':99,'multiline':False]['text':' Either value being nonzero means the user supplied a pool to share.','line_number':124,'multiline':False]['text':' But only one should be nonzero.','line_number':125,'multiline':False]['text':' If pool was created by another graph's capture_begin, first should be nonzero.','line_number':126,'multiline':False]['text':' If pool was created by graph_pool_handle, second should be nonzero.','line_number':127,'multiline':False]['text':' User did not ask us to share a mempool. Use our own id_ as our mempool_id_.','line_number':131,'multiline':False]['text':' Sets just the first value, to distinguish it from MempoolId_ts created by graph_pool_handle().','line_number':132,'multiline':False]['text':' Addendum: beginAllocateStreamToPool is now called before cudaStreamBeginCapture to prevent an','line_number':136,'multiline':False]['text':' autograd thread's free() call triggering an invalid cudaEventRecord in the caching allocator','line_number':137,'multiline':False]['text':' due to the capture status being updated _after_ a capture had already started.','line_number':138,'multiline':False]['text':' At this point, any NCCL watchdogs should be aware that we are in capture mode','line_number':141,'multiline':False]['text':' and therefore should not enqueue any additional work that could be event-queried.','line_number':142,'multiline':False]['text':' We still must wait on any existing work that has not been cleaned up.','line_number':143,'multiline':False]['text':' cudaStreamCaptureModeGlobal is the most conservative option to','line_number':150,'multiline':False]['text':' prevent potentially unsafe CUDA API calls during capture.  See','line_number':151,'multiline':False]['text':' https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85','line_number':152,'multiline':False]['text':' In typical graph usage some tensors (e.g. the tensors used for graph IO) are not freed','line_number':179,'multiline':False]['text':' between replays.','line_number':180,'multiline':False]['text':' If Pytorch compiles and runs with a CUDA 11.4+ toolkit, there's a chance the allocator backend','line_number':181,'multiline':False]['text':' is cudaMallocAsync.','line_number':182,'multiline':False]['text':' cudaMallocAsync is generally graph-safe, but if some tensors are not freed between replays,','line_number':183,'multiline':False]['text':' the graph's internal bookkeeping requires that we instantiate with','line_number':184,'multiline':False]['text':' cudaGraphInstantiateFlagAutoFreeOnLaunch. See','line_number':185,'multiline':False]['text':' cudaGraphLaunch','line_number':186,'multiline':False]['text':' https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1g1accfe1da0c605a577c22d9751a09597','line_number':187,'multiline':False]['text':' cudaGraphInstantiateWithFlags','line_number':188,'multiline':False]['text':' https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1ga2c652a24ba93e52b99a47bec0888233','line_number':189,'multiline':False]['text':' Trailing NULL, NULL, 0 arguments were recommended by Cuda driver people,','line_number':195,'multiline':False]['text':' who prefer not to report error message through these arguments moving forward','line_number':196,'multiline':False]['text':' (they prefer return value, or errors on api calls internal to the capture)','line_number':197,'multiline':False]['text':' check if debug path is set','line_number':228,'multiline':False]['text':' Now that we've instantiated graph_ into graph_exec_,','line_number':230,'multiline':False]['text':' we don't need graph_ anymore.','line_number':231,'multiline':False]['text':' Just like any RNG consumer kernel!','line_number':249,'multiline':False]['text':' graph_exec_ may be replayed in any stream.','line_number':260,'multiline':False]['text':' Workaround for bug in libcuda.so that causes replayed graphs with','line_number':266,'multiline':False]['text':' certain topologies to be corrupted (kernels elided, internal syncs','line_number':267,'multiline':False]['text':' ignored) when replayed back to back without a sync in between.','line_number':268,'multiline':False]['text':' The bug is fixed in CUDA 11.4+.','line_number':269,'multiline':False]['text':' most verbose output','line_number':292,'multiline':False]['text':' I'd prefer these checks throw exceptions, not print warnings,','line_number':305,'multiline':False]['text':' but the destructor calls reset(), and at least one CI build','line_number':306,'multiline':False]['text':' refuses to compile with a throwing destructor.','line_number':307,'multiline':False]['text':'','line_number':308,'multiline':False]['text':' Instead of calling reset() in the destructor to clean up, I could','line_number':309,'multiline':False]['text':' call reset() in the __del__ method of a thin Python wrapper,','line_number':310,'multiline':False]['text':' in which case reset would be allowed to throw exceptions.','line_number':311,'multiline':False]['text':' But Stackoverflow does not like user-defined __del__.','line_number':312,'multiline':False]['text':' __del__ prevents Graph instances from EVER being garbage collected','line_number':313,'multiline':False]['text':' if they participate in a reference cycle.','line_number':314,'multiline':False]['text':' And exceptions thrown in __del__ only print a warning anyway.','line_number':315,'multiline':False]['text':'','line_number':316,'multiline':False]['text':' Calling reset() in the C++ destructor, with warnings instead of exceptions','line_number':317,'multiline':False]['text':' if calls fail, is the compromise we chose.','line_number':318,'multiline':False]['text':'','line_number':319,'multiline':False]['text':' If capture_begin, the capture, or capture_end failed at some point, this CUDAGraph, the generator,','line_number':320,'multiline':False]['text':' and the allocator could end up in all kinds of weird states depending where failure occurred.','line_number':321,'multiline':False]['text':' If the user catches the failure exception in a script, or is running in REPL or (god forbid)','line_number':322,'multiline':False]['text':' a Jupyter notebook, I don't see an easy way for reset() to gracefully fix all such possible error states.','line_number':323,'multiline':False]['text':' notifyCaptureDestroy may throw. How should we handle this?','line_number':325,'multiline':False]['text':' Returns an id another graph's capture_begin can use to share the same memory pool as this graph.','line_number':341,'multiline':False]['text':' namespace at::cuda','line_number':356,'multiline':False]