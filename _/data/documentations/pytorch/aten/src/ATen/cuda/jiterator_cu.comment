['text':' N is still int64_t for the computation, but it's always safe to cast result to int','line_number':20,'multiline':False]['text':' Different kernels are compiled depending on what we're vectorizing up to (1, 2 or 4 elements)','line_number':26,'multiline':False]['text':'   fn_ptr is set to the appropriate function based on the vec size and GPU used','line_number':27,'multiline':False]['text':' TODO: Memory use can probably be optimized by re-using kernels across GPUs with','line_number':28,'multiline':False]['text':'   the same compute capability','line_number':29,'multiline':False]['text':' The cache key includes all the parameters to generate_code + vec_size + dev_idx','line_number':39,'multiline':False]['text':' DeviceIndex, e.g. int8_t, is not treated as a number by the stream, cast to int as a workaround','line_number':46,'multiline':False]['text':' cache miss!','line_number':56,'multiline':False]['text':' Generates program','line_number':57,'multiline':False]['text':'contiguous=','line_number':60,'multiline':True]['text':'dynamic_casting=','line_number':60,'multiline':True]['text':' Acquires the program','line_number':66,'multiline':False]['text':' size of `extra_args` is unknown at compile-time','line_number':71,'multiline':False]['text':' pack args for kernel launch','line_number':77,'multiline':False]['text':' since 3 slots are already filled in `args`','line_number':85,'multiline':False]['text':' pack args for kernel launch','line_number':98,'multiline':False]['text':' since 7 slots are already filled in `args`','line_number':110,'multiline':False]['text':'casting result to int is always safe, intermediate is int64 and won't overflow','line_number':125,'multiline':False]['text':' The cache key includes all the parameters to generate_code + dev_idx','line_number':136,'multiline':False]['text':'vectorized','line_number':157,'multiline':True]['text':'vec_size','line_number':157,'multiline':True]['text':' pack args for kernel launch','line_number':164,'multiline':False]['text':' since 7 slots are already filled in `args`','line_number':177,'multiline':False]['text':' Decides which of 4 kernel types to launch','line_number':202,'multiline':False]['text':' Variations are:','line_number':203,'multiline':False]['text':'   - Case 1: no dynamic casting and contiguous','line_number':204,'multiline':False]['text':'   - Case 2: no dynamic casting and noncontiguous','line_number':205,'multiline':False]['text':'   - Case 3: dynamic casting and contiguous','line_number':206,'multiline':False]['text':'   - Case 4: dynamic casting and noncontiguous','line_number':207,'multiline':False]['text':' These cases align with the non-jitted CUDALoops.cuh cases in gpu_kernel_impl','line_number':208,'multiline':False]['text':' Case 1: no dynamic casting and contiguous','line_number':212,'multiline':False]['text':' Case 2: no dynamic casting and noncontiguous','line_number':218,'multiline':False]['text':'is_input=','line_number':219,'multiline':True]['text':'is_input=','line_number':221,'multiline':True]['text':' Cases 3 and 4 are handled below','line_number':236,'multiline':False]['text':' Both require construction of one or more storers and loaders','line_number':237,'multiline':False]['text':' Creates load casts from inputs (note offset indexing into the iterators noutpus...n tensors)','line_number':239,'multiline':False]['text':' Creates store cast to output (the 0...noutpus-1 tensor in TensorIterator)','line_number':243,'multiline':False]['text':' Case 3: dynamic casting and contiguous','line_number':248,'multiline':False]['text':' Case 4: dynamic casting and noncontiguous','line_number':260,'multiline':False]['text':'is_input=','line_number':261,'multiline':True]['text':'is_input=','line_number':263,'multiline':True]['text':' Entrypoint for dynamic version of jitted GPU kernels, which accepts dynamic number of inputs','line_number':271,'multiline':False]['text':' and arbitrary types of input and extra args. This dynamic version is needed for jiterator with python interface,','line_number':272,'multiline':False]['text':' since the kernel definition is unknown at the compilation time.','line_number':273,'multiline':False]['text':' Similarly, launch_jitted_vectorized_kernel_dynamic and launch_jitted_unrolled_kernel_dynamic are created','line_number':274,'multiline':False]['text':' to handle arbitrary functions defined in python user code.','line_number':275,'multiline':False]['text':' For templated version, see note [Jiterator] in JitLoops.cuh for more details','line_number':276,'multiline':False]['text':' TODO: much of preamble is common to both jitted_gpu_kernel and gpu_kernel','line_number':284,'multiline':False]['text':'   Maybe it could be refactored?','line_number':285,'multiline':False]['text':' Computes if dynamic casting is needed','line_number':303,'multiline':False]['text':' Dynamic casting is needed if an input's or output's dtype differs from the common dtype','line_number':304,'multiline':False]['text':' namespace native','line_number':317,'multiline':False]['text':' namespace at::cuda','line_number':357,'multiline':False]['text':' AT_USE_JITERATOR()','line_number':359,'multiline':False]