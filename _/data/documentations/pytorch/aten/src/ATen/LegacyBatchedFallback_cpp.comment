['text':' Given a linear index, return the actual index.','line_number':12,'multiline':False]['text':' Example: Given linear_idx = 3, sizes = [5, 2], we would return [1, 0]','line_number':13,'multiline':False]['text':' Returns if an operator is in-place. An operator is inplace if:','line_number':42,'multiline':False]['text':' 1. The first argument is a Tensor and it is being written to','line_number':43,'multiline':False]['text':' 2. The first argument is being returned','line_number':44,'multiline':False]['text':' 3. No other arguments are aliased','line_number':45,'multiline':False]['text':' Here is an example of an in-place operator:','line_number':46,'multiline':False]['text':' add_(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)','line_number':47,'multiline':False]['text':' Check that the first argument is being written to','line_number':52,'multiline':False]['text':' Check that none of the other args are being aliased','line_number':57,'multiline':False]['text':' Check that the first tensor is being returned (i.e., output has a (a!))','line_number':64,'multiline':False]['text':' The general flow of the algorithm is as follows.','line_number':82,'multiline':False]['text':' - First, we figure out which arguments are BatchedTensors and save them','line_number':83,'multiline':False]['text':'   to a vector. We also store a vector of which index of the arguments list','line_number':84,'multiline':False]['text':'   each BatchedTensor appears in. This will be useful for bookkeeping later.','line_number':85,'multiline':False]['text':' - Next, we apply the MultiBatchVmapTransform to all of the BatchedTensors.','line_number':86,'multiline':False]['text':'   This returns a vector of VmapPhysicalView that hold tensors that contain','line_number':87,'multiline':False]['text':'   all of the collective batch dimensions at the front of the tensors.','line_number':88,'multiline':False]['text':' - Then, we attempt to call `op` once per slice of the inputs. To do this,','line_number':89,'multiline':False]['text':'   we repeatedly we slice the input arguments (if they are BatchedTensors),','line_number':90,'multiline':False]['text':'   put the sliced (or a not-sliced) version of the input onto the stack, invoke','line_number':91,'multiline':False]['text':'   the operator, and then pop the results off the stack.','line_number':92,'multiline':False]['text':' `self` is the Tensor being modified in-place','line_number':101,'multiline':False]['text':' Figure out which arguments are BatchedTensor. Save them to a vector.','line_number':109,'multiline':False]['text':' For each BatchedTensor, also record what position of `arguments` they came from.','line_number':110,'multiline':False]['text':' NOTE: [vmap-incompatible in-place operations]','line_number':127,'multiline':False]['text':' In-place operations on `self` are not possible if there exists some vmap','line_number':128,'multiline':False]['text':' level `l` such that `self` is not being vmapped on that level but another','line_number':129,'multiline':False]['text':' argument is. For example, let B0 be a batch dim inside vmap and consider','line_number':130,'multiline':False]['text':' vmap(Tensor.add_, in_dims=(None, 0))(torch.ones(3), torch.ones(B0, 3))','line_number':131,'multiline':False]['text':' - self is torch.ones(3) and does not participate in this vmap','line_number':132,'multiline':False]['text':' - other is BatchedTensor(torch.ones(B0, 3))','line_number':133,'multiline':False]['text':' There's no way to do self.add_(other) because `other` has more elements','line_number':134,'multiline':False]['text':' elements than `self` due to being vmapped over.','line_number':135,'multiline':False]['text':'','line_number':136,'multiline':False]['text':' In the vmap fallback, we should error out when we detect this.','line_number':137,'multiline':False]['text':' Find one vmap level to complain about','line_number':140,'multiline':False]['text':' The following prints out "vmap: aten::add_(tensor, ...) is not possible",','line_number':143,'multiline':False]['text':' but it would be better to print out "tensor.add_(...) is not possible".','line_number':144,'multiline':False]['text':' Afaict there's no official way to get the add_ and there is no way to','line_number':145,'multiline':False]['text':' tell if an operator has method or function variants.','line_number':146,'multiline':False]['text':' MultiBatchVmapTransform the BatchedTensor arguments. This returns','line_number':161,'multiline':False]['text':' VmapPhysicalViews that contain all of the batch dimensions.','line_number':162,'multiline':False]['text':' Compute the total number of batches','line_number':166,'multiline':False]['text':' Without a shape-checking API, we're unable to compute the correct shape of','line_number':172,'multiline':False]['text':' the output so we just error out.','line_number':173,'multiline':False]['text':' Strategy: For each batch, we are going to push slices (where applicable)','line_number':178,'multiline':False]['text':' of the arguments onto `stack`, and call `op`.','line_number':179,'multiline':False]['text':' We assume that torch::jit::Stack is backed by vector<IValue> for','line_number':185,'multiline':False]['text':' simplicity. When that is not the case, this code should be updated.','line_number':186,'multiline':False]['text':' argument isn't a BatchedTensor','line_number':190,'multiline':False]['text':' argument is a BatchedTensor','line_number':194,'multiline':False]['text':' Return the tensor that was written to in-place','line_number':206,'multiline':False]['text':' NOTE [vmap through backward and undefined grad]','line_number':216,'multiline':False]['text':' While vmapping through backward functions (to compute batched grad), it','line_number':217,'multiline':False]['text':' is possible for the backward function to return an undefined grad for some','line_number':218,'multiline':False]['text':' grad_input for each example. In that case, we return an undefined grad.','line_number':219,'multiline':False]['text':'','line_number':220,'multiline':False]['text':' It is theoretically posssible for *some* of the examples to produce an','line_number':221,'multiline':False]['text':' undefined grad (a kernel could peek at the gradient values and return an','line_number':222,'multiline':False]['text':' undefined tensor if it determines the gradient is full of zeros). We','line_number':223,'multiline':False]['text':' could handle this by treating the undefined grad as a zero-filled tensor','line_number':224,'multiline':False]['text':' of the correct shape while stacking the tensors together. However I expect','line_number':225,'multiline':False]['text':' this to happen very rarely (I have not been able to find an example in our','line_number':226,'multiline':False]['text':' codebase) so we just error out in this case.','line_number':227,'multiline':False]['text':' The general flow of the algorithm is as follows.','line_number':237,'multiline':False]['text':' - First, we figure out which arguments are BatchedTensors and save them','line_number':238,'multiline':False]['text':'   to a vector. We also store a vector of which index of the arguments list','line_number':239,'multiline':False]['text':'   each BatchedTensor appears in. This will be useful for bookkeeping later.','line_number':240,'multiline':False]['text':' - Next, we apply the MultiBatchVmapTransform to all of the BatchedTensors.','line_number':241,'multiline':False]['text':'   This returns a vector of VmapPhysicalView that hold tensors that contain','line_number':242,'multiline':False]['text':'   all of the collective batch dimensions at the front of the tensors.','line_number':243,'multiline':False]['text':' - Then, we attempt to call `op` once per slice of the inputs. To do this,','line_number':244,'multiline':False]['text':'   we repeatedly we slice the input arguments (if they are BatchedTensors),','line_number':245,'multiline':False]['text':'   put the sliced (or a not-sliced) version of the input onto the stack, invoke','line_number':246,'multiline':False]['text':'   the operator, and then pop the results off the stack.','line_number':247,'multiline':False]['text':' - Each result obtained from the previous step is a slice of the total result,','line_number':248,'multiline':False]['text':'   so we stack those tensors together to form the final result.','line_number':249,'multiline':False]['text':' Figure out which arguments are BatchedTensor. Save them to a vector.','line_number':273,'multiline':False]['text':' For each BatchedTensor, also record what position of `arguments` they came from.','line_number':274,'multiline':False]['text':' MultiBatchVmapTransform the BatchedTensor arguments. This returns','line_number':295,'multiline':False]['text':' VmapPhysicalViews that contain all of the batch dimensions.','line_number':296,'multiline':False]['text':' Compute the total number of batches','line_number':300,'multiline':False]['text':' Without a shape-checking API, we're unable to compute the correct shape of','line_number':305,'multiline':False]['text':' the output so we just error out.','line_number':306,'multiline':False]['text':' Strategy: For each batch, we are going to push slices (where applicable)','line_number':311,'multiline':False]['text':' of the arguments onto `stack`, call `op`, and store the result in','line_number':312,'multiline':False]['text':' `output_shards`.','line_number':313,'multiline':False]['text':'','line_number':314,'multiline':False]['text':' NOTE: [Output shards layout]','line_number':315,'multiline':False]['text':' Assume that the operator has three outputs: a, b, c.','line_number':316,'multiline':False]['text':' The layout of output_shards is as follows:','line_number':317,'multiline':False]['text':' [ a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, c3]','line_number':318,'multiline':False]['text':' This is so that we can call at::stack([a0...a3]), at::stack([b0...b3])','line_number':319,'multiline':False]['text':' more easily in the next step.','line_number':320,'multiline':False]['text':' We assume that torch::jit::Stack is backed by vector<IValue> for','line_number':328,'multiline':False]['text':' simplicity. When that is not the case, this code should be updated.','line_number':329,'multiline':False]['text':' argument isn't a BatchedTensor','line_number':333,'multiline':False]['text':' argument is a BatchedTensor','line_number':337,'multiline':False]['text':' Store the result into `output_shards`. See NOTE: [Output shards layout]','line_number':347,'multiline':False]['text':' to learn about the details of how we store the shards.','line_number':348,'multiline':False]['text':' For each output Tensor, stack the shards of the tensor together to form a return','line_number':356,'multiline':False]['text':' See NOTE [vmap through backward and undefined grad]','line_number':362,'multiline':False]['text':' namespace at','line_number':378,'multiline':False]