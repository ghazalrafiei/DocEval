['text':' namespace at','line_number':52,'multiline':False]['text':' namespace indexing','line_number':56,'multiline':False]['text':' namespace at','line_number':57,'multiline':False]['text':' namespace torch::autograd','line_number':63,'multiline':False]['text':' Tensor is a "generic" object holding a pointer to the underlying TensorImpl object, which','line_number':75,'multiline':False]['text':' has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.','line_number':76,'multiline':False]['text':'','line_number':77,'multiline':False]['text':' For example:','line_number':78,'multiline':False]['text':'','line_number':79,'multiline':False]['text':' void func(Tensor a) {','line_number':80,'multiline':False]['text':'   Tensor b = a;','line_number':81,'multiline':False]['text':'   ...','line_number':82,'multiline':False]['text':' }','line_number':83,'multiline':False]['text':'','line_number':84,'multiline':False]['text':' In this example, when we say Tensor b = a, we are creating a new object that points to the','line_number':85,'multiline':False]['text':' same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the','line_number':86,'multiline':False]['text':' destructor decrements the reference count by calling release() on the TensorImpl it points to.','line_number':87,'multiline':False]['text':' The existing constructors, operator overloads, etc. take care to implement the correct semantics.','line_number':88,'multiline':False]['text':'','line_number':89,'multiline':False]['text':' Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and','line_number':90,'multiline':False]['text':' special care must be taken to handle this.','line_number':91,'multiline':False]['text':' Create a Tensor with a +0 reference count. Special care must be','line_number':94,'multiline':False]['text':' taken to avoid decrementing this reference count at destruction','line_number':95,'multiline':False]['text':' time. Intended to support MaybeOwnedTraits<Tensor>.','line_number':96,'multiline':False]['text':' This constructor should not be used by end users and is an implementation','line_number':104,'multiline':False]['text':' detail invoked by autogenerated code.','line_number':105,'multiline':False]['text':' Implicitly move-constructible from TensorBase, but must be explicit to increase refcount','line_number':112,'multiline':False]['text':'implicit','line_number':114,'multiline':True]['text':' Creates a new wrapper from TensorImpl. Intentionally a free method because','line_number':116,'multiline':False]['text':' it should be used with care. Checks necessary invariants','line_number':117,'multiline':False]['text':' Aliased by Dimname overloads, so need explicit using','line_number':144,'multiline':False]['text':'/ Should be used if *this can reasonably be expected to be contiguous and','line_number':149,'multiline':False]['text':'/ performance is important.','line_number':150,'multiline':False]['text':'/ Compared to contiguous, it saves a reference count','line_number':151,'multiline':False]['text':'/ increment/decrement if *this is already contiguous, at the cost','line_number':152,'multiline':False]['text':'/ in all cases of an extra pointer of stack usage, an extra branch','line_number':153,'multiline':False]['text':'/ to access, and an extra branch at destruction time.','line_number':154,'multiline':False]['text':' Use .contiguous() instead. Trying to borrow from a prvalue Tensor','line_number':157,'multiline':False]['text':' will only lead to trouble and dangling references.','line_number':158,'multiline':False]['text':' The following overloads are very intruiging.  Consider the following','line_number':161,'multiline':False]['text':' program:','line_number':162,'multiline':False]['text':'','line_number':163,'multiline':False]['text':'    x[1] = 3;','line_number':164,'multiline':False]['text':'','line_number':165,'multiline':False]['text':' We would expect that the first entry of x is written to 3.  But how can we','line_number':166,'multiline':False]['text':' actually achieve this?  x[1] evaluates to a tensor...','line_number':167,'multiline':False]['text':'','line_number':168,'multiline':False]['text':' The answer is, using a ref-qualifier.  x[1] is an rvalue, which cannot be','line_number':169,'multiline':False]['text':' (profitably) assigned to in the traditional sense, so we overload','line_number':170,'multiline':False]['text':' assignment to mean, "Actually, copy 3 into the tensor data."  This is done','line_number':171,'multiline':False]['text':' with an rvalue-reference ref-qualified overload (the methods with && at the','line_number':172,'multiline':False]['text':' end of their type.)','line_number':173,'multiline':False]['text':'','line_number':174,'multiline':False]['text':' There's one more fly in the ointment: We also want','line_number':175,'multiline':False]['text':'','line_number':176,'multiline':False]['text':'    Tensor x = y;','line_number':177,'multiline':False]['text':'','line_number':178,'multiline':False]['text':' to work, and we want it NOT to copy.  So we need a traditional operator=','line_number':179,'multiline':False]['text':' overload.  But we MUST specify a mutable lvalue ref-qualifier, to','line_number':180,'multiline':False]['text':' disambiguate the traditional overload from the rvalue-reference','line_number':181,'multiline':False]['text':' ref-qualified overload.  Otherwise, it will be ambiguous, because','line_number':182,'multiline':False]['text':' a non ref-qualified method is eligible for all situations.','line_number':183,'multiline':False]['text':' Unfortunately, we have to write these constructors out manually','line_number':185,'multiline':False]['text':' to work around an MSVC bug:','line_number':186,'multiline':False]['text':'    error C2580: 'at::Tensor &at::Tensor::operator =(const at::Tensor &) &':','line_number':187,'multiline':False]['text':'    multiple versions of a defaulted special member functions are not allowed','line_number':188,'multiline':False]['text':' Tensor& operator=(const Tensor&) & = default;','line_number':189,'multiline':False]['text':' Tensor& operator=(Tensor&&) & = default;','line_number':190,'multiline':False]['text':' Also MSVC will wrongly issue the following warning with the aforementioned fix','line_number':192,'multiline':False]['text':'    warning C4522: 'at::Tensor': multiple assignment operators specified','line_number':193,'multiline':False]['text':' Let's just skip the warning.','line_number':194,'multiline':False]['text':'','line_number':195,'multiline':False]['text':' TODO: temporarily disabled','line_number':196,'multiline':False]['text':'non_blocking','line_number':232,'multiline':True]['text':'copy','line_number':232,'multiline':True]['text':' TODO: Deprecate me','line_number':235,'multiline':False]['text':'non_blocking','line_number':237,'multiline':True]['text':'copy','line_number':237,'multiline':True]['text':' These properties are checked in the Scalar constructor, but we already','line_number':309,'multiline':False]['text':' check them here to provide more useful diagnostics for the user.','line_number':310,'multiline':False]['text':' The Scalar(Tensor) constructor is explicit, so we need to call it.','line_number':318,'multiline':False]['text':'non_blocking','line_number':334,'multiline':True]['text':'copy','line_number':334,'multiline':True]['text':' TODO: The Python version also accepts arguments','line_number':337,'multiline':False]['text':'non_blocking','line_number':339,'multiline':True]['text':'copy','line_number':339,'multiline':True]['text':'non_blocking','line_number':343,'multiline':True]['text':'copy','line_number':343,'multiline':True]['text':'non_blocking','line_number':347,'multiline':True]['text':'copy','line_number':347,'multiline':True]['text':'non_blocking','line_number':351,'multiline':True]['text':'copy','line_number':351,'multiline':True]['text':'non_blocking','line_number':355,'multiline':True]['text':'copy','line_number':355,'multiline':True]['text':'non_blocking','line_number':359,'multiline':True]['text':'copy','line_number':359,'multiline':True]['text':' ~~~~~ Autograd API ~~~~~','line_number':362,'multiline':False]['text':'/ \fn bool is_leaf() const;','line_number':364,'multiline':False]['text':'/','line_number':365,'multiline':False]['text':'/ All Tensors that have `requires_grad()` which is ``false`` will be leaf Tensors by convention.','line_number':366,'multiline':False]['text':'/','line_number':367,'multiline':False]['text':'/ For Tensors that have `requires_grad()` which is ``true``, they will be leaf Tensors if they were','line_number':368,'multiline':False]['text':'/ created by the user. This means that they are not the result of an operation and so','line_number':369,'multiline':False]['text':'/ `grad_fn()` is `nullptr`.','line_number':370,'multiline':False]['text':'/','line_number':371,'multiline':False]['text':'/ Only leaf Tensors will have their `grad()` populated during a call to `backward()`.','line_number':372,'multiline':False]['text':'/ To get `grad()` populated for non-leaf Tensors, you can use `retain_grad()`.','line_number':373,'multiline':False]['text':'/','line_number':374,'multiline':False]['text':'/ Example:','line_number':375,'multiline':False]['text':'/ @code','line_number':376,'multiline':False]['text':'/ auto a = torch::rand(10, torch::requires_grad());','line_number':377,'multiline':False]['text':'/ std::cout << a.is_leaf() << std::endl; // prints `true`','line_number':378,'multiline':False]['text':'/','line_number':379,'multiline':False]['text':'/ auto b = torch::rand(10, torch::requires_grad()).to(torch::kCUDA);','line_number':380,'multiline':False]['text':'/ std::cout << b.is_leaf() << std::endl; // prints `false`','line_number':381,'multiline':False]['text':'/ // b was created by the operation that cast a cpu Tensor into a cuda Tensor','line_number':382,'multiline':False]['text':'/','line_number':383,'multiline':False]['text':'/ auto c = torch::rand(10, torch::requires_grad()) + 2;','line_number':384,'multiline':False]['text':'/ std::cout << c.is_leaf() << std::endl; // prints `false`','line_number':385,'multiline':False]['text':'/ // c was created by the addition operation','line_number':386,'multiline':False]['text':'/','line_number':387,'multiline':False]['text':'/ auto d = torch::rand(10).cuda();','line_number':388,'multiline':False]['text':'/ std::cout << d.is_leaf() << std::endl; // prints `true`','line_number':389,'multiline':False]['text':'/ // d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)','line_number':390,'multiline':False]['text':'/','line_number':391,'multiline':False]['text':'/ auto e = torch::rand(10).cuda().requires_grad_();','line_number':392,'multiline':False]['text':'/ std::cout << e.is_leaf() << std::endl; // prints `true`','line_number':393,'multiline':False]['text':'/ // e requires gradients and has no operations creating it','line_number':394,'multiline':False]['text':'/','line_number':395,'multiline':False]['text':'/ auto f = torch::rand(10, torch::device(torch::kCUDA).requires_grad(true));','line_number':396,'multiline':False]['text':'/ std::cout << f.is_leaf() << std::endl; // prints `true`','line_number':397,'multiline':False]['text':'/ // f requires grad, has no operation creating it','line_number':398,'multiline':False]['text':'/ @endcode','line_number':399,'multiline':False]['text':'/ \fn void backward(const Tensor & gradient={}, c10::optional<bool> retain_graph=c10::nullopt, bool create_graph=false, c10::optional<TensorList> inputs=c10::nullopt) const;','line_number':401,'multiline':False]['text':'/','line_number':402,'multiline':False]['text':'/ Computes the gradient of current tensor with respect to graph leaves.','line_number':403,'multiline':False]['text':'/','line_number':404,'multiline':False]['text':'/ The graph is differentiated using the chain rule. If the tensor is','line_number':405,'multiline':False]['text':'/ non-scalar (i.e. its data has more than one element) and requires','line_number':406,'multiline':False]['text':'/ gradient, the function additionally requires specifying ``gradient``.','line_number':407,'multiline':False]['text':'/ It should be a tensor of matching type and location, that contains','line_number':408,'multiline':False]['text':'/ the gradient of the differentiated function w.r.t. this Tensor.','line_number':409,'multiline':False]['text':'/','line_number':410,'multiline':False]['text':'/ This function accumulates gradients in the leaves - you might need to','line_number':411,'multiline':False]['text':'/ zero them before calling it.','line_number':412,'multiline':False]['text':'/','line_number':413,'multiline':False]['text':'/ \param gradient Gradient w.r.t. the','line_number':414,'multiline':False]['text':'/     tensor. If it is a tensor, it will be automatically converted','line_number':415,'multiline':False]['text':'/     to a Tensor that does not require grad unless ``create_graph`` is True.','line_number':416,'multiline':False]['text':'/     None values can be specified for scalar Tensors or ones that','line_number':417,'multiline':False]['text':'/     don't require grad. If a None value would be acceptable then','line_number':418,'multiline':False]['text':'/     this argument is optional.','line_number':419,'multiline':False]['text':'/ \param retain_graph If ``false``, the graph used to compute','line_number':420,'multiline':False]['text':'/     the grads will be freed. Note that in nearly all cases setting','line_number':421,'multiline':False]['text':'/     this option to True is not needed and often can be worked around','line_number':422,'multiline':False]['text':'/     in a much more efficient way. Defaults to the value of','line_number':423,'multiline':False]['text':'/     ``create_graph``.','line_number':424,'multiline':False]['text':'/ \param create_graph If ``true``, graph of the derivative will','line_number':425,'multiline':False]['text':'/     be constructed, allowing to compute higher order derivative','line_number':426,'multiline':False]['text':'/     products. Defaults to ``false``.','line_number':427,'multiline':False]['text':'/ \param inputs Inputs w.r.t. which the gradient will be accumulated into','line_number':428,'multiline':False]['text':'/     ``at::Tensor::grad``. All other Tensors will be ignored. If not','line_number':429,'multiline':False]['text':'/     provided, the gradient is accumulated into all the leaf Tensors','line_number':430,'multiline':False]['text':'/     that were used to compute the current tensor.','line_number':431,'multiline':False]['text':'/     When inputs are provided and a given input is not a leaf,','line_number':432,'multiline':False]['text':'/     the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).','line_number':433,'multiline':False]['text':'/     It is an implementation detail on which the user should not rely.','line_number':434,'multiline':False]['text':'/     See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.','line_number':435,'multiline':False]['text':' NB: Adding this wrapper to _backward here because we'd like our','line_number':437,'multiline':False]['text':' 'backwards' api to accept the 'inputs' argument optionally. Since code gen','line_number':438,'multiline':False]['text':' currently does not support optional of TensorList our approach is to replace','line_number':439,'multiline':False]['text':' backward in native_functions.yaml with _backward and call it here instead.','line_number':440,'multiline':False]['text':'/ \fn Tensor detach() const;','line_number':449,'multiline':False]['text':'/','line_number':450,'multiline':False]['text':'/ Returns a new Tensor, detached from the current graph.','line_number':451,'multiline':False]['text':'/ The result will never require gradient.','line_number':452,'multiline':False]['text':'/ \fn Tensor & detach_() const;','line_number':454,'multiline':False]['text':'/','line_number':455,'multiline':False]['text':'/ Detaches the Tensor from the graph that created it, making it a leaf.','line_number':456,'multiline':False]['text':'/ Views cannot be detached in-place.','line_number':457,'multiline':False]['text':'/ \fn void retain_grad() const;','line_number':459,'multiline':False]['text':'/','line_number':460,'multiline':False]['text':'/ Enables this Tensor to have their :attr:`grad` populated during','line_number':461,'multiline':False]['text':'/ :func:`backward`. This is a no-op for leaf tensors.','line_number':462,'multiline':False]['text':'/ \fn bool retains_grad() const;','line_number':464,'multiline':False]['text':'/','line_number':465,'multiline':False]['text':'/ Is ``true`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be','line_number':466,'multiline':False]['text':'/ populated during :func:`backward`, ``false`` otherwise.','line_number':467,'multiline':False]['text':'/ Return a mutable reference to the gradient. This is conventionally','line_number':474,'multiline':False]['text':'/ used as `t.grad() = x` to set a gradient to a completely new tensor.','line_number':475,'multiline':False]['text':'/ Note that this function work with a non-const Tensor and is not','line_number':476,'multiline':False]['text':'/ thread safe.','line_number':477,'multiline':False]['text':'/ This function returns an undefined tensor by default and returns a defined tensor','line_number':482,'multiline':False]['text':'/ the first time a call to `backward()` computes gradients for this Tensor.','line_number':483,'multiline':False]['text':'/ The attribute will then contain the gradients computed and future calls','line_number':484,'multiline':False]['text':'/ to `backward()` will accumulate (add) gradients into it.','line_number':485,'multiline':False]['text':' The Forward AD API functions below are low level and are not to be used by end','line_number':499,'multiline':False]['text':' users who should use the API provided in torch/csrc/autograd.h','line_number':500,'multiline':False]['text':'/ This function returns the forward gradient for this Tensor at the given level.','line_number':502,'multiline':False]['text':'/ This function can be used to set the value of the forward grad.','line_number':507,'multiline':False]['text':'/ Note that the given new_grad might not be used directly if it has different','line_number':508,'multiline':False]['text':'/ metadata (size/stride/storage offset) compared to this Tensor. In that case,','line_number':509,'multiline':False]['text':'/ new_grad content will be copied into a new Tensor','line_number':510,'multiline':False]['text':' STOP.  Thinking of adding a method here, which only makes use','line_number':516,'multiline':False]['text':' of other ATen methods?  Define it in native_functions.yaml.','line_number':517,'multiline':False]['text':'example','line_number':519,'multiline':False]['text':'Tensor * add(Tensor & b);','line_number':520,'multiline':False]['text':' Special C++ only overloads for std()-like functions (See gh-40287)','line_number':523,'multiline':False]['text':' These are needed because int -> bool conversion takes precedence over int -> IntArrayRef','line_number':524,'multiline':False]['text':' So, for example std(0) would select the std(unbiased=False) overload','line_number':525,'multiline':False]['text':' We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the','line_number':535,'multiline':False]['text':' at::kDouble and its friends to be TypeMeta's, but that hasn't happened yet.','line_number':536,'multiline':False]['text':' Before that change, we make this method to maintain BC for C++ usage like','line_number':537,'multiline':False]['text':' `x.to(y.dtype)`.','line_number':538,'multiline':False]['text':' TODO: remove following two after at::kDouble and its friends are TypeMeta's.','line_number':539,'multiline':False]['text':'scalar_type=','line_number':541,'multiline':True]['text':'scalar_type=','line_number':544,'multiline':True]['text':'/ NOTE: This is similar to the legacy `.data()` function on `Variable`, and is intended','line_number':552,'multiline':False]['text':'/ to be used from functions that need to access the `Variable`'s equivalent `Tensor`','line_number':553,'multiline':False]['text':'/ (i.e. `Tensor` that shares the same storage and tensor metadata with the `Variable`).','line_number':554,'multiline':False]['text':'/','line_number':555,'multiline':False]['text':'/ One notable difference with the legacy `.data()` function is that changes to the','line_number':556,'multiline':False]['text':'/ returned `Tensor`'s tensor metadata (e.g. sizes / strides / storage / storage_offset)','line_number':557,'multiline':False]['text':'/ will not update the original `Variable`, due to the fact that this function','line_number':558,'multiline':False]['text':'/ shallow-copies the `Variable`'s underlying TensorImpl.','line_number':559,'multiline':False]['text':'/ NOTE: `var.variable_data()` in C++ has the same semantics as `tensor.data`','line_number':564,'multiline':False]['text':'/ in Python, which create a new `Variable` that shares the same storage and','line_number':565,'multiline':False]['text':'/ tensor metadata with the original `Variable`, but with a completely new','line_number':566,'multiline':False]['text':'/ autograd history.','line_number':567,'multiline':False]['text':'/','line_number':568,'multiline':False]['text':'/ NOTE: If we change the tensor metadata (e.g. sizes / strides /','line_number':569,'multiline':False]['text':'/ storage / storage_offset) of a variable created from `var.variable_data()`, those','line_number':570,'multiline':False]['text':'/ changes will not update the original variable `var`. In `.variable_data()`, we set','line_number':571,'multiline':False]['text':'/ `allow_tensor_metadata_change_` to false to make such changes explicitly illegal,','line_number':572,'multiline':False]['text':'/ in order to prevent users from changing metadata of `var.variable_data()`','line_number':573,'multiline':False]['text':'/ and expecting the original variable `var` to also be updated.','line_number':574,'multiline':False]['text':' Hooks','line_number':579,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':580,'multiline':False]['text':'/ Registers a backward hook.','line_number':587,'multiline':False]['text':'/','line_number':588,'multiline':False]['text':'/ The hook will be called every time a gradient with respect to the Tensor is computed.','line_number':589,'multiline':False]['text':'/ The hook should have one of the following signature:','line_number':590,'multiline':False]['text':'/ ```','line_number':591,'multiline':False]['text':'/ hook(Tensor grad) -> Tensor','line_number':592,'multiline':False]['text':'/ ```','line_number':593,'multiline':False]['text':'/ ```','line_number':594,'multiline':False]['text':'/ hook(Tensor grad) -> void','line_number':595,'multiline':False]['text':'/ ```','line_number':596,'multiline':False]['text':'/ The hook should not modify its argument, but it can optionally return a new gradient','line_number':597,'multiline':False]['text':'/ which will be used in place of `grad`.','line_number':598,'multiline':False]['text':'/','line_number':599,'multiline':False]['text':'/ This function returns the index of the hook in the list which can be used to remove hook.','line_number':600,'multiline':False]['text':'/','line_number':601,'multiline':False]['text':'/ Example:','line_number':602,'multiline':False]['text':'/ @code','line_number':603,'multiline':False]['text':'/ auto v = torch::tensor({0., 0., 0.}, torch::requires_grad());','line_number':604,'multiline':False]['text':'/ auto h = v.register_hook([](torch::Tensor grad){ return grad * 2; }); // double the gradient','line_number':605,'multiline':False]['text':'/ v.backward(torch::tensor({1., 2., 3.}));','line_number':606,'multiline':False]['text':'/ // This prints:','line_number':607,'multiline':False]['text':'/ // ```','line_number':608,'multiline':False]['text':'/ //  2','line_number':609,'multiline':False]['text':'/ //  4','line_number':610,'multiline':False]['text':'/ //  6','line_number':611,'multiline':False]['text':'/ // [ CPUFloatType{3} ]','line_number':612,'multiline':False]['text':'/ // ```','line_number':613,'multiline':False]['text':'/ std::cout << v.grad() << std::endl;','line_number':614,'multiline':False]['text':'/ v.remove_hook(h);  // removes the hook','line_number':615,'multiline':False]['text':'/ @endcode','line_number':616,'multiline':False]['text':' Variable methods','line_number':622,'multiline':False]['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':623,'multiline':False]['text':' Helper creator for Tensor class which doesn't requires the users to pass','line_number':638,'multiline':False]['text':' in an intrusive_ptr instead it just converts the argument passed to','line_number':639,'multiline':False]['text':' requested intrusive_ptr type.','line_number':640,'multiline':False]['text':' namespace detail','line_number':646,'multiline':False]['text':' namespace at','line_number':648,'multiline':False]['text':' namespace at','line_number':653,'multiline':False]['text':' NOTE: this can be implemented without the special','line_number':663,'multiline':False]['text':' unsafe_borrow_t Tensor constructor as','line_number':664,'multiline':False]['text':'','line_number':665,'multiline':False]['text':' return borrow_type(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::reclaim(from.unsafeGetTensorImpl()));','line_number':666,'multiline':False]['text':'','line_number':667,'multiline':False]['text':' but that hurts inlining due to the nullptr check in the','line_number':668,'multiline':False]['text':' Tensor(c10::intrusive_ptr<...>) constructor. We already know','line_number':669,'multiline':False]['text':' that from.impl_ isn't null because from is a valid Tensor, so','line_number':670,'multiline':False]['text':' we needn't do the check again. (using __builtin_assume can','line_number':671,'multiline':False]['text':' avoid this, but wouldn't be portable to MSVC.)','line_number':672,'multiline':False]['text':' See above note: this can be implemented with public API','line_number':678,'multiline':False]['text':' similarly to createBorrow(), but that would hurt inlining.','line_number':679,'multiline':False]['text':' "leak" it, but it was already +0.','line_number':684,'multiline':False]['text':'borrow','line_number':695,'multiline':True]['text':' namespace c10','line_number':735,'multiline':False]['text':' namespace at','line_number':753,'multiline':False]