['text':' Checks if the batch dims in `bdims` appear at the front of the tensor.','line_number':8,'multiline':False]['text':' Takes a BatchedTensorImpl, permutes all of the batch dims to the front,','line_number':18,'multiline':False]['text':' and then returns a physical version of the Tensor.','line_number':19,'multiline':False]['text':'physical','line_number':56,'multiline':True]['text':' NB: fmap doesn't have a SmallVector variant, so we don't use it here.','line_number':61,'multiline':False]['text':' Given a Tensor or a BatchedTensor, returns the underlying physical tensor','line_number':103,'multiline':False]['text':' with all vmapped dimensions permuted to the front, if they exist, and a','line_number':104,'multiline':False]['text':' bitset of vmap levels that were present in the tensor.','line_number':105,'multiline':False]['text':' Given a Tensor or a BatchedTensor, creates a physical view of the tensor','line_number':115,'multiline':False]['text':' such that it has a batch dimension for each level in `requested_levels`','line_number':116,'multiline':False]['text':' and `requested_example_dim` number of non-batch-dimensions.','line_number':117,'multiline':False]['text':'','line_number':118,'multiline':False]['text':' This function is useful in preparing physical views on tensors that can','line_number':119,'multiline':False]['text':' then be passed into broadcasting operations. For example, when adding','line_number':120,'multiline':False]['text':' two BatchedTensors of sizes [B0, 3] and [B0, B1, 2, 3], where the Bi are the','line_number':121,'multiline':False]['text':' batch dimensions, we must align the batch dimensions and non-batch-dimensions','line_number':122,'multiline':False]['text':' (henceforth referred to as the "example" dimensions) separately to produce','line_number':123,'multiline':False]['text':' tensors of size [B0, 1, 1, 3] and [B0, B1, 2, 3] so that they can be added.','line_number':124,'multiline':False]['text':'','line_number':125,'multiline':False]['text':' Here's a direct example of using alignBatchDimsAtFront on the above two tensors.','line_number':126,'multiline':False]['text':'','line_number':127,'multiline':False]['text':' 1) alignBatchDimsAtFront([B0, 3], requested_levels={0, 1}, requested_example_dim=2)','line_number':128,'multiline':False]['text':' returns a physical view of size [B0, 1, 1, 3] by adding an extra dimension for','line_number':129,'multiline':False]['text':' level 1 and another extra dimension to pad the example dimensions to 2.','line_number':130,'multiline':False]['text':'','line_number':131,'multiline':False]['text':' 2) alignBatchDimsAtFront([B0, B1, 2, 3], requested_levels={0, 1}, requested_example_dim=2)','line_number':132,'multiline':False]['text':' returns a physical view of size [B0, B1, 2, 3]','line_number':133,'multiline':False]['text':'num_batch_dims','line_number':150,'multiline':True]['text':' Optimization: no need to do another view if the physical tensor is','line_number':155,'multiline':False]['text':' already the correct shape','line_number':156,'multiline':False]['text':' align the example dims (non-bdims dims) first','line_number':162,'multiline':False]['text':' aligned_sizes[-tensor_example_dim:] = tensor_sizes[-tensor_example_dim:]','line_number':163,'multiline':False]['text':' align the bdims','line_number':169,'multiline':False]['text':' Determine the level of the bdim','line_number':173,'multiline':False]['text':' The algorithm is as follows:','line_number':183,'multiline':False]['text':' 1. Figure out what all of the collective levels in `logical_tensors` is.','line_number':184,'multiline':False]['text':' 2. Move all batch dims to the front of the tensors and add extra dims','line_number':185,'multiline':False]['text':'    of size 1. At this point, every tensor will have a dimension for','line_number':186,'multiline':False]['text':'    each of the collective levels.','line_number':187,'multiline':False]['text':' 3. Compute the batch_sizes.','line_number':188,'multiline':False]['text':' 4. Expand each physical tensor so that they have output batch size equal','line_number':189,'multiline':False]['text':'    to `batch_sizes`','line_number':190,'multiline':False]['text':' Figure out all of the collective vmap levels in `logical_tensors`.','line_number':193,'multiline':False]['text':' Populate physical_tensors.','line_number':202,'multiline':False]['text':' This contains a list of regular (non-Batched) Tensors where all of the','line_number':203,'multiline':False]['text':' batch dims have been moved to the front of the tensor. Any previously','line_number':204,'multiline':False]['text':' non-existing batch dims get added to the tensors as new dimensions of size 1.','line_number':205,'multiline':False]['text':'logical_dim','line_number':209,'multiline':True]['text':' Compute batch_sizes','line_number':215,'multiline':False]['text':' Expand each physical_tensor so that it has batch sizes `batch_sizes`','line_number':226,'multiline':False]['text':'logical dim','line_number':250,'multiline':True]['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':267,'multiline':False]['text':' NB: It's possible that we didn't actually need to align `tensor`.','line_number':272,'multiline':False]['text':' For example, when adding two tensors of size (B, 2), and (3, 2), where','line_number':273,'multiline':False]['text':' the first Tensor is a BatchedTensor with batch dim B and the second is','line_number':274,'multiline':False]['text':' a regular Tensor, we will return views of size (B, 1, 2) and (1, 3, 2).','line_number':275,'multiline':False]['text':' However, the view on the second tensor is unnecessary: broadcasting','line_number':276,'multiline':False]['text':' semantics allow for the addition of two tensors of size (B, 1, 2) and (3, 2)!','line_number':277,'multiline':False]['text':'','line_number':278,'multiline':False]['text':' If this unnecessary view is a problem, consider optimizing it away in','line_number':279,'multiline':False]['text':' the future. This may involve creating a new type of VmapPhysicalView','line_number':280,'multiline':False]['text':' namespace at','line_number':301,'multiline':False]