['text':'*
 * Include this file if you want to register operators. It includes all
 * functionality needed to do so for you.
 ','line_number':3,'multiline':True]['text':' The first argument of the schema might be of type DispatchKeySet, in which case we remove it.','line_number':23,'multiline':False]['text':' We do this because every argument in a function schema is expected to be convertable','line_number':24,'multiline':False]['text':' to an ivalue, but DispatchKeySet is not a type we want the jit to be aware of.','line_number':25,'multiline':False]['text':' See Note [Plumbing Keys Through The Dispatcher]','line_number':26,'multiline':False]['text':'*
 * An instance of this class handles the registration for one or more operators.
 * Make sure you keep the RegisterOperators instance around since it will
 * deregister the operator it's responsible for in its destructor.
 *
 * Example:
 *
 * > namespace {
 * >   class my_kernel_cpu final : public c10::OperatorKernel {
 * >   public:
 * >     Tensor operator()(Tensor a, Tensor b) {...}
 * >   };
 * > }
 * >
 * > static auto registry = c10::RegisterOperators()
 * >     .op(c10::RegisterOperators::options()
 * >         .schema("my_op")
 * >         .kernel<my_kernel_cpu>(DispatchKey::CPU));
 ','line_number':34,'multiline':True]['text':' internal-only for registering stack based kernels','line_number':70,'multiline':False]['text':' internal-only for registering stack based catch-all kernels','line_number':76,'multiline':False]['text':' internal only for registering caffe2 ops','line_number':82,'multiline':False]['text':'*
     * Use this to specify the schema for an operator. You can also specify
     * the operator name only to have the function signature part of the
     * schema be inferred from the kernel function.
     *
     * Example:
     *
     * > // Infer function signature from my_kernel_cpu
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .kernel<my_kernel_cpu>(DispatchKey::CPU));
     * >
     * >
     * > // Explicitly specify full schema
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op(Tensor a) -> Tensor")
     * >         .kernel<my_kernel_cpu>(DispatchKey::CPU));
     ','line_number':89,'multiline':True]['text':'*
     * Use this to register an operator whose kernel is implemented as a functor.
     * The kernel is only called for inputs matching the given dispatch key.
     * You can register multiple kernels for different dispatch keys.
     *
     * Example:
     *
     * > namespace {
     * >   class my_kernel_cpu final : public c10::OperatorKernel {
     * >   public:
     * >     Tensor operator()(Tensor a, Tensor b) {...}
     * >   };
     * > }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .kernel<my_kernel_cpu>(DispatchKey::CPU));
     *
     * The functor constructor can take arguments to configure the kernel.
     * The arguments are defined in the kernel registration.
     * Example:
     *
     * > namespace {
     * >   class my_kernel_cpu final : public c10::OperatorKernel {
     * >   public:
     * >     explicit my_kernel_cpu(std::string some_configuration, int a, bool b)
     * >         : ... {...}
     * >
     * >     Tensor operator()(Tensor a, Tensor b) {...}
     * >   };
     * > }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .kernel<my_kernel_cpu>(DispatchKey::CPU, "some_configuration", 3, true));
     ','line_number':121,'multiline':True]['text':' enable_if: only enable it if KernelFunctor is actually a functor','line_number':160,'multiline':False]['text':'*
     * Use this to register an operator whose kernel is implemented as a functor.
     * The kernel is a catch-all kernel, meaning it's called independent from
     * the input. Dispatch is disabled for this operator.
     *
     * Example:
     *
     * > namespace {
     * >   class my_kernel_cpu final : public c10::OperatorKernel {
     * >   public:
     * >     Tensor operator()(Tensor a, Tensor b) {...}
     * >   };
     * > }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .catchAllKernel<my_kernel_cpu>());
     *
     * The functor constructor can take arguments to configure the kernel.
     * The arguments are defined in the kernel registration.
     * Example:
     *
     * > namespace {
     * >   class my_kernel_cpu final : public c10::OperatorKernel {
     * >   public:
     * >     explicit my_kernel_cpu(std::string some_configuration, int a, bool b)
     * >         : ... {...}
     * >
     * >     Tensor operator()(Tensor a, Tensor b) {...}
     * >   };
     * > }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .catchAllKernel<my_kernel_cpu>("some_configuration", 3, true));
     ','line_number':173,'multiline':True]['text':' enable_if: only enable it if KernelFunctor is actually a functor','line_number':212,'multiline':False]['text':'*
     * Use this to register an operator whose kernel is implemented by a function.
     * The kernel is only called for inputs matching the given dispatch key.
     * You can register multiple kernels for different dispatch keys.
     *
     * Example:
     *
     * > namespace { Tensor my_kernel_cpu(Tensor a, Tensor b) {...} }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(DispatchKey::CPU));
     ','line_number':225,'multiline':True]['text':' enable_if: only enable it if FuncType is actually a function','line_number':240,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoFunctor','line_number':249,'multiline':False]['text':'*
     * Use this to register an operator whose kernel is implemented by a function.
     * The kernel is a catch-all kernel, meaning it's called independent from
     * the input. Dispatch is disabled for this operator.
     *
     * Example:
     *
     * > namespace { Tensor my_kernel_cpu(Tensor a, Tensor b) {...} }
     * >
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .catchAllKernel<decltype(my_kernel_cpu), &my_kernel_cpu>());
     ','line_number':254,'multiline':True]['text':' enable_if: only enable it if FuncType is actually a function','line_number':269,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoFunctor','line_number':278,'multiline':False]['text':' enable_if: only enable it if FuncType is actually a function','line_number':284,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoFunctor','line_number':293,'multiline':False]['text':' enable_if: only enable it if FuncType is actually a function','line_number':299,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoFunctor','line_number':308,'multiline':False]['text':'*
     * Use this to register an operator whose kernel is implemented as a lambda.
     * The kernel is only called for inputs matching the given dispatch key.
     * You can register multiple kernels for different dispatch keys.
     *
     * The lambda must be stateless, i.e. not have a capture. If your kernel
     * needs to store some configuration parameters, write the kernel as a
     * functor instead.
     *
     * Example:
     *
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .kernel(DispatchKey::CPU, [] (Tensor a) -> Tensor {...}));
     ','line_number':313,'multiline':True]['text':' enable_if: only enable it if Lambda is a functor (note: lambdas are functors)','line_number':330,'multiline':False]['text':' We don't support stateful lambdas (i.e. lambdas with a capture), because their','line_number':337,'multiline':False]['text':' behavior would be nonobvious. A functor kernel with cache gets a new instance of','line_number':338,'multiline':False]['text':' its cache each time the kernel is looked up from the dispatch table.','line_number':339,'multiline':False]['text':' A lambda with a capture would be global and share its capture between all kernel lookups.','line_number':340,'multiline':False]['text':' So, instead of making users having to think about it (including the thread-safety','line_number':341,'multiline':False]['text':' issues this causes), let's just forbid stateful lambdas altogether.','line_number':342,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor','line_number':349,'multiline':False]['text':'*
     * Use this to register an operator whose kernel is implemented as a lambda.
     * The kernel is a catch-all kernel, meaning it's called independent from
     * the input. Dispatch is disabled for this operator.
     *
     * The lambda must be stateless, i.e. not have a capture. If your kernel
     * needs to store some configuration parameters, write the kernel as a
     * functor instead.
     *
     * Example:
     *
     * > static auto registry = c10::RegisterOperators()
     * >     .op(c10::RegisterOperators::options()
     * >         .schema("my_op")
     * >         .catchAllKernel([] (Tensor a) -> Tensor {...}));
     ','line_number':354,'multiline':True]['text':' enable_if: only enable it if Lambda is a functor (note: lambdas are functors)','line_number':371,'multiline':False]['text':' We don't support stateful lambdas (i.e. lambdas with a capture), because their','line_number':378,'multiline':False]['text':' behavior would be nonobvious.','line_number':379,'multiline':False]['text':' A lambda with a capture would be global and share its capture between all kernel lookups.','line_number':380,'multiline':False]['text':' This would be a likely source for unexpected race conditions, so we forbid it.','line_number':381,'multiline':False]['text':' If a kernel really needs global state, they can just have regular global state','line_number':382,'multiline':False]['text':' in their .cpp file next to the kernel lambda.','line_number':383,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor','line_number':390,'multiline':False]['text':' KernelRegistrationConfig accumulates all information from the config','line_number':418,'multiline':False]['text':' parameters passed to a RegisterOperators::op() call into one object.','line_number':419,'multiline':False]['text':'*
   * Call this to get an instance of registration options, which
   * can be passed to a call to RegisterOperators::op() to specify
   * these options for the operator registration.
   * See class doc comment for examples.
   ','line_number':442,'multiline':True]['text':'*
   * Call this to register an operator. See class doc comment for examples.
   ','line_number':452,'multiline':True]['text':' Regular mutator version of the && version above','line_number':460,'multiline':False]['text':'*
   * This is a shorthand for RegisterOperators::op(Options) where you can
   * specify the operator schema outside of the options parameter.
   * See class doc comment for examples.
   ','line_number':466,'multiline':True]['text':' internal only for registering caffe2 ops','line_number':475,'multiline':False]['text':'*
   * This API registers an operator based on a kernel function pointer.
   *
   * Given a kernel
   *
   * > namespace { Tensor my_kernel_cpu(Tensor a, Tensor b) {...} }
   *
   * This API looks like:
   *
   * > static auto registry = c10::RegisterOperators()
   * >     .op("my_op", &my_kernel_cpu);
   *
   * If your kernel is small and the overhead of calling it matters,
   * then this API might be the wrong choice since the following API
   * has a slightly lower overhead for calling into the kernel:
   *
   * > static auto registry = c10::RegisterOperators()
   * >     .op("my_op", c10::RegisterOperators::options()
   * >         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>());
   *
   * Or, alternatively, write your kernel as a functor:
   *
   * > namespace {
   * >   class my_kernel_cpu final : public c10::OperatorKernel {
   * >   public:
   * >     Tensor operator()(Tensor a, Tensor b) {...}
   * >   };
   * > }
   * >
   * > static auto registry = c10::RegisterOperators()
   * >     .op("my_op", c10::RegisterOperators::options()
   * >         .kernel<my_kernel_cpu>());
   ','line_number':486,'multiline':True]['text':' enable_if: only enable it if FuncType is actually a function, but not a stack based BoxedKernelFunction.','line_number':520,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor','line_number':528,'multiline':False]['text':'*
    * This API registers an operator based on a kernel lambda.
    *
    * This API looks like:
    *
    * > static auto registry = c10::RegisterOperators()
    * >     .op("my_op", [] (Tensor a, Tensor b) {...});
    *
    * This is equivalent to:
    *
    * > static auto registry = c10::RegisterOperators()
    * >     .op("my_op", c10::RegisterOperators::options()
    * >         .catchAllKernel([] (Tensor a, Tensor b) {...}));
    *
    ','line_number':533,'multiline':True]['text':' enable_if: only enable it if Lambda is actually a stateless lambda','line_number':549,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor','line_number':559,'multiline':False]['text':' enable_if: only enable it if Lambda is actually a functor but not a stateless lambda','line_number':566,'multiline':False]['text':' TODO Do schema inference without relying on WrapFunctionIntoRuntimeFunctor','line_number':576,'multiline':False]['text':' namespace c10','line_number':591,'multiline':False]['text':' Old-style API','line_number':594,'multiline':False]