['text':'
 * This file implements a variable fallback kernel for custom operators.
 * Since tensors always have the Autograd set, but custom operators
 * usually don't have a kernel registered for Autograd, the dispatcher
 * will call into this fallback kernel instead.
 * Note that this is not a correct autograd implementation. It will just
 * fallthrough to the custom operator implementation.
 * If you want a custom operator to work with autograd, you need to use
 * autograd::Function so that the custom operator implementation knows how to
 * do autograd.
 * Note also that ops from native_functions.yaml register their own variable
 * kernels, so this is never called for them.
 ','line_number':6,'multiline':True]['text':' TODO This whole file should be deleted and replaced with the mechanism','line_number':20,'multiline':False]['text':'      described in https://github.com/pytorch/pytorch/issues/29548','line_number':21,'multiline':False]['text':' Register fallthrough for Autograd backends dispatch keys','line_number':27,'multiline':False]['text':' NB: But not the private use ones; maybe the extension wants','line_number':28,'multiline':False]['text':' to override it themselves!','line_number':29,'multiline':False]['text':' NOTE [mobile/edge builds and the autograd fallback]','line_number':37,'multiline':False]['text':' To save on binary size, some of the mobile configs don't include the','line_number':38,'multiline':False]['text':' autograd kernels for built-in operators (VariableTypeEverything.cpp).','line_number':39,'multiline':False]['text':' For the mobile build:','line_number':40,'multiline':False]['text':' - we don't care about having a nice autograd fallback that warns if','line_number':41,'multiline':False]['text':' an operator has incorrect autograd support. If you're running','line_number':42,'multiline':False]['text':' a custom operator on mobile then it's already too late for us to warn','line_number':43,'multiline':False]['text':' or error on it.','line_number':44,'multiline':False]['text':' - for perf reasons, we do not want mobile to go through autograd_fallback','line_number':45,'multiline':False]['text':' for all operators (the boxing/unboxing adds overhead).','line_number':46,'multiline':False]['text':' As a result, on mobile we set the fallback to the fallthrough.','line_number':47,'multiline':False]['text':' see Note [ADInplaceOrView key]','line_number':85,'multiline':False]['text':' PyTorch has separate builds, some of which don't include autograd.','line_number':100,'multiline':False]['text':' So we define some behavior for when autograd isn't included and','line_number':101,'multiline':False]['text':' go through a layer of indirection (VariableHooksInterface) when it is.','line_number':102,'multiline':False]['text':' See aten/src/ATen/core/VariableHooksInterface.h for more details.','line_number':103,'multiline':False]['text':' namespace','line_number':111,'multiline':False]