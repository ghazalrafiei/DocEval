['text':' DO NOT DEFINE STATIC DATA IN THIS HEADER!','line_number':3,'multiline':False]['text':' See Note [Do not compile initializers with AVX]','line_number':4,'multiline':False]['text':' This file defines Vectorized<> for the quantized types.','line_number':18,'multiline':False]['text':'','line_number':19,'multiline':False]['text':'','line_number':20,'multiline':False]['text':' Currently, we simply use these classes as efficient converters between','line_number':21,'multiline':False]['text':' the quantized types and Vectorized<float>, usually in bandwidth-bound cases','line_number':22,'multiline':False]['text':' where doing the arithmetic in full-precision is acceptable (e.g.','line_number':23,'multiline':False]['text':' elementwise operators).','line_number':24,'multiline':False]['text':'','line_number':25,'multiline':False]['text':'','line_number':26,'multiline':False]['text':' Conversions are as follows:','line_number':27,'multiline':False]['text':'  Vectorized<qint8> -> 4x Vectorized<float>','line_number':28,'multiline':False]['text':'  Vectorized<quint8> -> 4x Vectorized<float>','line_number':29,'multiline':False]['text':'  Vectorized<qint32> -> 1x Vectorized<float>','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' The size of the returned float vector is specified by the special','line_number':32,'multiline':False]['text':' constexpr function float_num_vecs. The type of the value returned','line_number':33,'multiline':False]['text':' from dequantize (and expected as an argument to quantize) is','line_number':34,'multiline':False]['text':' specified by float_vec_return_type.','line_number':35,'multiline':False]['text':'','line_number':36,'multiline':False]['text':' When writing kernels with these vectors, it is expected that floating-','line_number':37,'multiline':False]['text':' point operations will be carried out in a loop over Vectorized<T>::float_num_vecs','line_number':38,'multiline':False]['text':' iterations.','line_number':39,'multiline':False]['text':' This function is for linkage only, will not be used','line_number':73,'multiline':False]['text':' Note: this function only convert inputs number of elements equal to at::vec::Vectorized<float>.size()','line_number':102,'multiline':False]['text':' Only handle first 128 bits','line_number':103,'multiline':False]['text':' Convert from 16*u8 to 16*int32','line_number':105,'multiline':False]['text':' Convert from 16*int32 to 16*float32','line_number':107,'multiline':False]['text':' Convert from float32 to int32 with truncation','line_number':112,'multiline':False]['text':' Convert from int32 to int16 using signed saturation','line_number':115,'multiline':False]['text':' Convert from int16 to uint8 using unsigned saturation','line_number':121,'multiline':False]['text':' This is the largest int32 value < int32_max exactly representable in float','line_number':142,'multiline':False]['text':' clang-format off','line_number':147,'multiline':False]['text':' clang-format on','line_number':165,'multiline':False]['text':' x','line_number':174,'multiline':False]['text':' If the floating point value is greater than int32_max,','line_number':177,'multiline':False]['text':' _mm512_cvtps_epi32 converts them to -ve. Clip at int32_float_max_val to','line_number':178,'multiline':False]['text':' Clip at int32_float_max_val to avoid this.','line_number':179,'multiline':False]['text':' y','line_number':182,'multiline':False]['text':' z','line_number':187,'multiline':False]['text':' w','line_number':192,'multiline':False]['text':' add zero point','line_number':203,'multiline':False]['text':' Additional 8-lane AVX512 version to take advantage when len is smaller','line_number':219,'multiline':False]['text':' based on fbgemm::QuantizeAvx2 (https://github.com/pytorch/FBGEMM)','line_number':220,'multiline':False]['text':' Not exactly the same behavior as the vectorized code.','line_number':241,'multiline':False]['text':' The vectorized code above always rounds to even in halfway cases','line_number':242,'multiline':False]['text':' (https://software.intel.com/en-us/node/523819), but std::nearbyint','line_number':243,'multiline':False]['text':' does the same only when the current rounding mode is FE_TONEAREST.','line_number':244,'multiline':False]['text':' However, in practice, this should not be a problem because most cases','line_number':245,'multiline':False]['text':' use the default rounding mode FE_TONEAREST.','line_number':246,'multiline':False]['text':' Note that we cannot implement the same behavior as the vectorized code','line_number':247,'multiline':False]['text':' using std::round because it does rounding away from zero in halfway','line_number':248,'multiline':False]['text':' cases.','line_number':249,'multiline':False]['text':' Broadcast constructor','line_number':282,'multiline':False]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':302,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':303,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':304,'multiline':False]['text':'precision=','line_number':334,'multiline':True]['text':' Load from memory constructor','line_number':375,'multiline':False]['text':'
 * Convert values from int32 back to int8/uint8
 ','line_number':400,'multiline':True]['text':' Add zero point ','line_number':426,'multiline':True]['text':' Pack to int16_t and saturate ','line_number':432,'multiline':True]['text':'
   * xyzw_clamped_v has results in the following layout so we need to
   * permute: x0-3 y0-3 z0-3 w0-3 x4-7 y4-7 z4-7 w4-7 x8-11 y8-11 z8-11 w8-11 x12-15 y12-15 z12-15 w12-15
   ','line_number':439,'multiline':True]['text':' Broadcast constructor','line_number':471,'multiline':False]['text':' This is needed because the compiler emits awful code for the default','line_number':477,'multiline':False]['text':' constructor for moving the enum','line_number':478,'multiline':False]['text':' This is added to avoid error: definition of implicit copy assignment operator','line_number':481,'multiline':False]['text':' for 'Vectorized<c10::qint8>' is deprecated because it has a user-declared','line_number':482,'multiline':False]['text':' copy constructor [-Werror,-Wdeprecated-copy]','line_number':483,'multiline':False]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':500,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':501,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':502,'multiline':False]['text':' Load from memory constructor','line_number':634,'multiline':False]['text':' Broadcast constructor','line_number':669,'multiline':False]['text':' This is added to avoid error: definition of implicit copy assignment operator','line_number':677,'multiline':False]['text':' for 'Vectorized<c10::quint8>' is deprecated because it has a user-declared','line_number':678,'multiline':False]['text':' copy constructor [-Werror,-Wdeprecated-copy]','line_number':679,'multiline':False]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':696,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':697,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':698,'multiline':False]['text':' Load from memory constructor','line_number':832,'multiline':False]['text':' NOTE: These are low-performance implementations that we fall back on.','line_number':845,'multiline':False]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':958,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':959,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':960,'multiline':False]['text':'precision=','line_number':980,'multiline':True]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':1102,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':1103,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':1104,'multiline':False]['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':1234,'multiline':False]['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':1235,'multiline':False]['text':' instructions while a loop would be compiled to one instruction.','line_number':1236,'multiline':False]['text':' defined(CPU_CAPABILITY_AVX512) && !defined(MSVC)','line_number':1336,'multiline':False]