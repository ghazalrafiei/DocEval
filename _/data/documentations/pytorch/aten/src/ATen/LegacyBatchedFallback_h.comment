['text':' If an operator doesn't have a batching rule implemented then we fallback','line_number':8,'multiline':False]['text':' to this implementation. The fallback only works on out-of-place operators','line_number':9,'multiline':False]['text':' that return only tensors with new memory. (e.g., no in-place operators, no','line_number':10,'multiline':False]['text':' view operations).','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':' The fallback effectively takes all of the BatchedTensors in `stack`, slices','line_number':13,'multiline':False]['text':' them, and runs `op` on all of the corresponding slices to produce slices','line_number':14,'multiline':False]['text':' of the outputs. The output slices then get `torch.stack`ed to create the','line_number':15,'multiline':False]['text':' final returns.','line_number':16,'multiline':False]['text':'','line_number':17,'multiline':False]['text':' The performance of the fallback is not very good because it introduces an','line_number':18,'multiline':False]['text':' extra copy from stacking the sliced outputs. Because of this, we prefer to','line_number':19,'multiline':False]['text':' write batching rules for operators whenever possible.','line_number':20,'multiline':False]['text':' namespace at','line_number':25,'multiline':False]