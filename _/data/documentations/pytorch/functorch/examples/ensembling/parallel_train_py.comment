['text':' Adapted from http://willwhitney.com/parallel-training-jax.html , which is a','line_number':9,'multiline':False]['text':' tutorial on Model Ensembling with JAX by Will Whitney.','line_number':10,'multiline':False]['text':'','line_number':11,'multiline':False]['text':' The original code comes with the following citation:','line_number':12,'multiline':False]['text':' @misc{Whitney2021Parallelizing,','line_number':13,'multiline':False]['text':'     author = {William F. Whitney},','line_number':14,'multiline':False]['text':'     title = { {Parallelizing neural networks on one GPU with JAX} },','line_number':15,'multiline':False]['text':'     year = {2021},','line_number':16,'multiline':False]['text':'     url = {http://willwhitney.com/parallel-training-jax.html},','line_number':17,'multiline':False]['text':' }','line_number':18,'multiline':False]['text':' GOAL: Demonstrate that it is possible to use eager-mode vmap','line_number':20,'multiline':False]['text':' to parallelize training over models.','line_number':21,'multiline':False]['text':' Step 1: Make some spirals','line_number':34,'multiline':False]['text':' Step 2: Define two-layer MLP and loss function','line_number':59,'multiline':False]['text':' NB: PyTorch is missing a "functional optimizer API" (possibly coming soon)','line_number':89,'multiline':False]['text':' so we are going to re-implement SGD here.','line_number':90,'multiline':False]['text':' Step 4: Let's verify this actually trains.','line_number':99,'multiline':False]['text':' We should see the loss decrease.','line_number':100,'multiline':False]['text':' Step 5: We're ready for multiple models. Let's define an init_fn','line_number':111,'multiline':False]['text':' that, given a number of models, returns to us all of the weights.','line_number':112,'multiline':False]['text':' Step 6: Now, can we try multiple models at the same time?','line_number':121,'multiline':False]['text':' The answer is: yes! `loss` is a 2-tuple, and we can see that the value keeps','line_number':122,'multiline':False]['text':' on decreasing','line_number':123,'multiline':False]['text':' Step 7: Now, the flaw with step 6 is that we were training on the same exact','line_number':137,'multiline':False]['text':' data. This can lead to all of the models in the ensemble overfitting in the','line_number':138,'multiline':False]['text':' same way. The solution that http://willwhitney.com/parallel-training-jax.html','line_number':139,'multiline':False]['text':' applies is to randomly subset the data in a way that the models do not recieve','line_number':140,'multiline':False]['text':' exactly the same data in each training step!','line_number':141,'multiline':False]['text':' Because the goal of this doc is to show that we can use eager-mode vmap to','line_number':142,'multiline':False]['text':' achieve similar things as JAX, the rest of this is left as an exercise to the reader.','line_number':143,'multiline':False]['text':' In conclusion, to achieve what http://willwhitney.com/parallel-training-jax.html','line_number':145,'multiline':False]['text':' does, we used the following additional items that PyTorch does not have:','line_number':146,'multiline':False]['text':' 1. NN module functional API that turns a module into a (state, state_less_fn) pair','line_number':147,'multiline':False]['text':' 2. Functional optimizers','line_number':148,'multiline':False]['text':' 3. A "functional" grad API (that effectively wraps autograd.grad)','line_number':149,'multiline':False]['text':' 4. Composability between the functional grad API and torch.vmap.','line_number':150,'multiline':False]