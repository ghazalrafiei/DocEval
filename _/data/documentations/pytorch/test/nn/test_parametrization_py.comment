['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':25,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':26,'multiline':False]['text':' torch/nn/utils/parametrize','line_number':27,'multiline':False]['text':' Define a couple matrix parametrizations','line_number':34,'multiline':False]['text':' Cayley map','line_number':42,'multiline':False]['text':' If X is skew-symmetric it returns an orthogonal matrix','line_number':43,'multiline':False]['text':' We call contiguous because solve returns a tensor with strides that are Fortran-contiguous','line_number':45,'multiline':False]['text':' and autograd raises a performance warning.','line_number':46,'multiline':False]['text':' This happens when we remove the parametrization with leave_parametrized=True,','line_number':47,'multiline':False]['text':' which does a set_ with a non-contiguous tensor while the gradient is contiguous','line_number':48,'multiline':False]['text':' Define a couple vector parametrizations','line_number':59,'multiline':False]['text':' Test unsafe flag','line_number':73,'multiline':False]['text':' default unsafe = False','line_number':75,'multiline':False]['text':' One parametrization with unsafe=True','line_number':78,'multiline':False]['text':' Two parametrizations with unsafe=True','line_number':93,'multiline':False]['text':' Test unsafe flag doesn't change expected behavior','line_number':109,'multiline':False]['text':' Result should be skew-symmetric','line_number':116,'multiline':False]['text':' Remove and check consistency','line_number':119,'multiline':False]['text':' Test one parametrization','line_number':126,'multiline':False]['text':' Result should be skew-symmetric','line_number':133,'multiline':False]['text':' Remove and check consistency','line_number':136,'multiline':False]['text':' Test two parametrizations at the same time and removing them','line_number':143,'multiline':False]['text':' Result should be orthogonal','line_number':146,'multiline':False]['text':' Structure tests','line_number':150,'multiline':False]['text':' Remove','line_number':157,'multiline':False]['text':' Add everything','line_number':164,'multiline':False]['text':' Basic tests','line_number':170,'multiline':False]['text':' Nothing weird has happpened','line_number':176,'multiline':False]['text':' Should not throw','line_number':177,'multiline':False]['text':' Remove first parametrization.','line_number':189,'multiline':False]['text':' Check that the model is still parametrized and so is the second parameter','line_number':190,'multiline':False]['text':' Still parametrized','line_number':192,'multiline':False]['text':' Parametrization removed','line_number':193,'multiline':False]['text':' Still parametrized','line_number':194,'multiline':False]['text':' Still parametrized','line_number':195,'multiline':False]['text':' Still parametrized','line_number':196,'multiline':False]['text':' Has been updated','line_number':197,'multiline':False]['text':' Keeps the same id','line_number':198,'multiline':False]['text':' Nothing weird has happened','line_number':199,'multiline':False]['text':' Should not throw','line_number':200,'multiline':False]['text':' Remove the second parametrization.','line_number':209,'multiline':False]['text':' Check that the module is not parametrized','line_number':210,'multiline':False]['text':' Not parametrized','line_number':212,'multiline':False]['text':' Has been updated','line_number':213,'multiline':False]['text':' Not parametrized','line_number':214,'multiline':False]['text':' Not parametrized','line_number':215,'multiline':False]['text':' Keeps the same id','line_number':216,'multiline':False]['text':' Not parametrized the module','line_number':217,'multiline':False]['text':' Resores the previous class','line_number':218,'multiline':False]['text':' Nothing weird has happeed','line_number':219,'multiline':False]['text':' Should not throw things are updated','line_number':221,'multiline':False]['text':' Test leave_parametrized=True','line_number':230,'multiline':False]['text':' We didn't change the dtype nor had multiple inputs, so the id should be the same','line_number':235,'multiline':False]['text':' Should not throw. Things are updated','line_number':239,'multiline':False]['text':' Add top level parametrization','line_number':258,'multiline':False]['text':' Result should be skew-symmetric','line_number':265,'multiline':False]['text':' Add nested parametrization','line_number':269,'multiline':False]['text':' Result should be skew-symmetric','line_number':280,'multiline':False]['text':' Remove nested param and check consistency','line_number':284,'multiline':False]['text':' Remove top level and check consistency','line_number':289,'multiline':False]['text':' Define a couple vector parametrizations','line_number':296,'multiline':False]['text':' Instantiate parametrizations on buffers. It should work as expected','line_number':307,'multiline':False]['text':' Remove parametrizations on buffers. It should work as expected','line_number':319,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':328,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':329,'multiline':False]['text':' A stateful parametrization','line_number':333,'multiline':False]['text':' Integrity tests','line_number':366,'multiline':False]['text':' Trying to save the whole parametrized model raises','line_number':371,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':376,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':377,'multiline':False]['text':' Implements a Cayley map where right_inverse is not quite the inverse of forward','line_number':396,'multiline':False]['text':' cayley(0) == Id, so B @ cayley(0) == B','line_number':413,'multiline':False]['text':' Register the skew-symmetric constraint. The result is now skew-symmetric','line_number':419,'multiline':False]['text':' Make the weight skew-symmetric before registering the parametrization','line_number':421,'multiline':False]['text':' X is not skew-symmetric, so it throws an error','line_number':426,'multiline':False]['text':' Make X skew-symmetric','line_number':429,'multiline':False]['text':' Having several parametrizations registered should work in the same way','line_number':435,'multiline':False]['text':' Register now the Cayley map. The result is now orthogonal','line_number':437,'multiline':False]['text':' X is not orthogonal, so it throws an error','line_number':439,'multiline':False]['text':' Test errors when registering a parametrization on an unparametrized tensor','line_number':448,'multiline':False]['text':' Register a parametrization on a non-existing parameter throws','line_number':456,'multiline':False]['text':' Removing parametrizations from an unparametrized tensor throws','line_number':461,'multiline':False]['text':' A correct parametrization with several outputs','line_number':466,'multiline':False]['text':' Cannot remove a parametrization with several outputs with `leave_parametrized=False`','line_number':475,'multiline':False]['text':' A parametrization with an incorrect number of outputs','line_number':480,'multiline':False]['text':' Makes param(*param.right_inverse(X)) fail','line_number':488,'multiline':False]['text':' A parametrization with a right_inverse that does not return a Tensor or Sequence[Tensor]','line_number':493,'multiline':False]['text':' right_inverse should return a Tensor or a Sequence[Tensor]','line_number':498,'multiline':False]['text':' If it's a sequence, it must to be a sequence of tensors','line_number':503,'multiline':False]['text':' A parametrization from one tensor to one tensor that changes the dtype','line_number':515,'multiline':False]['text':' For parametrizations that return one tensor, right_inverse may not change the dtype','line_number':523,'multiline':False]['text':' Doesn't return a tensor','line_number':528,'multiline':False]['text':' Forward must return a tensor','line_number':533,'multiline':False]['text':' A parametrization from one tensor to one tensor that changes the dtype','line_number':538,'multiline':False]['text':' forward should not change the initial dtype','line_number':543,'multiline':False]['text':' Change shape','line_number':548,'multiline':False]['text':' forward should not change the original shape','line_number':553,'multiline':False]['text':' Many to one that changes dtype','line_number':558,'multiline':False]['text':' forward should not change the original shape even for parametrizations with many inputs','line_number':566,'multiline':False]['text':' Returning a sequence of size one, although weird, it's correct','line_number':571,'multiline':False]['text':' Does not throw','line_number':582,'multiline':False]['text':' None of the operations above should have altered the weight','line_number':586,'multiline':False]['text':' Test errors when registering a parametrization on a parametrized tensor','line_number':591,'multiline':False]['text':' Has to return a tensor','line_number':600,'multiline':False]['text':' Cannot change dtype','line_number':611,'multiline':False]['text':' Cannot change shape','line_number':622,'multiline':False]['text':' The following checks are mostly due to bugs in the code of the parametrization','line_number':633,'multiline':False]['text':' right_inverse has to return a tensor','line_number':635,'multiline':False]['text':' Cannot change dtype','line_number':646,'multiline':False]['text':' Cannot change shape','line_number':657,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':668,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':669,'multiline':False]['text':' A parametrization with several outputs','line_number':672,'multiline':False]['text':' Form a rank-1 matrix from a pair of vectors','line_number':675,'multiline':False]['text':' We project the given matrix onto the rank 1 matrices','line_number':679,'multiline':False]['text':' S is ordered in a decreasing way.','line_number':681,'multiline':False]['text':' Simple parametrisation','line_number':685,'multiline':False]['text':' Test one parametrization','line_number':694,'multiline':False]['text':' Result should be rank 1','line_number':705,'multiline':False]['text':' Cannot remove a parametrization with multiple inputs and not leave it parametrized','line_number':709,'multiline':False]['text':' Remove parametrization and check consistency','line_number':711,'multiline':False]['text':' Registering parametrizations with one input on top of one with multiple inputs should work','line_number':719,'multiline':False]['text':' Projecting a rank 1 matrix onto the matrices of rank one does not change the matrix','line_number':722,'multiline':False]['text':' The matrix now is twice the initial matrix','line_number':725,'multiline':False]['text':' Multiplying by a scalar does not change the rank','line_number':727,'multiline':False]['text':' The model has now three parameters','line_number':730,'multiline':False]['text':' Test backward. Should not throw','line_number':735,'multiline':False]['text':' Same drill as before, removing should work as expected','line_number':742,'multiline':False]['text':' Cannot remove a parametrization with multiple inputs and not leave it parametrized','line_number':744,'multiline':False]['text':' Remove parametrization and check consistency','line_number':746,'multiline':False]['text':' The model has now two parameters','line_number':754,'multiline':False]['text':' Test backward. Should not throw','line_number':757,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':765,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':766,'multiline':False]['text':' Define a couple matrix parametrizations','line_number':770,'multiline':False]['text':' Test that the caching system works','line_number':785,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':791,'multiline':False]['text':'        and remove the `@skipIfNoLapack` (see #70995)','line_number':792,'multiline':False]['text':' test that the results are distinct objects for each module','line_number':822,'multiline':False]['text':' Emulate custom implementation of the deepcopying.','line_number':872,'multiline':False]['text':' Weights, biases and attributes should be equal but they must be different objects.','line_number':884,'multiline':False]['text':' General check that we are able to create deepcopy.','line_number':895,'multiline':False]['text':' Check that this works on models with several parametrized tensors.','line_number':898,'multiline':False]['text':' Check that this works on models where tensors have more than one parametrization.','line_number':901,'multiline':False]['text':' checks that final and original value are correct and the to_model is parametrized','line_number':934,'multiline':False]['text':' check that the transfer didn't affect the original value','line_number':942,'multiline':False]['text':' testing that changes to one set of parametrizations do not affect the other','line_number':945,'multiline':False]['text':' also test that parameters that don't exist in to_model get transferred','line_number':950,'multiline':False]['text':' check that previously missing params got transferred correctly','line_number':958,'multiline':False]['text':' check that the new transfer didn't change the value for the from_module','line_number':965,'multiline':False]['text':' check that transfer occurs successfully','line_number':987,'multiline':False]['text':' check that transfer doesn't affect the from_model weight','line_number':994,'multiline':False]['text':' check that weight and only weight was transferred','line_number':1025,'multiline':False]['text':' FIXME: Rewrite this test using functions not depending on LAPACK','line_number':1033,'multiline':False]['text':' and remove the `@skipIfNoLapack` (see #70995)','line_number':1034,'multiline':False]['text':' A parametrization with several outputs','line_number':1037,'multiline':False]['text':' Form a rank-1 matrix from a pair of vectors','line_number':1040,'multiline':False]['text':' We project the given matrix onto the rank 1 matrices','line_number':1044,'multiline':False]['text':' S is ordered in a decreasing way.','line_number':1046,'multiline':False]['text':' checks that final and original value are correct and the to_model is parametrized','line_number':1065,'multiline':False]['text':' check that the transfer didn't affect the original value','line_number':1077,'multiline':False]['text':' testing that changes to one set of parametrizations do not affect the other','line_number':1080,'multiline':False]['text':' also check that previously missing params got transferred correctly','line_number':1088,'multiline':False]['text':' check that the new transfer didn't change the value for the from_module','line_number':1099,'multiline':False]['text':' .parametrizations.weight.original should be trainable','line_number':1111,'multiline':False]['text':' u should be just a reused buffer','line_number':1115,'multiline':False]['text':' weight should be a plain attribute, not counted as a buffer or a param','line_number':1120,'multiline':False]['text':' it should also be sharing storage as `weight_orig`','line_number':1125,'multiline':False]['text':' self.assertEqual(m.parametrizations.weight.original.storage(), m.weight.storage())','line_number':1126,'multiline':False]['text':' spectral_norm is the only parametrization','line_number':1132,'multiline':False]['text':' We can register spectral_norm multiple times on the same parameter','line_number':1136,'multiline':False]['text':' and on multiple parameters in the same module','line_number':1137,'multiline':False]['text':' If we remove the parametrization on bias, weight is still parametrized','line_number':1142,'multiline':False]['text':' Removing a parametrization runs forward in eval mode if leave_parametrized=True','line_number':1143,'multiline':False]['text':' Neither weight and bias are parametrized','line_number':1150,'multiline':False]['text':' test correctness in training/eval modes and cpu/multi-gpu settings','line_number':1155,'multiline':False]['text':' TEST TRAINING BEHAVIOR','line_number':1187,'multiline':False]['text':' We perform GD first to modify the initial matrix','line_number':1189,'multiline':False]['text':' run forward again and assert that u and v are updated','line_number':1198,'multiline':False]['text':' assert that backprop reaches original weight','line_number':1202,'multiline':False]['text':' can't use gradcheck because the function changes as we','line_number':1203,'multiline':False]['text':' activate through it in training mode','line_number':1204,'multiline':False]['text':' test backward works with multiple forwards','line_number':1208,'multiline':False]['text':' it uses training mode so we need to reset `u` and `v` vectors','line_number':1209,'multiline':False]['text':' to same value at beginning for finite difference test to pass','line_number':1210,'multiline':False]['text':' Make sure we can compute gradients wrt to all the parameters in the case','line_number':1221,'multiline':False]['text':' of double forward','line_number':1222,'multiline':False]['text':' test removing','line_number':1226,'multiline':False]['text':' spectral norm module needs to be in eval mode if we'd like to','line_number':1227,'multiline':False]['text':' avoid doing another power iteration','line_number':1228,'multiline':False]['text':' TEST EVAL BEHAVIOR','line_number':1242,'multiline':False]['text':' assert eval gives same result as last training iteration','line_number':1252,'multiline':False]['text':' assert doing more iteartion in eval don't change things','line_number':1254,'multiline':False]['text':' FIXME: the code below is flaky when executed with DataParallel','line_number':1259,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/13818','line_number':1260,'multiline':False]['text':' test backward works with multiple forwards in mixed training','line_number':1264,'multiline':False]['text':' and eval modes','line_number':1265,'multiline':False]['text':' it uses training mode so we need to reset `u` and `v` vectors','line_number':1266,'multiline':False]['text':' to same value at beginning for finite difference test to pass','line_number':1267,'multiline':False]['text':' assert that backprop reaches weight_orig in eval','line_number':1286,'multiline':False]['text':' test that non-strict loading works','line_number':1311,'multiline':False]['text':' set W as a buffer','line_number':1323,'multiline':False]['text':' remove metadata info','line_number':1325,'multiline':False]['text':' remove W buffer','line_number':1327,'multiline':False]['text':' normal state_dict','line_number':1332,'multiline':False]['text':' test that re-wrapping does not matter','line_number':1334,'multiline':False]['text':' test that re-wrapping does not matter','line_number':1348,'multiline':False]['text':' Test normal loading','line_number':1352,'multiline':False]['text':' this should not run into incompatible shapes','line_number':1368,'multiline':False]['text':' check that u refers to the same dimension','line_number':1370,'multiline':False]['text':' naive forward','line_number':1378,'multiline':False]['text':' Orthogonal implements 6 algorithms (3x parametrizations times 2 options of use_trivialization)','line_number':1393,'multiline':False]['text':' Test that weight is equal to the Q part of the QR decomposition of W','line_number':1405,'multiline':False]['text':' (or of its transpose if the matrix is wide)','line_number':1406,'multiline':False]['text':' square/ tall / wide','line_number':1416,'multiline':False]['text':' Conv2d does not support complex yet','line_number':1419,'multiline':False]['text':' right_inverse for Cayley and matrix_exp not implemented for use_trivialization=False','line_number':1430,'multiline':False]['text':' See Note [right_inverse expm cayley]','line_number':1431,'multiline':False]['text':' We generate them every time to always start with fresh weights','line_number':1434,'multiline':False]['text':' We do not support householder for complex inputs','line_number':1440,'multiline':False]['text':' See Note [Householder complex]','line_number':1441,'multiline':False]['text':' Forwards works as expected','line_number':1457,'multiline':False]['text':' Intializing with a given orthogonal matrix works','line_number':1463,'multiline':False]['text':' Intializing with a non-orthogonal matrix makes m.weight be the Q part of the given matrix','line_number':1478,'multiline':False]['text':' We do not update the upper triangular part of the matrix if tall tril if wide','line_number':1494,'multiline':False]['text':' The gradient in the diagonal can only be imaginary because a skew-Hermitian','line_number':1500,'multiline':False]['text':' matrix has imaginary diagonal','line_number':1501,'multiline':False]['text':' add weight normalization','line_number':1556,'multiline':False]['text':' remove weight norm','line_number':1562,'multiline':False]['text':' test with dim=1','line_number':1567,'multiline':False]['text':' test with dim=None','line_number':1573,'multiline':False]