['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]['text':' This file was generated by running on PyTorch 1.0.1 on Python 2:','line_number':47,'multiline':False]['text':'','line_number':48,'multiline':False]['text':'     import torch','line_number':49,'multiline':False]['text':'     from torch import nn','line_number':50,'multiline':False]['text':'     m = nn.Conv2d(1, 1, 1)','line_number':51,'multiline':False]['text':'     torch.save(m, 'legacy_conv2d.pt')','line_number':52,'multiline':False]['text':'','line_number':53,'multiline':False]['text':' NB: This Pickle also contains some Unicode data!','line_number':54,'multiline':False]['text':' Negative stride check','line_number':71,'multiline':False]['text':' Test for https://github.com/pytorch/pytorch/issues/55781','line_number':90,'multiline':False]['text':' Disable MKLDNN explicitly, so that either NNPACK or THCNN will be used','line_number':96,'multiline':False]['text':' Negative stride check','line_number':115,'multiline':False]['text':' Zero stride check','line_number':121,'multiline':False]['text':' Negative stride check','line_number':133,'multiline':False]['text':' Compare module against functional: without strides/dilation, asymmetric padding','line_number':148,'multiline':False]['text':' Test dilation, symmetric padding','line_number':155,'multiline':False]['text':' Test non-zero padding_mode, requiring explicit padding','line_number':161,'multiline':False]['text':' Test connstruction with invalid padding string raises','line_number':169,'multiline':False]['text':' Test connstruction with same padding and strides raises','line_number':173,'multiline':False]['text':' Compare module against functional:','line_number':178,'multiline':False]['text':' without strides/dilation, both symmetric and asymmetric padding','line_number':179,'multiline':False]['text':' with dilation, symmetric padding','line_number':186,'multiline':False]['text':' Test non-zero padding_mode, requiring explicit padding','line_number':192,'multiline':False]['text':' Test connstruction with invalid padding string raises','line_number':200,'multiline':False]['text':' Test connstruction with same padding and strides raises','line_number':204,'multiline':False]['text':' Compare module against functional:','line_number':213,'multiline':False]['text':' without dilation, both symmetric and asymmetric padding','line_number':215,'multiline':False]['text':' with dilation, both symmetric and asymmetric padding','line_number':221,'multiline':False]['text':' Test non-zero padding_mode, requiring explicit padding','line_number':227,'multiline':False]['text':' Test connstruction with invalid padding string raises','line_number':235,'multiline':False]['text':' Test connstruction with same padding and strides raises','line_number':239,'multiline':False]['text':' inconsistent types should raise an exception','line_number':281,'multiline':False]['text':' but it should work with the same type','line_number':283,'multiline':False]['text':' inconsistent types should raise an exception','line_number':293,'multiline':False]['text':' but it should work with the same type','line_number':297,'multiline':False]['text':' inconsistent types should raise an exception','line_number':361,'multiline':False]['text':' but it should work with the same type','line_number':365,'multiline':False]['text':' just run it to ensure no exception raised.','line_number':402,'multiline':False]['text':' Conv1d','line_number':406,'multiline':False]['text':' Conv2d','line_number':413,'multiline':False]['text':' Conv3D','line_number':418,'multiline':False]['text':' Check that ConvTranspose3d can take a 5d output_size.','line_number':467,'multiline':False]['text':' For https://github.com/pytorch/pytorch/pull/1273','line_number':481,'multiline':False]['text':' Almost identical to the above `test_Conv2d_naive_groups`','line_number':482,'multiline':False]['text':' Almost identical to the above `test_Conv2d_naive_groups`','line_number':518,'multiline':False]['text':' Covering special case when group > 1, input-channel / group < 16 and output-channel is multiple of 16','line_number':519,'multiline':False]['text':' See also https://github.com/pytorch/pytorch/pull/18463#issuecomment-476563686','line_number':520,'multiline':False]['text':' and https://github.com/pytorch/pytorch/pull/18463#issuecomment-477001024','line_number':521,'multiline':False]['text':' CPU-only test for group conv3d fast implementation using bmm','line_number':558,'multiline':False]['text':' See: https://github.com/pytorch/pytorch/pull/36355','line_number':559,'multiline':False]['text':' in order to catch the hols in grouped convolution in nhwc support for earlier cudnn version','line_number':636,'multiline':False]['text':' desired behavior here is to have the memory_layout of conv.weight to','line_number':647,'multiline':False]['text':' dominante the layout of output.','line_number':648,'multiline':False]['text':' which is not the same as current behavior, we'll fix this in','line_number':649,'multiline':False]['text':' following up PRs and remove the `expectedFailure` tag','line_number':650,'multiline':False]['text':' Noncontiguous weights must be contiguous() before being','line_number':672,'multiline':False]['text':' passed to cuDNN','line_number':673,'multiline':False]['text':' Conv 1D','line_number':769,'multiline':False]['text':' Conv 2D','line_number':783,'multiline':False]['text':' Conv 3D','line_number':797,'multiline':False]['text':' We disable cudnn during forward to avoid finite difference imprecision issues','line_number':872,'multiline':False]['text':' Issue #15353: test mkldnn double backward, don't run gradgradcheck due','line_number':885,'multiline':False]['text':' to imprecision issues','line_number':886,'multiline':False]['text':' These sizes require huge cuDNN workspaces. Make sure we choose a','line_number':916,'multiline':False]['text':' reasonable algorithm that does not run out of memory','line_number':917,'multiline':False]['text':' Very similar to test_Conv2d_naive_groups but with special care to handle','line_number':955,'multiline':False]['text':' the number of groups == number of input channels','line_number':956,'multiline':False]['text':' FIXME: remove after adding non-contiguous grad tests for all modules','line_number':1045,'multiline':False]['text':' Double backward only runs with DoubleTensor due to precision reason','line_number':1064,'multiline':False]['text':' Cannot provide ggW when stride is > 1','line_number':1141,'multiline':False]['text':' Test padding='same' outputs the correct shape','line_number':1153,'multiline':False]['text':' in_size','line_number':1155,'multiline':False]['text':' kernel_size','line_number':1157,'multiline':False]['text':' dilation','line_number':1159,'multiline':False]['text':' stride','line_number':1161,'multiline':False]['text':' Compare F.conv1d padding='same' output against manual padding','line_number':1170,'multiline':False]['text':' Without strides/dilation','line_number':1171,'multiline':False]['text':' With dilation','line_number':1178,'multiline':False]['text':' Dilation with asymmetric padding','line_number':1185,'multiline':False]['text':' Compare F.conv2d padding='same' output against manual padding','line_number':1196,'multiline':False]['text':' Without strides/dilation','line_number':1197,'multiline':False]['text':' With dilation','line_number':1204,'multiline':False]['text':' Dilation with asymmetric padding','line_number':1210,'multiline':False]['text':' Compare F.conv3d padding='same' output against manual padding','line_number':1222,'multiline':False]['text':' Without strides/dilation','line_number':1223,'multiline':False]['text':' With dilation','line_number':1230,'multiline':False]['text':' Dilation with asymmetric padding','line_number':1235,'multiline':False]['text':' Test F.conv1d padding='valid' is the same as no padding','line_number':1243,'multiline':False]['text':' Test F.conv2d padding='valid' is the same as no padding','line_number':1252,'multiline':False]['text':' Test F.conv3d padding='valid' is the same as no padding','line_number':1261,'multiline':False]['text':' Test F.conv1d gradients work with padding='same'','line_number':1270,'multiline':False]['text':' Symmetric padding','line_number':1274,'multiline':False]['text':' Asymmetric padding','line_number':1286,'multiline':False]['text':' Test F.conv2d gradients work with padding='same'','line_number':1300,'multiline':False]['text':' Symmetric padding','line_number':1304,'multiline':False]['text':' Asymmetric padding','line_number':1316,'multiline':False]['text':' Test F.conv3d gradients work with padding='same'','line_number':1332,'multiline':False]['text':' Symmetric padding','line_number':1336,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/70702','line_number':1351,'multiline':False]['text':' Asymmetric padding','line_number':1355,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/70702','line_number':1370,'multiline':False]['text':' Test F.conv1d gradients work with padding='valid'','line_number':1376,'multiline':False]['text':' SciPy expects two 1-D inputs.','line_number':1398,'multiline':False]['text':' `same` padding in PyTorch conv1d is different','line_number':1405,'multiline':False]['text':' from SciPy','line_number':1406,'multiline':False]['text':' We have already taken care of padding','line_number':1409,'multiline':False]['text':' second input is flipped in SciPy's convolve','line_number':1412,'multiline':False]['text':' Global dtype for this test suite is torch.double','line_number':1420,'multiline':False]['text':' This leads to change in type-promotion','line_number':1421,'multiline':False]['text':' and conv1d outputs `complex128` for `complex64` input.','line_number':1422,'multiline':False]['text':' SciPy expects two 2-D inputs.','line_number':1436,'multiline':False]['text':' `same` padding in PyTorch conv2d is different','line_number':1443,'multiline':False]['text':' from SciPy','line_number':1444,'multiline':False]['text':' We have already taken care of padding','line_number':1449,'multiline':False]['text':' second input is flipped in SciPy's convolve2d','line_number':1452,'multiline':False]['text':' Global dtype for this test suite is torch.double','line_number':1460,'multiline':False]['text':' This leads to change in type-promotion','line_number':1461,'multiline':False]['text':' and conv1d outputs `complex128` for `complex64` input.','line_number':1462,'multiline':False]['text':' SciPy expects two 3-D inputs.','line_number':1476,'multiline':False]['text':' `same` padding in PyTorch conv3d is different','line_number':1483,'multiline':False]['text':' from SciPy','line_number':1484,'multiline':False]['text':' We have already taken care of padding','line_number':1491,'multiline':False]['text':' second input is flipped in SciPy's convolve','line_number':1494,'multiline':False]['text':' Global dtype for this test suite is torch.double','line_number':1505,'multiline':False]['text':' This leads to change in type-promotion','line_number':1506,'multiline':False]['text':' and conv1d outputs `complex128` for `complex64` input.','line_number':1507,'multiline':False]['text':' Test F.conv2d gradients work with padding='valid'','line_number':1514,'multiline':False]['text':' Test F.conv3d gradients work with padding='valid'','line_number':1530,'multiline':False]['text':' For inputs with no batch dim, verify output is the correct shape when output_size is set.','line_number':1547,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/75889','line_number':1548,'multiline':False]['text':' === slow ===','line_number':1558,'multiline':False]['text':' CUDA doesn't have a slow 3D implementation, so it goes to the dilated 3D implementation instead','line_number':1577,'multiline':False]['text':' FIXME: RuntimeError: CUDA out of memory.','line_number':1580,'multiline':False]['text':' subtest(((2, 6, 7, 8, 9), True, False, 3, torch.strided, torch._C._ConvBackend.SlowTranspose3d),','line_number':1581,'multiline':False]['text':'         decorators=[onlyNativeDeviceTypes, disableMkldnn, disablecuDNN], name='slow3d_transposed'),','line_number':1582,'multiline':False]['text':' FIXME: RuntimeError: CUDA out of memory.','line_number':1585,'multiline':False]['text':' subtest(((2, 6, 7, 8, 9), True, True, 3, torch.strided, torch._C._ConvBackend.SlowTranspose3d),','line_number':1586,'multiline':False]['text':'         decorators=[onlyNativeDeviceTypes, disableMkldnn, disablecuDNN], name='slow3d_dilated_transposed'),','line_number':1587,'multiline':False]['text':' === cuda ===','line_number':1606,'multiline':False]['text':' Note that disablecuDNN disables miopen as well.','line_number':1607,'multiline':False]['text':' === cudnn ===','line_number':1614,'multiline':False]['text':' FIXME: RuntimeError: CUDA out of memory.','line_number':1625,'multiline':False]['text':' subtest(((2, 6, 7, 8, 9), True, False, 3, torch.strided, torch._C._ConvBackend.CudnnTranspose),','line_number':1626,'multiline':False]['text':'         decorators=[onlyCUDA, skipCUDAIfNoCudnn, skipCUDAIfMiopen], name='cudnn3d_transposed'),','line_number':1627,'multiline':False]['text':' === miopen ===','line_number':1628,'multiline':False]['text':' === mkldnn ===','line_number':1647,'multiline':False]['text':' Transposed convolution is broken for mkldnn. See https://github.com/pytorch/pytorch/issues/68775.','line_number':1654,'multiline':False]['text':' Note: Tests for mobile backends are not currently supported. This comprises','line_number':1685,'multiline':False]['text':' NnpackSpatial, Winograd3x3Depthwise, and Xnnpack2d backends. Testing these','line_number':1686,'multiline':False]['text':' requires the ability to gate tests by whether PyTorch is built with USE_MOBILE=1.','line_number':1687,'multiline':False]['text':' Test with both bias and no bias.','line_number':1689,'multiline':False]['text':' Test with both stride=1 and stride>1 cases.','line_number':1691,'multiline':False]['text':' Test with both contiguous and non-contiguous inputs.','line_number':1693,'multiline':False]['text':' Build up inputs.','line_number':1698,'multiline':False]['text':' Note that weight and bias are not supported as mkldnn tensors during training.','line_number':1723,'multiline':False]['text':' Ensure correct backend is selected.','line_number':1731,'multiline':False]['text':' Ensure backward call succeeds.','line_number':1735,'multiline':False]['text':' mkldnn doesn't support gradcheck :(','line_number':1745,'multiline':False]['text':' FIXME: forward AD fails','line_number':1749,'multiline':False]['text':' Forward AD and forward-over-reverse AD smoke test in float32','line_number':1750,'multiline':False]['text':' TODO: remove this if we introduce per-op gradient tests for float32','line_number':1751,'multiline':False]['text':' Forward AD','line_number':1754,'multiline':False]['text':' Forward over reverse AD','line_number':1756,'multiline':False]['text':' Convert to float64 for gradcheck.','line_number':1763,'multiline':False]['text':' Set some backend-specific validation settings.','line_number':1770,'multiline':False]['text':' cuDNN introduces non-determinism','line_number':1773,'multiline':False]['text':' double backward doesn't support bias gradients','line_number':1778,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80837.','line_number':1786,'multiline':False]['text':' Disable MKLDNN explicitly','line_number':1805,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/82060, N > 1 will call in OneDNN path.','line_number':1812,'multiline':False]['text':' Disable MKLDNN explicitly','line_number':1819,'multiline':False]['text':' Here we just test the convolution correctly route to the fallback implementation','line_number':1878,'multiline':False]['text':' that is, it does not crash. The correctness of fallback implementation should be','line_number':1879,'multiline':False]['text':' covered in other tests','line_number':1880,'multiline':False]['text':' need floats to exercise https://github.com/pytorch/pytorch/issues/16018','line_number':1904,'multiline':False]['text':' forward','line_number':1935,'multiline':False]['text':' cuDNN may use algorithms such as FFT that don't guarantee a diff of 0','line_number':1942,'multiline':False]['text':' forward','line_number':1960,'multiline':False]['text':' backward','line_number':1966,'multiline':False]['text':' When computing the backward, we are using the `max(dim=1)`` to create','line_number':1968,'multiline':False]['text':' some sparsity. Without this sparsity, the rounding error would be','line_number':1969,'multiline':False]['text':' too large (as large as 1e-5) to satisfy the creterion (1e-6) of `assertEqual`','line_number':1970,'multiline':False]['text':' gradients are at the order of hundreds, we need to scale it to','line_number':1979,'multiline':False]['text':' the order of one so that we can compare','line_number':1980,'multiline':False]['text':' x has to have batch_size 1 to test contiguous checks','line_number':1989,'multiline':False]['text':' change the stride in dimension 0. the tensor is still contiguous because size[0] is 1','line_number':1993,'multiline':False]['text':' Check that grouped convolutions matches two half convolutions','line_number':2062,'multiline':False]['text':' load_state_dict will restore the stride & memory_layout on ref_conv.weight.','line_number':2121,'multiline':False]['text':' non-dilated conv: thnn_conv2d normal path (with im2col)','line_number':2146,'multiline':False]['text':' test when input chanels is 1 and not converted to channels last','line_number':2151,'multiline':False]['text':' non-dilated conv: thnn_conv2d fast path (skip im2col)','line_number':2154,'multiline':False]['text':' ic == oc == 1 here, so need to stick input to CL to activate channels last','line_number':2157,'multiline':False]['text':' dilated conv: slow_conv_dilated2d','line_number':2160,'multiline':False]['text':' transposed-conv: slow_conv_transpose2d','line_number':2165,'multiline':False]['text':' use FP64 channels-first conv as reference','line_number':2190,'multiline':False]['text':' load_state_dict will restore the stride & memory_layout on ref_conv.weight.','line_number':2193,'multiline':False]['text':' use FP64 channels-first conv as reference','line_number':2238,'multiline':False]['text':' load_state_dict will restore the stride & memory_layout on ref_conv.weight.','line_number':2241,'multiline':False]['text':' load_state_dict will restore the stride & memory_layout on ref_conv.weight.','line_number':2276,'multiline':False]['text':' Older versions of CudNN have Channels Last support disabled','line_number':2305,'multiline':False]['text':' This is because we have N111 weight that cannot handle','line_number':2309,'multiline':False]['text':' the ambiguous memory_format','line_number':2310,'multiline':False]['text':' torch.half is erroring out on Windows with CUDA 10.1 + cuDNN 7.6.4','line_number':2338,'multiline':False]['text':' returning CUDNN_STATUS_BAD_PARAM','line_number':2339,'multiline':False]['text':' Disabling that specific test for now [see issue # 33918]','line_number':2340,'multiline':False]['text':' Test that faster algorithms used for inference produce the same results','line_number':2352,'multiline':False]['text':' Validates depthwise3x3 bug reported in https://github.com/pytorch/pytorch/issues/60176','line_number':2353,'multiline':False]['text':' Test that _convolution_double_backward() outputs the correct grad shapes','line_number':2449,'multiline':False]['text':' for 3D input / weight when stride > 1. This is an ad-hoc regression test for a','line_number':2450,'multiline':False]['text':' specific case that was uncovered during the convolution consolidation effort.','line_number':2451,'multiline':False]['text':' The test can be safely deleted if _convolution_double_backward() is removed.','line_number':2452,'multiline':False]['text':' Make sure the correct shapes are computed.','line_number':2475,'multiline':False]