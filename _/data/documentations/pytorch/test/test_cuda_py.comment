['text':' Owner(s): ["module: cuda"]','line_number':1,'multiline':False]['text':' load_tests from common_utils is used to automatically filter tests for','line_number':37,'multiline':False]['text':' sharding on sandcastle. This line silences flake warnings','line_number':38,'multiline':False]['text':' noqa: F811','line_number':43,'multiline':False]['text':' noqa: F401','line_number':46,'multiline':False]['text':' noqa: F401','line_number':47,'multiline':False]['text':' Some GPUs don't support same address space on host and device side','line_number':90,'multiline':False]['text':' Assert this call doesn't raise.','line_number':129,'multiline':False]['text':' Testing the behaviour with None as an argument','line_number':137,'multiline':False]['text':' Testing the behaviour for No argument','line_number':143,'multiline':False]['text':' Testing the behaviour with None as an argument','line_number':148,'multiline':False]['text':' Testing the behaviour for No argument','line_number':154,'multiline':False]['text':' ensure out of memory error doesn't disturb subsequent kernel','line_number':169,'multiline':False]['text':' We used a lot of memory here, clean up so we don't affect other tests too much','line_number':187,'multiline':False]['text':' test invalid fraction value.','line_number':192,'multiline':False]['text':' test 0.499 allocation is ok.','line_number':205,'multiline':False]['text':' it will get OOM when try to allocate more than half memory.','line_number':212,'multiline':False]['text':' ensure out of memory error doesn't disturb subsequent kernel','line_number':218,'multiline':False]['text':' 10MB copies','line_number':230,'multiline':False]['text':' Test the case where the pinned data_ptr is not equal to the storage data_ptr.','line_number':239,'multiline':False]['text':' Pushes an 0.1 second spin to stream so if the copy is non blocking,','line_number':255,'multiline':False]['text':' stream will almost surely be active when we query().','line_number':256,'multiline':False]['text':' Creates source on the opposite device from destination.','line_number':265,'multiline':False]['text':' :4096:2:16:8','line_number':303,'multiline':False]['text':' different size (32 MiB) expected on Hopper GPU','line_number':304,'multiline':False]['text':' check default','line_number':316,'multiline':False]['text':' check default with bad user config','line_number':320,'multiline':False]['text':' check valid config','line_number':324,'multiline':False]['text':' this is really just checking that the environment variable is respected during testing','line_number':347,'multiline':False]['text':' and not overwritten by another function that doesn't revert it to the intitial value','line_number':348,'multiline':False]['text':' Performs the CPU->GPU copy in a background stream','line_number':528,'multiline':False]['text':' delay the copy','line_number':535,'multiline':False]['text':' In the native allocator, we expect "tmp"'s side-stream-tagged block will be reused','line_number':547,'multiline':False]['text':' in that side stream after result.copy_(tmp) in the main stream finishes.','line_number':548,'multiline':False]['text':' See issue #27366','line_number':555,'multiline':False]['text':' This test detects unexpected block reallocation. For reliable test,','line_number':557,'multiline':False]['text':' the stream to allocate tensors is isolated. The allocator will not','line_number':558,'multiline':False]['text':' reuse free blocks which were allocated from another stream.','line_number':559,'multiline':False]['text':' Record another stream on a shifted view tensor.','line_number':564,'multiline':False]['text':' Delete those tensors to make the block free soon.','line_number':574,'multiline':False]['text':' A new tensor should not be allocated to the block above.','line_number':578,'multiline':False]['text':' See issue #3266','line_number':587,'multiline':False]['text':' check that allocations are re-used after deletion','line_number':594,'multiline':False]['text':' check that the allocation is not re-used if it's in-use by a copy','line_number':601,'multiline':False]['text':' delay the copy by 1s','line_number':603,'multiline':False]['text':' delays re-use of `x` until after all operations in `stream`','line_number':622,'multiline':False]['text':' we've made a mess by allocating up to the device capacity. free any','line_number':626,'multiline':False]['text':' cached blocks in case it affects future tests.','line_number':627,'multiline':False]['text':' Tests for historic illegal memory access, see #17040.','line_number':630,'multiline':False]['text':' tests global reduction (should_global_reduce = true) in case of non-zero identity element','line_number':662,'multiline':False]['text':' test for complex types. Note 240k is divisible by 4','line_number':666,'multiline':False]['text':' Test two corner cases from older PyTorch (Issue #4858)','line_number':672,'multiline':False]['text':' test corner case from Issue #13867','line_number':699,'multiline':False]['text':' CUDA','line_number':726,'multiline':False]['text':' CUDA','line_number':727,'multiline':False]['text':' ROCm','line_number':728,'multiline':False]['text':' ROCm','line_number':729,'multiline':False]['text':' Test in-bound access works fine','line_number':770,'multiline':False]['text':' Test that indexing out of bounds causes assert','line_number':772,'multiline':False]['text':' Testing if THC_reduceAll received the correct index initialization.','line_number':785,'multiline':False]['text':' This affects the result of THC_reduceAll operations at extreme values','line_number':786,'multiline':False]['text':' Just making sure we can see the symbols','line_number':798,'multiline':False]['text':' ensure CUDA code coverage','line_number':806,'multiline':False]['text':' test shared memory impl','line_number':810,'multiline':False]['text':' test global memory impl','line_number':814,'multiline':False]['text':'   see `CUDAHistogramMemoryType` in SummaryOps.cu','line_number':815,'multiline':False]['text':'   50000 * sizeof(int64_t) == 390 KiB, which should exceed smem of any known GPU','line_number':816,'multiline':False]['text':' 35488 * 65536 as int32 would cause overflow to negative value','line_number':822,'multiline':False]['text':' giving negative bin offset','line_number':823,'multiline':False]['text':' Issue #24309: In extreme cases, the loop variable could overflow and continue','line_number':847,'multiline':False]['text':' the kernel loop with a negative index, causing a RuntimeError (invalid write):','line_number':848,'multiline':False]['text':' Make sure input.numel() > INT_MAX is handled:','line_number':858,'multiline':False]['text':' Issue #24309: In extreme cases, the loop variable could overflow and continue','line_number':863,'multiline':False]['text':' the kernel loop with a negative index, causing a RuntimeError (invalid write):','line_number':864,'multiline':False]['text':' this might create a reference cycle on self...','line_number':871,'multiline':False]['text':' delays the operation in the background stream','line_number':883,'multiline':False]['text':' Tests using grads outside the backward() stream context','line_number':896,'multiline':False]['text':' See "Stream semantics of backward passes" on https://pytorch.org/docs/stable/notes/cuda.html','line_number':897,'multiline':False]['text':' sync needed','line_number':903,'multiline':False]['text':' Tests that using grads in the same stream context as backward()','line_number':908,'multiline':False]['text':' is safe regardless what streams bwd ops ran on','line_number':909,'multiline':False]['text':' x was first used on "stream" so its AccumulateGrad leaf should run on "stream".','line_number':918,'multiline':False]['text':' The end of backward() should have synced "bwd_ambient_stream" with "stream"','line_number':919,'multiline':False]['text':' so it should be safe to use x.grad here without any syncs.','line_number':920,'multiline':False]['text':' Skip the test for ROCm as per https://github.com/pytorch/pytorch/issues/53190','line_number':924,'multiline':False]['text':' the out_of_place=False, iters=1 case stresses if proper syncs are inserted','line_number':955,'multiline':False]['text':' when grads are initially None and stolen by backward ops.','line_number':956,'multiline':False]['text':' See "Stream semantics of backward passes" on https://pytorch.org/docs/stable/notes/cuda.html','line_number':973,'multiline':False]['text':' This function tests if bwd ops running on a side stream properly sync with the GraphRoot.','line_number':982,'multiline':False]['text':' The potential bug it targets is a race condition. The test uses multiple trials and','line_number':983,'multiline':False]['text':' torch.cuda._sleep such that if the race condition exists, the test will almost certainly fail,','line_number':984,'multiline':False]['text':' but there's a chance it may spuriously pass. Passing does not guarantee the backend is bug-free,','line_number':985,'multiline':False]['text':' but failure does guarantee there is a bug.','line_number':986,'multiline':False]['text':' We need these streams to be different otherwise the test is meaningless.','line_number':989,'multiline':False]['text':' I don't think we need any manual record_streams below.','line_number':997,'multiline':False]['text':' a and b remain in scope for the entire test.','line_number':998,'multiline':False]['text':' c and grad remain in scope for each iteration, and there's a full sync between iterations.','line_number':999,'multiline':False]['text':' Long-running dummy kernel on bwd_ambient_stream delays filling of grad','line_number':1008,'multiline':False]['text':' Fills grad on bwd_ambient_stream','line_number':1010,'multiline':False]['text':' Bwd ops still run on fwd_bwd_ops_stream, so the following will likely fail if','line_number':1013,'multiline':False]['text':' bwd ops don't sync with bwd_ambient_stream before consuming grad.','line_number':1014,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/47028','line_number':1017,'multiline':False]['text':' assertEquals below run on bwd_ambient_stream, so this test may also fail','line_number':1018,'multiline':False]['text':' if backward() fails to sync with bwd_ambient_stream at the end.','line_number':1019,'multiline':False]['text':' Synchronizing here works around the issue until a proper fix can be made.','line_number':1020,'multiline':False]['text':' Tests if autograd callbacks sync properly with respect to leaf streams and','line_number':1027,'multiline':False]['text':' the user-facing stream surrounding backward(). If it fails, first suspect is','line_number':1028,'multiline':False]['text':' sync logic where  "final_callbacks_" are called in torch/csrc/autograd/engine.cpp','line_number':1029,'multiline':False]['text':' sets up a nontrivial structure of leaf streams','line_number':1042,'multiline':False]['text':' Use a hook on e to install the callback','line_number':1057,'multiline':False]['text':' The autograd engine should sync s2 with all leaf streams then run the callback clone_leaf_grads on s2.','line_number':1063,'multiline':False]['text':' If those things happened properly, checking the values of the cloned grads on s2 should be safe:','line_number':1064,'multiline':False]['text':' Simulates 2 consecutive unskipped iterations','line_number':1108,'multiline':False]['text':' Simulates a skipped iteration','line_number':1116,'multiline':False]['text':' Creates fp16 sparse tensor with duplicated indices (uncoalesced).  The uncoalesced representation','line_number':1166,'multiline':False]['text':' does not overflow in fp16, but the coalesced representation would, because 64000 + 64000 > fp16 max.','line_number':1167,'multiline':False]['text':' _amp_non_finite_check_and_unscale_ should report an overflow here.','line_number':1168,'multiline':False]['text':' sets a random value for load_state_dict to overwrite','line_number':1182,'multiline':False]['text':' Dummy scale() call to ensure the scale tensor is lazily initialized.','line_number':1186,'multiline':False]['text':' _run_scaling_case generalizes some single-optimizer test logic to avoid too much copy-pasting below.','line_number':1198,'multiline':False]['text':' Ensure scaling can be disabled without changing user control flow.','line_number':1200,'multiline':False]['text':' For functionality, test with a modest initial scale, and an unrealistically-large growth factor','line_number':1206,'multiline':False]['text':' so any potential errors with the growth factor handling will be magnified.','line_number':1207,'multiline':False]['text':' Allows run() to optionally return a different scaler instance.','line_number':1213,'multiline':False]['text':' If scaling was enabled, the scale factor should have been multiplied by the growth factor','line_number':1216,'multiline':False]['text':' len(data) - skipped times and the backoff factor "skipped" times.','line_number':1217,'multiline':False]['text':' Compares no scaling + no autocasting against scaling + autocasting.','line_number':1234,'multiline':False]['text':' NOTE(mkozuki): With current way of testing, `torch.optim.Adam` is failing in spite of `foreach` and `fused`.','line_number':1259,'multiline':False]['text':'   Giving some flexibility to this test might help.','line_number':1260,'multiline':False]['text':' sets atol=1e-3 because we're comparing pure fp32 arithmetic vs a mixture of fp16 and fp32','line_number':1266,'multiline':False]['text':' this will be picked up by try_pickle within run():','line_number':1270,'multiline':False]['text':' Compare non-fused optimizer vs fused one as the fused one unscales gradients','line_number':1288,'multiline':False]['text':' inside its cuda kernel unlike the other.','line_number':1289,'multiline':False]['text':' Make sure that the parameters become nonsense when scaled gradients are finite','line_number':1342,'multiline':False]['text':' but they get invalidated before `optimizer.step`, after `GradScaler.unscale_`','line_number':1343,'multiline':False]['text':' deliberately break grads','line_number':1370,'multiline':False]['text':' A reasonable value that actually has an effect, based on printouts of grads','line_number':1394,'multiline':False]['text':' A reasonable value that actually has an effect, based on printouts of grads','line_number':1416,'multiline':False]['text':' Tests gradient scaling with 2 models and 2 optimizers that both receive gradients from 2 losses.','line_number':1495,'multiline':False]['text':' Some of the logic here cannot reuse the generic helper functions created for the 1-optimizer cases.','line_number':1496,'multiline':False]['text':' As an additional stress test, separately unscale for one of the optimizers.','line_number':1520,'multiline':False]['text':' The loss scale should have been multiplied by the growth factor 3 times and the backoff factor once.','line_number':1536,'multiline':False]['text':' Note, these parameters should be very carefully tuned','line_number':1582,'multiline':False]['text':' Too small number makes it hard for the racing condition','line_number':1583,'multiline':False]['text':' to happen, while too large number sometimes cause hang','line_number':1584,'multiline':False]['text':' Hard sync so we don't need to worry about creating and using tensors','line_number':1596,'multiline':False]['text':' across streams or the fact that default streams are thread-local.','line_number':1597,'multiline':False]['text':' Those issues are not the target of this test.','line_number':1598,'multiline':False]['text':' Line up threads to increase likelihood of race conditions.','line_number':1600,'multiline':False]['text':' If all threads are sharing the same cublas handle,','line_number':1604,'multiline':False]['text':' the following sequence may occur:','line_number':1605,'multiline':False]['text':' thread 0 calls cublasSetStream()','line_number':1606,'multiline':False]['text':' thread 1 calls cublasSetStream()','line_number':1607,'multiline':False]['text':' thread 0 launches its raw gemm, which it thinks is in','line_number':1608,'multiline':False]['text':'          its own stream, but is actually in thread 1's stream.','line_number':1609,'multiline':False]['text':' thread 0 enqueues its div_, which IS is its own stream,','line_number':1610,'multiline':False]['text':'          but actually now races with its gemm.','line_number':1611,'multiline':False]['text':' Test is flaky on Windows (https://github.com/pytorch/pytorch/issues/57401)','line_number':1631,'multiline':False]['text':' This function is intended to test the lazy creation and reuse of per-thread','line_number':1636,'multiline':False]['text':' cudnn handles on each device in aten/src/ATen/cudnn/Handles.cpp.','line_number':1637,'multiline':False]['text':' Failure here likely indicates something wrong with that logic.','line_number':1638,'multiline':False]['text':' Hard sync so we don't need to worry about creating and using tensors','line_number':1651,'multiline':False]['text':' across streams or the fact that default streams are thread-local.','line_number':1652,'multiline':False]['text':' Those issues are not the target of this test.','line_number':1653,'multiline':False]['text':' Line up threads to increase likelihood of race conditions.','line_number':1655,'multiline':False]['text':' If all threads are sharing the same cudnn handle,','line_number':1659,'multiline':False]['text':' the following sequence may occur:','line_number':1660,'multiline':False]['text':' thread 0 calls setCuDNNStreamToCurrent()','line_number':1661,'multiline':False]['text':' thread 1 calls setCuDNNStreamToCurrent()','line_number':1662,'multiline':False]['text':' thread 0 launches its raw convolution, which it thinks is in','line_number':1663,'multiline':False]['text':'          its own stream, but is actually in thread 1's stream.','line_number':1664,'multiline':False]['text':' thread 0 enqueues its div_, which IS is its own stream,','line_number':1665,'multiline':False]['text':'          but now races with its convolution.','line_number':1666,'multiline':False]['text':' Hard sync so we don't need to worry about creating and using tensors','line_number':1705,'multiline':False]['text':' across streams or the fact that default streams are thread-local.','line_number':1706,'multiline':False]['text':' Those issues are not the target of this test.','line_number':1707,'multiline':False]['text':' Line up threads to increase likelihood of race conditions.','line_number':1709,'multiline':False]['text':' If all threads are sharing the same cublas handle,','line_number':1713,'multiline':False]['text':' the following sequence may occur:','line_number':1714,'multiline':False]['text':' thread 0 calls cublasSetStream()','line_number':1715,'multiline':False]['text':' thread 1 calls cublasSetStream()','line_number':1716,'multiline':False]['text':' thread 0 launches its raw gemm, which it thinks is in','line_number':1717,'multiline':False]['text':'          its own stream, but is actually in thread 1's stream.','line_number':1718,'multiline':False]['text':' thread 0 enqueues its div_, which IS is its own stream,','line_number':1719,'multiline':False]['text':'          but actually now races with its gemm.','line_number':1720,'multiline':False]['text':' helper to cast args','line_number':1741,'multiline':False]['text':' Try module.* variant, if requested:','line_number':1760,'multiline':False]['text':' Try Tensor.* variant:','line_number':1767,'multiline':False]['text':' Accounts for ops that return Tensors, iterables, and other non-Tensors.','line_number':1778,'multiline':False]['text':' For example, lstm_cell returns a tuple and equal returns bool.','line_number':1779,'multiline':False]['text':' If both torch.* and Tensor.* variants were found, check outputs are identical','line_number':1788,'multiline':False]['text':' Compare numerics to Python-side "autocasting" that (we expect) does the same thing','line_number':1794,'multiline':False]['text':' as the C++-side autocasting, and should be bitwise accurate.','line_number':1795,'multiline':False]['text':' TEST_WITH_ROCM','line_number':1823,'multiline':False]['text':' TEST_WITH_ROCM','line_number':1834,'multiline':False]['text':' Tests if CastPolicy::fp16 ops ignore double and int','line_number':1926,'multiline':False]['text':' Currently, no ops belonging to this policy support integer inputs.','line_number':1927,'multiline':False]['text':' Tests if CastPolicy::fp32 ops ignore double and int','line_number':1935,'multiline':False]['text':' Tests if CastPolicy::fp32_set_opt_dtype ops ignore double and int','line_number':1940,'multiline':False]['text':' Tests if CastPolicy::fp32_append_dtype ops ignore double and int','line_number':1945,'multiline':False]['text':' Currently, no ops belonging to this policy support integer inputs.','line_number':1946,'multiline':False]['text':' Puts one input tensor in a nested container.  y's contained Tensor won't receive a gradient,','line_number':2007,'multiline':False]['text':' because torch.autograd.Function can't hand gradients back to non-Tensor forward arguments.','line_number':2008,'multiline':False]['text':' Sets requires_grad=False explicitly so we don't lie about expecting a gradient.','line_number':2009,'multiline':False]['text':' Tests if custom_fwd becomes a no-op when mymm runs outside an autocast-enabled region.','line_number':2018,'multiline':False]['text':' Reported at https://github.com/pytorch/pytorch/issues/38958','line_number':2025,'multiline':False]['text':' The JIT here doesn't really matter, we just need to call','line_number':2035,'multiline':False]['text':' cat via the boxed API','line_number':2036,'multiline':False]['text':' cudnn RNNs require special backend handling (weights are cast to FP16 and reflattened)','line_number':2044,'multiline':False]['text':' so they get a dedicated test.','line_number':2045,'multiline':False]['text':' Despite the large number of RNN cases it tries, the test takes < 15 seconds on a Titan V (similar to V100).','line_number':2046,'multiline':False]['text':' seq, batch, features, hidden size','line_number':2051,'multiline':False]['text':' Autocast wrapper requires at::_cudnn_rnn is autograd-exposed.  This check can't guarantee','line_number':2093,'multiline':False]['text':' at::_cudnn_rnn is autograd-exposed, but if it fires, it indicates some funny business has','line_number':2094,'multiline':False]['text':' occurred and we should double check that at::_cudnn_rnn remains autograd-exposed.','line_number':2095,'multiline':False]['text':' Compares with default tolerances, even for FP16 execution.  Barring nondeterminism,','line_number':2110,'multiline':False]['text':' autocast and control results should be bitwise identical.','line_number':2111,'multiline':False]['text':' Reported at https://github.com/pytorch/pytorch/issues/48049','line_number':2125,'multiline':False]['text':' Test is used to check, if autocast recaches the same parameters','line_number':2126,'multiline':False]['text':' when executed in a `torch.no_grad()` block.','line_number':2127,'multiline':False]['text':' We need to run this test in a separate thread as the error we trigger','line_number':2232,'multiline':False]['text':' puts the cuda context in a bad state','line_number':2233,'multiline':False]['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':2251,'multiline':False]['text':' fail, so just set CWD to this script's directory','line_number':2252,'multiline':False]['text':' warmup','line_number':2286,'multiline':False]['text':' Control','line_number':2312,'multiline':False]['text':' Runs a graphed->eager->graphed sequence of RNG ops.','line_number':2333,'multiline':False]['text':' replay() plays 2 invocations of the op, so the sequence has 6','line_number':2334,'multiline':False]['text':' invocations total, matching Control.','line_number':2335,'multiline':False]['text':' replay() reads from graph_in and writes to graph_out.','line_number':2336,'multiline':False]['text':' If replay() updated RNG state correctly, graph_out','line_number':2343,'multiline':False]['text':' should now hold data equal to eager_out.','line_number':2344,'multiline':False]['text':' Do the same operations varying seeds','line_number':2350,'multiline':False]['text':' If the random seed was not updated then the graph would','line_number':2359,'multiline':False]['text':' generate the same output as in previous check.','line_number':2360,'multiline':False]['text':' Now repeat the same operations in non-graphed mode.','line_number':2366,'multiline':False]['text':' In the end, graph_out and eager_out must be equal','line_number':2373,'multiline':False]['text':' as they went under the same set of operations.','line_number':2374,'multiline':False]['text':' We hold references to all tensors used across streams up til this sync,','line_number':2380,'multiline':False]['text':' so no need to call record_stream on those tensors.','line_number':2381,'multiline':False]['text':' Torch ops to test with sample args (tuple) and kwargs (dict)','line_number':2393,'multiline':False]['text':' multinomial uses some uncapturable CUDA calls.','line_number':2395,'multiline':False]['text':' TODO: reenable multinomial tests if/when the implementation is capturable.','line_number':2396,'multiline':False]['text':' ("multinomial", (input.clone(), size, True), {}),','line_number':2397,'multiline':False]['text':' ("multinomial", (input.clone(), size // 2, False), {}),','line_number':2398,'multiline':False]['text':' TODO: reenable normal test, where std is a device','line_number':2399,'multiline':False]['text':' tensor, when graph test failures are fixed','line_number':2400,'multiline':False]['text':' ("normal", (input.clone() + 1, input.clone()), {}),','line_number':2401,'multiline':False]['text':' Tensor methods to test with sample args (tuple)','line_number':2408,'multiline':False]['text':' Each path runs a dummy op to increment the state a bit before creating controls.','line_number':2421,'multiline':False]['text':' Makes sure values haven't been populated yet','line_number':2456,'multiline':False]['text':' (in other words, makes sure capture didn't actually run ops).','line_number':2457,'multiline':False]['text':' We can only try this with the native allocator, for which captured','line_number':2458,'multiline':False]['text':' addresses are already backed by cudaMalloced memory.','line_number':2459,'multiline':False]['text':' If we try it with cudaMallocAsync, CUDA won't event consider','line_number':2460,'multiline':False]['text':' the captured addresses allocated until replay(), and if we','line_number':2461,'multiline':False]['text':' access them before replay() we get IMAs.','line_number':2462,'multiline':False]['text':' Set a new seed to check if graph would use it','line_number':2469,'multiline':False]['text':' Runs a dummy op prelude, as for controls, to make sure replay()','line_number':2472,'multiline':False]['text':' picks up the dummy op's state increment.','line_number':2473,'multiline':False]['text':' see above comment on TEST_CUDAMALLOCASYNC','line_number':2489,'multiline':False]['text':' Runs RNG ops that fill t1 and t2.','line_number':2494,'multiline':False]['text':' We hold references to all tensors used across streams up til this sync,','line_number':2503,'multiline':False]['text':' so no need to call record_stream on those tensors.','line_number':2504,'multiline':False]['text':' Adds an empty dict for kwargs, which none of the Tensor methods use','line_number':2511,'multiline':False]['text':' mixes unrelated eager ops with replays','line_number':2550,'multiline':False]['text':' These stat checks are specific to the native allocator.','line_number':2565,'multiline':False]['text':' Tensors used across streams (a and b) were held until just now, so no need to call record_stream on them.','line_number':2573,'multiline':False]['text':' appears to still be broken on Windows as of 11.4+','line_number':2578,'multiline':False]['text':' largeish to help expose race conditions','line_number':2588,'multiline':False]['text':' To reproduce data corruption, I need g0 and g1's kernels to run concurrently.','line_number':2622,'multiline':False]['text':' But replay() (especially cudaGraphLaunch) can incur significant CPU overhead.','line_number':2623,'multiline':False]['text':' The following pattern helps align device-side execution of g0 and g1's kernels.','line_number':2624,'multiline':False]['text':' If we used the native allocator and shared mempools,','line_number':2636,'multiline':False]['text':' we expect the concurrent replays corrupted each other.','line_number':2637,'multiline':False]['text':' If we EITHER','line_number':2641,'multiline':False]['text':'   - used the native allocator without sharing mempools, OR','line_number':2642,'multiline':False]['text':'   - used cudaMallocAsync, which ignores graph pool-sharing hints and should always be safe','line_number':2643,'multiline':False]['text':' we don't expect memory corruption.','line_number':2644,'multiline':False]['text':' Tensors used across streams (a, b, c) were held until just now, so no need to call record_stream on them.','line_number':2649,'multiline':False]['text':' Tests that replaying in capture order is valid','line_number':2689,'multiline':False]['text':' Tests that replaying as g0, g2, g1 is only valid if they don't share a pool','line_number':2697,'multiline':False]['text':' If we used the native allocator and shared mempools, g2's capture should have reused c's memory for f.','line_number':2703,'multiline':False]['text':' We replayed g2 then g1, so we expect g1's captured "e = c + 3" mistakenly filled e with "f's vals + 3".','line_number':2704,'multiline':False]['text':' Tensors used across streams (a, e, f) were held until just now, so no need to call record_stream on them.','line_number':2709,'multiline':False]['text':' this was annoying to write but stresses the expectations pretty rigorously','line_number':2724,'multiline':False]['text':' one from "b" plus a sneaky two from CUDAGraph's one-element rng seed and offset holders','line_number':2750,'multiline':False]['text':' + 1024 for CUDAGraph's rng seed and offset holders each','line_number':2751,'multiline':False]['text':' We only check the large pool, which isn't affected by rng offset holder','line_number':2753,'multiline':False]['text':' Allocation stat estimates assume input is created on the same stream as capture_begin()','line_number':2759,'multiline':False]['text':' (in other words, the same stream silo as the rng offset holder, which is not allocated from the','line_number':2760,'multiline':False]['text':' capture's private pool).','line_number':2761,'multiline':False]['text':' Double checks replay and stats before and after a call to empty_cache','line_number':2781,'multiline':False]['text':' Uses graph result b after graph has been deleted','line_number':2799,'multiline':False]['text':' b should be the only live reference remaining from the graph's private pool','line_number':2802,'multiline':False]['text':' del a, b before the next case is essential, otherwise overwriting a and b in the next case','line_number':2810,'multiline':False]['text':' can throw off its allocation/deallocation counts.','line_number':2811,'multiline':False]['text':' Tensors used across streams (a and b) were held until just now, so no need to call record_stream on them.','line_number':2813,'multiline':False]['text':' Makes sure graph capture defers attempting to reclaim allocations used across streams. See','line_number':2819,'multiline':False]['text':' "Q. Why skip process_events if a capture might be underway?" in c10/cuda/CUDACachingAllocator.cpp','line_number':2820,'multiline':False]['text':' potential_problem's allocation should still be outstanding. if DeviceCachingAllocator::malloc','line_number':2839,'multiline':False]['text':' mistakenly calls process_events, it will trigger cudaEventQueries on potential_problem's end-of-life','line_number':2840,'multiline':False]['text':' event, which will cause the capture to error.','line_number':2841,'multiline':False]['text':' Let's also see what happens if we record_stream on a tensor during capture.','line_number':2844,'multiline':False]['text':' dummy record_stream','line_number':2848,'multiline':False]['text':' dummy allocation triggers process_events, Hopefully successfully processes b's end-of-life event.','line_number':2854,'multiline':False]['text':' If this test is the first in the process to try cudnn rnns with dropout, it'll initialize','line_number':2859,'multiline':False]['text':' DropoutState's long-lived internal buffer. Calling code perceives this (correct) behavior','line_number':2860,'multiline':False]['text':' as a memory leak unless we skip the leak check.','line_number':2861,'multiline':False]['text':' Tests the interaction of cuda graph capture with DropoutState's syncs in ATen/native/cudnn/RNN.cpp.','line_number':2864,'multiline':False]['text':' In particular, if user runs a sequence of captured and noncaptured cudnn rnns, DropoutState should','line_number':2865,'multiline':False]['text':' avoid syncing noncapturing streams with captured events or vice versa.','line_number':2866,'multiline':False]['text':' warmup','line_number':2900,'multiline':False]['text':' capture','line_number':2910,'multiline':False]['text':' If the scale gets updated properly, these are the scale, growth tracker,','line_number':2918,'multiline':False]['text':' and grad values we expect.','line_number':2919,'multiline':False]['text':' This is a good stress test. It graphs four callables: two Modules and two python functions.','line_number':3009,'multiline':False]['text':' Resets RNC states before iterations for graphed and ungraphed models,','line_number':3036,'multiline':False]['text':' so dropout math should be bitwise identical for both.','line_number':3037,'multiline':False]['text':' We graphed the models in training mode. Eval should still run ungraphed.','line_number':3052,'multiline':False]['text':' Control (capturable=False)','line_number':3069,'multiline':False]['text':' capturable=True','line_number':3078,'multiline':False]['text':' Passing capturable=True to the constructor and running without graphs should still be','line_number':3098,'multiline':False]['text':' numerically correct, even if it's not ideal for performance.','line_number':3099,'multiline':False]['text':' Needs generalization if we want to extend this test to non-Adam-like optimizers.','line_number':3109,'multiline':False]['text':' mimicking `_test_graphed_optimizer` maladroitly to pass two param_groups to optimizer.__init__','line_number':3134,'multiline':False]['text':' `GradScaler` in-place updates gradients thus it's necessary to duplicate gradients.','line_number':3194,'multiline':False]['text':' Gradient Scaler','line_number':3200,'multiline':False]['text':' Control (capturable=False)','line_number':3210,'multiline':False]['text':' capturable=True','line_number':3220,'multiline':False]['text':' Passing capturable=True to the constructor and running without graphs should still be','line_number':3242,'multiline':False]['text':' numerically correct, even if it's not ideal for performance.','line_number':3243,'multiline':False]['text':' Exception would Corrupt Process and make other tests fail','line_number':3304,'multiline':False]['text':' self.assertTrue(throws_on_cuda_event("global"))','line_number':3305,'multiline':False]['text':' The test exercises a ROCm-specific feature.','line_number':3346,'multiline':False]['text':' make x the second block in a segment','line_number':3410,'multiline':False]['text':' create a bunch of tensors that all will tile into the','line_number':3415,'multiline':False]['text':' same segment to  exercise the history merging code','line_number':3416,'multiline':False]['text':' 512B is the minimum block size,','line_number':3417,'multiline':False]['text':' so we allocate all the tensors to this size to make sure','line_number':3418,'multiline':False]['text':' they tile evenly','line_number':3419,'multiline':False]['text':' exercise the history trimming code','line_number':3424,'multiline':False]['text':' the callback has to run outside of the collect','line_number':3526,'multiline':False]['text':' call so it doesn't actual fire until the next','line_number':3527,'multiline':False]['text':' method call after a gc.collect','line_number':3528,'multiline':False]['text':' floats are 4 bytes','line_number':3630,'multiline':False]['text':' floats are 4 bytes','line_number':3633,'multiline':False]['text':' test roundup_power2_divisions single value syntax','line_number':3639,'multiline':False]['text':' not supported with the cudaMallocAsync backend','line_number':3650,'multiline':False]['text':' should have reset the power2 divisions now','line_number':3657,'multiline':False]['text':' roundup_power2_divisions knob array syntax','line_number':3664,'multiline':False]['text':' not supported with the cudaMallocAsync backend','line_number':3673,'multiline':False]['text':' not supported with the cudaMallocAsync backend','line_number':3682,'multiline':False]['text':' C++ frame','line_number':3753,'multiline':False]['text':' script frame','line_number':3755,'multiline':False]['text':' python frame','line_number':3757,'multiline':False]['text':' fuzz','line_number':3780,'multiline':False]['text':' after may contain additional segments, but all of the segments in before','line_number':3907,'multiline':False]['text':' should be exactly equivalent to after','line_number':3908,'multiline':False]['text':' TODO: re-enable','line_number':4013,'multiline':False]['text':' def test_additional_free_error(self):','line_number':4014,'multiline':False]['text':'     def foo():','line_number':4015,'multiline':False]['text':'         return int8_cuda(MIN_BLOCK_SIZE),','line_number':4016,'multiline':False]['text':'     def foo2():','line_number':4018,'multiline':False]['text':'         return int8_cuda(MIN_BLOCK_SIZE),','line_number':4019,'multiline':False]['text':'     graph, outputs = cudagraphify(foo, [])','line_number':4021,'multiline':False]['text':'     pool_id = graph.pool()','line_number':4022,'multiline':False]['text':'     segments_before_checkpoint = get_cudagraph_segments(pool_id)','line_number':4024,'multiline':False]['text':'     state = torch._C._cuda_getCheckpointState(outputs[0].device.index, pool_id)','line_number':4026,'multiline':False]['text':' graph2, outputs2 = cudagraphify(foo2, [], pool=graph.pool())','line_number':4028,'multiline':False]['text':' with self.assertRaisesRegex(Exception, "being manually freed must be passed"):','line_number':4029,'multiline':False]['text':'     self.setCheckpointPoolState(outputs[0].device.index, state, [], [])','line_number':4030,'multiline':False]['text':' should not change, we did not pass it in to swap data ptrs','line_number':4103,'multiline':False]['text':' three allocations','line_number':4184,'multiline':False]['text':' three more allocations not in pool','line_number':4188,'multiline':False]['text':' two allocations','line_number':4193,'multiline':False]['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':4224,'multiline':False]['text':' fail, so just set CWD to this script's directory','line_number':4225,'multiline':False]