['text':' Owner(s): ["module: mkldnn"]','line_number':1,'multiline':False]['text':' We use this wrapper to run UTs of TorchVision models because of a memory-leak','line_number':19,'multiline':False]['text':' issue with JIT tracing that causes traced model objects to persist in the','line_number':20,'multiline':False]['text':' memory. Ref: https://github.com/pytorch/pytorch/issues/35600','line_number':21,'multiline':False]['text':' Memory requirement for running these UTs was thus increasing cumulatively, and','line_number':22,'multiline':False]['text':' invoked the Linux kernel OOM killer on linux.2xlarge PyTorch CI runners, which','line_number':23,'multiline':False]['text':' only have 16 GB RAM. Cumulatively, these UTs had been using more than 14 GB','line_number':24,'multiline':False]['text':' memory (as per psutils). So now we run each TorchVision model UTs in separate processes.','line_number':25,'multiline':False]['text':' PyTorch has divergent op support for AMP in JIT & eager modes','line_number':55,'multiline':False]['text':' so we disable AMP for JIT & leverage eager-mode AMP.','line_number':56,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/75956','line_number':57,'multiline':False]['text':' We rely upon eager-mode AMP support for BF16','line_number':70,'multiline':False]['text':' single-op partition shouldn't be created for softmax','line_number':169,'multiline':False]['text':' single-op partition shouldn't be created.','line_number':188,'multiline':False]['text':' [1, 2, 4], TODO: fix issue in pad calculation','line_number':198,'multiline':False]['text':' [1, 2], TODO: backend support for dilation','line_number':199,'multiline':False]['text':' TODO: oneDNN Graph does not fully support ceil_mode=True','line_number':220,'multiline':False]['text':' kernel_size is not Constant, shouldn't have any LLGA_FUSION_GROUP','line_number':244,'multiline':False]['text':' TODO: with shape specialization, should have 1 LLGA_FUSION_GROUP','line_number':245,'multiline':False]['text':' single-op partition shouldn't be created for softmax','line_number':255,'multiline':False]['text':' Just a sidenote - comparison of eager-mode & oneDNN Graph JIT outputs of','line_number':304,'multiline':False]['text':' addmm (which entails matmul-bias-add fusion) might require higher tolerance','line_number':305,'multiline':False]['text':' bounds for BF16. This is subject to change in the near future.','line_number':306,'multiline':False]['text':' alpha and beta are 1, by default','line_number':308,'multiline':False]['text':' single-op partition should be created for matmul with bias.','line_number':315,'multiline':False]['text':' single-op partitions shouldn't be created','line_number':326,'multiline':False]['text':' TODO: support more normalized_shape','line_number':342,'multiline':False]['text':' change the shape of the input, we should enter fallback graph','line_number':374,'multiline':False]['text':' test if relu_ is replace with relu by mutation removal pass','line_number':409,'multiline':False]['text':' test if relu is fused into the fusion group','line_number':411,'multiline':False]['text':' oneDNN graph does not have silu OP. The bridge will convert silu to sigmoid - mul','line_number':436,'multiline':False]['text':' Inplace op will become outplace op on the JIT graph','line_number':437,'multiline':False]['text':' Simply test if the output is accurate','line_number':479,'multiline':False]['text':' The output of the second partition is input to adaptive_avg_pool2d, which is','line_number':480,'multiline':False]['text':' unsupported by LLGA. In resnext101 32x16d, we encountered an accuracy issue.','line_number':481,'multiline':False]['text':' The pattern is as the following:','line_number':658,'multiline':False]['text':'      conv','line_number':659,'multiline':False]['text':'     |    \','line_number':660,'multiline':False]['text':' eltwise   \','line_number':661,'multiline':False]['text':'    |       \','line_number':662,'multiline':False]['text':'  ListConstruct','line_number':663,'multiline':False]['text':'','line_number':664,'multiline':False]['text':' The output of conv is used by a wildcard op: ListConstruct.','line_number':665,'multiline':False]['text':' Thus conv-eltwise cannot be selected into the same Partition.','line_number':666,'multiline':False]['text':' conv can exist in a single-op oneDNN Graph partition but not relu','line_number':670,'multiline':False]['text':' In shufflenet_v2_x1_0, channels_per_groups is computed as:','line_number':682,'multiline':False]['text':' channels_per_group = num_channels // groups','line_number':683,'multiline':False]['text':' JIT IR converts groups to Long dtype, which is unsupported','line_number':684,'multiline':False]['text':' by oneDNN Graph, viz. Long(requires_grad=0, device=cpu) = prim::Constant[value={2}]()','line_number':685,'multiline':False]['text':' This test just ensures that the bridge code can handle','line_number':686,'multiline':False]['text':' unsupported dtypes for inputs to ops unsupported','line_number':687,'multiline':False]['text':' by oneDNN Graph. In this particular UT, aten::floor_divide','line_number':688,'multiline':False]['text':' would be added as a wildcard in graph-construction stage.','line_number':689,'multiline':False]['text':' Simply test if the output is accurate','line_number':721,'multiline':False]['text':' The output of the second partition is input to adaptive_avg_pool2d, which is','line_number':722,'multiline':False]['text':' unsupported by LLGA, so it must be handled by PyTorch, which should receive','line_number':723,'multiline':False]['text':' correct strides info of the channels-last tensor.','line_number':724,'multiline':False]