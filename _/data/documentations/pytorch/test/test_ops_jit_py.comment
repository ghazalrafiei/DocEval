['text':' Owner(s): ["module: unknown"]','line_number':1,'multiline':False]['text':' variant testing is only done with torch.float and torch.cfloat to avoid','line_number':18,'multiline':False]['text':'   excessive test times and maximize signal to noise ratio','line_number':19,'multiline':False]['text':' Tests operators for consistency between JIT and eager, also checks','line_number':25,'multiline':False]['text':'   correctness of JIT specific alias schemas and intended','line_number':26,'multiline':False]['text':'   autodifferentiation behavior.','line_number':27,'multiline':False]['text':' Inherits from JitCommonTestCase instead of TestCase directly to share','line_number':28,'multiline':False]['text':'   functionality with original test_jit.py method operator tests','line_number':29,'multiline':False]['text':' Tests that the forward and backward passes of operations produce the','line_number':33,'multiline':False]['text':'   same values for the cross-product of op variants (function, method, inplace)','line_number':34,'multiline':False]['text':'   and runtimes (eager, traced, scripted).','line_number':35,'multiline':False]['text':' TODO WARNING: inplace x {traced, scripted} not currently tested','line_number':36,'multiline':False]['text':' Acquires variants to test','line_number':44,'multiline':False]['text':' TODO: inplace tests currently fail, fix and add inplace variant','line_number':48,'multiline':False]['text':' scripting strips the torch.ops prefix from these operators','line_number':52,'multiline':False]['text':' incorrectly; don't bother testing this case.  Count this','line_number':53,'multiline':False]['text':' as "testing"','line_number':54,'multiline':False]['text':' TODO: find better way to standardize on op registration itself..','line_number':58,'multiline':False]['text':' Test traced and scripted consistency','line_number':68,'multiline':False]['text':' scripting and check_alias_analysis do not work with lambdas','line_number':73,'multiline':False]['text':' lambdas are typically used as a way to simulate methods without','line_number':74,'multiline':False]['text':' functional variants, so rely on the other variant for testing','line_number':75,'multiline':False]['text':' for now','line_number':76,'multiline':False]['text':' Create accessor for script function variant','line_number':96,'multiline':False]['text':' run with disable_autodiff_subgraph_inlining(True) to test','line_number':99,'multiline':False]['text':'   autodiff support. Context manager forces the graph to contain','line_number':100,'multiline':False]['text':'   DifferentiableGraph nodes if they are present','line_number':101,'multiline':False]['text':' Check scripted forward, grad, and grad grad','line_number':103,'multiline':False]['text':' Processes the output for autograd','line_number':108,'multiline':False]['text':' Check traced forward, grad, and grad grad','line_number':125,'multiline':False]['text':' TODO: fix tracing here','line_number':126,'multiline':False]['text':' Check alias annotation schema for correctness (make','line_number':141,'multiline':False]['text':'   sure inputs that aren't supposed to be modified aren't)','line_number':142,'multiline':False]['text':' Note: only runs in float32 because schema isn't affected by dtype,','line_number':143,'multiline':False]['text':'   so running it on all dtypes is would be excessive','line_number':144,'multiline':False]['text':' TODO: no reason why we cant run this with tracing graph','line_number':146,'multiline':False]['text':' TODO: use script graph as well','line_number':151,'multiline':False]['text':' right now, tuple of outputs and tensor output supported','line_number':156,'multiline':False]['text':' TODO: list of tensor outputs','line_number':157,'multiline':False]['text':' Check autodifferentiation of nodes for traced and scripted graphs, only need to check once per sample','line_number':170,'multiline':False]['text':' Sandcastle doesn't fuse nodes','line_number':172,'multiline':False]['text':' fusible nodes are expected to be found in FusionGroups in the DifferentiableGraphs','line_number':174,'multiline':False]['text':' alias testing is only done with torch.float for the same reason','line_number':186,'multiline':False]['text':' NOTE: only tests on first sample','line_number':192,'multiline':False]['text':' [Scripting Data Preparation]','line_number':196,'multiline':False]['text':' Prepare data for test scripting','line_number':197,'multiline':False]['text':' Below we prepare strings of args/kwargs with and without type annotations.','line_number':198,'multiline':False]['text':' These strings are inserted into function template strings which is then torch scripted.','line_number':199,'multiline':False]['text':' - args string is ["t0"] corresponding to the "input" tensor required by the op','line_number':200,'multiline':False]['text':' - args_kw is the value of args and strings of kwargs used to call the op (without type annotations), for example,','line_number':201,'multiline':False]['text':' ["to", "1.0", "(1,)", "True", "tensor(1.0)"] -> def fn(t0): return variant(t0, 1.0, (1,), True, tensor(1.0))','line_number':202,'multiline':False]['text':' Prepare data for test tracing','line_number':215,'multiline':False]['text':' Test scripting:','line_number':231,'multiline':False]['text':' remove the first input tensor','line_number':241,'multiline':False]['text':' Required to avoid undefined value: tensor error in JIT','line_number':257,'multiline':False]['text':' compilation of the function template','line_number':258,'multiline':False]['text':' Test tracing:','line_number':277,'multiline':False]