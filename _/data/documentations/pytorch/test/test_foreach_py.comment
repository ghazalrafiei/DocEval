['text':' Owner(s): ["module: mta"]','line_number':1,'multiline':False]['text':' We need to distribute each scalar to the regular func and it needs','line_number':36,'multiline':False]['text':' special consideration as it is a keyword only argument to the','line_number':37,'multiline':False]['text':' regular func. (Strangely, it is not a keyword only argument to the','line_number':38,'multiline':False]['text':' foreach func)','line_number':39,'multiline':False]['text':' binary op with tensorlist and scalar.','line_number':42,'multiline':False]['text':' Some foreach functions don't have in-place implementations.','line_number':50,'multiline':False]['text':' note(mkozuki): inplace foreach functions are void functions.','line_number':68,'multiline':False]['text':' note(crcrpar): some methods e.g. `_binary_test` could call the given inplace function multiple times','line_number':83,'multiline':False]['text':' note(crcrpar): `zero_size` is `False` unless (dtype, device) == (torch.float32, "cuda")','line_number':101,'multiline':False]['text':' as the pair would go through `multi_tensor_apply_kernel` if inputs are not zero size.','line_number':102,'multiline':False]['text':' note(crcrpar): Make sure 0-size tensors are appropriately ignored by `multi_tensor_apply`','line_number':116,'multiline':False]['text':' which is originally reported in https://github.com/pytorch/pytorch/issues/94865.','line_number':117,'multiline':False]['text':' rel:','line_number':118,'multiline':False]['text':'   - https://github.com/pytorch/pytorch/pull/94655','line_number':119,'multiline':False]['text':'   - https://github.com/pytorch/pytorch/issues/100701','line_number':120,'multiline':False]['text':'   - https://github.com/pytorch/pytorch/pull/100811','line_number':121,'multiline':False]['text':' div promotes ints to floats, so we cannot go on the fastpath there','line_number':154,'multiline':False]['text':' 1D Tensor of scalars','line_number':265,'multiline':False]['text':' Tests of implicit broadcasting','line_number':296,'multiline':False]['text':' Match with error messages from regular non-foreach reference if no','line_number':338,'multiline':False]['text':' custom error message was provided.','line_number':339,'multiline':False]['text':' TODO: enable empty list case','line_number':351,'multiline':False]['text':' Regression test for https://github.com/pytorch/pytorch/issues/113156','line_number':360,'multiline':False]['text':' Empty lists','line_number':410,'multiline':False]['text':' One empty list','line_number':415,'multiline':False]['text':' Lists have different amount of tensors','line_number':421,'multiline':False]['text':' Corresponding tensors with different sizes that aren't compatible with broadcast','line_number':430,'multiline':False]['text':' If sizes are different then foreach chooses slow path, thus error messages are expected','line_number':431,'multiline':False]['text':' to be the same as torch regular function.','line_number':432,'multiline':False]['text':' different devices','line_number':446,'multiline':False]['text':' 0-strides','line_number':471,'multiline':False]['text':' different strides','line_number':482,'multiline':False]['text':' non contiguous','line_number':493,'multiline':False]['text':' sliced tensor','line_number':506,'multiline':False]['text':' note: Below three tests (postfixed with `_tensors_on_different_devices`)','line_number':542,'multiline':False]['text':' checks whether foreach works with lists of tensors on different devices','line_number':543,'multiline':False]['text':' but tensors of the same index are on the same device, e.g., ['cuda', 'cpu].','line_number':544,'multiline':False]['text':' tensors: ['cuda', 'cpu]','line_number':549,'multiline':False]['text':' `tensors1`: ['cuda', 'cpu']','line_number':576,'multiline':False]['text':' `tensors2`: ['cuda', 'cpu']','line_number':577,'multiline':False]['text':' tensors1: ['cuda', 'cpu]','line_number':603,'multiline':False]['text':' tensors2: ['cuda', 'cpu]','line_number':604,'multiline':False]['text':' tensors3: ['cuda', 'cpu]','line_number':605,'multiline':False]['text':' first tensorlist is zero-size when float32','line_number':606,'multiline':False]['text':' note(mkozuki): Limiting dtypes to FP32&FP64, we can safely run inplace ops.','line_number':618,'multiline':False]['text':' note: BFloat16 has the same number of exponent bits as FP32','line_number':622,'multiline':False]['text':' so if squared L2 norm overflows in BF16, then it also overflows in FP32.','line_number':623,'multiline':False]['text':' make sure that the min. of squared L2 norm value per tensor is greater than the max value of `dtype`.','line_number':633,'multiline':False]['text':' making sure the reference L2 norm values are in the range of FP16.','line_number':640,'multiline':False]['text':' test inputs larger than kChunkSize = 65536','line_number':651,'multiline':False]['text':' note: `_foreach_pow.Scalar` and `_foreach_pow.ScalarList` don't depend on `result`','line_number':735,'multiline':False]['text':' see: https://github.com/pytorch/pytorch/blob/5403c777/tools/autograd/derivatives.yaml#L3048-L3049','line_number':736,'multiline':False]['text':' For mul and div, the scalar is allowed to be on CPU too','line_number':784,'multiline':False]['text':' check exceptions of fast path','line_number':793,'multiline':False]['text':' Test reverse-mode & forward-mode AD if supported.','line_number':825,'multiline':False]['text':' note(crcrpar): without this, some unary functions fail, unlike inplace and/or complex.','line_number':841,'multiline':False]['text':' Skip `_foreach_pow.ScalarAndTensor(Scalar, Tensor[])`','line_number':852,'multiline':False]['text':' Call `clone` to avoid inplace modifications likewise','line_number':858,'multiline':False]['text':' `torch.testing._internal.common_utils.TestGradients._get_safe_inplace`','line_number':859,'multiline':False]['text':' lhs of float64 and rhs of complex.','line_number':886,'multiline':False]['text':' Test per-tensor `grad_fn` behavior.','line_number':893,'multiline':False]['text':' per-tensor `grad_fn` check.','line_number':895,'multiline':False]['text':' tensors have different shapes.','line_number':924,'multiline':False]['text':' TODO(crcrpar): Hide this inside torch/testing/_internal.','line_number':936,'multiline':False]['text':' would end up adding another layer to `foreach_inputs_sample_func.__call__`','line_number':937,'multiline':False]['text':' so that we can use this function as something like the first argument of `filter` function.','line_number':938,'multiline':False]['text':' Even after moving this function to testing, I personally think it'd be better to check the error message.','line_number':939,'multiline':False]