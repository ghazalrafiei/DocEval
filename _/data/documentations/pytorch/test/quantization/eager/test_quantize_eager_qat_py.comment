['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]['text':' ConvNd args','line_number':70,'multiline':False]['text':' BatchNormNd args','line_number':76,'multiline':False]['text':' num_features: out_channels','line_number':77,'multiline':False]['text':' affine: True','line_number':79,'multiline':False]['text':' track_running_stats: True','line_number':80,'multiline':False]['text':' Args for this module','line_number':81,'multiline':False]['text':' A hack to avoid resetting on undefined parameters','line_number':124,'multiline':False]['text':' exponential_average_factor is self.momentum set to','line_number':137,'multiline':False]['text':' (when it is available) only so that if gets updated','line_number':138,'multiline':False]['text':' in ONNX graph when this node is exported to ONNX.','line_number':139,'multiline':False]['text':' TODO: if statement only here to tell the jit to skip emitting this when it is None','line_number':146,'multiline':False]['text':' use cumulative moving average','line_number':149,'multiline':False]['text':' use exponential moving average','line_number':151,'multiline':False]['text':' we use running statistics from the previous batch, so this is an','line_number':154,'multiline':False]['text':' approximation of the approach mentioned in the whitepaper, but we only','line_number':155,'multiline':False]['text':' need to do one convolution in this case instead of two','line_number':156,'multiline':False]['text':' recovering original conv to get original batch_mean and batch_var','line_number':167,'multiline':False]['text':' TODO(jerryzh): extend','line_number':193,'multiline':False]['text':' ConvNd args','line_number':232,'multiline':False]['text':' BatchNorm2d args','line_number':237,'multiline':False]['text':' num_features: out_channels','line_number':238,'multiline':False]['text':' affine: True','line_number':240,'multiline':False]['text':' track_running_stats: True','line_number':241,'multiline':False]['text':' Args for this module','line_number':242,'multiline':False]['text':' Dynamic QAT without memoryless observers should fail','line_number':381,'multiline':False]['text':' make sure activation_post_process is inserted after Linear.','line_number':410,'multiline':False]['text':' make sure that Embedding has a noop for activation.','line_number':412,'multiline':False]['text':' make sure that FakeQuant zero_points are correct dtype','line_number':414,'multiline':False]['text':' make sure Embedding is now a QuantizedEmbedding','line_number':421,'multiline':False]['text':' make sure Linear is now a QuantizedLinear','line_number':423,'multiline':False]['text':' make sure not activation_post_process is inserted for EmbeddingBag','line_number':441,'multiline':False]['text':' make sure that FakeQuant zero_points are correct dtype','line_number':443,'multiline':False]['text':' Make sure EmbeddingBag is now a quantized EmbeddingBag.','line_number':449,'multiline':False]['text':' Also test that Linear has been quantized.','line_number':451,'multiline':False]['text':' Create model again for eval. Check result using quantized state_dict','line_number':484,'multiline':False]['text':' Check to make sure the model after prepare_qat has the same state_dict as original.','line_number':491,'multiline':False]['text':' Check model created using prepare has same state dict as quantized state_dict','line_number':500,'multiline':False]['text':' Test constructor parameters checks here.','line_number':601,'multiline':False]['text':' Test from_float checks here.','line_number':611,'multiline':False]['text':' Embedding QAT uses a NoopObserver class for activation,','line_number':631,'multiline':False]['text':' and a FakeQuant for weight, make sure that qconfig comparison','line_number':632,'multiline':False]['text':' functions properly for a mix of partial function and class in','line_number':633,'multiline':False]['text':' qconfig.','line_number':634,'multiline':False]['text':' make sure activation post process is removed','line_number':692,'multiline':False]['text':' verify fake quant module is removd','line_number':694,'multiline':False]['text':' verify that hooks are removed','line_number':696,'multiline':False]['text':' make sure no fake quantize module is inserted for eval mode','line_number':699,'multiline':False]['text':' make sure no activation_post_process is inserted for relu','line_number':730,'multiline':False]['text':' make sure ReLU module is not changed','line_number':733,'multiline':False]['text':' the approximate fusion will not work if bn.weight has 0','line_number':817,'multiline':False]['text':' align inputs and internal parameters','line_number':827,'multiline':False]['text':' functions are reversed for natural reading order','line_number':838,'multiline':False]['text':' backward','line_number':862,'multiline':False]['text':' bias','line_number':942,'multiline':False]['text':' bias','line_number':958,'multiline':False]['text':' align inputs and internal parameters','line_number':969,'multiline':False]['text':' make sure that calling model.train() does not override the','line_number':984,'multiline':False]['text':' bn freeze setting','line_number':985,'multiline':False]['text':' backward','line_number':1007,'multiline':False]['text':' without fake_quants, fused QAT module should match fp32 module','line_number':1055,'multiline':False]['text':' Only qnnpack support symmetric quantization','line_number':1067,'multiline':False]['text':' without fake_quants, fused QAT module should match fp32 module','line_number':1078,'multiline':False]