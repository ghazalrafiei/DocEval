['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]['text':' Note: these models are not for use outside of this file. While it's good','line_number':96,'multiline':False]['text':' to reuse code, we also need to be able to iterate on tests','line_number':97,'multiline':False]['text':' quickly when debugging. If a test model has a large number of callsites','line_number':98,'multiline':False]['text':' across various different files, speed of debugging on individual test cases','line_number':99,'multiline':False]['text':' decreases.','line_number':100,'multiline':False]['text':' conv1d','line_number':154,'multiline':False]['text':' conv1d - relu','line_number':156,'multiline':False]['text':' conv1d - bn (qat only)','line_number':159,'multiline':False]['text':' conv1d - bn - relu (qat only)','line_number':162,'multiline':False]['text':' conv2d','line_number':166,'multiline':False]['text':' conv2d - relu','line_number':168,'multiline':False]['text':' conv2d - bn (qat only)','line_number':171,'multiline':False]['text':' conv2d - bn - relu (qat only)','line_number':174,'multiline':False]['text':' conv3d','line_number':178,'multiline':False]['text':' conv3d - relu','line_number':180,'multiline':False]['text':' conv3d - bn (qat only)','line_number':183,'multiline':False]['text':' conv3d - bn - relu (qat only)','line_number':186,'multiline':False]['text':' linear','line_number':190,'multiline':False]['text':' linear - relu','line_number':192,'multiline':False]['text':' conv1d','line_number':197,'multiline':False]['text':' conv2d','line_number':206,'multiline':False]['text':' conv3d','line_number':216,'multiline':False]['text':' linear','line_number':226,'multiline':False]['text':' TODO: we can remove this call, and get all patterns from backend_config_dict in','line_number':302,'multiline':False]['text':' the future when the frontend refactor is done in fx graph mode quantization','line_number':303,'multiline':False]['text':' some of the patterns are moved to (native) backend_config_dict so we need to','line_number':305,'multiline':False]['text':' add them back here','line_number':306,'multiline':False]['text':' assume success if no exceptions','line_number':387,'multiline':False]['text':' assume success if no exceptions','line_number':402,'multiline':False]['text':' verify that matching graphs with matching node types but','line_number':407,'multiline':False]['text':' different counts of matchable nodes fails','line_number':408,'multiline':False]['text':' verify that matching graphs with non-matching node types fails','line_number':419,'multiline':False]['text':' verify that nodes before cat get matched','line_number':431,'multiline':False]['text':' verify that we can traverse up nodes which return dictionaries','line_number':463,'multiline':False]['text':' prevent conv2 from getting quantized, so we can test','line_number':511,'multiline':False]['text':' modules with equal types','line_number':512,'multiline':False]['text':' all of these should be matched','line_number':532,'multiline':False]['text':' 1. check static quant module mappings','line_number':579,'multiline':False]['text':' skip quants and dequants, for the purposes of Numerical Suite','line_number':582,'multiline':False]['text':' the ConvTranspose3d swap is not implemented in FX Graph','line_number':587,'multiline':False]['text':' mode quantization yet','line_number':588,'multiline':False]['text':' the GroupNorm swap is not implemented in FX Graph','line_number':590,'multiline':False]['text':' mode quantization yet','line_number':591,'multiline':False]['text':' nnq.ReLU6 is no longer swapped, because nn.ReLU6 can','line_number':593,'multiline':False]['text':' take quantized inputs','line_number':594,'multiline':False]['text':' verify relatedness','line_number':600,'multiline':False]['text':' 2. check static quant op mappings','line_number':607,'multiline':False]['text':' verify relatedness','line_number':610,'multiline':False]['text':' 3. check dynamic quant mappings','line_number':617,'multiline':False]['text':' TODO(future PR): enable correct weight extraction for these','line_number':620,'multiline':False]['text':' and remove from this list.','line_number':621,'multiline':False]['text':' verify relatedness','line_number':630,'multiline':False]['text':' 4. go through the ops mapped to each QuantizeHandler type, and verify','line_number':637,'multiline':False]['text':' correctness.','line_number':638,'multiline':False]['text':' these ops do not have quantized equivalents','line_number':683,'multiline':False]['text':' TODO(future PR): add support for all classes in','line_number':697,'multiline':False]['text':' RNNDynamicQuantizeHandler','line_number':698,'multiline':False]['text':' these ops use the same op signature for fp32 and quantized','line_number':705,'multiline':False]['text':' tensors','line_number':706,'multiline':False]['text':' torch.sum does not have quantized equivalents','line_number':716,'multiline':False]['text':' skip fusion patterns','line_number':726,'multiline':False]['text':' didn't match explicit quantize handler class, we can check if the','line_number':728,'multiline':False]['text':' operator is in the related op set directly','line_number':729,'multiline':False]['text':' verify that mobilenetv2 graph is able to be matched','line_number':798,'multiline':False]['text':' assume success if no exceptions','line_number':803,'multiline':False]['text':' assume success if no exceptions','line_number':807,'multiline':False]['text':' verify that mobilenetv2 graph is able to be matched','line_number':813,'multiline':False]['text':' assume success if no exceptions','line_number':821,'multiline':False]['text':' assume success if no exceptions','line_number':825,'multiline':False]['text':' test both the public API as well as the internal GraphModule API','line_number':840,'multiline':False]['text':' test both m vs mp and mp vs mq','line_number':842,'multiline':False]['text':' calibrate','line_number':897,'multiline':False]['text':' check activation result correctness','line_number':903,'multiline':False]['text':' calibrate','line_number':961,'multiline':False]['text':' check activation result correctness','line_number':966,'multiline':False]['text':' TODO(future PR): add Linear-ReLU, after #55393 is fixed.','line_number':1047,'multiline':False]['text':' For now, sigmoid is not supported for shadowing because the dtype','line_number':1209,'multiline':False]['text':' inference for it is not implemented yet. So, this is just testing','line_number':1210,'multiline':False]['text':' that shadowing models with method calls does not crash.','line_number':1211,'multiline':False]['text':' Note: shadowing relu by itself is currently not supported,','line_number':1360,'multiline':False]['text':' this test is just testing that it does not crash','line_number':1361,'multiline':False]['text':' Logging of the output of this class is not supported, because it is','line_number':1393,'multiline':False]['text':' neither a tensor or an RNN return type.','line_number':1394,'multiline':False]['text':' Scripting a model with loggers should succeed. If it fails because of','line_number':1428,'multiline':False]['text':' incorrect dtypes, we can blocklist the associated types from being instrumented.','line_number':1429,'multiline':False]['text':' quantize without tracing through UserModule','line_number':1459,'multiline':False]['text':' weight extraction should not crash','line_number':1471,'multiline':False]['text':' unshadowed activations should have loggers','line_number':1474,'multiline':False]['text':' add loggers, without retracing','line_number':1476,'multiline':False]['text':' note: converting again because we cannot copy a quantized linear','line_number':1477,'multiline':False]['text':' both fp32 and int8 models should have 2 loggers each, 2 for I/O','line_number':1482,'multiline':False]['text':' of linear, and 0 for I/O of user_module','line_number':1483,'multiline':False]['text':' shadowed activations should only have loggers for nodes where','line_number':1492,'multiline':False]['text':' the types are known and we can do a dtype cast','line_number':1493,'multiline':False]['text':' add shadow loggers, without retracing','line_number':1495,'multiline':False]['text':' 4 loggers for I/O of linear, 0 loggers for I/O of user_module','line_number':1499,'multiline':False]['text':' TODO(future PR): clean this up','line_number':1515,'multiline':False]['text':' 1. check static quant module mappings','line_number':1530,'multiline':False]['text':' TODO(future PR): look into whether shadowing embeddings','line_number':1537,'multiline':False]['text':' makes sense','line_number':1538,'multiline':False]['text':' the ConvTranspose3d swap is not implemented in FX Graph','line_number':1541,'multiline':False]['text':' mode quantization yet','line_number':1542,'multiline':False]['text':' the GroupNorm swap is not implemented in FX Graph','line_number':1544,'multiline':False]['text':' mode quantization yet','line_number':1545,'multiline':False]['text':' nnq.ReLU6 is no longer swapped, because nn.ReLU6 can','line_number':1547,'multiline':False]['text':' take quantized inputs','line_number':1548,'multiline':False]['text':' 2. check static quant op mappings','line_number':1560,'multiline':False]['text':' 3. check dynamic quant mappings','line_number':1570,'multiline':False]['text':' TODO(future PR): verify correct I/O for these and remove from','line_number':1573,'multiline':False]['text':' this list.','line_number':1574,'multiline':False]['text':' TODO(future PR): look into whether shadowing embeddings','line_number':1580,'multiline':False]['text':' makes sense','line_number':1581,'multiline':False]['text':' 4. go through the ops mapped to each QuantizeHandler type, and verify','line_number':1594,'multiline':False]['text':' correctness.','line_number':1595,'multiline':False]['text':' TODO(future PR): implement shadowing for binary ops','line_number':1612,'multiline':False]['text':' TODO(future PR): implement shadowing for RNN ops','line_number':1613,'multiline':False]['text':' Softmax has a different signature for the quantized','line_number':1648,'multiline':False]['text':' version, so it does not fit into the cases above.','line_number':1649,'multiline':False]['text':' embedding shadowing is not implemented, for now','line_number':1653,'multiline':False]['text':' test compare weights','line_number':1727,'multiline':False]['text':' test unshadowed activations','line_number':1735,'multiline':False]['text':' calibrate','line_number':1742,'multiline':False]['text':' check activation result correctness','line_number':1746,'multiline':False]['text':' test shadowed activations','line_number':1751,'multiline':False]['text':' calibrate','line_number':1763,'multiline':False]['text':' check activation result correctness','line_number':1766,'multiline':False]['text':' extract weights','line_number':1784,'multiline':False]['text':' match activations','line_number':1790,'multiline':False]['text':' match shadow activations','line_number':1802,'multiline':False]['text':' extract weights','line_number':1822,'multiline':False]['text':' verify that scale and zp were extracted correctly','line_number':1853,'multiline':False]['text':' for the first op, the scale+zp live as attributes on the module','line_number':1855,'multiline':False]['text':' for the second op, the scale and zp of input to second op','line_number':1863,'multiline':False]['text':' must equal to scale and zp of output of first op','line_number':1864,'multiline':False]['text':' verify running data works','line_number':1872,'multiline':False]['text':' input qparams of conv will be input qparams of adaptive_avg_pool','line_number':1889,'multiline':False]['text':' Note: this is not using quantization because quantized kernels do not','line_number':1951,'multiline':False]['text':' work on cuda yet.','line_number':1952,'multiline':False]['text':' Note: this is not using quantization because quantized kernels do not','line_number':1962,'multiline':False]['text':' work on cuda yet.','line_number':1963,'multiline':False]['text':' Note: this is not using quantization because quantized kernels do not','line_number':1979,'multiline':False]['text':' work on cuda yet.','line_number':1980,'multiline':False]['text':' the second argument leads to attempting to copy a','line_number':2036,'multiline':False]['text':' call_function node','line_number':2037,'multiline':False]['text':' note: FX graph mode quantization does not have good support','line_number':2059,'multiline':False]['text':' for kwargs-only right now, so we pass in two unquantized','line_number':2060,'multiline':False]['text':' models','line_number':2061,'multiline':False]['text':' test that input is valid','line_number':2083,'multiline':False]['text':' print('msp', msp)','line_number':2088,'multiline':False]['text':' TODO(future PR): enable layernorm','line_number':2189,'multiline':False]['text':' blocked on FX graph mode quant not inserting observer for','line_number':2190,'multiline':False]['text':' second arg, if the second arg is a module input','line_number':2191,'multiline':False]['text':' x = F.layer_norm(x, x.shape)','line_number':2192,'multiline':False]['text':' x = F.layer_norm(x, x.shape[1:])','line_number':2193,'multiline':False]['text':' x = x.reshape(1, -1) * 2','line_number':2194,'multiline':False]['text':' x = F.layer_norm(x.reshape(1, -1), x.shape[1:])','line_number':2195,'multiline':False]['text':' TODO(future PR): enable below after FX graph mode quantization handles','line_number':2198,'multiline':False]['text':' it, currently this is not supported','line_number':2199,'multiline':False]['text':' x = F.linear(input=x, weight=self.w1, bias=self.b1)','line_number':2200,'multiline':False]['text':' check behavior with save_activations enabled','line_number':2262,'multiline':False]['text':' after prepare calibration but before convert calibration, loggers','line_number':2266,'multiline':False]['text':' should not have anything saved','line_number':2267,'multiline':False]['text':' loggers should save each item after calibration','line_number':2270,'multiline':False]['text':' check behavior with save_activations disabled','line_number':2273,'multiline':False]['text':' after prepare calibration but before convert calibration, loggers','line_number':2277,'multiline':False]['text':' should not have anything saved','line_number':2278,'multiline':False]['text':' stats should be empty, but comparisons should be there','line_number':2281,'multiline':False]['text':' check that insertion deduplicates qconfigs','line_number':2299,'multiline':False]['text':' test that inserting a higher priority qconfig style with fewer elements than a lower priority qconfig will','line_number':2307,'multiline':False]['text':' result in adding None to the extra QConfigMappings at that same style+key','line_number':2308,'multiline':False]['text':' test that inserting a lower priority qconfig style with more elements thhan lower priority qconfig styles','line_number':2350,'multiline':False]['text':' will result in the new QConfigMapping having None at all previously existing styles+keys','line_number':2351,'multiline':False]['text':' test that the prepare/convert_n_shadows_model works as expected','line_number':2393,'multiline':False]['text':' with qconfig_multi_mapping and avoids unwanted matches','line_number':2394,'multiline':False]['text':' test QConfigMultiMapping.from_list_qconfig_mapping works as expected','line_number':2422,'multiline':False]['text':' test that the module ordering ignores None','line_number':2451,'multiline':False]['text':' test that input is valid','line_number':2534,'multiline':False]['text':' test unquantized','line_number':2599,'multiline':False]['text':' test per-tensor','line_number':2602,'multiline':False]['text':' test that input is valid','line_number':2612,'multiline':False]['text':' print('msp', msp)','line_number':2617,'multiline':False]['text':' print('msq', msq)','line_number':2622,'multiline':False]['text':' print(results)','line_number':2628,'multiline':False]['text':' print_comparisons_n_shadows_model(results)','line_number':2629,'multiline':False]['text':' get the last quantized output from results','line_number':2631,'multiline':False]['text':' verify that both fp32 and quantized output matches reference','line_number':2636,'multiline':False]['text':' print('shadow', output_shadow.shape, output_shadow)','line_number':2647,'multiline':False]['text':' print('shadow_ref', output_shadow_ref.shape, output_shadow_ref)','line_number':2648,'multiline':False]['text':' TODO(future PR): support first arg being a scalar','line_number':2721,'multiline':False]['text':' x = 1 + x','line_number':2722,'multiline':False]['text':' function not matchable by quantization','line_number':2726,'multiline':False]['text':' TODO(future PR): enable scripting (quant prepared LSTM not scriptable)','line_number':2824,'multiline':False]['text':' TODO(future PR): enable scripting (quant prepared LSTM not scriptable)','line_number':2861,'multiline':False]