['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]['text':' graph mode quantization based on fx','line_number':17,'multiline':False]['text':' test utils','line_number':155,'multiline':False]['text':' x = x + y','line_number':220,'multiline':False]['text':' x = y + x','line_number':222,'multiline':False]['text':' test train mode','line_number':305,'multiline':False]['text':' currently we don't check if the module are configured with qconfig before fusion','line_number':307,'multiline':False]['text':' TODO: if we decide to do that in the future, this test needs to','line_number':308,'multiline':False]['text':' be updated','line_number':309,'multiline':False]['text':' train mode fuse_fx is called in prepare_qat_fx','line_number':310,'multiline':False]['text':' test eval mode','line_number':328,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':330,'multiline':False]['text':' ConvBnRelu1d is not fused','line_number':340,'multiline':False]['text':' test eval mode','line_number':361,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':363,'multiline':False]['text':' linear - bn - leaky_relu is fused for onednn backend only','line_number':378,'multiline':False]['text':' test eval mode','line_number':389,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':391,'multiline':False]['text':' Make sure linear - bn - leaky_relu is not fused by default','line_number':400,'multiline':False]['text':' test eval mode','line_number':402,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':404,'multiline':False]['text':' linear - tanh is fused for onednn backend only','line_number':420,'multiline':False]['text':' test eval mode','line_number':430,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':432,'multiline':False]['text':' Make sure linear - tanh is not fused by default','line_number':441,'multiline':False]['text':' test eval mode','line_number':442,'multiline':False]['text':' fuse_fx is a top level api and only supports eval mode','line_number':444,'multiline':False]['text':' conv - bn - add - relu is fused for onednn backend only','line_number':459,'multiline':False]['text':' with_bn','line_number':462,'multiline':False]['text':' with_relu','line_number':463,'multiline':False]['text':' conv in the left','line_number':464,'multiline':False]['text':' with_two_conv','line_number':465,'multiline':False]['text':' use_torch_add','line_number':466,'multiline':False]['text':' test eval mode','line_number':477,'multiline':False]['text':' with_bn','line_number':494,'multiline':False]['text':' with_relu','line_number':495,'multiline':False]['text':' conv in the left','line_number':496,'multiline':False]['text':' with_two_conv','line_number':497,'multiline':False]['text':' use_torch_add','line_number':498,'multiline':False]['text':' test eval mode','line_number':501,'multiline':False]['text':' with_bn','line_number':529,'multiline':False]['text':' with_relu','line_number':530,'multiline':False]['text':' conv in the left','line_number':531,'multiline':False]['text':' two_conv','line_number':532,'multiline':False]['text':' use_torch_add','line_number':533,'multiline':False]['text':' test eval mode','line_number':548,'multiline':False]['text':' these qconfigs somehow fail equality where default_qconfig does not','line_number':682,'multiline':False]['text':' check bn and relu are gone since we replaced the whole pattern to conv','line_number':761,'multiline':False]['text':' check bn and relu are gone since we replaced the whole pattern to conv','line_number':820,'multiline':False]['text':' check conv module has two inputs','line_number':824,'multiline':False]['text':' check relu are gone since we replaced both patterns to conv','line_number':889,'multiline':False]['text':' eliminate the code that get the second output of maxpool, so that the pattern','line_number':929,'multiline':False]['text':' can be matched','line_number':930,'multiline':False]['text':' is_dynamic, ModuleClass, module_constructor_inputs,','line_number':1081,'multiline':False]['text':' inputs, quantized_node, weight_prepack_node','line_number':1082,'multiline':False]['text':' make sure seralization works','line_number':1222,'multiline':False]['text':' check load_state_dict restores states','line_number':1227,'multiline':False]['text':' make sure the qparams are preserved after copy','line_number':1238,'multiline':False]['text':' is_dynamic, ModuleClass, module_constructor_inputs,','line_number':1371,'multiline':False]['text':' inputs, quantized_node, weight_prepack_node','line_number':1372,'multiline':False]['text':' make sure seralization works','line_number':1485,'multiline':False]['text':' check load_state_dict restores states','line_number':1490,'multiline':False]['text':' make sure the qparams are preserved after copy','line_number':1501,'multiline':False]['text':' make sure seralization works','line_number':1570,'multiline':False]['text':' check load_state_dict restores states','line_number':1575,'multiline':False]['text':' make sure the qparams are preserved after copy','line_number':1586,'multiline':False]['text':' Get the actual value to avoid tensor size mismatch error, torch.Size([]) vs torch.Size([1])','line_number':1613,'multiline':False]['text':' check numerics','line_number':1675,'multiline':False]['text':' check numerics vs eager mode','line_number':1728,'multiline':False]['text':' QAT prepare','line_number':1823,'multiline':False]['text':' ensure that running an input on CUDA works without any needed changes','line_number':1826,'multiline':False]['text':' ensure all buffers and parameters are on the device we expect','line_number':1829,'multiline':False]['text':' make sure it runs','line_number':1885,'multiline':False]['text':' instantiate M and RefM and align the parameters','line_number':1932,'multiline':False]['text':' check prepared model','line_number':1955,'multiline':False]['text':' calibration','line_number':1961,'multiline':False]['text':' check converted/quantized model','line_number':1966,'multiline':False]['text':' quantize the reference model','line_number':1972,'multiline':False]['text':' float input','line_number':1985,'multiline':False]['text':' float output','line_number':1986,'multiline':False]['text':' input and output of first conv, observer for standalone module','line_number':1989,'multiline':False]['text':' will be inserted in the standalone module itself','line_number':1990,'multiline':False]['text':' for input and output of conv in the standalone module','line_number':1994,'multiline':False]['text':' standalone module will take float as input and output','line_number':2004,'multiline':False]['text':' so we'll see quantize and dequantize in the modoule','line_number':2005,'multiline':False]['text':' quantized input','line_number':2019,'multiline':False]['text':' quantized output','line_number':2020,'multiline':False]['text':' observer for input and output of first conv','line_number':2023,'multiline':False]['text':' for output of conv in the standalone module','line_number':2027,'multiline':False]['text':' quantizing input for conv','line_number':2032,'multiline':False]['text':' dequantizing output of standalone module','line_number':2035,'multiline':False]['text':' quantization of input happens in parent module','line_number':2039,'multiline':False]['text':' quantization of output happens in the quantized conv module','line_number':2040,'multiline':False]['text':' dequantization for output happens in parent module','line_number':2043,'multiline':False]['text':' first conv is quantized, second conv is not quantized','line_number':2074,'multiline':False]['text':' conv is quantized, linear is not quantized','line_number':2103,'multiline':False]['text':' first conv is quantized, second conv is not quantized','line_number':2166,'multiline':False]['text':' first conv is quantized, second conv is not quantized','line_number':2193,'multiline':False]['text':' global','line_number':2213,'multiline':False]['text':' global + object_type --> object_type','line_number':2215,'multiline':False]['text':' global + object_type + module_name_regex --> module_name_regex','line_number':2217,'multiline':False]['text':' global + object_type + module_name_regex + module_name --> module_name','line_number':2219,'multiline':False]['text':' test various FQNs: global, single child, multiple children','line_number':2291,'multiline':False]['text':' m3','line_number':2307,'multiline':False]['text':' m2','line_number':2316,'multiline':False]['text':' m1','line_number':2324,'multiline':False]['text':' test that function order overrides global qconfig','line_number':2335,'multiline':False]['text':' should not crash as in https://github.com/pytorch/pytorch/issues/75825','line_number':2418,'multiline':False]['text':' TODO: move QConfigMapping tests to test/quantization/core','line_number':2421,'multiline':False]['text':' Insert some entries','line_number':2437,'multiline':False]['text':' Override existing key','line_number':2443,'multiline':False]['text':' Insert some entries','line_number':2459,'multiline':False]['text':' Override existing key','line_number':2465,'multiline':False]['text':' Insert some entries','line_number':2483,'multiline':False]['text':' Override existing key','line_number':2489,'multiline':False]['text':' Insert some entries','line_number':2505,'multiline':False]['text':' Override existing key','line_number':2519,'multiline':False]['text':' No match','line_number':2530,'multiline':False]['text':' Override global qconfig','line_number':2625,'multiline':False]['text':' Verify the correct qconfig was used','line_number':2628,'multiline':False]['text':' Dummy classes for PrepareCustomConfig testing','line_number':2638,'multiline':False]['text':' Standalone modules','line_number':2768,'multiline':False]['text':' Float to observed mapping','line_number':2776,'multiline':False]['text':' Other','line_number':2784,'multiline':False]['text':' PrepareCustomConfig.to_dict also converts internal QConfigMappings and PrepareCustomConfigs to dicts','line_number':2805,'multiline':False]['text':' since sub is configured to have qconfig None, we should dequantize the output','line_number':2951,'multiline':False]['text':' of self.conv1 and quantize the input of self.conv2','line_number':2952,'multiline':False]['text':' dequantize after conv2 should happen after transpose since','line_number':2953,'multiline':False]['text':' it is configured with default_qconfig','line_number':2954,'multiline':False]['text':' nodes in Sub module instance is not quantized','line_number':2955,'multiline':False]['text':' Only nodes in Sub module instance are quantized','line_number':2970,'multiline':False]['text':' the first transpose is not quantized because the input is not quantized','line_number':2971,'multiline':False]['text':' make sure it runs','line_number':2993,'multiline':False]['text':' ensure scripting works','line_number':3079,'multiline':False]['text':' run one round to make sure model runs','line_number':3081,'multiline':False]['text':' disable fake_quant and observer','line_number':3086,'multiline':False]['text':' ensure the fake_quant and observer have been disabled.','line_number':3093,'multiline':False]['text':' enable them back','line_number':3099,'multiline':False]['text':' run it through input','line_number':3114,'multiline':False]['text':' save state_dict of model','line_number':3116,'multiline':False]['text':' Load the stats into new model','line_number':3125,'multiline':False]['text':' Verify that loaded state dict produces same results.','line_number':3134,'multiline':False]['text':' instantiate M and RefM and align the parameters','line_number':3216,'multiline':False]['text':' check prepared model','line_number':3268,'multiline':False]['text':' calibration','line_number':3274,'multiline':False]['text':' all activation observers are inserted in the top level module','line_number':3276,'multiline':False]['text':' check converted/quantized model','line_number':3282,'multiline':False]['text':' quantize the reference model','line_number':3296,'multiline':False]['text':' make sure it works','line_number':3381,'multiline':False]['text':' make sure it runs','line_number':3385,'multiline':False]['text':' make sure it works','line_number':3464,'multiline':False]['text':' make sure it runs','line_number':3468,'multiline':False]['text':' data dependent control flow is not traceable','line_number':3481,'multiline':False]['text':' make sure these modules are not traced','line_number':3516,'multiline':False]['text':' calibrate','line_number':3538,'multiline':False]['text':' copy','line_number':3540,'multiline':False]['text':' quantize, should run with no errors','line_number':3542,'multiline':False]['text':' test deepcopy','line_number':3560,'multiline':False]['text':' test state_dict','line_number':3564,'multiline':False]['text':' quantized input, quantized output','line_number':3635,'multiline':False]['text':' Expect each quantized linear op to have a scale and zero point','line_number':3758,'multiline':False]['text':' ensure it is scriptable','line_number':3762,'multiline':False]['text':' TODO: probably don't want to hardcode the attribute names, since they are generated','line_number':3770,'multiline':False]['text':' make sure it runs','line_number':3847,'multiline':False]['text':' This function looks at the node specified by the NodeInfo in the key of','line_number':3903,'multiline':False]['text':' node_info_to_non_tensor_args and checks that the args at specified indices','line_number':3904,'multiline':False]['text':' are not observed (since they are non tensors). If the args at those indices','line_number':3905,'multiline':False]['text':' are a tuple/list (which do not show up as nodes) the function checks the','line_number':3906,'multiline':False]['text':' individual elements of the tuple/list recursively.','line_number':3907,'multiline':False]['text':' this is a helper function (for easier recursion) that checks whether','line_number':3910,'multiline':False]['text':' arg_node is observed','line_number':3911,'multiline':False]['text':' This test checks that the model gets prepared correct, doesn't have observers','line_number':3931,'multiline':False]['text':' on specific ops (see _check_not_observed) and that the prepared model runs','line_number':3932,'multiline':False]['text':' TODO: make torch.transpose traceable by fx when using','line_number':3995,'multiline':False]['text':' variable nontensor arguments','line_number':3996,'multiline':False]['text':' func = lambda x, y, z: torch.transpose(x, y, z) # error','line_number':3997,'multiline':False]['text':' TODO: make torch.unsqueeze scriptable by fx when using','line_number':4027,'multiline':False]['text':' variable nontensor arguments','line_number':4028,'multiline':False]['text':' func = lambda x, y, z: torch.unsqueeze(x, y) # error','line_number':4029,'multiline':False]['text':' test linear packed weight','line_number':4216,'multiline':False]['text':' test conv packed weight','line_number':4233,'multiline':False]['text':' test load','line_number':4254,'multiline':False]['text':' check that random model weight/bias does not match ref weight/bias','line_number':4263,'multiline':False]['text':' check that weight/bias matches after load the state_dict','line_number':4272,'multiline':False]['text':' Test save to disk and load back','line_number':4279,'multiline':False]['text':' make sure quantization runs','line_number':4344,'multiline':False]['text':' TODO: probably don't want to hardcode the attribute names, since they are generated','line_number':4389,'multiline':False]['text':' if an observer is inserted after _user_func_with_complex_return_type,','line_number':4419,'multiline':False]['text':' the following call will fail','line_number':4420,'multiline':False]['text':' non-quantizeable node, quantized output','line_number':4475,'multiline':False]['text':' quantizeable node, quantized output','line_number':4496,'multiline':False]['text':' one for weights, one for activations','line_number':4510,'multiline':False]['text':' quantizeable node, quantized dictionary output','line_number':4518,'multiline':False]['text':' one for weights, one for activations','line_number':4532,'multiline':False]['text':' preserved attributes are also stored in meta so that it doesn't get lost','line_number':4555,'multiline':False]['text':' during deepcopy','line_number':4556,'multiline':False]['text':' make sure quantization runs','line_number':4603,'multiline':False]['text':' Make sure this runs without error','line_number':4628,'multiline':False]['text':' Checks that we have an observer for both input and output','line_number':4647,'multiline':False]['text':' check that there is a duplicated observer instance','line_number':4669,'multiline':False]['text':' checks for non-reference quantized model','line_number':4686,'multiline':False]['text':' checks for reference quantized model, for copy nodes we'll have','line_number':4698,'multiline':False]['text':' dequant - copy_node - quant patterns which will be fused later','line_number':4699,'multiline':False]['text':' in the backend lowering step','line_number':4700,'multiline':False]['text':' make sure the arg[1] of lstm module is a tuple','line_number':4758,'multiline':False]['text':' lstm[0].dequantize()','line_number':4809,'multiline':False]['text':' lstm[1][0].dequantize()','line_number':4810,'multiline':False]['text':' lstm[1][1].dequantize()','line_number':4811,'multiline':False]['text':' lstm[0], lstm[1], lstm[1][0], lstm[1][1]','line_number':4813,'multiline':False]['text':' No tuples are consumed','line_number':4815,'multiline':False]['text':' consume tuple (output, (hidden0, hidden1))','line_number':4841,'multiline':False]['text':' consume tuple (hidden0, hidden1)','line_number':4847,'multiline':False]['text':' Test consuming the whole tuple (output, (hidden0, hidden1))','line_number':4850,'multiline':False]['text':' lstm[0].dequantize()','line_number':4858,'multiline':False]['text':' lstm[1][0].dequantize()','line_number':4859,'multiline':False]['text':' lstm[1][1].dequantize()','line_number':4860,'multiline':False]['text':' lstm[0], lstm[1], lstm[1][0], lstm[1][1]','line_number':4862,'multiline':False]['text':' tuple(output_dq, tuple(hidden0_dq, hidden1_dq))','line_number':4864,'multiline':False]['text':' Test consuming just the hidden tuple (hidden0, hidden1)','line_number':4869,'multiline':False]['text':' lstm[1][0].dequantize()','line_number':4874,'multiline':False]['text':' lstm[1][1].dequantize()','line_number':4875,'multiline':False]['text':' lstm[1], lstm[1][0], lstm[1][1]','line_number':4877,'multiline':False]['text':' tuple(hidden0_dq, hidden1_dq)','line_number':4879,'multiline':False]['text':' Construct a BackendConfig that supports qint32 for certain ops','line_number':4899,'multiline':False]['text':' TODO: build a BackendConfig from scratch instead of modifying an existing one','line_number':4900,'multiline':False]['text':' uint16, [-16, 16)','line_number':4915,'multiline':False]['text':' uint16, [0, 1)','line_number':4917,'multiline':False]['text':' uint16, [-1, 1)','line_number':4919,'multiline':False]['text':' int16, [-16, 16)','line_number':4921,'multiline':False]['text':' uint8, [-1, 1)','line_number':4923,'multiline':False]['text':' FX graph mode quantization','line_number':4950,'multiline':False]['text':' Find the patterns [dq - op - q_to_specific_dtype] in the graph and','line_number':4973,'multiline':False]['text':' verify that qparams and dtypes are set correctly in the quantize ops','line_number':4974,'multiline':False]['text':' gates.add','line_number':4978,'multiline':False]['text':' fgate_cx.mul','line_number':4983,'multiline':False]['text':' igate_cgate.mul','line_number':4984,'multiline':False]['text':' fgate_cx_igate_cgate.add','line_number':4985,'multiline':False]['text':' ogate_cy.mul','line_number':4986,'multiline':False]['text':' Match preceding dequantize','line_number':4994,'multiline':False]['text':' Match following quantize with the specific qparams and dtypes','line_number':4996,'multiline':False]['text':' Ensure all patterns were matched','line_number':5005,'multiline':False]['text':' Construct graph manually because symbolic_trace does not insert tuple and getitem nodes','line_number':5038,'multiline':False]['text':' Break down tuple and reconstruct it again','line_number':5046,'multiline':False]['text':' Output tuple[1][0]','line_number':5054,'multiline':False]['text':' Do reroute','line_number':5059,'multiline':False]['text':' Assert that output reroutes to `b` directly, and all other nodes can be removed','line_number':5062,'multiline':False]['text':' noqa: E306','line_number':5064,'multiline':False]['text':' check that reference pattern for quantized conv module is fused','line_number':5396,'multiline':False]['text':' checking result match','line_number':5403,'multiline':False]['text':' test reference model','line_number':5620,'multiline':False]['text':' test quantized model','line_number':5629,'multiline':False]['text':' Ensure prepare_fx and prepare_qat_fx work in both training and eval modes','line_number':5765,'multiline':False]['text':' noqa: E128','line_number':5834,'multiline':False]['text':' Case 1: QConfig ranges fit within backend ranges, OK','line_number':5843,'multiline':False]['text':' Case 2: QConfig activation range falls outside backend range, should fail','line_number':5849,'multiline':False]['text':' Case 3: QConfig weight range falls outside backend range, should fail','line_number':5855,'multiline':False]['text':' Case 4: QConfig doesn't specify range, should fail','line_number':5861,'multiline':False]['text':' noqa: E128','line_number':5886,'multiline':False]['text':' Case 1: QConfig min scale value == backend min scale value, OK','line_number':5895,'multiline':False]['text':' Case 2: QConfig min scale value > backend min scale value, OK','line_number':5901,'multiline':False]['text':' Case 3: QConfig activation min scale value < backend min scale value, should fail','line_number':5907,'multiline':False]['text':' Case 3: QConfig weight min scale value < backend min scale value, should fail','line_number':5913,'multiline':False]['text':' Case 5: QConfig doesn't specify eps, should fail','line_number':5919,'multiline':False]['text':' TODO: Test these QConfigs once they are fixed, see https://github.com/pytorch/pytorch/issues/85862','line_number':5943,'multiline':False]['text':' (default_per_channel_symmetric_qnnpack_qconfig, "default_per_channel_symmetric_qnnpack_qconfig"),','line_number':5944,'multiline':False]['text':' (default_per_channel_symmetric_qnnpack_qat_qconfig, "default_per_channel_symmetric_qnnpack_qat_qconfig"),','line_number':5945,'multiline':False]['text':' make sure this runs','line_number':6017,'multiline':False]['text':' make sure this runs','line_number':6053,'multiline':False]['text':' make sure it runs','line_number':6093,'multiline':False]['text':' to avoid reduce_range','line_number':6108,'multiline':False]['text':' make sure it runs','line_number':6127,'multiline':False]['text':' for input and output activations','line_number':6147,'multiline':False]['text':' for weight','line_number':6150,'multiline':False]['text':' make sure it runs','line_number':6157,'multiline':False]['text':' we set a global default_qconfig, which will be ignored since the backend','line_number':6176,'multiline':False]['text':' we defined doesn't support anything','line_number':6177,'multiline':False]['text':' this is to make sure we don't validate the qconfig when BackendConfig does not','line_number':6178,'multiline':False]['text':' have fixed qparam op related configurations','line_number':6179,'multiline':False]['text':' make sure this runs','line_number':6182,'multiline':False]['text':' Three versions of channel shuffle','line_number':6191,'multiline':False]['text':' torch.channel_shuffle is equivalent to torch.nn.functional.channel_shuffle','line_number':6209,'multiline':False]['text':' noqa: E131','line_number':6259,'multiline':False]['text':' one for input of the pattern and one for output of the pattern','line_number':6279,'multiline':False]['text':' TODO Currently it's required that separate ops in a fused op/module have the same qconfig.','line_number':6336,'multiline':False]['text':'      Need to be able to support fusion of ops with different qconfigs','line_number':6337,'multiline':False]['text':' Since tanh must have 'fixed_qparams_qconfig' while linear should use','line_number':6338,'multiline':False]['text':' the global qconfig, we need to set qconfigs for them manually here for','line_number':6339,'multiline':False]['text':' fusion and cannot put such configs in onednn's default qconfig_mapping.','line_number':6340,'multiline':False]['text':' Known issue:','line_number':6341,'multiline':False]['text':' Cannot fuse linear - tanh and quantize standalone tanh at the same time.','line_number':6342,'multiline':False]['text':' qint32','line_number':6436,'multiline':False]['text':' quint8','line_number':6437,'multiline':False]['text':' back to qint32','line_number':6439,'multiline':False]['text':' back to quint8','line_number':6440,'multiline':False]['text':' adding two quint8's together','line_number':6441,'multiline':False]['text':' Set up a QConfigMapping that specifies different qparams and dtypes for different layers','line_number':6449,'multiline':False]['text':' Set up BackendConfig that supports the dtypes configured in the above QConfigMapping','line_number':6457,'multiline':False]['text':' Produce the reference quantized model','line_number':6480,'multiline':False]['text':' calibrate','line_number':6484,'multiline':False]['text':' Verify that the reference model is correct','line_number':6488,'multiline':False]['text':'','line_number':6489,'multiline':False]['text':' Reference model until add should be:','line_number':6490,'multiline':False]['text':' fp32_input -> q_to_int32 -> [dq -> linear1_fp32 -> q_to_int32] -> dq ->','line_number':6491,'multiline':False]['text':' q_to_uint8 -> [dq -> linear2_fp32 -> q_to_uint8] -> dq (linear2_dq) ->','line_number':6492,'multiline':False]['text':' q_to_int32 -> [dq -> sigmoid_fp32 -> q_to_int32] -> dq ->','line_number':6493,'multiline':False]['text':' q_to_uint8 -> [dq -> tanh_fp32 -> q_to_uint8] -> dq (tanh_dq)','line_number':6494,'multiline':False]['text':'','line_number':6495,'multiline':False]['text':' Complete reference model with add should be:','line_number':6496,'multiline':False]['text':' [(linear2_dq, tanh_dq) -> add_fp32 -> q_to_uint8] -> dq -> fp32_output','line_number':6497,'multiline':False]['text':' Find the patterns [dq - op_fp32 - q_to_specific_dtype] in the graph','line_number':6506,'multiline':False]['text':' Match preceding dequantize','line_number':6512,'multiline':False]['text':' Match following quantize with the specific dtypes','line_number':6516,'multiline':False]['text':' Match [dq - torch.add(linear2_dq, tanh_dq) - q]','line_number':6522,'multiline':False]['text':' This should pass','line_number':6565,'multiline':False]['text':' Ensure the quantized model has the expected op','line_number':6567,'multiline':False]['text':' This should pass','line_number':6607,'multiline':False]['text':' Ensure the quantized model has the expected op','line_number':6609,'multiline':False]['text':' This should pass','line_number':6631,'multiline':False]['text':' Ensure the quantized model has the expected op','line_number':6633,'multiline':False]['text':' Test linear','line_number':6713,'multiline':False]['text':' TODO: enable test for dynamic quant','line_number':6723,'multiline':False]['text':' Test linear-relu','line_number':6724,'multiline':False]['text':' Test linear-bn','line_number':6731,'multiline':False]['text':' we don't have linear_relu_dynamic','line_number':6771,'multiline':False]['text':' use_bias','line_number':6779,'multiline':False]['text':' has_relu','line_number':6780,'multiline':False]['text':' functional relu','line_number':6781,'multiline':False]['text':' when has_relu is False, we are using an nn.Identity and','line_number':6784,'multiline':False]['text':' we will insert observer/fake_quant for the output of nn.Identity since','line_number':6785,'multiline':False]['text':' it is a copy node, that's why we have extra observer/fake_quant','line_number':6786,'multiline':False]['text':' when has_relu is False','line_number':6787,'multiline':False]['text':' There should be 3 observers: after input, weight and activation.','line_number':6793,'multiline':False]['text':' one more observer for torch.nn.Identity when there is no relu','line_number':6794,'multiline':False]['text':' There should be 3 observers: after input, weight and activation.','line_number':6799,'multiline':False]['text':' we will have an extra quantize_per_tensor_dynamic + dequantize for','line_number':6813,'multiline':False]['text':' nn.Identity right now, but it will be fixed after we use','line_number':6814,'multiline':False]['text':' backend_config to configure the default pt backend','line_number':6815,'multiline':False]['text':' Ensure packed weights in lowered models are folded','line_number':6831,'multiline':False]['text':' use_bias','line_number':6861,'multiline':False]['text':' has_relu','line_number':6862,'multiline':False]['text':' functional relu','line_number':6863,'multiline':False]['text':' is_reference','line_number':6864,'multiline':False]['text':' activation and weight','line_number':6876,'multiline':False]['text':' weight','line_number':6881,'multiline':False]['text':' use_bias','line_number':6916,'multiline':False]['text':' has_relu','line_number':6917,'multiline':False]['text':' functional relu','line_number':6918,'multiline':False]['text':' is_reference','line_number':6919,'multiline':False]['text':' when has_relu is False, we are using an nn.Identity and','line_number':6925,'multiline':False]['text':' we will insert observer/fake_quant for the output of nn.Identity since','line_number':6926,'multiline':False]['text':' it is a copy node, that's why we have extra observer/fake_quant','line_number':6927,'multiline':False]['text':' when has_relu is False','line_number':6928,'multiline':False]['text':' activation, weight, bias and output','line_number':6930,'multiline':False]['text':' We have extra to and dequantize when is_reference is True','line_number':6933,'multiline':False]['text':' and has_relu is False since when has_relu is False, we','line_number':6934,'multiline':False]['text':' have an nn.Identity in the model, which is a CopyNode','line_number':6935,'multiline':False]['text':' and we would add extra quant - dequant for CopyNode in','line_number':6936,'multiline':False]['text':' reference patterns','line_number':6937,'multiline':False]['text':' we don't support static fp16 ops, so the linear function','line_number':6939,'multiline':False]['text':' is unfused','line_number':6940,'multiline':False]['text':' activation, weight, bias and output','line_number':6942,'multiline':False]['text':' dim','line_number':6968,'multiline':False]['text':' dims','line_number':7039,'multiline':False]['text':' use_bias','line_number':7041,'multiline':False]['text':' has_relu','line_number':7042,'multiline':False]['text':' functional relu','line_number':7043,'multiline':False]['text':' when has_relu is False, we are using an nn.Identity and','line_number':7046,'multiline':False]['text':' we will insert observer/fake_quant for the output of nn.Identity since','line_number':7047,'multiline':False]['text':' it is a copy node, that's why we have extra observer/fake_quant','line_number':7048,'multiline':False]['text':' when has_relu is False','line_number':7049,'multiline':False]['text':' There should be 3 observers: after input, weight and activation.','line_number':7052,'multiline':False]['text':' There should be 3 observers: after input, weight and activation.','line_number':7057,'multiline':False]['text':' Ensure packed weights in lowered models are folded','line_number':7083,'multiline':False]['text':' dim','line_number':7118,'multiline':False]['text':' testing for default int8 static quant','line_number':7138,'multiline':False]['text':' This tests the binary op should be quantized even when it is not feed with a','line_number':7154,'multiline':False]['text':' quantized input','line_number':7155,'multiline':False]['text':' testing for fp16 static quant','line_number':7166,'multiline':False]['text':' we are producing fp16 patterns','line_number':7167,'multiline':False]['text':' output_conv1, output_add1, output_add2 for scalar','line_number':7175,'multiline':False]['text':' output_conv1, output_conv2, output_add1, output_add2 for non-scalar','line_number':7176,'multiline':False]['text':' input_add, output_add for scalar','line_number':7186,'multiline':False]['text':' input_add1, input_add2, output_add for non-scalar','line_number':7187,'multiline':False]['text':' testing for fp16 static quant','line_number':7265,'multiline':False]['text':' we are producing fp16 patterns','line_number':7266,'multiline':False]['text':' input_sum1, output_sum1, output_sum2','line_number':7271,'multiline':False]['text':' testing for fp16 static quant','line_number':7288,'multiline':False]['text':' we are producing fp16 patterns','line_number':7289,'multiline':False]['text':' input_bmm1, input_bmm2, output_bmm','line_number':7295,'multiline':False]['text':' TODO: support call_method("bmm")','line_number':7303,'multiline':False]['text':' we can transform call_method("bmm") to call_function(torch.bmm)','line_number':7304,'multiline':False]['text':' self.checkGraphModeFxOp(','line_number':7305,'multiline':False]['text':'     BMMMethod(), data, quant_type,','line_number':7306,'multiline':False]['text':'     expected_node_occurrence=node_occurrence,','line_number':7307,'multiline':False]['text':'     custom_qconfig_dict=custom_qconfig_dict,','line_number':7308,'multiline':False]['text':'     print_debug_info=True)','line_number':7309,'multiline':False]['text':' check the model is scriptable','line_number':7347,'multiline':False]['text':' check the model is runnable','line_number':7349,'multiline':False]['text':' TODO(future PR): make more generic','line_number':7359,'multiline':False]['text':' inputs and outputs of the two conv, and output of cat','line_number':7466,'multiline':False]['text':' inputs and outputs of the two conv, and output of cat','line_number':7469,'multiline':False]['text':' output of cat','line_number':7475,'multiline':False]['text':' for two inputs','line_number':7478,'multiline':False]['text':' check cat is using the same observer for input and output','line_number':7491,'multiline':False]['text':' two inputs and one output of torch.cat are using same observer, so we have','line_number':7494,'multiline':False]['text':' 2 observers that's replicated','line_number':7495,'multiline':False]['text':' make sure the converted model runs','line_number':7499,'multiline':False]['text':' TODO: quantized batchnorm 1d module is missing','line_number':7506,'multiline':False]['text':' 1 : torch.nn.BatchNorm1d,','line_number':7507,'multiline':False]['text':' 1: ns.call_module(nnq.BatchNorm1d),','line_number':7523,'multiline':False]['text':' 1: ns.call_module(nn.BatchNorm1d),','line_number':7528,'multiline':False]['text':' is_module','line_number':7607,'multiline':False]['text':' is_reference','line_number':7609,'multiline':False]['text':' is_reference','line_number':7645,'multiline':False]['text':' is_module','line_number':7681,'multiline':False]['text':' TODO: change these to use backend_config','line_number':7868,'multiline':False]['text':' x = x.clamp_(-2, 2)  # Enable when quantized `clamp_` is ready','line_number':8014,'multiline':False]['text':' list of node that should occur in order','line_number':8021,'multiline':False]['text':' TODO: use get_default_qconfig_mapping once it handles fp16','line_number':8049,'multiline':False]['text':' add_scalar','line_number':8140,'multiline':False]['text':' mul_scalar','line_number':8142,'multiline':False]['text':' add_scalar_out','line_number':8144,'multiline':False]['text':' mul_scalar_out','line_number':8146,'multiline':False]['text':' add_scalar_relu','line_number':8148,'multiline':False]['text':' add_scalar_relu_out','line_number':8151,'multiline':False]['text':' mul_scalar_relu','line_number':8154,'multiline':False]['text':' mul_scalar_relu_out','line_number':8157,'multiline':False]['text':' prim::ListConstruct','line_number':8167,'multiline':False]['text':' prim::ListUnpack','line_number':8169,'multiline':False]['text':' prim::TupleConstruct','line_number':8171,'multiline':False]['text':' prim::TupleUnpack','line_number':8173,'multiline':False]['text':' chunk is not supported since observer only supports','line_number':8177,'multiline':False]['text':' observing single Tensor currently','line_number':8178,'multiline':False]['text':' This model is not executable since we just put all ops','line_number':8208,'multiline':False]['text':' in the same forward','line_number':8209,'multiline':False]['text':' not runnable','line_number':8213,'multiline':False]['text':' This checks that the dequantize from the output of first conv','line_number':8216,'multiline':False]['text':' is being propagated to the end, so that we don't insert extra','line_number':8217,'multiline':False]['text':' observers and also successfully fused two quantized::conv2d','line_number':8218,'multiline':False]['text':' patterns','line_number':8219,'multiline':False]['text':' one quantize_per_tensor for input','line_number':8220,'multiline':False]['text':' check exact counts of quantize and dequantize','line_number':8221,'multiline':False]['text':' input of conv and two outputs of getitem','line_number':8223,'multiline':False]['text':' output of the model and two outputs of getitem','line_number':8225,'multiline':False]['text':' Checking the is_reference output','line_number':8240,'multiline':False]['text':' not runnable','line_number':8244,'multiline':False]['text':' This model is not executable since we just put all ops','line_number':8263,'multiline':False]['text':' in the same forward','line_number':8264,'multiline':False]['text':' nothing to fuse so skipping the fuse step','line_number':8266,'multiline':False]['text':' not runnable','line_number':8273,'multiline':False]['text':' This checks that the dequantize from the output of first conv','line_number':8276,'multiline':False]['text':' is being propagated to the end, so that we don't insert extra','line_number':8277,'multiline':False]['text':' observers','line_number':8278,'multiline':False]['text':' check exact counts of quantize and dequantize','line_number':8279,'multiline':False]['text':' This model is not executable since we just put all ops','line_number':8332,'multiline':False]['text':' in the same forward','line_number':8333,'multiline':False]['text':' nothing to fuse so skipping the fuse step','line_number':8335,'multiline':False]['text':' not runnable','line_number':8339,'multiline':False]['text':' This checks that the dequantize from the output of first conv','line_number':8342,'multiline':False]['text':' is being propagated to the end, so that we don't insert extra','line_number':8343,'multiline':False]['text':' observers','line_number':8344,'multiline':False]['text':' check exact counts of quantize and dequantize','line_number':8345,'multiline':False]['text':' make sure it runs','line_number':8373,'multiline':False]['text':' testing prepare recognizes non-Tensor input for getitem','line_number':8415,'multiline':False]['text':' F.sigmoid is deprecated','line_number':8451,'multiline':False]['text':' F.tanh is deprecated','line_number':8459,'multiline':False]['text':' TODO(future PR): handle F.softmax','line_number':8462,'multiline':False]['text':' This model is not executable since we just put all ops','line_number':8467,'multiline':False]['text':' in the same forward','line_number':8468,'multiline':False]['text':' nothing to fuse so skipping the fuse step','line_number':8480,'multiline':False]['text':' check that prepare does not change model result','line_number':8485,'multiline':False]['text':' check the correct number of activation_post_process is inserted','line_number':8488,'multiline':False]['text':' not runnable','line_number':8496,'multiline':False]['text':' This checks that the dequantize from the output of first conv','line_number':8500,'multiline':False]['text':' is being propagated to the end, so that we don't insert extra','line_number':8501,'multiline':False]['text':' observers','line_number':8502,'multiline':False]['text':' check exact counts of quantize and dequantize','line_number':8503,'multiline':False]['text':' Verify that softmax scale and zero_point are correct','line_number':8542,'multiline':False]['text':' Note: QAT test succeeded by chance, to make it actually work','line_number':8576,'multiline':False]['text':' we need to fix eager mode FloatFunctional by removing','line_number':8577,'multiline':False]['text':' activation_post_process in add_scalar and mul_scalar','line_number':8578,'multiline':False]['text':' make sure numerics match with eager mode','line_number':8616,'multiline':False]['text':' FX Graph Mode and Eager Mode now diverages in numerics of add_scalar and mul_scalar','line_number':8622,'multiline':False]['text':' self.assertEqual(m(data), ref_m(data))','line_number':8623,'multiline':False]['text':' check dynamic quant','line_number':8640,'multiline':False]['text':' check static quantization','line_number':8656,'multiline':False]['text':' make sure it runs','line_number':8665,'multiline':False]['text':' check it works in None and static qconfig','line_number':8697,'multiline':False]['text':' make sure it runs','line_number':8707,'multiline':False]['text':' fp16 dynamic quant is not supported for qnnpack','line_number':8718,'multiline':False]['text':' Create fp32 versions of FX and Eager models','line_number':8761,'multiline':False]['text':' FX graph','line_number':8766,'multiline':False]['text':' Eager','line_number':8773,'multiline':False]['text':' verify results match','line_number':8780,'multiline':False]['text':' reshape will be quantized to fp16 as requested by this qconfig','line_number':8812,'multiline':False]['text':' input and weight of first and second linear, output of first and second linear','line_number':8824,'multiline':False]['text':' we insert placeholder observer for both input and output of reshape','line_number':8826,'multiline':False]['text':' dequantize after first linear, before reshape and before output','line_number':8836,'multiline':False]['text':' before reshape, to(fp16)','line_number':8838,'multiline':False]['text':' make sure it runs','line_number':8846,'multiline':False]['text':' TODO: use get_default_qconfig_mapping once it handles fp16','line_number':8865,'multiline':False]['text':' input and weight of linear, output of linear','line_number':8875,'multiline':False]['text':' input and output of sigmoid','line_number':8877,'multiline':False]['text':' make sure it runs','line_number':8884,'multiline':False]['text':' make sure everything runs','line_number':8929,'multiline':False]['text':' verify no crash','line_number':8986,'multiline':False]['text':' print('quantizing:', name, ' mode:', mode)','line_number':9293,'multiline':False]['text':' make sure graph module and script module are both runanble','line_number':9303,'multiline':False]['text':' set to train just before quantization','line_number':9308,'multiline':False]['text':' print('after observation root:', prepared.root)','line_number':9330,'multiline':False]['text':' print('after quantization root:', qgraph.root)','line_number':9333,'multiline':False]['text':' print('after quantization code:', qgraph.src)','line_number':9334,'multiline':False]['text':' print('quantized and scripted:', qgraph_script.graph)','line_number':9337,'multiline':False]['text':' comparing to eager mode quantization','line_number':9349,'multiline':False]['text':' calibration','line_number':9361,'multiline':False]['text':' print('ref after observation:', qeager)','line_number':9375,'multiline':False]['text':' print('ref after quantization:', qeager)','line_number':9380,'multiline':False]['text':' Eager Mode and FX Graph Mode QAT now differ in numerics both','line_number':9428,'multiline':False]['text':' in Post Training and QAT because FX Graph Mode uses same fake_quant instances','line_number':9429,'multiline':False]['text':' for input and output of CopyNode','line_number':9430,'multiline':False]['text':' self.assertEqual(eager_out, graph_out)','line_number':9431,'multiline':False]['text':' test eager and graph consistency','line_number':9464,'multiline':False]['text':' mobilenet/inception_v3/googlenet qat is not working due to AdaptiveAveragePool qat','line_number':9466,'multiline':False]['text':' we might observe the output of AdaptiveAveragePool in the future','line_number':9467,'multiline':False]['text':' and re-enable the test','line_number':9468,'multiline':False]['text':' because relu6 is replaced as relu in mobilenetv2','line_number':9473,'multiline':False]['text':' load pretrained model to compare with quantized model','line_number':9480,'multiline':False]['text':' turn off transform input for inception_v3 since','line_number':9482,'multiline':False]['text':' it's not quantized in eager mode and in fx graph','line_number':9483,'multiline':False]['text':' mode we can't skip quantizing a method right now','line_number':9484,'multiline':False]['text':' (might be supported in the future)','line_number':9485,'multiline':False]['text':' compare with eager mode quantized model when it is available','line_number':9491,'multiline':False]['text':' disable aux logits','line_number':9496,'multiline':False]['text':' print('differences between float and quantized')','line_number':9516,'multiline':False]['text':' print_diffs(diff_of_quant)','line_number':9517,'multiline':False]['text':' print('----------------------')','line_number':9518,'multiline':False]['text':' print('differences between graph mode and eager mode')','line_number':9519,'multiline':False]['text':' print_diffs(diff_from_eager)','line_number':9520,'multiline':False]['text':' print('----------------------')','line_number':9521,'multiline':False]['text':' Make sure EmbeddingBag is now a quantized EmbeddingBag.','line_number':9564,'multiline':False]['text':' Also test that Linear has been quantized.','line_number':9566,'multiline':False]['text':' Make sure EmbeddingBag is now a quantized EmbeddingBag.','line_number':9604,'multiline':False]['text':' Also test that Linear has been quantized.','line_number':9606,'multiline':False]['text':' try backward pass','line_number':9709,'multiline':False]['text':' During the lowering step in convert, fold_weight calls quantized::linear_prepack','line_number':9718,'multiline':False]['text':' which doesn't support QuantizedCuda backend','line_number':9719,'multiline':False]