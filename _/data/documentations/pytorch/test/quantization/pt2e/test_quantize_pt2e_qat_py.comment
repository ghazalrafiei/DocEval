['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]['text':' resetting dynamo cache','line_number':124,'multiline':False]['text':' PT2 export','line_number':128,'multiline':False]['text':' Verify that numerics match','line_number':158,'multiline':False]['text':' We don't want to impose any ordering requirements between move_exported_model_to_eval and convert_pt2e','line_number':162,'multiline':False]['text':' Verify: getitem output activation fake quantize','line_number':229,'multiline':False]['text':' Verify: getitem(bn, 0) or relu(getitem(bn, 0))','line_number':242,'multiline':False]['text':' Verify: conv / scale_factor.reshape [+ bias.reshape]','line_number':261,'multiline':False]['text':' Verify: conv literal args','line_number':276,'multiline':False]['text':' Verify: conv input activation fake quantize','line_number':287,'multiline':False]['text':' Verify: conv weight fake quantize','line_number':303,'multiline':False]['text':' Verify: conv(fq(input), fq(weight * scale_factor.reshape), zero_bias)','line_number':322,'multiline':False]['text':' Verify: scale_factor = bn_weight / sqrt(bn_running_var + eps)','line_number':338,'multiline':False]['text':' NB: Skip the test if this is a base class, this is to handle the test','line_number':357,'multiline':False]['text':' discovery logic in buck which finds and runs all tests here including','line_number':358,'multiline':False]['text':' the base class which we don't want to run','line_number':359,'multiline':False]['text':' stride, padding, dilation, transposed, output_padding, groups','line_number':400,'multiline':False]['text':' stride, padding, dilation, transposed, output_padding, groups','line_number':404,'multiline':False]['text':' Program capture','line_number':555,'multiline':False]['text':' Prepare QAT','line_number':562,'multiline':False]['text':' Verify that the metadata was copied from `conv_bn_getitem`, not `unrelated_getitem`','line_number':570,'multiline':False]['text':' QAT prepare + convert','line_number':626,'multiline':False]['text':' Extract the conv and relu nodes (bn was folded into conv)','line_number':636,'multiline':False]['text':' Extract the conv weight and bias nodes','line_number':649,'multiline':False]['text':' Assert that each set of conv, conv weight, and conv bias are in the same partition','line_number':662,'multiline':False]['text':' E.g. [('l__self___backbone1_conv', <class 'torch.nn.modules.conv.Conv2d'>)]','line_number':664,'multiline':False]['text':' Assert that different sets of convs and relus have different partitions','line_number':672,'multiline':False]['text':' Assert that "backbone" exists only in the second set of conv and relu's partition','line_number':678,'multiline':False]['text':' Assert that both weight and bias are quantized','line_number':694,'multiline':False]['text':' Assert that bias scale = weight scale * input scale','line_number':717,'multiline':False]['text':' Assert that args for the bias' quantize and dequantize ops','line_number':724,'multiline':False]['text':' are copied correctly after subgraph rewriting','line_number':725,'multiline':False]['text':' Assert that conv weight is quantized per channel','line_number':741,'multiline':False]['text':' Assert that args for the weight's quantize and dequantize ops','line_number':754,'multiline':False]['text':' are copied correctly after subgraph rewriting','line_number':755,'multiline':False]['text':' TODO: enable this in the next PR','line_number':768,'multiline':False]['text':' Hardtanh doesnt get quantized via xnnpack quantizer in this test','line_number':949,'multiline':False]['text':' because it relies on the propagation rules','line_number':950,'multiline':False]['text':' Need to fix this','line_number':951,'multiline':False]['text':' must be fixed model.eval()','line_number':990,'multiline':False]['text':' conv2d: 1 for act, 1 for weight, 1 for output','line_number':1011,'multiline':False]['text':' 3 x linear: 1 for act, 1 for output','line_number':1012,'multiline':False]['text':' There needs to be one for hardtanh','line_number':1022,'multiline':False]