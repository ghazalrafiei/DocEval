['text':' namespace','line_number':36,'multiline':False]['text':' Test Generalizing shapes to symbolic dimensions, guarding those symbolic','line_number':39,'multiline':False]['text':' dimensions and passing in runtime computed symbolic dimensions via inlined','line_number':40,'multiline':False]['text':' shape functions','line_number':41,'multiline':False]['text':'
  set up fused TensorExprGroup
  ','line_number':56,'multiline':True]['text':' clang-format off','line_number':93,'multiline':False]['text':' Graph Should Look Something like: (note: strides not yet handled)
  graph(%x_inp : Float(10, 5, strides=[5, 1], requires_grad=0, device=cpu),
      %y_inp : Float(4, 5, strides=[5, 1], requires_grad=0, device=cpu),
      %z_inp : Float(1, 1, strides=[1, 1], requires_grad=0, device=cpu)):
  %4 : bool = prim::TensorExprDynamicGuard[types=[Float(SS(-2), SS(-3), strides=[5, 1], requires_grad=0, device=cpu), Float(SS(-4), SS(-3), strides=[5, 1], requires_grad=0, device=cpu), Float(1, 1, strides=[1, 1], requires_grad=0, device=cpu)]](%x_inp, %y_inp, %z_inp)
  %5 : Tensor = prim::If(%4)
    block0():
      %15 : int[] = aten::size(%x_inp)
      %16 : int[] = aten::size(%y_inp)
      %17 : int = prim::Constant[value=1]()
      %18 : int = prim::Constant[value=0]()
      %elem.3 : int = aten::__getitem__(%15, %18) # <string>:40:10
      %elem.5 : int = aten::__getitem__(%15, %17) # <string>:40:10
      %elem.11 : int = aten::__getitem__(%16, %18) # <string>:40:10
      %cat_dim_size.48 : int = aten::add(%elem.3, %elem.11) # <string>:321:29
      %3 : Tensor = prim::TensorExprGroup_0[symbolic_shape_inputs=[-5, -4, -3, -2]](%x_inp, %y_inp, %z_inp, %cat_dim_size.48, %elem.11, %elem.5, %elem.3)
      -> (%3)
    block1():
      // FallbackGraph is inlined
      %14 : Tensor = prim::FallbackGraph_1(%x_inp, %y_inp, %z_inp)
      -> (%14)
  return ()
  with prim::TensorExprGroup_0 = graph(%x.1 : Float(SS(-2), SS(-3), strides=[5, 1], requires_grad=0, device=cpu),
        %y.1 : Float(SS(-4), SS(-3), strides=[5, 1], requires_grad=0, device=cpu),
        %z : Float(1, 1, strides=[1, 1], requires_grad=0, device=cpu),
        %SS_5 : int,
        %SS_4 : int,
        %SS_3 : int,
        %SS_2 : int):
    %3 : int = prim::Constant[value=0]()
    %4 : Tensor(SS(-2), SS(-3)) = aten::tanh(%x.1)
    %5 : Tensor(SS(-2), SS(-3)) = aten::erf(%4)
    %6 : Tensor(SS(-4), SS(-3)) = aten::relu(%y.1)
    %7 : Tensor[] = prim::ListConstruct(%5, %6)
    %8 : Tensor(SS(-5), SS(-3)) = aten::cat(%7, %3)
    %9 : Tensor(SS(-5), SS(-3)) = aten::hardswish(%8)
    %10 : Tensor(SS(-5), SS(-3)) = aten::mul(%9, %z)
    return (%9)
  ','line_number':94,'multiline':True]['text':' clang-format on','line_number':133,'multiline':False]['text':'
  Test that input to the kernel - (10, 5), (4, 5), (1, 1) - are correctly
  generalized to sym dimensions, and that the output - (10 + 4, 5)
  correctly preserves non-catted dim as sym shape and catted dim as new sym
  shape
  ','line_number':138,'multiline':True]['text':' 1 dims are preserved','line_number':151,'multiline':False]['text':' 5 made into sym shape','line_number':158,'multiline':False]['text':' 4, 10, 14 are different sym shapes','line_number':164,'multiline':False]['text':'
    Test guard behaves correctly at runtime and symbolic shapes are computed
    correctly. As we don't have TE Kernel support for dynamic shapes we're
    going to return all of the computed runtime symbolic dimensions as outputs
    of the graph on guard success, and return None on guard failure
  ','line_number':172,'multiline':True]['text':' Setting up guard to return sym shapes on guard success and None on failure','line_number':179,'multiline':False]['text':' Testing bad inputs','line_number':202,'multiline':False]['text':' sym shape mismatch','line_number':206,'multiline':False]['text':' discontiguous','line_number':207,'multiline':False]['text':' wrong dtype','line_number':209,'multiline':False]['text':' wrong # dims','line_number':210,'multiline':False]['text':' requires grad','line_number':212,'multiline':False]['text':' concrete dim mismatch (1)','line_number':213,'multiline':False]['text':' todo - reusing interpreter across iters gave error','line_number':219,'multiline':False]['text':' Test good inputs','line_number':227,'multiline':False]['text':' Testing that the sym shape calculation was correct','line_number':237,'multiline':False]['text':' Check that the constants have been moved out of the fused graph.','line_number':284,'multiline':False]['text':' This should result in not have any conditionals other than the one','line_number':285,'multiline':False]['text':' checking the result of TensorExprDynamicGuard.','line_number':286,'multiline':False]['text':' no other IFs due to constants.','line_number':290,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-easily-swappable-parameters)','line_number':301,'multiline':False]['text':' namespace','line_number':321,'multiline':False]['text':' Figure out how to fetch a function schema','line_number':324,'multiline':False]['text':' Ask someone else how to create a function schema / operator in C++','line_number':326,'multiline':False]['text':' Check vector initializer list syntax','line_number':333,'multiline':False]['text':' Test that we generate symbolic shapes for the output of a nonzero op','line_number':369,'multiline':False]['text':' Test that nonzero can also create concrete shapes','line_number':375,'multiline':False]['text':' The exact same arguments should return the exact same result','line_number':398,'multiline':False]['text':' Same shape but different symbols should return same shape','line_number':404,'multiline':False]['text':' but different symbolic indices','line_number':405,'multiline':False]['text':' Different concrete shape should be cached separately','line_number':413,'multiline':False]['text':' Show that cache can handle multiple functions','line_number':447,'multiline':False]['text':' Even when the expected outcome is the same, should not collide','line_number':456,'multiline':False]['text':' Don't lose cached objects','line_number':461,'multiline':False]['text':' SSA can infer that sym_dim is 64 as both tensors','line_number':467,'multiline':False]['text':' use the same sym_dim','line_number':468,'multiline':False]['text':' res0 and res1 should share the same symbolic symbol','line_number':489,'multiline':False]['text':' Also test that the shape cache also returns consistent result shapes','line_number':492,'multiline':False]['text':' namespace jit','line_number':498,'multiline':False]['text':' namespace torch','line_number':499,'multiline':False]