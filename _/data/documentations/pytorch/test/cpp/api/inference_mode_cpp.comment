['text':'
  Only the following combos of Autograd & ADInplaceOrView keys on tensors are
  valid:
    - Autograd=true, ADInplaceOrView=true (normal tensor)
    - Autograd=false, ADInplaceOrView=false (inference tensor)
  Tensors created in InferenceMode are mostly inference tensors. The only
  exception is that view of normal tensors created in InferenceMode still
  produce normal tensor.
','line_number':21,'multiline':True]['text':' namespace','line_number':45,'multiline':False]['text':' New tensor created through constructors are inference tensors.','line_number':64,'multiline':False]['text':' requires_grad doesn't change inference tensor behavior inside','line_number':69,'multiline':False]['text':' InferenceMode.','line_number':70,'multiline':False]['text':' Save `a` in an existing autograd session','line_number':85,'multiline':False]['text':' Performing backward should trigger error since `a`'s version has been','line_number':91,'multiline':False]['text':' bumped.','line_number':92,'multiline':False]['text':' go through kernels: CPU','line_number':103,'multiline':False]['text':' go through kernels: CPU','line_number':114,'multiline':False]['text':' go through kernels: CPU','line_number':125,'multiline':False]['text':' Note this is different from NoGradMode but makes sense.','line_number':127,'multiline':False]['text':' Due to issue #54614, this might run slower compared to InferenceMode','line_number':142,'multiline':False]['text':' since intermediate tensors are normal tensors, and they might dispatch to','line_number':143,'multiline':False]['text':' VariableType kernels. This is fine since users can easily fix it by','line_number':144,'multiline':False]['text':' moving it inside InferenceMode block.','line_number':145,'multiline':False]['text':' go through kernels:','line_number':147,'multiline':False]['text':' ADInplaceOrView(fallthrough), CPU','line_number':148,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':164,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':178,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':194,'multiline':False]['text':' inplace -> inplace','line_number':198,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':199,'multiline':False]['text':' inplace -> inplace -> view','line_number':203,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':205,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':220,'multiline':False]['text':' go through kernels: VariableType,','line_number':225,'multiline':False]['text':' ADInplaceOrView(fallthrough), CPU','line_number':226,'multiline':False]['text':' go through kernels: VariableType, ADInplaceOrView, CPU','line_number':230,'multiline':False]['text':' go through kernels: VariableType, ADInplaceOrView, CPU','line_number':234,'multiline':False]['text':' View ops on normal tensor produce normal tensors as output.','line_number':248,'multiline':False]['text':' - For view ops it has both dispatch keys since due to the way we create','line_number':249,'multiline':False]['text':'   view Tensors in alias_with_sizes_and_strides:','line_number':250,'multiline':False]['text':'   ```','line_number':251,'multiline':False]['text':'     auto impl = c10::make_intrusive<TensorImpl>(','line_number':252,'multiline':False]['text':'     Storage(self.storage()), self.key_set(), self.dtype());','line_number':253,'multiline':False]['text':'   ```','line_number':254,'multiline':False]['text':'   In addition, these view output tensors are normal in the sense they','line_number':255,'multiline':False]['text':'   have both Autograd and ADInplaceOrView keys. But they're still','line_number':256,'multiline':False]['text':'   special since they'll have CreationMeta::INFERENCE_MODE. In other','line_number':257,'multiline':False]['text':'   words they behave exactly the same as a view tensor created in','line_number':258,'multiline':False]['text':'   no_grad mode.','line_number':259,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':261,'multiline':False]['text':' view -> view','line_number':267,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':268,'multiline':False]['text':' view -> view -> inplace','line_number':274,'multiline':False]['text':' kernels: ADInplaceOrView, CPU','line_number':275,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':293,'multiline':False]['text':' go through kernels: VariableType,','line_number':306,'multiline':False]['text':' ADInplaceOrView, CPU','line_number':307,'multiline':False]['text':' add(Tensor, Tensor) is safe with inference tensor since it doesn't save','line_number':328,'multiline':False]['text':' any variable for backward.','line_number':329,'multiline':False]['text':' go through kernels: VariableType,','line_number':330,'multiline':False]['text':' ADInplaceOrView(fallthrough), CPU','line_number':331,'multiline':False]['text':' leaf inference tensor with requires_grad=true can still have gradient.','line_number':335,'multiline':False]['text':' Note this behavior is different from NoGradMode which has empty grad.','line_number':336,'multiline':False]['text':' mul(self, other) saves variable when requires_grad=true','line_number':342,'multiline':False]['text':' Inference tensor in TensorList input','line_number':346,'multiline':False]['text':' stack does not capture anymore, so disabled','line_number':347,'multiline':False]['text':' TODO: find alternative Function that captures a list (maybe custom fn)','line_number':348,'multiline':False]['text':'
      std::vector<torch::Tensor> inputs = {s, c};
      ASSERT_THROWS_WITH(
          torch::stack(inputs), // go through kernels: VariableType(ERROR)!,
                                // ADInplaceOrView(fallthrough), CPU
          "Inference tensors cannot be saved for backward.")
      ','line_number':349,'multiline':True]['text':' go through kernels: VariableType(ERROR!), InferenceMode,','line_number':372,'multiline':False]['text':' CPU','line_number':373,'multiline':False]['text':'out=','line_number':378,'multiline':True]['text':' go through kernels: VariableType(ERROR!),','line_number':378,'multiline':False]['text':' ADInplaceOrView, CPU','line_number':379,'multiline':False]['text':'out=','line_number':385,'multiline':True]['text':' go through kernels: VariableType,','line_number':385,'multiline':False]['text':' ADInplaceOrView(ERROR!), CPU','line_number':386,'multiline':False]['text':' view_as is a composite op which calls view() with only one tensor','line_number':401,'multiline':False]['text':' argument. So there isn't a mixed inference tensor and normal tensor','line_number':402,'multiline':False]['text':' inputs for view ops.','line_number':403,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':405,'multiline':False]['text':' This is fine since it's equivalent as s.view(c.sizes()) which','line_number':409,'multiline':False]['text':' isn't a mixed input scenario.','line_number':410,'multiline':False]['text':' go through kernels: VariableType, ADInplaceOrView, CPU','line_number':412,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':425,'multiline':False]['text':' go through kernels: ADInplaceOrView, CPU','line_number':444,'multiline':False]['text':' Suggested workaround','line_number':547,'multiline':False]['text':' Testing both copy_ from VariableTypeManual and add_ from generated code.','line_number':561,'multiline':False]['text':' If InferenceMode didn't set NoGradGuard automatically, this line','line_number':642,'multiline':False]['text':' would error out when trying to save `var1` and `var2` for backward.','line_number':643,'multiline':False]