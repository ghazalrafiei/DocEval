['text':' basic grad','line_number':73,'multiline':False]['text':' This is slightly different than the case above, because z doesn't even','line_number':139,'multiline':False]['text':' have a grad accumulator allocated.','line_number':140,'multiline':False]['text':' allow_unused=False, but grads contains None inside, should throw','line_number':147,'multiline':False]['text':' Test that certain nodes are not erroneously executed when an input','line_number':153,'multiline':False]['text':' is unreachable. See #39784','line_number':154,'multiline':False]['text':'inputs=','line_number':180,'multiline':True]['text':' Warning when grad is accessed for non-leaf tensor','line_number':189,'multiline':False]['text':' It should be possible to call retain_grad() multiple times','line_number':194,'multiline':False]['text':' If retain_grad is true for a non-leaf tensor,','line_number':198,'multiline':False]['text':' there should not be any warning when grad is accessed','line_number':199,'multiline':False]['text':' Gradient should be accumulated','line_number':205,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':206,'multiline':False]['text':'keep_graph=','line_number':207,'multiline':True]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':209,'multiline':False]['text':'keep_graph=','line_number':210,'multiline':True]['text':' It should be a no-op for leaves','line_number':217,'multiline':False]['text':' Needs to have backtrace as warning and then throw an error','line_number':225,'multiline':False]['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':241,'multiline':False]['text':'retain_graph=','line_number':242,'multiline':True]['text':'create_backward=','line_number':242,'multiline':True]['text':'check_nan=','line_number':263,'multiline':True]['text':'check_nan=','line_number':266,'multiline':True]['text':' setup the expected should and should not compute idx','line_number':377,'multiline':False]['text':' Test `needs_input_grad` method is working correctly.','line_number':389,'multiline':False]['text':' We have to test this within the backward function.','line_number':390,'multiline':False]['text':' calculate gradients','line_number':403,'multiline':False]['text':' grad_x','line_number':432,'multiline':False]['text':' needs_input1_grad= ','line_number':437,'multiline':True]['text':' needs_input2_grad= ','line_number':438,'multiline':True]['text':' grad_y','line_number':442,'multiline':False]['text':' needs_input1_grad= ','line_number':447,'multiline':True]['text':' needs_input2_grad= ','line_number':448,'multiline':True]['text':' grad_x and grad_y','line_number':452,'multiline':False]['text':' needs_input1_grad= ','line_number':457,'multiline':True]['text':' needs_input2_grad= ','line_number':458,'multiline':True]['text':' Custom Function should respect grad mode','line_number':550,'multiline':False]['text':' Change the value inplace','line_number':572,'multiline':False]['text':' Clone here because modifying leafs inplace is not allowed','line_number':586,'multiline':False]['text':' TODO ASSERT_THROWS_WITH(DoubleInplace::apply(x.clone()[0]), "only one','line_number':709,'multiline':False]['text':' output");','line_number':710,'multiline':False]['text':' Separate F1 and F2 by another operation','line_number':845,'multiline':False]['text':' NOTE: If this fails for apparently unrelated reasons in TSAN be aware of','line_number':890,'multiline':False]['text':' the TSAN limit on mutex: https://github.com/google/sanitizers/issues/950','line_number':891,'multiline':False]['text':' This should not stack overflow','line_number':916,'multiline':False]['text':' All the reentrant tasks should be prioritized over the MyFunction backward','line_number':967,'multiline':False]['text':' task.','line_number':968,'multiline':False]['text':' Clear static variable in case test get executed in a loop','line_number':972,'multiline':False]['text':'*
 * Tests for AutogradNotImplementedFallback
 * - Check that we created the NotImplemented kernel when inputs require grad
 *   but when no inputs require grad, we should not create this node
 * - check_inplace logic
 * - view ops
 * - TODO: Tests for debug-only checks? Don't need for now because CI doesn't
 * test non-NDEBUG builds.
 * - tensorlist input and output
 * - multiple outputs / non-tensor output
 * - rebase_history vs set_history
 ','line_number':1216,'multiline':True]['text':' This is not allowed. We test below that this calling into the boxed kernel','line_number':1247,'multiline':False]['text':' will raise an error','line_number':1248,'multiline':False]['text':' This is not allowed. We test below that this calling into the boxed kernel','line_number':1255,'multiline':False]['text':' will raise an error','line_number':1256,'multiline':False]['text':' If any inputs require grad,','line_number':1343,'multiline':False]['text':' # Should not have grad_fn if none require grad','line_number':1347,'multiline':False]['text':' TODO: Forward AD Tests?','line_number':1353,'multiline':False]['text':' namespace','line_number':1356,'multiline':False]['text':' Check in-place','line_number':1393,'multiline':False]['text':' Test in-place on view','line_number':1403,'multiline':False]['text':' Both are modified in-place!','line_number':1438,'multiline':False]['text':' Test inplace on view','line_number':1536,'multiline':False]['text':' raise on rebase_history when it refreshes grad_fn','line_number':1539,'multiline':False]['text':' base should not be aware of the views, so this is still okay','line_number':1542,'multiline':False]['text':' TODO add these tests if needed','line_number':1674,'multiline':False]['text':' test_once_differentiable','line_number':1675,'multiline':False]['text':' test_sparse_backward','line_number':1676,'multiline':False]['text':' test_save_output_nr','line_number':1677,'multiline':False]['text':' test_free_deep_graph_pyfunction','line_number':1678,'multiline':False]['text':' test_naughty_anomaly_access','line_number':1679,'multiline':False]['text':' test_naughty_autograd-function_stashing_ctx','line_number':1680,'multiline':False]['text':' test_custom_autograd_repeated_grad_grad','line_number':1681,'multiline':False]['text':' test_return_leaf','line_number':1682,'multiline':False]['text':' test_anomaly_detect_nan','line_number':1683,'multiline':False]['text':' test_no_grad_copy','line_number':1684,'multiline':False]