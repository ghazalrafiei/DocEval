['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,bugprone-narrowing-conversions)','line_number':59,'multiline':False]['text':' Expect the LSTM to have 64 outputs and 3 layers, with an input of batch','line_number':72,'multiline':False]['text':' 10 and 16 time steps (10 x 16 x n)','line_number':73,'multiline':False]['text':' layers','line_number':86,'multiline':False]['text':' Batchsize','line_number':87,'multiline':False]['text':' 64 hidden dims','line_number':88,'multiline':False]['text':' layers','line_number':91,'multiline':False]['text':' Batchsize','line_number':92,'multiline':False]['text':' 64 hidden dims','line_number':93,'multiline':False]['text':' Something is in the hiddens','line_number':95,'multiline':False]['text':' Expect the LSTM to have 32 outputs and 3 layers, with an input of batch','line_number':103,'multiline':False]['text':' 10 and 16 time steps (10 x 16 x n)','line_number':104,'multiline':False]['text':' layers','line_number':117,'multiline':False]['text':' Batchsize','line_number':118,'multiline':False]['text':' 32 hidden dims','line_number':119,'multiline':False]['text':' layers','line_number':122,'multiline':False]['text':' Batchsize','line_number':123,'multiline':False]['text':' 64 cell dims','line_number':124,'multiline':False]['text':' Something is in the hiddens','line_number':126,'multiline':False]['text':' Input size is: sequence length, batch size, input size','line_number':135,'multiline':False]['text':' Hiddens changed','line_number':156,'multiline':False]['text':' Input size is: sequence length, batch size, input size','line_number':162,'multiline':False]['text':' Hiddens changed','line_number':181,'multiline':False]['text':' Make sure the outputs match pytorch outputs','line_number':189,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers,modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)','line_number':213,'multiline':False]['text':' layers x B x 2','line_number':225,'multiline':False]['text':' layers x B x 2','line_number':230,'multiline':False]['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers,modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)','line_number':236,'multiline':False]['text':' Hiddens changed','line_number':312,'multiline':False]['text':' Hiddens changed','line_number':339,'multiline':False]['text':' This test assures that flatten_parameters does not crash,','line_number':395,'multiline':False]['text':' when bidirectional is set to true','line_number':396,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/19545','line_number':397,'multiline':False]['text':' This test is a port of python code introduced here:','line_number':440,'multiline':False]['text':' https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66','line_number':441,'multiline':False]['text':' Reverse forward of bidirectional GRU should act','line_number':442,'multiline':False]['text':' as regular forward of unidirectional GRU','line_number':443,'multiline':False]['text':' Now make sure the weights of the reverse gru layer match','line_number':461,'multiline':False]['text':' ones of the (reversed) bidirectional's:','line_number':462,'multiline':False]['text':' The hidden states of the reversed GRUs sits','line_number':481,'multiline':False]['text':' in the odd indices in the first dimension.','line_number':482,'multiline':False]['text':' Reverse forward of bidirectional LSTM should act','line_number':496,'multiline':False]['text':' as regular forward of unidirectional LSTM','line_number':497,'multiline':False]['text':' Now make sure the weights of the reverse lstm layer match','line_number':516,'multiline':False]['text':' ones of the (reversed) bidirectional's:','line_number':517,'multiline':False]['text':' The hidden states of the reversed LSTM sits','line_number':536,'multiline':False]['text':' in the odd indices in the first dimension.','line_number':537,'multiline':False]['text':' Create two GRUs with the same options','line_number':555,'multiline':False]['text':' Copy weights and biases from CPU GRU to CUDA GRU','line_number':561,'multiline':False]['text':'recurse=','line_number':564,'multiline':True]['text':' Move GRU to CUDA','line_number':573,'multiline':False]['text':' Create the same inputs','line_number':576,'multiline':False]['text':' Call forward on both GRUs','line_number':585,'multiline':False]['text':' Assert that the output and state are equal on CPU and CUDA','line_number':591,'multiline':False]['text':' Create two LSTMs with the same options','line_number':610,'multiline':False]['text':' Copy weights and biases from CPU LSTM to CUDA LSTM','line_number':616,'multiline':False]['text':'recurse=','line_number':619,'multiline':True]['text':' Move LSTM to CUDA','line_number':628,'multiline':False]['text':' Call forward on both LSTMs','line_number':639,'multiline':False]['text':' Assert that the output and state are equal on CPU and CUDA','line_number':645,'multiline':False]['text':' Create two LSTMs with the same options','line_number':664,'multiline':False]['text':' Copy weights and biases from CPU LSTM to CUDA LSTM','line_number':673,'multiline':False]['text':'recurse=','line_number':676,'multiline':True]['text':' Move LSTM to CUDA','line_number':685,'multiline':False]['text':' Call forward on both LSTMs','line_number':696,'multiline':False]['text':' Assert that the output and state are equal on CPU and CUDA','line_number':702,'multiline':False]['text':' Test passing optional argument to `RNN::forward_with_packed_input`','line_number':734,'multiline':False]['text':' Test passing optional argument to `LSTM::forward_with_packed_input`','line_number':752,'multiline':False]['text':' Test passing optional argument to `GRU::forward_with_packed_input`','line_number':770,'multiline':False]['text':' This test assures that pad_packed_sequence does not crash when packed with','line_number':801,'multiline':False]['text':' cuda tensors, https://github.com/pytorch/pytorch/issues/115027','line_number':802,'multiline':False]['text':' Create input on the GPU, sample 5x5','line_number':804,'multiline':False]