['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]['text':' Make the helper files in test/ importable','line_number':15,'multiline':False]['text':' Standard library','line_number':27,'multiline':False]['text':' Main loop','line_number':50,'multiline':False]['text':' Simulate these inputs being in the globals, like they would be if,','line_number':96,'multiline':False]['text':' e.g. they were defined outermost scope of a script','line_number':97,'multiline':False]['text':' fails','line_number':139,'multiline':False]['text':' Backwards tracing was broken for indexing by a constant,','line_number':205,'multiline':False]['text':' because it's internally implemented using as_strided,','line_number':206,'multiline':False]['text':' and we attempted to trace its derivative (which is not','line_number':207,'multiline':False]['text':' currently supported.)  It currently works because','line_number':208,'multiline':False]['text':' slice() is now not marked as traceable.','line_number':209,'multiline':False]['text':' There should be 4 int constants for the right sides of operators, plus one','line_number':252,'multiline':False]['text':' for the alpha argument for add and sub','line_number':253,'multiline':False]['text':' Scalar's get converted to 'wrapped' tensors of default tensor type.','line_number':266,'multiline':False]['text':' Wrapped tensors behave differently in certain promotion operations:','line_number':267,'multiline':False]['text':' float_tensor * double -> float but wrapped_float * double -> double.','line_number':268,'multiline':False]['text':' This can cause issues in check-trace if not handled correctly in','line_number':269,'multiline':False]['text':' `aten::isclose()`.','line_number':270,'multiline':False]['text':' Check that it behaves as expected','line_number':375,'multiline':False]['text':' test the different graph_executor path that happens when','line_number':383,'multiline':False]['text':' gradients are required and sizes are involved','line_number':384,'multiline':False]['text':' Check that it behaves as expected','line_number':412,'multiline':False]['text':' test the different graph_executor path that happens when','line_number':428,'multiline':False]['text':' gradients are required and sizes are involved','line_number':429,'multiline':False]['text':' Test that a trace of torch.full(x.shape) doesn't store the shape as a constant','line_number':433,'multiline':False]['text':' Test that the trace of setitem doesn't store shapes as constants','line_number':444,'multiline':False]['text':' Fix https://github.com/pytorch/pytorch/issues/43548','line_number':445,'multiline':False]['text':' Suppression: we are intentionally slicing a tensor, we don't care that it','line_number':456,'multiline':False]['text':' will be constantified','line_number':457,'multiline':False]['text':' Check that it behaves as expected','line_number':475,'multiline':False]['text':' test the different graph_executor path that happens when','line_number':487,'multiline':False]['text':' gradients are required and sizes are involved','line_number':488,'multiline':False]['text':' Warning 1.','line_number':525,'multiline':False]['text':' Warning 2.','line_number':527,'multiline':False]['text':' Warning 3.','line_number':531,'multiline':False]['text':' Warning 4.','line_number':532,'multiline':False]['text':' Warning 5.','line_number':533,'multiline':False]['text':' Warning 6.','line_number':534,'multiline':False]['text':' should be a tuple nested within another tuple','line_number':557,'multiline':False]['text':' check we recorded 'ones' and did not just record a constant','line_number':585,'multiline':False]['text':' TODO: implement','line_number':604,'multiline':False]['text':' should work to change the values and not the keys','line_number':641,'multiline':False]['text':' error to use something that doesn't have `x`','line_number':645,'multiline':False]['text':' it's okay to have additional elements in the dictionary, so long as 'x' is there','line_number':650,'multiline':False]['text':' three instead of 2 because the export/import in checkTrace adds a','line_number':788,'multiline':False]['text':' `self` module argument','line_number':789,'multiline':False]['text':' trivial identity','line_number':887,'multiline':False]['text':' unused input','line_number':894,'multiline':False]['text':' test outputs that do not get used in grad','line_number':897,'multiline':False]['text':' test autograd fallback','line_number':899,'multiline':False]['text':' more manual test of graph executor that can be used as a scratchpad','line_number':916,'multiline':False]['text':' By default, on Ampere or later GPUs, nn.Linear computes float tensors at TF32 precision.','line_number':948,'multiline':False]['text':' We want float tensors to be computed at full precision in order to use the default precision','line_number':949,'multiline':False]['text':' We're missing some attributes these modules had initially. Make sure we can','line_number':969,'multiline':False]['text':' still get the __repr__()','line_number':970,'multiline':False]['text':' XXX: indexing sequentials is broken','line_number':973,'multiline':False]['text':' All attributes that aren't parameters should raise','line_number':976,'multiline':False]['text':' Submodules can't be called','line_number':984,'multiline':False]['text':' Type casts','line_number':988,'multiline':False]['text':' state_dict + load_state_dict','line_number':1002,'multiline':False]['text':' Make sure trace kernel redispatches to the right lower kernel.','line_number':1102,'multiline':False]['text':' With `check_trace=True` it will run with `@torch.no_grad()` and break assert.','line_number':1107,'multiline':False]['text':' Make sure trace kernel redispatches to the right lower kernel.','line_number':1130,'multiline':False]['text':' With `check_trace=True` it will run with `@torch.no_grad()` and break assert.','line_number':1135,'multiline':False]['text':' testing peephole optimization of size is turned into a constant','line_number':1226,'multiline':False]['text':' in script fn','line_number':1227,'multiline':False]['text':' should work','line_number':1292,'multiline':False]['text':' Note that Bar's forward can only be traced, but not scripted','line_number':1366,'multiline':False]['text':' When tracing Bar as a submodule, we only want to script the','line_number':1375,'multiline':False]['text':' exported methods, and we want to keep the forwards still','line_number':1376,'multiline':False]['text':' being traced.','line_number':1377,'multiline':False]['text':' An autograd.Function with two outputs.','line_number':1423,'multiline':False]['text':' It swaps inputs so we can check if shape','line_number':1424,'multiline':False]['text':' handling is correct in TorchScript.','line_number':1425,'multiline':False]['text':' Generate JIT IR.','line_number':1445,'multiline':False]['text':' Expected output schema of the custom autograd.Function.','line_number':1449,'multiline':False]['text':' See if expected schema exists.','line_number':1454,'multiline':False]['text':' Also examine if the graph is runnable and produces','line_number':1457,'multiline':False]['text':' the right result.','line_number':1458,'multiline':False]['text':' no failure','line_number':1475,'multiline':False]['text':' constants not baked in','line_number':1478,'multiline':False]['text':' Note: neg op from the traced function should be properly inlined','line_number':1534,'multiline':False]['text':' This tests the logic in THPVariable_contiguous. There is short-circuiting','line_number':1683,'multiline':False]['text':' code that prevents us from even getting to VariableType::contiguous, since','line_number':1684,'multiline':False]['text':' it is an optimization that prevents us from acquiring the GIL for touching','line_number':1685,'multiline':False]['text':' the device. We needed to add the tracing logic directly into the','line_number':1686,'multiline':False]['text':' THPVariable_contiguous function only for the path where we are skipping','line_number':1687,'multiline':False]['text':' dispatch into contiguous. We should see an aten::contiguous in this trace!','line_number':1688,'multiline':False]['text':' Explicitly tracing module's forward method','line_number':1978,'multiline':False]['text':' Tracing module's directly','line_number':1983,'multiline':False]['text':' test we propagated shapes through the function','line_number':2121,'multiline':False]['text':' Test that we preserve the module hierarchy for a ScriptModule','line_number':2178,'multiline':False]['text':' submodule during tracing','line_number':2179,'multiline':False]['text':' for each of these checks, check that *BOTH* the underlying','line_number':2213,'multiline':False]['text':' _C.ScriptModule object has the expected method/param, as well as the','line_number':2214,'multiline':False]['text':' Python object that wraps it.','line_number':2215,'multiline':False]['text':' do some random operations and then return a dict of the same structure','line_number':2432,'multiline':False]['text':' a torch.jit.script function gets inserted as a CallFunction node','line_number':2603,'multiline':False]['text':' expect that the CallFunction node return type has shape information on it.','line_number':2614,'multiline':False]