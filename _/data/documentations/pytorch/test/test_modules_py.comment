['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]['text':' Check device placement and dtype for created parameters and buffers.','line_number':29,'multiline':False]['text':' Only verify floating point dtypes since that's what the kwarg or methods','line_number':30,'multiline':False]['text':' such as `float()` applies to.','line_number':31,'multiline':False]['text':' === Instantiate the module. ===','line_number':61,'multiline':False]['text':' === Do forward pass. ===','line_number':67,'multiline':False]['text':' === Compare outputs to a reference if one is specified. ===','line_number':71,'multiline':False]['text':' TODO: Handle precision','line_number':72,'multiline':False]['text':' === Use the method call and verify the parameters and buffers ===','line_number':78,'multiline':False]['text':' Tests passing factory kwargs (e.g. device / dtype) during module instantiation.','line_number':84,'multiline':False]['text':' They should be applied to any created parameters and buffers.','line_number':85,'multiline':False]['text':' Check if this module creates parameters or registers buffers.','line_number':94,'multiline':False]['text':' The mock magic here passes through to the real Parameter / register_buffer','line_number':95,'multiline':False]['text':' logic and is only used to check call inputs.','line_number':96,'multiline':False]['text':' Check if a parameter or buffer was created with a tensor not passed to the constructor.','line_number':105,'multiline':False]['text':' Instantiate module with the factory kwargs.','line_number':117,'multiline':False]['text':' Ensure device and dtype are passed to all UninitializedParameters and UninitializedBuffers.','line_number':124,'multiline':False]['text':' Check device placement and dtype for created parameters and buffers.','line_number':136,'multiline':False]['text':' Only verify floating point dtypes since that's what the kwarg applies to.','line_number':137,'multiline':False]['text':' === Instantiate the module. ===','line_number':155,'multiline':False]['text':' === Do forward pass on GPU ===','line_number':161,'multiline':False]['text':' === Move to CPU ===','line_number':167,'multiline':False]['text':' === Move back to GPU and forward pass ===','line_number':174,'multiline':False]['text':' === test cross-GPU transfer works','line_number':180,'multiline':False]['text':' Test module can be represented with repr and str without errors.','line_number':200,'multiline':False]['text':' Check that these methods do not raise errors','line_number':210,'multiline':False]['text':' Test that module can be pickled and unpickled.','line_number':216,'multiline':False]['text':' === Instantiate the module. ===','line_number':227,'multiline':False]['text':' === Do forward pass. ===','line_number':233,'multiline':False]['text':' === Check unpickled module gives the same output. ===','line_number':237,'multiline':False]['text':' Check if the inplace variant of the module gives the same result as the out of place','line_number':249,'multiline':False]['text':' variant.','line_number':250,'multiline':False]['text':' === Instantiate the module. ===','line_number':258,'multiline':False]['text':' === Inplace modules only supports inplace operations on the first argument ===','line_number':267,'multiline':False]['text':' ===  Do not allow the first input to be in input_kwargs ===','line_number':270,'multiline':False]['text':' === Out of place operation does not write to original tensor ===','line_number':276,'multiline':False]['text':' === Check that the inplace operation gives the same result ===','line_number':283,'multiline':False]['text':' === Check that the gradients are the same ===','line_number':292,'multiline':False]['text':' gradients needs to be retained to check for grad. This is useful when','line_number':311,'multiline':False]['text':' non-leafs are present in the graph.','line_number':312,'multiline':False]['text':' Check modules work with non-contiguous tensors','line_number':332,'multiline':False]['text':' Scalar tensors can not be made non-contiguous','line_number':340,'multiline':False]['text':' scalar tensors can not be non-contiguous','line_number':355,'multiline':False]['text':' === Instantiate the module. ===','line_number':368,'multiline':False]['text':' === Forward with default input','line_number':376,'multiline':False]['text':' === Construct non-contiguous tensors ===','line_number':394,'multiline':False]['text':' === Compare results with non-contiguous and contiguous tensors ===','line_number':398,'multiline':False]['text':' Check gradients','line_number':427,'multiline':False]['text':' === Set nondet tol for gradcheck to user-defined value if on CUDA and cudNN is enabled','line_number':431,'multiline':False]['text':' === Instantiate the module. ===','line_number':440,'multiline':False]['text':' === Lazy modules need to see an input to initialize params before gradcheck is run. ===','line_number':448,'multiline':False]['text':' === Perform gradient check on the input_args ===','line_number':454,'multiline':False]['text':' check total derivative','line_number':474,'multiline':False]['text':' check partial derivatives','line_number':480,'multiline':False]['text':' Turn off TF32 to compute at full precision https://github.com/pytorch/pytorch/issues/86798','line_number':513,'multiline':False]['text':' TODO: RNN / GRU / LSTM don't support backwards on eval mode for cuDNN; skip this in a','line_number':518,'multiline':False]['text':' nicer way for eval mode only.','line_number':519,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/79161','line_number':520,'multiline':False]['text':' Test cpu and gpu results are the same','line_number':528,'multiline':False]['text':' === Move input from cpu to device ===','line_number':546,'multiline':False]['text':' === Construct module on cpu and gpu ===','line_number':554,'multiline':False]['text':' === Lazy modules need to see an input to initialize params ===','line_number':562,'multiline':False]['text':' === Compare forward output between cpu and gpu ===','line_number':571,'multiline':False]['text':' === Run backwards on CPU and GPU and compare results ===','line_number':577,'multiline':False]['text':' TODO tighten it to a specific module','line_number':611,'multiline':False]['text':' Check that at least one Tensor input has dim == n','line_number':629,'multiline':False]['text':' Called after _check_dims, when we know that >= 1 tensor can be converted to mem_format','line_number':638,'multiline':False]['text':' === Instantiate the module. ===','line_number':674,'multiline':False]['text':' === Get output in (contiguous, contiguous) configuration. ===','line_number':681,'multiline':False]['text':' === Do backward pass. ===','line_number':684,'multiline':False]['text':' === Change memformat of input. ===','line_number':704,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/107861','line_number':708,'multiline':False]['text':' When inductor tests are turned on, the setting of requires_grad will be lost','line_number':709,'multiline':False]['text':' === Change memformat of module ===','line_number':725,'multiline':False]['text':' === Do forward pass. ===','line_number':728,'multiline':False]['text':' === Compare outputs to (contiguous, contiguous) output. ===','line_number':732,'multiline':False]['text':' === Check mem format of output. ===','line_number':736,'multiline':False]['text':' === Do backward pass. ===','line_number':739,'multiline':False]['text':' === Check mem format of grad_inputs. ===','line_number':767,'multiline':False]['text':' Test whether train and eval modes differ for each module. Use to verify','line_number':770,'multiline':False]['text':' that the ModuleInfo entry flag is correct.','line_number':771,'multiline':False]['text':' Run forward inputs through to see if the training flag is accessed during forward.','line_number':778,'multiline':False]['text':' === Instantiate the module. ===','line_number':783,'multiline':False]['text':' Remove training attribute and see if forward still works.','line_number':789,'multiline':False]['text':' === Do forward pass. ===','line_number':792,'multiline':False]