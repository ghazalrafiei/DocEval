['text':' Owner(s): ["module: mps"]','line_number':1,'multiline':False]['text':' Copied from `test_ops.py` for the purposes of duplicating `test_numpy_ref`','line_number':51,'multiline':False]['text':' precision issues','line_number':65,'multiline':False]['text':' Unimplemented ops','line_number':70,'multiline':False]['text':' unfold_backward is not implemented','line_number':73,'multiline':False]['text':' csr not supported','line_number':75,'multiline':False]['text':' missing `aten::_unique`.','line_number':81,'multiline':False]['text':' Correctness issues','line_number':85,'multiline':False]['text':' Random output','line_number':88,'multiline':False]['text':' CPU errors','line_number':91,'multiline':False]['text':' derivative for aten::nextafter is not implemented on CPU','line_number':92,'multiline':False]['text':' derivative for aten::floor_divide is not implemented on CPU','line_number':94,'multiline':False]['text':' derivative for aten::narrow_copy is not implemented on CPU','line_number':96,'multiline':False]['text':' derivative for aten::_histogramdd_from_bin_cts is not implemented on CPU','line_number':98,'multiline':False]['text':' derivative for aten::histogram is not implemented','line_number':100,'multiline':False]['text':' 'bool' object is not iterable','line_number':102,'multiline':False]['text':' 'float' object is not iterable','line_number':105,'multiline':False]['text':' "mse_backward_cpu_out" not implemented for 'Half'','line_number':107,'multiline':False]['text':' "smooth_l1_backward_cpu_out" not implemented for 'Half'','line_number':109,'multiline':False]['text':' cpu error: grad requires non-empty inputs','line_number':111,'multiline':False]['text':' trunc_tensor not working properly for float16','line_number':126,'multiline':False]['text':' round not working properly for float16','line_number':130,'multiline':False]['text':' Unsupported Border padding mode, forward pass success as fallback to cpu','line_number':135,'multiline':False]['text':' Unimplemented','line_number':137,'multiline':False]['text':' The result of pow(9 , 8) is showing 43046716, whereas it should've been 43046721.','line_number':140,'multiline':False]['text':' fixed in macOS 13. We are not raising error.','line_number':141,'multiline':False]['text':' Failures due to precision issues (due to fast-math). These has been fixed in MacOS 13.3+','line_number':147,'multiline':False]['text':' Unsupported Border padding mode, forward pass success as fallback to cpu','line_number':152,'multiline':False]['text':' Same issue as `argsort` and `sort` with duplicate elements (undefined behaviour).','line_number':155,'multiline':False]['text':' Forward pass is passing since `msort` doesn't return the indices, just the values, which match the CPU.','line_number':156,'multiline':False]['text':' On the backward pass for `sort` both are used (values and indices), thus resulting in a issmatch between CPU and MPS.','line_number':157,'multiline':False]['text':' Running `msort` with stable `sort` passes.','line_number':158,'multiline':False]['text':' The result of pow(9 , 8) is showing 43046716, whereas it should've been 43046721.','line_number':161,'multiline':False]['text':' fixed in macOS 13. We are not raising error.','line_number':162,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/106112 for more information','line_number':166,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/109166 for more information','line_number':168,'multiline':False]['text':' failed assertion `destination datatype must be fp32'','line_number':174,'multiline':False]['text':' Segfaults','line_number':181,'multiline':False]['text':' Same issue as `argsort` and `sort` with duplicate elements (undefined behaviour).','line_number':187,'multiline':False]['text':' Forward pass is passing since `msort` doesn't return the indices, just the values, which match the CPU.','line_number':188,'multiline':False]['text':' On the backward pass for `sort` both are used (values and indices), thus resulting in a issmatch between CPU and MPS.','line_number':189,'multiline':False]['text':' Running `msort` with stable `sort` passes.','line_number':190,'multiline':False]['text':' Supported complex OPS','line_number':227,'multiline':False]['text':' Those ops worked on MacOS12, but broken on MacOS13, see https://github.com/pytorch/pytorch/issues/85758','line_number':284,'multiline':False]['text':' Top 60','line_number':286,'multiline':False]['text':' expected failures','line_number':287,'multiline':False]['text':' The result of pow(9 , 8) is showing 43046716, whereas it should've been 43046721.','line_number':288,'multiline':False]['text':' fixed in macOS 13.3. Currently error is not raised.','line_number':289,'multiline':False]['text':' expected failures','line_number':291,'multiline':False]['text':' Failures due to precision issues (due to fast-math). These has been fixed in MacOS 13.3+','line_number':294,'multiline':False]['text':' Data type support starts from macOS 13','line_number':298,'multiline':False]['text':' square internally calls into power, and will type cast to int64, which supports starting from macOS 13','line_number':389,'multiline':False]['text':' cpu not giving nan for x/0.0','line_number':392,'multiline':False]['text':' inconsistency errors between cpu and mps, max seen atol is 2','line_number':395,'multiline':False]['text':' Failure due to precision issues (still present on 13.3+) as well as non-standard behavior of','line_number':400,'multiline':False]['text':' cpu ops for the negative integers.','line_number':401,'multiline':False]['text':' Example for torch.polygamma(1, tensor([-0.9, -1.0], dtype=torch.float32)):','line_number':402,'multiline':False]['text':' - CPU output: tensor([102.668, 1.129e+15])','line_number':403,'multiline':False]['text':' - MPS output: tensor([102.6681, inf])','line_number':404,'multiline':False]['text':' In the latter case, inf is probably correct (this is what scipy does).','line_number':405,'multiline':False]['text':' Failures due to precision issues (due to fast-math). These has been fixed in MacOS 13.3+','line_number':415,'multiline':False]['text':' CPU Error: cpu not giving nan for x/0.0','line_number':419,'multiline':False]['text':' test blow pass on macOS 12 as it falls back to cpu','line_number':422,'multiline':False]['text':' Argsort case using duplicate indices (undefined behaviour):','line_number':423,'multiline':False]['text':'  - CPU output: tensor([2546, 6917, 3181,  ..., 7128, 5133,   30], devuce='cpu')','line_number':424,'multiline':False]['text':'  - MPS output: tensor([2546, 6917, 3181,  ..., 7128,   30, 5133], device='mps:0')','line_number':425,'multiline':False]['text':' Elements from index 30 and 5133 are both equal.','line_number':426,'multiline':False]['text':' Since CPU is not using argsort with stable=True, these cases result in undefined behaviour.','line_number':427,'multiline':False]['text':' Same issue as `argsort` with duplicate indices. This test checks both the sorted values and the indices.','line_number':429,'multiline':False]['text':' The values of the sorted tensor match the CPU, but in case of the returned indices this results in undefined behaviour.','line_number':430,'multiline':False]['text':' Unsupported dtypes','line_number':432,'multiline':False]['text':' before macOS 13.2 it falls back to cpu and pass the forward pass','line_number':442,'multiline':False]['text':' Unsupported Border padding mode','line_number':443,'multiline':False]['text':' inconsistency errors between cpu and mps, max seen atol is 2','line_number':444,'multiline':False]['text':' Failure due to precision issue for fp16','line_number':449,'multiline':False]['text':' on both cpu and mps there are test cases that might produce inf result','line_number':450,'multiline':False]['text':' 'nn.functional.pairwise_distance': [torch.float16],','line_number':451,'multiline':False]['text':' test blow pass on macOS 12 as it falls back to cpu','line_number':453,'multiline':False]['text':' Argsort case using duplicate indices (undefined behaviour):','line_number':454,'multiline':False]['text':'  - CPU output: tensor([2546, 6917, 3181,  ..., 7128, 5133,   30], devuce='cpu')','line_number':455,'multiline':False]['text':'  - MPS output: tensor([2546, 6917, 3181,  ..., 7128,   30, 5133], device='mps:0')','line_number':456,'multiline':False]['text':' Elements from index 30 and 5133 are both equal.','line_number':457,'multiline':False]['text':' Since CPU is not using argsort with stable=True, these cases result in undefined behaviour.','line_number':458,'multiline':False]['text':' Same issue as `argsort` with duplicate indices. This test checks both the sorted values and the indices.','line_number':460,'multiline':False]['text':' The values of the sorted tensor match the CPU, but in case of the returned indices this results in undefined behaviour.','line_number':461,'multiline':False]['text':' Failure due to precision issues as well as non-standard behavior of cpu ops for the','line_number':464,'multiline':False]['text':' negative integers. Example for torch.polygamma(1, tensor([-0.9, -1.0], dtype=torch.float32)):','line_number':465,'multiline':False]['text':' - CPU output: tensor([102.668, 1.129e+15])','line_number':466,'multiline':False]['text':' - MPS output: tensor([102.6681, inf])','line_number':467,'multiline':False]['text':' In the latter case, inf is probably correct (this is what scipy does).','line_number':468,'multiline':False]['text':' Those ops are not expected to work','line_number':479,'multiline':False]['text':' Failures due to lack of op implementation on MPS backend','line_number':481,'multiline':False]['text':' Unsupported Border padding mode','line_number':524,'multiline':False]['text':' TODO: max_pool2d for integral types fails the numerical test','line_number':583,'multiline':False]['text':' MPS: input sizes must be divisible by output sizes','line_number':681,'multiline':False]['text':' Unsupported dtypes','line_number':685,'multiline':False]['text':' bmm is not supported for integral types','line_number':686,'multiline':False]['text':' Cannot convert a MPS Tensor to float64 dtype. The tensors','line_number':688,'multiline':False]['text':' input data is created with double in common_methods_invocations.py','line_number':689,'multiline':False]['text':' Convolution for integral types is not supported on MPS','line_number':694,'multiline':False]['text':' Unsupported dtypes','line_number':701,'multiline':False]['text':' GEMM on MPS is not supported for integral types','line_number':709,'multiline':False]['text':' new_zeros/new_ones: Cannot convert a MPS Tensor to float64 dtype as','line_number':728,'multiline':False]['text':' the MPS framework doesn't support float64','line_number':729,'multiline':False]['text':' returned output on CPU is float64','line_number':733,'multiline':False]['text':' trunc_tensor not working properly for float16','line_number':736,'multiline':False]['text':' round not working properly for float16','line_number':740,'multiline':False]['text':' Top 60 operators','line_number':745,'multiline':False]['text':' topk fails with duplicate indices','line_number':746,'multiline':False]['text':' Failures due to random output that they generate using','line_number':749,'multiline':False]['text':' Philox engine causing mismatch with CPU results','line_number':750,'multiline':False]['text':' random results','line_number':751,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/111479','line_number':766,'multiline':False]['text':' duplicate indices are used in the testcase - undefined behaviour','line_number':769,'multiline':False]['text':' zero to negative integer powers are undefined','line_number':771,'multiline':False]['text':' CPU Errors:','line_number':776,'multiline':False]['text':' "addmv_impl_cpu" not implemented for 'Half'','line_number':778,'multiline':False]['text':' cpu result off, showing random values','line_number':780,'multiline':False]['text':' cpu result off, showing random values','line_number':782,'multiline':False]['text':' random results','line_number':784,'multiline':False]['text':' mps vs cpu:','line_number':785,'multiline':False]['text':' Mismatched elements: 40 / 96 (41.7%)','line_number':786,'multiline':False]['text':' Greatest absolute difference: 17.892311096191406 at index (1, 0, 2) (up to 1e-05 allowed)','line_number':787,'multiline':False]['text':' Greatest relative difference: inf at index (1, 0, 0) (up to 1.3e-06 allowed)','line_number':788,'multiline':False]['text':' cuda(2.0.0.dev20230301+cu117) vs cpu:','line_number':789,'multiline':False]['text':' Mismatched elements: 56 / 96 (58.3%)','line_number':790,'multiline':False]['text':' Greatest absolute difference: 17.892311096191406 at index (1, 0, 2) (up to 1e-05 allowed)','line_number':791,'multiline':False]['text':' Greatest relative difference: inf at index (1, 0, 0) (up to 1.3e-06 allowed)','line_number':792,'multiline':False]['text':' Failures due to casting negative float to uint8 is undefined','line_number':795,'multiline':False]['text':' float output for float16 input on MPS','line_number':797,'multiline':False]['text':' Fill tensors with uninitialized data, causing mismatch with CPU.','line_number':802,'multiline':False]['text':' They occasionally match, thus skipping them.','line_number':803,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100175','line_number':804,'multiline':False]['text':' CPU: empty is returning all 0's and there is a mismatch with MPS','line_number':809,'multiline':False]['text':' allocation (MacOS 13). According to','line_number':810,'multiline':False]['text':' https://pytorch.org/docs/2.0/generated/torch.empty.html','line_number':811,'multiline':False]['text':' Unsupported','line_number':822,'multiline':False]['text':' input types 'tensor<1x3x9x9xf16>' and 'tensor<1xf32>' are not broadcast compatible','line_number':823,'multiline':False]['text':' If ops is not supported for complex types, expect it to fail','line_number':864,'multiline':False]['text':' Error input samples do not take a dtype argument.','line_number':871,'multiline':False]['text':' Exceptions are not raised','line_number':873,'multiline':False]['text':' unsupported float64 dtype','line_number':882,'multiline':False]['text':' unsupported complex dtypes','line_number':893,'multiline':False]['text':' MPS does not support tensor dimensions > 16','line_number':898,'multiline':False]['text':' memory overlapping checks','line_number':903,'multiline':False]['text':' unimplemented','line_number':906,'multiline':False]['text':' Same logic as test_cuda.py','line_number':922,'multiline':False]['text':' noqa: F811','line_number':925,'multiline':False]['text':' noqa: F811','line_number':926,'multiline':False]['text':' Determine whether to enable MPS memory leak check (uses same code as CUDA).','line_number':930,'multiline':False]['text':' Performs a gc if required (required if any memory is held)','line_number':946,'multiline':False]['text':' Acquires caching allocator and driver statistics before the test is run','line_number':952,'multiline':False]['text':' Don't check for leaks if an exception was thrown','line_number':957,'multiline':False]['text':' Compares caching allocator before/after statistics','line_number':960,'multiline':False]['text':' An increase in allocated memory is a discrepancy indicating a possible memory leak','line_number':961,'multiline':False]['text':' Short-circuits if no discrepancy detected','line_number':967,'multiline':False]['text':' Validates the discrepancy persists after garbage collection and','line_number':970,'multiline':False]['text':' is confirmed by the driver API','line_number':971,'multiline':False]['text':' Query memory multiple items to ensure leak was not transient','line_number':976,'multiline':False]['text':' Leak was false positive, exit loop','line_number':991,'multiline':False]['text':' Just raises a warning if the leak is not validated by the driver API','line_number':996,'multiline':False]['text':' A caching allocator discrepancy validated by the driver API is a failure','line_number':1005,'multiline':False]['text':' Expand TestCase class with Memory Leak Detection on MPS device','line_number':1014,'multiline':False]['text':' Wraps the tested method if we should do MPS memory check.','line_number':1022,'multiline':False]['text':' checks for leaks even if TEST_MPS_MEM_LEAK_CHECK is 0','line_number':1035,'multiline':False]['text':' Trigger an intentional memory leak','line_number':1047,'multiline':False]['text':' increasing to 8MB to force acquiring a new block and overcome blocksize differences across platforms','line_number':1050,'multiline':False]['text':' check if a runtime error for memory leak was emitted which would','line_number':1055,'multiline':False]['text':' confirm whether memory leak detection worked successfully or not.','line_number':1056,'multiline':False]['text':' Warm up / prebuild MPS shaders (otherwise check fails on 13.2)','line_number':1067,'multiline':False]['text':' If valid_channels_dim=False, add 1 to make channels dim indivisible by upscale_factor ** 2.','line_number':1083,'multiline':False]['text':' Function to imperatively ensure pixels are shuffled to the correct locations.','line_number':1109,'multiline':False]['text':' Used to validate the batch operations in pixel_shuffle.','line_number':1110,'multiline':False]['text':' Ensure unshuffle properly inverts shuffle.','line_number':1133,'multiline':False]['text':' If valid_height_dim=False, add 1 to make height dim indivisible by downscale_factor.','line_number':1143,'multiline':False]['text':' If valid_width_dim=False, add 1 to make width dim indivisible by downscale_factor.','line_number':1145,'multiline':False]['text':' For 1D - 2D, this is an error case.','line_number':1160,'multiline':False]['text':' For 3D - 5D, this is a success case for pixel_shuffle + pixel_unshuffle.','line_number':1161,'multiline':False]['text':' Error cases for pixel_unshuffle.','line_number':1177,'multiline':False]['text':' Convert the numpy array to a PyTorch Tensor,','line_number':1217,'multiline':False]['text':' and move the Tensor to the CPU/GPU based on the "device" parameter','line_number':1218,'multiline':False]['text':' Convert the numpy array to a PyTorch Tensor,','line_number':1227,'multiline':False]['text':' and move the Tensor to the CPU/GPU based on the "device" parameter','line_number':1228,'multiline':False]['text':' Inplace Relu modifies the initial input and it should match the output of Relu','line_number':1234,'multiline':False]['text':' Force execution on CPU even if a GPU kernel is available for the type.','line_number':1239,'multiline':False]['text':' uses `dot`','line_number':1277,'multiline':False]['text':' uses `addmv`','line_number':1281,'multiline':False]['text':' uses `bmm.out`','line_number':1288,'multiline':False]['text':' test backward pass','line_number':1317,'multiline':False]['text':' Because unfold does not support 3D sliding window we will split tensor to multiple tensors and calculate sum','line_number':1338,'multiline':False]['text':' sum_pool2d assumes tensor in (1, 1, n, m) view, so unsqueeze two times','line_number':1341,'multiline':False]['text':' Regression test for gh-36977','line_number':1370,'multiline':False]['text':' test backward pass','line_number':1414,'multiline':False]['text':' Regression test for https://github.com/pytorch/pytorch/issues/114692','line_number':1459,'multiline':False]['text':' to avoid extremum','line_number':1544,'multiline':False]['text':' Do a backward pass to check that it is valid for large','line_number':1549,'multiline':False]['text':' matrices','line_number':1550,'multiline':False]['text':' Test to detect issues in cdist gradient calculation','line_number':1557,'multiline':False]['text':' When the distances are 0','line_number':1558,'multiline':False]['text':' Check that the backward passs does not contain invalid','line_number':1568,'multiline':False]['text':' values such as nan or inf','line_number':1569,'multiline':False]['text':' test for broadcastable inputs','line_number':1641,'multiline':False]['text':' Use the same weights and bias as the ones from the cpu','line_number':1789,'multiline':False]['text':' test gradgrad','line_number':1824,'multiline':False]['text':' Check log_prob computation when value outside range','line_number':1895,'multiline':False]['text':' check cdf computation when value outside range','line_number':1902,'multiline':False]['text':' Large n for torch.half will raise an exception, do not test here.','line_number':1920,'multiline':False]['text':' Default type is long','line_number':1930,'multiline':False]['text':' randperm of 0 elements is an empty tensor','line_number':1934,'multiline':False]['text':' Test non-contiguous tensors','line_number':1941,'multiline':False]['text':' Test forward maxpool2d','line_number':1950,'multiline':False]['text':' Test with no batch dimension','line_number':1990,'multiline':False]['text':' test for max_pool1d','line_number':1994,'multiline':False]['text':' Test padding','line_number':1995,'multiline':False]['text':' test for max_pool1d','line_number':1997,'multiline':False]['text':' Test dilation','line_number':1998,'multiline':False]['text':' test for max_pool1d','line_number':2000,'multiline':False]['text':' Test ceil mode','line_number':2001,'multiline':False]['text':' test for max_pool1d','line_number':2003,'multiline':False]['text':' Test return indices','line_number':2005,'multiline':False]['text':' Test with no batch dimension','line_number':2007,'multiline':False]['text':' test for max_pool1d','line_number':2011,'multiline':False]['text':' Test padding','line_number':2012,'multiline':False]['text':' test for max_pool1d','line_number':2015,'multiline':False]['text':' Test dilation','line_number':2016,'multiline':False]['text':' test for max_pool1d','line_number':2019,'multiline':False]['text':' Test ceil mode','line_number':2020,'multiline':False]['text':' test for max_pool1d','line_number':2023,'multiline':False]['text':' make sure it doesn't crash','line_number':2037,'multiline':False]['text':' test non-contiguous case','line_number':2092,'multiline':False]['text':' This passes','line_number':2125,'multiline':False]['text':' Test forward batch norm','line_number':2130,'multiline':False]['text':' Running stats must be tracked in eval mode','line_number':2269,'multiline':False]['text':' This used to crash, see https://github.com/pytorch/pytorch/issues/98602','line_number':2299,'multiline':False]['text':' This used to crash, see https://github.com/pytorch/pytorch/issues/98602','line_number':2309,'multiline':False]['text':' TODO: Test non-contiguous','line_number':2390,'multiline':False]['text':' Regression test for https://github.com/pytorch/pytorch/issues/96113','line_number':2428,'multiline':False]['text':' Running stats must be tracked in eval mode','line_number':2566,'multiline':False]['text':' linear layer','line_number':2591,'multiline':False]['text':' conv layer','line_number':2629,'multiline':False]['text':' Test conv2d','line_number':2669,'multiline':False]['text':' Test conv transpose 2d','line_number':2742,'multiline':False]['text':' if (bias_shape is not None):','line_number':2779,'multiline':False]['text':'  print(cpu_bias.grad)','line_number':2780,'multiline':False]['text':'  print(bias.grad.to('cpu'))','line_number':2781,'multiline':False]['text':'  self.assertEqual(bias.grad, cpu_bias.grad)','line_number':2782,'multiline':False]['text':' Test sigmoid','line_number':2810,'multiline':False]['text':' Test tanh','line_number':2835,'multiline':False]['text':' Test pow','line_number':2886,'multiline':False]['text':' aten::pow.Tensor_Tensor','line_number':2889,'multiline':False]['text':' aten::pow.Tensor_Scalar','line_number':2899,'multiline':False]['text':' aten::pow.Scalar','line_number':2908,'multiline':False]['text':' Test addcmul','line_number':2919,'multiline':False]['text':' Integral types','line_number':2945,'multiline':False]['text':' Mixed types','line_number':2949,'multiline':False]['text':' Test addcdiv','line_number':2956,'multiline':False]['text':' clamp to avoid division by 0','line_number':2961,'multiline':False]['text':' test .out variant','line_number':2973,'multiline':False]['text':' value of 1 should be ignored internally','line_number':2978,'multiline':False]['text':' this test shouldn't cause any crash','line_number':2981,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/78642','line_number':3000,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/86975','line_number':3008,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/95883','line_number':3044,'multiline':False]['text':' Use the same weights and bias as the ones from the cpu','line_number':3051,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/87856','line_number':3082,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/91065','line_number':3106,'multiline':False]['text':' generate random binary numbers','line_number':3116,'multiline':False]['text':' check copy_cast(unit8 -> bool) on tensors with storage offset','line_number':3119,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/83995','line_number':3202,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/82543','line_number':3254,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/82543','line_number':3263,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/77750','line_number':3277,'multiline':False]['text':' contiguous view','line_number':3283,'multiline':False]['text':' 3, 4','line_number':3284,'multiline':False]['text':' 1, 2','line_number':3285,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/78107','line_number':3331,'multiline':False]['text':' both 'b' and 'c' are views of 'a'','line_number':3337,'multiline':False]['text':' 'b' has a storage offset of 0, while 'c' has a storage offset of 1','line_number':3338,'multiline':False]['text':' when copying from 'cpu' to 'mps', c will have a storage_offset of 1 which needs to be taking into account,','line_number':3339,'multiline':False]['text':' otherwise it ends with same value as 'b'','line_number':3340,'multiline':False]['text':' Test repeat','line_number':3369,'multiline':False]['text':' exercise single argument function signature','line_number':3395,'multiline':False]['text':' Prior to macos 13.3, input of dtype=torch.int64 returns dtype=torch.int32','line_number':3422,'multiline':False]['text':' test zero sized dimension','line_number':3461,'multiline':False]['text':' All non-zeros','line_number':3496,'multiline':False]['text':' dim=1','line_number':3502,'multiline':False]['text':' dim=(0, 1)','line_number':3508,'multiline':False]['text':' Test dtype casting, with and without simultaneous device change','line_number':3530,'multiline':False]['text':' Cast int8 and uint8 to float and compare results','line_number':3554,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/80009 for more details','line_number':3555,'multiline':False]['text':' Casting stride of strided tensor to CPU use to crash with "buffer is not large enough." assert','line_number':3581,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/79181#issuecomment-1154683435','line_number':3582,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/81567','line_number':3587,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/81567','line_number':3605,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/80844','line_number':3623,'multiline':False]['text':' create a list of contiguous view tensors (view tensor created by the slice op)','line_number':3630,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/82427','line_number':3639,'multiline':False]['text':' and https://github.com/pytorch/pytorch/issues/83692','line_number':3640,'multiline':False]['text':' Test should not crash','line_number':3642,'multiline':False]['text':' torch.full should work for uint8','line_number':3644,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/84995','line_number':3650,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/82663','line_number':3658,'multiline':False]['text':' Empty unary op should return tensor of the same size','line_number':3664,'multiline':False]['text':' test scalar','line_number':3671,'multiline':False]['text':' test zero sized tensor','line_number':3681,'multiline':False]['text':' test with expected','line_number':3699,'multiline':False]['text':' tests per-element unique on a higher rank tensor.','line_number':3709,'multiline':False]['text':' test sorted unique','line_number':3734,'multiline':False]['text':' test unsorted unique','line_number':3745,'multiline':False]['text':' Regression test for https://github.com/pytorch/pytorch/issues/104879','line_number':3789,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/85675','line_number':3823,'multiline':False]['text':' TODO: enable memory format test','line_number':3842,'multiline':False]['text':' self.assertEqual(cpu_result.is_contiguous(), mps_result.is_contiguous())','line_number':3843,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/85967','line_number':3845,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/86954','line_number':3852,'multiline':False]['text':' As y is on MPS and z on CPU, this dispatches to a copy operator','line_number':3868,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/95417','line_number':3872,'multiline':False]['text':' implicit type casting and copy','line_number':3879,'multiline':False]['text':' implicit device moving and copy','line_number':3882,'multiline':False]['text':' Regression test for https://github.com/pytorch/pytorch/issues/107867','line_number':3908,'multiline':False]['text':' See https://github.com/pytorch/pytorch/pull/84742','line_number':3911,'multiline':False]['text':' and https://github.com/pytorch/pytorch/pull/78319','line_number':3912,'multiline':False]['text':' Test dtype precedence (casting order) in binary operations by comparing to CPU result','line_number':3914,'multiline':False]['text':' Example values for all dtypes supported on the MPS backend','line_number':3915,'multiline':False]['text':' Test all combinations of dtypes, operations, dimensionality','line_number':3924,'multiline':False]['text':' bool minus bool is generally unsupported, so skip','line_number':3927,'multiline':False]['text':' print(f'{dtype1},{dtype2}: ({val1}).{binop}({val2})')','line_number':3932,'multiline':False]['text':' print(getattr(torch.tensor(val1, dtype=dtype1, device='mps'), binop)','line_number':3933,'multiline':False]['text':'            (torch.tensor(val2, dtype=dtype2, device='mps')))','line_number':3934,'multiline':False]['text':' print(getattr(torch.tensor(val1, dtype=dtype1, device='cpu'), binop)','line_number':3935,'multiline':False]['text':'            (torch.tensor(val2, dtype=dtype2, device='cpu')))','line_number':3936,'multiline':False]['text':' Test tensors created with torch.full','line_number':3957,'multiline':False]['text':' Randomly scale the values','line_number':3978,'multiline':False]['text':' Sanity check on CPU','line_number':3998,'multiline':False]['text':' Test MPS','line_number':4001,'multiline':False]['text':' Test out= variant','line_number':4003,'multiline':False]['text':' dtype','line_number':4010,'multiline':False]['text':' noncontiguous','line_number':4011,'multiline':False]['text':' dim','line_number':4012,'multiline':False]['text':' Test with axis -1','line_number':4039,'multiline':False]['text':' Test with axis -1','line_number':4074,'multiline':False]['text':' test with storage offsets','line_number':4125,'multiline':False]['text':' unfold on a 0-dimensional tensor should always return a 1-d dimensional','line_number':4153,'multiline':False]['text':' tensor of shape [size] (i.e., the second parameter to unfold)','line_number':4154,'multiline':False]['text':' negative input throws','line_number':4176,'multiline':False]['text':' n-d input, with n > 1 throws','line_number':4179,'multiline':False]['text':' minlength < 0 throws','line_number':4182,'multiline':False]['text':' n-d weights, with n > 1 throws','line_number':4187,'multiline':False]['text':' input and weights dim mismatch','line_number':4191,'multiline':False]['text':' 1-d input with no elements and default minlength','line_number':4195,'multiline':False]['text':' 1-d input with no elements and specified minlength','line_number':4198,'multiline':False]['text':' test tensor method without weights','line_number':4202,'multiline':False]['text':' test avoiding overflow for uint8 (#76979)','line_number':4208,'multiline':False]['text':' test minlength functionality','line_number':4212,'multiline':False]['text':' test weights','line_number':4218,'multiline':False]['text':' test non-contiguous inputs and weights','line_number':4229,'multiline':False]['text':' inputs are non-contiguous but weights are contiguous','line_number':4235,'multiline':False]['text':' inputs and weights are non-contiguous','line_number':4237,'multiline':False]['text':' weights are non-contiguous but inputs are contiguous','line_number':4241,'multiline':False]['text':' test bincount on non-contiguous slices','line_number':4245,'multiline':False]['text':' test large number of bins - global memory use','line_number':4252,'multiline':False]['text':' test large input size','line_number':4258,'multiline':False]['text':' L1 loss','line_number':4303,'multiline':False]['text':' create the criterion','line_number':4306,'multiline':False]['text':' forward pass','line_number':4314,'multiline':False]['text':' backward pass','line_number':4319,'multiline':False]['text':' chose 2 just to make the grad_output > 1 in backward pass','line_number':4321,'multiline':False]['text':' verify if changes in shape would cause cached graph lookup problems','line_number':4328,'multiline':False]['text':' Mean Squared Error','line_number':4332,'multiline':False]['text':' create the criterion','line_number':4335,'multiline':False]['text':' forward pass','line_number':4343,'multiline':False]['text':' backward pass','line_number':4348,'multiline':False]['text':' chose 2 just to make the grad_output > 1 in backward pass','line_number':4350,'multiline':False]['text':' verify if changes in shape would cause cached graph lookup problems','line_number':4357,'multiline':False]['text':' Binary Cross Enropy','line_number':4361,'multiline':False]['text':' create the criterion','line_number':4364,'multiline':False]['text':' input and target must be within [0..1]','line_number':4367,'multiline':False]['text':' forward pass','line_number':4375,'multiline':False]['text':' backward pass','line_number':4380,'multiline':False]['text':' chose 0.6 just to have the grad_output != 1','line_number':4382,'multiline':False]['text':' verify if changes in shape would cause cached graph lookup problems','line_number':4389,'multiline':False]['text':' in the comparison of signed vs. unsigned we should always cast to unsigned','line_number':4586,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/98191','line_number':4724,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/92311','line_number':4732,'multiline':False]['text':' Test forward argmin argmax','line_number':4740,'multiline':False]['text':' Test forward max','line_number':4814,'multiline':False]['text':' Note - don't test grad now','line_number':4815,'multiline':False]['text':' helper(2, 8, 4, 5, torch.int64)','line_number':4898,'multiline':False]['text':' median at even place','line_number':4933,'multiline':False]['text':' median at odd place','line_number':4934,'multiline':False]['text':' Test forward min','line_number':5084,'multiline':False]['text':' Test forward sum','line_number':5192,'multiline':False]['text':' Test forward prod','line_number':5257,'multiline':False]['text':' Test forward mean','line_number':5291,'multiline':False]['text':' Test std','line_number':5344,'multiline':False]['text':' verify if a change in shape of input would cause problems with graph caching','line_number':5441,'multiline':False]['text':' Test var','line_number':5444,'multiline':False]['text':' Test forward amax','line_number':5488,'multiline':False]['text':' Test forward amin','line_number':5510,'multiline':False]['text':' Test minimum and maximum','line_number':5532,'multiline':False]['text':' Test clamp_min','line_number':5566,'multiline':False]['text':' Test clamp_max','line_number':5587,'multiline':False]['text':' Test clamp','line_number':5609,'multiline':False]['text':' x=[0..1000)','line_number':5616,'multiline':False]['text':' x=[0..500)','line_number':5621,'multiline':False]['text':' x=[500..1000), to ensure max's are greater than mins','line_number':5626,'multiline':False]['text':' [200..600]: just an arbitrary range between [0..1000]','line_number':5631,'multiline':False]['text':' test optional scalar refs and cached graph keys by passing only max','line_number':5636,'multiline':False]['text':' test optional tensor refs and cached graph keys by passing only max','line_number':5645,'multiline':False]['text':' test strided x','line_number':5650,'multiline':False]['text':' test strided x, min_t, max_t','line_number':5655,'multiline':False]['text':' test strided min_t, max_t','line_number':5660,'multiline':False]['text':' test inplace clamping','line_number':5673,'multiline':False]['text':' clamp to avoid division by 0','line_number':5695,'multiline':False]['text':' align_corners is used for 2D interpolation only','line_number':5881,'multiline':False]['text':' backward pass (chose 0.6 just to have the grad_output != 1)','line_number':5898,'multiline':False]['text':' 1D interpolation','line_number':5903,'multiline':False]['text':' downsample with size','line_number':5905,'multiline':False]['text':' upsample with size','line_number':5906,'multiline':False]['text':' downsample with scale factor','line_number':5907,'multiline':False]['text':' upsample with scale factor','line_number':5908,'multiline':False]['text':' 2D interpolation','line_number':5909,'multiline':False]['text':' downsample_nearest with size','line_number':5911,'multiline':False]['text':' upsample_nearest with size','line_number':5912,'multiline':False]['text':' downsample_nearest with scale factor','line_number':5913,'multiline':False]['text':' upsample_nearest with scale factor','line_number':5914,'multiline':False]['text':' align_corners=True','line_number':5915,'multiline':False]['text':' Test concat forward','line_number':5919,'multiline':False]['text':' Test stack forward','line_number':5946,'multiline':False]['text':' All shapes must be same','line_number':5948,'multiline':False]['text':' Empty test - Currently failing! Empty tensor not handled!','line_number':5987,'multiline':False]['text':' helper([0, 2, 4, 5])','line_number':5988,'multiline':False]['text':' Test abs','line_number':5990,'multiline':False]['text':' Test concat forward','line_number':6081,'multiline':False]['text':' Empty test - Currently failing! Empty tensor not handled!','line_number':6119,'multiline':False]['text':' helper([0, 2, 4, 5], [2, 0, 4, 5], [2, 5, 0, 5])','line_number':6120,'multiline':False]['text':' Test isnan','line_number':6122,'multiline':False]['text':' make a selected row inf','line_number':6127,'multiline':False]['text':' Test reciprocal','line_number':6138,'multiline':False]['text':' Test sqrt','line_number':6158,'multiline':False]['text':' Test selu, elu, celu','line_number':6178,'multiline':False]['text':' Test empty shape too','line_number':6198,'multiline':False]['text':' Test glu','line_number':6204,'multiline':False]['text':' Test softplus','line_number':6227,'multiline':False]['text':' Test empty shape too','line_number':6245,'multiline':False]['text':' Test silu','line_number':6254,'multiline':False]['text':' Test empty shape too','line_number':6273,'multiline':False]['text':' needs to match the initial Tensor','line_number':6283,'multiline':False]['text':' Test adaptive avg pool2d - when the input size is a multiple of output size','line_number':6313,'multiline':False]['text':' Not testing for channels last right now','line_number':6314,'multiline':False]['text':' Output shape larger than input shape','line_number':6343,'multiline':False]['text':' Test max avg pool2d - when the input size is a multiple of output size','line_number':6358,'multiline':False]['text':' Not testing for channels last right now','line_number':6359,'multiline':False]['text':' GELU is not supported on CPU, so cast it to float','line_number':6408,'multiline':False]['text':' Test empty shape too','line_number':6422,'multiline':False]['text':' Test that gelu would raise an assert for integral types','line_number':6426,'multiline':False]['text':' Test multi threaded','line_number':6452,'multiline':False]['text':' Test hardtanh','line_number':6471,'multiline':False]['text':' Test empty shape too','line_number':6496,'multiline':False]['text':' check that both raise runtime error','line_number':6509,'multiline':False]['text':' Test sign','line_number':6593,'multiline':False]['text':' Test neg','line_number':6626,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/98074#issuecomment-1496088337','line_number':6646,'multiline':False]['text':' Test index add','line_number':6652,'multiline':False]['text':' test result dim=1','line_number':6674,'multiline':False]['text':' test float16','line_number':6677,'multiline':False]['text':' Test flip','line_number':6680,'multiline':False]['text':' empty dims','line_number':6695,'multiline':False]['text':' input.numel() == 1','line_number':6697,'multiline':False]['text':' input.numel() == 0','line_number':6699,'multiline':False]['text':' none of dims that needs to be flipped','line_number':6701,'multiline':False]['text':' Test index select','line_number':6704,'multiline':False]['text':' weight must be cloned for this to be differentiable','line_number':6750,'multiline':False]['text':' modifies weight in-place','line_number':6752,'multiline':False]['text':' weight must be cloned for this to be differentiable','line_number':6761,'multiline':False]['text':' modifies weight in-place','line_number':6763,'multiline':False]['text':' verify if changes in shape would cause cached graph lookup problems','line_number':6773,'multiline':False]['text':' test scalar index','line_number':6774,'multiline':False]['text':' Test pytorch gather','line_number':6776,'multiline':False]['text':' Indices should be taken from range of axis along which gathering is done','line_number':6782,'multiline':False]['text':' Test pytorch gather','line_number':6809,'multiline':False]['text':' Test pytorch scatter_add and scatter','line_number':6831,'multiline':False]['text':' Indices should be taken from range of axis along which gathering is done','line_number':6840,'multiline':False]['text':' Test scatter src','line_number':6894,'multiline':False]['text':' Test pytorch scatter_add and scatter for scalar input','line_number':6898,'multiline':False]['text':' Indices should be taken from range of axis along which gathering is done','line_number':6907,'multiline':False]['text':' Test pytorch scatter_reduce','line_number':6938,'multiline':False]['text':' Indices should be taken from range of axis along which gathering is done','line_number':6947,'multiline':False]['text':' for reduce in ["sum", "prod", "amax", "amin"]:','line_number':6958,'multiline':False]['text':' Test triu','line_number':6982,'multiline':False]['text':' Test inverse','line_number':7008,'multiline':False]['text':' Test tril','line_number':7023,'multiline':False]['text':' test eye','line_number':7049,'multiline':False]['text':' Test diag','line_number':7072,'multiline':False]['text':' cpu_grad = torch.randn(diag_result_cpu.shape)','line_number':7081,'multiline':False]['text':' grad = cpu_grad.to('mps')','line_number':7082,'multiline':False]['text':' diag_result.backward(gradient=grad)','line_number':7084,'multiline':False]['text':' diag_result_cpu.backward(gradient=cpu_grad)','line_number':7085,'multiline':False]['text':' self.assertEqual(x.grad, cpu_x.grad)','line_number':7088,'multiline':False]['text':' Test linspace','line_number':7094,'multiline':False]['text':' Test argange','line_number':7107,'multiline':False]['text':' Test rgange','line_number':7122,'multiline':False]['text':' Test softmax','line_number':7129,'multiline':False]['text':' Currently NOT testing backward for channels last backward','line_number':7141,'multiline':False]['text':' Test where','line_number':7188,'multiline':False]['text':' Test normal','line_number':7227,'multiline':False]['text':' test out','line_number':7242,'multiline':False]['text':' test without out','line_number':7252,'multiline':False]['text':' probability of drawing "1" is 0.5','line_number':7272,'multiline':False]['text':' We can't check reliably the mean and std.','line_number':7274,'multiline':False]['text':' Just make sure we don't return constant values','line_number':7275,'multiline':False]['text':' probability of drawing "1" is 0','line_number':7279,'multiline':False]['text':' probability of drawing "1" is 1','line_number':7283,'multiline':False]['text':' Check it works for different dtypes','line_number':7287,'multiline':False]['text':' Check that output is not all zeros or ones','line_number':7290,'multiline':False]['text':' explicit manual seeding by creating an MPS Generator','line_number':7299,'multiline':False]['text':' seed values were the same, so the random tensor contents should match','line_number':7305,'multiline':False]['text':' save generator's state to restore it later','line_number':7307,'multiline':False]['text':' generate random numbers without seeding','line_number':7310,'multiline':False]['text':' in this case, the random results must differ from the last generated random results','line_number':7312,'multiline':False]['text':' restore the previously saved state, and the results should match again','line_number':7315,'multiline':False]['text':' manual seeding on the "default" MPS generator using','line_number':7321,'multiline':False]['text':' the global torch.manual_seed()','line_number':7322,'multiline':False]['text':' manual seeding using torch.mps.manual_seed()','line_number':7325,'multiline':False]['text':' which should set the "default" MPS generator','line_number':7326,'multiline':False]['text':' like the global torch.manual_seed()','line_number':7327,'multiline':False]['text':' seed values were the same, so the random tensor contents should match','line_number':7330,'multiline':False]['text':' save the default generator's state to restore it later','line_number':7333,'multiline':False]['text':' generate random numbers without seeding','line_number':7336,'multiline':False]['text':' in this case, the random results must differ from the last generated random results','line_number':7338,'multiline':False]['text':' restore the previously saved state, and the results should match again','line_number':7341,'multiline':False]['text':' just running some ops each followed by a synchronize to wait for','line_number':7347,'multiline':False]['text':' MPS stream to finish running each of them','line_number':7348,'multiline':False]['text':' first garbage collect and empty the cached blocks','line_number':7361,'multiline':False]['text':' measure memory allocations from MPSAllocator','line_number':7364,'multiline':False]['text':' after garbage collection and emptying the cache the','line_number':7366,'multiline':False]['text':' current_allocated_memory must be zero','line_number':7367,'multiline':False]['text':' measure total memory allocations from Metal driver','line_number':7369,'multiline':False]['text':' allocate a new 8 MB tensor to force allocation of a new Metal Heap','line_number':7371,'multiline':False]['text':' get memory allocations after allocating tensor x','line_number':7373,'multiline':False]['text':' current and driver memory allocations must have','line_number':7376,'multiline':False]['text':' grown at this point','line_number':7377,'multiline':False]['text':' to verify this test, run XCode Instruments "Metal System Trace" or "Logging" tool,','line_number':7381,'multiline':False]['text':' press record, then run this python test, and press stop. Next expand','line_number':7382,'multiline':False]['text':' the os_signposts->PyTorchMPS and check if events or intervals are logged','line_number':7383,'multiline':False]['text':' like this example:','line_number':7384,'multiline':False]['text':' "aten::mps_convolution_backward_input:f32[1,128,6,6]:f32[128,64,3,3]:1,128,6,6 (id=G2, run=2)"','line_number':7385,'multiline':False]['text':' just running some ops to capture the OS Signposts traces for profiling','line_number':7388,'multiline':False]['text':' just running some ops to capture the OS Signposts traces for profiling','line_number':7395,'multiline':False]['text':' Test random_, random_.to and random_.from','line_number':7421,'multiline':False]['text':' We can't check reliably the mean and std.','line_number':7427,'multiline':False]['text':' Just make sure we don't return constant values','line_number':7428,'multiline':False]['text':' Test random_','line_number':7438,'multiline':False]['text':' Test exponential','line_number':7444,'multiline':False]['text':' Test add','line_number':7470,'multiline':False]['text':' fp16 isn't accurate when alpha is passed','line_number':7487,'multiline':False]['text':' TODO: remove or fix 'tol' when we fix problems with fp16','line_number':7488,'multiline':False]['text':' in-place output cannot be broadcasted.','line_number':7491,'multiline':False]['text':' create a scalar tensor','line_number':7492,'multiline':False]['text':' primary tensor is scalar','line_number':7495,'multiline':False]['text':' create a scalar tensor','line_number':7497,'multiline':False]['text':' secondary tensor is scalar','line_number':7500,'multiline':False]['text':' Test add','line_number':7512,'multiline':False]['text':' fp16 isn't accurate when alpha is passed','line_number':7524,'multiline':False]['text':' Test int32 tensor + int64 scalar add','line_number':7533,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/79835#issuecomment-1164984534','line_number':7534,'multiline':False]['text':' Float * Bool','line_number':7540,'multiline':False]['text':' Float * Int64','line_number':7544,'multiline':False]['text':' test in-place with storage_offset','line_number':7589,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100764','line_number':7597,'multiline':False]['text':' Test with num_dist = 1','line_number':7632,'multiline':False]['text':' Compare "real" with theoretical values','line_number':7641,'multiline':False]['text':' TODO: Add tests for data types','line_number':7645,'multiline':False]['text':' CPU','line_number':7781,'multiline':False]['text':' MPS','line_number':7785,'multiline':False]['text':' t should have size (10,)','line_number':7819,'multiline':False]['text':' out of bound ignore_index','line_number':7833,'multiline':False]['text':' default ignore_index','line_number':7835,'multiline':False]['text':' CPU','line_number':7878,'multiline':False]['text':' MPS','line_number':7885,'multiline':False]['text':' CPU','line_number':7900,'multiline':False]['text':' MPS','line_number':7905,'multiline':False]['text':' Zero Element Tensors','line_number':7995,'multiline':False]['text':' Multiple Element Tensors','line_number':8000,'multiline':False]['text':' This file was generated by running on PyTorch 1.0.1 on Python 2:','line_number':8059,'multiline':False]['text':'','line_number':8060,'multiline':False]['text':'     import torch','line_number':8061,'multiline':False]['text':'     from torch import nn','line_number':8062,'multiline':False]['text':'     m = nn.Conv2d(1, 1, 1)','line_number':8063,'multiline':False]['text':'     torch.save(m, 'legacy_conv2d.pt')','line_number':8064,'multiline':False]['text':'','line_number':8065,'multiline':False]['text':' NB: This Pickle also contains some Unicode data!','line_number':8066,'multiline':False]['text':' The test should not crash','line_number':8081,'multiline':False]['text':' Printing of non_contiguous should not crash','line_number':8092,'multiline':False]['text':' uninitialized grad','line_number':8106,'multiline':False]['text':' Force set to zeros.','line_number':8124,'multiline':False]['text':' Negative stride check','line_number':8159,'multiline':False]['text':' Test for https://github.com/pytorch/pytorch/issues/55781','line_number':8166,'multiline':False]['text':' Disable MKLDNN explicitly, so that either NNPACK or THCNN will be used','line_number':8172,'multiline':False]['text':' Negative stride check','line_number':8191,'multiline':False]['text':' Zero stride check','line_number':8197,'multiline':False]['text':' Input and weights on different devices','line_number':8203,'multiline':False]['text':' Test F.conv2d padding='valid' is the same as no padding','line_number':8213,'multiline':False]['text':' Test for https://github.com/pytorch/pytorch/issues/112998','line_number':8222,'multiline':False]['text':' This used to crash with MPSNDArrayConvolutionA14.mm:4352: failed assertion','line_number':8229,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/88331 for more detail','line_number':8260,'multiline':False]['text':' def test_conv2d_same_padding(self, device='mps'):','line_number':8286,'multiline':False]['text':' x = torch.rand(1, 1, 10, 11, device=device)','line_number':8287,'multiline':False]['text':' y = torch.rand(1, 1, 4, 5, device=device)','line_number':8288,'multiline':False]['text':' expect = F.conv2d(x, y, padding=(2, 2))[..., 1:, :]','line_number':8289,'multiline':False]['text':' actual = F.conv2d(x, y, padding='same')','line_number':8290,'multiline':False]['text':' self.assertEqual(expect.to('cpu'), actual.to('cpu'))','line_number':8291,'multiline':False]['text':' # With dilation','line_number':8293,'multiline':False]['text':' y = torch.rand(1, 1, 3, 4, device=device)','line_number':8294,'multiline':False]['text':' expect = F.conv2d(x, y, padding=(2, 3), dilation=2)','line_number':8295,'multiline':False]['text':' actual = F.conv2d(x, y, padding='same', dilation=2)','line_number':8296,'multiline':False]['text':' self.assertEqual(expect, actual)','line_number':8297,'multiline':False]['text':' # Dilation with asymmetric padding','line_number':8299,'multiline':False]['text':' y = torch.rand(1, 1, 4, 4, device=device)','line_number':8300,'multiline':False]['text':' expect = F.conv2d(x, y, padding=5, dilation=3)[..., 1:, 1:]','line_number':8301,'multiline':False]['text':' actual = F.conv2d(x, y, padding='same', dilation=3)','line_number':8302,'multiline':False]['text':' self.assertEqual(expect, actual)','line_number':8303,'multiline':False]['text':' Arbitrary input dimensions','line_number':8315,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/80856','line_number':8325,'multiline':False]['text':' backward pass (chose 0.6 just to have the grad_output != 1)','line_number':8361,'multiline':False]['text':' 1D Padding','line_number':8366,'multiline':False]['text':' verify if a change in shape of input would cause problems with graph caching','line_number':8368,'multiline':False]['text':' Replication 1D','line_number':8370,'multiline':False]['text':' Constant Pad 1D','line_number':8372,'multiline':False]['text':' Constant Pad 1D with single dimension input','line_number':8374,'multiline':False]['text':' 2D Padding','line_number':8377,'multiline':False]['text':' verify if a change in shape of input would cause problems with graph caching','line_number':8379,'multiline':False]['text':' this should make the padding (2, 2, 2, 2)','line_number':8381,'multiline':False]['text':' verify if a change in shape of padding would cause problems with graph caching','line_number':8383,'multiline':False]['text':' Constant Pad 2D','line_number':8385,'multiline':False]['text':' input size < pad size','line_number':8387,'multiline':False]['text':' pad dims < input dims','line_number':8389,'multiline':False]['text':' pad dims == input dims','line_number':8391,'multiline':False]['text':' input.numel() == 0 but output.numel() > 0','line_number':8393,'multiline':False]['text':' pad dims < input dims - 2','line_number':8395,'multiline':False]['text':' 3D Padding','line_number':8398,'multiline':False]['text':' verify if a change in shape of padding would cause problems with graph caching','line_number':8400,'multiline':False]['text':' case where input_d == pad_front/back for ReplicationPad3d','line_number':8402,'multiline':False]['text':' Constant Pad 3D','line_number':8404,'multiline':False]['text':' input size < pad size','line_number':8406,'multiline':False]['text':' check the workaround for the right padding bug in Monterey','line_number':8408,'multiline':False]['text':' Test beta=0, M=nan','line_number':8445,'multiline':False]['text':' Test transpose','line_number':8451,'multiline':False]['text':' Test beta=0, M=nan','line_number':8481,'multiline':False]['text':' Slicing with step','line_number':8489,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/78886','line_number':8490,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/78074','line_number':8518,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/79672','line_number':8533,'multiline':False]['text':' These tests were taken from test/test_view_ops.py','line_number':8548,'multiline':False]['text':' They are subset of those tests as currently only this subset is working.','line_number':8549,'multiline':False]['text':' This whole `class` will be removed when we add generic device testing. There','line_number':8550,'multiline':False]['text':' are no additional tests added apart from what is part of test_view_ops.py','line_number':8551,'multiline':False]['text':' test the fix for crash reported in','line_number':8556,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/94190','line_number':8557,'multiline':False]['text':' this print caused a crash prior to fix PR#94259','line_number':8562,'multiline':False]['text':' test the fix for fill_scalar_mps() mentioned in issue #94190','line_number':8564,'multiline':False]['text':' Note: only validates storage on native device types','line_number':8574,'multiline':False]['text':' because some accelerators, like XLA, do not expose storage','line_number':8575,'multiline':False]['text':' Returns true if v1 and v2 are views of the same base','line_number':8582,'multiline':False]['text':' Performs transpose if contiguous=True, else returns the input tensor as is','line_number':8588,'multiline':False]['text':' zero-dimensional tensor','line_number':8839,'multiline':False]['text':' stride[i] = stride[i + 1] * size[i + 1] is satisfied for 3 groups:','line_number':8850,'multiline':False]['text':'               [--1--|---2---|-3-] [--1--|----2---|-3-]','line_number':8853,'multiline':False]['text':' flatten returns the original object if start_dim=end_dim','line_number':8879,'multiline':False]['text':' Randomly change values in output','line_number':8935,'multiline':False]['text':' and verify that original is changed','line_number':8936,'multiline':False]['text':' as well.','line_number':8937,'multiline':False]['text':' Testing that the generated view_copy kernel and its derivative are implemented correctly','line_number':8950,'multiline':False]['text':' view_copy ops don't preserve view relationship','line_number':8957,'multiline':False]['text':' forward and backward give the same shape + result','line_number':8964,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/86052','line_number':8988,'multiline':False]['text':' .detach() makes y not a view, but contig tensor','line_number':8990,'multiline':False]['text':' with non-zero offset','line_number':8991,'multiline':False]['text':' should be viewable -- i.e. data_ptr is the same.','line_number':8999,'multiline':False]['text':' match NumPy semantics -- don't infer the size of dimension with a degree of freedom','line_number':9002,'multiline':False]['text':' test double expand','line_number':9017,'multiline':False]['text':' test non-contiguous','line_number':9020,'multiline':False]['text':' make sure it's compatible with unsqueeze','line_number':9025,'multiline':False]['text':' test -1 as target size','line_number':9031,'multiline':False]['text':' test expanding empty to empty','line_number':9035,'multiline':False]['text':' .data_ptr() on meta tensors is always 0 so they are equal regardless of the reshape','line_number':9050,'multiline':False]['text':' TODO: fix these once we have multi-dimensional empty tensors','line_number':9064,'multiline':False]['text':' Test 0D tensors','line_number':9096,'multiline':False]['text':' Test 1D tensors','line_number':9102,'multiline':False]['text':' Test 2D tensors','line_number':9108,'multiline':False]['text':' Test 3D tensor','line_number':9114,'multiline':False]['text':' Variable sections split','line_number':9134,'multiline':False]['text':' Invalid chunk sizes','line_number':9169,'multiline':False]['text':' unit test for special case transposed copy (see ATen/native/Copy.cpp for details)','line_number':9190,'multiline':False]['text':' TODO: is resize best put in test_view_ops?','line_number':9243,'multiline':False]['text':' TODO: OpInfo this','line_number':9269,'multiline':False]['text':' 0-dim','line_number':9271,'multiline':False]['text':' 1-dim','line_number':9277,'multiline':False]['text':' 2,3,4-dim','line_number':9283,'multiline':False]['text':' test size inference with empty tensors','line_number':9314,'multiline':False]['text':' change the stride in dimension 0. the tensor is still contiguous because size[0] is 1','line_number':9333,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/82921','line_number':9365,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/81557','line_number':9381,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/82711','line_number':9394,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/82563','line_number':9408,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/84511','line_number':9423,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/83180','line_number':9464,'multiline':False]['text':' With square kernels and equal stride','line_number':9516,'multiline':False]['text':' non-square kernels and unequal stride and with padding','line_number':9519,'multiline':False]['text':' grid_dim_contig_order specifies the dimension order that can','line_number':9563,'multiline':False]['text':' make grid to be contiguous.','line_number':9564,'multiline':False]['text':' i.e., grid.permute(grid_dim_contig_order) is contiguous.','line_number':9565,'multiline':False]['text':' e.g., with grid_dim_contig_order=[0, 3, 1, 2], grid should be','line_number':9566,'multiline':False]['text':'       initialized with contiguous tensor of shape [N, 2, H, W]','line_number':9567,'multiline':False]['text':'       and permuted to [N, H, W, 2] afterwards.','line_number':9568,'multiline':False]['text':' Compare against unvectorized CPU fallback','line_number':9595,'multiline':False]['text':' NOTE [ grid_sample CPU fallback ]','line_number':9597,'multiline':False]['text':' grid_sample uses AVX for 2d images, but that requires 32-bit indexing for','line_number':9598,'multiline':False]['text':' 32-bit floats. So we also have a fallback that is used only for float tensors','line_number':9599,'multiline':False]['text':' requiring 64-bit indexing. That requires too much memory to run on CI, so we','line_number':9600,'multiline':False]['text':' also export the fallback and test it here to ensure feature parity with','line_number':9601,'multiline':False]['text':' the vectorized version.','line_number':9602,'multiline':False]['text':' check that zero-dimensional input strides don't error out','line_number':9626,'multiline':False]['text':' test same size output','line_number':9636,'multiline':False]['text':' test larger output','line_number':9639,'multiline':False]['text':' test smaller output','line_number':9648,'multiline':False]['text':' test 1x1 inpput','line_number':9657,'multiline':False]['text':' testing empty grid','line_number':9666,'multiline':False]['text':' testing empty channel','line_number':9674,'multiline':False]['text':' testing empty batch','line_number':9682,'multiline':False]['text':' test known input','line_number':9693,'multiline':False]['text':' windows does not work for bfloat16 randing','line_number':9826,'multiline':False]['text':' Verifies that JIT script cannot handle the as_tuple kwarg','line_number':9860,'multiline':False]['text':' See Issue https://github.com/pytorch/pytorch/issues/45499.','line_number':9861,'multiline':False]['text':' Verifies that JIT tracing works fine','line_number':9872,'multiline':False]['text':' expect dst3 storage to be reused','line_number':9892,'multiline':False]['text':' discontiguous out','line_number':9896,'multiline':False]['text':' Test that MPS does not crash if nonzero called concurrently','line_number':9912,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100285','line_number':9913,'multiline':False]['text':' examples from https://www.tutorialspoint.com/numpy/numpy_advanced_indexing.htm','line_number':9931,'multiline':False]['text':' FIXME: uint8 fails for this testcase, needs further debugging','line_number':9959,'multiline':False]['text':' using advanced index for column','line_number':9969,'multiline':False]['text':' FIXME: use supported_dtypes once uint8 is fixed','line_number':9973,'multiline':False]['text':' MPS support binary op with uint8 natively starting from macOS 13.0','line_number':9986,'multiline':False]['text':' torch.randn / torch.rand don't work with all dtypes','line_number':10014,'multiline':False]['text':' Generate input data for all dtypes on Numpy them move to torch','line_number':10015,'multiline':False]['text':' torch.randn / torch.rand don't work with all dtypes','line_number':10060,'multiline':False]['text':' Generate input data for all dtypes on Numpy them move to torch','line_number':10061,'multiline':False]['text':' tests from 'test_indexing.py'','line_number':10085,'multiline':False]['text':' note: these broadcast together and are transposed to the first dim','line_number':10157,'multiline':False]['text':' TODO: replace with a better solution.','line_number':10240,'multiline':False]['text':' Currently, here using torchscript to put None into indices.','line_number':10241,'multiline':False]['text':' on C++ it gives indices as a list of 2 optional tensors: first is null and','line_number':10242,'multiline':False]['text':' the second is a valid tensor.','line_number':10243,'multiline':False]['text':' generate indices by random walk, this will create indices with','line_number':10268,'multiline':False]['text':' lots of duplicates interleaved with each other','line_number':10269,'multiline':False]['text':' abs for int64 is not supported on mps, fallback on 'cpu' to calculate it','line_number':10274,'multiline':False]['text':' note: these broadcast together and are transposed to the first dim','line_number':10317,'multiline':False]['text':' test index','line_number':10375,'multiline':False]['text':' test index_put, no accum','line_number':10378,'multiline':False]['text':' From the NumPy indexing example','line_number':10384,'multiline':False]['text':' From the NumPy indexing example','line_number':10391,'multiline':False]['text':' empty assignment should have no effect but not throw an exception','line_number':10403,'multiline':False]['text':' this isn't technically necessary, but matches NumPy stride calculations.','line_number':10435,'multiline':False]['text':' prefix with a 1,1, to ensure we are compatible with numpy which cuts off prefix 1s','line_number':10460,'multiline':False]['text':' (some of these ops already prefix a 1 to the size)','line_number':10461,'multiline':False]['text':' check prefix with  non-1s doesn't work','line_number':10494,'multiline':False]['text':' NumPy: ValueError','line_number':10496,'multiline':False]['text':' non-scalar indexed with scalars','line_number':10506,'multiline':False]['text':' indexing by a scalar should slice (not copy)','line_number':10513,'multiline':False]['text':' scalar indexed with scalar','line_number':10518,'multiline':False]['text':' non-scalar indexed with scalars','line_number':10529,'multiline':False]['text':' scalar indexed with scalars','line_number':10541,'multiline':False]['text':' From the NumPy indexing example','line_number':10551,'multiline':False]['text':' Check that it is a copy','line_number':10556,'multiline':False]['text':' But assignment should modify the original','line_number':10561,'multiline':False]['text':' index_put_','line_number':10631,'multiline':False]['text':' index','line_number':10635,'multiline':False]['text':' default','line_number':10718,'multiline':False]['text':' Test hidden/input batch size broadcasting','line_number':10755,'multiline':False]['text':' Test hx's hidden_size vs module's hidden_size broadcasting','line_number':10758,'multiline':False]['text':' Test input's input_size vs module's input_size broadcasting','line_number':10762,'multiline':False]['text':' this is just a smoke test; these modules are implemented through','line_number':10767,'multiline':False]['text':' autograd so no Jacobian test is needed','line_number':10768,'multiline':False]['text':' TODO: Remove once test_testing.py is running on MPS devices','line_number':10796,'multiline':False]['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':10801,'multiline':False]['text':' fail, so just set CWD to this script's directory','line_number':10802,'multiline':False]['text':' This can be changed once we actually implement 'lcm'','line_number':10807,'multiline':False]['text':' Should return fn, args, kwargs, string_version','line_number':10808,'multiline':False]['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':10847,'multiline':False]['text':' fail, so just set CWD to this script's directory','line_number':10848,'multiline':False]['text':' TODO: The NaN test is failing when all the tests in test_mps are run','line_number':10871,'multiline':False]['text':' together but passes when run separately. There seems to be memory','line_number':10872,'multiline':False]['text':' corruption which needs to be fixed for this test to be enabled.','line_number':10873,'multiline':False]['text':' with self.assertRaisesRegex(AssertionError, "Tensor-likes are not close!"):','line_number':10874,'multiline':False]['text':' torch.testing.assert_close(a, nan)','line_number':10875,'multiline':False]['text':' Ensures that cpu Tensor can be loaded on mps','line_number':10892,'multiline':False]['text':' Ensures that mps Tensors can be loaded on mps','line_number':10903,'multiline':False]['text':' Ensures that mps Tensors can be loaded on cpu','line_number':10914,'multiline':False]['text':' Ensures that `mps:0` Tensors can be loaded on mps','line_number':10925,'multiline':False]['text':' TODO: This is only used while some ops are being added.','line_number':10945,'multiline':False]['text':' This list should contain all ops and dtypes eventually','line_number':10946,'multiline':False]['text':' This can be generated automatically in the `new_mps_allowlist.txt` file','line_number':10947,'multiline':False]['text':' by doing `EXPECTTEST_ACCEPT=1 python test_mps.py TestConsistencyCPU`','line_number':10948,'multiline':False]['text':' You most likely do NOT want to modify this manually','line_number':10949,'multiline':False]['text':' for macOS 12','line_number':10985,'multiline':False]['text':' conv2d and conv_transpose2d results have a very small','line_number':10995,'multiline':False]['text':' difference compared to CPU/CUDA, so we use lower precision on FP32','line_number':10996,'multiline':False]['text':' Used for accept mode only','line_number':11004,'multiline':False]['text':'','line_number':11017,'multiline':False]['text':' Forward check','line_number':11018,'multiline':False]['text':'','line_number':11019,'multiline':False]['text':' for tensor_split(), the second tensor arg ("tensor_indices_or_sections") must be on CPU only','line_number':11028,'multiline':False]['text':'','line_number':11079,'multiline':False]['text':' Forward check','line_number':11080,'multiline':False]['text':'','line_number':11081,'multiline':False]['text':' for tensor_split(), the second tensor arg ("tensor_indices_or_sections") must be on CPU only','line_number':11091,'multiline':False]['text':'','line_number':11131,'multiline':False]['text':' Backward check','line_number':11132,'multiline':False]['text':'','line_number':11133,'multiline':False]['text':' We would've failed immediately anyway, but this error is clearer','line_number':11135,'multiline':False]['text':' We error instead of continuing so that all_backward_pass would not be True','line_number':11136,'multiline':False]['text':' rand_like does not work with certain dtypes, so cast to double and cast back','line_number':11154,'multiline':False]['text':' Compare computed gradients with cpu given random grad_output vector','line_number':11158,'multiline':False]['text':' Sometimes when the derivative is 0, we just don't bother creating the graph','line_number':11159,'multiline':False]['text':' allow_unused is needed in those cases.','line_number':11160,'multiline':False]['text':' for tensor_split(), the second tensor arg ("tensor_indices_or_sections") must be on CPU only','line_number':11184,'multiline':False]['text':' Copied from `TestCommon` in `test_ops.py`, just enough to duplicate the `test_numpy_ref` for MPS','line_number':11192,'multiline':False]['text':' Verifies, on teardown, that no OpInfo is still using dynamic dtypes in CI','line_number':11197,'multiline':False]['text':' Assure no opinfo entry has dynamic_dtypes','line_number':11207,'multiline':False]['text':' This is the MPS equivalent of `test_numpy_ref` from `test_ops.py`. It lives over here while','line_number':11215,'multiline':False]['text':' MPS still requires some fairly heavy special casing in the test framework.','line_number':11216,'multiline':False]['text':' When MPS becomes more consistent, this can probably be merged with that test using','line_number':11217,'multiline':False]['text':' `@dtypesIfMPS(torch.float32)`, but for now, the assertions themselves need to be loosened','line_number':11218,'multiline':False]['text':' MPS only supports float32','line_number':11220,'multiline':False]['text':' Unlike `test_numpy_ref`, this test compares in `float32` since at the time of this test's creation MPS','line_number':11223,'multiline':False]['text':' does not support float64 Tensors.','line_number':11224,'multiline':False]['text':' A few ops are currently broken on their reference inputs, but not their sample inputs. These should','line_number':11225,'multiline':False]['text':' get patched up and this workaround removed.','line_number':11226,'multiline':False]['text':' TODO: Actually instantiate that test for the "mps" device to better reflect what it is doing.','line_number':11244,'multiline':False]['text':' This requires mps to be properly registered in the device generic test framework which is not the','line_number':11245,'multiline':False]['text':' case right now. We can probably use `allow_mps` introduced in https://github.com/pytorch/pytorch/pull/87342','line_number':11246,'multiline':False]['text':' to achieve this.','line_number':11247,'multiline':False]