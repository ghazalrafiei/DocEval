['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]['text':' Empty output_sizes is not supported','line_number':39,'multiline':False]['text':' is clone decomposed?','line_number':40,'multiline':False]['text':' General ASAN Failure due to related to generating bool values.','line_number':42,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/74519','line_number':43,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/63034','line_number':44,'multiline':False]['text':' ASAN failure (paste: P501906539)','line_number':45,'multiline':False]['text':' ASAN failure','line_number':46,'multiline':False]['text':' ASAN failure','line_number':47,'multiline':False]['text':' ASAN failure','line_number':48,'multiline':False]['text':' Value out of range','line_number':51,'multiline':False]['text':' Value out of range','line_number':52,'multiline':False]['text':' Value out of range','line_number':53,'multiline':False]['text':' incorrect results','line_number':54,'multiline':False]['text':' incorrect results','line_number':55,'multiline':False]['text':' incorrect results (on CI not locally?)','line_number':56,'multiline':False]['text':' The following ops all show up directly in ts_native_functions.yaml,','line_number':58,'multiline':False]['text':' but run functionalized versions of the composite kernels in core.','line_number':59,'multiline':False]['text':' This means that we don't expect the ops to show directly in the LTC metrics.','line_number':60,'multiline':False]['text':' For some ops, we don't support all variants. Here we use formatted_name','line_number':74,'multiline':False]['text':' to uniquely identify the variant.','line_number':75,'multiline':False]['text':' no requires_grad','line_number':112,'multiline':False]['text':' run eager','line_number':119,'multiline':False]['text':' run lazy','line_number':123,'multiline':False]['text':' check numerics','line_number':128,'multiline':False]['text':' y and x should contiue to be aliased after the mark_step call.','line_number':147,'multiline':False]['text':' out will have some pending mutations, which will be synced by the .cpu() call.','line_number':154,'multiline':False]['text':' Calling a view op to ensure that functionalization wrapping occurs.','line_number':163,'multiline':False]['text':' check aliases','line_number':201,'multiline':False]['text':' noqa: B950','line_number':211,'multiline':False]['text':' Need to run mark step so that all random ops are computed in the right order','line_number':234,'multiline':False]['text':' noqa: B950','line_number':247,'multiline':False]['text':' Need to run mark step so that all random ops are computed in the right order','line_number':270,'multiline':False]['text':' TODO: after we move to master, add Lazy as a new Device here:','line_number':288,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py#L532','line_number':289,'multiline':False]['text':' Setup the dynamic shape mode','line_number':296,'multiline':False]['text':' Test that nonzero gives upper bounds sizes when symbolic shape mode is enabled','line_number':307,'multiline':False]['text':' FIXME: Add bindings to get upper bounds','line_number':313,'multiline':False]['text':' self.assertEqual(tuple(x2_lazy.size()), (6, 2))','line_number':314,'multiline':False]['text':' We should still be able to instantiate it and get the actual result','line_number':316,'multiline':False]['text':' Test that adaptive_avg_pool3d gives correct shapes with lazy backend','line_number':321,'multiline':False]