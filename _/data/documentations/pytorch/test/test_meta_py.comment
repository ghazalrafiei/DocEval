['text':' Owner(s): ["module: decompositions"]','line_number':1,'multiline':False]['text':' Cannot easily test m1 and m2 have same storage due to','line_number':78,'multiline':False]['text':' lack of Storage bindings.  Use version counter.','line_number':79,'multiline':False]['text':' Doing it this way ensures that we get VC bump even with leaves','line_number':82,'multiline':False]['text':' check the test is actually testing what it claims','line_number':100,'multiline':False]['text':' check the test is actually testing what it claims','line_number':117,'multiline':False]['text':' check the test is actually testing what it claims','line_number':146,'multiline':False]['text':' check the test is actually testing what it claims','line_number':158,'multiline':False]['text':' check the test is actually testing what it claims','line_number':169,'multiline':False]['text':' check the test is actually testing what it claims','line_number':179,'multiline':False]['text':' check the test is actually testing what it claims','line_number':189,'multiline':False]['text':' sanity','line_number':199,'multiline':False]['text':' check the test is actually testing what it claims','line_number':206,'multiline':False]['text':' Check that we can autograd with m as input without erroring;','line_number':212,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/87956','line_number':213,'multiline':False]['text':' check the test is actually testing what it claims','line_number':223,'multiline':False]['text':' check the test is actually testing what it claims','line_number':247,'multiline':False]['text':' NB: complex stuff is not actually exercised right now because','line_number':253,'multiline':False]['text':' we have a blanket exclusion for complex conversion','line_number':254,'multiline':False]['text':' channel_last and channel_last_3d related failures','line_number':353,'multiline':False]['text':' following ops fails if include_storage_offset = True, but these are a bit edge casey','line_number':356,'multiline':False]['text':' we should still fix them, leaving them here for tracking.','line_number':357,'multiline':False]['text':' aten._reshape_alias.default,  # repro with test_dispatch_symbolic_meta_outplace_all_strides_matmul_cuda_float32','line_number':358,'multiline':False]['text':' aten.view.default,  # repro with test_dispatch_symbolic_meta_outplace_all_strides_unflatten_cuda_float32','line_number':359,'multiline':False]['text':' The conj bit is not copied, see:','line_number':363,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/101836','line_number':364,'multiline':False]['text':' Prims are expected to model strides correctly','line_number':382,'multiline':False]['text':' Check if it's a view, by testing if any of the returns have','line_number':385,'multiline':False]['text':' a non-empty alias set','line_number':386,'multiline':False]['text':' TODO: check for TensorIterator','line_number':389,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/78050','line_number':405,'multiline':False]['text':' This environment variable controls whether or not we print expected failure','line_number':421,'multiline':False]['text':' lists at the end of a test suite run.  The intended usage looks like this:','line_number':422,'multiline':False]['text':'','line_number':423,'multiline':False]['text':' 1. Run `PYTORCH_COLLECT_EXPECT=1 python test/test_meta.py` on a CUDA build','line_number':424,'multiline':False]['text':'    of PyTorch that has LAPACK/MAGMA installed.  You can filter `-k test_meta`','line_number':425,'multiline':False]['text':'    or `-k test_dispatch_meta` to only focus on one or another list','line_number':426,'multiline':False]['text':' 2. Given the printed skip/xfail list, add them to the corresponding lists;','line_number':427,'multiline':False]['text':'    torch.* entries go in meta_function and aten.* entries go in meta_dispatch.','line_number':428,'multiline':False]['text':'    If there are preexisting entries, you need to merge in the entries.','line_number':429,'multiline':False]['text':'','line_number':430,'multiline':False]['text':' This is somewhat manual but typically you shouldn't need to do this, unless','line_number':431,'multiline':False]['text':' you've made a major change (e.g., added a new dtype to PyTorch) and need to','line_number':432,'multiline':False]['text':' refresh the lists.  If you want to do it from scratch, just clear out the','line_number':433,'multiline':False]['text':' preexisting lists before running.','line_number':434,'multiline':False]['text':'','line_number':435,'multiline':False]['text':' WARNING: Python dict literals will silently ignore duplicate keys','line_number':436,'multiline':False]['text':' Success forces pass; failure forces fail; skip unconditionally skips testing','line_number':477,'multiline':False]['text':' unlike print produce strides','line_number':480,'multiline':False]['text':' TODO: also handle cases where func raise an exception','line_number':523,'multiline':False]['text':' For now, only attempt if we managed to convert all tensor types','line_number':525,'multiline':False]['text':' (if any of them failed, we're in a mixed device situation and','line_number':526,'multiline':False]['text':' this isn't well supported)','line_number':527,'multiline':False]['text':' Special cases','line_number':529,'multiline':False]['text':' Use original indices_or_sections, this argument is data dependent','line_number':531,'multiline':False]['text':' Ensure boolean tensors use original','line_number':534,'multiline':False]['text':' Don't convert boolean tensors to meta as they will have nonzero','line_number':548,'multiline':False]['text':' called on them','line_number':549,'multiline':False]['text':' torch.ops.aten._ctc_loss.IntList has a meta kernel but','line_number':558,'multiline':False]['text':' torch.ops.aten._ctc_loss.Tensor does not','line_number':559,'multiline':False]['text':' Suppress warnings, this doesn't matter for test_meta.py','line_number':566,'multiline':False]['text':' but it does matter if you want to use this decorator','line_number':567,'multiline':False]['text':' for cross-ref testing, as some tests may be looking at','line_number':568,'multiline':False]['text':' errors','line_number':569,'multiline':False]['text':' Run the decomps and meta kernels registered','line_number':573,'multiline':False]['text':' to the python dispatcher instead of the regular dispatcher.','line_number':574,'multiline':False]['text':' This should be the same set of kernels','line_number':575,'multiline':False]['text':' that fake tensor runs in dynamic shapes mode.','line_number':576,'multiline':False]['text':' aten::_local_scalar_dense','line_number':716,'multiline':False]['text':' aten::_local_scalar_dense','line_number':717,'multiline':False]['text':' aten::_unique2, aten::unique_dim','line_number':718,'multiline':False]['text':' aten::unique_consecutive','line_number':719,'multiline':False]['text':' aten::geqrf','line_number':720,'multiline':False]['text':' aten::histc, aten::histc.out','line_number':721,'multiline':False]['text':' aten::kthvalue.values','line_number':722,'multiline':False]['text':' This is a __torch_function__ mode that, when enabled, interposes every','line_number':740,'multiline':False]['text':' Torch API call and runs the operator as normal, and then reruns it','line_number':741,'multiline':False]['text':' with meta inputs, and then checks that everything about the output agrees.','line_number':742,'multiline':False]['text':' Most of the logic deals with faithfully replicating the original tensor','line_number':743,'multiline':False]['text':' as a meta tensor, which is nontrivial because there are a lot of subsystems','line_number':744,'multiline':False]['text':' that may potentially be exercised.','line_number':745,'multiline':False]['text':'','line_number':746,'multiline':False]['text':' That being said, this class is a little overkill for what it is doing in','line_number':747,'multiline':False]['text':' this test file (since I could have just inlined __torch_function__ on the','line_number':748,'multiline':False]['text':' OpInfo call, and OpInfos generally have very regular inputs), but it will be','line_number':749,'multiline':False]['text':' useful for more comprehensive testing e.g., as seen in','line_number':750,'multiline':False]['text':' https://github.com/pytorch/pytorch/pull/75994  The big benefit is it is','line_number':751,'multiline':False]['text':' A LOT more efficient that torch dispatch mode (at the cost of less coverage)','line_number':752,'multiline':False]['text':' meta converter doesn't work correctly when no_dispatch() is on, so','line_number':769,'multiline':False]['text':' skip running the crossref test in this case','line_number':770,'multiline':False]['text':' these always fail','line_number':796,'multiline':False]['text':' NotImplementedError: 'aten::_local_scalar_dense'','line_number':798,'multiline':False]['text':' Shape of second output depends on data.','line_number':808,'multiline':False]['text':' these sometimes pass and sometimes fail','line_number':828,'multiline':False]['text':' at::nonzero doesn't have a Meta function','line_number':830,'multiline':False]['text':' For CompositeImplicitAutograd functions that fail before hitting the Mode','line_number':836,'multiline':False]['text':' Errors out in one of the tests, while ProxyTensor passes...','line_number':839,'multiline':False]['text':' Errors out in one of the tests, while ProxyTensor passes...','line_number':845,'multiline':False]['text':' aten::_unique2','line_number':863,'multiline':False]['text':' aten::_use_cudnn_ctc_loss','line_number':864,'multiline':False]['text':' aten::_use_cudnn_ctc_loss.Tensor','line_number':865,'multiline':False]['text':' aten::cudnn_grid_sampler','line_number':866,'multiline':False]['text':' aten::geqrf','line_number':867,'multiline':False]['text':' aten::histc','line_number':868,'multiline':False]['text':' aten::histc.out','line_number':869,'multiline':False]['text':' aten::kthvalue.values','line_number':870,'multiline':False]['text':' aten::linalg_eigvalsh.out','line_number':871,'multiline':False]['text':' aten::log_sigmoid_forward.output','line_number':873,'multiline':False]['text':' aten::unique_consecutive','line_number':874,'multiline':False]['text':' aten::unique_dim','line_number':875,'multiline':False]['text':' aten::upsample_nearest3d.vec','line_number':876,'multiline':False]['text':' If the computation dtype is different from the input','line_number':885,'multiline':False]['text':' dtype this will fail. CPU execution may also have a','line_number':886,'multiline':False]['text':' a different output from other devices.','line_number':887,'multiline':False]['text':' file issue','line_number':892,'multiline':False]['text':' aten::linalg_eigvalsh.out','line_number':893,'multiline':False]['text':' ROCm stuff; technically this should be expected failure but it's','line_number':899,'multiline':False]['text':' not worth it; these should get unified anyway','line_number':900,'multiline':False]['text':' contiguous','line_number':909,'multiline':False]['text':' transposed','line_number':912,'multiline':False]['text':' nondense','line_number':920,'multiline':False]['text':' channel_last','line_number':925,'multiline':False]['text':' channel_last_3d','line_number':929,'multiline':False]['text':' storage_offset','line_number':933,'multiline':False]['text':' save TLS','line_number':960,'multiline':False]['text':' Now match based on args, kwargs and number of required outputs','line_number':988,'multiline':False]['text':' Positional arguments must have the same type','line_number':996,'multiline':False]['text':' Number of outputs must match','line_number':1003,'multiline':False]['text':' Now try and match kwargs. Just need to ensure that the','line_number':1008,'multiline':False]['text':' remaining kwargs allow an out overload to be called. For example','line_number':1009,'multiline':False]['text':' we can throw away parameters like `dtype` that may be passed to the','line_number':1010,'multiline':False]['text':' functional version of the op since the `dtype` will already be present','line_number':1011,'multiline':False]['text':' in the `out` argument','line_number':1012,'multiline':False]['text':' This is to test torch ops that do not have an out parameter but have','line_number':1066,'multiline':False]['text':' aten op overloads that have out parameters. Additionally, Python decompositions','line_number':1067,'multiline':False]['text':' may register OpOverloadPacket's so decompositions need to be tested','line_number':1068,'multiline':False]['text':' to ensure all OpOverloads still function for the Meta key (e.g. if a python decomposition','line_number':1069,'multiline':False]['text':' is registered for an aten op aten.foo with overloads [default, out], the python','line_number':1070,'multiline':False]['text':' function needs to support receiving `out` arguments)','line_number':1071,'multiline':False]['text':' check to see if there is a potential out overload','line_number':1079,'multiline':False]['text':' NB: we're running these tests only on CUDA because there are some','line_number':1106,'multiline':False]['text':' inconsistencies between CUDA and CPU, and running on CUDA makes it easier','line_number':1107,'multiline':False]['text':' to ignore the CPU case when inconsistencies arise.  Ideally we deal','line_number':1108,'multiline':False]['text':' with the inconsistencies but this takes time.','line_number':1109,'multiline':False]['text':' Copies inputs to inplace operations to avoid inplace modifications','line_number':1111,'multiline':False]['text':'   to leaves requiring gradient','line_number':1112,'multiline':False]['text':' run the OpInfo sample inputs, cross-referencing them with the','line_number':1135,'multiline':False]['text':' meta implementation and check the results are the same.  All','line_number':1136,'multiline':False]['text':' the heavy lifting happens in MetaCrossRefFunctionMode','line_number':1137,'multiline':False]['text':' Special test for functions taking "device" kwarg','line_number':1148,'multiline':False]['text':' The crossref tests that replacing the device with "meta" works','line_number':1149,'multiline':False]['text':' This part makes sure that *_like functions work well with a "meta"','line_number':1150,'multiline':False]['text':' Tensor and their original device argument.','line_number':1151,'multiline':False]['text':' *_like functions take a Tensor as first argument','line_number':1157,'multiline':False]['text':' empty_like is not deterministic','line_number':1164,'multiline':False]['text':' test inputs <= 5 tensors to avoid combinatorial explosion','line_number':1215,'multiline':False]['text':' only test one dtype, as output stride behavior is the same for all dtypes','line_number':1263,'multiline':False]['text':' Only test on CUDA, as CUDA kernel's stride is the reference','line_number':1265,'multiline':False]['text':' only test one dtype, as output stride behavior is the same for all dtypes','line_number':1273,'multiline':False]['text':' Only test on CUDA, as CUDA kernel's stride is the reference','line_number':1275,'multiline':False]['text':' only test one dtype, as output stride behavior is the same for all dtypes','line_number':1283,'multiline':False]['text':' Only test on CUDA, as CUDA kernel's stride is the reference','line_number':1285,'multiline':False]['text':' The point of the test is that this should not error:','line_number':1339,'multiline':False]['text':' We have a fallthrough kernel registered to the AutogradMeta','line_number':1340,'multiline':False]['text':' key for custom ops, so it's fine that `foo()` doesn't have','line_number':1341,'multiline':False]['text':' an autograd kernel.','line_number':1342,'multiline':False]['text':' test functional call','line_number':1359,'multiline':False]['text':' test call with out parameters','line_number':1374,'multiline':False]['text':' handle optional weight and bias','line_number':1396,'multiline':False]['text':' input, (args) num_groups, (kwargs) weight, bias eps','line_number':1422,'multiline':False]['text':' test functional call','line_number':1444,'multiline':False]['text':' input, (args) num_groups, (kwargs) weight, bias eps','line_number':1453,'multiline':False]['text':' aten.fill_ returns an aliase','line_number':1484,'multiline':False]['text':' aten.fill returns a new tensor','line_number':1487,'multiline':False]['text':' sum','line_number':1568,'multiline':False]['text':' opinfo test is using aten.fill_, it's not testing aten.fill','line_number':1576,'multiline':False]