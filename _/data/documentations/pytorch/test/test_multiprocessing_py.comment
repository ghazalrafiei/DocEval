['text':' Owner(s): ["module: multiprocessing"]','line_number':1,'multiline':False]['text':' load_tests from common_utils is used to automatically filter tests for','line_number':22,'multiline':False]['text':' sharding on sandcastle. This line silences flake warnings','line_number':23,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/90940','line_number':32,'multiline':False]['text':' hide expected error message','line_number':115,'multiline':False]['text':' Multiply by two in a separate stream','line_number':124,'multiline':False]['text':' Check that the 10th available file-descriptor at the end of the','line_number':205,'multiline':False]['text':' test is no more than 4 higher than the 10th available at the','line_number':206,'multiline':False]['text':' start. This attempts to catch file descriptor leaks, but allows','line_number':207,'multiline':False]['text':' one-off initialization that may use up a file descriptor','line_number':208,'multiline':False]['text':' TODO: Disabled because this check is too flaky','line_number':209,'multiline':False]['text':' available_fds = self._get_next_fds(10)','line_number':210,'multiline':False]['text':' self.test_case.assertLessEqual(','line_number':211,'multiline':False]['text':'     available_fds[-1] - self.next_fds[-1], 5)','line_number':212,'multiline':False]['text':' dup uses the lowest-numbered unused descriptor for the new descriptor','line_number':220,'multiline':False]['text':' This will keep tests isolated from each-other','line_number':258,'multiline':False]['text':' Once the child process is done, it will set the event to notify the','line_number':279,'multiline':False]['text':' parent accordingly','line_number':280,'multiline':False]['text':' We need to delete this tensors to allow producer (child process)','line_number':311,'multiline':False]['text':' collect them properly','line_number':312,'multiline':False]['text':' Mark the event as done and join the process','line_number':315,'multiline':False]['text':' The test works but is very slow on MacOS, see https://github.com/pytorch/pytorch/pull/93183,','line_number':383,'multiline':False]['text':' so run it only once there. The delay is in waiting for the child process to terminate (join)','line_number':384,'multiline':False]['text':' queue serializes asynchronously','line_number':409,'multiline':False]['text':' Autograd only uses thread when GPUs are involved','line_number':431,'multiline':False]['text':' initialize CUDA outside of leak checker','line_number':451,'multiline':False]['text':' Check multiple small tensors which will likely use the same','line_number':524,'multiline':False]['text':' underlying cached allocation','line_number':525,'multiline':False]['text':' You might think this should be the case, but it's not!  After','line_number':549,'multiline':False]['text':' data from the CUDA caching allocator goes through IPC, the','line_number':550,'multiline':False]['text':' size of the storage is the size of the *cached cudaMalloc for','line_number':551,'multiline':False]['text':' the entire memory block* of the storage, not just the storage.','line_number':552,'multiline':False]['text':' See Note [CUDA IPC and the caching allocator] for more info','line_number':553,'multiline':False]['text':'','line_number':554,'multiline':False]['text':' self.assertEqual(storage_size, 5)','line_number':555,'multiline':False]['text':' Collect current process (producer) files, make sure nothing holds','line_number':557,'multiline':False]['text':' ref to the sent tensors','line_number':558,'multiline':False]['text':' We need to collect, as CUDA MP implementation holds one shared','line_number':562,'multiline':False]['text':' memory 'file' for performance reason','line_number':563,'multiline':False]['text':' Initialize CUDA','line_number':569,'multiline':False]['text':' Use a sleep kernel to test events. Without the event, the','line_number':615,'multiline':False]['text':' multiply happens before the add.','line_number':616,'multiline':False]['text':' about 30 ms','line_number':618,'multiline':False]['text':' must wait until subprocess records event','line_number':622,'multiline':False]['text':' notify parent child is ready','line_number':629,'multiline':False]['text':' wait for record in parent','line_number':630,'multiline':False]['text':' notify parent synchronization is done','line_number':632,'multiline':False]['text':' wait for until child process is ready','line_number':649,'multiline':False]['text':' spin for about 50 ms','line_number':650,'multiline':False]['text':' notify child event is recorded','line_number':652,'multiline':False]['text':' wait for synchronization in child','line_number':655,'multiline':False]['text':' create handle on different device from un-recorded event','line_number':670,'multiline':False]['text':' spin for about 50 ms','line_number':676,'multiline':False]['text':' create handle on different device from recorded event','line_number':680,'multiline':False]['text':' notify parent child is ready','line_number':686,'multiline':False]['text':' wait for record in parent','line_number':687,'multiline':False]['text':' notify synchronization is done in child','line_number':689,'multiline':False]['text':' wait for parent to finish before destructing child event','line_number':690,'multiline':False]['text':' wait for child to become ready','line_number':707,'multiline':False]['text':' spin for about 50 ms','line_number':708,'multiline':False]['text':' notify child event is recorded','line_number':710,'multiline':False]['text':' wait for synchronization in child','line_number':713,'multiline':False]['text':' notify child that parent is done','line_number':715,'multiline':False]['text':' spin for about 50 ms','line_number':724,'multiline':False]['text':' wait for parent process finished synchronization before','line_number':727,'multiline':False]['text':' destructing e1','line_number':728,'multiline':False]['text':' wait for event in child process is recorded','line_number':744,'multiline':False]['text':' This would cause an error if we tried to serialize the hooks,','line_number':779,'multiline':False]['text':' because it's a closure and pickle doesn't support closures.','line_number':780,'multiline':False]['text':' Check sharing a cudaMalloc allocation with different types of storage.','line_number':803,'multiline':False]['text':' (Issue #11422)','line_number':804,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/14997','line_number':833,'multiline':False]['text':' Don't use a regular Queue; it uses a background thread (which','line_number':860,'multiline':False]['text':' means we can't catch the exceptions)','line_number':861,'multiline':False]