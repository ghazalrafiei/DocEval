['text':' Owner(s): ["oncall: profiler"]','line_number':1,'multiline':False]['text':' This is not an exhaustive check, but for the purpose of unit testing','line_number':107,'multiline':False]['text':' it is sufficient.','line_number':108,'multiline':False]['text':' Vacuous case.','line_number':110,'multiline':False]['text':' Don't need to check parameter; we're done.','line_number':124,'multiline':False]['text':' For a complex workflow a gradient could correspond to','line_number':127,'multiline':False]['text':' different parameters at different points in a trace.','line_number':128,'multiline':False]['text':' However this will not happen in the relatively simple','line_number':129,'multiline':False]['text':' cases tested here, so if `extract_gradients` identifies','line_number':130,'multiline':False]['text':' the parameter corresponding to a particular gradient it','line_number':131,'multiline':False]['text':' must be the one we expect.','line_number':132,'multiline':False]['text':' Gradient detection through op inspection does not provide a','line_number':169,'multiline':False]['text':' reference to the parameter corresponding to the gradient.','line_number':170,'multiline':False]['text':' The first time we run a module none of the `.grad` fields','line_number':194,'multiline':False]['text':' have been initialized. This is fine; in that case we can','line_number':195,'multiline':False]['text':' detect everything we need in the profiled section.','line_number':196,'multiline':False]['text':' Op based detection should still identify the gradients.','line_number':203,'multiline':False]['text':' We can detect gradients even when `.backward()` is not called.','line_number':207,'multiline':False]['text':' Optimizer instrumentation runs late in the step, so we can detect','line_number':237,'multiline':False]['text':' gradients for both cold and warm start.','line_number':238,'multiline':False]['text':' Inspected state is cached, so if we replace gradients (as is the','line_number':253,'multiline':False]['text':' case for `set_to_none=True`) our python instrumentation will not','line_number':254,'multiline':False]['text':' see them.','line_number':255,'multiline':False]['text':' TODO(robieta): Should `.step()` be excluded from caching?','line_number':256,'multiline':False]['text':' Module and optimizer are thoroughly tested individually and should be','line_number':281,'multiline':False]['text':' additive. Thus we can manage with a lightweight check that they don't','line_number':282,'multiline':False]['text':' interact adversely.','line_number':283,'multiline':False]['text':'','line_number':370,'multiline':False]['text':' fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)','line_number':371,'multiline':False]['text':'','line_number':377,'multiline':False]['text':' copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)','line_number':378,'multiline':False]['text':'','line_number':380,'multiline':False]['text':' add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)','line_number':381,'multiline':False]['text':'','line_number':386,'multiline':False]['text':' copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)','line_number':387,'multiline':False]['text':'','line_number':391,'multiline':False]['text':' sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)','line_number':392,'multiline':False]['text':'','line_number':410,'multiline':False]['text':' fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)','line_number':411,'multiline':False]['text':' torch._C._jit_get_schemas_for_operator will reject any name that','line_number':439,'multiline':False]['text':' is missing a namespace. (denoted by the presence of "::") We want','line_number':440,'multiline':False]['text':' to check that we skip both annotations which have no schema','line_number':441,'multiline':False]['text':' (return empty tuple from SchemaMatcher.lookup_schemas) and','line_number':442,'multiline':False]['text':' annotations which cannot have schema (return None from','line_number':443,'multiline':False]['text':' SchemaMatcher.lookup_schemas).','line_number':444,'multiline':False]['text':' The python arg parser will convert the python scalar `2` to a Tensor','line_number':470,'multiline':False]['text':' to pass to `aten::mul`. As a result there is no op that "owns" the','line_number':471,'multiline':False]['text':' allocation. The Tensor deletions also do not happen in an op; they','line_number':472,'multiline':False]['text':' are collected as a result of the Python objects going out of scope.','line_number':473,'multiline':False]['text':' Out of place is identical regardless of Autograd.','line_number':506,'multiline':False]['text':' When Autograd is enabled a second Tensor `T2` is created to store','line_number':528,'multiline':False]['text':' the values of T0(v0) which are needed for backwards.','line_number':529,'multiline':False]['text':' T1 is the `2` in `.mul(2)`. The Python arg parser automatically','line_number':581,'multiline':False]['text':' converts Scalar arguments to Tensors. The same is true for `T4`','line_number':582,'multiline':False]['text':' and `.add_(2)`.','line_number':583,'multiline':False]['text':' `aten::mul` creates a temporary Tensor (T2), which is why the output','line_number':605,'multiline':False]['text':' is has ID three rather than two.','line_number':606,'multiline':False]['text':' Second time grads are already initialized.','line_number':687,'multiline':False]['text':' TODO: one with `.logsumexp(dim=0)`','line_number':735,'multiline':False]['text':' Second time grads are already initialized.','line_number':786,'multiline':False]['text':'','line_number':842,'multiline':False]['text':' If a Tensor is live we want the most recent ID','line_number':843,'multiline':False]['text':' Rely on internal asserts','line_number':867,'multiline':False]['text':' Use `__torch_dispatch__` to collect ground truth.','line_number':873,'multiline':False]['text':' Build map from observed live Tensors to the memory profiler's','line_number':881,'multiline':False]['text':' TensorKey representation.','line_number':882,'multiline':False]['text':' Deduplicate version bumps which don't change the category.','line_number':903,'multiline':False]['text':' PyTorch ops','line_number':914,'multiline':False]['text':' Marked regions.','line_number':920,'multiline':False]['text':' If we profile the first step then gradients will not have been','line_number':940,'multiline':False]['text':' created when we call `model.forward`, so if we don't call `.backward`','line_number':941,'multiline':False]['text':' then gradients are never created.','line_number':942,'multiline':False]['text':' On the first step we must rely on `AccumulateGrad`, since gradients','line_number':945,'multiline':False]['text':' did not exist when `model.forward` was called.','line_number':946,'multiline':False]['text':' After one step the python tracer will also flag gradients.','line_number':950,'multiline':False]['text':' The parameter gradients are not used but we still detect them with','line_number':954,'multiline':False]['text':' the python tracer.','line_number':955,'multiline':False]['text':' zero grads at the start so gradients are still live to be','line_number':964,'multiline':False]['text':' checked.','line_number':965,'multiline':False]['text':' Inputs which were allocated before profiling began','line_number':985,'multiline':False]['text':' Inputs which were allocated after profiling began','line_number':989,'multiline':False]['text':' Inputs which were allocated before profiling began','line_number':1012,'multiline':False]['text':' Inputs which were allocated after profiling began','line_number':1016,'multiline':False]['text':' For now we can't make any meaningful statements without a backward','line_number':1022,'multiline':False]['text':' pass. Here we simply ensure that passes don't generate false positive','line_number':1023,'multiline':False]['text':' category classifications.','line_number':1024,'multiline':False]['text':' Inputs which were allocated before profiling began','line_number':1046,'multiline':False]['text':' Inputs which were allocated after profiling began','line_number':1050,'multiline':False]['text':' NOTE: We expect that all unknown categories. This is simply a sanity','line_number':1114,'multiline':False]['text':'       check to ensure that we do not over-label.','line_number':1115,'multiline':False]['text':' We generate sequential IDs for Tensors; however platforms vary','line_number':1465,'multiline':False]['text':' slightly in the exact computation executed. If this results in','line_number':1466,'multiline':False]['text':' tensor creation the IDs will be shifted and the unit test will fail.','line_number':1467,'multiline':False]['text':' (Even though the behavior we're testing is unchanged.) To correct for','line_number':1468,'multiline':False]['text':' this we assign sequential numbers to the tensors which are actually','line_number':1469,'multiline':False]['text':' tested, effectively suppressing the extraneous implementation details.','line_number':1470,'multiline':False]['text':' We generally don't care about tiny allocations during memory','line_number':1481,'multiline':False]['text':' profiling and they add a lot of noise to the unit test.','line_number':1482,'multiline':False]['text':' On CPU the default behavior is to simply forward to malloc. That','line_number':1553,'multiline':False]['text':' means that when we free `x` the allocator doesn't actually know how','line_number':1554,'multiline':False]['text':' many bytes are in the allocation, and thus there's no point to','line_number':1555,'multiline':False]['text':' calling `c10::reportMemoryUsageToProfiler`. So in order to test that','line_number':1556,'multiline':False]['text':' memory profiler processes this case correctly we need to use CUDA','line_number':1557,'multiline':False]['text':' where we do always keep a record.','line_number':1558,'multiline':False]['text':' We never see `x` used so we don't know the storage is for a','line_number':1562,'multiline':False]['text':' Tensor, but we do still see the free event.','line_number':1563,'multiline':False]['text':' For empty we see the allocation and free, but not any use.','line_number':1566,'multiline':False]['text':' So this also cannot be identified as a Tensor.','line_number':1567,'multiline':False]['text':' Show `z` to the profiler','line_number':1572,'multiline':False]['text':' x','line_number':1578,'multiline':False]['text':'','line_number':1581,'multiline':False]['text':' y','line_number':1582,'multiline':False]['text':'','line_number':1585,'multiline':False]['text':' z','line_number':1586,'multiline':False]['text':' See above.','line_number':1593,'multiline':False]