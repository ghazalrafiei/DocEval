['text':' Owner(s): ["module: dataloader"]','line_number':1,'multiline':False]['text':' XXX: By default, dill writes the Pickler dispatch table to inject its','line_number':58,'multiline':False]['text':' own logic there. This globally affects the behavior of the standard library','line_number':59,'multiline':False]['text':' pickler for any user who transitively depends on this module!','line_number':60,'multiline':False]['text':' Undo this extension to avoid altering the behavior of the pickler globally.','line_number':61,'multiline':False]['text':' load_tests from torch.testing._internal.common_utils is used to automatically filter tests for','line_number':76,'multiline':False]['text':' sharding on sandcastle. This line silences flake warnings','line_number':77,'multiline':False]['text':' We want to use `spawn` if able because some of our tests check that the','line_number':84,'multiline':False]['text':' data loader terminiates gracefully. To prevent hanging in the testing','line_number':85,'multiline':False]['text':' process, such data loaders are run in a separate subprocess.','line_number':86,'multiline':False]['text':'','line_number':87,'multiline':False]['text':' We also want to test the `pin_memory=True` configuration, thus `spawn` is','line_number':88,'multiline':False]['text':' required to launch such processes and they initialize the CUDA context.','line_number':89,'multiline':False]['text':'','line_number':90,'multiline':False]['text':' Mixing different start method is a recipe for disaster (e.g., using a fork','line_number':91,'multiline':False]['text':' `mp.Event` with a spawn `mp.Process` segfaults). So we set this globally','line_number':92,'multiline':False]['text':' to avoid bugs.','line_number':93,'multiline':False]['text':'','line_number':94,'multiline':False]['text':' Get a multiprocessing context because some test / third party library will','line_number':95,'multiline':False]['text':' set start_method when imported, and setting again triggers `RuntimeError`.','line_number':96,'multiline':False]['text':' 60s of timeout?','line_number':100,'multiline':False]['text':' Yes, in environments where physical CPU resources are shared, e.g., CI, the','line_number':101,'multiline':False]['text':' time for a inter-process communication can be highly varying.  With 15~17s of','line_number':102,'multiline':False]['text':' timeout, we have observed flakiness in some CI builds (see','line_number':103,'multiline':False]['text':' pytorch/pytorch#14501, pytorch/pytorch#16608).  We follow the CPython','line_number':104,'multiline':False]['text':' multiprocessing setup and set the timeout to 60s here:','line_number':105,'multiline':False]['text':'','line_number':106,'multiline':False]['text':' https://github.com/python/cpython/blob/e8113f51a8bdf33188ee30a1c038a298329e7bfa/Lib/test/_test_multiprocessing.py#L73','line_number':107,'multiline':False]['text':' seconds','line_number':108,'multiline':False]['text':' collate_fn that returns the batch cloned; defined globally here for pickle purposes.','line_number':114,'multiline':False]['text':' Odd size splits','line_number':139,'multiline':False]['text':' Odd sized round-robin splits','line_number':145,'multiline':False]['text':' fractional splitting','line_number':204,'multiline':False]['text':' should raise since the sum of fractions is not 1','line_number':231,'multiline':False]['text':' should raise since fraction > 1','line_number':235,'multiline':False]['text':' A random_split without a specific generator should affect the default one','line_number':239,'multiline':False]['text':' A random_split with a specific generator should not affect the default one','line_number':247,'multiline':False]['text':' Testing slicing a subset initialized with a dataset','line_number':256,'multiline':False]['text':' Testing slicing of subset from random split','line_number':262,'multiline':False]['text':' Testing slicing a subset initialized with a subset','line_number':269,'multiline':False]['text':' Testing slicing of subset of subset from random split','line_number':276,'multiline':False]['text':' return less','line_number':479,'multiline':False]['text':' Adding an empty dataset somewhere is correctly handled','line_number':514,'multiline':False]['text':' this one goes to 11','line_number':526,'multiline':False]['text':' takes in dummy var so this can also be used as a `worker_init_fn`','line_number':554,'multiline':False]['text':' windows does not have faulthandler.register','line_number':558,'multiline':False]['text':' chain=False prevents the default behavior of killing the process','line_number':559,'multiline':False]['text':' Process `pid` must have called `set_faulthander_if_available`','line_number':565,'multiline':False]['text':' use the custom signal if available','line_number':568,'multiline':False]['text':' otherwise we can still use the handler given by faulthandler.enable()','line_number':571,'multiline':False]['text':' at the cost of killing the process.','line_number':572,'multiline':False]['text':' wait in parent process to give subprocess some time to print','line_number':575,'multiline':False]['text':' The following `ErrorTrackingProcess` stores the first encountered exception in','line_number':579,'multiline':False]['text':' its `.exception` attribute.','line_number':580,'multiline':False]['text':' Inspired by https://stackoverflow.com/a/33599967','line_number':581,'multiline':False]['text':' Why no *args?','line_number':584,'multiline':False]['text':'   py2 doesn't support def fn(x, *args, key=val, **kwargs)','line_number':585,'multiline':False]['text':' Setting disable_stderr=True may generate a lot of unrelated error outputs','line_number':586,'multiline':False]['text':' but could be helpful for debugging.','line_number':587,'multiline':False]['text':' Disable polluting stderr with errors that are supposed to happen.','line_number':597,'multiline':False]['text':' On platforms without `SIGUSR1`, `set_faulthander_if_available` sets','line_number':610,'multiline':False]['text':' `faulthandler.enable()`, and `print_traces_of_all_threads` may kill','line_number':611,'multiline':False]['text':' the process. So let's poll the exception first','line_number':612,'multiline':False]['text':' ESRCH means that os.kill can't finds alive proc','line_number':625,'multiline':False]['text':' Inspired by https://stackoverflow.com/a/26703365','line_number':697,'multiline':False]['text':' If all workers will call `sync_once`, they will be blocked until all workers','line_number':698,'multiline':False]['text':' reach the call (i.e., acting like a barrier).','line_number':699,'multiline':False]['text':' This can be used to ensure that each worker at least processes one data.','line_number':700,'multiline':False]['text':' See','line_number':757,'multiline':False]['text':'   test_large_sampler_indices','line_number':758,'multiline':False]['text':'   https://github.com/pytorch/pytorch/issues/48666','line_number':759,'multiline':False]['text':' flush library buffers that dup2 knows nothing about','line_number':782,'multiline':False]['text':' Can't use a with-block because otherwise the fd will be closed when this','line_number':783,'multiline':False]['text':' function ends.','line_number':784,'multiline':False]['text':' only error in the last worker','line_number':820,'multiline':False]['text':' only error in the last worker','line_number':841,'multiline':False]['text':' See TestDataLoader.test_proper_exit for usage','line_number':849,'multiline':False]['text':' 2 is the magical per-worker prefetch number...','line_number':876,'multiline':False]['text':' FIXME: change this after the number becomes configurable.','line_number':877,'multiline':False]['text':' ensure that the workers are still alive','line_number':905,'multiline':False]['text':' kill last worker','line_number':918,'multiline':False]['text':' Tries to trigger the __del__ clean-up rather than the automatic','line_number':921,'multiline':False]['text':' exiting of daemonic children. Technically it should be automatically','line_number':922,'multiline':False]['text':' triggered, but I don't want to rely on the implementation detail of','line_number':923,'multiline':False]['text':' Python gc.','line_number':924,'multiline':False]['text':' Should be used as worker_init_fn with TestWorkerInfoDataset.','line_number':934,'multiline':False]['text':' See _test_get_worker_info below for usage.','line_number':935,'multiline':False]['text':' test that WorkerInfo attributes are read-only','line_number':944,'multiline':False]['text':' get_worker_info returns None in main proc','line_number':959,'multiline':False]['text':' each `d` is a [worker_id, worker_pid] pair, which is set in','line_number':974,'multiline':False]['text':' _test_worker_info_init_fn','line_number':975,'multiline':False]['text':' get_worker_info returns None in main proc after data loading','line_number':977,'multiline':False]['text':' main proc dataset was never assigned this attribute','line_number':979,'multiline':False]['text':' test custom init function','line_number':988,'multiline':False]['text':' used with test_error_in_init','line_number':993,'multiline':False]['text':' used with test_error_in_init','line_number':999,'multiline':False]['text':' Make sure there is no TypeError','line_number':1153,'multiline':False]['text':' See NOTE [ DataLoader on Linux and open files limit ]','line_number':1164,'multiline':False]['text':' Tests if the child process forked by the DataLoader segfaults due to having more than 3 threads','line_number':1271,'multiline':False]['text':' in the parent process after at least one set_num_threads invocation in the parent process.','line_number':1272,'multiline':False]['text':' After forking, set_num_threads(1) in the child process entails handling some inherited data-structures','line_number':1273,'multiline':False]['text':' of the Caffe2 thread-pool of the parent process, culminating in a segfault.','line_number':1274,'multiline':False]['text':' Reference: https://github.com/pytorch/pytorch/issues/54752','line_number':1275,'multiline':False]['text':' This test runs in a subprocess, which can only initialize CUDA with spawn.','line_number':1292,'multiline':False]['text':' _test_timeout_pin_memory with pin_memory=True initializes CUDA when the iterator is','line_number':1293,'multiline':False]['text':' constructed.','line_number':1294,'multiline':False]['text':' Test that the data loader cleanly exit when the process errors','line_number':1311,'multiline':False]['text':'   1. having an reference to the iterator','line_number':1312,'multiline':False]['text':'   2. using a sampler that yields big elements s.t. _index_queues putters block','line_number':1313,'multiline':False]['text':'','line_number':1314,'multiline':False]['text':' More context: https://github.com/pytorch/pytorch/issues/48666','line_number':1315,'multiline':False]['text':' general','line_number':1329,'multiline':False]['text':' disable auto-batching','line_number':1335,'multiline':False]['text':' map-style','line_number':1348,'multiline':False]['text':' iterable-style','line_number':1366,'multiline':False]['text':' map-style dataset','line_number':1385,'multiline':False]['text':' no auto-batching','line_number':1387,'multiline':False]['text':' auto-batching','line_number':1390,'multiline':False]['text':' iterable-style dataset','line_number':1394,'multiline':False]['text':' no auto-batching','line_number':1396,'multiline':False]['text':' auto-batching','line_number':1399,'multiline':False]['text':' this IterableDataset isn't configured for each worker, so for','line_number':1400,'multiline':False]['text':' the equality test below to be valid, we cannot have more than 1 workers.','line_number':1401,'multiline':False]['text':' [no auto-batching] single process loading','line_number':1407,'multiline':False]['text':' non-batched should not convert ints into tensors','line_number':1413,'multiline':False]['text':' DataLoader should match len of the iterable-style dataset (if implemented)','line_number':1416,'multiline':False]['text':' [no auto-batching] multiprocessing loading','line_number':1419,'multiline':False]['text':' non-batched should not convert ints into tensors','line_number':1432,'multiline':False]['text':' DataLoader should match len of the iterable-style dataset (if implemented)','line_number':1435,'multiline':False]['text':' When loading more than len(dataset) data, after accessing len(dataloader),','line_number':1437,'multiline':False]['text':' we should get a warning. See NOTE [ IterableDataset and __len__ ].','line_number':1438,'multiline':False]['text':' [no auto-batching] test that workers exit gracefully','line_number':1457,'multiline':False]['text':' [auto-batching] single process loading','line_number':1470,'multiline':False]['text':' [auto-batching] multiprocessing loading','line_number':1478,'multiline':False]['text':' worker 0 should return 0 batches','line_number':1485,'multiline':False]['text':' worker 1 should return 1 batches','line_number':1486,'multiline':False]['text':' worker 2 should return 3 batches','line_number':1487,'multiline':False]['text':' [auto-batching] test that workers exit gracefully','line_number':1495,'multiline':False]['text':' [auto-batching & drop_last] single process loading','line_number':1507,'multiline':False]['text':' [auto-batching & drop_last] multiprocessing loading','line_number':1514,'multiline':False]['text':' worker 0 should return 0 batches','line_number':1521,'multiline':False]['text':' worker 1 should return 1 batches','line_number':1522,'multiline':False]['text':' worker 2 should return 3 batches','line_number':1523,'multiline':False]['text':' [auto-batching & drop_last] test that workers exit gracefully','line_number':1533,'multiline':False]['text':' chaining (concatenation)','line_number':1547,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/90940','line_number':1567,'multiline':False]['text':' windows and jetson devices don't support sharing cuda tensor; ROCm does not yet fully support IPC','line_number':1578,'multiline':False]['text':' test ctx object','line_number':1586,'multiline':False]['text':' Testing to make sure that function from global scope (e.g. imported from library) can be serialized','line_number':1594,'multiline':False]['text':' and used with multiprocess DataLoader','line_number':1595,'multiline':False]['text':' test ctx object','line_number':1609,'multiline':False]['text':' test sample with replacement','line_number':1710,'multiline':False]['text':' ensure at least one sample is drawn more than once','line_number':1711,'multiline':False]['text':' test sample without replacement and without specified num_samples','line_number':1719,'multiline':False]['text':' test sample without replacement and with specified num_samples','line_number':1727,'multiline':False]['text':' raise error when replacement is non-boolean','line_number':1752,'multiline':False]['text':' add 5 extra samples','line_number':1758,'multiline':False]['text':' test len method','line_number':1763,'multiline':False]['text':' test with iteration','line_number':1766,'multiline':False]['text':' test with dataloader, batch_size = 1','line_number':1770,'multiline':False]['text':' test with dataloader, batch_size = 6','line_number':1776,'multiline':False]['text':' add 5 extra samples','line_number':1785,'multiline':False]['text':' test len method','line_number':1790,'multiline':False]['text':' test with iteration','line_number':1793,'multiline':False]['text':' test with dataloader, batch_size = 1','line_number':1797,'multiline':False]['text':' test with dataloader, batch_size = 6','line_number':1803,'multiline':False]['text':' using a regular iterable','line_number':1872,'multiline':False]['text':' [(0, 1), (2, 3, 4), (5, 6), (7, 8, 9), ...]','line_number':1886,'multiline':False]['text':' using a regular iterable','line_number':1887,'multiline':False]['text':' Using NumPy generated states as the reference to test `_generate_state`','line_number':1936,'multiline':False]['text':' having the same result.','line_number':1937,'multiline':False]['text':' Test case: ((worker_id, base_seed), expected_state)','line_number':1938,'multiline':False]['text':' Takes 2.5min to finish, see https://github.com/pytorch/pytorch/issues/46065','line_number':1992,'multiline':False]['text':' TODO: test the case where the pin_memory_thread triggers an','line_number':2000,'multiline':False]['text':'       error/fatal signal. I haven't found out how to properly do that.','line_number':2001,'multiline':False]['text':' `hold_iter_reference` specifies whether we hold a reference to the','line_number':2006,'multiline':False]['text':' iterator. This is interesting because Python3 error traces holds a','line_number':2007,'multiline':False]['text':' reference to the frames, which hold references to all the local','line_number':2008,'multiline':False]['text':' variables including the iterator, and then the iterator dtor may','line_number':2009,'multiline':False]['text':' not be called before process end. It is important to see that the','line_number':2010,'multiline':False]['text':' processes still exit in both cases.','line_number':2011,'multiline':False]['text':' This test runs in a subprocess, which can only initialize CUDA with spawn.','line_number':2014,'multiline':False]['text':' DataLoader with pin_memory=True initializes CUDA when its iterator is constructed.','line_number':2015,'multiline':False]['text':' For windows, pin_memory sometimes causes CUDA oom.','line_number':2016,'multiline':False]['text':' `exit_method` controls the way the loader process ends.','line_number':2019,'multiline':False]['text':'   - `*_kill` means that `*` is killed by OS.','line_number':2020,'multiline':False]['text':'   - `*_error` means that `*` raises an error.','line_number':2021,'multiline':False]['text':'   - `None` means that no error happens.','line_number':2022,'multiline':False]['text':' In all cases, all processes should end properly.','line_number':2023,'multiline':False]['text':' TODO: Fix test for 'loader_kill' that would cause running out of shared memory.','line_number':2025,'multiline':False]['text':' Killing loader process would prevent DataLoader iterator clean up all queues','line_number':2026,'multiline':False]['text':' and worker processes','line_number':2027,'multiline':False]['text':' FIXME: This sometimes hangs. See #16608.','line_number':2036,'multiline':False]['text':' Event that the loader process uses to signal testing process','line_number':2047,'multiline':False]['text':' that various things are setup, including that the worker pids','line_number':2048,'multiline':False]['text':' are specified in `worker_pids` array.','line_number':2049,'multiline':False]['text':' Event that this process has finished setting up, and the','line_number':2052,'multiline':False]['text':' loader process can now proceed to trigger error events or','line_number':2053,'multiline':False]['text':' finish normally.','line_number':2054,'multiline':False]['text':' Wait for loader process to set everything up, e.g., starting','line_number':2066,'multiline':False]['text':' workers.','line_number':2067,'multiline':False]['text':' this may kill the process, needs to run after the above lines','line_number':2078,'multiline':False]['text':' We are certain that the workers have started now.','line_number':2082,'multiline':False]['text':' this may kill the process, needs to run after the above line','line_number':2097,'multiline':False]['text':' this may kill the process, needs to run after the above line','line_number':2107,'multiline':False]['text':' Sometimes, when the worker is being killed and is freeing its','line_number':2143,'multiline':False]['text':' resources, the unpickling in loader process will be met an','line_number':2144,'multiline':False]['text':' a `ConnectionRefusedError` as it can not open a socket to receive','line_number':2145,'multiline':False]['text':' resource. In such cases, the worker may not have fully exited,','line_number':2146,'multiline':False]['text':' and the loader can't know this via `is_alive` check or `SIGCHLD`','line_number':2147,'multiline':False]['text':' handler. So we permit this as an allowed error as well.','line_number':2148,'multiline':False]['text':' After all, we are happy as long as it terminates.','line_number':2149,'multiline':False]['text':' Should be a no-op','line_number':2257,'multiline':False]['text':' Should be a no-op','line_number':2289,'multiline':False]['text':' FIXME: fix the following hack that makes `default_collate` believe','line_number':2331,'multiline':False]['text':'        that it is in a worker process (since it tests','line_number':2332,'multiline':False]['text':'        `get_worker_info() != None`), even though it is not.','line_number':2333,'multiline':False]['text':' The 'fork' multiprocessing context doesn't work for CUDA so skip it','line_number':2352,'multiline':False]['text':' TODO: Skip this better in a better way when the test framework allows','line_number':2354,'multiline':False]['text':' Error case: default collate_fn doesn't currently support batches of nested tensors.','line_number':2376,'multiline':False]['text':' Following the current semantics, we'd need to stack them, which isn't possible atm.','line_number':2377,'multiline':False]['text':' Test Deterministic','line_number':2407,'multiline':False]['text':' No seed','line_number':2423,'multiline':False]['text':' Same seeds','line_number':2427,'multiline':False]['text':' Different seeds','line_number':2435,'multiline':False]['text':' The persistent workers always maintain the original','line_number':2554,'multiline':False]['text':' dataset through the dataloader lifetime','line_number':2555,'multiline':False]['text':' so the attributes will remain the same as the','line_number':2556,'multiline':False]['text':' first time the workers where spawned (dataloader iteration)','line_number':2557,'multiline':False]['text':' See NOTE [ DataLoader on Linux and open files limit ]','line_number':2577,'multiline':False]['text':' Changing the start value here doesn't have any effect in the dataset','line_number':2622,'multiline':False]['text':' cached by the workers. since they are not recreated between epochs','line_number':2623,'multiline':False]['text':' and can cache values safely','line_number':2624,'multiline':False]['text':' auto-collation','line_number':2688,'multiline':False]['text':' no auto-collation','line_number':2696,'multiline':False]['text':' Workaround for https://github.com/pytorch/pytorch/issues/50661','line_number':2718,'multiline':False]['text':' Classes from  `__main__` can not be correctly unpickled from spawned module','line_number':2719,'multiline':False]['text':' See https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming','line_number':2720,'multiline':False]['text':' Use half number of CPUs','line_number':2831,'multiline':False]['text':' Query the current affinity mask to avoid setting a disallowed one','line_number':2854,'multiline':False]['text':' Choose any','line_number':2858,'multiline':False]['text':' Call convolution on parent process','line_number':2875,'multiline':False]['text':' Tests crash reported in https://github.com/pytorch/pytorch/issues/53565','line_number':2890,'multiline':False]