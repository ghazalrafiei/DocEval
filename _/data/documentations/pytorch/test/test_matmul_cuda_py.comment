['text':' Owner(s): ["module: linear algebra"]','line_number':1,'multiline':False]['text':' Protects against includes accidentally setting the default dtype','line_number':41,'multiline':False]['text':'','line_number':56,'multiline':False]['text':' Check for catastrophic cuBLAS inaccuracy by measuring the deviation between','line_number':57,'multiline':False]['text':' results from the CUDA invocation of torch.addmm and the CPU invocation','line_number':58,'multiline':False]['text':' (which does not use CUDA backend).','line_number':59,'multiline':False]['text':'','line_number':60,'multiline':False]['text':' Get dims','line_number':61,'multiline':False]['text':' Disable reduced precision reductions in BFloat16 to bypass some kernels','line_number':63,'multiline':False]['text':' which fail the threshold check','line_number':64,'multiline':False]['text':' Make random tensors on CPU (seed set on common_utils.py import)','line_number':69,'multiline':False]['text':' (Not using numpy because it does not support bfloat16)','line_number':70,'multiline':False]['text':' *(B)FLOAT16 Special Handling*','line_number':76,'multiline':False]['text':' Backend does not tensorize float16 on CPU,','line_number':77,'multiline':False]['text':' and bloat16 may present accuracy issues,','line_number':78,'multiline':False]['text':' so convert to float32 for these cases','line_number':79,'multiline':False]['text':' (but keep same for other types, e.g. float32 and int*)','line_number':80,'multiline':False]['text':' Get CPU result','line_number':86,'multiline':False]['text':' *(B)FLOAT16 Special Handling*``','line_number':88,'multiline':False]['text':' Convert back to (b)float16','line_number':89,'multiline':False]['text':' Move arg tensors to CUDA','line_number':96,'multiline':False]['text':' Get CUDA result','line_number':101,'multiline':False]['text':' Move to CPU for comparison','line_number':103,'multiline':False]['text':' Compare','line_number':105,'multiline':False]['text':' imported 'tol' as 'xtol' to avoid aliasing in code above','line_number':112,'multiline':False]['text':' imported 'tol' as 'xtol' to avoid aliasing in code above','line_number':123,'multiline':False]['text':' perturb X, A, or B alignment','line_number':136,'multiline':False]['text':' linear','line_number':178,'multiline':False]['text':' test multiply the identity matrix','line_number':182,'multiline':False]['text':' baddbmm','line_number':188,'multiline':False]['text':' test multiply the identity matrix','line_number':197,'multiline':False]['text':' cross comparison','line_number':203,'multiline':False]['text':' According to https://docs.nvidia.com/cuda/cublas/#id99 8F_E5M2 MM is unsupported','line_number':231,'multiline':False]['text':' First, test plain multiplication.','line_number':368,'multiline':False]['text':' Second, test the linear operator itself.','line_number':383,'multiline':False]