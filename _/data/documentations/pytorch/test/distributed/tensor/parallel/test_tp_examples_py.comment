['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' Owner(s): ["oncall: distributed"]','line_number':2,'multiline':False]['text':' Ensure all tp ranks have same input.','line_number':48,'multiline':False]['text':' Ensure model are initialized the same way.','line_number':55,'multiline':False]['text':' Shard module and initialize optimizer.','line_number':58,'multiline':False]['text':' Sum gradients from different ranks, since input','line_number':92,'multiline':False]['text':' are different across ranks for sequence parallel.','line_number':93,'multiline':False]['text':' Ensure gradients are same.','line_number':99,'multiline':False]['text':' Ensure model weights are still same after update.','line_number':105,'multiline':False]['text':' Due to the trick we use for Partial aggregation, we only check the weight when local_rank = 0.','line_number':106,'multiline':False]['text':' Ensure all tp ranks have same input.','line_number':116,'multiline':False]['text':' Ensure model are initialized the same way.','line_number':122,'multiline':False]['text':' Shard module and initialize optimizer.','line_number':125,'multiline':False]['text':' TODO: need to revisit input_reshard API about why it failed multi-gpu tests.','line_number':138,'multiline':False]['text':' @parametrize("recompute_activation", [True, False])','line_number':139,'multiline':False]