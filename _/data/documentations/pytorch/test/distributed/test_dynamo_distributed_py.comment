['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]['text':' test an edge case where torch.where.scalar was decomposed to aten.where.self(tensor, tensor, tensor)','line_number':141,'multiline':False]['text':' and the tensors T(0.4) and T(0.5) were not wrapped in FakeTensors during DDPOptimizer compilation','line_number':142,'multiline':False]['text':' sandwich the custom in the middle so it comes before and after','line_number':158,'multiline':False]['text':' test special case where the 0th bucket (layers close to graph input) is at capacity, which would','line_number':165,'multiline':False]['text':' trigger a new bucket, but there are only trivial ops without parameters to put into the new bucket.','line_number':166,'multiline':False]['text':' optimize this case by fusing that 'empty bucket' back together with the previous full one','line_number':167,'multiline':False]['text':' test duplicated inputs','line_number':173,'multiline':False]['text':' Note: use @import_transformers_or_skip on your test case if you use this','line_number':179,'multiline':False]['text':' in a multiprocessing test','line_number':180,'multiline':False]['text':' This simulates DDP, but it doesn't actually do any process communication;','line_number':203,'multiline':False]['text':' it just has enough properties so that the dynamo distributed optimization is','line_number':204,'multiline':False]['text':' able to optimize.  Feel free to simulate more properties as necessary.  The','line_number':205,'multiline':False]['text':' other important thing is patching _active_ddp_module, which is what actually','line_number':206,'multiline':False]['text':' triggers DDP optimization','line_number':207,'multiline':False]['text':' Are these tests failing?  Check and see if TestFakeDistributedSingleProc has a','line_number':272,'multiline':False]['text':' single process version; if it's just a problem in the Dynamo distributed','line_number':273,'multiline':False]['text':' optimizer, you should be able to repro it single process!','line_number':274,'multiline':False]['text':' Activation checkpointing for Linear layers.','line_number':358,'multiline':False]['text':' noqa: E731','line_number':363,'multiline':False]['text':' Test with basic FSDP wrapping (outer wrap around whole model)','line_number':377,'multiline':False]['text':' Test with recursive wrapping, nested FSDP around each Linear','line_number':384,'multiline':False]['text':' Test with basic FSDP wrapping (outer wrap around whole model)','line_number':401,'multiline':False]['text':' Test with recursive wrapping, nested FSDP around each Linear','line_number':408,'multiline':False]['text':' noqa: E731','line_number':427,'multiline':False]['text':' Each FSDP module is a separate graph','line_number':435,'multiline':False]['text':' TODO(whc) Investigate why cudagraphs breaks inductor+fsdp for hf_bert','line_number':443,'multiline':False]['text':' TODO(whc) Investigate why cudagraphs breaks inductor+fsdp for hf_bert','line_number':491,'multiline':False]['text':' noqa: E731','line_number':507,'multiline':False]['text':' ensure compatibility with dynamo explain','line_number':587,'multiline':False]['text':' inp - 1000 * 1000 matrix of float32 (4 bytes) = 4MB','line_number':620,'multiline':False]['text':' hidden - 1000 * 1000 matrix of float32 (4 bytes) = 4MB','line_number':621,'multiline':False]['text':' 4MB','line_number':622,'multiline':False]['text':' ensure compatibility with dynamo explain','line_number':638,'multiline':False]['text':' channel dim must be > 64 for inductor to do layout optimization and use NHWC','line_number':669,'multiline':False]['text':' DDP will always do a 'first bucket' with a really small size;  so only a tiny model will escape this','line_number':708,'multiline':False]['text':' Test with basic FSDP wrapping (outer wrap around whole model)','line_number':851,'multiline':False]['text':' far from an exhaustive check of all the expected guards, just check a couple of them.','line_number':908,'multiline':False]['text':' Use `_param` and `_buf` each twice in this compiled forward','line_number':980,'multiline':False]['text':' to exercise if they are de-duplicated by TorchDynamo','line_number':981,'multiline':False]['text':' Share the buffer, meaning same tensor but different source','line_number':1016,'multiline':False]['text':' Use the same buffer tensor twice in the compiled forward,','line_number':1020,'multiline':False]['text':' including a data mutation to trigger de-dup logic','line_number':1021,'multiline':False]['text':' Check for no recompiles (if there were incorrect de-dup guards, then','line_number':1034,'multiline':False]['text':' the frame count would be equal to the number of forward calls)','line_number':1035,'multiline':False]['text':' force `_param` to be deterministic','line_number':1048,'multiline':False]['text':' Check for no recompiles, which could happen if incorrectly','line_number':1074,'multiline':False]['text':' passing args to the staticmethod (e.g. doubly passing `self`)','line_number':1075,'multiline':False]['text':' 3 is expected here for 1 forward.','line_number':1076,'multiline':False]['text':' Graph 1 should be add and imul','line_number':1077,'multiline':False]