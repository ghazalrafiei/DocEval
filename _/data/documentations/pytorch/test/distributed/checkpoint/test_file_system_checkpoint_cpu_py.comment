['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' The ShardedModels are borrowed from test/distributed/_sharded_tensor/test_sharded_tensor.py','line_number':93,'multiline':False]['text':' Load from file without any resharding','line_number':123,'multiline':False]['text':' pyre-fixme [28]: Unexpected keyword argument `dim` to call `dist._sharding_spec.api.ChunkShardingSpec.__init__`.','line_number':147,'multiline':False]['text':' Test save','line_number':158,'multiline':False]['text':' Create a new model','line_number':167,'multiline':False]['text':' This is not the correct hook for loading the state dict','line_number':169,'multiline':False]['text':' model_to_load._register_load_state_dict_pre_hook(pre_load_state_dict_hook, True)','line_number':170,'multiline':False]['text':' Test load.','line_number':179,'multiline':False]['text':' We hardcode the assumption of how many shards are around','line_number':207,'multiline':False]['text':' pyre-fixme [28]: Unexpected keyword argument `dim` to call `dist._sharding_spec.api.ChunkShardingSpec.__init__`.','line_number':211,'multiline':False]['text':' pyre-fixme [28]: Unexpected keyword argument `dim` to call `dist._sharding_spec.api.ChunkShardingSpec.__init__`.','line_number':219,'multiline':False]['text':' This requires the tensors to be [10, 20]','line_number':229,'multiline':False]['text':' This requires the tensors to be [10, 20]','line_number':259,'multiline':False]['text':' pyre-fixme [28]: Unexpected keyword argument `dim` to call `dist._sharding_spec.api.ChunkShardingSpec.__init__`.','line_number':319,'multiline':False]['text':' pyre-fixme [28]: Unexpected keyword argument `dim` to call `dist._sharding_spec.api.ChunkShardingSpec.__init__`.','line_number':328,'multiline':False]['text':' We can't use torch.allclose since each ST has a different sharding spec','line_number':352,'multiline':False]['text':' Freaky Friday the tensors','line_number':440,'multiline':False]