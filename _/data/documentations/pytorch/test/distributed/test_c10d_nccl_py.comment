['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' bfloat16 is only supported by CUDA 11+','line_number':69,'multiline':False]['text':' create nccl processgroup with opts','line_number':216,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':234,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':235,'multiline':False]['text':' self.num_gpus = torch.cuda.device_count()','line_number':237,'multiline':False]['text':' return rank to GPU map','line_number':253,'multiline':False]['text':' Every rank is root once','line_number':297,'multiline':False]['text':' Run with 1 input tensor','line_number':299,'multiline':False]['text':' test with multiple input tensors (multiple gpu in one rank)','line_number':307,'multiline':False]['text':' sparse allreduce call is wrapped in a try catch since the c10d API is only available in the nccl experimental branch','line_number':327,'multiline':False]['text':' work.result() returns a list of size 1, with the allreduce output as a dense tensor','line_number':332,'multiline':False]['text':' Rethrow the exception if it's a different error','line_number':339,'multiline':False]['text':' Sum','line_number':356,'multiline':False]['text':' Avg (only available for NCCL 2.10+)','line_number':367,'multiline':False]['text':' Premul Sum','line_number':378,'multiline':False]['text':' Product','line_number':392,'multiline':False]['text':' Min','line_number':400,'multiline':False]['text':' Max','line_number':406,'multiline':False]['text':' create some tensors to race with alltoall collective','line_number':431,'multiline':False]['text':' this triggers cudaFree','line_number':441,'multiline':False]['text':' single warmup','line_number':455,'multiline':False]['text':' test that the watchdog does not crash graphs with disallowed event query','line_number':472,'multiline':False]['text':' for every root tensor','line_number':510,'multiline':False]['text':' Premul sum','line_number':536,'multiline':False]['text':' Verification','line_number':581,'multiline':False]['text':' allgather_base is GPU number agnostic.','line_number':595,'multiline':False]['text':' Each rank contribute one tensor regardless of GPU counts','line_number':596,'multiline':False]['text':' Verification','line_number':602,'multiline':False]['text':' anticipate an error','line_number':616,'multiline':False]['text':' fails the check because output_t is not correctly sized','line_number':625,'multiline':False]['text':' anticipate an error','line_number':628,'multiline':False]['text':' fails the check because the dtype is different','line_number':636,'multiline':False]['text':' init input','line_number':656,'multiline':False]['text':' init output','line_number':661,'multiline':False]['text':' init input','line_number':694,'multiline':False]['text':' init output','line_number':701,'multiline':False]['text':' Verification','line_number':714,'multiline':False]['text':' init input','line_number':726,'multiline':False]['text':' init output','line_number':731,'multiline':False]['text':' throws error message from dispatcher','line_number':753,'multiline':False]['text':' init input','line_number':763,'multiline':False]['text':' init output','line_number':790,'multiline':False]['text':' init input','line_number':795,'multiline':False]['text':' test each rank to scatter','line_number':803,'multiline':False]['text':' init output','line_number':828,'multiline':False]['text':' init input','line_number':835,'multiline':False]['text':' test each rank to scatter','line_number':845,'multiline':False]['text':' Verification','line_number':850,'multiline':False]['text':' init output','line_number':861,'multiline':False]['text':' init input','line_number':866,'multiline':False]['text':' throws error message from dispatcher','line_number':888,'multiline':False]['text':' anticipate an error','line_number':906,'multiline':False]['text':' fails the check because output_t is not correctly sized','line_number':915,'multiline':False]['text':' anticipate an error','line_number':918,'multiline':False]['text':' fails the check because the dtype is different','line_number':926,'multiline':False]['text':'  GPU/rank','line_number':945,'multiline':False]['text':'   0         [1], [2], [3], [4]','line_number':946,'multiline':False]['text':'   1         [2], [3], [4], [5]','line_number':947,'multiline':False]['text':'   2         [3], [4], [5], [6]','line_number':948,'multiline':False]['text':'   3         [4], [5], [6], [7]','line_number':949,'multiline':False]['text':' Sum','line_number':951,'multiline':False]['text':' Min','line_number':973,'multiline':False]['text':' Max','line_number':980,'multiline':False]['text':' Product','line_number':989,'multiline':False]['text':' math package don't have math.perm until python 3.8, so','line_number':992,'multiline':False]['text':' we implement a naive version here.','line_number':993,'multiline':False]['text':' Test the input params overridden scenarios, aka, when the input is','line_number':1006,'multiline':False]['text':' a list and output is just one tensor.','line_number':1007,'multiline':False]['text':' Sum','line_number':1008,'multiline':False]['text':' Min','line_number':1017,'multiline':False]['text':' Max','line_number':1022,'multiline':False]['text':' Product','line_number':1027,'multiline':False]['text':' reduce_scatter_base is GPU number agnostic.','line_number':1060,'multiline':False]['text':' Each rank contribute one tensor regardless of GPU counts','line_number':1061,'multiline':False]['text':' Verification','line_number':1067,'multiline':False]['text':' Making the collective to operate on','line_number':1082,'multiline':False]['text':' 1, 2, 3, 4, .... len(local_device_ids) GPUs','line_number':1083,'multiline':False]['text':' Barrier will ensure that all previous work is completed','line_number':1095,'multiline':False]['text':' Generate the same random tensor','line_number':1111,'multiline':False]['text':' Both rank 0 and 1 will use the same CUDA device resulting in ncclInvalidUsage','line_number':1128,'multiline':False]['text':' Disable ASYNC_ERROR_HANDLING for this test to ensure we can programmatically','line_number':1138,'multiline':False]['text':' abort the process group.','line_number':1139,'multiline':False]['text':' First allreduce to initialize state.','line_number':1147,'multiline':False]['text':' Initialize DDP to ensure "destroy_process_group" will not call','line_number':1153,'multiline':False]['text':' ProcessGroupNCCL destructor since DDP holds a reference to process group.','line_number':1154,'multiline':False]['text':' Run a single iteration of DDP to initialize state.','line_number':1155,'multiline':False]['text':' Now simulate collective getting stuck and abort gets us unstuck','line_number':1161,'multiline':False]['text':' Schedule thread before we get stuck to abort pg.','line_number':1165,'multiline':False]['text':' We would get stuck here due to d2h if we didn't abort.','line_number':1169,'multiline':False]['text':' Disable ASYNC_ERROR_HANDLING for this test to ensure we can programmatically','line_number':1177,'multiline':False]['text':' abort the process group.','line_number':1178,'multiline':False]['text':' First allreduce to initialize state.','line_number':1186,'multiline':False]['text':' Destroy pg and validate pg is still in working condition since we hold a','line_number':1189,'multiline':False]['text':' reference above.','line_number':1190,'multiline':False]['text':' Now close pg and validate it no longer works.','line_number':1194,'multiline':False]['text':' Try another collective.','line_number':1197,'multiline':False]['text':' nccl is handled 'specially' inside init_process_group and its options class is different from the options','line_number':1211,'multiline':False]['text':' used by the other PG's.  There are specific edge cases for nccl that need to be tested.','line_number':1212,'multiline':False]['text':' test the default value coming from the `init_process_group` kwarg default','line_number':1219,'multiline':False]['text':' test that `kwarg` timeout takes effect','line_number':1224,'multiline':False]['text':' test that timeout value provided via `pg_options` kwarg is ignored and issues warning,','line_number':1230,'multiline':False]['text':' 'timeout' kwarg (or its kwdefault) taking precedence','line_number':1231,'multiline':False]['text':' TODO(whc) i verified that we are indeed emitting this warning, and i can't figure out why i can't catch it.','line_number':1236,'multiline':False]['text':' self.assertEqual(len(w), 1)','line_number':1237,'multiline':False]['text':' self.assertTrue("pg_options._timeout was specified" in str(w[-1].message))','line_number':1238,'multiline':False]['text':' test that timeout value provided via `pg_options` kwarg is ignored and issues warning,','line_number':1242,'multiline':False]['text':' 'timeout' kwarg taking precedence','line_number':1243,'multiline':False]['text':' allgather_base is GPU number agnostic.','line_number':1282,'multiline':False]['text':' Each rank contribute one tensor regardless of GPU counts','line_number':1283,'multiline':False]['text':' Verification','line_number':1289,'multiline':False]['text':' Test the optimization of new groups that contain all world','line_number':1294,'multiline':False]['text':' ranks use the "transparent" `ncclCommSplit` optimization.','line_number':1295,'multiline':False]['text':' Test lazy splitting behavior across each per-device backend.','line_number':1299,'multiline':False]['text':' split doesn't happen unless the original process group has lazily','line_number':1303,'multiline':False]['text':' created communicators, so first verify we haven't split even when','line_number':1304,'multiline':False]['text':' making the new group and running an operation on the original pg.','line_number':1305,'multiline':False]['text':' The new group will force a split of the original on first use.','line_number':1311,'multiline':False]['text':' Test `ncclCommSplit` for smaller subgroups of the world when','line_number':1317,'multiline':False]['text':' we've passed a specific device_id to init_process_group.','line_number':1318,'multiline':False]['text':' rank 0 hasn't split yet, but rank 1 did for the','line_number':1328,'multiline':False]['text':' nocolor... so split count matches rank count coincidentally','line_number':1329,'multiline':False]['text':' in each of the proceses this test spawned!','line_number':1330,'multiline':False]['text':' now everyone has split because rank 0 has performed a comm','line_number':1335,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':1345,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':1346,'multiline':False]['text':' Need to use TORCH_NCCL_BLOCKING_WAIT and not ASYNC_ERROR_HANDLING,','line_number':1366,'multiline':False]['text':' otherwise process will be taken down and we can't check for errors.','line_number':1367,'multiline':False]['text':' TODO: smaller timeout can fail since PG NCCl does health check in','line_number':1370,'multiline':False]['text':' constructor. Look into reducing this test's runtime.','line_number':1371,'multiline':False]['text':' provide sufficient timeout to initialize NCCL comm.','line_number':1373,'multiline':False]['text':' Simulate stuckness in rank 0.','line_number':1377,'multiline':False]['text':' Time out due to rank 0 not calling into allreduce.','line_number':1383,'multiline':False]['text':' Now when nonzero rank attempts to use communicator, original failure reason should be logged.j','line_number':1387,'multiline':False]['text':' Unblock rank 0','line_number':1395,'multiline':False]['text':' TODO: We can also test that if rank 0 attempts to use the communicator,','line_number':1398,'multiline':False]['text':' then we should error out with the info that it was aborted due to','line_number':1399,'multiline':False]['text':' timeout on another rank. Although this would only be the case after','line_number':1400,'multiline':False]['text':' the watchdog has run on the rank, and there is no reliable way','line_number':1401,'multiline':False]['text':' to confirm it has run.','line_number':1402,'multiline':False]['text':' This tests the backward compatibility of accepting an empty list as `device_ids`,','line_number':1422,'multiline':False]['text':' although we no longer document this in favor of the default value of `None`,','line_number':1423,'multiline':False]['text':' which is consistent with multi-device modules and CPU modules.','line_number':1424,'multiline':False]['text':' Input 2**15, so that the gradients will overflow with a','line_number':1518,'multiline':False]['text':' world_size of 2, unless we normalize the gradient by the','line_number':1519,'multiline':False]['text':' world_size before the reduction','line_number':1520,'multiline':False]['text':' Step model','line_number':1523,'multiline':False]['text':' The first softmax does NOT include fc3 in its autograd graph','line_number':1559,'multiline':False]['text':' whereas the second softmax DOES. If we pass only the first','line_number':1560,'multiline':False]['text':' tensor we see in the output to the reducer, it marks the','line_number':1561,'multiline':False]['text':' gradient for fc3 as ready (because it doesn't show up). If','line_number':1562,'multiline':False]['text':' downstream uses of this return value choose to differentiate','line_number':1563,'multiline':False]['text':' against the second output tensor, it would still receive a','line_number':1564,'multiline':False]['text':' gradient and a callback for this tensor, resulting in a crash.','line_number':1565,'multiline':False]['text':' Always run "backward" to ensure the reducer is called by autograd.','line_number':1586,'multiline':False]['text':' If we don't correctly capture the output tensors from the return value,','line_number':1587,'multiline':False]['text':' the reducer won't see a hook for the unused parameter, and throw an error.','line_number':1588,'multiline':False]['text':' The correct capture is what we're testing in this function.','line_number':1589,'multiline':False]['text':' Test with identity return value','line_number':1595,'multiline':False]['text':' Test with list return value','line_number':1601,'multiline':False]['text':' Test with tuple return value','line_number':1607,'multiline':False]['text':' Test with dict return value','line_number':1613,'multiline':False]['text':' Test with list with dict return value','line_number':1619,'multiline':False]['text':' Test with dict with list return value','line_number':1625,'multiline':False]['text':' Return the fc3 module so that the caller can invoke it','line_number':1677,'multiline':False]['text':' outside of the forward function. While this is bad practice,','line_number':1678,'multiline':False]['text':' we can use it to trigger a reducer error.','line_number':1679,'multiline':False]['text':' First test that finding unused params under these conditions is to','line_number':1718,'multiline':False]['text':' trigger an error when `backward` is called (because fc3 is an unused','line_number':1719,'multiline':False]['text':' parameter and will therefore be marked ready twice).','line_number':1720,'multiline':False]['text':' Only one such parameter in model.fc3, since bias=False','line_number':1738,'multiline':False]['text':' Then test that the default behavior can be overridden by setting','line_number':1750,'multiline':False]['text':' `find_unused_parameters=False`.','line_number':1751,'multiline':False]['text':' Test find_unused_parameters defaults to False','line_number':1759,'multiline':False]['text':' TODO: Combine the following tests once https://github.com/pytorch/pytorch/issues/55967','line_number':1767,'multiline':False]['text':' is resolved.','line_number':1768,'multiline':False]['text':' Compute loss and gradients for both outputs','line_number':1848,'multiline':False]['text':' After initialization, no parameter has their gradient set.','line_number':1901,'multiline':False]['text':' Run `forward` function with torch.no_grad()','line_number':1904,'multiline':False]['text':' No parameter should have their gradient set.','line_number':1909,'multiline':False]['text':' This is NOT the recommended way to implement accumulating grads, but','line_number':1913,'multiline':False]['text':' we would like to make sure DDP does not mess up with the underlying','line_number':1914,'multiline':False]['text':' module.','line_number':1915,'multiline':False]['text':' ensure accumulate grads works with no_grad','line_number':1931,'multiline':False]['text':' Check two model parameters over 4 iterations.','line_number':1936,'multiline':False]['text':' Use 4 iterations because we alternate between reducing and','line_number':1937,'multiline':False]['text':' not reducing and want to make sure we switch both ways.','line_number':1938,'multiline':False]['text':' Skip gradients sync without calling prepare_for_backward','line_number':1943,'multiline':False]['text':' Shuffle the input so that DDP input is different','line_number':1960,'multiline':False]['text':' need to create a separate file for the recovered FileStore, because','line_number':1979,'multiline':False]['text':' the original one will be deleted when destructing the first FileStore.','line_number':1980,'multiline':False]['text':' the file will be deleted by the recovered FileStore','line_number':1984,'multiline':False]['text':' not necessary to run barrier here, as DDP will synchronize','line_number':1987,'multiline':False]['text':' Carry out some trials with small buckets and some with big buckets.','line_number':2061,'multiline':False]['text':' Tuples of lists.  Each list describes per-layer characteristics for one trial.','line_number':2063,'multiline':False]['text':' Reducer.cpp sneakily creates one "initial bucket" that ignores the "bucket_cap_mb"','line_number':2086,'multiline':False]['text':' argument.  The following makes sure the initial bucket also complies.','line_number':2087,'multiline':False]['text':' Prints case-specific debugging info to narrow down failing case.','line_number':2120,'multiline':False]['text':' 3 iters:  First iter creates grads, second iter retests after rebucketing,','line_number':2126,'multiline':False]['text':' third iter tries zeroed grads.','line_number':2127,'multiline':False]['text':' Makes sure we still get info if an error occurred somewhere other than the asserts.','line_number':2175,'multiline':False]['text':' Tells DDP to use just one device.','line_number':2186,'multiline':False]['text':' Tells _test_grad_layout to construct ConvNet with all layers on this process's first assigned device.','line_number':2188,'multiline':False]['text':' DDP's default behavior for a multi-device module is "don't replicate."','line_number':2200,'multiline':False]['text':' Tells _test_grad_layout to constructs this process's ConvNet on 2 devices, with 2 layers on each device.','line_number':2202,'multiline':False]['text':' Register a DDP communication hook if any.','line_number':2252,'multiline':False]['text':' Get GPU model with simple_hook registered.','line_number':2267,'multiline':False]['text':' check whether the grads are equal to what simple_hook's then callback returns.','line_number':2270,'multiline':False]['text':' without the comm_hook, result would be 0.25 * torch.ones(2, 2).','line_number':2271,'multiline':False]['text':' Get GPU model with allreduce_hook registered.','line_number':2295,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2300,'multiline':False]['text':' For these default DDP comm hooks, the only state is process group.','line_number':2310,'multiline':False]['text':' Get GPU model with the hook registered.','line_number':2321,'multiline':False]['text':' The first arg 'process_group' is used for initializing the test environment,','line_number':2322,'multiline':False]['text':' so it cannot be replaced by 'state', although they have the same value.','line_number':2323,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2328,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2352,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2376,'multiline':False]['text':' Get GPU model with the hook registered.','line_number':2386,'multiline':False]['text':' Test the hook with different algorithmic configs.','line_number':2387,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2403,'multiline':False]['text':' Get GPU model with the built-in communication hook.','line_number':2417,'multiline':False]['text':' check whether the grads are equal to what DDP without hook would return.','line_number':2422,'multiline':False]['text':' Multiply the result by 10.','line_number':2516,'multiline':False]['text':' Divide the result by 2.','line_number':2520,'multiline':False]['text':' Get GPU model with allreduce_with_then_hook registered.','line_number':2525,'multiline':False]['text':' check whether the grads are equal to what allreduce returns multiplied by 5.','line_number':2530,'multiline':False]['text':' without the comm_hook, result would be still 0.25 * torch.ones(2, 2).','line_number':2531,'multiline':False]['text':' Each param value is multiplied by "rank + 1" twice in forward, so the grad','line_number':2571,'multiline':False]['text':' values produced by a particular rank should be 2. * (rank + 1).','line_number':2572,'multiline':False]['text':' Summing these over ranks and dividing by world size gives the expected result:','line_number':2573,'multiline':False]['text':' Set the seed to make the embedding and LSTM deterministic (even','line_number':2605,'multiline':False]['text':' across ranks since DDP broadcasts parameters from rank 0)','line_number':2606,'multiline':False]['text':' keep on CPU','line_number':2608,'multiline':False]['text':' Move the input to GPU explicitly for the local model','line_number':2632,'multiline':False]['text':' Let DDP move the input to GPU internally','line_number':2634,'multiline':False]['text':' set TORCH_NCCL_ENABLE_TIMING to enable timing for CUDAEvents','line_number':2661,'multiline':False]['text':' in ProcessGroup Work','line_number':2662,'multiline':False]['text':' N.B.: destroy_process_group is necessary to wait for','line_number':2699,'multiline':False]['text':' all pending works to finish.','line_number':2700,'multiline':False]['text':' intentionally using async ops.','line_number':2725,'multiline':False]['text':' N.B.: destroy_process_group is necessary to wait for','line_number':2730,'multiline':False]['text':' all pending works to finish.','line_number':2731,'multiline':False]['text':' DDP is expected to synchronize model parameter by broadcasting','line_number':2781,'multiline':False]['text':' from rank0 to other ranks. However, this is DDP's internal implementation,','line_number':2782,'multiline':False]['text':' which is subject to change in future versions.','line_number':2783,'multiline':False]['text':' The number of allreduce ops depend on DDP internal implementation, but','line_number':2793,'multiline':False]['text':' there should be at least one allreduce.','line_number':2794,'multiline':False]['text':' Not testing FSDP due to https://github.com/pytorch/pytorch/issues/90848.','line_number':2798,'multiline':False]['text':' We cannot disable workCleanupLoop() as hooks are fired in that thread.','line_number':2799,'multiline':False]['text':' two allgathers, one for size and another for values','line_number':2836,'multiline':False]['text':' Need to skip return code checking for these tests since the child','line_number':2844,'multiline':False]['text':' processes don't exit cleanly.','line_number':2845,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':2852,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':2853,'multiline':False]['text':' Note: we unset and restore TORCH_NCCL_ASYNC_ERROR_HANDLING for this test','line_number':2885,'multiline':False]['text':' since test_c10d_common runs with async error handling by default, but this','line_number':2886,'multiline':False]['text':' tests behavior when it is not enabled.','line_number':2887,'multiline':False]['text':' This allreduce does not block Python thread as allreduce enqueues','line_number':2896,'multiline':False]['text':' the cuda operation, and then wait only blocks the current cuda','line_number':2897,'multiline':False]['text':' stream.','line_number':2898,'multiline':False]['text':' Now the work scheduled next should hang forever since the previous','line_number':2902,'multiline':False]['text':' allreduce will never complete.','line_number':2903,'multiline':False]['text':' Operation would time out in blocking mode.','line_number':2925,'multiline':False]['text':' Run some GPU operations to make sure cuda has not gotten stuck.','line_number':2927,'multiline':False]['text':' It was observed cuda could get stuck if NCCL communicators were','line_number':2928,'multiline':False]['text':' not properly aborted before throwing RuntimeError.','line_number':2929,'multiline':False]['text':' Clean up structures (ex: files for FileStore before going down)','line_number':2932,'multiline':False]['text':' This should timeout','line_number':2994,'multiline':False]['text':' Initialize process_group.','line_number':3018,'multiline':False]['text':' Control gloo pg used as go-ahead signal/barrier','line_number':3022,'multiline':False]['text':' to coordinate btwn ranks.','line_number':3023,'multiline':False]['text':' This should timeout in about 1 second.','line_number':3029,'multiline':False]['text':' Watchdog may abort timed out work resulting in NCCL error instead of operation timed out.','line_number':3030,'multiline':False]['text':' Now do a barrier to tell other rank to go ahead.','line_number':3033,'multiline':False]['text':' Wait on rank 0 to fail.','line_number':3036,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':3051,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':3052,'multiline':False]['text':' No support for float16 for CPU tensors','line_number':3066,'multiline':False]['text':' The tensors to pass to broadcast are identical to the target','line_number':3077,'multiline':False]['text':' only on the process that is the root of the broadcast.','line_number':3078,'multiline':False]['text':' IntraNodeComm currently only supports sum and bf16.','line_number':3132,'multiline':False]['text':' Verify that it is not used in the next two configurations.','line_number':3133,'multiline':False]['text':' Verify that IntraNodeComm is used up to 10MB','line_number':3143,'multiline':False]['text':' Verify that IntraNodeComm is not used beyond 10MB','line_number':3159,'multiline':False]['text':' Test init_process_group accepts options','line_number':3193,'multiline':False]['text':' Test with new_group','line_number':3202,'multiline':False]['text':' test the process group works as expected','line_number':3204,'multiline':False]['text':' Tests functionality when passing nccl config','line_number':3230,'multiline':False]['text':' Tests if comms were configured','line_number':3233,'multiline':False]['text':' Test with new_group','line_number':3257,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':3493,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':3494,'multiline':False]['text':' TORCH_NCCL_BLOCKING_WAIT overrides TORCH_NCCL_ASYNC_ERROR_HANDLING hence tests','line_number':3532,'multiline':False]['text':' that use TORCH_NCCL_BLOCKING_WAIT will test it as expected.','line_number':3533,'multiline':False]['text':' self.num_gpus = torch.cuda.device_count()','line_number':3535,'multiline':False]['text':' embedded shape: (batch_size, sequence_length, embedding_dim)','line_number':3553,'multiline':False]['text':' flattened shape: (batch_size, embedding_dim)','line_number':3555,'multiline':False]['text':' output shape: (batch_size, 1)','line_number':3557,'multiline':False]['text':' set sparse metadata on the DDP model','line_number':3576,'multiline':False]['text':' forward pass','line_number':3579,'multiline':False]['text':' backward pass','line_number':3584,'multiline':False]['text':' Rethrow the exception if it's a different error','line_number':3591,'multiline':False]['text':' return rank to GPU map','line_number':3657,'multiline':False]['text':' we pass the base to the env, and the dump util will append rank','line_number':3661,'multiline':False]['text':' test some other primitives to make sure','line_number':3742,'multiline':False]['text':' their strings are valid','line_number':3743,'multiline':False]['text':' give the other thread some time to fill the cuda buffer','line_number':3827,'multiline':False]['text':' this will eventually cause the missing rank 0','line_number':3836,'multiline':False]['text':' to continue which will unblock the non-zero ranks','line_number':3837,'multiline':False]['text':' fill the cuda buffer, at around 1024 events','line_number':3844,'multiline':False]['text':' this will stall','line_number':3845,'multiline':False]['text':' the base test infra assumes processes exit with matching return codes,','line_number':3872,'multiline':False]['text':' but we want rank0 to abort and rank1 to exit cleanly in this test','line_number':3873,'multiline':False]['text':' wait for rank0 to crash before looking for its output file','line_number':3891,'multiline':False]['text':' we rely on rank0 holding off its abort long enough to dump the debug info','line_number':3892,'multiline':False]['text':' we force disabled timing in setup, since there is no 'disable' function','line_number':3908,'multiline':False]['text':' rank 0 will crash before it passes the sync, but rank1 will exit quickly and cleanly','line_number':3919,'multiline':False]['text':' the base test infra assumes processes exit with matching return codes,','line_number':3927,'multiline':False]['text':' but we want rank0 to abort and rank1 to exit cleanly in this test','line_number':3928,'multiline':False]['text':' wait for both rank0 and 1 to crash before looking for both ranks' output','line_number':3937,'multiline':False]['text':' file, and we rely on rank1 to sleep long enough to dump the debug info.','line_number':3938,'multiline':False]['text':' Set heartbeat timeout to a shorter one (default timeout is 2 min).','line_number':3953,'multiline':False]['text':' rank 0 will crash before it passes the sync, but rank1 will exit quickly and cleanly','line_number':3967,'multiline':False]['text':' Force rank 1 to idle so that it also gets debug info dump triggered.','line_number':3970,'multiline':False]