['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' Each transformer layer has multiple linear layers, so this policy, in','line_number':128,'multiline':False]['text':' combination with the parameter group construction, ensures different','line_number':129,'multiline':False]['text':' hyperparameter settings within one `FlatParameter`','line_number':130,'multiline':False]['text':' Test two different `zero_grad()` timings','line_number':177,'multiline':False]['text':' pre-forward','line_number':179,'multiline':False]['text':' pre-backward','line_number':185,'multiline':False]['text':' Perform the DDP optimizer step on CPU to match FSDP if needed','line_number':187,'multiline':False]['text':' Allow for FSDP prefixes','line_number':202,'multiline':False]['text':' not supported','line_number':352,'multiline':False]['text':' Freeze all biases (which happen to be in the same parameter group)','line_number':405,'multiline':False]['text':' ignore returned optimizer','line_number':436,'multiline':False]['text':' ignored','line_number':439,'multiline':False]['text':' ignored','line_number':440,'multiline':False]['text':' For the transformer model, every parameter is either a weight or a','line_number':449,'multiline':False]['text':' bias, so we only use the first two parameter groups. Moreover, we use','line_number':450,'multiline':False]['text':' Adam and AdamW in particular since they both use bias correction','line_number':451,'multiline':False]['text':' dependent on the step, which is incremented even if a parameter has a','line_number':452,'multiline':False]['text':' zero gradient but not if the gradient is `None`. This is to test that','line_number':453,'multiline':False]['text':' we are differentiating between a zero and `None` gradient correctly.','line_number':454,'multiline':False]['text':' Check that there exists a `FlatParameter` that has both a weight and','line_number':469,'multiline':False]['text':' a bias in this rank's shard','line_number':470,'multiline':False]['text':' Run one iteration to generate gradients','line_number':491,'multiline':False]['text':' Only set the weights' gradients to None','line_number':509,'multiline':False]['text':' Check that FSDP correctly exposes gradients even after forward','line_number':516,'multiline':False]['text':' (namely, `None` for weights and non-`None` for biases)','line_number':517,'multiline':False]['text':' Skip the check since we do not expose the gradients after forward','line_number':519,'multiline':False]['text':' for these strategies','line_number':520,'multiline':False]['text':' Not in this rank's shard','line_number':528,'multiline':False]['text':' Finish the iteration (backward pass and optimizer step)','line_number':538,'multiline':False]['text':' Run one more iteration to confirm bias corrections are correct','line_number':547,'multiline':False]['text':' make different from `inp1`','line_number':646,'multiline':False]['text':' For these loss lists: elem 0 is baseline; elem 1 is test','line_number':647,'multiline':False]['text':' Calls into `summon_full_params()`','line_number':705,'multiline':False]['text':' Force a world size of 2 since the tests hard code to the FSDP','line_number':727,'multiline':False]['text':' sharding strategy to check sharded parameter parity','line_number':728,'multiline':False]['text':' NOTE: This test needs to be changed if the FSDP sharding algorithm','line_number':754,'multiline':False]['text':' changes. It is still valuable until such a change to sanity check the','line_number':755,'multiline':False]['text':' `use_orig_params=True` implementation.','line_number':756,'multiline':False]['text':' 5 * 5 = 25 numel -> pad to 26 -> 13 on each rank','line_number':761,'multiline':False]['text':' 5 * 7 + (1) + 7 = 43 numel -> pad to 44 -> 22 on each rank,','line_number':763,'multiline':False]['text':' where the (1) is from intra-`FlatParameter` alignment padding','line_number':764,'multiline':False]['text':' 22 of weight on rank 0; 13 of weight, 1 alignment padding,','line_number':765,'multiline':False]['text':' and 7 of bias on rank 1','line_number':766,'multiline':False]['text':' For `NO_SHARD`, do nothing since the original parameters','line_number':794,'multiline':False]['text':' are unflattened','line_number':795,'multiline':False]['text':' For no reshard after forward strategies, do nothing since','line_number':801,'multiline':False]['text':' FSDP did not use sharded views after forward','line_number':802,'multiline':False]['text':' Otherwise, case on the parameter (see the model definition)','line_number':804,'multiline':False]['text':' Force a world size of 2 since the tests hard code to the FSDP','line_number':877,'multiline':False]['text':' sharding strategy','line_number':878,'multiline':False]['text':' first vs. second `weight`','line_number':895,'multiline':False]['text':' change `.data` vs. variable itself','line_number':896,'multiline':False]['text':' Check that the writeback propagates','line_number':905,'multiline':False]['text':' for brevity','line_number':914,'multiline':False]['text':' triggers a writeback','line_number':930,'multiline':False]['text':' change `.data` vs. variable itself','line_number':941,'multiline':False]['text':' not well-defined','line_number':954,'multiline':False]['text':' TODO: If we add `summon_full_params(with_grads=True)`, then replace','line_number':968,'multiline':False]['text':' the following. For now, we use the optimizer step as a surrogate for','line_number':969,'multiline':False]['text':' checking that gradients were written back.','line_number':970,'multiline':False]['text':' Generate an initial gradient','line_number':974,'multiline':False]['text':' Change the gradient through the original parameters','line_number':981,'multiline':False]['text':' for brevity','line_number':982,'multiline':False]['text':' triggers a writeback','line_number':1002,'multiline':False]['text':' Intentionally do not zero the gradient to check writeback','line_number':1004,'multiline':False]['text':' triggers a writeback','line_number':1012,'multiline':False]['text':' Check that writing back with mismatched shape errors','line_number':1020,'multiline':False]['text':' for brevity','line_number':1021,'multiline':False]['text':' Change the gradient to a new one with 1 added to each dimension','line_number':1024,'multiline':False]['text':' to force a shape mismatch when writing back','line_number':1025,'multiline':False]['text':' Change `lin1.weight.grad` since it exists on rank 0','line_number':1027,'multiline':False]['text':' Change `lin2.weight.grad` since it exists (partially) on rank 1','line_number':1040,'multiline':False]['text':' triggers a writeback','line_number':1052,'multiline':False]['text':' Test changing the parameter storage to no longer be a view into the','line_number':1064,'multiline':False]['text':' flat parameter','line_number':1065,'multiline':False]['text':' Test changing the parameter variable itself','line_number':1078,'multiline':False]['text':' Train forward -> full-precision unshard -> train forward','line_number':1114,'multiline':False]['text':' Train forward -> eval forward','line_number':1124,'multiline':False]['text':' Train forward/backward -> eval forward -> model checkpoint','line_number':1130,'multiline':False]['text':' Allow for FSDP prefixes','line_number':1158,'multiline':False]['text':' Save the *unsharded* original parameter shapes and check the shapes','line_number':1178,'multiline':False]['text':' match in the forward pass','line_number':1179,'multiline':False]['text':' Baseline is permitted to have padding','line_number':1240,'multiline':False]['text':' For `NO_SHARD`, `use_orig_params=True` presents unflattened','line_number':1243,'multiline':False]['text':' parameters, while `False` presents flattened ones','line_number':1244,'multiline':False]['text':' Gradient numel is different if right after `no_sync()` since','line_number':1248,'multiline':False]['text':' the gradient is unsharded, while the parameter is sharded','line_number':1249,'multiline':False]['text':' For `use_orig_params=False`, the unsharded gradient is','line_number':1251,'multiline':False]['text':' flattened, while for `True`, it is unflattened','line_number':1252,'multiline':False]['text':' Compute some reference gradients using one forward/backward','line_number':1263,'multiline':False]['text':' Run a forward/backward in `no_sync()`','line_number':1279,'multiline':False]['text':' Run a forward/backward outside `no_sync()`','line_number':1288,'multiline':False]['text':' Check that, since we accumulated gradients across 2 iterations, that','line_number':1294,'multiline':False]['text':' the new gradients are 2x the reference gradients','line_number':1295,'multiline':False]['text':' For each of these `no_sync()` backward passes, check that the','line_number':1340,'multiline':False]['text':' gradients are in the low precision parameter dtype (FP16)','line_number':1341,'multiline':False]['text':' For the backward pass outside `no_sync()`, check that the gradients','line_number':1350,'multiline':False]['text':' are cast to the full precision in preparation for the optimizer step','line_number':1351,'multiline':False]['text':' Freeze biases only and flatten both weights and biases into the same','line_number':1365,'multiline':False]['text':' `FlatParameter` to exercise non-uniform `requires_grad`','line_number':1366,'multiline':False]['text':' Define this to be large enough to trigger stack corruption','line_number':1376,'multiline':False]['text':' Check that this does not segfault','line_number':1383,'multiline':False]['text':' Check that this does not segfault','line_number':1391,'multiline':False]