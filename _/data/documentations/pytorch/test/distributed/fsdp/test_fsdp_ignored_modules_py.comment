['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' as opposed to `ignored_states`','line_number':140,'multiline':False]['text':' Initialize an FSDP-wrapped transformer model that has FSDP ignore','line_number':144,'multiline':False]['text':' the `nn.Transformer` module's parameters','line_number':145,'multiline':False]['text':' Unshare the output projection weight and embedding weight to be','line_number':154,'multiline':False]['text':' able to auto wrap every linear correctly','line_number':155,'multiline':False]['text':' Check that the wrapped model's flattened parameter does not include','line_number':166,'multiline':False]['text':' the ignored transformer module's parameters','line_number':167,'multiline':False]['text':' Subtract the numel contributed from alignment padding','line_number':189,'multiline':False]['text':' Check that we can run a few iterations','line_number':200,'multiline':False]['text':' Initialize an FSDP-wrapped nested model that first wraps the nested','line_number':233,'multiline':False]['text':' sequential's second linear layer (`layer1[1]`) and then wraps the','line_number':234,'multiline':False]['text':' overall model while ignoring the nested sequential (`layer1`)','line_number':235,'multiline':False]['text':' Check that the wrapped model's flattened parameter does not include','line_number':249,'multiline':False]['text':' the ignored nested sequential's parameters','line_number':250,'multiline':False]['text':' Subtract the numel contributed from alignment padding','line_number':263,'multiline':False]['text':' Check that we can run a few iterations','line_number':274,'multiline':False]['text':' Construct 2 flat parameters: one for `layer1` and one for the model','line_number':296,'multiline':False]['text':' Use `False` to avoid complexity of intra-flat-parameter padding','line_number':299,'multiline':False]['text':' Passing an FSDP module as an ignored module should error','line_number':335,'multiline':False]['text':' `fully_shard` does not allow to wrap the same model twice, so create','line_number':347,'multiline':False]['text':' a new local model here.','line_number':348,'multiline':False]['text':' To exercise different `FlatParameter` enumerations across ranks,','line_number':379,'multiline':False]['text':' we wrap `layer3` with FSDP, where `layer3` is registered as a module','line_number':380,'multiline':False]['text':' after `layer1`, which has the variable number of ignored modules','line_number':381,'multiline':False]['text':' the ignored modules/parameters contains submodule under model.layer1, which','line_number':441,'multiline':False]['text':' is out of the local root model.layer3.','line_number':442,'multiline':False]['text':' Check that passing `ignored_modules` not as uniformly `nn.Module`','line_number':466,'multiline':False]['text':' raises an error','line_number':467,'multiline':False]['text':' Check that passing both `ignored_modules` and `ignored_states`','line_number':474,'multiline':False]['text':' raises an error (and fold this only into `ignore_modules=True`)','line_number':475,'multiline':False]['text':' Check that passing `ignored_states` not as uniformly','line_number':486,'multiline':False]['text':' `nn.Parameter` or uniformly `nn.Module` raises an error','line_number':487,'multiline':False]