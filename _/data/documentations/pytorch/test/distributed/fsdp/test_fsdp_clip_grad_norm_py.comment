['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' Apply `NO_SHARD` to the encoder','line_number':119,'multiline':False]['text':' Apply `FULL_SHARD` to the decoder','line_number':125,'multiline':False]['text':' TODO: FSDP's `clip_grad_norm_()` is not a static method, so we','line_number':131,'multiline':False]['text':' must make the root module an FSDP instance','line_number':132,'multiline':False]['text':' Multiply gradients by a large factor to ensure that gradients will','line_number':169,'multiline':False]['text':' actually be clipped','line_number':170,'multiline':False]['text':' gradients may be `None` for `use_orig_params=True`','line_number':174,'multiline':False]['text':' Check that the gradients were modified by `clip_grad_norm_()`','line_number':194,'multiline':False]['text':' `None`','line_number':199,'multiline':False]['text':' Run an optimizer step to ensure gradients matched after clipping','line_number':203,'multiline':False]['text':' TODO: Gradient computation on CPU and GPU differ slightly causing','line_number':215,'multiline':False]['text':' drift unrelated to `clip_grad_norm_()`.','line_number':216,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/89133','line_number':217,'multiline':False]['text':' Run a few more iterations','line_number':220,'multiline':False]['text':' TODO: We cannot run too many iterations, or else there is drift:','line_number':221,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/89136','line_number':222,'multiline':False]['text':' exercise both','line_number':224,'multiline':False]['text':' Check that the total norm is in FP16 to match the gradient dtype','line_number':292,'multiline':False]['text':' As a best effort, check that each gradient has norm at most the max','line_number':294,'multiline':False]['text':' norm (since DDP does not support mixed precision natively, we cannot','line_number':295,'multiline':False]['text':' directly compare for parity)','line_number':296,'multiline':False]