['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' ensure biases have different gradients','line_number':110,'multiline':False]['text':' ensure biases have different gradients','line_number':138,'multiline':False]['text':' Flatten Bias0; then flatten weight and Bias1 together into `block1`','line_number':182,'multiline':False]['text':' Flatten Bias0; flatten Bias1; then flatten weight into `block2[1]`','line_number':189,'multiline':False]['text':' Flatten weight, Bias, bias into `block2[2]`','line_number':201,'multiline':False]['text':' Either register the parameter to a module to be wrapped with FSDP','line_number':236,'multiline':False]['text':' (`model.block2[2]`) or a module not to be wrapped with FSDP (`model`)','line_number':237,'multiline':False]['text':' For simplicity, we only add a single unmanaged parameter, but should','line_number':243,'multiline':False]['text':' be easy to generalize if needed','line_number':244,'multiline':False]['text':' The unmanaged parameters should be passed to this method in','line_number':251,'multiline':False]['text':' `model.parameters()` order since their parameter IDs will be assigned','line_number':252,'multiline':False]['text':' in order of the skipped IDs','line_number':253,'multiline':False]['text':' Assign a parameter ID to the unmanaged parameter','line_number':254,'multiline':False]['text':' last ID skipped','line_number':264,'multiline':False]['text':' Add a state entry for the unmanaged parameter','line_number':266,'multiline':False]['text':' Insert the ID into the parameter group in order','line_number':273,'multiline':False]['text':' NOTE: We exclude `self.bias` from either parameter group to test the','line_number':276,'multiline':False]['text':' case where the optimizer input does not include all model parameters','line_number':277,'multiline':False]['text':' Use `block1`'s parameters for the first parameter group to deviate','line_number':279,'multiline':False]['text':' from the `model.parameters()` order','line_number':280,'multiline':False]['text':' Deviate from the `model.parameters()` order further by rearranging','line_number':284,'multiline':False]['text':' `block2`'s parameters to be before `block0`'s parameters','line_number':285,'multiline':False]['text':' Simple and boring model to test interface and some corner cases that do not','line_number':289,'multiline':False]['text':' require complicated wrapping strategy.','line_number':290,'multiline':False]['text':' ignored if `wrap=False`','line_number':320,'multiline':False]['text':' Use a reversed parameter order for the optimizer input on odd ranks','line_number':342,'multiline':False]['text':' Keep these as arguments for parity with `_init_nested_model()`;','line_number':362,'multiline':False]['text':' these settings are not implemented since the transformer is','line_number':363,'multiline':False]['text':' wrapped with FSDP at the top-level, which means that there is','line_number':364,'multiline':False]['text':' only a single flat parameter, making these booleans vacuous','line_number':365,'multiline':False]['text':' set seed for determinism','line_number':387,'multiline':False]['text':' tensor state','line_number':424,'multiline':False]['text':' Check the values on CPU to be device-agnostic','line_number':426,'multiline':False]['text':' non-tensor state','line_number':433,'multiline':False]['text':' Check parameter keys are the same first for earlier erroring','line_number':456,'multiline':False]['text':' Check state values are the same','line_number':463,'multiline':False]['text':' Otherwise, only require the parameter keys to be isomorphic (e.g.','line_number':469,'multiline':False]['text':' between IDs and names)','line_number':470,'multiline':False]['text':' Use brute-force quadratic-time comparison since it is hard to','line_number':474,'multiline':False]['text':' hash a tensor by value instead of by object','line_number':475,'multiline':False]['text':' Check for at least one match (may be > 1 in toy edge cases, e.g.','line_number':477,'multiline':False]['text':' multiple biases); nonetheless, each having >= 1 match and the two','line_number':478,'multiline':False]['text':' lists having equal length imply that the list contents are equal','line_number':479,'multiline':False]['text':' not supported','line_number':556,'multiline':False]['text':' Non-target ranks get an empty state dict','line_number':580,'multiline':False]['text':' Check the losses to eliminate model drift as a source of error','line_number':591,'multiline':False]['text':' Do not check the parameter keys since the full/sharded optimizer state','line_number':594,'multiline':False]['text':' dict uses parameter names, while the non-wrapped equivalent uses','line_number':595,'multiline':False]['text':' parameter IDs','line_number':596,'multiline':False]['text':' Add checkpointing to ensure optim_state_dict and state_dict strip out','line_number':618,'multiline':False]['text':' checkpointing prefixes.','line_number':619,'multiline':False]['text':' Check that checkpointing prefix was indeed stripped.','line_number':631,'multiline':False]['text':' Exclude a parameter so that nonzero ranks are missing state','line_number':644,'multiline':False]['text':' To save CI costs, we test with the "harder" settings:','line_number':699,'multiline':False]['text':' To save CI costs, we test with the "harder" settings:','line_number':800,'multiline':False]['text':' We cannot test halve_world_size with SHARDED_STATE_DICT.','line_number':965,'multiline':False]['text':' First, run a wrapped model with full world size for a few iterations','line_number':1003,'multiline':False]['text':' Create a new process group with halved world size','line_number':1015,'multiline':False]['text':' Continue using the same group and hence world size','line_number':1021,'multiline':False]['text':' Second, run a wrapped model with (possibly) halved world size and','line_number':1023,'multiline':False]['text':' (possibly) differing `optim_input` across ranks','line_number':1024,'multiline':False]['text':' specify `wrap_alt` to change wrapping','line_number':1030,'multiline':False]['text':' Compute two sharded optim state dicts: (1) for the first model','line_number':1038,'multiline':False]['text':' according to the second model and (2) for the second model according','line_number':1039,'multiline':False]['text':' to the second model','line_number':1040,'multiline':False]['text':' As a sanity check, check that sharding the second model's full/sharded','line_number':1104,'multiline':False]['text':' optimizer state dict according to itself is equivalent to its local','line_number':1105,'multiline':False]['text':' optimizer's state dict','line_number':1106,'multiline':False]['text':' should all have matching parameter IDs','line_number':1108,'multiline':False]['text':' Check that sharding the first model's full/sharded optimizer state dict','line_number':1119,'multiline':False]['text':' according to the second model is equivalent to the second model's','line_number':1120,'multiline':False]['text':' local optimizer state dict','line_number':1121,'multiline':False]['text':' As a sanity check, check that we can load and run a few iterations','line_number':1132,'multiline':False]['text':' Create a normal wrapped model','line_number':1179,'multiline':False]['text':' save on all ranks to avoid having to broadcast from rank 0','line_number':1188,'multiline':False]['text':' Create a new model with the same structure but additional unmanaged','line_number':1191,'multiline':False]['text':' parameters, representing the model for which we want to load','line_number':1192,'multiline':False]['text':' If we add the unmanaged parameters to a module wrapped with FSDP,','line_number':1202,'multiline':False]['text':' then the flat parameter will be comprised of some unflattened','line_number':1203,'multiline':False]['text':' parameters with zero-dimensional tensor state (i.e. Adam "step")','line_number':1204,'multiline':False]['text':' and others without (i.e. the unmanaged parameters), which','line_number':1205,'multiline':False]['text':' triggers an error that we have to ensure correctness','line_number':1206,'multiline':False]['text':' If we add the unmanaged parameters to a module not wrapped with','line_number':1226,'multiline':False]['text':' FSDP, then we simply ignore them without erroring to enable','line_number':1227,'multiline':False]['text':' model parallelism use cases, where some parameters are managed','line_number':1228,'multiline':False]['text':' externally to FSDP','line_number':1229,'multiline':False]['text':' Add entries for the unmanaged parameters to be able to load','line_number':1242,'multiline':False]['text':' Check that we can load the optimizer state dict','line_number':1249,'multiline':False]['text':' Run a wrapped model for a few iterations','line_number':1283,'multiline':False]['text':' Broadcast instead of `torch.save()`/`torch.load()` so that all ranks','line_number':1295,'multiline':False]['text':' have the full state dict','line_number':1296,'multiline':False]['text':' Run a non-wrapped model for a few iterations','line_number':1300,'multiline':False]['text':' Re-key the wrapped model's optimizer state dict using parameter IDs','line_number':1306,'multiline':False]['text':' according to the non-wrapped model','line_number':1307,'multiline':False]['text':' Check that the re-keyed dict and actual dict are the same','line_number':1323,'multiline':False]['text':' As a sanity check, check that we can load and run a few iterations','line_number':1336,'multiline':False]['text':' Run a wrapped model for a few iterations','line_number':1361,'multiline':False]['text':' Run a non-wrapped model for a few iterations','line_number':1367,'multiline':False]['text':' Re-key the non-wrapped model's optimizer state dict using parameter','line_number':1373,'multiline':False]['text':' names (still according to itself)','line_number':1374,'multiline':False]['text':' Shard the non-wrapped model's re-keyed optimizer state dict, which','line_number':1391,'multiline':False]['text':' maps back to (flattened) parameter IDs','line_number':1392,'multiline':False]['text':' Check that this sharded optimizer state dict matches the wrapped','line_number':1406,'multiline':False]['text':' model's per-rank optimizer state dict','line_number':1407,'multiline':False]['text':' As a sanity check, check that we can load and run a few iterations','line_number':1420,'multiline':False]['text':' Check every method since they all accept `optim_input`','line_number':1430,'multiline':False]['text':' Sharded optim state dict','line_number':1465,'multiline':False]['text':' may not be defined due to previous method erroring','line_number':1473,'multiline':False]['text':' Full optim state dict','line_number':1481,'multiline':False]['text':' Rekey optim state dict','line_number':1504,'multiline':False]['text':' from `full_optim_state_dict()`','line_number':1513,'multiline':False]['text':' Do not use `lin1`, which is the parameter passed to the','line_number':1546,'multiline':False]['text':' optimizer and the one checked for "step" state to see if it','line_number':1547,'multiline':False]['text':' is tensor or float','line_number':1548,'multiline':False]['text':' or any optimizer with "step"','line_number':1557,'multiline':False]['text':' Run an iteration to construct optimizer state','line_number':1559,'multiline':False]['text':' Check that save and load does not error','line_number':1566,'multiline':False]['text':' `__setstate__()` will check the 0th parameter to see if "step" is','line_number':1576,'multiline':False]['text':' represented as a tensor or float, so it is imperative that its state','line_number':1577,'multiline':False]['text':' is non-empty.','line_number':1578,'multiline':False]['text':' Run an iteration as a sanity check','line_number':1580,'multiline':False]['text':' Train one batch and see if optim_state_dict are the same.','line_number':1630,'multiline':False]['text':' Eagerly initialize the states','line_number':1633,'multiline':False]['text':' Make optim1 has a different state.','line_number':1651,'multiline':False]['text':' Load the state back to see if load_optim_state_dict works.','line_number':1658,'multiline':False]['text':' Train one step to save original optimizer state dict and original optimizer param groups.','line_number':1686,'multiline':False]['text':' manually remove param_groups from optimizer state dict','line_number':1698,'multiline':False]['text':' passing the osd without param_groups to FSDP','line_number':1702,'multiline':False]['text':' check the state_dict sharded by FSDP does not contain param_groups.','line_number':1708,'multiline':False]['text':' train another step to make optim a different state.','line_number':1711,'multiline':False]['text':' manually add param_groups to state_dict_to_load before loading the optimizer state','line_number':1723,'multiline':False]['text':' First, run a wrapped model with full world size for a few iterations','line_number':1766,'multiline':False]['text':' Create a new process group with halved world size','line_number':1780,'multiline':False]['text':' Continue using the same group and hence world size','line_number':1786,'multiline':False]['text':' Second, run a wrapped model with (possibly) halved world size and','line_number':1788,'multiline':False]['text':' (possibly) differing `optim_input` across ranks','line_number':1789,'multiline':False]['text':' specify `wrap_alt` to change wrapping','line_number':1795,'multiline':False]['text':' Compute two sharded optim state dicts: (1) for the first model','line_number':1805,'multiline':False]['text':' according to the second model and (2) for the second model according','line_number':1806,'multiline':False]['text':' to the second model','line_number':1807,'multiline':False]['text':' As a sanity check, check that sharding the second model's full/sharded','line_number':1812,'multiline':False]['text':' optimizer state dict according to itself is equivalent to its local','line_number':1813,'multiline':False]['text':' optimizer's state dict','line_number':1814,'multiline':False]['text':' Check that sharding the first model's full/sharded optimizer state dict','line_number':1826,'multiline':False]['text':' according to the second model is equivalent to the second model's','line_number':1827,'multiline':False]['text':' local optimizer state dict','line_number':1828,'multiline':False]['text':' As a sanity check, check that we can load and run a few iterations','line_number':1842,'multiline':False]['text':' Test the default setting.','line_number':1870,'multiline':False]['text':' Test sharded state_dict without offload_to_cpu','line_number':1877,'multiline':False]['text':' Test full state_dict with rank0_only','line_number':1893,'multiline':False]['text':' Add customized value','line_number':1931,'multiline':False]