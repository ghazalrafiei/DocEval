['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' Owner(s): ["oncall: distributed"]','line_number':2,'multiline':False]['text':' construct a cuda device mesh','line_number':114,'multiline':False]['text':' check all dim groups','line_number':117,'multiline':False]['text':' construct a cuda device mesh','line_number':163,'multiline':False]['text':' check all dim groups','line_number':167,'multiline':False]['text':' test init_device_mesh with mesh_dim_names','line_number':206,'multiline':False]['text':' test init_device_mesh without mesh_dim_names','line_number':214,'multiline':False]['text':' make the random seed same across rank','line_number':365,'multiline':False]['text':' scatter on dim > 0 would generate non-contiguous tensor, verify that works','line_number':372,'multiline':False]['text':' We need to check numel() instead of size if a tensor is ([]) after unpadding,','line_number':410,'multiline':False]['text':' since the size could be ([0, 8]) after unpadding.','line_number':411,'multiline':False]['text':' unpad scattered_tensor','line_number':498,'multiline':False]['text':' We need to check numel() instead of size if a tensor is ([]) after unpadding,','line_number':505,'multiline':False]['text':' since the size could be ([0, 8]) after unpadding.','line_number':506,'multiline':False]['text':' check all dim groups','line_number':525,'multiline':False]['text':' check all dim groups','line_number':542,'multiline':False]['text':' transpose on a 2D tensor distributed over N nodes:','line_number':561,'multiline':False]['text':' i.e. transpose','line_number':571,'multiline':False]['text':' scatter on dim > 0 would generate non-contiguous tensor, verify that works','line_number':579,'multiline':False]['text':' check all dim groups','line_number':591,'multiline':False]['text':' i.e. transpose','line_number':606,'multiline':False]['text':' input_tensor = torch.cat(input_tensor_list, dim=scatter_dim)','line_number':610,'multiline':False]['text':' scatter on dim > 0 would generate non-contiguous tensor, verify that works','line_number':615,'multiline':False]