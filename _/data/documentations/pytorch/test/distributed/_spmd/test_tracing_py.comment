['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' make sure we set different random seeds for each rank','line_number':34,'multiline':False]['text':' otherwise we dont need DDP / SPMD','line_number':35,'multiline':False]['text':' (we would have the same parameters and inputs everywhere)','line_number':36,'multiline':False]['text':' check all dim groups','line_number':48,'multiline':False]['text':' multiply with 1 to trigger wait on read during tracing.','line_number':58,'multiline':False]['text':' use a local_tensor + 1 for tracing to make sure that we are not','line_number':61,'multiline':False]['text':' simply replaying recorded tensor value','line_number':62,'multiline':False]['text':' execute traced DeviceMesh communication','line_number':65,'multiline':False]['text':' check all dim groups','line_number':73,'multiline':False]['text':' multiply with 1 to trigger wait on read during tracing.','line_number':84,'multiline':False]['text':' use a local_tensor + 1 for tracing to make sure that we are not','line_number':88,'multiline':False]['text':' simply replaying recorded tensor value','line_number':89,'multiline':False]['text':' execute traced DeviceMesh communication','line_number':92,'multiline':False]['text':' check all dim groups','line_number':100,'multiline':False]['text':' multiply with 1 to trigger wait on read during tracing.','line_number':116,'multiline':False]['text':' use a local_tensor + 1 for tracing to make sure that we are not','line_number':119,'multiline':False]['text':' simply replaying recorded tensor value','line_number':120,'multiline':False]['text':' each rank have its own tensor, all_gather gives a big tensor','line_number':129,'multiline':False]['text':' use a local_tensor + 1 for tracing to make sure that we are not','line_number':145,'multiline':False]['text':' simply replaying recorded tensor value','line_number':146,'multiline':False]['text':' some dummy compute','line_number':228,'multiline':False]['text':' FIXME(@mrshenli): remove manual seed once dist.compile can synchronize','line_number':315,'multiline':False]['text':' module parameters.','line_number':316,'multiline':False]['text':' FIXME(@mrshenli): DDP by default divides gradients by world size.','line_number':327,'multiline':False]['text':' Should we match that behavior?','line_number':328,'multiline':False]['text':' materialize optimizer states','line_number':334,'multiline':False]['text':' NB: In *normal* use cases you don't need to do this, but some','line_number':338,'multiline':False]['text':' of the tests setup buffers which require_grad=True which is weird','line_number':339,'multiline':False]['text':' and so work around it','line_number':340,'multiline':False]['text':' test parameter parity','line_number':350,'multiline':False]['text':' FIXME(@mrshenli): DDP by default divides grads by world size, but','line_number':354,'multiline':False]['text':' torch.distributed.compile does not do that yet.','line_number':355,'multiline':False]['text':' FIXME(@mrshenli): remove manual seed once dist.compile can synchronize','line_number':372,'multiline':False]['text':' module parameters.','line_number':373,'multiline':False]['text':' check dedup is successful, where there should only be 1 allreduce','line_number':398,'multiline':False]['text':' FIXME(@mrshenli): remove manual seed once dist.compile can synchronize','line_number':417,'multiline':False]['text':' module parameters.','line_number':418,'multiline':False]['text':' FIXME(@mrshenli): gradients for bias is missing','line_number':420,'multiline':False]['text':' N.B.: this is not a complete subgraph representing','line_number':472,'multiline':False]['text':' original logic, as we are testing the ability to','line_number':473,'multiline':False]['text':' modify graph after DTensor expansion.','line_number':474,'multiline':False]['text':' FIXME: symbolic tracing treats bs=1 as constant, have to use bs > 1.','line_number':491,'multiline':False]['text':' checking transforms are indeed invoked.','line_number':495,'multiline':False]['text':' N.B.: this is not a complete subgraph representing','line_number':533,'multiline':False]['text':' original logic, as we are testing the ability to','line_number':534,'multiline':False]['text':' modify graph after DTensor expansion.','line_number':535,'multiline':False]['text':' checking transforms are indeed invoked.','line_number':568,'multiline':False]['text':' materialize optimizer states','line_number':605,'multiline':False]['text':' N.B.: setting requires_grad in forward, as deepcopy does not','line_number':627,'multiline':False]['text':' work for requires_grad=True buffers.','line_number':628,'multiline':False]['text':' backward grad expansion op should match local batch size','line_number':672,'multiline':False]['text':' instead of global batch size.','line_number':673,'multiline':False]['text':' materialize optimizer states','line_number':690,'multiline':False]['text':' test parameter parity','line_number':699,'multiline':False]['text':' FIXME(@mrshenli): DDP by default divides grads by world size, but','line_number':703,'multiline':False]['text':' torch.distributed.compile does not do that yet.','line_number':704,'multiline':False]['text':' FIXME: torch.tensor(x.numel()) is captured as a tensor constant','line_number':904,'multiline':False]