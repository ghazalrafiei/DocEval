['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' Generate tests for BAND.','line_number':82,'multiline':False]['text':' The bit that is set changes in every iteration to check','line_number':83,'multiline':False]['text':' that the output changes accordingly.','line_number':84,'multiline':False]['text':' Generate tests for BOR.','line_number':96,'multiline':False]['text':' These emulate a larger world size per iteration by having every','line_number':97,'multiline':False]['text':' rank contribute multiple values that are pre-OR'ed.','line_number':98,'multiline':False]['text':' Generate tests for XOR.','line_number':110,'multiline':False]['text':' These emulate a larger world size per iteration by having every','line_number':111,'multiline':False]['text':' rank contribute multiple values that are pre-XOR'ed.','line_number':112,'multiline':False]['text':' Execute 2x the number of operations to ensure we use every device.','line_number':241,'multiline':False]['text':' Every rank is root once','line_number':326,'multiline':False]['text':' Run with 1 input tensor','line_number':328,'multiline':False]['text':' Run with 2 input tensors','line_number':333,'multiline':False]['text':' Test overloaded convenience function','line_number':345,'multiline':False]['text':' Single input tests','line_number':420,'multiline':False]['text':' Multi input tests','line_number':431,'multiline':False]['text':' Test overloaded convenience function (defaults to using sum)','line_number':443,'multiline':False]['text':' _using_work_api tests are to make sure we still properly support work API.','line_number':462,'multiline':False]['text':' This should go away as we deprecate it.','line_number':463,'multiline':False]['text':' Single input tests','line_number':470,'multiline':False]['text':' Multi input tests','line_number':481,'multiline':False]['text':' Test overloaded convenience function (defaults to using sum)','line_number':493,'multiline':False]['text':' Sparse allreduce only works with c10d.ReduceOp.SUM.','line_number':672,'multiline':False]['text':' Preallocate tensors for input/output','line_number':804,'multiline':False]['text':' Take turns being the scatter root and accumulate work items','line_number':808,'multiline':False]['text':' Wait for work to complete','line_number':818,'multiline':False]['text':' Preallocate tensors for input/output','line_number':986,'multiline':False]['text':' Take turns being the gather root and accumulate work items','line_number':990,'multiline':False]['text':' Wait for work to complete','line_number':1000,'multiline':False]['text':' Take a column of 2D tensor, such that memory is not dense','line_number':1019,'multiline':False]['text':' Run with N input tensor per rank','line_number':1121,'multiline':False]['text':' Take a column of 2D tensor, such that memory is not dense','line_number':1150,'multiline':False]['text':' Note that this works around the data race discussed in','line_number':1169,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/75529, but we should','line_number':1170,'multiline':False]['text':' actually be able to pass the list directly into allgather when','line_number':1171,'multiline':False]['text':' that race is fixed.','line_number':1172,'multiline':False]['text':' One of output tensors does not match input list.','line_number':1208,'multiline':False]['text':' One of output tensors does not match input list.','line_number':1215,'multiline':False]['text':' Output lists have too many elements','line_number':1220,'multiline':False]['text':' Output is not a list of lists.','line_number':1229,'multiline':False]['text':' expected outputs','line_number':1247,'multiline':False]['text':' one iteration','line_number':1252,'multiline':False]['text':' one output tensor list','line_number':1254,'multiline':False]['text':' one tensor in output tensor list','line_number':1256,'multiline':False]['text':' Added to address https://github.com/pytorch/pytorch/issues/65231','line_number':1259,'multiline':False]['text':' In the failed tests, all assertEqual are passed on all processes.','line_number':1260,'multiline':False]['text':' However, one of the processes didn't call ProcessGroupGloo','line_number':1261,'multiline':False]['text':' destructor before exiting program. This is not surprising as the only','line_number':1262,'multiline':False]['text':' guarantee that Python makes is that garbage collection MAY happen','line_number':1263,'multiline':False]['text':' before the program exits. If GC didn't happen, the two threads in','line_number':1264,'multiline':False]['text':' ProcessGroup might be destructed before joined.','line_number':1265,'multiline':False]['text':' FIXME: it's still unclear why only this test require explicit','line_number':1266,'multiline':False]['text':' destroy_process_group()','line_number':1267,'multiline':False]['text':' Preallocate tensors for input/output','line_number':1382,'multiline':False]['text':' Issue sends','line_number':1386,'multiline':False]['text':' Issue recvs','line_number':1393,'multiline':False]['text':' Wait for sends to complete','line_number':1400,'multiline':False]['text':' Wait for recvs to complete','line_number':1405,'multiline':False]['text':' Test that every output other than our own contains the respective rank','line_number':1410,'multiline':False]['text':' Kick off allreduce operations','line_number':1423,'multiline':False]['text':' Note: leak the returned work handle','line_number':1428,'multiline':False]['text':' Barrier should ensure all previous work has completed','line_number':1431,'multiline':False]['text':' Run a few collectives so that we have called each process group','line_number':1450,'multiline':False]['text':' Run create/use/destroy twice','line_number':1470,'multiline':False]['text':' Run forward','line_number':1567,'multiline':False]['text':' The grads of all parameters should be None at this point.','line_number':1570,'multiline':False]['text':' Run backward','line_number':1576,'multiline':False]['text':' Now locally unused parameter should have grad updated on all ranks.','line_number':1579,'multiline':False]['text':' However the globally unused parameter should still have None grad.','line_number':1580,'multiline':False]['text':' Test on CPU','line_number':1587,'multiline':False]['text':' Test on GPU','line_number':1597,'multiline':False]['text':' Run forward','line_number':1648,'multiline':False]['text':' The grads of all parameters should be None at this point.','line_number':1651,'multiline':False]['text':' Run backward','line_number':1654,'multiline':False]['text':' Now locally unused parameter should have grad updated on all ranks.','line_number':1657,'multiline':False]['text':' Test on CPU','line_number':1662,'multiline':False]['text':' Test on GPU','line_number':1670,'multiline':False]['text':' Run a few iterations where we ignore the output.','line_number':1710,'multiline':False]['text':' Run a few iterations where we use the output.','line_number':1715,'multiline':False]['text':' Run a few iterations where we ignore the output.','line_number':1754,'multiline':False]['text':' Run a few iterations where we use the output.','line_number':1759,'multiline':False]['text':' test to make DDP constructor will not fail when module includes a ShardedTensor when ignored','line_number':1797,'multiline':False]['text':' Run with entire batch against single process version','line_number':1814,'multiline':False]['text':' Run with partial batch against multi process version','line_number':1817,'multiline':False]['text':' Check that the gradients are sparse and identical','line_number':1822,'multiline':False]['text':' ensure that all the three models start with the same set of parameters. By default they are randomized on construction','line_number':1871,'multiline':False]['text':' run the model for 6 iterations, with a checkpoint in the middle','line_number':1896,'multiline':False]['text':' zero out parameters of both DDP and non-DDP models and reload them from the DDP state dict','line_number':1899,'multiline':False]['text':' the non-DDP model needs to first remove the prefix of "module." from the DDP state dict','line_number':1913,'multiline':False]['text':' re-run the model with the same inputs for 6 iterations with no checkpoint','line_number':1922,'multiline':False]['text':' Ensure initialized weights and inputs are identical across processes','line_number':1936,'multiline':False]['text':' Test on CPU','line_number':1965,'multiline':False]['text':' Register DDP Communication Hook','line_number':1970,'multiline':False]['text':' check whether the grads are equal to what then callback returns.','line_number':1973,'multiline':False]['text':' without the comm_hook, result would be 0.25 * torch.ones(2, 2).','line_number':1974,'multiline':False]['text':' Register a DDP communication hook if any.','line_number':1988,'multiline':False]['text':' Get GPU model with simple_hook registered.','line_number':2003,'multiline':False]['text':' check whether the grads are equal to what simple_hook's then callback returns.','line_number':2006,'multiline':False]['text':' without the comm_hook, result would be 0.25 * torch.ones(2, 2).','line_number':2007,'multiline':False]['text':' Run forward','line_number':2073,'multiline':False]['text':' Run backward','line_number':2076,'multiline':False]['text':' Ensure initialized weights and inputs are identical across processes','line_number':2112,'multiline':False]['text':' Divide the result by 2 * world_size.','line_number':2125,'multiline':False]['text':' Prepare allreduced grad bucket tensors by running an async work.','line_number':2128,'multiline':False]['text':' Raise if there are multiple types per bucket.','line_number':2186,'multiline':False]['text':' In this case we create one bucket for all parameters.','line_number':2187,'multiline':False]['text':' Check that the grad of fc3 is not set.','line_number':2252,'multiline':False]['text':' Compute and accumulate gradients.','line_number':2255,'multiline':False]['text':' The reducer will have marked the grad of fc3 as ready, because','line_number':2259,'multiline':False]['text':' it doesn't show up in the autograd graph of `output`. Since fc3.weight','line_number':2260,'multiline':False]['text':' is considered being globally unused, it will be kept untouched as None.','line_number':2261,'multiline':False]['text':' The `zero_grad` function calls `detach_` and `zero_` on the grad','line_number':2276,'multiline':False]['text':' tensors of model parameters. If we tried to set the grad tensors','line_number':2277,'multiline':False]['text':' to a view of the reducer's bucket tensors, this would blow up.','line_number':2278,'multiline':False]['text':' Unused parameter only in the first iteration.','line_number':2281,'multiline':False]['text':' No support for float16 for CPU tensors','line_number':2308,'multiline':False]['text':' The tensors to pass to broadcast are identical to the target','line_number':2319,'multiline':False]['text':' only on the process that is the root of the broadcast.','line_number':2320,'multiline':False]