['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' mesh: 4 * 2','line_number':28,'multiline':False]['text':' replicate, shard','line_number':34,'multiline':False]['text':' shard, shard','line_number':42,'multiline':False]['text':' first dim','line_number':45,'multiline':False]['text':' second dim','line_number':50,'multiline':False]['text':' When the placements is [Shard(0)], we test for three different scenarios:','line_number':61,'multiline':False]['text':' 1) sharding resulting in empty shards on all or some of the ranks','line_number':62,'multiline':False]['text':' 2) sharding resulting in shards of different size across different ranks','line_number':63,'multiline':False]['text':' 3) sharding resulting in non-empty shards of same size across all ranks','line_number':64,'multiline':False]['text':' TODO: make this test cleaner and work for nD','line_number':76,'multiline':False]['text':' Check the local tensor of dtensor is exactly the same','line_number':80,'multiline':False]['text':' if we slice the global_tensor with local_size and global_offset','line_number':81,'multiline':False]['text':' Generating 6 two-d placements combinations','line_number':90,'multiline':False]['text':' mesh: 2 * 4','line_number':97,'multiline':False]['text':' TODO: make this test cleaner and work for nD','line_number':108,'multiline':False]['text':' Check the local tensor of dtensor is exactly the same','line_number':114,'multiline':False]['text':' if we slice the global_tensor with local_size and global_offset','line_number':115,'multiline':False]