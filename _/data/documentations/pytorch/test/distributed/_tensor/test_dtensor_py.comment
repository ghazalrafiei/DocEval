['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]['text':' Owner(s): ["oncall: distributed"]','line_number':2,'multiline':False]['text':' Test distribute_tensor on meta tensor','line_number':94,'multiline':False]['text':' Test from_local on meta tensor','line_number':103,'multiline':False]['text':' won't affect stride','line_number':153,'multiline':False]['text':' will affect stride after DT initialized','line_number':160,'multiline':False]['text':' if initialized from a transposed mat','line_number':163,'multiline':False]['text':' test dist tensor works with torch.Tensor during backwards','line_number':188,'multiline':False]['text':' do some operations on local tensor','line_number':190,'multiline':False]['text':' create the dist tensor with non leaf local tensor, dist tensor created','line_number':192,'multiline':False]['text':' should also be non leaf node','line_number':193,'multiline':False]['text':' do some random operations on dist tensor','line_number':196,'multiline':False]['text':' trigger .backward() on dist tensor directly','line_number':199,'multiline':False]['text':' run backward directly on dist tensor','line_number':202,'multiline':False]['text':' check it gradients flow back to original torch.Tensor','line_number':204,'multiline':False]['text':' test dist tensor works with torch.Tensor during backwards','line_number':299,'multiline':False]['text':' dist tensor created is a leaf node, do some operation on dist tensor','line_number':300,'multiline':False]['text':' do some operation on local tensor of the dist tensor','line_number':303,'multiline':False]['text':' call backward directly on torch.Tensor, and see if it works by','line_number':308,'multiline':False]['text':' propagating through dist tensor','line_number':309,'multiline':False]['text':' test the case when grad stride is different from fwd input.','line_number':315,'multiline':False]['text':' The manual change to grad stride leads to the failure of the copy op afterwards.','line_number':322,'multiline':False]['text':' so that we need a try-catch here.','line_number':323,'multiline':False]['text':' test the op produces new dtensor and autograd works','line_number':376,'multiline':False]['text':' test backward new_empty_strided with sharding works correctly','line_number':382,'multiline':False]['text':' Tests that if the output of some dtensor operations  isn't used in any compute,','line_number':393,'multiline':False]['text':' the output should be an AsyncCollectiveTensor (representing the fact that','line_number':394,'multiline':False]['text':' we haven't synced the collective yet).','line_number':395,'multiline':False]['text':' Make sure we haven't synced yet','line_number':402,'multiline':False]['text':' TODO: figure out why this is returning None','line_number':403,'multiline':False]['text':' self.assertTrue(_tensor_needs_wait(dt_out_redistribute))','line_number':404,'multiline':False]['text':' Make sure we haven't synced yet','line_number':414,'multiline':False]['text':' Assert that output is a `AsyncCollectiveTensor`','line_number':419,'multiline':False]['text':' Use the daa, requiring a sync','line_number':423,'multiline':False]['text':' this test ensure end to end from torch.Tensor -> dist tensor -> torch.Tensor works','line_number':432,'multiline':False]['text':' step 1. construct from construct local tensor','line_number':436,'multiline':False]['text':' do some operations on local tensor','line_number':440,'multiline':False]['text':' step 2. create the dist tensor with non leaf local tensor, dist tensor','line_number':442,'multiline':False]['text':' created should also be non leaf node','line_number':443,'multiline':False]['text':' do some random operations on dist tensor','line_number':446,'multiline':False]['text':' step 3. do some operation on local tensor of the dist tensor','line_number':450,'multiline':False]['text':' call backward directly on torch.Tensor, and see if it works by','line_number':455,'multiline':False]['text':' propagating all the way back to the original torch.Tensor','line_number':456,'multiline':False]['text':' modify placements, and dist_tensor's spec should not be changed','line_number':470,'multiline':False]['text':' note that DTensorSpec without real tensor data, so the hash would be the same','line_number':483,'multiline':False]['text':' as long as the mesh, placements and tensor properties are the same','line_number':484,'multiline':False]['text':' change the placements would change the hash','line_number':487,'multiline':False]['text':' construct a cuda device mesh','line_number':531,'multiline':False]['text':' construct from a cpu local tensor with cuda device mesh','line_number':534,'multiline':False]['text':' should automatically convert the dist tensor to cuda','line_number':535,'multiline':False]['text':' construct a cuda device mesh','line_number':584,'multiline':False]['text':' construct a dist tensor on 2d device mesh and test if works','line_number':587,'multiline':False]['text':' if shard on the same tensor dimension','line_number':597,'multiline':False]['text':' we should correctly construct the global tensor size','line_number':598,'multiline':False]['text':' construct a cuda device mesh','line_number':606,'multiline':False]['text':' construct a dist tensor on 3d device mesh and test if works','line_number':609,'multiline':False]['text':' construct a dist tensor on 3d device mesh with some shards on same dim','line_number':617,'multiline':False]['text':' sharding specs and its corresponding local shard offsets','line_number':631,'multiline':False]['text':' loop through all sharding specs and check local shard offsets','line_number':655,'multiline':False]['text':' test dtensor created in submesh, the operation should only','line_number':679,'multiline':False]['text':' be applied to the local shard inside the mesh, not the whole','line_number':680,'multiline':False]['text':' world, so only 0/2 really run the computation','line_number':681,'multiline':False]['text':' test scalar return value','line_number':695,'multiline':False]['text':' equal returns local result','line_number':700,'multiline':False]['text':' test 0-d tensor return value','line_number':708,'multiline':False]['text':' test List[torch.Tensor] return value','line_number':718,'multiline':False]['text':' test redistribute on a submesh','line_number':733,'multiline':False]['text':' Keep everything deterministic.','line_number':768,'multiline':False]['text':' when tensor size is 0, there is no padding needed for all the ranks.','line_number':790,'multiline':False]