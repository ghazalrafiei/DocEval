['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]['text':' Generate tests for BAND.','line_number':70,'multiline':False]['text':' The bit that is set changes in every iteration to check','line_number':71,'multiline':False]['text':' that the output changes accordingly.','line_number':72,'multiline':False]['text':' Generate tests for BOR.','line_number':84,'multiline':False]['text':' These emulate a larger world size per iteration by having every','line_number':85,'multiline':False]['text':' rank contribute multiple values that are pre-OR'ed.','line_number':86,'multiline':False]['text':' Generate tests for XOR.','line_number':98,'multiline':False]['text':' These emulate a larger world size per iteration by having every','line_number':99,'multiline':False]['text':' rank contribute multiple values that are pre-XOR'ed.','line_number':100,'multiline':False]['text':' TODO: add error check testing','line_number':170,'multiline':False]['text':' Every rank is root once','line_number':183,'multiline':False]['text':' Run with 1 input tensor','line_number':185,'multiline':False]['text':' TODO: UCC currently does not support multi tensor input','line_number':190,'multiline':False]['text':' Test overloaded convenience function','line_number':192,'multiline':False]['text':' TODO: test_broadcast_basics_cuda times out locally','line_number':203,'multiline':False]['text':' Single input tests','line_number':208,'multiline':False]['text':' TODO: UCC currently does not support multi tensor input','line_number':219,'multiline':False]['text':' Test overloaded convenience function (defaults to using sum)','line_number':221,'multiline':False]['text':' TODO: test_allreduce_basics_cuda times out locally','line_number':235,'multiline':False]['text':' TODO: Run with N input tensor per rank; for now, UCC only supports single tensor input so N=1','line_number':240,'multiline':False]['text':' TODO: test_reduce_basics_cuda times out locally','line_number':279,'multiline':False]['text':' Preallocate tensors for input/output','line_number':285,'multiline':False]['text':' Issue sends','line_number':289,'multiline':False]['text':' Issue recvs','line_number':296,'multiline':False]['text':' Wait for sends to complete','line_number':303,'multiline':False]['text':' Wait for recvs to complete','line_number':308,'multiline':False]['text':' Test that every output other than our own contains the respective rank','line_number':313,'multiline':False]['text':' TODO: test_barrier_implies_wait fails with numerical mismatch, will investigate later','line_number':319,'multiline':False]['text':' Kick off allreduce operations','line_number':325,'multiline':False]['text':' Note: leak the returned work handle','line_number':330,'multiline':False]['text':' Barrier should ensure all previous work has completed','line_number':333,'multiline':False]['text':' TODO: test_ucc_backend_2gpu_module and test_ucc_backend_4gpu_module','line_number':384,'multiline':False]['text':' require broadcast_coalesced which is not supported by ucc currently','line_number':385,'multiline':False]['text':' Run forward','line_number':425,'multiline':False]['text':' The grads of all parameters should be None at this point.','line_number':428,'multiline':False]['text':' Run backward','line_number':434,'multiline':False]['text':' Now locally unused parameter should have grad updated on all ranks.','line_number':437,'multiline':False]['text':' However the globally unused parameter should still have None grad.','line_number':438,'multiline':False]['text':' Test on CPU','line_number':445,'multiline':False]['text':' Test on GPU','line_number':455,'multiline':False]['text':' TODO: times out','line_number':467,'multiline':False]['text':' TODO: times out','line_number':474,'multiline':False]['text':' TODO: times out','line_number':481,'multiline':False]['text':' TODO: times out','line_number':488,'multiline':False]['text':' Run forward','line_number':514,'multiline':False]['text':' The grads of all parameters should be None at this point.','line_number':517,'multiline':False]['text':' Run backward','line_number':520,'multiline':False]['text':' Now locally unused parameter should have grad updated on all ranks.','line_number':523,'multiline':False]['text':' Test on CPU','line_number':528,'multiline':False]['text':' Test on GPU','line_number':536,'multiline':False]['text':' Run a few iterations where we ignore the output.','line_number':576,'multiline':False]['text':' Run a few iterations where we use the output.','line_number':581,'multiline':False]['text':' Run a few iterations where we ignore the output.','line_number':620,'multiline':False]['text':' Run a few iterations where we use the output.','line_number':625,'multiline':False]['text':' Run with entire batch against single process version','line_number':638,'multiline':False]['text':' Run with partial batch against multi process version','line_number':641,'multiline':False]['text':' Check that the gradients are sparse and identical','line_number':646,'multiline':False]['text':' ensure that all the three models start with the same set of parameters. By default they are randomized on construction','line_number':695,'multiline':False]['text':' run the model for 6 iterations, with a checkpoint in the middle','line_number':720,'multiline':False]['text':' zero out parameters of both DDP and non-DDP models and reload them from the DDP state dict','line_number':723,'multiline':False]['text':' the non-DDP model needs to first remove the prefix of "module." from the DDP state dict','line_number':737,'multiline':False]['text':' re-run the model with the same inputs for 6 iterations with no checkpoint','line_number':746,'multiline':False]['text':' Ensure initialized weights and inputs are identical across processes','line_number':760,'multiline':False]['text':' TODO: backward pass: input tensor has to be dense','line_number':772,'multiline':False]['text':' TODO: backward pass: input tensor has to be dense','line_number':778,'multiline':False]['text':' Test on CPU','line_number':792,'multiline':False]['text':' Register DDP Communication Hook','line_number':797,'multiline':False]['text':' check whether the grads are equal to what then callback returns.','line_number':800,'multiline':False]['text':' without the comm_hook, result would be 0.25 * torch.ones(2, 2).','line_number':801,'multiline':False]['text':' Register a DDP communication hook if any.','line_number':815,'multiline':False]['text':' Get GPU model with simple_hook registered.','line_number':830,'multiline':False]['text':' check whether the grads are equal to what simple_hook's then callback returns.','line_number':833,'multiline':False]['text':' without the comm_hook, result would be 0.25 * torch.ones(2, 2).','line_number':834,'multiline':False]['text':' Run forward','line_number':900,'multiline':False]['text':' Run backward','line_number':903,'multiline':False]['text':' TODO: backward pass: input tensor must be dense','line_number':931,'multiline':False]['text':' Ensure initialized weights and inputs are identical across processes','line_number':941,'multiline':False]['text':' Divide the result by 2 * world_size.','line_number':954,'multiline':False]['text':' Prepare allreduced grad bucket tensors by running an async work.','line_number':957,'multiline':False]['text':' includes reduce, broadcast, all_reduce, all_gather, reduce_scatter, barrier, all_to_all, scatter','line_number':1114,'multiline':False]