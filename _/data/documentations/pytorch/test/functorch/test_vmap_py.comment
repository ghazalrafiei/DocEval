['text':' Owner(s): ["module: functorch"]','line_number':1,'multiline':False]['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':3,'multiline':False]['text':' All rights reserved.','line_number':4,'multiline':False]['text':'','line_number':5,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':6,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':7,'multiline':False]['text':' This is the same thing as','line_number':153,'multiline':False]['text':' def returns_tuple_of_tensors(x):','line_number':154,'multiline':False]['text':'     return x, x','line_number':155,'multiline':False]['text':' should not throw','line_number':167,'multiline':False]['text':' diag_embed requires special testing because it is registered with conditional functionalization.','line_number':182,'multiline':False]['text':' Unsupported view op','line_number':206,'multiline':False]['text':' TODO: find a view op','line_number':212,'multiline':False]['text':' with self.assertRaisesRegex(RuntimeError, msg):','line_number':213,'multiline':False]['text':'     vmap(torch.ravel)(tensor)','line_number':214,'multiline':False]['text':' Don't support non-tensor returns. This is a limitation of vmap;','line_number':222,'multiline':False]['text':' functions that don't return tensors must be special cased','line_number':223,'multiline':False]['text':' Basic test','line_number':228,'multiline':False]['text':' Test that the batch dimension gets permuted to dim 2','line_number':234,'multiline':False]['text':' negative out_dim','line_number':240,'multiline':False]['text':' check that out_dims works on ALL outputs','line_number':246,'multiline':False]['text':' use out_dims with the maximum vmap-able tensor dims (64 dims)','line_number':252,'multiline':False]['text':' test something that is not the identity function','line_number':260,'multiline':False]['text':' Inner vmap has non-zero out_dim','line_number':294,'multiline':False]['text':' all vmaps have non-zero out_dim','line_number':299,'multiline':False]['text':' throwing in some negative out_dims','line_number':304,'multiline':False]['text':' testing fn that isn't the identity','line_number':309,'multiline':False]['text':' Test that we accept out_dims=(1,) for a function with one output.','line_number':320,'multiline':False]['text':' Too many out_dims','line_number':445,'multiline':False]['text':' Too few out_dims','line_number':451,'multiline':False]['text':' TODO(rzou): This error message isn't that great. It comes straight','line_number':458,'multiline':False]['text':' from maybe_wrap_dim. Consider doing a try-catch-(add some context) to','line_number':459,'multiline':False]['text':' the error message in the future in C++','line_number':460,'multiline':False]['text':' Implicit out_dims = 0; vmap will move the batch dim to the front.','line_number':471,'multiline':False]['text':' None in_dim for a Tensor means we don't map over it','line_number':487,'multiline':False]['text':' None in_dim for non-tensor arguments','line_number':492,'multiline':False]['text':' Same in_dim as out_dim, vmap over identity','line_number':512,'multiline':False]['text':' Different in_dim from out_dim, vmap over identity','line_number':517,'multiline':False]['text':' Same in_dim as out_dim, vmap over operation','line_number':526,'multiline':False]['text':' Different in_dim as out_dim, vmap over operation','line_number':530,'multiline':False]['text':' Basic nested test.','line_number':535,'multiline':False]['text':' Single layer of nesting','line_number':559,'multiline':False]['text':' Multiple layers of nesting','line_number':581,'multiline':False]['text':' The following should not throw','line_number':598,'multiline':False]['text':' The following should not throw','line_number':614,'multiline':False]['text':' the following are errors in jax (and will always be errors)','line_number':626,'multiline':False]['text':' The following should not throw','line_number':634,'multiline':False]['text':' the following should not throw','line_number':655,'multiline':False]['text':' The single warning here is the "vmap is experimental"','line_number':665,'multiline':False]['text':' warning, not a warning from the vmap fallback path.','line_number':666,'multiline':False]['text':' NB: One day we will implement a batching rule for torch.atan2.','line_number':671,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':672,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':673,'multiline':False]['text':' with warnings.catch_warnings(record=True) as wa:','line_number':685,'multiline':False]['text':'     with EnableVmapFallbackWarnings():','line_number':686,'multiline':False]['text':'         result = vmap(*vmap_args)(*inputs)','line_number':687,'multiline':False]['text':'     self.assertEqual(len(wa), 2)','line_number':688,'multiline':False]['text':'     self.assertRegex(str(wa[-1].message), FALLBACK_REGEX)','line_number':689,'multiline':False]['text':' We use a dummy function _test_functorch_fallback','line_number':720,'multiline':False]['text':' defined in prim_native_functions.cpp for this','line_number':721,'multiline':False]['text':' nested vmap','line_number':734,'multiline':False]['text':' big batch size (total 10000)','line_number':740,'multiline':False]['text':' TODO: No clue what is wrong here.','line_number':746,'multiline':False]['text':' NB: One day we will implement a batching rule for masked_fill','line_number':749,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':750,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':751,'multiline':False]['text':' NB: One day we will implement a batching rule for torch.var_mean','line_number':770,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':771,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':772,'multiline':False]['text':' fallback correctness on torch.var_mean','line_number':778,'multiline':False]['text':' nested vmap','line_number':783,'multiline':False]['text':' big batch size, nested vmap','line_number':789,'multiline':False]['text':' Test the in-place fallback on an in-place method that takes no','line_number':796,'multiline':False]['text':' additional Tensor arguments. This is the simplest case of the fallback.','line_number':797,'multiline':False]['text':' NB: One day we will implement a batching rule for acos_.','line_number':798,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':799,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':800,'multiline':False]['text':' Single vmap','line_number':807,'multiline':False]['text':' Single vmap + different out_dim produces a view(!)','line_number':814,'multiline':False]['text':' Nested vmap','line_number':821,'multiline':False]['text':' Nested vmap, large batch size','line_number':828,'multiline':False]['text':' NB: One day we will implement a batching rule for atan2_','line_number':836,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':837,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':838,'multiline':False]['text':' Single vmap','line_number':846,'multiline':False]['text':' Nested vmap','line_number':854,'multiline':False]['text':' big batch size (total 10000)','line_number':862,'multiline':False]['text':' ("Fallback isInplaceVmapCompatible check is broken")','line_number':870,'multiline':False]['text':' NB: One day we will implement a batching rule for atan2_','line_number':873,'multiline':False]['text':' If/when we do, this test should be replaced to test the fallback','line_number':874,'multiline':False]['text':' path on another operator to avoid bitrot.','line_number':875,'multiline':False]['text':' op(left, right): All of the levels in right are found in left','line_number':884,'multiline':False]['text':' op(left, right): Some of the levels in right are not found in left','line_number':897,'multiline':False]['text':' FIXME','line_number':928,'multiline':False]['text':' Runs vmap(get_vjp)(v), which should not error out.','line_number':1004,'multiline':False]['text':' The backward formula for convolution returns an undefined','line_number':1005,'multiline':False]['text':' Tensor for grad_bias because the original bias does not exist.','line_number':1006,'multiline':False]['text':'','line_number':1007,'multiline':False]['text':' In the future we'll probably add a batching rule for convolution','line_number':1008,'multiline':False]['text':' backward. When this happens, we should modify this test to use a','line_number':1009,'multiline':False]['text':' different op (and/or create and use a dummy operator) to avoid bitrot.','line_number':1010,'multiline':False]['text':' Case: `0` sized dim.','line_number':1052,'multiline':False]['text':' This test will raise an error if the vmap fallback gets invoked.','line_number':1069,'multiline':False]['text':' Here we test that decomps registered to FuncTorchBatchedDecomposition','line_number':1070,'multiline':False]['text':' are respected by the Python Dispatcher.','line_number':1071,'multiline':False]['text':' Case 1, autocast inside vmapped function','line_number':1090,'multiline':False]['text':' Case 2, autocast decorator inside vmapped function','line_number':1103,'multiline':False]['text':' Case 3, autocast is outside vmapped function','line_number':1116,'multiline':False]['text':' Mix of tensor and non-tensor inputs','line_number':1159,'multiline':False]['text':' Mix of tensor and non-tensor outputs','line_number':1169,'multiline':False]['text':' Tests vmap(op, in_dims, out_dims)(*inputs) by comparing the output to a','line_number':1250,'multiline':False]['text':' (slow) sequential map+stack fallback.','line_number':1251,'multiline':False]['text':'','line_number':1252,'multiline':False]['text':' check_view: Test if the first returned output is a view of the first input','line_number':1253,'multiline':False]['text':' check_propagates_grad: Test if the operation propagates gradients.','line_number':1254,'multiline':False]['text':' Assuming input[0] is a floating-point tensor. Check if the vmap','line_number':1274,'multiline':False]['text':' operation propagates the requires_grad flag to the zeroth output.','line_number':1275,'multiline':False]['text':' Some vmap operators are implemented in a way that assumes that','line_number':1276,'multiline':False]['text':' they are composite with respect to autograd. If the operator ever is','line_number':1277,'multiline':False]['text':' changed to not be composite with respect to autograd, then the','line_number':1278,'multiline':False]['text':' following check should fail.','line_number':1279,'multiline':False]['text':' All tests of TestVmapBase check that the slow vmap fallback is never invoked.','line_number':1295,'multiline':False]['text':' This is so that we can incrementally add batching rules for operators to','line_number':1296,'multiline':False]['text':' replace the slow vmap fallback path for said operators. To skip this check,','line_number':1297,'multiline':False]['text':' please use the allowVmapFallbackUsage decorator.','line_number':1298,'multiline':False]['text':'','line_number':1299,'multiline':False]['text':' NB: Don't add tests to TestVmapBase directly, unless you want them to run','line_number':1300,'multiline':False]['text':' on every subclass of TestVmapBase. Add them to e.g. TestVmapOperators.','line_number':1301,'multiline':False]['text':'','line_number':1302,'multiline':False]['text':' NB: TestVmapBase is a nested class. This prevents test runners from picking','line_number':1303,'multiline':False]['text':' it up and running it.','line_number':1304,'multiline':False]['text':' msg = (','line_number':1321,'multiline':False]['text':'     'Expected the test to not invoke the vmap fallback path, i.e., '','line_number':1322,'multiline':False]['text':'     'all of the operators being tested in this test should have batching '','line_number':1323,'multiline':False]['text':'     'rules implemented. If you are intentionally testing something to '','line_number':1324,'multiline':False]['text':'     'do with the fallback path, use allowVmapFallbackUsage. Otherwise, '','line_number':1325,'multiline':False]['text':'     'please make sure that batching rules are implemented for the '','line_number':1326,'multiline':False]['text':'     'operator(s) being tested.'','line_number':1327,'multiline':False]['text':' )','line_number':1328,'multiline':False]['text':' for captured_warning in wa:','line_number':1336,'multiline':False]['text':'     self.assertNotRegex(str(captured_warning.message), FALLBACK_REGEX, msg)','line_number':1337,'multiline':False]['text':' One day we'll implement a batching rule for torch.var_mean.','line_number':1342,'multiline':False]['text':' When that happens, please change the example to use an','line_number':1343,'multiline':False]['text':' operator that doesn't have a batching rule implemented.','line_number':1344,'multiline':False]['text':' One day we'll implement a batching rule for torch.var_mean.','line_number':1354,'multiline':False]['text':' When that happens, please change the example to use an','line_number':1355,'multiline':False]['text':' operator that doesn't have a batching rule implemented.','line_number':1356,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':1385,'multiline':False]['text':' Doubly nested vmap','line_number':1390,'multiline':False]['text':' test in-place','line_number':1432,'multiline':False]['text':' Some basic tests','line_number':1437,'multiline':False]['text':' Test that the per-examples are contiguous when using torch.contiguous_format','line_number':1444,'multiline':False]['text':' Check that this doesn't crash.','line_number':1466,'multiline':False]['text':' https://github.com/pytorch/functorch/issues/417','line_number':1467,'multiline':False]['text':' Single vmap: op(Tensor, Tensor)','line_number':1488,'multiline':False]['text':' Nested vmap: op(Tensor, Tensor)','line_number':1497,'multiline':False]['text':' Python number overload: op(Tensor, Number)','line_number':1500,'multiline':False]['text':' Single vmap: op(Tensor, Tensor)','line_number':1518,'multiline':False]['text':' Nested vmap: op(Tensor, Tensor)','line_number':1527,'multiline':False]['text':' Python number overload: op(Tensor, Number)','line_number':1532,'multiline':False]['text':' Single vmap: op(Tensor, Tensor)','line_number':1559,'multiline':False]['text':' Single vmap: op(Tensor, Tensor)','line_number':1588,'multiline':False]['text':' Nested vmap: op(Tensor, Tensor)','line_number':1597,'multiline':False]['text':' Python number overload: op(Tensor, Number) (and vice-versa)','line_number':1602,'multiline':False]['text':' Type promotion: op(Logical Scalar Tensor, Logical Scalar Tensor)','line_number':1608,'multiline':False]['text':' Type promotion: op(Tensor, Logical Scalar Tensor) (and vice-versa)','line_number':1613,'multiline':False]['text':' TODO(rzou): fix the following','line_number':1620,'multiline':False]['text':' # Test cross-device scalars','line_number':1621,'multiline':False]['text':' number = get_number(getter)','line_number':1622,'multiline':False]['text':' self._test_unary(lambda t: op(t, number), getter, device='cuda')','line_number':1623,'multiline':False]['text':' self._test_unary(lambda t: op(number, t), getter, device='cuda')','line_number':1624,'multiline':False]['text':' self._test_unary(lambda t: op(t, torch.tensor(number)), getter, device='cuda')','line_number':1625,'multiline':False]['text':' bdim at dim 0 test','line_number':1629,'multiline':False]['text':' bdim at dim -1 test','line_number':1635,'multiline':False]['text':' single vmap test','line_number':1642,'multiline':False]['text':' Each Tensor has shape [B0, 2, 3]; the expressions below','line_number':1644,'multiline':False]['text':' are just to get tensors of different strides that have shape [B0, 2, 3]','line_number':1645,'multiline':False]['text':' contiguous','line_number':1647,'multiline':False]['text':' non-contiguous','line_number':1649,'multiline':False]['text':' non-zero storage offset','line_number':1652,'multiline':False]['text':' non-contiguous strides, zero storage offset','line_number':1655,'multiline':False]['text':' non-contiguous strides, non-zero storage offset','line_number':1658,'multiline':False]['text':' Broadcast','line_number':1667,'multiline':False]['text':' transpose','line_number':1669,'multiline':False]['text':' select','line_number':1671,'multiline':False]['text':' diagonal','line_number':1673,'multiline':False]['text':' strided slice','line_number':1675,'multiline':False]['text':' Nested vmap test','line_number':1678,'multiline':False]['text':' Check that mal-formatted size/strides doesn't crash','line_number':1687,'multiline':False]['text':' All the Sanity check #1{a,b,c} cases check that','line_number':1692,'multiline':False]['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1693,'multiline':False]['text':' doesn't index memory that is out of bounds of xs[i]. This condition','line_number':1694,'multiline':False]['text':' is important to the correctness of the as_strided batching rule','line_number':1695,'multiline':False]['text':' (see NOTE: [When will the as_strided_batching_rule fail?])','line_number':1696,'multiline':False]['text':' Sanity check #1a: The maximum indexable location of','line_number':1698,'multiline':False]['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1699,'multiline':False]['text':' is less than or equal to the maximum indexable location of xs[i].','line_number':1700,'multiline':False]['text':' Sanity check #1b: The min indexable location of','line_number':1712,'multiline':False]['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1713,'multiline':False]['text':' is greater than or equal to the min indexable location of xs[i].','line_number':1714,'multiline':False]['text':' Sanity check #1c:','line_number':1719,'multiline':False]['text':' xs[i] is a zero-dim tensor, but','line_number':1720,'multiline':False]['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1721,'multiline':False]['text':' is not','line_number':1722,'multiline':False]['text':' shape mismatch','line_number':1760,'multiline':False]['text':' left arg is vmapped','line_number':1769,'multiline':False]['text':' right arg is vmapped','line_number':1774,'multiline':False]['text':' both args are vmapped','line_number':1779,'multiline':False]['text':' Quick hack b/c vmap can't accept a list of tensors as an argument','line_number':1789,'multiline':False]['text':' Unsafe view isn't exposed, so we get at it via','line_number':1811,'multiline':False]['text':' vmap(grad(matmul))','line_number':1812,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':1832,'multiline':False]['text':' Doubly nested vmap','line_number':1837,'multiline':False]['text':' correctness tests','line_number':1843,'multiline':False]['text':' check that torch.conj on a non-complex tensor returns the same tensor','line_number':1847,'multiline':False]['text':' check that contiguous returns the original tensor if the per-examples','line_number':1857,'multiline':False]['text':' are already contiguous','line_number':1858,'multiline':False]['text':' tests for torch.split(self, split_size: int, dim)','line_number':1896,'multiline':False]['text':' Single vmap: op(Tensor, Tensor)','line_number':1931,'multiline':False]['text':' Nested vmap: op(Tensor, Tensor)','line_number':1939,'multiline':False]['text':' test number as inputs','line_number':1944,'multiline':False]['text':' Let's test corner case when batch_size is 3 and cross' dim argument is not specified','line_number':1949,'multiline':False]['text':' According to the cross API, dim will be assigned to the first dim with value 3','line_number':1950,'multiline':False]['text':' In this test we ensure that found dim is not batch dim.','line_number':1951,'multiline':False]['text':' shape mismatch','line_number':1976,'multiline':False]['text':' left arg is vmapped','line_number':1985,'multiline':False]['text':' right arg is vmapped','line_number':1990,'multiline':False]['text':' both args are vmapped','line_number':1995,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':2023,'multiline':False]['text':' Doubly nested vmap','line_number':2028,'multiline':False]['text':' test when value is a batched tensor for fill_ operator','line_number':2034,'multiline':False]['text':' Runtime Error is thrown when the tensor being written to isn't being vmapped over','line_number':2040,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':2053,'multiline':False]['text':' Doubly nested vmap','line_number':2059,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':2086,'multiline':False]['text':' Doubly nested vmap','line_number':2091,'multiline':False]['text':' Interesting case #1: Batch dim directly before dim of size 2','line_number':2097,'multiline':False]['text':' Interesting case #2: Batch dim at end of tensor, success cases','line_number':2101,'multiline':False]['text':' view_as_complex requires that the dim with size 2 have stride 1','line_number':2102,'multiline':False]['text':' in order for the view to function property','line_number':2103,'multiline':False]['text':' Interesting case #3: Batch dim at end of tensor, failure cases','line_number':2108,'multiline':False]['text':' Invalid input: no dimension of size 2','line_number':2115,'multiline':False]['text':' Invalid input: Batch dim has size 2, but the logical last dim does','line_number':2122,'multiline':False]['text':' not have size 2','line_number':2123,'multiline':False]['text':' Single batch dim','line_number':2166,'multiline':False]['text':' Multiple batch dims','line_number':2179,'multiline':False]['text':' is_contiguous on empty tensor is True','line_number':2192,'multiline':False]['text':' is_contiguous with other memory formats','line_number':2201,'multiline':False]['text':' unsqueeze dim 0','line_number':2218,'multiline':False]['text':' unsqueeze last dim (positive)','line_number':2222,'multiline':False]['text':' unsqueeze last dim (negative)','line_number':2226,'multiline':False]['text':' nested vmaps','line_number':2230,'multiline':False]['text':' bdims in canonical order','line_number':2237,'multiline':False]['text':' wild bdims','line_number':2241,'multiline':False]['text':' movedim(tensor, int, int) variant','line_number':2252,'multiline':False]['text':' movedim(tensor, intlist, intlist) variant','line_number':2259,'multiline':False]['text':' shape mismatch','line_number':2272,'multiline':False]['text':' left arg is vmapped','line_number':2281,'multiline':False]['text':' right arg is vmapped','line_number':2286,'multiline':False]['text':' both args are vmapped','line_number':2291,'multiline':False]['text':' shape mismatch','line_number':2302,'multiline':False]['text':' left arg is vmapped','line_number':2311,'multiline':False]['text':' right arg is vmapped','line_number':2316,'multiline':False]['text':' both args are vmapped','line_number':2321,'multiline':False]['text':' Empty is non-deterministic so we just check that the shape of the','line_number':2340,'multiline':False]['text':' output tensor is what we expect and that the vmap fallback isn't used.','line_number':2341,'multiline':False]['text':' Empty is non-deterministic so we just check that the size and shape','line_number':2356,'multiline':False]['text':' of the output are what we expect and that the vmap fallback isn't used','line_number':2357,'multiline':False]['text':' contiguous case','line_number':2380,'multiline':False]['text':' expanded','line_number':2384,'multiline':False]['text':' some of these cases are pretty strange, just verifying that if','line_number':2388,'multiline':False]['text':' empty_strided allows them then BatchedTensor.new_empty_strided','line_number':2389,'multiline':False]['text':' can as well','line_number':2390,'multiline':False]['text':' Quick hack b/c vmap can't accept a list of tensors as an argument','line_number':2427,'multiline':False]['text':' These tests cannot be used with an operator that requires more','line_number':2456,'multiline':False]['text':' than 1 dimension after batching.','line_number':2457,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':2488,'multiline':False]['text':' Doubly nested vmap','line_number':2495,'multiline':False]['text':' Single vmap, various in_dims / out_dims','line_number':2523,'multiline':False]['text':' Doubly nested vmap','line_number':2529,'multiline':False]['text':' tests for torch.tensor_split(self, indices_or_sections: int, dim)','line_number':2630,'multiline':False]['text':' tests for torch.tensor_split(self, indices_or_sections: List[int], dim)','line_number':2638,'multiline':False]['text':' tests for torch.split(self, split_size: int, dim)','line_number':2651,'multiline':False]['text':' tests for torch.split(self, split_size: List[int], dim)','line_number':2659,'multiline':False]['text':' Special case: scalar tensor','line_number':2689,'multiline':False]['text':' also test some casting methods','line_number':2728,'multiline':False]['text':' We should error out if the view would produce an incorrect result','line_number':2764,'multiline':False]['text':' We should error out if the view would produce an incorrect result','line_number':2779,'multiline':False]['text':' (torch.nn.ConvTranspose2d, torch.conv_transpose2d, [2, 4, 15, 20])','line_number':2799,'multiline':False]['text':' fn: Single Input/Single Output','line_number':2897,'multiline':False]['text':' fn: Nested Input/Single Output','line_number':2906,'multiline':False]['text':' fn: Single Input/Nested Output','line_number':2916,'multiline':False]['text':' fn: Nested Input/Nested Output (first tensor is not vmapped).','line_number':2925,'multiline':False]['text':' fn: Nested Input/Nested Output (first argument is not a Tensor).','line_number':2936,'multiline':False]['text':' Incorrect `chunk_size`','line_number':2971,'multiline':False]['text':' Incorrect `out_dims`','line_number':2978,'multiline':False]['text':' fn: Single Input/Single Output','line_number':2992,'multiline':False]['text':' fn: Nested Input/Single Output','line_number':3000,'multiline':False]['text':' fn: Single Input/Nested Output','line_number':3009,'multiline':False]['text':' fn: Nested Input/Nested Output','line_number':3017,'multiline':False]['text':' Tests batched gradient computation of outputs = op(*args, **kwargs)','line_number':3076,'multiline':False]['text':' by comparing it to a sequential map+stack fallback.','line_number':3077,'multiline':False]['text':'','line_number':3078,'multiline':False]['text':' output_process_fn: a function that maps the outputs to the part','line_number':3079,'multiline':False]['text':'       that should be differentiated.','line_number':3080,'multiline':False]['text':' batch_size: the batch dim size for the batched grad','line_number':3081,'multiline':False]['text':' Tests batched second grad computation of outputs = op(*args, **kwargs).','line_number':3097,'multiline':False]['text':' by comparing it to a sequential map+stack fallback.','line_number':3098,'multiline':False]['text':'','line_number':3099,'multiline':False]['text':' output_process_fn: a function that maps the outputs to the part','line_number':3100,'multiline':False]['text':'       that should be differentiated.','line_number':3101,'multiline':False]['text':' batch_size: the batch dim size for the batched grad','line_number':3102,'multiline':False]['text':'','line_number':3103,'multiline':False]['text':' NB: we only test computing batched gradients in the second gradient','line_number':3104,'multiline':False]['text':' computation. One specific use case that does this is computing the hessian','line_number':3105,'multiline':False]['text':' matrix of a scalar-valued function; this is useful in Bayesian Logistic','line_number':3106,'multiline':False]['text':' Regression.','line_number':3107,'multiline':False]['text':' It might be useful to have a test that computes batched first gradients and','line_number':3108,'multiline':False]['text':' then uses those to compute batched second gradients in the future.','line_number':3109,'multiline':False]['text':' Same thing as summing together all of the outputs and calling .backward()','line_number':3116,'multiline':False]['text':' Check that there is no runtime error, exactness tests are done with opinfo','line_number':3300,'multiline':False]['text':' Make sure the function is non-trivially twice differentiable','line_number':3320,'multiline':False]['text':' Make sure the function is non-trivially twice differentiable','line_number':3334,'multiline':False]['text':' TODO: enable this when we get a bit closer to getting torch.vmap x torch.compile working.','line_number':3398,'multiline':False]['text':' @markDynamoStrictTest','line_number':3399,'multiline':False]['text':' NB: This test assumes that the first argument is being modified.','line_number':3414,'multiline':False]['text':' This is OK because it's what every other OpInfo-based test assumes,','line_number':3415,'multiline':False]['text':' but it is going to need a more robust solution eventually.','line_number':3416,'multiline':False]['text':' Check that we correctly raise an error when vmap is impossible','line_number':3418,'multiline':False]['text':' on the in-place operation','line_number':3419,'multiline':False]['text':' Error inputs check','line_number':3435,'multiline':False]['text':' Sample inputs check','line_number':3446,'multiline':False]['text':' Take too long with reference inputs','line_number':3448,'multiline':False]['text':' Atleast one tensor required for vmap.','line_number':3470,'multiline':False]['text':' -------------------- ALLOWED FAILURES --------------------------------','line_number':3491,'multiline':False]['text':' These are things that we either cannot fix or are not actually problems','line_number':3492,'multiline':False]['text':' dynamic mask','line_number':3496,'multiline':False]['text':' dynamic mask','line_number':3497,'multiline':False]['text':' works, can't check against for loop because of randomness inconsistency','line_number':3498,'multiline':False]['text':' randomness','line_number':3499,'multiline':False]['text':' randomness','line_number':3500,'multiline':False]['text':' dynamic op','line_number':3501,'multiline':False]['text':' dynamic op','line_number':3502,'multiline':False]['text':' dynamic op','line_number':3503,'multiline':False]['text':' dynamic op','line_number':3504,'multiline':False]['text':' returns a boolean','line_number':3505,'multiline':False]['text':' randomness is tested separately','line_number':3506,'multiline':False]['text':' randomness is tested separately','line_number':3507,'multiline':False]['text':' randomness is tested separately','line_number':3508,'multiline':False]['text':' randomness is tested separately','line_number':3509,'multiline':False]['text':' randomness is tested separately','line_number':3510,'multiline':False]['text':' randomness is tested separately','line_number':3511,'multiline':False]['text':' randomness is tested separately','line_number':3512,'multiline':False]['text':' randomness','line_number':3513,'multiline':False]['text':' we only support some cases','line_number':3514,'multiline':False]['text':' randomness','line_number':3515,'multiline':False]['text':' randomness','line_number':3516,'multiline':False]['text':' randomness','line_number':3517,'multiline':False]['text':' randomness','line_number':3518,'multiline':False]['text':' randomness','line_number':3519,'multiline':False]['text':' Our test runner can't handle this; manual test exists','line_number':3520,'multiline':False]['text':' no batching rule implemented, default doesnt work','line_number':3521,'multiline':False]['text':' empty tensor data is garbage so it's hard to make comparisons with it','line_number':3522,'multiline':False]['text':' randomness','line_number':3523,'multiline':False]['text':' randomness','line_number':3524,'multiline':False]['text':' random operation','line_number':3525,'multiline':False]['text':' random operation','line_number':3526,'multiline':False]['text':' sparse','line_number':3527,'multiline':False]['text':' sparse','line_number':3528,'multiline':False]['text':' Not composable autograd.Function','line_number':3529,'multiline':False]['text':' not always return the same result for the same input, see test_linalg_eigh for manual test','line_number':3531,'multiline':False]['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':3532,'multiline':False]['text':' ----------------------------------------------------------------------','line_number':3533,'multiline':False]['text':' ---------------------------- BUGS ------------------------------------','line_number':3535,'multiline':False]['text':' entries in here don't work and need to be fixed.','line_number':3536,'multiline':False]['text':' Each one of these is a bug','line_number':3537,'multiline':False]['text':' Exception not raised on error input','line_number':3540,'multiline':False]['text':' Exception not raised on error input','line_number':3541,'multiline':False]['text':' RuntimeError: Tensor must have a last dimension with stride 1','line_number':3543,'multiline':False]['text':' data_ptr','line_number':3544,'multiline':False]['text':' expected Tensor as element 0 in argument 0, but got tuple','line_number':3545,'multiline':False]['text':' data-dependent control flow error','line_number':3546,'multiline':False]['text':' embedding renorm vmap inplace incompatible','line_number':3547,'multiline':False]['text':' Batching rule not implemented for aten::narrow.Tensor','line_number':3548,'multiline':False]['text':' required rank 4 tensor to use channels_last format','line_number':3550,'multiline':False]['text':' NYI: querying is_contiguous inside of vmap','line_number':3564,'multiline':False]['text':' NYI: querying is_contiguous inside of vmap','line_number':3565,'multiline':False]['text':' NYI: querying is_contiguous inside of vmap','line_number':3566,'multiline':False]['text':' TypeError: object of type 'bool' has no len(); likely testrunner problem','line_number':3567,'multiline':False]['text':' NYI: querying is_contiguous inside of vmap','line_number':3568,'multiline':False]['text':' NYI: querying is_contiguous inside of vmap','line_number':3569,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':3570,'multiline':False]['text':' TypeError: expected Tensor as element 0 in argument 0, but got NotImplementedType','line_number':3572,'multiline':False]['text':' RuntimeError: Batching rule not implemented for aten::moveaxis.int;','line_number':3574,'multiline':False]['text':' the fallback path doesn't work on out= or view ops.','line_number':3575,'multiline':False]['text':' RuntimeError: NYI: querying is_contiguous inside of vmap for','line_number':3577,'multiline':False]['text':' memory_format other than torch.contiguous_format','line_number':3578,'multiline':False]['text':' RuntimeError: NYI: Tensor.clone(memory_format) inside vmap is only supported','line_number':3580,'multiline':False]['text':' with memory_format torch.preserve_format or torch.contiguous_format (got ChannelsLast)','line_number':3581,'multiline':False]['text':' RuntimeError: When vmap-ing torch.nn.functional.one_hot,','line_number':3583,'multiline':False]['text':' please provide an explicit positive num_classes argument.','line_number':3584,'multiline':False]['text':' RuntimeError: Expected all tensors to be on the same device,','line_number':3586,'multiline':False]['text':' but found at least two devices, cuda:0 and cpu!','line_number':3587,'multiline':False]['text':' RuntimeError: aten::_flash_attention_forward hit the vmap fallback which is currently disabled','line_number':3595,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':3599,'multiline':False]['text':' The following is often flaky, but just on windows.','line_number':3604,'multiline':False]['text':' We should investigate if it's actually a problem or not.','line_number':3605,'multiline':False]['text':' RuntimeError: Batch norm got a batched tensor as input while the running_mean or running_var,','line_number':3612,'multiline':False]['text':' which will be updated in place, were not batched.','line_number':3613,'multiline':False]['text':' Exception not raised on error input','line_number':3616,'multiline':False]['text':' Exception not raised on error input','line_number':3617,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':3620,'multiline':False]['text':' RuntimeError: output with shape [4, 4] doesn't match the broadcast shape [1, 4, 4]','line_number':3623,'multiline':False]['text':' outputs ints','line_number':3628,'multiline':False]['text':' TypeError: expected Tensor as element 0 in argument 0, but got float','line_number':3630,'multiline':False]['text':' needs to be fixed','line_number':3634,'multiline':False]['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':3649,'multiline':False]['text':' Batch norm got a batched tensor as input while the running_mean or running_var,','line_number':3651,'multiline':False]['text':' which will be updated in place, were not batched.','line_number':3652,'multiline':False]['text':' `index_put` OpInfo in pytorch/pytorch has','line_number':3660,'multiline':False]['text':' masked index as input which is not supported','line_number':3661,'multiline':False]['text':' TypeError: expected Tensor as element 0 in argument 0, but got float','line_number':3677,'multiline':False]['text':' Exception not raised on error input','line_number':3679,'multiline':False]['text':' Exception not raised on error input','line_number':3680,'multiline':False]['text':' works, can't check against for loop because of randomness inconsistency','line_number':3683,'multiline':False]['text':' randomness','line_number':3684,'multiline':False]['text':' randomness','line_number':3685,'multiline':False]['text':' outputs ints','line_number':3686,'multiline':False]['text':' Batching rule not implemented for aten::narrow.Tensor','line_number':3721,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':3766,'multiline':False]['text':' One or more of the overload doesn't have a Batch rule.','line_number':3769,'multiline':False]['text':' RuntimeError: Expected all tensors to be on the same device,','line_number':3771,'multiline':False]['text':' but found at least two devices, cuda:0 and cpu!','line_number':3772,'multiline':False]['text':' aten::argsort.stable hit the vmap fallback which is currently disabled','line_number':3774,'multiline':False]['text':' aten::searchsorted.Scalar hit the vmap fallback which is currently disabled','line_number':3775,'multiline':False]['text':' needs to be fixed','line_number':3778,'multiline':False]['text':' linalg_svd returns a tuple of three tensors, (U, S, Vh).','line_number':3816,'multiline':False]['text':' Given the same input, it may return different tensors,','line_number':3817,'multiline':False]['text':' because svd isn't unique. To test that the svd is correct, we multiply','line_number':3818,'multiline':False]['text':' U @ diag(S) @ Vh and check that the output from vmap matches the','line_number':3819,'multiline':False]['text':' output from a for-loop.','line_number':3820,'multiline':False]['text':' linalg_svd returns two tensors, (Q, L).','line_number':3837,'multiline':False]['text':' Given the same input, it may return different tensors,','line_number':3838,'multiline':False]['text':' because the eig decomposition isn't unique.','line_number':3839,'multiline':False]['text':' To test that eigh is correct, we multiply','line_number':3840,'multiline':False]['text':' Q @ diag(L) @ Qh and check that the output from vmap matches the','line_number':3841,'multiline':False]['text':' output from a for-loop.','line_number':3842,'multiline':False]['text':' There's no OpInfo for this','line_number':3860,'multiline':False]['text':' There's no OpInfo for these tests','line_number':3869,'multiline':False]['text':' negative dim','line_number':3874,'multiline':False]['text':' self batched, self logical rank 1, index logical rank 1','line_number':3882,'multiline':False]['text':' self batched, self logical rank 1, index logical rank 0','line_number':3890,'multiline':False]['text':' self not batched, self logical rank 0, index logical rank 1','line_number':3898,'multiline':False]['text':' self not batched, self logical rank 0, index logical rank 0','line_number':3906,'multiline':False]['text':' self not batched, self logical rank 0, index logical rank 1','line_number':3914,'multiline':False]['text':' self not batched, self logical rank 0, index logical rank 0','line_number':3922,'multiline':False]['text':' self batched, self logical rank > 1, index logical rank 0','line_number':3930,'multiline':False]['text':' There's no OpInfo for fill_.Tensor, so here's an extra test for it.','line_number':3941,'multiline':False]['text':' vfdev-5: Probably, we can remove this line. Flake8 reported as unused','line_number':4022,'multiline':False]['text':' test = functools.partial(_vmap_test, check_propagates_grad=False)','line_number':4023,'multiline':False]['text':' todo(chilli): test these better','line_number':4028,'multiline':False]['text':' Not testing correctness, just that they run','line_number':4029,'multiline':False]['text':' indexing innermost dim','line_number':4073,'multiline':False]['text':' indexing middle dim','line_number':4083,'multiline':False]['text':' indexing with slices','line_number':4093,'multiline':False]['text':' index_put_','line_number':4102,'multiline':False]['text':' boolean mask','line_number':4115,'multiline':False]['text':' triggers some composite compliance problem','line_number':4215,'multiline':False]['text':' simple reference implementation for comparison','line_number':4248,'multiline':False]['text':' can accept vector inputs','line_number':4272,'multiline':False]['text':' can accept vector inputs','line_number':4273,'multiline':False]['text':' can accept vector inputs','line_number':4274,'multiline':False]['text':' can accept vector inputs','line_number':4275,'multiline':False]['text':' accepts list of tensor inputs, has its own special test','line_number':4276,'multiline':False]['text':' throws in vmap on CUDA','line_number':4278,'multiline':False]['text':' IndexError: Dimension out of range (expected to be in range of [-1, 0], but got -2)','line_number':4279,'multiline':False]['text':' https://github.com/pytorch/pytorch/runs/8110653462?check_suite_focus=true','line_number':4280,'multiline':False]['text':' but it passes locally','line_number':4281,'multiline':False]['text':' using the sample input avoids numerical inconsistency issues','line_number':4290,'multiline':False]['text':' square inputs are more likely to pass linalg checks','line_number':4297,'multiline':False]['text':' special exception for first and last tensors so making giving 3 items avoids special cases','line_number':4303,'multiline':False]['text':' square inputs are more likely to pass linalg checks','line_number':4308,'multiline':False]['text':' Note: These are not a complete set of tests for all possible functions calling 'vmap_check_escaped'','line_number':4326,'multiline':False]['text':' OpInfo generates test with repeated samples in batch dim.','line_number':4373,'multiline':False]['text':' Thus we test explicitly with different samples across a batch.','line_number':4374,'multiline':False]['text':' for the always batched as first dim dummy argument','line_number':4435,'multiline':False]['text':' needs a special case because randperm doesn't take a batch size','line_number':4484,'multiline':False]['text':' RNG differs between eager and via dynamo trace on CUDA','line_number':4506,'multiline':False]['text':' RNG differs between eager and via dynamo trace on CUDA','line_number':4513,'multiline':False]['text':' Check that the randomness is within bounds...','line_number':4541,'multiline':False]['text':' ideally this is close to 0.5','line_number':4542,'multiline':False]['text':' I have no clue how to actually test correctness of alpha dropout because the docs','line_number':4573,'multiline':False]['text':' seem wrong: https://github.com/pytorch/pytorch/issues/74004','line_number':4574,'multiline':False]['text':' Check the "feature" pattern','line_number':4609,'multiline':False]['text':' I have no clue how to actually test correctness of alpha dropout because the docs','line_number':4646,'multiline':False]['text':' seem wrong: https://github.com/pytorch/pytorch/issues/74004','line_number':4647,'multiline':False]['text':' Check the "feature" pattern','line_number':4649,'multiline':False]['text':' RNG differs between eager and via dynamo trace on CUDA','line_number':4702,'multiline':False]['text':' RNG differs between eager and via dynamo trace on CUDA','line_number':4712,'multiline':False]['text':' because of in place updates, clone inputs','line_number':4741,'multiline':False]['text':' bug in pytorch, normal_ on views doesn't work','line_number':4765,'multiline':False]['text':' because of in place updates, clone inputs','line_number':4786,'multiline':False]['text':' [B0, B, N]','line_number':4943,'multiline':False]['text':' [N]','line_number':4945,'multiline':False]['text':' [B0, N] or [B, N]','line_number':4947,'multiline':False]['text':' gets to correct final size because using negative indices','line_number':4953,'multiline':False]['text':' checks on behavior are above, this just checks that jacfwd respects','line_number':5058,'multiline':False]['text':' the randomness param','line_number':5059,'multiline':False]['text':' x isn't batched so use bernoulli since it doesn't do inplace randomness','line_number':5065,'multiline':False]['text':' output from dropout should be a Tensor[B, 1, 3] (B=3)','line_number':5078,'multiline':False]['text':' We just verify that this doesn't raise an error for','line_number':5081,'multiline':False]['text':' `same` and `different` randomness.','line_number':5082,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issues/92283','line_number':5083,'multiline':False]['text':' dims should be something like [5, None, 10], with None indicating that a','line_number':5230,'multiline':False]['text':' random ragged structure should be used','line_number':5231,'multiline':False]['text':' Creates an NT matching another NT's number of components and','line_number':5241,'multiline':False]['text':' shape / ragged structure for all dims specified to be -1.','line_number':5242,'multiline':False]['text':' Different nested structure, same other dims','line_number':5315,'multiline':False]['text':' Same nested structure, different other dims','line_number':5324,'multiline':False]['text':' .shape calls don't work on NTs','line_number':5333,'multiline':False]['text':' TODO: Fix this somehow?','line_number':5334,'multiline':False]