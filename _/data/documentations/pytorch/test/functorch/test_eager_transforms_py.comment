['text':' Owner(s): ["module: functorch"]','line_number':1,'multiline':False]['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':3,'multiline':False]['text':' All rights reserved.','line_number':4,'multiline':False]['text':'','line_number':5,'multiline':False]['text':' This source code is licensed under the BSD-style license found in the','line_number':6,'multiline':False]['text':' LICENSE file in the root directory of this source tree.','line_number':7,'multiline':False]['text':' NB: numpy is a testing dependency!','line_number':52,'multiline':False]['text':' noqa: F401','line_number':59,'multiline':False]['text':' TestCase for _slice_argnums, an important helper function','line_number':67,'multiline':False]['text':' this makes it so the function from make_functional and this call have the same signature','line_number':164,'multiline':False]['text':' this makes it so the function from make_functional and this call have the same signature','line_number':177,'multiline':False]['text':' TODO: https://github.com/zou3519/functorch/issues/12','line_number':630,'multiline':False]['text':' Check list output','line_number':690,'multiline':False]['text':' Check dict output','line_number':696,'multiline':False]['text':' The only differential part of g is x ** 3','line_number':969,'multiline':False]['text':' This one is a little weird...','line_number':1032,'multiline':False]['text':' grad differentiates w.r.t. arg 0 by default','line_number':1064,'multiline':False]['text':' NB: the logic to check ctx.save_for_forward happens','line_number':1102,'multiline':False]['text':'     before we reach this!','line_number':1103,'multiline':False]['text':' TODO(soulitzer): https://github.com/pytorch/pytorch/issues/97827','line_number':1140,'multiline':False]['text':' grad differentiates w.r.t. arg 0 by default','line_number':1166,'multiline':False]['text':' regular autograd x vjp','line_number':1198,'multiline':False]['text':' TODO: support torch.autograd.function.once_differentiable','line_number':1207,'multiline':False]['text':' (or, if impossible, figure out how to raise a nice error)','line_number':1208,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/90224','line_number':1209,'multiline':False]['text':' grad x vjp','line_number':1214,'multiline':False]['text':' TODO: Check if the rtol is a problem','line_number':1544,'multiline':False]['text':' TODO: Check if the rtol is a problem','line_number':1587,'multiline':False]['text':' TODO: Check if the rtol is a problem','line_number':1595,'multiline':False]['text':' TODO: Check if the rtol is a problem','line_number':1602,'multiline':False]['text':' Create our inputs...','line_number':1626,'multiline':False]['text':' Construct our module','line_number':1633,'multiline':False]['text':' Tensor[2, 4] -> Tensor[3, 1]','line_number':1707,'multiline':False]['text':' Check list output','line_number':1907,'multiline':False]['text':' Check dict output','line_number':1911,'multiline':False]['text':' zero-dim output','line_number':2154,'multiline':False]['text':' zero-dim input','line_number':2162,'multiline':False]['text':' Mixed zero-dim input / zero-dim output','line_number':2169,'multiline':False]['text':' x is differentiable','line_number':2204,'multiline':False]['text':' testing tuple of argnums with the example that raised this issue originally','line_number':2208,'multiline':False]['text':' top left corner','line_number':2217,'multiline':False]['text':' bottom right corner','line_number':2218,'multiline':False]['text':' With chunk_size=1, we shouldn't `vmap` and hence not be limited','line_number':2263,'multiline':False]['text':' by it's constraints.','line_number':2264,'multiline':False]['text':' Function with Dynamic Op in Backward.','line_number':2267,'multiline':False]['text':' This should cause jacrev/vmap(vjp) to fail.','line_number':2268,'multiline':False]['text':' dynamic op in backward pass.','line_number':2280,'multiline':False]['text':' With `chunk_size=1`, we don't use vmap. So the following should work.','line_number':2287,'multiline':False]['text':' Should fail with `chunk_size=2`.','line_number':2293,'multiline':False]['text':' Verify complex input raises error','line_number':2300,'multiline':False]['text':' C -> C','line_number':2301,'multiline':False]['text':' Verify complex output raises error','line_number':2313,'multiline':False]['text':' R -> C','line_number':2314,'multiline':False]['text':' output unrelated to one input','line_number':2368,'multiline':False]['text':' output unrelated to all inputs','line_number':2376,'multiline':False]['text':' Test case from:','line_number':2386,'multiline':False]['text':' https://github.com/pytorch/functorch/issues/597','line_number':2387,'multiline':False]['text':' Check list output','line_number':2591,'multiline':False]['text':' Check dict output','line_number':2596,'multiline':False]['text':' Sanity check. We don't really assume this anywhere so','line_number':2660,'multiline':False]['text':' it's fine if this breaks one day.','line_number':2661,'multiline':False]['text':' Should not error','line_number':2745,'multiline':False]['text':' The tests here follow the cases in [Forward Grad View/inplace]','line_number':2850,'multiline':False]['text':' https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/autograd_meta.cpp#L18-L43','line_number':2851,'multiline':False]['text':' Case 1 in [Forward Grad View/inplace]','line_number':2854,'multiline':False]['text':' Case 2 in [Forward Grad View/inplace]','line_number':2883,'multiline':False]['text':' with view, propagate from view to base','line_number':2893,'multiline':False]['text':' Case 3 in [Forward Grad View/inplace]','line_number':2921,'multiline':False]['text':' Case 3: with view, propagate from base to view','line_number':2931,'multiline':False]['text':' Case 4 in [Forward Grad View/inplace]','line_number':2950,'multiline':False]['text':' Changes on the view must propagate to its base. Also:','line_number':2955,'multiline':False]['text':' - x is a regular Tensor','line_number':2956,'multiline':False]['text':' - y is a dual tensor','line_number':2957,'multiline':False]['text':' Case 5 in [Forward Grad View/inplace]','line_number':2982,'multiline':False]['text':' Changes on the base must propagate on all its views. Also:','line_number':2987,'multiline':False]['text':' - x is a regular Tensor','line_number':2988,'multiline':False]['text':' - y is a dual tensor','line_number':2989,'multiline':False]['text':' Use for testing miscellaneous helper functions','line_number':3012,'multiline':False]['text':' Can't use self.assertEqual because that relies on TLS','line_number':3065,'multiline':False]['text':' that is not available in multithread autograd','line_number':3066,'multiline':False]['text':' The override can be literally anything','line_number':3095,'multiline':False]['text':' The override can be literally anything','line_number':3116,'multiline':False]['text':' grad_input None case','line_number':3134,'multiline':False]['text':' grad_input has bdim, input does not have bdim','line_number':3140,'multiline':False]['text':' grad_input does not have bdim, input has bdim','line_number':3149,'multiline':False]['text':' This can happen if the user returns a fresh Tensor from the backward pass','line_number':3150,'multiline':False]['text':' that is unrelated to the input','line_number':3151,'multiline':False]['text':' grad_input has bdim, input has bdim','line_number':3160,'multiline':False]['text':' functorch version of the API is deprecated','line_number':3175,'multiline':False]['text':' the non-functorch version is not deprecated','line_number':3179,'multiline':False]['text':' Some of these pass, some of these don't','line_number':3184,'multiline':False]['text':' functorch version of the API is deprecated','line_number':3193,'multiline':False]['text':' the non-functorch version is not deprecated','line_number':3197,'multiline':False]['text':' TODO: there's a very interesting error message when the following','line_number':3249,'multiline':False]['text':' is on CPU','line_number':3250,'multiline':False]['text':' Honestly IDK what the result here is... but at least it runs','line_number':3284,'multiline':False]['text':' it is redundant to run this test twice on a machine that has GPUs','line_number':3318,'multiline':False]['text':' Some of these pass, some of these don't','line_number':3430,'multiline':False]['text':' state in func.names_map','line_number':3849,'multiline':False]['text':' TODO: should replace with F.mse_loss','line_number':3903,'multiline':False]['text':' Select amplitude and phase for the task','line_number':3913,'multiline':False]['text':' Compute with vmap+grad','line_number':3955,'multiline':False]['text':' Compute without vmap+grad','line_number':3960,'multiline':False]['text':' TODO: there appears to be precision issues for float32','line_number':3973,'multiline':False]['text':' TODO: We don't support inplace relu?','line_number':3976,'multiline':False]['text':' real example uses batch norm but it's numerically unstable in the first','line_number':3982,'multiline':False]['text':' iteration, when near 0, and won't produce same gradients. Uses group norm instead','line_number':3983,'multiline':False]['text':' Get some sample inputs...','line_number':4031,'multiline':False]['text':' compute with vmap + grad','line_number':4037,'multiline':False]['text':' compute without vmap + grad','line_number':4043,'multiline':False]['text':' Get some sample inputs...','line_number':4073,'multiline':False]['text':' compute some per sample grads with vmap + grad','line_number':4077,'multiline':False]['text':' compute some per sample grads without vmap + grad','line_number':4080,'multiline':False]['text':' This example mimics what a user might do when trying to find the optimal learning rate. They would','line_number':4252,'multiline':False]['text':' want to run a bunch of models with the same behavior (including the same dropout!) and have them','line_number':4253,'multiline':False]['text':' each run with different learning rates. Specifically, this is an example of using same randomness with vmap','line_number':4254,'multiline':False]['text':' NB: return looks weird because torch.vmap must return Tensors','line_number':4290,'multiline':False]['text':' have same initialization','line_number':4298,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':4315,'multiline':False]['text':' avoid cross batch reductions for for loop comparison','line_number':4323,'multiline':False]['text':' Right now the flavor of functionalize that also removes view ops','line_number':4378,'multiline':False]['text':' isn't being used with vmap','line_number':4379,'multiline':False]['text':' That's because {view}_copy ops don't have batching rules yet','line_number':4380,'multiline':False]['text':' (although we should probably fix that)','line_number':4381,'multiline':False]['text':' Check that outputs are the same','line_number':4383,'multiline':False]['text':' Inputs might have been mutated by f: check that they were mutated properly','line_number':4387,'multiline':False]['text':' See https://github.com/pytorch/functorch/issues/780','line_number':4423,'multiline':False]['text':' See Note [Fix vmap slice_scatter]','line_number':4449,'multiline':False]['text':' Ensure functionalize works with List[Optional[Tensor]] arguments.','line_number':4452,'multiline':False]['text':' See the fix / discussion at https://github.com/pytorch/pytorch/pull/76085','line_number':4453,'multiline':False]['text':' Ensure grad(functionalize(f)) works','line_number':4474,'multiline':False]['text':' TODO: move this test into test_fake_tensor.py','line_number':4511,'multiline':False]['text':' once functionalize() can be used in core tests.','line_number':4512,'multiline':False]['text':' There's a copy_ in the graph, because the input (x) was mutated.','line_number':4531,'multiline':False]['text':' To preserve semantics, functionalize() needs to propagate the mutation.','line_number':4532,'multiline':False]['text':' at::index has OptionalTensorList arguments,','line_number':4668,'multiline':False]['text':' test that here','line_number':4669,'multiline':False]['text':' See https://github.com/pytorch/pytorch/pull/77846','line_number':4689,'multiline':False]['text':' unnecessary, just here to test the dispatch','line_number':4735,'multiline':False]['text':' unnecessary, just here to test the dispatch','line_number':4758,'multiline':False]['text':' higher order grad. Requires a non-linearity','line_number':4809,'multiline':False]['text':' torch.compile is not supported on Windows','line_number':4876,'multiline':False]['text':' Triton only supports GPU with SM70 or later.','line_number':4877,'multiline':False]['text':' The model and inputs are a smaller version','line_number':4880,'multiline':False]['text':' of code at benchmark repo:','line_number':4881,'multiline':False]['text':' https://github.com/pytorch/benchmark/blob/main/userbenchmark/functorch/vmap_hessian_fc.py','line_number':4882,'multiline':False]['text':' torch.compile is not supported on Windows','line_number':4907,'multiline':False]