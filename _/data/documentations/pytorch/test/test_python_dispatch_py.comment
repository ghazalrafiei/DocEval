['text':' Owner(s): ["module: __torch_dispatch__"]','line_number':1,'multiline':False]['text':' noqa: F403','line_number':12,'multiline':False]['text':' Example 1','line_number':54,'multiline':False]['text':' Now we are secretly making the operator a view op so autograd needs to know how','line_number':58,'multiline':False]['text':' to handle it','line_number':59,'multiline':False]['text':' RuntimeError: impl("aten::neg", ...):','line_number':64,'multiline':False]['text':' Explicitly provided namespace (aten) in operator name does not match ...','line_number':65,'multiline':False]['text':' Example 2','line_number':72,'multiline':False]['text':' torch.ops.aten.mul.Tensor','line_number':76,'multiline':False]['text':' Assert that a user can't override the behavior of a (ns, op, dispatch_key)','line_number':82,'multiline':False]['text':' combination if someone overrided the behavior for the same before them','line_number':83,'multiline':False]['text':' Validate that lib2 is not affected by removing lib1','line_number':89,'multiline':False]['text':' Validate that the old behavior is restored for neg and mul','line_number':94,'multiline':False]['text':' 1 for `lib`, 1 for sys.getrefcount','line_number':108,'multiline':False]['text':' We gained an additional reference that gets cleared when the finalizer runs','line_number':110,'multiline':False]['text':' 1 for `lib`','line_number':112,'multiline':False]['text':' 1 for the finalizer','line_number':113,'multiline':False]['text':' 1 for sys.getrefcount','line_number':114,'multiline':False]['text':' del will definitely work if the following passes','line_number':126,'multiline':False]['text':' 1 for saved_op_impls','line_number':130,'multiline':False]['text':' 1 for sys.getrefcount','line_number':131,'multiline':False]['text':' This function should be the last user of lib._op_impls:','line_number':132,'multiline':False]['text':' - lib should not have a reference anymore (it was del'ed)','line_number':133,'multiline':False]['text':' - lib's finalizer should not have a reference anymore','line_number':134,'multiline':False]['text':' lib's finalizer should not have a reference anymore','line_number':139,'multiline':False]['text':' Example 1','line_number':143,'multiline':False]['text':' Validate that the old behavior is restored for sum','line_number':156,'multiline':False]['text':' Example 1: Invert the behavior of where's condition input','line_number':161,'multiline':False]['text':' overriding where's cuda kernel with Jiterator generated kernel','line_number':175,'multiline':False]['text':' behavior restored after deregistration','line_number':188,'multiline':False]['text':' Example 2: Use relu to approximate gelu for faster compute','line_number':192,'multiline':False]['text':' overriding gelu's cuda kernel with Jiterator generated relu kernel','line_number':206,'multiline':False]['text':' behavior restored after deregistration','line_number':215,'multiline':False]['text':' Example 3: Preventing exp from exploding for float16','line_number':219,'multiline':False]['text':' overriding exp's cuda kernel with clipped_exp kernel','line_number':233,'multiline':False]['text':' behavior restored after deregistration','line_number':242,'multiline':False]['text':' Example 4: simulate a hardware bug, where the adder is always off by 1','line_number':246,'multiline':False]['text':' behavior restored after deregistration','line_number':273,'multiline':False]['text':' RuntimeError: Explicitly provided dispatch key (Conjugate) is','line_number':287,'multiline':False]['text':' inconsistent with the dispatch key of the enclosing TORCH_LIBRARY_IMPL block','line_number':288,'multiline':False]['text':' Example 1','line_number':301,'multiline':False]['text':' Example 2','line_number':312,'multiline':False]['text':' Create a fragment','line_number':344,'multiline':False]['text':' Create another fragment','line_number':356,'multiline':False]['text':' alias_analysis="FROM_SCHEMA"','line_number':390,'multiline':False]['text':' functional op should not mutate','line_number':451,'multiline':False]['text':' check functional_result includes mutable_result','line_number':456,'multiline':False]['text':' check rest of functional_result is the mutated args','line_number':466,'multiline':False]['text':' check that functionalization kernel was indeed registered','line_number':471,'multiline':False]['text':' dtype for mm should be float32 since we registered a fallthrough','line_number':587,'multiline':False]['text':' ops that don't have a fallthrough registered should not be affected','line_number':589,'multiline':False]['text':' default behavior should have been restored','line_number':595,'multiline':False]['text':' TODO: figure out why broken','line_number':615,'multiline':False]['text':' self.assertEqual(saved_x._version, x._version)','line_number':616,'multiline':False]['text':' TODO: arguably this shouldn't pass and we should complain','line_number':634,'multiline':False]['text':' that out isn't a kwarg','line_number':635,'multiline':False]['text':' The expectation is that beta/alpha don't show up when they're','line_number':655,'multiline':False]['text':' defaulted.  This is even if the user explicitly specified it.','line_number':656,'multiline':False]['text':' What we are testing here is that we omit arg2','line_number':676,'multiline':False]['text':' if it is defaulted, even if a kwarg is set','line_number':677,'multiline':False]['text':' non-optional dtype','line_number':689,'multiline':False]['text':' optional dtype','line_number':690,'multiline':False]['text':' optional memory format','line_number':691,'multiline':False]['text':' There doesn't appear to be any layout signatures which are','line_number':692,'multiline':False]['text':' triggerable using tensor subclasses (need to use a mode)','line_number':693,'multiline':False]['text':' test all sequence types are permissible returns','line_number':721,'multiline':False]['text':' test invalid return gets reasonable error message','line_number':742,'multiline':False]['text':' Wobbles depending on NDEBUG mode of pybind11','line_number':752,'multiline':False]['text':' FIXME: We actually want this to emit a single detach. However,','line_number':765,'multiline':False]['text':' it currently emits two, for reasons unclear to us. Leaving','line_number':766,'multiline':False]['text':' this test here to make sure we don't regress even further (it','line_number':767,'multiline':False]['text':' would be bad if calling .detach() once emits 3+ detaches).','line_number':768,'multiline':False]['text':' For now, just make sure it doesn't crash.  Ideally, we should','line_number':775,'multiline':False]['text':' return some virtual storage that is safe to work with','line_number':776,'multiline':False]['text':' This is ludicrously big (8TB) and this should pass because wrapper','line_number':782,'multiline':False]['text':' subclasses don't allocate','line_number':783,'multiline':False]['text':' The big tests for code coverage are test_precedence_semantics in','line_number':802,'multiline':False]['text':' test_overrides.py; this is just to make sure it is wired up at all','line_number':803,'multiline':False]['text':' correctly for __torch_dispatch__','line_number':804,'multiline':False]['text':' TODO: figure out why x.requires_grad = False doesn't','line_number':868,'multiline':False]['text':' trigger an error for LoggingTensor','line_number':869,'multiline':False]['text':' TODO: figure out why this is broken','line_number':872,'multiline':False]['text':' self.assertEqual(escape[0]._version, x._version)','line_number':873,'multiline':False]['text':' Make sure these statements runs without error','line_number':885,'multiline':False]['text':' In particular checking that when internal detach returns','line_number':886,'multiline':False]['text':' subclasses, these are cleanly overwritten.','line_number':887,'multiline':False]['text':' Returns (TwoTensor, Tensor)','line_number':929,'multiline':False]['text':' make_fx() is not responsible for unwrapping tensor subclass inputs,','line_number':935,'multiline':False]['text':' so we do it manually here.','line_number':936,'multiline':False]['text':' Why? In general, make_fx(f)(*args) promises that the graph returned has the same calling','line_number':937,'multiline':False]['text':' convention as f(*args). Unwrapping tensor subclass inputs can potentially change','line_number':938,'multiline':False]['text':' the number of input args to the graph, breaking that assumption','line_number':939,'multiline':False]['text':' type: ignore[attr-defined]','line_number':965,'multiline':False]['text':' non-contiguous strides, non-zero storage offset','line_number':977,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1010,'multiline':False]['text':' Return a plain tensor from clone().','line_number':1021,'multiline':False]['text':' NB: The default Tensor.__torch_function__ implementation called for deepcopy','line_number':1025,'multiline':False]['text':' disables __torch_function__ by the time we get to clone(), so there is no need to','line_number':1026,'multiline':False]['text':' explicitly disable __torch_function__ for this subclass.','line_number':1027,'multiline':False]['text':' Ensure correct error is thrown for common error cases.','line_number':1036,'multiline':False]['text':' Default implementation of new_empty() returns a plain tensor.','line_number':1038,'multiline':False]['text':' new_empty() incorrectly returns a different type (i.e. a plain tensor).','line_number':1042,'multiline':False]['text':' Ensure a correctly implemented new_empty() causes deepcopy() to work.','line_number':1052,'multiline':False]['text':' NB: only the non-kwarg overload of _make_wrapper_subclass supports','line_number':1065,'multiline':False]['text':'     extra dispatch keys. We probably want to unify the two APIs','line_number':1066,'multiline':False]['text':'     in the future.','line_number':1067,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1068,'multiline':False]['text':' B has precedence over A due to the subclass relationship yet','line_number':1211,'multiline':False]['text':' modes take precedence over arguments','line_number':1212,'multiline':False]['text':' we can't nest two modes that call make_subclass because it only accepts vanilla tensors','line_number':1356,'multiline':False]['text':' make sure this doesn't error','line_number':1408,'multiline':False]['text':' If the pushed mode is the same instance as the current mode, we allow pushing an already active mode.','line_number':1414,'multiline':False]['text':' Wrong device here!','line_number':1530,'multiline':False]['text':' ...the real tensor is held as an element on the tensor.','line_number':1532,'multiline':False]['text':' A Tensor subclass that returns None when doing add','line_number':1557,'multiline':False]['text':' See LoggingTensor above for more details on the subclass','line_number':1558,'multiline':False]['text':' Make sure both run without error','line_number':1585,'multiline':False]['text':' The backward of acos does add then rsqrt so here we make sure that the','line_number':1592,'multiline':False]['text':' undefined Tensor generated by the user code is nicely handled.','line_number':1593,'multiline':False]['text':' If acos formula changes in the future, this can be replaced by any other','line_number':1594,'multiline':False]['text':' function that does add then something in the backward in a composite way','line_number':1595,'multiline':False]['text':' We want the wrapped Tensor to require gradients!','line_number':1605,'multiline':False]['text':' This argument still requires grad because it was passed','line_number':1677,'multiline':False]['text':' through directly...','line_number':1678,'multiline':False]['text':' But the output better not require grad, because that means','line_number':1681,'multiline':False]['text':' you did autograd again in torch dispatch (oops)','line_number':1682,'multiline':False]['text':' should not fail','line_number':1719,'multiline':False]['text':' This is a Direct Subclass, don't do that!','line_number':1723,'multiline':False]['text':' Details of the bug that this tests for:','line_number':1739,'multiline':False]['text':' Here, y dispatch keys are: {PythonTLSSnapshot, AutogradCPU, Conjugate, Python, CPU}','line_number':1740,'multiline':False]['text':' There are a few calls to the dispatcher that are going to happen here:','line_number':1741,'multiline':False]['text':'  - call_exp: User calling exp on y','line_number':1742,'multiline':False]['text':'    - PythonTLSSnapshot: records the TLS on entry and redispatch','line_number':1743,'multiline':False]['text':'    - AutogradCPU: no input requires grad, so does nothing and redispatch','line_number':1744,'multiline':False]['text':'    - Conjugate: no special implementation for exp: use the fallback that','line_number':1745,'multiline':False]['text':'                 first clone the Tensor (to materialize the conj) then redispatch','line_number':1746,'multiline':False]['text':'      - call_clone: conjugate fallback calling clone on y','line_number':1747,'multiline':False]['text':'        - PythonTLSSnapshot: records the TLS on entry and redispatch','line_number':1748,'multiline':False]['text':'        - (AutogradCPU: skipped as autograd added itself to the exclude set above)','line_number':1749,'multiline':False]['text':'        - Conjugate: special implementation for clone: just skip this key','line_number':1750,'multiline':False]['text':'        - Python: Reset the TLS based on the snapshot above and call the user implementation (this','line_number':1751,'multiline':False]['text':'                  actually calls into the dispatcher again but since we disable both our keys','line_number':1752,'multiline':False]['text':'                  before, not detailed here)','line_number':1753,'multiline':False]['text':'        - exit Python: restore the TLS and exit','line_number':1754,'multiline':False]['text':'        - exit Conjugate: nothing was inplace so just exit','line_number':1755,'multiline':False]['text':'        - exit PythonTLSSnapshot: done with this call, reset the saved TLS to empty','line_number':1756,'multiline':False]['text':'    - Python: Reset the TLS again based on the snapshot. <- this used to fail','line_number':1757,'multiline':False]['text':'    - More steps....','line_number':1758,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1768,'multiline':False]['text':' this will just return the original TensorImpl since is_contiguous = True','line_number':1818,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/79079','line_number':1947,'multiline':False]['text':' type: ignore[attr-defined]','line_number':1954,'multiline':False]['text':' TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.','line_number':2087,'multiline':False]['text':' Calling the overload that has kwargs causes us to go down the first overload path,','line_number':2088,'multiline':False]['text':' which will **always** specialize sizes.','line_number':2089,'multiline':False]['text':' We should probably eventually fix this so that the first overload can just handle dynamic shapes.','line_number':2090,'multiline':False]['text':' This tests the correctness of `torch.utils._python_dispatch.return_and_correct_aliasing`,','line_number':2247,'multiline':False]['text':' a util for wrapper subclasses to promise correct aliasing behavior.','line_number':2248,'multiline':False]['text':' It's probably overkill to test every OpInfo,','line_number':2249,'multiline':False]['text':' so I picked a sampling of ops with representative schemas.','line_number':2250,'multiline':False]['text':' out-of-place','line_number':2252,'multiline':False]['text':' out-of-place (TensorList input)','line_number':2253,'multiline':False]['text':' out-of-place (Optional TensorList input)','line_number':2254,'multiline':False]['text':' inplace','line_number':2255,'multiline':False]['text':' view','line_number':2256,'multiline':False]['text':' inplace-view','line_number':2257,'multiline':False]['text':' view (multi-return)','line_number':2258,'multiline':False]['text':' mutable op (returns outputs and mutates some inputs)','line_number':2259,'multiline':False]['text':' conv2d has a default arg 'int[2] strides=0',','line_number':2279,'multiline':False]['text':' which torchscript expands into 'int[2] strides=[0, 0]'','line_number':2280,'multiline':False]['text':' Make sure that _return_and_correct_aliasing can handle this case','line_number':2281,'multiline':False]['text':' (I'm using inference_mode to make sure conv2d doesn't decompose and goes to torch_dispatch)','line_number':2282,'multiline':False]['text':' Make sure that _return_and_correct_aliasing can handle kwargs w mutable tensors','line_number':2287,'multiline':False]