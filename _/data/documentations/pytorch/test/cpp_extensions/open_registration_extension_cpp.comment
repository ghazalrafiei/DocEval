['text':' register guard','line_number':34,'multiline':False]['text':' namespace at::detail','line_number':42,'multiline':False]['text':' Since this custom device is just for testing, not bothering to implement kernels.','line_number':47,'multiline':False]['text':' namespace','line_number':51,'multiline':False]['text':' do nothing','line_number':58,'multiline':False]['text':' namespace at::native','line_number':66,'multiline':False]['text':' for testing this field will mutate when clone() is called by shallow_copy_from.','line_number':68,'multiline':False]['text':' define the constructor','line_number':72,'multiline':False]['text':' we need to register two functions for serialization','line_number':82,'multiline':False]['text':'check if BackendMeta serialization correctly','line_number':116,'multiline':False]['text':' a fake set function is exposed to the Python side','line_number':128,'multiline':False]['text':' A dummy storageImpl for our custom device, that secretly uses the CPU','line_number':137,'multiline':False]['text':' Register our dummy storageImpl create method.','line_number':148,'multiline':False]['text':' basic dummy add function','line_number':161,'multiline':False]['text':' Since this custom device is just for testing, not bothering to implement kernels.','line_number':164,'multiline':False]['text':' basic abs function','line_number':168,'multiline':False]['text':' A dummy allocator for our custom device, that secretly uses the CPU','line_number':173,'multiline':False]['text':' Register our dummy allocator','line_number':193,'multiline':False]['text':' basic dummy empty function, so we can directly construct tensors on the custom device','line_number':197,'multiline':False]['text':' This dummy test device will just use the CPU allocator, and ignores pinned memory.','line_number':198,'multiline':False]['text':' Not bothering to implement.','line_number':224,'multiline':False]['text':' basic dummy copy_() function, so we can copy from the custom device to/from CPU','line_number':228,'multiline':False]['text':' Some dummy asserts for the basic use case: inputs are the same size / dtype, all contiguous.','line_number':237,'multiline':False]['text':' Some set operations for the basic use case','line_number':261,'multiline':False]['text':'resize_storage=','line_number':269,'multiline':True]['text':' Some set operations for the basic use case','line_number':273,'multiline':False]['text':'resize_storage=','line_number':283,'multiline':True]['text':' basic dummy functions related to pin_memory.','line_number':287,'multiline':False]['text':' record pinned data ptr','line_number':297,'multiline':False]['text':' Only CPU tensors can be pinned','line_number':305,'multiline':False]['text':' This macro does the heavy lifting.','line_number':333,'multiline':False]['text':' With TORCH_LIBRARY_IMPL, you can register custom kernels for your backend.','line_number':334,'multiline':False]['text':' For open registration, we're registering all of our kernels to the PrivateUse1 dispatch key.','line_number':335,'multiline':False]['text':' Later in this file, we map a custom device to the PrivateUse1 device type,','line_number':336,'multiline':False]['text':' which allows user code that puts a tensor on your custom_device to eventually get plumbed','line_number':337,'multiline':False]['text':' into the kernels registered here.','line_number':338,'multiline':False]['text':'','line_number':339,'multiline':False]['text':' This macro registers your kernels to the PyTorch Dispatcher.','line_number':340,'multiline':False]['text':' More details on the dispatcher can be found at http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/.','line_number':341,'multiline':False]['text':' This basic implementation doesn't bother dealing with different device indices','line_number':367,'multiline':False]['text':' (e.g. custom_device:0 vs. custom_device:1).','line_number':368,'multiline':False]['text':' We could do that by letting the user pass in a device index in our exposed device function.','line_number':369,'multiline':False]['text':' Note that if you do that, you'll also need to register a device guard to core.','line_number':370,'multiline':False]['text':' See `c10/core/impl/DeviceGuardImplInterface.h:C10_REGISTER_GUARD_IMPL`.','line_number':371,'multiline':False]['text':' Constructors','line_number':396,'multiline':False]['text':' this is used to register generator','line_number':404,'multiline':False]['text':' Here, we're exposing a custom device object that corresponds to our custom backend.','line_number':487,'multiline':False]['text':' We do this using pybind: exposing an "extension_name.custom_device()" function in python,','line_number':488,'multiline':False]['text':' that's implemented in C++.','line_number':489,'multiline':False]['text':' The implementation in this file maps directly to the `PrivateUse1` device type.','line_number':490,'multiline':False]['text':' Co-opting this file to more easily test torch.compile'ing of custom autograd functions in C++','line_number':506,'multiline':False]