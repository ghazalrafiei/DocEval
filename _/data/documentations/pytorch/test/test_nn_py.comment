['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]['text':' load_tests from common_utils is used to automatically filter tests for','line_number':58,'multiline':False]['text':' sharding on sandcastle. This line silences flake warnings','line_number':59,'multiline':False]['text':' WARNING: If you add a new top-level test case to this file, you MUST','line_number':70,'multiline':False]['text':' update test/run_test.py to list it, otherwise it will NOT be run in','line_number':71,'multiline':False]['text':' CI.','line_number':72,'multiline':False]['text':' Test for buggy use of THPMemoryFormat_New','line_number':135,'multiline':False]['text':' NB: dead code','line_number':199,'multiline':False]['text':' uninitialized grad','line_number':238,'multiline':False]['text':' Force set to zeros.','line_number':255,'multiline':False]['text':' test remove_duplicate','line_number':359,'multiline':False]['text':' no extra information or sub-modules','line_number':433,'multiline':False]['text':' one liner extra information','line_number':438,'multiline':False]['text':' sub-modules repr','line_number':443,'multiline':False]['text':' test the option to not remove duplicate module instances','line_number':505,'multiline':False]['text':' can we overwrite a non-persistent buffer with a persistent one?','line_number':604,'multiline':False]['text':' can we overwrite a persistent buffer with a non-persistent one?','line_number':608,'multiline':False]['text':' Assigning None removes the buffer but if we then assign a new Tensor','line_number':617,'multiline':False]['text':' to the same property, it should still be marked as a buffer.','line_number':618,'multiline':False]['text':' Assigning a Parameter removes the buffer.','line_number':626,'multiline':False]['text':' check order of the index','line_number':858,'multiline':False]['text':' test for negative support','line_number':872,'multiline':False]['text':' test for error case','line_number':885,'multiline':False]['text':' check order of the index','line_number':985,'multiline':False]['text':' verify the right exception is thrown when trying to "forward" through a ModuleList','line_number':989,'multiline':False]['text':' verify the right exception is thrown when trying to "forward" through a ModuleDict','line_number':1086,'multiline':False]['text':' The actual replication code from DP cannot be used on CPU so doing it manually here','line_number':1181,'multiline':False]['text':' Do a view here so that we can check the base later','line_number':1190,'multiline':False]['text':' Check reverse works','line_number':1281,'multiline':False]['text':' Check copy works','line_number':1290,'multiline':False]['text':' Check all keys are present and have shallow copied values','line_number':1293,'multiline':False]['text':' Unit test for set_default','line_number':1317,'multiline':False]['text':' 1. Ensure parameter is correctly inserted when','line_number':1318,'multiline':False]['text':'    the key is not present in `ParameterDict`','line_number':1319,'multiline':False]['text':' 2. Ensure parameter is NOT inserted when the','line_number':1327,'multiline':False]['text':'    key is already present in `ParameterDict`','line_number':1328,'multiline':False]['text':' 3. Ensure `None` is inserted when the key is not','line_number':1332,'multiline':False]['text':'    present in `Parameter` and parameter is not specified','line_number':1333,'multiline':False]['text':' Check __or__ and __ror__ works','line_number':1364,'multiline':False]['text':' The actual replication code from DP cannot be used on CPU so doing it manually here','line_number':1419,'multiline':False]['text':' Do a view here so that we can check the base later','line_number':1428,'multiline':False]['text':' Test that applying an in-place operation to a module would bump','line_number':1497,'multiline':False]['text':' the module's parameters' version counter.','line_number':1498,'multiline':False]['text':' Test that applying an in-place operation to a module would bump','line_number':1507,'multiline':False]['text':' the module's parameters' gradients' version counter.','line_number':1508,'multiline':False]['text':' Test that if the conversion function passed to `module._apply()`','line_number':1519,'multiline':False]['text':' changes the TensorImpl type of `module`'s parameters, the `module`'s','line_number':1520,'multiline':False]['text':' parameters are always overwritten, regardless of the value of','line_number':1521,'multiline':False]['text':' `torch.__future__.get_overwrite_module_params_on_conversion()`.','line_number':1522,'multiline':False]['text':' Test that under the current default settings','line_number':1531,'multiline':False]['text':' (`torch.__future__.get_overwrite_module_params_on_conversion() == False`),','line_number':1532,'multiline':False]['text':' a view to a module's parameters is not pointing to the same storage as','line_number':1533,'multiline':False]['text':' its base variable after converting the module to a different dtype.','line_number':1534,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1546,'multiline':False]['text':' a view to a module's parameters is still pointing to the same storage as','line_number':1547,'multiline':False]['text':' its base variable after converting the module to a different dtype.','line_number':1548,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1556,'multiline':False]['text':' `float_module.double()` doesn't preserve previous references to','line_number':1557,'multiline':False]['text':' `float_module`'s parameters or gradients.','line_number':1558,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1570,'multiline':False]['text':' applying an in-place operation to a module would bump the module's','line_number':1571,'multiline':False]['text':' original parameters' version counter.','line_number':1572,'multiline':False]['text':' Test that the in-place operation bumps the original parameter's version counter','line_number':1578,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1583,'multiline':False]['text':' applying an in-place operation to a module would bump the module's','line_number':1584,'multiline':False]['text':' original parameters' gradients' version counter.','line_number':1585,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1596,'multiline':False]['text':' applying an out-of-place operation to a module doesn't bump','line_number':1597,'multiline':False]['text':' the module's original parameters' version counter.','line_number':1598,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':1605,'multiline':False]['text':' applying an out-of-place operation to a module doesn't bump','line_number':1606,'multiline':False]['text':' the module's original parameters' gradients' version counter.','line_number':1607,'multiline':False]['text':' This should work though','line_number':1676,'multiline':False]['text':' This Module has 4 or 5 parameters called:','line_number':1700,'multiline':False]['text':' 'weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0', weight_hr_l0','line_number':1701,'multiline':False]['text':' Applying weight norm on one of them causes it to become a tensor','line_number':1703,'multiline':False]['text':' Removing the weight norm reparametrization restores the Parameter','line_number':1710,'multiline':False]['text':' Make sure that, upon removal of the reparametrization, the','line_number':1717,'multiline':False]['text':' `._parameters` and `.named_parameters` contain the right params.','line_number':1718,'multiline':False]['text':' Specifically, the original weight ('weight_ih_l0') should be placed','line_number':1719,'multiline':False]['text':' back in the parameters, while the reparametrization components','line_number':1720,'multiline':False]['text':' ('weight_ih_l0_v' and 'weight_ih_l0_g') should be removed.','line_number':1721,'multiline':False]['text':' add weight normalization','line_number':1741,'multiline':False]['text':' remove weight norm','line_number':1747,'multiline':False]['text':' test with dim=1','line_number':1753,'multiline':False]['text':' test with dim=None','line_number':1759,'multiline':False]['text':' For float16, the forward of the Module doesn't work but we must still be able','line_number':1769,'multiline':False]['text':' to register the weight norm as this is often done before sending the Module to','line_number':1770,'multiline':False]['text':' CUDA.','line_number':1771,'multiline':False]['text':' Test whether loading from older checkpoints works without triggering warnings','line_number':1800,'multiline':False]['text':' Test whether loading from older checkpoints works without triggering warnings','line_number':1812,'multiline':False]['text':' weight_orig should be trainable','line_number':1832,'multiline':False]['text':' weight_u should be just a reused buffer','line_number':1835,'multiline':False]['text':' weight should be a plain attribute, not counted as a buffer or a param','line_number':1839,'multiline':False]['text':' it should also be sharing storage as `weight_orig`','line_number':1842,'multiline':False]['text':' weight should be converted back as a parameter','line_number':1850,'multiline':False]['text':' test correctness in training/eval modes and cpu/multi-gpu settings','line_number':1858,'multiline':False]['text':' TEST TRAINING BEHAVIOR','line_number':1882,'multiline':False]['text':' assert that u and v are updated','line_number':1884,'multiline':False]['text':' assert that backprop reaches weight_orig','line_number':1890,'multiline':False]['text':' can't use gradcheck because the function changes as we','line_number':1891,'multiline':False]['text':' activate through it in training mode','line_number':1892,'multiline':False]['text':' test backward works with multiple forwards','line_number':1896,'multiline':False]['text':' it uses training mode so we need to reset `u` and `v` vectors','line_number':1897,'multiline':False]['text':' to same value at beginning for finite difference test to pass','line_number':1898,'multiline':False]['text':' test removing','line_number':1911,'multiline':False]['text':' TEST EVAL BEHAVIOR','line_number':1922,'multiline':False]['text':' assert eval gives same result as last training iteration','line_number':1933,'multiline':False]['text':' assert doing more iteartion in eval don't change things','line_number':1935,'multiline':False]['text':' FIXME: the code below is flaky when executed with DataParallel','line_number':1940,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/13818','line_number':1941,'multiline':False]['text':' test backward works with multiple forwards in mixed training','line_number':1945,'multiline':False]['text':' and eval modes','line_number':1946,'multiline':False]['text':' it uses training mode so we need to reset `u` and `v` vectors','line_number':1947,'multiline':False]['text':' to same value at beginning for finite difference test to pass','line_number':1948,'multiline':False]['text':' assert that backprop reaches weight_orig in eval','line_number':1967,'multiline':False]['text':' Test backward compatibility','line_number':1978,'multiline':False]['text':' At version None -> 1: weight becomes not a buffer and v vector becomes a buffer','line_number':1979,'multiline':False]['text':' test that non-strict loading works','line_number':1989,'multiline':False]['text':' set W as a buffer','line_number':2001,'multiline':False]['text':' remove metadata info','line_number':2003,'multiline':False]['text':' remove W buffer','line_number':2005,'multiline':False]['text':' craft a version None state_dict','line_number':2010,'multiline':False]['text':' remove metadata info','line_number':2013,'multiline':False]['text':' remove v vector','line_number':2014,'multiline':False]['text':' set W as a buffer','line_number':2015,'multiline':False]['text':' normal state_dict','line_number':2017,'multiline':False]['text':' We want to still load a user-crafted state_dict, one without metadata','line_number':2022,'multiline':False]['text':' test that re-wrapping does not matter','line_number':2025,'multiline':False]['text':' test that re-wrapping does not matter','line_number':2039,'multiline':False]['text':' since in loading version None state dict, we assume that the','line_number':2045,'multiline':False]['text':' values in the state dict have gone through at lease one','line_number':2046,'multiline':False]['text':' forward, we only test for equivalence when activate_times > 0.','line_number':2047,'multiline':False]['text':' test that re-wrapping does not matter','line_number':2057,'multiline':False]['text':' Test normal loading','line_number':2061,'multiline':False]['text':' this should not run into incompatible shapes','line_number':2076,'multiline':False]['text':' check that u refers to the same dimension','line_number':2078,'multiline':False]['text':' naive forward','line_number':2085,'multiline':False]['text':' Leave first row be all True to maintain the nt's size unchanged','line_number':2144,'multiline':False]['text':' Mask is not bool','line_number':2159,'multiline':False]['text':' Mask size is not 2','line_number':2163,'multiline':False]['text':' Input size is not 3','line_number':2167,'multiline':False]['text':' Mask size does not match input','line_number':2172,'multiline':False]['text':' Mask is not padding format','line_number':2177,'multiline':False]['text':' Skip the test for ROCm as per https://github.com/pytorch/pytorch/issues/53190','line_number':2192,'multiline':False]['text':' TODO(#50743): the following segfaults with check_batched_grad=True','line_number':2198,'multiline':False]['text':' Reference https://github.com/pytorch/pytorch/pull/75507#issuecomment-1110291545','line_number':2275,'multiline':False]['text':' Also test if a DDP state_dict can be loaded from a local model.','line_number':2297,'multiline':False]['text':' wrong size','line_number':2347,'multiline':False]['text':' BatchNormNd','line_number':2371,'multiline':False]['text':' Added num_batches_tracked buffer at version 2. For state dict with','line_number':2372,'multiline':False]['text':' earlier versions or no versions, it should provide default value of 0.','line_number':2373,'multiline':False]['text':' version 1','line_number':2377,'multiline':False]['text':' no version','line_number':2381,'multiline':False]['text':' Save and load, ensure we can still call state_dict','line_number':2410,'multiline':False]['text':' without running into issues.','line_number':2411,'multiline':False]['text':' Note that torch.save / torch.load is not recommended','line_number':2413,'multiline':False]['text':' to save / load modules.','line_number':2414,'multiline':False]['text':' Ensure we can run state_dict without issues','line_number':2418,'multiline':False]['text':' Test to ensure submodules run the hook as well.','line_number':2437,'multiline':False]['text':' Test state dict works as expected after model construction','line_number':2446,'multiline':False]['text':' Test state dict works as expected after forward','line_number':2448,'multiline':False]['text':' load_state_dict shouldn't cause a reference cycle involving Tensors','line_number':2479,'multiline':False]['text':' skip some of the error handling','line_number':2504,'multiline':False]['text':' use sequential to verify nesting','line_number':2507,'multiline':False]['text':' Ensure state_dict contains the extra state by loading it into another module.','line_number':2552,'multiline':False]['text':' Test various types of extra state.','line_number':2573,'multiline':False]['text':' Make sure parameters and persistent buffers were assigned','line_number':2599,'multiline':False]['text':' Make sure that ordering of parameters and buffers is preserved','line_number':2605,'multiline':False]['text':' Make sure outputs are the same','line_number':2621,'multiline':False]['text':' must create optimizer only after loading state_dict when assign=True','line_number':2656,'multiline':False]['text':' Assigned tensor is allowed to have different properties than initial','line_number':2676,'multiline':False]['text':' tensor except for shape','line_number':2677,'multiline':False]['text':' loading should be ok if stride is different','line_number':2689,'multiline':False]['text':' Make sure Variables are not saved as parameters','line_number':2747,'multiline':False]['text':' It shouldn't be possible to replace a parameter with a Variable','line_number':2753,'multiline':False]['text':' But replacing it with None should be fine','line_number':2758,'multiline':False]['text':' Check that None can be shadowed','line_number':2772,'multiline':False]['text':' Assign second object','line_number':2781,'multiline':False]['text':' Remove and add the object back. Order should be unchanged.','line_number':2790,'multiline':False]['text':' Replace object with another one. Order should be unchanged.','line_number':2798,'multiline':False]['text':' Remove and reassign an attribute. It should appear at the end of the list now.','line_number':2803,'multiline':False]['text':' should be stored in l._buffers','line_number':2823,'multiline':False]['text':' this is just a smoke test; these modules are implemented through','line_number':2847,'multiline':False]['text':' autograd so no Jacobian test is needed','line_number':2848,'multiline':False]['text':' Tests losses whose inputs should have the same size.','line_number':2869,'multiline':False]['text':' Ensure warnings are being shown','line_number':2891,'multiline':False]['text':' Trigger Warning','line_number':2893,'multiline':False]['text':' Check warning occurs','line_number':2895,'multiline':False]['text':' cudnn has an unexpected problem with target length 256, see issue #53505','line_number':3009,'multiline':False]['text':' check that we don't have NaN','line_number':3044,'multiline':False]['text':' Test hidden/input batch size broadcasting','line_number':3063,'multiline':False]['text':' Test hx's hidden_size vs module's hidden_size broadcasting','line_number':3066,'multiline':False]['text':' Test input's input_size vs module's input_size broadcasting','line_number':3070,'multiline':False]['text':' this is just a smoke test; these modules are implemented through','line_number':3075,'multiline':False]['text':' autograd so no Jacobian test is needed','line_number':3076,'multiline':False]['text':' this is just a smoke test; these modules are implemented through','line_number':3112,'multiline':False]['text':' autograd so no Jacobian test is needed','line_number':3113,'multiline':False]['text':' this is a deterministic test for TransformerDecoderLayer','line_number':3150,'multiline':False]['text':' set constant weights of the model','line_number':3166,'multiline':False]['text':' deterministic input','line_number':3174,'multiline':False]['text':' deterministic input','line_number':3184,'multiline':False]['text':' deterministic input','line_number':3196,'multiline':False]['text':' deterministic input','line_number':3209,'multiline':False]['text':' key_padding_mask','line_number':3238,'multiline':False]['text':' key_padding_mask','line_number':3252,'multiline':False]['text':' memory_key_padding_mask','line_number':3268,'multiline':False]['text':' memory_key_padding_mask','line_number':3282,'multiline':False]['text':' this is a deterministic test for TransformerDecoderLayer with gelu activation','line_number':3300,'multiline':False]['text':' set constant weights of the model','line_number':3316,'multiline':False]['text':' deterministic input','line_number':3324,'multiline':False]['text':' deterministic input','line_number':3331,'multiline':False]['text':' deterministic input','line_number':3340,'multiline':False]['text':' deterministic input','line_number':3350,'multiline':False]['text':' set constant weights of the model','line_number':3393,'multiline':False]['text':' this is a deterministic test for TransformerDecoder','line_number':3403,'multiline':False]['text':' deterministic input','line_number':3416,'multiline':False]['text':' deterministic input','line_number':3425,'multiline':False]['text':' deterministic input','line_number':3436,'multiline':False]['text':' deterministic input','line_number':3448,'multiline':False]['text':' key_padding_mask','line_number':3478,'multiline':False]['text':' key_padding_mask','line_number':3492,'multiline':False]['text':' memory_key_padding_mask','line_number':3508,'multiline':False]['text':' memory_key_padding_mask','line_number':3522,'multiline':False]['text':' multiple layers no norm','line_number':3539,'multiline':False]['text':' deterministic input','line_number':3542,'multiline':False]['text':' multiple layers no norm','line_number':3551,'multiline':False]['text':' deterministic input','line_number':3554,'multiline':False]['text':' multiple layers with norm','line_number':3584,'multiline':False]['text':' d_model = 4','line_number':3585,'multiline':False]['text':' deterministic input','line_number':3589,'multiline':False]['text':' multiple layers with norm','line_number':3598,'multiline':False]['text':' deterministic input','line_number':3601,'multiline':False]['text':' gelu activation test cases','line_number':3631,'multiline':False]['text':' deterministic input','line_number':3641,'multiline':False]['text':' deterministic input','line_number':3649,'multiline':False]['text':' deterministic input','line_number':3659,'multiline':False]['text':' deterministic input','line_number':3670,'multiline':False]['text':' LSTM with projections has different hx size','line_number':3725,'multiline':False]['text':' Weights will no longer view onto the same chunk of memory','line_number':3739,'multiline':False]['text':' Make sure these still share storage','line_number':3760,'multiline':False]['text':' LSTM with projections has different hx size','line_number':3781,'multiline':False]['text':' Incorrect encoder_input batch size','line_number':3883,'multiline':False]['text':' Incorrect decoder_input batch size','line_number':3888,'multiline':False]['text':' Incorrect encoder_input input size','line_number':3893,'multiline':False]['text':' Incorrect decoder_input input size','line_number':3898,'multiline':False]['text':' Incorrect nhead','line_number':3903,'multiline':False]['text':' Incorrect src_mask','line_number':3910,'multiline':False]['text':' Incorrect tgt_mask','line_number':3916,'multiline':False]['text':' Incorrect memory_mask','line_number':3922,'multiline':False]['text':' Incorrect src_key_padding_mask','line_number':3929,'multiline':False]['text':' Incorrect tgt_key_padding_mask','line_number':3936,'multiline':False]['text':' Incorrect memory_key_padding_mask','line_number':3943,'multiline':False]['text':' Correct activations','line_number':3950,'multiline':False]['text':' Incorrect activation','line_number':3954,'multiline':False]['text':' Incorrect activation','line_number':3983,'multiline':False]['text':' prime number so that no size can divide it.','line_number':3996,'multiline':False]['text':' Incorrect input batch size','line_number':4028,'multiline':False]['text':' Incorrect hidden batch size','line_number':4033,'multiline':False]['text':' Incorrect input size','line_number':4038,'multiline':False]['text':' Incorrect hidden size','line_number':4043,'multiline':False]['text':' Incorrect hidden[0]','line_number':4048,'multiline':False]['text':' prime number so that no size can divide it.','line_number':4061,'multiline':False]['text':' Incorrect input batch size','line_number':4085,'multiline':False]['text':' Incorrect hidden batch size','line_number':4089,'multiline':False]['text':' Incorrect input size','line_number':4095,'multiline':False]['text':' Incorrect hidden size','line_number':4099,'multiline':False]['text':' Incorrect hidden[0]','line_number':4105,'multiline':False]['text':' Incorrect proj size = hidden size','line_number':4111,'multiline':False]['text':' Incorrect proj size != hidden size','line_number':4117,'multiline':False]['text':' Incorrect cell size != hidden size','line_number':4123,'multiline':False]['text':' input and weights are not at the same device','line_number':4149,'multiline':False]['text':' input and hiddens are not at the same device','line_number':4157,'multiline':False]['text':' hidden tensors are not at the same CUDA device','line_number':4171,'multiline':False]['text':' input and weights are not at the same device','line_number':4196,'multiline':False]['text':' input and hiddens are not at the same device','line_number':4201,'multiline':False]['text':' hidden tensors are not at the same CUDA device','line_number':4206,'multiline':False]['text':' check grad weights separately, as nested dict','line_number':4322,'multiline':False]['text':' checking LSTM with projections','line_number':4395,'multiline':False]['text':' Because of dropout randomness, can only compare dropout=0 and dropout=1','line_number':4460,'multiline':False]['text':' runs on CPU to acquire expected output','line_number':4471,'multiline':False]['text':' adds weight normalization','line_number':4476,'multiline':False]['text':' moves to CUDA','line_number':4479,'multiline':False]['text':' otherwise, subsequent warnings will be hidden, and further tests rely on them','line_number':4483,'multiline':False]['text':' remove weight norm','line_number':4487,'multiline':False]['text':' deletes an attribute of original LSTM','line_number':4503,'multiline':False]['text':' verifies that moving to CUDA with only some attributes defined','line_number':4507,'multiline':False]['text':' does not throw an error','line_number':4508,'multiline':False]['text':' recompute the weight and make sure that module can be used','line_number':4510,'multiline':False]['text':' otherwise, subsequent warnings will be hidden, and further tests rely on them','line_number':4513,'multiline':False]['text':' checking the assumption that cuDNN sticks dropout in between','line_number':4521,'multiline':False]['text':' RNN layers','line_number':4522,'multiline':False]['text':' checking error message when RNN has seq_len = 0','line_number':4564,'multiline':False]['text':' Check that backward does not cause a hard error','line_number':4587,'multiline':False]['text':' Function to imperatively ensure pixels are shuffled to the correct locations.','line_number':4682,'multiline':False]['text':' Used to validate the batch operations in pixel_shuffle.','line_number':4683,'multiline':False]['text':' If valid_channels_dim=False, add 1 to make channels dim indivisible by upscale_factor ** 2.','line_number':4695,'multiline':False]['text':' Ensure unshuffle properly inverts shuffle.','line_number':4716,'multiline':False]['text':' If valid_height_dim=False, add 1 to make height dim indivisible by downscale_factor.','line_number':4726,'multiline':False]['text':' If valid_width_dim=False, add 1 to make width dim indivisible by downscale_factor.','line_number':4728,'multiline':False]['text':' For 1D - 2D, this is an error case.','line_number':4743,'multiline':False]['text':' For 3D - 5D, this is a success case for pixel_shuffle + pixel_unshuffle.','line_number':4744,'multiline':False]['text':' Error cases for pixel_shuffle.','line_number':4747,'multiline':False]['text':' Error cases for pixel_unshuffle.','line_number':4752,'multiline':False]['text':' These tests should be OpInfo'd','line_number':4802,'multiline':False]['text':' test hardtanh backward for large tensor','line_number':5052,'multiline':False]['text':' ref backward path for hardtanh','line_number':5059,'multiline':False]['text':' test NC11 and N1HW; test mixed dtype','line_number':5097,'multiline':False]['text':' see #42588, grad is channels_last contiguous, but grad.suggest_memory_format (rightly) return "contiguous"','line_number':5181,'multiline':False]['text':' not channels_last','line_number':5182,'multiline':False]['text':' THNN','line_number':5191,'multiline':False]['text':' cuDNN','line_number':5198,'multiline':False]['text':' keep running stats in FP32','line_number':5212,'multiline':False]['text':' input','line_number':5260,'multiline':False]['text':' running_mean','line_number':5261,'multiline':False]['text':' running_var','line_number':5262,'multiline':False]['text':' 0 needs to have forward grad because otherwise we won't even run batch_norm_jvp','line_number':5274,'multiline':False]['text':' Instantiate BN with buffers that are not None','line_number':5283,'multiline':False]['text':' Use buffers for normalization but don't update them','line_number':5285,'multiline':False]['text':' Store initial values','line_number':5287,'multiline':False]['text':' Forward random tensor','line_number':5291,'multiline':False]['text':' Ensure none of the buffers has been updated','line_number':5293,'multiline':False]['text':' test that when `num_batches_tracked` is not in loaded state_dict,','line_number':5321,'multiline':False]['text':' meta num_batches_tracked is still replaced with singleton 0 tensor','line_number':5322,'multiline':False]['text':' TODO: Create an OpInfo for pdist','line_number':5334,'multiline':False]['text':' Merge into OpInfo?','line_number':5370,'multiline':False]['text':' test for backward in https://github.com/pytorch/pytorch/issues/15511','line_number':5371,'multiline':False]['text':' shape[0] should be able to be (roughly) arbitrarily large, but the kernel','line_number':5377,'multiline':False]['text':' is currently limited to smaller sizes (see issue above); this is just testing','line_number':5378,'multiline':False]['text':' a floor.','line_number':5379,'multiline':False]['text':' just run a single backward, as gradcheck/gradgradcheck is expensive here','line_number':5383,'multiline':False]['text':' dt3 is used as dtype for target = [1, -1], so let's skip unsigned type','line_number':5395,'multiline':False]['text':' When target.requires_grad=True, its impl is in Python, while the other is in TH.','line_number':5563,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/27692 reports','line_number':5571,'multiline':False]['text':' that l1_loss get a wrong result for big batch size','line_number':5572,'multiline':False]['text':' Check cosine_similarity input/output shapes','line_number':5622,'multiline':False]['text':' Check numerical precision, issue #18057','line_number':5629,'multiline':False]['text':' Check dividing by 0.','line_number':5635,'multiline':False]['text':' previous behavior: <x,y>/max(eps, ||x|| * ||y||)','line_number':5636,'multiline':False]['text':' current: <x/max(eps, ||x||), y/max(eps,||y||)>','line_number':5637,'multiline':False]['text':' if f(x,y) is the cosine similarity, then','line_number':5638,'multiline':False]['text':' df/dx = y/(||x|| * ||y||) - (x * <x,y> * ||y||/||x||)/(||x|| * ||y||)^2','line_number':5639,'multiline':False]['text':' the tests below check division by zero in the backward formula when','line_number':5640,'multiline':False]['text':' x := input2 = 0, y := input1 != 0.','line_number':5641,'multiline':False]['text':' For these inputs the gradient wrt x simplifies to g(x,y) := y/(||x|| * ||y||)','line_number':5642,'multiline':False]['text':' Previous test checks g(x,y) == y/eps,','line_number':5643,'multiline':False]['text':' Current test checks g(x,y) == (y/||y||)/eps.','line_number':5644,'multiline':False]['text':' Check type promotion, issue #61454','line_number':5651,'multiline':False]['text':' Check broadcasting #109333','line_number':5656,'multiline':False]['text':' assert no error','line_number':5672,'multiline':False]['text':' 2D affine','line_number':5704,'multiline':False]['text':' assert no error','line_number':5708,'multiline':False]['text':' check for warning for empty span along dimension','line_number':5711,'multiline':False]['text':' Ensure warnings are being shown','line_number':5713,'multiline':False]['text':' Should not trigger warning','line_number':5715,'multiline':False]['text':' Check no warning occurs','line_number':5717,'multiline':False]['text':' Should trigger warning','line_number':5719,'multiline':False]['text':' Check warning occurs','line_number':5721,'multiline':False]['text':' 3D affine','line_number':5739,'multiline':False]['text':' assert no error','line_number':5743,'multiline':False]['text':' check for warning for empty span along dimension','line_number':5746,'multiline':False]['text':' Ensure warnings are being shown','line_number':5748,'multiline':False]['text':' Should not trigger warning','line_number':5750,'multiline':False]['text':' Check no warning occurs','line_number':5752,'multiline':False]['text':' Should trigger warning','line_number':5754,'multiline':False]['text':' Check warning occurs','line_number':5756,'multiline':False]['text':' Backward pass of native C++ and CUDA kernels branch depending on whether input requires gradient,','line_number':5779,'multiline':False]['text':' so we test both cases.','line_number':5780,'multiline':False]['text':' grid_dim_contig_order specifies the dimension order that can','line_number':5784,'multiline':False]['text':' make grid to be contiguous.','line_number':5785,'multiline':False]['text':' i.e., grid.permute(grid_dim_contig_order) is contiguous.','line_number':5786,'multiline':False]['text':' e.g., with grid_dim_contig_order=[0, 3, 1, 2], grid should be','line_number':5787,'multiline':False]['text':'       initialized with contiguous tensor of shape [N, 2, H, W]','line_number':5788,'multiline':False]['text':'       and permuted to [N, H, W, 2] afterwards.','line_number':5789,'multiline':False]['text':' Compare against unvectorized CPU fallback','line_number':5816,'multiline':False]['text':' NOTE [ grid_sample CPU fallback ]','line_number':5818,'multiline':False]['text':' grid_sample uses AVX for 2d images, but that requires 32-bit indexing for','line_number':5819,'multiline':False]['text':' 32-bit floats. So we also have a fallback that is used only for float tensors','line_number':5820,'multiline':False]['text':' requiring 64-bit indexing. That requires too much memory to run on CI, so we','line_number':5821,'multiline':False]['text':' also export the fallback and test it here to ensure feature parity with','line_number':5822,'multiline':False]['text':' the vectorized version.','line_number':5823,'multiline':False]['text':' check that zero-dimensional input strides don't error out','line_number':5850,'multiline':False]['text':' test same size output','line_number':5861,'multiline':False]['text':' test larger output','line_number':5864,'multiline':False]['text':' test smaller output','line_number':5873,'multiline':False]['text':' test 1x1 inpput','line_number':5882,'multiline':False]['text':' testing empty grid','line_number':5891,'multiline':False]['text':' testing empty channel','line_number':5899,'multiline':False]['text':' testing empty batch','line_number':5907,'multiline':False]['text':' test known input on CPU','line_number':5918,'multiline':False]['text':' See NOTE [ grid_sample CPU fallback ]','line_number':6022,'multiline':False]['text':' explicit check for gradient edge cases','line_number':6030,'multiline':False]['text':' See NOTE [ grid_sample CPU fallback ]','line_number':6111,'multiline':False]['text':' do gradcheck','line_number':6119,'multiline':False]['text':' Backward pass of native C++ and CUDA kernels branch depending on whether input requires gradient,','line_number':6140,'multiline':False]['text':' so we test both cases.','line_number':6141,'multiline':False]['text':' check that zero-dimensional input strides don't error out','line_number':6165,'multiline':False]['text':' test same size output','line_number':6178,'multiline':False]['text':' test larger output','line_number':6181,'multiline':False]['text':' test smaller output','line_number':6192,'multiline':False]['text':' test 1x1 inpput','line_number':6203,'multiline':False]['text':' testing empty grid','line_number':6213,'multiline':False]['text':' testing empty channel','line_number':6223,'multiline':False]['text':' testing empty batch','line_number':6233,'multiline':False]['text':' do gradcheck','line_number':6246,'multiline':False]['text':' Unnormalized inquiry indices','line_number':6291,'multiline':False]['text':' Note that even though we are trying to create normalized indices','line_number':6298,'multiline':False]['text':' which results in x.0 and x.5 indices after unnormalization,','line_number':6299,'multiline':False]['text':' because of the numerical error,','line_number':6300,'multiline':False]['text':' the rounding direction might not always be expected as designed.','line_number':6301,'multiline':False]['text':' The best we could do is to ensure the rounding behaviors across','line_number':6302,'multiline':False]['text':' different implementations for different dimensions are','line_number':6303,'multiline':False]['text':' exactly the same.','line_number':6304,'multiline':False]['text':' 2D grid sample x-dim interpolation','line_number':6313,'multiline':False]['text':' The input_tensor_2d_x is of shape','line_number':6314,'multiline':False]['text':' [batch_size, channel_size, non_test_dim_size, test_dim_size]','line_number':6315,'multiline':False]['text':' The grid_tensor_2d_x is of shape','line_number':6322,'multiline':False]['text':' [batch_size, 1, num_inqueries]','line_number':6323,'multiline':False]['text':' The output_tensor_2d_x is of shape','line_number':6331,'multiline':False]['text':' [batch_size, channel_size, 1, num_inqueries]','line_number':6332,'multiline':False]['text':' 2D grid sample y-dim interpolation','line_number':6340,'multiline':False]['text':' The input_tensor_2d_y is of shape','line_number':6341,'multiline':False]['text':' [batch_size, channel_size, test_dim_size, non_test_dim_size]','line_number':6342,'multiline':False]['text':' The grid_tensor_2d_y is of shape','line_number':6344,'multiline':False]['text':' [batch_size, 1, num_inqueries]','line_number':6345,'multiline':False]['text':' The output_tensor_2d_y is of shape','line_number':6351,'multiline':False]['text':' [batch_size, channel_size, 1, num_inqueries]','line_number':6352,'multiline':False]['text':' 3D grid sample x-dim interpolation','line_number':6361,'multiline':False]['text':' The input_tensor_3d_x is of shape','line_number':6362,'multiline':False]['text':' [batch_size, channel_size, non_test_dim_size, non_test_dim_size, test_dim_size]','line_number':6363,'multiline':False]['text':' The grid_tensor_3d_x is of shape','line_number':6366,'multiline':False]['text':' [batch_size, 1, 1, num_inqueries]','line_number':6367,'multiline':False]['text':' The output_tensor_3d_x is of shape','line_number':6376,'multiline':False]['text':' [batch_size, channel_size, 1, 1, num_inqueries]','line_number':6377,'multiline':False]['text':' 3D grid sample y-dim interpolation','line_number':6386,'multiline':False]['text':' The input_tensor_3d_y is of shape','line_number':6387,'multiline':False]['text':' [batch_size, channel_size, non_test_dim_size, test_dim_size, non_test_dim_size]','line_number':6388,'multiline':False]['text':' The grid_tensor_3d_y is of shape','line_number':6390,'multiline':False]['text':' [batch_size, 1, 1, num_inqueries]','line_number':6391,'multiline':False]['text':' The output_tensor_3d_y is of shape','line_number':6397,'multiline':False]['text':' [batch_size, channel_size, 1, 1, num_inqueries]','line_number':6398,'multiline':False]['text':' 3D grid sample z-dim interpolation','line_number':6407,'multiline':False]['text':' The input_tensor_3d_z is of shape','line_number':6408,'multiline':False]['text':' [batch_size, channel_size, non_test_dim_size, non_test_dim_size, test_dim_size]','line_number':6409,'multiline':False]['text':' The grid_tensor_3d_z is of shape','line_number':6411,'multiline':False]['text':' [batch_size, 1, 1, num_inqueries]','line_number':6412,'multiline':False]['text':' The output_tensor_3d_z is of shape','line_number':6418,'multiline':False]['text':' [batch_size, channel_size, 1, 1, num_inqueries]','line_number':6419,'multiline':False]['text':' test known input on CPU','line_number':6431,'multiline':False]['text':' do gradcheck','line_number':6443,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6451,'multiline':False]['text':' test CPU against CUDA','line_number':6456,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6466,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6472,'multiline':False]['text':' test known input on CPU','line_number':6480,'multiline':False]['text':' do gradcheck','line_number':6494,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6503,'multiline':False]['text':' test CPU against CUDA','line_number':6508,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6519,'multiline':False]['text':' python2 requires this so other tests can trigger','line_number':6525,'multiline':False]['text':' gh-76616: nn.ChannelShuffle will return alias of self with an empty input tensor','line_number':6532,'multiline':False]['text':' test float scale factor up & downsampling','line_number':6545,'multiline':False]['text':' test output against known input: result must match opencv','line_number':6571,'multiline':False]['text':' Both OpenCV and PyTorch give a slightly different result on PPC','line_number':6584,'multiline':False]['text':' test float scale factor up & downsampling','line_number':6605,'multiline':False]['text':' note we allocated grad_output to be larger so out of bound access','line_number':6691,'multiline':False]['text':' would be visible in grad_input','line_number':6692,'multiline':False]['text':' Test buffer overflow issue due to inaccurate floating point','line_number':6714,'multiline':False]['text':' representation for integer values. See issue below for details.','line_number':6715,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/88939','line_number':6716,'multiline':False]['text':' reset the corner value and expect the output is changed as well','line_number':6728,'multiline':False]['text':' the output won't be changed on buffer overflow','line_number':6729,'multiline':False]['text':' We set the size larger than the floating point exactly representable range','line_number':6735,'multiline':False]['text':' float: exact representable range (-2**24,2**24)','line_number':6736,'multiline':False]['text':' bfloat16: exact representable range (-2**8, 2**8)','line_number':6740,'multiline':False]['text':' half: exact representable range (-2**11, 2**11)','line_number':6744,'multiline':False]['text':' TODO: turn on cuda test after buffer overflow issue is fixed in cuda kernel','line_number':6749,'multiline':False]['text':' devices = ['cpu'] + (['cuda'] if torch.cuda.is_available() else [])','line_number':6750,'multiline':False]['text':' no half precision support on cpu or bfloat16 on cuda yet','line_number':6768,'multiline':False]['text':' TODO: addmm: computation on CPU is not implemented for Strided + Strided @ SparseBsr','line_number':6849,'multiline':False]['text':' subtest(torch.sparse_bsr, name='weightBSR'),','line_number':6850,'multiline':False]['text':' subtest(torch.sparse_bsc, name='weightBSC'),','line_number':6851,'multiline':False]['text':' input.size(1) not divisible by \prod(kernel_size)','line_number':6963,'multiline':False]['text':' input.size(2) not matching the total number of sliding blocks','line_number':6972,'multiline':False]['text':' should be 4 * 1 = 4 sliding blocks','line_number':6984,'multiline':False]['text':' input wrong dimension','line_number':6991,'multiline':False]['text':' calculated output shape is too small','line_number':6995,'multiline':False]['text':' args validation','line_number':7014,'multiline':False]['text':' not raise','line_number':7027,'multiline':False]['text':' input shapes','line_number':7030,'multiline':False]['text':' out-of-bound targets','line_number':7037,'multiline':False]['text':' cluster sizes','line_number':7044,'multiline':False]['text':' 5 targets in head, 3 clusters, dimensionality 16','line_number':7049,'multiline':False]['text':' 5 targets in this cluster, dimensionality 8','line_number':7050,'multiline':False]['text':' test no_batch_dim support','line_number':7055,'multiline':False]['text':' log_probs actually returns log_proba','line_number':7063,'multiline':False]['text':' forward returns the same thing as log_probs','line_number':7070,'multiline':False]['text':' predict','line_number':7078,'multiline':False]['text':' argmax in shortlist','line_number':7081,'multiline':False]['text':' argmax outside of shortlist','line_number':7090,'multiline':False]['text':' half of the argmax in shortlist, half in clusters','line_number':7099,'multiline':False]['text':' Regression test for #55657','line_number':7128,'multiline':False]['text':' Test for issue #73165','line_number':7139,'multiline':False]['text':' necessary to have an anchor point for comparison, in case the','line_number':7151,'multiline':False]['text':' convert_sync_batchnorm updates in place','line_number':7152,'multiline':False]['text':' Test batch_norm_backward_elemt gives the same answer for all','line_number':7192,'multiline':False]['text':' combinations of contiguous as channels_last input','line_number':7193,'multiline':False]['text':' The target of this test is to test the functionality and accuracy of','line_number':7213,'multiline':False]['text':'   those single-GPU cuda kernels used in SyncBatchNorm','line_number':7214,'multiline':False]['text':' They are:','line_number':7215,'multiline':False]['text':'   fwd: torch.batch_norm_stats, torch.batch_norm_gather_stats_with_counts, torch.batch_norm_elemt','line_number':7216,'multiline':False]['text':'   bwd: torch.batch_norm_backward_reduce, torch.batch_norm_backward_elemt','line_number':7217,'multiline':False]['text':' Flatten Tensor','line_number':7233,'multiline':False]['text':' Unflatten Tensor (unflattened_size as a tuple of ints and list of ints)','line_number':7242,'multiline':False]['text':' Unflatten NamedTensor','line_number':7249,'multiline':False]['text':' Wrong type for unflattened_size (tuple of floats)','line_number':7257,'multiline':False]['text':' Wrong type for unflattened_size (list of lists and list of tuples)','line_number':7264,'multiline':False]['text':' Wrong type for unflattened_size (tuple of lists)','line_number':7271,'multiline':False]['text':' Wrong type for unflattened_size (tuple of dicts)','line_number':7278,'multiline':False]['text':' test for https://github.com/pytorch/pytorch/issues/108072','line_number':7309,'multiline':False]['text':' Padding can be a list, or tuple (regression test for gh-54452)','line_number':7315,'multiline':False]['text':' With dtype enable, it's good enough to test against three floating types','line_number':7443,'multiline':False]['text':' TODO: CUDA is not implemented yet','line_number':7504,'multiline':False]['text':' create a new test that is identical but that sets module.training to False','line_number':7512,'multiline':False]['text':' For bad reasons this would create LongTensors that requires gradients','line_number':7561,'multiline':False]['text':' Remove requires_grad to avoid this','line_number':7562,'multiline':False]['text':' Currently we don't support conv2d/conv3d for LongTensor in CUDA','line_number':7574,'multiline':False]['text':' The following are helpers for TestNN.test_affine_*','line_number':7677,'multiline':False]['text':' end TestNN.test_affine_* helpers','line_number':7857,'multiline':False]['text':' default case track_running_stats=False','line_number':7862,'multiline':False]['text':' check that eval mode doesn't change behavior','line_number':7877,'multiline':False]['text':' If track_running_stats=True and momentum=1, running_mean/var should be','line_number':7892,'multiline':False]['text':' equal to mean/var of the input (with unbias correction)','line_number':7893,'multiline':False]['text':' in eval mode, adding X * std to a channel in input should make the','line_number':7907,'multiline':False]['text':' corresponding channel in output have mean X','line_number':7908,'multiline':False]['text':' THNN','line_number':7916,'multiline':False]['text':' cuDNN','line_number':7923,'multiline':False]['text':' inclusive','line_number':7938,'multiline':False]['text':' test that LN normalizes to mean 0 and stddev 1','line_number':7942,'multiline':False]['text':' test that LN applies weight and bias correctly','line_number':7955,'multiline':False]['text':' layer norm input shape is normalized to m x n, cpu vectorized on n,','line_number':7987,'multiline':False]['text':' so make sure n exceeds vector length','line_number':7988,'multiline':False]['text':' fp32','line_number':7992,'multiline':False]['text':' bf16','line_number':7998,'multiline':False]['text':' bf16 mixed type','line_number':8004,'multiline':False]['text':' test that GN normalizes to mean 0 and stddev 1','line_number':8029,'multiline':False]['text':' test that GN applies weight and bias correctly','line_number':8044,'multiline':False]['text':' bfloat16 input and bfloat16 parameters','line_number':8085,'multiline':False]['text':' bfloat16 input and float parameters','line_number':8087,'multiline':False]['text':' float input and float parameters','line_number':8089,'multiline':False]['text':' bfloat16/half input grad and float parameters','line_number':8097,'multiline':False]['text':' float input grad and float parameters','line_number':8099,'multiline':False]['text':' bfloat16/half input grad and bfloat16/half parameters','line_number':8101,'multiline':False]['text':' Need higher tolerances atol=1e-4 and rtol=1e-4 on macos','line_number':8103,'multiline':False]['text':' Full bf16/half has lower precision compared with mixed bf16/half and fp32.','line_number':8107,'multiline':False]['text':' Use Amp to keep module parameters in acc dtype, i.e. float, for better numerical stability','line_number':8108,'multiline':False]['text':' scipy before 1.0.0 do not support homogeneous coordinate','line_number':8154,'multiline':False]['text':' scipy.ndimage.affine_transform, so we need to skip.','line_number':8155,'multiline':False]['text':' scipy before 1.0.0 do not support homogeneous coordinate','line_number':8193,'multiline':False]['text':' scipy.ndimage.affine_transform, so we need to skip.','line_number':8194,'multiline':False]['text':' scipy before 1.0.0 do not support homogeneous coordinate','line_number':8241,'multiline':False]['text':' scipy.ndimage.affine_transform, so we need to skip.','line_number':8242,'multiline':False]['text':' scipy before 1.0.0 do not support homogeneous coordinate','line_number':8281,'multiline':False]['text':' scipy.ndimage.affine_transform, so we need to skip.','line_number':8282,'multiline':False]['text':' scipy before 1.0.0 do not support homogeneous coordinate','line_number':8332,'multiline':False]['text':' scipy.ndimage.affine_transform, so we need to skip.','line_number':8333,'multiline':False]['text':' channels_last case','line_number':8405,'multiline':False]['text':' channels_last_3d case','line_number':8409,'multiline':False]['text':' non-contiguous case','line_number':8413,'multiline':False]['text':' sigfpe reported in https://github.com/pytorch/pytorch/issues/94125','line_number':8420,'multiline':False]['text':' Create an appropriately-sized input with a single spatial element.','line_number':8492,'multiline':False]['text':' Single spatial element should be fine in eval.','line_number':8498,'multiline':False]['text':' parameters in bfloat16/Half is not recommended','line_number':8617,'multiline':False]['text':' Tests for regression reported in https://github.com/pytorch/pytorch/issues/92166','line_number':8638,'multiline':False]['text':' Assert assertion errors are raised for invalid circular padding values','line_number':8695,'multiline':False]['text':' Should raise error when trying to wrap around more than once','line_number':8697,'multiline':False]['text':' Should raise error when negative padding results in negative output shape','line_number':8700,'multiline':False]['text':' assert that relfection padding errors when pad >= input size','line_number':8703,'multiline':False]['text':' assert that pad doesn't return a view into the input tensor','line_number':8713,'multiline':False]['text':' forward','line_number':8754,'multiline':False]['text':' backward','line_number':8763,'multiline':False]['text':' forward center, edge','line_number':8778,'multiline':False]['text':' forward corner','line_number':8791,'multiline':False]['text':' backward center, edge','line_number':8801,'multiline':False]['text':' backward corner','line_number':8811,'multiline':False]['text':' forward center','line_number':8826,'multiline':False]['text':' backward center','line_number':8830,'multiline':False]['text':' RuntimeError: cannot reshape tensor of 0 elements into shape [1, 0, -1]','line_number':8850,'multiline':False]['text':' A NestedTensor with no tensors inside it doesn't have dim 3 (or dim','line_number':8865,'multiline':False]['text':' 2, for that matter) so it can't hit the fast path, nor can we give a','line_number':8866,'multiline':False]['text':' result.','line_number':8867,'multiline':False]['text':' RuntimeError: cannot reshape tensor of 0 elements into shape [1, 0, -1]','line_number':8878,'multiline':False]['text':' RuntimeError: cannot reshape tensor of 0 elements into shape [1, 0, -1]','line_number':8888,'multiline':False]['text':' RuntimeError: cannot reshape tensor of 0 elements into shape [1, 0, -1]','line_number':8898,'multiline':False]['text':' RuntimeError: cannot reshape tensor of 0 elements into shape [1, 0, -1]','line_number':8909,'multiline':False]['text':' Test if CPU and GPU results match','line_number':8942,'multiline':False]['text':' Test if CPU and GPU results match','line_number':8969,'multiline':False]['text':' cuda throws device assert for invalid data','line_number':9136,'multiline':False]['text':' One off tests to ensure scalars from nn.yaml are properly applied','line_number':9186,'multiline':False]['text':' One off tests to ensure scalars from nn.yaml are properly applied','line_number':9201,'multiline':False]['text':' One off tests to ensure scalars from nn.yaml are properly applied','line_number':9220,'multiline':False]['text':' verify that bogus reduction strings are errors','line_number':9242,'multiline':False]['text':' FIXME: should we allow derivatives on these?','line_number':9286,'multiline':False]['text':' Select every other element in the innermost dimension to','line_number':9295,'multiline':False]['text':' make it non-contiguous.','line_number':9296,'multiline':False]['text':' beta hyper-parameter is called delta for Huber','line_number':9306,'multiline':False]['text':' Huber loss should be larger than smooth L1 loss by a factor of beta.','line_number':9314,'multiline':False]['text':' Test the non-vectorized case.','line_number':9318,'multiline':False]['text':' Test the vectorized case (innermost dim > 32).','line_number':9325,'multiline':False]['text':' Test the non-contiguous case.','line_number':9332,'multiline':False]['text':' We don't want to make propagating NaN a hard requirement on ops, but for','line_number':9376,'multiline':False]['text':' these easy ones, we should make them do so.','line_number':9377,'multiline':False]['text':' Forward AD does not support XLA because XLA tensors don't have storage','line_number':9412,'multiline':False]['text':' Checks upsampling','line_number':9424,'multiline':False]['text':' Checks downsampling','line_number':9429,'multiline':False]['text':' consistency CUDA/CPU check','line_number':9434,'multiline':False]['text':' Here we check if output matches OpenCV's INTER_NEAREST-like result','line_number':9448,'multiline':False]['text':' compute expected output as OpenCV','line_number':9453,'multiline':False]['text':' Checks https://github.com/pytorch/pytorch/issues/62237','line_number':9464,'multiline':False]['text':' for s in [1.00001, 0.99999]:  # 0.9999 case is broken','line_number':9467,'multiline':False]['text':' See issue: https://github.com/pytorch/pytorch/issues/62396','line_number':9468,'multiline':False]['text':' checks data duplication if output_size == 2 * input_size','line_number':9476,'multiline':False]['text':' for s in [2.00001, 1.99999]:  # 1.99999 case is broken','line_number':9477,'multiline':False]['text':' See issue: https://github.com/pytorch/pytorch/issues/62396','line_number':9478,'multiline':False]['text':' input is [[[0, 1, 2, 3, ..., 9]]]','line_number':9483,'multiline':False]['text':' expected out is [[[0, 0, 1, 1, 2, 2, ..., 9, 9]]]','line_number':9484,'multiline':False]['text':' Here we check if output matches Scikit-Image/Scipy-like result','line_number':9490,'multiline':False]['text':' Checks https://github.com/pytorch/pytorch/issues/34808','line_number':9491,'multiline':False]['text':' compute expected output as scikit-image/scipy','line_number':9496,'multiline':False]['text':' Forward AD does not support XLA because XLA tensors don't have storage','line_number':9509,'multiline':False]['text':' Assert that memory format is carried through to the output','line_number':9520,'multiline':False]['text':' test forward when input's height is not same as width','line_number':9523,'multiline':False]['text':' test backward when input's height is not same as width','line_number':9532,'multiline':False]['text':' Assert that cpu and cuda handle channels_last memory format in the same way','line_number':9548,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/54590','line_number':9549,'multiline':False]['text':' Here we check if output matches OpenCV's INTER_NEAREST-like result','line_number':9575,'multiline':False]['text':' compute expected output as OpenCV','line_number':9581,'multiline':False]['text':' Here we check if output matches Scikit-Image/Scipy-like result','line_number':9597,'multiline':False]['text':' Checks https://github.com/pytorch/pytorch/issues/34808','line_number':9598,'multiline':False]['text':' compute expected output as Scikit-Image/Scipy','line_number':9604,'multiline':False]['text':' Forward AD does not support XLA because XLA tensors don't have storage','line_number':9620,'multiline':False]['text':' Assert that memory format is carried through to the output','line_number':9634,'multiline':False]['text':' Assert that cpu and cuda handle channels_last memory format in the same way','line_number':9645,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/54590','line_number':9646,'multiline':False]['text':' make the data asymmetric; ensure that cuda/cpu handle channels_last appropriately.','line_number':9651,'multiline':False]['text':' Here we check if output matches OpenCV's INTER_NEAREST-like result','line_number':9667,'multiline':False]['text':' compute expected output as OpenCV','line_number':9674,'multiline':False]['text':' Here we check if output matches Scikit-Image/Scipy-like result','line_number':9693,'multiline':False]['text':' Checks https://github.com/pytorch/pytorch/issues/34808','line_number':9694,'multiline':False]['text':' compute expected output as Scikit-Image/Scipy','line_number':9701,'multiline':False]['text':' Forward AD does not support XLA because XLA tensors don't have storage','line_number':9723,'multiline':False]['text':' test float scale factor up & downsampling','line_number':9727,'multiline':False]['text':' Assert that memory format is carried through to the output','line_number':9737,'multiline':False]['text':' Bilinear backward is nondeterministic because of atomicAdd usage','line_number':9743,'multiline':False]['text':' Assert that cpu and cuda give same results','line_number':9762,'multiline':False]['text':' This expected result is obtain using PIL.Image.resize','line_number':9816,'multiline':False]['text':' for c in range(3):','line_number':9817,'multiline':False]['text':'   a_in = t_in.numpy()[0, c, ...]','line_number':9818,'multiline':False]['text':'   pil_in = Image.fromarray(a_in)','line_number':9819,'multiline':False]['text':'   pil_out = pil_in.resize((2, 2), resample=Image.LINEAR)','line_number':9820,'multiline':False]['text':' Check output value consistency between resized_input_uint8 and resized input_float','line_number':9850,'multiline':False]['text':' FIXME if-clause shows the current behaviour which is definitely unexpected.','line_number':9880,'multiline':False]['text':' Ideally we want to fix it such that both the ui8 and f32 outputs are also channels_last','line_number':9881,'multiline':False]['text':' See for more details: https://github.com/pytorch/pytorch/pull/100373','line_number':9882,'multiline':False]['text':' - tolerances for bicubic mode are in general higher than for','line_number':9894,'multiline':False]['text':'   bilinear mode, because the bicubic kernel may create','line_number':9895,'multiline':False]['text':'   [intermediate] values outside of the [0, 255] range, which need','line_number':9896,'multiline':False]['text':'   to be clipped in uint8 path, but not in float path. This isn't','line_number':9897,'multiline':False]['text':'   an issue with bilinear kernel.','line_number':9898,'multiline':False]['text':' - Also in bicubic mode, when antialias=False, we have to use','line_number':9899,'multiline':False]['text':'   bigger tolerances than when antialias=True. This is partially','line_number':9900,'multiline':False]['text':'   due to the fact that when False, the float path uses the -0.75','line_number':9901,'multiline':False]['text':'   constant while the uint8 path uses the -0.5 constant in the','line_number':9902,'multiline':False]['text':'   bicubic kernel (when True, they both use -0.5). This difference','line_number':9903,'multiline':False]['text':'   in constants exists for historical reasons. Should both paths','line_number':9904,'multiline':False]['text':'   use the -0.5 constant, we would have closer results and we would','line_number':9905,'multiline':False]['text':'   be able to lower the tolerances.','line_number':9906,'multiline':False]['text':' Non-regression test for https://github.com/pytorch/pytorch/pull/101403','line_number':9926,'multiline':False]['text':' test output against known input: align_corners=False result must match opencv','line_number':9945,'multiline':False]['text':' This expected result is obtain using PIL.Image.resize','line_number':9965,'multiline':False]['text':' for c in range(3):','line_number':9966,'multiline':False]['text':'   a_in = t_in.numpy()[0, c, ...]','line_number':9967,'multiline':False]['text':'   pil_in = Image.fromarray(a_in)','line_number':9968,'multiline':False]['text':'   pil_out = pil_in.resize((2, 2), resample=Image.BICUBIC)','line_number':9969,'multiline':False]['text':' test float scale factor up & downsampling','line_number':9982,'multiline':False]['text':' Assert that memory format is carried through to the output','line_number':9992,'multiline':False]['text':' Test that mask type 0 (LxL attention mask), mask type 1 (BxL padding mask),','line_number':10021,'multiline':False]['text':' and mask type 2 (generic BxHxLxL mask) are processed correctly on the','line_number':10022,'multiline':False]['text':' fast path and the results match explicit slow calculation.','line_number':10023,'multiline':False]['text':' mask_type == 0 => attention mask of shape LxL','line_number':10028,'multiline':False]['text':' mask_type == 1 => padding mask of shape BxL','line_number':10032,'multiline':False]['text':' mask_type == 2 =>  shape BxHxLxL','line_number':10036,'multiline':False]['text':' CUDA path doesn't support padding mask when the number of heads is odd','line_number':10045,'multiline':False]['text':' In result, should only fill the entirely masked out rows since those are non-deterministic (*may* be 0)','line_number':10065,'multiline':False]['text':' Converts rows with all True's to False','line_number':10066,'multiline':False]['text':' Test that softmax with mask type 0 (LxL attention mask), mask type 1 (BxL padding mask),','line_number':10077,'multiline':False]['text':' and mask type 2 (BxHxLxL generic mask) gives the same result on CPU and on CUDA.','line_number':10078,'multiline':False]['text':' mask_type == 0 => attention mask of shape LxL','line_number':10082,'multiline':False]['text':' mask_type == 1 => padding mask of shape BxL','line_number':10084,'multiline':False]['text':' mask_type == 2 => generic mask of shape BxHxLxL','line_number':10086,'multiline':False]['text':' CUDA path doesn't support padding mask when the number of heads is odd','line_number':10093,'multiline':False]['text':' Compute softmax on a given device','line_number':10097,'multiline':False]['text':' In result, should only fill the entirely masked out rows since those are non-deterministic (*may* be 0)','line_number':10107,'multiline':False]['text':' Fill rows with all True's with 0','line_number':10108,'multiline':False]['text':' BxL => src_key_padding_mask','line_number':10124,'multiline':False]['text':' In result, should only fill the entirely masked out rows since those are non-deterministic (*may* be 0)','line_number':10141,'multiline':False]['text':' Converts rows with all True's to False','line_number':10142,'multiline':False]['text':' Make sure the optional argument works as well','line_number':10160,'multiline':False]['text':' In result, should only fill the entirely masked out rows since those are non-deterministic (*may* be 0)','line_number':10168,'multiline':False]['text':' Converts rows with all True's to False','line_number':10169,'multiline':False]['text':' 1 = BxL => src_key_padding_mask','line_number':10181,'multiline':False]['text':' In this test, the forward pass is expected to produce nan's because when dim=0, we only have unspecified values','line_number':10189,'multiline':False]['text':' 1 = BxL => src_key_padding_mask','line_number':10194,'multiline':False]['text':' BxL => src_key_padding_mask','line_number':10210,'multiline':False]['text':' LxL => src_mask','line_number':10231,'multiline':False]['text':' Non-even sizes and non-zero shifts test fallback paths in vectorized kernel','line_number':10275,'multiline':False]['text':' Note: dim1 > 1024 is needed to exercise the vectorized (non-persistent) path, (16, 30576) is BERT-esque','line_number':10276,'multiline':False]['text':' Note: With the largest tests we can hit upper limit of fp16 when we','line_number':10283,'multiline':False]['text':' sum, so scale the input down to stay in a nicer range.','line_number':10284,'multiline':False]['text':' Note; Don't want to bprop back through slice op','line_number':10288,'multiline':False]['text':' workaround to reduce memory usage vs. self.assertEqual, see #84944','line_number':10320,'multiline':False]['text':' x is half','line_number':10323,'multiline':False]['text':' Illegal memory access https://github.com/pytorch/pytorch/issues/52715','line_number':10327,'multiline':False]['text':' invalid configuration argument https://github.com/pytorch/pytorch/issues/52716','line_number':10328,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/84144','line_number':10343,'multiline':False]['text':' generate a tensor with big numbers that are exactly representable in dtype','line_number':10350,'multiline':False]['text':' and are at a constant offset from tensor with small numbers','line_number':10351,'multiline':False]['text':' the logsoftmax of a small and big tensors should be equal','line_number':10352,'multiline':False]['text':' test non-persistent softmax kernel','line_number':10359,'multiline':False]['text':' Test that saving an LSTM in PyTorch 1.7 and older can still be','line_number':10363,'multiline':False]['text':' loaded in newer versions of PyTorch.','line_number':10364,'multiline':False]['text':' Get a state dict for PyTorch 1.7 LSTM. Before PyTorch 1.8, proj_size','line_number':10369,'multiline':False]['text':' didn't exist.','line_number':10370,'multiline':False]['text':' load a model','line_number':10375,'multiline':False]['text':' Compute sum of the large tensor sizes:','line_number':10427,'multiline':False]['text':' (im.numel() + small_image.numel() + small_image.grad.numel() +','line_number':10428,'multiline':False]['text':'   large_view.grad.numel()) * sizeof(dtype)','line_number':10429,'multiline':False]['text':' Test 64-bit indexing with grid_sample (gh-41656)','line_number':10433,'multiline':False]['text':' Try accessing the corners, there should be no segfault','line_number':10434,'multiline':False]['text':' Compare sampling with large strides to the same op on a contiguous tensor','line_number':10443,'multiline':False]['text':' Compute sum of the large tensor sizes:','line_number':10472,'multiline':False]['text':' (im.numel() + small_image.numel() + small_image.grad.numel() +','line_number':10473,'multiline':False]['text':'   large_view.grad.numel()) * sizeof(dtype)','line_number':10474,'multiline':False]['text':' Test 64-bit indexing with grid_sample (gh-41656)','line_number':10478,'multiline':False]['text':' Try accessing the corners, there should be no segfault','line_number':10479,'multiline':False]['text':' Compare sampling with large strides to the same op on a contiguous tensor','line_number':10486,'multiline':False]['text':' All values positive','line_number':10559,'multiline':False]['text':' Shape unchanged','line_number':10561,'multiline':False]['text':' One choice per draw','line_number':10563,'multiline':False]['text':' All values positive','line_number':10579,'multiline':False]['text':' Each experiment should result in 1 draw.','line_number':10581,'multiline':False]['text':' check results is asymptotically as expected.','line_number':10584,'multiline':False]['text':' ~z is approximately N(0,1) for unbiased count','line_number':10586,'multiline':False]['text':' A (lazy) approximate 99% two-sided test:','line_number':10588,'multiline':False]['text':' occurs with prob alpha~>=0.01 if unbiased','line_number':10589,'multiline':False]['text':' "hard" and "not hard" should propagate same gradient.','line_number':10593,'multiline':False]['text':' 2eps = 1x addition + 1x subtraction.','line_number':10605,'multiline':False]['text':' checks that undefined gradients doen't hamper the backward','line_number':10649,'multiline':False]['text':' see #11872','line_number':10650,'multiline':False]['text':' Assert that we have good error message around unsupported CuDNN double backward','line_number':10668,'multiline':False]['text':' NB: we trigger double backward using .backward() instead of autograd.grad due to','line_number':10669,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/37874','line_number':10670,'multiline':False]['text':' Here we avoid the backward(create_graph=True) memory leak','line_number':10679,'multiline':False]['text':' described in https://github.com/pytorch/pytorch/issues/7343','line_number':10680,'multiline':False]['text':' Merge into OpInfo?','line_number':10685,'multiline':False]['text':' LSTM cell reuses output which was resized','line_number':10686,'multiline':False]['text':' GRU cell reuses output which was resized','line_number':10695,'multiline':False]['text':' launch grid_y == 2**16 (larger than maximum y-dimension limit 65535)','line_number':10771,'multiline':False]['text':' test_upsamplingNearest2d_launch_fail should run OK on ROCm','line_number':10778,'multiline':False]['text':' currently only length 2 and 1 right now, but left flexible for additional potential cases','line_number':10826,'multiline':False]['text':' cudnn requires a cpu target','line_number':10835,'multiline':False]['text':' batched case. log_probs.shape = (T, N, C), targets = (N, S), input_lengths/target_lengths = (N,)','line_number':10847,'multiline':False]['text':' batched case. input.shape = (T, N, C), targets = (S,), input_lengths/target_lengths = (N,)','line_number':10849,'multiline':False]['text':' unbatched case. input.shape = (T, C), targets = (S,), input_lengths/target_lengths = (N,)','line_number':10851,'multiline':False]['text':' test output values','line_number':10874,'multiline':False]['text':' test gradient values','line_number':10878,'multiline':False]['text':' checking the output's shape','line_number':10887,'multiline':False]['text':' batch dim case should be (N,). no batch dim case should be ()','line_number':10888,'multiline':False]['text':' checking the gradient's shape','line_number':10892,'multiline':False]['text':' batch dim case should have shape (T, N, C). no batch dim case should have shape (T, C)','line_number':10893,'multiline':False]['text':' Test that under the current default settings','line_number':10927,'multiline':False]['text':' (`torch.__future__.get_overwrite_module_params_on_conversion() == False`),','line_number':10928,'multiline':False]['text':' a view to a module's parameters is not pointing to the same storage as','line_number':10929,'multiline':False]['text':' its base variable after converting the module to a different device.','line_number':10930,'multiline':False]['text':' Without using `torch.no_grad()`, this will leak CUDA memory.','line_number':10935,'multiline':False]['text':' (Issue is filed at https://github.com/pytorch/pytorch/issues/21875)','line_number':10936,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':10944,'multiline':False]['text':' a view to a module's parameters is still pointing to the same storage as','line_number':10945,'multiline':False]['text':' its base variable after converting the module to a different device.','line_number':10946,'multiline':False]['text':' Test that if `torch.__future__.get_overwrite_module_params_on_conversion() == True`,','line_number':10954,'multiline':False]['text':' `cpu_module.to("cuda")` doesn't preserve previous references to','line_number':10955,'multiline':False]['text':' `cpu_module`'s parameters or gradients.','line_number':10956,'multiline':False]['text':' should be bitwise equal','line_number':10974,'multiline':False]['text':' should be bitwise equal','line_number':10979,'multiline':False]['text':' currently fails on XLA','line_number':11037,'multiline':False]['text':' 1st pass','line_number':11053,'multiline':False]['text':' 2nd pass','line_number':11058,'multiline':False]['text':' track_running_stats=False','line_number':11068,'multiline':False]['text':' 1st pass','line_number':11074,'multiline':False]['text':' set eval','line_number':11079,'multiline':False]['text':' 2nd pass','line_number':11082,'multiline':False]['text':' Test bfloat16 input with float module','line_number':11106,'multiline':False]['text':' Compare affine against no-op weights and bias','line_number':11116,'multiline':False]['text':' With weights all ones and bias all zeros','line_number':11127,'multiline':False]['text':' Without any weights or bias','line_number':11133,'multiline':False]['text':' TODO: Test fails with cudnn, see gh-62034','line_number':11157,'multiline':False]['text':' cudnn_enabled = [False, True]','line_number':11158,'multiline':False]['text':' Test bfloat16 input with float module','line_number':11161,'multiline':False]['text':' 1st pass','line_number':11178,'multiline':False]['text':' reset stats','line_number':11185,'multiline':False]['text':' 2nd pass','line_number':11190,'multiline':False]['text':' reset stats','line_number':11197,'multiline':False]['text':' 3rd (combined) pass','line_number':11202,'multiline':False]['text':' Merge into OpInfo?','line_number':11255,'multiline':False]['text':' input_length, vary_lengths, zero_lengths','line_number':11268,'multiline':False]['text':' Compute sequences separately','line_number':11370,'multiline':False]['text':' Use packed format','line_number':11382,'multiline':False]['text':' Check forward','line_number':11387,'multiline':False]['text':' Check backward','line_number':11393,'multiline':False]['text':' enforce_sorted, lengths','line_number':11407,'multiline':False]['text':' training pass','line_number':11425,'multiline':False]['text':' eval pass','line_number':11434,'multiline':False]['text':' bfloat16/half compute','line_number':11456,'multiline':False]['text':' fp32 compute','line_number':11463,'multiline':False]['text':' test softmax with large input value which casues exp() to overflow','line_number':11507,'multiline':False]['text':' t should have size (10,)','line_number':11512,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issue/85005','line_number':11541,'multiline':False]['text':' workaround to reduce memory usage vs. self.assertEqual, see #84944','line_number':11558,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issue/108345','line_number':11574,'multiline':False]['text':' Test with k-dimensional loss.','line_number':11676,'multiline':False]['text':' Test with k-dimensional loss.','line_number':11692,'multiline':False]['text':' Ensure result with unit weights is equivalent to result without weights.','line_number':11700,'multiline':False]['text':' Test with k-dimensional loss.','line_number':11723,'multiline':False]['text':' Ensure result with unit weights is equivalent to result without weights.','line_number':11731,'multiline':False]['text':' Test with k-dimensional loss.','line_number':11740,'multiline':False]['text':' Get one-hot representation of the target.','line_number':11748,'multiline':False]['text':' Need to put the C dim at index 1.','line_number':11750,'multiline':False]['text':' Skip this case for now because soft and hard label CE are not consistent','line_number':11754,'multiline':False]['text':' in the way they apply class weights (see issue #61309).','line_number':11755,'multiline':False]['text':' Ensure loss computed with class indices matches loss','line_number':11759,'multiline':False]['text':' computed with one-hot class probs.','line_number':11760,'multiline':False]['text':' construct target probablity that should have the same result as label_smoothing','line_number':11790,'multiline':False]['text':' Need to put the C dim at index 1.','line_number':11792,'multiline':False]['text':' y_k^ls = y_k * (1 - label_smoothing) + label_smoothing / n_classes','line_number':11797,'multiline':False]['text':' Get one-hot representation of the target.','line_number':11798,'multiline':False]['text':' Test with k-dimensional loss.','line_number':11818,'multiline':False]['text':' use with label_smoothing','line_number':11825,'multiline':False]['text':' manually smoothing target','line_number':11829,'multiline':False]['text':' class_proba^ls = class_proba * (1 - label_smoothing) +','line_number':11830,'multiline':False]['text':'                  label_smoothing / n_classes','line_number':11831,'multiline':False]['text':' Default ignore_index','line_number':11859,'multiline':False]['text':' Check that we correctly tally the denominator for `mean`','line_number':11865,'multiline':False]['text':' i.e. we don't count the ignored_idx at all.','line_number':11866,'multiline':False]['text':' negative ignore_index','line_number':11869,'multiline':False]['text':' Check that we correctly tally the denominator for `mean`','line_number':11876,'multiline':False]['text':' i.e. we don't count the ignored_idx at all.','line_number':11877,'multiline':False]['text':' positive ignore_index','line_number':11880,'multiline':False]['text':' Check that we correctly tally the denominator for `mean`','line_number':11887,'multiline':False]['text':' i.e. we don't count the ignored_idx at all.','line_number':11888,'multiline':False]['text':' Ref: https://github.com/pytorch/pytorch/issue/85005','line_number':11891,'multiline':False]['text':' workaround to reduce memory usage vs. self.assertEqual, see #84944','line_number':11910,'multiline':False]['text':' this isn't actually documented, but was broken previously:','line_number':11960,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/36499','line_number':11961,'multiline':False]['text':' Check that clip_grad_norm_ raises an error if the total norm of the','line_number':11969,'multiline':False]['text':' parameters' gradients is non-finite','line_number':11970,'multiline':False]['text':' Each entry in test_cases has the following values, in this order:','line_number':11977,'multiline':False]['text':'','line_number':11978,'multiline':False]['text':' grad_only_one_elem    If True, only one element of the parameter's','line_number':11979,'multiline':False]['text':'                       gradient is set to the scalar grad, and the','line_number':11980,'multiline':False]['text':'                       rest of the elements are 0. If False, all grad','line_number':11981,'multiline':False]['text':'                       elements are equal to the scalar.','line_number':11982,'multiline':False]['text':'','line_number':11983,'multiline':False]['text':' prefix_finite_grad_param  If True, prefix a parameter that has a grad','line_number':11984,'multiline':False]['text':'                           of 1.','line_number':11985,'multiline':False]['text':'','line_number':11986,'multiline':False]['text':' scalars           Scalars to use as the parameter's grad, through','line_number':11987,'multiline':False]['text':'                   multiplication','line_number':11988,'multiline':False]['text':'','line_number':11989,'multiline':False]['text':' norms_nonfinite   Norm types that should produce nonfinite total norm','line_number':11990,'multiline':False]['text':'','line_number':11991,'multiline':False]['text':' norms_finite      Norm types that should produce finite total norm','line_number':11992,'multiline':False]['text':' Test errors from an infinite grad','line_number':11994,'multiline':False]['text':' Test errors from a NaN grad','line_number':12000,'multiline':False]['text':' Test a grad that should never error','line_number':12006,'multiline':False]['text':' Test a grad that will overflow to inf for only some norm orders','line_number':12012,'multiline':False]['text':' Should only throw an error if the total norm is expected to be','line_number':12047,'multiline':False]['text':' nonfinite and `error_if_nonfinite=True`','line_number':12048,'multiline':False]['text':' Grad should not change if error is thrown','line_number':12057,'multiline':False]['text':' Merge into OpInfo?','line_number':12103,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/54897','line_number':12116,'multiline':False]['text':' Ensure gradients are computed correctly with a low threshold.','line_number':12139,'multiline':False]['text':' Merge into OpInfo?','line_number':12158,'multiline':False]['text':' Merge into OpInfo?','line_number':12170,'multiline':False]['text':' Inplace threshold is okay, because it is idempotent','line_number':12211,'multiline':False]['text':' Test for `nn.TripletMarginWithDistanceLoss` and','line_number':12218,'multiline':False]['text':' `F.triplet_margin_with_distance_loss`.  Checks','line_number':12219,'multiline':False]['text':' for parity against the respective non-distance-agnostic','line_number':12220,'multiline':False]['text':' implementations of triplet margin loss (``nn.TripletMarginLoss`','line_number':12221,'multiline':False]['text':' and `F.triplet_margin_loss`) under *default args*.','line_number':12222,'multiline':False]['text':' Test forward, functional','line_number':12232,'multiline':False]['text':' Test forward, module','line_number':12237,'multiline':False]['text':' Test backward','line_number':12244,'multiline':False]['text':' Test for parity between `nn.TripletMarginWithDistanceLoss` and','line_number':12252,'multiline':False]['text':' `F.triplet_margin_with_distance_loss`.','line_number':12253,'multiline':False]['text':' Test backward','line_number':12273,'multiline':False]['text':' Test forward parity','line_number':12285,'multiline':False]['text':' Trigger warning','line_number':12304,'multiline':False]['text':' Check warning occurs','line_number':12306,'multiline':False]['text':' Test meta module instantiation.','line_number':12322,'multiline':False]['text':' Test materializing meta module on a real device.','line_number':12327,'multiline':False]['text':' Test creating meta module from materialized module.','line_number':12334,'multiline':False]['text':' params/buffers of parent should have been materialized on device','line_number':12363,'multiline':False]['text':' parameters/buffers of children submodules should still be on meta','line_number':12367,'multiline':False]['text':' this is a deterministic test for TransformerEncoderLayer','line_number':12386,'multiline':False]['text':' set constant weights of the model','line_number':12410,'multiline':False]['text':' deterministic input','line_number':12418,'multiline':False]['text':' 0 values are NOT masked. This shouldn't mask anything.','line_number':12424,'multiline':False]['text':' TODO: enable fast path for calls with a mask!','line_number':12426,'multiline':False]['text':' 1 values are masked. Since there is only 1 input embedding this','line_number':12430,'multiline':False]['text':' will result in nan.','line_number':12431,'multiline':False]['text':' deterministic input','line_number':12437,'multiline':False]['text':' all 0 which is no masking','line_number':12445,'multiline':False]['text':' deterministic input','line_number':12457,'multiline':False]['text':' all 0','line_number':12482,'multiline':False]['text':' NestedTensor is only supported for the fast path','line_number':12504,'multiline':False]['text':' currently, which won't be used if training.','line_number':12505,'multiline':False]['text':' Fast path requires inference mode.','line_number':12558,'multiline':False]['text':' Batched inputs','line_number':12578,'multiline':False]['text':' Attention mask of shape (src_len, src_len)','line_number':12581,'multiline':False]['text':' Padding mask of shape (batch_size, src_len)','line_number':12586,'multiline':False]['text':' Provide both masks','line_number':12591,'multiline':False]['text':' this is a deterministic test for TransformerEncoderLayer with gelu activation','line_number':12599,'multiline':False]['text':' set constant weights of the model','line_number':12622,'multiline':False]['text':' deterministic input','line_number':12630,'multiline':False]['text':' deterministic input','line_number':12636,'multiline':False]['text':' deterministic input','line_number':12644,'multiline':False]['text':' Fast path requires inference mode.','line_number':12668,'multiline':False]['text':' Should accept a single Tensor as input','line_number':12695,'multiline':False]['text':' Small gradients should be left unchanged','line_number':12741,'multiline':False]['text':' Should accept a single Tensor as input','line_number':12754,'multiline':False]['text':' reference issue: https://github.com/pytorch/pytorch/issues/111484','line_number':12763,'multiline':False]['text':'  3D tensor','line_number':12789,'multiline':False]['text':'  ChannelsFirst','line_number':12804,'multiline':False]['text':'  ChannelsLast not supported for 3dim','line_number':12809,'multiline':False]['text':'  4D tensor','line_number':12811,'multiline':False]['text':'  ChannelsFirst NCHW','line_number':12834,'multiline':False]['text':'  ChannelsLast NHWC','line_number':12839,'multiline':False]['text':'  5D tensor','line_number':12846,'multiline':False]['text':'  ChannelsFirst NCHW','line_number':12869,'multiline':False]['text':'  ChannelsLast NHWC','line_number':12874,'multiline':False]['text':' issue gh-38137','line_number':12884,'multiline':False]['text':' Make sure it does not throw an exception','line_number':12886,'multiline':False]