['text':' Owner(s): ["module: sparse"]','line_number':1,'multiline':False]['text':' load_tests from torch.testing._internal.common_utils is used to automatically filter tests for','line_number':58,'multiline':False]['text':' sharding on sandcastle. This line silences flake warnings','line_number':59,'multiline':False]['text':' batched grad doesn't support sparse','line_number':62,'multiline':False]['text':' TODO: enable stride/alias checking','line_number':94,'multiline':False]['text':' empty_like excluded for now due to sparse complex','line_number':96,'multiline':False]['text':' aten._to_dense.default this one is getting called with csc','line_number':97,'multiline':False]['text':' Check warn-once:','line_number':162,'multiline':False]['text':' TODO: Put this in torch.cuda.randn','line_number':219,'multiline':False]['text':' make it valid index','line_number':253,'multiline':False]['text':' make it uncoalesced','line_number':255,'multiline':False]['text':' Test .indices() and .values()','line_number':296,'multiline':False]['text':' Make sure that coalesce handles duplicate indices correctly','line_number':311,'multiline':False]['text':' Our code below doesn't work when nnz is 0, because','line_number':327,'multiline':False]['text':' then it's a 0D tensor, not a 2D tensor.','line_number':328,'multiline':False]['text':' this tests correctness','line_number':365,'multiline':False]['text':' Test coalesce doesn't create autograd graph cycles (gh-52253)','line_number':370,'multiline':False]['text':' Sanity check that the helper class works as expected','line_number':372,'multiline':False]['text':' Test that integer overflow is detected when computing numel','line_number':396,'multiline':False]['text':' of a sparse tensor with large dimensions (gh-57416). Notice','line_number':397,'multiline':False]['text':' that numel is computed internally when constructing a','line_number':398,'multiline':False]['text':' tensor, hence the overflow may appear during the tensor','line_number':399,'multiline':False]['text':' construction step.','line_number':400,'multiline':False]['text':' indices inconsistent with size','line_number':418,'multiline':False]['text':' values inconsistent with size','line_number':423,'multiline':False]['text':' Tests triple to_dense for memory corruption','line_number':459,'multiline':False]['text':' Only run autograd test for float64','line_number':469,'multiline':False]['text':' we don't have to_dense for half types on CPU because it is implemented','line_number':484,'multiline':False]['text':' with a slower add_ operation','line_number':485,'multiline':False]['text':' tensor with value','line_number':545,'multiline':False]['text':' tensor with multiple values','line_number':554,'multiline':False]['text':' tensor without value','line_number':564,'multiline':False]['text':' Tests double to_dense for memory corruption','line_number':595,'multiline':False]['text':' Duplicate indices','line_number':687,'multiline':False]['text':' Duplicate indices','line_number':774,'multiline':False]['text':' This is for testing torch.copy_(SparseTensor, SparseTensor)','line_number':827,'multiline':False]['text':' hybrid sparse','line_number':830,'multiline':False]['text':' test copy','line_number':834,'multiline':False]['text':' test type conversion (when x1.copy_(x2), x1.dtype should stay the same)','line_number':839,'multiline':False]['text':' test no broadcast','line_number':852,'multiline':False]['text':' test raise error on copy_() between dense and sparse Tensors','line_number':855,'multiline':False]['text':' test autograd','line_number':858,'multiline':False]['text':' This is for testing torch.copy_(SparseTensor, SparseTensor) across GPU devices','line_number':874,'multiline':False]['text':' hybrid sparse','line_number':877,'multiline':False]['text':' test across gpu devices','line_number':888,'multiline':False]['text':' test between cpu and gpu','line_number':889,'multiline':False]['text':' test autograd','line_number':891,'multiline':False]['text':' trivial checks','line_number':949,'multiline':False]['text':' if valid permutation, test for correctness','line_number':969,'multiline':False]['text':' if s is coalesced, and perm does not touch 0-dim,','line_number':973,'multiline':False]['text':' the result has to be coalesced as well','line_number':974,'multiline':False]['text':' otherwise check if exception is thrown','line_number':982,'multiline':False]['text':' if nnz=0, it is not true that t == t.to_dense().to_sparse()','line_number':989,'multiline':False]['text':' unless t.sparse_dim == t.dim (i.e. t is not hybrid)','line_number':990,'multiline':False]['text':' Transpose is `colasced`-preserving if the indices tensor is empty.','line_number':1005,'multiline':False]['text':' nnz should not grow unbounded (gh-34964)','line_number':1065,'multiline':False]['text':' shapes: list of tuples (sparse_dims, nnz, sizes)','line_number':1078,'multiline':False]['text':' mismatched sizes','line_number':1093,'multiline':False]['text':' hybrid sparse/dense','line_number':1096,'multiline':False]['text':' cat along dense dim','line_number':1099,'multiline':False]['text':' mismatched dimensions','line_number':1103,'multiline':False]['text':' wrapped dimension','line_number':1106,'multiline':False]['text':' sparse with dense','line_number':1110,'multiline':False]['text':' basic case','line_number':1130,'multiline':False]['text':' hybrid sparse/dense, unsqueeze along sparse dim','line_number':1133,'multiline':False]['text':' unsqueeze along dense dimensions','line_number':1137,'multiline':False]['text':' wrapped dimensions','line_number':1141,'multiline':False]['text':' bounds','line_number':1145,'multiline':False]['text':' hybrid sparse/dense, select sparse dim, result is dense','line_number':1166,'multiline':False]['text':' hybrid sparse/dense, select sparse dim, result is sparse','line_number':1171,'multiline':False]['text':' hybrid sparse/dense, select dense dim, result is sparse','line_number':1176,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/82150','line_number':1183,'multiline':False]['text':' empty checks','line_number':1189,'multiline':False]['text':' sum should not promote','line_number':1192,'multiline':False]['text':' NOTE: indices are negative','line_number':1228,'multiline':False]['text':' creates all possible valid indices into dim d of lenght idx_len','line_number':1231,'multiline':False]['text':' NOTE: index_select for dense does not support negative indices,','line_number':1235,'multiline':False]['text':' hence + sizes[d]. See https://github.com/pytorch/pytorch/issues/76347','line_number':1236,'multiline':False]['text':' tests the nnz > sizes[d] branch','line_number':1238,'multiline':False]['text':' tests the nnz <= sizes[d] branch','line_number':1243,'multiline':False]['text':' will trigger brute-force algo','line_number':1251,'multiline':False]['text':' will trigger more sophisticated algos','line_number':1257,'multiline':False]['text':' empty index','line_number':1263,'multiline':False]['text':' non-contigous index','line_number':1270,'multiline':False]['text':' case nnz > size[d]','line_number':1274,'multiline':False]['text':' case nnz <= size[d]','line_number':1280,'multiline':False]['text':' brute-force','line_number':1286,'multiline':False]['text':' more sophisticated algos','line_number':1288,'multiline':False]['text':' idx_small to (sort) and (binary) search into t_sparse','line_number':1303,'multiline':False]['text':' idx_large to (sort) and (binary) search into idx_large','line_number':1305,'multiline':False]['text':' NOTE: when coalesced=True, the (binary) search will be','line_number':1306,'multiline':False]['text':' done over t_sparse anyway, as it is already sorted.','line_number':1307,'multiline':False]['text':' NOTE: GRAIN_SIZE = 32768','line_number':1314,'multiline':False]['text':' case nnz <= size[d]','line_number':1315,'multiline':False]['text':' > 2 * GRAIN_SIZE','line_number':1316,'multiline':False]['text':' case nnz > size[d]','line_number':1319,'multiline':False]['text':' Compare each matrix against result from mm()','line_number':1373,'multiline':False]['text':' deterministic and non-deterministic results should either be','line_number':1434,'multiline':False]['text':' equal or within a small relative difference','line_number':1435,'multiline':False]['text':' adding a graph break before self.assertFalse(weight._indices().is_contiguous())','line_number':1494,'multiline':False]['text':' makes the test pass so some existent sparse related bug','line_number':1495,'multiline':False]['text':' Test code from issue https://github.com/pytorch/pytorch/issues/45113','line_number':1522,'multiline':False]['text':' Create coalesced sparse tensor with non-contiguous indices','line_number':1525,'multiline':False]['text':' Create un/coalesced sparse tensor','line_number':1533,'multiline':False]['text':' gradcheck will likely fail with low-precision input dtypes.','line_number':1572,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/79914','line_number':1613,'multiline':False]['text':' Issues with 0-dim indices/values','line_number':1624,'multiline':False]['text':' TODO: Re-enable these','line_number':1627,'multiline':False]['text':' test_shape(2, 3, [2, 3, 4, 5])','line_number':1628,'multiline':False]['text':' test_shape(2, 3, [2, 2, 0])','line_number':1629,'multiline':False]['text':' Non contiguous dense tensor','line_number':1684,'multiline':False]['text':' Non contiguous sparse indices tensor','line_number':1700,'multiline':False]['text':' Non contiguous sparse values tensor','line_number':1706,'multiline':False]['text':' Non contiguous sparse indices and values tensors','line_number':1713,'multiline':False]['text':' fp32','line_number':1744,'multiline':False]['text':' bfloat16','line_number':1749,'multiline':False]['text':' to compare with reference','line_number':1753,'multiline':False]['text':' Unsupported arguments should error','line_number':1768,'multiline':False]['text':' use a dense dim = 1 to test for squeeze','line_number':1811,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/16501','line_number':1816,'multiline':False]['text':' dim out of range','line_number':1826,'multiline':False]['text':' dim 0 appears multiple times in the list of dims','line_number':1829,'multiline':False]['text':' sum an empty tensor','line_number':1832,'multiline':False]['text':' test values().sum()','line_number':1841,'multiline':False]['text':' TODO: add back inplace support','line_number':1896,'multiline':False]['text':' check that coalesce is out of place if the original tensor is not','line_number':1916,'multiline':False]['text':' coalesced.','line_number':1917,'multiline':False]['text':' check that coalesce is in-place if the original tensor is','line_number':1920,'multiline':False]['text':' coalesced.','line_number':1921,'multiline':False]['text':' check no side effects for the coalesce flag.','line_number':2017,'multiline':False]['text':' check no side effects for the coalesce flag.','line_number':2034,'multiline':False]['text':' check repetitions and matchings in the intersection','line_number':2049,'multiline':False]['text':' check coalesce','line_number':2054,'multiline':False]['text':' TODO: This is also testing that, if coalesce is a no-op,','line_number':2072,'multiline':False]['text':' the indices don't get permuted. I don't know if we actually','line_number':2073,'multiline':False]['text':' want to give this invariant.','line_number':2074,'multiline':False]['text':' check no side effects for the coalesce flag','line_number':2089,'multiline':False]['text':' check no side effects for the coalesce flag','line_number':2106,'multiline':False]['text':' To test masked semantics we need to make sure that','line_number':2151,'multiline':False]['text':' sparsity_pattern(lhs) == sparsity_pattern(lhs.grad).','line_number':2152,'multiline':False]['text':' lhs.sparse_mask(lhs_mask) accomplishes that.','line_number':2153,'multiline':False]['text':' SparseTensor has the following invariants:','line_number':2230,'multiline':False]['text':' - sparse_dim + dense_dim = len(SparseTensor.shape)','line_number':2231,'multiline':False]['text':' - SparseTensor._indices().shape = (sparse_dim, nnz)','line_number':2232,'multiline':False]['text':' - SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])','line_number':2233,'multiline':False]['text':' tests https://github.com/pytorch/pytorch/issues/43699','line_number':2269,'multiline':False]['text':' hybrid sparse input','line_number':2281,'multiline':False]['text':' test uncoalesced input','line_number':2292,'multiline':False]['text':' test on empty sparse tensor','line_number':2302,'multiline':False]['text':' dim < 0','line_number':2330,'multiline':False]['text':' dim > input.dim()','line_number':2331,'multiline':False]['text':' start > size of dim','line_number':2332,'multiline':False]['text':' start+length > size of dim','line_number':2333,'multiline':False]['text':' dim > sparseDim + denseDim','line_number':2339,'multiline':False]['text':' test in-place op on uncoalesced input','line_number':2356,'multiline':False]['text':' hybrid sparse input','line_number':2377,'multiline':False]['text':' test uncoalesced input','line_number':2388,'multiline':False]['text':' test on empty sparse tensor','line_number':2398,'multiline':False]['text':' empty tensors are coalesced at creation (nnz < 2) we must force the uncoalesced state','line_number':2406,'multiline':False]['text':' hybrid sparse input','line_number':2442,'multiline':False]['text':' test uncoalesced input','line_number':2453,'multiline':False]['text':' test on empty sparse tensor','line_number':2463,'multiline':False]['text':' test coalesce on integral dtype tensor','line_number':2498,'multiline':False]['text':' test in-place op on uncoalesced input','line_number':2505,'multiline':False]['text':' hybrid sparse input','line_number':2522,'multiline':False]['text':' test uncoalesced input','line_number':2533,'multiline':False]['text':' test on empty sparse tensor','line_number':2543,'multiline':False]['text':' empty tensors are coalesced at creation (nnz < 2) we must force the uncoalesced state','line_number':2551,'multiline':False]['text':' CUDA sparse tensors currently requires the size to be','line_number':2664,'multiline':False]['text':' specified if nDimV > 0','line_number':2665,'multiline':False]['text':' not really, but we only really want to run this once','line_number':2674,'multiline':False]['text':' have to include size with cuda sparse tensors','line_number':2688,'multiline':False]['text':' (sparse_dim, nnz): (1, 1)','line_number':2771,'multiline':False]['text':' (nnz, ...): (2, 2)','line_number':2772,'multiline':False]['text':' (sparse_dim, nnz): (1, 1)','line_number':2777,'multiline':False]['text':' (nnz, ...): (2, 0)','line_number':2778,'multiline':False]['text':' both indices/values are CUDA','line_number':2838,'multiline':False]['text':' both correct','line_number':2875,'multiline':False]['text':' only indices correct','line_number':2884,'multiline':False]['text':' An empty tensor's data_ptr is always equal to 0','line_number':2895,'multiline':False]['text':' only values correct','line_number':2897,'multiline':False]['text':' neither correct','line_number':2906,'multiline':False]['text':' An empty tensor's data_ptr is always equal to 0','line_number':2913,'multiline':False]['text':' complex support','line_number':2915,'multiline':False]['text':' just run once, we test both cpu and cuda','line_number':2924,'multiline':False]['text':' not really, but we only really want to run this once','line_number':2955,'multiline':False]['text':' not really, but we only really want to run this once','line_number':2991,'multiline':False]['text':' Check that y can be added to t. Currently, this requires that','line_number':3017,'multiline':False]['text':' sparse_dim and dense_dim match.','line_number':3018,'multiline':False]['text':' Here we make sure that the original data are preserved after resizing','line_number':3042,'multiline':False]['text':' 1. Expand the size of some dense dimensions [Supported]','line_number':3048,'multiline':False]['text':' 2. Expand the size of some sparse dimensions [Supported]','line_number':3057,'multiline':False]['text':' 3. Change the shapes of both sparse and dense dimensions when nnz is zero [Supported]','line_number':3062,'multiline':False]['text':' 4. Add dims to dense dimensions [Not Supported]','line_number':3071,'multiline':False]['text':' 5. Remove dims from dense dimensions [Not Supported]','line_number':3082,'multiline':False]['text':' 6. Change the number of sparse dimensions on a non-empty sparse tensor [Not Supported]','line_number':3088,'multiline':False]['text':' 7. Shrink the size of some sparse dimensions on a non-empty sparse tensor [Not Supported]','line_number':3094,'multiline':False]['text':' 8. Shrink the size of some dense dimensions on a non-empty sparse tensor [Not Supported]','line_number':3100,'multiline':False]['text':' scalar sparse tensor','line_number':3118,'multiline':False]['text':' make it valid index','line_number':3194,'multiline':False]['text':' make it uncoalesced','line_number':3196,'multiline':False]['text':' Test inplace','line_number':3233,'multiline':False]['text':' Test out argument','line_number':3237,'multiline':False]['text':' softmax on empty lines results nan, replace with zeros to match the definition','line_number':3296,'multiline':False]['text':' softmax is non-linear operation, so sparse tensors must','line_number':3308,'multiline':False]['text':' be coalesced.','line_number':3309,'multiline':False]['text':' compute pool indices','line_number':3318,'multiline':False]['text':' compute max','line_number':3333,'multiline':False]['text':' apply exp to (v - mx) and sum the results','line_number':3341,'multiline':False]['text':' normalize with the sum of exponents','line_number':3349,'multiline':False]['text':' replace nan-s with zeros','line_number':3377,'multiline':False]['text':' replace nan-s with zeros','line_number':3454,'multiline':False]['text':' replace nan-s with zeros','line_number':3459,'multiline':False]['text':' Check dim out of bounds','line_number':3475,'multiline':False]['text':' Check sparse softmax definition','line_number':3482,'multiline':False]['text':' check Python sparse softmax','line_number':3484,'multiline':False]['text':' check C++ sparse softmax','line_number':3490,'multiline':False]['text':' check C++ sparse log_softmax','line_number':3495,'multiline':False]['text':' Check autograd support on sparse softmax','line_number':3499,'multiline':False]['text':' check softmax Jacobian definition for dense input','line_number':3501,'multiline':False]['text':' check softmax Jacobian from autograd, dense input','line_number':3507,'multiline':False]['text':' check softmax Jacobian from autograd, sparse input','line_number':3511,'multiline':False]['text':' log_softmax Jacobian from autograd, dense input','line_number':3522,'multiline':False]['text':' log_softmax Jacobian from autograd, sparse input','line_number':3525,'multiline':False]['text':' check dtype argument','line_number':3535,'multiline':False]['text':' create a sparse tensor with shape (0,..., 3) it has no materialize values','line_number':3560,'multiline':False]['text':' gradient','line_number':3565,'multiline':False]['text':' TODO: Check after why ROCm's cusparseXcsrgemm2Nnz function doesn't return the same nnz value as CUDA','line_number':3582,'multiline':False]['text':' We convert grad to dense since dense and sparse mm','line_number':3626,'multiline':False]['text':' implementations handle materialized zeroes differently.','line_number':3627,'multiline':False]['text':' dense implementation','line_number':3635,'multiline':False]['text':' cpp implementation','line_number':3638,'multiline':False]['text':' Check result is truly coalesced','line_number':3642,'multiline':False]['text':' check autograd support on sparse matmul','line_number':3649,'multiline':False]['text':' For cuda, `nondet_tol` is set with `1e-5`','line_number':3654,'multiline':False]['text':' This is because cuSparse sometimes returns approximate zero values like `~e-323`','line_number':3655,'multiline':False]['text':' TODO: Check this cuSparse issue.','line_number':3656,'multiline':False]['text':' This happens when you do chain multiplication `torch.sparse.mm` operations','line_number':3657,'multiline':False]['text':' This is not a matrix','line_number':3669,'multiline':False]['text':' Shapes does not','line_number':3672,'multiline':False]['text':' Sparse.mm backward used to wrong with non-contiguous grads,','line_number':3685,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/102493.','line_number':3686,'multiline':False]['text':' ensure that is_coalesced is estimated correctly','line_number':3735,'multiline':False]['text':' This case always coalesce inputs and that could lead to loss of precision,','line_number':3761,'multiline':False]['text':' hence it is inhibited for float16/bfloat16 by providing already coalesced tensors.','line_number':3762,'multiline':False]['text':' to_dense is problematic for boolean non-coalesced CUDA tensors','line_number':3765,'multiline':False]['text':' see https://github.com/pytorch/pytorch/issues/81648','line_number':3766,'multiline':False]['text':' NOTE: addcmul_out is not implemented for bool.','line_number':3774,'multiline':False]['text':' TODO: uncomment once backward is implemented for sparse tensors that broadcast in dense dims.','line_number':3796,'multiline':False]['text':' def check_autograd(x, y):','line_number':3797,'multiline':False]['text':'     if dtype in {torch.double, torch.cdouble}:','line_number':3798,'multiline':False]['text':'         xa = x.detach().clone().requires_grad_(True)','line_number':3799,'multiline':False]['text':'         ya = y.detach().clone().requires_grad_(True)','line_number':3800,'multiline':False]['text':'         gradcheck(lambda a, b: (a * b).to_dense(), (xa, ya), masked=True)','line_number':3801,'multiline':False]['text':'         gradcheck(lambda a, b: (a * b).to_dense(), (ya, xa), masked=True)','line_number':3802,'multiline':False]['text':' TODO: uncomment once supported','line_number':3813,'multiline':False]['text':' check_autograd(x, y)','line_number':3814,'multiline':False]['text':' check broadcasting in dense dims','line_number':3816,'multiline':False]['text':' TODO: uncomment once supported','line_number':3821,'multiline':False]['text':' check_autograd(x, y)','line_number':3822,'multiline':False]['text':' check commutativity','line_number':3836,'multiline':False]['text':' check correctness','line_number':3839,'multiline':False]['text':' check in-placeness for dense','line_number':3842,'multiline':False]['text':' check in-placeness for sparse','line_number':3847,'multiline':False]['text':' for sparse','line_number':3849,'multiline':False]['text':' check scalar multiplication','line_number':3866,'multiline':False]['text':' check correctness and dtype','line_number':3872,'multiline':False]['text':' check scalar as 0-dim sparse tensor','line_number':3877,'multiline':False]['text':' check non-coalesced 0-dim scalar','line_number':3885,'multiline':False]['text':' we skip torch.bool because for such tensors','line_number':3886,'multiline':False]['text':' coalesce.to_dense != to_dense','line_number':3887,'multiline':False]['text':' Case 1: sparse broadcasts over dense','line_number':3900,'multiline':False]['text':' Case 2: dense broadcasts over sparse','line_number':3906,'multiline':False]['text':' some normal cases','line_number':3953,'multiline':False]['text':' noncontigous diags','line_number':3956,'multiline':False]['text':' noncontigous offsets','line_number':3958,'multiline':False]['text':' noncontigous diags + offsets','line_number':3960,'multiline':False]['text':' correct dimensionality, 2d, 2d , and shapes match, but the number of diagonals is zero','line_number':3962,'multiline':False]['text':' forward rotation of upper diagonals','line_number':3964,'multiline':False]['text':' rotation exausts input space to read from','line_number':3966,'multiline':False]['text':' Simple cases repeated with special output format','line_number':3968,'multiline':False]['text':' vector diags','line_number':3971,'multiline':False]['text':' Scalar offset','line_number':3973,'multiline':False]['text':' offsets out of range','line_number':3975,'multiline':False]['text':' creating a coo tensor with nnz == 0 is always coalesced','line_number':4000,'multiline':False]['text':' same for a coo tensor with only 1 nnz','line_number':4002,'multiline':False]['text':' two or more nnz coalesced is false as it can't be verified without an expensive check','line_number':4004,'multiline':False]['text':' even if there are no duplicates','line_number':4006,'multiline':False]['text':' to_dense uses coalesce which isn't implemented for bool','line_number':4079,'multiline':False]['text':' test 0x0 sparse_coo_tensor','line_number':4134,'multiline':False]['text':' reduce over all dimensions','line_number':4206,'multiline':False]['text':' FIXME: for now reductions with non-zero reduction identity and','line_number':4211,'multiline':False]['text':' unspecified mask are not supported for sparse COO','line_number':4212,'multiline':False]['text':' tensors, see torch.masked.prod implementation','line_number':4213,'multiline':False]['text':' for details.','line_number':4214,'multiline':False]['text':' nnz zero sparse tensors should always be coalesced at creation','line_number':4248,'multiline':False]['text':' but we can force them into the uncoalesed state','line_number':4250,'multiline':False]['text':' return the coalesced state for indices/values access','line_number':4253,'multiline':False]['text':' TODO: this sort of aliasing will need to be handled by','line_number':4255,'multiline':False]['text':' functionalization','line_number':4256,'multiline':False]['text':' An utility class used in TestSparseAny.test_dataloader method.','line_number':4264,'multiline':False]['text':' column index is out of range','line_number':4287,'multiline':False]['text':' index is out of range','line_number':4301,'multiline':False]['text':' First, consider the case where invariant checks are disabled','line_number':4320,'multiline':False]['text':' "globally" (read: within the context of this test method','line_number':4321,'multiline':False]['text':' caller) as defined by check_sparse_tensor_invariants(False)','line_number':4322,'multiline':False]['text':' decorator:','line_number':4323,'multiline':False]['text':' Enable the invariant checks in a local context:','line_number':4326,'multiline':False]['text':' Leaving the local context must restore the "global" state of','line_number':4330,'multiline':False]['text':' the invariant check feature:','line_number':4331,'multiline':False]['text':' Since invariant checks are disabled by default, we can','line_number':4334,'multiline':False]['text':' create an invalid sparse tensor without raising an','line_number':4335,'multiline':False]['text':' exception:','line_number':4336,'multiline':False]['text':' Or, when disabling the invariants check explicitly:','line_number':4340,'multiline':False]['text':' Enabling invariant check via constructor's optional argument','line_number':4344,'multiline':False]['text':' will raise an exception when sparse tensor invariants are','line_number':4345,'multiline':False]['text':' violated:','line_number':4346,'multiline':False]['text':' Check that the global invariant check flag has been restored','line_number':4350,'multiline':False]['text':' after raising the exception above:','line_number':4351,'multiline':False]['text':' Next, consider the case where invariant checks are enabled','line_number':4354,'multiline':False]['text':' within a local context:','line_number':4355,'multiline':False]['text':' Since invariant checks are now enabled by default, an','line_number':4359,'multiline':False]['text':' attempt to create an invalid sparse tensor will lead to','line_number':4360,'multiline':False]['text':' an exception:','line_number':4361,'multiline':False]['text':' Similarly, when enabling the invariant checks','line_number':4365,'multiline':False]['text':' explicitly, invalid sparse tensor construction will lead','line_number':4366,'multiline':False]['text':' to an exception:','line_number':4367,'multiline':False]['text':' However, invariants check can be disabled via','line_number':4371,'multiline':False]['text':' constructor's optional argument so that the invalid','line_number':4372,'multiline':False]['text':' tensor is succesfully constructed:','line_number':4373,'multiline':False]['text':' Check that the invariant check flag has been restored','line_number':4377,'multiline':False]['text':' when leaving the constructor:','line_number':4378,'multiline':False]['text':' Double-check restoring the global state when leaving the','line_number':4381,'multiline':False]['text':' local context:','line_number':4382,'multiline':False]['text':' Test nesting of pre-defined context managers','line_number':4385,'multiline':False]['text':' Test an attempt to re-use an activate context manager instance','line_number':4395,'multiline':False]['text':' all layouts must produce semantically the same tensors','line_number':4420,'multiline':False]['text':' Ensure that the inputs generation covers all layout,','line_number':4454,'multiline':False]['text':' non-hybrid/hybrid, non-batch/batch, and contiguity','line_number':4455,'multiline':False]['text':' combinations:','line_number':4456,'multiline':False]['text':' indices are contiguous per-patch','line_number':4472,'multiline':False]['text':' TODO: remove after gh-104868 is resolved','line_number':4517,'multiline':False]['text':' TODO: remove this if-block after gh-107370 is resolved','line_number':4543,'multiline':False]['text':' TODO: implement batch support in _convert_indices_from_csr_to_coo','line_number':4574,'multiline':False]['text':' TODO: to support conversion strided->hybrid','line_number':4591,'multiline':False]['text':' CSR/CSC/BSR/BSC, to_sparse() requires extra keyword','line_number':4592,'multiline':False]['text':' argument, either nof_batch_dims or','line_number':4593,'multiline':False]['text':' nof_dense_dims','line_number':4594,'multiline':False]['text':' Used to check that the explicit conversion methods','line_number':4618,'multiline':False]['text':' are consistent with the `to_sparse(*, layout,','line_number':4619,'multiline':False]['text':' blocksize)` method.','line_number':4620,'multiline':False]['text':' unreachable','line_number':4632,'multiline':False]['text':' TODO: The following exception cases all correspond to','line_number':4634,'multiline':False]['text':' not implemented conversions','line_number':4635,'multiline':False]['text':' to_sparse method uses unsafe construction of sparse','line_number':4685,'multiline':False]['text':' tensors. Here we explicitly validate the results to','line_number':4686,'multiline':False]['text':' make sure that the sparse tensors are consistent','line_number':4687,'multiline':False]['text':' with the corresponding sparse tensor invariants.','line_number':4688,'multiline':False]['text':' Check r is truly coalesced when r.is_coalesced == True','line_number':4707,'multiline':False]['text':' unreachable','line_number':4715,'multiline':False]['text':' Finally, we'll test tensor equality:','line_number':4717,'multiline':False]['text':' Also, check consistency with explicit conversion methods:','line_number':4720,'multiline':False]['text':' Check inverse conversion from sparse compressed block tensors','line_number':4724,'multiline':False]['text':' extra tests','line_number':4739,'multiline':False]['text':' See gh-90910','line_number':4741,'multiline':False]['text':' See gh-91007','line_number':4749,'multiline':False]['text':'  Checking invariant rop(inp, ...).to_dense() == rop(inp.to_dense(), ...)','line_number':4775,'multiline':False]['text':' we count samples to avoid false-positive test reports','line_number':4780,'multiline':False]['text':' we count samples to avoid false-positive test reports','line_number':4803,'multiline':False]['text':' <mth name> = (<supported layouts>, <exception message on other layouts>)','line_number':4826,'multiline':False]['text':' This function does not check the following cases:','line_number':4857,'multiline':False]['text':' - batch or hybrid tensors because addmm does not support','line_number':4858,'multiline':False]['text':'   such inputs yet','line_number':4859,'multiline':False]['text':' - check_forward_ad=True because of the lack of sparse tensor','line_number':4860,'multiline':False]['text':'   support in aten::view_as_real, torch._VF._make_dual, etc.','line_number':4861,'multiline':False]['text':' that is, the validation returns the sparse sample','line_number':4907,'multiline':False]['text':' wrapped within ErrorInput instance','line_number':4908,'multiline':False]['text':' Check rop(inp, ...).shape == inp.shape','line_number':4914,'multiline':False]['text':' Check rop(inp, ...).sparse_dim() == inp.sparse_dim()','line_number':4917,'multiline':False]['text':' Check rop(inp, ...).dense_dim() == inp.dense_dim()','line_number':4920,'multiline':False]['text':' Check invariant rop(inp, ...).to_dense() == rop(inp.to_dense(), ...)','line_number':4923,'multiline':False]['text':' this is strided op issue, so skipping the sample silently here','line_number':4927,'multiline':False]['text':' Check op(inp).shape == inp.shape','line_number':4980,'multiline':False]['text':' Check op(inp, layout=torch.strided).dense_dim() == inp.dim()','line_number':4985,'multiline':False]['text':' Check op(inp, layout=torch.sparse_coo).sparse_dim() == batch_dim + inp.sparse_dim()','line_number':4988,'multiline':False]['text':' Check op(inp, layout=torch.sparse_coo).dense_dim() == inp.dense_dim()','line_number':4990,'multiline':False]['text':' Check op(inp).sparse_dim() == inp.sparse_dim()','line_number':4995,'multiline':False]['text':' Check op(inp).dense_dim() == inp.dense_dim()','line_number':4997,'multiline':False]['text':' Check invariant `x.sparse_mask(mask).<indices> == mask.<indices>`','line_number':5021,'multiline':False]['text':' Check invariant:','line_number':5041,'multiline':False]['text':'  x.sparse_mask(mask).to_dense() == x.mul(sparse_xyz_tensor(<mask indices>,','line_number':5042,'multiline':False]['text':'                                          ones_like(<mask values>)).to_dense())','line_number':5043,'multiline':False]['text':' Check invariant `mask.to_dense().sparse_mask(mask) == mask`','line_number':5048,'multiline':False]['text':' FIXME: RuntimeError: indices expected sparse','line_number':5069,'multiline':False]['text':' coordinate tensor layout but got SparseCsr. Likely','line_number':5070,'multiline':False]['text':' works when gh-107126 is fixed.','line_number':5071,'multiline':False]['text':' TODO: fix gh-104868  to enable batched samples:','line_number':5077,'multiline':False]['text':' FIXME: RuntimeError: sparse_mask(): the','line_number':5081,'multiline':False]['text':' number of sparse dimensions in `self`','line_number':5082,'multiline':False]['text':' should match that of the `mask`. Got','line_number':5083,'multiline':False]['text':' `self.sparse_dim() == 3` !=','line_number':5084,'multiline':False]['text':' `mask.sparse_dim() == 2','line_number':5085,'multiline':False]['text':' FIXME: RuntimeError: expected','line_number':5087,'multiline':False]['text':' col_indices to be a contiguous tensor','line_number':5088,'multiline':False]['text':' per batch','line_number':5089,'multiline':False]['text':' Blocksize should be a tuple/list/torch.Size containing two values','line_number':5112,'multiline':False]['text':' e.g., TestSparseUnaryUfuncsCPU and TestSparseUnaryUfuncsCUDA','line_number':5127,'multiline':False]['text':' e.g., TestSparseCPU and TestSparseCUDA','line_number':5132,'multiline':False]