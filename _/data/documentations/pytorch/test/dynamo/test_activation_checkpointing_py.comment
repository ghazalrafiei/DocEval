['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]['text':' The original version and the checkpointed version of the same function','line_number':130,'multiline':False]['text':' should produce the same outputs and the same gradients under torch.compile.','line_number':131,'multiline':False]['text':' Run original version','line_number':133,'multiline':False]['text':' Run checkpointed version','line_number':146,'multiline':False]['text':' Check that outputs and gradients are equal','line_number':159,'multiline':False]['text':' mm recomputed in the bwd','line_number':188,'multiline':False]['text':' This goes through VariableBuilder','line_number':198,'multiline':False]['text':' mm recomputed in the bwd','line_number':207,'multiline':False]['text':' mm recomputed in the bwd','line_number':227,'multiline':False]['text':' mm recomputed in the bwd','line_number':249,'multiline':False]['text':' Ensures that tags are passed on through decompositions as well','line_number':281,'multiline':False]['text':' fw_compiler = functools.partial(count_ops, freq=2, op=torch.ops.aten.mm.default)','line_number':328,'multiline':False]['text':' bw_compiler = functools.partial(','line_number':329,'multiline':False]['text':'     count_ops, freq=6, op=torch.ops.aten.mm.default','line_number':330,'multiline':False]['text':' )  # mm recomputed in the bwd','line_number':331,'multiline':False]['text':' backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)','line_number':332,'multiline':False]['text':' x = torch.utils.checkpoint.checkpoint(gn, x, y)','line_number':348,'multiline':False]['text':' fw_compiler = functools.partial(count_ops, freq=2, op=torch.ops.aten.mm.default)','line_number':354,'multiline':False]['text':' bw_compiler = functools.partial(','line_number':355,'multiline':False]['text':'     count_ops, freq=6, op=torch.ops.aten.mm.default','line_number':356,'multiline':False]['text':' )  # mm recomputed in the bwd','line_number':357,'multiline':False]['text':' backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)','line_number':358,'multiline':False]['text':' backend = "aot_eager"','line_number':359,'multiline':False]['text':' Figure out a way to test the number of inductor_random calls','line_number':366,'multiline':False]['text':' rand decomps do not have have numerical results as eager','line_number':383,'multiline':False]['text':' One graph for torch.sin on the input, and other for torch.cos.','line_number':409,'multiline':False]['text':' one for checkpoint, and 3 for x, y, z','line_number':442,'multiline':False]['text':' We would've expected 6 here','line_number':514,'multiline':False]['text':' (2 matmul recompute and 2 mm ops per fwd matmul, so 2 + 2 * 2 = 6)','line_number':515,'multiline':False]['text':' if we didn't enable selective checkpointing.','line_number':516,'multiline':False]['text':' Saves output of all compute ops, except second mm','line_number':545,'multiline':False]['text':' (i.e. we will hint the partitioner to recompute second mm in backward pass)','line_number':546,'multiline':False]['text':' Q: How do we come to this number 4?','line_number':581,'multiline':False]['text':' A: We have 2 matmuls in the forward pass, each matmul contributes 2 `mm` ops in the backward pass,','line_number':582,'multiline':False]['text':' so we have at least 4 `mm` ops in backward pass. It's "at least" because whether second matmul in','line_number':583,'multiline':False]['text':' the forward pass is recomputed in the backward pass is up to the partitioner to decide.','line_number':584,'multiline':False]['text':' Regardless of whether `preserve_rng_state` is True or False,','line_number':722,'multiline':False]['text':' we will always preserve RNG state when using `torch.compile`.','line_number':723,'multiline':False]['text':' NOTE: This unit test expects `dropout` to be recomputed (notice the count for `native_dropout` is 1).','line_number':740,'multiline':False]['text':' NOTE: when `preserve_rng_state` is False, gradient will mismatch between torch.compile and eager,','line_number':753,'multiline':False]['text':' because eager version doesn't preserve RNG state while torch.compile still does.','line_number':754,'multiline':False]['text':' Hence when `preserve_rng_state` is False, we skip the output and gradient comparison','line_number':755,'multiline':False]['text':' between torch.compile and eager.','line_number':756,'multiline':False]