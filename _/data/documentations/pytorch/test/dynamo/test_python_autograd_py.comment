['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]['text':' We need to start with some tensors whose values were not computed','line_number':33,'multiline':False]['text':' inside the autograd. This function constructs leaf nodes.','line_number':34,'multiline':False]['text':' This performs a pointwise multiplication of a Variable, tracking gradients','line_number':42,'multiline':False]['text':' defined later in the notebook','line_number':44,'multiline':False]['text':' names of the inputs to the original computation','line_number':58,'multiline':False]['text':' names of the outputs of the original computation','line_number':60,'multiline':False]['text':' apply chain rule','line_number':62,'multiline':False]['text':' this map holds dL/dX for all values X','line_number':76,'multiline':False]['text':' It starts by initializing the 'seed' dL/dL, which is 1','line_number':78,'multiline':False]['text':' print(f'd{L.name} ------------------------')','line_number':80,'multiline':False]['text':' look up dL_dentries. If a variable is never used to compute the loss,','line_number':82,'multiline':False]['text':' we consider its gradient None, see the note below about zeros for more information.','line_number':83,'multiline':False]['text':' propagate the gradient information backward','line_number':87,'multiline':False]['text':' optimize for the case where some gradient pathways are zero. See','line_number':91,'multiline':False]['text':' The note below for more details.','line_number':92,'multiline':False]['text':' perform chain rule propagation specific to each compute','line_number':95,'multiline':False]['text':' Accumulate the gradient produced for each input.','line_number':98,'multiline':False]['text':' Each use of a variable produces some gradient dL_dinput for that','line_number':99,'multiline':False]['text':' use. The multivariate chain rule tells us it is safe to sum','line_number':100,'multiline':False]['text':' all the contributions together.','line_number':101,'multiline':False]['text':' print some information to understand the values of each intermediate','line_number':108,'multiline':False]['text':' for name, value in dL_d.items():','line_number':109,'multiline':False]['text':'    print(f'd{L.name}_d{name} = {value.name}')','line_number':110,'multiline':False]['text':' print(f'------------------------')','line_number':111,'multiline':False]['text':' peephole optimization','line_number':118,'multiline':False]['text':' define forward','line_number':121,'multiline':False]['text':' print(f'{r.name} = {self.name} * {rhs.name}')','line_number':123,'multiline':False]['text':' record what the inputs and outputs of the op were','line_number':125,'multiline':False]['text':' define backprop','line_number':129,'multiline':False]['text':' partial derivative of r = self*rhs','line_number':133,'multiline':False]['text':' partial derivative of r = self*rhs','line_number':134,'multiline':False]['text':' chain rule propagation from outputs to inputs of multiply','line_number':136,'multiline':False]['text':' finally, we record the compute we did on the tape','line_number':142,'multiline':False]['text':' Add follows a similar pattern to Mul, but it doesn't end up','line_number':148,'multiline':False]['text':' capturing any variables.','line_number':149,'multiline':False]['text':' print(f'{r.name} = {self.name} + {rhs.name}')','line_number':151,'multiline':False]['text':' print(f'{r.name} = {self.name}.sum()')','line_number':169,'multiline':False]['text':' only works for scalars','line_number':183,'multiline':False]['text':' print(f'{r.name} = {self.name}.expand({sizes})')','line_number':185,'multiline':False]['text':' force two frames','line_number':277,'multiline':False]