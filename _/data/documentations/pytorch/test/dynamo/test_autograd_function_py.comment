['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]['text':' Test there is graph break in forward function','line_number':24,'multiline':False]['text':' Note that forward, setup_context, and backward are @staticmethods','line_number':82,'multiline':False]['text':' inputs is a Tuple of all of the inputs passed to forward.','line_number':91,'multiline':False]['text':' output is the output of the forward().','line_number':92,'multiline':False]['text':' This function has only a single output, so it gets only one gradient','line_number':97,'multiline':False]['text':' Sound behaviors, tested for working capture','line_number':219,'multiline':False]['text':' I pulled all of these test cases from test_autograd.py','line_number':516,'multiline':False]['text':' In the future, we should make the Dynamo test suite actually','line_number':517,'multiline':False]['text':' run on test_autograd.py (it's disabled right now) and delete these.','line_number':518,'multiline':False]['text':' this is equivalent, but uses the output of .forward() in .backward()','line_number':563,'multiline':False]['text':' handling clone and view is so dynamo fakefication passes, it's not','line_number':735,'multiline':False]['text':' intended to be handling user code','line_number':736,'multiline':False]['text':' access some data from `x`, where `x` is a tensor subclass','line_number':752,'multiline':False]['text':' create and return a tensor subclass from within a torch.autograd.Function','line_number':754,'multiline':False]['text':' Weird that this is needed, but not having this breaks a lot of things','line_number':765,'multiline':False]