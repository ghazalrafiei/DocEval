['text':' Owner(s): ["module: inductor"]','line_number':1,'multiline':False]['text':' not tested','line_number':60,'multiline':False]['text':' not tested','line_number':64,'multiline':False]['text':' not tested','line_number':65,'multiline':False]['text':' not tested except upsampling and interpolate ops','line_number':69,'multiline':False]['text':' Success forces pass; failure forces fail; skip unconditionally skips testing','line_number':75,'multiline':False]['text':' Note, in these skip/xfail dictionaries use a string as the key','line_number':157,'multiline':False]['text':' for the default test, and a tuple of two strings for variants','line_number':158,'multiline':False]['text':' flaky','line_number':164,'multiline':False]['text':' flaky','line_number':165,'multiline':False]['text':' Jiterator kernel is not expected to work with inductor','line_number':180,'multiline':False]['text':' flaky','line_number':186,'multiline':False]['text':' Tensors are not alike','line_number':196,'multiline':False]['text':' half_to_float is only valid for the CUDA implementation','line_number':204,'multiline':False]['text':' intentionally not handled','line_number':243,'multiline':False]['text':' Note: if you get a "AssertionError: Couldn't find OpInfo for ..." error for an OpInfo you are sure','line_number':283,'multiline':False]['text':' exists, you might be trying to use a test variant and you need to replace, for example,','line_number':284,'multiline':False]['text':' "max.reduction_no_dim" with ("max", "reduction_no_dim") as the key of one of these dictionaries','line_number':285,'multiline':False]['text':' key can be either op_name, or (op_name, deivce_type), or (op_name, device_type, dtype)','line_number':304,'multiline':False]['text':' the return value of empty is undefined','line_number':306,'multiline':False]['text':' decomp affects accuracy','line_number':315,'multiline':False]['text':' Following tests are failing with strict comparision but atol=1 is acceptable due roundings errors','line_number':351,'multiline':False]['text':' Temporarily skip interpolat bicubic tests:','line_number':356,'multiline':False]['text':' We have better precision than eager','line_number':367,'multiline':False]['text':' Always test with all sample for following ops','line_number':373,'multiline':False]['text':' failed_reasons[device_type, op_key].add(repr(e))','line_number':414,'multiline':False]['text':' inductor kernels failing this test intermittently','line_number':432,'multiline':False]['text':' Skip dtype=torch.uint8 for all ops except upsample and interpolate:','line_number':453,'multiline':False]['text':' with open("test_output.txt", "a") as f:','line_number':467,'multiline':False]['text':'     print(f"CONSIDERING OP {op_name} on {device_type} with {dtype} |','line_number':468,'multiline':False]['text':' {inductor_skips[device_type].get(op_name, set())}", flush=True, file=f)','line_number':469,'multiline':False]['text':'     print(f"CONSIDERING OP {op_name} on {device_type} with {dtype} |','line_number':470,'multiline':False]['text':' {inductor_skips[device_type].get(op_name, set())}", flush=True)','line_number':471,'multiline':False]['text':' with open("test_output.txt", "a") as f:','line_number':474,'multiline':False]['text':'     print(f"SKIPPING OP {op_name} on {device_type}", flush=True, file=f)','line_number':475,'multiline':False]['text':'     print(f"SKIPPING OP {op_name} on {device_type}", flush=True)','line_number':476,'multiline':False]['text':' TODO: OpInfo really ought to error out for this case, but it's','line_number':504,'multiline':False]['text':' not exercised in test_ops_gradients atm.  The problem is not','line_number':505,'multiline':False]['text':' complex32 per-se (which is supported by data movement only ops)','line_number':506,'multiline':False]['text':' but that when we do backwards we expect other ops like add to work','line_number':507,'multiline':False]['text':' TODO - enable this, running into errors','line_number':552,'multiline':False]['text':' (','line_number':554,'multiline':False]['text':'     lambda: torch._inductor.config.patch(','line_number':555,'multiline':False]['text':'         {"fallback_random": True, "implicit_fallbacks": True}','line_number':556,'multiline':False]['text':'     ),','line_number':557,'multiline':False]['text':'     {"assert_equal": True},','line_number':558,'multiline':False]['text':' ),','line_number':559,'multiline':False]['text':' UNCOMMENT TO DEBUG SEGFAULTS','line_number':571,'multiline':False]['text':' with open("test_output.txt", "a") as f:','line_number':573,'multiline':False]['text':'     print(f"RUNNING OP {op_name} on {device_type} with {dtype}", flush=True, file=f)','line_number':574,'multiline':False]['text':'     print(f"RUNNING OP {op_name} on {device_type} with {dtype}", flush=True)','line_number':575,'multiline':False]['text':' opinfo test case have already place the input on the correct device','line_number':577,'multiline':False]['text':' so we don't need do additional copy by setting copy_to_cuda=False','line_number':578,'multiline':False]['text':' skip checking gradient on CPU for now','line_number':608,'multiline':False]['text':' with open("test_output.txt", "a") as f:','line_number':634,'multiline':False]['text':'     print(f"SUCCEEDED OP {op_name} on {device_type} with {dtype}", flush=True, file=f)','line_number':635,'multiline':False]