['text':' Owner(s): ["module: inductor"]','line_number':1,'multiline':False]['text':' too slow','line_number':135,'multiline':False]['text':' We use the patch context manager instead of using it as a decorator.','line_number':236,'multiline':False]['text':' In this way, we can ensure that the attribute is patched and unpatched correctly','line_number':237,'multiline':False]['text':' even if this run_and_get_cpp_code function is called multiple times.','line_number':238,'multiline':False]['text':' Store expected dtypes so we can check actual result gives the correct types','line_number':285,'multiline':False]['text':' Eager model may fail if the dtype is not supported','line_number':290,'multiline':False]['text':' check_lowp is ignored here, it's kept just to be able to call `common` with extra arg','line_number':301,'multiline':False]['text':' downcast the model back if needed','line_number':327,'multiline':False]['text':' if not called:','line_number':348,'multiline':False]['text':'     exp = torch._dynamo.explain(run)(*example_inputs)','line_number':349,'multiline':False]['text':'     print("Explain:", exp[0])','line_number':350,'multiline':False]['text':'     for graph in exp[2]:','line_number':351,'multiline':False]['text':'         print("Graph", graph)','line_number':352,'multiline':False]['text':' In case of input mutations, check that inputs are the same','line_number':388,'multiline':False]['text':' our testing sometimes uses higher precision inputs for the reference','line_number':395,'multiline':False]['text':' generate random unit norm gradients','line_number':417,'multiline':False]['text':' See Note [Detaching inputs that never need gradients]','line_number':429,'multiline':False]['text':' There are a handful of ops that can return None gradients, into of zero gradients.','line_number':430,'multiline':False]['text':' If all inputs to an AOTAutograd graph are supposed to get None gradients,','line_number':431,'multiline':False]['text':' AOTAutograd will end up forcing all of the outputs of the forward to not require grad.','line_number':432,'multiline':False]['text':' There's no easy fix to this (see the note above), although one option is to','line_number':433,'multiline':False]['text':' force any derivative formulas in core to return tensors of zeros instead of None.','line_number':434,'multiline':False]['text':' Find indexing expressions','line_number':543,'multiline':False]['text':' Remove store value and mask','line_number':548,'multiline':False]['text':' indirect indexing involves a `tmp` variable','line_number':557,'multiline':False]['text':' fix https://github.com/pytorch/pytorch/issues/115071','line_number':674,'multiline':False]['text':' should not inplace write to the input','line_number':683,'multiline':False]['text':' generator not yet supported in dynamo','line_number':740,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/93380','line_number':762,'multiline':False]['text':' From torchbench.pyhpc_equation_of_state','line_number':794,'multiline':False]['text':' Test torch._test_inductor_realize forces a buffer to be realized','line_number':816,'multiline':False]['text':' From torchbench.pyhpc_equation_of_state','line_number':828,'multiline':False]['text':' e.g. x=[1, 2, 3], n=2 => returns [1, 1, 2, 2, 3, 3]','line_number':867,'multiline':False]['text':' this should be collapsed to direct indexing','line_number':873,'multiline':False]['text':' e.g. x=[1, 2, 3], n=2 => returns [1, 2, 3, 1, 2, 3]','line_number':881,'multiline':False]['text':' this should be collapsed to direct indexing','line_number':888,'multiline':False]['text':' match ternary operator','line_number':903,'multiline':False]['text':' Constant propagate a constant that's negative','line_number':935,'multiline':False]['text':' Wrapping is constant-folded','line_number':940,'multiline':False]['text':' Operation where we can't prove that the index is always positive or negative','line_number':943,'multiline':False]['text':' It has wrapping but no assert','line_number':948,'multiline':False]['text':' We currently don't do constant propagation with float constants','line_number':951,'multiline':False]['text':' Constant is propagated as we can prove that the result is always negative.','line_number':959,'multiline':False]['text':' Mismatched elements: 2 / 10 (20.0%)','line_number':998,'multiline':False]['text':' Greatest absolute difference: 0.0029296875 at index (8,) (up to 1e-05 allowed)','line_number':999,'multiline':False]['text':' Greatest relative difference: 0.0017482517482517483 at index (6,) (up to 0.001 allowed)','line_number':1000,'multiline':False]['text':' FIXME: a.argmax','line_number':1034,'multiline':False]['text':' FIXME: a.argmin','line_number':1042,'multiline':False]['text':' small sized reductions will get unrolled','line_number':1095,'multiline':False]['text':' make sure things also work if they aren't unrolled','line_number':1099,'multiline':False]['text':' fp16 nyi for cpu','line_number':1103,'multiline':False]['text':' Requires masked loading for the intermediate reduction','line_number':1116,'multiline':False]['text':' Persistent reductions','line_number':1229,'multiline':False]['text':' Non-persistent reduction','line_number':1233,'multiline':False]['text':' NOTE: use assertEqual to check dtypes which self.common doesn't do','line_number':1302,'multiline':False]['text':' Test that float arguments are truncated to int when dtype is set explicitly','line_number':1313,'multiline':False]['text':' example taken from hf_BigBird','line_number':1401,'multiline':False]['text':' example taken from hf_BigBird','line_number':1417,'multiline':False]['text':' tensor with shape 0 in any dimension','line_number':1433,'multiline':False]['text':' x.view(dtype)','line_number':1458,'multiline':False]['text':' without manual_seed, there is some chance this test fails due to:','line_number':1500,'multiline':False]['text':' https://github.com/openai/triton/issues/530','line_number':1501,'multiline':False]['text':' with *100 we are always getting a number exactly at .5 which we don't do right in half','line_number':1504,'multiline':False]['text':' TODO(voz): Re-enable this test ASAP https://github.com/pytorch/pytorch/issues/82763','line_number':1526,'multiline':False]['text':' a much more elaborate test is required to match finfo max's for float and half','line_number':1542,'multiline':False]['text':' divide a scalar','line_number':1607,'multiline':False]['text':' treat boolean as integer','line_number':1620,'multiline':False]['text':' FIXME: returns the wrong dtype','line_number':1654,'multiline':False]['text':' 4.5 GiB','line_number':1742,'multiline':False]['text':' Test 64-bit indexing works correctly','line_number':1748,'multiline':False]['text':' self.common OOMs here because it copies inputs to check for mutations','line_number':1755,'multiline':False]['text':' Test 64-bit indexing works correctly when inputs are less than 32-bit','line_number':1765,'multiline':False]['text':' but intermediate tensors require 64-bit indexing','line_number':1766,'multiline':False]['text':' self.common OOMs here because it copies inputs to check for mutations','line_number':1776,'multiline':False]['text':' Can't use assertEqual as it expands broadcasted inputs','line_number':1793,'multiline':False]['text':' Test 64-bit indexing is used when input views a tensor that can be','line_number':1800,'multiline':False]['text':' indexed with 32-bit strides but the storage offset pushes it over','line_number':1801,'multiline':False]['text':' INT_MAX','line_number':1802,'multiline':False]['text':' Test 64-bit indexing is used when input numel is less than INT_MAX','line_number':1816,'multiline':False]['text':' but stride calculations go above INT_MAX','line_number':1817,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/98979','line_number':1965,'multiline':False]['text':' noqa: UP008','line_number':2101,'multiline':False]['text':' TODO: Autograd internal assertion','line_number':2117,'multiline':False]['text':' For gpu path, there is an accuracy issue','line_number':2150,'multiline':False]['text':' fails dynamic check which bn is fused, and there will not have loops vars.','line_number':2154,'multiline':False]['text':' TODO: GPU path doesn't support channels_last now.','line_number':2192,'multiline':False]['text':' For gpu path, there is an accuracy issue','line_number':2209,'multiline':False]['text':' Define a BatchNorm using functional BN.','line_number':2213,'multiline':False]['text':' TODO: if statement only here to tell the jit to skip emitting this when it is None','line_number':2242,'multiline':False]['text':' type: ignore[has-type]','line_number':2243,'multiline':False]['text':' type: ignore[has-type]','line_number':2244,'multiline':False]['text':' use cumulative moving average','line_number':2245,'multiline':False]['text':' use exponential moving average','line_number':2249,'multiline':False]['text':' If buffers are not to be tracked, ensure that they won't be updated','line_number':2259,'multiline':False]['text':' Perform the forward pass','line_number':2308,'multiline':False]['text':' no to channels last permuting before kernel','line_number':2315,'multiline':False]['text':' in out should do channels last in inference','line_number':2318,'multiline':False]['text':' Create the convolution layer','line_number':2323,'multiline':False]['text':' should be channels last permuting before kernel','line_number':2330,'multiline':False]['text':' 0d tensor','line_number':2421,'multiline':False]['text':' negative index out of range','line_number':2446,'multiline':False]['text':' negative index out of range','line_number':2447,'multiline':False]['text':' cpu doesn't understand fp16, and there are explicit .cpu() calls','line_number':2601,'multiline':False]['text':' TODO: https://github.com/pytorch/pytorch/issues/92627','line_number':2606,'multiline':False]['text':' Mismatched elements: 10 / 2352 (0.4%)','line_number':2666,'multiline':False]['text':' Greatest absolute difference: 5.7220458984375e-05 at index (0, 3, 12, 12) (up to 1e-05 allowed)','line_number':2667,'multiline':False]['text':' Greatest relative difference: 0.06512477175897748 at index (0, 4, 11, 9) (up to 0.001 allowed)','line_number':2668,'multiline':False]['text':' transposed conv','line_number':2675,'multiline':False]['text':' Test stride or padding or dilation is 1 element list.','line_number':2690,'multiline':False]['text':' only weight is channels_last','line_number':2726,'multiline':False]['text':' only activation is channels_last','line_number':2732,'multiline':False]['text':' activation and weight are all channels_last','line_number':2738,'multiline':False]['text':' only weight is channels_last','line_number':2762,'multiline':False]['text':' only weight is channels_last','line_number':2781,'multiline':False]['text':' only activation is channels_last','line_number':2786,'multiline':False]['text':' activation and weight are all channels_last','line_number':2791,'multiline':False]['text':' lowering to avg_pool2d case','line_number':2809,'multiline':False]['text':' no-op case','line_number':2815,'multiline':False]['text':' Big kernel size, use fallback','line_number':2822,'multiline':False]['text':' with padding','line_number':3038,'multiline':False]['text':' with padding','line_number':3062,'multiline':False]['text':' Too big kernel size, use fallback','line_number':3080,'multiline':False]['text':' From https://github.com/pytorch/pytorch/issues/94775','line_number':3091,'multiline':False]['text':' ceil mode turns on','line_number':3093,'multiline':False]['text':' From https://github.com/pytorch/pytorch/issues/93384','line_number':3104,'multiline':False]['text':' dialtion is not 1, use fallback','line_number':3106,'multiline':False]['text':' Large kernel size, use fallback','line_number':3186,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/100987','line_number':3198,'multiline':False]['text':' Mismatched elements: 127 / 746496 (0.0%)','line_number':3230,'multiline':False]['text':' Greatest absolute difference: 0.0009765625 at index (1, 62, 7, 16) (up to 1e-05 allowed)','line_number':3231,'multiline':False]['text':' Greatest relative difference: 0.05187467899332306 at index (14, 18, 11, 0) (up to 0.001 allowed)','line_number':3232,'multiline':False]['text':' Unrolled reduction','line_number':3396,'multiline':False]['text':' too painful to match types of bn model','line_number':3448,'multiline':False]['text':' From yolov3','line_number':3451,'multiline':False]['text':' Absolute difference: 0.0003662109375 (up to 0.0001 allowed)','line_number':3598,'multiline':False]['text':' Relative difference: 1.8804297408767818e-05 (up to 1e-05 allowed)','line_number':3599,'multiline':False]['text':' if we have a copy there will be more than 1 kernel','line_number':3657,'multiline':False]['text':' TODO: Remove dtype once https://github.com/pytorch/pytorch/issues/94010 is fixed','line_number':3765,'multiline':False]['text':' Mismatched elements: 9 / 256 (3.5%)','line_number':3772,'multiline':False]['text':' Greatest absolute difference: 2.491354329061828e+28 at index (6, 6) (up to 1e-05 allowed)','line_number':3773,'multiline':False]['text':' Greatest relative difference: 2.9793410720160818e-05 at index (4, 5) (up to 1.3e-06 allowed)','line_number':3774,'multiline':False]['text':' power of 0.5 is special-cased, arbitrary power would still produce triton codegen error','line_number':3780,'multiline':False]['text':' fails dynamic check for 'has a dynamic dimension'','line_number':3874,'multiline':False]['text':' accuracy issues with relatively large matmuls','line_number':3912,'multiline':False]['text':' Constant folding was explicitly turned off due to issue #108388','line_number':3916,'multiline':False]['text':' Turn it back on for test','line_number':3917,'multiline':False]['text':' test no-op','line_number':3925,'multiline':False]['text':' noqa: E731','line_number':3930,'multiline':False]['text':' noqa: E731','line_number':3934,'multiline':False]['text':' noqa: E731','line_number':3938,'multiline':False]['text':' noqa: E731','line_number':3942,'multiline':False]['text':' test dtype conversion','line_number':3956,'multiline':False]['text':' test broadcasted shape bail','line_number':3965,'multiline':False]['text':' noqa: E731','line_number':3966,'multiline':False]['text':' can't use self.common because input is modified inplace','line_number':4031,'multiline':False]['text':' again with bool types','line_number':4210,'multiline':False]['text':' Repro for https://github.com/pytorch/pytorch/issues/97968','line_number':4228,'multiline':False]['text':' no redundant copy','line_number':4371,'multiline':False]['text':' difference in rnn is too large between half and float inputs','line_number':4485,'multiline':False]['text':' Mismatched elements: 154697 / 1486848 (10.4%)','line_number':4609,'multiline':False]['text':' Greatest absolute difference: 0.0001976490020751953 at index (0, 0, 101, 243) (up to 1e-05 allowed)','line_number':4610,'multiline':False]['text':' Greatest relative difference: 7.332530120481928 at index (1, 1, 258, 301) (up to 1.3e-06 allowed)','line_number':4611,'multiline':False]['text':' Mismatched elements: 10 / 196608 (0.0%)','line_number':4623,'multiline':False]['text':' Greatest absolute difference: 1.3869255781173706e-05 at index (2, 1, 88, 65) (up to 1e-05 allowed)','line_number':4624,'multiline':False]['text':' Greatest relative difference: 0.0033082996811011046 at index (3, 1, 88, 91) (up to 1.3e-06 allowed)','line_number':4625,'multiline':False]['text':' Test that index propagation doesn't generate bad index_expr calls like','line_number':4634,'multiline':False]['text':' ops.index_expr(0.5*x, dtype) where the expression is not integral','line_number':4635,'multiline':False]['text':' Test that float indexing expressions participate in type promotion','line_number':4649,'multiline':False]['text':' Repro for https://github.com/pytorch/pytorch/issues/93351','line_number':4725,'multiline':False]['text':' The following 2 tests are meant to check the logic that drops','line_number':4781,'multiline':False]['text':' xmask from triton load/store if xnumel = 1','line_number':4782,'multiline':False]['text':' This test is meant to check for issues from the logic','line_number':4802,'multiline':False]['text':' that drops xmask from trito load/store if XBLOCK divides xnumel','line_number':4803,'multiline':False]['text':' assumption is that XBLOCK is always a divisor of 1024','line_number':4811,'multiline':False]['text':' so xmask will be dropped iff xnumel is multiple of 1024','line_number':4812,'multiline':False]['text':' NOTE: this test fails when none of the inputs require grad.','line_number':4886,'multiline':False]['text':' That seems like an inductor bug.','line_number':4887,'multiline':False]['text':' TODO, fix: See https://github.com/pytorch/pytorch/issues/94693','line_number':4972,'multiline':False]['text':' Mismatched elements: 1212 / 76800 (1.6%)','line_number':5180,'multiline':False]['text':' Greatest absolute difference: 0.001953125 at index (0, 0, 93) (up to 1e-05 allowed)','line_number':5181,'multiline':False]['text':' Greatest relative difference: 1.0 at index (3, 19, 4) (up to 0.001 allowed)','line_number':5182,'multiline':False]['text':' workaround for https://github.com/openai/triton/issues/558','line_number':5238,'multiline':False]['text':' a, b[0] are not broadcastable','line_number':5259,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/97104','line_number':5260,'multiline':False]['text':' from GPT2ForSequenceClassification','line_number':5386,'multiline':False]['text':' src can be a scalar','line_number':5594,'multiline':False]['text':' Mismatched elements: 1 / 1885 (0.1%)','line_number':5596,'multiline':False]['text':' Greatest absolute difference: 0.00018310546875 at index (0, 0, 3) (up to 1e-05 allowed)','line_number':5597,'multiline':False]['text':' Greatest relative difference: 0.0022371364653243847 at index (0, 0, 3) (up to 0.001 allowed)','line_number':5598,'multiline':False]['text':' src can be a scalar','line_number':5650,'multiline':False]['text':' issue #1150','line_number':5749,'multiline':False]['text':' same seed, same values','line_number':5853,'multiline':False]['text':' different calls, different values','line_number':5858,'multiline':False]['text':' same seed, same values','line_number':5879,'multiline':False]['text':' different calls, different values','line_number':5884,'multiline':False]['text':' Ideally, we would like to use torch.compile for these operators. But','line_number':5896,'multiline':False]['text':' currently the plan is to introduce these operators at the partitioner','line_number':5897,'multiline':False]['text':' level, obviating the need to support them fully through the','line_number':5898,'multiline':False]['text':' torch.compile stack. To ensure that we have good enough debugging with','line_number':5899,'multiline':False]['text':' minifiers, we have ensure that they work with make_fx. This test uses','line_number':5900,'multiline':False]['text':' make_fx to do the testing. In future, we can move on torch.compile.','line_number':5901,'multiline':False]['text':' same seed, same values','line_number':5962,'multiline':False]['text':' different calls, different values','line_number':5965,'multiline':False]['text':' Check non-multiple of 4 numel','line_number':5970,'multiline':False]['text':' rand_like with kwargs `device` of str type','line_number':6029,'multiline':False]['text':' rand_like with `device` which is different from `x.device`','line_number':6044,'multiline':False]['text':' From https://github.com/pytorch/torchdynamo/issues/1200','line_number':6108,'multiline':False]['text':' From https://github.com/pytorch/torchdynamo/issues/1352','line_number':6133,'multiline':False]['text':' Window size is too big. Should fallback','line_number':6161,'multiline':False]['text':' From https://github.com/pytorch/pytorch/issues/93384','line_number':6187,'multiline':False]['text':' dilation is not 1. Should fallback','line_number':6189,'multiline':False]['text':' codegen mm kernel from template','line_number':6324,'multiline':False]['text':' dropped elements should match','line_number':6363,'multiline':False]['text':' dropped should be close to 0.33','line_number':6367,'multiline':False]['text':' eval mode should be all ones','line_number':6377,'multiline':False]['text':' second run is same result as first','line_number':6397,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/94055','line_number':6457,'multiline':False]['text':' Unrolled reduction','line_number':6497,'multiline':False]['text':' Persistent reduction','line_number':6501,'multiline':False]['text':' Non-persistent reduction','line_number':6505,'multiline':False]['text':' Unrolled reduction','line_number':6521,'multiline':False]['text':' Persistent reduction','line_number':6527,'multiline':False]['text':' Non-persistent reduction','line_number':6533,'multiline':False]['text':' TODO: Remove once https://github.com/pytorch/pytorch/issues/94017 is resolved','line_number':6691,'multiline':False]['text':' non-contiguous inputs for lerp','line_number':6811,'multiline':False]['text':' contiguous inputs for lerp','line_number':6816,'multiline':False]['text':' by matmul, inputs should be deallocated','line_number':6886,'multiline':False]['text':' TODO: should not be necessary, ref-cycle ?','line_number':6887,'multiline':False]['text':' do an extra run to make sure we are deallocating on warmup and record','line_number':6900,'multiline':False]['text':' for some reason, TorchDispatch doesnt capture the','line_number':6914,'multiline':False]['text':' cuda mm call (even without cudagraphs)','line_number':6915,'multiline':False]['text':' Shape padding causes the inputs to all get specialized, so the codegen','line_number':6945,'multiline':False]['text':' test fails','line_number':6946,'multiline':False]['text':' channel dim must be > 64 for inductor to do layout optimization and use NHWC','line_number':6980,'multiline':False]['text':' Importantly, since inductor._config.keep_output_stride is True,','line_number':6996,'multiline':False]['text':' the outputs should have matching strides here.','line_number':6997,'multiline':False]['text':' Constant must not get matched as constant','line_number':7007,'multiline':False]['text':' TIMM convit_base model: https://github.com/pytorch/pytorch/issues/97877.','line_number':7011,'multiline':False]['text':' TODO: support cuda path.','line_number':7012,'multiline':False]['text':' From HF hf_BigBird model.','line_number':7037,'multiline':False]['text':' Repro from vision_maskrcnn','line_number':7052,'multiline':False]['text':' If assume_static_by_default is set, the calls above will trigger','line_number':7127,'multiline':False]['text':' 3 function compilation:','line_number':7128,'multiline':False]['text':'   1. assuming 'n' is static (equals 2)','line_number':7129,'multiline':False]['text':'   2. making 'n' dynamic, but with the guard 'end <= x.shape[0]'','line_number':7130,'multiline':False]['text':'      (from: torch._inductor.ir.SliceView.create)','line_number':7131,'multiline':False]['text':' Negative index triggers new compilation.','line_number':7135,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/93374','line_number':7169,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/94725','line_number':7174,'multiline':False]['text':' expanded dim should not cause copy in require_stride_order','line_number':7243,'multiline':False]['text':' inputs','line_number':7293,'multiline':False]['text':' as_strided inputs are depend on input's size and stide.','line_number':7383,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96446','line_number':7401,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96728','line_number':7409,'multiline':False]['text':' noqa: B015','line_number':7414,'multiline':False]['text':' noqa: B015','line_number':7422,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/97127','line_number':7429,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/97124','line_number':7438,'multiline':False]['text':' Calling div only torch.SymInt arguments is not yet supported.','line_number':7583,'multiline':False]['text':' To support this behavior, we need to allow const-propping tensors that store symint data.','line_number':7584,'multiline':False]['text':' For now, dynamo will explicitly graph break when it encounters user code with this behavior.','line_number':7585,'multiline':False]['text':' It's a view so it doens't generate a kernel','line_number':7635,'multiline':False]['text':' domain for erfinv is (-1, 1)','line_number':7665,'multiline':False]['text':' to pass lowp check on GPU','line_number':7696,'multiline':False]['text':' to pass lowp check on GPU','line_number':7697,'multiline':False]['text':' The first two values should be the same, attention output','line_number':7705,'multiline':False]['text':' and logsumexp since dropout is not being set','line_number':7706,'multiline':False]['text':' Causes a @pointwise(size_hints) where size_hints is 2D','line_number':7773,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/102857','line_number':7829,'multiline':False]['text':' before the fix, the wrong index expression','line_number':7916,'multiline':False]['text':' 'i1 + 3 * i0' is cached.','line_number':7917,'multiline':False]['text':' needed to introduce config that exceed max shared memory usage','line_number':7948,'multiline':False]['text':' Use shape (2**24, 65) rather than (2**24, 128) potentially avoid OOM in','line_number':7964,'multiline':False]['text':' CI while still keep the same up-rounded size-hints.','line_number':7965,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/113013','line_number':7989,'multiline':False]['text':' noqa: B902','line_number':8042,'multiline':False]['text':' You cannot copy functions in Python, so we use closures here to','line_number':8045,'multiline':False]['text':' create objects with different ids. Otherwise, unittest.skip','line_number':8046,'multiline':False]['text':' would modify all methods sharing the same object id. Also, by','line_number':8047,'multiline':False]['text':' using a default argument, we create a copy instead of a','line_number':8048,'multiline':False]['text':' reference. Otherwise, we would lose access to the value.','line_number':8049,'multiline':False]['text':' Copy __dict__ which may contain test metadata','line_number':8055,'multiline':False]['text':' kernel0 reduces from 256 to (xnumel=8, rnumel=8192), which means it reduces 256 by 256 into an array of','line_number':8167,'multiline':False]['text':' size 8 by accumulating 8192 elements at once note that rnumel is equal to 512 * 16, so rnumel which is','line_number':8168,'multiline':False]['text':' at slot 3 should be in the divisible by 16 descriptor','line_number':8169,'multiline':False]['text':' kernel1 reduces from 8 elements to a single scalar.','line_number':8175,'multiline':False]['text':' The function with the constrained tensor should be optimized, but','line_number':8215,'multiline':False]['text':' the other should not:','line_number':8216,'multiline':False]['text':' there are a couple extra tensors created in `insertable_tensor_check`','line_number':8265,'multiline':False]['text':' See https://github.com/pytorch/pytorch/issues/100348','line_number':8268,'multiline':False]['text':' asserts not implemented in Rocm yet','line_number':8283,'multiline':False]['text':' indirect indexing involves a `tmp` variable','line_number':8293,'multiline':False]['text':' aten.index','line_number':8318,'multiline':False]['text':' Check that there's indirect indexing...','line_number':8326,'multiline':False]['text':' We elide the assert for static shapes','line_number':8329,'multiline':False]['text':' ...but we generate an upper bound for dynamic shapes','line_number':8332,'multiline':False]['text':' aten.index_put','line_number':8340,'multiline':False]['text':' Correctness','line_number':8351,'multiline':False]['text':' We have an indirect store via atomic_add','line_number':8356,'multiline':False]['text':' We cannot elide he assert in this case','line_number':8358,'multiline':False]['text':' Disable constant propagation, so we isolate value range analysis','line_number':8433,'multiline':False]['text':' this cannot be optimized away, value too large','line_number':8458,'multiline':False]['text':' Disable constant propagation, so we isolate value range analysis','line_number':8462,'multiline':False]['text':' this can be optimized away, value too large','line_number':8484,'multiline':False]['text':' Disable index propagation, so the indirect indexing isn't optimized away','line_number':8510,'multiline':False]['text':' load should be masked','line_number':8520,'multiline':False]['text':' split(tensor, sympy.Integer), split(tensor, sympy.Expr)','line_number':8596,'multiline':False]['text':' TODO: Triton's JITFunction._type_of has no support for complex','line_number':8763,'multiline':False]['text':' complex,','line_number':8764,'multiline':False]['text':' torch.complex64,','line_number':8774,'multiline':False]['text':' torch.complex128,','line_number':8775,'multiline':False]