['text':' Owner(s): ["module: inductor"]','line_number':1,'multiline':False]['text':' TorchDispatch doesn't work in our cuda invocation for some reason','line_number':92,'multiline':False]['text':' For CPU and mkldnn enable, we always using channles last','line_number':113,'multiline':False]['text':' Check that _flat_weights are not functional_tensor, otherwise','line_number':428,'multiline':False]['text':' deepcopy will fail during recompilation.','line_number':429,'multiline':False]['text':' num of mkldnn_rnn_layer call + 2 view call on the concatenated hy, cy.','line_number':440,'multiline':False]['text':' Change input sizes','line_number':443,'multiline':False]['text':' This case is unsupported','line_number':521,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/100344.','line_number':527,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/100988.','line_number':547,'multiline':False]['text':' TODO: OMP parallel reduction order is not deterministic.','line_number':598,'multiline':False]['text':' Hence, the accurarcy might vary up and down. For short term,','line_number':599,'multiline':False]['text':' we increase the tolerance and will fix it later by using','line_number':600,'multiline':False]['text':' aten parallel.','line_number':601,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/93365','line_number':605,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/98149','line_number':616,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/96484','line_number':625,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/101340','line_number':636,'multiline':False]['text':' From llama model.','line_number':712,'multiline':False]['text':' From HF AllenaiLongformerBase.','line_number':747,'multiline':False]['text':' For quantized_decomposed.dequantize_per_tensor','line_number':803,'multiline':False]['text':' Refer to torch/ao/quantization/fx/_decomposed.py','line_number':804,'multiline':False]['text':' For quantized_decomposed.quantize_per_tensor','line_number':810,'multiline':False]['text':' Refer to torch/ao/quantization/fx/_decomposed.py','line_number':811,'multiline':False]['text':' fp16 inputs are not supported for C++ wrappers on CPU yet','line_number':1267,'multiline':False]['text':' fp16 inputs are not supported for C++ wrappers on CPU yet','line_number':1415,'multiline':False]['text':' constant needs to be propagated on fallback','line_number':1577,'multiline':False]['text':' The moset inner loop variable is used in the index_expr','line_number':1728,'multiline':False]['text':' The moset inner loop variable is used in the index_expr','line_number':1833,'multiline':False]['text':' Most inner loop variable irrevalant','line_number':1848,'multiline':False]['text':' Most inner loop variable irrevalant but max value is greater than','line_number':1866,'multiline':False]['text':' the max value of INT32','line_number':1867,'multiline':False]['text':' Most inner loop variable irrevalant but min value is greater than','line_number':1881,'multiline':False]['text':' the min value of INT32','line_number':1882,'multiline':False]['text':' Currently, we enabled AVX2 and AVX512 for vectorization. If the platform is not','line_number':1971,'multiline':False]['text':' supported, the vectorization will not work and skip this test case. For ARM or','line_number':1972,'multiline':False]['text':' other platforms support, we just need to add the ISA info to the supported_vector_isa','line_number':1973,'multiline':False]['text':' and include proper aten vectorization head file.','line_number':1974,'multiline':False]['text':' Current, there are some limitations as follows.','line_number':1981,'multiline':False]['text':'   rsqrt:','line_number':1982,'multiline':False]['text':'     assert [both a fallback and a decomp for same kernel: aten.rsqrt.default]','line_number':1983,'multiline':False]['text':'   round:','line_number':1984,'multiline':False]['text':'     couldn't find symbolic meta function/decomposition','line_number':1985,'multiline':False]['text':'   fmod/logical_and/logic_or:','line_number':1986,'multiline':False]['text':'     vec kernel has not support to_type','line_number':1987,'multiline':False]['text':' From part of timm HaloAttn:','line_number':2226,'multiline':False]['text':' (https://github.com/rwightman/pytorch-image-models/blob/main/timm/layers/halo_attn.py#L97).','line_number':2227,'multiline':False]['text':' Fixed https://github.com/pytorch/pytorch/issues/94269 accuracy issue.','line_number':2228,'multiline':False]['text':' def and use','line_number':2273,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/98573','line_number':2341,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/100800','line_number':2351,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/100466','line_number':2362,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/104515','line_number':2371,'multiline':False]['text':' 2 generated kernels (one for var_mean, the other for result)','line_number':2632,'multiline':False]['text':' TODO: support vectorization for int div','line_number':2645,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/113016','line_number':2649,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/113016','line_number':2658,'multiline':False]['text':' https://github.com/pytorch/pytorch/issues/113018','line_number':2667,'multiline':False]['text':' FIXME: returns the wrong dtype','line_number':2711,'multiline':False]['text':' we are doing direct load/store, make sure we do not generate','line_number':2715,'multiline':False]['text':' redundant type casts','line_number':2716,'multiline':False]