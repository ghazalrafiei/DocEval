['text':' Owner(s): ["NNC"]','line_number':1,'multiline':False]['text':' Compared to aten logic, NNC coudl save addtional BF16/Fp32 conversion.','line_number':697,'multiline':False]['text':' Take d = a + b - c as an example, the aten logic is as follows at','line_number':698,'multiline':False]['text':' operator level:','line_number':699,'multiline':False]['text':'    tmp = to_bf16(to_fp32(a) + to_fp32(b))','line_number':700,'multiline':False]['text':'    d = to_bf16(to_fp32(tmp) + to_fp32(c))','line_number':701,'multiline':False]['text':' But NNC could fuse the compression and remove the redudant conversions.','line_number':702,'multiline':False]['text':' The final statement is as follows','line_number':703,'multiline':False]['text':'    d = to_bf16(to_fp32(a) + to_fp32(b) + to_fp32(c))','line_number':704,'multiline':False]['text':' Hence, we simulate NNC computation by feeding fp32 tensors and converting','line_number':705,'multiline':False]['text':' the result tensor back to bf16. The simulation could avoid the numeric','line_number':706,'multiline':False]['text':' deviation to simplify the result comprasion','line_number':707,'multiline':False]['text':' lgamma_cuda does not support BF16','line_number':894,'multiline':False]['text':' nans','line_number':916,'multiline':False]['text':' TODO: reenable. Currently all of the tests fail','line_number':917,'multiline':False]['text':' traced = torch.jit.trace(torch_fn, (ins, ins))','line_number':918,'multiline':False]['text':' x = warmup_and_run_forward(traced, rand_a, rand_b)','line_number':919,'multiline':False]['text':' y = torch_fn(nans, rand_b)','line_number':920,'multiline':False]['text':' try:','line_number':921,'multiline':False]['text':'     np.testing.assert_allclose(x.cpu().numpy(), y.cpu().numpy())','line_number':922,'multiline':False]['text':'     print("Succeeded on dev=", dev, "function=", torch_fn)','line_number':923,'multiline':False]['text':' except AssertionError:','line_number':924,'multiline':False]['text':'     # Print extra info before exiting:','line_number':925,'multiline':False]['text':'     print("Failed on dev=", dev, "function=", torch_fn)','line_number':926,'multiline':False]['text':'     # np.testing.assert_allclose(x.cpu().numpy(), y.cpu().numpy())','line_number':927,'multiline':False]['text':' random floats','line_number':1009,'multiline':False]['text':' div by 0','line_number':1022,'multiline':False]['text':' numerators and denominatos are nan','line_number':1029,'multiline':False]['text':' Test channels-last','line_number':1090,'multiline':False]['text':' This test checks that we correctly handle fusion group with just aten::cat in it.','line_number':1101,'multiline':False]['text':' Note that the test only makes sense with min_fusion_group=1, otherwise no','line_number':1102,'multiline':False]['text':' fusion groups would be formed at all.','line_number':1103,'multiline':False]['text':' TODO: Fix and re-enable the test.','line_number':1104,'multiline':False]['text':' now test with only empty tensors','line_number':1173,'multiline':False]['text':' Use eager mode as reference.','line_number':1284,'multiline':False]['text':' A wild broadcast appears.','line_number':1388,'multiline':False]['text':' Mismatched shapes shouldn't reach codegen.','line_number':1396,'multiline':False]['text':' Changing a static dimension fails guards.','line_number':1405,'multiline':False]['text':' x, y, z = [torch.rand(4, 7).cuda() for _ in range(3)]','line_number':1406,'multiline':False]['text':' xn, yn, zn = [t.cpu().numpy() for t in (x, y, z)]','line_number':1407,'multiline':False]['text':' res = test(x, y, z)','line_number':1408,'multiline':False]['text':' print(test.graph_for(x, y, z))','line_number':1409,'multiline':False]['text':' np.testing.assert_allclose(res.cpu().numpy(), xn * yn * zn)','line_number':1410,'multiline':False]['text':' smaller, easier to debug example','line_number':1517,'multiline':False]['text':' more dims','line_number':1534,'multiline':False]['text':' Now do the aliasing','line_number':1575,'multiline':False]['text':' A bug reported internally similar to the one reported in #48533','line_number':1654,'multiline':False]