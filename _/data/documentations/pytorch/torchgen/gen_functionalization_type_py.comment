['text':' Note: [Mutable Ops Not Using Functionalization]','line_number':49,'multiline':False]['text':' Ops in this list currently do not work with functionalization and should be fixed.','line_number':50,'multiline':False]['text':' It will be BC-breaking, but we should fix their schemas.','line_number':56,'multiline':False]['text':' should be inplace?','line_number':57,'multiline':False]['text':' See Note [resize_ in Functionalization]','line_number':59,'multiline':False]['text':' This function is used as for testing purposes only.','line_number':62,'multiline':False]['text':' This file contains codegen that relates to the functionalization pass.','line_number':67,'multiline':False]['text':' It includes:','line_number':68,'multiline':False]['text':' - gen_functionalization_definition','line_number':69,'multiline':False]['text':'     Generates dispatcher kernel definitions for the functionalization pass.','line_number':70,'multiline':False]['text':' - gen_functionalization_registration','line_number':71,'multiline':False]['text':'     Generates dispatcher kernel registrations for the functionalization pass.','line_number':72,'multiline':False]['text':' - gen_functionalization_view_inverse_declaration','line_number':73,'multiline':False]['text':'     Generates a declaration for an "inverse view", for every view op','line_number':74,'multiline':False]['text':'     that is needed in functionalization. We manually implement their definitions.','line_number':75,'multiline':False]['text':' - gen_composite_view_copy_kernel','line_number':76,'multiline':False]['text':'     Generates view_copy() composite kernels for all view_copy operators.','line_number':77,'multiline':False]['text':' Generates the body of the default composite C++ kernel for a {view}_copy NativeFunction','line_number':80,'multiline':False]['text':' See Note [view_copy NativeFunctions]','line_number':81,'multiline':False]['text':' We can make view_copy work in more cases by using reshape()','line_number':94,'multiline':False]['text':' when a normal view call would ordinarily fail.','line_number':95,'multiline':False]['text':' This also makes LTC more efficient, because they don't need to include','line_number':96,'multiline':False]['text':' clone() calls in their graph (which is normally needed by reshape).','line_number':97,'multiline':False]['text':' view_copy is a native signature, since we're generating an at::native:: kernel','line_number':111,'multiline':False]['text':' Functionalization always operates on symints though','line_number':112,'multiline':False]['text':' view is a dispatcher signature, since we're calling into the at::_ops API','line_number':117,'multiline':False]['text':' view ops today always return either a Tensor or a list of Tensors','line_number':125,'multiline':False]['text':' If the return type is a list, we need to clone each tensor in the list.','line_number':135,'multiline':False]['text':' The default generated composite kernel for {view}_copy() operators just clones','line_number':143,'multiline':False]['text':' the input tensor, and runs the underlying view on the clone.','line_number':144,'multiline':False]['text':' We need to wrap / unwrap various arguments from the op in the functionalization kernels.','line_number':183,'multiline':False]['text':' Some op schemas include non-owning types though (like TensorList),','line_number':184,'multiline':False]['text':' and when we unwrap them we expect to get out an owning type!.','line_number':185,'multiline':False]['text':' We also return a lambda that tells you how to conver the non-owning type argument into the owning type.','line_number':186,'multiline':False]['text':' There are technically other non-owning types out there (like IntArrayRef),','line_number':192,'multiline':False]['text':' but functionalization only actually cares about the ones involving tensors.','line_number':193,'multiline':False]['text':' unwraps all tensor-like arguments, returning:','line_number':197,'multiline':False]['text':' (1) a string containing all of the logic that does the unwrapping','line_number':198,'multiline':False]['text':' (2) a context, to be used by translate(), with all of the relevant bindings.','line_number':199,'multiline':False]['text':' for tensor inputs, we want to unwrap them before passing them into the redispatch calls.','line_number':207,'multiline':False]['text':' For most ops, the functionalization needs to sync any pending updates on the input tensors','line_number':209,'multiline':False]['text':' before calling the operator, since otherwise the operator will act on stale data.','line_number':210,'multiline':False]['text':' For view ops though, we can continue to defer syncing until the tensor is used by','line_number':211,'multiline':False]['text':' a non-view operator.','line_number':212,'multiline':False]['text':' for non-tensor inputs, we want to pass them directly into the redispatch calls.','line_number':231,'multiline':False]['text':' converts  all tensor-like arguments to meta tensors, which are used to compute stride info. Returns:','line_number':237,'multiline':False]['text':' (1) a string containing all of the logic that does the conversions.','line_number':238,'multiline':False]['text':' (2) a context, to be used by translate(), with all of the relevant bindings.','line_number':239,'multiline':False]['text':' for tensor inputs, we want to unwrap them before passing them into the redispatch calls.','line_number':245,'multiline':False]['text':' for non-tensor inputs, we want to pass them directly into the redispatch calls.','line_number':251,'multiline':False]['text':' The functionalization codegen currently expects view op schemas to have this form:','line_number':257,'multiline':False]['text':' foo(Tensor(a), ...) -> Tensor(a) (e.g. transpose)','line_number':258,'multiline':False]['text':' foo(Tensor(a!), ...) -> Tensor(a!) (e.g. transpose_)','line_number':259,'multiline':False]['text':' The first argument is a tensor with an alias semantics (annotations)','line_number':265,'multiline':False]['text':' No other arguments have aliasing semantics','line_number':270,'multiline':False]['text':' Generates the Functionalization kernel for:','line_number':277,'multiline':False]['text':' - ops that create aliases (e.g. transpose())','line_number':278,'multiline':False]['text':' - ops that are views AND mutations (e.g. transpose_())','line_number':279,'multiline':False]['text':' This op is both an inplace op AND a view op.','line_number':284,'multiline':False]['text':' See Note [Functionalization Pass - Inplace View Ops] for details.','line_number':285,'multiline':False]['text':' I currently have the view meta call into the out-of-place variant of the view, to avoid','line_number':286,'multiline':False]['text':' having to define an extra ~20 inplace {view}_inverse_ functions.','line_number':287,'multiline':False]['text':' Most view ops don't have NativeFunctionGroup's both, because we don't define out= variants for view ops.','line_number':288,'multiline':False]['text':' I'm assuming that every inplace-view op has a corresponding out-of-place view op,','line_number':289,'multiline':False]['text':' with the same name but the trailing underscore removed.','line_number':290,'multiline':False]['text':' This is currently asserted at parse time in gen.py (see error_check_native_functions).','line_number':291,'multiline':False]['text':' the "view_copy" op name that the functionalization kernels need to call','line_number':301,'multiline':False]['text':' Sometimes the functionalization pass needs to no-op (e.g. if it was passed non-functional tensors)','line_number':303,'multiline':False]['text':' "no-op"ing in this context is just redispatching to the original op.','line_number':304,'multiline':False]['text':' The meta API call should use the same arguments, but convert all tensors to meta tensors first.','line_number':324,'multiline':False]['text':' See Note [Functionalization Pass - Inplace View Ops] for more details','line_number':331,'multiline':False]['text':' Given a NativeFunction, and a variable name corresponding to the output of redispatching on the function,','line_number':439,'multiline':False]['text':' this returns two lists of names, consisting of:','line_number':440,'multiline':False]['text':' - the names of returns corresponding to the original (mutable) inputs of the outer function','line_number':441,'multiline':False]['text':' - the names of returns corresponding to the (immutable) outputs of the inner redispatched function','line_number':442,'multiline':False]['text':' When functionalization "no-op's" and redispatches on a mutable operator, we need to take care so that:','line_number':460,'multiline':False]['text':'  - For fresh outputs, we return the result of the redispatch (without wrapping outputs)','line_number':461,'multiline':False]['text':'  - For outputs that were aliased to inputs, we return the inputs directly (since some of them might have been wrapped)','line_number':462,'multiline':False]['text':' Just get all of the return names, and immediately return them','line_number':467,'multiline':False]['text':' The outer function may have a mix of aliased and non-aliased outputs,','line_number':482,'multiline':False]['text':' But the inner functional op that we're transforming to should only have non-aliased outputs','line_number':483,'multiline':False]['text':' First, take all of the newly created outputs from the inner call and wrap them into functional tensors','line_number':488,'multiline':False]['text':' Next, take all of the mutated outputs from the inner call corresponding to mutated inputs,','line_number':501,'multiline':False]['text':' and propagate the mutations','line_number':502,'multiline':False]['text':' Finally, we return:','line_number':514,'multiline':False]['text':' - Any mutable arguments that also returns','line_number':515,'multiline':False]['text':' - Any immutable returns that were created wrapping the output from the inner call','line_number':516,'multiline':False]['text':' Generates the Functionalization kernel for:','line_number':526,'multiline':False]['text':' - mutation ops (inplace and out= ops)','line_number':527,'multiline':False]['text':' mutation case','line_number':532,'multiline':False]['text':' all mutable inputs must be functional tensors in order to participate in functionalization','line_number':556,'multiline':False]['text':' These are used in the cases where we don't functionalize and redispatch to the inplace op','line_number':579,'multiline':False]['text':' case 1: we hit an inplace op that doesn't have an out-of-place equivalent','line_number':580,'multiline':False]['text':' case 2: we hit an inplace ops but our inputs are not functional tensors (in which case our kernel just no-ops)','line_number':581,'multiline':False]['text':' call the out-of-place variant of the op','line_number':587,'multiline':False]['text':' We don't want to run the inplace meta func for ops like .set_(), because:','line_number':620,'multiline':False]['text':' (1) they're unnecessary: inplace meta checks are only useful for ops like add_(),','line_number':621,'multiline':False]['text':'     where broadcasting will work for the out-of-place case but should fail on the inplace call','line_number':622,'multiline':False]['text':' (2) They'll also fail without adding extra infra: we'd need to convert the input storage argument','line_number':623,'multiline':False]['text':'     into a meta storage','line_number':624,'multiline':False]['text':' The below functions generate RegisterFunctionalization.cpp','line_number':666,'multiline':False]['text':' These files provide the kernels that run the functionalization pass, which can be opted into','line_number':667,'multiline':False]['text':' per backend (e.g. XLA or Vulkan), or as a composable transform (functionalize() in functorch).','line_number':668,'multiline':False]['text':' See Note [Functionalization Pass: View Inverses].','line_number':671,'multiline':False]['text':' For every (non-composite) view op, we need a corresponding "inverse view" function.','line_number':675,'multiline':False]['text':' This generates the declarations so we get a good compiler error when someone adds a new view.','line_number':676,'multiline':False]['text':' Don't generate kernels in mobile build','line_number':698,'multiline':False]['text':' functionalization needs to register kernels for view + view_inplace ops','line_number':703,'multiline':False]['text':' See Note [Functionalization <> torch.Tensor constructor]','line_number':704,'multiline':False]['text':' Gets a hand-written functionalization kernel','line_number':719,'multiline':False]['text':' See Note [Functionalization <> torch.Tensor constructor]','line_number':734,'multiline':False]['text':' See Note [resize_ in Functionalization]','line_number':737,'multiline':False]['text':' functionalization needs to generate and register kernels for inplace ops.','line_number':740,'multiline':False]['text':' We *also* need to directly register CompositeImplicitAUtograd kernels','line_number':741,'multiline':False]['text':' so that they decompose properly before functioanlization.','line_number':742,'multiline':False]['text':' Note: Ideally this code should never have to look at NativeFunction','line_number':750,'multiline':False]['text':' (and instead only need to operate on grouped NativeFunctions).','line_number':751,'multiline':False]['text':' The only reason currently is because we need to emit direct dispatch registrations','line_number':752,'multiline':False]['text':' For CompositeImplicitAutograd operators, which are potentially ungrouped.','line_number':753,'multiline':False]['text':' Don't generate kernels in mobile build','line_number':756,'multiline':False]['text':' Case 1: emit view -> view_copy kernels for the functionalization pass','line_number':761,'multiline':False]['text':' invariant: NativeFunctionsViewGroup's always have a view_copy operator','line_number':764,'multiline':False]['text':' if the view is not composite (implicit autograd)','line_number':765,'multiline':False]['text':' Invariant: all mutable operators that we need to handle in functionalization','line_number':772,'multiline':False]['text':' should have been properly grouped up.','line_number':773,'multiline':False]['text':' TODO: The below ops all have "problematic" schemas that prevent them from','line_number':774,'multiline':False]['text':' getting functionalized. Instead of bending over backwards to get things to work,','line_number':775,'multiline':False]['text':' I think we should either:','line_number':776,'multiline':False]['text':' (1) fix their schemas (BC-breaking)','line_number':777,'multiline':False]['text':' (2) hand-write their functionalization kernels','line_number':778,'multiline':False]['text':' Case 2: emit inplace -> out-of-place kernels for the functionalization pass','line_number':783,'multiline':False]