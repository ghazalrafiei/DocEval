['text':' The function isn't used by this key (since only functional ops have a kernel for this key),','line_number':143,'multiline':False]['text':' so we need to not include it to avoid a defined-but-not-used error.','line_number':144,'multiline':False]['text':' Generates Register{dispatch}.cpp (e.g., RegisterCPU.cpp).','line_number':203,'multiline':False]['text':'','line_number':204,'multiline':False]['text':'   - The primary function of this file is to register all of the','line_number':205,'multiline':False]['text':'     implementations for the given dispatch key to the dispatcher,','line_number':206,'multiline':False]['text':'     so they are available for use in PyTorch.  If dispatch is','line_number':207,'multiline':False]['text':'     None, we generate schema (def) registrations and catchall','line_number':208,'multiline':False]['text':'     registrations.','line_number':209,'multiline':False]['text':'   - The secondary function of this file is to generate a wrapper','line_number':210,'multiline':False]['text':'     around functions.  In CPUType these wrappers do nothing','line_number':211,'multiline':False]['text':'     (and should be removed), but in other cases they handle','line_number':212,'multiline':False]['text':'     DeviceGuard. A small extra benefit of wrappers is they','line_number':213,'multiline':False]['text':'     are not overloaded, so they can be used in the registration','line_number':214,'multiline':False]['text':'     API without having to disambiguate which overload you want','line_number':215,'multiline':False]['text':'     (as would be the case if you directly registered native::','line_number':216,'multiline':False]['text':'     functions).','line_number':217,'multiline':False]['text':'   - The tertiary function of this file is to generate *static*','line_number':218,'multiline':False]['text':'     cpp API bindings which can be used to bypass dispatcher','line_number':219,'multiline':False]['text':'     directly to kernels, but with user-friendly cpp-style API','line_number':220,'multiline':False]['text':' Selector object to determine which operators to generate','line_number':232,'multiline':False]['text':' registration code for.','line_number':233,'multiline':False]['text':' Whether or not we are actually code-genning for ROCm','line_number':236,'multiline':False]['text':' Whether or not to generate symint registrations or not.  External users','line_number':239,'multiline':False]['text':' of codegen who don't care about symints can set this to false to get','line_number':240,'multiline':False]['text':' non-SymInt codegen','line_number':241,'multiline':False]['text':' The class that all unstructured native functions live under. This is used to improve','line_number':244,'multiline':False]['text':' compiler error messages when a kernel writer adds a native function with the wrong signature.','line_number':245,'multiline':False]['text':' This is only used in unstructured kernels, since structured kernels already live in a class.','line_number':246,'multiline':False]['text':' Finally, this field is currently Optional because it is only used by external backends.','line_number':247,'multiline':False]['text':' It would be nice if we can add the same logic to in-tree kernels too, but that requires updating','line_number':248,'multiline':False]['text':' all of the existing kernel signatures scattered across aten/src/ATen/native.','line_number':249,'multiline':False]['text':' Only set to true in lightweight dispatch. If lightweight dispatch is enabled we are registering','line_number':252,'multiline':False]['text':' operators into JIT op registry, thus we need to avoid generating code to register into the dispatcher.','line_number':253,'multiline':False]['text':' Only tensor like arguments are eligible','line_number':266,'multiline':False]['text':' Note: We call gen_structured() if the operator is marked structured, regardless of the backend.','line_number':276,'multiline':False]['text':' gen_structured() has special logic to handle auto-generated kernels.','line_number':277,'multiline':False]['text':' The prefix is just to ensure uniqueness. The Dispatcher API doesn't guarantee unique kernel names.','line_number':293,'multiline':False]['text':' Defer to composites for meta implementation','line_number':391,'multiline':False]['text':' Inplace list operations are not supported','line_number':394,'multiline':False]['text':' We want to generate inplace/out wrappers, that don't have a kernel for the backend.','line_number':403,'multiline':False]['text':' See Note [Direct dispatch bindings]','line_number':423,'multiline':False]['text':' TODO: dedupe this with the structured codegen','line_number':428,'multiline':False]['text':' short circuit for inplace_meta','line_number':449,'multiline':False]['text':' TODO: handle in place on tensor list','line_number':453,'multiline':False]['text':' short circuit for generated inplace/out wrappers','line_number':462,'multiline':False]['text':' Backends that require device guards presumably also require device checks.','line_number':484,'multiline':False]['text':' default','line_number':493,'multiline':False]['text':' kernel is creating a tensor','line_number':500,'multiline':False]['text':' CUDA requires special handling','line_number':504,'multiline':False]['text':' kernel is operating on existing tensors','line_number':510,'multiline':False]['text':' There is precedence for which argument we use to do','line_number':512,'multiline':False]['text':' device guard.  This describes the precedence order.','line_number':513,'multiline':False]['text':' Only tensor like arguments are eligible','line_number':525,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #','line_number':560,'multiline':False]['text':'','line_number':561,'multiline':False]['text':'                           STRUCTURED','line_number':562,'multiline':False]['text':'','line_number':563,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #','line_number':564,'multiline':False]['text':' returns the definition of a ctor, as well as how to construct','line_number':656,'multiline':False]['text':' this class to a variable named op','line_number':657,'multiline':False]['text':' TODO: Make sure out argument is guaranteed to be self','line_number':662,'multiline':False]['text':' TODO: Move to OptionalMPSGuard.','line_number':708,'multiline':False]['text':' TODO: Now, there is something interesting going on here.  In the code below,','line_number':739,'multiline':False]['text':' we generate CompositeExplicitAutogradNonFunctional implementations of functional and inplace','line_number':740,'multiline':False]['text':' based on the out implementation.  But in fact, out is definable by','line_number':741,'multiline':False]['text':' functional too (just not very efficiently), and this is honestly the','line_number':742,'multiline':False]['text':' MORE likely situation for a backend implementor.  How do we pick?','line_number':743,'multiline':False]['text':' Well, taking a page from Haskell type classes and default methods,','line_number':744,'multiline':False]['text':' we could conceivably register a circular definition (out in terms','line_number':745,'multiline':False]['text':' of functional, and functional in terms of out) and just require','line_number':746,'multiline':False]['text':' someone to implement one or the other.  We'd have to do a little bit','line_number':747,'multiline':False]['text':' of work to not register one of these "weak" definitions unless there','line_number':748,'multiline':False]['text':' is a strong definition somewhere in the DAG!  So it's not implemented yet.','line_number':749,'multiline':False]['text':' Never generate a default implementation for out, that's what you','line_number':755,'multiline':False]['text':' have to define as a backend implementor','line_number':756,'multiline':False]['text':' Note [Direct dispatch bindings]','line_number':759,'multiline':False]['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':760,'multiline':False]['text':' Signature of the non-dispatched function we'll expose in a header','line_number':761,'multiline':False]['text':' (e.g., at::cpu::add).  We don't generate methods (TODO: do this','line_number':762,'multiline':False]['text':' when CPUTensor class is a thing); nor do we generate fallback','line_number':763,'multiline':False]['text':' bindings for manual_cpp_binding functions.','line_number':764,'multiline':False]['text':' Signature of the wrapper function we'll register to the dispatcher','line_number':769,'multiline':False]['text':' Construct the body of the wrapper function with signature sig','line_number':800,'multiline':False]['text':' We'll use context to keep track of any variables we've brought','line_number':802,'multiline':False]['text':' into scope while generating code','line_number':803,'multiline':False]['text':' Initialize the class corresponding to this structured','line_number':806,'multiline':False]['text':' operator; feeding it the output argument(s) if it is known','line_number':807,'multiline':False]['text':' TODO: dedup this branch','line_number':815,'multiline':False]['text':' Translate the input native arguments into structured','line_number':842,'multiline':False]['text':' arguments for the meta call','line_number':843,'multiline':False]['text':' If this function group has precomputed elements, the meta function','line_number':852,'multiline':False]['text':' returns a struct containing them which must be saved so that it','line_number':853,'multiline':False]['text':' can be unpacked when generating code to call the impl.','line_number':854,'multiline':False]['text':' Put all of the contents of the precompute struct into the context','line_number':857,'multiline':False]['text':' so that translate will be able to return the correct args for the','line_number':858,'multiline':False]['text':' call to the impl.','line_number':859,'multiline':False]['text':' Add a use of the precompute struct so FB internal compilers don't','line_number':873,'multiline':False]['text':' complain that there is an unused variable.','line_number':874,'multiline':False]['text':' After running meta, op.outputs_ is guaranteed to be valid;','line_number':879,'multiline':False]['text':' add it to the context','line_number':880,'multiline':False]['text':' TODO: Stop hardcoding that the output type is a Tensor.  Note','line_number':893,'multiline':False]['text':' that for the codegen here this is fine because outputs_ is','line_number':894,'multiline':False]['text':' hardcoded to be tensor already','line_number':895,'multiline':False]['text':' With the expanded context, do the impl call (if not a meta','line_number':902,'multiline':False]['text':' function)','line_number':903,'multiline':False]['text':' TODO: https://github.com/pytorch/pytorch/issues/53023','line_number':908,'multiline':False]['text':' TODO: I think this means structured won't work with method','line_number':918,'multiline':False]['text':' only functions (but maybe you're saved by faithful? iunno.)','line_number':919,'multiline':False]['text':' NB: Originally I wrote this as an at::redispatch call, but','line_number':920,'multiline':False]['text':' I got in trouble because that meant I needed a DispatchKeySet','line_number':921,'multiline':False]['text':' in the wrapper function, which meant I needed a DispatchKeySet','line_number':922,'multiline':False]['text':' in the DispatchKeyFunctions declarations, but the defined API','line_number':923,'multiline':False]['text':' there does NOT permit a dispatch key set.  I think you can','line_number':924,'multiline':False]['text':' probably unwind this by calling some function to do the TLS','line_number':925,'multiline':False]['text':' fetch and get the DispatchKeySet when you don't have it, but','line_number':926,'multiline':False]['text':' I didn't do it for this version','line_number':927,'multiline':False]['text':' Go over each output, and check if there is a proxy created for it.','line_number':938,'multiline':False]['text':' If so, copy it over to the original output.','line_number':939,'multiline':False]['text':' Destructively return the final tensors','line_number':946,'multiline':False]['text':' TODO: Do this in translate instead','line_number':947,'multiline':False]['text':' small optimization','line_number':950,'multiline':False]['text':' For an overview of what this template code looks like, see','line_number':969,'multiline':False]['text':' https://github.com/pytorch/rfcs/pull/9','line_number':970,'multiline':False]['text':' Silence mypy's "Missing return statement" error','line_number':988,'multiline':False]