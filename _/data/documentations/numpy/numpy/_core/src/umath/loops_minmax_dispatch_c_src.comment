['text':'@targets
 ** $maxopt baseline
 ** neon asimd
 ** sse2 avx2 avx512_skx
 ** vsx2
 ** vx vxe
 *','line_number':1,'multiline':True]['text':' Provides the various *_LOOP macros','line_number':16,'multiline':False]['text':'******************************************************************************
 ** Scalar intrinsics
 *****************************************************************************','line_number':19,'multiline':True]['text':' signed/unsigned int','line_number':22,'multiline':False]['text':' fp, propagates NaNs','line_number':25,'multiline':False]['text':' fp, ignores NaNs','line_number':34,'multiline':False]['text':' special optimization for fp scalars propagates NaNs','line_number':42,'multiline':False]['text':' since there're no C99 support for it','line_number':43,'multiline':False]['text':'*begin repeat
 * #type = npy_float, npy_double#
 * #sfx = f32, f64#
 * #c_sfx = f, d#
 * #isa_sfx = s, d#
 * #sse_type = __m128, __m128d#
 ','line_number':45,'multiline':True]['text':'*begin repeat1
 * #op = max, min#
 * #neon_instr = fmax, fmin#
 ','line_number':52,'multiline':True]['text':' X86 handle second operand','line_number':62,'multiline':False]['text':' SSE2','line_number':71,'multiline':False]['text':' __aarch64__','line_number':83,'multiline':False]['text':'*end repeat1*','line_number':84,'multiline':True]['text':'*end repeat*','line_number':85,'multiline':True]['text':' NPY_DISABLE_OPTIMIZATION','line_number':86,'multiline':False]['text':' mapping to double if its possible','line_number':87,'multiline':False]['text':'*begin repeat
 * #op = max, min, maxp, minp#
 ','line_number':89,'multiline':True]['text':'*end repeat*','line_number':94,'multiline':True]['text':'******************************************************************************
 ** Defining the SIMD kernels
 *****************************************************************************','line_number':97,'multiline':True]['text':'*begin repeat
 * #sfx = s8, u8, s16, u16, s32, u32, s64, u64, f32, f64#
 * #simd_chk = NPY_SIMD*8, NPY_SIMD_F32, NPY_SIMD_F64#
 * #is_fp = 0*8, 1, 1#
 * #scalar_sfx = i*8, f, d#
 ','line_number':100,'multiline':True]['text':'*begin repeat1
 * # intrin = max, min, maxp, minp#
 * # fp_only = 0, 0, 1, 1#
 ','line_number':106,'multiline':True]['text':' propagates NaNs','line_number':114,'multiline':False]['text':' contiguous input.','line_number':121,'multiline':False]['text':' Scalar - finish up any remaining iterations','line_number':155,'multiline':False]['text':' contiguous inputs and output.','line_number':163,'multiline':False]['text':' Note, 6x unroll was chosen for best results on Apple M1','line_number':169,'multiline':False]['text':' To avoid memory bandwidth bottleneck','line_number':172,'multiline':False]['text':' Scalar - finish up any remaining iterations','line_number':220,'multiline':False]['text':' non-contiguous for float 32/64-bit memory access','line_number':227,'multiline':False]['text':' simd_chk && (!fp_only || (is_fp && fp_only))','line_number':268,'multiline':False]['text':'*end repeat1*','line_number':271,'multiline':True]['text':'*end repeat*','line_number':272,'multiline':True]['text':'******************************************************************************
 ** Defining ufunc inner functions
 *****************************************************************************','line_number':274,'multiline':True]['text':'*begin repeat
 * #TYPE = UBYTE, USHORT, UINT, ULONG, ULONGLONG,
 *         BYTE, SHORT, INT, LONG, LONGLONG,
 *         FLOAT, DOUBLE, LONGDOUBLE#
 *
 * #BTYPE = BYTE, SHORT, INT,  LONG, LONGLONG,
 *          BYTE, SHORT, INT, LONG, LONGLONG,
 *          FLOAT, DOUBLE, LONGDOUBLE#
 * #type = npy_ubyte, npy_ushort, npy_uint, npy_ulong, npy_ulonglong,
 *         npy_byte, npy_short, npy_int, npy_long, npy_longlong,
 *         npy_float, npy_double, npy_longdouble#
 *
 * #is_fp = 0*10, 1*3#
 * #is_unsigned = 1*5, 0*5, 0*3#
 * #scalar_sfx = i*10, f, d, l#
 ','line_number':277,'multiline':True]['text':'*begin repeat1
 * #len = 8, 16, 32, 64#
 ','line_number':295,'multiline':True]['text':'*end repeat1*','line_number':312,'multiline':True]['text':'*begin repeat1
 * # kind = maximum, minimum, fmax, fmin#
 * # intrin = max, min, maxp, minp#
 * # fp_only = 0, 0, 1, 1#
 ','line_number':315,'multiline':True]['text':' reduce and contiguous','line_number':334,'multiline':False]['text':' no overlap and operands are binary contiguous','line_number':345,'multiline':False]['text':' unroll scalars faster than non-contiguous vector load/store on Arm','line_number':352,'multiline':False]['text':' TO_SIMD_SFX','line_number':367,'multiline':False]['text':' scalar unrolls','line_number':369,'multiline':False]['text':' Note, 8x unroll was chosen for best results on Apple M1','line_number':371,'multiline':False]['text':' Note, 4x unroll was chosen for best results on Apple M1','line_number':417,'multiline':False]['text':' Note, we can't just load all, do all ops, then store all here.
             * Sometimes ufuncs are called with `accumulate`, which makes the
             * assumption that previous iterations have finished before next
             * iteration.  For example, the output of iteration 2 depends on the
             * result of iteration 1.
             ','line_number':420,'multiline':True]['text':'*begin repeat2
             * #unroll = 0, 1, 2, 3#
             ','line_number':427,'multiline':True]['text':'*end repeat2*','line_number':433,'multiline':True]['text':' NPY_DISABLE_OPTIMIZATION','line_number':436,'multiline':False]['text':' !fp_only || (is_fp && fp_only)','line_number':479,'multiline':False]['text':'*end repeat1*','line_number':480,'multiline':True]['text':'*end repeat*','line_number':481,'multiline':True]