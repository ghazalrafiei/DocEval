['text':'!/bin/bash','line_number':1,'multiline':False]['text':' MODEL="$LLAMA_MODEL_DIR/openllama-3b-v2-q8_0.gguf" # This is the model the readme uses.','line_number':10,'multiline':False]['text':' An f16 model. Note in this case with "-g", you get an f32-format .BIN file that isn't yet supported if you use it with "main --lora" with GPU inferencing.','line_number':11,'multiline':False]