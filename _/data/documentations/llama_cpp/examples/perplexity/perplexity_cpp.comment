['text':' possible loss of data','line_number':14,'multiline':False]['text':' Subtract the maximum logit value from the current logit value for numerical stability','line_number':87,'multiline':False]['text':' Download: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research','line_number':147,'multiline':False]['text':' Run `./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw`','line_number':148,'multiline':False]['text':' Output: `perplexity: 13.5106 [114/114]`','line_number':149,'multiline':False]['text':' BOS tokens will be added for each chunk before eval','line_number':150,'multiline':False]['text':'fprintf(stderr, "%s: evaluating %d...%d using %d batches\n", __func__, start, end, num_batches);','line_number':204,'multiline':False]['text':' clear the KV cache','line_number':210,'multiline':False]['text':'fprintf(stderr, "    Batch %d: starts at %d, size is %d, n_past is %d\n",j,batch_start,batch_size,j * n_batch);','line_number':217,'multiline':False]['text':'fprintf(stderr, "%s : failed to eval\n", __func__);','line_number':219,'multiline':False]['text':' save original token and restore it after eval','line_number':223,'multiline':False]['text':' add BOS token for the first batch of each chunk','line_number':226,'multiline':False]['text':'fprintf(stderr, "%s: using tokens %d...%d\n",__func__,params.n_ctx - params.ppl_stride + start, params.n_ctx + start);','line_number':252,'multiline':False]['text':' Calculate probability of next token, given the previous ones.','line_number':255,'multiline':False]['text':' perplexity is e^(average negative log-likelihood)','line_number':267,'multiline':False]['text':' Download: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research','line_number':285,'multiline':False]['text':' Run `./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw`','line_number':286,'multiline':False]['text':' Output: `perplexity: 13.5106 [114/114]`','line_number':287,'multiline':False]['text':' BOS tokens will be added for each chunk before eval','line_number':288,'multiline':False]['text':' clear the KV cache','line_number':338,'multiline':False]['text':' save original token and restore it after eval','line_number':345,'multiline':False]['text':' add BOS token for the first batch of each chunk','line_number':348,'multiline':False]['text':' restore the original token in case it was set to BOS','line_number':358,'multiline':False]['text':' We get the logits for all the tokens in the context window (params.n_ctx)','line_number':378,'multiline':False]['text':' from llama_eval above.  Now, based on https://huggingface.co/docs/transformers/perplexity,','line_number':379,'multiline':False]['text':' calculate the perplexity over the last half of the window (so the model always has','line_number':380,'multiline':False]['text':' some context to predict the token).','line_number':381,'multiline':False]['text':'','line_number':382,'multiline':False]['text':' We rely on the fact that attention in the forward pass only looks at previous','line_number':383,'multiline':False]['text':' tokens here, so the logits returned for each token are an accurate representation','line_number':384,'multiline':False]['text':' of what the model would have predicted at that point.','line_number':385,'multiline':False]['text':'','line_number':386,'multiline':False]['text':' Example, we have a context window of 512, we will compute perplexity for each of the','line_number':387,'multiline':False]['text':' last 256 tokens.  Then, we split the input up into context window size chunks to','line_number':388,'multiline':False]['text':' process the entire prompt.','line_number':389,'multiline':False]['text':' perplexity is e^(average negative log-likelihood)','line_number':395,'multiline':False]['text':' Calculates hellaswag score (acc_norm) from prompt','line_number':445,'multiline':False]['text':'','line_number':446,'multiline':False]['text':' Data extracted from the HellaSwag validation dataset (MIT license) https://github.com/rowanz/hellaswag/blob/master/data/hellaswag_val.jsonl','line_number':447,'multiline':False]['text':' All used data fields are preprocessed as in https://github.com/EleutherAI/lm-evaluation-harness/blob/df3da98c5405deafd519c2ddca52bb7c3fe36bef/lm_eval/tasks/hellaswag.py#L62-L68','line_number':448,'multiline':False]['text':'','line_number':449,'multiline':False]['text':' All 10042 tasks should be extracted to keep the results standardized like other implementations.','line_number':450,'multiline':False]['text':'','line_number':451,'multiline':False]['text':' Datafile layout:','line_number':452,'multiline':False]['text':' ['??'] denotes json fields','line_number':453,'multiline':False]['text':' 6 lines per task:','line_number':454,'multiline':False]['text':' ['activity_label'] + ": " +['ctx']  - The first part of the query, the context','line_number':455,'multiline':False]['text':' ['label'] - The index the best common sense ending aka gold ending','line_number':456,'multiline':False]['text':' ['endings'][0] - Endings added to the first part of the query','line_number':457,'multiline':False]['text':' ['endings'][1]','line_number':458,'multiline':False]['text':' ['endings'][2]','line_number':459,'multiline':False]['text':' ['endings'][3]','line_number':460,'multiline':False]['text':' This is needed as usual for LLaMA models','line_number':481,'multiline':False]['text':' Number of tasks to use when computing the score','line_number':484,'multiline':False]['text':' The tasks should be randomized so the score stabilizes quickly.','line_number':489,'multiline':False]['text':' The random seed should not impact the final result if the computation is done over enough tasks, so kept hardcoded for now','line_number':492,'multiline':False]['text':' Dataholder for hellaswag tasks','line_number':495,'multiline':False]['text':' Select and read data from prompt lines','line_number':506,'multiline':False]['text':' Select a random example of those left in the prompt','line_number':511,'multiline':False]['text':' Delete the selected random example from the prompt','line_number':523,'multiline':False]['text':' Tokenize the context to count tokens','line_number':541,'multiline':False]['text':' Do the 1st ending','line_number':555,'multiline':False]['text':' In this case we include the context when evaluating','line_number':556,'multiline':False]['text':'auto query_embd = ::llama_tokenize(ctx, hs_data[task_idx].context + hs_data[task_idx].ending[0], add_bos);','line_number':557,'multiline':False]['text':' Stop if query wont fit the ctx window','line_number':561,'multiline':False]['text':' Speedup small evaluations by evaluating atleast 32 tokens','line_number':567,'multiline':False]['text':' clear the KV cache','line_number':572,'multiline':False]['text':' Calculate the logprobs over the ending','line_number':587,'multiline':False]['text':' Calculate the mean token logprob for acc_norm','line_number':598,'multiline':False]['text':' Do the remaining endings','line_number':601,'multiline':False]['text':' For these, we use the bare ending with n_past = context_size','line_number':602,'multiline':False]['text':'','line_number':603,'multiline':False]['text':' Tokenize the query','line_number':606,'multiline':False]['text':' Stop if query wont fit the ctx window','line_number':611,'multiline':False]['text':' Speedup small evaluations by evaluating atleast 32 tokens','line_number':617,'multiline':False]['text':' No, resizing to 32 is actually slightly slower (at least on CUDA)','line_number':618,'multiline':False]['text':'if (query_size < 32) {','line_number':619,'multiline':False]['text':'    query_embd.resize(32);','line_number':620,'multiline':False]['text':'}','line_number':621,'multiline':False]['text':' Evaluate the query','line_number':623,'multiline':False]['text':' Calculate the logprobs over the ending','line_number':633,'multiline':False]['text':' Calculate the mean token logprob for acc_norm','line_number':643,'multiline':False]['text':'            printf("task %lu, ending %lu, whole_len %lu, context_len %lu, ending_logprob_count %lu, ending_logprob %.4f\n",','line_number':647,'multiline':False]['text':'                task_idx,ending_idx,whole_size,context_size, hs_data[task_idx].ending_logprob_count[ending_idx], hs_data[task_idx].ending_logprob[ending_idx] );','line_number':648,'multiline':False]['text':' Find the ending with maximum logprob','line_number':651,'multiline':False]['text':'        printf("max logprob ending idx %lu, gold ending idx %lu\n", ending_logprob_max_idx, hs_data[task_idx].gold_ending_idx);','line_number':661,'multiline':False]['text':' If the gold ending got the maximum logprobe add one accuracy point','line_number':663,'multiline':False]['text':' Print the accumulated accuracy mean x 100','line_number':668,'multiline':False]['text':' load the model and apply lora adapter, if any','line_number':713,'multiline':False]['text':' print system information','line_number':726,'multiline':False]