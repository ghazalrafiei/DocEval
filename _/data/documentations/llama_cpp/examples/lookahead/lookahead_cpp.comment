['text':' n-gram container','line_number':19,'multiline':False]['text':' [n_vocab][G][N - 1]','line_number':32,'multiline':False]['text':' for each token of the vocab, keep a ring-buffer of capacity G of n-grams of size N - 1','line_number':33,'multiline':False]['text':' lookahead window','line_number':44,'multiline':False]['text':' n-gram size','line_number':45,'multiline':False]['text':' max verification n-grams','line_number':46,'multiline':False]['text':' LOG_DISABLE_LOGS','line_number':54,'multiline':False]['text':' init llama.cpp','line_number':56,'multiline':False]['text':' load the target model','line_number':62,'multiline':False]['text':' Tokenize the prompt','line_number':65,'multiline':False]['text':' eval the prompt','line_number':95,'multiline':False]['text':' used to determine end of generation','line_number':112,'multiline':False]['text':' for each decoded batch, we have at most W + G + 1 distinct sequences:','line_number':115,'multiline':False]['text':' seq_id == 0           : the current input token','line_number':116,'multiline':False]['text':' seq_id [1, W]         : tokens from the past N - 1 Jacobi iterations','line_number':117,'multiline':False]['text':' seq_id [W + 1, W + G] : verification n-grams','line_number':118,'multiline':False]['text':' target model sampling context','line_number':121,'multiline':False]['text':' verification n-grams','line_number':124,'multiline':False]['text':' tokens for the past N - 1 Jacobi iterations','line_number':127,'multiline':False]['text':' there are different ways to init these tokens','line_number':134,'multiline':False]['text':' initialize randomly from the prompt tokens','line_number':136,'multiline':False]['text':' initialize with a sequence of increasing numbers','line_number':139,'multiline':False]['text':' the input token belongs both to all sequences','line_number':147,'multiline':False]['text':' here we keep adding new n-grams as we go','line_number':153,'multiline':False]['text':' debug','line_number':156,'multiline':False]['text':' sample first token','line_number':161,'multiline':False]['text':' debug','line_number':176,'multiline':False]['text':' build the mask from https://lmsys.org/blog/2023-11-21-lookahead-decoding/','line_number':182,'multiline':False]['text':'','line_number':183,'multiline':False]['text':' Example for W = 5, N = 4, G = 2:','line_number':184,'multiline':False]['text':' (I = input, L = lookahead, V = verification)','line_number':185,'multiline':False]['text':'','line_number':186,'multiline':False]['text':' Batch:  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20','line_number':187,'multiline':False]['text':' T:        -2 -2 -2 -2 -1 -1 -1 -1 -1  0  0  0  0  0  0','line_number':188,'multiline':False]['text':' Info:   I  L  L  L  L  L  L  L  L  L  L  L  L  L  L  V  V  V  V  V  V','line_number':189,'multiline':False]['text':' Pos:    0  1  2  3  4  1  2  3  4  5  2  3  4  5  6  1  2  3  1  2  3   (+ n_past)','line_number':190,'multiline':False]['text':' Logits: 1  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1','line_number':191,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':192,'multiline':False]['text':' Seq:    0','line_number':193,'multiline':False]['text':'         1              1              1','line_number':194,'multiline':False]['text':'         2  2              2              2','line_number':195,'multiline':False]['text':'         3  3  3              3              3','line_number':196,'multiline':False]['text':'         4  4  4  4              4              4','line_number':197,'multiline':False]['text':'         5  5  5  5  5              5              5','line_number':198,'multiline':False]['text':'         6                                            6  6  6','line_number':199,'multiline':False]['text':'         7                                                     7  7  7','line_number':200,'multiline':False]['text':' ---------------------------------------------------------------------','line_number':201,'multiline':False]['text':'                                       |  |  |  |  |  |  |  |  |  |  |','line_number':202,'multiline':False]['text':'                                       V  V  V  V  V  |  |  |  |  |  |','line_number':203,'multiline':False]['text':'                                         j_tokens     |  |  |  |  |  |','line_number':204,'multiline':False]['text':'                                                      V  V  V  V  V  V','line_number':205,'multiline':False]['text':'                                                             id','line_number':206,'multiline':False]['text':' current token - first token of the first level','line_number':210,'multiline':False]['text':' verification n-grams - queue this before the lookahead tokens for less KV cache fragmentation','line_number':213,'multiline':False]['text':' fill the remaining W - 1 tokens for the first level','line_number':241,'multiline':False]['text':' fill the rest of the levels','line_number':251,'multiline':False]['text':' if no active ngrams are left, it means the sampled token does not pass the verification','line_number':269,'multiline':False]['text':' no more matches -> create a new batch','line_number':281,'multiline':False]['text':' sample the next token','line_number':287,'multiline':False]['text':' print','line_number':292,'multiline':False]['text':' print light cyan','line_number':299,'multiline':False]['text':' verify across active n-grams','line_number':318,'multiline':False]['text':' print known n-grams starting with token id (debug)','line_number':331,'multiline':False]['text':' update lookahead tokens','line_number':352,'multiline':False]['text':' sample from the last level','line_number':363,'multiline':False]['text':' there are different ways to init these tokens','line_number':369,'multiline':False]['text':' random init','line_number':371,'multiline':False]['text':' init from the previous level','line_number':374,'multiline':False]['text':' update observed ngrams','line_number':381,'multiline':False]['text':' the first token of the n-gram is determined by the index in the container so it is not stored','line_number':383,'multiline':False]['text':' n-gram generation','line_number':386,'multiline':False]['text':' ref: https://github.com/hao-ai-lab/LookaheadDecoding/issues/14#issuecomment-1826198518','line_number':387,'multiline':False]['text':' first token of the n-gram','line_number':389,'multiline':False]['text':' filter-out repeating n-grams','line_number':395,'multiline':False]['text':' KV cache management','line_number':440,'multiline':False]['text':' if no verification token matched, we simply remove all cells from this batch -> no fragmentation','line_number':441,'multiline':False]['text':' if a verification token matched, we keep the best sequence and remove the rest','line_number':445,'multiline':False]['text':' this leads to some KV cache fragmentation','line_number':446,'multiline':False]