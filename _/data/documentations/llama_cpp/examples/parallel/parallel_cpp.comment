['text':' A basic application simulating a server with multiple clients.','line_number':1,'multiline':False]['text':' The clients submit requests to the server and they are processed in parallel.','line_number':2,'multiline':False]['text':' trim whitespace from the beginning and end of a string','line_number':13,'multiline':False]['text':' Define a split string function to ...','line_number':87,'multiline':False]['text':' number of simultaneous "clients" to simulate','line_number':107,'multiline':False]['text':' requests to simulate','line_number':110,'multiline':False]['text':' insert new requests as soon as the previous one is done','line_number':113,'multiline':False]['text':' LOG_DISABLE_LOGS','line_number':122,'multiline':False]['text':' init llama.cpp','line_number':124,'multiline':False]['text':' load the target model','line_number':130,'multiline':False]['text':' load the prompts from an external file if there are any','line_number':134,'multiline':False]['text':' Output each line of the input params.prompts vector and copy to k_prompts','line_number':138,'multiline':False]['text':' the max batch size is as large as the context to handle cases where we get very long input prompt from multiple','line_number':169,'multiline':False]['text':' users. regardless of the size, the main loop will chunk the batch into a maximum of params.n_batch tokens at a time','line_number':170,'multiline':False]['text':' assign the system KV cache to all parallel sequences','line_number':197,'multiline':False]['text':' decode any currently ongoing sequences','line_number':215,'multiline':False]['text':' all sequences have ended - clear the entire KV cache','line_number':229,'multiline':False]['text':' insert new sequences for decoding','line_number':237,'multiline':False]['text':' do not prepend BOS because we have a system prompt!','line_number':252,'multiline':False]['text':' extract the logits only for the last token','line_number':260,'multiline':False]['text':' insert new requests one-by-one','line_number':273,'multiline':False]['text':'if (cont_batching) {','line_number':274,'multiline':False]['text':'    break;','line_number':275,'multiline':False]['text':'}','line_number':276,'multiline':False]['text':' process in chunks of params.n_batch','line_number':285,'multiline':False]['text':' experiment: process in powers of 2','line_number':289,'multiline':False]['text':'if (i + n_batch > (int32_t) batch.n_tokens && n_batch > 32) {','line_number':290,'multiline':False]['text':'    n_batch /= 2;','line_number':291,'multiline':False]['text':'    i -= n_batch;','line_number':292,'multiline':False]['text':'    continue;','line_number':293,'multiline':False]['text':'}','line_number':294,'multiline':False]['text':' unused','line_number':306,'multiline':False]['text':' if you get here, it means the KV cache is full - try increasing it via the context size','line_number':312,'multiline':False]['text':' retry with half the batch size to try to find a free slot in the KV cache','line_number':321,'multiline':False]['text':'printf("client %d, seq %d, token %d, pos %d, batch %d\n",','line_number':335,'multiline':False]['text':'        client.id, client.seq_id, client.sampled, client.n_decoded, client.i_batch);','line_number':336,'multiline':False]['text':' start measuring generation time after the first token to make sure all concurrent clients','line_number':343,'multiline':False]['text':' have their prompt already processed','line_number':344,'multiline':False]['text':'printf("client %d, seq %d, token %d, pos %d, batch %d: %s\n",','line_number':353,'multiline':False]['text':'        client.id, client.seq_id, id, client.n_decoded, client.i_batch, token_str.c_str());','line_number':354,'multiline':False]['text':' basic reverse prompt','line_number':361,'multiline':False]['text':' delete only the generated part of the sequence, i.e. keep the system prompt in the cache','line_number':367,'multiline':False]