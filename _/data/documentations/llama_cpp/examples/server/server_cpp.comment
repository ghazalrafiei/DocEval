['text':' crash the server in debug mode, otherwise send an http 500 error','line_number':10,'multiline':False]['text':' auto generated files (update with ./deps.sh)','line_number':17,'multiline':False]['text':'','line_number':68,'multiline':False]['text':' base64 utils (TODO: move to common in the future)','line_number':69,'multiline':False]['text':'','line_number':70,'multiline':False]['text':'','line_number':142,'multiline':False]['text':' parallel','line_number':143,'multiline':False]['text':'','line_number':144,'multiline':False]['text':' TODO: can become bool if we can't find use of more states','line_number':175,'multiline':False]['text':' remember the prompt to avoid reprocessing all prompt','line_number':192,'multiline':False]['text':' RNG seed','line_number':194,'multiline':False]['text':' number of tokens to keep from initial prompt','line_number':195,'multiline':False]['text':' new tokens to predict','line_number':196,'multiline':False]['text':' before of this image','line_number':214,'multiline':False]['text':' completion token output with probabilities','line_number':217,'multiline':False]['text':' TODO: reuse llama_detokenize','line_number':273,'multiline':False]['text':' format incomplete utf-8 multibyte character for output','line_number':307,'multiline':False]['text':' if the size is 1 and first bit is 1, meaning it's a partial character','line_number':311,'multiline':False]['text':'   (size > 1 meaning it's already a known token)','line_number':312,'multiline':False]['text':' convert a vector of completion_token_output to json','line_number':323,'multiline':False]['text':' Fallback null to default value','line_number':351,'multiline':False]['text':' used to determine the slot that has been used the longest','line_number':367,'multiline':False]['text':' generation props','line_number':370,'multiline':False]['text':' context size per slot','line_number':371,'multiline':False]['text':' sampling','line_number':399,'multiline':False]['text':' multimodal','line_number':403,'multiline':False]['text':' stats','line_number':406,'multiline':False]['text':' ms','line_number':413,'multiline':False]['text':' ms','line_number':414,'multiline':False]['text':' multitasks','line_number':416,'multiline':False]['text':' llama_set_rng_seed(ctx, params.seed); in batched the seed matter???????','line_number':442,'multiline':False]['text':' no budget || limitless','line_number':455,'multiline':False]['text':' total context for all clients / slots','line_number':525,'multiline':False]['text':' system prompt','line_number':527,'multiline':False]['text':' this should be the antiprompt','line_number':533,'multiline':False]['text':' slots / clients','line_number':536,'multiline':False]['text':' also guards id_gen, and queue_multitasks','line_number':542,'multiline':False]['text':'verbosity=','line_number':565,'multiline':True]['text':' request larger context for the image embedding','line_number':571,'multiline':False]['text':' create slots','line_number':604,'multiline':False]['text':' empty system prompt','line_number':624,'multiline':False]['text':' TODO: currently, we tokenize using special tokens by default','line_number':631,'multiline':False]['text':'       this is not always correct (see https://github.com/ggerganov/llama.cpp/pull/4160#issuecomment-1824826216)','line_number':632,'multiline':False]['text':'       but it's better compared to completely ignoring ChatML and other chat templates','line_number':633,'multiline':False]['text':' If `add_bos` is true, we only add BOS, when json_prompt is a string,','line_number':636,'multiline':False]['text':' or the first element of the json_prompt array is a string.','line_number':637,'multiline':False]['text':' infill','line_number':734,'multiline':False]['text':' process prompt','line_number':835,'multiline':False]['text':' example: system prompt [img-102] user [img-103] describe [img-134] -> [{id: 102, prefix: 'system prompt '}, {id: 103, prefix: ' user '}, {id: 134, prefix: ' describe '}]}','line_number':836,'multiline':False]['text':' multimodal doesn't support cache prompt','line_number':876,'multiline':False]['text':' clear the entire KV cache','line_number':896,'multiline':False]['text':' assign the system KV cache to all parallel sequences','line_number':919,'multiline':False]['text':' release all slots','line_number':930,'multiline':False]['text':' remember which tokens were sampled - used for repetition penalties during sampling','line_number':985,'multiline':False]['text':' search stop word and delete it','line_number':989,'multiline':False]['text':' check if there is incomplete UTF-8 character at the end','line_number':993,'multiline':False]['text':' continuation byte: 10xxxxxx','line_number':1000,'multiline':False]['text':' 2-byte character: 110xxxxx ...','line_number':1005,'multiline':False]['text':' 3-byte character: 1110xxxx ...','line_number':1010,'multiline':False]['text':' 4-byte character: 11110xxx ...','line_number':1015,'multiline':False]['text':' else 1-byte character or invalid byte','line_number':1018,'multiline':False]['text':' check if there is any token to predict','line_number':1042,'multiline':False]['text':' no send the stop word in the response','line_number':1045,'multiline':False]['text':' add the token to slot queue and cache','line_number':1048,'multiline':False]['text':' check the limits','line_number':1062,'multiline':False]['text':' continue','line_number':1088,'multiline':False]['text':'pad2square =','line_number':1100,'multiline':True]['text':' parent multitask, if any, needs to be updated','line_number':1290,'multiline':False]['text':' when a completion task's prompt array is not a singleton, we split it into multiple requests','line_number':1343,'multiline':False]['text':' entering new func scope','line_number':1346,'multiline':False]['text':' otherwise, it's a single-prompt task, we actually queue it','line_number':1350,'multiline':False]['text':' for now, tasks that have associated parent multitasks just get erased once multitask picks up the result','line_number':1369,'multiline':False]['text':' never reached','line_number':1387,'multiline':False]['text':'return task_result{-1, false, false, {}};','line_number':1388,'multiline':False]['text':' for multiple images processing','line_number':1391,'multiline':False]['text':' process prefix prompt','line_number':1400,'multiline':False]['text':' unused','line_number':1412,'multiline':False]['text':' process image with llm','line_number':1421,'multiline':False]['text':' append prefix of next image','line_number':1443,'multiline':False]['text':' no more images, then process suffix prompt','line_number':1445,'multiline':False]['text':' has next image','line_number':1448,'multiline':False]['text':' subtasks inherit everything else (infill mode, embedding mode, etc.)','line_number':1481,'multiline':False]['text':' queue up the multitask so we can track its subtask progression','line_number':1485,'multiline':False]['text':' send error result','line_number':1504,'multiline':False]['text':' send error result','line_number':1523,'multiline':False]['text':' release slot linked with the task id','line_number':1528,'multiline':False]['text':' remove finished multitasks from the queue of multitasks, and add the corresponding result to the result queue','line_number':1541,'multiline':False]['text':' all subtasks done == multitask is done','line_number':1547,'multiline':False]['text':' collect json results into one json result','line_number':1553,'multiline':False]['text':' attend tasks','line_number':1575,'multiline':False]['text':' update the system prompt wait until all slots are idle state','line_number':1578,'multiline':False]['text':' avoid 100% usage of cpu all time','line_number':1594,'multiline':False]['text':' Shift context','line_number':1602,'multiline':False]['text':' decode any currently ongoing sequences','line_number':1629,'multiline':False]['text':' release the slot','line_number':1632,'multiline':False]['text':' process in chunks of params.n_batch','line_number':1657,'multiline':False]['text':' assign workload to the slots','line_number':1660,'multiline':False]['text':' empty prompt passed -> release the slot and send empty response','line_number':1667,'multiline':False]['text':' need process the prompt','line_number':1676,'multiline':False]['text':' TODO: this should not be hardcoded','line_number':1696,'multiline':False]['text':' always add BOS','line_number':1702,'multiline':False]['text':' add BOS if there isn't system prompt','line_number':1710,'multiline':False]['text':' if input prompt is too big, truncate it','line_number':1721,'multiline':False]['text':' push the prompt into the sampling context (do not apply grammar)','line_number':1753,'multiline':False]['text':' we have to evaluate at least 1 token to generate logits.','line_number':1773,'multiline':False]['text':' process the prefix of first image','line_number':1786,'multiline':False]['text':' extract the logits only for the last token','line_number':1799,'multiline':False]['text':' unused','line_number':1829,'multiline':False]['text':' if you get here, it means the KV cache is full - try increasing it via the context size','line_number':1837,'multiline':False]['text':' retry with half the batch size to try to find a free slot in the KV cache','line_number':1844,'multiline':False]['text':' prompt evaluated for embedding','line_number':1857,'multiline':False]['text':' for llama_sample_token_greedy we need to sort candidates','line_number':1883,'multiline':False]['text':'','line_number':2055,'multiline':True]['text':' split string by , and /','line_number':2163,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':2182,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':2190,'multiline':False]['text':' llama.cpp completion api semantics ','line_number':2375,'multiline':True]['text':' openai api json semantics ','line_number':2377,'multiline':True]['text':' Map OpenAI parameters to llama.cpp parameters','line_number':2383,'multiline':False]['text':' OpenAI 'messages' to llama.cpp 'prompt'','line_number':2385,'multiline':False]['text':' Handle 'stop' field','line_number':2409,'multiline':False]['text':' Ensure there is ChatML-specific end sequence among stop words','line_number':2416,'multiline':False]['text':' return value is vector as there is one case where we might need to generate two responses','line_number':2471,'multiline':False]['text':' We have to send this as two updates to conform to openai behavior','line_number':2510,'multiline':False]['text':' Some idiosyncrasy in task processing logic makes several trailing calls','line_number':2536,'multiline':False]['text':' with empty content, we ignore these at the calee site.','line_number':2537,'multiline':False]['text':' own arguments required by this example','line_number':2636,'multiline':False]['text':' struct that contains llama context and inference','line_number':2640,'multiline':False]['text':' load the model','line_number':2662,'multiline':False]['text':' this is only called if no index.html is found in the public --path','line_number':2676,'multiline':False]['text':' this is only called if no index.js is found in the public --path','line_number':2683,'multiline':False]['text':' this is only called if no index.html is found in the public --path','line_number':2690,'multiline':False]['text':' this is only called if no index.html is found in the public --path','line_number':2697,'multiline':False]['text':'req','line_number':2704,'multiline':True]['text':' cancel','line_number':2772,'multiline':False]['text':' TODO: add mount point without "/v1" prefix -- how?','line_number':2801,'multiline':False]['text':' cancel request','line_number':2864,'multiline':False]['text':' cancel','line_number':2924,'multiline':False]['text':' set timeouts and change hostname and port','line_number':3019,'multiline':False]['text':' Set the base directory for serving static files','line_number':3029,'multiline':False]['text':' to make it ctrl+clickable:','line_number':3032,'multiline':False]['text':' run the HTTP server in a thread - see comment below','line_number':3040,'multiline':False]['text':' GG: if I put the main loop inside a thread, it crashes on the first request when build in Debug!?','line_number':3051,'multiline':False]['text':'     "Bus error: 10" - this is on macOS, it does not crash on Linux','line_number':3052,'multiline':False]['text':'std::thread t2([&]()','line_number':3053,'multiline':False]['text':');','line_number':3061,'multiline':False]