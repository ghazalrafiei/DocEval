['text':' possible loss of data','line_number':31,'multiline':False]['text':' LOG_DISABLE_LOGS','line_number':123,'multiline':False]['text':' TODO: Dump params ?','line_number':125,'multiline':False]['text':'LOG("Params perplexity: %s\n", LOG_TOSTR(params.perplexity));','line_number':126,'multiline':False]['text':' save choice to use color for later','line_number':128,'multiline':False]['text':' (note for later: this is a slightly awkward choice)','line_number':129,'multiline':False]['text':' load the model and apply lora adapter, if any','line_number':185,'multiline':False]['text':' print system information','line_number':207,'multiline':False]['text':' fopen to check for existing session','line_number':219,'multiline':False]['text':' Should not run without any tokens','line_number':258,'multiline':False]['text':' Tokenize negative prompt','line_number':264,'multiline':False]['text':' debug message about similarity of saved session, if applicable','line_number':288,'multiline':False]['text':' remove any "future" tokens that we might have inherited from the previous session','line_number':309,'multiline':False]['text':' if we will use the cache for the full prompt without reaching the end of the cache, force','line_number':317,'multiline':False]['text':' reevaluation of the last token token to recalculate the cached logits','line_number':318,'multiline':False]['text':' number of tokens to keep when resetting context','line_number':325,'multiline':False]['text':' prefix & suffix for instruct mode','line_number':330,'multiline':False]['text':' chatml prefix & suffix','line_number':337,'multiline':False]['text':' in instruct mode, we inject a prefix and a suffix to each input by the user','line_number':344,'multiline':False]['text':' similar for chatml mode','line_number':349,'multiline':False]['text':' enable interactive mode if interactive start is specified','line_number':355,'multiline':False]['text':' the first thing we will do is to output the prompt, so set color accordingly','line_number':477,'multiline':False]['text':' predict','line_number':486,'multiline':False]['text':' Note: n_ctx - 4 here is to match the logic for commandline prompt handling via','line_number':488,'multiline':False]['text':' --prompt or --file which uses the same value.','line_number':489,'multiline':False]['text':' Ensure the input doesn't exceed the context size by truncating embd if necessary.','line_number':492,'multiline':False]['text':' infinite text generation via context swapping','line_number':503,'multiline':False]['text':' if we run out of context:','line_number':504,'multiline':False]['text':' - take the n_keep first tokens from the original prompt (via n_past)','line_number':505,'multiline':False]['text':' - take half of the last (n_ctx - n_keep) tokens and recompute the logits in batches','line_number':506,'multiline':False]['text':' try to reuse a matching prefix from the loaded session instead of re-eval (via n_past)','line_number':536,'multiline':False]['text':' evaluate tokens in batches','line_number':558,'multiline':False]['text':' embd is typically prepared beforehand to fit within a batch, but not always','line_number':559,'multiline':False]['text':' Guidance context should have the same data with these modifications:','line_number':565,'multiline':False]['text':'','line_number':566,'multiline':False]['text':' * Replace the initial prompt','line_number':567,'multiline':False]['text':' * Shift everything by guidance_offset','line_number':568,'multiline':False]['text':' optionally save the session on first sample (for faster prompt loading next time)','line_number':626,'multiline':False]['text':' echo this to console','line_number':642,'multiline':False]['text':' decrement remaining sampling budget','line_number':645,'multiline':False]['text':' some user input remains from prompt or interaction, forward it to processing','line_number':650,'multiline':False]['text':' push the prompt in the sampling context in order to apply repetition penalties later','line_number':655,'multiline':False]['text':' for the prompt, we don't apply grammar rules','line_number':656,'multiline':False]['text':' display text','line_number':666,'multiline':False]['text':' reset color to default if there is no pending user input','line_number':681,'multiline':False]['text':' if not currently processing queued inputs;','line_number':686,'multiline':False]['text':' check for reverse prompt in the last n_prev tokens','line_number':688,'multiline':False]['text':' Check if each of the reverse prompts appears at the end of the output.','line_number':694,'multiline':False]['text':' If we're not running interactively, the reverse prompt might be tokenized with some following characters','line_number':695,'multiline':False]['text':' so we'll compensate for that by widening the search window a bit.','line_number':696,'multiline':False]['text':' deal with end of text token in interactive mode','line_number':717,'multiline':False]['text':' tokenize and inject first reverse prompt','line_number':723,'multiline':False]['text':' color user input only','line_number':754,'multiline':False]['text':' done taking input, reset color','line_number':764,'multiline':False]['text':' Add tokens to embd only if the input buffer is non-empty','line_number':767,'multiline':False]['text':' Entering a empty line lets the user pass control back','line_number':768,'multiline':False]['text':' append input suffix if any','line_number':770,'multiline':False]['text':' instruct mode: insert instruction prefix','line_number':780,'multiline':False]['text':' chatml mode: insert user chat prefix','line_number':786,'multiline':False]['text':' instruct mode: insert response suffix','line_number':805,'multiline':False]['text':' chatml mode: insert assistant chat suffix','line_number':810,'multiline':False]['text':' do not echo this again','line_number':828,'multiline':False]['text':' end of text token','line_number':839,'multiline':False]['text':' In interactive mode, respect the maximum number of tokens and drop back to user input when reached.','line_number':845,'multiline':False]['text':' We skip this logic when n_predict == -1 (infinite) or -2 (stop at context size).','line_number':846,'multiline':False]['text':' LOG_DISABLE_LOGS','line_number':870,'multiline':False]