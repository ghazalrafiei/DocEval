['text':' total length of the sequence including the prompt','line_number':29,'multiline':False]['text':' init LLM','line_number':32,'multiline':False]['text':' initialize the model','line_number':36,'multiline':False]['text':' model_params.n_gpu_layers = 99; // offload all layers to the GPU','line_number':40,'multiline':False]['text':' initialize the context','line_number':49,'multiline':False]['text':' tokenize the prompt','line_number':65,'multiline':False]['text':' make sure the KV cache is big enough to hold all the prompt and generated tokens','line_number':75,'multiline':False]['text':' print the prompt token-by-token','line_number':82,'multiline':False]['text':' create a llama_batch with size 512','line_number':92,'multiline':False]['text':' we use this object to submit token data for decoding','line_number':93,'multiline':False]['text':' evaluate the initial prompt','line_number':97,'multiline':False]['text':' llama_decode will output logits only for the last token of the prompt','line_number':102,'multiline':False]['text':' main loop','line_number':110,'multiline':False]['text':' sample the next token','line_number':118,'multiline':False]['text':' sample the most likely token','line_number':132,'multiline':False]['text':' is it an end of stream?','line_number':135,'multiline':False]['text':' prepare the next batch','line_number':145,'multiline':False]['text':' push this new token for next evaluation','line_number':148,'multiline':False]['text':' evaluate the current batch with the transformer model','line_number':156,'multiline':False]