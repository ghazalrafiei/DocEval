['text':' TODO: use common/sampling.h','line_number':42,'multiline':False]['text':' out of user input, sample next token','line_number':46,'multiline':False]['text':' const int32_t repeat_last_n   = sparams.repeat_last_n < 0 ? n_ctx : sparams.repeat_last_n;','line_number':52,'multiline':False]['text':' const float   repeat_penalty  = sparams.repeat_penalty;','line_number':53,'multiline':False]['text':' const float   alpha_presence  = sparams.presence_penalty;','line_number':54,'multiline':False]['text':' const float   alpha_frequency = sparams.frequency_penalty;','line_number':55,'multiline':False]['text':' const bool    penalize_nl     = sparams.penalize_nl;','line_number':59,'multiline':False]['text':' Apply params.logit_bias map','line_number':66,'multiline':False]['text':' Greedy sampling','line_number':80,'multiline':False]['text':' Temperature sampling','line_number':93,'multiline':False]['text':' replaces the base64 image tag in the prompt with `replacement`','line_number':133,'multiline':False]['text':'argc','line_number':176,'multiline':True]['text':' load and preprocess the image','line_number':183,'multiline':False]['text':' llava chat format is "<system_prompt>\nUSER:<image_embeddings>\n<textual_prompt>\nASSISTANT:"','line_number':213,'multiline':False]['text':' generate the response','line_number':218,'multiline':False]['text':'verbosity=','line_number':242,'multiline':True]['text':' we need a longer context size to process image embeddings','line_number':255,'multiline':False]['text':' process the prompt','line_number':306,'multiline':False]