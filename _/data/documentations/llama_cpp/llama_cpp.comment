['text':' possible loss of data','line_number':81,'multiline':False]['text':'','line_number':97,'multiline':False]['text':' logging','line_number':98,'multiline':False]['text':'','line_number':99,'multiline':False]['text':'','line_number':109,'multiline':False]['text':' helpers','line_number':110,'multiline':False]['text':'','line_number':111,'multiline':False]['text':' Check for non-negative tolerance','line_number':134,'multiline':False]['text':' Exact equality check','line_number':139,'multiline':False]['text':' Check for infinities','line_number':144,'multiline':False]['text':' Regular comparison using the provided absolute tolerance','line_number':149,'multiline':False]['text':' NOLINT','line_number':171,'multiline':False]['text':'','line_number':180,'multiline':False]['text':' gguf constants (sync with gguf.py)','line_number':181,'multiline':False]['text':'','line_number':182,'multiline':False]['text':' NOLINT','line_number':563,'multiline':False]['text':' helper to handle gguf constants','line_number':572,'multiline':False]['text':' usage:','line_number':573,'multiline':False]['text':'','line_number':574,'multiline':False]['text':'   const auto tn = LLM_TN(LLM_ARCH_LLAMA);','line_number':575,'multiline':False]['text':'','line_number':576,'multiline':False]['text':'   std::string name = tn(LLM_TENSOR_OUTPUT);                     -> "output"','line_number':577,'multiline':False]['text':'   std::string name = tn(LLM_TENSOR_TOKEN_EMBD, "bias");         -> "token_embd.bias"','line_number':578,'multiline':False]['text':'   std::string name = tn(LLM_TENSOR_ATTN_NORM, "weight", 3);     -> "blk.3.attn_norm.weight"','line_number':579,'multiline':False]['text':'','line_number':580,'multiline':False]['text':'','line_number':607,'multiline':False]['text':' gguf helpers','line_number':608,'multiline':False]['text':'','line_number':609,'multiline':False]['text':' escape quotes','line_number':660,'multiline':False]['text':'','line_number':681,'multiline':False]['text':' ggml helpers','line_number':682,'multiline':False]['text':'','line_number':683,'multiline':False]['text':'','line_number':696,'multiline':False]['text':' llama helpers','line_number':697,'multiline':False]['text':'','line_number':698,'multiline':False]['text':' fallback to malloc / free','line_number':750,'multiline':False]['text':' useful in cases where CUDA can try to allocate PINNED memory','line_number':751,'multiline':False]['text':' NOLINT','line_number':771,'multiline':False]['text':' use FILE * so we don't have to re-open the file to mmap','line_number':783,'multiline':False]['text':' this really shouldn't fail','line_number':803,'multiline':False]['text':' same','line_number':813,'multiline':False]['text':' -1 = max value ','line_number':867,'multiline':True]['text':' prefetch/readahead impairs performance on NUMA systems','line_number':871,'multiline':False]['text':' Advise the kernel to preload the mapped memory','line_number':882,'multiline':False]['text':' advise the kernel not to use readahead','line_number':889,'multiline':False]['text':' (because the next page might not belong on the same node)','line_number':890,'multiline':False]['text':' PrefetchVirtualMemory is only present on Windows 8 and above, so we dynamically load it','line_number':927,'multiline':False]['text':' may fail on pre-Windows 8 systems','line_number':931,'multiline':False]['text':' advise the kernel to preload the mapped memory','line_number':935,'multiline':False]['text':' Represents some region of memory being locked using mlock or VirtualLock;','line_number':966,'multiline':False]['text':' will automatically unlock on destruction.','line_number':967,'multiline':False]['text':' NOLINT','line_number':984,'multiline':False]['text':' Check if the resource limit is fine after all','line_number':1028,'multiline':False]['text':' It failed but this was only the first try; increase the working','line_number':1069,'multiline':False]['text':' set size and try again.','line_number':1070,'multiline':False]['text':' Per MSDN: "The maximum number of pages that a process can lock','line_number':1077,'multiline':False]['text':' is equal to the number of pages in its minimum working set minus','line_number':1078,'multiline':False]['text':' a small overhead."','line_number':1079,'multiline':False]['text':' Hopefully a megabyte is enough overhead:','line_number':1080,'multiline':False]['text':' The minimum must be <= the maximum, so we need to increase both:','line_number':1082,'multiline':False]['text':'','line_number':1136,'multiline':False]['text':' globals','line_number':1137,'multiline':False]['text':'','line_number':1138,'multiline':False]['text':' We save the log callback globally','line_number':1147,'multiline':False]['text':' available llama models','line_number':1154,'multiline':False]['text':' context size the model was trained on','line_number':1177,'multiline':False]['text':' context size used during inference','line_number':1239,'multiline':False]['text':' number of threads to use for generation','line_number':1241,'multiline':False]['text':' number of threads to use for batch processing','line_number':1242,'multiline':False]['text':' These hyperparameters are not exposed in GGUF, because all','line_number':1248,'multiline':False]['text':' existing YaRN models use the same values for them.','line_number':1249,'multiline':False]['text':' normalization','line_number':1260,'multiline':False]['text':' attention','line_number':1270,'multiline':False]['text':' attention bias','line_number':1277,'multiline':False]['text':' normalization','line_number':1284,'multiline':False]['text':' ff','line_number':1288,'multiline':False]['text':' w1','line_number':1289,'multiline':False]['text':' w2','line_number':1290,'multiline':False]['text':' w3','line_number':1291,'multiline':False]['text':' ff MoE','line_number':1293,'multiline':False]['text':' ff bias','line_number':1299,'multiline':False]['text':' b2','line_number':1300,'multiline':False]['text':' b3','line_number':1301,'multiline':False]['text':' ring-buffer of cached KV data','line_number':1315,'multiline':False]['text':' Note: The value of head isn't only used to optimize searching','line_number':1319,'multiline':False]['text':' for a free KV slot. llama_decode_internal also uses it, so it','line_number':1320,'multiline':False]['text':' cannot be freely changed after a slot has been allocated.','line_number':1321,'multiline':False]['text':' used cells (i.e. at least one seq_id)','line_number':1324,'multiline':False]['text':' computed before each graph build','line_number':1326,'multiline':False]['text':' per layer','line_number':1331,'multiline':False]['text':' default LLaMA special tokens','line_number':1374,'multiline':False]['text':' -1 unknown, 1 add, 0 don't add.','line_number':1381,'multiline':False]['text':' -1 unknown, 1 add, 0 don't add.','line_number':1382,'multiline':False]['text':' gguf metadata','line_number':1428,'multiline':False]['text':' context','line_number':1431,'multiline':False]['text':' the model memory buffer','line_number':1434,'multiline':False]['text':' model memory mapped file','line_number':1437,'multiline':False]['text':' objects representing data potentially being locked in memory','line_number':1440,'multiline':False]['text':' for quantize-stats only','line_number':1444,'multiline':False]['text':' key + value cache for the self attention','line_number':1489,'multiline':False]['text':' number of tokens sampled','line_number':1502,'multiline':False]['text':' number of tokens in eval calls for the prompt (with batch size > 1)','line_number':1503,'multiline':False]['text':' number of eval calls','line_number':1504,'multiline':False]['text':' decode output (2-dimensional array: [n_tokens][n_vocab])','line_number':1506,'multiline':False]['text':' input embedding (1-dimensional array: [n_embd])','line_number':1510,'multiline':False]['text':' reusable buffer for `struct ggml_graph_plan.work_data`','line_number':1513,'multiline':False]['text':' memory buffers used to evaluate the model','line_number':1516,'multiline':False]['text':'','line_number':1531,'multiline':False]['text':' kv cache helpers','line_number':1532,'multiline':False]['text':'','line_number':1533,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':1598,'multiline':False]['text':' find an empty slot of size "n_tokens" in the cache','line_number':1610,'multiline':False]['text':' updates the cache head','line_number':1611,'multiline':False]['text':' Note: On success, it's important that cache.head points','line_number':1612,'multiline':False]['text':' to the first cell of the slot.','line_number':1613,'multiline':False]['text':'LLAMA_LOG_ERROR("%s: failed to find a slot for %d tokens\n", __func__, n_tokens);','line_number':1649,'multiline':False]['text':' find how many cells are currently in use','line_number':1667,'multiline':False]['text':' keep count of the number of used cells','line_number':1707,'multiline':False]['text':' If we freed up a slot, set head to it so searching can start there.','line_number':1716,'multiline':False]['text':' If we freed up a slot, set head to it so searching can start there.','line_number':1753,'multiline':False]['text':' If we freed up a slot, set head to it so searching can start there.','line_number':1783,'multiline':False]['text':' Otherwise we just start the next search from the beginning.','line_number':1784,'multiline':False]['text':'','line_number':1788,'multiline':False]['text':' model loading and saving','line_number':1789,'multiline':False]['text':'','line_number':1790,'multiline':False]['text':' Shouldn't be possible to end up here, but just in case...','line_number':1917,'multiline':False]['text':' Currently, we should never end up here so it would be a bug if we do.','line_number':1965,'multiline':False]['text':'.no_alloc = ','line_number':2014,'multiline':True]['text':'.ctx      = ','line_number':2015,'multiline':True]['text':' determine file type based on the number of tensors for each quantization and print meta data','line_number':2047,'multiline':False]['text':' TODO: make optional','line_number':2048,'multiline':False]['text':' this is a way to mark that we have "guessed" the file type','line_number':2089,'multiline':False]['text':' print type counts','line_number':2118,'multiline':False]['text':' TODO: ggml_set_backend','line_number':2225,'multiline':False]['text':' prefetch','line_number':2303,'multiline':False]['text':' unused tensors should have been caught by load_data already','line_number':2323,'multiline':False]['text':' allocate temp buffer if not using mmap','line_number':2329,'multiline':False]['text':' old code:','line_number':2351,'multiline':False]['text':'ggml_cuda_transform_tensor(lt.data, lt.ggml_tensor);','line_number':2352,'multiline':False]['text':' TODO: test if this works !!','line_number':2354,'multiline':False]['text':'','line_number':2377,'multiline':False]['text':' load LLaMA models','line_number':2378,'multiline':False]['text':'','line_number':2379,'multiline':False]['text':' K-quants','line_number':2405,'multiline':False]['text':' get metadata as string','line_number':2450,'multiline':False]['text':' get general kv','line_number':2461,'multiline':False]['text':' get hparams kv','line_number':2464,'multiline':False]['text':' n_head_kv is optional, default to n_head','line_number':2482,'multiline':False]['text':' rope_freq_base (optional)','line_number':2493,'multiline':False]['text':' rope_freq_scale (inverse of the kv) is optional','line_number':2502,'multiline':False]['text':' try the old key name','line_number':2505,'multiline':False]['text':' sanity check for n_rot (optional)','line_number':2510,'multiline':False]['text':' gpt-neox n_rot = rotary_pct * (n_embd / n_head)','line_number':2521,'multiline':False]['text':' gpt-j n_rot = rotary_dim','line_number':2522,'multiline':False]['text':' arch-specific KVs','line_number':2525,'multiline':False]['text':' TODO: This should probably be in llama.h','line_number':2640,'multiline':False]['text':' determine vocab type','line_number':2670,'multiline':False]['text':' default special tokens','line_number':2679,'multiline':False]['text':' read bpe merges and populate bpe ranks','line_number':2688,'multiline':False]['text':' default special tokens','line_number':2713,'multiline':False]['text':' determine the newline token: LLaMA "<0x0A>" == 10 == '\n', Falcon 193 == '\n'','line_number':2744,'multiline':False]['text':' special tokens','line_number':2753,'multiline':False]['text':' Handle add_bos_token and add_eos_token','line_number':2779,'multiline':False]['text':' build special tokens cache','line_number':2792,'multiline':False]['text':' TODO: It is unclear (to me) at this point, whether special tokes are guaranteed to be of a deterministic type,','line_number':2794,'multiline':False]['text':'  and will always be correctly labeled in 'added_tokens.json' etc.','line_number':2795,'multiline':False]['text':' The assumption is, since special tokens aren't meant to be exposed to end user, they are designed','line_number':2796,'multiline':False]['text':'  to be unmatchable by the tokenizer, therefore tokens from the vocab, which are unmatchable by the tokenizer','line_number':2797,'multiline':False]['text':'  are special tokens.','line_number':2798,'multiline':False]['text':' From testing, this appears to correlate 1:1 with special tokens.','line_number':2799,'multiline':False]['text':'','line_number':2800,'multiline':False]['text':' Counting special tokens and verifying in only one direction','line_number':2802,'multiline':False]['text':'  is sufficient to detect difference in those two sets.','line_number':2803,'multiline':False]['text':'','line_number':2804,'multiline':False]['text':' Count all non-normal tokens in the vocab while iterating','line_number':2814,'multiline':False]['text':' Skip single character tokens','line_number':2819,'multiline':False]['text':' Split token string representation in two, in all possible ways','line_number':2823,'multiline':False]['text':'  and check if both halves can be matched to a valid token','line_number':2824,'multiline':False]['text':' check if we didnt partition in the middle of a utf sequence','line_number':2829,'multiline':False]['text':' skip over the rest of multibyte utf sequence','line_number':2840,'multiline':False]['text':' Some tokens are multibyte, but they are utf sequences with equivalent text length of 1','line_number':2846,'multiline':False]['text':'  it's faster to re-filter them here, since there are way less candidates now','line_number':2847,'multiline':False]['text':' Calculate a total "utf" length of a token string representation','line_number':2849,'multiline':False]['text':' And skip the ones which are one character','line_number':2856,'multiline':False]['text':' At this point what we have left are special tokens only','line_number':2858,'multiline':False]['text':' Count manually found special tokens','line_number':2861,'multiline':False]['text':' If this manually found special token is not marked as such, flag a mismatch','line_number':2864,'multiline':False]['text':' hparams','line_number':2894,'multiline':False]['text':' TODO: fix','line_number':2897,'multiline':False]['text':' a.k.a. n_embd_head, n_head_dim','line_number':2905,'multiline':False]['text':' general kv','line_number':2928,'multiline':False]['text':' special tokens','line_number':2931,'multiline':False]['text':' create the ggml context','line_number':2963,'multiline':False]['text':'.mem_size   =','line_number':2972,'multiline':True]['text':'.mem_buffer =','line_number':2973,'multiline':True]['text':'.no_alloc   =','line_number':2974,'multiline':True]['text':' prepare memory for the weights','line_number':3002,'multiline':False]['text':' output','line_number':3017,'multiline':False]['text':' NOLINT','line_number':3048,'multiline':False]['text':' NOLINT','line_number':3049,'multiline':False]['text':' optional bias tensors','line_number':3060,'multiline':False]['text':' MoE branch','line_number':3081,'multiline':False]['text':' NOLINT','line_number':3145,'multiline':False]['text':' NOLINT','line_number':3146,'multiline':False]['text':' TODO: CPU-only for now','line_number':3173,'multiline':False]['text':' output','line_number':3177,'multiline':False]['text':' NOLINT','line_number':3210,'multiline':False]['text':' NOLINT','line_number':3211,'multiline':False]['text':' output','line_number':3247,'multiline':False]['text':' NOLINT','line_number':3280,'multiline':False]['text':' NOLINT','line_number':3281,'multiline':False]['text':' TODO: CPU-only for now','line_number':3370,'multiline':False]['text':' output','line_number':3376,'multiline':False]['text':' NOLINT','line_number':3409,'multiline':False]['text':' NOLINT','line_number':3410,'multiline':False]['text':' output','line_number':3447,'multiline':False]['text':' NOLINT','line_number':3478,'multiline':False]['text':' NOLINT','line_number':3479,'multiline':False]['text':' output','line_number':3507,'multiline':False]['text':'
                        llama_model_loader: - tensor    4:         blk.0.attn_output.weight f16      [  2560,  2560,     1,     1 ]
                        ','line_number':3539,'multiline':True]['text':' NOLINT','line_number':3542,'multiline':False]['text':' NOLINT','line_number':3543,'multiline':False]['text':' NOLINT','line_number':3603,'multiline':False]['text':' NOLINT','line_number':3604,'multiline':False]['text':' print memory requirements','line_number':3636,'multiline':False]['text':' this is the total memory required to run the inference','line_number':3638,'multiline':False]['text':' weights in VRAM not in memory','line_number':3641,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':3659,'multiline':False]['text':' defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST)','line_number':3665,'multiline':False]['text':' populate `tensors_by_name`','line_number':3668,'multiline':False]['text':' loading time will be recalculate after the first eval, so','line_number':3689,'multiline':False]['text':' we take page faults deferred by mmap() into consideration','line_number':3690,'multiline':False]['text':'','line_number':3727,'multiline':False]['text':' llm_build','line_number':3728,'multiline':False]['text':'','line_number':3729,'multiline':False]['text':' ffn_gate is parallel to ffn_up','line_number':3748,'multiline':False]['text':' Persimmon: n_rot = n_embd_head/2','line_number':3782,'multiline':False]['text':' Other:     n_rot = n_embd_head','line_number':3783,'multiline':False]['text':' we rotate only the first n_rot dimensions','line_number':3821,'multiline':False]['text':' compute the transposed [n_tokens, n_embd] V matrix','line_number':3849,'multiline':False]['text':'struct ggml_tensor * v_cur_t = ggml_transpose(ctx, v_cur); // TODO: reshape above is likely not needed','line_number':3851,'multiline':False]['text':' important: storing RoPE-ed version of K in the KV cache!','line_number':3863,'multiline':False]['text':' if max_alibi_bias > 0 then apply ALiBi','line_number':3986,'multiline':False]['text':' temporary branch until we figure out how to handle ggml_alibi through ggml_add','line_number':4023,'multiline':False]['text':' TODO: n_head or n_head_kv','line_number':4028,'multiline':False]['text':' TODO: K-shift is likely not working','line_number':4029,'multiline':False]['text':' TODO: change to ggml_add','line_number':4030,'multiline':False]['text':'n_past','line_number':4031,'multiline':True]['text':' split cached v into n_head heads','line_number':4045,'multiline':False]['text':' user-specified context size (can be different from n_ctx_train)','line_number':4084,'multiline':False]['text':' size of KV cache to consider (n_kv <= n_ctx)','line_number':4102,'multiline':False]['text':' index of where we store new KV data in the cache','line_number':4103,'multiline':False]['text':' TODO: consider making the entire interface noexcept','line_number':4114,'multiline':False]['text':' all initializations should be done in init()','line_number':4151,'multiline':False]['text':'.mem_size   =','line_number':4156,'multiline':True]['text':'.mem_buffer =','line_number':4157,'multiline':True]['text':'.no_alloc   =','line_number':4158,'multiline':True]['text':' inp_pos - contains the positions','line_number':4182,'multiline':False]['text':' KQ_scale','line_number':4186,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4190,'multiline':False]['text':' shift the entire K-cache if needed','line_number':4194,'multiline':False]['text':' norm','line_number':4202,'multiline':False]['text':' self-attention','line_number':4208,'multiline':False]['text':' compute Q and K and RoPE them','line_number':4210,'multiline':False]['text':' feed-forward network','line_number':4257,'multiline':False]['text':' MoE branch','line_number':4271,'multiline':False]['text':' [n_tokens, num_experts]','line_number':4277,'multiline':False]['text':' [n_tokens, num_experts]','line_number':4280,'multiline':False]['text':' select experts','line_number':4283,'multiline':False]['text':' [n_tokens, num_experts_per_tok]','line_number':4284,'multiline':False]['text':' [n_tokens, num_experts_per_tok]','line_number':4291,'multiline':False]['text':' [n_tokens, num_experts_per_tok]','line_number':4296,'multiline':False]['text':' compute expert outputs','line_number':4299,'multiline':False]['text':' [n_tokens, n_embd]','line_number':4314,'multiline':False]['text':' [n_tokens, n_embd]','line_number':4317,'multiline':False]['text':' input for next layer','line_number':4338,'multiline':False]['text':' lm_head','line_number':4349,'multiline':False]['text':' inp_pos - contains the positions','line_number':4367,'multiline':False]['text':' KQ_scale','line_number':4371,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4375,'multiline':False]['text':' shift the entire K-cache if needed','line_number':4379,'multiline':False]['text':' self-attention','line_number':4392,'multiline':False]['text':' apply ALiBi for 13B model','line_number':4428,'multiline':False]['text':' feed-forward network','line_number':4440,'multiline':False]['text':' input for next layer','line_number':4458,'multiline':False]['text':' lm_head','line_number':4469,'multiline':False]['text':' inp_pos - contains the positions','line_number':4487,'multiline':False]['text':' KQ_scale','line_number':4491,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4495,'multiline':False]['text':' shift the entire K-cache if needed','line_number':4499,'multiline':False]['text':' self-attention','line_number':4513,'multiline':False]['text':' Falcon-40B','line_number':4516,'multiline':False]['text':' using mode = 2 for neox mode','line_number':4540,'multiline':False]['text':' feed forward','line_number':4563,'multiline':False]['text':' !! use the attn norm, not the result','line_number':4565,'multiline':False]['text':' input for next layer','line_number':4579,'multiline':False]['text':' norm','line_number':4585,'multiline':False]['text':' inp_pos - contains the positions','line_number':4610,'multiline':False]['text':' KQ_scale','line_number':4614,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4618,'multiline':False]['text':' self-attention','line_number':4635,'multiline':False]['text':' add the input','line_number':4661,'multiline':False]['text':' FF','line_number':4665,'multiline':False]['text':' inp_pos - contains the positions','line_number':4710,'multiline':False]['text':' KQ_scale','line_number':4714,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4718,'multiline':False]['text':' self attention','line_number':4735,'multiline':False]['text':' split qkv','line_number':4743,'multiline':False]['text':' Q/K Layernorm','line_number':4768,'multiline':False]['text':' RoPE the first n_rot of q/k, pass the other half, and concat.','line_number':4781,'multiline':False]['text':' get the second half of tmpq, e.g tmpq[n_rot:, :, :]','line_number':4798,'multiline':False]['text':' ggml currently only supports concatenation on dim=2','line_number':4827,'multiline':False]['text':' so we need to permute qrot, qpass, concat, then permute back.','line_number':4828,'multiline':False]['text':' TODO: not tested, could be broken','line_number':4863,'multiline':False]['text':' feed-forward network','line_number':4873,'multiline':False]['text':' KQ_scale','line_number':4920,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':4924,'multiline':False]['text':' self-attention','line_number':4936,'multiline':False]['text':' feed-forward network','line_number':4964,'multiline':False]['text':' input for next layer','line_number':4982,'multiline':False]['text':' lm_head','line_number':4993,'multiline':False]['text':' KQ_scale','line_number':5011,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':5015,'multiline':False]['text':' self-attention','line_number':5032,'multiline':False]['text':' Add the input','line_number':5058,'multiline':False]['text':' FF','line_number':5062,'multiline':False]['text':' KQ_scale','line_number':5105,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':5109,'multiline':False]['text':' self-attention','line_number':5122,'multiline':False]['text':' Add the input','line_number':5152,'multiline':False]['text':' feed forward','line_number':5156,'multiline':False]['text':' input for next layer','line_number':5175,'multiline':False]['text':' inp_pos - contains the positions','line_number':5204,'multiline':False]['text':' KQ_scale','line_number':5208,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':5212,'multiline':False]['text':' shift the entire K-cache if needed','line_number':5216,'multiline':False]['text':' norm','line_number':5224,'multiline':False]['text':' self-attention','line_number':5231,'multiline':False]['text':' compute Q and K and RoPE them','line_number':5233,'multiline':False]['text':' feed-forward network','line_number':5268,'multiline':False]['text':' input for next layer','line_number':5287,'multiline':False]['text':' lm_head','line_number':5299,'multiline':False]['text':' inp_pos - contains the positions','line_number':5317,'multiline':False]['text':' KQ_scale','line_number':5321,'multiline':False]['text':' KQ_mask (mask for 1 head, it will be broadcasted to all heads)','line_number':5325,'multiline':False]['text':' shift the entire K-cache if needed','line_number':5329,'multiline':False]['text':' self-attention','line_number':5342,'multiline':False]['text':' using mode = 2 for neox mode','line_number':5361,'multiline':False]['text':' feed-forward forward','line_number':5385,'multiline':False]['text':' input for next layer','line_number':5403,'multiline':False]['text':' lm_head','line_number':5414,'multiline':False]['text':'','line_number':5424,'multiline':False]['text':' tensor offloading helpers','line_number':5425,'multiline':False]['text':'','line_number':5426,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5427,'multiline':False]['text':' force offload','line_number':5432,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5439,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5513,'multiline':False]['text':'{ "inp_tokens",                 OFFLOAD_FUNC_NR  }, // TODO: missing K-quants get_rows kernel','line_number':5515,'multiline':False]['text':'{ "inp_embd",                   OFFLOAD_FUNC_NR  }, // TODO: missing K-quants get_rows kernel','line_number':5516,'multiline':False]['text':' this is often used for KQ ops (e.g. rope)','line_number':5519,'multiline':False]['text':' check if we should build the worst-case graph (for memory measurement)','line_number':5614,'multiline':False]['text':' keep track of the input that has already been allocated','line_number':5617,'multiline':False]['text':' TODO: set to false after finishing refactoring','line_number':5628,'multiline':False]['text':' number of non-view tensors that have been processed by the callback','line_number':5631,'multiline':False]['text':' this callback allows us to apply custom logic to each tensor (e.g. ggml-alloc, offloading, etc.)','line_number':5633,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5634,'multiline':False]['text':'','line_number':5642,'multiline':False]['text':' allocate input tensors and set input data','line_number':5643,'multiline':False]['text':'','line_number':5644,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5645,'multiline':False]['text':' view tensors are not processed further','line_number':5742,'multiline':False]['text':'','line_number':5751,'multiline':False]['text':' offload layers','line_number':5752,'multiline':False]['text':'','line_number':5753,'multiline':False]['text':' TODO: will be removed with backend v2','line_number':5754,'multiline':False]['text':'#define LLAMA_OFFLOAD_DEBUG','line_number':5756,'multiline':False]['text':' should we offload the final norm? yes if we are not computing embeddings','line_number':5767,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':5785,'multiline':False]['text':' check the global map for what offload function to use for this tensor','line_number':5788,'multiline':False]['text':' if a tensor hasn't been offloaded, we warn the user','line_number':5793,'multiline':False]['text':' count the number of layers and respect the provided n_gpu_layers','line_number':5803,'multiline':False]['text':' this is needed for compatibility with Metal for example','line_number':5845,'multiline':False]['text':' apply offload function to the tensor','line_number':5863,'multiline':False]['text':' decode a batch of tokens by evaluating the transformer','line_number':5950,'multiline':False]['text':'','line_number':5951,'multiline':False]['text':'   - lctx:      llama context','line_number':5952,'multiline':False]['text':'   - batch:     batch to evaluate','line_number':5953,'multiline':False]['text':'','line_number':5954,'multiline':False]['text':' return 0 on success','line_number':5955,'multiline':False]['text':' return positive int on warning','line_number':5956,'multiline':False]['text':' return negative int on error','line_number':5957,'multiline':False]['text':'','line_number':5958,'multiline':False]['text':' NOLINT','line_number':5978,'multiline':False]['text':' TODO: needs fix after #3228','line_number':5983,'multiline':False]['text':'ggml_mpi_eval_init(lctx.ctx_mpi, &n_tokens, &n_past, &n_threads);','line_number':5985,'multiline':False]['text':' helpers for smoother batch API transition','line_number':5997,'multiline':False]['text':' after deprecating the llama_eval calls, these will be removed','line_number':5998,'multiline':False]['text':' if we have enough unused cells before the current head ->','line_number':6029,'multiline':False]['text':'   better to start searching from the beginning of the cache, hoping to fill it','line_number':6030,'multiline':False]['text':' a heuristic, to avoid attending the full cache if it is not yet utilized','line_number':6039,'multiline':False]['text':' after enough generations, the benefit from this heuristic disappears','line_number':6040,'multiline':False]['text':' if we start defragmenting the cache, the benefit from this will be more important','line_number':6041,'multiline':False]['text':'kv_self.n = llama_kv_cache_cell_max(kv_self);','line_number':6043,'multiline':False]['text':'printf("kv_self.n = %5d, kv_self.used = %5d, kv_self.head = %5d\n", kv_self.n, kv_self.used, kv_self.head);','line_number':6045,'multiline':False]['text':' HACK: ggml-alloc may change the tensor backend when reusing a parent, so force output to be on the CPU here if needed','line_number':6076,'multiline':False]['text':' LLAMA_LOG_INFO("graph build time: %.3f ms (%d nodes, %d leafs)\n", (ggml_time_us() - t_start_us)/1000.0, gf->n_nodes, gf->n_leafs);','line_number':6083,'multiline':False]['text':' for big prompts, if BLAS is enabled, it is better to use only one thread','line_number':6085,'multiline':False]['text':' otherwise, the threads are spin-lock waiting for the BLAS calls and are degrading the performance','line_number':6086,'multiline':False]['text':' TODO: this is mostly important for Apple Silicon where CBLAS is still performing very well','line_number':6087,'multiline':False]['text':'       we still need some threads to process all non-mul_mat ops, but not too much to avoid interfering','line_number':6088,'multiline':False]['text':'       with the BLAS calls. need a better solution','line_number':6089,'multiline':False]['text':' update the kv ring buffer','line_number':6119,'multiline':False]['text':' Ensure kv cache head points to a valid index.','line_number':6130,'multiline':False]['text':' print timing information per ggml operation (for debugging purposes)','line_number':6137,'multiline':False]['text':' requires GGML_PERF to be defined','line_number':6138,'multiline':False]['text':' plot the computation graph in dot format (for debugging purposes)','line_number':6142,'multiline':False]['text':'if (n_past%100 == 0) {','line_number':6143,'multiline':False]['text':'    ggml_graph_dump_dot(gf, NULL, "llama.dot");','line_number':6144,'multiline':False]['text':'}','line_number':6145,'multiline':False]['text':' extract logits','line_number':6147,'multiline':False]['text':' TODO: do not compute and extract logits if only embeddings are needed','line_number':6148,'multiline':False]['text':'       need to update the graphs to skip "result_output"','line_number':6149,'multiline':False]['text':' extract embeddings','line_number':6170,'multiline':False]['text':' measure the performance only for the single-token evals','line_number':6178,'multiline':False]['text':' get a more accurate load time, upon first eval','line_number':6188,'multiline':False]['text':' TODO: fix this','line_number':6189,'multiline':False]['text':'','line_number':6198,'multiline':False]['text':' tokenizer','line_number':6199,'multiline':False]['text':'','line_number':6200,'multiline':False]['text':' SPM tokenizer','line_number':6276,'multiline':False]['text':' original implementation:','line_number':6277,'multiline':False]['text':' https://github.com/ggerganov/llama.cpp/commit/074bea2eb1f1349a0118239c4152914aecaa1be4','line_number':6278,'multiline':False]['text':' split string into utf8 chars','line_number':6298,'multiline':False]['text':' seed the work queue with all possible 2-character tokens.','line_number':6313,'multiline':False]['text':' keep substituting the highest frequency pairs for as long as we can.','line_number':6318,'multiline':False]['text':' if one of the symbols already got merged, skip it.','line_number':6326,'multiline':False]['text':' merge the right sym into the left one','line_number':6332,'multiline':False]['text':'LLAMA_LOG_INFO("left = '%*s' size = %zu\n", (int) left_sym.n, left_sym.text, bigram.size);','line_number':6336,'multiline':False]['text':' remove the right sym from the chain','line_number':6338,'multiline':False]['text':' find more substitutions','line_number':6344,'multiline':False]['text':' Do we need to support is_unused?','line_number':6360,'multiline':False]['text':' output any symbols that did not form tokens as bytes.','line_number':6369,'multiline':False]['text':' Do we need to support is_unused?','line_number':6407,'multiline':False]['text':' BPE tokenizer','line_number':6419,'multiline':False]['text':' adapted from https://github.com/cmp-nct/ggllm.cpp [MIT License]','line_number':6420,'multiline':False]['text':' tried to simplify unicode stuff, so most likely does not work 100% correctly!','line_number':6421,'multiline':False]['text':' TODO: there are a lot of common parts between spm and bpe tokenizers, should be refactored and reused','line_number':6423,'multiline':False]['text':' build token(s)','line_number':6472,'multiline':False]['text':' Skip this bigram if it's outdated','line_number':6486,'multiline':False]['text':' merge the right sym into the left one','line_number':6489,'multiline':False]['text':' remove the right sym from the chain','line_number':6493,'multiline':False]['text':' left side of current symbol','line_number':6499,'multiline':False]['text':' right side of current symbol','line_number':6500,'multiline':False]['text':' add the fnished tokens to the final list keeping correct order for next and prev','line_number':6503,'multiline':False]['text':' GPT2 system regex:  's|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+','line_number':6578,'multiline':False]['text':' forward backward lookups','line_number':6598,'multiline':False]['text':' handling contractions','line_number':6602,'multiline':False]['text':' 's|'t|'m|'d','line_number':6604,'multiline':False]['text':' push previous content as token','line_number':6610,'multiline':False]['text':' 're|'ve|'ll','line_number':6620,'multiline':False]['text':' current token + next token can be defined','line_number':6629,'multiline':False]['text':' push previous content as token','line_number':6631,'multiline':False]['text':' the contraction','line_number':6634,'multiline':False]['text':' final','line_number':6681,'multiline':False]['text':' #define PRETOKENIZERDEBUG','line_number':6753,'multiline':False]['text':' for each special token','line_number':6757,'multiline':False]['text':' for each text fragment','line_number':6762,'multiline':False]['text':' if a fragment is text ( not yet processed )','line_number':6767,'multiline':False]['text':' loop over the text','line_number':6774,'multiline':False]['text':' find the first occurrence of a given special token in this fragment','line_number':6776,'multiline':False]['text':'  passing offset argument only limit the "search area" but match coordinates','line_number':6777,'multiline':False]['text':'  are still relative to the source full raw_text','line_number':6778,'multiline':False]['text':' no occurrences found, stop processing this fragment for a given special token','line_number':6781,'multiline':False]['text':' check if match is within bounds of offset <-> length','line_number':6784,'multiline':False]['text':' if match is further than base offset','line_number':6792,'multiline':False]['text':'  then we have some text to the left of it','line_number':6793,'multiline':False]['text':' left','line_number':6795,'multiline':False]['text':' special token','line_number':6806,'multiline':False]['text':' right','line_number':6810,'multiline':False]['text':' repeat for the right side','line_number':6828,'multiline':False]['text':' OG tokenizer behavior:','line_number':6853,'multiline':False]['text':'','line_number':6854,'multiline':False]['text':' tokenizer.encode('', add_bos=True)  returns [1]','line_number':6855,'multiline':False]['text':' tokenizer.encode('', add_bos=False) returns []','line_number':6856,'multiline':False]['text':' without adding this leading whitespace, we do not get the same results as the original tokenizer','line_number':6878,'multiline':False]['text':' TODO: It's likely possible to get rid of this string copy entirely','line_number':6880,'multiline':False]['text':'  by modifying llm_tokenizer_x to operate with string offsets like pre-tokenizer','line_number':6881,'multiline':False]['text':'  and passing 'add space prefix' as bool argument','line_number':6882,'multiline':False]['text':'','line_number':6883,'multiline':False]['text':' prefix with space if the first token is not special','line_number':6886,'multiline':False]['text':' if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN)','line_number':6896,'multiline':False]['text':' if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN)','line_number':6916,'multiline':False]['text':'','line_number':6927,'multiline':False]['text':' grammar - internal','line_number':6928,'multiline':False]['text':'','line_number':6929,'multiline':False]['text':' bit value so far (unshifted)','line_number':6932,'multiline':False]['text':' num bytes remaining; -1 indicates invalid sequence','line_number':6933,'multiline':False]['text':' buffer for partially generated UTF-8 sequence from accepted tokens','line_number':6940,'multiline':False]['text':' Decodes a UTF-8 string which may end in an incomplete sequence. Adds a terminating 0 for use as','line_number':6950,'multiline':False]['text':' pointer. If an invalid sequence is encountered, returns `llama_partial_utf8.n_remain == -1`.','line_number':6951,'multiline':False]['text':' common english strings have the same number of codepoints and bytes. `+ 1` for the terminating 0.','line_number':6958,'multiline':False]['text':' continue previous decode, if applicable','line_number':6963,'multiline':False]['text':' invalid sequence, abort','line_number':6967,'multiline':False]['text':' decode any subsequent utf-8 sequences, which may end in an incomplete one','line_number':6980,'multiline':False]['text':' invalid sequence, abort','line_number':6987,'multiline':False]['text':' returns true iff pos points to the end of one of the definitions of a rule','line_number':7010,'multiline':False]['text':' NOLINT','line_number':7013,'multiline':False]['text':' NOLINT','line_number':7014,'multiline':False]['text':' returns true iff chr satisfies the char range at pos (regular or inverse range)','line_number':7019,'multiline':False]['text':' asserts that pos is pointing to a char range element','line_number':7020,'multiline':False]['text':' NOLINT','line_number':7028,'multiline':False]['text':' inclusive range, e.g. [a-z]','line_number':7032,'multiline':False]['text':' exact char match, e.g. [a] or "a"','line_number':7036,'multiline':False]['text':' returns true iff some continuation of the given partial UTF-8 sequence could satisfy the char','line_number':7045,'multiline':False]['text':' range at pos (regular or inverse range)','line_number':7046,'multiline':False]['text':' asserts that pos is pointing to a char range element','line_number':7047,'multiline':False]['text':' invalid sequence or 7-bit char split across 2 bytes (overlong)','line_number':7058,'multiline':False]['text':' range of possible code points this partial UTF-8 sequence could complete to','line_number':7063,'multiline':False]['text':' inclusive range, e.g. [a-z]','line_number':7077,'multiline':False]['text':' exact char match, e.g. [a] or "a"','line_number':7083,'multiline':False]['text':' transforms a grammar pushdown stack into N possible stacks, all ending','line_number':7095,'multiline':False]['text':' at a character range (terminal element)','line_number':7096,'multiline':False]['text':' init new stack without the top (pos)','line_number':7114,'multiline':False]['text':' if this rule ref is followed by another element, add that to stack','line_number':7117,'multiline':False]['text':' if alternate is nonempty, add to stack','line_number':7121,'multiline':False]['text':' scan to end of alternate def','line_number':7126,'multiline':False]['text':' there's another alternate def of this rule to process','line_number':7130,'multiline':False]['text':' end of alternate (LLAMA_GRETYPE_END, LLAMA_GRETYPE_ALT) or middle of char range','line_number':7143,'multiline':False]['text':' (LLAMA_GRETYPE_CHAR_ALT, LLAMA_GRETYPE_CHAR_RNG_UPPER); stack should never be left on','line_number':7144,'multiline':False]['text':' those','line_number':7145,'multiline':False]['text':' takes a set of possible pushdown stacks on a grammar, which are required to','line_number':7150,'multiline':False]['text':' be positioned at a character range (see `llama_grammar_advance_stack`), and','line_number':7151,'multiline':False]['text':' produces the N possible stacks if the given char is accepted at those','line_number':7152,'multiline':False]['text':' positions','line_number':7153,'multiline':False]['text':' update top of stack to next element, if any','line_number':7170,'multiline':False]['text':' reached end of full codepoints in token, reject iff it ended in a partial sequence','line_number':7208,'multiline':False]['text':' that cannot satisfy this position in grammar','line_number':7209,'multiline':False]['text':' update top of stack to next element, if any','line_number':7223,'multiline':False]['text':' REVIEW','line_number':7243,'multiline':False]['text':'','line_number':7257,'multiline':False]['text':' grammar - external','line_number':7258,'multiline':False]['text':'','line_number':7259,'multiline':False]['text':' copy rule definitions into vectors','line_number':7267,'multiline':False]['text':' loop over alternates of start rule to build initial stacks','line_number':7276,'multiline':False]['text':' if alternate is nonempty, add to stack','line_number':7282,'multiline':False]['text':' scan to end of alternate def','line_number':7287,'multiline':False]['text':' there's another alternate def of this rule to process','line_number':7291,'multiline':False]['text':' redirect elements in stacks to point to new rules','line_number':7308,'multiline':False]['text':'','line_number':7324,'multiline':False]['text':' sampling','line_number':7325,'multiline':False]['text':'','line_number':7326,'multiline':False]['text':' Sort the logits in descending order','line_number':7340,'multiline':False]['text':' Sort scores in descending order','line_number':7370,'multiline':False]['text':' Compute the cumulative probabilities','line_number':7398,'multiline':False]['text':' Check if the running sum is at least p or if we have kept at least min_keep tokens','line_number':7405,'multiline':False]['text':' we set the last index to i+1 to indicate that the current iterate should be included in the set','line_number':7406,'multiline':False]['text':' Resize the output vector to keep only the top-p tokens','line_number':7413,'multiline':False]['text':' scale by max prob','line_number':7430,'multiline':False]['text':' first token always matches','line_number':7431,'multiline':False]['text':' prob too small','line_number':7435,'multiline':False]['text':' Resize the output vector to keep only the matching tokens','line_number':7439,'multiline':False]['text':' Compute the first and second derivatives','line_number':7455,'multiline':False]['text':' Calculate absolute value of second derivatives','line_number':7466,'multiline':False]['text':' Normalize the second derivatives','line_number':7471,'multiline':False]['text':' Check if the running sum is greater than z or if we have kept at least min_keep tokens','line_number':7491,'multiline':False]['text':' Resize the output vector to keep only the tokens above the tail location','line_number':7498,'multiline':False]['text':' Reference implementation:','line_number':7507,'multiline':False]['text':' https://github.com/huggingface/transformers/compare/main...cimeister:typical-sampling:typical-pr','line_number':7508,'multiline':False]['text':' Compute the softmax of logits and calculate entropy','line_number':7513,'multiline':False]['text':' Compute the absolute difference between negative log probability and entropy for each candidate','line_number':7523,'multiline':False]['text':' Sort tokens based on the shifted_scores and their corresponding indices','line_number':7530,'multiline':False]['text':' Compute the cumulative probabilities','line_number':7538,'multiline':False]['text':' Check if the running sum is greater than typical or if we have kept at least min_keep tokens','line_number':7546,'multiline':False]['text':' Resize the output vector to keep only the locally typical tokens','line_number':7553,'multiline':False]['text':' Replace the data in candidates with the new_candidates data','line_number':7560,'multiline':False]['text':' Create a frequency map to count occurrences of each token in last_tokens','line_number':7600,'multiline':False]['text':' Apply frequency and presence penalties to the candidates','line_number':7606,'multiline':False]['text':' The academic publication that described this technique actually just only divided, but that would cause tokens with negative logits to become more likely, which is obviously wrong.','line_number':7615,'multiline':False]['text':' This is common fix for this problem, which is to multiply by the penalty instead of dividing.','line_number':7616,'multiline':False]['text':' Estimate s_hat using the most probable m tokens','line_number':7733,'multiline':False]['text':' Compute k from the estimated s_hat and target surprise value','line_number':7745,'multiline':False]['text':' Sample the next word X using top-k sampling','line_number':7749,'multiline':False]['text':' Compute error as the difference between observed surprise and target surprise value','line_number':7757,'multiline':False]['text':' Update mu using the learning rate and error','line_number':7764,'multiline':False]['text':' Truncate the words with surprise values greater than mu','line_number':7779,'multiline':False]['text':' Normalize the probabilities of the remaining words','line_number':7792,'multiline':False]['text':' Sample the next word X from the remaining words','line_number':7795,'multiline':False]['text':' Compute error as the difference between observed surprise and target surprise value','line_number':7799,'multiline':False]['text':' Update mu using the learning rate and error','line_number':7806,'multiline':False]['text':' Find max element','line_number':7818,'multiline':False]['text':' Note terminating 0 in decoded string','line_number':7868,'multiline':False]['text':'','line_number':7880,'multiline':False]['text':' Beam search','line_number':7881,'multiline':False]['text':'','line_number':7882,'multiline':False]['text':' Cumulative beam probability (renormalized relative to all beams)','line_number':7886,'multiline':False]['text':' Initialize end-of-beam to false. Callback sets this to true.','line_number':7887,'multiline':False]['text':' Sort beams by probability. In case of ties, prefer beams at eob.','line_number':7888,'multiline':False]['text':' Shift off first n tokens and discard them.','line_number':7892,'multiline':False]['text':' A struct for calculating logit-related info.','line_number':7902,'multiline':False]['text':' never used','line_number':7919,'multiline':False]['text':' Return top k token_data by logit.','line_number':7922,'multiline':False]['text':' min-heap by logit','line_number':7924,'multiline':False]['text':' Re-calculated on each loop iteration','line_number':7955,'multiline':False]['text':' Used to communicate to/from callback on beams state.','line_number':7958,'multiline':False]['text':' Collapse beams to a single beam given by index.','line_number':7971,'multiline':False]['text':' Min-heaps are used to efficiently collect the top-k elements (k=n_beams).','line_number':7979,'multiline':False]['text':' The repetitive patterns below reflect the 2 stages of heaps:','line_number':7980,'multiline':False]['text':'  * Gather elements until the vector is full, then call std::make_heap() on it.','line_number':7981,'multiline':False]['text':'  * If the heap is full and a new element is found that should be included, pop the','line_number':7982,'multiline':False]['text':'    least element to the back(), replace it with the new, then push it into the heap.','line_number':7983,'multiline':False]['text':' Min-heaps use a greater-than comparator.','line_number':7985,'multiline':False]['text':' beam is at end-of-sentence, so just copy it to next_beams if its probability is high enough.','line_number':7988,'multiline':False]['text':' beam is not at end-of-sentence, so branch with next top_k tokens.','line_number':8000,'multiline':False]['text':' Find common_prefix_length based on beams.','line_number':8037,'multiline':False]['text':' Requires beams is not empty.','line_number':8038,'multiline':False]['text':' Construct beams_state to send back to caller via the callback function.','line_number':8053,'multiline':False]['text':' Side effect: set common_prefix_length = find_common_prefix_length();','line_number':8054,'multiline':False]['text':' Loop:','line_number':8063,'multiline':False]['text':'  * while i < n_predict, AND','line_number':8064,'multiline':False]['text':'  * any of the beams have not yet reached end-of-beam (eob), AND','line_number':8065,'multiline':False]['text':'  * the highest probability beam(s) (plural in case of ties) are not at end-of-sentence','line_number':8066,'multiline':False]['text':'    (since all other beam probabilities can only decrease)','line_number':8067,'multiline':False]['text':' Start with one empty beam w/ probability = 1.0 and !eob.','line_number':8069,'multiline':False]['text':' Sets common_prefix_length','line_number':8073,'multiline':False]['text':' Update values (p,eob) that callback may have changed.','line_number':8074,'multiline':False]['text':' Zero-out next_beam probabilities to place them last in following min-heap.','line_number':8079,'multiline':False]['text':' next_beams become the beams of next/final iteration. Swap them to re-use memory.','line_number':8085,'multiline':False]['text':' As beams grow, the cumulative probabilities decrease.','line_number':8093,'multiline':False]['text':' Renormalize them to avoid floating point underflow.','line_number':8094,'multiline':False]['text':' Assumes beams is non-empty.  Uses llama_beam::operator<() for ordering.','line_number':8101,'multiline':False]['text':' Copy (p,eob) for each beam which may have been changed by the callback.','line_number':8106,'multiline':False]['text':'','line_number':8129,'multiline':False]['text':' quantization','line_number':8130,'multiline':False]['text':'','line_number':8131,'multiline':False]['text':' do nothing ','line_number':8136,'multiline':True]['text':' unreachable','line_number':8182,'multiline':False]['text':' if blocks aren't divisible by thread count','line_number':8193,'multiline':False]['text':' num blocks for this thread','line_number':8199,'multiline':False]['text':' number of elements for this thread','line_number':8200,'multiline':False]['text':' number of input bytes for this thread','line_number':8201,'multiline':False]['text':' TODO: avoid hardcoded tensor names - use the TN_* constants','line_number':8221,'multiline':False]['text':' In the 70B model we have 8 heads sharing the same attn_v weights. As a result, the attn_v.weight tensor is','line_number':8249,'multiline':False]['text':' 8x smaller compared to attn_q.weight. Hence, we can get a nice boost in quantization accuracy with','line_number':8250,'multiline':False]['text':' nearly negligible increase in model size by quantizing this tensor with more bits:','line_number':8251,'multiline':False]['text':' for the 8-expert model, bumping this to Q8_0 trades just ~128MB','line_number':8255,'multiline':False]['text':' TODO: explore better strategies','line_number':8256,'multiline':False]['text':' for the 8-expert model, bumping this to Q8_0 trades just ~128MB','line_number':8262,'multiline':False]['text':' TODO: explore better strategies','line_number':8263,'multiline':False]['text':' This can be used to reduce the size of the Q5_K_S model.','line_number':8306,'multiline':False]['text':' The associated PPL increase is fully in line with the size reduction','line_number':8307,'multiline':False]['text':'else {','line_number':8308,'multiline':False]['text':'    if (ftype == LLAMA_FTYPE_MOSTLY_Q5_K_S) new_type = GGML_TYPE_Q4_K;','line_number':8309,'multiline':False]['text':'}','line_number':8310,'multiline':False]['text':' K-quants','line_number':8352,'multiline':False]['text':' mmap consistently increases speed Linux, and also increases speed on Windows with','line_number':8372,'multiline':False]['text':' hot cache. It may cause a slowdown on macOS, possibly related to free memory.','line_number':8373,'multiline':False]['text':' prefetch ','line_number':8382,'multiline':True]['text':' copy the KV pairs from the input file','line_number':8398,'multiline':False]['text':' TODO: avoid hardcoded tensor names - use the TN_* constants','line_number':8408,'multiline':False]['text':' populate the original tensors so we get an initial meta data','line_number':8435,'multiline':False]['text':' fail fast on write errors','line_number':8442,'multiline':False]['text':' placeholder for the meta data','line_number':8448,'multiline':False]['text':' This used to be a regex, but <regex> has an extreme cost to compile times.','line_number':8470,'multiline':False]['text':' ends with 'weight'?','line_number':8471,'multiline':False]['text':' quantize only 2D tensors','line_number':8473,'multiline':False]['text':' do not quantize expert gating tensors','line_number':8478,'multiline':False]['text':' If we've decided to quantize to the same type the tensor is already','line_number':8491,'multiline':False]['text':' in then there's nothing to do.','line_number':8492,'multiline':False]['text':' upper bound on size','line_number':8518,'multiline':False]['text':' update the gguf meta data as we go','line_number':8576,'multiline':False]['text':' write tensor data + padding','line_number':8580,'multiline':False]['text':' go back to beginning of file and write the updated meta data','line_number':8585,'multiline':False]['text':' print histogram for all tensors','line_number':8600,'multiline':False]['text':' verify magic and version','line_number':8635,'multiline':False]['text':' create a temporary ggml context to store the lora tensors','line_number':8656,'multiline':False]['text':' todo: calculate size from biggest possible tensor','line_number':8657,'multiline':False]['text':' create a name -> tensor map of the model to accelerate lookups','line_number':8667,'multiline':False]['text':' load base model','line_number':8673,'multiline':False]['text':'use_mmap','line_number':8679,'multiline':True]['text':'kv_overrides','line_number':8679,'multiline':True]['text':' maybe this should in llama_model_loader','line_number':8693,'multiline':False]['text':' prefetch ','line_number':8695,'multiline':True]['text':' read tensors and apply','line_number':8699,'multiline':False]['text':' check for lora suffix and get the type of tensor','line_number':8729,'multiline':False]['text':' LLAMA_LOG_INFO("%s: %s => %s (lora type %s) \n", __func__, name.c_str(),base_name.c_str(), lora_type.c_str());','line_number':8740,'multiline':False]['text':' create ggml tensor','line_number':8747,'multiline':False]['text':' load tensor data','line_number':8769,'multiline':False]['text':' check if we have both A and B tensors and apply','line_number':8778,'multiline':False]['text':' GGML_USE_CUBLAS','line_number':8796,'multiline':False]['text':' load from base model','line_number':8802,'multiline':False]['text':' TODO: throw','line_number':8804,'multiline':False]['text':' TODO: not tested!! maybe not working!','line_number':8809,'multiline':False]['text':' w = w + BA*s','line_number':8838,'multiline':False]['text':' we won't need these tensors again, reset the context to save memory','line_number':8873,'multiline':False]['text':' TODO: this should be in a destructor, it will leak on failure','line_number':8885,'multiline':False]['text':'','line_number':8897,'multiline':False]['text':' interface implementation','line_number':8898,'multiline':False]['text':'','line_number':8899,'multiline':False]['text':'.n_gpu_layers                =','line_number':8902,'multiline':True]['text':'.main_gpu                    =','line_number':8903,'multiline':True]['text':'.tensor_split                =','line_number':8904,'multiline':True]['text':'.progress_callback           =','line_number':8905,'multiline':True]['text':'.progress_callback_user_data =','line_number':8906,'multiline':True]['text':'.kv_overrides                =','line_number':8907,'multiline':True]['text':'.vocab_only                  =','line_number':8908,'multiline':True]['text':'.use_mmap                    =','line_number':8909,'multiline':True]['text':'.use_mlock                   =','line_number':8910,'multiline':True]['text':'.seed                        =','line_number':8922,'multiline':True]['text':'.n_ctx                       =','line_number':8923,'multiline':True]['text':'.n_batch                     =','line_number':8924,'multiline':True]['text':'.n_threads                   =','line_number':8925,'multiline':True]['text':' TODO: better default','line_number':8925,'multiline':False]['text':'.n_threads_batch             =','line_number':8926,'multiline':True]['text':'.rope_scaling_type           =','line_number':8927,'multiline':True]['text':'.rope_freq_base              =','line_number':8928,'multiline':True]['text':'.rope_freq_scale             =','line_number':8929,'multiline':True]['text':'.yarn_ext_factor             =','line_number':8930,'multiline':True]['text':'.yarn_attn_factor            =','line_number':8931,'multiline':True]['text':'.yarn_beta_fast              =','line_number':8932,'multiline':True]['text':'.yarn_beta_slow              =','line_number':8933,'multiline':True]['text':'.yarn_orig_ctx               =','line_number':8934,'multiline':True]['text':'.type_k                      =','line_number':8935,'multiline':True]['text':'.type_v                      =','line_number':8936,'multiline':True]['text':'.mul_mat_q                   =','line_number':8937,'multiline':True]['text':'.logits_all                  =','line_number':8938,'multiline':True]['text':'.embedding                   =','line_number':8939,'multiline':True]['text':'.offload_kqv                 =','line_number':8940,'multiline':True]['text':'.nthread                     =','line_number':8948,'multiline':True]['text':'.ftype                       =','line_number':8949,'multiline':True]['text':'.allow_requantize            =','line_number':8950,'multiline':True]['text':'.quantize_output_tensor      =','line_number':8951,'multiline':True]['text':'.only_copy                   =','line_number':8952,'multiline':True]['text':'.pure                        =','line_number':8953,'multiline':True]['text':' needed to initialize f16 tables','line_number':8974,'multiline':False]['text':' never scale if scaling type is none','line_number':9073,'multiline':False]['text':' negative indicates 'not set'','line_number':9076,'multiline':False]['text':' reserve memory for context buffers','line_number':9097,'multiline':False]['text':' resized during inference','line_number':9123,'multiline':False]['text':' the compute buffer is used to store the tensor and graph structs, while the allocator buffer is used for the tensor data','line_number':9136,'multiline':False]['text':' create measure allocator','line_number':9139,'multiline':False]['text':' build worst-case graph','line_number':9142,'multiline':False]['text':' not actually used by llama_build_graph, but required to choose between token and embedding inputs graph','line_number':9145,'multiline':False]['text':'ggml_metal_graph_find_concurrency(ctx->ctx_metal, gf, false);','line_number':9156,'multiline':False]['text':'ggml_allocr_set_parse_seq(ctx->alloc, ggml_metal_get_concur_list(ctx->ctx_metal), ggml_metal_if_optimized(ctx->ctx_metal));','line_number':9157,'multiline':False]['text':' measure memory requirements for the graph','line_number':9160,'multiline':False]['text':' recreate allocator with exact memory requirements','line_number':9165,'multiline':False]['text':'ggml_allocr_set_parse_seq(ctx->alloc, ggml_metal_get_concur_list(ctx->ctx_metal), ggml_metal_if_optimized(ctx->ctx_metal));','line_number':9172,'multiline':False]['text':' calculate total VRAM usage','line_number':9179,'multiline':False]['text':' this allocates all Metal resources and memory buffers','line_number':9210,'multiline':False]['text':' Enter a blocking eval loop with dummy input, letting rank=0 drive the process','line_number':9246,'multiline':False]['text':' TODO: needs fix after #3228','line_number':9247,'multiline':False]['text':'const std::vector<llama_token> tmp(ctx->model.hparams.n_ctx, llama_token_bos(ctx));','line_number':9249,'multiline':False]['text':'while (!llama_eval(ctx, tmp.data(), tmp.size(), 0, 0)) {};','line_number':9250,'multiline':False]['text':'.n_cells            = ','line_number':9390,'multiline':True]['text':'.n_max_seq          = ','line_number':9391,'multiline':True]['text':'.token_count        = ','line_number':9392,'multiline':True]['text':'.used_cells         = ','line_number':9393,'multiline':True]['text':'.max_contiguous     = ','line_number':9394,'multiline':True]['text':'.max_contiguous_idx = ','line_number':9395,'multiline':True]['text':'.cells              = ','line_number':9396,'multiline':True]['text':'.cells_sequences    = ','line_number':9397,'multiline':True]['text':' Returns the *maximum* size of the state','line_number':9514,'multiline':False]['text':' we don't know size of rng until we actually serialize it. so reserve more than enough memory for its serialized state.','line_number':9516,'multiline':False]['text':' for reference, std::mt19937(1337) serializes to 6701 bytes.','line_number':9517,'multiline':False]['text':' llama_context_data','line_number':9545,'multiline':False]['text':'* copy state data into either a buffer or file depending on the passed in context
 *
 * file context:
 * llama_file file("/path", "wb");
 * llama_data_file_context data_ctx(&file);
 * llama_copy_state_data(ctx, &data_ctx);
 *
 * buffer context:
 * std::vector<uint8_t> buf(max_size, 0);
 * llama_data_buffer_context data_ctx(&buf.data());
 * llama_copy_state_data(ctx, &data_ctx);
 *
','line_number':9585,'multiline':True]['text':' copy rng','line_number':9599,'multiline':False]['text':' copy logits','line_number':9614,'multiline':False]['text':' If there is a gap between the size and the capacity, write padding','line_number':9626,'multiline':False]['text':' Create a buffer filled with zeros','line_number':9629,'multiline':False]['text':' copy embeddings','line_number':9634,'multiline':False]['text':' copy kv cache','line_number':9645,'multiline':False]['text':' no_alloc ','line_number':9668,'multiline':True]['text':'n_threads','line_number':9695,'multiline':True]['text':' our data is now in the kout2d_data and vout2d_data buffers','line_number':9699,'multiline':False]['text':' write them to file','line_number':9700,'multiline':False]['text':' Sets the state reading from the specified source address','line_number':9730,'multiline':False]['text':' set rng','line_number':9734,'multiline':False]['text':' set logits','line_number':9749,'multiline':False]['text':' set embeddings','line_number':9767,'multiline':False]['text':' set kv cache','line_number':9781,'multiline':False]['text':' no_alloc ','line_number':9806,'multiline':True]['text':'n_threads','line_number':9830,'multiline':True]['text':' sanity checks','line_number':9870,'multiline':False]['text':' load the prompt','line_number':9889,'multiline':False]['text':' restore the context state','line_number':9902,'multiline':False]['text':' save the prompt','line_number':9938,'multiline':False]['text':' save the context state using stream saving','line_number':9942,'multiline':False]['text':'n_tokens       =','line_number':9992,'multiline':True]['text':'tokens         =','line_number':9993,'multiline':True]['text':'embd           =','line_number':9994,'multiline':True]['text':'pos            =','line_number':9995,'multiline':True]['text':'n_seq_id       =','line_number':9996,'multiline':True]['text':'seq_id         =','line_number':9997,'multiline':True]['text':'logits         =','line_number':9998,'multiline':True]['text':'all_pos_0      =','line_number':9999,'multiline':True]['text':'all_pos_1      =','line_number':10000,'multiline':True]['text':'all_seq_id     =','line_number':10001,'multiline':True]['text':' LLAMA_LOG_ERROR("%s: too many tokens\n", __func__);','line_number':10121,'multiline':False]['text':' does not write null-terminator to buf','line_number':10142,'multiline':False]['text':' NOLINT','line_number':10155,'multiline':False]['text':' TODO: for now we accept all unsupported token types,','line_number':10170,'multiline':False]['text':' suppressing them like CONTROL tokens.','line_number':10171,'multiline':False]['text':' GGML_ASSERT(false);','line_number':10172,'multiline':False]['text':' TODO: for now we accept all unsupported token types,','line_number':10188,'multiline':False]['text':' suppressing them like CONTROL tokens.','line_number':10189,'multiline':False]['text':' GGML_ASSERT(false);','line_number':10190,'multiline':False]['text':'.t_start_ms  =','line_number':10203,'multiline':True]['text':'.t_end_ms    =','line_number':10204,'multiline':True]['text':'.t_load_ms   =','line_number':10205,'multiline':True]['text':'.t_sample_ms =','line_number':10206,'multiline':True]['text':'.t_p_eval_ms =','line_number':10207,'multiline':True]['text':'.t_eval_ms   =','line_number':10208,'multiline':True]['text':'.n_sample =','line_number':10210,'multiline':True]['text':'.n_p_eval =','line_number':10211,'multiline':True]['text':'.n_eval   =','line_number':10212,'multiline':True]['text':' For internal test use','line_number':10290,'multiline':False]