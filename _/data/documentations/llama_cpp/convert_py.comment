['text':'!/usr/bin/env python3','line_number':1,'multiline':False]['text':'','line_number':47,'multiline':False]['text':' data types','line_number':48,'multiline':False]['text':'','line_number':49,'multiline':False]['text':' Mini Q8_0 quantization in Python!','line_number':89,'multiline':False]['text':' Much faster implementation of block quantization contributed by @Cebtenzzre','line_number':95,'multiline':False]['text':' Quantized types skipped here because they may also map to np.float32','line_number':111,'multiline':False]['text':' TODO: match this with `llama_ftype`','line_number':125,'multiline':False]['text':' TODO: rename to LLAMAFileType','line_number':126,'multiline':False]['text':' TODO: move to `gguf.py`','line_number':127,'multiline':False]['text':' except 1d tensors','line_number':132,'multiline':False]['text':' except 1d tensors','line_number':133,'multiline':False]['text':' 1D tensors are always F32.','line_number':139,'multiline':False]['text':'','line_number':149,'multiline':False]['text':' hparams loading','line_number':150,'multiline':False]['text':'','line_number':151,'multiline':False]['text':' path to the directory containing the model files','line_number':175,'multiline':False]['text':' try transformer naming first','line_number':180,'multiline':False]['text':' try transformer naming first','line_number':183,'multiline':False]['text':' next: try baichuan naming','line_number':186,'multiline':False]['text':' guessed','line_number':195,'multiline':False]['text':' guessed','line_number':196,'multiline':False]['text':' TODO: verify this','line_number':198,'multiline':False]['text':' LLaMA v2 70B params.json','line_number':265,'multiline':False]['text':' {"dim": 8192, "multiple_of": 4096, "ffn_dim_multiplier": 1.3, "n_heads": 64, "n_kv_heads": 8, "n_layers": 80, "norm_eps": 1e-05, "vocab_size": -1}','line_number':266,'multiline':False]['text':' hack to determine LLaMA v1 vs v2 vs CodeLlama','line_number':275,'multiline':False]['text':' Mixtral','line_number':277,'multiline':False]['text':' CodeLlama','line_number':280,'multiline':False]['text':' LLaMA v2','line_number':283,'multiline':False]['text':' LLaMA v1','line_number':286,'multiline':False]['text':'','line_number':464,'multiline':False]['text':' data loading','line_number':465,'multiline':False]['text':' TODO: reuse (probably move to gguf.py?)','line_number':466,'multiline':False]['text':'','line_number':467,'multiline':False]['text':' print( "permute debug " + str(weights.shape[0]) + " x " + str(weights.shape[1]) + " nhead " + str(n_head) + " nheadkv " + str(n_kv_head) )','line_number':471,'multiline':False]['text':' double-check:','line_number':531,'multiline':False]['text':' Should be okay if it maps to the same numpy type?','line_number':555,'multiline':False]['text':' Where this was read from.','line_number':578,'multiline':False]['text':' For GGML models (which have vocab built in), the vocab.','line_number':580,'multiline':False]['text':' Original LLaMA models have each file contain one part of each tensor.','line_number':584,'multiline':False]['text':' Use a dict instead of a set to preserve order.','line_number':585,'multiline':False]['text':' only one file; don't go through this procedure since there might','line_number':591,'multiline':False]['text':' be quantized tensors','line_number':592,'multiline':False]['text':' the tensor is just duplicated in every file','line_number':595,'multiline':False]['text':' split by columns','line_number':600,'multiline':False]['text':' split by rows','line_number':603,'multiline':False]['text':' Use the first non-None vocab, if any.','line_number':622,'multiline':False]['text':' Transformers models put different tensors in different files, but','line_number':629,'multiline':False]['text':' don't split individual tensors between files.','line_number':630,'multiline':False]['text':' Functionality that simulates `torch.load` but where individual tensors are','line_number':662,'multiline':False]['text':' only loaded into memory on demand, not all at once.','line_number':663,'multiline':False]['text':' PyTorch can't do this natively as of time of writing:','line_number':664,'multiline':False]['text':' - https://github.com/pytorch/pytorch/issues/64327','line_number':665,'multiline':False]['text':' This allows us to de-shard without multiplying RAM usage, and also','line_number':666,'multiline':False]['text':' conveniently drops the PyTorch dependency (though we still need numpy).','line_number':667,'multiline':False]['text':' getattr used here as a workaround for mypy not being smart enough to determine','line_number':723,'multiline':False]['text':' the staticmethods have a __func__ attribute.','line_number':724,'multiline':False]['text':' Use mmap for the actual data to avoid race conditions with the file offset.','line_number':757,'multiline':False]['text':' A zip file, i.e. PyTorch format','line_number':791,'multiline':False]['text':' Probably safetensors','line_number':794,'multiline':False]['text':' Not reached.','line_number':811,'multiline':False]['text':' TODO: better logic to determine model name','line_number':870,'multiline':False]['text':' NOTE: `all_tokens` returns the base vocabulary and added tokens','line_number':917,'multiline':False]['text':' meta data','line_number':959,'multiline':False]['text':' meta data','line_number':992,'multiline':False]['text':' tensor info','line_number':997,'multiline':False]['text':' tensor data','line_number':1004,'multiline':False]['text':' HF models permut or pack some of the tensors, so we need to undo that','line_number':1048,'multiline':False]['text':' tmp[f"model.layers.{i}.self_attn.v_proj.weight"] =              model[f"model.layers.{i}.self_attn.v_proj.weight"]','line_number':1054,'multiline':False]['text':' Support the following patterns:','line_number':1084,'multiline':False]['text':' - x.00.pth, x.01.pth, etc.','line_number':1086,'multiline':False]['text':' - x-00001-of-00002.bin, x-00002-of-00002.bin, etc.','line_number':1088,'multiline':False]['text':' x.bin, x.bin.1, etc.','line_number':1090,'multiline':False]['text':' No matches.  This should only happen if the file was named, e.g.,','line_number':1112,'multiline':False]['text':' foo.0, and there was no file named foo.  Oh well, try to process it','line_number':1113,'multiline':False]['text':' as a single file.','line_number':1114,'multiline':False]['text':' Be extra-friendly and accept either a file or a directory:','line_number':1121,'multiline':False]['text':' Check if it's a set of safetensors files first','line_number':1123,'multiline':False]['text':' Try the PyTorch patterns too, with lower priority','line_number':1127,'multiline':False]['text':' Use `.parent` instead of /.. to handle the symlink case better.','line_number':1148,'multiline':False]['text':' We currently only support Q8_0 output on little endian systems.','line_number':1185,'multiline':False]['text':' FIXME: Try to respect vocab_dir somehow?','line_number':1240,'multiline':False]['text':' FIXME: Try to respect vocab_dir somehow?','line_number':1257,'multiline':False]