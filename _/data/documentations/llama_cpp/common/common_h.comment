['text':' Various helper functions and utilities','line_number':1,'multiline':False]['text':' _WIN32','line_number':24,'multiline':False]['text':' build info','line_number':34,'multiline':False]['text':'','line_number':40,'multiline':False]['text':' CLI argument parsing','line_number':41,'multiline':False]['text':'','line_number':42,'multiline':False]['text':' RNG seed','line_number':46,'multiline':False]['text':' number of threads to use for batch processing (-1 = use n_threads)','line_number':49,'multiline':False]['text':' new tokens to predict','line_number':50,'multiline':False]['text':' context size','line_number':51,'multiline':False]['text':' batch size for prompt processing (must be >=32 to use BLAS)','line_number':52,'multiline':False]['text':' number of tokens to keep from initial prompt','line_number':53,'multiline':False]['text':' number of tokens to draft during speculative decoding','line_number':54,'multiline':False]['text':' max number of chunks to process (-1 = unlimited)','line_number':55,'multiline':False]['text':' number of parallel sequences to decode','line_number':56,'multiline':False]['text':' number of sequences to decode','line_number':57,'multiline':False]['text':' speculative decoding accept probability','line_number':58,'multiline':False]['text':' speculative decoding split probability','line_number':59,'multiline':False]['text':' number of layers to store in VRAM (-1 - use default)','line_number':60,'multiline':False]['text':' number of layers to store in VRAM for the draft model (-1 - use default)','line_number':61,'multiline':False]['text':' the GPU that is used for scratch and small tensors','line_number':62,'multiline':False]['text':' how split tensors should be distributed across GPUs','line_number':63,'multiline':False]['text':' if non-zero then use beam search of given width.','line_number':64,'multiline':False]['text':' RoPE base frequency','line_number':65,'multiline':False]['text':' RoPE frequency scaling factor','line_number':66,'multiline':False]['text':' YaRN extrapolation mix factor','line_number':67,'multiline':False]['text':' YaRN magnitude scaling factor','line_number':68,'multiline':False]['text':' YaRN low correction dim','line_number':69,'multiline':False]['text':' YaRN high correction dim','line_number':70,'multiline':False]['text':' YaRN original context length','line_number':71,'multiline':False]['text':' TODO: better to be int32_t for alignment','line_number':72,'multiline':False]['text':'       pinging @cebtenzzre','line_number':73,'multiline':False]['text':' // sampling parameters','line_number':75,'multiline':False]['text':' model path','line_number':78,'multiline':False]['text':' draft model for speculative decoding','line_number':79,'multiline':False]['text':' model alias','line_number':80,'multiline':False]['text':' store the external prompt file name','line_number':82,'multiline':False]['text':' path to file for saving/loading prompt eval state','line_number':83,'multiline':False]['text':' string to prefix user inputs with','line_number':84,'multiline':False]['text':' string to suffix user inputs with','line_number':85,'multiline':False]['text':' string upon seeing which more user input is prompted','line_number':86,'multiline':False]['text':' directory in which to save YAML log files','line_number':87,'multiline':False]['text':' TODO: avoid tuple, use struct','line_number':91,'multiline':False]['text':' lora adapter path with user defined scale','line_number':92,'multiline':False]['text':' base model path for the lora adapter','line_number':93,'multiline':False]['text':' stride for perplexity calculations. If left at 0, the pre-existing approach will be used.','line_number':95,'multiline':False]['text':' = 0 -> ppl output is as usual, = 1 -> ppl output is num_tokens, ppl, one per line','line_number':96,'multiline':False]['text':'                                       (which is more convenient to use for plotting)','line_number':97,'multiline':False]['text':'','line_number':98,'multiline':False]['text':' compute HellaSwag score over random tasks from datafile supplied in prompt','line_number':99,'multiline':False]['text':' number of tasks to use when computing the HellaSwag score','line_number':100,'multiline':False]['text':' if true, use mul_mat_q kernels instead of cuBLAS','line_number':102,'multiline':False]['text':' do not randomize prompt if none provided','line_number':103,'multiline':False]['text':' use color to distinguish generations and inputs','line_number':104,'multiline':False]['text':' interactive mode','line_number':105,'multiline':False]['text':' chatml mode (used for models trained on chatml syntax)','line_number':106,'multiline':False]['text':' save user input and generations to prompt cache','line_number':107,'multiline':False]['text':' open the prompt cache read-only and do not update it','line_number':108,'multiline':False]['text':' get only sentence embedding','line_number':110,'multiline':False]['text':' escape "\n", "\r", "\t", "\'", "\"", and "\\"','line_number':111,'multiline':False]['text':' wait for user input immediately','line_number':112,'multiline':False]['text':' reverse the usage of `\`','line_number':113,'multiline':False]['text':' improves compatibility with subprocesses and limited consoles','line_number':114,'multiline':False]['text':' insert new sequences for decoding on-the-fly','line_number':115,'multiline':False]['text':' prefix BOS to user inputs, preceding input_prefix','line_number':117,'multiline':False]['text':' ignore generated EOS tokens','line_number':118,'multiline':False]['text':' instruction mode (used for Alpaca models)','line_number':119,'multiline':False]['text':' return logits for all tokens in the batch','line_number':120,'multiline':False]['text':' use mmap for faster loads','line_number':121,'multiline':False]['text':' use mlock to keep model in memory','line_number':122,'multiline':False]['text':' attempt optimizations that help on some NUMA systems','line_number':123,'multiline':False]['text':' print prompt tokens before generation','line_number':124,'multiline':False]['text':' use infill mode','line_number':125,'multiline':False]['text':' dump the KV cache contents for debugging purposes','line_number':126,'multiline':False]['text':' disable KV offloading','line_number':127,'multiline':False]['text':' KV cache data type for the K','line_number':129,'multiline':False]['text':' KV cache data type for the V','line_number':130,'multiline':False]['text':' multimodal models (see examples/llava)','line_number':132,'multiline':False]['text':' path to multimodal projector','line_number':133,'multiline':False]['text':' path to an image file','line_number':134,'multiline':False]['text':'','line_number':149,'multiline':False]['text':' String parsing','line_number':150,'multiline':False]['text':'','line_number':151,'multiline':False]['text':'','line_number':155,'multiline':False]['text':' Model utils','line_number':156,'multiline':False]['text':'','line_number':157,'multiline':False]['text':' TODO: avoid tuplue, use struct','line_number':159,'multiline':False]['text':' Batch utils','line_number':165,'multiline':False]['text':'','line_number':176,'multiline':False]['text':' Vocab utils','line_number':177,'multiline':False]['text':'','line_number':178,'multiline':False]['text':' tokenizes a string into a vector of tokens','line_number':180,'multiline':False]['text':' should work similar to Python's `tokenizer.encode`','line_number':181,'multiline':False]['text':' tokenizes a token into a piece','line_number':194,'multiline':False]['text':' should work similar to Python's `tokenizer.id_to_piece`','line_number':195,'multiline':False]['text':' TODO: these should be moved in llama.h C-style API under single `llama_detokenize` function','line_number':200,'multiline':False]['text':'       that takes into account the tokenizer type and decides how to handle the leading space','line_number':201,'multiline':False]['text':'','line_number':202,'multiline':False]['text':' detokenizes a vector of tokens into a string','line_number':203,'multiline':False]['text':' should work similar to Python's `tokenizer.decode`','line_number':204,'multiline':False]['text':' removes the leading space from the first non-BOS token','line_number':205,'multiline':False]['text':' detokenizes a vector of tokens into a string','line_number':210,'multiline':False]['text':' should work similar to Python's `tokenizer.decode`','line_number':211,'multiline':False]['text':' Uses the value from the model metadata if possible, otherwise','line_number':216,'multiline':False]['text':' defaults to true when model type is SPM, otherwise false.','line_number':217,'multiline':False]['text':'','line_number':220,'multiline':False]['text':' YAML utils','line_number':221,'multiline':False]['text':'','line_number':222,'multiline':False]['text':'','line_number':234,'multiline':False]['text':' KV cache utils','line_number':235,'multiline':False]['text':'','line_number':236,'multiline':False]['text':' Dump the KV cache view with the number of sequences per cell.','line_number':238,'multiline':False]['text':' Dump the KV cache view showing individual sequences in each cell (long output).','line_number':241,'multiline':False]