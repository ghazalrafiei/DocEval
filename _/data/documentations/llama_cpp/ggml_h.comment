['text':'','line_number':3,'multiline':False]['text':' GGML Tensor Library','line_number':4,'multiline':False]['text':'','line_number':5,'multiline':False]['text':' This documentation is still a work in progress.','line_number':6,'multiline':False]['text':' If you wish some specific topics to be covered, feel free to drop a comment:','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':'   https://github.com/ggerganov/whisper.cpp/issues/40','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' ## Overview','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':' This library implements:','line_number':13,'multiline':False]['text':'','line_number':14,'multiline':False]['text':'  - a set of tensor operations','line_number':15,'multiline':False]['text':'  - automatic differentiation','line_number':16,'multiline':False]['text':'  - basic optimization algorithms','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':' The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,','line_number':19,'multiline':False]['text':' but is not limited to, the following:','line_number':20,'multiline':False]['text':'','line_number':21,'multiline':False]['text':'  - linear regression','line_number':22,'multiline':False]['text':'  - support vector machines','line_number':23,'multiline':False]['text':'  - neural networks','line_number':24,'multiline':False]['text':'','line_number':25,'multiline':False]['text':' The library allows the user to define a certain function using the available tensor operations. This function','line_number':26,'multiline':False]['text':' definition is represented internally via a computation graph. Each tensor operation in the function definition','line_number':27,'multiline':False]['text':' corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the','line_number':28,'multiline':False]['text':' function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized','line_number':29,'multiline':False]['text':' using one of the available optimization algorithms.','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' For example, here we define the function: f(x) = a*x^2 + b','line_number':32,'multiline':False]['text':'','line_number':33,'multiline':False]['text':'   {','line_number':34,'multiline':False]['text':'       struct ggml_init_params params = {','line_number':35,'multiline':False]['text':'           .mem_size   = 16*1024*1024,','line_number':36,'multiline':False]['text':'           .mem_buffer = NULL,','line_number':37,'multiline':False]['text':'       };','line_number':38,'multiline':False]['text':'','line_number':39,'multiline':False]['text':'       // memory allocation happens here','line_number':40,'multiline':False]['text':'       struct ggml_context * ctx = ggml_init(params);','line_number':41,'multiline':False]['text':'','line_number':42,'multiline':False]['text':'       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);','line_number':43,'multiline':False]['text':'','line_number':44,'multiline':False]['text':'       ggml_set_param(ctx, x); // x is an input variable','line_number':45,'multiline':False]['text':'','line_number':46,'multiline':False]['text':'       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);','line_number':47,'multiline':False]['text':'       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);','line_number':48,'multiline':False]['text':'       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);','line_number':49,'multiline':False]['text':'       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);','line_number':50,'multiline':False]['text':'','line_number':51,'multiline':False]['text':'       ...','line_number':52,'multiline':False]['text':'   }','line_number':53,'multiline':False]['text':'','line_number':54,'multiline':False]['text':' Notice that the function definition above does not involve any actual computation. The computation is performed only','line_number':55,'multiline':False]['text':' when the user explicitly requests it. For example, to compute the function's value at x = 2.0:','line_number':56,'multiline':False]['text':'','line_number':57,'multiline':False]['text':'   {','line_number':58,'multiline':False]['text':'       ...','line_number':59,'multiline':False]['text':'','line_number':60,'multiline':False]['text':'       struct ggml_cgraph * gf = ggml_new_graph(ctx);','line_number':61,'multiline':False]['text':'       ggml_build_forward_expand(gf, f);','line_number':62,'multiline':False]['text':'','line_number':63,'multiline':False]['text':'       // set the input variable and parameter values','line_number':64,'multiline':False]['text':'       ggml_set_f32(x, 2.0f);','line_number':65,'multiline':False]['text':'       ggml_set_f32(a, 3.0f);','line_number':66,'multiline':False]['text':'       ggml_set_f32(b, 4.0f);','line_number':67,'multiline':False]['text':'','line_number':68,'multiline':False]['text':'       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);','line_number':69,'multiline':False]['text':'','line_number':70,'multiline':False]['text':'       printf("f = %f\n", ggml_get_f32_1d(f, 0));','line_number':71,'multiline':False]['text':'','line_number':72,'multiline':False]['text':'       ...','line_number':73,'multiline':False]['text':'   }','line_number':74,'multiline':False]['text':'','line_number':75,'multiline':False]['text':' The actual computation is performed in the ggml_graph_compute() function.','line_number':76,'multiline':False]['text':'','line_number':77,'multiline':False]['text':' The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the','line_number':78,'multiline':False]['text':' ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know','line_number':79,'multiline':False]['text':' in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory','line_number':80,'multiline':False]['text':' and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was','line_number':81,'multiline':False]['text':' actually needed.','line_number':82,'multiline':False]['text':'','line_number':83,'multiline':False]['text':' The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic','line_number':84,'multiline':False]['text':' differentiation and optimization algorithms.','line_number':85,'multiline':False]['text':'','line_number':86,'multiline':False]['text':' The described approach allows to define the function graph once and then compute its forward or backward graphs','line_number':87,'multiline':False]['text':' multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way','line_number':88,'multiline':False]['text':' the user can avoid the memory allocation overhead at runtime.','line_number':89,'multiline':False]['text':'','line_number':90,'multiline':False]['text':' The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class','line_number':91,'multiline':False]['text':' citizens, but in theory the library can be extended to support FP8 and integer data types.','line_number':92,'multiline':False]['text':'','line_number':93,'multiline':False]['text':' Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary','line_number':94,'multiline':False]['text':' and binary operations. Most of the available operations fall into one of these two categories. With time, it became','line_number':95,'multiline':False]['text':' clear that the library needs to support more complex operations. The way to support these operations is not clear','line_number':96,'multiline':False]['text':' yet, but a few examples are demonstrated in the following operations:','line_number':97,'multiline':False]['text':'','line_number':98,'multiline':False]['text':'   - ggml_permute()','line_number':99,'multiline':False]['text':'   - ggml_conv_1d_1s()','line_number':100,'multiline':False]['text':'   - ggml_conv_1d_2s()','line_number':101,'multiline':False]['text':'','line_number':102,'multiline':False]['text':' For each tensor operator, the library implements a forward and backward computation function. The forward function','line_number':103,'multiline':False]['text':' computes the output tensor value given the input tensor values. The backward function computes the adjoint of the','line_number':104,'multiline':False]['text':' input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a','line_number':105,'multiline':False]['text':' calculus class, or watch the following video:','line_number':106,'multiline':False]['text':'','line_number':107,'multiline':False]['text':'   What is Automatic Differentiation?','line_number':108,'multiline':False]['text':'   https://www.youtube.com/watch?v=wG_nF1awSSY','line_number':109,'multiline':False]['text':'','line_number':110,'multiline':False]['text':'','line_number':111,'multiline':False]['text':' ## Tensor data (struct ggml_tensor)','line_number':112,'multiline':False]['text':'','line_number':113,'multiline':False]['text':' The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of','line_number':114,'multiline':False]['text':' the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains','line_number':115,'multiline':False]['text':' pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:','line_number':116,'multiline':False]['text':'','line_number':117,'multiline':False]['text':'   {','line_number':118,'multiline':False]['text':'       struct ggml_tensor * c = ggml_add(ctx, a, b);','line_number':119,'multiline':False]['text':'','line_number':120,'multiline':False]['text':'       assert(c->src[0] == a);','line_number':121,'multiline':False]['text':'       assert(c->src[1] == b);','line_number':122,'multiline':False]['text':'   }','line_number':123,'multiline':False]['text':'','line_number':124,'multiline':False]['text':' The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the','line_number':125,'multiline':False]['text':' number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows','line_number':126,'multiline':False]['text':' to store tensors that are not contiguous in memory, which is useful for operations such as transposition and','line_number':127,'multiline':False]['text':' permutation. All tensor operations have to take the stride into account and not assume that the tensor is','line_number':128,'multiline':False]['text':' contiguous in memory.','line_number':129,'multiline':False]['text':'','line_number':130,'multiline':False]['text':' The data of the tensor is accessed via the "data" pointer. For example:','line_number':131,'multiline':False]['text':'','line_number':132,'multiline':False]['text':'   {','line_number':133,'multiline':False]['text':'       const int nx = 2;','line_number':134,'multiline':False]['text':'       const int ny = 3;','line_number':135,'multiline':False]['text':'','line_number':136,'multiline':False]['text':'       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, nx, ny);','line_number':137,'multiline':False]['text':'','line_number':138,'multiline':False]['text':'       for (int y = 0; y < ny; y++) {','line_number':139,'multiline':False]['text':'           for (int x = 0; x < nx; x++) {','line_number':140,'multiline':False]['text':'               *(float *) ((char *) a->data + y*a->nb[1] + x*a->nb[0]) = x + y;','line_number':141,'multiline':False]['text':'           }','line_number':142,'multiline':False]['text':'       }','line_number':143,'multiline':False]['text':'','line_number':144,'multiline':False]['text':'       ...','line_number':145,'multiline':False]['text':'   }','line_number':146,'multiline':False]['text':'','line_number':147,'multiline':False]['text':' Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.','line_number':148,'multiline':False]['text':'','line_number':149,'multiline':False]['text':' ## The matrix multiplication operator (ggml_mul_mat)','line_number':150,'multiline':False]['text':'','line_number':151,'multiline':False]['text':' TODO','line_number':152,'multiline':False]['text':'','line_number':153,'multiline':False]['text':'','line_number':154,'multiline':False]['text':' ## Multi-threading','line_number':155,'multiline':False]['text':'','line_number':156,'multiline':False]['text':' TODO','line_number':157,'multiline':False]['text':'','line_number':158,'multiline':False]['text':'','line_number':159,'multiline':False]['text':' ## Overview of ggml.c','line_number':160,'multiline':False]['text':'','line_number':161,'multiline':False]['text':' TODO','line_number':162,'multiline':False]['text':'','line_number':163,'multiline':False]['text':'','line_number':164,'multiline':False]['text':' ## SIMD optimizations','line_number':165,'multiline':False]['text':'','line_number':166,'multiline':False]['text':' TODO','line_number':167,'multiline':False]['text':'','line_number':168,'multiline':False]['text':'','line_number':169,'multiline':False]['text':' ## Debugging ggml','line_number':170,'multiline':False]['text':'','line_number':171,'multiline':False]['text':' TODO','line_number':172,'multiline':False]['text':'','line_number':173,'multiline':False]['text':'','line_number':174,'multiline':False]['text':' TODO: support for clang','line_number':190,'multiline':False]['text':' "ggml"','line_number':211,'multiline':False]['text':' bump this on quantization format changes','line_number':214,'multiline':False]['text':' do not change this','line_number':215,'multiline':False]['text':' used to copy the number of elements and stride in bytes of tensors into local variables.','line_number':262,'multiline':False]['text':' main purpose is to reduce code duplication and improve readability.','line_number':263,'multiline':False]['text':'','line_number':264,'multiline':False]['text':' example:','line_number':265,'multiline':False]['text':'','line_number':266,'multiline':False]['text':'    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);','line_number':267,'multiline':False]['text':'    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);','line_number':268,'multiline':False]['text':'','line_number':269,'multiline':False]['text':' convert FP16 <-> FP32','line_number':312,'multiline':False]['text':' GGML_TYPE_Q4_2 = 4, support has been removed','line_number':327,'multiline':False]['text':' GGML_TYPE_Q4_3 (5) support has been removed','line_number':328,'multiline':False]['text':' k-quantizations','line_number':333,'multiline':False]['text':' model file types','line_number':352,'multiline':False]['text':' except 1d tensors','line_number':356,'multiline':False]['text':' except 1d tensors','line_number':357,'multiline':False]['text':' except 1d tensors','line_number':358,'multiline':False]['text':' tok_embeddings.weight and output.weight are F16','line_number':359,'multiline':False]['text':' except 1d tensors','line_number':360,'multiline':False]['text':' except 1d tensors','line_number':361,'multiline':False]['text':' except 1d tensors','line_number':362,'multiline':False]['text':' except 1d tensors','line_number':363,'multiline':False]['text':' except 1d tensors','line_number':364,'multiline':False]['text':' except 1d tensors','line_number':365,'multiline':False]['text':' except 1d tensors','line_number':366,'multiline':False]['text':' except 1d tensors','line_number':367,'multiline':False]['text':' available tensor operations:','line_number':370,'multiline':False]['text':' normalize','line_number':392,'multiline':False]['text':' nearest interpolate','line_number':425,'multiline':False]['text':' ggml object','line_number':484,'multiline':False]['text':' n-dimensional tensor','line_number':498,'multiline':False]['text':' number of elements','line_number':505,'multiline':False]['text':' stride in bytes:','line_number':506,'multiline':False]['text':' nb[0] = ggml_type_size(type)','line_number':507,'multiline':False]['text':' nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding','line_number':508,'multiline':False]['text':' nb[i] = nb[i-1] * ne[i-1]','line_number':509,'multiline':False]['text':' compute data','line_number':511,'multiline':False]['text':' op params - allocated as int32_t for alignment','line_number':514,'multiline':False]['text':' performance','line_number':522,'multiline':False]['text':' extra things e.g. for ggml-cuda.cu','line_number':534,'multiline':False]['text':' the compute plan that needs to be prepared for ggml_graph_compute()','line_number':541,'multiline':False]['text':' since https://github.com/ggerganov/ggml/issues/287','line_number':542,'multiline':False]['text':' size of work buffer, calculated by `ggml_graph_plan()`','line_number':544,'multiline':False]['text':' work buffer, to be allocated by caller before calling to `ggml_graph_compute()`','line_number':545,'multiline':False]['text':' abort ggml_graph_compute when true','line_number':549,'multiline':False]['text':' computation graph','line_number':565,'multiline':False]['text':' performance','line_number':579,'multiline':False]['text':' scratch buffer','line_number':585,'multiline':False]['text':' memory pool','line_number':593,'multiline':False]['text':' bytes','line_number':594,'multiline':False]['text':' if NULL, memory will be allocated internally','line_number':595,'multiline':False]['text':' don't allocate memory for the tensor data','line_number':596,'multiline':False]['text':' compute types','line_number':600,'multiline':False]['text':' NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.','line_number':602,'multiline':False]['text':' This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.','line_number':603,'multiline':False]['text':' ith = thread index, nth = number of threads','line_number':613,'multiline':False]['text':' work buffer for all threads','line_number':616,'multiline':False]['text':' misc','line_number':621,'multiline':False]['text':' call this once at the beginning of the program','line_number':623,'multiline':False]['text':' call once for better performance on NUMA systems','line_number':631,'multiline':False]['text':' true if init detected that system has >1 NUMA node','line_number':632,'multiline':False]['text':' same as ggml_nbytes() but padded to GGML_MEM_ALIGN','line_number':640,'multiline':False]['text':' size in bytes for all elements in a block','line_number':644,'multiline':False]['text':' size in bytes for all elements in a row','line_number':645,'multiline':False]['text':' ggml_type_size()/ggml_blck_size() as float','line_number':648,'multiline':False]['text':' unary or op name','line_number':656,'multiline':False]['text':' TODO: temporary until model loading of ggml examples is refactored','line_number':662,'multiline':False]['text':' returns 1 for scalars','line_number':672,'multiline':False]['text':' use this to compute the memory overhead of a tensor','line_number':676,'multiline':False]['text':' main','line_number':679,'multiline':False]['text':' Context tensor enumeration and lookup','line_number':732,'multiline':False]['text':' Converts a flat index into coordinates','line_number':741,'multiline':False]['text':'','line_number':766,'multiline':False]['text':' operations on tensors with backpropagation','line_number':767,'multiline':False]['text':'','line_number':768,'multiline':False]['text':' in-place, returns view(a)','line_number':774,'multiline':False]['text':' dst = a','line_number':805,'multiline':False]['text':' view(dst, nb1, nb2, nb3, offset) += b','line_number':806,'multiline':False]['text':' return dst','line_number':807,'multiline':False]['text':' return scalar','line_number':880,'multiline':False]['text':' sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]','line_number':885,'multiline':False]['text':' mean along rows','line_number':890,'multiline':False]['text':' argmax along rows','line_number':895,'multiline':False]['text':' if a is the same shape as b, and a is not parameter, return a','line_number':900,'multiline':False]['text':' otherwise, return a new tensor: repeat(a) to fit in b','line_number':901,'multiline':False]['text':' sums repetitions in a into shape of b','line_number':907,'multiline':False]['text':' concat a and b on dim 2','line_number':913,'multiline':False]['text':' used in stable-diffusion','line_number':914,'multiline':False]['text':' a - x','line_number':1004,'multiline':False]['text':' b - dy','line_number':1005,'multiline':False]['text':' normalize along rows','line_number':1011,'multiline':False]['text':' group normalize along ne0*ne1*n_groups','line_number':1032,'multiline':False]['text':' used in stable-diffusion','line_number':1033,'multiline':False]['text':' TODO: eps is hardcoded to 1e-6 for now','line_number':1034,'multiline':False]['text':' a - x','line_number':1045,'multiline':False]['text':' b - dy','line_number':1046,'multiline':False]['text':' A: k columns, n rows => [ne03, ne02, n, k]','line_number':1053,'multiline':False]['text':' B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]','line_number':1054,'multiline':False]['text':' result is n columns, m rows => [ne03 * x, ne02 * y, m, n]','line_number':1055,'multiline':False]['text':' indirect matrix multiplication','line_number':1061,'multiline':False]['text':'  ggml_mul_mat_id(ctx, as, ids, id, b) ~= ggml_mul_mat(as[ids[id]], b)','line_number':1062,'multiline':False]['text':' A: m columns, n rows,','line_number':1071,'multiline':False]['text':' B: p columns, n rows,','line_number':1072,'multiline':False]['text':' result is m columns, p rows','line_number':1073,'multiline':False]['text':'','line_number':1079,'multiline':False]['text':' operations on tensors without backpropagation','line_number':1080,'multiline':False]['text':'','line_number':1081,'multiline':False]['text':' in-place, returns view(a)','line_number':1088,'multiline':False]['text':' b -> view(a,offset,nb1,nb2,3), return modified a','line_number':1094,'multiline':False]['text':' b -> view(a,offset,nb1,nb2,3), return view(a)','line_number':1104,'multiline':False]['text':' b -> view(a,offset,nb1,nb2,3), return modified a','line_number':1126,'multiline':False]['text':' b -> view(a,offset,nb1,nb2,3), return view(a)','line_number':1134,'multiline':False]['text':' a -> b, return view(b)','line_number':1142,'multiline':False]['text':' a -> b, in-place, return view(b)','line_number':1148,'multiline':False]['text':' make contiguous','line_number':1154,'multiline':False]['text':' make contiguous, in-place','line_number':1159,'multiline':False]['text':' make contiguous, with new shape','line_number':1164,'multiline':False]['text':' return view(a), b specifies the new shape','line_number':1191,'multiline':False]['text':' TODO: when we start computing gradient, make a copy instead of view','line_number':1192,'multiline':False]['text':' return view(a)','line_number':1198,'multiline':False]['text':' TODO: when we start computing gradient, make a copy instead of view','line_number':1199,'multiline':False]['text':' return view(a)','line_number':1211,'multiline':False]['text':' TODO: when we start computing gradient, make a copy instead of view','line_number':1212,'multiline':False]['text':' offset in bytes','line_number':1228,'multiline':False]['text':' row stride in bytes','line_number':1240,'multiline':False]['text':' row   stride in bytes','line_number':1249,'multiline':False]['text':' slice stride in bytes','line_number':1250,'multiline':False]['text':' row   stride in bytes','line_number':1260,'multiline':False]['text':' slice stride in bytes','line_number':1261,'multiline':False]['text':' alias for ggml_permute(ctx, a, 1, 0, 2, 3)','line_number':1273,'multiline':False]['text':' supports 3D: a->ne[2] == b->ne[1]','line_number':1278,'multiline':False]['text':' set elements above the diagonal to -INF','line_number':1294,'multiline':False]['text':' in-place, returns view(a)','line_number':1300,'multiline':False]['text':' set elements above the diagonal to 0','line_number':1306,'multiline':False]['text':' in-place, returns view(a)','line_number':1312,'multiline':False]['text':' in-place, returns view(a)','line_number':1322,'multiline':False]['text':' fused soft_max(a*scale + mask)','line_number':1327,'multiline':False]['text':' mask is optional','line_number':1328,'multiline':False]['text':' in-place, returns view(a)','line_number':1340,'multiline':False]['text':' rotary position embedding','line_number':1346,'multiline':False]['text':' if mode & 1 == 1, skip n_past elements (DEPRECATED)','line_number':1347,'multiline':False]['text':' if mode & 2 == 1, GPT-NeoX style','line_number':1348,'multiline':False]['text':' if mode & 4 == 1, ChatGLM style','line_number':1349,'multiline':False]['text':'','line_number':1350,'multiline':False]['text':' b is an int32 vector with size a->ne[2], it contains the positions','line_number':1351,'multiline':False]['text':' in-place, returns view(a)','line_number':1360,'multiline':False]['text':' custom RoPE','line_number':1369,'multiline':False]['text':' in-place, returns view(a)','line_number':1385,'multiline':False]['text':' compute correction dims for YaRN RoPE scaling','line_number':1401,'multiline':False]['text':' xPos RoPE, in-place, returns view(a)','line_number':1405,'multiline':False]['text':' rotary position embedding backward, i.e compute dx from dy','line_number':1414,'multiline':False]['text':' a - dy','line_number':1415,'multiline':False]['text':' alibi position embedding','line_number':1433,'multiline':False]['text':' in-place, returns view(a)','line_number':1434,'multiline':False]['text':' clamp','line_number':1442,'multiline':False]['text':' in-place, returns view(a)','line_number':1443,'multiline':False]['text':' stride','line_number':1466,'multiline':False]['text':' padding','line_number':1467,'multiline':False]['text':' dilation','line_number':1468,'multiline':False]['text':' conv_1d with padding = half','line_number':1470,'multiline':False]['text':' alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)','line_number':1471,'multiline':False]['text':' kernel size is a->ne[0] x a->ne[1]','line_number':1499,'multiline':False]['text':' stride is equal to kernel size','line_number':1500,'multiline':False]['text':' padding is zero','line_number':1501,'multiline':False]['text':' example:','line_number':1502,'multiline':False]['text':' a:     16   16    3  768','line_number':1503,'multiline':False]['text':' b:   1024 1024    3    1','line_number':1504,'multiline':False]['text':' res:   64   64  768    1','line_number':1505,'multiline':False]['text':' used in sam','line_number':1506,'multiline':False]['text':' kernel size is a->ne[0] x a->ne[1]','line_number':1512,'multiline':False]['text':' stride is 1','line_number':1513,'multiline':False]['text':' padding is half','line_number':1514,'multiline':False]['text':' example:','line_number':1515,'multiline':False]['text':' a:      3    3    256  256','line_number':1516,'multiline':False]['text':' b:     64   64    256    1','line_number':1517,'multiline':False]['text':' res:   64   64    256    1','line_number':1518,'multiline':False]['text':' used in sam','line_number':1519,'multiline':False]['text':' kernel size','line_number':1541,'multiline':False]['text':' stride','line_number':1542,'multiline':False]['text':' padding','line_number':1543,'multiline':False]['text':' the result will have 2*p0 padding for the first dimension','line_number':1545,'multiline':False]['text':' and 2*p1 padding for the second dimension','line_number':1546,'multiline':False]['text':' nearest interpolate','line_number':1558,'multiline':False]['text':' used in stable-diffusion','line_number':1559,'multiline':False]['text':' pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]','line_number':1565,'multiline':False]['text':' sort rows','line_number':1574,'multiline':False]['text':' top k elements per row','line_number':1585,'multiline':False]['text':' partition into non-overlapping windows with padding if needed','line_number':1614,'multiline':False]['text':' example:','line_number':1615,'multiline':False]['text':' a:   768   64   64    1','line_number':1616,'multiline':False]['text':' w:    14','line_number':1617,'multiline':False]['text':' res: 768   14   14    25','line_number':1618,'multiline':False]['text':' used in sam','line_number':1619,'multiline':False]['text':' reverse of ggml_win_part','line_number':1625,'multiline':False]['text':' used in sam','line_number':1626,'multiline':False]['text':' used in sam','line_number':1644,'multiline':False]['text':' used in sam','line_number':1651,'multiline':False]['text':' custom operators','line_number':1664,'multiline':False]['text':' custom operators v2','line_number':1741,'multiline':False]['text':' loss function','line_number':1797,'multiline':False]['text':'','line_number':1810,'multiline':False]['text':' automatic differentiation','line_number':1811,'multiline':False]['text':'','line_number':1812,'multiline':False]['text':' graph allocation in a context','line_number':1822,'multiline':False]['text':' size = GGML_DEFAULT_GRAPH_SIZE, grads = false','line_number':1823,'multiline':False]['text':' zero grads','line_number':1828,'multiline':False]['text':' ggml_graph_plan() has to be called before ggml_graph_compute()','line_number':1834,'multiline':False]['text':' when plan.work_size > 0, caller must allocate memory for plan.work_data','line_number':1835,'multiline':False]['text':'= GGML_DEFAULT_N_THREADS','line_number':1836,'multiline':True]['text':' same as ggml_graph_compute() but the work data is allocated as a part of the context','line_number':1839,'multiline':False]['text':' note: the drawback of this API is that you must have ensured that the context has enough memory for the work data','line_number':1840,'multiline':False]['text':' print info and performance information for the graph','line_number':1848,'multiline':False]['text':' dump the graph into a file using the dot format','line_number':1851,'multiline':False]['text':' build gradient checkpointing backward graph gb for gf using provided checkpoints','line_number':1854,'multiline':False]['text':' gb_tmp will contain original backward graph with rewritten backward process nodes,','line_number':1855,'multiline':False]['text':' but without the second forward pass nodes.','line_number':1856,'multiline':False]['text':'','line_number':1864,'multiline':False]['text':' optimization','line_number':1865,'multiline':False]['text':'','line_number':1866,'multiline':False]['text':' optimization methods','line_number':1868,'multiline':False]['text':' linesearch methods','line_number':1874,'multiline':False]['text':' optimization return values','line_number':1883,'multiline':False]['text':' optimization parameters','line_number':1902,'multiline':False]['text':'','line_number':1903,'multiline':False]['text':'   see ggml.c (ggml_opt_default_params) for default values','line_number':1904,'multiline':False]['text':'','line_number':1905,'multiline':False]['text':' delta-based convergence test','line_number':1913,'multiline':False]['text':'','line_number':1914,'multiline':False]['text':'   if past == 0 - disabled','line_number':1915,'multiline':False]['text':'   if past > 0:','line_number':1916,'multiline':False]['text':'     stop if |f(x) - f(x_past)| < delta * max(1, |f(x)|)','line_number':1917,'multiline':False]['text':'','line_number':1918,'multiline':False]['text':' maximum number of iterations without improvement','line_number':1922,'multiline':False]['text':'','line_number':1923,'multiline':False]['text':'   if 0 - disabled','line_number':1924,'multiline':False]['text':'   if > 0:','line_number':1925,'multiline':False]['text':'     assume convergence if no cost improvement in this number of iterations','line_number':1926,'multiline':False]['text':'','line_number':1927,'multiline':False]['text':' ADAM parameters','line_number':1935,'multiline':False]['text':' schedule multiplier (fixed, decay or warmup)','line_number':1939,'multiline':False]['text':' weight decay for AdamW, use 0.0f to disable','line_number':1940,'multiline':False]['text':' minimum number of tensor dimension to apply weight decay','line_number':1941,'multiline':False]['text':' learning rate','line_number':1942,'multiline':False]['text':' epsilon for numerical stability','line_number':1945,'multiline':False]['text':' epsilon for convergence test','line_number':1946,'multiline':False]['text':' epsilon for convergence test','line_number':1947,'multiline':False]['text':' gradient clipping','line_number':1948,'multiline':False]['text':' LBFGS parameters','line_number':1951,'multiline':False]['text':' number of corrections to approximate the inv. Hessian','line_number':1953,'multiline':False]['text':' convergence tolerance','line_number':1957,'multiline':False]['text':' line search tolerance','line_number':1958,'multiline':False]['text':' number of parameter elements','line_number':1972,'multiline':False]['text':' current gradient','line_number':1980,'multiline':False]['text':' first moment','line_number':1981,'multiline':False]['text':' second moment','line_number':1982,'multiline':False]['text':' past function values','line_number':1983,'multiline':False]['text':' current parameters','line_number':1990,'multiline':False]['text':' previous parameters','line_number':1991,'multiline':False]['text':' current gradient','line_number':1992,'multiline':False]['text':' previous gradient','line_number':1993,'multiline':False]['text':' search direction','line_number':1994,'multiline':False]['text':' past function values','line_number':1995,'multiline':False]['text':' the L-BFGS memory alpha','line_number':1996,'multiline':False]['text':' the L-BFGS memory ys','line_number':1997,'multiline':False]['text':' the L-BFGS memory s','line_number':1998,'multiline':False]['text':' the L-BFGS memory y','line_number':1999,'multiline':False]['text':' optimize the function defined by the tensor f','line_number':2011,'multiline':False]['text':' initialize optimizer context','line_number':2017,'multiline':False]['text':' continue optimizing the function defined by the tensor f','line_number':2024,'multiline':False]['text':' continue optimizing the function defined by the tensor f','line_number':2030,'multiline':False]['text':'','line_number':2040,'multiline':False]['text':' quantization','line_number':2041,'multiline':False]['text':'','line_number':2042,'multiline':False]['text':' TODO: these would probably get removed in favor of the more general ggml_quantize_chunk','line_number':2044,'multiline':False]['text':'','line_number':2059,'multiline':False]['text':' gguf','line_number':2060,'multiline':False]['text':'','line_number':2061,'multiline':False]['text':' marks the end of the enum','line_number':2077,'multiline':False]['text':' if not NULL, create a ggml_context and allocate the tensor data in it','line_number':2085,'multiline':False]['text':'GGML_API struct gguf_context * gguf_init_from_buffer(..);','line_number':2091,'multiline':False]['text':' will abort if the wrong type is used for the key','line_number':2109,'multiline':False]['text':' overrides existing values or adds a new one','line_number':2132,'multiline':False]['text':' set or add KV pairs from another context','line_number':2148,'multiline':False]['text':' manage tensor info','line_number':2151,'multiline':False]['text':' writing gguf files can be done in 2 ways:','line_number':2156,'multiline':False]['text':'','line_number':2157,'multiline':False]['text':' - write the entire gguf_context to a binary file in a single pass:','line_number':2158,'multiline':False]['text':'','line_number':2159,'multiline':False]['text':'   gguf_write_to_file(ctx, fname);','line_number':2160,'multiline':False]['text':'','line_number':2161,'multiline':False]['text':' - first prepare a file with a placeholder for the meta data, write the tensor data, then write the meta data:','line_number':2162,'multiline':False]['text':'','line_number':2163,'multiline':False]['text':'   FILE * f = fopen(fname, "wb");','line_number':2164,'multiline':False]['text':'   fseek(f, gguf_get_meta_size(ctx), SEEK_SET);','line_number':2165,'multiline':False]['text':'   fwrite(f, ...);','line_number':2166,'multiline':False]['text':'   void * data = gguf_meta_get_meta_data(ctx);','line_number':2167,'multiline':False]['text':'   fseek(f, 0, SEEK_SET);','line_number':2168,'multiline':False]['text':'   fwrite(f, data, gguf_get_meta_size(ctx));','line_number':2169,'multiline':False]['text':'   free(data);','line_number':2170,'multiline':False]['text':'   fclose(f);','line_number':2171,'multiline':False]['text':'','line_number':2172,'multiline':False]['text':' write the entire context to a binary file','line_number':2174,'multiline':False]['text':' get the size in bytes of the meta data (header, kv pairs, tensor info) including padding','line_number':2177,'multiline':False]['text':'','line_number':2181,'multiline':False]['text':' system info','line_number':2182,'multiline':False]['text':'','line_number':2183,'multiline':False]['text':'','line_number':2205,'multiline':False]['text':' Internal types and functions exposed for tests and benchmarks','line_number':2206,'multiline':False]['text':'','line_number':2207,'multiline':False]['text':' restrict not standard in C++','line_number':2210,'multiline':False]