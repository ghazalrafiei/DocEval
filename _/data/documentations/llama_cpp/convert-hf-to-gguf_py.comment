['text':'!/usr/bin/env python3','line_number':1,'multiline':False]['text':'##### MODEL DEFINITIONS ######','line_number':26,'multiline':False]['text':' we don't need these','line_number':98,'multiline':False]['text':' convert any unsupported data types to float32','line_number':104,'multiline':False]['text':' map tensor names','line_number':110,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':119,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':123,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':127,'multiline':False]['text':' there's only one .safetensors file','line_number':192,'multiline':False]['text':' there's only one .bin file','line_number':196,'multiline':False]['text':' type: ignore[attr-defined]','line_number':233,'multiline':False]['text':' convert any unsupported data types to float32','line_number':364,'multiline':False]['text':' Map bloom-style qkv_linear to gpt-style qkv_linear','line_number':371,'multiline':False]['text':' bloom: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py#L238-L252  # noqa','line_number':372,'multiline':False]['text':' gpt-2: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L312  # noqa','line_number':373,'multiline':False]['text':' map tensor names','line_number':396,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':405,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':409,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':413,'multiline':False]['text':' we don't need these','line_number':446,'multiline':False]['text':' convert any unsupported data types to float32','line_number':452,'multiline':False]['text':' map tensor names','line_number':458,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':467,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':471,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':475,'multiline':False]['text':' note: MPT output is tied to (same as) wte in original model;','line_number':483,'multiline':False]['text':' for easier implementation in llama.cpp it's duplicated in GGUF, though :/','line_number':484,'multiline':False]['text':' Collect tensors from generator object','line_number':528,'multiline':False]['text':' we don't need these','line_number':547,'multiline':False]['text':' convert any unsupported data types to float32','line_number':553,'multiline':False]['text':' map tensor names','line_number':559,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':568,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':572,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':576,'multiline':False]['text':' old name','line_number':608,'multiline':False]['text':' old name','line_number':612,'multiline':False]['text':' old name','line_number':616,'multiline':False]['text':' not in config.json','line_number':619,'multiline':False]['text':' qkv tensor transform','line_number':620,'multiline':False]['text':' old name','line_number':632,'multiline':False]['text':' old name','line_number':636,'multiline':False]['text':' old name','line_number':640,'multiline':False]['text':' convert any unsupported data types to float32','line_number':648,'multiline':False]['text':' QKV tensor transform','line_number':652,'multiline':False]['text':' The original query_key_value tensor contains n_head_kv "kv groups",','line_number':653,'multiline':False]['text':' each consisting of n_head/n_head_kv query weights followed by one key','line_number':654,'multiline':False]['text':' and one value weight (shared by all query heads in the kv group).','line_number':655,'multiline':False]['text':' This layout makes it a big pain to work with in GGML.','line_number':656,'multiline':False]['text':' So we rearrange them here,, so that we have n_head query weights','line_number':657,'multiline':False]['text':' followed by n_head_kv key weights followed by n_head_kv value weights,','line_number':658,'multiline':False]['text':' in contiguous fashion.','line_number':659,'multiline':False]['text':' ref: https://github.com/jploski/ggml/blob/falcon40b/examples/falcon/convert-hf-to-ggml.py','line_number':660,'multiline':False]['text':' map tensor names','line_number':671,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':680,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':684,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':688,'multiline':False]['text':' refact uses Alibi. So this is from config.json which might be used by training.','line_number':723,'multiline':False]['text':' convert any unsupported data types to float32','line_number':764,'multiline':False]['text':' map tensor names','line_number':770,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':779,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':783,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':787,'multiline':False]['text':' self.gguf_writer.add_bos_token_id(71013)','line_number':816,'multiline':False]['text':' self.gguf_writer.add_eos_token_id(71013)','line_number':817,'multiline':False]['text':' TODO: FP16 conversion produces garbage outputs. (Q8_0 does not, so..?)','line_number':827,'multiline':False]['text':' type: ignore[attr-defined]','line_number':889,'multiline':False]['text':' we don't need these','line_number':947,'multiline':False]['text':' convert any unsupported data types to float32','line_number':953,'multiline':False]['text':' map tensor names','line_number':959,'multiline':False]['text':' if f32 desired, convert any float16 to float32','line_number':968,'multiline':False]['text':' TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32','line_number':972,'multiline':False]['text':' if f16 desired, convert any float32 2-dim weight tensors to float16','line_number':976,'multiline':False]['text':'##### CONVERSION LOGIC ######','line_number':983,'multiline':False]['text':' output in the same directory as the model by default','line_number':1024,'multiline':False]