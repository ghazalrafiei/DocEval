['text':'!/bin/bash','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Helper script for deploying llama.cpp server with a single Bash command','line_number':3,'multiline':False]['text':'','line_number':4,'multiline':False]['text':' - Works on Linux and macOS','line_number':5,'multiline':False]['text':' - Supports: CPU, CUDA, Metal, OpenCL','line_number':6,'multiline':False]['text':' - Can run all GGUF models from HuggingFace','line_number':7,'multiline':False]['text':' - Can serve requests in parallel','line_number':8,'multiline':False]['text':' - Always builds latest llama.cpp from GitHub','line_number':9,'multiline':False]['text':'','line_number':10,'multiline':False]['text':' Limitations','line_number':11,'multiline':False]['text':'','line_number':12,'multiline':False]['text':' - Chat templates are poorly supported (base models recommended)','line_number':13,'multiline':False]['text':' - Might be unstable!','line_number':14,'multiline':False]['text':'','line_number':15,'multiline':False]['text':' Usage:','line_number':16,'multiline':False]['text':'   ./server-llm.sh [--port] [--repo] [--wtype] [--backend] [--gpu-id] [--n-parallel] [--n-kv] [--verbose]','line_number':17,'multiline':False]['text':'','line_number':18,'multiline':False]['text':'   --port:       port number, default is 8888','line_number':19,'multiline':False]['text':'   --repo:       path to a repo containing GGUF model files','line_number':20,'multiline':False]['text':'   --wtype:      weights type (f16, q8_0, q4_0, q4_1), default is user-input','line_number':21,'multiline':False]['text':'   --backend:    cpu, cuda, metal, opencl, depends on the OS','line_number':22,'multiline':False]['text':'   --gpu-id:     gpu id, default is 0','line_number':23,'multiline':False]['text':'   --n-parallel: number of parallel requests, default is 8','line_number':24,'multiline':False]['text':'   --n-kv:       KV cache size, default is 4096','line_number':25,'multiline':False]['text':'   --verbose:    verbose output','line_number':26,'multiline':False]['text':'','line_number':27,'multiline':False]['text':' Example:','line_number':28,'multiline':False]['text':'','line_number':29,'multiline':False]['text':'   bash -c "$(curl -s https://ggml.ai/server-llm.sh)"','line_number':30,'multiline':False]['text':'','line_number':31,'multiline':False]['text':' required utils: curl, git, make','line_number':35,'multiline':False]['text':' parse arguments','line_number':49,'multiline':False]['text':' if macOS, use metal backend by default','line_number':55,'multiline':False]['text':' -gt 0 ]]; do','line_number':82,'multiline':False]['text':' available weights types','line_number':136,'multiline':False]['text':' sample repos','line_number':144,'multiline':False]['text':' ask for repo until index of sample repo is provided or an URL','line_number':193,'multiline':False]['text':' check if the input is a number','line_number':198,'multiline':False]['text':'repos[@]} ]]; then','line_number':200,'multiline':False]['text':' remove suffix','line_number':215,'multiline':False]['text':' find GGUF files in the source','line_number':220,'multiline':False]['text':' TODO: better logic','line_number':221,'multiline':False]['text':' list all files in the provided git repo','line_number':225,'multiline':False]['text':' determine iw by grepping the filename with wtypes','line_number':228,'multiline':False]['text':' uppercase','line_number':232,'multiline':False]['text':' ask for weights type until provided and available','line_number':255,'multiline':False]['text':' check file if the model has been downloaded before','line_number':271,'multiline':False]['text':' check if we should download the file','line_number':274,'multiline':False]['text':' - if $wfile does not exist','line_number':275,'multiline':False]['text':' - if $wfile exists but $chk does not exist','line_number':276,'multiline':False]['text':' - if $wfile exists and $chk exists but $wfile is newer than $chk','line_number':277,'multiline':False]['text':' TODO: better logic using git lfs info','line_number':278,'multiline':False]['text':' download the weights file','line_number':293,'multiline':False]['text':' -L "$url"','line_number':294,'multiline':False]['text':' create a check file if successful','line_number':296,'multiline':False]['text':' get latest llama.cpp and build','line_number':305,'multiline':False]['text':' if the dir exists and there isn't a file "__ggml_script__" in it, abort','line_number':312,'multiline':False]['text':' mark that that the directory is made by this script','line_number':332,'multiline':False]['text':' build','line_number':339,'multiline':False]['text':' run the server','line_number':366,'multiline':False]