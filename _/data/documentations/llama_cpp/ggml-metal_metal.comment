['text':' delta','line_number':12,'multiline':False]['text':' nibbles / quants','line_number':13,'multiline':False]['text':' delta','line_number':18,'multiline':False]['text':' min','line_number':19,'multiline':False]['text':' nibbles / quants','line_number':20,'multiline':False]['text':' delta','line_number':25,'multiline':False]['text':' 5-th bit of quants','line_number':26,'multiline':False]['text':' nibbles / quants','line_number':27,'multiline':False]['text':' delta','line_number':32,'multiline':False]['text':' min','line_number':33,'multiline':False]['text':' 5-th bit of quants','line_number':34,'multiline':False]['text':' nibbles / quants','line_number':35,'multiline':False]['text':' delta','line_number':40,'multiline':False]['text':' quants','line_number':41,'multiline':False]['text':' assuming SIMD group size is 32','line_number':44,'multiline':False]['text':' general-purpose kernel for addition, multiplication and division of two tensors','line_number':51,'multiline':False]['text':' pros: works for non-contiguous tensors, supports broadcast across all dims','line_number':52,'multiline':False]['text':' cons: not very efficient','line_number':53,'multiline':False]['text':' assumption: src1 is a row','line_number':202,'multiline':False]['text':' broadcast src1 into src0','line_number':203,'multiline':False]['text':' BEWARE !!!','line_number':272,'multiline':False]['text':' Simply using "tanh" instead of "precise::tanh" will sometimes results in NaNs!','line_number':273,'multiline':False]['text':' This was observed with Falcon 7B and 40B models','line_number':274,'multiline':False]['text':'','line_number':275,'multiline':False]['text':' parallel max','line_number':373,'multiline':False]['text':' find the max value in the block','line_number':380,'multiline':False]['text':' parallel sum','line_number':399,'multiline':False]['text':' This barrier fixes a failing test','line_number':407,'multiline':False]['text':' ref: https://github.com/ggerganov/ggml/pull/621#discussion_r1425156335','line_number':408,'multiline':False]['text':' parallel max','line_number':459,'multiline':False]['text':' parallel sum','line_number':486,'multiline':False]['text':' This barrier fixes a failing test','line_number':496,'multiline':False]['text':' ref: https://github.com/ggerganov/ggml/pull/621#discussion_r1425156335','line_number':497,'multiline':False]['text':' MEAN','line_number':582,'multiline':False]['text':' parallel sum','line_number':583,'multiline':False]['text':' reduce','line_number':588,'multiline':False]['text':' recenter and VARIANCE','line_number':598,'multiline':False]['text':' reduce','line_number':607,'multiline':False]['text':' parallel sum','line_number':640,'multiline':False]['text':' partial sum for thread in warp','line_number':701,'multiline':False]['text':' function for calculate inner product between half a q4_0 block and 16 floats (yl), sumy is SUM(yl[i])','line_number':760,'multiline':False]['text':' il indicates where the q4 quants begin (0 or QK4_0/4)','line_number':761,'multiline':False]['text':' we assume that the yl's have been multiplied with the appropriate scale factor','line_number':762,'multiline':False]['text':' that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)','line_number':763,'multiline':False]['text':' function for calculate inner product between half a q4_1 block and 16 floats (yl), sumy is SUM(yl[i])','line_number':780,'multiline':False]['text':' il indicates where the q4 quants begin (0 or QK4_0/4)','line_number':781,'multiline':False]['text':' we assume that the yl's have been multiplied with the appropriate scale factor','line_number':782,'multiline':False]['text':' that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)','line_number':783,'multiline':False]['text':' function for calculate inner product between half a q5_0 block and 16 floats (yl), sumy is SUM(yl[i])','line_number':801,'multiline':False]['text':' il indicates where the q5 quants begin (0 or QK5_0/4)','line_number':802,'multiline':False]['text':' we assume that the yl's have been multiplied with the appropriate scale factor','line_number':803,'multiline':False]['text':' that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)','line_number':804,'multiline':False]['text':' function for calculate inner product between half a q5_1 block and 16 floats (yl), sumy is SUM(yl[i])','line_number':822,'multiline':False]['text':' il indicates where the q5 quants begin (0 or QK5_1/4)','line_number':823,'multiline':False]['text':' we assume that the yl's have been multiplied with the appropriate scale factor','line_number':824,'multiline':False]['text':' that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)','line_number':825,'multiline':False]['text':' putting them in the kernel cause a significant performance penalty','line_number':844,'multiline':False]['text':' each SIMD group works on 4 rows','line_number':845,'multiline':False]['text':' number of SIMD groups in a thread group','line_number':846,'multiline':False]['text':'Note: This is a template, but strictly speaking it only applies to','line_number':847,'multiline':False]['text':'      quantizations where the block size is 32. It also does not','line_number':848,'multiline':False]['text':'      giard against the number of rows not being divisible by','line_number':849,'multiline':False]['text':'      N_DST, so this is another explicit assumption of the implementation.','line_number':850,'multiline':False]['text':' src1 vector cache','line_number':882,'multiline':False]['text':' each thread in a SIMD group deals with half a block.','line_number':890,'multiline':False]['text':' each thread in a SIMD group deals with NB_Q8_0 quants at a time','line_number':1040,'multiline':False]['text':' Assumes row size (ne00) is a multiple of 4','line_number':1462,'multiline':False]['text':' YaRN algorithm based on LlamaYaRNScaledRotaryEmbedding.py from https://github.com/jquesnelle/yarn','line_number':1570,'multiline':False]['text':' MIT licensed. Copyright (c) 2023 Jeffrey Quesnelle and Bowen Peng.','line_number':1571,'multiline':False]['text':' Get n-d rotational scaling corrected for extrapolation','line_number':1576,'multiline':False]['text':' Get n-d magnitude scaling corrected for interpolation','line_number':1583,'multiline':False]['text':' Apparently solving `n_rot = 2pi * x * base^((2 * max_pos_emb) / n_dims)` for x, we get','line_number':1590,'multiline':False]['text':' `corr_fac(n_rot) = n_dims * log(max_pos_emb / (n_rot * 2pi)) / (2 * log(base))`','line_number':1591,'multiline':False]['text':' start and end correction dims','line_number':1599,'multiline':False]['text':' simplified from `(ib * n_dims + ic) * inv_ndims`','line_number':1708,'multiline':False]['text':' bitonic sort implementation following the CUDA kernels as reference','line_number':1857,'multiline':False]['text':' bitonic sort','line_number':1872,'multiline':False]['text':' initialize indices','line_number':1881,'multiline':False]['text':' absolute max','line_number':2121,'multiline':False]['text':' absolute max','line_number':2179,'multiline':False]['text':'============================================ k-quants ======================================================','line_number':2330,'multiline':False]['text':' scales and mins, quantized with 4 bits','line_number':2345,'multiline':False]['text':' quants','line_number':2346,'multiline':False]['text':' super-block scale for quantized scales','line_number':2347,'multiline':False]['text':' super-block scale for quantized mins','line_number':2348,'multiline':False]['text':' 84 bytes / block','line_number':2350,'multiline':False]['text':' quants - high bit','line_number':2353,'multiline':False]['text':' quants - low 2 bits','line_number':2354,'multiline':False]['text':' scales, quantized with 6 bits','line_number':2358,'multiline':False]['text':' super-block scale','line_number':2360,'multiline':False]['text':' super-block scales/mins','line_number':2365,'multiline':False]['text':' 4-bit quants','line_number':2367,'multiline':False]['text':' super-block scale for quantized scales','line_number':2371,'multiline':False]['text':' super-block scale for quantized mins','line_number':2372,'multiline':False]['text':' scales and mins, quantized with 6 bits','line_number':2373,'multiline':False]['text':' 4--bit quants','line_number':2374,'multiline':False]['text':' super-block scales/mins','line_number':2380,'multiline':False]['text':' 8-bit block scales','line_number':2381,'multiline':False]['text':' quants, high bit','line_number':2382,'multiline':False]['text':' quants, low 4 bits','line_number':2383,'multiline':False]['text':' super-block scale for quantized scales','line_number':2387,'multiline':False]['text':' super-block scale for quantized mins','line_number':2388,'multiline':False]['text':' scales and mins, quantized with 6 bits','line_number':2389,'multiline':False]['text':' quants, high bit','line_number':2390,'multiline':False]['text':' quants, low 4 bits','line_number':2391,'multiline':False]['text':' 176 bytes / block','line_number':2393,'multiline':False]['text':' quants, lower 4 bits','line_number':2397,'multiline':False]['text':' quants, upper 2 bits','line_number':2398,'multiline':False]['text':' scales, quantized with 8 bits','line_number':2399,'multiline':False]['text':' super-block scale','line_number':2400,'multiline':False]['text':' 210 bytes / block','line_number':2402,'multiline':False]['text':'====================================== dot products =========================','line_number':2420,'multiline':False]['text':' 0...3','line_number':2461,'multiline':False]['text':' 0...7','line_number':2462,'multiline':False]['text':' 0 or 1','line_number':2463,'multiline':False]['text':' 0...3','line_number':2464,'multiline':False]['text':' 0 or 1','line_number':2465,'multiline':False]['text':' 0...15','line_number':2513,'multiline':False]['text':' 0...1','line_number':2514,'multiline':False]['text':'const uint16_t kmask1 = 0x3030;','line_number':2629,'multiline':False]['text':'const uint16_t kmask2 = 0x0f0f;','line_number':2630,'multiline':False]['text':' 0 or 1','line_number':2634,'multiline':False]['text':' 0 or 2','line_number':2635,'multiline':False]['text':' One would think that the Metal compiler would figure out that ip and il can only have','line_number':2640,'multiline':False]['text':' 4 possible states, and optimize accordingly. Well, no. It needs help, and we do it','line_number':2641,'multiline':False]['text':' with these two tales.','line_number':2642,'multiline':False]['text':'','line_number':2643,'multiline':False]['text':' Possible masks for the high bit','line_number':2644,'multiline':False]['text':' ip = 0, il = 0','line_number':2645,'multiline':False]['text':' ip = 0, il = 2','line_number':2646,'multiline':False]['text':' ip = 1, il = 0','line_number':2647,'multiline':False]['text':' ip = 1, il = 2','line_number':2648,'multiline':False]['text':' Possible masks for the low 2 bits','line_number':2650,'multiline':False]['text':' 0, 4, 8, 12','line_number':2786,'multiline':False]['text':' 0, 0, 1, 1','line_number':2787,'multiline':False]['text':' 0, 4, 0, 4','line_number':2788,'multiline':False]['text':' 0...3','line_number':2872,'multiline':False]['text':' 0...7','line_number':2873,'multiline':False]['text':' 0 or 1','line_number':2874,'multiline':False]['text':' 0...3','line_number':2875,'multiline':False]['text':'const int first_row = (r0 * N_SIMDGROUP + sgitg) * N_DST;','line_number':2881,'multiline':False]['text':' 0...7','line_number':2981,'multiline':False]['text':' 0...3','line_number':2982,'multiline':False]['text':' 0, 4, 8, 12','line_number':3205,'multiline':False]['text':' 0, 0, 1, 1','line_number':3207,'multiline':False]['text':' 0, 4, 0, 4','line_number':3208,'multiline':False]['text':' 0 or 1','line_number':3323,'multiline':False]['text':'============================= templates and their specializations =============================','line_number':3407,'multiline':False]['text':' NOTE: this is not dequantizing - we are simply fitting the template','line_number':3409,'multiline':False]['text':' extract the 5-th bits for x0 and x1','line_number':3471,'multiline':False]['text':' combine the 4-bits from qs with the 5th bit','line_number':3475,'multiline':False]['text':' extract the 5-th bits for x0 and x1','line_number':3499,'multiline':False]['text':' combine the 4-bits from qs with the 5th bit','line_number':3503,'multiline':False]['text':'const int64_t i = tgpig;','line_number':3694,'multiline':False]['text':'const int64_t r = ((device int32_t *) src1)[i];','line_number':3695,'multiline':False]['text':' 8 simdgroup matrices from matrix A','line_number':3768,'multiline':False]['text':' 4 simdgroup matrices from matrix B','line_number':3769,'multiline':False]['text':' each thread take 4 simdgroup matrices from matrix A','line_number':3771,'multiline':False]['text':' each thread take 2 simdgroup matrices from matrix B','line_number':3772,'multiline':False]['text':' 2 thread for each row in matrix A to load numbers','line_number':3774,'multiline':False]['text':' 4 thread for each row in matrix B to load numbers','line_number':3775,'multiline':False]['text':' simdgroup matrix is of shape 8x8','line_number':3776,'multiline':False]['text':' each block_q contains 16*nl weights','line_number':3779,'multiline':False]['text':' if this block is of 64x32 shape or smaller','line_number':3808,'multiline':False]['text':' a thread shouldn't load data outside of the matrix','line_number':3812,'multiline':False]['text':' load data and store to threadgroup memory','line_number':3838,'multiline':False]['text':' load matrices from threadgroup memory and conduct outer products','line_number':3858,'multiline':False]['text':' block is smaller than 64x32, we should avoid writing data outside of the matrix','line_number':3891,'multiline':False]['text':'','line_number':4023,'multiline':False]['text':' get rows','line_number':4024,'multiline':False]['text':'','line_number':4025,'multiline':False]['text':'template [[host_name("kernel_get_rows_f32")]]  kernel get_rows_t kernel_get_rows<float4x4,   1, dequantize_f32>;','line_number':4041,'multiline':False]['text':'template [[host_name("kernel_get_rows_f16")]]  kernel get_rows_t kernel_get_rows<half4x4,    1, dequantize_f16>;','line_number':4042,'multiline':False]['text':'','line_number':4054,'multiline':False]['text':' matrix-matrix multiplication','line_number':4055,'multiline':False]['text':'','line_number':4056,'multiline':False]['text':'','line_number':4090,'multiline':False]['text':' indirect matrix-matrix multiplication','line_number':4091,'multiline':False]['text':'','line_number':4092,'multiline':False]['text':'','line_number':4138,'multiline':False]['text':' matrix-vector multiplication','line_number':4139,'multiline':False]['text':'','line_number':4140,'multiline':False]