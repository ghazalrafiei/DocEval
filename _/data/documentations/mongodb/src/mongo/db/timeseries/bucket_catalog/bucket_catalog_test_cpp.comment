['text':'*
 *    Copyright (C) 2020-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':' Check that each group of objects has compatible schema with itself, but that inserting the','line_number':114,'multiline':False]['text':' first object in new group closes the existing bucket and opens a new one','line_number':115,'multiline':False]['text':' Start background job.','line_number':190,'multiline':False]['text':' Once we hit the failpoint once, turn it off.','line_number':193,'multiline':False]['text':' Make sure we start and end with a clean slate.','line_number':259,'multiline':False]['text':' We don't expect to close a bucket if we are on the first group.','line_number':284,'multiline':False]['text':' Otherwise we expect that we are in fact closing a bucket because we have','line_number':288,'multiline':False]['text':' an incompatible schema change.','line_number':289,'multiline':False]['text':' Should have compatible schema, no expected bucket closure.','line_number':294,'multiline':False]['text':' Validate the bucket document against the schema.','line_number':315,'multiline':False]['text':' Register the reopened bucket with the catalog.','line_number':338,'multiline':False]['text':' The first insert should be able to take commit rights','line_number':357,'multiline':False]['text':' A subsequent insert into the same bucket should land in the same batch, but not be able to','line_number':368,'multiline':False]['text':' claim commit rights','line_number':369,'multiline':False]['text':' The batch hasn't actually been committed yet.','line_number':381,'multiline':False]['text':' Still not finished.','line_number':386,'multiline':False]['text':' The batch should contain both documents since they belong in the same bucket and happened','line_number':389,'multiline':False]['text':' in the same commit epoch. Nothing else has been committed in this bucket yet.','line_number':390,'multiline':False]['text':' Once the commit has occurred, the waiter should be notified.','line_number':394,'multiline':False]['text':' Inserts should all be into three distinct buckets (and therefore batches).','line_number':439,'multiline':False]['text':' Check metadata in buckets.','line_number':447,'multiline':False]['text':' Committing one bucket should only return the one document in that bucket and should not','line_number':460,'multiline':False]['text':' affect the other bucket.','line_number':461,'multiline':False]['text':'numberOfStripes=','line_number':470,'multiline':True]['text':' Inserts should be into different buckets (and therefore batches) because they went through','line_number':489,'multiline':False]['text':' different bucket catalogs.','line_number':490,'multiline':False]['text':' Committing one bucket should only return the one document in that bucket and should not','line_number':493,'multiline':False]['text':' affect the other bucket.','line_number':494,'multiline':False]['text':' Check metadata in buckets.','line_number':529,'multiline':False]['text':' Check metadata in buckets.','line_number':565,'multiline':False]['text':' Check metadata in buckets.','line_number':606,'multiline':False]['text':' Inserts should all be into three distinct buckets (and therefore batches).','line_number':637,'multiline':False]['text':' Check metadata in buckets.','line_number':641,'multiline':False]['text':' Committing one bucket should only return the one document in that bucket and should not','line_number':650,'multiline':False]['text':' affect the other bucket.','line_number':651,'multiline':False]['text':' The numCommittedMeasurements returned when committing should accumulate as more entries in','line_number':659,'multiline':False]['text':' the bucket are committed.','line_number':660,'multiline':False]['text':' Clear the buckets for the database of tenant1.','line_number':691,'multiline':False]['text':' Clear the buckets for the database of tenant2.','line_number':696,'multiline':False]['text':' Insert before finish so there's a second batch live at the same time.','line_number':715,'multiline':False]['text':' Verify the second batch still commits one doc, and that the first batch only commited one.','line_number':729,'multiline':False]['text':' BucketCatalog::prepareCommit uses dassert, so it will only invariant in debug mode. Ensure we','line_number':744,'multiline':False]['text':' die here in non-debug mode as well.','line_number':745,'multiline':False]['text':' Creating a new bucket should return all fields from the initial measurement.','line_number':765,'multiline':False]['text':' Inserting a new measurement with the same fields should return an empty set of new fields.','line_number':781,'multiline':False]['text':' Insert a new measurement with the a new field.','line_number':795,'multiline':False]['text':' Fill up the bucket.','line_number':809,'multiline':False]['text':' When a bucket overflows, committing to the new overflow bucket should return the fields of','line_number':824,'multiline':False]['text':' the first measurement as new fields.','line_number':825,'multiline':False]['text':' Insert before finish so there's a second batch live at the same time.','line_number':855,'multiline':False]['text':' Even though bucket has been cleared, finish should still report success. Basically, in this','line_number':908,'multiline':False]['text':' case we know that the write succeeded, so it must have happened before the namespace drop','line_number':909,'multiline':False]['text':' operation got the collection lock. So the write did actually happen, but is has since been','line_number':910,'multiline':False]['text':' removed, and that's fine for our purposes. The finish just records the result to the batch','line_number':911,'multiline':False]['text':' and updates some statistics.','line_number':912,'multiline':False]['text':' Insert before clear so there's a second batch live at the same time.','line_number':956,'multiline':False]['text':' Now clear the bucket. Since there's a prepared batch it should conflict.','line_number':968,'multiline':False]['text':' Now try to prepare the second batch. Ensure it aborts the batch.','line_number':971,'multiline':False]['text':' Make sure we didn't clear the bucket state when we aborted the second batch.','line_number':977,'multiline':False]['text':' Make sure a subsequent insert, which opens a new bucket, doesn't corrupt the old bucket','line_number':980,'multiline':False]['text':' state and prevent us from finishing the first batch.','line_number':981,'multiline':False]['text':' Clean up this batch','line_number':993,'multiline':False]['text':' Make sure we can finish the cleanly prepared batch.','line_number':997,'multiline':False]['text':' Batch 2 will not be able to commit until batch 1 has finished.','line_number':1092,'multiline':False]['text':' Finish the first batch.','line_number':1101,'multiline':False]['text':' Batch 2 will not be able to commit until batch 1 has finished.','line_number':1145,'multiline':False]['text':' If we abort the third batch, it should abort the second one too, as it isn't prepared.','line_number':1154,'multiline':False]['text':' However, since the first batch is prepared, we can't abort it or clean up the bucket. We','line_number':1155,'multiline':False]['text':' can then finish the first batch, which will allow the second batch to proceed. It should','line_number':1156,'multiline':False]['text':' recognize it has been aborted and clean up the bucket.','line_number':1157,'multiline':False]['text':' Wait for the batch 2 task to finish preparing commit. Since batch 1 finished, batch 2 should','line_number':1162,'multiline':False]['text':' be unblocked. Note that after aborting batch 3, batch 2 was not in a prepared state, so we','line_number':1163,'multiline':False]['text':' expect the prepareCommit() call to fail.','line_number':1164,'multiline':False]['text':' Make sure a new batch ends up in a new bucket.','line_number':1168,'multiline':False]['text':' Batch 1 and 2 use the same bucket.','line_number':1199,'multiline':False]['text':' Batch 1 will be in a prepared state now. Abort the second batch so that bucket 1 will be','line_number':1205,'multiline':False]['text':' closed after batch 1 finishes.','line_number':1206,'multiline':False]['text':' Ensure a batch started after batch 2 aborts, does not insert future measurements into the','line_number':1212,'multiline':False]['text':' aborted batch/bucket.','line_number':1213,'multiline':False]['text':' Batch 2 is the first batch to commit the time field.','line_number':1244,'multiline':False]['text':' Batch 1 was the first batch to insert the time field, but by commit time it was already','line_number':1251,'multiline':False]['text':' committed by batch 2.','line_number':1252,'multiline':False]['text':' 0','line_number':1261,'multiline':False]['text':' 1','line_number':1262,'multiline':False]['text':' 2','line_number':1263,'multiline':False]['text':' 3','line_number':1264,'multiline':False]['text':' 4','line_number':1265,'multiline':False]['text':' 5','line_number':1266,'multiline':False]['text':' 6','line_number':1267,'multiline':False]['text':' 7','line_number':1268,'multiline':False]['text':' 8','line_number':1269,'multiline':False]['text':' 9','line_number':1270,'multiline':False]['text':' 10','line_number':1271,'multiline':False]['text':' 11','line_number':1272,'multiline':False]['text':' 12','line_number':1273,'multiline':False]['text':' 13','line_number':1274,'multiline':False]['text':' 14','line_number':1275,'multiline':False]['text':' 15','line_number':1276,'multiline':False]['text':' 16','line_number':1277,'multiline':False]['text':' 17','line_number':1278,'multiline':False]['text':' 18','line_number':1279,'multiline':False]['text':' 19','line_number':1280,'multiline':False]['text':' 20','line_number':1281,'multiline':False]['text':' 21','line_number':1282,'multiline':False]['text':' Missing _id field.','line_number':1308,'multiline':False]['text':' Bad _id type.','line_number':1312,'multiline':False]['text':' Missing control field.','line_number':1318,'multiline':False]['text':' Bad control type.','line_number':1322,'multiline':False]['text':' Bad control.version type.','line_number':1326,'multiline':False]['text':' Bad control.min type.','line_number':1336,'multiline':False]['text':' Bad control.max type.','line_number':1343,'multiline':False]['text':' Missing control.min.time.','line_number':1351,'multiline':False]['text':' Missing control.max.time.','line_number':1358,'multiline':False]['text':' Missing data field.','line_number':1369,'multiline':False]['text':' Bad data type.','line_number':1373,'multiline':False]['text':' control.closed: true','line_number':1409,'multiline':False]['text':' control.closed: false','line_number':1424,'multiline':False]['text':' No control.closed','line_number':1439,'multiline':False]['text':' Bucket document to reopen.','line_number':1454,'multiline':False]['text':' Insert a measurement that is compatible with the reopened bucket.','line_number':1473,'multiline':False]['text':' No buckets are closed.','line_number':1483,'multiline':False]['text':' The reopened bucket already contains three committed measurements.','line_number':1492,'multiline':False]['text':' Verify that the min and max is updated correctly when inserting new measurements.','line_number':1495,'multiline':False]['text':' Bucket document to reopen.','line_number':1505,'multiline':False]['text':' Insert a measurement that is compatible with the reopened bucket.','line_number':1521,'multiline':False]['text':' No buckets are closed.','line_number':1532,'multiline':False]['text':' The reopened bucket already contains three committed measurements.','line_number':1541,'multiline':False]['text':' Verify that the min and max is updated correctly when inserting new measurements.','line_number':1544,'multiline':False]['text':' Bucket document to reopen.','line_number':1554,'multiline':False]['text':' Insert a measurement that is incompatible with the reopened bucket.','line_number':1573,'multiline':False]['text':' The reopened bucket gets closed as the schema is incompatible.','line_number':1583,'multiline':False]['text':' Since the reopened bucket was incompatible, we opened a new one.','line_number':1592,'multiline':False]['text':' Bucket document to reopen.','line_number':1599,'multiline':False]['text':'validateDecompression','line_number':1613,'multiline':True]['text':' Insert a measurement that is compatible with the reopened bucket.','line_number':1624,'multiline':False]['text':' No buckets are closed.','line_number':1634,'multiline':False]['text':' The reopened bucket already contains three committed measurements.','line_number':1643,'multiline':False]['text':' Verify that the min and max is updated correctly when inserting new measurements.','line_number':1646,'multiline':False]['text':' Bucket document to reopen.','line_number':1656,'multiline':False]['text':'validateDecompression','line_number':1670,'multiline':True]['text':' Insert a measurement that is incompatible with the reopened bucket.','line_number':1681,'multiline':False]['text':' The reopened bucket gets closed as the schema is incompatible.','line_number':1691,'multiline':False]['text':' Since the reopened bucket was incompatible, we opened a new one.','line_number':1700,'multiline':False]['text':' Insert a measurement with a unique meta value, guaranteeing we will open a new bucket but not','line_number':1710,'multiline':False]['text':' close an old one except under memory pressure.','line_number':1711,'multiline':False]['text':' Ensure we start out with no buckets archived or closed due to memory pressure.','line_number':1730,'multiline':False]['text':' With a memory limit of 10000 bytes, we should be guaranteed to hit the memory limit with no','line_number':1734,'multiline':False]['text':' more than 1000 buckets since an open bucket takes up at least 10 bytes (in reality,','line_number':1735,'multiline':False]['text':' significantly more, but this is definitely a safe assumption).','line_number':1736,'multiline':False]['text':' When we first hit the limit, we should try to archive some buckets prior to closing anything.','line_number':1745,'multiline':False]['text':' However, depending on how the buckets are distributed over the stripes, it's possible that','line_number':1746,'multiline':False]['text':' the current stripe will not have enough open buckets to archive to drop below the limit, and','line_number':1747,'multiline':False]['text':' may immediately close a bucket it has just archived. We should be able to guarantee that we','line_number':1748,'multiline':False]['text':' have archived a bucket prior to closing it though.','line_number':1749,'multiline':False]['text':' If we continue to open more new buckets with distinct meta values, eventually we'll run out','line_number':1754,'multiline':False]['text':' of open buckets to archive and have to start closing archived buckets to relieve memory','line_number':1755,'multiline':False]['text':' pressure. Again, an archived bucket should take up more than 10 bytes in the catalog, so we','line_number':1756,'multiline':False]['text':' should be fine with a maximum of 1000 iterations.','line_number':1757,'multiline':False]['text':' We should have closed some (additional) buckets by now.','line_number':1767,'multiline':False]['text':' An absurdly low limit that only allows us one open bucket at a time.','line_number':1774,'multiline':False]['text':' Try to insert with no open bucket. Should hint to re-open.','line_number':1785,'multiline':False]['text':' Actually insert so we do have an open bucket to test against.','line_number':1801,'multiline':False]['text':' placeholder initialization, will be set properlybelow','line_number':1802,'multiline':False]['text':' Time backwards should hint to re-open.','line_number':1821,'multiline':False]['text':' Time forward should not hint to re-open.','line_number':1837,'multiline':False]['text':' Now let's insert something with a different meta, so we open a new bucket, see we're past the','line_number':1853,'multiline':False]['text':' memory limit, and archive the existing bucket.','line_number':1854,'multiline':False]['text':' If we try to insert something that could fit in the archived bucket, we should get it back as','line_number':1876,'multiline':False]['text':' a candidate.','line_number':1877,'multiline':False]['text':' Insert a document so we have a base bucket','line_number':1897,'multiline':False]['text':' Incompatible schema would close the existing bucket, so we should expect to open a new bucket','line_number':1915,'multiline':False]['text':' and proceed to insert the document.','line_number':1916,'multiline':False]['text':' Insert a document so we have a base bucket and we can test that we soft close it when we','line_number':1938,'multiline':False]['text':' reopen a conflicting bucket.','line_number':1939,'multiline':False]['text':' We should be able to pass in a valid bucket and insert into it.','line_number':1974,'multiline':False]['text':' Verify the old bucket was soft-closed','line_number':1992,'multiline':False]['text':' Verify that if we try another insert for the soft-closed bucket, we get a query-based','line_number':1997,'multiline':False]['text':' reopening candidate.','line_number':1998,'multiline':False]['text':' Insert a document so we have a base bucket and we can test that we archive it when we reopen','line_number':2015,'multiline':False]['text':' a conflicting bucket.','line_number':2016,'multiline':False]['text':' If we advance the catalog era, then we shouldn't use a bucket that was fetched during a','line_number':2043,'multiline':False]['text':' previous era.','line_number':2044,'multiline':False]['text':' We should get an WriteConflict back if we pass in an outdated bucket.','line_number':2059,'multiline':False]['text':' First attempt to insert to a series should trigger a reopening request to check for a bucket','line_number':2075,'multiline':False]['text':' on disk.','line_number':2076,'multiline':False]['text':' A subsequent attempt while the first one is still outstanding should conflict and yield a','line_number':2090,'multiline':False]['text':' InsertWaiter.','line_number':2091,'multiline':False]['text':' Stage and prepare an insert.','line_number':2107,'multiline':False]['text':' Stage and abort another insert on the same bucket, so that new inserts can't land without','line_number':2123,'multiline':False]['text':' reopening.','line_number':2124,'multiline':False]['text':' A subsequent attempt to reopen a bucket should conflict and yield a InsertWaiter.','line_number':2139,'multiline':False]['text':' First attempt to insert to a series should trigger a reopening request to check for a bucket','line_number':2155,'multiline':False]['text':' on disk.','line_number':2156,'multiline':False]['text':' Stage an insert for the same series, but a different bucket.','line_number':2170,'multiline':False]['text':' Ensure it blocks until we resolve the reopening request.','line_number':2184,'multiline':False]['text':' Simplify test by restricting to a single stripe.','line_number':2193,'multiline':False]['text':' Inject an archived record.','line_number':2205,'multiline':False]['text':' Should try to reopen archived bucket.','line_number':2215,'multiline':False]['text':' A second attempt should block.','line_number':2231,'multiline':False]['text':' Simplify test by restricting to a single stripe.','line_number':2246,'multiline':False]['text':' Inject an archived record.','line_number':2258,'multiline':False]['text':' Should try to reopen archived bucket.','line_number':2268,'multiline':False]['text':' Inject another archived record on the same series, but a different bucket.','line_number':2284,'multiline':False]['text':' A second attempt should block.','line_number':2292,'multiline':False]['text':' Initialize the side bucket catalog.','line_number':2310,'multiline':False]['text':' Create dummy bucket and populate bucket state registry.','line_number':2315,'multiline':False]['text':' Create and populate stripe.','line_number':2323,'multiline':False]['text':' Create execution stats controller.','line_number':2336,'multiline':False]['text':' Ensure we start out with no buckets archived or closed due to memory pressure.','line_number':2341,'multiline':False]['text':' Set the catalog memory usage to be above the memory usage threshold by the amount of memory','line_number':2350,'multiline':False]['text':' used by the idle bucket.','line_number':2351,'multiline':False]['text':' When we exceed the memory usage threshold we will first try to archive idle buckets to try','line_number':2355,'multiline':False]['text':' to get below the threshold. If this does not get us beneath the threshold, we will then try','line_number':2356,'multiline':False]['text':' to close archived buckets, until we hit the global expiry max count limit (or if we run out','line_number':2357,'multiline':False]['text':' of idle buckets in this stripe). Then, we try to close any archived buckets. In this','line_number':2358,'multiline':False]['text':' particular execution we should expect not to close any buckets, but we should archive one.','line_number':2359,'multiline':False]['text':' Clears the list of idle buckets - usually this is done within the expire idle buckets','line_number':2374,'multiline':False]['text':' function, but that requires setting up more state with the bucket's idleListEntry. This gets','line_number':2375,'multiline':False]['text':' around that for testing purposes.','line_number':2376,'multiline':False]['text':' Set the memory usage to be back at the threshold. Now, when we run expire idle buckets again,','line_number':2379,'multiline':False]['text':' because there are no idle buckets left to archive, we will close the bucket that we','line_number':2380,'multiline':False]['text':' previously archived.','line_number':2381,'multiline':False]['text':'storageCacheSize=','line_number':2401,'multiline':True]['text':'workloadCardinality=','line_number':2401,'multiline':True]['text':'storageCacheSize=','line_number':2406,'multiline':True]['text':'workloadCardinality=','line_number':2406,'multiline':True]['text':'storageCacheSize=','line_number':2411,'multiline':True]['text':'workloadCardinality=','line_number':2411,'multiline':True]['text':'storageCacheSize=','line_number':2418,'multiline':True]['text':'workloadCardinality=','line_number':2418,'multiline':True]['text':'storageCacheSize=','line_number':2425,'multiline':True]['text':'workloadCardinality=','line_number':2425,'multiline':True]['text':' namespace','line_number':2430,'multiline':False]['text':' namespace mongo::timeseries::bucket_catalog','line_number':2431,'multiline':False]