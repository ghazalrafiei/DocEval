['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':' namespace','line_number':124,'multiline':False]['text':' Avoid divide by zero error in random number generator.','line_number':135,'multiline':False]['text':' Make a handle to a valid no-op.','line_number':200,'multiline':False]['text':' Simulate preparing a heartbeat request so that the target's ping stats are initialized.','line_number':210,'multiline':False]['text':' Pretend we sent a request so that _untrackHeartbeatHandle_inlock succeeds.','line_number':214,'multiline':False]['text':' remove handle from queued heartbeats','line_number':231,'multiline':False]['text':' Parse and validate the response.  At the end of this step, if responseStatus is OK then','line_number':234,'multiline':False]['text':' hbResponse is valid.','line_number':235,'multiline':False]['text':' Ignore metadata.','line_number':270,'multiline':False]['text':' Reject heartbeat responses (and metadata) from nodes with mismatched replica set IDs.','line_number':274,'multiline':False]['text':' It is problematic to perform this check in the heartbeat reconfiguring logic because it','line_number':275,'multiline':False]['text':' is possible for two mismatched replica sets to have the same replica set name and','line_number':276,'multiline':False]['text':' configuration version. A heartbeat reconfiguration would not take place in that case.','line_number':277,'multiline':False]['text':' Additionally, this is where we would stop further processing of the metadata from an','line_number':278,'multiline':False]['text':' unknown replica set.','line_number':279,'multiline':False]['text':' Ignore metadata.','line_number':288,'multiline':False]['text':' It is safe to update our commit point via heartbeat propagation as long as the','line_number':292,'multiline':False]['text':' the new commit point we learned of is on the same branch of history as our own','line_number':293,'multiline':False]['text':' oplog.','line_number':294,'multiline':False]['text':' The node that sent the heartbeat is not guaranteed to be our sync source.','line_number':298,'multiline':False]['text':' Asynchronous stepdown could happen, but it will wait for _mutex and execute','line_number':304,'multiline':False]['text':' after this function, so we cannot and don't need to wait for it to finish.','line_number':305,'multiline':False]['text':' Arbiters are always expected to report null durable optimes (and wall times).','line_number':309,'multiline':False]['text':' If that is not the case here, make sure to correct these times before ingesting them.','line_number':310,'multiline':False]['text':' TODO(sz) Because the term is duplicated in ReplSetMetaData, we can get rid of this','line_number':334,'multiline':False]['text':' and update tests.','line_number':335,'multiline':False]['text':' Postpone election timeout if we have a successful heartbeat response from the primary.','line_number':338,'multiline':False]['text':' Leaving networkTime units as ms since the average ping calulation may be affected.','line_number':356,'multiline':False]['text':' If a member's opTime has moved forward or config is newer, try to update the','line_number':364,'multiline':False]['text':' lastCommitted. Even if we've only updated the config, this is still safe.','line_number':365,'multiline':False]['text':' Wake up replication waiters on optime changes or updated configs.','line_number':367,'multiline':False]['text':' Try to wake up the stepDown waiter when a new node becomes electable.','line_number':370,'multiline':False]['text':' When receiving a heartbeat response indicating that the remote is in a state past','line_number':375,'multiline':False]['text':' STARTUP_2, the primary will initiate a reconfig to remove the 'newlyAdded' field for that','line_number':376,'multiline':False]['text':' node (if present). This field is normally set when we add new members with votes:1 to the','line_number':377,'multiline':False]['text':' set.','line_number':378,'multiline':False]['text':' Abort catchup if we have caught up to the latest known optime after heartbeat refreshing.','line_number':409,'multiline':False]['text':' Cancel catchup takeover if the last applied write by any node in the replica set was made','line_number':414,'multiline':False]['text':' in the current term, which implies that the primary has caught up.','line_number':415,'multiline':False]['text':' Update the cached member state if different than the current topology member state','line_number':437,'multiline':False]['text':' Don't schedule a priority takeover if any takeover is already scheduled.','line_number':466,'multiline':False]['text':' Add randomized offset to calculated priority takeover delay.','line_number':469,'multiline':False]['text':' Don't schedule a catchup takeover if any takeover is already scheduled.','line_number':486,'multiline':False]['text':'*
 * This callback is purely for logging and has no effect on any other operations
 ','line_number':510,'multiline':True]['text':' namespace','line_number':532,'multiline':False]['text':' This log output is used in js tests so please leave it.','line_number':557,'multiline':False]['text':' kill all write operations which are no longer safe to run on step down. Also, operations that','line_number':574,'multiline':False]['text':' have taken global lock in S mode and operations blocked on prepare conflict will be killed to','line_number':575,'multiline':False]['text':' avoid 3-way deadlock between read, prepared transaction and step down thread.','line_number':576,'multiline':False]['text':' This node has already stepped down due to reconfig. So, signal anyone who is waiting on the','line_number':581,'multiline':False]['text':' step down event.','line_number':582,'multiline':False]['text':' We need to release the mutex before yielding locks for prepared transactions, which might','line_number':588,'multiline':False]['text':' check out sessions, to avoid deadlocks with checked-out sessions accessing this mutex.','line_number':589,'multiline':False]['text':' Clear the node's election candidate metrics since it is no longer primary.','line_number':597,'multiline':False]['text':' Update _canAcceptNonLocalWrites.','line_number':602,'multiline':False]['text':' We've just stepped down due to the "term", so it's impossible to step down again','line_number':609,'multiline':False]['text':' for the same term.','line_number':610,'multiline':False]['text':' Allow force reconfigs to proceed even if we are not a writable primary yet.','line_number':659,'multiline':False]['text':' Prevent heartbeat reconfigs from running concurrently with an election.','line_number':670,'multiline':False]['text':' Unlock the lock because isSelf performs network I/O.','line_number':733,'multiline':False]['text':' If this node is listed in the members of incoming config, accept the config.','line_number':736,'multiline':False]['text':' We always check the config when _selfIndex is not valid, in order to be able to','line_number':788,'multiline':False]['text':' recover from transient DNS errors.','line_number':789,'multiline':False]['text':' If the configs are the same, so is our index.','line_number':799,'multiline':False]['text':' If this node absent in newConfig, and this node was not previously initialized,','line_number':809,'multiline':False]['text':' return to kConfigUninitialized immediately, rather than storing the config and','line_number':810,'multiline':False]['text':' transitioning into the RS_REMOVED state.  See SERVER-15740.','line_number':811,'multiline':False]['text':' Don't write the no-op for config learned via heartbeats.','line_number':844,'multiline':False]['text':' writeOplog ','line_number':846,'multiline':True]['text':' Wait for durability of the new config document.','line_number':849,'multiline':False]['text':' Anyone changing the storage engine is responsible for copying the on-disk','line_number':853,'multiline':False]['text':' configuration between the old engine and the new.','line_number':854,'multiline':False]['text':' Donor access blockers are removed from donor nodes via the shard split op observer.','line_number':892,'multiline':False]['text':' Donor access blockers are removed from recipient nodes when the node applies the','line_number':893,'multiline':False]['text':' recipient config. When the recipient primary steps up it will delete its state','line_number':894,'multiline':False]['text':' document, the call to remove access blockers there will be a no-op.','line_number':895,'multiline':False]['text':' Start data replication after the config has been installed.','line_number':911,'multiline':False]['text':' Initializing minvalid is not allowed to be interrupted.  Make sure it','line_number':916,'multiline':False]['text':' can't be interrupted by a storage change by taking the global lock first.','line_number':917,'multiline':False]['text':' Do not conduct an election during a reconfig, as the node may not be electable post-reconfig.','line_number':960,'multiline':False]['text':' If there is an election in-progress, there can be at most one. No new election can happen as','line_number':961,'multiline':False]['text':' we have already set our ReplicationCoordinatorImpl::_rsConfigState state to','line_number':962,'multiline':False]['text':' "kConfigReconfiguring" which prevents new elections from happening.','line_number':963,'multiline':False]['text':' Wait for the election to complete and the node's Role to be set to follower.','line_number':972,'multiline':False]['text':' Primary node will be either unelectable or removed after the configuration change.','line_number':992,'multiline':False]['text':' So, finish the reconfig under RSTL, so that the step down occurs safely.','line_number':993,'multiline':False]['text':' We need to release the mutex before yielding locks for prepared transactions, which','line_number':1002,'multiline':False]['text':' might check out sessions, to avoid deadlocks with checked-out sessions accessing','line_number':1003,'multiline':False]['text':' this mutex.','line_number':1004,'multiline':False]['text':' Clear the node's election candidate metrics since it is no longer primary.','line_number':1012,'multiline':False]['text':' Update _canAcceptNonLocalWrites.','line_number':1015,'multiline':False]['text':' Release the rstl lock as the node might have stepped down due to','line_number':1018,'multiline':False]['text':' other unconditional step down code paths like learning new term via heartbeat &','line_number':1019,'multiline':False]['text':' liveness timeout. And, no new election can happen as we have already set our','line_number':1020,'multiline':False]['text':' ReplicationCoordinatorImpl::_rsConfigState state to "kConfigReconfiguring" which','line_number':1021,'multiline':False]['text':' prevents new elections from happening. So, its safe to release the RSTL lock.','line_number':1022,'multiline':False]['text':' If we do not have an index, we should pass -1 as our index to avoid falsely adding ourself to','line_number':1055,'multiline':False]['text':' the data structures inside of the TopologyCoordinator.','line_number':1056,'multiline':False]['text':' Clear lastCommittedOpTime by passing in a default constructed OpTimeAndWallTime, and','line_number':1066,'multiline':False]['text':' indicating that this is `forInitiate`.','line_number':1067,'multiline':False]['text':' Used in tests that wait for the post member state update action to complete.','line_number':1083,'multiline':False]['text':' eg. Closing connections upon being removed.','line_number':1084,'multiline':False]['text':' The target's HostAndPort should be safe to store, because it cannot change without a','line_number':1098,'multiline':False]['text':' reconfig. On reconfig, all current heartbeats get cancelled and new requests are sent out, so','line_number':1099,'multiline':False]['text':' there should not be a situation where the target node's HostAndPort changes but this','line_number':1100,'multiline':False]['text':' heartbeat handle remains active.','line_number':1101,'multiline':False]['text':' Heartbeat callbacks will remove themselves from _heartbeatHandles when they execute with','line_number':1120,'multiline':False]['text':' CallbackCanceled status, so it's better to leave the handles in the list, for now.','line_number':1121,'multiline':False]['text':' Only cancel heartbeats that are scheduled. If a heartbeat request has already been','line_number':1140,'multiline':False]['text':' sent, we should wait for the response instead.','line_number':1141,'multiline':False]['text':' Track the members that we have cancelled heartbeats.','line_number':1148,'multiline':False]['text':' reschedule = ','line_number':1172,'multiline':True]['text':' Scan liveness table for problems and mark nodes as down by calling into topocoord.','line_number':1182,'multiline':False]['text':' Don't mind potential asynchronous stepdown as this is the last step of','line_number':1184,'multiline':False]['text':' liveness check.','line_number':1185,'multiline':False]['text':' reschedule = ','line_number':1189,'multiline':True]['text':' Scan liveness table for earliest date; schedule a run at (that date plus election','line_number':1193,'multiline':False]['text':' timeout).','line_number':1194,'multiline':False]['text':' Nobody here but us.','line_number':1201,'multiline':False]['text':' don't bother to schedule; one is already scheduled and pending.','line_number':1206,'multiline':False]['text':' It is possible we will schedule the next timeout in the past.','line_number':1213,'multiline':False]['text':' DelayableTimeoutCallback schedules its work immediately if it's given a time <= now().','line_number':1214,'multiline':False]['text':' If we missed the timeout, it means that on our last check the earliest live member was','line_number':1215,'multiline':False]['text':' just barely fresh and it has become stale since then. We must schedule another liveness','line_number':1216,'multiline':False]['text':' check to continue conducting liveness checks and be able to step down from primary if we','line_number':1217,'multiline':False]['text':' lose contact with a majority of nodes.','line_number':1218,'multiline':False]['text':' We ignore shutdown errors; any other error triggers an fassert.','line_number':1219,'multiline':False]['text':' reschedule = ','line_number':1228,'multiline':True]['text':' We log at level 5 except when:','line_number':1250,'multiline':False]['text':' * This is the first time we're scheduling after becoming an electable secondary.','line_number':1251,'multiline':False]['text':' * We are not going to reschedule the election timeout because we are shutting down or','line_number':1252,'multiline':False]['text':'   no longer an electable secondary.','line_number':1253,'multiline':False]['text':' * It has been at least a second since we last logged at level 4.','line_number':1254,'multiline':False]['text':'','line_number':1255,'multiline':False]['text':' In those instances we log at level 4.  This routine is called on every replication batch,','line_number':1256,'multiline':False]['text':' which would produce many log lines per second, so this logging strategy provides a','line_number':1257,'multiline':False]['text':' compromise which allows us to see the election timeout being rescheduled without spamming','line_number':1258,'multiline':False]['text':' the logs.','line_number':1259,'multiline':False]['text':' The log level here is 4 once per second, otherwise 5.','line_number':1290,'multiline':False]['text':' If it is not a single node replica set, no need to start an election after stepdown timeout.','line_number':1314,'multiline':False]['text':' We should always reschedule this callback even if we do not make it to the election','line_number':1322,'multiline':False]['text':' process.','line_number':1323,'multiline':False]['text':' namespace repl','line_number':1410,'multiline':False]['text':' namespace mongo','line_number':1411,'multiline':False]