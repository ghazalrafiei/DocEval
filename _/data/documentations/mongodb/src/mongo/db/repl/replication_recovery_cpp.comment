['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':'*
 * Tracks and logs operations applied during recovery.
 ','line_number':118,'multiline':True]['text':'*
 * OplogBuffer adaptor for a DBClient query on the oplog.
 * Implements only functions used by OplogApplier::getNextApplierBatch().
 ','line_number':165,'multiline':True]['text':' Check that the first document matches our appliedThrough point then skip it since it's','line_number':188,'multiline':False]['text':' already been applied.','line_number':189,'multiline':False]['text':' This should really be impossible because we check above that the top of the oplog is','line_number':191,'multiline':False]['text':' strictly > appliedThrough. If this fails it represents a serious bug in either the','line_number':192,'multiline':False]['text':' storage engine or query's implementation of the oplog scan.','line_number':193,'multiline':False]['text':' A non-existent recoveryTS means the checkpoint is unstable. If the recoveryTS exists but','line_number':291,'multiline':False]['text':' is null, that means a stable checkpoint was taken at a null timestamp. This should never','line_number':292,'multiline':False]['text':' happen.','line_number':293,'multiline':False]['text':' namespace','line_number':303,'multiline':False]['text':' We support only recovery from stable checkpoints during initial sync.','line_number':356,'multiline':False]['text':' We pass in "none" for the stable timestamp so that recoverFromOplog asks storage','line_number':368,'multiline':False]['text':' for the recoveryTimestamp just like on replica set recovery.','line_number':369,'multiline':False]['text':' Ensure 'recoverFromOplogAsStandalone' with 'takeUnstableCheckpointOnShutdown'','line_number':373,'multiline':False]['text':' is safely idempotent when it succeeds.','line_number':374,'multiline':False]['text':' Initial sync will reconstruct prepared transactions when it is completely done.','line_number':389,'multiline':False]['text':' This may take an IS lock on the oplog collection.','line_number':409,'multiline':False]['text':' Initial Sync will take over so no cleanup is needed.','line_number':455,'multiline':False]['text':' If we were passed in a stable timestamp, we are in rollback recovery and should recover from','line_number':467,'multiline':False]['text':' that stable timestamp. Otherwise, we're recovering at startup. If this storage engine','line_number':468,'multiline':False]['text':' supports recover to stable timestamp or enableMajorityReadConcern=false, we ask it for the','line_number':469,'multiline':False]['text':' recovery timestamp. If the storage engine returns a timestamp, we recover from that point.','line_number':470,'multiline':False]['text':' However, if the storage engine returns "none", the storage engine does not have a stable','line_number':471,'multiline':False]['text':' checkpoint and we must recover from an unstable checkpoint instead.','line_number':472,'multiline':False]['text':' This may take an IS lock on the oplog collection.','line_number':480,'multiline':False]['text':' Truncation may need to adjust the initialDataTimestamp so we let it complete first.','line_number':485,'multiline':False]['text':' isMajorityDataAvailable ','line_number':488,'multiline':True]['text':' Oplog is empty. There are no oplog entries to apply, so we exit recovery and go into','line_number':494,'multiline':False]['text':' initial sync.','line_number':495,'multiline':False]['text':' Take only unstable checkpoints during the recovery process.','line_number':550,'multiline':False]['text':' Allow "oldest" timestamp to move forward freely.','line_number':553,'multiline':False]['text':' Clear the appliedThrough so this reflects in the first stable checkpoint. See','line_number':562,'multiline':False]['text':' _recoverFromUnstableCheckpoint for details.','line_number':563,'multiline':False]['text':' The appliedThrough would be null if we shut down cleanly or crashed as a primary. Either','line_number':580,'multiline':False]['text':' way we are consistent at the top of the oplog.','line_number':581,'multiline':False]['text':' If the appliedThrough is not null, then we shut down uncleanly during secondary oplog','line_number':584,'multiline':False]['text':' application and must apply from the appliedThrough to the top of the oplog.','line_number':585,'multiline':False]['text':' When `recoverFromOplog` truncates the oplog, that also happens to set the "oldest','line_number':592,'multiline':False]['text':' timestamp" to the truncation point[1]. `_applyToEndOfOplog` will then perform writes','line_number':593,'multiline':False]['text':' before the truncation point. Doing so violates the constraint that all updates must be','line_number':594,'multiline':False]['text':' timestamped newer than the "oldest timestamp". So we will need to move the "oldest','line_number':595,'multiline':False]['text':' timestamp" back to the `startPoint`.','line_number':596,'multiline':False]['text':'','line_number':597,'multiline':False]['text':' Before doing so, we will remove any pins. Forcing the oldest timestamp backwards will','line_number':598,'multiline':False]['text':' error if there are pins in place, as those pin requests will no longer be satisfied.','line_number':599,'multiline':False]['text':' Recovering from an unstable checkpoint has no history in the first place. Thus, clearing','line_number':600,'multiline':False]['text':' pins has no real effect on history being held.','line_number':601,'multiline':False]['text':'','line_number':602,'multiline':False]['text':' [1] This is arguably incorrect. On rollback for nodes that are not keeping history to','line_number':603,'multiline':False]['text':' the "majority point", the "oldest timestamp" likely needs to go back in time. The','line_number':604,'multiline':False]['text':' oplog's `cappedTruncateAfter` method was a convenient location for this logic, which,','line_number':605,'multiline':False]['text':' unfortunately, conflicts with the usage above.','line_number':606,'multiline':False]['text':' When we're recovering for a restore, we may be recovering a large number of oplog','line_number':612,'multiline':False]['text':' entries, so we want to take unstable checkpoints to reduce cache pressure and allow','line_number':613,'multiline':False]['text':' resumption in case of a crash.','line_number':614,'multiline':False]['text':' `_recoverFromUnstableCheckpoint` is only expected to be called on startup.','line_number':624,'multiline':False]['text':' Ensure the `appliedThrough` is set to the top of oplog, specifically if the node was','line_number':628,'multiline':False]['text':' previously running as a primary. If a crash happens before the first stable checkpoint on','line_number':629,'multiline':False]['text':' upgrade, replication recovery will know it must apply from this point and not assume the','line_number':630,'multiline':False]['text':' datafiles contain any writes that were taken before the crash.','line_number':631,'multiline':False]['text':' Force the set `appliedThrough` to become durable on disk in a checkpoint. This method would','line_number':634,'multiline':False]['text':' typically take a stable checkpoint, but because we're starting up from a checkpoint that','line_number':635,'multiline':False]['text':' has no checkpoint timestamp, the stable checkpoint "degrades" into an unstable checkpoint.','line_number':636,'multiline':False]['text':'','line_number':637,'multiline':False]['text':' Not waiting for checkpoint durability here can result in a scenario where the node takes','line_number':638,'multiline':False]['text':' writes and persists them to the oplog, but crashes before a stable checkpoint persists a','line_number':639,'multiline':False]['text':' "recovery timestamp". The typical startup path for data-bearing nodes is to use the recovery','line_number':640,'multiline':False]['text':' timestamp to determine where to play oplog forward from. As this method shows, when a','line_number':641,'multiline':False]['text':' recovery timestamp does not exist, the applied through is used to determine where to start','line_number':642,'multiline':False]['text':' playing oplog entries from.','line_number':643,'multiline':False]['text':'stableCheckpoint','line_number':644,'multiline':True]['text':' Now that we have set the initial data timestamp and taken an unstable checkpoint with the','line_number':646,'multiline':False]['text':' appliedThrough being the topOfOplog, it is safe to clear the appliedThrough. This minValid','line_number':647,'multiline':False]['text':' document write would never get into an unstable checkpoint because we will no longer take','line_number':648,'multiline':False]['text':' unstable checkpoints from now on, except when gTakeUnstableCheckpointOnShutdown is true in','line_number':649,'multiline':False]['text':' certain standalone restore cases. Additionally, future stable checkpoints are guaranteed to','line_number':650,'multiline':False]['text':' be taken with the appliedThrough cleared. Therefore, if this node crashes before the first','line_number':651,'multiline':False]['text':' stable checkpoint, it can safely recover from the last unstable checkpoint with a correct','line_number':652,'multiline':False]['text':' appliedThrough value. Otherwise, if this node crashes after the first stable checkpoint, it','line_number':653,'multiline':False]['text':' can safely recover from a stable checkpoint (with an empty appliedThrough).','line_number':654,'multiline':False]['text':' Check if we have any unapplied ops in our oplog. It is important that this is done after','line_number':667,'multiline':False]['text':' deleting the ragged end of the oplog.','line_number':668,'multiline':False]['text':' We've applied all the valid oplog we have.','line_number':672,'multiline':False]['text':' Make sure we skip validation checks that are only intended for primaries while recovering.','line_number':694,'multiline':False]['text':' The oplog buffer will fetch all entries >= the startPoint timestamp, but it skips the first','line_number':708,'multiline':False]['text':' op on startup, which is why the startPoint is described as "exclusive".','line_number':709,'multiline':False]['text':' If we're doing unstable checkpoints during the recovery process (as we do during the special','line_number':739,'multiline':False]['text':' startupRecoveryForRestore mode), we need to advance the consistency marker for each batch so','line_number':740,'multiline':False]['text':' the next time we recover we won't start all the way over.  Further, we can advance the oldest','line_number':741,'multiline':False]['text':' timestamp to avoid keeping too much history.','line_number':742,'multiline':False]['text':'','line_number':743,'multiline':False]['text':' If we're recovering from a stable checkpoint (except the special startupRecoveryForRestore','line_number':744,'multiline':False]['text':' mode, which discards history before the top of oplog), we aren't doing new checkpoints during','line_number':745,'multiline':False]['text':' recovery so there is no point in advancing the consistency marker and we cannot advance','line_number':746,'multiline':False]['text':' "oldest" becaue it would be later than "stable".','line_number':747,'multiline':False]['text':' We must set appliedThrough before applying anything at all, so we know','line_number':757,'multiline':False]['text':' any unstable checkpoints we take are "dirty".  A null appliedThrough indicates','line_number':758,'multiline':False]['text':' a clean shutdown which may not be the case if we had started applying a batch.','line_number':759,'multiline':False]['text':' The applied up to timestamp will be null if no oplog entries were applied.','line_number':777,'multiline':False]['text':' We may crash before setting appliedThrough. If we have a stable checkpoint, we will recover','line_number':782,'multiline':False]['text':' to that checkpoint at a replication consistent point, and applying the oplog is safe.','line_number':783,'multiline':False]['text':' If we don't have a stable checkpoint, then we must be in startup recovery, and not rollback','line_number':784,'multiline':False]['text':' recovery, because we only roll back to a stable timestamp when we have a stable checkpoint.','line_number':785,'multiline':False]['text':' It is safe to do startup recovery from an unstable checkpoint provided we recover to the','line_number':786,'multiline':False]['text':' end of the oplog and discard history before it, as _recoverFromUnstableCheckpoint does.','line_number':787,'multiline':False]['text':' Advance all_durable timestamp to the last applied timestamp. This is needed because','line_number':790,'multiline':False]['text':' the last applied entry during recovery could be a no-op entry which doesn't do','line_number':791,'multiline':False]['text':' timestamped writes or advance the all_durable timestamp. We may set the stable','line_number':792,'multiline':False]['text':' timestamp to this last applied timestamp later and we require the stable timestamp to','line_number':793,'multiline':False]['text':' be less than or equal to the all_durable timestamp.','line_number':794,'multiline':False]['text':' OplogInterfaceLocal creates a backwards iterator over the oplog collection.','line_number':807,'multiline':False]['text':' Fetch the oplog collection.','line_number':823,'multiline':False]['text':' Find an oplog entry <= truncateAfterTimestamp.','line_number':837,'multiline':False]['text':' Parse the response.','line_number':848,'multiline':False]['text':' Truncate the oplog AFTER the oplog entry found to be <= truncateAfterTimestamp.','line_number':860,'multiline':False]['text':' Truncating the oplog sets the storage engine's maximum durable timestamp to the new top','line_number':867,'multiline':False]['text':' of the oplog.  It is illegal for this maximum durable timestamp to be before the oldest','line_number':868,'multiline':False]['text':' timestamp, so if the oldest timestamp is ahead of that point, we need to move it back.','line_number':869,'multiline':False]['text':' Since the stable timestamp is never behind the oldest and also must not be ahead of the','line_number':870,'multiline':False]['text':' maximum durable timesatmp, it has to be moved back as well.  This usually happens when','line_number':871,'multiline':False]['text':' the truncateAfterTimestamp does not exist in the oplog because there was a hole open when','line_number':872,'multiline':False]['text':' we crashed; in that case the oldest timestamp and the stable timestamp will be the','line_number':873,'multiline':False]['text':' timestamp immediately prior to the hole.','line_number':874,'multiline':False]['text':' We're moving the stable timestamp backwards, so we need to force it.','line_number':880,'multiline':False]['text':' The initialDataTimestamp may also be at the hole; move it back.','line_number':886,'multiline':False]['text':'inclusive','line_number':895,'multiline':True]['text':' aboutToDelete callback ','line_number':895,'multiline':True]['text':' There are no holes in the oplog that necessitate truncation.','line_number':908,'multiline':False]['text':' Clear the oplogTruncateAfterPoint now that we have removed any holes that might exist in the','line_number':927,'multiline':False]['text':' oplog -- and so that we do not truncate future entries erroneously.','line_number':928,'multiline':False]['text':' Set up read on oplog collection.','line_number':935,'multiline':False]['text':' namespace repl','line_number':976,'multiline':False]['text':' namespace mongo','line_number':977,'multiline':False]