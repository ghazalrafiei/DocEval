['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':' IWYU pragma: no_include "cxxabi.h"','line_number':35,'multiline':False]['text':' Failpoint for initial sync','line_number':109,'multiline':False]['text':' Failpoint which fails initial sync and leaves an oplog entry in the buffer.','line_number':112,'multiline':False]['text':' Failpoint which causes the initial sync function to hang after getting the oldest active','line_number':115,'multiline':False]['text':' transaction timestamp from the sync source.','line_number':116,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before creating shared data and','line_number':119,'multiline':False]['text':' splitting control flow between the oplog fetcher and the cloners.','line_number':120,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before copying databases.','line_number':123,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before finishing.','line_number':126,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before creating the oplog.','line_number':129,'multiline':False]['text':' Failpoint which stops the applier.','line_number':132,'multiline':False]['text':' Failpoint which causes the initial sync function to hang after cloning all databases.','line_number':135,'multiline':False]['text':' Failpoint which skips clearing _initialSyncState after a successful initial sync attempt.','line_number':138,'multiline':False]['text':' Failpoint which causes the initial sync function to fail and hang before starting a new attempt.','line_number':141,'multiline':False]['text':' Failpoint which fails initial sync before it applies the next batch of oplog entries.','line_number':144,'multiline':False]['text':' Failpoint which fasserts if applying a batch fails.','line_number':147,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before stopping the oplog fetcher.','line_number':150,'multiline':False]['text':' Failpoint which causes the initial sync function to hang before choosing a sync source.','line_number':153,'multiline':False]['text':' Failpoint which causes the initial sync function to hang after finishing.','line_number':156,'multiline':False]['text':' Failpoint which causes the initial sync function to hang after resetting the in-memory FCV.','line_number':159,'multiline':False]['text':' Failpoints for synchronization, shared with cloners.','line_number':162,'multiline':False]['text':' Used to reset the oldest timestamp during initial sync to a non-null timestamp.','line_number':175,'multiline':False]['text':' Set and unset by the InitialSyncTest fixture to cause initial sync to pause so that the','line_number':197,'multiline':False]['text':' Initial Sync Fuzzer can run commands on the sync source.','line_number':198,'multiline':False]['text':' namespace','line_number':212,'multiline':False]['text':' dependency list ','line_number':216,'multiline':True]['text':' autoReconnect ','line_number':252,'multiline':True]['text':' Start first initial sync attempt.','line_number':304,'multiline':False]['text':' Transition directly from PreStart to Complete if not started yet.','line_number':329,'multiline':False]['text':' Nothing to do if we are already in ShuttingDown or Complete state.','line_number':337,'multiline':False]['text':' We actually hold the required lock, but the lock object itself is not passed through.','line_number':371,'multiline':False]['text':' We return an empty BSON object after an initial sync attempt has been successfully','line_number':439,'multiline':False]['text':' completed. When an initial sync attempt completes successfully, initialSyncCompletes is','line_number':440,'multiline':False]['text':' incremented and then _initialSyncState is cleared. We check that _initialSyncState has been','line_number':441,'multiline':False]['text':' cleared because an initial sync attempt can fail even after initialSyncCompletes is','line_number':442,'multiline':False]['text':' incremented, and we also check that initialSyncCompletes is positive because an initial sync','line_number':443,'multiline':False]['text':' attempt can also fail before _initialSyncState is initialized.','line_number':444,'multiline':False]['text':' Only include the beginFetchingTimestamp if it's different from the beginApplyingTimestamp.','line_number':484,'multiline':False]['text':' Wait up to 10 seconds.','line_number':537,'multiline':False]['text':' 'opCtx' is passed through from startup().','line_number':560,'multiline':False]['text':' This might not be necessary if we failed initial sync.','line_number':584,'multiline':False]['text':' A node coming out of initial sync must guarantee at least one oplog document is visible','line_number':594,'multiline':False]['text':' such that others can sync from this node. Oplog visibility is only advanced when applying','line_number':595,'multiline':False]['text':' oplog entries during initial sync. Correct the visibility to match the initial sync time','line_number':596,'multiline':False]['text':' before transitioning to steady state replication.','line_number':597,'multiline':False]['text':' We set the initial data timestamp before clearing the initial sync flag. See comments in','line_number':609,'multiline':False]['text':' clearInitialSyncFlag.','line_number':610,'multiline':False]['text':' This completion guard invokes _finishInitialSyncAttempt on destruction.','line_number':651,'multiline':False]['text':' Lock guard must be declared after completion guard because completion guard destructor','line_number':661,'multiline':False]['text':' has to run outside lock.','line_number':662,'multiline':False]['text':' Set the oldestTimestamp to one because WiredTiger does not allow us to set it to zero','line_number':680,'multiline':False]['text':' since that would also set the all_durable point to zero. We specifically don't set','line_number':681,'multiline':False]['text':' the stable timestamp here because that will trigger taking a first stable checkpoint even','line_number':682,'multiline':False]['text':' though the initialDataTimestamp is still set to kAllowUnstableCheckpointsSentinel.','line_number':683,'multiline':False]['text':' Clear the oplog buffer.','line_number':699,'multiline':False]['text':' Get sync source.','line_number':702,'multiline':False]['text':' _scheduleWorkAndSaveHandle_inlock() is shutdown-aware.','line_number':707,'multiline':False]['text':' Cancellation should be treated the same as other errors. In this case, the most likely cause','line_number':732,'multiline':False]['text':' of a failed _chooseSyncSourceCallback() task is a cancellation triggered by','line_number':733,'multiline':False]['text':' InitialSyncer::shutdown() or the task executor shutting down.','line_number':734,'multiline':False]['text':' This log output is used in js tests so please leave it.','line_number':786,'multiline':False]['text':' There is no need to schedule separate task to create oplog collection since we are already in','line_number':798,'multiline':False]['text':' a callback and we are certain there's no existing operation context (required for creating','line_number':799,'multiline':False]['text':' collections and dropping user databases) attached to the current thread.','line_number':800,'multiline':False]['text':' Schedule rollback ID checker.','line_number':809,'multiline':False]['text':' Report exception as an initial syncer failure.','line_number':821,'multiline':False]['text':' truncate oplog; drop user databases.','line_number':827,'multiline':False]['text':' This code can make untimestamped writes (deletes) to the _mdb_catalog on top of existing','line_number':835,'multiline':False]['text':' timestamped updates.','line_number':836,'multiline':False]['text':' We are not replicating nor validating these writes.','line_number':839,'multiline':False]['text':' 1.) Truncate the oplog.','line_number':842,'multiline':False]['text':' 1a.) Create the oplog.','line_number':850,'multiline':False]['text':' 2a.) Abort any index builds started during initial sync.','line_number':858,'multiline':False]['text':' 2b.) Drop user databases.','line_number':862,'multiline':False]['text':' Since the beginFetchingOpTime is retrieved before significant work is done copying','line_number':877,'multiline':False]['text':' data from the sync source, we allow the OplogEntryFetcher to use its default retry strategy','line_number':878,'multiline':False]['text':' which retries up to 'numInitialSyncOplogFindAttempts' times'.  This will fail relatively','line_number':879,'multiline':False]['text':' quickly in the presence of network errors, allowing us to choose a different sync source.','line_number':880,'multiline':False]['text':' This is the top of the oplog before we query for the oldest active transaction timestamp. If','line_number':914,'multiline':False]['text':' that query returns that there are no active transactions, we will use this as the','line_number':915,'multiline':False]['text':' beginFetchingTimestamp.','line_number':916,'multiline':False]['text':' Obtain the oldest active transaction timestamp from the remote by querying their transactions','line_number':941,'multiline':False]['text':' table. To prevent oplog holes (primary) or a stale lastAppliedSnapshot (secondary) from','line_number':942,'multiline':False]['text':' causing this query to return an inaccurate timestamp, we specify an afterClusterTime of the','line_number':943,'multiline':False]['text':' defaultBeginFetchingOpTime so that we wait for all previous writes to be visible.','line_number':944,'multiline':False]['text':' find network timeout ','line_number':968,'multiline':True]['text':' getMore network timeout ','line_number':969,'multiline':True]['text':' Set beginFetchingOpTime if the oldest active transaction timestamp actually exists. Otherwise','line_number':1004,'multiline':False]['text':' use the sync source's top of the oplog from before querying for the oldest active transaction','line_number':1005,'multiline':False]['text':' timestamp. This will mean that even if a transaction is started on the sync source after','line_number':1006,'multiline':False]['text':' querying for the oldest active transaction timestamp, the node will still fetch its oplog','line_number':1007,'multiline':False]['text':' entries.','line_number':1008,'multiline':False]['text':' Since the beginFetchingOpTime is retrieved before significant work is done copying','line_number':1028,'multiline':False]['text':' data from the sync source, we allow the OplogEntryFetcher to use its default retry strategy','line_number':1029,'multiline':False]['text':' which retries up to 'numInitialSyncOplogFindAttempts' times'.  This will fail relatively','line_number':1030,'multiline':False]['text':' quickly in the presence of network errors, allowing us to choose a different sync source.','line_number':1031,'multiline':False]['text':' As part of reading the FCV, we ensure the source node's all_durable timestamp has advanced','line_number':1076,'multiline':False]['text':' to at least the timestamp of the last optime that we found in the lastOplogEntryFetcher.','line_number':1077,'multiline':False]['text':' When document locking is used, there could be oplog "holes" which would result in','line_number':1078,'multiline':False]['text':' inconsistent initial sync data if we didn't do this.','line_number':1079,'multiline':False]['text':' find network timeout ','line_number':1095,'multiline':True]['text':' getMore network timeout ','line_number':1096,'multiline':True]['text':' Changing the featureCompatibilityVersion during initial sync is unsafe.','line_number':1147,'multiline':False]['text':' (Generic FCV reference): This FCV check should exist across LTS binary versions.','line_number':1148,'multiline':False]['text':' Since we don't guarantee that we always clone the "admin.system.version" collection first','line_number':1158,'multiline':False]['text':' and collection/index creation can depend on FCV, we set the in-memory FCV value to match','line_number':1159,'multiline':False]['text':' the version on the sync source. We won't persist the FCV on disk nor will we update our','line_number':1160,'multiline':False]['text':' minWireVersion until we clone the actual document.','line_number':1161,'multiline':False]['text':' This is where the flow of control starts to split into two parallel tracks:','line_number':1177,'multiline':False]['text':' - oplog fetcher','line_number':1178,'multiline':False]['text':' - data cloning and applier','line_number':1179,'multiline':False]['text':' Create oplog applier.','line_number':1188,'multiline':False]['text':' _startupComponent_inlock is shutdown-aware.','line_number':1250,'multiline':False]['text':' This could have been done with a scheduleWorkAt but this is used only by JS tests where','line_number':1260,'multiline':False]['text':' we run with multiple threads so it's fine to spin on this thread.','line_number':1261,'multiline':False]['text':' This log output is used in js tests so please leave it.','line_number':1262,'multiline':False]['text':' runOnExecutorEvent ensures the future is not ready unless an error has occurred.','line_number':1280,'multiline':False]['text':' The completion guard must run on the main executor, and never inline.  In unit tests,','line_number':1289,'multiline':False]['text':' without the executor call, it would run on the wrong executor.  In both production','line_number':1290,'multiline':False]['text':' and in unit tests, if the cloner finishes very quickly, the callback could run','line_number':1291,'multiline':False]['text':' in-line and result in self-deadlock.','line_number':1292,'multiline':False]['text':' In the shutdown case, it is possible the completion guard will be run','line_number':1302,'multiline':False]['text':' from this thread (since the lambda holding another copy didn't schedule).','line_number':1303,'multiline':False]['text':' If it does, we will self-deadlock if we're holding the lock, so release it.','line_number':1304,'multiline':False]['text':' In unit tests, this reset ensures the completion guard does not run during the','line_number':1307,'multiline':False]['text':' destruction of the lambda (which occurs on the wrong executor), except in the','line_number':1308,'multiline':False]['text':' shutdown case.','line_number':1309,'multiline':False]['text':' Start (and therefore finish) the cloners outside the lock.  This ensures onCompletion','line_number':1313,'multiline':False]['text':' is not run with the mutex held, which would result in self-deadlock.','line_number':1314,'multiline':False]['text':' When the OplogFetcher completes early (instead of being canceled at shutdown), we log and let','line_number':1329,'multiline':False]['text':' our reference to 'onCompletionGuard' go out of scope. Since we know the','line_number':1330,'multiline':False]['text':' DatabasesCloner/MultiApplier will still have a reference to it, the actual function within','line_number':1331,'multiline':False]['text':' the guard won't be fired yet.','line_number':1332,'multiline':False]['text':' It is up to the DatabasesCloner and MultiApplier to determine if they can proceed without any','line_number':1333,'multiline':False]['text':' additional data going into the oplog buffer.','line_number':1334,'multiline':False]['text':' It is not common for the OplogFetcher to return with an OK status. The only time it returns','line_number':1335,'multiline':False]['text':' an OK status is when the 'stopReplProducer' fail point is enabled, which causes the','line_number':1336,'multiline':False]['text':' OplogFetcher to ignore the current sync source response and return early.','line_number':1337,'multiline':False]['text':' During normal operation, this call to onCompletion->setResultAndCancelRemainingWork_inlock','line_number':1345,'multiline':False]['text':' is a no-op because the other thread running the DatabasesCloner or MultiApplier will already','line_number':1346,'multiline':False]['text':' have called it with the success/failed status.','line_number':1347,'multiline':False]['text':' The OplogFetcher does not finish on its own because of the oplog tailing query it runs on the','line_number':1348,'multiline':False]['text':' sync source. The most common OplogFetcher completion status is CallbackCanceled due to either','line_number':1349,'multiline':False]['text':' a shutdown request or completion of the data cloning and oplog application phases.','line_number':1350,'multiline':False]['text':' This could have been done with a scheduleWorkAt but this is used only by JS tests where','line_number':1363,'multiline':False]['text':' we run with multiple threads so it's fine to spin on this thread.','line_number':1364,'multiline':False]['text':' This log output is used in js tests so please leave it.','line_number':1365,'multiline':False]['text':' Since the stopTimestamp is retrieved after we have done all the work of retrieving collection','line_number':1383,'multiline':False]['text':' data, we handle retries within this class by retrying for','line_number':1384,'multiline':False]['text':' 'initialSyncTransientErrorRetryPeriodSeconds' (default 24 hours).  This is the same retry','line_number':1385,'multiline':False]['text':' strategy used when retrieving collection data, and avoids retrieving all the data and then','line_number':1386,'multiline':False]['text':' throwing it away due to a transient network outage.','line_number':1387,'multiline':False]['text':' It is not valid to schedule the retry from within this callback,','line_number':1415,'multiline':False]['text':' hence we schedule a lambda to schedule the retry.','line_number':1416,'multiline':False]['text':' Since the stopTimestamp is retrieved after we have done all the','line_number':1418,'multiline':False]['text':' work of retrieving collection data, we handle retries within this','line_number':1419,'multiline':False]['text':' class by retrying for','line_number':1420,'multiline':False]['text':' 'initialSyncTransientErrorRetryPeriodSeconds' (default 24 hours).','line_number':1421,'multiline':False]['text':' This is the same retry strategy used when retrieving collection','line_number':1422,'multiline':False]['text':' data, and avoids retrieving all the data and then throwing it','line_number':1423,'multiline':False]['text':' away due to a transient network outage.','line_number':1424,'multiline':False]['text':' If scheduling failed, we're shutting down and cannot retry.','line_number':1440,'multiline':False]['text':' So just continue with the original failed status.','line_number':1441,'multiline':False]['text':' Release the _mutex to write to disk.','line_number':1457,'multiline':False]['text':' If the beginFetchingTimestamp is different from the stopTimestamp, it indicates that','line_number':1465,'multiline':False]['text':' there are oplog entries fetched by the oplog fetcher that need to be written to the oplog','line_number':1466,'multiline':False]['text':' and/or there are operations that need to be applied.','line_number':1467,'multiline':False]['text':' Oplog at sync source has not advanced since we started cloning databases, so we use the last','line_number':1475,'multiline':False]['text':' oplog entry to seed the oplog before checking the rollback ID.','line_number':1476,'multiline':False]['text':' StorageInterface::insertDocument() has to be called outside the lock because we may','line_number':1485,'multiline':False]['text':' override its behavior in tests. See InitialSyncerReturnsCallbackCanceledAndDoesNot-','line_number':1486,'multiline':False]['text':' ScheduleRollbackCheckerIfShutdownAfterInsertingInsertOplogSeedDocument in','line_number':1487,'multiline':False]['text':' initial_syncer_test.cpp','line_number':1488,'multiline':False]['text':'','line_number':1489,'multiline':False]['text':' Note that the initial seed oplog insertion is not timestamped, this is safe to do as the','line_number':1490,'multiline':False]['text':' logic for navigating the oplog is reliant on the timestamp value of the oplog document','line_number':1491,'multiline':False]['text':' itself. Additionally, this also prevents confusion in the storage engine as the last','line_number':1492,'multiline':False]['text':' insertion can be produced at precisely the stable timestamp, which could lead to invalid','line_number':1493,'multiline':False]['text':' data consistency due to the stable timestamp signalling that no operations before or at','line_number':1494,'multiline':False]['text':' that point will be rolled back. So transactions shouldn't happen at precisely that point.','line_number':1495,'multiline':False]['text':' This sets the error in 'onCompletionGuard' and shuts down the OplogFetcher on error.','line_number':1516,'multiline':False]['text':' Schedule MultiApplier if we have operations to apply.','line_number':1556,'multiline':False]['text':' If the oplog fetcher is no longer running (completed successfully) and the oplog buffer is','line_number':1583,'multiline':False]['text':' empty, we are not going to make any more progress with this initial sync. Report progress so','line_number':1584,'multiline':False]['text':' far and return a RemoteResultsUnavailable error.','line_number':1585,'multiline':False]['text':' If there are no operations at the moment to apply and the oplog fetcher is still waiting on','line_number':1604,'multiline':False]['text':' the sync source, we'll check the oplog buffer again in','line_number':1605,'multiline':False]['text':' '_opts.getApplierBatchCallbackRetryWait' ms.','line_number':1606,'multiline':False]['text':' Report exception as an initial syncer failure.','line_number':1620,'multiline':False]['text':' Set to cause initial sync to fassert instead of restart if applying a batch fails, so that','line_number':1633,'multiline':False]['text':' tests can be robust to network errors but not oplog idempotency errors.','line_number':1634,'multiline':False]['text':' Update oplog visibility after applying a batch so that while applying transaction oplog','line_number':1651,'multiline':False]['text':' entries, the TransactionHistoryIterator can get earlier oplog entries associated with the','line_number':1652,'multiline':False]['text':' transaction. Note that setting the oplog visibility timestamp here will be safe even if','line_number':1653,'multiline':False]['text':' initial sync was restarted because until initial sync ends, no one else will try to read our','line_number':1654,'multiline':False]['text':' oplog. It is also safe even if we tried to read from our own oplog because we never try to','line_number':1655,'multiline':False]['text':' read from the oplog before applying at least one batch and therefore setting a value for the','line_number':1656,'multiline':False]['text':' oplog visibility timestamp.','line_number':1657,'multiline':False]['text':' Success!','line_number':1696,'multiline':False]['text':' Since _finishInitialSyncAttempt can be called from any component's callback function or','line_number':1701,'multiline':False]['text':' scheduled task, it is possible that we may not be in a TaskExecutor-managed thread when this','line_number':1702,'multiline':False]['text':' function is invoked.','line_number':1703,'multiline':False]['text':' For example, if CollectionCloner fails while inserting documents into the','line_number':1704,'multiline':False]['text':' CollectionBulkLoader, we will get here via one of CollectionCloner's TaskRunner callbacks','line_number':1705,'multiline':False]['text':' which has an active OperationContext bound to the current Client. This would lead to an','line_number':1706,'multiline':False]['text':' invariant when we attempt to create a new OperationContext for _tearDown(opCtx).','line_number':1707,'multiline':False]['text':' To avoid this, we schedule _finishCallback against the TaskExecutor rather than calling it','line_number':1708,'multiline':False]['text':' here synchronously.','line_number':1709,'multiline':False]['text':' Unless dismissed, a scope guard will schedule _finishCallback() upon exiting this function.','line_number':1711,'multiline':False]['text':' Since it is a requirement that _finishCallback be called outside the lock (which is possible','line_number':1712,'multiline':False]['text':' if the task scheduling fails and we have to invoke _finishCallback() synchronously), we','line_number':1713,'multiline':False]['text':' declare the scope guard before the lock guard.','line_number':1714,'multiline':False]['text':' This increments the number of failed attempts for the current initial sync request.','line_number':1761,'multiline':False]['text':' This increments the number of failed attempts across all initial sync attempts since','line_number':1763,'multiline':False]['text':' process startup.','line_number':1764,'multiline':False]['text':' Scope guard will invoke _finishCallback().','line_number':1774,'multiline':False]['text':' Check if need to do more retries.','line_number':1784,'multiline':False]['text':' Scope guard will invoke _finishCallback().','line_number':1791,'multiline':False]['text':' Scope guard will invoke _finishCallback().','line_number':1812,'multiline':False]['text':' Next initial sync attempt scheduled successfully and we do not need to call _finishCallback()','line_number':1816,'multiline':False]['text':' until the next initial sync attempt finishes.','line_number':1817,'multiline':False]['text':' After running callback function, clear '_onCompletion' to release any resources that might be','line_number':1822,'multiline':False]['text':' held by this function object.','line_number':1823,'multiline':False]['text':' '_onCompletion' must be moved to a temporary copy and destroyed outside the lock in case','line_number':1824,'multiline':False]['text':' there is any logic that's invoked at the function object's destruction that might call into','line_number':1825,'multiline':False]['text':' this InitialSyncer. 'onCompletion' must be destroyed outside the lock and this should happen','line_number':1826,'multiline':False]['text':' before we transition the state to Complete.','line_number':1827,'multiline':False]['text':' This log output is used in js tests so please leave it.','line_number':1838,'multiline':False]['text':' Any _retryingOperation is no longer active.  This must be done before signalling state','line_number':1847,'multiline':False]['text':' Complete.','line_number':1848,'multiline':False]['text':' Completion callback must be invoked outside mutex.','line_number':1851,'multiline':False]['text':' Destroy the remaining reference to the completion callback before we transition the state to','line_number':1860,'multiline':False]['text':' Complete so that callers can expect any resources bound to '_onCompletion' to be released','line_number':1861,'multiline':False]['text':' before InitialSyncer::join() returns.','line_number':1862,'multiline':False]['text':' Clear the initial sync progress after an initial sync attempt has been successfully','line_number':1871,'multiline':False]['text':' completed.','line_number':1872,'multiline':False]['text':' Destroy shared references to executors.','line_number':1877,'multiline':False]['text':' find network timeout ','line_number':1908,'multiline':True]['text':' getMore network timeout ','line_number':1909,'multiline':True]['text':' We should check our current state because shutdown() could have been called before','line_number':1925,'multiline':False]['text':' we re-acquired the lock.','line_number':1926,'multiline':False]['text':' Basic sanity check on begin/stop timestamps.','line_number':1936,'multiline':False]['text':' Check if any ops occurred while cloning or any ops need to be fetched.','line_number':1955,'multiline':False]['text':' Fall through to scheduling _getNextApplierBatchCallback().','line_number':1963,'multiline':False]['text':' Check for rollback if we have applied far enough to be consistent.','line_number':1965,'multiline':False]['text':' Get another batch to apply.','line_number':1971,'multiline':False]['text':' _scheduleWorkAndSaveHandle_inlock() is shutdown-aware.','line_number':1972,'multiline':False]['text':' We should check our current state because shutdown() could have been called before','line_number':1987,'multiline':False]['text':' we re-acquired the lock.','line_number':1988,'multiline':False]['text':' The status was OK or some error other than a retriable error, so clear the retriable error','line_number':2019,'multiline':False]['text':' state and indicate that we should not retry.','line_number':2020,'multiline':False]['text':' It is necessary to check if shutdown or attempt cancelling happens before starting a','line_number':2091,'multiline':False]['text':' component; otherwise the component may call a callback function in line which will','line_number':2092,'multiline':False]['text':' cause a deadlock when the callback attempts to obtain the initial syncer mutex.','line_number':2093,'multiline':False]['text':' If the fail-point is active, delay the apply batch by returning an empty batch so that','line_number':2121,'multiline':False]['text':' _getNextApplierBatchCallback() will reschedule itself at a later time.','line_number':2122,'multiline':False]['text':' See InitialSyncerInterface::Options::getApplierBatchCallbackRetryWait.','line_number':2123,'multiline':False]['text':' Obtain next batch of operations from OplogApplier.','line_number':2128,'multiline':False]['text':' We want a batch boundary after the beginApplyingTimestamp, to make sure all oplog entries','line_number':2133,'multiline':False]['text':' that are part of a transaction before that timestamp are written out before we start applying','line_number':2134,'multiline':False]['text':' entries after them.  This is because later entries may be commit or prepare and thus','line_number':2135,'multiline':False]['text':' expect to read the partial entries from the oplog.','line_number':2136,'multiline':False]['text':' Wait for enough space.','line_number':2164,'multiline':False]['text':' Buffer docs for later application.','line_number':2167,'multiline':False]['text':' TODO: updates metrics with "info".','line_number':2172,'multiline':False]['text':' A non-network error occured, so clear any network error and use the default restart','line_number':2239,'multiline':False]['text':' strategy.','line_number':2240,'multiline':False]['text':' namespace repl','line_number':2251,'multiline':False]['text':' namespace mongo','line_number':2252,'multiline':False]