['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':' IWYU pragma: no_include "cxxabi.h"','line_number':36,'multiline':False]['text':' The count of items in the buffer','line_number':164,'multiline':False]['text':'*
 * Returns new thread pool for thread pool task executor.
 ','line_number':167,'multiline':True]['text':'*
 * Returns a new thread pool task executor.
 ','line_number':188,'multiline':True]['text':'*
 * Schedules a task using the executor. This task is always run unless the task executor is
 * shutting down.
 ','line_number':202,'multiline':True]['text':' namespace','line_number':219,'multiline':False]['text':' This function acquires the LockManager locks on oplog, so it cannot be called while holding','line_number':238,'multiline':False]['text':' ReplicationCoordinatorImpl's mutex.','line_number':239,'multiline':False]['text':' We've shut down the external state, don't start again.','line_number':245,'multiline':False]['text':' No need to log OplogBuffer::startup because the blocking queue implementation','line_number':252,'multiline':False]['text':' does not start any threads or access the storage layer.','line_number':253,'multiline':False]['text':' Using noop observer now that BackgroundSync no longer implements the','line_number':258,'multiline':False]['text':' OplogApplier::Observer interface. During steady state replication, there is no need to','line_number':259,'multiline':False]['text':' log details on every batch we apply.','line_number':260,'multiline':False]['text':' Get the pointer while holding the lock so that _stopDataReplication_inlock() won't','line_number':283,'multiline':False]['text':' leave the unique pointer empty if the _syncSourceFeedbackThread's function starts','line_number':284,'multiline':False]['text':' after _stopDataReplication_inlock's move.','line_number':285,'multiline':False]['text':' Make sue no other _stopDataReplication calls are in progress.','line_number':294,'multiline':False]['text':' _syncSourceFeedbackThread should be joined before _bgSync's shutdown because it has','line_number':306,'multiline':False]['text':' a pointer of _bgSync.','line_number':307,'multiline':False]['text':' Clear the buffer. This unblocks the OplogFetcher if it is blocked with a full queue, but','line_number':324,'multiline':False]['text':' ensures that it won't add anything. It will also unblock the OplogApplier pipeline if it','line_number':325,'multiline':False]['text':' is waiting for an operation to be past the secondaryDelaySecs point.','line_number':326,'multiline':False]['text':' Once the writer pool's shutdown() is called, scheduling new tasks will return error, so','line_number':343,'multiline':False]['text':' we shutdown writer pool after the applier exits to avoid new tasks being scheduled.','line_number':344,'multiline':False]['text':' Perform additional shutdown steps below that must be done outside _threadMutex.','line_number':405,'multiline':False]['text':' Stop the NoOpWriter before grabbing the mutex to avoid creating a deadlock as the','line_number':407,'multiline':False]['text':' NoOpWriter itself can block on the ReplicationCoordinator mutex. It is safe to access','line_number':408,'multiline':False]['text':' _noopWriter outside of _threadMutex because _noopWriter is protected by its own mutex.','line_number':409,'multiline':False]['text':' We must wait for _taskExecutor outside of _threadMutex, since _taskExecutor is used to','line_number':414,'multiline':False]['text':' run the dropPendingCollectionReaper, which takes database locks. It is safe to access','line_number':415,'multiline':False]['text':' _taskExecutor outside of _threadMutex because once _startedThreads is set to true, the','line_number':416,'multiline':False]['text':' _taskExecutor pointer never changes.','line_number':417,'multiline':False]['text':' The oplog truncate after point must be cleared, if we are still primary for shutdown, so','line_number':420,'multiline':False]['text':' nothing gets truncated unnecessarily on startup. There are no oplog holes on clean','line_number':421,'multiline':False]['text':' primary shutdown. Stepdown is similarly safe from holes and halts updates to and clears','line_number':422,'multiline':False]['text':' the truncate point. The other replication states do need truncation if the truncate point','line_number':423,'multiline':False]['text':' is set: e.g. interruption mid batch application can leave oplog holes.','line_number':424,'multiline':False]['text':' Permit writing to the oplog before we step up to primary.','line_number':453,'multiline':False]['text':' Writes to 'local.system.replset' must be untimestamped.','line_number':465,'multiline':False]['text':' ReplSetTest assumes that immediately after the replSetInitiate command returns, it can','line_number':478,'multiline':False]['text':' allow other nodes to initial sync with no retries and they will succeed. Unfortunately,','line_number':479,'multiline':False]['text':' initial sync will fail if it finds its sync source has an empty oplog. Thus, we need to','line_number':480,'multiline':False]['text':' wait here until the seed document is visible in our oplog.','line_number':481,'multiline':False]['text':' Take an unstable checkpoint to ensure that the FCV document is persisted to disk.','line_number':484,'multiline':False]['text':' stableCheckpoint ','line_number':486,'multiline':True]['text':' A primary periodically updates the oplogTruncateAfterPoint to allow replication to','line_number':517,'multiline':False]['text':' proceed without danger of unidentifiable oplog holes on unclean shutdown due to parallel','line_number':518,'multiline':False]['text':' writes.','line_number':519,'multiline':False]['text':'','line_number':520,'multiline':False]['text':' Initialize the oplogTruncateAfterPoint so that user writes are safe on unclean shutdown','line_number':521,'multiline':False]['text':' between completion of transition to primary and the first async oplogTruncateAfterPoint','line_number':522,'multiline':False]['text':' update.','line_number':523,'multiline':False]['text':' Tell the system to start updating the oplogTruncateAfterPoint asynchronously and to use','line_number':526,'multiline':False]['text':' the truncate point, rather than last applied, to update the repl durable timestamp.','line_number':527,'multiline':False]['text':'','line_number':528,'multiline':False]['text':' The truncate point must be used while primary for repl's durable timestamp because','line_number':529,'multiline':False]['text':' otherwise we could truncate last applied writes on startup recovery after an unclean','line_number':530,'multiline':False]['text':' shutdown that were previously majority confirmed to the user.','line_number':531,'multiline':False]['text':' Clear the appliedThrough marker so on startup we'll use the top of the oplog. This must','line_number':534,'multiline':False]['text':' be done before we add anything to our oplog.','line_number':535,'multiline':False]['text':' As far as the storage system is concerned, we're still secondary here, and will be until we','line_number':549,'multiline':False]['text':' change readWriteAbility.  So new and resumed lock-free reads will read from lastApplied.  We','line_number':550,'multiline':False]['text':' just advanced lastApplied by writing the no-op, so we need to signal oplog waiters.','line_number':551,'multiline':False]['text':' This constant was based on data described in SERVER-44634. It is in relation to how long','line_number':558,'multiline':False]['text':' the first majority committed write takes after a new term has started.','line_number':559,'multiline':False]['text':' SERVER-44634: Disable flow control for a grace period after stepup. Because writes may','line_number':561,'multiline':False]['text':' stop while a node wins election and steps up, it's likely to determine there's majority','line_number':562,'multiline':False]['text':' point lag. Moreover, because there are no writes in the system, flow control will believe','line_number':563,'multiline':False]['text':' secondaries are unable to process oplog entries. This can result in an undesirable "slow','line_number':564,'multiline':False]['text':' start" phenomena.','line_number':565,'multiline':False]['text':' It is only necessary to check the system indexes on the first transition to primary.','line_number':578,'multiline':False]['text':' On subsequent transitions to primary the indexes will have already been created.','line_number':579,'multiline':False]['text':' Create the pre-images collection if it doesn't exist yet in the non-serverless environment.','line_number':588,'multiline':False]['text':' tenantId ','line_number':591,'multiline':True]['text':' Writes to 'local.system.replset' must be untimestamped.','line_number':630,'multiline':False]['text':' The no-op write doesn't affect the correctness of the safe reconfig protocol','line_number':645,'multiline':False]['text':' and so it doesn't have to be written in the same WUOW as the config write. In','line_number':646,'multiline':False]['text':' fact, the no-op write is only needed for some corner cases where the','line_number':647,'multiline':False]['text':' committed snapshot is dropped after a force reconfig that changes the config','line_number':648,'multiline':False]['text':' content or a safe reconfig that changes writeConcernMajorityJournalDefault.','line_number':649,'multiline':False]['text':' Make sure there's always a last vote document.','line_number':697,'multiline':False]['text':' If we are casting a vote in a new election immediately after stepping down, we','line_number':755,'multiline':False]['text':' don't want to have this process interrupted due to us stepping down, since we','line_number':756,'multiline':False]['text':' want to be able to cast our vote for a new primary right away. Both the write's lock','line_number':757,'multiline':False]['text':' acquisition and the "waitUntilDurable" lock acquisition must be uninterruptible.','line_number':758,'multiline':False]['text':'','line_number':759,'multiline':False]['text':' It is not safe to take an uninterruptible lock during STARTUP2, so we only take this lock','line_number':760,'multiline':False]['text':' if we are primary or secondary.  We do not have the RSTL but that is OK because we never','line_number':761,'multiline':False]['text':' move in to STARTUP2 from PRIMARY or SECONDARY, so the consequence of a stale state is','line_number':762,'multiline':False]['text':' only that we don't take an uninterruptible lock when we should.','line_number':763,'multiline':False]['text':' We only want to replace the last vote document if the new last vote document','line_number':782,'multiline':False]['text':' would have a higher term. We check the term of the current last vote document','line_number':783,'multiline':False]['text':' and insert the new document in a WriteUnitOfWork to synchronize the two','line_number':784,'multiline':False]['text':' operations. We have already ensured at startup time that there is an old','line_number':785,'multiline':False]['text':' document.','line_number':786,'multiline':False]['text':' TODO: handle WriteConflictExceptions below','line_number':830,'multiline':False]['text':' If we are doing an initial sync do not read from the oplog.','line_number':832,'multiline':False]['text':' Called earlier for config servers.','line_number':893,'multiline':False]['text':' If this mongod has a router service, it needs to run stepdown','line_number':897,'multiline':False]['text':' hooks even if the shard-role isn't initialized yet.','line_number':898,'multiline':False]['text':' TODO SERVER-82588: Update this code once CatalogCacheLoader is split between','line_number':899,'multiline':False]['text':' router and shard roles.','line_number':900,'multiline':False]['text':' Temporarily turn off flow control ticketing. Getting a ticket can stall on a ticket being','line_number':919,'multiline':False]['text':' available, which may have to wait for the ticket refresher to run, which in turn blocks','line_number':920,'multiline':False]['text':' on the repl _mutex to check whether we are primary or not: this is a deadlock because','line_number':921,'multiline':False]['text':' stepdown already holds the repl _mutex!','line_number':922,'multiline':False]['text':' As opCtx does not expose a method to allow skipping flow control on purpose we mark the','line_number':923,'multiline':False]['text':' operation as having Immediate priority. This will skip flow control and ticket acquisition.','line_number':924,'multiline':False]['text':' It is fine to do this since the system is essentially shutting down at this point.','line_number':925,'multiline':False]['text':' Tell the system to stop updating the oplogTruncateAfterPoint asynchronously and to go','line_number':929,'multiline':False]['text':' back to using last applied to update repl's durable timestamp instead of the truncate','line_number':930,'multiline':False]['text':' point.','line_number':931,'multiline':False]['text':' Interrupt the current JournalFlusher thread round, so it recognizes that it is no longer','line_number':934,'multiline':False]['text':' primary. Otherwise the asynchronously running thread could race with setting the truncate','line_number':935,'multiline':False]['text':' point to null below. This would leave the truncate point potentially stale in a','line_number':936,'multiline':False]['text':' non-PRIMARY state, where last applied would be used to update repl's durable timestamp','line_number':937,'multiline':False]['text':' and confirm majority writes. Startup recovery could truncate majority confirmed writes','line_number':938,'multiline':False]['text':' back to the stale truncate after point.','line_number':939,'multiline':False]['text':'','line_number':940,'multiline':False]['text':' This makes sure the JournalFlusher is not stuck waiting for a lock that stepdown might','line_number':941,'multiline':False]['text':' hold before doing an update write to the truncate point.','line_number':942,'multiline':False]['text':' Wait for another round of journal flushing. This will ensure that we wait for the current','line_number':945,'multiline':False]['text':' round to completely finish and have no chance of racing with unsetting the truncate point','line_number':946,'multiline':False]['text':' below. It is possible that the JournalFlusher will not check for the interrupt signaled','line_number':947,'multiline':False]['text':' above, if writing is imminent, so we must make sure that the code completes fully.','line_number':948,'multiline':False]['text':' We can clear the oplogTruncateAfterPoint because we know there are no user writes during','line_number':951,'multiline':False]['text':' stepdown and therefore presently no oplog holes.','line_number':952,'multiline':False]['text':' If the node is shutting down or it lost quorum just as it was becoming primary,','line_number':961,'multiline':False]['text':' don't run the sharding onStepUp machinery. The onStepDown counterpart to these','line_number':962,'multiline':False]['text':' methods is already idempotent, so the machinery will remain in the stepped down','line_number':963,'multiline':False]['text':' state.','line_number':964,'multiline':False]['text':' Load the clusterId into memory. Use local readConcern, since we can't use','line_number':976,'multiline':False]['text':' majority readConcern in drain mode because the global lock prevents replication.','line_number':977,'multiline':False]['text':' This is safe, since if the clusterId write is rolled back, any writes that depend','line_number':978,'multiline':False]['text':' on it will also be rolled back.','line_number':979,'multiline':False]['text':'','line_number':980,'multiline':False]['text':' Since we *just* wrote the cluster ID to the config.version document (via the call','line_number':981,'multiline':False]['text':' to ShardingCatalogManager::initializeConfigDatabaseIfNeeded above), this read can','line_number':982,'multiline':False]['text':' only meaningfully fail if the node is shutting down.','line_number':983,'multiline':False]['text':' Called earlier for config servers.','line_number':1011,'multiline':False]['text':' Note, these must be done after the configTime is recovered via','line_number':1020,'multiline':False]['text':' VectorClockMutable::recoverDirect above, because they may trigger filtering metadata','line_number':1021,'multiline':False]['text':' refreshes which should use the recovered configTime.','line_number':1022,'multiline':False]['text':' Schedule a drop of the temporary collections used by aggregations ($out','line_number':1029,'multiline':False]['text':' specifically).','line_number':1030,'multiline':False]['text':' If this mongod has a router service, it needs to run stepdown','line_number':1033,'multiline':False]['text':' hooks even if the shard-role isn't initialized yet.','line_number':1034,'multiline':False]['text':' TODO SERVER-82588: Update this code once CatalogCacheLoader is split between','line_number':1035,'multiline':False]['text':' router and shard roles.','line_number':1036,'multiline':False]['text':' The code above will only be executed after a stepdown happens, however the code below','line_number':1041,'multiline':False]['text':' needs to be executed also on startup, and the enabled check might fail in shards during','line_number':1042,'multiline':False]['text':' startup. Create uuid index on config.rangeDeletions if needed','line_number':1043,'multiline':False]['text':' If the node is shutting down or it lost quorum just as it was becoming primary,','line_number':1053,'multiline':False]['text':' don't run the sharding onStepUp machinery. The onStepDown counterpart to these','line_number':1054,'multiline':False]['text':' methods is already idempotent, so the machinery will remain in the stepped down','line_number':1055,'multiline':False]['text':' state.','line_number':1056,'multiline':False]['text':' (Ignore FCV check): TODO(SERVER-75389): add why FCV is ignored here.','line_number':1067,'multiline':False]['text':' Create indexes in config.shard.indexes if needed.','line_number':1069,'multiline':False]['text':' If the node is shutting down or it lost quorum just as it was becoming primary,','line_number':1073,'multiline':False]['text':' don't run the sharding onStepUp machinery. The onStepDown counterpart to these','line_number':1074,'multiline':False]['text':' methods is already idempotent, so the machinery will remain in the stepped down','line_number':1075,'multiline':False]['text':' state.','line_number':1076,'multiline':False]['text':' Create indexes in config.shard.collections if needed.','line_number':1090,'multiline':False]['text':' If the node is shutting down or it lost quorum just as it was becoming primary,','line_number':1093,'multiline':False]['text':' don't run the sharding onStepUp machinery. The onStepDown counterpart to these','line_number':1094,'multiline':False]['text':' methods is already idempotent, so the machinery will remain in the stepped down','line_number':1095,'multiline':False]['text':' state.','line_number':1096,'multiline':False]['text':' unsharded','line_number':1111,'multiline':False]['text':' Note this must be called after the config server has created the cluster ID and also','line_number':1119,'multiline':False]['text':' after the onStepUp logic for the shard role because this triggers sharding state','line_number':1120,'multiline':False]['text':' initialization which will transition some components into the "primary" state, like','line_number':1121,'multiline':False]['text':' the TransactionCoordinatorService, and they would fail if the onStepUp logic','line_number':1122,'multiline':False]['text':' attempted the same transition.','line_number':1123,'multiline':False]['text':' TODO: SERVER-82965 Remove condition after v8.0 becomes last-lts.','line_number':1125,'multiline':False]['text':' Acquire the GlobalLock in mode IX to conflict with database drops which acquire the','line_number':1181,'multiline':False]['text':' GlobalLock in mode X. Additionally, acquire the GlobalLock in IX instead of IS to prevent','line_number':1182,'multiline':False]['text':' lock upgrade when removing the temporary collections.','line_number':1183,'multiline':False]['text':' The local db is special because it isn't replicated. It is cleared at startup even on','line_number':1190,'multiline':False]['text':' replica set members.','line_number':1191,'multiline':False]['text':' Notify the DropPendingCollectionReaper if there are any drop-pending collections with','line_number':1232,'multiline':False]['text':' drop optimes before or at the committed optime.','line_number':1233,'multiline':False]['text':' This should never be called if the storage engine has not been initialized.','line_number':1267,'multiline':False]['text':' If in state PRIMARY, the oplogTruncateAfterPoint must be used for the Durable timestamp','line_number':1283,'multiline':False]['text':' in order to avoid majority confirming any writes that could later be truncated.','line_number':1284,'multiline':False]['text':' All other repl states use the 'lastApplied'.','line_number':1291,'multiline':False]['text':'','line_number':1292,'multiline':False]['text':' Setting 'rollbackSafe' will ensure that a safe lastApplied value is returned if we're in','line_number':1293,'multiline':False]['text':' ROLLBACK state. 'lastApplied' may be momentarily set to an opTime from a divergent branch','line_number':1294,'multiline':False]['text':' of history during rollback, so a benign default value will be returned instead to prevent','line_number':1295,'multiline':False]['text':' a divergent 'lastApplied' from being used to forward the 'lastDurable' after rollback.','line_number':1296,'multiline':False]['text':'','line_number':1297,'multiline':False]['text':' No concurrency control is necessary and it is still safe if the node goes into ROLLBACK','line_number':1298,'multiline':False]['text':' after getting the token because the JournalFlusher is shut down during rollback, before a','line_number':1299,'multiline':False]['text':' divergent 'lastApplied' value is present. The JournalFlusher will start up again in','line_number':1300,'multiline':False]['text':' ROLLBACK and never transition from non-ROLLBACK to ROLLBACK with a divergent','line_number':1301,'multiline':False]['text':' 'lastApplied' value.','line_number':1302,'multiline':False]['text':'rollbackSafe=','line_number':1304,'multiline':True]['text':' FCV is set to "5.0" or higher.','line_number':1347,'multiline':False]['text':' FCV is set to lower than "5.0".','line_number':1351,'multiline':False]['text':' namespace repl','line_number':1358,'multiline':False]['text':' namespace mongo','line_number':1359,'multiline':False]