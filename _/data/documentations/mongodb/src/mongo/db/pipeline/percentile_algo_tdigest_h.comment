['text':'*
 *    Copyright (C) 2023-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':'*
 * For the description of t-digest algorithm see
 * https://github.com/tdunning/t-digest/blob/main/docs/t-digest-paper/histo.pdf
 ','line_number':45,'multiline':True]['text':' q ','line_number':51,'multiline':True]['text':' delta ','line_number':51,'multiline':True]['text':'----------------------------------------------------------------------------------------------','line_number':53,'multiline':False]['text':' PercentileAlgorithm interface','line_number':54,'multiline':False]['text':'----------------------------------------------------------------------------------------------','line_number':55,'multiline':False]['text':'*
     * The raw input is buffered by TDigest and, when the buffer is full, merged into the
     * datastructure that represents prior inputs. The ammortized runtime complexity is
     * O(log(delta)) where 'delta' is the compaction paramenter of t-digest.
     * NaN values are ignored.
     ','line_number':57,'multiline':True]['text':'*
     * All values from 'inputs' are merged with the current buffer and processed at _once_. This
     * allows the clients to increase the default buffer size.
     ','line_number':65,'multiline':True]['text':'*
     * We compute percentile by linearly scanning centroids to find the one that matches the rank of
     * the requested percentile and then doing a linear interpolation between centroid means. We are
     * currently not optimizing for accessing multiple percentiles as we don't think that would
     * result in noticeable performance gains for accumulators and expressions should not be using
     * t-digest.
     ','line_number':71,'multiline':True]['text':'----------------------------------------------------------------------------------------------','line_number':83,'multiline':False]['text':' Implementation details of t-digest','line_number':84,'multiline':False]['text':'','line_number':85,'multiline':False]['text':' All methods are public to enable unit testing. The production clients should be accessing','line_number':86,'multiline':False]['text':' this class through the PercentileAlgorithm interface.','line_number':87,'multiline':False]['text':'----------------------------------------------------------------------------------------------','line_number':88,'multiline':False]['text':'*
     * Each centroid represents a "summary" of a few processed datapoints by storing the number of
     * the datapoints (the 'weight' of the centroid) and the average of their values (the 'mean').
     ','line_number':90,'multiline':True]['text':' The computation below ensures that the interpolated mean is within the bounds of the','line_number':97,'multiline':False]['text':' source means so long as the source weights aren't wildly imbalanced.','line_number':98,'multiline':False]['text':'','line_number':99,'multiline':False]['text':' Proof: let m,m1,m2 be the means of the combined, original and other centroid, and','line_number':100,'multiline':False]['text':' similarly w,w1,w2 for the weights.','line_number':101,'multiline':False]['text':'','line_number':102,'multiline':False]['text':' Let dm = m2-m1 and let d = (m2-m1)x(w2/w) be the correction to the mean m1 as computed','line_number':103,'multiline':False]['text':' by machine double operations, that is, m = m1+d. Rounding and multiplication by a','line_number':104,'multiline':False]['text':' positive constant can not change the sign of an operation so sgn(dm)=sgn(d) and therefore','line_number':105,'multiline':False]['text':' m lies on the correct side of m1. Thus computed m would lie on the correct side of m2 if','line_number':106,'multiline':False]['text':' |d| ≤ |dm|, which holds when the maximum relative error (1+eps) introduced by the','line_number':107,'multiline':False]['text':' rounding in (m2-m1) is compensated for by the multiplication by the interpolation','line_number':108,'multiline':False]['text':' parameter (w2/w), where eps is the maximum relative error for double float calculations,','line_number':109,'multiline':False]['text':' 2^-53. Since the maximum combined relative error introduced by the two machine','line_number':110,'multiline':False]['text':' calculations in (w2/w) is (1+eps)^2, require that (w2/w)(1+eps)^2 ≤ 1/(1+eps), or (w2/w)','line_number':111,'multiline':False]['text':' ≤ (1+eps)^3 ~ 1+3ε. Thus the caclculated combined mean can only lay outside the bounds of','line_number':112,'multiline':False]['text':' the source means if the original centroid weight w1 is less than about 3/2^53 of the','line_number':113,'multiline':False]['text':' combined weight w. We can avoid this restriction by flipping the computation based on','line_number':114,'multiline':False]['text':' which of the weights is higher.','line_number':115,'multiline':False]['text':' Semantically, 'weight' should be an integer. But 1) there's probably a cost to converting','line_number':132,'multiline':False]['text':' to doubles for each division, and 2) the weight range over which the calculations for','line_number':133,'multiline':False]['text':' adding centroids are accurate are (roughly) the range of doubles.','line_number':134,'multiline':False]['text':' The scaling functions. 'q' must be from [0.0, 1.0] and 'delta' must be positive. While there','line_number':139,'multiline':False]['text':' are no specific restrictions on 'delta', its expected to fall between 10^2 and 10^6.','line_number':140,'multiline':False]['text':'','line_number':141,'multiline':False]['text':' The inverse is on 'q' and treats 'delta' as parameter. The "limit" function is also','line_number':142,'multiline':False]['text':' parametrized by 'delta' and is equal to k_inverse(1 + k(q)) but might be folded for some of','line_number':143,'multiline':False]['text':' the scaling functions for better performance.','line_number':144,'multiline':False]['text':'','line_number':145,'multiline':False]['text':' k0 - a linear scaling function. Fast but not biased to give accurate results for extreme','line_number':146,'multiline':False]['text':' percentiles. Limits weight of all centroids to 2n/delta, where n is the number of processed','line_number':147,'multiline':False]['text':' inputs. Limits the number of centroids to delta.','line_number':148,'multiline':False]['text':'','line_number':149,'multiline':False]['text':' k1 - the scaling function from the paper. Expensive to compute. k1_limit() isn't foldable to','line_number':150,'multiline':False]['text':' a closed form. Limits the number of centroids to delta.','line_number':151,'multiline':False]['text':'','line_number':152,'multiline':False]['text':' k2 - a scaling function with asymptotic behaviour at 0 and 1 similar to k1, but cheaper to','line_number':153,'multiline':False]['text':' compute. Limits the number of centroids to 2*delta. Origin: Meta's Folly library.','line_number':154,'multiline':False]['text':'','line_number':155,'multiline':False]['text':' Perf notes: changing delta and buffer size affects how often k*_limit has to be computed so','line_number':156,'multiline':False]['text':' the gains from using a particular scaling function might vary considerably depending on the','line_number':157,'multiline':False]['text':' data pattern, delta and the buffer size. According to the micro-benchmarks run on normally','line_number':158,'multiline':False]['text':' distributed unsorted data with delta = 1000 and buffer_size = 5*delta:','line_number':159,'multiline':False]['text':'    1. The win from using k2 instead of k1 is ~6%.','line_number':160,'multiline':False]['text':'    2. The win from using k0 instead of k2 is ~3%.','line_number':161,'multiline':False]['text':' 1 / (2 * pi) = 0.159154943...','line_number':173,'multiline':False]['text':' Creates an empty digest.','line_number':203,'multiline':False]['text':' Creates a digest from the provided parts. It's the responsibility of the caller to','line_number':206,'multiline':False]['text':' ensure the resulting digest is valid.','line_number':207,'multiline':False]['text':' The ability to merge digests is needed for sharding, but when building a digest on a single','line_number':216,'multiline':False]['text':' node, merging the sorted data directly is faster. The micro-benchmarks show ~8% improvement','line_number':217,'multiline':False]['text':' on a dataset of size 10^7 when merging directly from a buffer (however, this amounts to ~65','line_number':218,'multiline':False]['text':' milliseconds, compared to ~7500 milliseconds of running the accumulator on a collection with','line_number':219,'multiline':False]['text':' 10^7 documents).','line_number':220,'multiline':False]['text':'','line_number':221,'multiline':False]['text':' The paper glosses over _why_ merging would produce a valid t-digest. Essentially, merging','line_number':222,'multiline':False]['text':' shouldn't be able to shift a centroid into an area of a fast growth of the scaling function.','line_number':223,'multiline':False]['text':' Some scaling functions support mergeability and others don't but the paper doesn't seem to','line_number':224,'multiline':False]['text':' formalize the exact criteria for mergeability.','line_number':225,'multiline':False]['text':' The input is assumed to be already sorted and not contain NaN or Infinity values. Neither of','line_number':228,'multiline':False]['text':' the assumptions are checked.','line_number':229,'multiline':False]['text':' Sorts data in the pending buffer and merges it with the prior centroids.','line_number':232,'multiline':False]['text':' The sizes of centroids are controlled by the scaling function and, conceptually, the','line_number':255,'multiline':False]['text':' algorithm can be implemented using k() alone. However, the scaling functions that allow for','line_number':256,'multiline':False]['text':' more accuracy of extreme percentiles are expensive to compute and the runtime can be','line_number':257,'multiline':False]['text':' improved by using during merge a derived from k() "limit" function instead of k() itself.','line_number':258,'multiline':False]['text':' The "compaction parameter". Defines the upper bound on the number of centroids and their','line_number':261,'multiline':False]['text':' sizes (see the specific scaling functions above for details).','line_number':262,'multiline':False]['text':' Buffer for the incoming inputs. When the buffer is full, the inputs are sorted and merged','line_number':265,'multiline':False]['text':' into '_centroids'. The max size is set in constructors to bufferCoeff * delta. The','line_number':266,'multiline':False]['text':' coefficient has been determined empirically from benchmarks.','line_number':267,'multiline':False]['text':' Centroids are ordered by their means. The ordering is maintained during merges.','line_number':272,'multiline':False]['text':' The number of inputs that are represented by '_centroids' (the number of incorporated inputs','line_number':275,'multiline':False]['text':' can be higher, if '_buffer' isn't empty and if any infinite values have been encountered).','line_number':276,'multiline':False]['text':' We are tracking infinities separately because, while they can be compared to other doubles','line_number':279,'multiline':False]['text':' with a mathematically expected result, no arithmetics can be done on them.','line_number':280,'multiline':False]['text':' Min and max values of the inputs that have been already merged into the centroids. We need to','line_number':284,'multiline':False]['text':' track these to interpolate the values of the extreme centroids and to answer 0.0 and 1.0','line_number':285,'multiline':False]['text':' percentiles accurately. The min and max of all incorporated inputs can be different, if','line_number':286,'multiline':False]['text':' '_buffer' isn't empty.','line_number':287,'multiline':False]['text':'*
 * Outputs json-like representation of the given digest, similar to:
 * {n: 6, min: -0.2, max: 9.2, s: 3, centroids: [{w: 2, m: 1.5}, {w: 3, m: 7.81}, {w: 1, m: 9.2}]}
 ','line_number':292,'multiline':True]['text':' namespace mongo','line_number':299,'multiline':False]