['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]['text':'*
 * Drops all chunks from the persisted metadata whether the collection's epoch has changed.
 ','line_number':115,'multiline':True]['text':' Drop the 'config.cache.chunks.<ns>' collection','line_number':126,'multiline':False]['text':'*
 * Takes a CollectionAndChangedChunks object and persists the changes to the shard's metadata
 * collections.
 *
 * Returns ConflictingOperationInProgress if a chunk is found with a new epoch.
 ','line_number':136,'multiline':True]['text':' Update the collection entry in case there are any updates.','line_number':146,'multiline':False]['text':' Mark as refreshing so secondaries are aware of it.','line_number':158,'multiline':False]['text':'upsert','line_number':165,'multiline':True]['text':' Mark the chunk metadata as done refreshing.','line_number':181,'multiline':False]['text':'*
 * Takes a DatabaseType object and persists the changes to the shard's metadata
 * collections.
 ','line_number':193,'multiline':True]['text':' Update the databases collection entry for 'dbName' in case there are any new updates.','line_number':198,'multiline':False]['text':'upsert','line_number':205,'multiline':True]['text':'*
 * This function will throw on error!
 *
 * Retrieves the persisted max chunk version for 'nss', if there are any persisted chunks. If there
 * are none -- meaning there's no persisted metadata for 'nss' --, returns a
 * ChunkVersion::UNSHARDED() version.
 *
 * It is unsafe to call this when a task for 'nss' is running concurrently because the collection
 * could be dropped and recreated or have its shard key refined between reading the collection epoch
 * and retrieving the chunk, which would make the returned ChunkVersion corrupt.
 ','line_number':213,'multiline':True]['text':' Must read the collections entry to get the epoch to pass into ChunkType for shard's chunk','line_number':225,'multiline':False]['text':' collection.','line_number':226,'multiline':False]['text':' There is no persisted metadata.','line_number':229,'multiline':False]['text':' Chunks was in the middle of refresh last time and we didn't finish cleanly. The version','line_number':240,'multiline':False]['text':' on the cached collection does not represent the maximum version in the cached chunks.','line_number':241,'multiline':False]['text':' Furthermore, since we don't bump the versions during refineShardKey and we don't store','line_number':242,'multiline':False]['text':' the epoch in the cache chunks, we can't tell whether the chunks are pre or post refined.','line_number':243,'multiline':False]['text':' Therefore, we have no choice but to just throw away the cache and start from scratch.','line_number':244,'multiline':False]['text':'*
 * This function will throw on error!
 *
 * Tries to find persisted chunk metadata with chunk versions GTE to 'version'.
 *
 * If 'version's epoch matches persisted metadata, returns persisted metadata GTE 'version'.
 * If 'version's epoch doesn't match persisted metadata, returns all persisted metadata.
 * If collections entry does not exist, throws NamespaceNotFound error. Can return an empty
 * chunks vector in CollectionAndChangedChunks without erroring, if collections entry IS found.
 ','line_number':266,'multiline':True]['text':' If the persisted epoch doesn't match what the CatalogCache requested, read everything.','line_number':282,'multiline':False]['text':' If the epochs are the same we can safely take the timestamp from the shard coll entry.','line_number':283,'multiline':False]['text':'*
 * Attempts to read the collection and chunk metadata. May not read a complete diff if the metadata
 * for the collection is being updated concurrently. This is safe if those updates are appended.
 *
 * If the epoch changes while reading the chunks, returns an empty object.
 ','line_number':312,'multiline':True]['text':' Found a collections entry, but the chunks are being updated.','line_number':325,'multiline':False]['text':' Make sure the collections entry epoch has not changed since we began reading chunks --','line_number':329,'multiline':False]['text':' an epoch change between reading the collections entry and reading the chunk metadata','line_number':330,'multiline':False]['text':' would invalidate the chunks.','line_number':331,'multiline':False]['text':' The collection was dropped and recreated or had its shard key refined since we began.','line_number':335,'multiline':False]['text':' Return empty results.','line_number':336,'multiline':False]['text':'*
 * Sends _flushDatabaseCacheUpdates to the primary to force it to refresh its routing table for
 * database 'dbName' and then waits for the refresh to replicate to this node.
 ','line_number':360,'multiline':True]['text':' namespace','line_number':412,'multiline':False]['text':' No need to increment the term since this interruption is only to prevent the secondary','line_number':464,'multiline':False]['text':' refresh thread from getting stuck or waiting on an incorrect opTime.','line_number':465,'multiline':False]['text':' Prevent further scheduling, then interrupt ongoing tasks.','line_number':480,'multiline':False]['text':' If the collecction is never registered on the sharding catalog there is no need to refresh.','line_number':496,'multiline':False]['text':' Further, attempting to refesh config.collections or config.chunks would trigger recursive','line_number':497,'multiline':False]['text':' refreshes, and, if this is running on a config server secondary, the refresh would not','line_number':498,'multiline':False]['text':' succeed if the primary is unavailable, unnecessarily reducing availability.','line_number':499,'multiline':False]['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':518,'multiline':False]['text':' We may have missed an OperationContextGroup interrupt since this operation','line_number':525,'multiline':False]['text':' began but before the OperationContext was added to the group. So we'll check','line_number':526,'multiline':False]['text':' that we're still in the same _term.','line_number':527,'multiline':False]['text':' The admin and config database have fixed metadata that does not need to be refreshed.','line_number':545,'multiline':False]['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':561,'multiline':False]['text':' We may have missed an OperationContextGroup interrupt since this operation began','line_number':568,'multiline':False]['text':' but before the OperationContext was added to the group. So we'll check that we're','line_number':569,'multiline':False]['text':' still in the same _term.','line_number':570,'multiline':False]['text':' If there are no tasks for the specified namespace, everything must have been completed','line_number':603,'multiline':False]['text':' Because of an optimization where a namespace drop clears all tasks except the','line_number':618,'multiline':False]['text':' active it is possible that the task number we are waiting on will never actually','line_number':619,'multiline':False]['text':' be written. Because of this we move the task number to the drop which can only be','line_number':620,'multiline':False]['text':' in the active task or in the one after the active.','line_number':621,'multiline':False]['text':' Wait for the active task to complete','line_number':632,'multiline':False]['text':' Increase the use_count of the condition variable shared pointer, because the entire','line_number':636,'multiline':False]['text':' task list might get deleted during the unlocked interval','line_number':637,'multiline':False]['text':' It is not safe to use taskList after this call, because it will unlock and lock the','line_number':640,'multiline':False]['text':' tasks mutex, so we just loop around.','line_number':641,'multiline':False]['text':' It is only correct to wait again on condVar if the taskNum has not changed, meaning','line_number':642,'multiline':False]['text':' that it must still be the same task list.','line_number':643,'multiline':False]['text':' If there are no tasks for the specified namespace, everything must have been completed','line_number':670,'multiline':False]['text':' Because of an optimization where a namespace drop clears all tasks except the','line_number':685,'multiline':False]['text':' active it is possible that the task number we are waiting on will never actually','line_number':686,'multiline':False]['text':' be written. Because of this we move the task number to the drop which can only be','line_number':687,'multiline':False]['text':' in the active task or in the one after the active.','line_number':688,'multiline':False]['text':' The task is for a drop.','line_number':690,'multiline':False]['text':' Wait for the active task to complete','line_number':700,'multiline':False]['text':' Increase the use_count of the condition variable shared pointer, because the entire','line_number':704,'multiline':False]['text':' task list might get deleted during the unlocked interval','line_number':705,'multiline':False]['text':' It is not safe to use taskList after this call, because it will unlock and lock the','line_number':708,'multiline':False]['text':' tasks mutex, so we just loop around.','line_number':709,'multiline':False]['text':' It is only correct to wait again on condVar if the taskNum has not changed, meaning','line_number':710,'multiline':False]['text':' that it must still be the same task list.','line_number':711,'multiline':False]['text':' Read the local metadata.','line_number':734,'multiline':False]['text':' Get the max version the loader has.','line_number':745,'multiline':False]['text':' Enqueued tasks have the latest metadata','line_number':753,'multiline':False]['text':' If there are no enqueued tasks, get the max persisted','line_number':758,'multiline':False]['text':' Refresh the loader's metadata from the config server. The caller's request will','line_number':762,'multiline':False]['text':' then be serviced from the loader's up-to-date metadata.','line_number':763,'multiline':False]['text':' Metadata was found remotely','line_number':815,'multiline':False]['text':' -- otherwise we would have received NamespaceNotFound rather than Status::OK().','line_number':816,'multiline':False]['text':' Return metadata for CatalogCache that's GTE catalogCacheSinceVersion,','line_number':817,'multiline':False]['text':' from the loader's persisted and enqueued metadata.','line_number':818,'multiline':False]['text':' Raising a ConflictingOperationInProgress error here will cause the CatalogCache','line_number':832,'multiline':False]['text':' to attempt the refresh as secondary instead of failing the operation','line_number':833,'multiline':False]['text':' After finding metadata remotely, we must have found metadata locally.','line_number':839,'multiline':False]['text':' Get the enqueued metadata first. Otherwise we could miss data between reading persisted and','line_number':909,'multiline':False]['text':' enqueued, if an enqueued task finished after the persisted read but before the enqueued read.','line_number':910,'multiline':False]['text':' No persisted metadata found','line_number':918,'multiline':False]['text':' There are no tasks in the queue. Return the persisted metadata.','line_number':949,'multiline':False]['text':' There is a task in the queue and:','line_number':953,'multiline':False]['text':' - nothing is persisted, OR','line_number':954,'multiline':False]['text':' - the last task in the queue was a drop, OR','line_number':955,'multiline':False]['text':' - the epoch changed in the enqueued metadata.','line_number':956,'multiline':False]['text':' Whichever the cause, the persisted metadata is out-dated/non-existent. Return enqueued','line_number':957,'multiline':False]['text':' results.','line_number':958,'multiline':False]['text':' There can be overlap between persisted and enqueued metadata because enqueued work can','line_number':962,'multiline':False]['text':' be applied while persisted was read. We must remove this overlap.','line_number':963,'multiline':False]['text':' Remove chunks from 'persisted' that are GTE the minimum in 'enqueued' -- this is','line_number':967,'multiline':False]['text':' the overlap.','line_number':968,'multiline':False]['text':' Append 'enqueued's chunks to 'persisted', which no longer overlaps','line_number':976,'multiline':False]['text':' The collection info in enqueued metadata may be more recent than the persisted metadata','line_number':982,'multiline':False]['text':' If task list does not have a term that matches, there's no valid task data to collect.','line_number':1003,'multiline':False]['text':' Only return task data of tasks scheduled in the same term as the given 'term': older term','line_number':1007,'multiline':False]['text':' task data is no longer valid.','line_number':1008,'multiline':False]['text':' Return all the results if 'catalogCacheSinceVersion's epoch does not match. Otherwise, trim','line_number':1011,'multiline':False]['text':' the results to be GTE to 'catalogCacheSinceVersion'.','line_number':1012,'multiline':False]['text':' Ensure that this node is primary before using or persisting the information fetched from the','line_number':1031,'multiline':False]['text':' config server. This prevents using incorrect filtering information in split brain scenarios.','line_number':1032,'multiline':False]['text':' Ensure that this node is primary before using or persisting the information fetched from the','line_number':1061,'multiline':False]['text':' config server. This prevents using incorrect filtering information in split brain scenarios.','line_number':1062,'multiline':False]['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':1093,'multiline':False]['text':' If task completed successfully, remove it from work queue.','line_number':1124,'multiline':False]['text':' Return if have no more work','line_number':1129,'multiline':False]['text':' If shutting down need to remove tasks to end waiting on its completion.','line_number':1135,'multiline':False]['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':1174,'multiline':False]['text':' If task completed successfully, remove it from work queue.','line_number':1203,'multiline':False]['text':' Return if have no more work','line_number':1208,'multiline':False]['text':' If shutting down need to remove tasks to end waiting on its completion.','line_number':1214,'multiline':False]['text':' If this task is from an old term and no longer valid, do not execute and return true so that','line_number':1257,'multiline':False]['text':' the task gets removed from the task list','line_number':1258,'multiline':False]['text':' Check if this is a drop task','line_number':1265,'multiline':False]['text':' The namespace was dropped. The persisted metadata for the collection must be cleared.','line_number':1267,'multiline':False]['text':' If this task is from an old term and no longer valid, do not execute and return true so that','line_number':1296,'multiline':False]['text':' the task gets removed from the task list','line_number':1297,'multiline':False]['text':' Check if this is a drop task','line_number':1304,'multiline':False]['text':' The database was dropped. The persisted metadata for the collection must be cleared.','line_number':1306,'multiline':False]['text':' Keep trying to load the metadata until we get a complete view without updates being','line_number':1355,'multiline':False]['text':' concurrently applied.','line_number':1356,'multiline':False]['text':' Blocking call to wait for the notification, get the most recent value, and','line_number':1366,'multiline':False]['text':' recreate the notification under lock so that we don't miss any notifications.','line_number':1367,'multiline':False]['text':' Wait until the local lastApplied timestamp is the one from the notification.','line_number':1369,'multiline':False]['text':' Load the metadata.','line_number':1375,'multiline':False]['text':' Check that no updates were concurrently applied while we were loading the metadata: this','line_number':1379,'multiline':False]['text':' could cause the loaded metadata to provide an incomplete view of the chunk ranges.','line_number':1380,'multiline':False]['text':' As an optimization, on collection drop, clear any pending tasks in order to prevent any','line_number':1458,'multiline':False]['text':' throw-away work from executing. Because we have no way to differentiate whether the','line_number':1459,'multiline':False]['text':' active tasks is currently being operated on by a thread or not, we must leave the front','line_number':1460,'multiline':False]['text':' intact.','line_number':1461,'multiline':False]['text':' No need to schedule a drop if one is already currently active.','line_number':1464,'multiline':False]['text':' Tasks must have contiguous versions, unless a complete reload occurs.','line_number':1469,'multiline':False]['text':' As an optimization, on database drop, clear any pending tasks in order to prevent any','line_number':1487,'multiline':False]['text':' throw-away work from executing. Because we have no way to differentiate whether the','line_number':1488,'multiline':False]['text':' active tasks is currently being operated on by a thread or not, we must leave the front','line_number':1489,'multiline':False]['text':' intact.','line_number':1490,'multiline':False]['text':' No need to schedule a drop if one is already currently active.','line_number':1493,'multiline':False]['text':' Task data is no longer valid. Go on to the next task in the list.','line_number':1532,'multiline':False]['text':' The current task is a drop -> the aggregated results aren't interesting so we','line_number':1537,'multiline':False]['text':' overwrite the CollAndChangedChunks and unset the flag','line_number':1538,'multiline':False]['text':' The current task has a new epoch -> the aggregated results aren't interesting so we','line_number':1541,'multiline':False]['text':' overwrite them','line_number':1542,'multiline':False]['text':' The current task is not a drop neither an update and the epochs match -> we add its','line_number':1545,'multiline':False]['text':' results to the aggregated results','line_number':1546,'multiline':False]['text':' Make sure we do not append a duplicate chunk. The diff query is GTE, so there can','line_number':1548,'multiline':False]['text':' be duplicates of the same exact versioned chunk across tasks. This is no problem','line_number':1549,'multiline':False]['text':' for our diff application algorithms, but it can return unpredictable numbers of','line_number':1550,'multiline':False]['text':' chunks for testing purposes. Eliminate unpredictable duplicates for testing','line_number':1551,'multiline':False]['text':' stability.','line_number':1552,'multiline':False]['text':' Keep the most recent version of these fields','line_number':1566,'multiline':False]['text':' namespace mongo','line_number':1576,'multiline':False]