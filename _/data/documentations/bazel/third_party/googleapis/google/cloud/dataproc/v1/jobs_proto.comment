['text':' Copyright 2016 Google Inc.','line_number':1,'multiline':False]['text':'','line_number':2,'multiline':False]['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]['text':'','line_number':6,'multiline':False]['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]['text':'','line_number':8,'multiline':False]['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]['text':' limitations under the License.','line_number':13,'multiline':False]['text':' The JobController provides methods to manage jobs.','line_number':29,'multiline':False]['text':' Submits a job to a cluster.','line_number':31,'multiline':False]['text':' Gets the resource representation for a job in a project.','line_number':36,'multiline':False]['text':' Lists regions/{region}/jobs in a project.','line_number':41,'multiline':False]['text':' Starts a job cancellation request. To access the job resource','line_number':46,'multiline':False]['text':' after cancellation, call','line_number':47,'multiline':False]['text':' [regions/{region}/jobs.list](/dataproc/reference/rest/v1/projects.regions.jobs/list) or','line_number':48,'multiline':False]['text':' [regions/{region}/jobs.get](/dataproc/reference/rest/v1/projects.regions.jobs/get).','line_number':49,'multiline':False]['text':' Deletes the job from the project. If the job is active, the delete fails,','line_number':54,'multiline':False]['text':' and the response returns `FAILED_PRECONDITION`.','line_number':55,'multiline':False]['text':' The runtime logging config of the job.','line_number':61,'multiline':False]['text':' The Log4j level for job execution. When running an','line_number':63,'multiline':False]['text':' [Apache Hive](http://hive.apache.org/) job, Cloud','line_number':64,'multiline':False]['text':' Dataproc configures the Hive client to an equivalent verbosity level.','line_number':65,'multiline':False]['text':' Level is unspecified. Use default level for log4j.','line_number':67,'multiline':False]['text':' Use ALL level for log4j.','line_number':70,'multiline':False]['text':' Use TRACE level for log4j.','line_number':73,'multiline':False]['text':' Use DEBUG level for log4j.','line_number':76,'multiline':False]['text':' Use INFO level for log4j.','line_number':79,'multiline':False]['text':' Use WARN level for log4j.','line_number':82,'multiline':False]['text':' Use ERROR level for log4j.','line_number':85,'multiline':False]['text':' Use FATAL level for log4j.','line_number':88,'multiline':False]['text':' Turn off log4j.','line_number':91,'multiline':False]['text':' The per-package log levels for the driver. This may include','line_number':95,'multiline':False]['text':' "root" package name to configure rootLogger.','line_number':96,'multiline':False]['text':' Examples:','line_number':97,'multiline':False]['text':'   'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'','line_number':98,'multiline':False]['text':' A Cloud Dataproc job for running','line_number':102,'multiline':False]['text':' [Apache Hadoop MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)','line_number':103,'multiline':False]['text':' jobs on [Apache Hadoop YARN](https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).','line_number':104,'multiline':False]['text':' [Required] Indicates the location of the driver's main class. Specify','line_number':106,'multiline':False]['text':' either the jar file that contains the main class or the main class name.','line_number':107,'multiline':False]['text':' To specify both, add the jar file to `jar_file_uris`, and then specify','line_number':108,'multiline':False]['text':' the main class name in this property.','line_number':109,'multiline':False]['text':' The HCFS URI of the jar file containing the main class.','line_number':111,'multiline':False]['text':' Examples:','line_number':112,'multiline':False]['text':'     'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'','line_number':113,'multiline':False]['text':'     'hdfs:/tmp/test-samples/custom-wordcount.jar'','line_number':114,'multiline':False]['text':'     'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'','line_number':115,'multiline':False]['text':' The name of the driver's main class. The jar file containing the class','line_number':118,'multiline':False]['text':' must be in the default CLASSPATH or specified in `jar_file_uris`.','line_number':119,'multiline':False]['text':' [Optional] The arguments to pass to the driver. Do not','line_number':123,'multiline':False]['text':' include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job','line_number':124,'multiline':False]['text':' properties, since a collision may occur that causes an incorrect job','line_number':125,'multiline':False]['text':' submission.','line_number':126,'multiline':False]['text':' [Optional] Jar file URIs to add to the CLASSPATHs of the','line_number':129,'multiline':False]['text':' Hadoop driver and tasks.','line_number':130,'multiline':False]['text':' [Optional] HCFS (Hadoop Compatible Filesystem) URIs of files to be copied','line_number':133,'multiline':False]['text':' to the working directory of Hadoop drivers and distributed tasks. Useful','line_number':134,'multiline':False]['text':' for naively parallel tasks.','line_number':135,'multiline':False]['text':' [Optional] HCFS URIs of archives to be extracted in the working directory of','line_number':138,'multiline':False]['text':' Hadoop drivers and tasks. Supported file types:','line_number':139,'multiline':False]['text':' .jar, .tar, .tar.gz, .tgz, or .zip.','line_number':140,'multiline':False]['text':' [Optional] A mapping of property names to values, used to configure Hadoop.','line_number':143,'multiline':False]['text':' Properties that conflict with values set by the Cloud Dataproc API may be','line_number':144,'multiline':False]['text':' overwritten. Can include properties set in /etc/hadoop/conf/*-site and','line_number':145,'multiline':False]['text':' classes in user code.','line_number':146,'multiline':False]['text':' [Optional] The runtime log config for job execution.','line_number':149,'multiline':False]['text':' A Cloud Dataproc job for running [Apache Spark](http://spark.apache.org/)','line_number':153,'multiline':False]['text':' applications on YARN.','line_number':154,'multiline':False]['text':' [Required] The specification of the main method to call to drive the job.','line_number':156,'multiline':False]['text':' Specify either the jar file that contains the main class or the main class','line_number':157,'multiline':False]['text':' name. To pass both a main jar and a main class in that jar, add the jar to','line_number':158,'multiline':False]['text':' `CommonJob.jar_file_uris`, and then specify the main class name in `main_class`.','line_number':159,'multiline':False]['text':' The HCFS URI of the jar file that contains the main class.','line_number':161,'multiline':False]['text':' The name of the driver's main class. The jar file that contains the class','line_number':164,'multiline':False]['text':' must be in the default CLASSPATH or specified in `jar_file_uris`.','line_number':165,'multiline':False]['text':' [Optional] The arguments to pass to the driver. Do not include arguments,','line_number':169,'multiline':False]['text':' such as `--conf`, that can be set as job properties, since a collision may','line_number':170,'multiline':False]['text':' occur that causes an incorrect job submission.','line_number':171,'multiline':False]['text':' [Optional] HCFS URIs of jar files to add to the CLASSPATHs of the','line_number':174,'multiline':False]['text':' Spark driver and tasks.','line_number':175,'multiline':False]['text':' [Optional] HCFS URIs of files to be copied to the working directory of','line_number':178,'multiline':False]['text':' Spark drivers and distributed tasks. Useful for naively parallel tasks.','line_number':179,'multiline':False]['text':' [Optional] HCFS URIs of archives to be extracted in the working directory','line_number':182,'multiline':False]['text':' of Spark drivers and tasks. Supported file types:','line_number':183,'multiline':False]['text':' .jar, .tar, .tar.gz, .tgz, and .zip.','line_number':184,'multiline':False]['text':' [Optional] A mapping of property names to values, used to configure Spark.','line_number':187,'multiline':False]['text':' Properties that conflict with values set by the Cloud Dataproc API may be','line_number':188,'multiline':False]['text':' overwritten. Can include properties set in','line_number':189,'multiline':False]['text':' /etc/spark/conf/spark-defaults.conf and classes in user code.','line_number':190,'multiline':False]['text':' [Optional] The runtime log config for job execution.','line_number':193,'multiline':False]['text':' A Cloud Dataproc job for running','line_number':197,'multiline':False]['text':' [Apache PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)','line_number':198,'multiline':False]['text':' applications on YARN.','line_number':199,'multiline':False]['text':' [Required] The HCFS URI of the main Python file to use as the driver. Must','line_number':201,'multiline':False]['text':' be a .py file.','line_number':202,'multiline':False]['text':' [Optional] The arguments to pass to the driver.  Do not include arguments,','line_number':205,'multiline':False]['text':' such as `--conf`, that can be set as job properties, since a collision may','line_number':206,'multiline':False]['text':' occur that causes an incorrect job submission.','line_number':207,'multiline':False]['text':' [Optional] HCFS file URIs of Python files to pass to the PySpark','line_number':210,'multiline':False]['text':' framework. Supported file types: .py, .egg, and .zip.','line_number':211,'multiline':False]['text':' [Optional] HCFS URIs of jar files to add to the CLASSPATHs of the','line_number':214,'multiline':False]['text':' Python driver and tasks.','line_number':215,'multiline':False]['text':' [Optional] HCFS URIs of files to be copied to the working directory of','line_number':218,'multiline':False]['text':' Python drivers and distributed tasks. Useful for naively parallel tasks.','line_number':219,'multiline':False]['text':' [Optional] HCFS URIs of archives to be extracted in the working directory of','line_number':222,'multiline':False]['text':' .jar, .tar, .tar.gz, .tgz, and .zip.','line_number':223,'multiline':False]['text':' [Optional] A mapping of property names to values, used to configure PySpark.','line_number':226,'multiline':False]['text':' Properties that conflict with values set by the Cloud Dataproc API may be','line_number':227,'multiline':False]['text':' overwritten. Can include properties set in','line_number':228,'multiline':False]['text':' /etc/spark/conf/spark-defaults.conf and classes in user code.','line_number':229,'multiline':False]['text':' [Optional] The runtime log config for job execution.','line_number':232,'multiline':False]['text':' A list of queries to run on a cluster.','line_number':236,'multiline':False]['text':' [Required] The queries to execute. You do not need to terminate a query','line_number':238,'multiline':False]['text':' with a semicolon. Multiple queries can be specified in one string','line_number':239,'multiline':False]['text':' by separating each with a semicolon. Here is an example of an Cloud','line_number':240,'multiline':False]['text':' Dataproc API snippet that uses a QueryList to specify a HiveJob:','line_number':241,'multiline':False]['text':'','line_number':242,'multiline':False]['text':'     "hiveJob": {','line_number':243,'multiline':False]['text':'       "queryList": {','line_number':244,'multiline':False]['text':'         "queries": [','line_number':245,'multiline':False]['text':'           "query1",','line_number':246,'multiline':False]['text':'           "query2",','line_number':247,'multiline':False]['text':'           "query3;query4",','line_number':248,'multiline':False]['text':'         ]','line_number':249,'multiline':False]['text':'       }','line_number':250,'multiline':False]['text':'     }','line_number':251,'multiline':False]['text':' A Cloud Dataproc job for running [Apache Hive](https://hive.apache.org/)','line_number':255,'multiline':False]['text':' queries on YARN.','line_number':256,'multiline':False]['text':' [Required] The sequence of Hive queries to execute, specified as either','line_number':258,'multiline':False]['text':' an HCFS file URI or a list of queries.','line_number':259,'multiline':False]['text':' The HCFS URI of the script that contains Hive queries.','line_number':261,'multiline':False]['text':' A list of queries.','line_number':264,'multiline':False]['text':' [Optional] Whether to continue executing queries if a query fails.','line_number':268,'multiline':False]['text':' The default value is `false`. Setting to `true` can be useful when executing','line_number':269,'multiline':False]['text':' independent parallel queries.','line_number':270,'multiline':False]['text':' [Optional] Mapping of query variable names to values (equivalent to the','line_number':273,'multiline':False]['text':' Hive command: `SET name="value";`).','line_number':274,'multiline':False]['text':' [Optional] A mapping of property names and values, used to configure Hive.','line_number':277,'multiline':False]['text':' Properties that conflict with values set by the Cloud Dataproc API may be','line_number':278,'multiline':False]['text':' overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,','line_number':279,'multiline':False]['text':' /etc/hive/conf/hive-site.xml, and classes in user code.','line_number':280,'multiline':False]['text':' [Optional] HCFS URIs of jar files to add to the CLASSPATH of the','line_number':283,'multiline':False]['text':' Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes','line_number':284,'multiline':False]['text':' and UDFs.','line_number':285,'multiline':False]['text':' A Cloud Dataproc job for running [Apache Spark SQL](http://spark.apache.org/sql/)','line_number':289,'multiline':False]['text':' queries.','line_number':290,'multiline':False]['text':' [Required] The sequence of Spark SQL queries to execute, specified as','line_number':292,'multiline':False]['text':' either an HCFS file URI or as a list of queries.','line_number':293,'multiline':False]['text':' The HCFS URI of the script that contains SQL queries.','line_number':295,'multiline':False]['text':' A list of queries.','line_number':298,'multiline':False]['text':' [Optional] Mapping of query variable names to values (equivalent to the','line_number':302,'multiline':False]['text':' Spark SQL command: SET `name="value";`).','line_number':303,'multiline':False]['text':' [Optional] A mapping of property names to values, used to configure','line_number':306,'multiline':False]['text':' Spark SQL's SparkConf. Properties that conflict with values set by the','line_number':307,'multiline':False]['text':' Cloud Dataproc API may be overwritten.','line_number':308,'multiline':False]['text':' [Optional] HCFS URIs of jar files to be added to the Spark CLASSPATH.','line_number':311,'multiline':False]['text':' [Optional] The runtime log config for job execution.','line_number':314,'multiline':False]['text':' A Cloud Dataproc job for running [Apache Pig](https://pig.apache.org/)','line_number':318,'multiline':False]['text':' queries on YARN.','line_number':319,'multiline':False]['text':' [Required] The sequence of Pig queries to execute, specified as an HCFS','line_number':321,'multiline':False]['text':' file URI or a list of queries.','line_number':322,'multiline':False]['text':' The HCFS URI of the script that contains the Pig queries.','line_number':324,'multiline':False]['text':' A list of queries.','line_number':327,'multiline':False]['text':' [Optional] Whether to continue executing queries if a query fails.','line_number':331,'multiline':False]['text':' The default value is `false`. Setting to `true` can be useful when executing','line_number':332,'multiline':False]['text':' independent parallel queries.','line_number':333,'multiline':False]['text':' [Optional] Mapping of query variable names to values (equivalent to the Pig','line_number':336,'multiline':False]['text':' command: `name=[value]`).','line_number':337,'multiline':False]['text':' [Optional] A mapping of property names to values, used to configure Pig.','line_number':340,'multiline':False]['text':' Properties that conflict with values set by the Cloud Dataproc API may be','line_number':341,'multiline':False]['text':' overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,','line_number':342,'multiline':False]['text':' /etc/pig/conf/pig.properties, and classes in user code.','line_number':343,'multiline':False]['text':' [Optional] HCFS URIs of jar files to add to the CLASSPATH of','line_number':346,'multiline':False]['text':' the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.','line_number':347,'multiline':False]['text':' [Optional] The runtime log config for job execution.','line_number':350,'multiline':False]['text':' Cloud Dataproc job config.','line_number':354,'multiline':False]['text':' [Required] The name of the cluster where the job will be submitted.','line_number':356,'multiline':False]['text':' [Output-only] A cluster UUID generated by the Cloud Dataproc service when','line_number':359,'multiline':False]['text':' the job is submitted.','line_number':360,'multiline':False]['text':' Cloud Dataproc job status.','line_number':364,'multiline':False]['text':' The job state.','line_number':366,'multiline':False]['text':' The job state is unknown.','line_number':368,'multiline':False]['text':' The job is pending; it has been submitted, but is not yet running.','line_number':371,'multiline':False]['text':' Job has been received by the service and completed initial setup;','line_number':374,'multiline':False]['text':' it will soon be submitted to the cluster.','line_number':375,'multiline':False]['text':' The job is running on the cluster.','line_number':378,'multiline':False]['text':' A CancelJob request has been received, but is pending.','line_number':381,'multiline':False]['text':' Transient in-flight resources have been canceled, and the request to','line_number':384,'multiline':False]['text':' cancel the running job has been issued to the cluster.','line_number':385,'multiline':False]['text':' The job cancellation was successful.','line_number':388,'multiline':False]['text':' The job has completed successfully.','line_number':391,'multiline':False]['text':' The job has completed, but encountered an error.','line_number':394,'multiline':False]['text':' [Output-only] A state message specifying the overall job state.','line_number':398,'multiline':False]['text':' [Output-only] Optional job state details, such as an error','line_number':401,'multiline':False]['text':' description if the state is <code>ERROR</code>.','line_number':402,'multiline':False]['text':' [Output-only] The time when this state was entered.','line_number':405,'multiline':False]['text':' Encapsulates the full scoping used to reference a job.','line_number':409,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':411,'multiline':False]['text':' belongs to.','line_number':412,'multiline':False]['text':' [Optional] The job ID, which must be unique within the project. The job ID','line_number':415,'multiline':False]['text':' is generated by the server upon job submission or provided by the user as a','line_number':416,'multiline':False]['text':' means to perform retries without creating duplicate jobs. The ID must','line_number':417,'multiline':False]['text':' contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or','line_number':418,'multiline':False]['text':' hyphens (-). The maximum length is 512 characters.','line_number':419,'multiline':False]['text':' A Cloud Dataproc job resource.','line_number':423,'multiline':False]['text':' [Optional] The fully qualified reference to the job, which can be used to','line_number':425,'multiline':False]['text':' obtain the equivalent REST path of the job resource. If this property','line_number':426,'multiline':False]['text':' is not specified when a job is created, the server generates a','line_number':427,'multiline':False]['text':' <code>job_id</code>.','line_number':428,'multiline':False]['text':' [Required] Job information, including how, when, and where to','line_number':431,'multiline':False]['text':' run the job.','line_number':432,'multiline':False]['text':' [Required] The application/framework-specific portion of the job.','line_number':435,'multiline':False]['text':' Job is a Hadoop job.','line_number':437,'multiline':False]['text':' Job is a Spark job.','line_number':440,'multiline':False]['text':' Job is a Pyspark job.','line_number':443,'multiline':False]['text':' Job is a Hive job.','line_number':446,'multiline':False]['text':' Job is a Pig job.','line_number':449,'multiline':False]['text':' Job is a SparkSql job.','line_number':452,'multiline':False]['text':' [Output-only] The job status. Additional application-specific','line_number':456,'multiline':False]['text':' status information may be contained in the <code>type_job</code>','line_number':457,'multiline':False]['text':' and <code>yarn_applications</code> fields.','line_number':458,'multiline':False]['text':' [Output-only] The previous job status.','line_number':461,'multiline':False]['text':' [Output-only] A URI pointing to the location of the stdout of the job's','line_number':464,'multiline':False]['text':' driver program.','line_number':465,'multiline':False]['text':' [Output-only] If present, the location of miscellaneous control files','line_number':468,'multiline':False]['text':' which may be used as part of job setup and handling. If not present,','line_number':469,'multiline':False]['text':' control files may be placed in the same location as `driver_output_uri`.','line_number':470,'multiline':False]['text':' A request to submit a job.','line_number':474,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':476,'multiline':False]['text':' belongs to.','line_number':477,'multiline':False]['text':' [Required] The Cloud Dataproc region in which to handle the request.','line_number':480,'multiline':False]['text':' [Required] The job resource.','line_number':483,'multiline':False]['text':' A request to get the resource representation for a job in a project.','line_number':487,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':489,'multiline':False]['text':' belongs to.','line_number':490,'multiline':False]['text':' [Required] The Cloud Dataproc region in which to handle the request.','line_number':493,'multiline':False]['text':' [Required] The job ID.','line_number':496,'multiline':False]['text':' A request to list jobs in a project.','line_number':500,'multiline':False]['text':' A matcher that specifies categories of job states.','line_number':502,'multiline':False]['text':' Match all jobs, regardless of state.','line_number':504,'multiline':False]['text':' Only match jobs in non-terminal states: PENDING, RUNNING, or','line_number':507,'multiline':False]['text':' CANCEL_PENDING.','line_number':508,'multiline':False]['text':' Only match jobs in terminal states: CANCELLED, DONE, or ERROR.','line_number':511,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':515,'multiline':False]['text':' belongs to.','line_number':516,'multiline':False]['text':' [Required] The Cloud Dataproc region in which to handle the request.','line_number':519,'multiline':False]['text':' [Optional] The number of results to return in each response.','line_number':522,'multiline':False]['text':' [Optional] The page token, returned by a previous call, to request the','line_number':525,'multiline':False]['text':' next page of results.','line_number':526,'multiline':False]['text':' [Optional] If set, the returned jobs list includes only jobs that were','line_number':529,'multiline':False]['text':' submitted to the named cluster.','line_number':530,'multiline':False]['text':' [Optional] Specifies enumerated categories of jobs to list','line_number':533,'multiline':False]['text':' (default = match ALL jobs).','line_number':534,'multiline':False]['text':' A list of jobs in a project.','line_number':538,'multiline':False]['text':' [Output-only] Jobs list.','line_number':540,'multiline':False]['text':' [Optional] This token is included in the response if there are more results','line_number':543,'multiline':False]['text':' to fetch. To fetch additional results, provide this value as the','line_number':544,'multiline':False]['text':' `page_token` in a subsequent <code>ListJobsRequest</code>.','line_number':545,'multiline':False]['text':' A request to cancel a job.','line_number':549,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':551,'multiline':False]['text':' belongs to.','line_number':552,'multiline':False]['text':' [Required] The Cloud Dataproc region in which to handle the request.','line_number':555,'multiline':False]['text':' [Required] The job ID.','line_number':558,'multiline':False]['text':' A request to delete a job.','line_number':562,'multiline':False]['text':' [Required] The ID of the Google Cloud Platform project that the job','line_number':564,'multiline':False]['text':' belongs to.','line_number':565,'multiline':False]['text':' [Required] The Cloud Dataproc region in which to handle the request.','line_number':568,'multiline':False]['text':' [Required] The job ID.','line_number':571,'multiline':False]