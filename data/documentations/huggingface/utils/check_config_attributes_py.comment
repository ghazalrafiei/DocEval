['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' All paths are set with the intent you should run this script from the root of the repo with the command','line_number':24,'multiline':False]
['text':' python utils/check_config_docstrings.py','line_number':25,'multiline':False]
['text':' This is to make sure the transformers module imported is the one in the repo.','line_number':29,'multiline':False]
['text':' used to compute the property `self.chunk_length`','line_number':35,'multiline':False]
['text':' used as `self.bert_model = BertModel(config, ...)`','line_number':37,'multiline':False]
['text':' not used in modeling files, but it's an important information','line_number':40,'multiline':False]
['text':' used internally in the configuration class file','line_number':42,'multiline':False]
['text':' used internally in the configuration class file','line_number':44,'multiline':False]
['text':' used during training (despite we don't have training script for these models yet)','line_number':46,'multiline':False]
['text':' `ignore_value` used during training (despite we don't have training script for these models yet)','line_number':48,'multiline':False]
['text':' `norm` used in conversion script (despite not using in the modeling file)','line_number':49,'multiline':False]
['text':' used during preprocessing and collation, see `collating_graphormer.py`','line_number':51,'multiline':False]
['text':' used internally in the configuration class file','line_number':53,'multiline':False]
['text':' used internally in the configuration class file','line_number':55,'multiline':False]
['text':' `tokenizer_class` get default value `T5Tokenizer` intentionally','line_number':56,'multiline':False]
['text':' used internally in the configuration class file','line_number':59,'multiline':False]
['text':' used internally in the configuration class file','line_number':61,'multiline':False]
['text':' used internally in the configuration class file','line_number':63,'multiline':False]
['text':' having default values other than `1e-5` - we can't fix them without breaking','line_number':65,'multiline':False]
['text':' having default values other than `1e-5` - we can't fix them without breaking','line_number':67,'multiline':False]
['text':' having default values other than `1e-5` - we can't fix them without breaking','line_number':69,'multiline':False]
['text':' having default values other than `1e-5` - we can't fix them without breaking','line_number':71,'multiline':False]
['text':' having default values other than `1e-5` - we can't fix them without breaking','line_number':73,'multiline':False]
['text':' used internally to calculate the feature size','line_number':75,'multiline':False]
['text':' used internally to calculate the feature size','line_number':77,'multiline':False]
['text':' used internally to calculate the feature size','line_number':79,'multiline':False]
['text':' used internally to calculate `mlp_dim`','line_number':81,'multiline':False]
['text':' For (head) training, but so far not implemented','line_number':83,'multiline':False]
['text':' Not used, but providing useful information to users','line_number':85,'multiline':False]
['text':' Actually used in the config or generation config, in that case necessary for the sub-components generation','line_number':87,'multiline':False]
['text':' Actually used in the config or generation config, in that case necessary for the sub-components generation','line_number':99,'multiline':False]
['text':' TODO (ydshieh): Check the failing cases, try to fix them or move some cases to the above block once we are sure','line_number':117,'multiline':False]
['text':' For backward compatibility with trust remote code models','line_number':131,'multiline':False]
['text':' TODO: @Arthur (for `alignment_head` and `alignment_layer`)','line_number':149,'multiline':False]
['text':' TODO: @Younes (for `is_decoder`)','line_number':151,'multiline':False]
['text':' check if we can find `config.xxx`, `getattr(config, "xxx", ...)` or `getattr(self.config, "xxx", ...)`','line_number':177,'multiline':False]
['text':' Deal with multi-line cases','line_number':184,'multiline':False]
['text':' `SequenceSummary` is called with `SequenceSummary(config)`','line_number':193,'multiline':False]
['text':' common and important attributes, even if they do not always appear in the modeling files','line_number':209,'multiline':False]
['text':' Special cases to be allowed','line_number':224,'multiline':False]
['text':' Allow if the default value in the configuration class is different from the one in `PretrainedConfig`','line_number':229,'multiline':False]
['text':' Allow cases without checking the default value in the configuration class','line_number':235,'multiline':False]
['text':' configuration class specific cases','line_number':241,'multiline':False]
['text':' Get the parameters in `__init__` of the configuration class, and the default values if any','line_number':256,'multiline':False]
['text':' If `attribute_map` exists, an attribute can have different names to be used in the modeling files, and as long','line_number':261,'multiline':False]
['text':' as one variant is used, the test should pass','line_number':262,'multiline':False]
['text':' Get the path to modeling source files','line_number':267,'multiline':False]
['text':' Let's check against all frameworks: as long as one framework uses an attribute, we are good.','line_number':270,'multiline':False]
['text':' Get the source code strings','line_number':273,'multiline':False]
['text':' `attributes` here is all the variant names for `config_param`','line_number':282,'multiline':False]
['text':' some configuration classes have non-empty `attribute_map`, and both names could be used in the','line_number':284,'multiline':False]
['text':' corresponding modeling files. As long as one of them appears, it is fine.','line_number':285,'multiline':False]
['text':' Skip deprecated models','line_number':299,'multiline':False]
['text':' Some config classes are not in `CONFIG_MAPPING` (e.g. `CLIPVisionConfig`, `Blip2VisionConfig`, etc.)','line_number':302,'multiline':False]
