['text':' Copyright 2022 - Intel Corp. All rights reserved.','line_number':1,'multiline':False]
['text':' Authors: Mayank Kumar Raunak, Javier Turek, Nicole Backage','line_number':2,'multiline':False]
['text':' pad','line_number':59,'multiline':False]
['text':' save the perplexity differences to filename','line_number':135,'multiline':False]
['text':' initialize variables to record relevant information','line_number':175,'multiline':False]
['text':' Initialize the transformer model','line_number':180,'multiline':False]
['text':' Compute perplexity of initial transformer model for comparison','line_number':185,'multiline':False]
['text':' Do LM backprop','line_number':200,'multiline':False]
['text':' Update learning rate schedule','line_number':203,'multiline':False]
['text':' Compute perplexity after back-propagating on the selected context','line_number':205,'multiline':False]
['text':' Periodically save the stored (X, IG(X)) pairs','line_number':208,'multiline':False]
['text':' Reset the pretrained model to the original pretrained GPT-2 weights after each iteration','line_number':212,'multiline':False]
['text':' Generate objective set and training set','line_number':241,'multiline':False]
['text':' Designate the first number (100) articles that are long enough to be used','line_number':242,'multiline':False]
['text':' as our objective set, rest (that are long enough) are training data for','line_number':243,'multiline':False]
['text':' secondary learner','line_number':244,'multiline':False]
['text':' We will use the first 512 pairs from our dataset as a test set for','line_number':288,'multiline':False]
['text':' our secondary learner and the rest to train','line_number':289,'multiline':False]
['text':' secondary learner model set up','line_number':295,'multiline':False]
['text':' TODO in original code this is written as number of actual batches seen','line_number':302,'multiline':False]
['text':' not number of items seen but other places it is number of items instead.','line_number':303,'multiline':False]
['text':' improve consistency! changed this to epochs for clarity','line_number':304,'multiline':False]
['text':' Iterate through batches until we've used max_steps batches','line_number':306,'multiline':False]
['text':' model trains fairly quickly so we won't wait for a full epoch','line_number':320,'multiline':False]
['text':' eval is triggered at eval_freq and end of epochs','line_number':321,'multiline':False]
['text':' Compute performance of the secondary learner after this batch','line_number':330,'multiline':False]
['text':' embeddings are from the pretrained model','line_number':376,'multiline':False]
['text':' this calls __init__','line_number':411,'multiline':False]
