['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' noqa: F401','line_number':8,'multiline':False]
['text':' need the parent dir module','line_number':22,'multiline':False]
['text':' noqa','line_number':24,'multiline':False]
['text':' Tell lightning we are training the student','line_number':39,'multiline':False]
['text':' We copy good generation parameters to student by default','line_number':41,'multiline':False]
['text':' Use teacher's tokenizer','line_number':53,'multiline':False]
['text':' self.different_encoder determines whether we need to run the teacher encoder','line_number':74,'multiline':False]
['text':' To save RAM, delete teacher encoder and freeze student encoder.','line_number':78,'multiline':False]
['text':' T5','line_number':81,'multiline':False]
['text':' type: List[int], List[int]','line_number':89,'multiline':False]
['text':' Intermediate supervision: Decide which layers to supervise','line_number':91,'multiline':False]
['text':' student layer should emulate hidden states of the teacher layer it was copied from','line_number':99,'multiline':False]
['text':' mask has False at padding_idx','line_number':116,'multiline':False]
['text':' (bs * seq_length * voc_size) modulo the 1s in mask','line_number':119,'multiline':False]
['text':' (bs * seq_length * voc_size) modulo the 1s in mask','line_number':120,'multiline':False]
['text':' (bs * seq_length, voc_size) modulo the 1s in mask','line_number':121,'multiline':False]
['text':' (bs * seq_length, voc_size) modulo the 1s in mask','line_number':122,'multiline':False]
['text':' noinspection PyCallingNonCallable','line_number':148,'multiline':False]
['text':' Same cross entropy vs. label smoothing logic as finetune.py','line_number':159,'multiline':False]
['text':' Same behavior as modeling_bart.py, besides ignoring pad_token_id','line_number':162,'multiline':False]
['text':' use this unless self.different_base_models','line_number':176,'multiline':False]
['text':' compute encoder hidden state loss','line_number':178,'multiline':False]
['text':' since we are not passing labels, never let this default to True','line_number':201,'multiline':False]
['text':' Intermediate supervision of decoder hidden states','line_number':205,'multiline':False]
['text':' NOTE: if --student argument was specified and the teacher and student base models','line_number':241,'multiline':False]
['text':' are different, the models still have to have the same tokenizer, specified by','line_number':242,'multiline':False]
['text':' --tokenizer_name. So, for example, you can distill from t5_large to t5_small but not','line_number':243,'multiline':False]
['text':' from bart to t5. This s because if the tokenizers are different, the output space','line_number':244,'multiline':False]
['text':' for the two models is also different and their logits are not comparable.','line_number':245,'multiline':False]
['text':' DISTILL WITH TEACHER','line_number':288,'multiline':False]
