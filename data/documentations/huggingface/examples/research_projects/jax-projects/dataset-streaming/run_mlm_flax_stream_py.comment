['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2021 The HuggingFace Team All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.','line_number':30,'multiline':False]
['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':226,'multiline':False]
['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':229,'multiline':False]
['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':244,'multiline':False]
['text':' We only compute loss on masked tokens','line_number':250,'multiline':False]
['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':252,'multiline':False]
['text':' 10% of the time, we replace masked input tokens with random word','line_number':256,'multiline':False]
['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':263,'multiline':False]
['text':' concatenate tokenized samples to list (excluding "id" and "text")','line_number':291,'multiline':False]
['text':' Concatenated tokens are split to lists of length `max_seq_length`.','line_number':296,'multiline':False]
['text':' Note that remainedr of % max_seq_length are thrown away.','line_number':297,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':325,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':326,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':327,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':331,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':332,'multiline':False]
['text':' Setup logging','line_number':348,'multiline':False]
['text':' Log on each process the small summary:','line_number':355,'multiline':False]
['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':362,'multiline':False]
['text':' Set seed before initializing model.','line_number':365,'multiline':False]
['text':' Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)','line_number':368,'multiline':False]
['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':369,'multiline':False]
['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':370,'multiline':False]
['text':'','line_number':371,'multiline':False]
['text':' For CSV/JSON files, this script will use the column called 'text' or the first column if no column called','line_number':372,'multiline':False]
['text':' 'text' is found. You can easily tweak this behavior (see below).','line_number':373,'multiline':False]
['text':' Downloading and loading a dataset from the hub.','line_number':375,'multiline':False]
['text':' Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.','line_number':406,'multiline':False]
['text':' We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more','line_number':407,'multiline':False]
['text':' efficient when it receives the `special_tokens_mask`.','line_number':408,'multiline':False]
['text':' Data collator','line_number':429,'multiline':False]
['text':' This one will take care of randomly masking the tokens.','line_number':430,'multiline':False]
['text':' Initialize our training','line_number':433,'multiline':False]
['text':' Store some constant','line_number':446,'multiline':False]
['text':' define number steps per stream epoch','line_number':451,'multiline':False]
['text':' Create learning rate schedule','line_number':454,'multiline':False]
['text':' We use Optax's "masking" functionality to not apply weight decay','line_number':467,'multiline':False]
['text':' to bias and LayerNorm scale parameters. decay_mask_fn returns a','line_number':468,'multiline':False]
['text':' mask boolean with the same structure as the parameters.','line_number':469,'multiline':False]
['text':' The mask is True for parameters that should be decayed.','line_number':470,'multiline':False]
['text':' Note that this mask is specifically adapted for FlaxBERT-like models.','line_number':471,'multiline':False]
['text':' For other models, one should correct the layer norm parameter naming','line_number':472,'multiline':False]
['text':' accordingly.','line_number':473,'multiline':False]
['text':' create adam optimizer','line_number':479,'multiline':False]
['text':' Setup train state','line_number':489,'multiline':False]
['text':' Define gradient update step fn','line_number':492,'multiline':False]
['text':' compute loss, ignore padded input tokens','line_number':501,'multiline':False]
['text':' take average','line_number':505,'multiline':False]
['text':' Create parallel version of the train step','line_number':521,'multiline':False]
['text':' Define eval fn','line_number':524,'multiline':False]
['text':' compute loss, ignore padded input tokens','line_number':530,'multiline':False]
['text':' compute accuracy','line_number':534,'multiline':False]
['text':' summarize metrics','line_number':537,'multiline':False]
['text':' Replicate the train state on each device','line_number':545,'multiline':False]
['text':' ======================== Training ================================','line_number':560,'multiline':False]
['text':' Once the end of the dataset stream is reached, the training iterator','line_number':564,'multiline':False]
['text':' is reinitialized and reshuffled and a new eval dataset is randomly chosen.','line_number':565,'multiline':False]
['text':' process input samples','line_number':574,'multiline':False]
['text':' Model forward','line_number':577,'multiline':False]
['text':' ======================== Evaluating ==============================','line_number':593,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':595,'multiline':False]
['text':' process input samples','line_number':600,'multiline':False]
['text':' Model forward','line_number':604,'multiline':False]
['text':' normalize eval metrics','line_number':609,'multiline':False]
['text':' Update progress bar','line_number':615,'multiline':False]
['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':625,'multiline':False]
['text':' update tqdm bar','line_number':635,'multiline':False]
