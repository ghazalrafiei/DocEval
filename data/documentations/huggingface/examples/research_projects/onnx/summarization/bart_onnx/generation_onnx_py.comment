['text':' Do this twice so we got 2 different decoders for further work.','line_number':84,'multiline':False]
['text':' Update here to use different decoder for different values of past.','line_number':243,'multiline':False]
['text':' init sequence length tensors','line_number':260,'multiline':False]
['text':' pre-process distribution','line_number':270,'multiline':False]
['text':' argmax','line_number':273,'multiline':False]
['text':' add code that transfomers next_tokens to tokens_to_add','line_number':276,'multiline':False]
['text':' add token and increase length by one','line_number':281,'multiline':False]
['text':' update sequence length','line_number':284,'multiline':False]
['text':' stop when there is a </s> in each sentence, or if we exceed the maximul length','line_number':290,'multiline':False]
['text':' increase cur_len','line_number':294,'multiline':False]
['text':' special case if pad_token_id is not defined','line_number':316,'multiline':False]
['text':' Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.','line_number':318,'multiline':False]
['text':' TorchScript compatible BeamSearchScorer','line_number':339,'multiline':False]
['text':' placeholder for TorchScript compatibility','line_number':355,'multiline':False]
['text':' placeholder for TorchScript compatibility','line_number':356,'multiline':False]
['text':' NOTE: TorchScript does not support List of Modules','line_number':381,'multiline':False]
['text':'       Rewritten BeamHypotheses with tensors and list of tensors.','line_number':382,'multiline':False]
['text':' ignoring bos_token','line_number':389,'multiline':False]
['text':' NOTE: work around difference of torch.sum(empty_tensor) == 0, while error in onnx.','line_number':416,'multiline':False]
['text':' Bug: https://msdata.visualstudio.com/Vienna/_workitems/edit/1486599','line_number':417,'multiline':False]
['text':' pad the batch','line_number':474,'multiline':False]
['text':' next tokens for this sentence','line_number':480,'multiline':False]
['text':' add to generated hypotheses if end of sentence','line_number':486,'multiline':False]
['text':' if beam_token does not belong to top num_beams tokens, it should not be added','line_number':488,'multiline':False]
['text':' add next predicted token since it is not eos_token','line_number':498,'multiline':False]
['text':' once the beam for next step is full, don't add more tokens to it.','line_number':504,'multiline':False]
['text':' Check if we are done so that we can save a pad step if all(done)','line_number':514,'multiline':False]
['text':' finalize all open beam hypotheses and add to generated hypotheses','line_number':534,'multiline':False]
['text':' all open beam hypotheses are added to the beam hypothesis','line_number':539,'multiline':False]
['text':' beam hypothesis class automatically keeps the best beams','line_number':540,'multiline':False]
['text':' select the best hypotheses','line_number':547,'multiline':False]
['text':' NOTE: torch.Tensor.new_zeros() is not scriptable','line_number':548,'multiline':False]
['text':' retrieve best hypotheses','line_number':554,'multiline':False]
['text':' NOTE: lambda is not scriptable','line_number':556,'multiline':False]
['text':' append to lists','line_number':565,'multiline':False]
['text':' prepare for adding eos','line_number':569,'multiline':False]
['text':' shorter batches are padded if needed','line_number':572,'multiline':False]
['text':' fill with hypotheses and eos_token_id if the latter fits in','line_number':577,'multiline':False]
['text':' if decoder past is not included in output','line_number':624,'multiline':False]
['text':' speedy decoding is disabled and no need to reorder','line_number':625,'multiline':False]
['text':' adjust tokens for Bart, *e.g.*','line_number':654,'multiline':False]
['text':' (batch_size * num_beams, vocab_size)','line_number':659,'multiline':False]
['text':' pre-process distribution','line_number':661,'multiline':False]
['text':' reshape for beam search','line_number':665,'multiline':False]
['text':' special case if pad_token_id is not defined','line_number':711,'multiline':False]
['text':' logger.warning(f"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.")','line_number':713,'multiline':False]
