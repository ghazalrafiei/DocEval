['text':' sum of exp(x_i)','line_number':19,'multiline':False]
['text':' sum of x_i * exp(x_i)','line_number':20,'multiline':False]
['text':' logits, pooled_output','line_number':77,'multiline':False]
['text':' logits, hidden_states(?), entropy','line_number':82,'multiline':False]
['text':' Add last layer','line_number':91,'multiline':False]
['text':' last-layer hidden state, (all hidden states), (all attentions), all highway exits','line_number':102,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':196,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':197,'multiline':False]
['text':' If a 2D ou 3D attention mask is provided for the cross-attention','line_number':200,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':201,'multiline':False]
['text':' fp16 compatibility','line_number':209,'multiline':False]
['text':' Prepare head mask if needed','line_number':212,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':213,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':214,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':215,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':216,'multiline':False]
['text':' add hidden_states and attentions if they are here','line_number':235,'multiline':False]
['text':' sequence_output, pooled_output, (hidden_states), (attentions), highway exits','line_number':236,'multiline':False]
['text':' start from 1!','line_number':242,'multiline':False]
['text':' Pooler','line_number':257,'multiline':False]
['text':' "return" pooler_output','line_number':260,'multiline':False]
['text':' BertModel','line_number':262,'multiline':False]
['text':' "return" bmodel_output','line_number':264,'multiline':False]
['text':' Dropout and classification','line_number':266,'multiline':False]
['text':' sequence_output, pooled_output, (hidden_states), (attentions), highway exits','line_number':344,'multiline':False]
['text':' add hidden states and attention if they are here','line_number':350,'multiline':False]
['text':'  We are doing regression','line_number':362,'multiline':False]
['text':' work with highway exits','line_number':369,'multiline':False]
['text':'  We are doing regression','line_number':377,'multiline':False]
['text':' exclude the final highway, of course','line_number':387,'multiline':False]
['text':' use the highway of the last layer','line_number':395,'multiline':False]
['text':' (loss), logits, (hidden_states), (attentions), (highway_exits)','line_number':397,'multiline':False]
