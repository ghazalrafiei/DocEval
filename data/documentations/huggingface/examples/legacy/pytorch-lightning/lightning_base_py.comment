['text':' update this and the import above to support new schedulers from transformers.optimization','line_number':50,'multiline':False]
['text':' '': get_constant_schedule,             # not supported for now','line_number':56,'multiline':False]
['text':' '': get_constant_schedule_with_warmup, # not supported for now','line_number':57,'multiline':False]
['text':' TODO: move to self.save_hyperparameters()','line_number':76,'multiline':False]
['text':' self.save_hyperparameters()','line_number':77,'multiline':False]
['text':' can also expand arguments into trainer signature for easier reading','line_number':78,'multiline':False]
['text':' TODO: consider num_tpu_cores','line_number':166,'multiline':False]
['text':' Log results','line_number':278,'multiline':False]
['text':' Log and save results to file','line_number':286,'multiline':False]
['text':'  To allow all pl args uncomment the following line','line_number':296,'multiline':False]
['text':'  parser = pl.Trainer.add_argparse_args(parser)','line_number':297,'multiline':False]
['text':' can pass WandbLogger() here','line_number':345,'multiline':False]
['text':' init model','line_number':353,'multiline':False]
['text':' add custom checkpoints','line_number':357,'multiline':False]
['text':' TODO: remove with PyTorch 1.6 since pl uses native amp','line_number':369,'multiline':False]
