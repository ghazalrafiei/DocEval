['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' TODO clean up all this to leverage built-in features of tokenizers','line_number':102,'multiline':False]
['text':' bert-base-multilingual-cased sometimes output "nothing ([]) when calling tokenize with just a space.','line_number':116,'multiline':False]
['text':' Use the real label id for the first token of the word, and padding ids for the remaining tokens','line_number':119,'multiline':False]
['text':' Account for [CLS] and [SEP] with "- 2" and with "- 3" for RoBERTa.','line_number':122,'multiline':False]
['text':' The convention in BERT is:','line_number':128,'multiline':False]
['text':' (a) For sequence pairs:','line_number':129,'multiline':False]
['text':'  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]','line_number':130,'multiline':False]
['text':'  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1','line_number':131,'multiline':False]
['text':' (b) For single sequences:','line_number':132,'multiline':False]
['text':'  tokens:   [CLS] the dog is hairy . [SEP]','line_number':133,'multiline':False]
['text':'  type_ids:   0   0   0   0  0     0   0','line_number':134,'multiline':False]
['text':'','line_number':135,'multiline':False]
['text':' Where "type_ids" are used to indicate whether this is the first','line_number':136,'multiline':False]
['text':' sequence or the second sequence. The embedding vectors for `type=0` and','line_number':137,'multiline':False]
['text':' `type=1` were learned during pre-training and are added to the wordpiece','line_number':138,'multiline':False]
['text':' embedding vector (and position vector). This is not *strictly* necessary','line_number':139,'multiline':False]
['text':' since the [SEP] token unambiguously separates the sequences, but it makes','line_number':140,'multiline':False]
['text':' it easier for the model to learn the concept of sequences.','line_number':141,'multiline':False]
['text':'','line_number':142,'multiline':False]
['text':' For classification tasks, the first vector (corresponding to [CLS]) is','line_number':143,'multiline':False]
['text':' used as the "sentence vector". Note that this only makes sense because','line_number':144,'multiline':False]
['text':' the entire model is fine-tuned.','line_number':145,'multiline':False]
['text':' roberta uses an extra separator b/w pairs of sentences','line_number':149,'multiline':False]
['text':' The mask has 1 for real tokens and 0 for padding tokens. Only real','line_number':165,'multiline':False]
['text':' tokens are attended to.','line_number':166,'multiline':False]
['text':' Zero-pad up to the sequence length.','line_number':169,'multiline':False]
['text':' Use cross entropy ignore_index as padding label id so that only','line_number':220,'multiline':False]
['text':' real label ids contribute to the loss later.','line_number':221,'multiline':False]
['text':' Load data features from cache or dataset file','line_number':234,'multiline':False]
['text':' Make sure only the first process in distributed training processes the dataset,','line_number':240,'multiline':False]
['text':' and the others will use the cache.','line_number':241,'multiline':False]
['text':' TODO clean up all this to leverage built-in features of tokenizers','line_number':250,'multiline':False]
['text':' xlnet has a cls token at the end','line_number':257,'multiline':False]
['text':' roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805','line_number':262,'multiline':False]
['text':' Use cross entropy ignore_index as padding label id so that only','line_number':289,'multiline':False]
['text':' real label ids contribute to the loss later.','line_number':290,'multiline':False]
['text':' TODO clean up all this to leverage built-in features of tokenizers','line_number':304,'multiline':False]
['text':' xlnet has a cls token at the end','line_number':311,'multiline':False]
['text':' roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805','line_number':316,'multiline':False]
