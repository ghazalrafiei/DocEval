['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2021 The HuggingFace Team. All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.','line_number':19,'multiline':False]
['text':' Here to have a nice missing dependency error message early on','line_number':30,'multiline':False]
['text':' Will error if the minimal version of Transformers is not installed. Remove at your own risks.','line_number':55,'multiline':False]
['text':' A list of all multilingual tokenizer which require lang attribute.','line_number':72,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':320,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':321,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':322,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':326,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':327,'multiline':False]
['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':341,'multiline':False]
['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':342,'multiline':False]
['text':' Setup logging','line_number':345,'multiline':False]
['text':' The default of training_args.log_level is passive, so we set log level at info here to have that default.','line_number':353,'multiline':False]
['text':' Log on each process the small summary:','line_number':363,'multiline':False]
['text':' Detecting last checkpoint.','line_number':382,'multiline':False]
['text':' Set seed before initializing model.','line_number':397,'multiline':False]
['text':' Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)','line_number':400,'multiline':False]
['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':401,'multiline':False]
['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':402,'multiline':False]
['text':'','line_number':403,'multiline':False]
['text':' For CSV/JSON files this script will use the first column for the full texts and the second column for the','line_number':404,'multiline':False]
['text':' summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).','line_number':405,'multiline':False]
['text':'','line_number':406,'multiline':False]
['text':' In distributed training, the load_dataset function guarantee that only one local process can concurrently','line_number':407,'multiline':False]
['text':' download the dataset.','line_number':408,'multiline':False]
['text':' Downloading and loading a dataset from the hub.','line_number':410,'multiline':False]
['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':434,'multiline':False]
['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':435,'multiline':False]
['text':' Load pretrained model and tokenizer','line_number':437,'multiline':False]
['text':'','line_number':438,'multiline':False]
['text':' Distributed training:','line_number':439,'multiline':False]
['text':' The .from_pretrained methods guarantee that only one local process can concurrently','line_number':440,'multiline':False]
['text':' download model & vocab.','line_number':441,'multiline':False]
['text':' We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch','line_number':467,'multiline':False]
['text':' on a small vocab and want a smaller embedding size, remove this test.','line_number':468,'multiline':False]
['text':' Preprocessing the datasets.','line_number':504,'multiline':False]
['text':' We need to tokenize inputs and targets.','line_number':505,'multiline':False]
['text':' For multilingual translation models like mBART-50 and M2M100 we need to force the target language token','line_number':530,'multiline':False]
['text':' as the first generated token. We ask the user to explicitly provide this as --forced_bos_token argument.','line_number':531,'multiline':False]
['text':' Get the column names for input/target.','line_number':537,'multiline':False]
['text':' Temporarily set max_target_length for training.','line_number':556,'multiline':False]
['text':' remove pairs where at least one record is None','line_number':567,'multiline':False]
['text':' Tokenize targets with the `text_target` keyword argument','line_number':578,'multiline':False]
['text':' If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore','line_number':581,'multiline':False]
['text':' padding in the loss.','line_number':582,'multiline':False]
['text':' Data collator','line_number':638,'multiline':False]
['text':' Metric','line_number':647,'multiline':False]
['text':' rougeLSum expects newline after each sentence','line_number':654,'multiline':False]
['text':' Replace -100s used for padding as we can't decode them','line_number':664,'multiline':False]
['text':' Some simple post-processing','line_number':670,'multiline':False]
['text':' Override the decoding parameters of Seq2SeqTrainer','line_number':679,'multiline':False]
['text':' Initialize our Trainer','line_number':689,'multiline':False]
['text':' Training','line_number':700,'multiline':False]
['text':' Saves the tokenizer too for easy upload','line_number':708,'multiline':False]
['text':' Evaluation','line_number':720,'multiline':False]
['text':' For xla_spawn (TPUs)','line_number':783,'multiline':False]
