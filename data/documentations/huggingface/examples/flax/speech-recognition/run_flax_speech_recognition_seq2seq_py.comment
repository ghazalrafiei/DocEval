['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.','line_number':19,'multiline':False]
['text':' Will error if the minimal version of Transformers is not installed. Remove at your own risk.','line_number':62,'multiline':False]
['text':' split inputs and labels since they have to be of different lengths and need','line_number':297,'multiline':False]
['text':' different padding methods','line_number':298,'multiline':False]
['text':' dataloader returns a list of features which we convert to a dict','line_number':301,'multiline':False]
['text':' reformat list to dict and set to pytorch format','line_number':305,'multiline':False]
['text':' if bos token is appended in previous tokenization step,','line_number':322,'multiline':False]
['text':' cut bos token here as it's append later anyways','line_number':323,'multiline':False]
['text':' replace padding with -100 to ignore correctly when computing the loss','line_number':331,'multiline':False]
['text':' 1. Parse input arguments','line_number':374,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':375,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':376,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':377,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':381,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':382,'multiline':False]
['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':387,'multiline':False]
['text':' information sent is the one passed as arguments along with your JAX/Flax versions.','line_number':388,'multiline':False]
['text':' 2. Setup logging','line_number':391,'multiline':False]
['text':' Make one log on every process with the configuration for debugging.','line_number':392,'multiline':False]
['text':' Set the verbosity to info of the Transformers logger.','line_number':398,'multiline':False]
['text':' We only want one process per machine to log things on the screen.','line_number':399,'multiline':False]
['text':' Check the output dir is valid','line_number':410,'multiline':False]
['text':' Handle the repository creation','line_number':422,'multiline':False]
['text':' 3. Load dataset','line_number':433,'multiline':False]
['text':' 5. Load pretrained model, tokenizer, and feature extractor','line_number':475,'multiline':False]
['text':' 6. Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,','line_number':508,'multiline':False]
['text':' so we just need to set the correct target sampling rate.','line_number':509,'multiline':False]
['text':' 7. Preprocessing the datasets.','line_number':514,'multiline':False]
['text':' We need to read the audio files as arrays and tokenize the targets.','line_number':515,'multiline':False]
['text':' We only need to set the task id when the language is specified (i.e. in a multilingual setting)','line_number':536,'multiline':False]
['text':' process audio','line_number':540,'multiline':False]
['text':' process audio length','line_number':543,'multiline':False]
['text':' process targets','line_number':547,'multiline':False]
['text':' filter training data with inputs longer than max_input_length','line_number':559,'multiline':False]
['text':' for large datasets it is advised to run the preprocessing on a','line_number':569,'multiline':False]
['text':' single machine first with `args.preprocessing_only` since there will mostly likely','line_number':570,'multiline':False]
['text':' be a timeout when running the script in distributed mode.','line_number':571,'multiline':False]
['text':' In a second step `args.preprocessing_only` can then be set to `False` to load the','line_number':572,'multiline':False]
['text':' cached dataset','line_number':573,'multiline':False]
['text':' 8. Load Metric','line_number':579,'multiline':False]
['text':' replace padded labels by the padding token','line_number':583,'multiline':False]
['text':' we do not want to group tokens when computing the metrics','line_number':588,'multiline':False]
['text':' 9. Save feature extractor, tokenizer and config','line_number':594,'multiline':False]
['text':' Enable tensorboard only on the master node','line_number':611,'multiline':False]
['text':' Initialize our training','line_number':629,'multiline':False]
['text':' Store some constant','line_number':633,'multiline':False]
['text':' Create learning rate schedule','line_number':641,'multiline':False]
['text':' We use Optax's "masking" functionality to not apply weight decay','line_number':648,'multiline':False]
['text':' to bias and LayerNorm scale parameters. decay_mask_fn returns a','line_number':649,'multiline':False]
['text':' mask boolean with the same structure as the parameters.','line_number':650,'multiline':False]
['text':' The mask is True for parameters that should be decayed.','line_number':651,'multiline':False]
['text':' find out all LayerNorm parameters','line_number':654,'multiline':False]
['text':' create adam optimizer','line_number':665,'multiline':False]
['text':' Setup train state','line_number':675,'multiline':False]
['text':' label smoothed cross entropy','line_number':678,'multiline':False]
['text':' ignore padded tokens from loss, i.e. where labels are not set to -100','line_number':695,'multiline':False]
['text':' Define gradient update step fn','line_number':702,'multiline':False]
['text':' true loss = total loss / total samples','line_number':716,'multiline':False]
['text':' true grad = total grad / total samples','line_number':720,'multiline':False]
['text':' Define eval fn','line_number':728,'multiline':False]
['text':' true loss = total loss / total samples','line_number':736,'multiline':False]
['text':' Define generation function','line_number':743,'multiline':False]
['text':' Create parallel version of the train and eval step','line_number':752,'multiline':False]
['text':' Replicate the train state on each device','line_number':759,'multiline':False]
['text':' ======================== Training ================================','line_number':772,'multiline':False]
['text':' Generate an epoch by shuffling sampling indices from the train dataset and create a data loader','line_number':777,'multiline':False]
['text':' train','line_number':786,'multiline':False]
['text':' ======================== Evaluating ==============================','line_number':801,'multiline':False]
['text':' Model forward','line_number':814,'multiline':False]
['text':' generation','line_number':822,'multiline':False]
['text':' normalize eval metrics','line_number':828,'multiline':False]
['text':' compute WER metric','line_number':832,'multiline':False]
['text':' Print metrics and update progress bar','line_number':839,'multiline':False]
['text':' Save metrics','line_number':844,'multiline':False]
['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':849,'multiline':False]
