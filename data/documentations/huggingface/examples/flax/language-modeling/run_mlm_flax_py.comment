['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2021 The HuggingFace Team All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.','line_number':34,'multiline':False]
['text':' Handle dict or lists with proper padding and conversion to tensor.','line_number':312,'multiline':False]
['text':' If special token mask has been preprocessed, pop it from the dict.','line_number':315,'multiline':False]
['text':' We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)','line_number':330,'multiline':False]
['text':' We only compute loss on masked tokens','line_number':336,'multiline':False]
['text':' 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])','line_number':338,'multiline':False]
['text':' 10% of the time, we replace masked input tokens with random word','line_number':342,'multiline':False]
['text':' The rest of the time (10% of the time) we keep the masked input tokens unchanged','line_number':349,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':385,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':386,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':387,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':391,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':392,'multiline':False]
['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':406,'multiline':False]
['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':407,'multiline':False]
['text':' Setup logging','line_number':421,'multiline':False]
['text':' Log on each process the small summary:','line_number':428,'multiline':False]
['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':431,'multiline':False]
['text':' Set seed before initializing model.','line_number':434,'multiline':False]
['text':' Handle the repository creation','line_number':437,'multiline':False]
['text':' Retrieve of infer repo_name','line_number':439,'multiline':False]
['text':' Create repo and retrieve repo_id','line_number':443,'multiline':False]
['text':' Clone repo locally','line_number':445,'multiline':False]
['text':' Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)','line_number':448,'multiline':False]
['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':449,'multiline':False]
['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':450,'multiline':False]
['text':'','line_number':451,'multiline':False]
['text':' For CSV/JSON files, this script will use the column called 'text' or the first column if no column called','line_number':452,'multiline':False]
['text':' 'text' is found. You can easily tweak this behavior (see below).','line_number':453,'multiline':False]
['text':'','line_number':454,'multiline':False]
['text':' In distributed training, the load_dataset function guarantees that only one local process can concurrently','line_number':455,'multiline':False]
['text':' download the dataset.','line_number':456,'multiline':False]
['text':' Downloading and loading a dataset from the hub.','line_number':458,'multiline':False]
['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':518,'multiline':False]
['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':519,'multiline':False]
['text':' Load pretrained model and tokenizer','line_number':521,'multiline':False]
['text':' Distributed training:','line_number':523,'multiline':False]
['text':' The .from_pretrained methods guarantee that only one local process can concurrently','line_number':524,'multiline':False]
['text':' download model & vocab.','line_number':525,'multiline':False]
['text':' Preprocessing the datasets.','line_number':566,'multiline':False]
['text':' First we tokenize all the texts.','line_number':567,'multiline':False]
['text':' When using line_by_line, we just tokenize each nonempty line.','line_number':577,'multiline':False]
['text':' Remove empty lines','line_number':581,'multiline':False]
['text':' Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.','line_number':601,'multiline':False]
['text':' We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more','line_number':602,'multiline':False]
['text':' efficient when it receives the `special_tokens_mask`.','line_number':603,'multiline':False]
['text':' Main data processing function that will concatenate all texts from our dataset and generate chunks of','line_number':615,'multiline':False]
['text':' max_seq_length.','line_number':616,'multiline':False]
['text':' Concatenate all texts.','line_number':618,'multiline':False]
['text':' We drop the small remainder, we could add padding if the model supported it instead of this drop, you can','line_number':621,'multiline':False]
['text':' customize this part to your needs.','line_number':622,'multiline':False]
['text':' Split by chunks of max_len.','line_number':625,'multiline':False]
['text':' Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a','line_number':632,'multiline':False]
['text':' remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value','line_number':633,'multiline':False]
['text':' might be slower to preprocess.','line_number':634,'multiline':False]
['text':'','line_number':635,'multiline':False]
['text':' To speed up this part, we use multiprocessing. See the documentation of the map method for more information:','line_number':636,'multiline':False]
['text':' https://huggingface.co/docs/datasets/process#map','line_number':637,'multiline':False]
['text':' Enable tensorboard only on the master node','line_number':645,'multiline':False]
['text':' Data collator','line_number':663,'multiline':False]
['text':' This one will take care of randomly masking the tokens.','line_number':664,'multiline':False]
['text':' Initialize our training','line_number':667,'multiline':False]
['text':' Store some constant','line_number':691,'multiline':False]
['text':' Create learning rate schedule','line_number':699,'multiline':False]
['text':' We use Optax's "masking" functionality to not apply weight decay','line_number':712,'multiline':False]
['text':' to bias and LayerNorm scale parameters. decay_mask_fn returns a','line_number':713,'multiline':False]
['text':' mask boolean with the same structure as the parameters.','line_number':714,'multiline':False]
['text':' The mask is True for parameters that should be decayed.','line_number':715,'multiline':False]
['text':' find out all LayerNorm parameters','line_number':718,'multiline':False]
['text':' create adam optimizer','line_number':729,'multiline':False]
['text':' We use the default parameters here to initialize adafactor,','line_number':731,'multiline':False]
['text':' For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74','line_number':732,'multiline':False]
['text':' Setup train state','line_number':746,'multiline':False]
['text':' Define gradient update step fn','line_number':749,'multiline':False]
['text':' compute loss, ignore padded input tokens','line_number':758,'multiline':False]
['text':' take average','line_number':762,'multiline':False]
['text':' true loss = total loss / total samples','line_number':772,'multiline':False]
['text':' true grad = total grad / total samples','line_number':776,'multiline':False]
['text':' Create parallel version of the train step','line_number':785,'multiline':False]
['text':' Define eval fn','line_number':788,'multiline':False]
['text':' compute loss, ignore padded input tokens','line_number':794,'multiline':False]
['text':' compute accuracy','line_number':798,'multiline':False]
['text':' summarize metrics','line_number':801,'multiline':False]
['text':' Replicate the train state on each device','line_number':809,'multiline':False]
['text':' ======================== Training ================================','line_number':815,'multiline':False]
['text':' Create sampling rng','line_number':819,'multiline':False]
['text':' Generate an epoch by shuffling sampling indices from the train dataset','line_number':822,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':824,'multiline':False]
['text':' Gather the indexes for creating the batch and do a training step','line_number':828,'multiline':False]
['text':' Model forward','line_number':833,'multiline':False]
['text':' Save metrics','line_number':841,'multiline':False]
['text':' ======================== Evaluating ==============================','line_number':855,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':857,'multiline':False]
['text':' Model forward','line_number':866,'multiline':False]
['text':' normalize eval metrics','line_number':872,'multiline':False]
['text':' Update progress bar','line_number':878,'multiline':False]
['text':' Save metrics','line_number':881,'multiline':False]
['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':886,'multiline':False]
['text':' Eval after training','line_number':894,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':897,'multiline':False]
['text':' Model forward','line_number':906,'multiline':False]
['text':' normalize eval metrics','line_number':912,'multiline':False]
