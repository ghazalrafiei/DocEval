['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2021 The HuggingFace Team All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.','line_number':31,'multiline':False]
['text':' inputs contain all nonnoise tokens, sentinels for all noise spans','line_number':282,'multiline':False]
['text':' and one EOS token.','line_number':283,'multiline':False]
['text':' minor hack to get the targets length to be equal to inputs length','line_number':295,'multiline':False]
['text':' which is more likely to have been set to a nice round number.','line_number':296,'multiline':False]
['text':' convert list to dict and tensorize input','line_number':338,'multiline':False]
['text':' to check that tokens are correctly preprocessed, one can run `self.tokenizer.batch_decode(input_ids)` and `self.tokenizer.batch_decode(labels)` here...','line_number':367,'multiline':False]
['text':' input_ids tokens and sentinel tokens are >= 0, tokens < 0 are','line_number':397,'multiline':False]
['text':' masked tokens coming after sentinel tokens and should be removed','line_number':398,'multiline':False]
['text':' avoid degeneracy by ensuring positive numbers of noise and nonnoise tokens.','line_number':429,'multiline':False]
['text':' num_noise_tokens should be less than num_noise_tokens and num_nonnoise_tokens','line_number':431,'multiline':False]
['text':' avoid degeneracy by ensuring positive number of noise spans','line_number':434,'multiline':False]
['text':' pick the lengths of the noise spans and the non-noise spans','line_number':437,'multiline':False]
['text':' count length of sub segments assuming that list is sorted','line_number':451,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':502,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':503,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':504,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':508,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':509,'multiline':False]
['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':523,'multiline':False]
['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':524,'multiline':False]
['text':' Setup logging','line_number':538,'multiline':False]
['text':' Log on each process the small summary:','line_number':545,'multiline':False]
['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':548,'multiline':False]
['text':' Set seed before initializing model.','line_number':551,'multiline':False]
['text':' Handle the repository creation','line_number':554,'multiline':False]
['text':' Retrieve of infer repo_name','line_number':556,'multiline':False]
['text':' Create repo and retrieve repo_id','line_number':560,'multiline':False]
['text':' Clone repo locally','line_number':562,'multiline':False]
['text':' Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)','line_number':565,'multiline':False]
['text':' or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/','line_number':566,'multiline':False]
['text':' (the dataset will be downloaded automatically from the datasets Hub).','line_number':567,'multiline':False]
['text':'','line_number':568,'multiline':False]
['text':' For CSV/JSON files, this script will use the column called 'text' or the first column if no column called','line_number':569,'multiline':False]
['text':' 'text' is found. You can easily tweak this behavior (see below).','line_number':570,'multiline':False]
['text':' Downloading and loading a dataset from the hub.','line_number':572,'multiline':False]
['text':' See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at','line_number':632,'multiline':False]
['text':' https://huggingface.co/docs/datasets/loading_datasets.','line_number':633,'multiline':False]
['text':' Load pretrained model and tokenizer','line_number':635,'multiline':False]
['text':' Preprocessing the datasets.','line_number':674,'multiline':False]
['text':' First we tokenize all the texts.','line_number':675,'multiline':False]
['text':' Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.','line_number':684,'multiline':False]
['text':' Since we make sure that all sequences are of the same length, no attention_mask is needed.','line_number':685,'multiline':False]
['text':' T5-like span masked language modeling will fuse consecutively masked tokens to a single sentinel token.','line_number':697,'multiline':False]
['text':' To ensure that the input length is `max_seq_length`, we need to increase the maximum length','line_number':698,'multiline':False]
['text':' according to `mlm_probability` and `mean_noise_span_length`. We can also define the label length accordingly.','line_number':699,'multiline':False]
['text':' Main data processing function that will concatenate all texts from our dataset and generate chunks of expanded_inputs_length.','line_number':706,'multiline':False]
['text':' Concatenate all texts.','line_number':708,'multiline':False]
['text':' We drop the small remainder, we could add padding if the model supported it instead of this drop, you can','line_number':711,'multiline':False]
['text':' customize this part to your needs.','line_number':712,'multiline':False]
['text':' Split by chunks of max_len.','line_number':715,'multiline':False]
['text':' Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a','line_number':722,'multiline':False]
['text':' remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value','line_number':723,'multiline':False]
['text':' might be slower to preprocess.','line_number':724,'multiline':False]
['text':'','line_number':725,'multiline':False]
['text':' To speed up this part, we use multiprocessing. See the documentation of the map method for more information:','line_number':726,'multiline':False]
['text':' https://huggingface.co/docs/datasets/process#map','line_number':727,'multiline':False]
['text':' Enable tensorboard only on the master node','line_number':735,'multiline':False]
['text':' Initialize our training','line_number':753,'multiline':False]
['text':' Data collator','line_number':773,'multiline':False]
['text':' This one will take care of randomly masking the tokens.','line_number':774,'multiline':False]
['text':' Store some constant','line_number':785,'multiline':False]
['text':' Create learning rate schedule','line_number':796,'multiline':False]
['text':' We use Optax's "masking" functionality to not apply weight decay','line_number':809,'multiline':False]
['text':' to bias and LayerNorm scale parameters. decay_mask_fn returns a','line_number':810,'multiline':False]
['text':' mask boolean with the same structure as the parameters.','line_number':811,'multiline':False]
['text':' The mask is True for parameters that should be decayed.','line_number':812,'multiline':False]
['text':' find out all LayerNorm parameters','line_number':815,'multiline':False]
['text':' create adam optimizer','line_number':826,'multiline':False]
['text':' We use the default parameters here to initialize adafactor,','line_number':828,'multiline':False]
['text':' For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74','line_number':829,'multiline':False]
['text':' Setup train state','line_number':842,'multiline':False]
['text':' Define gradient update step fn','line_number':845,'multiline':False]
['text':' compute loss','line_number':854,'multiline':False]
['text':' Create parallel version of the train step','line_number':870,'multiline':False]
['text':' Define eval fn','line_number':873,'multiline':False]
['text':' compute loss','line_number':879,'multiline':False]
['text':' compute accuracy','line_number':882,'multiline':False]
['text':' summarize metrics','line_number':885,'multiline':False]
['text':' Replicate the train state on each device','line_number':893,'multiline':False]
['text':' ======================== Training ================================','line_number':899,'multiline':False]
['text':' Create sampling rng','line_number':903,'multiline':False]
['text':' Generate an epoch by shuffling sampling indices from the train dataset','line_number':906,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':908,'multiline':False]
['text':' Gather the indexes for creating the batch and do a training step','line_number':912,'multiline':False]
['text':' Model forward','line_number':922,'multiline':False]
['text':' Save metrics','line_number':930,'multiline':False]
['text':' ======================== Evaluating ==============================','line_number':944,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':946,'multiline':False]
['text':' Model forward','line_number':955,'multiline':False]
['text':' get eval metrics','line_number':961,'multiline':False]
['text':' Update progress bar','line_number':965,'multiline':False]
['text':' Save metrics','line_number':968,'multiline':False]
['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':973,'multiline':False]
['text':' Eval after training','line_number':981,'multiline':False]
['text':' Avoid using jax.numpy here in case of TPU training','line_number':984,'multiline':False]
['text':' Model forward','line_number':993,'multiline':False]
['text':' get eval metrics','line_number':999,'multiline':False]
