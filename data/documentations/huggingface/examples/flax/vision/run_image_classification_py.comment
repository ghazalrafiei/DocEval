['text':'!/usr/bin/env python','line_number':1,'multiline':False]
['text':' coding=utf-8','line_number':2,'multiline':False]
['text':' Copyright 2021 The HuggingFace Team All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' for dataset and preprocessing','line_number':37,'multiline':False]
['text':' See all possible arguments in src/transformers/training_args.py','line_number':265,'multiline':False]
['text':' or by passing the --help flag to this script.','line_number':266,'multiline':False]
['text':' We now keep distinct sets of args, for a cleaner separation of concerns.','line_number':267,'multiline':False]
['text':' If we pass only one argument to the script and it's the path to a json file,','line_number':271,'multiline':False]
['text':' let's parse it to get our arguments.','line_number':272,'multiline':False]
['text':' Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The','line_number':286,'multiline':False]
['text':' information sent is the one passed as arguments along with your Python/PyTorch versions.','line_number':287,'multiline':False]
['text':' Make one log on every process with the configuration for debugging.','line_number':301,'multiline':False]
['text':' Setup logging, we only want one process per machine to log things on the screen.','line_number':307,'multiline':False]
['text':' Set the verbosity to info of the Transformers logger (on main process only):','line_number':314,'multiline':False]
['text':' set seed for random transforms and torch dataloaders','line_number':317,'multiline':False]
['text':' Handle the repository creation','line_number':320,'multiline':False]
['text':' Retrieve of infer repo_name','line_number':322,'multiline':False]
['text':' Create repo and retrieve repo_id','line_number':326,'multiline':False]
['text':' Clone repo locally','line_number':328,'multiline':False]
['text':' Initialize datasets and pre-processing transforms','line_number':331,'multiline':False]
['text':' We use torchvision here for faster pre-processing','line_number':332,'multiline':False]
['text':' Note that here we are using some default pre-processing, for maximum accuray','line_number':333,'multiline':False]
['text':' one should tune this part and carefully select what transformations to use.','line_number':334,'multiline':False]
['text':' Load pretrained model and tokenizer','line_number':360,'multiline':False]
['text':' Store some constant','line_number':400,'multiline':False]
['text':' Create data loaders','line_number':417,'multiline':False]
['text':' Enable tensorboard only on the master node','line_number':438,'multiline':False]
['text':' Initialize our training','line_number':456,'multiline':False]
['text':' Create learning rate schedule','line_number':460,'multiline':False]
['text':' create adam optimizer','line_number':469,'multiline':False]
['text':' Setup train state','line_number':478,'multiline':False]
['text':' Define gradient update step fn','line_number':485,'multiline':False]
['text':' Define eval fn','line_number':506,'multiline':False]
['text':' summarize metrics','line_number':512,'multiline':False]
['text':' Create parallel version of the train and eval step','line_number':518,'multiline':False]
['text':' Replicate the train state on each device','line_number':522,'multiline':False]
['text':' ======================== Training ================================','line_number':535,'multiline':False]
['text':' Create sampling rng','line_number':538,'multiline':False]
['text':' train','line_number':544,'multiline':False]
['text':' ======================== Evaluating ==============================','line_number':562,'multiline':False]
['text':' Model forward','line_number':567,'multiline':False]
['text':' normalize eval metrics','line_number':575,'multiline':False]
['text':' Print metrics and update progress bar','line_number':579,'multiline':False]
['text':' Save metrics','line_number':588,'multiline':False]
['text':' save checkpoint after each epoch and push checkpoint to the hub','line_number':593,'multiline':False]
