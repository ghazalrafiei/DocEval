['text':' Wrap a tf.data.Dataset around it','line_number':98,'multiline':False]
['text':' This next block attempts to parse out which elements of the dataset should be appended to the labels list','line_number':104,'multiline':False]
['text':' that is passed to the metric_fn','line_number':105,'multiline':False]
['text':' If the dataset inputs are split into a 2-tuple of inputs and labels,','line_number':118,'multiline':False]
['text':' assume the second element is the labels','line_number':119,'multiline':False]
['text':' If all batches are unidimensional or same length, do a simple concatenation','line_number':145,'multiline':False]
['text':' Welp, they're not the same length. Let's do some padding','line_number':149,'multiline':False]
['text':' i keeps track of which part of the concatenated array we're writing the next batch to','line_number':155,'multiline':False]
['text':' If it's a dict with only one key, just return the array','line_number':167,'multiline':False]
['text':' If it's a list with only one element, just return the array','line_number':175,'multiline':False]
['text':' This dense conditional recognizes the case where we have an encoder-decoder model, but','line_number':192,'multiline':False]
['text':' avoids getting tangled up when we just have a model with a layer called 'encoder'','line_number':193,'multiline':False]
['text':' The whole predict/generate loop is handled inside this method','line_number':209,'multiline':False]
['text':' This converts any dict-subclass to a regular dict','line_number':231,'multiline':False]
['text':' Keras REALLY doesn't like it when we pass around a BatchEncoding or other derived class','line_number':232,'multiline':False]
['text':' This is the critical bit - Keras passes a dict containing the loss and standard metric values for this epoch','line_number':261,'multiline':False]
['text':' in the logs argument. Ordinarily, this is so the callback can read them, but in this case we write a bunch of','line_number':262,'multiline':False]
['text':' new keys in there, which will then get read by the History callback and treated like any other metric value.','line_number':263,'multiline':False]
['text':' I promise that I have it in writing from Chollet that this is okay.','line_number':264,'multiline':False]
['text':' Create repo and retrieve repo_id','line_number':337,'multiline':False]
['text':' Although we can access model.history, we have no guarantees that the History callback will fire before this','line_number':352,'multiline':False]
['text':' one, so we keep track of it here too','line_number':353,'multiline':False]
['text':' The last upload is still running, don't start another','line_number':359,'multiline':False]
['text':' Don't accidentally write things that Keras will read later','line_number':368,'multiline':False]
['text':' The last upload is still running, don't start another','line_number':374,'multiline':False]
['text':' Makes sure the latest version of the model is uploaded','line_number':395,'multiline':False]
