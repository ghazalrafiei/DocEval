['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 The Google Flax Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' flatten the weights to chunk','line_number':129,'multiline':False]
['text':' If this weight is going to tip up over the maximal size, we split.','line_number':134,'multiline':False]
['text':' Add the last block','line_number':144,'multiline':False]
['text':' If we only have one shard, we return it','line_number':147,'multiline':False]
['text':' Otherwise, let's build the index','line_number':151,'multiline':False]
['text':' Add the metadata','line_number':160,'multiline':False]
['text':' Those are private to be exposed as typed property on derived classes.','line_number':204,'multiline':False]
['text':' Those are public as their type is generic to every derived classes.','line_number':208,'multiline':False]
['text':' To check if the model was intialized automatically.','line_number':214,'multiline':False]
['text':' randomly initialized parameters','line_number':218,'multiline':False]
['text':' get the shape of the parameters','line_number':230,'multiline':False]
['text':' save required_params as set','line_number':233,'multiline':False]
['text':' initialize the parameters','line_number':236,'multiline':False]
['text':' don't set params if the model is not initialized','line_number':288,'multiline':False]
['text':' taken from https://github.com/deepmind/jmp/blob/3a8318abc3292be38582794dbf7b094e6583b192/jmp/_src/policy.py#L27','line_number':310,'multiline':False]
['text':' Load the index','line_number':477,'multiline':False]
['text':' load using msgpack utils','line_number':481,'multiline':False]
['text':' the state dict is unflattened to the match the format of model.params','line_number':503,'multiline':False]
['text':' Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.','line_number':512,'multiline':False]
['text':' Alternativelly, the model can also have a custom `generate` function.','line_number':513,'multiline':False]
['text':' Not relevant for Flax Models','line_number':660,'multiline':False]
['text':' Load config if we don't provide a configuration','line_number':688,'multiline':False]
['text':' Add the dtype to model_kwargs','line_number':713,'multiline':False]
['text':' This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the','line_number':716,'multiline':False]
['text':' index of the files.','line_number':717,'multiline':False]
['text':' Load model','line_number':720,'multiline':False]
['text':' Load from a Flax checkpoint','line_number':726,'multiline':False]
['text':' Load from a sharded Flax checkpoint','line_number':729,'multiline':False]
['text':' Load from a safetensors checkpoint','line_number':735,'multiline':False]
['text':' Load from a PyTorch checkpoint','line_number':738,'multiline':False]
['text':' Load from a sharded pytorch checkpoint','line_number':743,'multiline':False]
['text':' At this stage we don't have a weight file so we will raise an error.','line_number':746,'multiline':False]
['text':' Load from a sharded safetensors checkpoint','line_number':750,'multiline':False]
['text':' Load from URL or cache if already cached','line_number':778,'multiline':False]
['text':' Maybe the checkpoint is sharded, we try to grab the index name in this case.','line_number':794,'multiline':False]
['text':' Maybe the checkpoint is pytorch sharded, we try to grab the pytorch index name in this case.','line_number':802,'multiline':False]
['text':' If we still haven't found anything, look for `safetensors`.','line_number':810,'multiline':False]
['text':' No support for sharded safetensors yet, so we'll raise an error if that's all we find.','line_number':812,'multiline':False]
['text':' Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None','line_number':818,'multiline':False]
['text':' result when internet is up, the repo and revision exist, but the file does not.','line_number':819,'multiline':False]
['text':' Otherwise, maybe there is a TF or Torch model file.  We try those to give a helpful error','line_number':821,'multiline':False]
['text':' message.','line_number':822,'multiline':False]
['text':' Raise any environment error raise by `cached_file`. It will have a helpful error message adapted','line_number':851,'multiline':False]
['text':' to the original exception.','line_number':852,'multiline':False]
['text':' For any other exception, we throw a generic error.','line_number':855,'multiline':False]
['text':' We'll need to download and cache each checkpoint shard if the checkpoint is sharded.','line_number':872,'multiline':False]
['text':' resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.','line_number':874,'multiline':False]
['text':' init random models','line_number':901,'multiline':False]
['text':' make sure all arrays are stored as jnp.arrays','line_number':911,'multiline':False]
['text':' NOTE: This is to prevent a bug this will be fixed in Flax >= v0.3.4:','line_number':912,'multiline':False]
['text':' https://github.com/google/flax/issues/1261','line_number':913,'multiline':False]
['text':' keep the params on CPU if we don't want to initialize','line_number':917,'multiline':False]
['text':' if flax model contains batch norm layers','line_number':920,'multiline':False]
['text':' if model is base model only use model_prefix key','line_number':921,'multiline':False]
['text':' if model is head model and we are loading weights from base model','line_number':929,'multiline':False]
['text':' we initialize new params dict with base_model_prefix','line_number':930,'multiline':False]
['text':' if model is base model only use model_prefix key','line_number':941,'multiline':False]
['text':' if model is head model and we are loading weights from base model','line_number':945,'multiline':False]
['text':' we initialize new params dict with base_model_prefix','line_number':946,'multiline':False]
['text':' flatten dicts','line_number':950,'multiline':False]
['text':' Disabling warning when porting pytorch weights to flax, flax does not uses num_batches_tracked','line_number':958,'multiline':False]
['text':' Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not','line_number':970,'multiline':False]
['text':' matching the weights in the model.','line_number':971,'multiline':False]
['text':' add missing keys as random parameters if we are initializing','line_number':986,'multiline':False]
['text':' remove unexpected keys to not be saved again','line_number':991,'multiline':False]
['text':' dictionary of key: dtypes for the model params','line_number':1035,'multiline':False]
['text':' extract keys of parameters not in jnp.float32','line_number':1037,'multiline':False]
['text':' raise a warning if any of the parameters are not in jnp.float32','line_number':1041,'multiline':False]
['text':' If it is a model with generation capabilities, attempt to load the generation config','line_number':1058,'multiline':False]
['text':' set correct parameters','line_number':1082,'multiline':False]
['text':' get abs dir','line_number':1156,'multiline':False]
['text':' save config as well','line_number':1158,'multiline':False]
['text':' If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be','line_number':1161,'multiline':False]
['text':' loaded from the Hub.','line_number':1162,'multiline':False]
['text':' save model','line_number':1170,'multiline':False]
['text':' Clean the folder from a previous save','line_number':1175,'multiline':False]
['text':' Save the index as well','line_number':1199,'multiline':False]
['text':' the shard item are unflattened, to save them we need to flatten them again','line_number':1209,'multiline':False]
['text':' To update the docstring, we need to copy the method, otherwise we change the original docstring.','line_number':1253,'multiline':False]
['text':' copy __call__ function to be sure docstring is changed only for this function','line_number':1262,'multiline':False]
['text':' delete existing docstring','line_number':1264,'multiline':False]
['text':' set correct docstring','line_number':1266,'multiline':False]
