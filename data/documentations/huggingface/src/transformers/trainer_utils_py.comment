['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020-present the HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' set seed first','line_number':64,'multiline':False]
['text':' Enable PyTorch deterministic mode. This potentially requires either the environment','line_number':68,'multiline':False]
['text':' variable 'CUDA_LAUNCH_BLOCKING' or 'CUBLAS_WORKSPACE_CONFIG' to be set,','line_number':69,'multiline':False]
['text':' depending on the CUDA version, so we set them both here','line_number':70,'multiline':False]
['text':' Enable CUDNN deterministic mode','line_number':75,'multiline':False]
['text':' ^^ safe to call this function even if cuda is not available','line_number':97,'multiline':False]
['text':' Remove speed metrics','line_number':263,'multiline':False]
['text':' map trainer methods to metrics prefix','line_number':428,'multiline':False]
['text':' soft dependency on psutil','line_number':441,'multiline':False]
['text':' noqa','line_number':447,'multiline':False]
['text':' can't sleep or will not catch the peak right (this comment is here on purpose)','line_number':498,'multiline':False]
['text':' time.sleep(0.001) # 1msec','line_number':499,'multiline':False]
['text':' deal with nested calls of eval during train - simply ignore those','line_number':510,'multiline':False]
['text':' gpu','line_number':529,'multiline':False]
['text':' cpu','line_number':538,'multiline':False]
['text':' deal with nested calls of eval during train - simply ignore those','line_number':549,'multiline':False]
['text':' this sends a signal to peak_monitor_func to complete its loop','line_number':553,'multiline':False]
['text':' first ensure all objects get collected and their memory is freed','line_number':556,'multiline':False]
['text':' concepts:','line_number':567,'multiline':False]
['text':' - alloc_delta:  the difference of allocated memory between the end and the start','line_number':568,'multiline':False]
['text':' - peaked_delta: the difference between the peak memory and the current memory','line_number':569,'multiline':False]
['text':' in order to know how much memory the measured code consumed one needs to sum these two','line_number':570,'multiline':False]
['text':' gpu','line_number':572,'multiline':False]
['text':' cpu','line_number':593,'multiline':False]
['text':' reset - cycle finished','line_number':602,'multiline':False]
['text':' deal with nested calls of eval during train - simply ignore those','line_number':610,'multiline':False]
['text':' since we don't have a way to return init metrics, we push them into the first of train/val/predict','line_number':614,'multiline':False]
['text':' if we need additional debug info, enable the following','line_number':626,'multiline':False]
['text':' for t in ["begin", "end"]:','line_number':627,'multiline':False]
['text':'     if stage in self.cpu and t in self.cpu[stage]:','line_number':628,'multiline':False]
['text':'         metrics[f"{stage}_mem_cpu_{t}"] = self.cpu[stage][t]','line_number':629,'multiline':False]
['text':'     if self.torch is not None and stage in self.gpu and t in self.gpu[stage]:','line_number':630,'multiline':False]
['text':'         metrics[f"{stage}_mem_gpu_{t}"] = self.gpu[stage][t]','line_number':631,'multiline':False]
['text':' since memory can be allocated before init, and it might be difficult to track overall','line_number':633,'multiline':False]
['text':' memory usage, in particular for GPU, let's report memory usage at the point init was called','line_number':634,'multiline':False]
['text':' if we also wanted to report any additional memory allocations in between init and','line_number':639,'multiline':False]
['text':' whatever the next stage was we could also report this:','line_number':640,'multiline':False]
['text':' if self.cpu["init"]["end"] != self.cpu[stage]["begin"]:','line_number':641,'multiline':False]
['text':'     metrics[f"after_init_mem_cpu_delta"] = self.cpu[stage]["begin"] - self.cpu["init"]["end"]','line_number':642,'multiline':False]
['text':' if self.torch is not None and self.gpu["init"]["end"] != self.gpu[stage]["begin"]:','line_number':643,'multiline':False]
['text':'     metrics[f"after_init_mem_gpu_delta"] = self.gpu[stage]["begin"] - self.gpu["init"]["end"]','line_number':644,'multiline':False]
['text':' init doesn't have metrics to update so we just save that data for later stages to retrieve','line_number':654,'multiline':False]
['text':' TypeError: len() of unsized object','line_number':666,'multiline':False]
