['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa: F401','line_number':72,'multiline':False]
['text':' This is used to set the max input length for a model with infinite size input','line_number':116,'multiline':False]
['text':' This is used when we need something big but slightly smaller than VERY_LARGE_INTEGER','line_number':117,'multiline':False]
['text':' Define type aliases and NamedTuples','line_number':119,'multiline':False]
['text':' Slow tokenizers used to be saved in three separated files','line_number':128,'multiline':False]
['text':' Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file','line_number':133,'multiline':False]
['text':' After this point:','line_number':289,'multiline':False]
['text':' Extended properties and methods only available for fast (Rust-based) tokenizers','line_number':290,'multiline':False]
['text':' provided by HuggingFace tokenizers library.','line_number':291,'multiline':False]
['text':' Convert to TensorType','line_number':696,'multiline':False]
['text':' Get a function reference for the correct framework','line_number':700,'multiline':False]
['text':' noqa: F811','line_number':725,'multiline':False]
['text':' we have a ragged list so handle explicitly','line_number':735,'multiline':False]
['text':' Do the tensor conversion in batch','line_number':741,'multiline':False]
['text':' Removing this for now in favor of controlling the shape with `prepend_batch_axis`','line_number':750,'multiline':False]
['text':' # at-least2d','line_number':751,'multiline':False]
['text':' if tensor.ndim > 2:','line_number':752,'multiline':False]
['text':'     tensor = tensor.squeeze(0)','line_number':753,'multiline':False]
['text':' elif tensor.ndim < 2:','line_number':754,'multiline':False]
['text':'     tensor = tensor[None, :]','line_number':755,'multiline':False]
['text':' This check catches things like APEX blindly calling "to" on all inputs to a module','line_number':785,'multiline':False]
['text':' Otherwise it passes the casts down and casts the LongTensor containing the token idxs','line_number':786,'multiline':False]
['text':' into a HalfTensor','line_number':787,'multiline':False]
['text':' We directly set the hidden value to allow initialization with special tokens','line_number':846,'multiline':False]
['text':' which are not yet in the vocabulary. Necessary for serialization/de-serialization','line_number':847,'multiline':False]
['text':' TODO clean this up at some point (probably by switching to fast tokenizers)','line_number':848,'multiline':False]
['text':' for legacy purpose we default to stripping. `test_add_tokens_tokenizer` depends on this','line_number':949,'multiline':False]
['text':' for legacy purpose we default to stripping. `False` depends on this','line_number':963,'multiline':False]
['text':' if we are adding tokens that were not part of the vocab, we ought to add them','line_number':970,'multiline':False]
['text':' first name has to correspond to main model input name','line_number':1556,'multiline':False]
['text':' to make sure `tokenizer.pad(...)` works correctly','line_number':1557,'multiline':False]
['text':' inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)','line_number':1564,'multiline':False]
['text':' For backward compatibility we fallback to set model_max_length from max_len if provided','line_number':1570,'multiline':False]
['text':' Padding and truncation side are right by default and overridden in subclasses. If specified in the kwargs, it','line_number':1574,'multiline':False]
['text':' is changed.','line_number':1575,'multiline':False]
['text':' By default, cleaning tokenization spaces for both fast and slow tokenizers','line_number':1590,'multiline':False]
['text':' By default, do not split special tokens for both fast and slow tokenizers','line_number':1593,'multiline':False]
['text':' Use to store when we have already noticed a deprecation warning (avoid overlogging).','line_number':1596,'multiline':False]
['text':' Stores a Jinja template that formats chat histories into tokenizable strings','line_number':1599,'multiline':False]
['text':' For backward compatibility, allow to try to setup 'max_len_single_sentence'.','line_number':1620,'multiline':False]
['text':' For backward compatibility, allow to try to setup 'max_len_sentences_pair'.','line_number':1634,'multiline':False]
['text':' Indicates it's a Conversation object','line_number':1728,'multiline':False]
['text':' priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template`','line_number':1731,'multiline':False]
['text':' Compilation function uses a cache to avoid recompiling the same template','line_number':1738,'multiline':False]
['text':' There's only one sequence here, so "longest" makes no sense','line_number':1746,'multiline':False]
['text':' At this point pretrained_model_name_or_path is either a directory or a model identifier name','line_number':1939,'multiline':False]
['text':' kept only for legacy','line_number':1941,'multiline':False]
['text':' kept only for legacy','line_number':1942,'multiline':False]
['text':' tokenizer_file used to initialize a slow from a fast. Properly copy the `addedTokens` instead of adding in random orders','line_number':1944,'multiline':False]
['text':' Try to get the tokenizer config to see if there are versioned tokenizer files.','line_number':1949,'multiline':False]
['text':' Get files from url, cache, or disk depending on the case','line_number':1975,'multiline':False]
['text':' We instantiate fast tokenizers based on a slow tokenizer if we don't have access to the tokenizer.json','line_number':2055,'multiline':False]
['text':' file or if `from_slow` is set to True.','line_number':2056,'multiline':False]
['text':' Prepare tokenizer initialization kwargs','line_number':2074,'multiline':False]
['text':' Did we saved some inputs and kwargs to reload ?','line_number':2075,'multiline':False]
['text':' First attempt. We get tokenizer_class from tokenizer_config to check mismatch between tokenizers.','line_number':2080,'multiline':False]
['text':' For backward compatibility with odl format.','line_number':2093,'multiline':False]
['text':' tests_ignore','line_number':2101,'multiline':False]
['text':' Second attempt. If we have not yet found tokenizer_class, let's try to use the config.','line_number':2103,'multiline':False]
['text':' skip if an error occurred.','line_number':2114,'multiline':False]
['text':' Third attempt. If we have not yet found the original type of the tokenizer,','line_number':2117,'multiline':False]
['text':' we are loading we see if we can infer it from the type of the configuration file','line_number':2118,'multiline':False]
['text':' tests_ignore','line_number':2119,'multiline':False]
['text':' Fallback: use pattern matching on the string.','line_number':2124,'multiline':False]
['text':' Update with newly provided kwargs','line_number':2147,'multiline':False]
['text':' Set max length if needed','line_number':2150,'multiline':False]
['text':' if we're using a pretrained model, ensure the tokenizer','line_number':2152,'multiline':False]
['text':' wont index sequences longer than the number of positional embeddings','line_number':2153,'multiline':False]
['text':' TODO(PVP) - uncomment following line in Transformers v5','line_number':2158,'multiline':False]
['text':' init_kwargs["model_max_length"] = model_max_length','line_number':2159,'multiline':False]
['text':' TODO(PVP) - remove in Transformers v5','line_number':2160,'multiline':False]
['text':' ---','line_number':2161,'multiline':False]
['text':' ---','line_number':2165,'multiline':False]
['text':' Merge resolved_vocab_files arguments in init_kwargs.','line_number':2167,'multiline':False]
['text':'### Handle tokenizer serialization of added and special tokens','line_number':2179,'multiline':False]
['text':' if we have info on the slow added tokens','line_number':2182,'multiline':False]
['text':' begin legacy: read the added_tokens_file and update kwargs with special_tokens_map if modified','line_number':2195,'multiline':False]
['text':' This value has already been redefined by the kwargs','line_number':2201,'multiline':False]
['text':' We keep this new value and ignore the one stored in the special_tokens_map_file','line_number':2202,'multiline':False]
['text':' slow -> slow|fast, legacy: convert the `"added_tokens.json"` file to `added_tokens_decoder`.','line_number':2215,'multiline':False]
['text':' this is for legacy purpose. We don't add the tokens after init for efficiency.','line_number':2216,'multiline':False]
['text':' if index not in added_tokens_decoder and str_token not in added_tokens_map:','line_number':2229,'multiline':False]
['text':' allows converting a fast -> slow: add the `tokenizer.json`'s `"added_tokens"` to the slow tokenizer','line_number':2236,'multiline':False]
['text':' if `tokenizer_config.json` is `None`','line_number':2237,'multiline':False]
['text':' This is for slow so can be done before','line_number':2239,'multiline':False]
['text':' end legacy','line_number':2247,'multiline':False]
['text':' Passing AddedTokens and not strings to the class to prevent it from casting the string to a different AddedToken','line_number':2249,'multiline':False]
['text':' convert {'__type': 'AddedToken', 'content': '<ent>', 'lstrip': False, 'normalized': True, ...} to AddedTokens','line_number':2250,'multiline':False]
['text':' Instantiate the tokenizer.','line_number':2258,'multiline':False]
['text':' This method should be deleted in Transformers v5','line_number':2276,'multiline':False]
['text':' Its only purpose is to potentially throw a warning','line_number':2277,'multiline':False]
['text':' that incorrectly defined max lengths of T5's tokenizer are used','line_number':2278,'multiline':False]
['text':' which we will correct in Transformers v5.','line_number':2279,'multiline':False]
['text':' Don't save "special" for previous tokenizers','line_number':2292,'multiline':False]
['text':' Let's save the init kwargs','line_number':2378,'multiline':False]
['text':' Let's save the special tokens map (only the strings)','line_number':2380,'multiline':False]
['text':' Let's make sure we properly save the special tokens.','line_number':2387,'multiline':False]
['text':' no typefields, this way old fast and slow can load it','line_number':2398,'multiline':False]
['text':' Process added tokens seperatly: allows previous versions to ignore it!','line_number':2401,'multiline':False]
['text':' Add tokenizer class to the tokenizer config to be able to reload it with from_pretrained','line_number':2407,'multiline':False]
['text':' Remove the Fast at the end unless we have a special `PreTrainedTokenizerFast`','line_number':2409,'multiline':False]
['text':' If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be','line_number':2418,'multiline':False]
['text':' loaded from the Hub.','line_number':2419,'multiline':False]
['text':' remove private information','line_number':2423,'multiline':False]
['text':' Sanitize AddedTokens in special_tokens_map','line_number':2434,'multiline':False]
['text':' kept for forward compatibility, will be removed in transoformers 5. Typefields are not saved for FC, special should not be save either','line_number':2436,'multiline':False]
['text':' the new get_added_vocab() also returns special tokens and tokens that have an index < vocab_size','line_number':2486,'multiline':False]
['text':' Backward compatibility for previous behavior, maybe we should deprecate it:','line_number':2600,'multiline':False]
['text':' If you only set max_length, it activates truncation for max_length','line_number':2601,'multiline':False]
['text':' Get padding strategy','line_number':2615,'multiline':False]
['text':' Default to pad to the longest sequence in the batch','line_number':2642,'multiline':False]
['text':' Get truncation strategy','line_number':2650,'multiline':False]
['text':' Default to truncate the longest sequences in pairs of inputs','line_number':2669,'multiline':False]
['text':' Set max length if needed','line_number':2677,'multiline':False]
['text':' Test if we have a padding token','line_number':2705,'multiline':False]
['text':' Check that we will truncate to a multiple of pad_to_multiple_of if both are provided','line_number':2713,'multiline':False]
['text':' To avoid duplicating','line_number':2776,'multiline':False]
['text':' The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the','line_number':2798,'multiline':False]
['text':' input mode in this case.','line_number':2799,'multiline':False]
['text':' Leave back tokenizer in input mode','line_number':2806,'multiline':False]
['text':' Input type checking for clearer error','line_number':2838,'multiline':False]
['text':' Strings are fine','line_number':2841,'multiline':False]
['text':' List are fine as long as they are...','line_number':2844,'multiline':False]
['text':' ... empty','line_number':2846,'multiline':False]
['text':' ... list of strings','line_number':2849,'multiline':False]
['text':' ... list with an empty list or with a list of strings','line_number':2852,'multiline':False]
['text':' Backward compatibility for 'truncation_strategy', 'pad_to_max_length'','line_number':2971,'multiline':False]
['text':' Backward compatibility for 'truncation_strategy', 'pad_to_max_length'','line_number':3069,'multiline':False]
['text':' If we have a list of dicts, let's convert it in a dict of lists','line_number':3211,'multiline':False]
['text':' We do this to allow using this method as a collate_fn function in PyTorch Dataloader','line_number':3212,'multiline':False]
['text':' The model's main input name, usually `input_ids`, has be passed for padding','line_number':3216,'multiline':False]
['text':' If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects','line_number':3230,'multiline':False]
['text':' and rebuild them afterwards if no return_tensors is specified','line_number':3231,'multiline':False]
['text':' Note that we lose the specific device the tensor may be on for PyTorch','line_number':3232,'multiline':False]
['text':' first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.','line_number':3236,'multiline':False]
['text':' At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.','line_number':3241,'multiline':False]
['text':' Convert padding_strategy in PaddingStrategy','line_number':3258,'multiline':False]
['text':' Backward compatibility for 'truncation_strategy', 'pad_to_max_length'','line_number':3379,'multiline':False]
['text':' Load from model defaults','line_number':3411,'multiline':False]
['text':' Compute the total size of the returned encodings','line_number':3419,'multiline':False]
['text':' Truncation: Handle max sequence length','line_number':3422,'multiline':False]
['text':' Add special tokens','line_number':3437,'multiline':False]
['text':' Build output dictionary','line_number':3445,'multiline':False]
['text':' Check lengths','line_number':3455,'multiline':False]
['text':' Padding','line_number':3458,'multiline':False]
['text':' Load from model defaults','line_number':3627,'multiline':False]
['text':' Initialize attention mask if not present.','line_number':3641,'multiline':False]
['text':' Convert inputs to python lists','line_number':3747,'multiline':False]
['text':' cache the property','line_number':3791,'multiline':False]
['text':' docstyle-ignore','line_number':3967,'multiline':False]
['text':' mBART-specific kwargs that should be ignored by other models.','line_number':3987,'multiline':False]
['text':' Process tgt_texts','line_number':4003,'multiline':False]
['text':' Defaults to FULL_TOKENIZER_FILE and then try to look at some newer versions.','line_number':4038,'multiline':False]
['text':' No point going further since the versions are sorted.','line_number':4045,'multiline':False]
['text':' To update the docstring, we need to copy the method, otherwise we change the original docstring.','line_number':4051,'multiline':False]
