['text':' Let's spare some time by deactivating altogether','line_number':55,'multiline':False]
['text':' Internal bookkeeping','line_number':59,'multiline':False]
['text':' Batch data is simple tensor, just fetch the slice','line_number':75,'multiline':False]
['text':' Batch data is assumed to be BaseModelOutput (or dict)','line_number':78,'multiline':False]
['text':' Convert ModelOutput to tuple first','line_number':82,'multiline':False]
['text':' Those are stored as lists of tensors so need specific unbatching.','line_number':90,'multiline':False]
['text':' This can happen for optional data that get passed around','line_number':97,'multiline':False]
['text':' Take correct batch data, but make it looked like batch_size=1','line_number':100,'multiline':False]
['text':' For compatibility with other methods within transformers','line_number':101,'multiline':False]
['text':' Take correct batch data, but make it looked like batch_size=1','line_number':105,'multiline':False]
['text':' For compatibility with other methods within transformers','line_number':106,'multiline':False]
['text':' This is typically a list, so no need to `unsqueeze`.','line_number':109,'multiline':False]
['text':' Recreate the element by reusing the original class to make it look','line_number':111,'multiline':False]
['text':' batch_size=1','line_number':112,'multiline':False]
['text':' We are currently unrolling a batch so we just need to return','line_number':119,'multiline':False]
['text':' the current item within a batch','line_number':120,'multiline':False]
['text':' We're out of items within a batch','line_number':123,'multiline':False]
['text':' We now have a batch of "inferred things".','line_number':126,'multiline':False]
['text':' Try to infer the size of the batch','line_number':128,'multiline':False]
['text':' could be last batch so we can't unroll as many','line_number':139,'multiline':False]
['text':' elements.','line_number':140,'multiline':False]
['text':' Setting internal index to unwrap the batch','line_number':142,'multiline':False]
['text':' We're not unrolling batches','line_number':147,'multiline':False]
['text':' Try to return next item','line_number':182,'multiline':False]
['text':' When a preprocess iterator ends, we can start lookig at the next item','line_number':185,'multiline':False]
['text':' ChunkIterator will keep feeding until ALL elements of iterator','line_number':186,'multiline':False]
['text':' all have created their subiterator and have been iterating against.','line_number':187,'multiline':False]
['text':'','line_number':188,'multiline':False]
['text':' Another way to look at it, is we're basically flattening lists of lists','line_number':189,'multiline':False]
['text':' into a single list, but with generators','line_number':190,'multiline':False]
['text':' Extremely similar to PipelineIterator in its unpacking mechanism','line_number':247,'multiline':False]
['text':' BUT, we have an extra required item which is the presence of `is_last`','line_number':248,'multiline':False]
['text':' That is because everything is flattened by `PipelineChunkIterator` we','line_number':249,'multiline':False]
['text':' need to keep track of how to regroup here in the original `process`','line_number':250,'multiline':False]
['text':' boundaries so that `process` and `postprocess` see the same data.','line_number':251,'multiline':False]
['text':' This iterator accumulates items (possibly while unbatching) until it','line_number':253,'multiline':False]
['text':' its a `is_last` and then just passes it on to the caller.','line_number':254,'multiline':False]
['text':' could be last batch so we can't unroll as many','line_number':278,'multiline':False]
['text':' elements.','line_number':279,'multiline':False]
