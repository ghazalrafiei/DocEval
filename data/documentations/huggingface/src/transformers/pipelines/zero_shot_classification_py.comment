['text':' Override for tokenizers not supporting padding','line_number':111,'multiline':False]
['text':' tokenizers might yell that we want to truncate','line_number':127,'multiline':False]
['text':' to a value that is not even reached by the input.','line_number':128,'multiline':False]
['text':' In that case we don't want to truncate.','line_number':129,'multiline':False]
['text':' It seems there's not a really better way to catch that','line_number':130,'multiline':False]
['text':' exception.','line_number':131,'multiline':False]
['text':' `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported','line_number':225,'multiline':False]
['text':' softmax over the entailment vs. contradiction dim for each label independently','line_number':249,'multiline':False]
['text':' softmax the "entailment" logits over all candidate labels','line_number':256,'multiline':False]
