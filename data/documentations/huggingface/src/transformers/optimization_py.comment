['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' as LambdaLR multiplies by lr_init','line_number':233,'multiline':False]
['text':' as LambdaLR multiplies by lr_init','line_number':239,'multiline':False]
['text':' Note: this implementation is adapted from','line_number':316,'multiline':False]
['text':' https://github.com/google-research/big_vision/blob/f071ce68852d56099437004fd70057597a95f6ef/big_vision/utils.py#L930','line_number':317,'multiline':False]
['text':' All other schedulers require `num_warmup_steps`','line_number':374,'multiline':False]
['text':' All other schedulers require `num_training_steps`','line_number':384,'multiline':False]
['text':' add_ with alpha','line_number':435,'multiline':False]
['text':' State initialization','line_number':469,'multiline':False]
['text':' Exponential moving average of gradient values','line_number':472,'multiline':False]
['text':' Exponential moving average of squared gradient values','line_number':474,'multiline':False]
['text':' Decay the first and second moment running average coefficient','line_number':482,'multiline':False]
['text':' In-place operations to update the averages at the same time','line_number':483,'multiline':False]
['text':' No bias correction for Bert','line_number':489,'multiline':False]
['text':' Just adding the square of the weights to the loss function is *not*','line_number':496,'multiline':False]
['text':' the correct way of using L2 regularization/weight decay with Adam,','line_number':497,'multiline':False]
['text':' since that will interact with the m and v parameters in strange ways.','line_number':498,'multiline':False]
['text':'','line_number':499,'multiline':False]
['text':' Instead we want to decay the weights in a manner that doesn't interact','line_number':500,'multiline':False]
['text':' with the m/v parameters. This is equivalent to adding the square','line_number':501,'multiline':False]
['text':' of the weights to the loss with plain (non-momentum) SGD.','line_number':502,'multiline':False]
['text':' Add weight decay at the end (fixed version)','line_number':503,'multiline':False]
['text':' add_ with alpha','line_number':608,'multiline':False]
['text':' copy from fairseq's adafactor implementation:','line_number':650,'multiline':False]
['text':' https://github.com/huggingface/transformers/blob/8395f14de6068012787d83989c3627c3df6a252b/src/transformers/optimization.py#L505','line_number':651,'multiline':False]
['text':' State Initialization','line_number':683,'multiline':False]
['text':' Exponential moving average of gradient values','line_number':688,'multiline':False]
['text':' Approximation of exponential moving average of square of gradient','line_number':723,'multiline':False]
['text':' if called before stepping','line_number':777,'multiline':False]
