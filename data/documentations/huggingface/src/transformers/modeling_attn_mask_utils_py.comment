['text':' Copyright 2023 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]
['text':' limitations under the License.','line_number':13,'multiline':False]
['text':' If shape is not cached, create a new causal mask and cache it','line_number':79,'multiline':False]
['text':' create causal mask','line_number':83,'multiline':False]
['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':84,'multiline':False]
['text':' create causal mask','line_number':111,'multiline':False]
['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':112,'multiline':False]
['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':131,'multiline':False]
['text':' expanded_attn_mask + causal_4d_mask can cause some overflow','line_number':138,'multiline':False]
['text':' add lower triangular sliding window mask if necessary','line_number':164,'multiline':False]
['text':' fmt: off','line_number':191,'multiline':False]
['text':' fmt: on','line_number':233,'multiline':False]
['text':' Get the index of the first non-zero value for every sample in the batch.','line_number':235,'multiline':False]
['text':' In the above example, indices = [[2], [0], [1]]]','line_number':236,'multiline':False]
['text':' Find the batch indexes that have unattended tokens on the leftmost side (e.g. [0, 0, 1, 1, 1]), for which the first rows of the','line_number':240,'multiline':False]
['text':' expanded mask will be completely unattended.','line_number':241,'multiline':False]
['text':' Avoid unmasking tokens at relevant target positions (on the row axis), by rather unmasking possibly several times the first row that should always be unmasked as we filtered out the batch above.','line_number':252,'multiline':False]
['text':' TODO: we may drop support for 3D attention mask as the refactor from Patrick maybe dropped this case','line_number':255,'multiline':False]
['text':' Broadcast [left_masked_rows, 1], [left_masked_rows, max_len]','line_number':259,'multiline':False]
['text':' Broadcast [left_masked_rows, 1, 1], [1, num_masks, 1], [left_masked_rows, 1, max_len]','line_number':262,'multiline':False]
['text':' Broadcast [left_masked_rows, 1], [left_masked_rows, max_len]','line_number':269,'multiline':False]
['text':' 4d mask is passed through the layers','line_number':304,'multiline':False]
['text':' Adapted from _prepare_4d_causal_attention_mask','line_number':317,'multiline':False]
['text':' torch.jit.trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`','line_number':337,'multiline':False]
['text':' used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.','line_number':338,'multiline':False]
['text':' TODO: Fix this as well when using torchdynamo with fullgraph=True.','line_number':339,'multiline':False]
['text':' For query_length == 1, causal attention and bi-directional attention are the same.','line_number':347,'multiline':False]
['text':' Unfortunately, for query_length > 1 and key_value_length != query_length, we cannot generally ignore the attention mask, as SDPA causal mask generation','line_number':352,'multiline':False]
['text':' may be wrong. We will set `is_causal=False` in SDPA and rely on Transformers attention_mask instead, hence not setting it to None here.','line_number':353,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/108108','line_number':354,'multiline':False]
['text':' See the comment above (https://github.com/pytorch/pytorch/issues/108108).','line_number':357,'multiline':False]
['text':' Ugly: we set it to True here to dispatch in the following controlflow to `to_causal_4d`.','line_number':358,'multiline':False]
['text':' From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend','line_number':379,'multiline':False]
['text':' produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213','line_number':380,'multiline':False]
['text':' torch.jit.trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`','line_number':421,'multiline':False]
['text':' used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.','line_number':422,'multiline':False]
['text':' TODO: Fix this as well when using torchdynamo with fullgraph=True.','line_number':423,'multiline':False]
['text':' For query_length == 1, causal attention and bi-directional attention are the same.','line_number':430,'multiline':False]
['text':' Unfortunately, for query_length > 1 and key_value_length != query_length, we can not generally ignore the attention mask, as SDPA causal mask generation','line_number':435,'multiline':False]
['text':' may be wrong. We will set is_causal=False in SDPA and rely on Transformers attention_mask instead, hence not setting it to None here.','line_number':436,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/108108','line_number':437,'multiline':False]
