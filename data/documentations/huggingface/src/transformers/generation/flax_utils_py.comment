['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 The Google AI Flax Team Authors, and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' Only use this arg if not None, otherwise just remove from model_kwargs','line_number':178,'multiline':False]
['text':' retrieve decoder_start_token_id for encoder-decoder models','line_number':186,'multiline':False]
['text':' fall back to bos_token_id if necessary','line_number':187,'multiline':False]
['text':' `kwargs`/`model_kwargs` is often used to handle optional forward pass inputs like `attention_mask`. If','line_number':253,'multiline':False]
['text':' `prepare_inputs_for_generation` doesn't accept them, then a stricter check can be made ;)','line_number':254,'multiline':False]
['text':' Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call','line_number':308,'multiline':False]
['text':' priority: `generation_config` argument > `model.generation_config` (the default generation config)','line_number':311,'multiline':False]
['text':' legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,','line_number':313,'multiline':False]
['text':' two conditions must be met','line_number':314,'multiline':False]
['text':' 1) the generation config must have been created from the model config (`_from_model_config` field);','line_number':315,'multiline':False]
['text':' 2) the generation config must have seen no modification since its creation (the hash is the same).','line_number':316,'multiline':False]
['text':' All unused kwargs must be model kwargs','line_number':332,'multiline':False]
['text':' set init values','line_number':338,'multiline':False]
['text':' decoder-only models should use left-padding for generation (can't be checked with `trace=True`)','line_number':356,'multiline':False]
['text':' add encoder_outputs to model_kwargs','line_number':370,'multiline':False]
['text':' prepare decoder_input_ids for generation','line_number':373,'multiline':False]
['text':' Prepare `max_length` depending on other stopping criteria.','line_number':381,'multiline':False]
['text':' 20 is the default max_length of the generation config','line_number':385,'multiline':False]
['text':' broadcast input_ids & encoder_outputs','line_number':446,'multiline':False]
['text':' generation starts after the last token that is forced','line_number':528,'multiline':False]
['text':' init values','line_number':574,'multiline':False]
['text':' per batch-item holding current token in loop.','line_number':585,'multiline':False]
['text':' per batch-item state bit indicating if sentence has finished.','line_number':589,'multiline':False]
['text':' For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop','line_number':592,'multiline':False]
['text':' and pass it the `encoder_outputs`, which are part of the `model_kwargs`.','line_number':593,'multiline':False]
['text':' initialize model specific kwargs','line_number':595,'multiline':False]
['text':' initialize state','line_number':598,'multiline':False]
['text':' apply min_length, ...','line_number':619,'multiline':False]
['text':' The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU','line_number':638,'multiline':False]
['text':' init values','line_number':662,'multiline':False]
['text':' per batch-item holding current token in loop.','line_number':674,'multiline':False]
['text':' per batch-item state bit indicating if sentence has finished.','line_number':678,'multiline':False]
['text':' For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop','line_number':681,'multiline':False]
['text':' and pass it the `encoder_outputs`, which are part of the `model_kwargs`.','line_number':682,'multiline':False]
['text':' initialize model specific kwargs','line_number':685,'multiline':False]
['text':' initialize state','line_number':688,'multiline':False]
['text':' apply min_length, ...','line_number':712,'multiline':False]
['text':' apply top_p, top_k, temperature','line_number':714,'multiline':False]
['text':' The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU','line_number':735,'multiline':False]
['text':' ignore scalars (e.g. cache index)','line_number':767,'multiline':False]
['text':' ignore scalars (e.g. cache index)','line_number':774,'multiline':False]
['text':' ignore scalars (e.g. cache index)','line_number':788,'multiline':False]
['text':' init values','line_number':796,'multiline':False]
['text':' record the prompt length of decoder','line_number':812,'multiline':False]
['text':' per batch,beam-item holding current token in loop.','line_number':815,'multiline':False]
['text':' per batch,beam-item state bit indicating if sentence has finished.','line_number':820,'multiline':False]
['text':' per batch,beam-item score, logprobs','line_number':823,'multiline':False]
['text':' For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop','line_number':827,'multiline':False]
['text':' and pass it the `encoder_outputs`, which are part of the `model_kwargs`.','line_number':828,'multiline':False]
['text':' flatten beam dim','line_number':831,'multiline':False]
['text':' initialize model specific kwargs','line_number':840,'multiline':False]
['text':' initialize state','line_number':843,'multiline':False]
['text':' 1. is less than max length?','line_number':857,'multiline':False]
['text':' 2. can the new beams still improve?','line_number':860,'multiline':False]
['text':' early_stopping == False -> apply heuristic = always get the best score from `cur_len`. See the discussion','line_number':861,'multiline':False]
['text':' below for more details.','line_number':862,'multiline':False]
['text':' https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565','line_number':863,'multiline':False]
['text':' early_stopping == "never" -> compute the best score from max_length or cur_len, depending on the sign of','line_number':864,'multiline':False]
['text':'   length_penalty. Positive length_penalty favors longer sequences, thus we use max_length there.','line_number':865,'multiline':False]
['text':' 3. is there still a beam that has not finished?','line_number':879,'multiline':False]
['text':' 1. Forward current tokens','line_number':886,'multiline':False]
['text':' Collect the current position slice along length to feed the fast','line_number':887,'multiline':False]
['text':' autoregressive decoder model.  Flatten the beam dimension into batch','line_number':888,'multiline':False]
['text':' dimension for feeding into the model.','line_number':889,'multiline':False]
['text':' unflatten beam dimension','line_number':890,'multiline':False]
['text':' Unflatten beam dimension in attention cache arrays','line_number':891,'multiline':False]
['text':' adapt logits for FlaxMarianMTModel','line_number':906,'multiline':False]
['text':' 2. Compute log probs','line_number':909,'multiline':False]
['text':' get log probabilities from logits,','line_number':910,'multiline':False]
['text':' process logits with processors (*e.g.* min_length, ...), and','line_number':911,'multiline':False]
['text':' add new logprobs to existing running logprobs scores.','line_number':912,'multiline':False]
['text':' 3. Retrieve top-K','line_number':922,'multiline':False]
['text':' Each item in batch has num_beams * vocab_size candidate sequences.','line_number':923,'multiline':False]
['text':' For each item, get the top 2*k candidates with the highest log-','line_number':924,'multiline':False]
['text':' probabilities. We gather the top 2*K beams here so that even if the best','line_number':925,'multiline':False]
['text':' K sequences reach EOS simultaneously, we have another K sequences','line_number':926,'multiline':False]
['text':' remaining to continue the live beam search.','line_number':927,'multiline':False]
['text':' Gather the top 2*K scores from _all_ beams.','line_number':928,'multiline':False]
['text':' Gather 2*k top beams.','line_number':929,'multiline':False]
['text':' Recover the beam index by floor division.','line_number':930,'multiline':False]
['text':' Recover token id by modulo division and expand Id array for broadcasting.','line_number':931,'multiline':False]
['text':' Update sequences for the 2*K top-k new sequences.','line_number':932,'multiline':False]
['text':' 4. Check which sequences have ended','line_number':942,'multiline':False]
['text':' Update current sequences:','line_number':943,'multiline':False]
['text':' Did any of these sequences reach an end marker?','line_number':944,'multiline':False]
['text':' To prevent these just finished sequences from being added to the current sequences','line_number':945,'multiline':False]
['text':' set of active beam search sequences, set their log probs to a very large','line_number':946,'multiline':False]
['text':' negative value.','line_number':947,'multiline':False]
['text':' 5. Get running sequences scores for next','line_number':950,'multiline':False]
['text':' Determine the top k beam indices (from top 2*k beams) from log probs','line_number':951,'multiline':False]
['text':' and gather top k beams (from top 2*k beams).','line_number':952,'multiline':False]
['text':' 6. Process topk logits','line_number':958,'multiline':False]
['text':' Further process log probs:','line_number':959,'multiline':False]
['text':' - add length penalty','line_number':960,'multiline':False]
['text':' - make sure no scores can be added anymore if beam is full','line_number':961,'multiline':False]
['text':' - make sure still running sequences cannot be chosen as finalized beam','line_number':962,'multiline':False]
['text':' 7. Get scores, sequences, is sentence finished for next.','line_number':970,'multiline':False]
['text':' Combine sequences, scores, and flags along the beam dimension and compare','line_number':971,'multiline':False]
['text':' new finished sequence scores to existing finished scores and select the','line_number':972,'multiline':False]
['text':' best from the new set of beams','line_number':973,'multiline':False]
['text':' 8. Update model kwargs.','line_number':982,'multiline':False]
['text':' Determine the top k beam indices from the original set of all beams.','line_number':983,'multiline':False]
['text':' With these, gather the top k beam-associated caches.','line_number':984,'multiline':False]
['text':' Always run first iteration outside of `lax.while_loop` to avoid calling `beam_search_cond_fn`','line_number':1000,'multiline':False]
['text':' when `state.cur_len` equals `decoder_prompt_len`. This also helps to comply with TPU when','line_number':1001,'multiline':False]
['text':' the very first prompt has sequence length > 1.','line_number':1002,'multiline':False]
['text':' Account for the edge-case where there are no finished sequences for a','line_number':1010,'multiline':False]
['text':' particular batch item. If so, return running sequences for that batch item.','line_number':1011,'multiline':False]
['text':' Take best beams for each batch (the score is sorted in descending order)','line_number':1016,'multiline':False]
