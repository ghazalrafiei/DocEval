['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Inc. team','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' if score < 0 then repetition penalty has to be multiplied to reduce the token probabilities','line_number':334,'multiline':False]
['text':' if score < 0 then hallucination penalty has to be multiplied to increase the token probabilities','line_number':389,'multiline':False]
['text':' Remove tokens with cumulative top_p above the threshold (token with 0 are kept)','line_number':450,'multiline':False]
['text':' Keep at least min_tokens_to_keep','line_number':452,'multiline':False]
['text':' scatter sorted tensors to original indexing','line_number':455,'multiline':False]
['text':' Safety check','line_number':507,'multiline':False]
['text':' Remove all tokens with a probability less than the last token of the top-k','line_number':508,'multiline':False]
['text':' calculate entropy','line_number':580,'multiline':False]
['text':' shift and sort','line_number':585,'multiline':False]
['text':' Remove tokens with cumulative mass above the threshold','line_number':591,'multiline':False]
['text':' Determine which indices to remove','line_number':657,'multiline':False]
['text':' Keep the words with the 'min_tokens_to_keep'-highest probabilities','line_number':661,'multiline':False]
['text':' Safety check','line_number':662,'multiline':False]
['text':' Calculate the adaptive cutoff','line_number':734,'multiline':False]
['text':' Keep the words with the 'min_tokens_to_keep'-highest probabilities','line_number':740,'multiline':False]
['text':' Safety check','line_number':741,'multiline':False]
['text':' Initialize an empty list of dictionaries, one for each hypothesis (index) in the range of num_hypos','line_number':765,'multiline':False]
['text':' Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)','line_number':770,'multiline':False]
['text':' Before decoding the next token, prevent decoding of ngrams that have already appeared','line_number':794,'multiline':False]
['text':' return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet','line_number':805,'multiline':False]
['text':' B x num_beams','line_number':924,'multiline':False]
['text':' Bias variables that will be populated on the first call (for retrocompatibility purposes, the vocabulary size','line_number':1007,'multiline':False]
['text':' is infered in the first usage, which inhibits initializing here)','line_number':1008,'multiline':False]
['text':' 1 - Prepares the bias tensors. This is only needed the first time the logit processor is called.','line_number':1014,'multiline':False]
['text':' 2 - prepares an empty bias to add','line_number':1018,'multiline':False]
['text':' 3 - include the bias from length = 1','line_number':1021,'multiline':False]
['text':' 4 - include the bias from length > 1, after determining which biased sequences may be completed.','line_number':1024,'multiline':False]
['text':' the sequence is of length 1, already applied','line_number':1026,'multiline':False]
['text':' the sequence is longer than the context, ignore','line_number':1028,'multiline':False]
['text':' 5 - apply the bias to the scores','line_number':1042,'multiline':False]
['text':' Check biased tokens out of bounds','line_number':1049,'multiline':False]
['text':' Precompute the bias tensors to be applied. Sequences of length 1 are kept separately, as they can be applied','line_number':1061,'multiline':False]
['text':' with simpler logic.','line_number':1062,'multiline':False]
['text':' Filter EOS token from bad_words_ids','line_number':1148,'multiline':False]
['text':' Forbidding a sequence is equivalent to setting its bias to -inf','line_number':1157,'multiline':False]
['text':' hamming diversity: penalise using same token in current group which was used in previous groups at','line_number':1355,'multiline':False]
['text':' the same time step','line_number':1356,'multiline':False]
['text':' predicted tokens of last time step of previous groups','line_number':1367,'multiline':False]
['text':' set all nan values to 0.0','line_number':1483,'multiline':False]
['text':' set all +/-inf values to max/min possible value','line_number':1486,'multiline':False]
['text':' To support negative logits we compute the penalty of the absolute value and add to the original logit','line_number':1579,'multiline':False]
['text':' support for the kwargs','line_number':1814,'multiline':False]
['text':' this variable is mostly just used for testing','line_number':1819,'multiline':False]
['text':' suppress <|notimestamps|> which is handled by without_timestamps','line_number':1833,'multiline':False]
['text':' timestamps have to appear in pairs, except directly before eos_token; mask logits accordingly','line_number':1836,'multiline':False]
['text':' has to be non-timestamp','line_number':1845,'multiline':False]
['text':' cannot be normal text tokens','line_number':1847,'multiline':False]
['text':' `timestamps` shouldn't decrease; forbid timestamp tokens smaller than the last','line_number':1852,'multiline':False]
['text':' The following lines of code are copied from: https://github.com/openai/whisper/pull/914/files#r1137085090','line_number':1853,'multiline':False]
['text':' Avoid to emit <|0.00|> again','line_number':1857,'multiline':False]
['text':' apply the `max_initial_timestamp` option','line_number':1862,'multiline':False]
['text':' if sum of probability over timestamps is above any other token, sample timestamp','line_number':1870,'multiline':False]
['text':' simple check to make sure we have compatible batch sizes between our','line_number':1931,'multiline':False]
['text':' logits scores (cond + uncond) and input ids (cond only)','line_number':1932,'multiline':False]
['text':' even -> first codebook, odd -> second codebook','line_number':1977,'multiline':False]
['text':' create scores full of -inf except for the eos_token_id','line_number':2136,'multiline':False]
