['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The HuggingFace Inc. team','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Safety check','line_number':139,'multiline':False]
['text':' Boolean mask containing all tokens with a probability less than the last token of the top-k','line_number':140,'multiline':False]
['text':' Also include the token that is higher than top_p (the first false = shift and insert a True on the left)','line_number':177,'multiline':False]
['text':' Ensure min tokens to keep','line_number':180,'multiline':False]
['text':' Mask the values that do not fit the criteria','line_number':189,'multiline':False]
['text':' Undo the topk sorting: converts the 2D matrix of per-row original indices of shape (batch_size, vocab_size)','line_number':192,'multiline':False]
['text':' to a 3D tensor of shape (batch_size, vocab_size, 2) containing the original score coordinate, from which we','line_number':193,'multiline':False]
['text':' can scatter (i.e. `scatter_indices[row, col, :]` is a tensor containing `[row, topk_indices[row, col]]`)','line_number':194,'multiline':False]
['text':' applies eos token masking if the first argument is true','line_number':229,'multiline':False]
['text':' We want to populate the penalties in the positions of `input_ids`. Since XLA can't handle shapes unknown','line_number':255,'multiline':False]
['text':' before runtime, `tf.unique` can't be used. Therefore, we may have redundant updates, when a given row has','line_number':256,'multiline':False]
['text':' the same token multiple times.','line_number':257,'multiline':False]
['text':' Gathers the penalties to apply','line_number':259,'multiline':False]
['text':' Scatters the penalties','line_number':264,'multiline':False]
['text':' the sequence length has dynamic size, hence the dynamic shape','line_number':267,'multiline':False]
['text':' stores the information about bad words in three tensors:','line_number':316,'multiline':False]
['text':' 1. a rectangular tensor with the forbidden sequences (padded with `-1`), for full data comparisons','line_number':317,'multiline':False]
['text':' 2. a tensor with the unpadded length of each forbidden sequence, for quick length comparisons','line_number':319,'multiline':False]
['text':' 3. a tensor containing the last token for each sequence, for easy access to the tokens that may be banned','line_number':324,'multiline':False]
['text':' If the bad sequence only has one token, always mask it','line_number':330,'multiline':False]
['text':' Otherwise, if the bad sequence is longer than the current length they can't ever match','line_number':338,'multiline':False]
['text':' Finaly, runs the actual comparison. Can only be called if the previous comparisons do not yield','line_number':346,'multiline':False]
['text':' an answer (otherwise we get indexing exceptions)','line_number':347,'multiline':False]
['text':' Compares the current row against all bad word sequences, obtaining a mask with the matches.','line_number':362,'multiline':False]
['text':' We want to mask some banned tokens, at a score level. Since the banned tokens depend on the previous','line_number':368,'multiline':False]
['text':' `input_ids`, they may have a different length for each row, and they may even be empty for some rows.','line_number':369,'multiline':False]
['text':' To remain simple and XLA-compatible, we work on a per-row fashion.','line_number':370,'multiline':False]
['text':' TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix it if it becomes','line_number':371,'multiline':False]
['text':' a frequent choke point. (make `cur_len` a tensor?)','line_number':372,'multiline':False]
['text':' Copied from fairseq for no_repeat_ngram in beam_search','line_number':404,'multiline':False]
['text':' return no banned tokens if we haven't generated ngram_size tokens yet','line_number':406,'multiline':False]
['text':' Before decoding the next token, prevent decoding of ngrams that have already appeared','line_number':418,'multiline':False]
['text':' TODO (joao): enable XLA on this logits processor. See discussion and attempts in','line_number':428,'multiline':False]
['text':' https://github.com/huggingface/transformers/pull/16974','line_number':429,'multiline':False]
['text':' create banned_tokens boolean mask','line_number':436,'multiline':False]
['text':' sets the score to 0 in the bos_token_id column','line_number':465,'multiline':False]
['text':' sets the score to -inf everywhere else','line_number':467,'multiline':False]
['text':' sets the score to 0 in the eos_token_id column','line_number':498,'multiline':False]
['text':' sets the score to -inf everywhere else','line_number':500,'multiline':False]
['text':' Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the','line_number':558,'multiline':False]
['text':' index of the array corresponds to the index of the token to be forced, for XLA compatibility.','line_number':559,'multiline':False]
['text':' Indexes without forced tokens will have an negative value.','line_number':560,'multiline':False]
['text':' If the current length is geq than the length of force_token_array, the processor does nothing.','line_number':580,'multiline':False]
['text':' Otherwise, it may force a certain token.','line_number':582,'multiline':False]
['text':' Only valid (positive) tokens are forced','line_number':585,'multiline':False]
['text':' Otherwise, the processor does nothing.','line_number':587,'multiline':False]
