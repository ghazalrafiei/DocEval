['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent','line_number':558,'multiline':False]
['text':' to a beam search approach were the first (and only) beam is always selected','line_number':559,'multiline':False]
['text':' 2. reshape scores as [batch_size, vocab_size, # generation steps] with # generation steps being','line_number':563,'multiline':False]
['text':' seq_len - input_length','line_number':564,'multiline':False]
['text':' 3. Optionally normalize the logits (across the vocab dimension)','line_number':568,'multiline':False]
['text':' 4. cut beam_indices to longest beam length','line_number':572,'multiline':False]
['text':' 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards','line_number':580,'multiline':False]
['text':' 6. Define which indices contributed to scores','line_number':583,'multiline':False]
['text':' 7. Compute scores','line_number':589,'multiline':False]
['text':' 8. Mask out transition_scores of beams that stopped early','line_number':592,'multiline':False]
['text':' Excludes arguments that are handled before calling any model function','line_number':624,'multiline':False]
['text':' `kwargs`/`model_kwargs` is often used to handle optional forward pass inputs like `attention_mask`. If','line_number':631,'multiline':False]
['text':' `prepare_inputs_for_generation` doesn't accept them, then a stricter check can be made ;)','line_number':632,'multiline':False]
['text':' 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call','line_number':714,'multiline':False]
['text':' priority: `generation_config` argument > `model.generation_config` (the default generation config)','line_number':717,'multiline':False]
['text':' legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,','line_number':719,'multiline':False]
['text':' two conditions must be met','line_number':720,'multiline':False]
['text':' 1) the generation config must have been created from the model config (`_from_model_config` field);','line_number':721,'multiline':False]
['text':' 2) the generation config must have seen no modification since its creation (the hash is the same).','line_number':722,'multiline':False]
['text':' All unused kwargs must be model kwargs','line_number':738,'multiline':False]
['text':' 2. Cast input dtypes to tf.int32 unless they're floats (which happens for some image models)','line_number':742,'multiline':False]
['text':' 3. Set generation parameters if not already defined','line_number':765,'multiline':False]
['text':' 4. Define model inputs','line_number':786,'multiline':False]
['text':' inputs_ids now has to be defined and cannot be None anymore','line_number':790,'multiline':False]
['text':' 5. Prepare other model kwargs','line_number':793,'multiline':False]
['text':' decoder-only models should use left-padding for generation','line_number':806,'multiline':False]
['text':' if model is encoder decoder encoder_outputs are created and added to `model_kwargs`','line_number':816,'multiline':False]
['text':' 6. Prepare model inputs which will be used for auto-regressive generation','line_number':821,'multiline':False]
['text':' 7. Prepare `max_length` depending on other stopping criteria.','line_number':833,'multiline':False]
['text':' 20 is the default max_length of the generation config','line_number':837,'multiline':False]
['text':' If the input length is a tensor (i.e. dynamic length), skip length checks','line_number':853,'multiline':False]
['text':' 8. determine generation mode','line_number':871,'multiline':False]
['text':' 9. prepare distribution pre_processing samplers','line_number':892,'multiline':False]
['text':' 10. go into different generation modes','line_number':899,'multiline':False]
['text':' 11. run greedy search','line_number':906,'multiline':False]
['text':' 11. run contrastive search','line_number':923,'multiline':False]
['text':' 11. prepare logits warper','line_number':937,'multiline':False]
['text':' 12. expand input_ids with `num_return_sequences` additional sequences per batch','line_number':940,'multiline':False]
['text':' 13. run sample','line_number':948,'multiline':False]
['text':' 11. broadcast inputs to the desired number of beams','line_number':970,'multiline':False]
['text':' 12. run beam search','line_number':979,'multiline':False]
['text':' 11. prepare logits warper','line_number':1002,'multiline':False]
['text':' 12. broadcast inputs to the desired number of beams','line_number':1005,'multiline':False]
['text':' 13. run beam sample (beam search with sampling)','line_number':1014,'multiline':False]
['text':' Check if input is input_ids and padded -> only then is attention_mask defined','line_number':1041,'multiline':False]
['text':' 1. get encoder and store encoder outputs','line_number':1050,'multiline':False]
['text':' 2. prepare encoder args and encoder kwargs from model kwargs','line_number':1053,'multiline':False]
['text':' 3. vision models don't use `attention_mask`.','line_number':1067,'multiline':False]
['text':' in Keras, the first input must always be passed','line_number':1070,'multiline':False]
['text':' 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,','line_number':1086,'multiline':False]
['text':' we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.','line_number':1087,'multiline':False]
['text':' 2. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.','line_number':1095,'multiline':False]
['text':' no user input -> use decoder_start_token_id as decoder_input_ids','line_number':1099,'multiline':False]
['text':' user input but doesn't start with decoder_start_token_id -> prepend decoder_start_token_id (and adjust','line_number':1102,'multiline':False]
['text':' decoder_attention_mask if provided)','line_number':1103,'multiline':False]
['text':' retrieve decoder_start_token_id for encoder-decoder models','line_number':1117,'multiline':False]
['text':' fall back to bos_token_id if necessary','line_number':1118,'multiline':False]
['text':' 1. retrieve all kwargs that are non-None or non-model input related.','line_number':1182,'multiline':False]
['text':' some encoder-decoder models have different names for model and encoder','line_number':1183,'multiline':False]
['text':' 2. check whether model_input_name is passed as kwarg','line_number':1196,'multiline':False]
['text':' if yes and `inputs` is None use kwarg inputs','line_number':1197,'multiline':False]
['text':' 3. In the presence of `inputs_embeds` for text models:','line_number':1207,'multiline':False]
['text':' - decoder-only models should complain if the user attempts to pass `inputs_embeds`, but the model','line_number':1208,'multiline':False]
['text':' doesn't have its forwarding implemented. `inputs_embeds` is kept in `model_kwargs` and can coexist with','line_number':1209,'multiline':False]
['text':' input_ids (`inputs_embeds` will be used in the 1st generation step, as opposed to `input_ids`)','line_number':1210,'multiline':False]
['text':' - encoder-decoder models should complain if the user attempts to pass `inputs_embeds` and `input_ids`, and','line_number':1211,'multiline':False]
['text':' pull the former to inputs. It will be used in place of `input_ids` to get the encoder hidden states.','line_number':1212,'multiline':False]
['text':' In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of','line_number':1224,'multiline':False]
['text':' the attention mask) can rely on the actual model input.','line_number':1225,'multiline':False]
['text':' 4. if `inputs` is still None, try to create `input_ids` from BOS token','line_number':1234,'multiline':False]
['text':' make dummy input_ids with value -100, as a sanity check ensuring that they won't be used for encoding','line_number':1251,'multiline':False]
['text':' If there is some tensor in `model_kwargs`, we can infer the batch size from it. This is helpful with','line_number':1258,'multiline':False]
['text':' soft-prompting or in multimodal implementations built on top of decoder-only language models.','line_number':1259,'multiline':False]
['text':' update past_key_values','line_number':1281,'multiline':False]
['text':' update attention mask','line_number':1284,'multiline':False]
['text':' One 1 for decoder_start_token_id, 0s for the currently-unfilled locations in the past_key_values tensor,','line_number':1307,'multiline':False]
['text':' 1s for the actual input_ids','line_number':1308,'multiline':False]
['text':' 0s for the currently-unfilled locations in the past_key_values tensor, 1s for the actual input_ids','line_number':1320,'multiline':False]
['text':' Write the last slice to the first open location in the padded past_key_values array','line_number':1374,'multiline':False]
['text':' and then truncate the last slice off the array','line_number':1375,'multiline':False]
['text':' Write the last slice to the first open location in the padded past_key_values array','line_number':1385,'multiline':False]
['text':' and then truncate the last slice off the array','line_number':1386,'multiline':False]
['text':' The padded version of `past_key_values` has a length of `max_length - 1`, as `past_key_values` holds information relative to','line_number':1401,'multiline':False]
['text':' previous autoregressive generation steps (step 0 has no past_key_values, step 1 has 1 past_key_values value, ..., the last step','line_number':1402,'multiline':False]
['text':' has `max_length - 1` past_key_values values).','line_number':1403,'multiline':False]
['text':' The new index of past_key_values to be filled corresponds to the current length of the sequence, with two','line_number':1408,'multiline':False]
['text':' subtractions: -1 because past_key_values holds information regarding previous generation steps (read comment above)','line_number':1409,'multiline':False]
['text':' and -1 again because in an array the index is the length of the array minus 1.','line_number':1410,'multiline':False]
['text':' sets the updated variables (mask and past_key_values)','line_number':1415,'multiline':False]
['text':' instantiate warpers list','line_number':1430,'multiline':False]
['text':' In beam methods, we need to keep at least one non-eos token to explore continuations that might have a','line_number':1433,'multiline':False]
['text':' better score (i.e. keep len(generation_config.eos_token_id) + 1)','line_number':1434,'multiline':False]
['text':' instantiate processors list','line_number':1463,'multiline':False]
['text':' generation starts after the last token that is forced','line_number':1496,'multiline':False]
['text':' 1. init greedy_search values','line_number':1607,'multiline':False]
['text':' TODO (Joao): fix cache format or find programatic way to detect cache index','line_number':1629,'multiline':False]
['text':' GPT2 and other models has a slightly different cache structure, with a different batch axis','line_number':1630,'multiline':False]
['text':' some models, like XLNet, need more than the last token in the presence of past_key_values','line_number':1633,'multiline':False]
['text':' 2. init `attentions`, `hidden_states`, and `scores` tuples','line_number':1636,'multiline':False]
['text':' 3. init tensors to use for "xla-compileable" generate function','line_number':1642,'multiline':False]
['text':' initialize `generated` (`input_ids` padded with `pad_token_id`), `finished_sequences`','line_number':1645,'multiline':False]
['text':' 4. define "xla-compile-able" stop-condition and auto-regressive function','line_number':1650,'multiline':False]
['text':' define condition fn','line_number':1651,'multiline':False]
['text':' define condition fn','line_number':1656,'multiline':False]
['text':' forward pass to get next token logits','line_number':1664,'multiline':False]
['text':' pre-process distribution','line_number':1673,'multiline':False]
['text':' Store scores, attentions and hidden_states when required','line_number':1676,'multiline':False]
['text':' argmax','line_number':1692,'multiline':False]
['text':' update `generated` and `cur_len`','line_number':1708,'multiline':False]
['text':' update model_kwargs','line_number':1713,'multiline':False]
['text':' if we don't cache past_key_values key values we need the whole input','line_number':1728,'multiline':False]
['text':' let's throw out `past_key_values` since we don't want `None` tensors','line_number':1730,'multiline':False]
['text':' 5. run generation','line_number':1735,'multiline':False]
['text':' 1st generation step has to be run before to initialize `past_key_values`','line_number':1736,'multiline':False]
['text':' 2-to-n generation steps can then be run in autoregressive fashion','line_number':1741,'multiline':False]
['text':' only in case 1st generation step does NOT yield EOS token though','line_number':1742,'multiline':False]
['text':' 6. prepare outputs','line_number':1751,'multiline':False]
['text':' cut for backward compatibility','line_number':1753,'multiline':False]
['text':' if model is an encoder-decoder, retrieve encoder attention weights','line_number':1758,'multiline':False]
['text':' and hidden states','line_number':1759,'multiline':False]
['text':' 1. init greedy_search values','line_number':1890,'multiline':False]
['text':' TODO (Joao): fix cache format or find programatic way to detect cache index','line_number':1913,'multiline':False]
['text':' GPT2 and other models has a slightly different cache structure, with a different batch axis','line_number':1914,'multiline':False]
['text':' some models, like XLNet, need more than the last token in the presence of past_key_values','line_number':1917,'multiline':False]
['text':' 2. init `attentions`, `hidden_states`, and `scores` tuples','line_number':1920,'multiline':False]
['text':' 3. init tensors to use for "xla-compileable" generate function','line_number':1926,'multiline':False]
['text':' initialize `generated` (pre-populated with `pad_token_id`), `finished_sequences`','line_number':1929,'multiline':False]
['text':' 4. define "xla-compile-able" stop-condition and auto-regressive function','line_number':1934,'multiline':False]
['text':' forward pass to get next token logits','line_number':1944,'multiline':False]
['text':' pre-process distribution','line_number':1953,'multiline':False]
['text':' Store scores, attentions and hidden_states when required','line_number':1957,'multiline':False]
['text':' sample','line_number':1973,'multiline':False]
['text':' update `generated` and `cur_len`','line_number':1998,'multiline':False]
['text':' update model_kwargs','line_number':2003,'multiline':False]
['text':' if we don't cache past_key_values key values we need the whole input','line_number':2018,'multiline':False]
['text':' let's throw out `past_key_values` since we don't want `None` tensors','line_number':2020,'multiline':False]
['text':' 5. run generation','line_number':2025,'multiline':False]
['text':' 1st generation step has to be run before to initialize `past_key_values`','line_number':2026,'multiline':False]
['text':' 2-to-n generation steps can then be run in autoregressive fashion','line_number':2031,'multiline':False]
['text':' only in case 1st generation step does NOT yield EOS token though','line_number':2032,'multiline':False]
['text':' 6. prepare outputs','line_number':2041,'multiline':False]
['text':' cut for backward compatibility','line_number':2043,'multiline':False]
['text':' if model is an encoder-decoder, retrieve encoder attention weights','line_number':2048,'multiline':False]
['text':' and hidden states','line_number':2049,'multiline':False]
['text':' pushes all dimentions before the batch to the end, so we get (batch, beam_id, ...)','line_number':2085,'multiline':False]
['text':' transposes back to the original dimensions','line_number':2091,'multiline':False]
['text':' 1. init beam_search values','line_number':2225,'multiline':False]
['text':' TODO (Joao): fix cache format or find programatic way to detect cache index','line_number':2256,'multiline':False]
['text':' GPT2 and other models has a slightly different cache structure, with a different batch axis','line_number':2257,'multiline':False]
['text':' some models, like XLNet, need more than the last token in the presence of past_key_values','line_number':2260,'multiline':False]
['text':' 2. init `attentions`, `hidden_states`, and `scores` tuples','line_number':2263,'multiline':False]
['text':' 3. init tensors to use for "xla-compileable" generate function','line_number':2269,'multiline':False]
['text':' store the prompt length of decoder','line_number':2271,'multiline':False]
['text':' per batch, beam-item holding current token in loop, pre-populated with `pad_token_id`','line_number':2274,'multiline':False]
['text':' per batch,beam-item state bit indicating if sentence has finished.','line_number':2281,'multiline':False]
['text':' per batch, beam-item score, logprobs','line_number':2284,'multiline':False]
['text':' per batch beam indices','line_number':2290,'multiline':False]
['text':' flatten beam dim','line_number':2294,'multiline':False]
['text':' 4. define "xla-compile-able" stop-condition and auto-regressive function','line_number':2302,'multiline':False]
['text':' define stop-condition and auto-regressive function','line_number':2303,'multiline':False]
['text':' 1. is less than max length?','line_number':2320,'multiline':False]
['text':' 2. can the new beams still improve?','line_number':2323,'multiline':False]
['text':' early_stopping == False -> apply heuristic = always get the best score from `cur_len - decoder_prompt_len`. See the discussion','line_number':2324,'multiline':False]
['text':' below for more details.','line_number':2325,'multiline':False]
['text':' https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565','line_number':2326,'multiline':False]
['text':' early_stopping == "never" -> compute the best score from max_length or cur_len, depending on the sign of','line_number':2327,'multiline':False]
['text':'   length_penalty. Positive length_penalty favors longer sequences, thus we use max_length there.','line_number':2328,'multiline':False]
['text':' 3. is there still a beam that has not finished?','line_number':2340,'multiline':False]
['text':' 1. Forward current tokens','line_number':2361,'multiline':False]
['text':' 2. Compute log probs','line_number':2377,'multiline':False]
['text':' get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and','line_number':2378,'multiline':False]
['text':' add new logprobs to existing running logprobs scores.','line_number':2379,'multiline':False]
['text':' Store scores, attentions and hidden_states when required','line_number':2391,'multiline':False]
['text':' 3. Retrieve top-K','line_number':2413,'multiline':False]
['text':' Each item in batch has num_beams * vocab_size candidate sequences. For each item, get the top 2*k','line_number':2414,'multiline':False]
['text':' candidates with the highest log-probabilities. We gather the top 2*K beams here so that even if the','line_number':2415,'multiline':False]
['text':' best K sequences reach EOS simultaneously, we have another K sequences remaining to continue the live','line_number':2416,'multiline':False]
['text':' beam search.','line_number':2417,'multiline':False]
['text':' Gather the top 2*K scores from _all_ beams.','line_number':2418,'multiline':False]
['text':' Gather 2*k top beams.','line_number':2419,'multiline':False]
['text':' Recover the beam index by floor division.','line_number':2420,'multiline':False]
['text':' Recover token id by modulo division and expand Id array for broadcasting.','line_number':2421,'multiline':False]
['text':' Update sequences for the 2*K top-k new sequences.','line_number':2422,'multiline':False]
['text':' writes the new token','line_number':2434,'multiline':False]
['text':' we want to store the beam indices with batch information -> real beam index = beam index % num beams','line_number':2446,'multiline':False]
['text':' 4. Check which sequences have ended','line_number':2464,'multiline':False]
['text':' Update current sequences: Did the top `num_beams` sequences reach an end marker?','line_number':2465,'multiline':False]
['text':' To prevent these just finished sequences from being added to the current sequences','line_number':2466,'multiline':False]
['text':' set of active beam search sequences, set their log probs to a very large negative value.','line_number':2467,'multiline':False]
['text':' non-top `num_beams` eos tokens can't be used to finish a beam, but the others can't be used in the next','line_number':2486,'multiline':False]
['text':' running sentences either','line_number':2487,'multiline':False]
['text':' 5. Get running sequences scores for next','line_number':2490,'multiline':False]
['text':' Determine the top k beam indices (from top 2*k beams) from log probs and gather top k beams','line_number':2491,'multiline':False]
['text':' (from top 2*k beams).','line_number':2492,'multiline':False]
['text':' 6. Process topk logits','line_number':2498,'multiline':False]
['text':' Further process log probs:','line_number':2499,'multiline':False]
['text':' - add length penalty','line_number':2500,'multiline':False]
['text':' - make sure no scores can be added anymore if beam is full','line_number':2501,'multiline':False]
['text':' - make sure still running sequences cannot be chosen as finalized beam','line_number':2502,'multiline':False]
['text':' 7. Get scores, sequences, is sentence finished for next.','line_number':2512,'multiline':False]
['text':' Combine sequences, scores, and flags along the beam dimension and compare new finished sequence scores','line_number':2513,'multiline':False]
['text':' to existing finished scores and select the best from the new set of beams','line_number':2514,'multiline':False]
['text':' 8. Prepare data for the next iteration','line_number':2524,'multiline':False]
['text':' Determine the top k beam indices from the original set of all beams. With these, gather the top k','line_number':2525,'multiline':False]
['text':' beam-associated caches.','line_number':2526,'multiline':False]
['text':' if we don't cache past_key_values key values we need the whole input','line_number':2554,'multiline':False]
['text':' let's throw out `past_key_values` since we don't want `None` tensors','line_number':2556,'multiline':False]
['text':' 5. run generation','line_number':2572,'multiline':False]
['text':' 1st generation step has to be run before to initialize `past_key_values` (if active)','line_number':2573,'multiline':False]
['text':' 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does','line_number':2598,'multiline':False]
['text':' NOT yield EOS token though)','line_number':2599,'multiline':False]
['text':' 6. prepare outputs','line_number':2630,'multiline':False]
['text':' Account for the edge-case where there are no finished sequences for a particular batch item. If so, return','line_number':2631,'multiline':False]
['text':' running sequences for that batch item.','line_number':2632,'multiline':False]
['text':' Apply the length penalty so that running scores match the finalized scores if they are used','line_number':2637,'multiline':False]
['text':' Take best beams for each batch (the score is sorted in descending order)','line_number':2641,'multiline':False]
['text':' Cut for backward compatibility','line_number':2647,'multiline':False]
['text':' if model is an encoder-decoder, retrieve encoder attention weights and hidden states','line_number':2653,'multiline':False]
['text':' 1. init greedy_search values','line_number':2767,'multiline':False]
['text':' In contrastive search, we always use cache','line_number':2787,'multiline':False]
['text':' TODO (Joao): fix cache format or find programatic way to detect cache index','line_number':2791,'multiline':False]
['text':' GPT2 and other models has a slightly different cache structure, with a different batch axis','line_number':2792,'multiline':False]
['text':' 2. init `attentions`, `hidden_states`, and `scores` tuples','line_number':2796,'multiline':False]
['text':' 3. init tensors to use for "xla-compileable" generate function','line_number':2802,'multiline':False]
['text':' initialize `generated` (`input_ids` padded with `pad_token_id`), `finished_sequences`','line_number':2805,'multiline':False]
['text':' 4. define "xla-compile-able" stop-condition and auto-regressive function','line_number':2810,'multiline':False]
['text':' define condition fn','line_number':2811,'multiline':False]
['text':' define condition fn','line_number':2818,'multiline':False]
['text':' if the first step in the loop, encode all the prefix and obtain: (1) past_key_values;','line_number':2824,'multiline':False]
['text':' (2) last_hidden_states; (3) logit_for_next_step; (4) update model kwargs for the next step','line_number':2825,'multiline':False]
['text':' prepare inputs','line_number':2827,'multiline':False]
['text':' encode the given prefix and prepare model inputs; encoder-decoder model process the prefix and save','line_number':2832,'multiline':False]
['text':' the `encoder_outputs`','line_number':2833,'multiline':False]
['text':' last decoder hidden states will be used to compute the degeneration penalty (cosine similarity with','line_number':2838,'multiline':False]
['text':' previous tokens)','line_number':2839,'multiline':False]
['text':' XLA: last_hidden_states normally grows at each step, but in XLA it is padded so as to be used across','line_number':2845,'multiline':False]
['text':' iterations (with fixed shapes)','line_number':2846,'multiline':False]
['text':' next logit for contrastive search to select top-k candidate tokens','line_number':2850,'multiline':False]
['text':' Expands model inputs top_k times, for batched forward passes (akin to beam search).','line_number':2868,'multiline':False]
['text':' contrastive_search main logic start:','line_number':2892,'multiline':False]
['text':' contrastive search decoding consists of two steps: (1) candidate tokens recall; (2) candidate re-rank by','line_number':2893,'multiline':False]
['text':' degeneration penalty','line_number':2894,'multiline':False]
['text':' Store scores, attentions and hidden_states when required','line_number':2901,'multiline':False]
['text':' Replicates the new past_key_values to match the `top_k` candidates','line_number':2917,'multiline':False]
['text':' compute the candidate tokens by the language model and collects their hidden_states','line_number':2922,'multiline':False]
['text':' name is different for encoder-decoder and decoder-only models','line_number':2932,'multiline':False]
['text':' compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the','line_number':2941,'multiline':False]
['text':' model confidence','line_number':2942,'multiline':False]
['text':' converts indices to a dimension of top_k to the stacked top_k * batch_size dimension, for indexing','line_number':2945,'multiline':False]
['text':' without a need to reshape on tensors that have these two dimensions stacked','line_number':2946,'multiline':False]
['text':' prepare for the next step: (1) next token_id; (2) past_key_values; (3) last_hidden_states for computing','line_number':2949,'multiline':False]
['text':' the degeneration penalty; (4) logits for selecting next top-k candidates; (5) selected tokens scores','line_number':2950,'multiline':False]
['text':' (model confidence minus degeneration penalty); (6) decoder hidden_states','line_number':2951,'multiline':False]
['text':' XLA: last_hidden_states normally grows at each step, but in XLA it is padded so as to be used across','line_number':2955,'multiline':False]
['text':' iterations (with fixed shapes)','line_number':2956,'multiline':False]
['text':' Rebuilds the relevant parts of the model output for the selected token, for use in the next iteration','line_number':2968,'multiline':False]
['text':' contrastive_search main logic end','line_number':2992,'multiline':False]
['text':' update `generated` and `cur_len`','line_number':3007,'multiline':False]
['text':' NOTE: 1) relative to other generation strategies, contrastive search is always running forward','line_number':3013,'multiline':False]
['text':' passes one step ahead -- hence the `cur_len=cur_len + 1`; 2) the attention mask here is expanded from','line_number':3014,'multiline':False]
['text':' [batch_size, ...] to [batch_size*top_k, ...] -- hence the `batch_size=batch_size * top_k`','line_number':3015,'multiline':False]
['text':' 5. run generation','line_number':3037,'multiline':False]
['text':' 1st generation step has to be run before to initialize `past_key_values`','line_number':3038,'multiline':False]
['text':' 2-to-n generation steps can then be run in autoregressive fashion','line_number':3043,'multiline':False]
['text':' only in case 1st generation step does NOT yield EOS token though','line_number':3044,'multiline':False]
['text':' 6. prepare outputs','line_number':3053,'multiline':False]
['text':' cut for backward compatibility','line_number':3055,'multiline':False]
['text':' if model is an encoder-decoder, retrieve encoder attention weights','line_number':3060,'multiline':False]
['text':' and hidden states','line_number':3061,'multiline':False]
['text':' Safety check','line_number':3111,'multiline':False]
['text':' Remove all tokens with a probability less than the last token of the top-k','line_number':3112,'multiline':False]
['text':' expects logits to be of dim (batch_size, vocab_size)','line_number':3119,'multiline':False]
['text':' Remove tokens with cumulative probability above the threshold (token with 0 are kept)','line_number':3123,'multiline':False]
['text':' Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)','line_number':3127,'multiline':False]
['text':' Shift the indices to the right to keep also the first token above the threshold','line_number':3136,'multiline':False]
['text':' scatter sorted tensors to original indexing','line_number':3141,'multiline':False]
['text':' broadcast batch dim to shape','line_number':3149,'multiline':False]
['text':' transform batch_indices to pair_indices','line_number':3151,'multiline':False]
['text':' scatter values to pair indices','line_number':3153,'multiline':False]
