['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Inc. team','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' self._beam_hyps[i*self.num_beam_groups+j] is the beam_hyps of the j-th group in the i-th mini-batch.','line_number':182,'multiline':False]
['text':' If group_beam_search is not used, the list consists of `batch_size` beam_hyps.','line_number':183,'multiline':False]
['text':' self._done[i*self.num_beam_groups+j] indicates whether the generation of the beam_hyps of the j-th group','line_number':193,'multiline':False]
['text':' in the i-th mini-batch is complete.','line_number':194,'multiline':False]
['text':' add up to the length which the next_scores is calculated on (including decoder prompt)','line_number':227,'multiline':False]
['text':' pad the batch','line_number':258,'multiline':False]
['text':' next tokens for this sentence','line_number':264,'multiline':False]
['text':' add to generated hypotheses if end of sentence','line_number':270,'multiline':False]
['text':' if beam_token does not belong to top num_beams tokens, it should not be added','line_number':272,'multiline':False]
['text':' add next predicted token since it is not eos_token','line_number':289,'multiline':False]
['text':' once the beam for next step is full, don't add more tokens to it.','line_number':295,'multiline':False]
['text':' Check if we are done so that we can save a pad step if all(done)','line_number':305,'multiline':False]
['text':' finalize all open beam hypotheses and add to generated hypotheses','line_number':335,'multiline':False]
['text':' all open beam hypotheses are added to the beam hypothesis','line_number':340,'multiline':False]
['text':' beam hypothesis class automatically keeps the best beams','line_number':341,'multiline':False]
['text':' select the best hypotheses','line_number':350,'multiline':False]
['text':' retrieve best hypotheses','line_number':356,'multiline':False]
['text':' append hyp to lists','line_number':368,'multiline':False]
['text':' append indices to list','line_number':371,'multiline':False]
['text':' prepare for adding eos','line_number':376,'multiline':False]
['text':' shorter batches are padded if needed','line_number':386,'multiline':False]
['text':' fill with hypotheses and eos_token_id if the latter fits in','line_number':395,'multiline':False]
['text':' inserting only the first eos_token_id','line_number':403,'multiline':False]
['text':' add up to the length which the next_scores is calculated on (including decoder prompt)','line_number':560,'multiline':False]
['text':' pad the batch','line_number':590,'multiline':False]
['text':' next tokens for this sentence.','line_number':596,'multiline':False]
['text':' add to generated hypotheses if end of sentence','line_number':602,'multiline':False]
['text':' if beam_token does not belong to top num_beams tokens, it should not be added','line_number':604,'multiline':False]
['text':' add next predicted token since it is not eos_token','line_number':624,'multiline':False]
['text':' once the beam for next step is full, don't add more tokens to it.','line_number':630,'multiline':False]
['text':' Check if we are done so that we can save a pad step if all(done)','line_number':653,'multiline':False]
['text':' sent_beam_tokens are the next {num_beams} number of tokens that are under consideration for this beam','line_number':676,'multiline':False]
['text':' (candidate next tokens)','line_number':677,'multiline':False]
['text':' 1. Adding "advance_tokens"','line_number':679,'multiline':False]
['text':'     using ConstraintStateList.advance(), we propose new tokens to be added into this "candidate list" that will','line_number':680,'multiline':False]
['text':'     advance us in fulfilling the constraints.','line_number':681,'multiline':False]
['text':' 2. Selecting best candidates such that we end up with highest probable candidates','line_number':683,'multiline':False]
['text':'     that fulfill our constraints.','line_number':684,'multiline':False]
['text':' initialize states','line_number':689,'multiline':False]
['text':' need to make new hypothesis that advance the constraints','line_number':698,'multiline':False]
['text':' pre_seq = ith sequence generated before this step.','line_number':707,'multiline':False]
['text':' input_ids -> (topk) generic beam search best model next tokens','line_number':709,'multiline':False]
['text':'           -> (advance) constraints forcing the next token','line_number':710,'multiline':False]
['text':' either way, we need to sort them into "banks" later, so store a "ConstraintListState" for all types of','line_number':711,'multiline':False]
['text':' hypotheses.','line_number':712,'multiline':False]
['text':' since adding each `advance_token` leads to a different hypothesis, create new state instance.','line_number':723,'multiline':False]
['text':' prevent duplicates, which are basically bound to happen in this process.','line_number':729,'multiline':False]
['text':' idx -> global idx across all the batches','line_number':731,'multiline':False]
['text':' Basically, `sent_beam_indices` often chooses very little among `input_ids` the generated sequences that','line_number':736,'multiline':False]
['text':' actually fulfill our constraints. For example, let constraints == ["loves pies"] and','line_number':737,'multiline':False]
['text':'     pre_seq_1 = "The child loves pies and" pre_seq_2 = "The child plays in the playground and"','line_number':739,'multiline':False]
['text':' Without this step, if `sent_beam_indices` is something like [1,1], then','line_number':741,'multiline':False]
['text':'     1. `pre_seq_1` won't be added to the list of (topk) hypothesis since it's not in the indices and','line_number':742,'multiline':False]
['text':'     2.  it won't be added to the list of (advance) hypothesis since it's completed already. (this is','line_number':743,'multiline':False]
['text':'         the else part of `if constraints_completed[seq_idx]`)','line_number':744,'multiline':False]
['text':'     3. it ends up simply getting removed from consideration.','line_number':745,'multiline':False]
['text':' #3 might be fine and actually desired, since it's likely that it's a low-probability output anyways,','line_number':747,'multiline':False]
['text':' especially if it's not in the list of `sent_beam_indices`. But this often leads to lengthened beam','line_number':748,'multiline':False]
['text':' search times, since completed sequences keep getting removed after all this effort for constrained','line_number':749,'multiline':False]
['text':' generation.','line_number':750,'multiline':False]
['text':' Here, we basically take `pre_seq_1` and to "push" it into the considered list of hypotheses, by simply','line_number':752,'multiline':False]
['text':' appending the next likely token in the vocabulary and adding it to the list of hypotheses.','line_number':753,'multiline':False]
['text':' some next probable token','line_number':755,'multiline':False]
['text':' but still don't want to have duplicates','line_number':764,'multiline':False]
['text':' Then we end up with {sorted among bank C}, {sorted among bank C-1}, ..., {sorted among bank 0}','line_number':785,'multiline':False]
['text':' finalize all open beam hypotheses and add to generated hypotheses','line_number':824,'multiline':False]
['text':' all open beam hypotheses are added to the beam hypothesis','line_number':829,'multiline':False]
['text':' beam hypothesis class automatically keeps the best beams','line_number':830,'multiline':False]
['text':' due to overly complex constraints or other factors, sometimes we can't gaurantee a successful','line_number':845,'multiline':False]
['text':' generation. In these cases we simply return the highest scoring outputs.','line_number':846,'multiline':False]
['text':' select the best hypotheses','line_number':858,'multiline':False]
['text':' retrieve best hypotheses','line_number':864,'multiline':False]
['text':' append to lists','line_number':874,'multiline':False]
['text':' append indices to list','line_number':877,'multiline':False]
['text':' prepare for adding eos','line_number':882,'multiline':False]
['text':' shorter batches are padded if needed','line_number':893,'multiline':False]
['text':' fill with hypotheses and eos_token_id if the latter fits in','line_number':902,'multiline':False]
['text':' inserting only the first eos_token_id','line_number':910,'multiline':False]
['text':' This 'else' case exists for retrocompatibility','line_number':958,'multiline':False]
['text':' `True`: stop as soon as at least `num_beams` hypotheses are finished','line_number':980,'multiline':False]
['text':' `False`: heuristic -- compute best possible score from `cur_len`, even though it is not entirely accurate','line_number':983,'multiline':False]
['text':'  when `length_penalty` is positive. See the discussion below for more details.','line_number':984,'multiline':False]
['text':' https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565','line_number':985,'multiline':False]
['text':' `"never"`: compute the best possible score, depending on the signal of `length_penalty`','line_number':990,'multiline':False]
['text':' `length_penalty` > 0.0 -> max denominator is obtaned from `max_length`, not from `cur_len` -> min','line_number':992,'multiline':False]
['text':' abs(`highest_attainable_score`) is obtained -> `highest_attainable_score` is negative, hence we obtain','line_number':993,'multiline':False]
['text':' its max this way','line_number':994,'multiline':False]
['text':' the opposite logic applies here (max `highest_attainable_score` from `cur_len`)','line_number':1001,'multiline':False]
