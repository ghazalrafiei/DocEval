['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa','line_number':46,'multiline':False]
['text':' See all GPTNeoX models at https://huggingface.co/models?filter=gpt_neox','line_number':57,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':61,'multiline':False]
['text':' Compute QKV','line_number':174,'multiline':False]
['text':' Attention heads [batch, seq_len, hidden_size]','line_number':175,'multiline':False]
['text':'   --> [batch, seq_len, (np * 3 * head_size)]','line_number':176,'multiline':False]
['text':' [batch, seq_len, (num_heads * 3 * head_size)]','line_number':179,'multiline':False]
['text':'   --> [batch, seq_len, num_heads, 3 * head_size]','line_number':180,'multiline':False]
['text':' [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]','line_number':184,'multiline':False]
['text':' Compute rotary embeddings on rotary_ndims','line_number':189,'multiline':False]
['text':' Compute token offset for rotary embeddings (when decoding)','line_number':195,'multiline':False]
['text':' Cache QKV values','line_number':204,'multiline':False]
['text':' Compute attention','line_number':212,'multiline':False]
['text':' Reshape outputs','line_number':215,'multiline':False]
['text':' tensor: [bs, seq_len, hidden_size]','line_number':230,'multiline':False]
['text':' -> [bs, seq_len, num_attention_heads, attn_head_size]','line_number':232,'multiline':False]
['text':' -> [bs, num_attention_heads, seq_len, attn_head_size]','line_number':234,'multiline':False]
['text':' tensor [bs, num_attention_heads, seq_len, attn_head_size]','line_number':243,'multiline':False]
['text':' -> [bs, seq_len, num_attention_heads, attn_head_size]','line_number':245,'multiline':False]
['text':' -> [bs, seq_len, hidden_size]','line_number':247,'multiline':False]
['text':' q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]','line_number':251,'multiline':False]
['text':' compute causal mask from causal mask buffer','line_number':252,'multiline':False]
['text':' dynamically increase the causal mask with the key length, if needed.','line_number':256,'multiline':False]
['text':' Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.','line_number':280,'multiline':False]
['text':' Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`','line_number':281,'multiline':False]
['text':' Apply the attention mask','line_number':286,'multiline':False]
['text':' Mask heads if we want to','line_number':292,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':312,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':313,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':314,'multiline':False]
['text':' Compute QKV','line_number':329,'multiline':False]
['text':' Attention heads [batch, seq_len, hidden_size]','line_number':330,'multiline':False]
['text':'   --> [batch, seq_len, (np * 3 * head_size)]','line_number':331,'multiline':False]
['text':' [batch, seq_len, (num_heads * 3 * head_size)]','line_number':334,'multiline':False]
['text':'   --> [batch, seq_len, num_heads, 3 * head_size]','line_number':335,'multiline':False]
['text':' [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]','line_number':339,'multiline':False]
['text':' Compute rotary embeddings on rotary_ndims','line_number':346,'multiline':False]
['text':' Compute token offset for rotary embeddings (when decoding)','line_number':352,'multiline':False]
['text':' Cache QKV values','line_number':361,'multiline':False]
['text':' GPT-neo-X casts query and key in fp32 to apply rotary embedding in full precision','line_number':369,'multiline':False]
['text':' Permute to get the expected shape for Flash Attention','line_number':376,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':381,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':382,'multiline':False]
['text':' cast them back in float16 / bfloat16 just to be sure everything works as expected.','line_number':383,'multiline':False]
['text':' This might slowdown training & inference so it is recommended to not cast the LayerNorms','line_number':384,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':387,'multiline':False]
['text':' Compute attention','line_number':405,'multiline':False]
['text':' Reshape outputs','line_number':410,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':422,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':448,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':451,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input with num_heads->num_attention_heads','line_number':482,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':504,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':508,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LlamaRotary->GPTNeoXRotary','line_number':527,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':538,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':548,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':554,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->GPTNeoX','line_number':564,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':578,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->GPTNeoX','line_number':584,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':605,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':618,'multiline':False]
['text':' output_attn: attn_output, present, (attn_weights)','line_number':697,'multiline':False]
['text':' pseudocode:','line_number':702,'multiline':False]
['text':' x = x + attn(ln1(x)) + mlp(ln2(x))','line_number':703,'multiline':False]
['text':' pseudocode:','line_number':708,'multiline':False]
['text':' x = x + attn(ln1(x))','line_number':709,'multiline':False]
['text':' x = x + mlp(ln2(x))','line_number':710,'multiline':False]
['text':' hidden_states, present, (attn_weights)','line_number':717,'multiline':False]
['text':' hidden_states, (attn_weights)','line_number':719,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':794,'multiline':False]
['text':' Attention mask.','line_number':863,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':870,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':871,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':872,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':873,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':874,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':877,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':878,'multiline':False]
['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':879,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':880,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':881,'multiline':False]
['text':' fp16 compatibility','line_number':882,'multiline':False]
['text':' Prepare head mask if needed','line_number':885,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':886,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':887,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':888,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':889,'multiline':False]
['text':' Add last hidden state','line_number':939,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':966,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':1050,'multiline':False]
['text':' we are doing next-token prediction; shift prediction scores and input ids by one','line_number':1052,'multiline':False]
['text':' cut decoder_input_ids if past is used','line_number':1074,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1078,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1082,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':1089,'multiline':False]
['text':' if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly','line_number':1095,'multiline':False]
['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1099,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1146,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1260,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1342,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':1398,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1403,'multiline':False]
