['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 SHI Labs and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':59,'multiline':False]
['text':' Base docstring','line_number':62,'multiline':False]
['text':' Image classification docstring','line_number':66,'multiline':False]
['text':' See all Nat models at https://huggingface.co/models?filter=nat','line_number':73,'multiline':False]
['text':' drop_path and NatDropPath are from the timm library.','line_number':76,'multiline':False]
['text':' TODO: Support arbitrary patch sizes.','line_number':222,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.drop_path','line_number':265,'multiline':False]
['text':' work with diff dim tensors, not just 2D ConvNets','line_number':279,'multiline':False]
['text':' binarize','line_number':281,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Nat','line_number':286,'multiline':False]
['text':' rpb is learnable relative positional biases; same concept is used Swin.','line_number':314,'multiline':False]
['text':' Apply the scale factor before computing attention weights. It's usually more efficient because','line_number':337,'multiline':False]
['text':' attention weights are typically a bigger tensor compared to query.','line_number':338,'multiline':False]
['text':' It gives identical results because scalars are commutable in matrix multiplication.','line_number':339,'multiline':False]
['text':' Compute NA between "query" and "key" to get the raw attention scores, and add relative positional biases.','line_number':342,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':345,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':348,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':349,'multiline':False]
['text':' Prune linear layers','line_number':389,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':395,'multiline':False]
['text':' add attentions if we output them','line_number':407,'multiline':False]
['text':' pad hidden_states if they are smaller than kernel size','line_number':475,'multiline':False]
['text':' patch merging layer','line_number':522,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':584,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':596,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':601,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':633,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':634,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':692,'multiline':False]
['text':' Classifier head','line_number':777,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':782,'multiline':False]
['text':' Add layer norms to hidden states of out_features','line_number':870,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':876,'multiline':False]
['text':' TODO can we simplify this?','line_number':939,'multiline':False]
