['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all DistilBERT models at https://huggingface.co/models?filter=distilbert','line_number':71,'multiline':False]
['text':' assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'','line_number':179,'multiline':False]
['text':' assert key.size() == value.size()','line_number':180,'multiline':False]
['text':' (bs, n_heads, q_length, dim_per_head)','line_number':193,'multiline':False]
['text':' (bs, n_heads, k_length, dim_per_head)','line_number':194,'multiline':False]
['text':' (bs, n_heads, k_length, dim_per_head)','line_number':195,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':199,'multiline':False]
['text':' (bs, n_heads, qlen, klen)','line_number':200,'multiline':False]
['text':' scores.masked_fill_(mask, -float('inf'))            # (bs, n_heads, q_length, k_length)','line_number':201,'multiline':False]
['text':' (bs, n_heads, qlen, klen)','line_number':205,'multiline':False]
['text':' (bs, n_heads, qlen, klen)','line_number':206,'multiline':False]
['text':' Mask heads if we want to','line_number':208,'multiline':False]
['text':' (bs, n_heads, qlen, dim_per_head)','line_number':212,'multiline':False]
['text':' (bs, q_length, dim)','line_number':213,'multiline':False]
['text':' (bs, q_length, dim)','line_number':214,'multiline':False]
['text':' removed: src_enc=None, src_len=None','line_number':293,'multiline':False]
['text':' Self-Attention','line_number':302,'multiline':False]
['text':' (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)','line_number':305,'multiline':False]
['text':' To handle these `output_attentions` or `output_hidden_states` cases returning tuples','line_number':306,'multiline':False]
['text':' assert type(sa_output) == tuple','line_number':307,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':309,'multiline':False]
['text':' Feed Forward Network','line_number':311,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':312,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':313,'multiline':False]
['text':' docstyle-ignore','line_number':348,'multiline':False]
['text':' Add last layer','line_number':382,'multiline':False]
['text':' Embeddings','line_number':415,'multiline':False]
['text':' Encoder','line_number':416,'multiline':False]
['text':' (bs, seq_length)','line_number':450,'multiline':False]
['text':' Prepare head mask if needed','line_number':454,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':455,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':456,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':457,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':458,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':464,'multiline':False]
['text':' last-layer hidden-state, (all hidden_states), (all attentions)','line_number':475,'multiline':False]
['text':' INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #','line_number':489,'multiline':False]
['text':' Embeddings','line_number':592,'multiline':False]
['text':' The output weights are the same as the input embeddings, but there is','line_number':640,'multiline':False]
['text':' an output-only bias for each token.','line_number':641,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':732,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':733,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':734,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':735,'multiline':False]
['text':' (bs, seq_len, dim)','line_number':829,'multiline':False]
['text':' (bs, dim)','line_number':830,'multiline':False]
['text':' (bs, dim)','line_number':831,'multiline':False]
['text':' (bs, dim)','line_number':832,'multiline':False]
['text':' (bs, dim)','line_number':833,'multiline':False]
['text':' (bs, seq_len, dim)','line_number':1017,'multiline':False]
['text':' (bs, dim)','line_number':1018,'multiline':False]
['text':' (bs, dim)','line_number':1019,'multiline':False]
['text':' (bs, dim)','line_number':1020,'multiline':False]
['text':' (bs, max_query_len, dim)','line_number':1111,'multiline':False]
['text':' (bs, max_query_len, dim)','line_number':1112,'multiline':False]
['text':' (bs, max_query_len, 2)','line_number':1113,'multiline':False]
