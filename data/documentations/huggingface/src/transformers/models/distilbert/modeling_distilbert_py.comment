['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa','line_number':58,'multiline':False]
['text':' See all DistilBERT models at https://huggingface.co/models?filter=distilbert','line_number':73,'multiline':False]
['text':' UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE #','line_number':77,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':80,'multiline':False]
['text':' (bs, max_seq_length, dim)','line_number':141,'multiline':False]
['text':' Setting the position-ids to the registered buffer in constructor, it helps','line_number':145,'multiline':False]
['text':' when tracing the model without passing position-ids, solves','line_number':146,'multiline':False]
['text':' isues similar to issue #5664','line_number':147,'multiline':False]
['text':' (max_seq_length)','line_number':151,'multiline':False]
['text':' (bs, max_seq_length)','line_number':152,'multiline':False]
['text':' (bs, max_seq_length, dim)','line_number':154,'multiline':False]
['text':' (bs, max_seq_length, dim)','line_number':156,'multiline':False]
['text':' (bs, max_seq_length, dim)','line_number':157,'multiline':False]
['text':' (bs, max_seq_length, dim)','line_number':158,'multiline':False]
['text':' Have an even number of multi heads that divide the dimensions','line_number':172,'multiline':False]
['text':' Raise value errors for even multi-head attention nodes','line_number':174,'multiline':False]
['text':' Prune linear layers','line_number':191,'multiline':False]
['text':' Update hyper params','line_number':196,'multiline':False]
['text':' assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'','line_number':223,'multiline':False]
['text':' assert key.size() == value.size()','line_number':224,'multiline':False]
['text':' (bs, n_heads, q_length, dim_per_head)','line_number':238,'multiline':False]
['text':' (bs, n_heads, k_length, dim_per_head)','line_number':239,'multiline':False]
['text':' (bs, n_heads, k_length, dim_per_head)','line_number':240,'multiline':False]
['text':' (bs, n_heads, q_length, dim_per_head)','line_number':242,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':243,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':244,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':247,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':249,'multiline':False]
['text':' (bs, n_heads, q_length, k_length)','line_number':250,'multiline':False]
['text':' Mask heads if we want to','line_number':252,'multiline':False]
['text':' (bs, n_heads, q_length, dim_per_head)','line_number':256,'multiline':False]
['text':' (bs, q_length, dim)','line_number':257,'multiline':False]
['text':' (bs, q_length, dim)','line_number':258,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':273,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':277,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':278,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':279,'multiline':False]
['text':' Flash attention requires the input to have the shape','line_number':310,'multiline':False]
['text':' batch_size x seq_length x head_dim x hidden_dim','line_number':311,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':318,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':319,'multiline':False]
['text':' cast them back in the correct dtype just to be sure everything works as expected.','line_number':320,'multiline':False]
['text':' This might slowdown training & inference so it is recommended to not cast the LayerNorms','line_number':321,'multiline':False]
['text':' in fp32. (LlamaRMSNorm handles it correctly)','line_number':322,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':325,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward with causal=True->causal=False','line_number':353,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':379,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':382,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input with num_heads->n_heads','line_number':413,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':435,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':439,'multiline':False]
['text':' Have an even number of Configure multi-heads','line_number':484,'multiline':False]
['text':' Self-Attention','line_number':510,'multiline':False]
['text':' (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)','line_number':520,'multiline':False]
['text':' To handle these `output_attentions` or `output_hidden_states` cases returning tuples','line_number':521,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':526,'multiline':False]
['text':' Feed Forward Network','line_number':528,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':529,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':530,'multiline':False]
['text':' docstyle-ignore','line_number':553,'multiline':False]
['text':' Add last layer','line_number':604,'multiline':False]
['text':' INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #','line_number':615,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':631,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':632,'multiline':False]
['text':' Embeddings','line_number':706,'multiline':False]
['text':' Encoder','line_number':707,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':710,'multiline':False]
['text':' no resizing needs to be done if the length stays the same','line_number':733,'multiline':False]
['text':' move position_embeddings to correct device','line_number':758,'multiline':False]
['text':' Prepare head mask if needed','line_number':809,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':812,'multiline':False]
['text':' (bs, seq_length)','line_number':818,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':847,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':912,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':913,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':914,'multiline':False]
['text':' (bs, seq_length, dim)','line_number':915,'multiline':False]
['text':' (bs, seq_length, vocab_size)','line_number':916,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':952,'multiline':False]
['text':' (bs, seq_len, dim)','line_number':1009,'multiline':False]
['text':' (bs, dim)','line_number':1010,'multiline':False]
['text':' (bs, dim)','line_number':1011,'multiline':False]
['text':' (bs, dim)','line_number':1012,'multiline':False]
['text':' (bs, dim)','line_number':1013,'multiline':False]
['text':' (bs, num_labels)','line_number':1014,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1069,'multiline':False]
['text':' (bs, max_query_len, dim)','line_number':1131,'multiline':False]
['text':' (bs, max_query_len, dim)','line_number':1133,'multiline':False]
['text':' (bs, max_query_len, 2)','line_number':1134,'multiline':False]
['text':' (bs, max_query_len)','line_number':1136,'multiline':False]
['text':' (bs, max_query_len)','line_number':1137,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':1141,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1146,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1185,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1279,'multiline':False]
['text':' (bs * num_choices, seq_len, dim)','line_number':1367,'multiline':False]
['text':' (bs * num_choices, dim)','line_number':1368,'multiline':False]
['text':' (bs * num_choices, dim)','line_number':1369,'multiline':False]
['text':' (bs * num_choices, dim)','line_number':1370,'multiline':False]
['text':' (bs * num_choices, dim)','line_number':1371,'multiline':False]
['text':' (bs * num_choices, 1)','line_number':1372,'multiline':False]
['text':' (bs, num_choices)','line_number':1374,'multiline':False]
