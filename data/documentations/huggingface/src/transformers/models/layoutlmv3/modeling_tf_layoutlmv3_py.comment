['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Microsoft Research and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all LayoutLMv3 models at https://huggingface.co/models?filter=layoutlmv3','line_number':62,'multiline':False]
['text':' When running on CPU, `tf.keras.layers.Conv2D` doesn't support `NCHW` format.','line_number':93,'multiline':False]
['text':' So change the input format from `NCHW` to `NHWC`.','line_number':94,'multiline':False]
['text':' LayoutLMv1 sums the spatial embeddings, but LayoutLMv3 concatenates them.','line_number':192,'multiline':False]
['text':' batch_size','line_number':341,'multiline':False]
['text':' seq_length','line_number':342,'multiline':False]
['text':' batch_size, num_heads, seq_length, attention_head_size','line_number':347,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':376,'multiline':False]
['text':' batch_size, num_heads, attention_head_size, seq_length','line_number':380,'multiline':False]
['text':' Apply the attention mask (is precomputed for all layers in TFLayoutLMv3Model call() function)','line_number':389,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':392,'multiline':False]
['text':' Use the trick of CogView paper to stabilize training.','line_number':393,'multiline':False]
['text':' Mask heads if we want to.','line_number':398,'multiline':False]
['text':' batch_size, seq_length, num_heads, attention_head_size','line_number':405,'multiline':False]
['text':' batch_size, seq_length, num_heads * attention_head_size','line_number':409,'multiline':False]
['text':' Copied from models.roberta.modeling_tf_roberta.TFRobertaSelfOutput','line_number':430,'multiline':False]
['text':' add attentions if we output them','line_number':487,'multiline':False]
['text':' Copied from models.roberta.modeling_tf_bert.TFRobertaIntermediate','line_number':502,'multiline':False]
['text':' Copied from models.roberta.modeling_tf_bert.TFRobertaOutput','line_number':532,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':590,'multiline':False]
['text':' the negative relative positions are assigned to the interval [0, num_buckets / 2]','line_number':647,'multiline':False]
['text':' we deal with this by assigning absolute relative positions to the interval [0, num_buckets / 2]','line_number':648,'multiline':False]
['text':' and then offsetting the positive relative positions by num_buckets / 2 at the end','line_number':649,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':653,'multiline':False]
['text':' the other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':657,'multiline':False]
['text':' scale is [0, num_buckets - max_exact_buckets]','line_number':662,'multiline':False]
['text':' scale is [max_exact_buckets, num_buckets]','line_number':663,'multiline':False]
['text':' batch_size, seq_length, seq_length, num_heads --> batch_size, num_heads, seq_length, seq_length','line_number':682,'multiline':False]
['text':' left','line_number':691,'multiline':False]
['text':' bottom','line_number':692,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertMainLayer._prune_heads','line_number':855,'multiline':False]
['text':' We should not hardcode max_len to 1000, but it is done by the reference implementation,','line_number':864,'multiline':False]
['text':' so we keep it for compatibility with the pretrained weights. The more correct approach','line_number':865,'multiline':False]
['text':' would have been to pass on max_len=config.max_2d_position_embeddings - 1.','line_number':866,'multiline':False]
['text':' (width, width + 1)','line_number':871,'multiline':False]
['text':' (height + 1, height)','line_number':875,'multiline':False]
['text':' add [CLS] token','line_number':895,'multiline':False]
['text':' add position embeddings','line_number':900,'multiline':False]
['text':' Adapted from transformers.modelling_utils.ModuleUtilsMixin.get_extended_attention_mask','line_number':908,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':912,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':913,'multiline':False]
['text':' Provided a padding mask of dimensions [batch_size, seq_length].','line_number':917,'multiline':False]
['text':' Make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length].','line_number':918,'multiline':False]
['text':' (batch_size, 1, seq_length)','line_number':919,'multiline':False]
['text':' (batch_size, 1, 1, seq_length)','line_number':920,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':924,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':925,'multiline':False]
['text':' positions we want to attend and -10000.0 for masked positions.','line_number':926,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':927,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':928,'multiline':False]
['text':' Gets a tensor with masks for each head (H).','line_number':940,'multiline':False]
['text':' 1, num_heads','line_number':941,'multiline':False]
['text':' 1, 1, num_heads','line_number':942,'multiline':False]
['text':' 1, 1, num_heads, 1','line_number':943,'multiline':False]
['text':' 1, 1, num_heads, 1, 1','line_number':944,'multiline':False]
['text':' seq_length, 1, num_heads, 1, 1','line_number':947,'multiline':False]
['text':' Gets a tensor with masks for each layer (L) and head (H).','line_number':949,'multiline':False]
['text':' seq_length, 1, num_heads','line_number':950,'multiline':False]
['text':' seq_length, 1, num_heads, 1','line_number':951,'multiline':False]
['text':' seq_length, 1, num_heads, 1, 1','line_number':952,'multiline':False]
['text':' This method can be called with a variety of modalities:','line_number':980,'multiline':False]
['text':' 1. text + layout','line_number':981,'multiline':False]
['text':' 2. text + layout + image','line_number':982,'multiline':False]
['text':' 3. image','line_number':983,'multiline':False]
['text':' The complexity of this method is mostly just due to handling of these different modalities.','line_number':984,'multiline':False]
['text':' Determine which integer dtype to use.','line_number':1005,'multiline':False]
['text':' embed image','line_number':1037,'multiline':False]
['text':' calculate attention mask','line_number':1040,'multiline':False]
['text':' calculate bounding boxes','line_number':1047,'multiline':False]
['text':' calculate position IDs','line_number':1055,'multiline':False]
['text':' calculate embeddings','line_number':1068,'multiline':False]
['text':' Prepare head mask if needed','line_number':1087,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':1088,'multiline':False]
['text':' attention_probs has shape batch_size x num_heads x seq_length x seq_length','line_number':1089,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':1090,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':1091,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':1262,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':1400,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':1515,'multiline':False]
['text':' only take the text part of the output representations','line_number':1610,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':1652,'multiline':False]
