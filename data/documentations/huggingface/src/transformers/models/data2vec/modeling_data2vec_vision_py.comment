['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Meta Platforms and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':49,'multiline':False]
['text':' Base docstring','line_number':52,'multiline':False]
['text':' Image classification docstring','line_number':56,'multiline':False]
['text':' See all Data2VecVision models at https://huggingface.co/models?filter=data2vec-vision','line_number':62,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitModelOutputWithPooling with Beit->Data2VecVision','line_number':67,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.drop_path','line_number':93,'multiline':False]
['text':' work with diff dim tensors, not just 2D ConvNets','line_number':107,'multiline':False]
['text':' binarize','line_number':109,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Data2VecVision','line_number':114,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitEmbeddings with Beit->Data2VecVision','line_number':129,'multiline':False]
['text':' replace the masked visual tokens by mask_tokens','line_number':160,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitPatchEmbeddings with Beit->Data2VecVision','line_number':175,'multiline':False]
['text':' interpolate the position embedding to the corresponding size','line_number':211,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitSelfAttention with Beit->Data2VecVision','line_number':225,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':268,'multiline':False]
['text':' Add relative position bias if present.','line_number':273,'multiline':False]
['text':' Add shared relative position bias if provided.','line_number':277,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':281,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':284,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':285,'multiline':False]
['text':' Mask heads if we want to','line_number':288,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitSelfOutput with Beit->Data2VecVision','line_number':303,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitAttention with Beit->Data2VecVision','line_number':322,'multiline':False]
['text':' Prune linear layers','line_number':337,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':343,'multiline':False]
['text':' add attentions if we output them','line_number':359,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitIntermediate with Beit->Data2VecVision','line_number':363,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitOutput with Beit->Data2VecVision','line_number':380,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitLayer with Beit->Data2VecVision,BEiT->Data2VecVision','line_number':394,'multiline':False]
['text':' in Data2VecVision, layernorm is applied before self-attention','line_number':426,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':432,'multiline':False]
['text':' apply lambda_1 if present','line_number':434,'multiline':False]
['text':' first residual connection','line_number':438,'multiline':False]
['text':' in Data2VecVision, layernorm is also applied after self-attention','line_number':441,'multiline':False]
['text':' second residual connection','line_number':450,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitRelativePositionBias with Beit->Data2VecVision','line_number':458,'multiline':False]
['text':' 2*Wh-1 * 2*Ww-1, nH','line_number':466,'multiline':False]
['text':' cls to token & token 2 cls & cls to cls','line_number':467,'multiline':False]
['text':' get pair-wise relative position index for each token inside the window','line_number':469,'multiline':False]
['text':' 2, Wh, Ww','line_number':472,'multiline':False]
['text':' 2, Wh*Ww','line_number':473,'multiline':False]
['text':' 2, Wh*Ww, Wh*Ww','line_number':474,'multiline':False]
['text':' Wh*Ww, Wh*Ww, 2','line_number':475,'multiline':False]
['text':' shift to start from 0','line_number':476,'multiline':False]
['text':' Wh*Ww, Wh*Ww','line_number':482,'multiline':False]
['text':' Wh*Ww,Wh*Ww,nH','line_number':492,'multiline':False]
['text':' nH, Wh*Ww, Wh*Ww','line_number':494,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitEncoder with Beit->Data2VecVision','line_number':497,'multiline':False]
['text':' stochastic depth decay rule','line_number':507,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitPreTrainedModel with Beit->Data2VecVision,beit->data2vec_vision','line_number':568,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':583,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':584,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitModel with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,True->False','line_number':635,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':649,'multiline':False]
['text':' Prepare head mask if needed','line_number':693,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':694,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':695,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':696,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':697,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitPooler with Beit->Data2VecVision','line_number':725,'multiline':False]
['text':' Mean pool the final hidden states of the patch tokens','line_number':735,'multiline':False]
['text':' Pool by simply taking the final hidden state of the [CLS] token','line_number':739,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitForImageClassification with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,beit->data2vec_vision','line_number':752,'multiline':False]
['text':' Classifier head','line_number':760,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':763,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitConvModule with Beit->Data2VecVision','line_number':835,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitPyramidPoolingBlock with Beit->Data2VecVision','line_number':873,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitPyramidPoolingModule with Beit->Data2VecVision','line_number':891,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitUperHead with Beit->Data2VecVision','line_number':931,'multiline':False]
['text':' e.g. (1, 2, 3, 6)','line_number':943,'multiline':False]
['text':' e.g. [768, 768, 768, 768]','line_number':944,'multiline':False]
['text':' PSP Module','line_number':949,'multiline':False]
['text':' FPN Module','line_number':962,'multiline':False]
['text':' skip the top layer','line_number':965,'multiline':False]
['text':' build laterals','line_number':988,'multiline':False]
['text':' build top-down path','line_number':993,'multiline':False]
['text':' build outputs','line_number':1001,'multiline':False]
['text':' append psp feature','line_number':1003,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitFCNHead with Beit->Data2VecVision','line_number':1017,'multiline':False]
['text':' just take the relevant feature maps','line_number':1072,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitForSemanticSegmentation with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,microsoft/beit-base-finetuned-ade-640-640->facebook/data2vec-vision-base,beit->data2vec_vision','line_number':1087,'multiline':False]
['text':' FPNs','line_number':1095,'multiline':False]
['text':' Semantic segmentation head(s)','line_number':1114,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1118,'multiline':False]
['text':' upsample logits to the images' original size','line_number':1122,'multiline':False]
['text':' compute weighted loss','line_number':1130,'multiline':False]
['text':' we need the intermediate hidden states','line_number':1185,'multiline':False]
['text':' only keep certain features, and reshape','line_number':1191,'multiline':False]
['text':' note that we do +1 as the encoder_hidden_states also includes the initial embeddings','line_number':1192,'multiline':False]
['text':' apply FPNs','line_number':1200,'multiline':False]
