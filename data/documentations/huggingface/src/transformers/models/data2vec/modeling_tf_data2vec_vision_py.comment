['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Meta Platforms and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':56,'multiline':False]
['text':' Base docstring','line_number':59,'multiline':False]
['text':' Image classification docstring','line_number':63,'multiline':False]
['text':' See all Data2VecVision models at https://huggingface.co/models?filter=data2vec-vision','line_number':69,'multiline':False]
['text':' replace the masked visual tokens by mask_tokens','line_number':182,'multiline':False]
['text':' since TF doesn't support eager tensor assignment','line_number':185,'multiline':False]
['text':' following torch.nn.Linear','line_number':224,'multiline':False]
['text':' When running on CPU, `tf.keras.layers.Conv2D` doesn't support `NCHW` format.','line_number':243,'multiline':False]
['text':' So change the input format from `NCHW` to `NHWC`.','line_number':244,'multiline':False]
['text':' shape = (batch_size, in_height, in_width, in_channels=num_channels)','line_number':245,'multiline':False]
['text':' Change the 2D spatial dimensions to a single temporal dimension.','line_number':250,'multiline':False]
['text':' shape = (batch_size, num_patches, out_channels=embed_dim)','line_number':251,'multiline':False]
['text':' Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]','line_number':303,'multiline':False]
['text':' Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]','line_number':306,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':325,'multiline':False]
['text':' (batch size, num_heads, seq_len_q, seq_len_k)','line_number':326,'multiline':False]
['text':' Add relative position bias if present.','line_number':330,'multiline':False]
['text':' Passing `0.0` to the `relative_position_bias()` layer because otherwise Keras','line_number':332,'multiline':False]
['text':' might complain about `Layer.call()` not being invoked properly. In this case this input','line_number':333,'multiline':False]
['text':' i.e., 0.0 is not going to be used in any calculations so we're safe.','line_number':334,'multiline':False]
['text':' Add shared relative position bias if provided.','line_number':337,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':341,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':344,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':345,'multiline':False]
['text':' Mask heads if we want to','line_number':348,'multiline':False]
['text':' (batch_size, seq_len_q, all_head_size)','line_number':355,'multiline':False]
['text':' add attentions if we output them','line_number':437,'multiline':False]
['text':' Copied from transformers.models.vit.modeling_tf_vit.TFViTIntermediate with ViT->Data2VecVision','line_number':453,'multiline':False]
['text':' Using `layers.Activation` instead of `tf.identity` to better control `training`','line_number':527,'multiline':False]
['text':' behaviour.','line_number':528,'multiline':False]
['text':' in Data2VecVision, layernorm is applied before self-attention','line_number':586,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':594,'multiline':False]
['text':' apply lambda_1 if present','line_number':596,'multiline':False]
['text':' first residual connection','line_number':600,'multiline':False]
['text':' in Data2VecVision, layernorm is also applied after self-attention','line_number':603,'multiline':False]
['text':' second residual connection','line_number':612,'multiline':False]
['text':' Taken and modified from here:','line_number':620,'multiline':False]
['text':' https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/beit/beit.py#L28','line_number':621,'multiline':False]
['text':' +3 for cls_token_pos_len','line_number':628,'multiline':False]
['text':' window_size can be something like (14, 14)','line_number':629,'multiline':False]
['text':' [2*Wh-1 * 2*Ww-1, nH]','line_number':640,'multiline':False]
['text':' cls to token & token 2 cls & cls to cls','line_number':641,'multiline':False]
['text':' get pair-wise relative position index for each token inside the window','line_number':646,'multiline':False]
['text':' [2, Wh, Ww]','line_number':648,'multiline':False]
['text':' [2, Wh*Ww]','line_number':649,'multiline':False]
['text':' [2, Wh*Ww, Wh*Ww]','line_number':651,'multiline':False]
['text':' [Wh*Ww, Wh*Ww, 2]','line_number':652,'multiline':False]
['text':' [Wh*Ww, Wh*Ww]','line_number':658,'multiline':False]
['text':' [Wh*Ww + 1, Wh*Ww + 1]','line_number':670,'multiline':False]
['text':' stochastic depth decay rule','line_number':689,'multiline':False]
['text':' Passing `0.0` to the `relative_position_bias()` layer because otherwise Keras','line_number':717,'multiline':False]
['text':' might complain about `Layer.call()` not being invoked properly. In this case this input','line_number':718,'multiline':False]
['text':' i.e., 0.0 is not going to be used in any calculations so we're safe.','line_number':719,'multiline':False]
['text':' We are setting the `data_format` like so because from here on we will revert to the','line_number':775,'multiline':False]
['text':' NCHW output format','line_number':776,'multiline':False]
['text':' Prepare head mask if needed','line_number':809,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':810,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':811,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':812,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':813,'multiline':False]
['text':' Mean pool the final hidden states of the patch tokens','line_number':876,'multiline':False]
['text':' Pool by simply taking the final hidden state of the [CLS] token','line_number':880,'multiline':False]
['text':' Classifier head','line_number':1051,'multiline':False]
['text':' Figure out which axis we're pooling on','line_number':1182,'multiline':False]
['text':' Figure out the potential pooling windows','line_number':1191,'multiline':False]
['text':' This is the key idea - the torch op always uses only two','line_number':1192,'multiline':False]
['text':' consecutive pooling window sizes, like 3 and 4. Therefore,','line_number':1193,'multiline':False]
['text':' if we pool with both possible sizes, we simply need to gather','line_number':1194,'multiline':False]
['text':' the 'correct' pool at each position to reimplement the torch op.','line_number':1195,'multiline':False]
['text':' For resizes to 1, or integer resizes, we can take quick shortcuts','line_number':1207,'multiline':False]
['text':' When upscaling by an integer factor we can also take a quick shortcut','line_number':1220,'multiline':False]
['text':' For non-integer resizes, we pool with both possible window sizes and concatenate them','line_number':1224,'multiline':False]
['text':' When we're actually upscaling instead, then we build the pools a bit differently','line_number':1234,'multiline':False]
['text':' We compute vectors of the start and end positions for each pooling window','line_number':1241,'multiline':False]
['text':' Each (start, end) pair here corresponds to a single output position','line_number':1242,'multiline':False]
['text':' pool_selector is a boolean array of shape (output_dim,) where 1 indicates that output position','line_number':1248,'multiline':False]
['text':' has a big receptive field and 0 indicates that that output position has a small receptive field','line_number':1249,'multiline':False]
['text':' Since we concatenated the small and big pools, we need to do a bit of','line_number':1252,'multiline':False]
['text':' pointer arithmetic to get the indices of the big pools','line_number':1253,'multiline':False]
['text':' Finally, we use the pool_selector to generate a list of indices, one per output position','line_number':1257,'multiline':False]
['text':' Gathering from those indices yields the final, correct pooling','line_number':1260,'multiline':False]
['text':' We break the task down into each possible case','line_number':1269,'multiline':False]
['text':' Firstly, if we're resizing down to 1, it's just tf.reduce_mean','line_number':1270,'multiline':False]
['text':' Secondly, if we're resizing by an integer factor on both dimensions, we can take a quick shortcut','line_number':1277,'multiline':False]
['text':' Finally, if we can't take the shortcut, we do a 1D pool on each axis. pseudo_1d_pool will take a shortcut','line_number':1289,'multiline':False]
['text':' for dimensions where an integer resize is possible. It can also handle upscaling.','line_number':1290,'multiline':False]
['text':' e.g. (1, 2, 3, 6)','line_number':1356,'multiline':False]
['text':' e.g. [768, 768, 768, 768]','line_number':1357,'multiline':False]
['text':' PSP Module','line_number':1361,'multiline':False]
['text':' FPN Module','line_number':1372,'multiline':False]
['text':' skip the top layer','line_number':1375,'multiline':False]
['text':' build laterals','line_number':1407,'multiline':False]
['text':' build top-down path','line_number':1412,'multiline':False]
['text':' build outputs','line_number':1418,'multiline':False]
['text':' append psp feature','line_number':1420,'multiline':False]
['text':' just take the relevant feature maps','line_number':1522,'multiline':False]
['text':' FPNs','line_number':1556,'multiline':False]
['text':' Semantic segmentation head(s)','line_number':1568,'multiline':False]
['text':' upsample logits to the images' original size','line_number':1575,'multiline':False]
['text':' compute weighted loss','line_number':1584,'multiline':False]
['text':' Copied from https://www.tensorflow.org/text/tutorials/transformer#loss_and_metrics.','line_number':1587,'multiline':False]
['text':' Utility to mask the index to ignore during computing the loss.','line_number':1588,'multiline':False]
['text':' we need the intermediate hidden states','line_number':1649,'multiline':False]
['text':' only keep certain features, and reshape','line_number':1654,'multiline':False]
['text':' note that we do +1 as the encoder_hidden_states also includes the initial embeddings','line_number':1655,'multiline':False]
['text':' We do it this way so TF can always infer the non-batch dims at compile time','line_number':1660,'multiline':False]
['text':' apply FPNs','line_number':1666,'multiline':False]
['text':' Tranpose the logits to maintain consistency in the output formats.','line_number':1675,'multiline':False]
