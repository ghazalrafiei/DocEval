['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 Google Research and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' large models','line_number':53,'multiline':False]
['text':' base models','line_number':66,'multiline':False]
['text':' medium models','line_number':79,'multiline':False]
['text':' small models','line_number':92,'multiline':False]
['text':' tiny models','line_number':105,'multiline':False]
['text':' mini models','line_number':118,'multiline':False]
['text':' Additional properties','line_number':372,'multiline':False]
['text':' If the token is part of the never_split set','line_number':434,'multiline':False]
['text':' Input type checking for clearer error','line_number':645,'multiline':False]
['text':' Check that query has a valid type','line_number':648,'multiline':False]
['text':' we pad in batch afterwards','line_number':911,'multiline':False]
['text':' we pad in batch afterwards','line_number':914,'multiline':False]
['text':' we pad in batch afterwards','line_number':915,'multiline':False]
['text':' We convert the whole batch to tensors at the end','line_number':919,'multiline':False]
['text':' simply set the prev_labels to zeros','line_number':1233,'multiline':False]
['text':' FIRST: parse both the table and question in terms of numeric values','line_number':1240,'multiline':False]
['text':' SECOND: add numeric-related features (and not parse them in these functions):','line_number':1245,'multiline':False]
['text':' Load from model defaults','line_number':1250,'multiline':False]
['text':' Check lengths','line_number':1289,'multiline':False]
['text':' Padding','line_number':1299,'multiline':False]
['text':' We could fit the table.','line_number':1362,'multiline':False]
['text':' Try to drop a row to fit the table.','line_number':1365,'multiline':False]
['text':' tokenize column headers','line_number':1388,'multiline':False]
['text':' tokenize cell values','line_number':1396,'multiline':False]
['text':' Two extra spots of SEP and CLS.','line_number':1421,'multiline':False]
['text':' First row is header row.','line_number':1441,'multiline':False]
['text':' Don't add partial words. Find the starting word piece and check if it','line_number':1449,'multiline':False]
['text':' fits in the token budget.','line_number':1450,'multiline':False]
['text':' We don't allow dynamic trimming if a cell_trim_length is set.','line_number':1486,'multiline':False]
['text':' add [CLS] token at the beginning','line_number':1514,'multiline':False]
['text':' add [SEP] token between question and table tokens','line_number':1539,'multiline':False]
['text':' original code from tf_example_utils.py of the original implementation','line_number':1577,'multiline':False]
['text':' first, we add any numeric value spans to the question:','line_number':1640,'multiline':False]
['text':' Create a dictionary that maps a table cell to the set of all relations','line_number':1641,'multiline':False]
['text':' this cell has with any value in the question.','line_number':1642,'multiline':False]
['text':' For each cell add a special feature for all its word pieces.','line_number':1657,'multiline':False]
['text':' We don't search for answers in the header.','line_number':1773,'multiline':False]
['text':' Maps answer coordinates to indexes this can fail if tokens / rows have','line_number':1798,'multiline':False]
['text':' been pruned.','line_number':1799,'multiline':False]
['text':' Load from model defaults','line_number':1871,'multiline':False]
['text':' Initialize attention mask if not present.','line_number':1885,'multiline':False]
['text':' Everything related to converting logits to predictions','line_number':1932,'multiline':False]
['text':' converting to numpy arrays to work with PT/TF','line_number':1979,'multiline':False]
['text':' input data is of type float32','line_number':1984,'multiline':False]
['text':' np.log(np.finfo(np.float32).max) = 88.72284','line_number':1985,'multiline':False]
['text':' Any value over 88.72284 will overflow when passed through the exponential, sending a warning','line_number':1986,'multiline':False]
['text':' We disable this warning by truncating the logits.','line_number':1987,'multiline':False]
['text':' Compute probabilities from token logits','line_number':1990,'multiline':False]
['text':' collect input_ids, segment ids, row ids and column ids of batch. Shape (batch_size, seq_len)','line_number':2002,'multiline':False]
['text':' next, get answer coordinates for every example in the batch','line_number':2008,'multiline':False]
['text':' Select the answers above the classification threshold.','line_number':2030,'multiline':False]
['text':' End of everything related to converting logits to predictions','line_number':2049,'multiline':False]
['text':' Copied from transformers.models.bert.tokenization_bert.BasicTokenizer','line_number':2052,'multiline':False]
['text':' union() returns a new set by concatenating the two sets.','line_number':2101,'multiline':False]
['text':' This was added on November 1st, 2018 for the multilingual and Chinese','line_number':2105,'multiline':False]
['text':' models. This is also applied to the English models now, but it doesn't','line_number':2106,'multiline':False]
['text':' matter since the English models were not trained on any Chinese data','line_number':2107,'multiline':False]
['text':' and generally don't have any Chinese data in them (there are Chinese','line_number':2108,'multiline':False]
['text':' characters in the vocabulary because Wikipedia does have some Chinese','line_number':2109,'multiline':False]
['text':' words in the English Wikipedia.).','line_number':2110,'multiline':False]
['text':' prevents treating the same character with different unicode codepoints as different characters','line_number':2113,'multiline':False]
['text':' This defines a "chinese character" as anything in the CJK Unicode block:','line_number':2178,'multiline':False]
['text':'   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)','line_number':2179,'multiline':False]
['text':'','line_number':2180,'multiline':False]
['text':' Note that the CJK Unicode block is NOT all Japanese and Korean characters,','line_number':2181,'multiline':False]
['text':' despite its name. The modern Korean Hangul alphabet is a different block,','line_number':2182,'multiline':False]
['text':' as is Japanese Hiragana and Katakana. Those alphabets are used to write','line_number':2183,'multiline':False]
['text':' space-separated words, so they are not treated specially and handled','line_number':2184,'multiline':False]
['text':' like the all of the other languages.','line_number':2185,'multiline':False]
['text':'','line_number':2188,'multiline':False]
['text':'','line_number':2189,'multiline':False]
['text':'','line_number':2190,'multiline':False]
['text':'','line_number':2191,'multiline':False]
['text':'','line_number':2192,'multiline':False]
['text':'','line_number':2194,'multiline':False]
['text':'','line_number':2195,'multiline':False]
['text':' Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer','line_number':2214,'multiline':False]
['text':' Below: utilities for TAPAS tokenizer (independent from PyTorch/Tensorflow).','line_number':2272,'multiline':False]
['text':' This includes functions to parse numeric values (dates and numbers) from both the table and questions in order','line_number':2273,'multiline':False]
['text':' to create the column_ranks, inv_column_ranks, numeric_values, numeric values_scale and numeric_relations in','line_number':2274,'multiline':False]
['text':' prepare_for_model of TapasTokenizer.','line_number':2275,'multiline':False]
['text':' These are meant to be used in an academic setup, for production use cases Gold mine or Aqua should be used.','line_number':2276,'multiline':False]
['text':' taken from constants.py of the original implementation','line_number':2279,'multiline':False]
['text':' URL: https://github.com/google-research/tapas/blob/master/tapas/utils/constants.py','line_number':2280,'multiline':False]
['text':' Connects header to cell.','line_number':2282,'multiline':False]
['text':' Connects cell to header.','line_number':2283,'multiline':False]
['text':' Connects query to headers.','line_number':2284,'multiline':False]
['text':' Connects query to cells.','line_number':2285,'multiline':False]
['text':' Connects row to cells.','line_number':2286,'multiline':False]
['text':' Connects cells to row.','line_number':2287,'multiline':False]
['text':' Annotation value is same as cell value','line_number':2288,'multiline':False]
['text':' Annotation value is less than cell value','line_number':2289,'multiline':False]
['text':' Annotation value is greater than cell value','line_number':2290,'multiline':False]
['text':' The original raw question string.','line_number':2321,'multiline':False]
['text':' The question string after normalization.','line_number':2322,'multiline':False]
['text':' Below: all functions from number_utils.py as well as 2 functions (namely get_all_spans and normalize_for_match)','line_number':2326,'multiline':False]
['text':' from text_utils.py of the original implementation. URL's:','line_number':2327,'multiline':False]
['text':' - https://github.com/google-research/tapas/blob/master/tapas/utils/number_utils.py','line_number':2328,'multiline':False]
['text':' - https://github.com/google-research/tapas/blob/master/tapas/utils/text_utils.py','line_number':2329,'multiline':False]
['text':' Constants for parsing date expressions.','line_number':2332,'multiline':False]
['text':' Masks that specify (by a bool) which of (year, month, day) will be populated.','line_number':2333,'multiline':False]
['text':' Pairs of patterns to pass to 'datetime.strptime' and masks specifying which','line_number':2342,'multiline':False]
['text':' fields will be set by the corresponding pattern.','line_number':2343,'multiline':False]
['text':' This mapping is used to convert date patterns to regex patterns.','line_number':2368,'multiline':False]
['text':' Weekday as locale’s full name.','line_number':2370,'multiline':False]
['text':' Month as locale’s full name.','line_number':2371,'multiline':False]
['text':' Year with century as a decimal number.','line_number':2372,'multiline':False]
['text':' Month as locale’s abbreviated name.','line_number':2373,'multiline':False]
['text':' Day of the month as a zero-padded decimal number.','line_number':2374,'multiline':False]
['text':' Month as a zero-padded decimal number.','line_number':2375,'multiline':False]
['text':' Make sure we didn't miss any of the fields.','line_number':2388,'multiline':False]
['text':' Following DynSp:','line_number':2401,'multiline':False]
['text':' https://github.com/Microsoft/DynSP/blob/master/util.py#L414.','line_number':2402,'multiline':False]
['text':' Following DynSp:','line_number':2439,'multiline':False]
['text':' https://github.com/Microsoft/DynSP/blob/master/util.py#L293.','line_number':2440,'multiline':False]
['text':' Doesn't parse ordinal expressions such as '18th of february 1655'.','line_number':2472,'multiline':False]
['text':' Below: all functions from number_annotation_utils.py and 2 functions (namely filter_invalid_unicode','line_number':2604,'multiline':False]
['text':' and filter_invalid_unicode_from_table) from text_utils.py of the original implementation. URL's:','line_number':2605,'multiline':False]
['text':' - https://github.com/google-research/tapas/blob/master/tapas/utils/number_annotation_utils.py','line_number':2606,'multiline':False]
['text':' - https://github.com/google-research/tapas/blob/master/tapas/utils/text_utils.py','line_number':2607,'multiline':False]
['text':' All dates fields are cased to float to produce a simple primitive value.','line_number':2636,'multiline':False]
['text':' Primitive values are simple floats, nothing to do here.','line_number':2674,'multiline':False]
['text':' The type can only be Date at this point which means the primitive type','line_number':2677,'multiline':False]
['text':' is a float triple.','line_number':2678,'multiline':False]
['text':' logging.log_every_n(logging.INFO, f'Can\'t consolidate types: {debug_info} {row_index_to_values} {max_count}', 100)','line_number':2721,'multiline':False]
['text':' Extract the first matching value.','line_number':2736,'multiline':False]
['text':' to do: add table id support','line_number':2801,'multiline':False]
['text':' First, filter table on invalid unicode','line_number':2833,'multiline':False]
['text':' Second, replace cell values by Cell objects','line_number':2836,'multiline':False]
['text':' Third, add numeric_value attributes to these Cell objects','line_number':2841,'multiline':False]
