['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 Google Research and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' soft dependency','line_number':59,'multiline':False]
['text':' On the first call, check whether a compatible version of TensorFlow is installed','line_number':64,'multiline':False]
['text':' TensorFlow Probability depends on a recent stable release of TensorFlow','line_number':65,'multiline':False]
['text':' large models','line_number':78,'multiline':False]
['text':' base models','line_number':84,'multiline':False]
['text':' small models','line_number':90,'multiline':False]
['text':' mini models','line_number':96,'multiline':False]
['text':' tiny models','line_number':102,'multiline':False]
['text':' See all TAPAS models at https://huggingface.co/models?filter=tapas','line_number':108,'multiline':False]
['text':' create absolute position embeddings','line_number':222,'multiline':False]
['text':' when self.config.reset_position_index_per_cell is set to True, create relative position embeddings','line_number':225,'multiline':False]
['text':' shape (batch_size, seq_len)','line_number':227,'multiline':False]
['text':' shape (batch_size, seq_len)','line_number':229,'multiline':False]
['text':' shape (batch_size, seq_len)','line_number':231,'multiline':False]
['text':' shape (max_rows * max_columns,). First absolute position for every cell','line_number':233,'multiline':False]
['text':' ? shape (batch_size, seq_len). First absolute position of the cell for every token','line_number':235,'multiline':False]
['text':' shape (1, seq_len)','line_number':237,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfAttention with Bert->Tapas','line_number':259,'multiline':False]
['text':' Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]','line_number':290,'multiline':False]
['text':' Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]','line_number':293,'multiline':False]
['text':' If this is instantiated as a cross-attention module, the keys','line_number':310,'multiline':False]
['text':' and values come from an encoder; the attention mask needs to be','line_number':311,'multiline':False]
['text':' such that the encoder's padding tokens are not attended to.','line_number':312,'multiline':False]
['text':' reuse k,v, cross_attentions','line_number':316,'multiline':False]
['text':' if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.','line_number':336,'multiline':False]
['text':' Further calls to cross_attention layer can then reuse all cross-attention','line_number':337,'multiline':False]
['text':' key/value_states (first "if" case)','line_number':338,'multiline':False]
['text':' if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of','line_number':339,'multiline':False]
['text':' all previous decoder key/value_states. Further calls to uni-directional self-attention','line_number':340,'multiline':False]
['text':' can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)','line_number':341,'multiline':False]
['text':' if encoder bi-directional self-attention `past_key_value` is always `None`','line_number':342,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':345,'multiline':False]
['text':' (batch size, num_heads, seq_len_q, seq_len_k)','line_number':346,'multiline':False]
['text':' Apply the attention mask is (precomputed for all layers in TFTapasModel call() function)','line_number':352,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':355,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':358,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':359,'multiline':False]
['text':' Mask heads if we want to','line_number':362,'multiline':False]
['text':' (batch_size, seq_len_q, all_head_size)','line_number':369,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfOutput with Bert->Tapas','line_number':392,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertAttention with Bert->Tapas','line_number':423,'multiline':False]
['text':' add attentions (possibly with past_key_value) if we output them','line_number':458,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertIntermediate with Bert->Tapas','line_number':475,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertOutput with Bert->Tapas','line_number':505,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertLayer with Bert->Tapas','line_number':536,'multiline':False]
['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':562,'multiline':False]
['text':' if decoder, the last output is tuple of self-attn cache','line_number':576,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':581,'multiline':False]
['text':' cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple','line_number':591,'multiline':False]
['text':' add cross attentions if we output attention weights','line_number':604,'multiline':False]
['text':' add cross-attn cache to positions 3,4 of present_key_value tuple','line_number':606,'multiline':False]
['text':' add attentions if we output them','line_number':614,'multiline':False]
['text':' if decoder, return the attn key/values as the last output','line_number':616,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertEncoder with Bert->Tapas','line_number':640,'multiline':False]
['text':' Add last layer','line_number':692,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertPooler with Bert->Tapas','line_number':719,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':733,'multiline':False]
['text':' to the first token.','line_number':734,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform with Bert->Tapas','line_number':749,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead with Bert->Tapas','line_number':787,'multiline':False]
['text':' The output weights are the same as the input embeddings, but there is','line_number':797,'multiline':False]
['text':' an output-only bias for each token.','line_number':798,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertMLMHead with Bert->Tapas','line_number':836,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':922,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':923,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':924,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':925,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':926,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':929,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':930,'multiline':False]
['text':' positions we want to attend and -10000.0 for masked positions.','line_number':931,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':932,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':933,'multiline':False]
['text':' Prepare head mask if needed','line_number':939,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':940,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':941,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':942,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':943,'multiline':False]
['text':' cell selection heads','line_number':1295,'multiline':False]
['text':' First, compute the token logits (batch_size, seq_len) - without temperature','line_number':1364,'multiline':False]
['text':' Next, average the logits per cell (batch_size, max_num_cols*max_num_rows)','line_number':1367,'multiline':False]
['text':' Finally, average the logits per column (batch_size, max_num_cols)','line_number':1370,'multiline':False]
['text':' Mask columns that do not appear in the example.','line_number':1377,'multiline':False]
['text':' base model','line_number':1399,'multiline':False]
['text':' dropout','line_number':1402,'multiline':False]
['text':' Construct indices for the table.','line_number':1514,'multiline':False]
['text':' Construct indices for the table.','line_number':1531,'multiline':False]
['text':' Masks.','line_number':1544,'multiline':False]
['text':' Table cells only, without question tokens and table headers.','line_number':1548,'multiline':False]
['text':' <float32>[batch_size, seq_length]','line_number':1551,'multiline':False]
['text':' Mask for cells that exist in the table (i.e. that are not padding).','line_number':1555,'multiline':False]
['text':' Compute logits per token. These are used to select individual cells.','line_number':1558,'multiline':False]
['text':' Compute logits per column. These are used to select a column.','line_number':1561,'multiline':False]
['text':' Aggregate logits.','line_number':1568,'multiline':False]
['text':' Total loss calculation','line_number':1573,'multiline':False]
['text':' Semi-supervised cell selection in case of no aggregation:','line_number':1580,'multiline':False]
['text':' If the answer (the denotation) appears directly in the table we might','line_number':1581,'multiline':False]
['text':' select the answer without applying any aggregation function. There are','line_number':1582,'multiline':False]
['text':' some ambiguous cases, see utils._calculate_aggregate_mask for more info.','line_number':1583,'multiline':False]
['text':' `aggregate_mask` is 1 for examples where we chose to aggregate and 0','line_number':1584,'multiline':False]
['text':'  for examples where we chose to select the answer directly.','line_number':1585,'multiline':False]
['text':' `labels` encodes the positions of the answer appearing in the table.','line_number':1586,'multiline':False]
['text':' <float32>[batch_size]','line_number':1594,'multiline':False]
['text':' Cell selection log-likelihood','line_number':1606,'multiline':False]
['text':' Compute cell selection loss per example.','line_number':1612,'multiline':False]
['text':' Supervised cell selection','line_number':1630,'multiline':False]
['text':' For the not supervised case, do not assign loss for cell selection','line_number':1636,'multiline':False]
['text':' Semi-supervised regression loss and supervised loss for aggregations','line_number':1639,'multiline':False]
['text':' Note that `aggregate_mask` is None if the setting is supervised.','line_number':1642,'multiline':False]
['text':' Add regression loss for numeric answers which require aggregation.','line_number':1673,'multiline':False]
['text':' Zero loss for examples with answer_loss > cutoff.','line_number':1685,'multiline':False]
['text':' if no label ids are provided, set them to zeros in order to properly compute logits','line_number':1695,'multiline':False]
['text':' Beginning of everything related to segmented tensors','line_number':1856,'multiline':False]
['text':' Flatten the batch dimensions, as segments ops do not support batching.','line_number':2012,'multiline':False]
['text':' However if `values` has extra dimensions to the right keep them','line_number':2013,'multiline':False]
['text':' unflattened. Segmented ops support vector-valued operations.','line_number':2014,'multiline':False]
['text':' Unflatten the values.','line_number':2023,'multiline':False]
['text':' First find the column we should select. We use the column with maximum','line_number':2129,'multiline':False]
['text':' number of selected cells.','line_number':2130,'multiline':False]
['text':' Check if there are no selected cells in the column. In that case the model','line_number':2133,'multiline':False]
['text':' should predict the special column id 0, which means "select nothing".','line_number':2134,'multiline':False]
['text':' Reduce the labels and logits to per-cell from per-token.','line_number':2141,'multiline':False]
['text':' Mask for the selected column.','line_number':2145,'multiline':False]
['text':' Compute the log-likelihood for cells, but only for the selected column.','line_number':2149,'multiline':False]
['text':' We need to normalize the loss by the number of cells in the column.','line_number':2153,'multiline':False]
['text':' Set the probs outside the selected column (selected by the *model*)','line_number':2159,'multiline':False]
['text':' to 0. This ensures backwards compatibility with models that select','line_number':2160,'multiline':False]
['text':' cells from multiple columns.','line_number':2161,'multiline':False]
['text':' Never select cells with the special column id 0.','line_number':2166,'multiline':False]
['text':' tf.Tensor(batch_size,)','line_number':2201,'multiline':False]
['text':' Index 0 corresponds to "no aggregation".','line_number':2205,'multiline':False]
['text':' Cell selection examples according to current model.','line_number':2207,'multiline':False]
['text':' Examples with non-empty cell selection supervision.','line_number':2209,'multiline':False]
['text':' Prepare "no aggregation" targets for cell selection examples.','line_number':2247,'multiline':False]
['text':' Use aggregation supervision as the target.','line_number':2250,'multiline':False]
['text':' <float32>[batch_size]','line_number':2256,'multiline':False]
['text':' Accumulate loss only for examples requiring cell selection','line_number':2259,'multiline':False]
['text':' (no aggregation).','line_number':2260,'multiline':False]
['text':' Index 0 corresponds to "no aggregation".','line_number':2281,'multiline':False]
['text':' Predict some aggregation in case of an answer that needs aggregation.','line_number':2283,'multiline':False]
['text':' This increases the probability of all aggregation functions, in a way','line_number':2284,'multiline':False]
['text':' similar to MML, but without considering whether the function gives the','line_number':2285,'multiline':False]
['text':' correct answer.','line_number':2286,'multiline':False]
['text':' Add aggregation loss for numeric answers that need aggregation.','line_number':2323,'multiline':False]
['text':' The token logits where already divided by the temperature and used for','line_number':2353,'multiline':False]
['text':' computing cell selection errors so we need to multiply it again here','line_number':2354,'multiline':False]
['text':' <float32>[batch_size, seq_length]','line_number':2362,'multiline':False]
['text':' Mask non-numeric table values to zero.','line_number':2367,'multiline':False]
['text':' The sum of all probabilities exept that correspond to other cells','line_number':2373,'multiline':False]
['text':' The sum of all probabilities exept that correspond to other cells','line_number':2377,'multiline':False]
['text':' <float32>[batch_size, num_aggregation_labels - 1]','line_number':2390,'multiline':False]
['text':' <float32>[batch_size, num_aggregation_labels - 1]','line_number':2393,'multiline':False]
['text':' float32 (batch_size,)','line_number':2443,'multiline':False]
['text':' <float32>[batch_size]','line_number':2448,'multiline':False]
