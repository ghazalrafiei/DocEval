['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all GroupViT models at https://huggingface.co/models?filter=groupvit','line_number':48,'multiline':False]
['text':' contrastive loss function, adapted from','line_number':52,'multiline':False]
['text':' https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html','line_number':53,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->groupvit','line_number':58,'multiline':False]
['text':' Straight through.','line_number':67,'multiline':False]
['text':' more stable https://github.com/pytorch/pytorch/issues/41663','line_number':76,'multiline':False]
['text':' ~Gumbel(logits,tau)','line_number':83,'multiline':False]
['text':' Straight through.','line_number':87,'multiline':False]
['text':' Reparametrization trick.','line_number':92,'multiline':False]
['text':' number of group token','line_number':118,'multiline':False]
['text':' [batch_size, groups, height*width, groups] -> [batch_size, groups, height, width]','line_number':119,'multiline':False]
['text':' [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]','line_number':140,'multiline':False]
['text':' [batch_size, heightxwidth, num_groups] -> [batch_size, num_groups, heightxwidth] -> [batch_size, num_groups, height, width]','line_number':146,'multiline':False]
['text':' [batch_size, num_groups, height, width]','line_number':150,'multiline':False]
['text':' [batch_size, query_length, channels]','line_number':196,'multiline':False]
['text':' [batch_size, key_length, channels]','line_number':199,'multiline':False]
['text':' [batch_size, key_length, channels]','line_number':202,'multiline':False]
['text':' [batch_size, query_length, key_length]','line_number':205,'multiline':False]
['text':' norm on group_tokens','line_number':224,'multiline':False]
['text':' norm on x','line_number':234,'multiline':False]
['text':' [B, num_output_groups, C] <- [B, num_group_tokens, C]','line_number':250,'multiline':False]
['text':' [batch_size, num_output_groups, channels]','line_number':264,'multiline':False]
['text':' we add a small number to avoid floating point error in the interpolation','line_number':393,'multiline':False]
['text':' see discussion at https://github.com/facebookresearch/dino/issues/8','line_number':394,'multiline':False]
['text':' add positional encoding to each token','line_number':418,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->GroupViT','line_number':429,'multiline':False]
['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':438,'multiline':False]
['text':' get query proj','line_number':625,'multiline':False]
['text':' apply the causal_attention_mask first','line_number':648,'multiline':False]
['text':' this operation is a bit akward, but it's required to','line_number':669,'multiline':False]
['text':' make sure that attn_weights keeps its gradient.','line_number':670,'multiline':False]
['text':' In order to do so, attn_weights have to reshaped','line_number':671,'multiline':False]
['text':' twice and have to be reused in the following','line_number':672,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->GroupViT','line_number':697,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':763,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':764,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder, CLIP_TEXT->GROUPVIT_TEXT','line_number':1043,'multiline':False]
['text':' For `pooled_output` computation','line_number':1053,'multiline':False]
['text':' CLIP's text model uses causal mask, prepare it here.','line_number':1085,'multiline':False]
['text':' https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324','line_number':1086,'multiline':False]
['text':' expand attention_mask','line_number':1090,'multiline':False]
['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':1092,'multiline':False]
['text':' The `eos_token_id` was incorrect before PR #24773: Let's keep what have been done here.','line_number':1108,'multiline':False]
['text':' A CLIP model with such `eos_token_id` in the config can't work correctly with extra new tokens added','line_number':1109,'multiline':False]
['text':' ------------------------------------------------------------','line_number':1110,'multiline':False]
['text':' text_embeds.shape = [batch_size, sequence_length, transformer.width]','line_number':1111,'multiline':False]
['text':' take features from the eot embedding (eot_token is the highest number in each sequence)','line_number':1112,'multiline':False]
['text':' casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14','line_number':1113,'multiline':False]
['text':' The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)','line_number':1119,'multiline':False]
['text':' We need to get the first position of `eos_token_id` value (`pad_token_ids` might equal to `eos_token_id`)','line_number':1122,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1145,'multiline':False]
['text':' normalize the last hidden state','line_number':1235,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1257,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1346,'multiline':False]
['text':' Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1375,'multiline':False]
['text':' Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1426,'multiline':False]
['text':' pooled_output','line_number':1440,'multiline':False]
['text':' Use GROUPVIT model's config for some fields (if specified) instead of those of vision & text components.','line_number':1483,'multiline':False]
['text':' normalized features','line_number':1517,'multiline':False]
['text':' cosine similarity as logits','line_number':1521,'multiline':False]
['text':' grouped features','line_number':1528,'multiline':False]
['text':' [batch_size_image, num_group, hidden_size]','line_number':1529,'multiline':False]
['text':' [batch_size_image*num_group, hidden_size]','line_number':1531,'multiline':False]
['text':' [batch_size_image, num_group, height, width]','line_number':1537,'multiline':False]
['text':' normalized features','line_number':1540,'multiline':False]
['text':' [batch_size_image x num_group, batch_size_text]','line_number':1542,'multiline':False]
['text':' [batch_size_image, batch_size_text, num_group]','line_number':1544,'multiline':False]
['text':' [batch_size_image, batch_size_text, height x width]','line_number':1549,'multiline':False]
['text':' [batch_size_image, batch_size_text, height, width]','line_number':1552,'multiline':False]
