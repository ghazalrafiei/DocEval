['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' soft dependency','line_number':51,'multiline':False]
['text':' On the first call, check whether a compatible version of TensorFlow is installed','line_number':56,'multiline':False]
['text':' TensorFlow Probability depends on a recent stable release of TensorFlow','line_number':57,'multiline':False]
['text':' See all GroupViT models at https://huggingface.co/models?filter=groupvit','line_number':70,'multiline':False]
['text':' Copied from transformers.models.bart.modeling_tf_bart._expand_mask','line_number':77,'multiline':False]
['text':' contrastive loss function, adapted from','line_number':91,'multiline':False]
['text':' https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html','line_number':92,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.clip_loss with clip->groupvit','line_number':101,'multiline':False]
['text':' Straight through.','line_number':110,'multiline':False]
['text':' TensorFlow expects axis to be -1 or between [0, 3).  But received: -2','line_number':115,'multiline':False]
['text':' This is why the following code snippet is used.','line_number':116,'multiline':False]
['text':' ~Gumbel(logits,tau)','line_number':129,'multiline':False]
['text':' Straight through.','line_number':133,'multiline':False]
['text':' TensorFlow expects axis to be -1 or between [0, 3).  But received: -2','line_number':138,'multiline':False]
['text':' This is why the following code snippet is used.','line_number':139,'multiline':False]
['text':' Reparametrization trick.','line_number':145,'multiline':False]
['text':' number of group token','line_number':171,'multiline':False]
['text':' [batch_size, groups, height x width, groups] -> [batch_size, groups, height, width]','line_number':172,'multiline':False]
['text':' [batch_size, num_groups, height x width] -> [batch_size, height x width, num_groups]','line_number':200,'multiline':False]
['text':' [batch_size, height x width, num_groups] -> [batch_size, num_groups, height x width] -> [batch_size, num_groups, height, width]','line_number':206,'multiline':False]
['text':' [batch_size, num_groups, height, width]','line_number':210,'multiline':False]
['text':' [batch_size, query_length, channels]','line_number':326,'multiline':False]
['text':' [batch_size, key_length, channels]','line_number':329,'multiline':False]
['text':' [batch_size, key_length, channels]','line_number':332,'multiline':False]
['text':' [batch_size, query_length, key_length]','line_number':335,'multiline':False]
['text':' norm on group_tokens','line_number':371,'multiline':False]
['text':' norm on x','line_number':383,'multiline':False]
['text':' [B, num_output_groups, C] <- [B, num_group_tokens, C]','line_number':402,'multiline':False]
['text':' [batch_size, num_output_groups, channels]','line_number':416,'multiline':False]
['text':' Adapted from transformers.models.vit.modeling_tf_vit.TFViTPatchEmbeddings with ViT->GroupViT','line_number':456,'multiline':False]
['text':' hidden_size is a member as it will be required in the call method','line_number':468,'multiline':False]
['text':' When running on CPU, `tf.keras.layers.Conv2D` doesn't support `NCHW` format.','line_number':509,'multiline':False]
['text':' So change the input format from `NCHW` to `NHWC`.','line_number':510,'multiline':False]
['text':' shape = (batch_size, in_height, in_width, in_channels=num_channels)','line_number':511,'multiline':False]
['text':' Change the 2D spatial dimensions to a single temporal dimension.','line_number':516,'multiline':False]
['text':' shape = (batch_size, num_patches, out_channels=embed_dim)','line_number':517,'multiline':False]
['text':' In the TFGroupViTVisionEmbeddings the embeddings from this layer will be layer normalized','line_number':519,'multiline':False]
['text':' LayerNormalization layer needs to have static last dimension (otherwise the test_keras_save_load fails with symbolic tensors)','line_number':520,'multiline':False]
['text':' This is why we have used the hidden_size in the reshape method','line_number':521,'multiline':False]
['text':' Adapted from transformers.vit.modeling_tf_vit.TFViTEmbeddings','line_number':535,'multiline':False]
['text':' add positional encoding to each token','line_number':606,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.TFCLIPTextEmbeddings with CLIP->GroupViT','line_number':617,'multiline':False]
['text':' Adapted from transformers.models.clip.modeling_tf_clip.TFCLIPAttention','line_number':850,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfAttention.transpose_for_scores','line_number':888,'multiline':False]
['text':' Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]','line_number':890,'multiline':False]
['text':' Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]','line_number':893,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':922,'multiline':False]
['text':' (batch size, num_heads, seq_len_q, seq_len_k)','line_number':923,'multiline':False]
['text':' apply the causal_attention_mask first','line_number':928,'multiline':False]
['text':' Apply the causal attention mask (precomputed for all layers in TFCLIPModel call() function)','line_number':930,'multiline':False]
['text':' Apply the attention mask (precomputed for all layers in TFCLIPModel call() function)','line_number':934,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':937,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':940,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':941,'multiline':False]
['text':' (batch_size, seq_len_q, embed_dim)','line_number':947,'multiline':False]
['text':' In TFBert, attention weights are returned after dropout.','line_number':951,'multiline':False]
['text':' However, in CLIP, they are returned before dropout.','line_number':952,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.TFCLIPEncoderLayer with CLIP->GroupViT','line_number':975,'multiline':False]
['text':' add attentions if we output them','line_number':1023,'multiline':False]
['text':' Adapted from transformers.models.clip.modeling_tf_clip.TFGroupViTTextEncoder','line_number':1045,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.TFCLIPTextTransformer with CLIPText->GroupViTText, CLIPEncoder->GroupViTTextEncoder','line_number':1159,'multiline':False]
['text':' For `pooled_output` computation','line_number':1170,'multiline':False]
['text':' CLIP's text model uses causal mask, prepare it here.','line_number':1189,'multiline':False]
['text':' https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324','line_number':1190,'multiline':False]
['text':' check attention mask and invert','line_number':1193,'multiline':False]
['text':' [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]','line_number':1194,'multiline':False]
['text':' The `eos_token_id` was incorrect before PR #24773: Let's keep what have been done here.','line_number':1211,'multiline':False]
['text':' A CLIP model with such `eos_token_id` in the config can't work correctly with extra new tokens added','line_number':1212,'multiline':False]
['text':' ------------------------------------------------------------','line_number':1213,'multiline':False]
['text':' text_embeds.shape = [batch_size, n_ctx, transformer.width]','line_number':1214,'multiline':False]
['text':' take features from the eot embedding (eot_token is the highest number in each sequence)','line_number':1215,'multiline':False]
['text':' The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)','line_number':1223,'multiline':False]
['text':' It is possible with an unspecified sequence length for seq_length to be','line_number':1246,'multiline':False]
['text':' a runtime value, which is unsupported by tf.constant. Per the TensorFlow','line_number':1247,'multiline':False]
['text':' docs, tf.fill can handle runtime dynamic shapes:','line_number':1248,'multiline':False]
['text':' https://www.tensorflow.org/api_docs/python/tf/fill','line_number':1249,'multiline':False]
['text':' set an additive 2D attention mask with all places being masked','line_number':1252,'multiline':False]
['text':' set diagonal & lower triangular parts to 0 (i.e. the places not to be masked)','line_number':1255,'multiline':False]
['text':' TIP: think the 2D matrix as the space of (query_seq, key_seq)','line_number':1256,'multiline':False]
['text':' to_mask = tf.linalg.band_part(to_mask, -1, 0)','line_number':1258,'multiline':False]
['text':' Adapted from transformers.models.clip.modeling_tf_clip.TFCLIPVisionTransformer','line_number':1278,'multiline':False]
['text':' normalize the last hidden state','line_number':1307,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.TFCLIPTextMainLayer with CLIP->GroupViT','line_number':1337,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_tf_clip.TFCLIPVisionMainLayer with CLIP->GroupViT','line_number':1394,'multiline':False]
['text':' Adapted from transformers.models.clip.modeling_tf_clip.TFCLIPMainLayer','line_number':1438,'multiline':False]
['text':' normalized features','line_number':1629,'multiline':False]
['text':' cosine similarity as logits','line_number':1633,'multiline':False]
['text':' grouped features','line_number':1640,'multiline':False]
['text':' [batch_size_image, num_group, hidden_size]','line_number':1641,'multiline':False]
['text':' [batch_size_image*num_group, hidden_size]','line_number':1643,'multiline':False]
['text':' [batch_size_image, num_group, height, width]','line_number':1651,'multiline':False]
['text':' normalized features','line_number':1654,'multiline':False]
['text':' [batch_size_image x num_group, batch_size_text]','line_number':1658,'multiline':False]
['text':' [batch_size_image, batch_size_text, num_group]','line_number':1660,'multiline':False]
['text':' [batch_size_image, batch_size_text, height x width]','line_number':1666,'multiline':False]
['text':' [batch_size_image, batch_size_text, height, width]','line_number':1669,'multiline':False]
['text':' TODO: As is this currently fails with saved_model=True, because','line_number':2127,'multiline':False]
['text':' TensorFlow cannot trace through nested dataclasses. Reference:','line_number':2128,'multiline':False]
['text':' https://github.com/huggingface/transformers/pull/16886','line_number':2129,'multiline':False]
