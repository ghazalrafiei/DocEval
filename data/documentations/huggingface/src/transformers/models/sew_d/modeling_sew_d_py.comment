['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 ASAPP Inc. and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':42,'multiline':False]
['text':' Base docstring','line_number':45,'multiline':False]
['text':' CTC docstring','line_number':49,'multiline':False]
['text':' Audio class docstring','line_number':53,'multiline':False]
['text':' See all SEW models at https://huggingface.co/models?filter=sew-d','line_number':68,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices','line_number':72,'multiline':False]
['text':' epsilon is used for probabilistic rounding','line_number':108,'multiline':False]
['text':' make sure num masked span <= sequence_length','line_number':116,'multiline':False]
['text':' make sure num_masked span is also <= input_length - (mask_length - 1)','line_number':120,'multiline':False]
['text':' compute number of masked spans in batch','line_number':126,'multiline':False]
['text':' SpecAugment mask to fill','line_number':133,'multiline':False]
['text':' compute num of masked spans for this input','line_number':143,'multiline':False]
['text':' get random indices to mask','line_number':146,'multiline':False]
['text':' pick first sampled index that will serve as a dummy index to pad vector','line_number':151,'multiline':False]
['text':' to ensure same dimension for all batches due to probabilistic rounding','line_number':152,'multiline':False]
['text':' Picking first sample just pads those vectors twice.','line_number':153,'multiline':False]
['text':' this case can only happen if `input_length` is strictly smaller then','line_number':155,'multiline':False]
['text':' `sequence_length` in which case the last token has to be a padding','line_number':156,'multiline':False]
['text':' token which we can use as a dummy mask id','line_number':157,'multiline':False]
['text':' expand masked indices to masked spans','line_number':169,'multiline':False]
['text':' add offset to the starting indexes so that indexes now create a span','line_number':175,'multiline':False]
['text':' ensure that we cannot have indices larger than sequence_length','line_number':182,'multiline':False]
['text':' scatter indices to mask','line_number':186,'multiline':False]
['text':' Copied from transformers.models.deberta_v2.modeling_deberta_v2.make_log_bucket_position','line_number':192,'multiline':False]
['text':' Copied from transformers.models.deberta_v2.modeling_deberta_v2.build_relative_position','line_number':208,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.c2p_dynamic_expand','line_number':240,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.p2c_dynamic_expand','line_number':246,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.pos_dynamic_expand','line_number':252,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.get_mask','line_number':257,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SEWD','line_number':277,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SEWD','line_number':299,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SEWD','line_number':327,'multiline':False]
['text':' Copied from transformers.models.sew.modeling_sew.SEWPositionalConvEmbedding with SEW->SEWD','line_number':352,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->SEW','line_number':386,'multiline':False]
['text':' Copied from transformers.models.sew.modeling_sew.SEWUpsampling with SEW->SEWD','line_number':398,'multiline':False]
['text':' transform embedding channels to sequence length','line_number':411,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->SEWD','line_number':421,'multiline':False]
['text':' make sure hidden_states require grad for gradient_checkpointing','line_number':450,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.ContextPooler','line_number':477,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':486,'multiline':False]
['text':' to the first token.','line_number':487,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.XSoftmax with deberta->deberta_v2','line_number':500,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.DropoutContext','line_number':564,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.XDropout','line_number':573,'multiline':False]
['text':' StableDropout only calls this function when training.','line_number':602,'multiline':False]
['text':' TODO: We should check if the opset_version being used to export','line_number':604,'multiline':False]
['text':' is > 12 here, but there's no good way to do that. As-is, if the','line_number':605,'multiline':False]
['text':' opset_version < 12, export will fail with a CheckerError.','line_number':606,'multiline':False]
['text':' Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:','line_number':607,'multiline':False]
['text':' if opset_version < 12:','line_number':608,'multiline':False]
['text':'   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)','line_number':609,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.StableDropout','line_number':613,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.DebertaSelfOutput with DebertaV2->SEWD, DebertaLayerNorm->LayerNorm, hidden_dropout_prob->activation_dropout','line_number':663,'multiline':False]
['text':' Copied from transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention with attention_probs_dropout_prob->attention_dropout, hidden_dropout_prob->activation_dropout','line_number':678,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':778,'multiline':False]
['text':' bsz x height x length x dimension','line_number':799,'multiline':False]
['text':' bsz x height x query x key','line_number':831,'multiline':False]
['text':' .split(self.all_head_size, dim=-1)','line_number':850,'multiline':False]
['text':' .split(self.all_head_size, dim=-1)','line_number':854,'multiline':False]
['text':' content->position','line_number':857,'multiline':False]
['text':' position->content','line_number':869,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.DebertaAttention with Deberta->SEWD','line_number':896,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->SEWD','line_number':933,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.DebertaOutput with DebertaLayerNorm->LayerNorm, hidden_dropout_prob->activation_dropout','line_number':949,'multiline':False]
['text':' Copied from transformers.models.deberta.modeling_deberta.DebertaLayer with Deberta->SEWD','line_number':965,'multiline':False]
['text':' Copied from transformers.models.deberta_v2.modeling_deberta_v2.ConvLayer','line_number':1000,'multiline':False]
['text':' Copied from transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder with DebertaV2->SEWD','line_number':1037,'multiline':False]
['text':' make sure padded tokens output 0','line_number':1195,'multiline':False]
['text':' apply pooling formula to get real output_lengths','line_number':1199,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':1255,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':1256,'multiline':False]
['text':' 1D convolutional layer output length formula taken','line_number':1287,'multiline':False]
['text':' from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html','line_number':1288,'multiline':False]
['text':' these two operations makes sure that all values before the output lengths idxs are attended to','line_number':1303,'multiline':False]
['text':' Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps','line_number':1359,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1377,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states','line_number':1380,'multiline':False]
['text':' `config.apply_spec_augment` can set masking to False','line_number':1392,'multiline':False]
['text':' generate indices & apply SpecAugment along time axis','line_number':1396,'multiline':False]
['text':' apply SpecAugment along time axis with given mask_time_indices','line_number':1400,'multiline':False]
['text':' generate indices & apply SpecAugment along feature axis','line_number':1414,'multiline':False]
['text':' compute reduced attention_mask corresponding to feature vectors','line_number':1459,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV_2_VEC_2->SEWD','line_number':1488,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1510,'multiline':False]
['text':' Note that `tie_weights` is usually used to tie input and output embedding weights. The method is re-purposed to','line_number':1521,'multiline':False]
['text':' correctly load adapter layers for SEWD so that we do not have to introduce a new API to','line_number':1522,'multiline':False]
['text':' [`PreTrainedModel`]. While slightly hacky, SEWD never has to tie input and output embeddings, so that it is','line_number':1523,'multiline':False]
['text':' ok to repurpose this function here.','line_number':1524,'multiline':False]
['text':' retrieve loss input_lengths from attention_mask','line_number':1606,'multiline':False]
['text':' assuming that padded tokens are filled with -100','line_number':1612,'multiline':False]
['text':' when not being attended to','line_number':1613,'multiline':False]
['text':' ctc_loss doesn't support fp16','line_number':1618,'multiline':False]
['text':' Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV_2_VEC_2->SEWD','line_number':1648,'multiline':False]
['text':' transformer layers + input embeddings','line_number':1658,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1664,'multiline':False]
