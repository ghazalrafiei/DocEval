['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 Studio Ousia and the HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License','line_number':14,'multiline':False]
['text':' Mask token behave like a normal word, i.e. include the space before it','line_number':260,'multiline':False]
['text':' we add 2 special tokens for downstream tasks','line_number':263,'multiline':False]
['text':' for more information about lstrip and rstrip, see https://github.com/huggingface/transformers/pull/2778','line_number':264,'multiline':False]
['text':' Original fairseq vocab and spm vocab must be "aligned":','line_number':284,'multiline':False]
['text':' Vocab    |    0    |    1    |   2    |    3    |  4  |  5  |  6  |   7   |   8   |  9','line_number':285,'multiline':False]
['text':' -------- | ------- | ------- | ------ | ------- | --- | --- | --- | ----- | ----- | ----','line_number':286,'multiline':False]
['text':' fairseq  | '<s>'   | '<pad>' | '</s>' | '<unk>' | ',' | '.' | '▁' | 's'   | '▁de' | '-'','line_number':287,'multiline':False]
['text':' spm      | '<unk>' | '<s>'   | '</s>' | ','     | '.' | '▁' | 's' | '▁de' | '-'   | '▁a'','line_number':288,'multiline':False]
['text':' Mimic fairseq token-to-id alignment for the first 4 token','line_number':290,'multiline':False]
['text':' The first "real" token "," has position 4 in the original fairseq vocab and position 3 in the spm vocab','line_number':293,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.vocab_size','line_number':350,'multiline':False]
['text':' Add the <mask> token','line_number':352,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.get_vocab','line_number':354,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer._tokenize','line_number':360,'multiline':False]
['text':' TODO check if the t5/llama PR also applies here','line_number':362,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer._convert_token_to_id','line_number':365,'multiline':False]
['text':' Need to return unknown token if the SP model returned 0','line_number':372,'multiline':False]
['text':' for backward compatibility','line_number':395,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer.__call__','line_number':403,'multiline':False]
['text':' Input type checking for clearer error','line_number':469,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._encode_plus','line_number':549,'multiline':False]
['text':' prepare_for_model will create the attention_mask and token_type_ids','line_number':605,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._batch_encode_plus','line_number':630,'multiline':False]
['text':' input_ids is a list of tuples (one for each example in the batch)','line_number':668,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._check_entity_input_format','line_number':739,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._create_input_sequence','line_number':758,'multiline':False]
['text':' whitespace should be prepended to the following token','line_number':788,'multiline':False]
['text':' add special tokens to input ids','line_number':844,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._batch_prepare_for_model','line_number':912,'multiline':False]
['text':' we pad in batch afterward','line_number':961,'multiline':False]
['text':' we pad in batch afterward','line_number':966,'multiline':False]
['text':' we pad in batch afterward','line_number':967,'multiline':False]
['text':' We convert the whole batch to tensors at the end','line_number':972,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer.prepare_for_model','line_number':995,'multiline':False]
['text':' Backward compatibility for 'truncation_strategy', 'pad_to_max_length'','line_number':1047,'multiline':False]
['text':' Compute lengths','line_number':1057,'multiline':False]
['text':' Load from model defaults','line_number':1079,'multiline':False]
['text':' Compute the total size of the returned word encodings','line_number':1087,'multiline':False]
['text':' Truncation: Handle max sequence length and max_entity_length','line_number':1090,'multiline':False]
['text':' truncate words up to max_length','line_number':1093,'multiline':False]
['text':' Add special tokens','line_number':1106,'multiline':False]
['text':' 1 * <s> token','line_number':1110,'multiline':False]
['text':' 1 * <s> token & 2 * <sep> tokens','line_number':1111,'multiline':False]
['text':' Build output dictionary','line_number':1118,'multiline':False]
['text':' Set max entity length','line_number':1128,'multiline':False]
['text':' truncate entities up to max_entity_length','line_number':1159,'multiline':False]
['text':' Check lengths','line_number':1202,'multiline':False]
['text':' Padding','line_number':1205,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer.pad','line_number':1225,'multiline':False]
['text':' If we have a list of dicts, let's convert it in a dict of lists','line_number':1288,'multiline':False]
['text':' We do this to allow using this method as a collate_fn function in PyTorch Dataloader','line_number':1289,'multiline':False]
['text':' The model's main input name, usually `input_ids`, has be passed for padding','line_number':1293,'multiline':False]
['text':' If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects','line_number':1307,'multiline':False]
['text':' and rebuild them afterwards if no return_tensors is specified','line_number':1308,'multiline':False]
['text':' Note that we lose the specific device the tensor may be on for PyTorch','line_number':1309,'multiline':False]
['text':' first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.','line_number':1313,'multiline':False]
['text':' At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.','line_number':1319,'multiline':False]
['text':' Convert padding_strategy in PaddingStrategy','line_number':1336,'multiline':False]
['text':' Copied from transformers.models.luke.tokenization_luke.LukeTokenizer._pad','line_number':1386,'multiline':False]
['text':' Load from model defaults','line_number':1425,'multiline':False]
['text':' Initialize attention mask if not present.','line_number':1450,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.build_inputs_with_special_tokens','line_number':1551,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.get_special_tokens_mask','line_number':1578,'multiline':False]
['text':' Copied from transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.create_token_type_ids_from_sequences','line_number':1607,'multiline':False]
