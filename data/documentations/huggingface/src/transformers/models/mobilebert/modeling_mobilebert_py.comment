['text':' MIT License','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Copyright (c) 2020  The Google AI Language Team Authors, The HuggingFace Inc. team and github/lonePatient','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Permission is hereby granted, free of charge, to any person obtaining a copy','line_number':5,'multiline':False]
['text':' of this software and associated documentation files (the "Software"), to deal','line_number':6,'multiline':False]
['text':' in the Software without restriction, including without limitation the rights','line_number':7,'multiline':False]
['text':' to use, copy, modify, merge, publish, distribute, sublicense, and/or sell','line_number':8,'multiline':False]
['text':' copies of the Software, and to permit persons to whom the Software is','line_number':9,'multiline':False]
['text':' furnished to do so, subject to the following conditions:','line_number':10,'multiline':False]
['text':'','line_number':11,'multiline':False]
['text':' The above copyright notice and this permission notice shall be included in all','line_number':12,'multiline':False]
['text':' copies or substantial portions of the Software.','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR','line_number':15,'multiline':False]
['text':' IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,','line_number':16,'multiline':False]
['text':' FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE','line_number':17,'multiline':False]
['text':' AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER','line_number':18,'multiline':False]
['text':' LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,','line_number':19,'multiline':False]
['text':' OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE','line_number':20,'multiline':False]
['text':' SOFTWARE.','line_number':21,'multiline':False]
['text':' TokenClassification docstring','line_number':62,'multiline':False]
['text':' QuestionAnswering docstring','line_number':67,'multiline':False]
['text':' SequenceClassification docstring','line_number':74,'multiline':False]
['text':' Load weights from TF model','line_number':97,'multiline':False]
['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v','line_number':113,'multiline':False]
['text':' which are not required for using pretrained model','line_number':114,'multiline':False]
['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':193,'multiline':False]
['text':' From the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited','line_number':221,'multiline':False]
['text':' Devices (https://arxiv.org/abs/2004.02984)','line_number':222,'multiline':False]
['text':'','line_number':223,'multiline':False]
['text':' The embedding table in BERT models accounts for a substantial proportion of model size. To compress','line_number':224,'multiline':False]
['text':' the embedding layer, we reduce the embedding dimension to 128 in MobileBERT.','line_number':225,'multiline':False]
['text':' Then, we apply a 1D convolution with kernel size 3 on the raw token embedding to produce a 512','line_number':226,'multiline':False]
['text':' dimensional output.','line_number':227,'multiline':False]
['text':' Add positional embeddings and token type embeddings, then layer','line_number':239,'multiline':False]
['text':' normalize and perform dropout.','line_number':240,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':285,'multiline':False]
['text':' Apply the attention mask is (precomputed for all layers in BertModel forward() function)','line_number':289,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':291,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':293,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':294,'multiline':False]
['text':' Mask heads if we want to','line_number':296,'multiline':False]
['text':' Prune linear layers','line_number':338,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':344,'multiline':False]
['text':' Run a linear projection of `hidden_size` then add a residual','line_number':367,'multiline':False]
['text':' with `layer_input`.','line_number':368,'multiline':False]
['text':' add attentions if we output them','line_number':370,'multiline':False]
['text':' This method can return three different tuples of values. These different values make use of bottlenecks,','line_number':449,'multiline':False]
['text':' which are linear layers used to project the hidden states to a lower-dimensional vector, reducing memory','line_number':450,'multiline':False]
['text':' usage. These linear layer have weights that are learned during training.','line_number':451,'multiline':False]
['text':'','line_number':452,'multiline':False]
['text':' If `config.use_bottleneck_attention`, it will return the result of the bottleneck layer four times for the','line_number':453,'multiline':False]
['text':' key, query, value, and "layer input" to be used by the attention layer.','line_number':454,'multiline':False]
['text':' This bottleneck is used to project the hidden. This last layer input will be used as a residual tensor','line_number':455,'multiline':False]
['text':' in the attention self output, after the attention scores have been computed.','line_number':456,'multiline':False]
['text':'','line_number':457,'multiline':False]
['text':' If not `config.use_bottleneck_attention` and `config.key_query_shared_bottleneck`, this will return','line_number':458,'multiline':False]
['text':' four values, three of which have been passed through a bottleneck: the query and key, passed through the same','line_number':459,'multiline':False]
['text':' bottleneck, and the residual layer to be applied in the attention self output, through another bottleneck.','line_number':460,'multiline':False]
['text':'','line_number':461,'multiline':False]
['text':' Finally, in the last case, the values for the query, key and values are the hidden states without bottleneck,','line_number':462,'multiline':False]
['text':' and the residual layer will be this value passed through a bottleneck.','line_number':463,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':536,'multiline':False]
['text':' Add last layer','line_number':593,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':612,'multiline':False]
['text':' to the first token.','line_number':613,'multiline':False]
['text':' The output weights are the same as the input embeddings, but there is','line_number':644,'multiline':False]
['text':' an output-only bias for each token.','line_number':645,'multiline':False]
['text':' Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`','line_number':649,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':695,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':696,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':826,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':884,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':885,'multiline':False]
['text':' Prepare head mask if needed','line_number':888,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':889,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':890,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':891,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':892,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':935,'multiline':False]
['text':' resize dense output embedings at first','line_number':945,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1044,'multiline':False]
['text':' resize dense output embedings at first','line_number':1054,'multiline':False]
['text':' -100 index = padding token','line_number':1106,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1142,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification with Bert->MobileBert all-casing','line_number':1238,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1252,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertForQuestionAnswering with Bert->MobileBert all-casing','line_number':1342,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1351,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':1411,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1416,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertForMultipleChoice with Bert->MobileBert all-casing','line_number':1446,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1458,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertForTokenClassification with Bert->MobileBert all-casing','line_number':1543,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1556,'multiline':False]
