['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]
['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]
['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]
['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]
['text':' limitations under the License.','line_number':19,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Persimmon','line_number':43,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':54,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':64,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':70,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Persimmon','line_number':80,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':94,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Persimmon','line_number':100,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':121,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.rotate_half','line_number':127,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':135,'multiline':False]
['text':' Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP with GPTNeoX->Persimmon','line_number':164,'multiline':False]
['text':' Copied from transformers.models.bloom.modeling_bloom.BloomAttention._split_heads','line_number':247,'multiline':False]
['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':275,'multiline':False]
['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':278,'multiline':False]
['text':' [batch_size, num_heads, seq_length, head_dim] -> [batch_size, seq_length, num_heads, head_dim]','line_number':285,'multiline':False]
['text':' Partial rotary embedding','line_number':301,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]','line_number':310,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim]','line_number':313,'multiline':False]
['text':' Specific to RoPE models with partial rotation','line_number':318,'multiline':False]
['text':' upcast attention to fp32','line_number':337,'multiline':False]
['text':' Self Attention','line_number':403,'multiline':False]
['text':' Fully Connected','line_number':414,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':568,'multiline':False]
['text':' retrieve input_ids and inputs_embeds','line_number':598,'multiline':False]
['text':' embed positions','line_number':634,'multiline':False]
['text':' decoder layers','line_number':645,'multiline':False]
['text':' add hidden states from the last decoder layer','line_number':683,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with LLAMA->PERSIMMON,Llama->Persimmon','line_number':704,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':711,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings','line_number':714,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings','line_number':718,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings','line_number':722,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings','line_number':726,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder','line_number':730,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder','line_number':734,'multiline':False]
['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':785,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':803,'multiline':False]
['text':' Flatten the tokens','line_number':806,'multiline':False]
['text':' Enable model parallelism','line_number':810,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation','line_number':826,'multiline':False]
['text':' Keep only the unprocessed tokens:','line_number':839,'multiline':False]
['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':840,'multiline':False]
['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':841,'multiline':False]
['text':' input)','line_number':842,'multiline':False]
['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':845,'multiline':False]
['text':' input_ids based on the past_length.','line_number':846,'multiline':False]
['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':849,'multiline':False]
['text':' If we are about to go beyond the maximum cache length, we need to crop the input attention mask.','line_number':851,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':861,'multiline':False]
['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':867,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->PERSIMMON,Llama->Persimmon','line_number':908,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':916,'multiline':False]
