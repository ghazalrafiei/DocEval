['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':'','line_number':15,'multiline':False]
['text':' Original implementation: https://github.com/pytorch/fairseq/tree/master/examples/wmt19','line_number':16,'multiline':False]
['text':' Authors:','line_number':17,'multiline':False]
['text':' - @alexeib Alexei Baevski','line_number':18,'multiline':False]
['text':' - @edunov Sergey Edunov','line_number':19,'multiline':False]
['text':' - @michaelauli Michael Auli','line_number':20,'multiline':False]
['text':' - @myleott Myle Ott','line_number':21,'multiline':False]
['text':' - @nng555 Nathan Ng','line_number':22,'multiline':False]
['text':' - David Grangier','line_number':23,'multiline':False]
['text':' - Kyra Yee','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':' Paper: Facebook FAIR's WMT19 News Translation Task Submission https://arxiv.org/abs/1907.06616','line_number':26,'multiline':False]
['text':'','line_number':27,'multiline':False]
['text':' See all FSMT models at https://huggingface.co/models?filter=fsmt','line_number':62,'multiline':False]
['text':' Porting notes:','line_number':64,'multiline':False]
['text':' this one is modeled after BartModel*','line_number':65,'multiline':False]
['text':'','line_number':66,'multiline':False]
['text':' Currently only translation (fairseq also has weights for LM)','line_number':67,'multiline':False]
['text':'','line_number':68,'multiline':False]
['text':' fairseq provides weights for ru-en, en-ru and de-en, en-de pairs. All have been ported.','line_number':69,'multiline':False]
['text':' - ru-en, en-ru use asymmetric vocab','line_number':70,'multiline':False]
['text':' - de-en, en-de use a merged single vocab (but the code works as if they are separate)','line_number':71,'multiline':False]
['text':'','line_number':72,'multiline':False]
['text':' Differences with Bart:','line_number':73,'multiline':False]
['text':' - not using bos token','line_number':74,'multiline':False]
['text':' - 2 separate vocabs (src and target)','line_number':75,'multiline':False]
['text':' - embed weights aren't tied','line_number':76,'multiline':False]
['text':' - uses a model Ensemble (but that part isn't ported/implemented yet) - so we','line_number':77,'multiline':False]
['text':'   aren't getting as good of a BLEU score','line_number':78,'multiline':False]
['text':' - uses a projection layer at the end of the decoder','line_number':79,'multiline':False]
['text':' - doesn't use final_logits_bias','line_number':80,'multiline':False]
['text':' - beam search: stops as soon as num_beams == len(hypos) (whereas transformers','line_number':81,'multiline':False]
['text':'   is not satisfied there and will continue searching until the next cycles','line_number':82,'multiline':False]
['text':'   aren't promising something better), comparing BLEU scores - the transformers','line_number':83,'multiline':False]
['text':'   algorithm is slightly superior, therefore using the latter. But if you want','line_number':84,'multiline':False]
['text':'   to match fairseq outputs, you need to pass ``early_stopping=True`` to ``generate()``.','line_number':85,'multiline':False]
['text':'','line_number':86,'multiline':False]
['text':' SinusoidalPositionalEmbedding is slightly different from Bart's - generates','line_number':87,'multiline':False]
['text':' different embeddings. This implementation is copied verbatim from fairseq with','line_number':88,'multiline':False]
['text':' some small changes to make it work here.','line_number':89,'multiline':False]
['text':'','line_number':90,'multiline':False]
['text':' Other changes:','line_number':91,'multiline':False]
['text':'  - doesn't support use_cache as Bart's version does','line_number':92,'multiline':False]
['text':'','line_number':93,'multiline':False]
['text':'','line_number':94,'multiline':False]
['text':' FSMTConfig changes with BartConfig','line_number':95,'multiline':False]
['text':'','line_number':96,'multiline':False]
['text':'    Differences with BART:','line_number':97,'multiline':False]
['text':'    - src/tgt vocabs aren't shared','line_number':98,'multiline':False]
['text':'    - token embeddings aren't shared','line_number':99,'multiline':False]
['text':'    - needs a language pair','line_number':100,'multiline':False]
['text':'    - scale_embedding are True','line_number':101,'multiline':False]
['text':'','line_number':102,'multiline':False]
['text':'    some unused args were removed too','line_number':103,'multiline':False]
['text':'','line_number':104,'multiline':False]
['text':'','line_number':105,'multiline':False]
['text':' TODO:','line_number':106,'multiline':False]
['text':' - port model ensemble (fs uses 4 model checkpoints)','line_number':107,'multiline':False]
['text':' - solve beam search discrepancies','line_number':108,'multiline':False]
['text':' docstyle-ignore','line_number':109,'multiline':False]
['text':' Helper Functions, mostly for making masks','line_number':377,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':386,'multiline':False]
['text':' Helper Modules','line_number':404,'multiline':False]
['text':' type: List[EncoderLayer]','line_number':475,'multiline':False]
['text':' check attention mask and invert','line_number':509,'multiline':False]
['text':' We assume zeros hidden states correspond to padding tokens','line_number':521,'multiline':False]
['text':' and create `position_ids` where inputs_embeds[:, :, 0] == 0','line_number':522,'multiline':False]
['text':' B x T x C -> T x B x C','line_number':534,'multiline':False]
['text':' check if head_mask has a correct number of layers specified if desired','line_number':539,'multiline':False]
['text':' T x B x C -> B x T x C','line_number':546,'multiline':False]
['text':' B x T x C -> T x B x C','line_number':548,'multiline':False]
['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':549,'multiline':False]
['text':' skip the layer','line_number':551,'multiline':False]
['text':' T x B x C -> B x T x C','line_number':564,'multiline':False]
['text':' Self Attention','line_number':618,'multiline':False]
['text':' adds keys to layer state','line_number':622,'multiline':False]
['text':' Cross attention','line_number':632,'multiline':False]
['text':' mutates layer state','line_number':639,'multiline':False]
['text':' Fully Connected','line_number':647,'multiline':False]
['text':' layer_state = cache for decoding','line_number':660,'multiline':False]
['text':' type: List[DecoderLayer]','line_number':683,'multiline':False]
['text':' check attention mask and invert','line_number':742,'multiline':False]
['text':' embed positions','line_number':749,'multiline':False]
['text':' happens after we embed them','line_number':753,'multiline':False]
['text':' We assume zeros hidden states correspond to padding tokens','line_number':756,'multiline':False]
['text':' and create `position_ids` where inputs_embeds[:, :, 0] == 0','line_number':757,'multiline':False]
['text':' Convert to FSMT output format: (BS, seq_len, model_dim) -> (seq_len, BS, model_dim)','line_number':769,'multiline':False]
['text':' decoder layers','line_number':773,'multiline':False]
['text':' check if head_mask has a correct number of layers specified if desired','line_number':779,'multiline':False]
['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':787,'multiline':False]
['text':' add hidden states from the last decoder layer','line_number':818,'multiline':False]
['text':' Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)','line_number':824,'multiline':False]
['text':' otherwise self_attention','line_number':861,'multiline':False]
['text':' get here for encoder decoder cause of static_kv','line_number':896,'multiline':False]
['text':' reuse k,v and encoder_padding_mask','line_number':897,'multiline':False]
['text':' previous time steps are cached - no need to recompute key and value if they are static','line_number':900,'multiline':False]
['text':' Update cache','line_number':926,'multiline':False]
['text':' This is part of a workaround to get around fork/join parallelism not supporting Optional types.','line_number':942,'multiline':False]
['text':' don't attend to padding symbols','line_number':950,'multiline':False]
['text':' make sure that attn_weights are included in graph','line_number':966,'multiline':False]
['text':' saved states are stored with shape (bsz, num_heads, seq_len, head_dim)','line_number':987,'multiline':False]
['text':' Public API','line_number':1023,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1045,'multiline':False]
['text':' make masks if user doesn't supply','line_number':1093,'multiline':False]
['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=False','line_number':1118,'multiline':False]
['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1126,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1182,'multiline':False]
['text':' TODO(SS): do we need to ignore pad tokens in labels?','line_number':1243,'multiline':False]
['text':' encoder_outputs is defined. input_ids not needed','line_number':1275,'multiline':False]
['text':' change this to avoid caching (presumably for debugging)','line_number':1283,'multiline':False]
['text':' get the correct batch idx from decoder layer's batch dim for cross and self-attn','line_number':1293,'multiline':False]
['text':' in ___init__','line_number':1330,'multiline':False]
['text':' in forward put the weights on the correct dtype and device of the param','line_number':1333,'multiline':False]
['text':' zero pad','line_number':1353,'multiline':False]
['text':' The series of casts and type-conversions here are carefully','line_number':1366,'multiline':False]
['text':' balanced to both work with ONNX export and XLA. In particular XLA','line_number':1367,'multiline':False]
['text':' prefers ints, cumsum defaults to output longs, and ONNX doesn't know','line_number':1368,'multiline':False]
['text':' how to handle the dtype kwarg in cumsum.','line_number':1369,'multiline':False]
['text':' expand embeddings if needed','line_number':1383,'multiline':False]
