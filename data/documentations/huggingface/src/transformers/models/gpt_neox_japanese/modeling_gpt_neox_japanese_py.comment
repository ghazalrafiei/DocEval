['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 ABEJA, Inc. and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all GPTNeoXJapanese models at https://huggingface.co/models?filter=gpt_neox_japanese','line_number':39,'multiline':False]
['text':' Activate bias if the last layer','line_number':86,'multiline':False]
['text':' Compute QKV','line_number':101,'multiline':False]
['text':' Attention heads [batch, seq_len, hidden_size]','line_number':102,'multiline':False]
['text':'   --> [batch, seq_len, (np * 3 * head_size)]','line_number':103,'multiline':False]
['text':' [batch, seq_len, (num_heads * 3 * head_size)]','line_number':106,'multiline':False]
['text':'   --> [batch, seq_len, num_heads, 3 * head_size]','line_number':107,'multiline':False]
['text':' [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]','line_number':111,'multiline':False]
['text':' Compute rotary embeddings on rotary_ndims','line_number':116,'multiline':False]
['text':' Compute token offset for rotary embeddings (when decoding)','line_number':122,'multiline':False]
['text':' Cache QKV values','line_number':133,'multiline':False]
['text':' Compute attention','line_number':141,'multiline':False]
['text':' Reshape outputs','line_number':144,'multiline':False]
['text':' tensor: [bs, seq_len, hidden_size]','line_number':159,'multiline':False]
['text':' -> [bs, seq_len, num_attention_heads, attn_head_size]','line_number':161,'multiline':False]
['text':' -> [bs, num_attention_heads, seq_len, attn_head_size]','line_number':163,'multiline':False]
['text':' tensor [bs, num_attention_heads, seq_len, attn_head_size]','line_number':172,'multiline':False]
['text':' -> [bs, seq_len, num_attention_heads, attn_head_size]','line_number':174,'multiline':False]
['text':' -> [bs, seq_len, hidden_size]','line_number':176,'multiline':False]
['text':' q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]','line_number':188,'multiline':False]
['text':' compute causal mask from causal mask buffer','line_number':189,'multiline':False]
['text':' Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.','line_number':214,'multiline':False]
['text':' Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`','line_number':215,'multiline':False]
['text':' Apply the attention mask','line_number':221,'multiline':False]
['text':' Mask heads if we want to','line_number':228,'multiline':False]
['text':' Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoXRotaryEmbedding->RotaryEmbedding','line_number':236,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':247,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':257,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':263,'multiline':False]
['text':' Project back to h.','line_number':314,'multiline':False]
['text':' activate bias only last layer','line_number':331,'multiline':False]
['text':' output_attn: a, present, (attentions)','line_number':355,'multiline':False]
['text':' attn_output = (atten_output + bias) + residual','line_number':358,'multiline':False]
['text':' attn_output = (mlp_output + mlp_bias) + atten_output','line_number':368,'multiline':False]
['text':' hidden_states, present, (attentions)','line_number':378,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':452,'multiline':False]
['text':' Attention mask.','line_number':524,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':529,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':530,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':531,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':532,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':533,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':536,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':537,'multiline':False]
['text':' positions we want to attend and -10000.0 for masked positions.','line_number':538,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':539,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':540,'multiline':False]
['text':' fp16 compatibility','line_number':541,'multiline':False]
['text':' Prepare head mask if needed','line_number':544,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':545,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':546,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':547,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':548,'multiline':False]
['text':' Add last hidden state','line_number':577,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':606,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':689,'multiline':False]
['text':' we are doing next-token prediction; shift prediction scores and input ids by one','line_number':692,'multiline':False]
['text':' if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly','line_number':713,'multiline':False]
['text':' cut decoder_input_ids if past is used','line_number':717,'multiline':False]
