['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The Salesforce Authors and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all InstructBLIP models at https://huggingface.co/models?filter=instructblip','line_number':52,'multiline':False]
['text':' Copied from transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput with Blip2->InstructBlip','line_number':57,'multiline':False]
['text':' Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->InstructBlip','line_number':90,'multiline':False]
['text':' shape = [*, width, grid, grid]','line_number':113,'multiline':False]
['text':' Copied from transformers.models.blip_2.modeling_blip_2.Blip2Attention with Blip2->InstructBlip','line_number':122,'multiline':False]
['text':' small tweak here compared to CLIP, no bias here','line_number':140,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':176,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':181,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':184,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':185,'multiline':False]
['text':' Mask heads if we want to','line_number':188,'multiline':False]
['text':' Copied from transformers.models.blip.modeling_blip.BlipMLP','line_number':204,'multiline':False]
['text':' Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->InstructBlip','line_number':220,'multiline':False]
['text':' Copied from transformers.models.blip_2.modeling_blip_2.Blip2PreTrainedModel._init_weights with Blip2->InstructBlip','line_number':286,'multiline':False]
['text':' Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip','line_number':402,'multiline':False]
['text':' Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP','line_number':489,'multiline':False]
['text':' If this is instantiated as a cross-attention module, the keys','line_number':612,'multiline':False]
['text':' and values come from an encoder; the attention mask needs to be','line_number':613,'multiline':False]
['text':' such that the encoder's padding tokens are not attended to.','line_number':614,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':636,'multiline':False]
['text':' fp16 compatibility','line_number':645,'multiline':False]
['text':' Apply the attention mask is (precomputed for all layers in BertModel forward() function)','line_number':659,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':662,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':669,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':670,'multiline':False]
['text':' Mask heads if we want to','line_number':673,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->InstructBlipQFormer','line_number':689,'multiline':False]
['text':' Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerAttention with Blip2->InstructBlip','line_number':704,'multiline':False]
['text':' Prune linear layers','line_number':719,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':725,'multiline':False]
['text':' add attentions if we output them','line_number':750,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->InstructBlipQFormer','line_number':754,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->InstructBlipQFormer','line_number':770,'multiline':False]
['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':817,'multiline':False]
['text':' add cross attentions if we output attention weights','line_number':846,'multiline':False]
['text':' Copied from transformers.models.blip_2.modeling_blip_2.Blip2QFormerEncoder with Blip2->InstructBlip','line_number':888,'multiline':False]
['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':995,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1086,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1087,'multiline':False]
['text':' Provided a padding mask of dimensions [batch_size, seq_length]','line_number':1091,'multiline':False]
['text':' - the model is an encoder, so make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1092,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':1099,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':1100,'multiline':False]
['text':' positions we want to attend and -10000.0 for masked positions.','line_number':1101,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':1102,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':1103,'multiline':False]
['text':' fp16 compatibility','line_number':1104,'multiline':False]
['text':' past_key_values_length','line_number':1151,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1172,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1173,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1176,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1177,'multiline':False]
['text':' Prepare head mask if needed','line_number':1195,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':1196,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':1197,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':1198,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':1199,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1268,'multiline':False]
['text':' warn users about unexpected behavior when using multi-GPU + InstructBLIP + `accelerate`.','line_number':1302,'multiline':False]
['text':' For `generate` compatibility','line_number':1312,'multiline':False]
['text':' step 1: forward the images through the vision encoder,','line_number':1376,'multiline':False]
['text':' to get image embeddings of shape (batch_size, seq_len, hidden_size)','line_number':1377,'multiline':False]
['text':' step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention','line_number':1386,'multiline':False]
['text':' difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former','line_number':1389,'multiline':False]
['text':' step 3: use the language model, conditioned on the query outputs and the prompt','line_number':1407,'multiline':False]
['text':' we compute the loss here since we need to take into account the sequence length of the query embeds','line_number':1431,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':1435,'multiline':False]
['text':' Flatten the tokens','line_number':1439,'multiline':False]
['text':' preprocess for `accelerate`','line_number':1498,'multiline':False]
['text':' concatenate query embeddings with prompt embeddings','line_number':1536,'multiline':False]
['text':' the InstructBLIP authors used inconsistent tokenizer/model files during training,','line_number':1546,'multiline':False]
['text':' with the tokenizer's bos token being set to </s> which has ID=2,','line_number':1547,'multiline':False]
['text':' whereas the model's text config has bos token id = 0','line_number':1548,'multiline':False]
