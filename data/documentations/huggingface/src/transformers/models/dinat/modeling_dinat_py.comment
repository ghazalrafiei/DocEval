['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 SHI Labs and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':59,'multiline':False]
['text':' Base docstring','line_number':62,'multiline':False]
['text':' Image classification docstring','line_number':66,'multiline':False]
['text':' See all Dinat models at https://huggingface.co/models?filter=dinat','line_number':73,'multiline':False]
['text':' drop_path and DinatDropPath are from the timm library.','line_number':76,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatEncoderOutput with Nat->Dinat','line_number':80,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatModelOutput with Nat->Dinat','line_number':114,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatImageClassifierOutput with Nat->Dinat','line_number':151,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatEmbeddings with Nat->Dinat','line_number':187,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatPatchEmbeddings with Nat->Dinat','line_number':210,'multiline':False]
['text':' TODO: Support arbitrary patch sizes.','line_number':227,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatDownsampler with Nat->Dinat','line_number':247,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.drop_path','line_number':271,'multiline':False]
['text':' work with diff dim tensors, not just 2D ConvNets','line_number':285,'multiline':False]
['text':' binarize','line_number':287,'multiline':False]
['text':' Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->Dinat','line_number':292,'multiline':False]
['text':' rpb is learnable relative positional biases; same concept is used Swin.','line_number':321,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NeighborhoodAttention.transpose_for_scores with Nat->Dinat','line_number':330,'multiline':False]
['text':' Apply the scale factor before computing attention weights. It's usually more efficient because','line_number':345,'multiline':False]
['text':' attention weights are typically a bigger tensor compared to query.','line_number':346,'multiline':False]
['text':' It gives identical results because scalars are commutable in matrix multiplication.','line_number':347,'multiline':False]
['text':' Compute NA between "query" and "key" to get the raw attention scores, and add relative positional biases.','line_number':350,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':353,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':356,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':357,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NeighborhoodAttentionOutput','line_number':370,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NeighborhoodAttentionModule.prune_heads','line_number':391,'multiline':False]
['text':' Prune linear layers','line_number':399,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':405,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NeighborhoodAttentionModule.forward','line_number':410,'multiline':False]
['text':' add attentions if we output them','line_number':418,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatIntermediate with Nat->Dinat','line_number':422,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatOutput with Nat->Dinat','line_number':438,'multiline':False]
['text':' pad hidden_states if they are smaller than kernel size x dilation','line_number':492,'multiline':False]
['text':' patch merging layer','line_number':540,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatStage.forward','line_number':548,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatEncoder.forward with Nat->Dinat','line_number':591,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':605,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':617,'multiline':False]
['text':' rearrange b h w c -> b c h w','line_number':622,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':654,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':655,'multiline':False]
['text':' Copied from transformers.models.nat.modeling_nat.NatModel with Nat->Dinat, NAT->DINAT','line_number':696,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':713,'multiline':False]
['text':' Classifier head','line_number':798,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':803,'multiline':False]
['text':' Add layer norms to hidden states of out_features','line_number':891,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':897,'multiline':False]
