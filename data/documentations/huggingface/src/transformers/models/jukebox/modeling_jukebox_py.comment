['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The OpenAI Team Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all Jukebox models at https://huggingface.co/models?filter=jukebox','line_number':39,'multiline':False]
['text':' Safety check','line_number':56,'multiline':False]
['text':' Remove all tokens with a probability less than the last token of the top-k','line_number':59,'multiline':False]
['text':' Remove tokens with cumulative probability above the threshold','line_number':67,'multiline':False]
['text':' Shift the indices to the right to keep also the first token above the threshold','line_number':69,'multiline':False]
['text':' indices_to_remove = sorted_indices[sorted_indices_to_remove]','line_number':73,'multiline':False]
['text':' Break total_length into hops/windows of size n_ctx separated by hop_length','line_number':114,'multiline':False]
['text':' Last hop could be smaller, we make it n_ctx to maximise context','line_number':119,'multiline':False]
['text':' Top level used','line_number':126,'multiline':False]
['text':' set metadata offset, sample_length and lyrics tokens','line_number':146,'multiline':False]
['text':' alignment_hop has shape (bs, n_ctx, nb_relevant_lyric_tokens)','line_number':160,'multiline':False]
['text':' indices_hop is a list of len=bs, each entry of len hps.nb_relevant_lyric_tokens','line_number':161,'multiline':False]
['text':' Combine attn for each hop into attn for full range','line_number':165,'multiline':False]
['text':' Use indices to place them into correct place for corresponding source tokens','line_number':166,'multiline':False]
['text':' Note each item has different length lyrics','line_number':169,'multiline':False]
['text':' remove token padding, and last lyric index','line_number':177,'multiline':False]
['text':' returns a mask of shape 1 x 1 x query_length x key_value_length or None if masking is not needed.','line_number':194,'multiline':False]
['text':' Masked dense','line_number':199,'multiline':False]
['text':' Masked summary','line_number':202,'multiline':False]
['text':' 64, 32, ...','line_number':320,'multiline':False]
['text':' 32, 64 ...','line_number':370,'multiline':False]
['text':' Calculate new centres','line_number':414,'multiline':False]
['text':' nb_discrete_codes, batch_size * seq_length','line_number':415,'multiline':False]
['text':' nb_discrete_codes','line_number':420,'multiline':False]
['text':' Update centres','line_number':424,'multiline':False]
['text':' nb_discrete_codes','line_number':427,'multiline':False]
['text':' prob of each bin','line_number':434,'multiline':False]
['text':' entropy ie how diverse','line_number':435,'multiline':False]
['text':' Normalise','line_number':453,'multiline':False]
['text':' Calculate latent code latent_states','line_number':465,'multiline':False]
['text':' (batch_size * latent_states , codebook_weights)','line_number':471,'multiline':False]
['text':' Preprocess.','line_number':483,'multiline':False]
['text':' Quantise','line_number':486,'multiline':False]
['text':' Postprocess.','line_number':489,'multiline':False]
['text':' Dequantise','line_number':496,'multiline':False]
['text':' Postprocess','line_number':499,'multiline':False]
['text':' Preprocess','line_number':508,'multiline':False]
['text':' Init codebook if not inited','line_number':511,'multiline':False]
['text':' Quantise and dequantise through bottleneck','line_number':515,'multiline':False]
['text':' Update embeddings','line_number':519,'multiline':False]
['text':' Loss','line_number':525,'multiline':False]
['text':' Passthrough','line_number':528,'multiline':False]
['text':' Postprocess','line_number':531,'multiline':False]
['text':' Be extra paranoid and make sure the encoder weights can't','line_number':568,'multiline':False]
['text':' change from straight-through estimator','line_number':569,'multiline':False]
['text':' embed_tokens','line_number':607,'multiline':False]
['text':' Decode','line_number':663,'multiline':False]
['text':' Use only lowest level','line_number':667,'multiline':False]
['text':' Encode','line_number':697,'multiline':False]
['text':' Encode/Decode','line_number':766,'multiline':False]
['text':' a single channel is always used in original code','line_number':789,'multiline':False]
['text':' Sequence of length seq_len is factored as [blocks, seq_len // blocks]','line_number':844,'multiline':False]
['text':' length of the encoder input ids','line_number':872,'multiline':False]
['text':' Generate appropriate mask to mask out all positions before current','line_number':885,'multiline':False]
['text':' Might take up lot of memory for dense, so can cache it','line_number':886,'multiline':False]
['text':' only keep music queries and lyrics keys/values','line_number':903,'multiline':False]
['text':' in Tensorflow implem: fct merge_states','line_number':912,'multiline':False]
['text':' in Tensorflow implem: fct split_states','line_number':920,'multiline':False]
['text':' For sample, query_len= 1, key_len = value_len = sample_t','line_number':936,'multiline':False]
['text':' For sample, query_len= 1, key_len = value_len = sample_t','line_number':952,'multiline':False]
['text':' For sample, query_len= 1, key_len = value_len = sample_t','line_number':981,'multiline':False]
['text':' For sample, query_len= 1, key_len = value_len = sample_t','line_number':1019,'multiline':False]
['text':' batch_size, blocks, embed_dim','line_number':1029,'multiline':False]
['text':' batch_size, blocks, embed_dim','line_number':1032,'multiline':False]
['text':' For sample, query_len= 1, key_len = value_len = sample_t','line_number':1039,'multiline':False]
['text':' Orders of attn_func','line_number':1233,'multiline':False]
['text':' Blocks','line_number':1263,'multiline':False]
['text':' attend to the lyrics','line_number':1265,'multiline':False]
['text':' Merged piped model uses this setup','line_number':1341,'multiline':False]
['text':' Preprocess.','line_number':1369,'multiline':False]
['text':' Target','line_number':1381,'multiline':False]
['text':' Shift by 1, and fill in start token','line_number':1383,'multiline':False]
['text':' Pos emb and dropout','line_number':1392,'multiline':False]
['text':' Transformer','line_number':1396,'multiline':False]
['text':' Piped doesnt add x_cond','line_number':1397,'multiline':False]
['text':' Predictions','line_number':1404,'multiline':False]
['text':' Note order! Lyric is first','line_number':1413,'multiline':False]
['text':' Loss','line_number':1415,'multiline':False]
['text':' Pos emb, dropout is identity at eval time','line_number':1439,'multiline':False]
['text':' Predictions','line_number':1481,'multiline':False]
['text':' Adjust logits','line_number':1484,'multiline':False]
['text':' Sample and replace hidden_states','line_number':1487,'multiline':False]
['text':' Preprocess.','line_number':1523,'multiline':False]
['text':' Fill up key/value cache for past context by runing forward pass.','line_number':1540,'multiline':False]
['text':' We do so in chunks instead of doing the whole past in one forward pass to reduce max memory usage.','line_number':1541,'multiline':False]
['text':' Predictions','line_number':1576,'multiline':False]
['text':' the input of the encoder and decoder can be merged into (lyrics, music tokens)','line_number':1579,'multiline':False]
['text':' Predictions','line_number':1597,'multiline':False]
['text':' Adjust logits','line_number':1600,'multiline':False]
['text':' only music tokens are sampled','line_number':1603,'multiline':False]
['text':' setting correct argument for the `JukeboxDecoder`','line_number':1629,'multiline':False]
['text':' Embed music_tokens','line_number':1652,'multiline':False]
['text':' Run conditioner','line_number':1657,'multiline':False]
['text':' Check if [pos_start,pos_end] in [pos_min, pos_max)','line_number':1684,'multiline':False]
['text':' Interpolate so that [pos_start, ..., pos_end] <-> position tensor of length n_ctx','line_number':1696,'multiline':False]
['text':' Bin each value to bins_','line_number':1706,'multiline':False]
['text':' [0,1) -> [0,1..,embed_dim) -> [0,1...,embed_dim-1','line_number':1707,'multiline':False]
['text':' Start embedding of length 1','line_number':1746,'multiline':False]
['text':' Empty genre slots are denoted by -1. We mask these out.','line_number':1748,'multiline':False]
['text':' Pos embedding of length n_ctx','line_number':1753,'multiline':False]
['text':' Passing functions instead of the vqvae module to avoid getting params, only used in the','line_number':1824,'multiline':False]
['text':' forward loop','line_number':1825,'multiline':False]
['text':' Audio conditioning : conditioning on music tokens (either from audio or from previous levels or both)','line_number':1840,'multiline':False]
['text':' metadata conditioning : contioning on timing, genres, and artist','line_number':1846,'multiline':False]
['text':' define encoder-decoder or encoder and decoder','line_number':1851,'multiline':False]
['text':' encoder-decoder transformer','line_number':1854,'multiline':False]
['text':' Separate encoder-decoder transformer','line_number':1870,'multiline':False]
['text':' decoder model on the tokens','line_number':1891,'multiline':False]
['text':' Set sample_length to match this level','line_number':1914,'multiline':False]
['text':' Set offset','line_number':1917,'multiline':False]
['text':' here since metadata has the full token_list, we just need to selected the ones that are relevant','line_number':1919,'multiline':False]
['text':' Set lyric tokens','line_number':1921,'multiline':False]
['text':' whats the index of each current character in original array','line_number':1936,'multiline':False]
['text':' Some of the input tokens might be shifted to take into account the voccabulary fusion','line_number':1996,'multiline':False]
['text':' If not masking loss, model may have generated lyric/midi tokens which are now shifted <0 by bin_shift','line_number':2001,'multiline':False]
['text':' Get latents','line_number':2022,'multiline':False]
['text':' Currently audio_conditioning only uses immediately above layer','line_number':2101,'multiline':False]
['text':' the prime_sample function will be used with music_tokens set to None','line_number':2104,'multiline':False]
['text':' the preprocess returns the full tokens (Lyrics and Music tokens), shifted','line_number':2193,'multiline':False]
['text':' Sample a partial window of length<n_ctx with tokens_to_sample new tokens on level=level','line_number':2341,'multiline':False]
['text':' Sample a single window of length=n_ctx at position=start on level=level','line_number':2358,'multiline':False]
['text':' get music_tokens already sampled at current level','line_number':2364,'multiline':False]
['text':' Nothing new to sample','line_number':2380,'multiline':False]
['text':' get music_tokens_conds from level above','line_number':2383,'multiline':False]
['text':' if there are no levels above should return None!','line_number':2385,'multiline':False]
['text':' set metadata offset, sample_length and lyrics tokens','line_number':2387,'multiline':False]
['text':' Update music_tokens with new sample','line_number':2412,'multiline':False]
['text':' Sample total_length tokens at level=level with hop_length=hop_length','line_number':2417,'multiline':False]
['text':' total length of the signal, might be bit different from the actual generated length','line_number':2530,'multiline':False]
['text':' Set correct total_length, hop_length, labels and sampling_kwargs for level','line_number':2538,'multiline':False]
['text':' Decode sample','line_number':2556,'multiline':False]
['text':' vqvae levels are reversed','line_number':2558,'multiline':False]
