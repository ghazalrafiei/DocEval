['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 SHI Labs and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all OneFormer models at https://huggingface.co/models?filter=oneformer','line_number':51,'multiline':False]
['text':' Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention','line_number':63,'multiline':False]
['text':' batch_size, height*width, num_heads, hidden_dim','line_number':73,'multiline':False]
['text':' -> batch_size, height*width, num_heads*hidden_dim','line_number':74,'multiline':False]
['text':' -> batch_size, num_heads*hidden_dim, height*width','line_number':75,'multiline':False]
['text':' -> batch_size*num_heads, hidden_dim, height, width','line_number':76,'multiline':False]
['text':' batch_size, num_queries, num_heads, num_points, 2','line_number':80,'multiline':False]
['text':' -> batch_size, num_heads, num_queries, num_points, 2','line_number':81,'multiline':False]
['text':' -> batch_size*num_heads, num_queries, num_points, 2','line_number':82,'multiline':False]
['text':' batch_size*num_heads, hidden_dim, num_queries, num_points','line_number':84,'multiline':False]
['text':' (batch_size, num_queries, num_heads, num_levels, num_points)','line_number':89,'multiline':False]
['text':' -> (batch_size, num_heads, num_queries, num_levels, num_points)','line_number':90,'multiline':False]
['text':' -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)','line_number':91,'multiline':False]
['text':' Copied from transformers.models.maskformer.modeling_maskformer.dice_loss','line_number':103,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.sigmoid_cross_entropy_loss','line_number':134,'multiline':False]
['text':' Copied from transformers.models.maskformer.modeling_maskformer.pair_wise_dice_loss','line_number':154,'multiline':False]
['text':' using broadcasting to get a [num_queries, NUM_CLASSES] matrix','line_number':171,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.pair_wise_sigmoid_cross_entropy_loss','line_number':177,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.sample_point','line_number':206,'multiline':False]
['text':' use nn.function.grid_sample to get features for points in `point_coordinates` via bilinear interpolation','line_number':231,'multiline':False]
['text':' Refactored from https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/matcher.py#L93','line_number':239,'multiline':False]
['text':' iterate through batch size','line_number':300,'multiline':False]
['text':' Compute the classification cost. Contrary to the loss, we don't use the NLL,','line_number':303,'multiline':False]
['text':' but approximate it in 1 - proba[target class].','line_number':304,'multiline':False]
['text':' The 1 is a constant that doesn't change the matching, it can be ommitted.','line_number':305,'multiline':False]
['text':' all masks share the same set of points for efficient matching!','line_number':311,'multiline':False]
['text':' get ground truth labels','line_number':314,'multiline':False]
['text':' compute the sigmoid ce loss','line_number':331,'multiline':False]
['text':' Compute the dice loss','line_number':333,'multiline':False]
['text':' final cost matrix','line_number':335,'multiline':False]
['text':' do the assigmented using the hungarian algorithm in scipy','line_number':338,'multiline':False]
['text':' It could be stacked in one tensor','line_number':342,'multiline':False]
['text':' pointwise mask loss parameters','line_number':396,'multiline':False]
['text':' get the maximum size in the batch','line_number':412,'multiline':False]
['text':' compute finel size','line_number':415,'multiline':False]
['text':' get metadata','line_number':418,'multiline':False]
['text':' pad the tensors to the size of the biggest one','line_number':423,'multiline':False]
['text':' [batch_size, hidden_dim]','line_number':446,'multiline':False]
['text':' shape = (batch_size, num_queries)','line_number':489,'multiline':False]
['text':' shape = (batch_size, num_queries)','line_number':491,'multiline':False]
['text':' permute pred_logits (batch_size, num_queries, num_labels) -> (batch_size, num_labels, num_queries)','line_number':496,'multiline':False]
['text':' shape (batch_size * num_queries, height, width)','line_number':525,'multiline':False]
['text':' shape (batch_size, num_queries, height, width)','line_number':527,'multiline':False]
['text':' pad all and stack the targets to the num_labels dimension','line_number':528,'multiline':False]
['text':' upsample predictions to the target size, we have to add one dim to use interpolate','line_number':529,'multiline':False]
['text':' sample point_coords','line_number':537,'multiline':False]
['text':' get ground-truth labels','line_number':545,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerLoss.calculate_uncertainty','line_number':559,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerLoss.sample_points_using_uncertainty','line_number':577,'multiline':False]
['text':' Get random point coordinates','line_number':611,'multiline':False]
['text':' Get sampled prediction value for the point coordinates','line_number':613,'multiline':False]
['text':' Calculate the uncertainties based on the sampled prediction values of the points','line_number':615,'multiline':False]
['text':' permute predictions following indices','line_number':634,'multiline':False]
['text':' permute labels following indices','line_number':640,'multiline':False]
['text':' retrieve the matching between the outputs of the last layer and the labels','line_number':689,'multiline':False]
['text':' compute the average number of target masks for normalization purposes','line_number':691,'multiline':False]
['text':' get all the losses','line_number':693,'multiline':False]
['text':' in case of auxiliary losses, we repeat this process with the output of each intermediate layer.','line_number':701,'multiline':False]
['text':' Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoderOutput with Mask2->One','line_number':756,'multiline':False]
['text':' Modified from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrFrozenBatchNorm2d with DeformableDetr->OneFormerPixelDecoder','line_number':922,'multiline':False]
['text':' Modified from transformers.models.detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OneFormerPixelDecoderEncoder','line_number':960,'multiline':False]
['text':' check if dim_per_head is power of 2','line_number':973,'multiline':False]
['text':' add position embeddings to the hidden states before projecting to queries and keys','line_number':1008,'multiline':False]
['text':' we invert the attention_mask','line_number':1021,'multiline':False]
['text':' batch_size, num_queries, n_heads, n_levels, n_points, 2','line_number':1033,'multiline':False]
['text':' PyTorch implementation','line_number':1047,'multiline':False]
['text':' Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.','line_number':1105,'multiline':False]
['text':' Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrEncoder with DeformableDetrEncoder->OneFormerPixelDecoderEncoderOnly','line_number':1145,'multiline':False]
['text':' Modified from from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoder with Mask2->One','line_number':1268,'multiline':False]
['text':'  positional encoding','line_number':1275,'multiline':False]
['text':' Create input projection layers','line_number':1283,'multiline':False]
['text':' extra fpn levels','line_number':1316,'multiline':False]
['text':' Place convs into top-down order (from low to high resolution)','line_number':1350,'multiline':False]
['text':' to make the top-down computation in forward clearer.','line_number':1351,'multiline':False]
['text':' Then, apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)','line_number':1379,'multiline':False]
['text':' Prepare encoder inputs (by flattening)','line_number':1388,'multiline':False]
['text':' Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder','line_number':1411,'multiline':False]
['text':' Also provide spatial_shapes, level_start_index and valid_ratios','line_number':1412,'multiline':False]
['text':' append `out` with extra FPN levels','line_number':1443,'multiline':False]
['text':' Reverse feature maps into top-down order (from low to high resolution)','line_number':1444,'multiline':False]
['text':' Following FPN implementation, we use nearest upsampling here','line_number':1449,'multiline':False]
['text':' Modified from from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelLevelModule with Mask2->One','line_number':1468,'multiline':False]
['text':' Modified from transformers.models.detr.modeling_detr.DetrAttention with Detr->OneFormer','line_number':1495,'multiline':False]
['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':1551,'multiline':False]
['text':' for the decoder','line_number':1552,'multiline':False]
['text':' add position embeddings to the hidden states before projecting to queries and keys','line_number':1556,'multiline':False]
['text':' add key-value position embeddings to the key value states','line_number':1561,'multiline':False]
['text':' get query proj','line_number':1566,'multiline':False]
['text':' get key, value proj','line_number':1568,'multiline':False]
['text':' cross_attentions','line_number':1570,'multiline':False]
['text':' self_attention','line_number':1574,'multiline':False]
['text':' this operation is a bit awkward, but it's required to','line_number':1604,'multiline':False]
['text':' make sure that attn_weights keeps its gradient.','line_number':1605,'multiline':False]
['text':' In order to do so, attn_weights have to reshaped','line_number':1606,'multiline':False]
['text':' twice and have to be reused in the following','line_number':1607,'multiline':False]
['text':' Implementation of Feedforward model','line_number':1773,'multiline':False]
['text':' refactored from original implementation','line_number':1835,'multiline':False]
['text':' Masked Cross Attention','line_number':1894,'multiline':False]
['text':' here we do not apply masking on padded region','line_number':1899,'multiline':False]
['text':' Self Attention','line_number':1904,'multiline':False]
['text':' Fully Connected','line_number':1912,'multiline':False]
['text':' Implementation of Feedforward model','line_number':1984,'multiline':False]
['text':' prediction heads on learnable query features','line_number':2220,'multiline':False]
['text':' must use bool type','line_number':2283,'multiline':False]
['text':' If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.','line_number':2284,'multiline':False]
['text':' this is a workaround to make torchscript happy, as torchscript','line_number':2294,'multiline':False]
['text':' doesn't support dictionary with non-homogeneous values, such','line_number':2295,'multiline':False]
['text':' as a dict having both a Tensor and a list.','line_number':2296,'multiline':False]
['text':' flatten NxCxHxW to HWxNxC','line_number':2350,'multiline':False]
['text':' QxNxC','line_number':2356,'multiline':False]
['text':' Copied from transformers.models.maskformer.modeling_maskformer.MaskFormerSinePositionEmbedding with Mask->One','line_number':2375,'multiline':False]
['text':' Copied from transformers.models.maskformer.modeling_maskformer.PredictionBlock','line_number':2415,'multiline':False]
['text':' Maintain submodule indexing as if part of a Sequential block','line_number':2420,'multiline':False]
['text':' NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights','line_number':2436,'multiline':False]
['text':' lazily create causal attention mask, with full attention between the vision tokens','line_number':2653,'multiline':False]
['text':' pytorch uses additive attention mask; fill with -inf','line_number':2654,'multiline':False]
['text':' zero out the lower diagonal','line_number':2657,'multiline':False]
['text':' [batch_size, num_channels]','line_number':2718,'multiline':False]
['text':' weight each loss by `self.weight_dict[<LOSS_NAME>]` including auxiliary losses','line_number':3087,'multiline':False]
