['text':' Copyright 2022 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]
['text':' limitations under the License.','line_number':13,'multiline':False]
['text':' The simple map of names for "automated" rules.','line_number':137,'multiline':False]
['text':' megatron-lm layers to merge across tp ranks','line_number':147,'multiline':False]
['text':' deprecated','line_number':154,'multiline':False]
['text':' transformers layers to split across tp ranks','line_number':158,'multiline':False]
['text':' Format the message.','line_number':177,'multiline':False]
['text':' Print and recurse (if needed).','line_number':184,'multiline':False]
['text':' version 1.0 stores [num_heads * hidden_size * num_splits, :]','line_number':216,'multiline':False]
['text':' other versions store [num_heads * num_splits * hidden_size, :]','line_number':222,'multiline':False]
['text':' Input is [num_splits * num_heads * hidden_size, :]','line_number':247,'multiline':False]
['text':' version 1.0 stores [num_heads * hidden_size * num_splits, :]','line_number':250,'multiline':False]
['text':' other versions store [num_heads * num_splits * hidden_size, :]','line_number':256,'multiline':False]
['text':' Load Megatron-LM checkpoint arguments from the state dict','line_number':329,'multiline':False]
['text':' Create Transformers GPT2 config from Megatron-LM arguments','line_number':349,'multiline':False]
['text':' in the very early days this used to be "gelu_new"','line_number':358,'multiline':False]
['text':' The regex to extract layer names.','line_number':398,'multiline':False]
['text':' Convert.','line_number':401,'multiline':False]
['text':' Embeddings','line_number':404,'multiline':False]
['text':' Convert and store the position embeddings.','line_number':408,'multiline':False]
['text':' Convert and store the word embeddings.','line_number':414,'multiline':False]
['text':' Transformer Layers','line_number':427,'multiline':False]
['text':' The number of heads.','line_number':429,'multiline':False]
['text':' The hidden_size per head.','line_number':431,'multiline':False]
['text':' The transformer.','line_number':441,'multiline':False]
['text':' Extract the layers.','line_number':447,'multiline':False]
['text':' Match the name.','line_number':449,'multiline':False]
['text':' Stop if that's not a layer','line_number':451,'multiline':False]
['text':' The index of the layer.','line_number':455,'multiline':False]
['text':' The name of the operation.','line_number':457,'multiline':False]
['text':' Is it a weight or a bias?','line_number':459,'multiline':False]
['text':' The name of the layer.','line_number':462,'multiline':False]
['text':' For layernorm(s), simply store the layer norm.','line_number':478,'multiline':False]
['text':' Transpose the QKV matrix.','line_number':483,'multiline':False]
['text':' Insert a tensor of 1x1xDxD bias.','line_number':487,'multiline':False]
['text':' Insert a "dummy" tensor for masked_bias.','line_number':493,'multiline':False]
['text':' Megatron stores (3*D) x D but transformers-GPT2 expects D x 3*D.','line_number':504,'multiline':False]
['text':' Store.','line_number':506,'multiline':False]
['text':' Transpose the bias.','line_number':509,'multiline':False]
['text':' Store. No change of shape.','line_number':516,'multiline':False]
['text':' Transpose the weights.','line_number':519,'multiline':False]
['text':' Copy the bias.','line_number':524,'multiline':False]
['text':' The final layernorm.','line_number':532,'multiline':False]
['text':' For LM head, transformers' wants the matrix to weight embeddings.','line_number':538,'multiline':False]
['text':' It should be done!','line_number':542,'multiline':False]
['text':' Print the structure of converted state dict.','line_number':545,'multiline':False]
['text':' Add tokenizer class info to config','line_number':549,'multiline':False]
['text':' see https://github.com/huggingface/transformers/issues/13906)','line_number':550,'multiline':False]
['text':' Store the config to file.','line_number':561,'multiline':False]
['text':' Save tokenizer based on args','line_number':565,'multiline':False]
['text':' Store the state_dict to file.','line_number':570,'multiline':False]
['text':' Save the model','line_number':574,'multiline':False]
['text':' Save the index as well','line_number':582,'multiline':False]
['text':' Search in directory above this','line_number':604,'multiline':False]
['text':' load the transformers model state dict and config','line_number':615,'multiline':False]
['text':' Saving the tracker file','line_number':626,'multiline':False]
['text':' create `release` dir in args.load_path','line_number':631,'multiline':False]
['text':' megatron args','line_number':635,'multiline':False]
['text':' params dtype','line_number':665,'multiline':False]
['text':' save dummy optim state dict','line_number':674,'multiline':False]
['text':' Convert.','line_number':705,'multiline':False]
['text':' Embedding layer','line_number':711,'multiline':False]
['text':' Cut out extra padding we don't need','line_number':718,'multiline':False]
['text':' Expanding embedding to larger size by replicating final entry','line_number':721,'multiline':False]
['text':' Same size!','line_number':725,'multiline':False]
['text':' Split into new tensor model parallel sizes','line_number':729,'multiline':False]
['text':' Transformer layers','line_number':742,'multiline':False]
['text':' The number of heads.','line_number':759,'multiline':False]
['text':' The hidden_size per head.','line_number':761,'multiline':False]
['text':' Stop if that's not a layer','line_number':780,'multiline':False]
['text':' The index of the layer.','line_number':784,'multiline':False]
['text':' The name of the operation.','line_number':786,'multiline':False]
['text':' Is it a weight or a bias?','line_number':788,'multiline':False]
['text':' handle layernorm','line_number':792,'multiline':False]
['text':' handle attention K, V, Q weights','line_number':797,'multiline':False]
['text':' transformers stores D X (3*D) but Megatron-LM expects (3*D) X D.','line_number':799,'multiline':False]
['text':' handle attention K, V, Q bias','line_number':811,'multiline':False]
['text':' handle attention and mlp weights','line_number':822,'multiline':False]
['text':' handle attention and mlp bias','line_number':830,'multiline':False]
['text':' skip','line_number':837,'multiline':False]
['text':' handle final layernorm','line_number':852,'multiline':False]
['text':' add the LM head','line_number':860,'multiline':False]
['text':' saving the state dict as per the tp_rank and pp_rank','line_number':865,'multiline':False]
