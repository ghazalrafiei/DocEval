['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 T5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right','line_number':58,'multiline':False]
['text':' layer norm should always be calculated in float32','line_number':84,'multiline':False]
['text':' the dtype of the computation','line_number':124,'multiline':False]
['text':' the dtype of the computation','line_number':162,'multiline':False]
['text':' the dtype of the computation','line_number':184,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':252,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':254,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':258,'multiline':False]
['text':' detect if we're initializing by absence of existing cache data.','line_number':298,'multiline':False]
['text':' update key, value caches with our new 1d spatial slices','line_number':306,'multiline':False]
['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key positions','line_number':315,'multiline':False]
['text':' that have already been generated and cached, not the remaining zero elements.','line_number':316,'multiline':False]
['text':' if key and values are already calculated, only the last query position bias should be taken','line_number':338,'multiline':False]
['text':' q, k, v projections','line_number':364,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':365,'multiline':False]
['text':' reshape to (batch_size, seq_length, n_heads, head_dim)','line_number':369,'multiline':False]
['text':' counter-act scaling in dot_product_attention_weights function','line_number':374,'multiline':False]
['text':' for fast decoding causal attention mask should be shifted','line_number':377,'multiline':False]
['text':' create causal attention_mask; attention_mask has to be defined when model is causal','line_number':381,'multiline':False]
['text':' fast decoding for generate requires special attention_mask','line_number':385,'multiline':False]
['text':' broadcast causal attention mask & attention mask to fit for merge','line_number':394,'multiline':False]
['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':405,'multiline':False]
['text':' and cache the keys and values step by step.','line_number':406,'multiline':False]
['text':' replace masked positions with -10_000','line_number':412,'multiline':False]
['text':' compute position bias (only for first layer)','line_number':422,'multiline':False]
['text':' create dropout rng','line_number':430,'multiline':False]
['text':' Softmax(QK^T)','line_number':435,'multiline':False]
['text':' multiply with value states','line_number':447,'multiline':False]
['text':' bring back to (batch_size, seq_length, d_model)','line_number':450,'multiline':False]
['text':' apply output matrix','line_number':453,'multiline':False]
['text':' the dtype of the computation','line_number':467,'multiline':False]
['text':' add attentions if we output them','line_number':498,'multiline':False]
['text':' the dtype of the computation','line_number':504,'multiline':False]
['text':' add attentions if we output them','line_number':531,'multiline':False]
['text':' the dtype of the computation','line_number':538,'multiline':False]
['text':' Keep self-attention outputs and relative position weights','line_number':579,'multiline':False]
['text':' Keep cross-attention outputs and relative position weights','line_number':593,'multiline':False]
['text':' Apply Feed Forward layer','line_number':596,'multiline':False]
['text':' returns hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights),','line_number':603,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':604,'multiline':False]
['text':' the dtype of the computation','line_number':611,'multiline':False]
['text':' the dtype of the computation','line_number':645,'multiline':False]
['text':' Prepare head mask if needed','line_number':683,'multiline':False]
['text':' We share the position biases between the layers - the first layer store them','line_number':708,'multiline':False]
['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':709,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':710,'multiline':False]
['text':' the dtype of the computation','line_number':732,'multiline':False]
['text':' Add last layer','line_number':777,'multiline':False]
['text':' init input tensors','line_number':958,'multiline':False]
['text':' prepare encoder inputs','line_number':1012,'multiline':False]
['text':' prepare decoder inputs','line_number':1016,'multiline':False]
['text':' Handle any PRNG if needed','line_number':1020,'multiline':False]
['text':' init input variables to retrieve cache','line_number':1050,'multiline':False]
['text':' we only need to call the decoder to init the cache','line_number':1068,'multiline':False]
['text':' Handle any PRNG if needed','line_number':1109,'multiline':False]
['text':' Handle any PRNG if needed','line_number':1183,'multiline':False]
['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be','line_number':1190,'multiline':False]
['text':' passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that','line_number':1191,'multiline':False]
['text':' it can be changed by FlaxT5Attention module','line_number':1192,'multiline':False]
['text':' add updated cache to model output','line_number':1222,'multiline':False]
['text':' the dtype of the computation','line_number':1280,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1330,'multiline':False]
['text':' Decode','line_number':1340,'multiline':False]
['text':' the dtype of the computation','line_number':1410,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1441,'multiline':False]
['text':' prepare encoder inputs','line_number':1475,'multiline':False]
['text':' Handle any PRNG if needed','line_number':1479,'multiline':False]
['text':' the dtype of the computation','line_number':1497,'multiline':False]
['text':' Encode','line_number':1553,'multiline':False]
['text':' Decode','line_number':1565,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':1580,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1581,'multiline':False]
['text':' Handle any PRNG if needed','line_number':1661,'multiline':False]
['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be','line_number':1668,'multiline':False]
['text':' passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that','line_number':1669,'multiline':False]
['text':' it can be changed by FlaxT5Attention module','line_number':1670,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':1688,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1689,'multiline':False]
['text':' add updated cache to model output','line_number':1730,'multiline':False]
['text':' initializing the cache','line_number':1748,'multiline':False]
['text':' Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.','line_number':1752,'multiline':False]
['text':' But since the decoder uses a causal mask, those positions are masked anyways.','line_number':1753,'multiline':False]
['text':' Thus we can create a single static attention_mask here, which is more efficient for compilation','line_number':1754,'multiline':False]
