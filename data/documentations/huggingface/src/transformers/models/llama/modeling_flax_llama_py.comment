['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 Meta AI, EleutherAI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]
['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]
['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]
['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]
['text':' limitations under the License.','line_number':19,'multiline':False]
['text':' use `jax.numpy.sqrt` as `jax.lax.rsqrt` does not match `torch.rsqrt`','line_number':163,'multiline':False]
['text':' Copied from transformers.models.gpt_neo.modeling_flax_gpt_neo.FlaxGPTNeoSelfAttention._concatenate_to_cache','line_number':224,'multiline':False]
['text':' detect if we're initializing by absence of existing cache data.','line_number':231,'multiline':False]
['text':' update key, value caches with our new 1d spatial slices','line_number':239,'multiline':False]
['text':' causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.','line_number':248,'multiline':False]
['text':' During fast autoregressive decoding, we feed one position at a time,','line_number':292,'multiline':False]
['text':' and cache the keys and values step by step.','line_number':293,'multiline':False]
['text':' transform boolean mask into float mask','line_number':297,'multiline':False]
['text':' usual dot product attention','line_number':304,'multiline':False]
['text':' residual connection','line_number':377,'multiline':False]
['text':' residual connection','line_number':384,'multiline':False]
['text':' Copied from transformers.models.gpt_neo.modeling_flax_gpt_neo.FlaxGPTNeoPreTrainedModel with GPTNeo->Llama, GPT_NEO->LLAMA, transformer->model','line_number':390,'multiline':False]
['text':' init input tensors','line_number':414,'multiline':False]
['text':' init input variables to retrieve cache','line_number':442,'multiline':False]
['text':' Handle any PRNG if needed','line_number':483,'multiline':False]
['text':' if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be changed by FlaxLlamaAttention module','line_number':490,'multiline':False]
['text':' add updated cache to model output','line_number':511,'multiline':False]
['text':' this contains possible `None` values - `FlaxLlamaModule` will filter them out','line_number':563,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_flax_gptj.FlaxGPTJForCausalLM with GPTJ->Llama','line_number':695,'multiline':False]
['text':' initializing the cache','line_number':700,'multiline':False]
['text':' Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.','line_number':704,'multiline':False]
['text':' But since Llama uses a causal mask, those positions are masked anyways.','line_number':705,'multiline':False]
['text':' Thus we can create a single static attention_mask here, which is more efficient for compilation','line_number':706,'multiline':False]
