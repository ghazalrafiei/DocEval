['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->UMT5','line_number':53,'multiline':False]
['text':' UMT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':64,'multiline':False]
['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':65,'multiline':False]
['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':66,'multiline':False]
['text':' half-precision inputs is done in fp32','line_number':67,'multiline':False]
['text':' convert into half-precision if necessary','line_number':72,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->UMT5','line_number':79,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->UMT5','line_number':102,'multiline':False]
['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':118,'multiline':False]
['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':119,'multiline':False]
['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':120,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->UMT5','line_number':132,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':168,'multiline':False]
['text':' move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)','line_number':180,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':214,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':216,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':220,'multiline':False]
['text':' shape (query_length, key_length)','line_number':237,'multiline':False]
['text':' shape (query_length, key_length, num_heads)','line_number':239,'multiline':False]
['text':' shape (1, num_heads, query_length, key_length)','line_number':240,'multiline':False]
['text':' use encoder_hidden_states if cross attention','line_number':254,'multiline':False]
['text':' checking that the `sequence_length` of the `past_key_value` is the same as the he provided','line_number':256,'multiline':False]
['text':' `encoder_hidden_states` to support prefix tuning','line_number':257,'multiline':False]
['text':' reuse k,v, cross_attentions','line_number':259,'multiline':False]
['text':' reuse k, v, self_attention','line_number':266,'multiline':False]
['text':' compute positional bias','line_number':273,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':289,'multiline':False]
['text':' if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.','line_number':292,'multiline':False]
['text':' Further calls to cross_attention layer can then reuse all cross-attention','line_number':293,'multiline':False]
['text':' key/value_states (first "if" case)','line_number':294,'multiline':False]
['text':' if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of','line_number':295,'multiline':False]
['text':' all previous decoder key/value_states. Further calls to uni-directional self-attention','line_number':296,'multiline':False]
['text':' can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)','line_number':297,'multiline':False]
['text':' if encoder bi-directional self-attention `past_key_value` is always `None`','line_number':298,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':302,'multiline':False]
['text':' Mask heads if we want to','line_number':306,'multiline':False]
['text':'  attn_output = torch.bmm(attn_probs, value_states) ?','line_number':310,'multiline':False]
['text':' attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) ?','line_number':312,'multiline':False]
['text':' add attentions if we output them','line_number':340,'multiline':False]
['text':' add attentions if we output them','line_number':368,'multiline':False]
['text':' Self Attention','line_number':395,'multiline':False]
['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':396,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':406,'multiline':False]
['text':' Cross-Attention Block','line_number':412,'multiline':False]
['text':' cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple','line_number':417,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':426,'multiline':False]
['text':' Apply Feed Forward layer','line_number':434,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':437,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ClassificationHead with T5->UMT5','line_number':454,'multiline':False]
['text':' Used for testing weights initialization','line_number':498,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':510,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':511,'multiline':False]
['text':' Mesh TensorFlow FF initialization','line_number':526,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':527,'multiline':False]
['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':528,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':546,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':547,'multiline':False]
['text':' shift inputs to the right','line_number':568,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':570,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':580,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':595,'multiline':False]
['text':' required mask seq length can be calculated via length of past','line_number':648,'multiline':False]
['text':' initialize past_key_values with `None` if past does not exist','line_number':663,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':667,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':668,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':671,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':672,'multiline':False]
['text':' Prepare head mask if needed','line_number':689,'multiline':False]
['text':' past_key_value is always None with gradient checkpointing','line_number':715,'multiline':False]
['text':' Add last layer','line_number':745,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':964,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_input_embeddings','line_number':967,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.set_input_embeddings','line_number':971,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model._tie_weights','line_number':977,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_encoder','line_number':983,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_decoder','line_number':987,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads','line_number':991,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1047,'multiline':False]
['text':' Decode','line_number':1067,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1139,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_input_embeddings','line_number':1142,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_input_embeddings','line_number':1146,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration._tie_weights','line_number':1152,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_output_embeddings','line_number':1158,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_output_embeddings','line_number':1162,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder','line_number':1166,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_decoder','line_number':1170,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1226,'multiline':False]
['text':' Convert encoder inputs in embeddings if needed','line_number':1228,'multiline':False]
['text':' get decoder inputs from shifting lm labels to the right','line_number':1248,'multiline':False]
['text':' Decode','line_number':1251,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':1270,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1271,'multiline':False]
['text':' move labels to correct device to enable PP','line_number':1279,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation','line_number':1299,'multiline':False]
['text':' cut decoder_input_ids if past_key_values is used','line_number':1313,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1317,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1321,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels','line_number':1338,'multiline':False]
['text':' config_class = UMT5Config','line_number':1372,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1384,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings','line_number':1387,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.set_input_embeddings','line_number':1391,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel._tie_weights','line_number':1396,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder','line_number':1401,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel._prune_heads','line_number':1405,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with T5->UMT5, t5-small->google/umt5-small','line_number':1416,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.__init__ with T5->UMT5','line_number':1469,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1475,'multiline':False]
['text':' Copied from models.bart.modeling_bart.BartModel.forward different to other models, T5 automatically creates','line_number':1515,'multiline':False]
['text':' decoder_input_ids from input_ids if no decoder_input_ids are provided','line_number':1516,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1623,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_input_embeddings','line_number':1626,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.set_input_embeddings','line_number':1630,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering._tie_weights','line_number':1636,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_encoder','line_number':1642,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_decoder','line_number':1646,'multiline':False]
['text':' Copied from models.bart.modeling_bart.BartModel.forward','line_number':1687,'multiline':False]
['text':'   different to other models, T5 automatically creates decoder_input_ids from','line_number':1688,'multiline':False]
['text':'   input_ids if no decoder_input_ids are provided','line_number':1689,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1702,'multiline':False]
['text':' Decode','line_number':1722,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':1747,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1752,'multiline':False]
