['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX','line_number':4,'multiline':False]
['text':' and OPT implementations in this library. It has been modified from its','line_number':5,'multiline':False]
['text':' original forms to accommodate minor architectural differences compared','line_number':6,'multiline':False]
['text':' to GPT-NeoX and OPT used by the Meta AI team that trained the model.','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':9,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':10,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':15,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':16,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':17,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':18,'multiline':False]
['text':' limitations under the License.','line_number':19,'multiline':False]
['text':' noqa','line_number':50,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':60,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Mistral','line_number':73,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Mistral','line_number':91,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':102,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':112,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':118,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.rotate_half','line_number':128,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':136,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.repeat_kv','line_number':180,'multiline':False]
['text':' Specific to RoPE models','line_number':276,'multiline':False]
['text':' repeat k/v heads if n_kv_heads < n_heads','line_number':279,'multiline':False]
['text':' upcast attention to fp32','line_number':299,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':328,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':332,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':333,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':334,'multiline':False]
['text':' overwrite attention_mask with padding_mask','line_number':352,'multiline':False]
['text':' Because the input can be padded, the absolute sequence length depends on the max position id.','line_number':374,'multiline':False]
['text':' Activate slicing cache only if the config has a value `sliding_windows` attribute','line_number':393,'multiline':False]
['text':' Specific to RoPE models','line_number':418,'multiline':False]
['text':' repeat k/v heads if n_kv_heads < n_heads','line_number':421,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':426,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':427,'multiline':False]
['text':' cast them back in float16 just to be sure everything works as expected.','line_number':428,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':431,'multiline':False]
['text':' Reashape to the expected shape for Flash Attention','line_number':447,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':505,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':508,'multiline':False]
['text':' On the first iteration we need to properly re-create the padding mask','line_number':573,'multiline':False]
['text':' by slicing it on the proper place','line_number':574,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':595,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':599,'multiline':False]
['text':' Self Attention','line_number':662,'multiline':False]
['text':' Fully Connected','line_number':673,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':827,'multiline':False]
['text':' retrieve input_ids and inputs_embeds','line_number':857,'multiline':False]
['text':' 2d mask is passed through the layers','line_number':904,'multiline':False]
['text':' 4d mask is passed through the layers','line_number':907,'multiline':False]
['text':' decoder layers','line_number':918,'multiline':False]
['text':' add hidden states from the last decoder layer','line_number':957,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':984,'multiline':False]
['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1052,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':1071,'multiline':False]
['text':' Flatten the tokens','line_number':1074,'multiline':False]
['text':' Enable model parallelism','line_number':1078,'multiline':False]
['text':' Omit tokens covered by past_key_values','line_number':1097,'multiline':False]
['text':' Keep only the unprocessed tokens:','line_number':1107,'multiline':False]
['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':1108,'multiline':False]
['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':1109,'multiline':False]
['text':' input)','line_number':1110,'multiline':False]
['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':1113,'multiline':False]
['text':' input_ids based on the past_length.','line_number':1114,'multiline':False]
['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':1117,'multiline':False]
['text':' If we are about to go beyond the maximum cache length, we need to crop the input attention mask.','line_number':1119,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':1129,'multiline':False]
['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1135,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Mistral, LLAMA->MISTRAL','line_number':1176,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1184,'multiline':False]
