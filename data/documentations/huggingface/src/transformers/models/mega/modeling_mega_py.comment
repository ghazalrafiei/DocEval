['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The Mega Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all Mega models at https://huggingface.co/models?filter=mega','line_number':55,'multiline':False]
['text':' registering a buffer here allows model tracing when not passing optional token type IDs','line_number':71,'multiline':False]
['text':' more info at transformers issue #5664','line_number':72,'multiline':False]
['text':' get the word embeddings if only IDs are provided','line_number':86,'multiline':False]
['text':' the original Mega implementation did not include token type embeddings, so we add','line_number':92,'multiline':False]
['text':' an option to use them if desired; if embeddings are present and token type IDs are','line_number':93,'multiline':False]
['text':' not provided, we will use a registered buffer (which helps with tracing)','line_number':94,'multiline':False]
['text':' access token type embeddings','line_number':104,'multiline':False]
['text':' add the token type embeddings to the word embeddings','line_number':106,'multiline':False]
['text':' seq_len * 2 - 1','line_number':128,'multiline':False]
['text':' seq_len * 3 - 1','line_number':130,'multiline':False]
['text':' (seq_len * 3 - 1) * seq_len','line_number':132,'multiline':False]
['text':' seq_len x (3 * seq_len - 2)','line_number':135,'multiline':False]
['text':' alpha and beta parameters for the rotary bias; beta renamed to b_param to avoid clashes with tf/flax weight handling','line_number':162,'multiline':False]
['text':' in loading pretrained weights','line_number':163,'multiline':False]
['text':' (batch_size X sequence_length X feature_dimension)','line_number':213,'multiline':False]
['text':' -> (batch_size X feature_dimension X sequence_length)','line_number':214,'multiline':False]
['text':' -> (batch_size X sequence_length X feature_dimension)','line_number':215,'multiline':False]
['text':' (sequence_length X batch_size X feature_dimension)','line_number':224,'multiline':False]
['text':' -> (batch_size X feature_dimension X sequence_length)','line_number':225,'multiline':False]
['text':' -> (sequence_length X batch_size X feature_dimension)','line_number':226,'multiline':False]
['text':' add this layernorm class to ALL_LAYERNORM_LAYERS','line_number':316,'multiline':False]
['text':' renamed delta (damping_factor) and alpha (decay_factor) to be more descriptive of what the parameters are doing','line_number':339,'multiline':False]
['text':' renamed gamma (kernel_projection_matrix) and beta (ema_expansion_matrix) respectively to avoid HF renaming','line_number':342,'multiline':False]
['text':' things and align with the paper's description of these params' behavior','line_number':343,'multiline':False]
['text':' renamed omega to residual_weight to describe what it's doing','line_number':346,'multiline':False]
['text':' convert the alpha and delta parameters (kernel_dim x EMA projection size x 1) to [0, 1] with sigmoid','line_number':353,'multiline':False]
['text':' computes the kernel used for efficient damped EMA applied via FFT convolution','line_number':360,'multiline':False]
['text':' p and q have shape (kernel_dim x ema_projection_size x 1)','line_number':362,'multiline':False]
['text':' extend the kernel to (kernel_dim X ema_projection_size X sequence_length) and','line_number':364,'multiline':False]
['text':' multiply q by sequential ints up to the sequence length','line_number':365,'multiline':False]
['text':' (kernel_dim X ema_projection_size X sequence_length) -> (kernel_dim, sequence_length)','line_number':368,'multiline':False]
['text':' this is a wrapper for repeated use of EMA calculation via FFT (fast Fourier transform) convolution','line_number':389,'multiline':False]
['text':' (kernel_dim X ema_projection_size X 1)','line_number':399,'multiline':False]
['text':' (kernel_dim X ema_projection_size X 1+sequence_length)','line_number':401,'multiline':False]
['text':' (kernel_dim X ema_projection_size X sequence_length) * (kernel_dim X ema_projection_size X 1)','line_number':407,'multiline':False]
['text':' -> (kernel_dim X ema_projection_size X sequence_length)','line_number':408,'multiline':False]
['text':' past_state will be (batch_size, kernel_dim, ema_projection_size)','line_number':410,'multiline':False]
['text':' (kernel_dim X ema_projection_size) * (batch_size X kernel_dim X ema_projection_size)','line_number':412,'multiline':False]
['text':' -> (batch_size X kernel_dim X ema_projection_size)','line_number':413,'multiline':False]
['text':' (kernel_dim X ema_projection_size X sequence_length)','line_number':419,'multiline':False]
['text':' return a tuple:','line_number':432,'multiline':False]
['text':' (sequence_length, batch_size, kernel_dim)','line_number':433,'multiline':False]
['text':' (batch_size, kernel_dim, ema_projection_size)','line_number':434,'multiline':False]
['text':' (kernel_dim X ema_projection_size) x (batch_size X kernel_dim X 1)','line_number':439,'multiline':False]
['text':' -> (batch_size X kernel_dim X ema_projection_size)','line_number':440,'multiline':False]
['text':' (batch_size X kernel_dim)','line_number':444,'multiline':False]
['text':' (1 X batch_size X kernel_dim), (batch_size X kernel_dim X ema_projection_size)','line_number':446,'multiline':False]
['text':' sequence_length X batch_size X hidden_size','line_number':486,'multiline':False]
['text':' (sequence_length x batch_size x hidden_size) -> (batch_size x hidden_size x sequence_length)','line_number':489,'multiline':False]
['text':' mask the input: output is a tensor with 0 in the masked positions','line_number':491,'multiline':False]
['text':' (batch_size X hidden_size) -> (1 x batch_size x hidden_size)','line_number':501,'multiline':False]
['text':' if incremental decoding, return the new state along with the output','line_number':504,'multiline':False]
['text':' (hidden_size x sequence_length)','line_number':507,'multiline':False]
['text':' split the kernel for each direction of EMA','line_number':513,'multiline':False]
['text':' (hidden_size X 2*sequence_length - 1)','line_number':515,'multiline':False]
['text':' (batch_size X hidden_size X sequence_length) -> (sequence_length X batch_size X hidden_size)','line_number':523,'multiline':False]
['text':' Attention dropout is standard dropout','line_number':548,'multiline':False]
['text':' (batch_size X source_sequence_length) --> (batch_size X 1 X 1)','line_number':576,'multiline':False]
['text':' (target_sequence_length X source_sequence_length)','line_number':581,'multiline':False]
['text':' source_sequence_length','line_number':586,'multiline':False]
['text':' (target_sequence_length X source_sequence_length)','line_number':589,'multiline':False]
['text':' (batch_size X target_sequence_length X source_sequence_length)','line_number':592,'multiline':False]
['text':' (target_sequence_length X source_sequence_length)','line_number':606,'multiline':False]
['text':' source_sequence_length','line_number':611,'multiline':False]
['text':' (target_sequence_length X source_sequence_length)','line_number':614,'multiline':False]
['text':' scaled attention','line_number':617,'multiline':False]
['text':' (batch_size X target_sequence_length X source_sequence_length)','line_number':619,'multiline':False]
['text':' make sure the inputs only have a sequence length of 1 if we're doing incremental decoding','line_number':683,'multiline':False]
['text':' expect past_key_values to have (self_key, self_value, self_ema, cross_key, cross_value)','line_number':686,'multiline':False]
['text':' use the self-attention cache to get the position id of the current step','line_number':690,'multiline':False]
['text':' we still need the position id if we're doing incremental decoding (past_key_values will be None for the first step)','line_number':695,'multiline':False]
['text':' (target_sequence_length X batch_size X 2*hidden_size + shared_representation_size)','line_number':702,'multiline':False]
['text':' split the query projections into separate components','line_number':704,'multiline':False]
['text':' - residual_weight is passed through sigmoid and sent through elementwise multiplication to the gated/weighted targets prior to being added to the query directly','line_number':705,'multiline':False]
['text':' - target_gate is a silu-gated tensor that is multiplied by the attention-weighted target below prior to residual connection','line_number':706,'multiline':False]
['text':' - attention_query is the part that is passed to the attention function','line_number':707,'multiline':False]
['text':' (target_sequence_length X batch_size X hidden_size)','line_number':714,'multiline':False]
['text':' (source_sequence_length X batch_size X shared_representation_size)','line_number':723,'multiline':False]
['text':' (source_sequence_length X batch_size X hidden_size)','line_number':725,'multiline':False]
['text':' (target_sequence_length X batch_size X shared_representation_size)','line_number':728,'multiline':False]
['text':' -> (batch_size X target_sequence_length X shared_representation_size)','line_number':729,'multiline':False]
['text':' if we're doing incremental decoding, k and v are None and need to be overwritten with past values','line_number':736,'multiline':False]
['text':' if we're returning the cache for later use, store these now for later return (can be done without having past_key_values provided)','line_number':741,'multiline':False]
['text':' This is part of a workaround to get around fork/join parallelism','line_number':747,'multiline':False]
['text':' not supporting Optional types.','line_number':748,'multiline':False]
['text':' (batch_size X target_sequence_length X hidden_size)','line_number':769,'multiline':False]
['text':' -> (target_sequence_length X batch_size X hidden_size)','line_number':770,'multiline':False]
['text':' (target_sequence_length X batch_size X hidden_size)','line_number':772,'multiline':False]
['text':' attention dropout is standard dropout','line_number':807,'multiline':False]
['text':' (batch_size X number of chunks X 1)','line_number':845,'multiline':False]
['text':' (batch_size X number of chunks X 1 X 1)','line_number':847,'multiline':False]
['text':' (sequence_length X sequence_length)','line_number':855,'multiline':False]
['text':' (1 X sequence_length)','line_number':860,'multiline':False]
['text':' (batch_size X number of chunks X sequence_length X sequence_length)','line_number':863,'multiline':False]
['text':' (sequence_length X sequence_length)','line_number':879,'multiline':False]
['text':' (1 X sequence_length)','line_number':884,'multiline':False]
['text':' scaled attention','line_number':887,'multiline':False]
['text':' (batch_size x number of chunks x chunk_size x chunk_size) if chunking','line_number':890,'multiline':False]
['text':' (batch_size x 1 x sequence_length x sequence_length) otherwise','line_number':891,'multiline':False]
['text':' apply causal mask (presumed to be 1/0 for not masked / masked)','line_number':894,'multiline':False]
['text':' additive, but convert to 0/-inf (which is not explicitly in the Mega source code)','line_number':895,'multiline':False]
['text':' 1 for tokens which are *not masked*','line_number':902,'multiline':False]
['text':' 0 for tokens which are *masked*','line_number':903,'multiline':False]
['text':' replace masked tokens with -inf to make softmax ignore them','line_number':904,'multiline':False]
['text':' need to invert the padding mask to match what mega original did','line_number':905,'multiline':False]
['text':' store inputs for residual connection and handle pre-norm if requested','line_number':966,'multiline':False]
['text':' (sequence_length X batch_size X hidden_size) -> (sequence_length X batch_size X intermediate_size)','line_number':971,'multiline':False]
['text':' unpack the incremental state if provided','line_number':974,'multiline':False]
['text':' assumed to be (self K, self V, self EMA state, cross K, cross V)','line_number':975,'multiline':False]
['text':' also assumes that incremental decoding is working one token at a time, so input sequence length must be 1','line_number':976,'multiline':False]
['text':' the first 3 items in the saved states will be these regardless of whether cross-attention is present','line_number':980,'multiline':False]
['text':' ema output is (sequence_length x batch_size x hidden_size)','line_number':985,'multiline':False]
['text':' updated_ema_state will be None if use_cache=False; otherwise (batch_size, config.ndim)','line_number':986,'multiline':False]
['text':' (sequence_length X batch_size X hidden_size)','line_number':992,'multiline':False]
['text':' -> (sequence_length X batch_size X 2*hidden_size + config.shared_representation_size + config.intermediate_size)','line_number':993,'multiline':False]
['text':' - residual_weight -> sigmoid -> applied to residual connection in torch.addcmul','line_number':994,'multiline':False]
['text':' - query_key_gates -> split into two components: query_key becomes query and key for attention input, gates becomes gating for self-attention output','line_number':995,'multiline':False]
['text':' - intermediate_state -> added to weighted attention output, sent through activation, and has inputs subtracted during','line_number':996,'multiline':False]
['text':'   torch.addcmul to create the final layer output','line_number':997,'multiline':False]
['text':' (sequence_length X batch_size X hidden_size)','line_number':1009,'multiline':False]
['text':' (sequence_length X batch_size X shared_representation_size + intermediate_size)','line_number':1012,'multiline':False]
['text':' split into two different tensors: one for Q/K usage and the other for gating self-attention','line_number':1015,'multiline':False]
['text':' (sequence_length X batch_size X shared_representation_size)','line_number':1020,'multiline':False]
['text':' -> (sequence_length X batch_size X 1 X shared_representation_size)','line_number':1021,'multiline':False]
['text':' -> (sequence_length X batch_size X 2 X shared_representation_size)','line_number':1022,'multiline':False]
['text':' (sequence_length X batch_size X 2 X shared_representation_size)','line_number':1025,'multiline':False]
['text':' -> 2 tensors of (sequence_length X batch_size X shared_representation_size)','line_number':1026,'multiline':False]
['text':' (sequence_length X batch_size X dimension)','line_number':1029,'multiline':False]
['text':' -> (batch_size X sequence_length X dimension)','line_number':1030,'multiline':False]
['text':' where `dimension` is either shared_representation_size (queries and keys) or intermediate_size (values)','line_number':1031,'multiline':False]
['text':' combine history and current to save updated state (if history is provided)','line_number':1037,'multiline':False]
['text':' when chunking is applied, the past states will be None at the end of the chunk, in','line_number':1038,'multiline':False]
['text':' which case, proceed as if no K/V history had been provided','line_number':1039,'multiline':False]
['text':' saved states are stored with shape (batch_size X sequence_length X dimension)','line_number':1040,'multiline':False]
['text':' if not chunking, store as-is','line_number':1046,'multiline':False]
['text':' if we're chunking and have reached the end of a chunk, wipe out the saved state','line_number':1053,'multiline':False]
['text':' potentially differs from seq_len because of incremental decoding','line_number':1060,'multiline':False]
['text':' if we're not chunking, treat the entire sequence as one long chunk','line_number':1062,'multiline':False]
['text':' (batch_size X sequence_length X dimension) -> (batch_size X 1 X sequence_length X dimension)','line_number':1063,'multiline':False]
['text':' (batch_size X sequence_length) -> (batch_size X 1 X sequence_length)','line_number':1068,'multiline':False]
['text':' otherwise, split the sequences in the batch into `n_chunks` chunks of size `chunk_size`','line_number':1071,'multiline':False]
['text':' (batch_size X sequence_length X dimension) -> (batch_size X n_chunks X chunk_size X dimension)','line_number':1075,'multiline':False]
['text':' (batch_size X sequence_length X dimension) -> (batch_size X n_chunks X chunk_size X dimension)','line_number':1085,'multiline':False]
['text':' this is in the original Mega implementation to work around fork/join parallelism not supporting optional types','line_number':1092,'multiline':False]
['text':' (batch_size x n_chunks x chunk_size x intermediate_size) -> (sequence_length X batch_size X intermediate_size)','line_number':1101,'multiline':False]
['text':' (sequence_length X batch_size X intermediate_size) -> (sequence_length X batch_size X hidden_size)','line_number':1106,'multiline':False]
['text':' (sequence_length X batch_size X hidden_size)','line_number':1109,'multiline':False]
['text':' incremental decoding in the MegaMultiDimensionDampedEma module requires that the attention mask has the same','line_number':1250,'multiline':False]
['text':' sequence length as the input tensor; if we're caching incremental states, we assume the input','line_number':1251,'multiline':False]
['text':' sequence length is 1 (Mega will break otherwise), so we take the padding mask for the final','line_number':1252,'multiline':False]
['text':' token in the input (mask is received as [batch X sequence length])','line_number':1253,'multiline':False]
['text':' optional cross attention','line_number':1272,'multiline':False]
['text':' update the hidden state from cross attention','line_number':1287,'multiline':False]
['text':' store cross-attention k/v if caching','line_number':1289,'multiline':False]
['text':' optional NFFN follows cross attention','line_number':1293,'multiline':False]
['text':' copied from transformers.models.roberta.modeling_roberta.RobertaPooler with Roberta->Mega','line_number':1317,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':1325,'multiline':False]
['text':' to the first token.','line_number':1326,'multiline':False]
['text':' delta & alpha','line_number':1348,'multiline':False]
['text':' beta [1, -1, 1, -1, ...] seems more stable.','line_number':1351,'multiline':False]
['text':' gamma & omega','line_number':1357,'multiline':False]
['text':' linear layers covered separately by the generic nn.Linear init below','line_number':1372,'multiline':False]
['text':' initializes all linear layers in the entire network','line_number':1376,'multiline':False]
['text':' Initialize weights and apply final processing (retained from RoBERTa code)','line_number':1475,'multiline':False]
['text':' Mega expects the causal mask to be a 2D square matrix of (from) x (to) over the input sequence length','line_number':1557,'multiline':False]
['text':' the HF utility function generates a 3D causal mask which includes batch size, so we'll create a dummy','line_number':1558,'multiline':False]
['text':' mask with the correct device and all ones','line_number':1559,'multiline':False]
['text':' get rid of batch dimension in the generated mask; result is (sequence_length X sequence_length)','line_number':1563,'multiline':False]
['text':' if using cache, make sure we have a tuple of tuples which matches the length of our hidden layers','line_number':1569,'multiline':False]
['text':' get embeddings (batch X sequence length X embed dim)','line_number':1575,'multiline':False]
['text':' transpose for Mega --> (seq len X batch X embed dim)','line_number':1580,'multiline':False]
['text':' we expect encoder hidden states to also have batch first in line','line_number':1583,'multiline':False]
['text':' with typical Hugging Face behavior (which is also how we return them)','line_number':1584,'multiline':False]
['text':' Mega expects sequence length first, so do the same transpose here','line_number':1585,'multiline':False]
['text':' pass through mega layers','line_number':1589,'multiline':False]
['text':' store layer-wise hidden states in the way that the user expects','line_number':1609,'multiline':False]
['text':' (seq len X batch X embed dim) --> (batch X seq len X embed dim)','line_number':1610,'multiline':False]
['text':' transpose final hidden states','line_number':1622,'multiline':False]
['text':' optional pooling layer','line_number':1625,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1669,'multiline':False]
['text':' we are doing next-token prediction; shift prediction scores and input ids by one','line_number':1769,'multiline':False]
['text':' if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly','line_number':1790,'multiline':False]
['text':' cut decoder_input_ids if past is used','line_number':1794,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1832,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1923,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2013,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2102,'multiline':False]
['text':' copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->Mega','line_number':2160,'multiline':False]
['text':' take <s> token (equiv. to [CLS])','line_number':2174,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2198,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':2250,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':2255,'multiline':False]
