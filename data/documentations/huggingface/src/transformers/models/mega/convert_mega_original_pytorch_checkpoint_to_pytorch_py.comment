['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' utilities to import the model weights and config file','line_number':29,'multiline':False]
['text':' PyTorch + new model classes','line_number':33,'multiline':False]
['text':' import the EncoderLayer class used to pretrain','line_number':40,'multiline':False]
['text':' !! NOTE !! this requires the version of fairseq that is built when you install the Mega source','line_number':41,'multiline':False]
['text':' define the wrapper classes used to train the MLM  (see colab notebook below)','line_number':48,'multiline':False]
['text':' https://colab.research.google.com/drive/1qfUO6o5HRdxBblWlw058HVyvaEPhPpH8?usp=sharing','line_number':49,'multiline':False]
['text':' MegaLM outputs hidden states','line_number':50,'multiline':False]
['text':' Mega expects embeddings to be (time, batch, embedding size), but','line_number':73,'multiline':False]
['text':' Hugging Face returns tokens as (batch, time)','line_number':74,'multiline':False]
['text':' to make things more confusing, Mega expects the attention mask to','line_number':78,'multiline':False]
['text':' be (batch, time), but with values of 0 (normal token) and 1 (ignore token)','line_number':79,'multiline':False]
['text':' which is the opposite of what HF returns','line_number':80,'multiline':False]
['text':' get token embeddings from IDs','line_number':84,'multiline':False]
['text':' pass through the Mega layers','line_number':87,'multiline':False]
['text':' input is (time, batch, encoder dim) and output is the same','line_number':88,'multiline':False]
['text':' return according to the shape specified','line_number':92,'multiline':False]
['text':' (T, B, H) --> (B, T, H)','line_number':94,'multiline':False]
['text':' renamed from MegaForMaskedLM to avoid confusion with new module','line_number':100,'multiline':False]
['text':' code to convert the checkpoint located in the user-specified location','line_number':122,'multiline':False]
['text':' load the original encoder','line_number':127,'multiline':False]
['text':' load its weights','line_number':130,'multiline':False]
['text':' create a new config from the old one','line_number':144,'multiline':False]
['text':' new arguments added for HF implementation','line_number':169,'multiline':False]
['text':' the originl checkpoint just uses nn.Embedding for the word embeddings','line_number':177,'multiline':False]
['text':' we use a wrapper module for embeddings to add support for positional embeddings','line_number':178,'multiline':False]
['text':' modify the state dictionary of the original checkpoint to account for naming issues in the Hugging Face','line_number':181,'multiline':False]
['text':' ecosystem -- any names containing "beta" or "gamma" aren't safe to use and are renamed upon _load_pretrained,','line_number':182,'multiline':False]
['text':' also renaming previously confusing parameter names','line_number':183,'multiline':False]
['text':' have to handle gamma, beta, and alpha differently due to their use','line_number':188,'multiline':False]
['text':' in multiple modules within the original repository;','line_number':189,'multiline':False]
['text':' beta is used in EMA, MovingAverageGatedAttention, and RotaryRelativePositionalBias, and must be renamed due to flax/tf weights','line_number':190,'multiline':False]
['text':' the EMA sublayer was renamed from "move" to "ema_gate" for readability, so that is also done here','line_number':191,'multiline':False]
['text':' EMA sub-layers were always called "move" in the original repo','line_number':193,'multiline':False]
['text':' beta is used in EMA and MovingAverageGatedAttention, and must be renamed due to flax/tf weights','line_number':200,'multiline':False]
['text':' alpha is used in EMA and positional bias; renaming to improve readability','line_number':208,'multiline':False]
['text':' delta is only used in EMA; renaming to improve readability','line_number':211,'multiline':False]
['text':' omega is only used in EMA; renaming to improve readability','line_number':214,'multiline':False]
['text':' now attempt to load the state dictionary with updated names','line_number':228,'multiline':False]
['text':' note that we now call it `mega.layers` instead of `mega.encoders` due to hugging face style','line_number':229,'multiline':False]
['text':' load the MLM head weights directly','line_number':232,'multiline':False]
['text':' test on a randomly generated input sequence','line_number':240,'multiline':False]
['text':' mask a few tokens to make sure masking is applied appropriately :)','line_number':243,'multiline':False]
['text':' run forward passes','line_number':246,'multiline':False]
['text':' print shapes and diff','line_number':250,'multiline':False]
['text':' 0.0','line_number':253,'multiline':False]
