['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Meta and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Autocast world','line_number':174,'multiline':False]
['text':' This is not available in all DeepSpeed versions.','line_number':188,'multiline':False]
['text':' assumes all on same device','line_number':206,'multiline':False]
['text':' DISCREPANCY: c_hidden is not the per-head channel dimension, as','line_number':387,'multiline':False]
['text':' stated in the supplement, but the overall channel dimension.','line_number':388,'multiline':False]
['text':' [*, Q/K/V, H * C_hidden]','line_number':402,'multiline':False]
['text':' [*, Q/K, H, C_hidden]','line_number':407,'multiline':False]
['text':' [*, H, Q/K, C_hidden]','line_number':412,'multiline':False]
['text':' [*, Q, H, C_hidden]','line_number':425,'multiline':False]
['text':' [*, Q, H * C_hidden]','line_number':429,'multiline':False]
['text':' [*, Q, C_q]','line_number':432,'multiline':False]
['text':' [*, H, Q/K, C_hidden]','line_number':483,'multiline':False]
['text':' [*, H, Q, K]','line_number':487,'multiline':False]
['text':' [*, H, Q, C_hidden]','line_number':493,'multiline':False]
['text':' [*, I, J]','line_number':568,'multiline':False]
['text':' [*, I, J, C_in]','line_number':577,'multiline':False]
['text':' [*, I, 1, 1, J]','line_number':580,'multiline':False]
['text':' [*, H, I, J]','line_number':583,'multiline':False]
['text':' [*, 1, H, I, J]','line_number':586,'multiline':False]
['text':' To be replaced by torch vmap','line_number':644,'multiline':False]
['text':' This computation is chunked so as not to exceed our 2.5x','line_number':736,'multiline':False]
['text':' budget with a large intermediate tensor','line_number':737,'multiline':False]
['text':' We start by fully manifesting a. In addition to the input, this','line_number':759,'multiline':False]
['text':' brings total memory consumption to 2x z (disregarding size of chunks)','line_number':760,'multiline':False]
['text':' [*, N, N, c]','line_number':761,'multiline':False]
['text':' Slices start:end from the dim dimension of t','line_number':775,'multiline':False]
['text':' "Reorient" the z_cache (see below), filling it with quadrants','line_number':781,'multiline':False]
['text':' 3---recovered from the z_cache---and 4---recovered from z---','line_number':782,'multiline':False]
['text':' of the input tensor z.','line_number':783,'multiline':False]
['text':' If n is odd, we need to shrink the z_cache by one row','line_number':787,'multiline':False]
['text':' Move the 3rd quadrant of z into the','line_number':790,'multiline':False]
['text':' Get the fourth quadrant of z','line_number':795,'multiline':False]
['text':' Insert said quadrant into the rotated z-cache','line_number':799,'multiline':False]
['text':' Initialize the z cache to the left half of z.','line_number':807,'multiline':False]
['text':' We need to reorient the z-cache at the halfway point, and we','line_number':816,'multiline':False]
['text':' don't want a single chunk to straddle that point. We contract one','line_number':817,'multiline':False]
['text':' of the chunks in the middle to address that problem.','line_number':818,'multiline':False]
['text':' b_chunk_dim == row_dim','line_number':835,'multiline':False]
['text':' In this case, the b-dimension (b_chunk_dim) is partially','line_number':836,'multiline':False]
['text':' overwritten at the end of each iteration. We need to','line_number':837,'multiline':False]
['text':' restore the missing component from the z-cache.','line_number':838,'multiline':False]
['text':' The g dimension (col_dim) is parallel to and ahead of the','line_number':855,'multiline':False]
['text':' overwrites in z. We can extract the g chunk normally.','line_number':856,'multiline':False]
['text':' Write the columns into z in-place','line_number':864,'multiline':False]
['text':' Subclass `EsMPreTrainedModel` to deal with special init','line_number':946,'multiline':False]
['text':' Add external attention bias.','line_number':1033,'multiline':False]
['text':' Do not attend to padding tokens.','line_number':1037,'multiline':False]
['text':' noqa: E712','line_number':1040,'multiline':False]
['text':' Update sequence state','line_number':1226,'multiline':False]
['text':' Self attention with bias + mlp.','line_number':1229,'multiline':False]
['text':' Update pairwise state','line_number':1235,'multiline':False]
['text':' Axial attention with triangular bias.','line_number':1238,'multiline':False]
['text':' MLP over pairs.','line_number':1249,'multiline':False]
['text':' All tensors are of shape ..., bins.','line_number':1257,'multiline':False]
['text':' Shapes are:','line_number':1263,'multiline':False]
['text':'     self.probs: ... x bins','line_number':1264,'multiline':False]
['text':'     true      : ...','line_number':1265,'multiline':False]
['text':' Logits are ..., 37, bins.','line_number':1275,'multiline':False]
['text':' Note an additional offset is used so that the 0th position','line_number':1306,'multiline':False]
['text':' is reserved for masked pairs.','line_number':1307,'multiline':False]
['text':' Add 1 to adjust for padding index.','line_number':1327,'multiline':False]
['text':' noqa: E712','line_number':1331,'multiline':False]
['text':' NOTE: The ReLU's applied to the inputs are absent from the supplement','line_number':1388,'multiline':False]
['text':' pseudocode but present in the source. For maximal compatibility with','line_number':1389,'multiline':False]
['text':' the pretrained weights, I'm going with the source.','line_number':1390,'multiline':False]
['text':' [*, C_hidden]','line_number':1392,'multiline':False]
['text':' [*, no_angles * 2]','line_number':1404,'multiline':False]
['text':' [*, no_angles, 2]','line_number':1407,'multiline':False]
['text':' These linear layers differ from their specifications in the','line_number':1438,'multiline':False]
['text':' supplement. There, they lack bias and use Glorot initialization.','line_number':1439,'multiline':False]
['text':' Here as in the official source, they have bias and use the default','line_number':1440,'multiline':False]
['text':' Lecun initialization.','line_number':1441,'multiline':False]
['text':'######################################','line_number':1486,'multiline':False]
['text':' Generate scalar and point activations','line_number':1487,'multiline':False]
['text':'######################################','line_number':1488,'multiline':False]
['text':' [*, N_res, H * C_hidden]','line_number':1489,'multiline':False]
['text':' [*, N_res, H, C_hidden]','line_number':1493,'multiline':False]
['text':' [*, N_res, H, 2 * C_hidden]','line_number':1496,'multiline':False]
['text':' [*, N_res, H, C_hidden]','line_number':1499,'multiline':False]
['text':' [*, N_res, H * P_q * 3]','line_number':1502,'multiline':False]
['text':' This is kind of clunky, but it's how the original does it','line_number':1505,'multiline':False]
['text':' [*, N_res, H * P_q, 3]','line_number':1506,'multiline':False]
['text':' [*, N_res, H, P_q, 3]','line_number':1511,'multiline':False]
['text':' [*, N_res, H * (P_q + P_v) * 3]','line_number':1514,'multiline':False]
['text':' [*, N_res, H * (P_q + P_v), 3]','line_number':1517,'multiline':False]
['text':' [*, N_res, H, (P_q + P_v), 3]','line_number':1522,'multiline':False]
['text':' [*, N_res, H, P_q/P_v, 3]','line_number':1525,'multiline':False]
['text':'#########################','line_number':1528,'multiline':False]
['text':' Compute attention scores','line_number':1529,'multiline':False]
['text':'#########################','line_number':1530,'multiline':False]
['text':' [*, N_res, N_res, H]','line_number':1531,'multiline':False]
['text':' [*, H, N_res, N_res]','line_number':1538,'multiline':False]
['text':' [*, H, N_res, C_hidden]','line_number':1542,'multiline':False]
['text':' [*, H, C_hidden, N_res]','line_number':1543,'multiline':False]
['text':' [*, H, N_res, C_hidden]','line_number':1547,'multiline':False]
['text':' [*, H, C_hidden, N_res]','line_number':1548,'multiline':False]
['text':' [*, N_res, N_res, H, P_q, 3]','line_number':1554,'multiline':False]
['text':' [*, N_res, N_res, H, P_q]','line_number':1558,'multiline':False]
['text':' [*, N_res, N_res, H]','line_number':1564,'multiline':False]
['text':' [*, N_res, N_res]','line_number':1566,'multiline':False]
['text':' [*, H, N_res, N_res]','line_number':1570,'multiline':False]
['text':'###############','line_number':1577,'multiline':False]
['text':' Compute output','line_number':1578,'multiline':False]
['text':'###############','line_number':1579,'multiline':False]
['text':' [*, N_res, H, C_hidden]','line_number':1580,'multiline':False]
['text':' [*, N_res, H * C_hidden]','line_number':1583,'multiline':False]
['text':' [*, H, 3, N_res, P_v]','line_number':1586,'multiline':False]
['text':' [*, N_res, H, P_v, 3]','line_number':1592,'multiline':False]
['text':' [*, N_res, H * P_v]','line_number':1596,'multiline':False]
['text':' [*, N_res, H * P_v, 3]','line_number':1599,'multiline':False]
['text':' [*, N_res, H, C_z]','line_number':1605,'multiline':False]
['text':' [*, N_res, H * C_z]','line_number':1608,'multiline':False]
['text':' [*, N_res, C_s]','line_number':1611,'multiline':False]
['text':' [*, 6]','line_number':1636,'multiline':False]
['text':' Buffers to be lazily initialized later','line_number':1693,'multiline':False]
['text':' self.default_frames','line_number':1694,'multiline':False]
['text':' self.group_idx','line_number':1695,'multiline':False]
['text':' self.atom_mask','line_number':1696,'multiline':False]
['text':' self.lit_positions','line_number':1697,'multiline':False]
['text':' [*, N]','line_number':1738,'multiline':False]
['text':' [*, N, C_s]','line_number':1741,'multiline':False]
['text':' [*, N, N, C_z]','line_number':1744,'multiline':False]
['text':' [*, N, C_s]','line_number':1754,'multiline':False]
['text':' [*, N]','line_number':1758,'multiline':False]
['text':' [*, N, C_s]','line_number':1768,'multiline':False]
['text':' [*, N]','line_number':1781,'multiline':False]
['text':' To hew as closely as possible to AlphaFold, we convert our','line_number':1784,'multiline':False]
['text':' quaternion-based transformations to rotation-matrix ones','line_number':1785,'multiline':False]
['text':' here','line_number':1786,'multiline':False]
['text':' [*, N, 7, 2]','line_number':1794,'multiline':False]
['text':' Lazily initialize the residue constants on the correct device','line_number':1872,'multiline':False]
['text':' Separated purely to make testing less annoying','line_number':1874,'multiline':False]
['text':' [*, N, 8]  # [*, N]','line_number':1877,'multiline':False]
['text':' Lazily initialize the residue constants on the correct device','line_number':1878,'multiline':False]
['text':' This parameter means the axial attention will be computed','line_number':1915,'multiline':False]
['text':' in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).','line_number':1916,'multiline':False]
['text':' It's equivalent to running a for loop over chunks of the dimension we're iterative over,','line_number':1917,'multiline':False]
['text':' where the chunk_size is the size of the chunks, so 128 would mean to parse 128-lengthed chunks.','line_number':1918,'multiline':False]
['text':' First 'recycle' is just the standard forward pass through the model.','line_number':1940,'multiline':False]
['text':' === Recycling ===','line_number':1957,'multiline':False]
['text':' === Structure module ===','line_number':1964,'multiline':False]
['text':' Distogram needs the N, CA, C coordinates, and bin constants same as alphafold.','line_number':1973,'multiline':False]
['text':' Coords are [... L x 3 x 3], where it's [N, CA, C] x 3 coordinates.','line_number':1988,'multiline':False]
['text':' Infer CB coordinates.','line_number':1997,'multiline':False]
['text':' [..., L, L]','line_number':2003,'multiline':False]
['text':' TODO Add information to the docstring about any methods that convert to PDB format, or otherwise prepare','line_number':2007,'multiline':False]
['text':'      the outputs for downstream use.','line_number':2008,'multiline':False]
['text':' 0 is padding, N is unknown residues, N + 1 is mask.','line_number':2052,'multiline':False]
['text':' Remember that t is shifted from residue_constants by 1 (0 is padding).','line_number':2080,'multiline':False]
['text':' B x L','line_number':2112,'multiline':False]
['text':' === ESM ===','line_number':2121,'multiline':False]
['text':' We get sequence and pair representations from whatever version of ESM /','line_number':2130,'multiline':False]
['text':' configuration we are using. The sequence representation esm_s is always','line_number':2131,'multiline':False]
['text':' present. The pair embedding esm_z may be present depending on the','line_number':2132,'multiline':False]
['text':' configuration of the model. If esm_z is not used by the model then it','line_number':2133,'multiline':False]
['text':' is returned as None here.','line_number':2134,'multiline':False]
['text':' Convert esm_s and esm_z, if present, to the precision used by the trunk and','line_number':2137,'multiline':False]
['text':' the structure module. These tensors may be a lower precision if, for example,','line_number':2138,'multiline':False]
['text':' we're running the language model in fp16 precision.','line_number':2139,'multiline':False]
['text':' === preprocessing ===','line_number':2147,'multiline':False]
['text':' Documenting what we expect:','line_number':2157,'multiline':False]
['text':' Add BERT mask for the loss to use, if available.','line_number':2174,'multiline':False]
['text':' Of course, this doesn't respect the true mask because it doesn't know about it...','line_number':2187,'multiline':False]
['text':' We're not going to properly mask change of index tensors:','line_number':2188,'multiline':False]
['text':'    "residx_atom14_to_atom37",','line_number':2189,'multiline':False]
['text':'    "residx_atom37_to_atom14",','line_number':2190,'multiline':False]
['text':' avoid indexing on different devices','line_number':2211,'multiline':False]
['text':' B = batch size, L = sequence length.','line_number':2219,'multiline':False]
['text':' Use the first padding index as eos during inference.','line_number':2229,'multiline':False]
['text':' _, esm_z, esm_s = self.esm(esmaa, return_pairs=self.config.esmfold_config.use_esm_attn_map)','line_number':2232,'multiline':False]
['text':' Because we do not support use_esm_attn_map in the HF port as it is not used in any public models,','line_number':2233,'multiline':False]
['text':' esm_z is always None','line_number':2234,'multiline':False]
['text':' B, L, nLayers, C','line_number':2238,'multiline':False]
['text':' Returns the raw outputs of the model given an input sequence.','line_number':2261,'multiline':False]
['text':' B=1 x L','line_number':2276,'multiline':False]
