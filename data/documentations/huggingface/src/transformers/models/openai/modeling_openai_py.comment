['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' See all OpenAI GPT models at https://huggingface.co/models?filter=openai-gpt','line_number':51,'multiline':False]
['text':' This was used when we had a single embedding matrix for positions and tokens','line_number':75,'multiline':False]
['text':' init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)','line_number':76,'multiline':False]
['text':' del init_params[1]','line_number':77,'multiline':False]
['text':' Check that the token and position embeddings weight dimensions map those of the init parameters.','line_number':80,'multiline':False]
['text':' Pop position and token embedding arrays','line_number':96,'multiline':False]
['text':' names[1:n_transfer], init_params[1:n_transfer]):','line_number':100,'multiline':False]
['text':' skip "model/"','line_number':101,'multiline':False]
['text':' Ensure that the pointer and array have compatible shapes.','line_number':124,'multiline':False]
['text':' in Attention: n_state=768 (nx=n_embd)','line_number':139,'multiline':False]
['text':' [switch nx => n_state from Block to Attention to keep identical to TF implementation]','line_number':140,'multiline':False]
['text':' Prune conv1d layers','line_number':165,'multiline':False]
['text':' Update hyper params','line_number':168,'multiline':False]
['text':' w = w * self.bias + -1e9 * (1 - self.bias)  # TF implementation method: mask_attn_weights','line_number':177,'multiline':False]
['text':' XD: self.b may be larger than w, so we need to crop it','line_number':178,'multiline':False]
['text':' Apply the attention mask','line_number':183,'multiline':False]
['text':' Mask heads if we want to','line_number':189,'multiline':False]
['text':' in Tensorflow implementation: fct merge_states','line_number':201,'multiline':False]
['text':' in Tensorflow implementation: fct split_states','line_number':205,'multiline':False]
['text':' a, (attentions)','line_number':226,'multiline':False]
['text':' in MLP: n_state=3072 (4 * n_embd)','line_number':230,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':283,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':284,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':412,'multiline':False]
['text':' Code is different from when we had a single embedding matrix  from position and token embeddings','line_number':464,'multiline':False]
['text':' Attention mask.','line_number':467,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':469,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':470,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':471,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':472,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':473,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':476,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':477,'multiline':False]
['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':478,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':479,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':480,'multiline':False]
['text':' fp16 compatibility','line_number':481,'multiline':False]
['text':' Prepare head mask if needed','line_number':484,'multiline':False]
['text':' Add last layer','line_number':512,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':541,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':593,'multiline':False]
['text':' Flatten the tokens','line_number':596,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':635,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':759,'multiline':False]
['text':' Ensure the batch size is > 1 if there is no padding.','line_number':809,'multiline':False]
