['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Google LLC., LongT5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' TODO: Update before the merge','line_number':53,'multiline':False]
['text':' Handle cases when an empty input sequence is given','line_number':65,'multiline':False]
['text':' pad tensor to multiple of block_len','line_number':82,'multiline':False]
['text':' If 0 is in output_shape, we cannot apply reshape because of incompatibility with ONNX conversion','line_number':87,'multiline':False]
['text':' [batch_size, num_blocks, block_len] -> [batch_size, num_blocks + 2, block_len]','line_number':103,'multiline':False]
['text':' We use indexing approach here:','line_number':108,'multiline':False]
['text':' https://numpy.org/doc/stable/user/basics.indexing.html#dealing-with-variable-numbers-of-indices-within-programs','line_number':109,'multiline':False]
['text':' [batch_size, num_blocks, 3 * block_len, ...]','line_number':114,'multiline':False]
['text':' [block_len, 3 * block_len]','line_number':122,'multiline':False]
['text':' [batch_size, num_blocks, block_len]','line_number':138,'multiline':False]
['text':' [batch_size, num_block, 3 * block_len]','line_number':140,'multiline':False]
['text':' [batch_size, num_block, block_len, 3 * block_len]','line_number':145,'multiline':False]
['text':' [batch_size, 1, num_block, block_len, 3 * block_len]','line_number':148,'multiline':False]
['text':' set padding tokens to -1','line_number':183,'multiline':False]
['text':' [batch_size, seq_len]','line_number':185,'multiline':False]
['text':' [batch_size, seq_len // global_block_size]','line_number':188,'multiline':False]
['text':' (batch..., seq_len, global_seq_len))','line_number':214,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->LongT5','line_number':222,'multiline':False]
['text':' LongT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':233,'multiline':False]
['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':234,'multiline':False]
['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':235,'multiline':False]
['text':' half-precision inputs is done in fp32','line_number':236,'multiline':False]
['text':' convert into half-precision if necessary','line_number':241,'multiline':False]
['text':' noqa','line_number':251,'multiline':False]
['text':' using the normal LongT5LayerNorm','line_number':255,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->LongT5','line_number':264,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->LongT5','line_number':305,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention with T5->LongT5','line_number':324,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':338,'multiline':False]
['text':' Prune linear layers','line_number':355,'multiline':False]
['text':' Update hyper params','line_number':360,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':394,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':396,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':400,'multiline':False]
['text':' shape (query_length, key_length)','line_number':419,'multiline':False]
['text':' shape (query_length, key_length)','line_number':421,'multiline':False]
['text':' shape (query_length, key_length, num_heads)','line_number':426,'multiline':False]
['text':' shape (1, num_heads, query_length, key_length)','line_number':427,'multiline':False]
['text':' Input is (batch_size, seq_length, dim)','line_number':445,'multiline':False]
['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':446,'multiline':False]
['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':447,'multiline':False]
['text':' self-attn','line_number':472,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':473,'multiline':False]
['text':' cross-attn','line_number':476,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':477,'multiline':False]
['text':' self-attn','line_number':482,'multiline':False]
['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':483,'multiline':False]
['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':486,'multiline':False]
['text':' the provided `key_value_states` to support prefix tuning','line_number':487,'multiline':False]
['text':' cross-attn','line_number':488,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':489,'multiline':False]
['text':' cross-attn','line_number':492,'multiline':False]
['text':' get query states','line_number':496,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':497,'multiline':False]
['text':' get key/value states','line_number':499,'multiline':False]
['text':' compute scores','line_number':507,'multiline':False]
['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':510,'multiline':False]
['text':' if key and values are already calculated','line_number':522,'multiline':False]
['text':' we want only the last query position bias','line_number':523,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':528,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':540,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':543,'multiline':False]
['text':' Mask heads if we want to','line_number':545,'multiline':False]
['text':' (batch_size, seq_length, dim)','line_number':549,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':575,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention.prune_heads','line_number':586,'multiline':False]
['text':' Prune linear layers','line_number':593,'multiline':False]
['text':' Update hyper params','line_number':598,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket','line_number':604,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':633,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':635,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':639,'multiline':False]
['text':' (block_length, 3 * block_length)','line_number':662,'multiline':False]
['text':' (block_length, 3 * block_length)','line_number':665,'multiline':False]
['text':' (block_length, 3 * block_length, num_heads)','line_number':670,'multiline':False]
['text':' (1, 1, num_heads, block_length, 3 * block_length)','line_number':672,'multiline':False]
['text':' get query/key/value states -> (batch_size, seq_length, n_heads, dim_per_head)','line_number':694,'multiline':False]
['text':' Split into blocks -> (batch_size, num_blocks, block_len, n_heads, dim_per_head)','line_number':699,'multiline':False]
['text':' Concatenate 3 blocks for keys and values -> (batch_size, num_blocks, 3 * block_len, n_heads, dim_per_head)','line_number':704,'multiline':False]
['text':' Compute scores','line_number':708,'multiline':False]
['text':' (batch_size, num_block, n_heads, block_len, 3 * block_len)','line_number':711,'multiline':False]
['text':' position_bias shape: # (1, 1, n_heads, block_len, 3 * block_len)','line_number':714,'multiline':False]
['text':' Replace masked positions with -1e10 (according to the original implementation)','line_number':725,'multiline':False]
['text':' We need to adjust position bias shape to be sum with mask','line_number':727,'multiline':False]
['text':' (batch_size, num_blocks, n_heads, block_len, 3 * block_len)','line_number':731,'multiline':False]
['text':' (batch_size, num_blocks, n_heads, block_len, 3 * block_len)','line_number':733,'multiline':False]
['text':' Mask heads if we want to','line_number':736,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':768,'multiline':False]
['text':' Relativen attention bias & Layer norm for global attention','line_number':778,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention.prune_heads','line_number':783,'multiline':False]
['text':' Prune linear layers','line_number':790,'multiline':False]
['text':' Update hyper params','line_number':795,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket','line_number':801,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':830,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':832,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':836,'multiline':False]
['text':' (block_length, 3 * block_length)','line_number':859,'multiline':False]
['text':' (block_length, 3 * block_length)','line_number':862,'multiline':False]
['text':' (block_length, 3 * block_length, num_heads)','line_number':867,'multiline':False]
['text':' (1, 1, num_heads, block_length, 3 * block_length)','line_number':869,'multiline':False]
['text':' (batch_size, 1, seq_len, global_seq_len)','line_number':874,'multiline':False]
['text':' (batch_size, seq_len, global_seq_len)','line_number':877,'multiline':False]
['text':' (batch_size, seq_len, global_seq_len, num_heads)','line_number':885,'multiline':False]
['text':' (batch_size, num_heads, seq_len, global_seq_len)','line_number':888,'multiline':False]
['text':' (batch_size, num_heads, seq_len, global_seq_len)','line_number':890,'multiline':False]
['text':' Prepare components for transient-global attention','line_number':912,'multiline':False]
['text':' Obtain block_ids and global_segment_ids','line_number':913,'multiline':False]
['text':' global_seq_len := seq_len // self.global_block_size','line_number':914,'multiline':False]
['text':' shapes: (batch_size, seq_len) & (batch_size, global_seq_len)','line_number':915,'multiline':False]
['text':' Create global inputs','line_number':920,'multiline':False]
['text':' get query states -> (batch_size, seq_length, n_heads, dim_per_head)','line_number':925,'multiline':False]
['text':' Get global/side key/value states  shape: (batch_size, global_seq_len, n_heads, dim_per_head)','line_number':929,'multiline':False]
['text':' Split into blocks -> (batch_size, num_blocks, block_len, n_heads, dim_per_head)','line_number':933,'multiline':False]
['text':' Concatenate 3 blocks for keys and values -> (batch_size, num_blocks, 3 * block_len, n_heads, dim_per_head)','line_number':938,'multiline':False]
['text':' Tile side inputs across local key/value blocks','line_number':942,'multiline':False]
['text':' New shape: (batch_size, num_blocks, global_seq_len, n_heads, dim_per_head)','line_number':943,'multiline':False]
['text':' Concatenate "local" and "side"/"global" key/value states to allow each token to attend global aggregated ones','line_number':949,'multiline':False]
['text':' New shape: (batch_size, num_blocks, 3 * block_len + global_seq_len, n_heads, dim_per_head)','line_number':950,'multiline':False]
['text':' Compute scores -> (batch_size, num_block, n_heads, block_len, 3 * block_len + global_seq_len)','line_number':954,'multiline':False]
['text':' We need to adjust position bias shape to be sum with mask','line_number':958,'multiline':False]
['text':' Replace masked positions with -10_000 (according to the original implementation)','line_number':960,'multiline':False]
['text':' position_bias shape: # (1, 1, n_heads, block_len, 3 * block_len)','line_number':966,'multiline':False]
['text':' (batch_size, 1, n_heads, block_len, 3 * block_len)','line_number':979,'multiline':False]
['text':' Calculate global/side bias - shape: # (batch_size, num_heads, seq_len, global_seq_len)','line_number':983,'multiline':False]
['text':' (batch_size, num_heads, seq_len, global_seq_len)','line_number':986,'multiline':False]
['text':' (batch_size, num_blocks, num_heads, block_len, global_seq_len)','line_number':988,'multiline':False]
['text':' (batch_size, num_blocks, num_heads, block_len, 3 * block_len + global_seq_len)','line_number':991,'multiline':False]
['text':' (batch_size, num_blocks, n_heads, block_len, 3 * block_len + global_seq_len)','line_number':995,'multiline':False]
['text':' Mask heads if we want to','line_number':999,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->LongT5','line_number':1015,'multiline':False]
['text':' add attentions if we output them','line_number':1044,'multiline':False]
['text':' to accept past_key_value and use_cache kwargs','line_number':1064,'multiline':False]
['text':' add attentions if we output them','line_number':1075,'multiline':False]
['text':' to accept past_key_value and use_cache kwargs','line_number':1097,'multiline':False]
['text':' add attentions if we output them','line_number':1108,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->LongT5','line_number':1112,'multiline':False]
['text':' add attentions if we output them','line_number':1145,'multiline':False]
['text':' Keep self-attention outputs and relative position weights','line_number':1213,'multiline':False]
['text':' clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/','line_number':1215,'multiline':False]
['text':' the actual query length is unknown for cross attention','line_number':1222,'multiline':False]
['text':' if using past key value states. Need to inject it here','line_number':1223,'multiline':False]
['text':' clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/','line_number':1242,'multiline':False]
['text':' Combine self attn and cross attn key value states','line_number':1247,'multiline':False]
['text':' Keep cross-attention outputs and relative position weights','line_number':1251,'multiline':False]
['text':' Apply Feed Forward layer','line_number':1254,'multiline':False]
['text':' clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/','line_number':1257,'multiline':False]
['text':' hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1269,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel.dummy_inputs','line_number':1284,'multiline':False]
['text':' Used for testing weights initialization','line_number':1297,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':1301,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':1302,'multiline':False]
['text':' Mesh TensorFlow FF initialization','line_number':1305,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':1306,'multiline':False]
['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':1307,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':1325,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':1326,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->LongT5','line_number':1341,'multiline':False]
['text':' shift inputs to the right','line_number':1352,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':1354,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':1364,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1390,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Stack.get_input_embeddings','line_number':1393,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings','line_number':1397,'multiline':False]
['text':' required mask seq length can be calculated via length of past','line_number':1443,'multiline':False]
['text':' initialize past_key_values with `None` if past does not exist','line_number':1452,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1456,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1457,'multiline':False]
['text':' We use local attention in encoder self-attention, otherwise standard self & cross attentions are used','line_number':1458,'multiline':False]
['text':' we need to use both local attention mask and standard extended mask for transient-global attention','line_number':1465,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1468,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1469,'multiline':False]
['text':' Prepare head mask if needed','line_number':1486,'multiline':False]
['text':' past_key_value is always None with gradient checkpointing','line_number':1516,'multiline':False]
['text':' layer_outputs is a tuple with:','line_number':1535,'multiline':False]
['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1536,'multiline':False]
['text':' We share the position biases between the layers - the first layer store them','line_number':1542,'multiline':False]
['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1543,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':1544,'multiline':False]
['text':' append next layer key value states','line_number':1548,'multiline':False]
['text':' Add last layer','line_number':1560,'multiline':False]
['text':' Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1740,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1775,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1850,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1856,'multiline':False]
['text':' Decode','line_number':1876,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1934,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':2012,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':2018,'multiline':False]
['text':' Convert encoder inputs in embeddings if needed','line_number':2020,'multiline':False]
['text':' get decoder inputs from shifting lm labels to the right','line_number':2040,'multiline':False]
['text':' Decode','line_number':2043,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':2062,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':2063,'multiline':False]
['text':' TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666','line_number':2074,'multiline':False]
['text':' cut decoder_input_ids if past_key_values is used','line_number':2104,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':2108,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':2112,'multiline':False]
['text':' if decoder past is not included in output','line_number':2132,'multiline':False]
['text':' speedy decoding is disabled and no need to reorder','line_number':2133,'multiline':False]
['text':' get the correct batch idx from layer past batch dim','line_number':2140,'multiline':False]
['text':' batch dim of `past` is at 2nd position','line_number':2141,'multiline':False]
['text':' need to set correct `past` for each of the four key / value states','line_number':2144,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2173,'multiline':False]
