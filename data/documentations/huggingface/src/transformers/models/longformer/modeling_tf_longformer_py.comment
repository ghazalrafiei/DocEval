['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The Allen Institute for AI team and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all Longformer models at https://huggingface.co/models?filter=longformer','line_number':64,'multiline':False]
['text':' bool attention mask with True in locations of global attention','line_number':401,'multiline':False]
['text':' last token is separation token and should not be counted and in the middle are two separation tokens','line_number':408,'multiline':False]
['text':' Copied from transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead with Roberta->Longformer','line_number':418,'multiline':False]
['text':' The output weights are the same as the input embeddings, but there is','line_number':433,'multiline':False]
['text':' an output-only bias for each token.','line_number':434,'multiline':False]
['text':' project back to size of vocabulary with bias','line_number':469,'multiline':False]
['text':' Create the position ids from the input token ids. Any padded tokens remain padded.','line_number':566,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertIntermediate with Bert->Longformer','line_number':585,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertOutput with Bert->Longformer','line_number':615,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertPooler with Bert->Longformer','line_number':646,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':660,'multiline':False]
['text':' to the first token.','line_number':661,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_tf_bert.TFBertSelfOutput with Bert->Longformer','line_number':676,'multiline':False]
['text':' separate projection layers for tokens with global attention','line_number':737,'multiline':False]
['text':' retrieve input args','line_number':813,'multiline':False]
['text':' project hidden states','line_number':823,'multiline':False]
['text':' normalize query','line_number':835,'multiline':False]
['text':' attn_probs = (batch_size, seq_len, num_heads, window*2+1)','line_number':840,'multiline':False]
['text':' values to pad for attention probs','line_number':845,'multiline':False]
['text':' cast to fp32/fp16 then replace 1's with -inf','line_number':847,'multiline':False]
['text':' diagonal mask with zeros everywhere and -inf inplace of padding','line_number':850,'multiline':False]
['text':' pad local attention probs','line_number':857,'multiline':False]
['text':' compute global attn indices required through out forward fn','line_number':869,'multiline':False]
['text':' this function is only relevant for global attention','line_number':877,'multiline':False]
['text':' softmax sometimes inserts NaN if all positions are masked, replace them with 0','line_number':891,'multiline':False]
['text':' Make sure to create a mask with the proper shape:','line_number':892,'multiline':False]
['text':' if is_global_attn==True => [batch_size, seq_len, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1]','line_number':893,'multiline':False]
['text':' if is_global_attn==False => [batch_size, seq_len, self.num_heads, self.one_sided_attn_window_size * 2 + 1]','line_number':894,'multiline':False]
['text':' apply dropout','line_number':923,'multiline':False]
['text':' if global attention, compute sum of global and local attn','line_number':927,'multiline':False]
['text':' compute value for global attention and overwrite to attention output','line_number':948,'multiline':False]
['text':' Leave attn_output unchanged','line_number':962,'multiline':False]
['text':' make sure that local attention probabilities are set to 0 for indices of global attn','line_number':965,'multiline':False]
['text':' Make sure to create a mask with the proper shape:','line_number':966,'multiline':False]
['text':' if is_global_attn==True => [batch_size, seq_len, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1]','line_number':967,'multiline':False]
['text':' if is_global_attn==False => [batch_size, seq_len, self.num_heads, self.one_sided_attn_window_size * 2 + 1]','line_number':968,'multiline':False]
['text':' group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size window_overlap * 2','line_number':1013,'multiline':False]
['text':' matrix multiplication','line_number':1022,'multiline':False]
['text':' bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim','line_number':1023,'multiline':False]
['text':' bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim','line_number':1024,'multiline':False]
['text':' bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap','line_number':1025,'multiline':False]
['text':' multiply','line_number':1027,'multiline':False]
['text':' convert diagonals into columns','line_number':1029,'multiline':False]
['text':' allocate space for the overall attention matrix where the chunks are combined. The last dimension','line_number':1033,'multiline':False]
['text':' has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to','line_number':1034,'multiline':False]
['text':' window_overlap previous words). The following column is attention score from each word to itself, then','line_number':1035,'multiline':False]
['text':' followed by window_overlap columns for the upper triangle.','line_number':1036,'multiline':False]
['text':' copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions','line_number':1038,'multiline':False]
['text':' - copying the main diagonal and the upper triangle','line_number':1039,'multiline':False]
['text':' TODO: This code is most likely not very efficient and should be improved','line_number':1040,'multiline':False]
['text':' - copying the lower triangle','line_number':1049,'multiline':False]
['text':' merging upper and lower triangle','line_number':1087,'multiline':False]
['text':' separate batch_size and num_heads dimensions again','line_number':1092,'multiline':False]
['text':' create correct upper triangle bool mask','line_number':1107,'multiline':False]
['text':' pad to full matrix','line_number':1113,'multiline':False]
['text':' create lower mask','line_number':1118,'multiline':False]
['text':' combine with upper mask','line_number':1121,'multiline':False]
['text':' broadcast to full matrix','line_number':1124,'multiline':False]
['text':' inf tensor used for masking','line_number':1127,'multiline':False]
['text':' mask','line_number':1130,'multiline':False]
['text':' group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap','line_number':1159,'multiline':False]
['text':' group batch_size and num_heads dimensions into one','line_number':1170,'multiline':False]
['text':' pad seq_len with w at the beginning of the sequence and another window overlap at the end','line_number':1176,'multiline':False]
['text':' chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap','line_number':1180,'multiline':False]
['text':' padding value is not important because it will be overwritten','line_number':1213,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten','line_number':1256,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlapL+window_overlapwindow_overlap+window_overlap','line_number':1259,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlapL+window_overlapwindow_overlap','line_number':1262,'multiline':False]
['text':' total_num_heads x num_chunks, window_overlap x hidden_dim+window_overlap','line_number':1266,'multiline':False]
['text':' define frame size and frame stride (similar to convolution)','line_number':1277,'multiline':False]
['text':' chunk with overlap','line_number':1282,'multiline':False]
['text':' helper variable','line_number':1304,'multiline':False]
['text':' max number of global attn indices in batch','line_number':1308,'multiline':False]
['text':' indices of global attn','line_number':1311,'multiline':False]
['text':' helper variable','line_number':1314,'multiline':False]
['text':' location of the non-padding values within global attention indices','line_number':1319,'multiline':False]
['text':' location of the padding values within global attention indices','line_number':1322,'multiline':False]
['text':' select global key vectors','line_number':1344,'multiline':False]
['text':' create only global key vectors','line_number':1347,'multiline':False]
['text':' (batch_size, seq_len, num_heads, max_num_global_attn_indices)','line_number':1359,'multiline':False]
['text':' (batch_size, max_num_global_attn_indices, seq_len, num_heads)','line_number':1362,'multiline':False]
['text':' scatter mask','line_number':1370,'multiline':False]
['text':' (batch_size, seq_len, num_heads, max_num_global_attn_indices)','line_number':1377,'multiline':False]
['text':' concat to attn_probs','line_number':1380,'multiline':False]
['text':' (batch_size, seq_len, num_heads, extra attention count + 2*window+1)','line_number':1381,'multiline':False]
['text':' cut local attn probs to global only','line_number':1396,'multiline':False]
['text':' select global value vectors','line_number':1399,'multiline':False]
['text':' create only global value vectors','line_number':1402,'multiline':False]
['text':' compute attn output only global','line_number':1414,'multiline':False]
['text':' reshape attn probs','line_number':1417,'multiline':False]
['text':' compute attn output with global','line_number':1420,'multiline':False]
['text':' prepare global hidden states','line_number':1441,'multiline':False]
['text':' global key, query, value','line_number':1449,'multiline':False]
['text':' normalize','line_number':1454,'multiline':False]
['text':' compute attn scores','line_number':1462,'multiline':False]
['text':' scatter mask','line_number':1486,'multiline':False]
['text':' mask global attn scores','line_number':1494,'multiline':False]
['text':' compute global attn probs','line_number':1502,'multiline':False]
['text':' apply layer head masking','line_number':1505,'multiline':False]
['text':' dropout','line_number':1522,'multiline':False]
['text':' global attn output','line_number':1525,'multiline':False]
['text':' get only non zero global attn output','line_number':1543,'multiline':False]
['text':' overwrite values with global attention','line_number':1553,'multiline':False]
['text':' add attentions if we output them','line_number':1640,'multiline':False]
['text':' bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)','line_number':1703,'multiline':False]
['text':' bzs x num_attn_heads x num_global_attn x seq_len => bzs x num_attn_heads x seq_len x num_global_attn','line_number':1706,'multiline':False]
['text':' Add last layer','line_number':1709,'multiline':False]
['text':' undo padding','line_number':1714,'multiline':False]
['text':' unpad `hidden_states` because the calling function is expecting a length == input_ids.size(1)','line_number':1715,'multiline':False]
['text':' one value per layer','line_number':1756,'multiline':False]
['text':' merge `global_attention_mask` and `attention_mask`','line_number':1834,'multiline':False]
['text':' is index masked or global attention','line_number':1854,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':1859,'multiline':False]
['text':' Sizes are [batch_size, to_seq_length, 1, 1]','line_number':1860,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':1861,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':1862,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':1863,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend locally and 0.0 for','line_number':1867,'multiline':False]
['text':' masked and global attn positions, this operation will create a tensor which is 0.0 for','line_number':1868,'multiline':False]
['text':' positions we want to attend and -10000.0 for masked positions.','line_number':1869,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':1870,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':1871,'multiline':False]
['text':' padding','line_number':1920,'multiline':False]
['text':' pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings','line_number':1937,'multiline':False]
['text':' no attention on the padding tokens','line_number':1946,'multiline':False]
['text':' pad with token_type_id = 0','line_number':1947,'multiline':False]
['text':' longformer self attention expects attention mask to have 0 (no attn), 1 (local attn), 2 (global attn)','line_number':1960,'multiline':False]
['text':' (global_attention_mask + 1) => 1 for local attention, 2 for global attention','line_number':1961,'multiline':False]
['text':' => final attention_mask => 0 for no attention, 1 for local attention 2 for global attention','line_number':1962,'multiline':False]
['text':' simply use `global_attention_mask` as `attention_mask`','line_number':1966,'multiline':False]
['text':' if no `attention_mask` is given','line_number':1967,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':2185,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':2283,'multiline':False]
['text':' set global attention on question tokens','line_number':2349,'multiline':False]
['text':' put global attention on all tokens until `config.sep_token_id` is reached','line_number':2361,'multiline':False]
['text':' take <s> token (equiv. to [CLS])','line_number':2435,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':2462,'multiline':False]
['text':' global attention on cls token','line_number':2512,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':2576,'multiline':False]
['text':' names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model','line_number':2702,'multiline':False]
