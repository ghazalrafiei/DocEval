['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The Allen Institute for AI team and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all Longformer models at https://huggingface.co/models?filter=longformer','line_number':51,'multiline':False]
['text':' size: batch_size x 1','line_number':403,'multiline':False]
['text':' bool attention mask with True in locations of global attention','line_number':404,'multiline':False]
['text':' last token is separation token and should not be counted and in the middle are two separation tokens','line_number':409,'multiline':False]
['text':' The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.','line_number':427,'multiline':False]
['text':' self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load','line_number':443,'multiline':False]
['text':' any TensorFlow checkpoint file','line_number':444,'multiline':False]
['text':' Create the position ids from the input token ids. Any padded tokens remain padded.','line_number':456,'multiline':False]
['text':' separate projection layers for tokens with global attention','line_number':513,'multiline':False]
['text':' project hidden states','line_number':555,'multiline':False]
['text':' normalize query','line_number':565,'multiline':False]
['text':' values to pad for attention probs','line_number':575,'multiline':False]
['text':' cast to fp32/fp16 then replace 1's with -inf','line_number':578,'multiline':False]
['text':' diagonal mask with zeros everywhere and -inf inplace of padding','line_number':582,'multiline':False]
['text':' pad local attention probs','line_number':587,'multiline':False]
['text':' compute local attention probs from global attention keys and contact over window dim','line_number':600,'multiline':False]
['text':' compute global attn indices required through out forward fn','line_number':602,'multiline':False]
['text':' calculate global attn probs from global key','line_number':609,'multiline':False]
['text':' concat to local_attn_probs','line_number':619,'multiline':False]
['text':' (batch_size, seq_len, num_heads, extra attention count + 2*window+1)','line_number':620,'multiline':False]
['text':' free memory','line_number':623,'multiline':False]
['text':' use fp32 for numerical stability','line_number':628,'multiline':False]
['text':' softmax sometimes inserts NaN if all positions are masked, replace them with 0','line_number':636,'multiline':False]
['text':' free memory','line_number':640,'multiline':False]
['text':' apply dropout','line_number':643,'multiline':False]
['text':' compute local attention output with global attention value and add','line_number':648,'multiline':False]
['text':' compute sum of global and local attn','line_number':650,'multiline':False]
['text':' compute local attn only','line_number':659,'multiline':False]
['text':' compute value for global attention and overwrite to attention output','line_number':667,'multiline':False]
['text':' TODO: remove the redundant computation','line_number':668,'multiline':False]
['text':' get only non zero global attn output','line_number':680,'multiline':False]
['text':' overwrite values with global attention','line_number':685,'multiline':False]
['text':' The attention weights for tokens with global attention are','line_number':689,'multiline':False]
['text':' just filler values, they were never used to compute the output.','line_number':690,'multiline':False]
['text':' Fill with 0 now, the correct values are in 'global_attn_probs'.','line_number':691,'multiline':False]
['text':' padding value is not important because it will be overwritten','line_number':706,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten','line_number':748,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap','line_number':751,'multiline':False]
['text':' total_num_heads x num_chunks x window_overlap*window_overlap','line_number':754,'multiline':False]
['text':' non-overlapping chunks of size = 2w','line_number':765,'multiline':False]
['text':' use `as_strided` to make the chunks overlap with an overlap size = window_overlap','line_number':772,'multiline':False]
['text':' When exporting to ONNX, use this separate logic','line_number':780,'multiline':False]
['text':' have to use slow implementation since as_strided, unfold and 2d-tensor indexing aren't supported (yet) in ONNX export','line_number':781,'multiline':False]
['text':' TODO replace this with','line_number':783,'multiline':False]
['text':' > return hidden_states.unfold(dimension=1, size=window_overlap * 2, step=window_overlap).transpose(2, 3)','line_number':784,'multiline':False]
['text':' once `unfold` is supported','line_number':785,'multiline':False]
['text':' the case hidden_states.size(1) == window_overlap * 2 can also simply return hidden_states.unsqueeze(1), but that's control flow','line_number':786,'multiline':False]
['text':' group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size window_overlap * 2','line_number':832,'multiline':False]
['text':' matrix multiplication','line_number':839,'multiline':False]
['text':' bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim','line_number':840,'multiline':False]
['text':' bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim','line_number':841,'multiline':False]
['text':' bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap','line_number':842,'multiline':False]
['text':' multiply','line_number':843,'multiline':False]
['text':' convert diagonals into columns','line_number':845,'multiline':False]
['text':' allocate space for the overall attention matrix where the chunks are combined. The last dimension','line_number':850,'multiline':False]
['text':' has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to','line_number':851,'multiline':False]
['text':' window_overlap previous words). The following column is attention score from each word to itself, then','line_number':852,'multiline':False]
['text':' followed by window_overlap columns for the upper triangle.','line_number':853,'multiline':False]
['text':' copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions','line_number':859,'multiline':False]
['text':' - copying the main diagonal and the upper triangle','line_number':860,'multiline':False]
['text':' - copying the lower triangle','line_number':867,'multiline':False]
['text':' separate batch_size and num_heads dimensions again','line_number':876,'multiline':False]
['text':' group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap','line_number':897,'multiline':False]
['text':' group batch_size and num_heads dimensions into one','line_number':906,'multiline':False]
['text':' pad seq_len with w at the beginning of the sequence and another window overlap at the end','line_number':909,'multiline':False]
['text':' chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap','line_number':912,'multiline':False]
['text':' helper variable','line_number':931,'multiline':False]
['text':' max number of global attn indices in batch','line_number':934,'multiline':False]
['text':' indices of global attn','line_number':937,'multiline':False]
['text':' helper variable','line_number':940,'multiline':False]
['text':' location of the non-padding values within global attention indices','line_number':945,'multiline':False]
['text':' location of the padding values within global attention indices','line_number':948,'multiline':False]
['text':' create only global key vectors','line_number':968,'multiline':False]
['text':' (batch_size, seq_len, num_heads, max_num_global_attn_indices)','line_number':975,'multiline':False]
['text':' need to transpose since ONNX export only supports consecutive indexing: https://pytorch.org/docs/stable/onnx.html#writes-sets','line_number':978,'multiline':False]
['text':' cut local attn probs to global only','line_number':997,'multiline':False]
['text':' get value vectors for global only','line_number':999,'multiline':False]
['text':' use `matmul` because `einsum` crashes sometimes with fp16','line_number':1005,'multiline':False]
['text':' attn = torch.einsum('blhs,bshd->blhd', (selected_attn_probs, selected_v))','line_number':1006,'multiline':False]
['text':' compute attn output only global','line_number':1007,'multiline':False]
['text':' reshape attn probs','line_number':1012,'multiline':False]
['text':' compute attn output with global','line_number':1017,'multiline':False]
['text':' prepare global hidden states','line_number':1035,'multiline':False]
['text':' global key, query, value','line_number':1041,'multiline':False]
['text':' normalize','line_number':1046,'multiline':False]
['text':' reshape','line_number':1049,'multiline':False]
['text':' (batch_size * self.num_heads, max_num_global_attn_indices, head_dim)','line_number':1054,'multiline':False]
['text':' batch_size * self.num_heads, seq_len, head_dim)','line_number':1057,'multiline':False]
['text':' batch_size * self.num_heads, seq_len, head_dim)','line_number':1060,'multiline':False]
['text':' compute attn scores','line_number':1062,'multiline':False]
['text':' need to transpose since ONNX export only supports consecutive indexing: https://pytorch.org/docs/stable/onnx.html#writes-sets','line_number':1077,'multiline':False]
['text':' compute global attn probs','line_number':1091,'multiline':False]
['text':' use fp32 for numerical stability','line_number':1094,'multiline':False]
['text':' apply layer head masking','line_number':1096,'multiline':False]
['text':' global attn output','line_number':1112,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertSelfOutput','line_number':1132,'multiline':False]
['text':' Prune linear layers','line_number':1161,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':1167,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertIntermediate','line_number':1196,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertOutput','line_number':1212,'multiline':False]
['text':' Record `is_global_attn == True` to enable ONNX export','line_number':1290,'multiline':False]
['text':' All local attentions.','line_number':1294,'multiline':False]
['text':' check if head_mask has a correct number of layers specified if desired','line_number':1297,'multiline':False]
['text':' bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)','line_number':1330,'multiline':False]
['text':' bzs x num_attn_heads x num_global_attn x seq_len => bzs x num_attn_heads x seq_len x num_global_attn','line_number':1334,'multiline':False]
['text':' Add last layer','line_number':1337,'multiline':False]
['text':' undo padding if necessary','line_number':1341,'multiline':False]
['text':' unpad `hidden_states` because the calling function is expecting a length == input_ids.size(1)','line_number':1342,'multiline':False]
['text':' Copied from transformers.models.bert.modeling_bert.BertPooler','line_number':1362,'multiline':False]
['text':' We "pool" the model by simply taking the hidden state corresponding','line_number':1370,'multiline':False]
['text':' to the first token.','line_number':1371,'multiline':False]
['text':' Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead with Roberta->Longformer','line_number':1378,'multiline':False]
['text':' project back to size of vocabulary with bias','line_number':1396,'multiline':False]
['text':' To tie those two weights if they get disconnected (on TPU or when the bias is resized)','line_number':1402,'multiline':False]
['text':' For accelerate compatibility and to not break backward compatibility','line_number':1403,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':1424,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':1425,'multiline':False]
['text':' one value per layer','line_number':1548,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1559,'multiline':False]
['text':' padding','line_number':1586,'multiline':False]
['text':' this path should be recorded in the ONNX export, it is fine with padding_len == 0 as well','line_number':1599,'multiline':False]
['text':' pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings','line_number':1608,'multiline':False]
['text':' no attention on the padding tokens','line_number':1621,'multiline':False]
['text':' pad with token_type_id = 0','line_number':1622,'multiline':False]
['text':' longformer self attention expects attention mask to have 0 (no attn), 1 (local attn), 2 (global attn)','line_number':1627,'multiline':False]
['text':' (global_attention_mask + 1) => 1 for local attention, 2 for global attention','line_number':1628,'multiline':False]
['text':' => final attention_mask => 0 for no attention, 1 for local attention 2 for global attention','line_number':1629,'multiline':False]
['text':' simply use `global_attention_mask` as `attention_mask`','line_number':1633,'multiline':False]
['text':' if no `attention_mask` is given','line_number':1634,'multiline':False]
['text':' merge `global_attention_mask` and `attention_mask`','line_number':1715,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1728,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1729,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1772,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1886,'multiline':False]
['text':' global attention on cls token','line_number':1922,'multiline':False]
['text':' take <s> token (equiv. to [CLS])','line_number':1988,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2012,'multiline':False]
['text':' set global attention on question tokens automatically','line_number':2080,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':2105,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':2110,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2150,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2237,'multiline':False]
['text':' set global attention on question tokens','line_number':2271,'multiline':False]
['text':' put global attention on all tokens after `config.sep_token_id`','line_number':2274,'multiline':False]
