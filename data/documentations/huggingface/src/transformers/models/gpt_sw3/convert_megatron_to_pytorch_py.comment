['text':' Copyright 2022 The HuggingFace Inc. team and the AI-Sweden team. All rights reserved.','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]
['text':' limitations under the License.','line_number':13,'multiline':False]
['text':' Format the message.','line_number':26,'multiline':False]
['text':' Print and recurse (if needed).','line_number':33,'multiline':False]
['text':' Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :]','line_number':46,'multiline':False]
['text':' for compatibility with later versions of NVIDIA Megatron-LM.','line_number':47,'multiline':False]
['text':' The inverse operation is performed inside Megatron-LM to read checkpoints:','line_number':48,'multiline':False]
['text':' https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209','line_number':49,'multiline':False]
['text':' If param is the weight tensor of the self-attention block, the returned tensor','line_number':50,'multiline':False]
['text':' will have to be transposed one more time to be read by HuggingFace GPT2.','line_number':51,'multiline':False]
['text':' other versions store [num_heads * num_splits * hidden_size, :]','line_number':53,'multiline':False]
['text':' For LM head, transformers' wants the matrix to weight embeddings.','line_number':110,'multiline':False]
['text':' 1e-5','line_number':128,'multiline':False]
['text':' 0.02','line_number':129,'multiline':False]
['text':' True','line_number':130,'multiline':False]
['text':' This identifies the 6.7B (7B) model which uses a different tokenizer','line_number':134,'multiline':False]
['text':' <|endoftext|>','line_number':136,'multiline':False]
['text':' <|endoftext|>','line_number':137,'multiline':False]
['text':' <unk>','line_number':138,'multiline':False]
['text':' <s>','line_number':140,'multiline':False]
['text':' <|endoftext|>','line_number':141,'multiline':False]
['text':' <pad>','line_number':142,'multiline':False]
['text':' Load the model.','line_number':155,'multiline':False]
['text':' Load the config.','line_number':158,'multiline':False]
['text':' Convert.','line_number':166,'multiline':False]
['text':' Print the structure of converted state dict.','line_number':170,'multiline':False]
['text':' Store the config to file.','line_number':176,'multiline':False]
['text':' Store the state_dict to file.','line_number':180,'multiline':False]
