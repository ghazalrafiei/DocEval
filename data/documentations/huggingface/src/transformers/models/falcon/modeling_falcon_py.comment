['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 the Falcon authors and HuggingFace Inc. team.  All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa','line_number':57,'multiline':False]
['text':' NOTE(Hesslow): Unfortunately we did not fuse matmul and bias during training, this means that there's one additional quantization to bfloat16 between the operations.','line_number':73,'multiline':False]
['text':' In order not to degrade the quality of our HF-port, we keep these characteristics in the final model.','line_number':74,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.rotate_half','line_number':83,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':91,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':120,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Falcon','line_number':133,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':144,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':154,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':160,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Falcon','line_number':170,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':184,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Falcon','line_number':190,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':211,'multiline':False]
['text':' Note: alibi will added to the attention bias that will be applied to the query, key product of attention','line_number':245,'multiline':False]
['text':' => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)','line_number':246,'multiline':False]
['text':' => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)','line_number':247,'multiline':False]
['text':' => the query_length dimension will then be broadcasted correctly','line_number':248,'multiline':False]
['text':' This is more or less identical to T5's relative position bias:','line_number':249,'multiline':False]
['text':' https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527','line_number':250,'multiline':False]
['text':' Copied from transformers.models.bloom.modeling_bloom.dropout_add','line_number':256,'multiline':False]
['text':' Layer-wise attention scaling','line_number':300,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaAttention._init_rope with Llama->Falcon','line_number':316,'multiline':False]
['text':' Copied from transformers.models.bloom.modeling_bloom.BloomAttention._merge_heads','line_number':375,'multiline':False]
['text':' What we want to achieve is:','line_number':386,'multiline':False]
['text':' batch_size * num_heads, seq_length, head_dim -> batch_size, seq_length, num_heads * head_dim','line_number':387,'multiline':False]
['text':' First view to decompose the batch size','line_number':391,'multiline':False]
['text':' batch_size * num_heads, seq_length, head_dim -> batch_size, num_heads, seq_length, head_dim','line_number':392,'multiline':False]
['text':' batch_size, num_heads, seq_length, head_dim -> batch_size, seq_length, num_heads, head_dim','line_number':395,'multiline':False]
['text':' batch_size, seq_length, num_heads, head_dim -> batch_size, seq_length, num_heads * head_dim','line_number':398,'multiline':False]
['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':418,'multiline':False]
['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':420,'multiline':False]
['text':' concatenate along seq_length dimension:','line_number':438,'multiline':False]
['text':'  - key: [batch_size, self.num_heads, kv_length, head_dim]','line_number':439,'multiline':False]
['text':'  - value: [batch_size, self.num_heads, kv_length, head_dim]','line_number':440,'multiline':False]
['text':' SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,','line_number':450,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/112577.','line_number':451,'multiline':False]
['text':' The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case query_length == 1.','line_number':465,'multiline':False]
['text':' It is unclear why neither dropout nor head_mask is applied here (while it is with alibi).','line_number':474,'multiline':False]
['text':' change view to [batch_size, num_heads, q_length, kv_length]','line_number':505,'multiline':False]
['text':' cast attention scores to fp32, compute scaled softmax and cast back to initial dtype - [batch_size, num_heads, q_length, kv_length]','line_number':508,'multiline':False]
['text':' `float16` has a minimum value of -65504.0, whereas `bfloat16` and `float32` have a minimum value of `-3.4e+38`','line_number':510,'multiline':False]
['text':' [batch_size, num_heads, q_length, kv_length]','line_number':517,'multiline':False]
['text':' change view [batch_size, num_heads, q_length, kv_length]','line_number':523,'multiline':False]
['text':' matmul: [batch_size * num_heads, q_length, head_dim]','line_number':526,'multiline':False]
['text':' change view [batch_size, q_length, num_heads * head_dim]','line_number':529,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':547,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':551,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':552,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':553,'multiline':False]
['text':' overwrite attention_mask with padding_mask','line_number':573,'multiline':False]
['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':576,'multiline':False]
['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':578,'multiline':False]
['text':' concatenate along seq_length dimension:','line_number':596,'multiline':False]
['text':'  - key: [batch_size, self.num_heads, kv_length, head_dim]','line_number':597,'multiline':False]
['text':'  - value: [batch_size, self.num_heads, kv_length, head_dim]','line_number':598,'multiline':False]
['text':' TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache','line_number':604,'multiline':False]
['text':' to be able to avoid many of these transpose/reshape/view.','line_number':605,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':615,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':616,'multiline':False]
['text':' cast them back in float16 just to be sure everything works as expected.','line_number':617,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':620,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':648,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':674,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':677,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input','line_number':708,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':730,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':734,'multiline':False]
['text':' FalconAttention originally implemented both a forward with & without SDPA','line_number':766,'multiline':False]
['text':' The layer norm before self-attention','line_number':783,'multiline':False]
['text':' The layer norm before the MLP','line_number':785,'multiline':False]
['text':' Self attention.','line_number':817,'multiline':False]
['text':' MLP.','line_number':843,'multiline':False]
['text':' hidden_states, present, attentions','line_number':856,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':953,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':954,'multiline':False]
['text':' Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa','line_number':966,'multiline':False]
['text':' NOTE: Falcon supported SDPA from PyTorch 2.0. We keep it like that for backward compatibility (automatically use SDPA for torch>=2.0).','line_number':969,'multiline':False]
['text':' Embedding + LN Embedding','line_number':998,'multiline':False]
['text':' Transformer blocks','line_number':1001,'multiline':False]
['text':' Final Layer Norm','line_number':1006,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1011,'multiline':False]
['text':' Compute alibi tensor: check build_alibi_tensor documentation','line_number':1073,'multiline':False]
['text':' 2d mask is passed through the layers','line_number':1097,'multiline':False]
['text':' output_attentions=True can not be supported when using SDPA, and we fall back on','line_number':1100,'multiline':False]
['text':' the manual implementation that requires a 4D causal mask in all cases.','line_number':1101,'multiline':False]
['text':' We don't call _prepare_4d_causal_attention_mask_for_sdpa as we need to mask alibi using the 4D attention_mask untouched.','line_number':1113,'multiline':False]
['text':' We take care to integrate alibi bias in the attention_mask here.','line_number':1118,'multiline':False]
['text':' From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend','line_number':1128,'multiline':False]
['text':' produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213','line_number':1129,'multiline':False]
['text':' PyTorch SDPA does not support head_mask, we fall back on the eager implementation in this case.','line_number':1135,'multiline':False]
['text':' 4d mask is passed through the layers','line_number':1140,'multiline':False]
['text':' Prepare head mask if needed','line_number':1145,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':1146,'multiline':False]
['text':' attention_probs has shape batch_size x num_heads x N x N','line_number':1147,'multiline':False]
['text':' head_mask has shape n_layer x batch x num_heads x N x N','line_number':1148,'multiline':False]
['text':' Add last hidden state','line_number':1186,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1215,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1235,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1239,'multiline':False]
['text':' Note: versions of Falcon with alibi do not use position_ids. It is used with RoPE.','line_number':1244,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':1246,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':1307,'multiline':False]
['text':' Flatten the tokens','line_number':1311,'multiline':False]
['text':' Get a copy of `beam_idx` on all the devices where we need those indices.','line_number':1340,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1376,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1500,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1580,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':1627,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':1632,'multiline':False]
