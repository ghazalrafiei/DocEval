['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 Salesforce and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' See all CTRL models at https://huggingface.co/models?filter=ctrl','line_number':38,'multiline':False]
['text':' create the sinusoidal pattern for the positional encoding','line_number':48,'multiline':False]
['text':' calculate attention','line_number':63,'multiline':False]
['text':' Apply the attention mask','line_number':74,'multiline':False]
['text':' Mask heads if we want to','line_number':79,'multiline':False]
['text':' Prune linear layers','line_number':109,'multiline':False]
['text':' Update hyper params','line_number':115,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':224,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':225,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':336,'multiline':False]
['text':' Attention mask.','line_number':421,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':426,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':427,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':428,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':429,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':430,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':433,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':434,'multiline':False]
['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':435,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':436,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':437,'multiline':False]
['text':' fp16 compatibility','line_number':438,'multiline':False]
['text':' Prepare head mask if needed','line_number':441,'multiline':False]
['text':' inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded','line_number':453,'multiline':False]
['text':' `self.pos_encoding` won't be sent to the correct device along the model, so we do it manually.','line_number':459,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':519,'multiline':False]
['text':' only last tokens for inputs_ids if past is defined in kwargs','line_number':529,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':533,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':537,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':616,'multiline':False]
['text':' Flatten the tokens','line_number':619,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':669,'multiline':False]
