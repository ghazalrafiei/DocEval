['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->MT5','line_number':110,'multiline':False]
['text':' MT5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':121,'multiline':False]
['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':122,'multiline':False]
['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':123,'multiline':False]
['text':' half-precision inputs is done in fp32','line_number':124,'multiline':False]
['text':' convert into half-precision if necessary','line_number':129,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->MT5','line_number':136,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->MT5','line_number':159,'multiline':False]
['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':175,'multiline':False]
['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':176,'multiline':False]
['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':177,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerFF with T5->MT5','line_number':189,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention with T5->MT5','line_number':208,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':222,'multiline':False]
['text':' Prune linear layers','line_number':239,'multiline':False]
['text':' Update hyper params','line_number':244,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':278,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':280,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':284,'multiline':False]
['text':' shape (query_length, key_length)','line_number':303,'multiline':False]
['text':' shape (query_length, key_length)','line_number':305,'multiline':False]
['text':' shape (query_length, key_length, num_heads)','line_number':310,'multiline':False]
['text':' shape (1, num_heads, query_length, key_length)','line_number':311,'multiline':False]
['text':' Input is (batch_size, seq_length, dim)','line_number':329,'multiline':False]
['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':330,'multiline':False]
['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':331,'multiline':False]
['text':' self-attn','line_number':356,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':357,'multiline':False]
['text':' cross-attn','line_number':360,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':361,'multiline':False]
['text':' self-attn','line_number':366,'multiline':False]
['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':367,'multiline':False]
['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':370,'multiline':False]
['text':' the provided `key_value_states` to support prefix tuning','line_number':371,'multiline':False]
['text':' cross-attn','line_number':372,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':373,'multiline':False]
['text':' cross-attn','line_number':376,'multiline':False]
['text':' get query states','line_number':380,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':381,'multiline':False]
['text':' get key/value states','line_number':383,'multiline':False]
['text':' compute scores','line_number':391,'multiline':False]
['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':394,'multiline':False]
['text':' if key and values are already calculated','line_number':406,'multiline':False]
['text':' we want only the last query position bias','line_number':407,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':412,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':424,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':427,'multiline':False]
['text':' Mask heads if we want to','line_number':429,'multiline':False]
['text':' (batch_size, seq_length, dim)','line_number':433,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->MT5','line_number':444,'multiline':False]
['text':' add attentions if we output them','line_number':473,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->MT5','line_number':477,'multiline':False]
['text':' add attentions if we output them','line_number':510,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Block with T5->MT5','line_number':514,'multiline':False]
['text':' Keep self-attention outputs and relative position weights','line_number':568,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':570,'multiline':False]
['text':' the actual query length is unknown for cross attention','line_number':581,'multiline':False]
['text':' if using past key value states. Need to inject it here','line_number':582,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':601,'multiline':False]
['text':' Combine self attn and cross attn key value states','line_number':610,'multiline':False]
['text':' Keep cross-attention outputs and relative position weights','line_number':614,'multiline':False]
['text':' Apply Feed Forward layer','line_number':617,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':620,'multiline':False]
['text':' hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':636,'multiline':False]
['text':' Load weights from TF model','line_number':654,'multiline':False]
['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v','line_number':666,'multiline':False]
['text':' which are not required for using pretrained model','line_number':667,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ClassificationHead with T5->MT5','line_number':745,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel with T5->MT5, t5->mt5','line_number':764,'multiline':False]
['text':' Used for testing weights initialization','line_number':792,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':799,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':800,'multiline':False]
['text':' Mesh TensorFlow FF initialization','line_number':815,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':816,'multiline':False]
['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':817,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':835,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':836,'multiline':False]
['text':' shift inputs to the right','line_number':857,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':859,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':869,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Stack with T5->MT5','line_number':875,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':889,'multiline':False]
['text':' Model parallel','line_number':891,'multiline':False]
['text':' Check validity of device_map','line_number':905,'multiline':False]
['text':' Load onto devices','line_number':913,'multiline':False]
['text':' Set embed_tokens to first layer','line_number':919,'multiline':False]
['text':' Set final layer norm to last device','line_number':921,'multiline':False]
['text':' Model parallel','line_number':961,'multiline':False]
['text':' required mask seq length can be calculated via length of past','line_number':993,'multiline':False]
['text':' initialize past_key_values with `None` if past does not exist','line_number':1000,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1007,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1008,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1011,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1012,'multiline':False]
['text':' Prepare head mask if needed','line_number':1031,'multiline':False]
['text':' Model parallel','line_number':1046,'multiline':False]
['text':' Ensure that attention_mask is always on the same device as hidden_states','line_number':1049,'multiline':False]
['text':' past_key_value is always None with gradient checkpointing','line_number':1078,'multiline':False]
['text':' layer_outputs is a tuple with:','line_number':1097,'multiline':False]
['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1098,'multiline':False]
['text':' We share the position biases between the layers - the first layer store them','line_number':1104,'multiline':False]
['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1105,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':1106,'multiline':False]
['text':' append next layer key value states','line_number':1110,'multiline':False]
['text':' Model Parallel: If it's the last layer for that device, put things on the next device','line_number':1119,'multiline':False]
['text':' Add last layer','line_number':1128,'multiline':False]
['text':' Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1304,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.__init__ with T5->MT5','line_number':1341,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1358,'multiline':False]
['text':' Model parallel','line_number':1361,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.parallelize','line_number':1366,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.deparallelize','line_number':1386,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_input_embeddings','line_number':1400,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.set_input_embeddings','line_number':1404,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_encoder','line_number':1410,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.get_decoder','line_number':1414,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads','line_number':1418,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Model.forward with T5->MT5, t5->mt5','line_number':1429,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1475,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1481,'multiline':False]
['text':' Set device for model parallelism','line_number':1501,'multiline':False]
['text':' Decode','line_number':1512,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.__init__ with T5->MT5','line_number':1566,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1587,'multiline':False]
['text':' Model parallel','line_number':1590,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.parallelize','line_number':1595,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.deparallelize','line_number':1616,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_input_embeddings','line_number':1631,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_input_embeddings','line_number':1635,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_output_embeddings','line_number':1641,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_output_embeddings','line_number':1645,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder','line_number':1649,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_decoder','line_number':1653,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward with T5->MT5, t5->mt5','line_number':1659,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1713,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1719,'multiline':False]
['text':' Convert encoder inputs in embeddings if needed','line_number':1721,'multiline':False]
['text':' get decoder inputs from shifting lm labels to the right','line_number':1744,'multiline':False]
['text':' Set device for model parallelism','line_number':1747,'multiline':False]
['text':' Decode','line_number':1758,'multiline':False]
['text':' Set device for model parallelism','line_number':1776,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':1783,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1784,'multiline':False]
['text':' move labels to correct device to enable PP','line_number':1792,'multiline':False]
['text':' TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666','line_number':1795,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation','line_number':1813,'multiline':False]
['text':' cut decoder_input_ids if past_key_values is used','line_number':1827,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1831,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1835,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels','line_number':1852,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration._reorder_cache','line_number':1856,'multiline':False]
['text':' if decoder past is not included in output','line_number':1858,'multiline':False]
['text':' speedy decoding is disabled and no need to reorder','line_number':1859,'multiline':False]
['text':' get the correct batch idx from layer past batch dim','line_number':1866,'multiline':False]
['text':' batch dim of `past` is at 2nd position','line_number':1867,'multiline':False]
['text':' need to set correct `past` for each of the four key / value states','line_number':1870,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.__init__ with T5->MT5','line_number':1911,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1921,'multiline':False]
['text':' Model parallel','line_number':1924,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.parallelize','line_number':1929,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.deparallelize','line_number':1948,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings','line_number':1960,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.set_input_embeddings','line_number':1964,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder','line_number':1969,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel._prune_heads','line_number':1973,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with T5->MT5, t5->mt5','line_number':1984,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.__init__ with T5->MT5','line_number':2037,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2043,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.forward','line_number':2050,'multiline':False]
['text':' Copied from models.bart.modeling_bart.BartModel.forward different to other models, T5 automatically creates','line_number':2084,'multiline':False]
['text':' decoder_input_ids from input_ids if no decoder_input_ids are provided','line_number':2085,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.__init__ with T5->MT5','line_number':2172,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':2194,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_input_embeddings','line_number':2199,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.set_input_embeddings','line_number':2203,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_encoder','line_number':2209,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_decoder','line_number':2213,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.forward','line_number':2219,'multiline':False]
['text':' Copied from models.bart.modeling_bart.BartModel.forward','line_number':2255,'multiline':False]
['text':'   different to other models, T5 automatically creates decoder_input_ids from','line_number':2256,'multiline':False]
['text':'   input_ids if no decoder_input_ids are provided','line_number':2257,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':2270,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':2276,'multiline':False]
['text':' Decode','line_number':2296,'multiline':False]
['text':' If we are on multi-GPU, split add a dimension','line_number':2321,'multiline':False]
['text':' sometimes the start/end positions are outside our model inputs, we ignore these terms','line_number':2326,'multiline':False]
