['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 Toshiyuki Sakamoto(tanreinama) and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':'###################################################','line_number':43,'multiline':False]
['text':' This dict contains ids and associated url','line_number':44,'multiline':False]
['text':' for the pretrained weights provided with the models','line_number':45,'multiline':False]
['text':'###################################################','line_number':46,'multiline':False]
['text':' See all GPTSAN-japanese models at https://huggingface.co/models?filter=gptsan-japanese','line_number':49,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.router_z_loss_func','line_number':53,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.load_balancing_loss_func','line_number':74,'multiline':False]
['text':' cast the expert indices to int64, otherwise one-hot encoding will fail','line_number':94,'multiline':False]
['text':' For a given token, determine if it was routed to a given expert.','line_number':103,'multiline':False]
['text':' cast to float32 otherwise mean will fail','line_number':106,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersTop1Router with SwitchTransformers->GPTSanJapanese','line_number':148,'multiline':False]
['text':' float32 is used to ensure stability. See the discussion of "selective precision" in','line_number':184,'multiline':False]
['text':' https://arxiv.org/abs/2101.03961.','line_number':185,'multiline':False]
['text':' We also store the previous dtype to cast back the output to the previous dtype','line_number':186,'multiline':False]
['text':' Multiply the token inputs by the uniform distribution - adding some noise','line_number':191,'multiline':False]
['text':' Shape: [num_groups, tokens_per_group, num_experts]','line_number':194,'multiline':False]
['text':' Apply Softmax and cast back to the original `dtype`','line_number':198,'multiline':False]
['text':' Mask tokens outside expert capacity. Sum over each sequence','line_number':232,'multiline':False]
['text':' mask if the token routed to to the expert will overflow','line_number':234,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersSparseMLP with SwitchTransformers->GPTSanJapanese','line_number':242,'multiline':False]
['text':' Step 1: Get the correct router according to its class','line_number':250,'multiline':False]
['text':' Step 2: Get the experts','line_number':253,'multiline':False]
['text':' Step 1: Get the router_mask from the router as wel as the probabilities','line_number':270,'multiline':False]
['text':' The routers introduced might not always map all the tokens, to a router, which means that some hidden states','line_number':274,'multiline':False]
['text':' can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.','line_number':275,'multiline':False]
['text':' Check if it is a sparse layer, if not then it is a dense layer','line_number':335,'multiline':False]
['text':' Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->GPTSanJapanese','line_number':353,'multiline':False]
['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':402,'multiline':False]
['text':' for the decoder','line_number':403,'multiline':False]
['text':' get query proj','line_number':408,'multiline':False]
['text':' get key, value proj','line_number':410,'multiline':False]
['text':' `past_key_value[0].shape[2] == key_value_states.shape[1]`','line_number':411,'multiline':False]
['text':' is checking that the `sequence_length` of the `past_key_value` is the same as','line_number':412,'multiline':False]
['text':' the provided `key_value_states` to support prefix tuning','line_number':413,'multiline':False]
['text':' reuse k,v, cross_attentions','line_number':419,'multiline':False]
['text':' cross_attentions','line_number':423,'multiline':False]
['text':' reuse k, v, self_attention','line_number':427,'multiline':False]
['text':' self_attention','line_number':433,'multiline':False]
['text':' if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.','line_number':438,'multiline':False]
['text':' Further calls to cross_attention layer can then reuse all cross-attention','line_number':439,'multiline':False]
['text':' key/value_states (first "if" case)','line_number':440,'multiline':False]
['text':' if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of','line_number':441,'multiline':False]
['text':' all previous decoder key/value_states. Further calls to uni-directional self-attention','line_number':442,'multiline':False]
['text':' can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)','line_number':443,'multiline':False]
['text':' if encoder bi-directional self-attention `past_key_value` is always `None`','line_number':444,'multiline':False]
['text':' this operation is a bit awkward, but it's required to','line_number':481,'multiline':False]
['text':' make sure that attn_weights keeps its gradient.','line_number':482,'multiline':False]
['text':' In order to do so, attn_weights have to be reshaped','line_number':483,'multiline':False]
['text':' twice and have to be reused in the following','line_number':484,'multiline':False]
['text':' Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be','line_number':503,'multiline':False]
['text':' partitioned across GPUs when using tensor-parallelism.','line_number':504,'multiline':False]
['text':' Self Attention','line_number':570,'multiline':False]
['text':' decoder uni-directional self-attention cached key/values tuple is at positions 1,2','line_number':571,'multiline':False]
['text':' add present self-attn cache to positions 1,2 of present_key_value tuple','line_number':573,'multiline':False]
['text':' hidden, present, (attentions)','line_number':591,'multiline':False]
['text':' hidden, (attentions)','line_number':593,'multiline':False]
['text':' Used for testing weights initialization','line_number':704,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':715,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':716,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':722,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':723,'multiline':False]
['text':' Mesh TensorFlow FF initialization','line_number':728,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':729,'multiline':False]
['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':730,'multiline':False]
['text':' Multi-headed attention','line_number':738,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':747,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':748,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right','line_number':757,'multiline':False]
['text':' shift inputs to the right','line_number':768,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':770,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':780,'multiline':False]
['text':' dummy for input_ids was None','line_number':924,'multiline':False]
['text':' `spout` is a special input vector specific to GPTSAN','line_number':931,'multiline':False]
['text':' This controls the output by projecting embedded information such as the class of sentences during learning.','line_number':932,'multiline':False]
['text':' It should passed instead of the first past_key_value.','line_number':933,'multiline':False]
['text':' See the original GPTSAN repository for details','line_number':934,'multiline':False]
['text':' If there is an attention_mask, increase first one for spout','line_number':937,'multiline':False]
['text':' 1st token should be spout','line_number':940,'multiline':False]
['text':' update attention_mask','line_number':941,'multiline':False]
['text':' `num_precontext` is the number of tokens that refer to each other in prefix-lm','line_number':944,'multiline':False]
['text':' created per batch, so dimension of num_precontext should be [batch, 1]','line_number':945,'multiline':False]
['text':' num_precontext Should be [batch,1]','line_number':948,'multiline':False]
['text':' Make vector from `spout` of GPTSAN to the same shape as past_key_values','line_number':962,'multiline':False]
['text':' projecting `spout` vector','line_number':963,'multiline':False]
['text':' make same shape as past_key_values','line_number':976,'multiline':False]
['text':' Token position considering spout and pasts','line_number':983,'multiline':False]
['text':' positions for get position_embeddings','line_number':989,'multiline':False]
['text':' When padding with padding_side="left", zeros line up on the left side of attention_mask, so position_embeddings is shifted accordingly','line_number':998,'multiline':False]
['text':' attention_mask is applied per batch','line_number':1002,'multiline':False]
['text':' Create a mask to be used when making the prefix Input length of Prefix-LM variable','line_number':1006,'multiline':False]
['text':' Marge prefix_lm_mask and attention_mask','line_number':1016,'multiline':False]
['text':' Prepare head mask if needed','line_number':1019,'multiline':False]
['text':' n_layer x batch x n_heads x N x N','line_number':1023,'multiline':False]
['text':' outputs','line_number':1025,'multiline':False]
['text':' extra_position_embeddings are extra position embeddings that are only created when extending the model with code from the original GPTSAN repository. Not used in the default model.','line_number':1034,'multiline':False]
['text':' However, it is created when you create an additional layer and partially train only that location.','line_number':1035,'multiline':False]
['text':' Therefore, convert_gptsan_tf_checkpoint_to_pytorch.py is used when converting and loading models created in the original GPTSAN repository.','line_number':1036,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':1233,'multiline':False]
['text':' Compute the router loss (z_loss + auxiliary loss) for each router in the encoder and decoder','line_number':1239,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersForConditionalGeneration.prepare_decoder_input_ids_from_labels with SwitchTransformers->GPTSanJapanese','line_number':1302,'multiline':False]
['text':' Copied from transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.resize_token_embeddings with MBart->GPTSanJapanese','line_number':1306,'multiline':False]
['text':' Copied from transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration._resize_final_logits_bias with MBart->GPTSanJapanese','line_number':1312,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersForConditionalGeneration.set_output_embeddings with SwitchTransformers->GPTSanJapanese','line_number':1328,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersForConditionalGeneration.get_output_embeddings with SwitchTransformers->GPTSanJapanese','line_number':1332,'multiline':False]
['text':' Copied from transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersForConditionalGeneration._unpack_router_logits with SwitchTransformers->GPTSanJapanese','line_number':1336,'multiline':False]
