['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 Microsoft and the HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa','line_number':52,'multiline':False]
['text':' See all Phi models at https://huggingface.co/models?filter=phi','line_number':63,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama._get_unpad_data','line_number':67,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Phi','line_number':80,'multiline':False]
['text':' Build here to make `torch.jit.trace` work.','line_number':91,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':101,'multiline':False]
['text':' x: [bs, num_attention_heads, seq_len, head_size]','line_number':107,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Phi','line_number':117,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':131,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Phi','line_number':137,'multiline':False]
['text':' Different from paper, but it uses a different permutation in order to obtain the same calculation','line_number':158,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.rotate_half','line_number':164,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb','line_number':172,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Phi','line_number':201,'multiline':False]
['text':' Copied from transformers.models.persimmon.modeling_persimmon.PersimmonAttention with Persimmon->Phi,persimmon->phi','line_number':217,'multiline':False]
['text':' Copied from transformers.models.bloom.modeling_bloom.BloomAttention._split_heads','line_number':286,'multiline':False]
['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':314,'multiline':False]
['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':317,'multiline':False]
['text':' [batch_size, num_heads, seq_length, head_dim] -> [batch_size, seq_length, num_heads, head_dim]','line_number':324,'multiline':False]
['text':' Partial rotary embedding','line_number':340,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]','line_number':349,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim]','line_number':352,'multiline':False]
['text':' Specific to RoPE models with partial rotation','line_number':357,'multiline':False]
['text':' upcast attention to fp32','line_number':376,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__','line_number':406,'multiline':False]
['text':' TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.','line_number':410,'multiline':False]
['text':' flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.','line_number':411,'multiline':False]
['text':' Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).','line_number':412,'multiline':False]
['text':' PhiFlashAttention2 attention does not support output_attentions','line_number':424,'multiline':False]
['text':' [batch_size, seq_length, 3 x hidden_size]','line_number':430,'multiline':False]
['text':' 3 x [batch_size, seq_length, num_heads, head_dim]','line_number':433,'multiline':False]
['text':' [batch_size, num_heads, seq_length, head_dim] -> [batch_size, seq_length, num_heads, head_dim]','line_number':440,'multiline':False]
['text':' Partial rotary embedding','line_number':450,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]','line_number':459,'multiline':False]
['text':' [batch_size, seq_length, num_heads, head_dim]','line_number':462,'multiline':False]
['text':' Flash attention requires the input to have the shape','line_number':472,'multiline':False]
['text':' batch_size x seq_length x head_dim x hidden_dim','line_number':473,'multiline':False]
['text':' In PEFT, usually we cast the layer norms in float32 for training stability reasons','line_number':480,'multiline':False]
['text':' therefore the input hidden states gets silently casted in float32. Hence, we need','line_number':481,'multiline':False]
['text':' cast them back in the correct dtype just to be sure everything works as expected.','line_number':482,'multiline':False]
['text':' This might slowdown training & inference so it is recommended to not cast the LayerNorms','line_number':483,'multiline':False]
['text':' in fp32.','line_number':484,'multiline':False]
['text':' Handle the case where the model is quantized','line_number':487,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward','line_number':515,'multiline':False]
['text':' TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.','line_number':541,'multiline':False]
['text':' Contains at least one padding token in the sequence','line_number':544,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input','line_number':575,'multiline':False]
['text':' There is a memcpy here, that is very bad.','line_number':597,'multiline':False]
['text':' The -q_len: slice assumes left padding.','line_number':601,'multiline':False]
['text':' Self Attention','line_number':660,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':821,'multiline':False]
['text':' retrieve input_ids and inputs_embeds','line_number':851,'multiline':False]
['text':' Attention mask.','line_number':888,'multiline':False]
['text':' 2d mask is passed through the layers','line_number':890,'multiline':False]
['text':' 4d mask is passed through the layers','line_number':893,'multiline':False]
['text':' decoder layers','line_number':900,'multiline':False]
['text':' add hidden states from the last decoder layer','line_number':938,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi,bias=False->bias=True','line_number':958,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':965,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings','line_number':968,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings','line_number':972,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings','line_number':976,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings','line_number':980,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder','line_number':984,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder','line_number':988,'multiline':False]
['text':' decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)','line_number':1039,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':1058,'multiline':False]
['text':' Flatten the tokens','line_number':1061,'multiline':False]
['text':' Enable model parallelism','line_number':1065,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation','line_number':1081,'multiline':False]
['text':' Keep only the unprocessed tokens:','line_number':1094,'multiline':False]
['text':' 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where','line_number':1095,'multiline':False]
['text':' some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as','line_number':1096,'multiline':False]
['text':' input)','line_number':1097,'multiline':False]
['text':' 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard','line_number':1100,'multiline':False]
['text':' input_ids based on the past_length.','line_number':1101,'multiline':False]
['text':' 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.','line_number':1104,'multiline':False]
['text':' If we are about to go beyond the maximum cache length, we need to crop the input attention mask.','line_number':1106,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':1116,'multiline':False]
['text':' if `inputs_embeds` are passed, we only want to use them in the 1st generation step','line_number':1122,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM._reorder_cache','line_number':1139,'multiline':False]
['text':' Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->PHI,Llama->Phi with self.transformer->self.model, transformer_outputs->model_outputs','line_number':1164,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1172,'multiline':False]
['text':' Copied from transformers.models.mpt.modeling_mpt.MptForTokenClassification with MPT->PHI,Mpt->Phi,self.transformer->self.model,transformer_outputs->model_outputs','line_number':1279,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1295,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':1342,'multiline':False]
