['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Microsoft Research and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all Table Transformer models at https://huggingface.co/models?filter=table-transformer','line_number':60,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrDecoderOutput with DETR->TABLE_TRANSFORMER,Detr->TableTransformer','line_number':65,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrModelOutput with DETR->TABLE_TRANSFORMER,Detr->TableTransformer','line_number':96,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->TableTransformer,DetrImageProcessor->DetrImageProcessor','line_number':137,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->TableTransformer','line_number':200,'multiline':False]
['text':' move reshapes to the beginning','line_number':228,'multiline':False]
['text':' to make it user-friendly','line_number':229,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.replace_batch_norm with Detr->TableTransformer','line_number':240,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrConvEncoder with Detr->TableTransformer','line_number':265,'multiline':False]
['text':' replace batch norm by frozen batch norm','line_number':295,'multiline':False]
['text':' send pixel_values through the model to get list of feature maps','line_number':314,'multiline':False]
['text':' downsample pixel_mask to match shape of corresponding feature_map','line_number':319,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrConvModel with Detr->TableTransformer','line_number':325,'multiline':False]
['text':' send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples','line_number':337,'multiline':False]
['text':' position encoding','line_number':341,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrSinePositionEmbedding with Detr->TableTransformer','line_number':347,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding with Detr->TableTransformer','line_number':385,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.build_position_encoding with Detr->TableTransformer','line_number':409,'multiline':False]
['text':' TODO find a better way of exposing other arguments','line_number':413,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrAttention with DETR->TABLE_TRANSFORMER,Detr->TableTransformer','line_number':423,'multiline':False]
['text':' if key_value_states are provided this layer is used as a cross-attention layer','line_number':517,'multiline':False]
['text':' for the decoder','line_number':518,'multiline':False]
['text':' add position embeddings to the hidden states before projecting to queries and keys','line_number':522,'multiline':False]
['text':' add key-value position embeddings to the key value states','line_number':527,'multiline':False]
['text':' get query proj','line_number':532,'multiline':False]
['text':' get key, value proj','line_number':534,'multiline':False]
['text':' cross_attentions','line_number':536,'multiline':False]
['text':' self_attention','line_number':540,'multiline':False]
['text':' this operation is a bit awkward, but it's required to','line_number':571,'multiline':False]
['text':' make sure that attn_weights keeps its gradient.','line_number':572,'multiline':False]
['text':' In order to do so, attn_weights have to reshaped','line_number':573,'multiline':False]
['text':' twice and have to be reused in the following','line_number':574,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrEncoderLayer.__init__ with Detr->TableTransformer','line_number':600,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrDecoderLayer.__init__ with Detr->TableTransformer','line_number':673,'multiline':False]
['text':' Self Attention','line_number':732,'multiline':False]
['text':' Cross-Attention Block','line_number':746,'multiline':False]
['text':' Fully Connected','line_number':764,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrClassificationHead with Detr->TableTransformer','line_number':779,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':815,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':816,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':905,'multiline':False]
['text':' expand attention_mask','line_number':951,'multiline':False]
['text':' [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]','line_number':953,'multiline':False]
['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':961,'multiline':False]
['text':' skip the layer','line_number':965,'multiline':False]
['text':' we add object_queries as extra input to the encoder_layer','line_number':971,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrDecoder with DETR->TABLE_TRANSFORMER,Detr->TableTransformer','line_number':996,'multiline':False]
['text':' in TABLE_TRANSFORMER, the decoder uses layernorm after the last decoder layer output','line_number':1018,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1022,'multiline':False]
['text':' [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]','line_number':1103,'multiline':False]
['text':' expand encoder attention mask','line_number':1108,'multiline':False]
['text':' [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]','line_number':1110,'multiline':False]
['text':' optional intermediate hidden states','line_number':1115,'multiline':False]
['text':' decoder layers','line_number':1118,'multiline':False]
['text':' add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)','line_number':1124,'multiline':False]
['text':' finally, apply layernorm','line_number':1164,'multiline':False]
['text':' add hidden states from the last decoder layer','line_number':1167,'multiline':False]
['text':' stack intermediate decoder activations','line_number':1171,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrModel.__init__ with Detr->TableTransformer','line_number':1198,'multiline':False]
['text':' Create backbone + positional encoding','line_number':1202,'multiline':False]
['text':' Create projection layer','line_number':1207,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1215,'multiline':False]
['text':' First, sent pixel_values + pixel_mask through Backbone to obtain the features','line_number':1286,'multiline':False]
['text':' pixel_values should be of shape (batch_size, num_channels, height, width)','line_number':1287,'multiline':False]
['text':' pixel_mask should be of shape (batch_size, height, width)','line_number':1288,'multiline':False]
['text':' get final feature map and downsampled mask','line_number':1291,'multiline':False]
['text':' Second, apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)','line_number':1297,'multiline':False]
['text':' Third, flatten the feature map + object queries of shape NxCxHxW to NxCxHW, and permute it to NxHWxC','line_number':1300,'multiline':False]
['text':' In other words, turn their shape into (batch_size, sequence_length, hidden_size)','line_number':1301,'multiline':False]
['text':' Fourth, sent flattened_features + flattened_mask + object queries through encoder','line_number':1307,'multiline':False]
['text':' flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)','line_number':1308,'multiline':False]
['text':' flattened_mask is a Tensor of shape (batch_size, heigth*width)','line_number':1309,'multiline':False]
['text':' If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True','line_number':1319,'multiline':False]
['text':' Fifth, sent query embeddings + object queries through the decoder (which is conditioned on the encoder output)','line_number':1327,'multiline':False]
['text':' decoder outputs consists of (dec_features, dec_hidden, dec_attn)','line_number':1331,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrForObjectDetection.__init__ with Detr->TableTransformer','line_number':1367,'multiline':False]
['text':' DETR encoder-decoder model','line_number':1371,'multiline':False]
['text':' Object detection heads','line_number':1374,'multiline':False]
['text':' We add one for the "no object" class','line_number':1377,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1382,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrForObjectDetection._set_aux_loss','line_number':1386,'multiline':False]
['text':' this is a workaround to make torchscript happy, as torchscript','line_number':1388,'multiline':False]
['text':' doesn't support dictionary with non-homogeneous values, such','line_number':1389,'multiline':False]
['text':' as a dict having both a Tensor and a list.','line_number':1390,'multiline':False]
['text':' First, sent images through TABLE_TRANSFORMER base model to obtain encoder + decoder outputs','line_number':1450,'multiline':False]
['text':' class logits + predicted bounding boxes','line_number':1465,'multiline':False]
['text':' First: create the matcher','line_number':1471,'multiline':False]
['text':' Second: create the criterion','line_number':1475,'multiline':False]
['text':' Third: compute the losses, based on outputs and labels','line_number':1484,'multiline':False]
['text':' Fourth: compute total loss, as a weighted sum of the various losses','line_number':1496,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.dice_loss','line_number':1529,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.sigmoid_focal_loss','line_number':1549,'multiline':False]
['text':' add modulating factor','line_number':1570,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrLoss with Detr->TableTransformer,detr->table_transformer','line_number':1581,'multiline':False]
['text':' removed logging parameter, which was part of the original implementation','line_number':1617,'multiline':False]
['text':' Count the number of predictions that are NOT "no-object" (which is the last class)','line_number':1649,'multiline':False]
['text':' TODO use valid to mask invalid areas due to padding in loss','line_number':1693,'multiline':False]
['text':' upsample predictions to the target size','line_number':1698,'multiline':False]
['text':' permute predictions following indices','line_number':1713,'multiline':False]
['text':' permute targets following indices','line_number':1719,'multiline':False]
['text':' Retrieve the matching between the outputs of the last layer and the targets','line_number':1748,'multiline':False]
['text':' Compute the average number of target boxes across all nodes, for normalization purposes','line_number':1751,'multiline':False]
['text':' (Niels): comment out function below, distributed training to be added','line_number':1754,'multiline':False]
['text':' if is_dist_avail_and_initialized():','line_number':1755,'multiline':False]
['text':'     torch.distributed.all_reduce(num_boxes)','line_number':1756,'multiline':False]
['text':' (Niels) in original implementation, num_boxes is divided by get_world_size()','line_number':1757,'multiline':False]
['text':' Compute all the requested losses','line_number':1760,'multiline':False]
['text':' In case of auxiliary losses, we repeat this process with the output of each intermediate layer.','line_number':1765,'multiline':False]
['text':' Intermediate masks losses are too costly to compute, we ignore them.','line_number':1771,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrMLPPredictionHead with Detr->TableTransformer,detr->table_transformer','line_number':1780,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.DetrHungarianMatcher with Detr->TableTransformer','line_number':1802,'multiline':False]
['text':' We flatten to compute the cost matrices in a batch','line_number':1853,'multiline':False]
['text':' [batch_size * num_queries, num_classes]','line_number':1854,'multiline':False]
['text':' [batch_size * num_queries, 4]','line_number':1855,'multiline':False]
['text':' Also concat the target labels and boxes','line_number':1857,'multiline':False]
['text':' Compute the classification cost. Contrary to the loss, we don't use the NLL,','line_number':1861,'multiline':False]
['text':' but approximate it in 1 - proba[target class].','line_number':1862,'multiline':False]
['text':' The 1 is a constant that doesn't change the matching, it can be ommitted.','line_number':1863,'multiline':False]
['text':' Compute the L1 cost between boxes','line_number':1866,'multiline':False]
['text':' Compute the giou cost between boxes','line_number':1869,'multiline':False]
['text':' Final cost matrix','line_number':1872,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr._upcast','line_number':1881,'multiline':False]
['text':' Protects from numerical overflows in multiplications by upcasting to the equivalent higher type','line_number':1883,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.box_area','line_number':1890,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.box_iou','line_number':1907,'multiline':False]
['text':' [N,M,2]','line_number':1912,'multiline':False]
['text':' [N,M,2]','line_number':1913,'multiline':False]
['text':' [N,M,2]','line_number':1915,'multiline':False]
['text':' [N,M]','line_number':1916,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.generalized_box_iou','line_number':1924,'multiline':False]
['text':' degenerate boxes gives inf / nan results','line_number':1932,'multiline':False]
['text':' so do an early check','line_number':1933,'multiline':False]
['text':' [N,M,2]','line_number':1943,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr._max_by_axis','line_number':1949,'multiline':False]
['text':' type: (List[List[int]]) -> List[int]','line_number':1951,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.NestedTensor','line_number':1959,'multiline':False]
['text':' Copied from transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list','line_number':1981,'multiline':False]
