['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' This try... catch... is not beautiful but honestly this tokenizer was not made to be used','line_number':210,'multiline':False]
['text':' in a library like ours, at all.','line_number':211,'multiline':False]
['text':' Priority on pickle files (support PyTorch and TF)','line_number':215,'multiline':False]
['text':' Loading a torch-saved transfo-xl vocab dict with pickle results in an integer','line_number':227,'multiline':False]
['text':' Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.','line_number':228,'multiline':False]
['text':' We therefore load it with torch, if it's available.','line_number':229,'multiline':False]
['text':' these are not required to initialize the parent class as only used when tokenizing.','line_number':272,'multiline':False]
['text':' Insert sym into vocab','line_number':417,'multiline':False]
['text':' Shift following indices in sym2idx','line_number':421,'multiline':False]
['text':' Delete token from added_tokens','line_number':426,'multiline':False]
['text':' logger.info(f'encounter unk {sym}')','line_number':473,'multiline':False]
['text':' assert '<eos>' not in sym','line_number':474,'multiline':False]
['text':' Backward compatibility with pre-trained models','line_number':477,'multiline':False]
['text':' convert to lower case','line_number':508,'multiline':False]
['text':' empty delimiter '' will evaluate False','line_number':512,'multiline':False]
['text':' lm1b','line_number':518,'multiline':False]
['text':' Work out how cleanly we can divide the dataset into bsz parts.','line_number':537,'multiline':False]
['text':' Trim off any extra elements that wouldn't cleanly fit (remainders).','line_number':540,'multiline':False]
['text':' Evenly divide the data across the bsz batches.','line_number':543,'multiline':False]
['text':' Number of mini-batches','line_number':546,'multiline':False]
['text':' index iterator','line_number':600,'multiline':False]
['text':' sentence iterator','line_number':603,'multiline':False]
['text':' streams for each data in the batch','line_number':609,'multiline':False]
['text':' data   : [n_retain+bptt x bsz]','line_number':618,'multiline':False]
['text':' target : [bptt x bsz]','line_number':619,'multiline':False]
['text':' number of new tokens to fill in','line_number':631,'multiline':False]
['text':' first n_retain tokens are retained from last batch','line_number':633,'multiline':False]
['text':' sent_stream is an iterator','line_number':656,'multiline':False]
['text':' sent_stream is an iterator','line_number':688,'multiline':False]
['text':' redirect to the cache, if necessary','line_number':703,'multiline':False]
['text':' Instantiate tokenizer.','line_number':718,'multiline':False]
['text':' the vocab will load from file when build_vocab() is called','line_number':756,'multiline':False]
