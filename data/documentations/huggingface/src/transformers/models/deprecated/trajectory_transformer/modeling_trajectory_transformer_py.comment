['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The Trajectory Transformers paper authors and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all TrajectoryTransformer models at https://huggingface.co/models?filter=trajectory_transformer','line_number':46,'multiline':False]
['text':' Load weights from TF model','line_number':65,'multiline':False]
['text':' adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v','line_number':77,'multiline':False]
['text':' which are not required for using pretrained model','line_number':78,'multiline':False]
['text':' [ batch_size x n_models x output_dim ]','line_number':251,'multiline':False]
['text':' key, query, value projections for all heads','line_number':265,'multiline':False]
['text':' regularization','line_number':270,'multiline':False]
['text':' output projection','line_number':274,'multiline':False]
['text':' causal mask to ensure that attention is only applied to the left in the input sequence','line_number':277,'multiline':False]
['text':' mask previous value estimates','line_number':286,'multiline':False]
['text':' calculate query, key, values for all heads in batch and move head forward to be the batch dim','line_number':301,'multiline':False]
['text':' [ batch_size x n_heads x sequence_length x head_dim ]','line_number':302,'multiline':False]
['text':' causal self-attention','line_number':329,'multiline':False]
['text':' [ batch_size x n_heads x sequence_length x sequence_length ]','line_number':330,'multiline':False]
['text':' [ batch_size x sequence_length x embedding_dim ]','line_number':340,'multiline':False]
['text':' re-assemble all head outputs side by side','line_number':341,'multiline':False]
['text':' output projection','line_number':344,'multiline':False]
['text':' MLP','line_number':361,'multiline':False]
['text':' input embedding stem (+1 for stop token)','line_number':409,'multiline':False]
['text':' transformer','line_number':414,'multiline':False]
['text':' decoder head','line_number':416,'multiline':False]
['text':' [ batch_size x padded_sequence_length' x embedding_dim ]','line_number':458,'multiline':False]
['text':' [ batch_size x sequence_length x embedding_dim ]','line_number':526,'multiline':False]
['text':' forward the GPT model','line_number':527,'multiline':False]
['text':' each index maps to a (learnable) vector','line_number':528,'multiline':False]
['text':' each position maps to a (learnable) vector','line_number':529,'multiline':False]
['text':' [ batch_size x sequence_length x embedding_dim ]','line_number':566,'multiline':False]
['text':' if we are given some desired targets also calculate the loss','line_number':578,'multiline':False]
['text':' make weights','line_number':582,'multiline':False]
