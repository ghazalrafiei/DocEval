['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The Salesforce Team Authors and The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all BLIP models at https://huggingface.co/models?filter=blip','line_number':53,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.contrastive_loss','line_number':57,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.clip_loss with clip->blip','line_number':62,'multiline':False]
['text':' shape = [*, width, grid, grid]','line_number':249,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPTextEmbeddings with CLIP->Blip','line_number':258,'multiline':False]
['text':' position_ids (1, len position emb) is contiguous in memory and exported when serialized','line_number':267,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':333,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':338,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':341,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':342,'multiline':False]
['text':' Mask heads if we want to','line_number':345,'multiline':False]
['text':' Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Blip','line_number':361,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':748,'multiline':False]
['text':' pooled_output','line_number':821,'multiline':False]
['text':' Use BLIP model's config for some fields (if specified) instead of those of vision & text components.','line_number':863,'multiline':False]
['text':' normalized features','line_number':892,'multiline':False]
['text':' cosine similarity as logits','line_number':896,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':944,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1117,'multiline':False]
['text':' labels are already shifted right, see: https://github.com/huggingface/transformers/pull/23153','line_number':1203,'multiline':False]
['text':' vision projection layer','line_number':1333,'multiline':False]
['text':' text projection layer','line_number':1336,'multiline':False]
['text':' image text matching head','line_number':1339,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1353,'multiline':False]
