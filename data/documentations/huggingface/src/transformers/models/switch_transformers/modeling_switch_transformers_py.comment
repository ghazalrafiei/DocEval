['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 SwitchTransformers Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':'###################################################','line_number':53,'multiline':False]
['text':' This dict contains ids and associated url','line_number':54,'multiline':False]
['text':' for the pretrained weights provided with the models','line_number':55,'multiline':False]
['text':'###################################################','line_number':56,'multiline':False]
['text':' See all SwitchTransformers models at https://huggingface.co/models?filter=switch_transformers','line_number':67,'multiline':False]
['text':' cast the expert indices to int64, otherwise one-hot encoding will fail','line_number':110,'multiline':False]
['text':' For a given token, determine if it was routed to a given expert.','line_number':119,'multiline':False]
['text':' cast to float32 otherwise mean will fail','line_number':122,'multiline':False]
['text':' float32 is used to ensure stability. See the discussion of "selective precision" in','line_number':165,'multiline':False]
['text':' https://arxiv.org/abs/2101.03961.','line_number':166,'multiline':False]
['text':' We also store the previous dtype to cast back the output to the previous dtype','line_number':167,'multiline':False]
['text':' Multiply the token inputs by the uniform distribution - adding some noise','line_number':172,'multiline':False]
['text':' Shape: [num_groups, tokens_per_group, num_experts]','line_number':175,'multiline':False]
['text':' Apply Softmax and cast back to the original `dtype`','line_number':179,'multiline':False]
['text':' Mask tokens outside expert capacity. Sum over each sequence','line_number':213,'multiline':False]
['text':' mask if the token routed to to the expert will overflow','line_number':215,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerNorm with T5->SwitchTransformers','line_number':223,'multiline':False]
['text':' SwitchTransformers uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':234,'multiline':False]
['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':235,'multiline':False]
['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':236,'multiline':False]
['text':' half-precision inputs is done in fp32','line_number':237,'multiline':False]
['text':' convert into half-precision if necessary','line_number':242,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->SwitchTransformers','line_number':252,'multiline':False]
['text':' Step 1: Get the correct router according to its class','line_number':282,'multiline':False]
['text':' Step 2: Get the experts','line_number':285,'multiline':False]
['text':' Step 1: Get the router_mask from the router as wel as the probabilities','line_number':302,'multiline':False]
['text':' The routers introduced might not always map all the tokens, to a router, which means that some hidden states','line_number':306,'multiline':False]
['text':' can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.','line_number':307,'multiline':False]
['text':' Check if it is a sparse layer, if not then it is a dense layer','line_number':334,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention with T5->SwitchTransformers','line_number':360,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':374,'multiline':False]
['text':' Prune linear layers','line_number':391,'multiline':False]
['text':' Update hyper params','line_number':396,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':430,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':432,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':436,'multiline':False]
['text':' shape (query_length, key_length)','line_number':455,'multiline':False]
['text':' shape (query_length, key_length)','line_number':457,'multiline':False]
['text':' shape (query_length, key_length, num_heads)','line_number':462,'multiline':False]
['text':' shape (1, num_heads, query_length, key_length)','line_number':463,'multiline':False]
['text':' Input is (batch_size, seq_length, dim)','line_number':481,'multiline':False]
['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':482,'multiline':False]
['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':483,'multiline':False]
['text':' self-attn','line_number':508,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':509,'multiline':False]
['text':' cross-attn','line_number':512,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':513,'multiline':False]
['text':' self-attn','line_number':518,'multiline':False]
['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':519,'multiline':False]
['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':522,'multiline':False]
['text':' the provided `key_value_states` to support prefix tuning','line_number':523,'multiline':False]
['text':' cross-attn','line_number':524,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':525,'multiline':False]
['text':' cross-attn','line_number':528,'multiline':False]
['text':' get query states','line_number':532,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':533,'multiline':False]
['text':' get key/value states','line_number':535,'multiline':False]
['text':' compute scores','line_number':543,'multiline':False]
['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':546,'multiline':False]
['text':' if key and values are already calculated','line_number':558,'multiline':False]
['text':' we want only the last query position bias','line_number':559,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':564,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':576,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':579,'multiline':False]
['text':' Mask heads if we want to','line_number':581,'multiline':False]
['text':' (batch_size, seq_length, dim)','line_number':585,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->SwitchTransformers','line_number':596,'multiline':False]
['text':' add attentions if we output them','line_number':627,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->SwitchTransformers','line_number':631,'multiline':False]
['text':' add attentions if we output them','line_number':664,'multiline':False]
['text':' Keep self-attention outputs and relative position weights','line_number':725,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':727,'multiline':False]
['text':' the actual query length is unknown for cross attention','line_number':734,'multiline':False]
['text':' if using past key value states. Need to inject it here','line_number':735,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':754,'multiline':False]
['text':' Combine self attn and cross attn key value states','line_number':759,'multiline':False]
['text':' Keep cross-attention outputs and relative position weights','line_number':763,'multiline':False]
['text':' Apply Feed Forward layer','line_number':766,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':774,'multiline':False]
['text':' hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights), (router_tuple)','line_number':786,'multiline':False]
['text':' Used for testing weights initialization','line_number':813,'multiline':False]
['text':' Mesh TensorFlow embeddings initialization','line_number':820,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624','line_number':821,'multiline':False]
['text':' Mesh TensorFlow FF initialization','line_number':826,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56','line_number':827,'multiline':False]
['text':' and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89','line_number':828,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':836,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':837,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':848,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':849,'multiline':False]
['text':' shift inputs to the right','line_number':868,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':870,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':880,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':910,'multiline':False]
['text':' required mask seq length can be calculated via length of past','line_number':966,'multiline':False]
['text':' initialize past_key_values with `None` if past does not exist','line_number':981,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':985,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':986,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':989,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':990,'multiline':False]
['text':' Prepare head mask if needed','line_number':1007,'multiline':False]
['text':' past_key_value is always None with gradient checkpointing','line_number':1038,'multiline':False]
['text':' layer_outputs is a tuple with:','line_number':1061,'multiline':False]
['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1062,'multiline':False]
['text':' We share the position biases between the layers - the first layer store them','line_number':1068,'multiline':False]
['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1069,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':1070,'multiline':False]
['text':' append next layer key value states','line_number':1074,'multiline':False]
['text':' Add last layer','line_number':1089,'multiline':False]
['text':' Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1278,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1309,'multiline':False]
['text':' Model parallel','line_number':1312,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1390,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1405,'multiline':False]
['text':' Decode','line_number':1427,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1490,'multiline':False]
['text':' Model parallel','line_number':1493,'multiline':False]
['text':' FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask','line_number':1577,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1583,'multiline':False]
['text':' Convert encoder inputs in embeddings if needed','line_number':1585,'multiline':False]
['text':' get decoder inputs from shifting lm labels to the right','line_number':1607,'multiline':False]
['text':' Decode','line_number':1610,'multiline':False]
['text':' Rescale output before projecting on vocab','line_number':1630,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586','line_number':1631,'multiline':False]
['text':' Compute the router loss (z_loss + auxiliary loss) for each router in the encoder and decoder','line_number':1643,'multiline':False]
['text':' move labels to correct device to enable PP','line_number':1664,'multiline':False]
['text':' cut decoder_input_ids if past_key_values is used','line_number':1721,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1725,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1729,'multiline':False]
['text':' if decoder past is not included in output','line_number':1749,'multiline':False]
['text':' speedy decoding is disabled and no need to reorder','line_number':1750,'multiline':False]
['text':' get the correct batch idx from layer past batch dim','line_number':1757,'multiline':False]
['text':' batch dim of `past` is at 2nd position','line_number':1758,'multiline':False]
['text':' need to set correct `past` for each of the four key / value states','line_number':1761,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1798,'multiline':False]
['text':' Model parallel','line_number':1801,'multiline':False]
