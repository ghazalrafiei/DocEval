['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. & Google team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':48,'multiline':False]
['text':' See all Pix2StructVision models at https://huggingface.co/models?filter=pix2struct','line_number':70,'multiline':False]
['text':' Adapted from transformers.models.t5.modeling_t5.T5LayerNorm with T5->Pix2Struct','line_number':74,'multiline':False]
['text':' T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean','line_number':85,'multiline':False]
['text':' Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated','line_number':86,'multiline':False]
['text':' w/o mean and there is no bias. Additionally we want to make sure that the accumulation for','line_number':87,'multiline':False]
['text':' half-precision inputs is done in fp32','line_number':88,'multiline':False]
['text':' convert into half-precision if necessary','line_number':93,'multiline':False]
['text':' noqa','line_number':103,'multiline':False]
['text':' using the normal Pix2StructLayerNorm','line_number':107,'multiline':False]
['text':' the row and column indices are stored in the first and second position of the flattened_patches','line_number':133,'multiline':False]
['text':' flattened_patches: `batch_size`, `seq_len`, `hidden_size` + 2','line_number':134,'multiline':False]
['text':' sum all embeddings together','line_number':144,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':161,'multiline':False]
['text':' Input is (batch_size, seq_length, dim)','line_number':180,'multiline':False]
['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':181,'multiline':False]
['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':182,'multiline':False]
['text':' get query states','line_number':189,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':190,'multiline':False]
['text':' get key/value states','line_number':193,'multiline':False]
['text':' compute scores','line_number':197,'multiline':False]
['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':198,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':214,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':222,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':225,'multiline':False]
['text':' Mask heads if we want to','line_number':228,'multiline':False]
['text':' (batch_size, seq_length, dim)','line_number':234,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5DenseGatedActDense->Pix2StructVisionMlp,T5Config->Pix2StructVisionConfig,config.d_model->config.hidden_size,dropout_rate->dropout_rate','line_number':246,'multiline':False]
['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':262,'multiline':False]
['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':263,'multiline':False]
['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':264,'multiline':False]
['text':' in Pix2StructVision, layernorm is applied before self-attention','line_number':295,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':305,'multiline':False]
['text':' first residual connection','line_number':307,'multiline':False]
['text':' in Pix2StructVision, layernorm is also applied after self-attention','line_number':310,'multiline':False]
['text':' second residual connection','line_number':312,'multiline':False]
['text':' Used for testing weights initialization','line_number':393,'multiline':False]
['text':' Mesh TensorFlow attention initialization to avoid scaling before softmax','line_number':414,'multiline':False]
['text':' See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136','line_number':415,'multiline':False]
['text':' Upcast the input in `fp32` and cast it back to desired `dtype` to avoid','line_number':455,'multiline':False]
['text':' `trunc_normal_cpu` not implemented in `half` issues','line_number':456,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->Pix2Struct','line_number':470,'multiline':False]
['text':' shift inputs to the right','line_number':481,'multiline':False]
['text':' Item assignment is not supported natively for proxies.','line_number':483,'multiline':False]
['text':' replace possible -100 values in labels by `pad_token_id`','line_number':493,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':556,'multiline':False]
['text':' check where `flattened_patches` is not 0','line_number':616,'multiline':False]
['text':' Prepare head mask if needed','line_number':619,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':620,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':621,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':622,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':623,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5DenseGatedActDense with T5->Pix2StructText,d_model->hidden_size','line_number':650,'multiline':False]
['text':' To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.','line_number':666,'multiline':False]
['text':' See https://github.com/huggingface/transformers/issues/20287','line_number':667,'multiline':False]
['text':' we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``','line_number':668,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerFF.forward','line_number':688,'multiline':False]
['text':' Mesh TensorFlow initialization to avoid scaling before softmax','line_number':708,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket','line_number':720,'multiline':False]
['text':' now relative_position is in the range [0, inf)','line_number':749,'multiline':False]
['text':' half of the buckets are for exact increments in positions','line_number':751,'multiline':False]
['text':' The other half of the buckets are for logarithmically bigger bins in positions up to max_distance','line_number':755,'multiline':False]
['text':' Adapted from transformers.models.t5.modeling_t5.T5Attention.compute_bias','line_number':768,'multiline':False]
['text':' shape (query_length, key_length)','line_number':775,'multiline':False]
['text':' shape (query_length, key_length)','line_number':777,'multiline':False]
['text':' shape (query_length, key_length, num_heads)','line_number':782,'multiline':False]
['text':' shape (1, num_heads, query_length, key_length)','line_number':783,'multiline':False]
['text':' Input is (batch_size, seq_length, dim)','line_number':801,'multiline':False]
['text':' Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)','line_number':802,'multiline':False]
['text':' past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)','line_number':803,'multiline':False]
['text':' self-attn','line_number':824,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':825,'multiline':False]
['text':' cross-attn','line_number':828,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':829,'multiline':False]
['text':' self-attn','line_number':834,'multiline':False]
['text':' (batch_size, n_heads, key_length, dim_per_head)','line_number':835,'multiline':False]
['text':' checking that the `sequence_length` of the `past_key_value` is the same as','line_number':838,'multiline':False]
['text':' the provided `key_value_states` to support prefix tuning','line_number':839,'multiline':False]
['text':' cross-attn','line_number':840,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':841,'multiline':False]
['text':' cross-attn','line_number':844,'multiline':False]
['text':' get query states','line_number':848,'multiline':False]
['text':' (batch_size, n_heads, seq_length, dim_per_head)','line_number':849,'multiline':False]
['text':' get key/value states','line_number':852,'multiline':False]
['text':' compute scores','line_number':860,'multiline':False]
['text':' equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9','line_number':863,'multiline':False]
['text':' if key and values are already calculated','line_number':875,'multiline':False]
['text':' we want only the last query position bias','line_number':876,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':881,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':891,'multiline':False]
['text':' (batch_size, n_heads, seq_length, key_length)','line_number':894,'multiline':False]
['text':' Mask heads if we want to','line_number':897,'multiline':False]
['text':' (batch_size, seq_length, dim)','line_number':902,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,self.SelfAttention->self.attention,config.d_model->config.hidden_size','line_number':915,'multiline':False]
['text':' add attentions if we output them','line_number':944,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,self.EncDecAttention->self.attention,config.d_model->config.hidden_size','line_number':948,'multiline':False]
['text':' add attentions if we output them','line_number':981,'multiline':False]
['text':' Keep self-attention outputs and relative position weights','line_number':1037,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':1039,'multiline':False]
['text':' the actual query length is unknown for cross attention','line_number':1046,'multiline':False]
['text':' if using past key value states. Need to inject it here','line_number':1047,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':1066,'multiline':False]
['text':' Combine self attn and cross attn key value states','line_number':1071,'multiline':False]
['text':' Keep cross-attention outputs and relative position weights','line_number':1075,'multiline':False]
['text':' Apply Feed Forward layer','line_number':1078,'multiline':False]
['text':' clamp inf values to enable fp16 training','line_number':1081,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1324,'multiline':False]
['text':' Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._reorder_cache','line_number':1328,'multiline':False]
['text':' if decoder past is not included in output','line_number':1330,'multiline':False]
['text':' speedy decoding is disabled and no need to reorder','line_number':1331,'multiline':False]
['text':' get the correct batch idx from layer past batch dim','line_number':1338,'multiline':False]
['text':' batch dim of `past` is at 2nd position','line_number':1339,'multiline':False]
['text':' need to set correct `past` for each of the four key / value states','line_number':1342,'multiline':False]
['text':' required mask seq length can be calculated via length of past','line_number':1429,'multiline':False]
['text':' initialize past_key_values with `None` if past does not exist','line_number':1440,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':1444,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':1445,'multiline':False]
['text':' If a 2D or 3D attention mask is provided for the cross-attention','line_number':1448,'multiline':False]
['text':' we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':1449,'multiline':False]
['text':' Prepare head mask if needed','line_number':1459,'multiline':False]
['text':' past_key_value is always None with gradient checkpointing','line_number':1493,'multiline':False]
['text':' layer_outputs is a tuple with:','line_number':1512,'multiline':False]
['text':' hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)','line_number':1513,'multiline':False]
['text':' We share the position biases between the layers - the first layer store them','line_number':1519,'multiline':False]
['text':' layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),','line_number':1520,'multiline':False]
['text':' (cross-attention position bias), (cross-attention weights)','line_number':1521,'multiline':False]
['text':' append next layer key value states','line_number':1525,'multiline':False]
['text':' Add last layer','line_number':1539,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':1545,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1591,'multiline':False]
['text':' update vocab size','line_number':1609,'multiline':False]
['text':' Encode if needed (training, first prediction pass)','line_number':1702,'multiline':False]
['text':' get decoder inputs from shifting lm labels to the right','line_number':1722,'multiline':False]
['text':' Always attend to the first token','line_number':1729,'multiline':False]
['text':' Decode','line_number':1732,'multiline':False]
['text':' cut decoder_input_ids if past_key_values is used','line_number':1781,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':1785,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':1789,'multiline':False]
