['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Salesforce authors, The EleutherAI, and HuggingFace Teams. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' See all CodeGen models at https://huggingface.co/models?filter=codegen','line_number':50,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_gptj.create_sinusoidal_positions','line_number':54,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_gptj.rotate_every_two','line_number':61,'multiline':False]
['text':' in einsum notation: rearrange(x, '... d j -> ... (d j)')','line_number':66,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_gptj.apply_rotary_pos_emb','line_number':69,'multiline':False]
['text':' compute causal mask from causal mask buffer','line_number':134,'multiline':False]
['text':' Keep the attention weights computation in fp32 to avoid overflow issues','line_number':138,'multiline':False]
['text':' Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.','line_number':146,'multiline':False]
['text':' Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`','line_number':147,'multiline':False]
['text':' Apply the attention mask','line_number':152,'multiline':False]
['text':' Mask heads if we want to','line_number':159,'multiline':False]
['text':' TODO(enijkamp): factor out number of logical TPU-v4 cores or make forward pass agnostic','line_number':181,'multiline':False]
['text':' Note that this cast is quite ugly, but is not implemented before ROPE as k_rot in the original codebase is always in fp32.','line_number':227,'multiline':False]
['text':' Reference: https://github.com/salesforce/CodeGen/blob/f210c3bb1216c975ad858cd4132c0fdeabf4bfc2/codegen1/jaxformer/hf/codegen/modeling_codegen.py#L38','line_number':228,'multiline':False]
['text':' compute self-attention: V x Softmax(QK^T)','line_number':233,'multiline':False]
['text':' a, present, (attentions)','line_number':244,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_gptj.GPTJMLP with GPTJ->CodeGen','line_number':247,'multiline':False]
['text':' in MLP: intermediate_size= 4 * embed_dim','line_number':249,'multiline':False]
['text':' Copied from transformers.models.gptj.modeling_gptj.GPTJBlock with GPTJ->CodeGen','line_number':267,'multiline':False]
['text':' output_attn: a, present, (attentions)','line_number':297,'multiline':False]
['text':' hidden_states, present, (attentions)','line_number':308,'multiline':False]
['text':' Slightly different from Mesh Transformer JAX which uses truncated_normal for initialization','line_number':329,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':330,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':422,'multiline':False]
['text':' Attention mask.','line_number':486,'multiline':False]
['text':' We create a 3D attention mask from a 2D tensor mask.','line_number':491,'multiline':False]
['text':' Sizes are [batch_size, 1, 1, to_seq_length]','line_number':492,'multiline':False]
['text':' So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]','line_number':493,'multiline':False]
['text':' this attention mask is more simple than the triangular masking of causal attention','line_number':494,'multiline':False]
['text':' used in OpenAI GPT, we just need to prepare the broadcast dimension here.','line_number':495,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':498,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':499,'multiline':False]
['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':500,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':501,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':502,'multiline':False]
['text':' fp16 compatibility','line_number':503,'multiline':False]
['text':' Prepare head mask if needed','line_number':506,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':507,'multiline':False]
['text':' attention_probs has shape bsz x num_attention_heads x N x N','line_number':508,'multiline':False]
['text':' head_mask has shape n_layer x batch x num_attention_heads x N x N','line_number':509,'multiline':False]
['text':' Add last hidden state','line_number':572,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':601,'multiline':False]
['text':' Omit tokens covered by past_key_values','line_number':612,'multiline':False]
['text':' Some generation methods already pass only the last input ID','line_number':616,'multiline':False]
['text':' Default to old behavior: keep only final ID','line_number':620,'multiline':False]
['text':' create position_ids on the fly for batch generation','line_number':631,'multiline':False]
['text':' make sure sampling in fp16 works correctly and','line_number':690,'multiline':False]
['text':' compute loss in fp32 to match with mesh-tf version','line_number':691,'multiline':False]
['text':' https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179','line_number':692,'multiline':False]
['text':' move labels to correct device to enable model parallelism','line_number':697,'multiline':False]
['text':' Shift so that tokens < n predict n','line_number':699,'multiline':False]
['text':' Flatten the tokens','line_number':702,'multiline':False]
