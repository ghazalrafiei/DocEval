['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 Microsoft Research and The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' General docstring','line_number':52,'multiline':False]
['text':' Base docstring','line_number':55,'multiline':False]
['text':' Image classification docstring','line_number':59,'multiline':False]
['text':' See all BEiT models at https://huggingface.co/models?filter=beit','line_number':65,'multiline':False]
['text':' work with diff dim tensors, not just 2D ConvNets','line_number':108,'multiline':False]
['text':' binarize','line_number':110,'multiline':False]
['text':' Based on timm implementation, which can be found here:','line_number':129,'multiline':False]
['text':' https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py','line_number':130,'multiline':False]
['text':' replace the masked visual tokens by mask_tokens','line_number':161,'multiline':False]
['text':' interpolate the position embedding to the corresponding size','line_number':211,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':267,'multiline':False]
['text':' Add relative position bias if present.','line_number':272,'multiline':False]
['text':' Add shared relative position bias if provided.','line_number':276,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':280,'multiline':False]
['text':' This is actually dropping out entire tokens to attend to, which might','line_number':283,'multiline':False]
['text':' seem a bit unusual, but is taken from the original Transformer paper.','line_number':284,'multiline':False]
['text':' Mask heads if we want to','line_number':287,'multiline':False]
['text':' Prune linear layers','line_number':334,'multiline':False]
['text':' Update hyper params and store pruned heads','line_number':340,'multiline':False]
['text':' add attentions if we output them','line_number':356,'multiline':False]
['text':' in BEiT, layernorm is applied before self-attention','line_number':418,'multiline':False]
['text':' add self attentions if we output attention weights','line_number':424,'multiline':False]
['text':' apply lambda_1 if present','line_number':426,'multiline':False]
['text':' first residual connection','line_number':430,'multiline':False]
['text':' in BEiT, layernorm is also applied after self-attention','line_number':433,'multiline':False]
['text':' second residual connection','line_number':442,'multiline':False]
['text':' 2*Wh-1 * 2*Ww-1, nH','line_number':457,'multiline':False]
['text':' cls to token & token 2 cls & cls to cls','line_number':458,'multiline':False]
['text':' get pair-wise relative position index for each token inside the window','line_number':460,'multiline':False]
['text':' 2, Wh, Ww','line_number':463,'multiline':False]
['text':' 2, Wh*Ww','line_number':464,'multiline':False]
['text':' 2, Wh*Ww, Wh*Ww','line_number':465,'multiline':False]
['text':' Wh*Ww, Wh*Ww, 2','line_number':466,'multiline':False]
['text':' shift to start from 0','line_number':467,'multiline':False]
['text':' Wh*Ww, Wh*Ww','line_number':473,'multiline':False]
['text':' Wh*Ww,Wh*Ww,nH','line_number':483,'multiline':False]
['text':' nH, Wh*Ww, Wh*Ww','line_number':485,'multiline':False]
['text':' stochastic depth decay rule','line_number':497,'multiline':False]
['text':' Slightly different from the TF version which uses truncated_normal for initialization','line_number':572,'multiline':False]
['text':' cf https://github.com/pytorch/pytorch/pull/5617','line_number':573,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':637,'multiline':False]
['text':' Prepare head mask if needed','line_number':681,'multiline':False]
['text':' 1.0 in head_mask indicate we keep the head','line_number':682,'multiline':False]
['text':' attention_probs has shape bsz x n_heads x N x N','line_number':683,'multiline':False]
['text':' input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]','line_number':684,'multiline':False]
['text':' and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]','line_number':685,'multiline':False]
['text':' Mean pool the final hidden states of the patch tokens','line_number':722,'multiline':False]
['text':' Pool by simply taking the final hidden state of the [CLS] token','line_number':726,'multiline':False]
['text':' Classifier head','line_number':746,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':750,'multiline':False]
['text':' -100 index = padding token','line_number':817,'multiline':False]
['text':' Classifier head','line_number':846,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':849,'multiline':False]
['text':' e.g. (1, 2, 3, 6)','line_number':1023,'multiline':False]
['text':' e.g. [768, 768, 768, 768]','line_number':1024,'multiline':False]
['text':' PSP Module','line_number':1029,'multiline':False]
['text':' FPN Module','line_number':1042,'multiline':False]
['text':' skip the top layer','line_number':1045,'multiline':False]
['text':' build laterals','line_number':1068,'multiline':False]
['text':' build top-down path','line_number':1073,'multiline':False]
['text':' build outputs','line_number':1081,'multiline':False]
['text':' append psp feature','line_number':1083,'multiline':False]
['text':' just take the relevant feature maps','line_number':1147,'multiline':False]
['text':' FPNs','line_number':1169,'multiline':False]
['text':' Semantic segmentation head(s)','line_number':1188,'multiline':False]
['text':' Initialize weights and apply final processing','line_number':1192,'multiline':False]
['text':' upsample logits to the images' original size','line_number':1196,'multiline':False]
['text':' compute weighted loss','line_number':1204,'multiline':False]
['text':' we need the intermediate hidden states','line_number':1259,'multiline':False]
['text':' only keep certain features, and reshape','line_number':1265,'multiline':False]
['text':' note that we do +1 as the encoder_hidden_states also includes the initial embeddings','line_number':1266,'multiline':False]
['text':' apply FPNs','line_number':1274,'multiline':False]
['text':' initialize weights and apply final processing','line_number':1341,'multiline':False]
