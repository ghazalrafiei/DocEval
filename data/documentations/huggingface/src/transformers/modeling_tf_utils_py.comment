['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' make sure only labels that are not equal to -100 affect the loss','line_number':207,'multiline':False]
['text':' Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway','line_number':213,'multiline':False]
['text':' make sure only labels that are not equal to -100 affect the loss','line_number':215,'multiline':False]
['text':' Data-dependent conditionals are forbidden in XLA','line_number':252,'multiline':False]
['text':' make sure only labels that are not equal to -100','line_number':257,'multiline':False]
['text':' are taken into account as loss','line_number':258,'multiline':False]
['text':' Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway','line_number':269,'multiline':False]
['text':' make sure only labels that are not equal to -100 or -1','line_number':271,'multiline':False]
['text':' are taken into account as loss','line_number':272,'multiline':False]
['text':' Avoid possible division by zero later','line_number':274,'multiline':False]
['text':' Masked positions will have a loss of NaN because -100 and -1 are not valid labels','line_number':275,'multiline':False]
['text':' MeanSquaredError returns a scalar loss if the labels are 1D, so avoid that','line_number':290,'multiline':False]
['text':' make sure only labels that are not equal to -100','line_number':338,'multiline':False]
['text':' are taken into account as loss','line_number':339,'multiline':False]
['text':' make sure only labels that are not equal to -100','line_number':346,'multiline':False]
['text':' are taken into account as loss','line_number':347,'multiline':False]
['text':' Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway','line_number':349,'multiline':False]
['text':' Just zero out samples where label is -100, no reduction','line_number':352,'multiline':False]
['text':' Pure conv models (such as ConvNext) do not have `output_attentions`. If the signature has','line_number':373,'multiline':False]
['text':' `output_attentions`, it will be present here in `kwargs`, even if unset (in that case, as `None`)','line_number':374,'multiline':False]
['text':' isolates the actual `**kwargs` for the decorated function','line_number':410,'multiline':False]
['text':' move any arg into kwargs, if they exist','line_number':415,'multiline':False]
['text':' Encoder Decoder models delegate the application of the configuration options to their inner models.','line_number':418,'multiline':False]
['text':' Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This','line_number':427,'multiline':False]
['text':' function does not follow wrapper chains (i.e. ignores `functools.wraps()`), meaning that without the line below','line_number':428,'multiline':False]
['text':' Keras would attempt to check the first argument against the literal signature of the wrapper.','line_number':429,'multiline':False]
['text':' EagerTensors don't allow to use the .name property so we check for a real Tensor','line_number':505,'multiline':False]
['text':' Tensor names have always the pattern `name:id` then we check only the','line_number':507,'multiline':False]
['text':' `name` part','line_number':508,'multiline':False]
['text':' Populates any unspecified argument with their default value, according to the signature.','line_number':559,'multiline':False]
['text':' When creating a SavedModel TF calls the method with LayerCall.__call__(args, **kwargs)','line_number':564,'multiline':False]
['text':' So to respect the proper output we have to add this exception','line_number':565,'multiline':False]
['text':' `args` in this case is always the first parameter, then `input_ids`','line_number':571,'multiline':False]
['text':' If this weight is going to tip up over the maximal size, we split.','line_number':671,'multiline':False]
['text':' Add the last block','line_number':681,'multiline':False]
['text':' If we only have one shard, we return it','line_number':684,'multiline':False]
['text':' Otherwise, let's build the index','line_number':688,'multiline':False]
['text':' Add the metadata','line_number':698,'multiline':False]
['text':' Load the index','line_number':725,'multiline':False]
['text':' Since TF adds the name of the class to its weights, and uses the index and not the name of the layer to load','line_number':730,'multiline':False]
['text':' the weight, we have to get rid of the first prefix of the name of the layer.','line_number':731,'multiline':False]
['text':' Read the H5 file','line_number':789,'multiline':False]
['text':' Retrieve the name of each layer from the H5 file','line_number':792,'multiline':False]
['text':' Compute missing and unexpected sub layers','line_number':796,'multiline':False]
['text':' Store the weights in list of tuples that looks like [(weight_object, value_of_weight),...]','line_number':797,'multiline':False]
['text':' If the current weight is found','line_number':810,'multiline':False]
['text':' Check if the shape of the current weight and the one from the H5 file are different','line_number':812,'multiline':False]
['text':' If yes we reshape the weight from the H5 file accordingly to the current weight','line_number':814,'multiline':False]
['text':' If the two shapes are not compatible we raise an issue','line_number':815,'multiline':False]
['text':' We create the tuple that will be loaded and add it to the final list','line_number':829,'multiline':False]
['text':' Read the H5 file','line_number':889,'multiline':False]
['text':' Retrieve the name of each layer from the H5 file','line_number':891,'multiline':False]
['text':' Find the missing layers from the high level list of layers','line_number':894,'multiline':False]
['text':' Find the unexpected layers from the high level list of layers','line_number':897,'multiline':False]
['text':' Compute missing and unexpected sub layers','line_number':903,'multiline':False]
['text':' Store the weights in list of tuples that looks like [(weight_object, value_of_weight),...]','line_number':904,'multiline':False]
['text':' if layer_name from the H5 file belongs to the layers from the instantiated model','line_number':906,'multiline':False]
['text':' Get the H5 layer object from its name','line_number':908,'multiline':False]
['text':' Get all the weights as a list from the layer object','line_number':910,'multiline':False]
['text':' Create a dict from the H5 saved model that looks like {"weight_name": weight_value}','line_number':914,'multiline':False]
['text':' And a set with only the names','line_number':915,'multiline':False]
['text':' TF names always start with the model name so we ignore it','line_number':917,'multiline':False]
['text':' Add the updated name to the final list for computing missing/unexpected values','line_number':925,'multiline':False]
['text':' Loop over each weights from the instantiated model and compare with the weights from the H5 file','line_number':928,'multiline':False]
['text':' TF names always start with the model name so we ignore it','line_number':930,'multiline':False]
['text':' here we check if the current weight is among the weights from the H5 file','line_number':940,'multiline':False]
['text':' If yes, get the weight_value of the corresponding weight from the H5 file','line_number':941,'multiline':False]
['text':' If not, make the value to None','line_number':942,'multiline':False]
['text':' Retrocompatibility patch: some embeddings are stored with the weights name (e.g. Bart's','line_number':945,'multiline':False]
['text':' `model.shared/embeddings:0` are stored as `model.shared/weights:0`)','line_number':946,'multiline':False]
['text':' Add the updated name to the final list for computing missing/unexpected values','line_number':951,'multiline':False]
['text':' If the current weight is found','line_number':954,'multiline':False]
['text':' Check if the shape of the current weight and the one from the H5 file are different','line_number':956,'multiline':False]
['text':' If yes we reshape the weight from the H5 file accordingly to the current weight','line_number':958,'multiline':False]
['text':' If the two shapes are not compatible we raise an issue','line_number':959,'multiline':False]
['text':' We create the tuple that will be loaded and add it to the final list','line_number':973,'multiline':False]
['text':' Load all the weights','line_number':976,'multiline':False]
['text':' Compute the missing and unexpected layers','line_number':979,'multiline':False]
['text':' Read the safetensors file','line_number':987,'multiline':False]
['text':' Find the missing layers from the high level list of layers','line_number':992,'multiline':False]
['text':' Find the unexpected layers from the high level list of layers','line_number':994,'multiline':False]
['text':' Check if the shape of the current weight and the one from the H5 file are different','line_number':1001,'multiline':False]
['text':' If yes we reshape the weight from the H5 file accordingly to the current weight','line_number':1003,'multiline':False]
['text':' If the two shapes are not compatible we raise an issue','line_number':1004,'multiline':False]
['text':' weight.assign() might break if weight is a DTensor','line_number':1014,'multiline':False]
['text':' initialize new embeddings','line_number':1034,'multiline':False]
['text':' Copy token embeddings from the previous ones','line_number':1035,'multiline':False]
['text':' if the new size is greater than the old one, we extend the current embeddings with a padding until getting new size','line_number':1037,'multiline':False]
['text':' and we create a mask to properly identify the padded values and be replaced by the values of the newly created','line_number':1038,'multiline':False]
['text':' embeddings','line_number':1039,'multiline':False]
['text':' if the new size if lower than the old one, we take the current embeddings until the new size','line_number':1047,'multiline':False]
['text':' a list of re pattern of tensor names to ignore from the model when loading the model weights','line_number':1085,'multiline':False]
['text':' (and avoid unnecessary warnings).','line_number':1086,'multiline':False]
['text':' a list of re pattern of tensor names to ignore from the weights when loading the model weights','line_number':1088,'multiline':False]
['text':' (and avoid unnecessary warnings).','line_number':1089,'multiline':False]
['text':' 2 is the most correct arbitrary size. I will not be taking questions','line_number':1103,'multiline':False]
['text':' But let's make the batch size 1 to save memory anyway','line_number':1106,'multiline':False]
['text':' Some models have token_type_ids but with a vocab_size of 1','line_number':1110,'multiline':False]
['text':' This is just here to make sure we don't call the superclass build()','line_number':1136,'multiline':False]
['text':' Save config and origin of the pretrained weights if given in model','line_number':1146,'multiline':False]
['text':' switch to float if need + fp16 compatibility','line_number':1196,'multiline':False]
['text':' Layers may not have the same dimensions','line_number':1297,'multiline':False]
['text':' Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.','line_number':1308,'multiline':False]
['text':' Alternativelly, the model can also have a custom `generate` function.','line_number':1309,'multiline':False]
['text':' We avoid tf.train.checkpoint or saving weights in TF format, even though that includes optimizer','line_number':1331,'multiline':False]
['text':' state for us, because it requires special handling for objects like custom losses, which we use','line_number':1332,'multiline':False]
['text':' internally and which users are likely to use too','line_number':1333,'multiline':False]
['text':' If this isn't a local path, check that the remote repo exists and has a checkpoint in it','line_number':1362,'multiline':False]
['text':' Now make sure the repo actually has a checkpoint in it.','line_number':1370,'multiline':False]
['text':' Assuming the repo is real and we got a checkpoint, load the weights and the optimizer state into the model.','line_number':1379,'multiline':False]
['text':' The optimizer state includes the iteration count, so learning rate schedules should resume as normal too.','line_number':1380,'multiline':False]
['text':' Finally, return the epoch number from the checkpoint. This isn't a property of the model, so we can't','line_number':1386,'multiline':False]
['text':' set it directly, but the user can pass it to fit().','line_number':1387,'multiline':False]
['text':' noqa:F821','line_number':1392,'multiline':False]
['text':' TODO Matt: This is a workaround for older versions of datasets that are missing the `cols_to_retain`','line_number':1460,'multiline':False]
['text':'            argument. We should remove this once the minimum supported version of datasets is > 2.3.2','line_number':1461,'multiline':False]
['text':' Backwards compatibility for older versions of datasets. Previously, if `columns` or `label_cols`','line_number':1475,'multiline':False]
['text':' were a single element list, the returned element spec would be a single element. Now, passing [feature]','line_number':1476,'multiline':False]
['text':' will return a dict structure {"feature": feature}, and passing a single string will return a single element.','line_number':1477,'multiline':False]
['text':' "passthrough" for workflow backward compatibility','line_number':1510,'multiline':False]
['text':' This argument got renamed, we need to support both versions','line_number':1525,'multiline':False]
['text':' This will be true in TF 2.8 or greater','line_number':1551,'multiline':False]
['text':' We hardcode the most common renamings; models with weirder names can set `self._label_to_output_map`','line_number':1586,'multiline':False]
['text':' Newer TF train steps leave this out','line_number':1592,'multiline':False]
['text':' If the inputs are mutable dictionaries, make a shallow copy of them because we will modify','line_number':1595,'multiline':False]
['text':' them during input/label pre-processing. This avoids surprising the user by wrecking their data.','line_number':1596,'multiline':False]
['text':' In addition, modifying mutable Python inputs makes XLA compilation impossible.','line_number':1597,'multiline':False]
['text':' When using a dummy loss, we ensure that separate labels are copied to the correct model arguments,','line_number':1603,'multiline':False]
['text':' if those keys are not already present in the input dict','line_number':1604,'multiline':False]
['text':' If y is a tensor and the model only has one label-like input, map y to that input','line_number':1606,'multiline':False]
['text':' Otherwise, copy keys from y to x as long as they weren't already present in x','line_number':1613,'multiline':False]
['text':' Rename labels at this point to match output heads','line_number':1628,'multiline':False]
['text':' Run forward pass.','line_number':1631,'multiline':False]
['text':' This next block matches outputs to label keys. Tensorflow's standard method for doing this','line_number':1642,'multiline':False]
['text':' can get very confused if any of the keys contain nested values (e.g. lists/tuples of Tensors)','line_number':1643,'multiline':False]
['text':' If the labels are a dict, match keys from the output by name','line_number':1653,'multiline':False]
['text':' If the labels are a tuple/list, match keys to the output by order, skipping the loss.','line_number':1656,'multiline':False]
['text':' Remove unused fields in case those cause problems','line_number':1661,'multiline':False]
['text':' If the labels are a single tensor, match them to the first non-loss tensor in the output','line_number':1663,'multiline':False]
['text':' Run backwards pass.','line_number':1672,'multiline':False]
['text':' Collect metrics to return','line_number':1676,'multiline':False]
['text':' We hardcode the most common renamings; models with weirder names can set `self._label_to_output_map`','line_number':1693,'multiline':False]
['text':' Newer versions leave this out','line_number':1699,'multiline':False]
['text':' If the inputs are mutable dictionaries, make a shallow copy of them because we will modify','line_number':1702,'multiline':False]
['text':' them during input/label pre-processing. This avoids surprising the user by wrecking their data.','line_number':1703,'multiline':False]
['text':' In addition, modifying mutable Python inputs makes XLA compilation impossible.','line_number':1704,'multiline':False]
['text':' When using a dummy loss, we ensure that separate labels are copied to the correct model arguments,','line_number':1710,'multiline':False]
['text':' if those keys are not already present in the input dict','line_number':1711,'multiline':False]
['text':' If y is a tensor and the model only has one label-like input, map y to that input','line_number':1714,'multiline':False]
['text':' Otherwise, copy keys from y to x as long as they weren't already present in x','line_number':1721,'multiline':False]
['text':' Rename labels at this point to match output heads','line_number':1736,'multiline':False]
['text':' Run forward pass.','line_number':1739,'multiline':False]
['text':' This next block matches outputs to label keys. Tensorflow's standard method for doing this','line_number':1749,'multiline':False]
['text':' can get very confused if any of the keys contain nested values (e.g. lists/tuples of Tensors)','line_number':1750,'multiline':False]
['text':' If the labels are a dict, match keys from the output by name','line_number':1760,'multiline':False]
['text':' If the labels are a tuple/list, match keys to the output by order, skipping the loss.','line_number':1763,'multiline':False]
['text':' Remove unused fields in case those cause problems','line_number':1768,'multiline':False]
['text':' If the labels are a single tensor, match them to the first non-loss tensor in the output','line_number':1770,'multiline':False]
['text':' Collect metrics to return','line_number':1780,'multiline':False]
['text':' Avoids a circular import by doing this when necessary.','line_number':1830,'multiline':False]
['text':' tests_ignore','line_number':1831,'multiline':False]
['text':' Overwrite for models with output embeddings','line_number':1888,'multiline':False]
['text':' TODO (joao): flagged for replacement (by `_v2_resized_token_embeddings`) due to embeddings refactor','line_number':1989,'multiline':False]
['text':' Run the new code path if the model has a keras embeddings layer','line_number':1991,'multiline':False]
['text':' Update base model and current model config','line_number':2000,'multiline':False]
['text':' Update base model and current model config','line_number':2023,'multiline':False]
['text':' TODO (joao): flagged for delection due to embeddings refactor','line_number':2029,'multiline':False]
['text':' If the variable holds the weights themselves, return them','line_number':2031,'multiline':False]
['text':' Otherwise, try to get them from the layer's attributes','line_number':2034,'multiline':False]
['text':' The reason why the attributes don't exist might be','line_number':2044,'multiline':False]
['text':' because the model is not built, so retry getting','line_number':2045,'multiline':False]
['text':' the argument after building the model','line_number':2046,'multiline':False]
['text':' TODO (joao): flagged for replacement (by `_v2_resize_token_embeddings`) due to embeddings refactor','line_number':2060,'multiline':False]
['text':' if word embeddings are not tied, make sure that lm head bias is resized as well','line_number':2064,'multiline':False]
['text':' if word embeddings are not tied, make sure that lm head decoder is resized as well','line_number':2071,'multiline':False]
['text':' If word embeddings are not tied, make sure that lm head bias is resized as well','line_number':2087,'multiline':False]
['text':' If word embeddings are not tied, make sure that lm head decoder is resized as well.','line_number':2093,'multiline':False]
['text':' TODO (joao): this one probably needs a v2 version with other models','line_number':2097,'multiline':False]
['text':' TODO (joao): flagged for replacement (by `_v2_get_resized_lm_head_bias`) due to embeddings refactor','line_number':2120,'multiline':False]
['text':' initialize new bias','line_number':2128,'multiline':False]
['text':' Determine the size difference (depending on the shape)','line_number':2176,'multiline':False]
['text':' Copy the old bias values to the new bias','line_number':2180,'multiline':False]
['text':' TODO (joao): flagged for replacement (by `_v2_get_resized_embeddings`) due to embeddings refactor','line_number':2247,'multiline':False]
['text':' Get the initialization range for the embeddings','line_number':2280,'multiline':False]
['text':' default value','line_number':2281,'multiline':False]
['text':' most common','line_number':2283,'multiline':False]
['text':' e.g. T5','line_number':2284,'multiline':False]
['text':' e.g BART','line_number':2285,'multiline':False]
['text':' Get a new (initialized) embeddings layer','line_number':2291,'multiline':False]
['text':' exact same scoped name except "/embeddings:0"','line_number':2296,'multiline':False]
['text':' Copy the old embeddings to the new embeddings','line_number':2300,'multiline':False]
['text':' If `torch_dtype` is in the config with a torch dtype class as the value, we need to change it to string.','line_number':2404,'multiline':False]
['text':' (Although TF doesn't care about this attribute, we can't just remove it or set it to `None`.)','line_number':2405,'multiline':False]
['text':' Save configuration file','line_number':2425,'multiline':False]
['text':' If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be','line_number':2428,'multiline':False]
['text':' loaded from the Hub.','line_number':2429,'multiline':False]
['text':' If we save using the predefined names, we can load using `from_pretrained`','line_number':2437,'multiline':False]
['text':' Clean the folder from a previous save','line_number':2443,'multiline':False]
['text':' If we have a shard file that is not going to be replaced, we delete it, but only from the main process','line_number':2446,'multiline':False]
['text':' in distributed settings to avoid race conditions.','line_number':2447,'multiline':False]
['text':' Save the index as well','line_number':2465,'multiline':False]
['text':' Not relevant for TF models','line_number':2647,'multiline':False]
['text':' Load config if we don't provide a configuration','line_number':2675,'multiline':False]
['text':' This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the','line_number':2699,'multiline':False]
['text':' index of the files.','line_number':2700,'multiline':False]
['text':' Load model','line_number':2702,'multiline':False]
['text':' Load from a PyTorch checkpoint in priority if from_pt','line_number':2708,'multiline':False]
['text':' Load from a sharded PyTorch checkpoint','line_number':2711,'multiline':False]
['text':' Load from a safetensors checkpoint','line_number':2717,'multiline':False]
['text':' Load from a TF 2.0 checkpoint','line_number':2720,'multiline':False]
['text':' Load from a sharded TF 2.0 checkpoint','line_number':2723,'multiline':False]
['text':' Load from a sharded safetensors checkpoint','line_number':2729,'multiline':False]
['text':' At this stage we don't have a weight file so we will raise an error.','line_number':2733,'multiline':False]
['text':' set correct filename','line_number':2757,'multiline':False]
['text':' Load from URL or cache if already cached','line_number':2766,'multiline':False]
['text':' Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None','line_number':2782,'multiline':False]
['text':' result when internet is up, the repo and revision exist, but the file does not.','line_number':2783,'multiline':False]
['text':' Did not find the safetensors file, let's fallback to TF.','line_number':2785,'multiline':False]
['text':' No support for sharded safetensors yet, so we'll raise an error if that's all we find.','line_number':2786,'multiline':False]
['text':' Maybe the checkpoint is sharded, we try to grab the index name in this case.','line_number':2792,'multiline':False]
['text':' Maybe the checkpoint is sharded, we try to grab the index name in this case.','line_number':2799,'multiline':False]
['text':' Otherwise, maybe there is a PyTorch or Flax model file.  We try those to give a helpful error','line_number':2806,'multiline':False]
['text':' message.','line_number':2807,'multiline':False]
['text':' Raise any environment error raise by `cached_file`. It will have a helpful error message adapted','line_number':2831,'multiline':False]
['text':' to the original exception.','line_number':2832,'multiline':False]
['text':' For any other exception, we throw a generic error.','line_number':2835,'multiline':False]
['text':' We'll need to download and cache each checkpoint shard if the checkpoint is sharded.','line_number':2852,'multiline':False]
['text':' resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.','line_number':2854,'multiline':False]
['text':' composed models, *e.g.* TFRag, require special treatment when it comes to loading','line_number':2882,'multiline':False]
['text':' pre-trained weights.','line_number':2883,'multiline':False]
['text':' Instantiate model.','line_number':2887,'multiline':False]
['text':' TODO Matt: This is a temporary workaround to allow weight renaming, but requires a method','line_number':2891,'multiline':False]
['text':'            to be defined for each class that requires a rename. We can probably just have a class-level','line_number':2892,'multiline':False]
['text':'            dict and a single top-level method or something and cut down a lot of boilerplate code','line_number':2893,'multiline':False]
['text':' Load from a PyTorch checkpoint','line_number':2899,'multiline':False]
['text':' we might need to extend the variable scope for composite models','line_number':2909,'multiline':False]
['text':' build the network with dummy inputs','line_number':2912,'multiline':False]
['text':' build the network with dummy inputs','line_number':2914,'multiline':False]
['text':' Load from a PyTorch checkpoint','line_number':2920,'multiline':False]
['text':' We load in TF format here because PT weights often need to be transposed, and this is much','line_number':2921,'multiline':False]
['text':' faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.','line_number':2922,'multiline':False]
['text':' No need to build the model again','line_number':2926,'multiline':False]
['text':' 'by_name' allow us to do transfer learning by skipping/adding layers','line_number':2934,'multiline':False]
['text':' see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357','line_number':2935,'multiline':False]
['text':' If it is a model with generation capabilities, attempt to load the generation config','line_number':3019,'multiline':False]
['text':' (`use_auth_token` is deprecated: we have to keep it here as we don't have **kwargs)','line_number':3061,'multiline':False]
['text':' Deprecation warning will be sent after for repo_url and organization','line_number':3122,'multiline':False]
['text':' Save all files.','line_number':3142,'multiline':False]
['text':' This is a Keras model and we might be able to fish out its History and make a model card out of it','line_number':3145,'multiline':False]
['text':' TODO (joao): flagged for delection due to embeddings refactor','line_number':3251,'multiline':False]
['text':' We should use a standard multi-head attention module with absolute positional embedding for that.','line_number':3371,'multiline':False]
['text':' Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276','line_number':3372,'multiline':False]
['text':' We can probably just use the multi-head attention module of PyTorch >=1.1.0','line_number':3373,'multiline':False]
['text':' e.g. [batch, num choices, seq length, hidden dims]','line_number':3419,'multiline':False]
['text':' A tensor full of shape [batch] or [batch, num choices] full of sequence length','line_number':3423,'multiline':False]
['text':' else:','line_number':3427,'multiline':False]
['text':' cls_index = cls_index[..., tf.newaxis]','line_number':3428,'multiline':False]
['text':' cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))','line_number':3429,'multiline':False]
['text':' shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states','line_number':3430,'multiline':False]
['text':' shape of output: (batch, num choices, hidden_size)','line_number':3434,'multiline':False]
