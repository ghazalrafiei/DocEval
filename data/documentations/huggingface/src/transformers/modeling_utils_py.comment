['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':' Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':5,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':6,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':11,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':12,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':13,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':14,'multiline':False]
['text':' limitations under the License.','line_number':15,'multiline':False]
['text':' noqa: F401','line_number':44,'multiline':False]
['text':' # Save the original initialization functions','line_number':191,'multiline':False]
['text':' # Restore the original initialization functions','line_number':199,'multiline':False]
['text':' For nn.DataParallel compatibility in PyTorch 1.5','line_number':208,'multiline':False]
['text':' For nn.DataParallel compatibility in PyTorch > 1.5','line_number':226,'multiline':False]
['text':' Adding fix for https://github.com/pytorch/xla/issues/4152','line_number':245,'multiline':False]
['text':' Fixes issue where the model code passes a value that is out of range for XLA_USE_BF16=1','line_number':246,'multiline':False]
['text':' and XLA_DOWNCAST_BF16=1 so the conversion would cast it to -inf','line_number':247,'multiline':False]
['text':' NOTE: `is_torch_tpu_available()` is checked last as it induces a graph break in torch dynamo','line_number':248,'multiline':False]
['text':' if no floating dtype was found return whatever the first dtype is','line_number':259,'multiline':False]
['text':' For nn.DataParallel compatibility in PyTorch > 1.5','line_number':262,'multiline':False]
['text':' fallback to the last dtype','line_number':275,'multiline':False]
['text':' fallback to buffer dtype','line_number':278,'multiline':False]
['text':' if no floating dtype was found return whatever the first dtype is','line_number':305,'multiline':False]
['text':' when bnb serialization is used the weights in the state dict can be strings','line_number':365,'multiline':False]
['text':' check: https://github.com/huggingface/transformers/pull/24416 for more details','line_number':366,'multiline':False]
['text':' If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`','line_number':372,'multiline':False]
['text':' If this weight is going to tip up over the maximal size, we split, but only if we have put at least one','line_number':380,'multiline':False]
['text':' weight in the current shard.','line_number':381,'multiline':False]
['text':' If we only have one shard, we return it','line_number':391,'multiline':False]
['text':' Otherwise, let's build the index','line_number':395,'multiline':False]
['text':' Add the metadata','line_number':407,'multiline':False]
['text':' Load the index','line_number':436,'multiline':False]
['text':' load safe due to preference','line_number':453,'multiline':False]
['text':' load safe since we have no other choice','line_number':459,'multiline':False]
['text':' If strict=True, error before loading any of the state dicts.','line_number':468,'multiline':False]
['text':' Make sure memory is freed before we load the next state dict.','line_number':489,'multiline':False]
['text':' Return the same thing as PyTorch load_state_dict function.','line_number':493,'multiline':False]
['text':' Check format of the archive','line_number':502,'multiline':False]
['text':' Convert old format to new format if needed from a PyTorch state_dict','line_number':554,'multiline':False]
['text':' copy state_dict so _load_from_state_dict can modify it','line_number':569,'multiline':False]
['text':' PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants','line_number':577,'multiline':False]
['text':' so we need to apply the function recursively.','line_number':578,'multiline':False]
['text':' Parameters of module and children will start with prefix. We can exit early if there are none in this','line_number':582,'multiline':False]
['text':' state_dict','line_number':583,'multiline':False]
['text':' In sharded models, each shard has only part of the full state_dict, so only gather','line_number':588,'multiline':False]
['text':' parameters that are in the current state_dict.','line_number':589,'multiline':False]
['text':' because zero3 puts placeholders in model params, this context','line_number':593,'multiline':False]
['text':' manager gathers (unpartitions) the params of the current layer, then loads from','line_number':594,'multiline':False]
['text':' the state dict and then re-partitions them again','line_number':595,'multiline':False]
['text':' Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so','line_number':607,'multiline':False]
['text':' it's safe to delete it.','line_number':608,'multiline':False]
['text':' dematerialize param storage for keys that are going to be replaced by state_dict, by','line_number':646,'multiline':False]
['text':' putting those on the meta device','line_number':647,'multiline':False]
['text':' selectively switch to the meta device only those params/buffers that will','line_number':651,'multiline':False]
['text':' be next replaced from state_dict. This a complex way to do p.to_("meta")','line_number':652,'multiline':False]
['text':' since we have no in-place to_ for tensors.','line_number':653,'multiline':False]
['text':' isinstance returns False for Params on meta device, so switch after the check','line_number':656,'multiline':False]
['text':' left for now but could be removed, see below','line_number':666,'multiline':False]
['text':' XXX: remaining features to implement to be fully compatible with _load_state_dict_into_model','line_number':689,'multiline':False]
['text':' - deepspeed zero 3 support','line_number':690,'multiline':False]
['text':' - need to copy metadata if any - see _load_state_dict_into_model','line_number':691,'multiline':False]
['text':' - handling error_msgs - mimicking the error handling in module._load_from_state_dict()','line_number':692,'multiline':False]
['text':' - Is there a situation where some keys aren't in `loaded_state_dict_keys` and in which case','line_number':693,'multiline':False]
['text':'   they won't get loaded.','line_number':694,'multiline':False]
['text':' First part of the test is always true as load_state_dict_keys always contains state_dict keys.','line_number':716,'multiline':False]
['text':' We convert floating dtypes to the `dtype` passed. We want to keep the buffers/params','line_number':726,'multiline':False]
['text':' in int/uint/bool and not cast them.','line_number':727,'multiline':False]
['text':' For backward compatibility with older versions of `accelerate`','line_number':738,'multiline':False]
['text':' TODO: @sgugger replace this check with version check at the next `accelerate` release','line_number':739,'multiline':False]
['text':' For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model','line_number':745,'multiline':False]
['text':' find next higher level module that is defined in device_map:','line_number':762,'multiline':False]
['text':' bert.lm_head.weight -> bert.lm_head -> bert -> ''','line_number':763,'multiline':False]
['text':' TODO: group all errors and raise at the end.','line_number':767,'multiline':False]
['text':' For backward compatibility with older versions of `accelerate`','line_number':777,'multiline':False]
['text':' T5 has a mask that can compare sequence ids, we can simulate this here with this transposition','line_number':883,'multiline':False]
['text':' Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow','line_number':884,'multiline':False]
['text':' /transformer/transformer_layers.py#L270','line_number':885,'multiline':False]
['text':' encoder_extended_attention_mask = (encoder_extended_attention_mask ==','line_number':886,'multiline':False]
['text':' encoder_extended_attention_mask.transpose(-1, -2))','line_number':887,'multiline':False]
['text':' fp16 compatibility','line_number':888,'multiline':False]
['text':' in case past_key_values are used we need to add a prefix ones mask to the causal mask','line_number':904,'multiline':False]
['text':' causal and attention masks must have same type with pytorch version < 1.3','line_number':905,'multiline':False]
['text':' show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`','line_number':940,'multiline':False]
['text':' We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]','line_number':945,'multiline':False]
['text':' ourselves in which case we just need to make it broadcastable to all heads.','line_number':946,'multiline':False]
['text':' Provided a padding mask of dimensions [batch_size, seq_length]','line_number':950,'multiline':False]
['text':' - if the model is a decoder, apply a causal mask in addition to the padding mask','line_number':951,'multiline':False]
['text':' - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]','line_number':952,'multiline':False]
['text':' Since attention_mask is 1.0 for positions we want to attend and 0.0 for','line_number':964,'multiline':False]
['text':' masked positions, this operation will create a tensor which is 0.0 for','line_number':965,'multiline':False]
['text':' positions we want to attend and the dtype's smallest value for masked positions.','line_number':966,'multiline':False]
['text':' Since we are adding it to the raw scores before the softmax, this is','line_number':967,'multiline':False]
['text':' effectively the same as removing these entirely.','line_number':968,'multiline':False]
['text':' fp16 compatibility','line_number':969,'multiline':False]
['text':' We can specify head_mask for each layer','line_number':1006,'multiline':False]
['text':' switch to float if need + fp16 compatibility','line_number':1008,'multiline':False]
['text':' For 4bit models, we need to multiply the number of parameters by 2 as half of the parameters are','line_number':1049,'multiline':False]
['text':' used for the 4bit quantization (uint8 tensors are stored)','line_number':1050,'multiline':False]
['text':' a list of `re` patterns of `state_dict` keys that should be removed from the list of missing','line_number':1142,'multiline':False]
['text':' keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.','line_number':1143,'multiline':False]
['text':' a list of `re` patterns of `state_dict` keys that should be removed from the list of','line_number':1145,'multiline':False]
['text':' unexpected keys we find (keys inside the checkpoint but not the model) and avoid unnecessary','line_number':1146,'multiline':False]
['text':' warnings.','line_number':1147,'multiline':False]
['text':' a list of `state_dict` keys to ignore when saving the model (useful for keys that aren't','line_number':1149,'multiline':False]
['text':' trained, but which are either deterministic or tied variables)','line_number':1150,'multiline':False]
['text':' a list of `state_dict` keys that are potentially tied to another key in the state_dict.','line_number':1152,'multiline':False]
['text':' Flash Attention 2 support','line_number':1158,'multiline':False]
['text':' SDPA support','line_number':1161,'multiline':False]
['text':' Has support for a `Cache` instance as `past_key_values`','line_number':1164,'multiline':False]
['text':' Save config and origin of the pretrained weights if given in model','line_number':1189,'multiline':False]
['text':' Overwrite the class attribute to make it an instance attribute, so models like','line_number':1198,'multiline':False]
['text':' `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute','line_number':1199,'multiline':False]
['text':' when a different component (e.g. language_model) is used.','line_number':1200,'multiline':False]
['text':' Remove the attribute now that is has been consumed, so it's no saved in the config.','line_number':1214,'multiline':False]
['text':' override default dtype if needed','line_number':1229,'multiline':False]
['text':' We do not want to modify the config inplace in _from_config.','line_number':1234,'multiline':False]
['text':' this immediately partitions the model across all gpus, to avoid the overhead in time','line_number':1244,'multiline':False]
['text':' and memory copying it on CPU or each GPU first','line_number':1245,'multiline':False]
['text':' restore default dtype if it was modified','line_number':1251,'multiline':False]
['text':' Here we use config._attn_implementation_internal to check whether the attention implementation was explicitely set by the user.','line_number':1273,'multiline':False]
['text':' The property `PretrainedConfig._attn_implementation` is never `None`, for backward compatibility (always fall back on "eager").','line_number':1274,'multiline':False]
['text':' The `hasattr` here is used as some Transformers tests for some reason do not call PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)','line_number':1275,'multiline':False]
['text':' If a config is passed with a preset attn_implementation, we skip the automatic dispatch and use the user-provided config, with hard checks that the requested attention implementation is available.','line_number':1292,'multiline':False]
['text':' use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.','line_number':1310,'multiline':False]
['text':' Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.','line_number':1361,'multiline':False]
['text':' Alternativelly, the model can also have a custom `generate` function.','line_number':1362,'multiline':False]
['text':' The check `torch.empty(0).device.type != "cuda"` is needed as the model may be initialized after `torch.set_default_device` has been called,','line_number':1427,'multiline':False]
['text':' or the model may be initialized under the context manager `with torch.device("cuda"):`.','line_number':1428,'multiline':False]
['text':' Overwrite for models with output embeddings','line_number':1534,'multiline':False]
['text':' this can happen if the name corresponds to the position in a list module list of layers','line_number':1618,'multiline':False]
['text':' in this case the decoder has added a cross-attention that the encoder does not have','line_number':1619,'multiline':False]
['text':' thus skip this step and subtract one layer pos from encoder','line_number':1620,'multiline':False]
['text':' tie weights recursively','line_number':1643,'multiline':False]
['text':' if the module does not appear in _no_split_modules, we also check the children','line_number':1686,'multiline':False]
['text':' Update base model and current model config','line_number':1728,'multiline':False]
['text':' Tie weights again if needed','line_number':1732,'multiline':False]
['text':' Update new_num_tokens with the actual size of new_embeddings','line_number':1747,'multiline':False]
['text':' if word embeddings are not tied, make sure that lm head is resized as well','line_number':1757,'multiline':False]
['text':' Build new embeddings','line_number':1841,'multiline':False]
['text':' When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init','line_number':1843,'multiline':False]
['text':' because the shape of the new embedding layer is used across various modeling files','line_number':1844,'multiline':False]
['text':' as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading','line_number':1845,'multiline':False]
['text':' to errors when training.','line_number':1846,'multiline':False]
['text':' initialize all new embeddings (in particular added tokens)','line_number':1854,'multiline':False]
['text':' Copy token embeddings from the previous weights','line_number':1857,'multiline':False]
['text':' numbers of tokens to copy','line_number':1859,'multiline':False]
['text':' Build new lm head','line_number':1921,'multiline':False]
['text':' When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init','line_number':1925,'multiline':False]
['text':' because the shape of the new embedding layer is used across various modeling files','line_number':1926,'multiline':False]
['text':' as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading','line_number':1927,'multiline':False]
['text':' to errors when training.','line_number':1928,'multiline':False]
['text':' initialize new lm head (in particular added tokens)','line_number':1936,'multiline':False]
['text':' Copy old lm head weights to new lm head','line_number':1959,'multiline':False]
['text':' Copy bias weights to new lm head','line_number':1965,'multiline':False]
['text':' Prune heads if needed','line_number':1986,'multiline':False]
['text':' Initialize weights','line_number':1991,'multiline':False]
['text':' Tie weights should be skipped when not initializing all weights','line_number':1994,'multiline':False]
['text':' since from_pretrained(...) calls tie weights anyways','line_number':1995,'multiline':False]
['text':' save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads','line_number':2008,'multiline':False]
['text':' Unfortunately we have to store it as list for JSON','line_number':2011,'multiline':False]
['text':' For old GC format (transformers < 4.35.0) for models that live on the Hub','line_number':2037,'multiline':False]
['text':' we will fall back to the overwritten `_set_gradient_checkpointing` methid','line_number':2038,'multiline':False]
['text':' When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True','line_number':2051,'multiline':False]
['text':' we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334','line_number':2052,'multiline':False]
['text':' When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate','line_number':2053,'multiline':False]
['text':' the gradients to make sure the gradient flows.','line_number':2054,'multiline':False]
['text':' Apply it on the top-level module in case the top-level modules supports it','line_number':2060,'multiline':False]
['text':' for example, LongT5Stack inherits from `PreTrainedModel`.','line_number':2061,'multiline':False]
['text':' For old GC format (transformers < 4.35.0) for models that live on the Hub','line_number':2087,'multiline':False]
['text':' we will fall back to the overwritten `_set_gradient_checkpointing` methid','line_number':2088,'multiline':False]
['text':' Checks if the model has been loaded in 8-bit','line_number':2193,'multiline':False]
['text':' If the model has adapters attached, you can save the adapters','line_number':2204,'multiline':False]
['text':' Only save the model itself if we are using distributed training','line_number':2233,'multiline':False]
['text':' save the string version of dtype to the config, e.g. convert torch.float32 => "float32"','line_number':2236,'multiline':False]
['text':' we currently don't use this setting automatically, but may start to use with v5','line_number':2237,'multiline':False]
['text':' Attach architecture to the config','line_number':2241,'multiline':False]
['text':' If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be','line_number':2244,'multiline':False]
['text':' loaded from the Hub.','line_number':2245,'multiline':False]
['text':' Save the config','line_number':2249,'multiline':False]
['text':' Save the model','line_number':2283,'multiline':False]
['text':' Translate state_dict from smp to hf if saving with smp >= 1.10','line_number':2287,'multiline':False]
['text':' Handle the case where some state_dict keys shouldn't be saved','line_number':2292,'multiline':False]
['text':' Safetensors does not allow tensor aliasing.','line_number':2298,'multiline':False]
['text':' We're going to remove aliases before saving','line_number':2299,'multiline':False]
['text':' Sometimes in the state_dict we have non-tensor objects.','line_number':2302,'multiline':False]
['text':' e.g. in bitsandbytes we have some `str` objects in the state_dict','line_number':2303,'multiline':False]
['text':' In the non-tensor case, fall back to the pointer of the object itself','line_number':2307,'multiline':False]
['text':' These are all the pointers of shared tensors.','line_number':2310,'multiline':False]
['text':' Removing the keys which are declared as known duplicates on','line_number':2314,'multiline':False]
['text':' load. This allows to make sure the name which is kept is consistent.','line_number':2315,'multiline':False]
['text':' When not all duplicates have been cleaned, still remove those keys, but put a clear warning.','line_number':2325,'multiline':False]
['text':' If the link between tensors was done at runtime then `from_pretrained` will not get','line_number':2326,'multiline':False]
['text':' the key back leading to random tensor. A proper warning will be shown','line_number':2327,'multiline':False]
['text':' during reload (if applicable), but since the file is not necessarily compatible with','line_number':2328,'multiline':False]
['text':' the config, better show a proper warning.','line_number':2329,'multiline':False]
['text':' Shard the model if it is too big.','line_number':2342,'multiline':False]
['text':' Clean the folder from a previous save','line_number':2351,'multiline':False]
['text':' If we have a shard file that is not going to be replaced, we delete it, but only from the main process','line_number':2354,'multiline':False]
['text':' in distributed settings to avoid race conditions.','line_number':2355,'multiline':False]
['text':' make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005','line_number':2358,'multiline':False]
['text':' Save the model','line_number':2371,'multiline':False]
['text':' At some point we will need to deal better with save_function (used for TPU and other distributed','line_number':2374,'multiline':False]
['text':' joyfulness), but for now this enough.','line_number':2375,'multiline':False]
['text':' Save the index as well','line_number':2386,'multiline':False]
['text':' Checks if the model has been loaded in 8-bit','line_number':2425,'multiline':False]
['text':' Checks if the model has been loaded in 8-bit','line_number':2436,'multiline':False]
['text':' For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.','line_number':2443,'multiline':False]
['text':' the correct API should be to load the model with the desired dtype directly through `from_pretrained`.','line_number':2444,'multiline':False]
['text':' Checks if the model is quantized','line_number':2463,'multiline':False]
['text':' Checks if the model is quantized','line_number':2473,'multiline':False]
['text':' We make a call to the config file first (which may be absent) to get the commit hash as soon as possible','line_number':2788,'multiline':False]
['text':' change device_map into a map if we passed an int, a str or a torch.device','line_number':2828,'multiline':False]
['text':' The max memory utils require PyTorch >= 1.10 to have torch.cuda.mem_get_info.','line_number':2855,'multiline':False]
['text':' We force the `dtype` to be float16, this is a requirement from `bitsandbytes`','line_number':2906,'multiline':False]
['text':' Load config if we don't provide a configuration','line_number':2939,'multiline':False]
['text':' Need to protect the import','line_number':3013,'multiline':False]
['text':' Force-set to `True` for more mem efficiency','line_number':3050,'multiline':False]
['text':' This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the','line_number':3111,'multiline':False]
['text':' index of the files.','line_number':3112,'multiline':False]
['text':' Load model','line_number':3115,'multiline':False]
['text':' Keep in fp32 modules','line_number':3118,'multiline':False]
['text':' Load from a TF 1.0 checkpoint in priority if from_tf','line_number':3129,'multiline':False]
['text':' Load from a TF 2.0 checkpoint in priority if from_tf','line_number':3134,'multiline':False]
['text':' Load from a Flax checkpoint in priority if from_flax','line_number':3139,'multiline':False]
['text':' Load from a safetensors checkpoint','line_number':3144,'multiline':False]
['text':' Load from a sharded safetensors checkpoint','line_number':3153,'multiline':False]
['text':' Load from a PyTorch checkpoint','line_number':3161,'multiline':False]
['text':' Load from a sharded PyTorch checkpoint','line_number':3168,'multiline':False]
['text':' At this stage we don't have a weight file so we will raise an error.','line_number':3173,'multiline':False]
['text':' set correct filename','line_number':3214,'multiline':False]
['text':' Load from URL or cache if already cached','line_number':3225,'multiline':False]
['text':' Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None','line_number':3241,'multiline':False]
['text':' result when internet is up, the repo and revision exist, but the file does not.','line_number':3242,'multiline':False]
['text':' Maybe the checkpoint is sharded, we try to grab the index name in this case.','line_number':3244,'multiline':False]
['text':' This repo has no safetensors file of any kind, we switch to PyTorch.','line_number':3266,'multiline':False]
['text':' Maybe the checkpoint is sharded, we try to grab the index name in this case.','line_number':3272,'multiline':False]
['text':' Otherwise, maybe there is a TF or Flax model file.  We try those to give a helpful error','line_number':3281,'multiline':False]
['text':' message.','line_number':3282,'multiline':False]
['text':' Raise any environment error raise by `cached_file`. It will have a helpful error message adapted','line_number':3315,'multiline':False]
['text':' to the original exception.','line_number':3316,'multiline':False]
['text':' For any other exception, we throw a generic error.','line_number':3319,'multiline':False]
['text':' We'll need to download and cache each checkpoint shard if the checkpoint is sharded.','line_number':3336,'multiline':False]
['text':' rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.','line_number':3338,'multiline':False]
['text':' load pt weights early so that we know which dtype to init the model under','line_number':3377,'multiline':False]
['text':' Time to load the checkpoint','line_number':3380,'multiline':False]
['text':' set dtype to instantiate the model under:','line_number':3383,'multiline':False]
['text':' 1. If torch_dtype is not None, we use that dtype','line_number':3384,'multiline':False]
['text':' 2. If torch_dtype is "auto", we auto-detect dtype from the loaded state_dict, by checking its first','line_number':3385,'multiline':False]
['text':'    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype','line_number':3386,'multiline':False]
['text':' we also may have config.torch_dtype available, but we won't rely on it till v5','line_number':3387,'multiline':False]
['text':' free CPU memory','line_number':3404,'multiline':False]
['text':' Check if `_keep_in_fp32_modules` is not None','line_number':3415,'multiline':False]
['text':' In case some weights need to be kept in float32 and accelerate is not installed,','line_number':3425,'multiline':False]
['text':' we later on want to take the path where state_dict is not None, that is the one','line_number':3426,'multiline':False]
['text':' that do not require accelerate.','line_number':3427,'multiline':False]
['text':' Instantiate model.','line_number':3432,'multiline':False]
['text':' We do not want to modify the config inplace in from_pretrained.','line_number':3443,'multiline':False]
['text':' Let's make sure we don't run the init function of buffer modules','line_number':3449,'multiline':False]
['text':' make sure we use the model's config since the __init__ call might have copied it','line_number':3452,'multiline':False]
['text':' Check first if we are `from_pt`','line_number':3455,'multiline':False]
['text':' We keep some modules such as the lm_head in their original dtype for numerical stability reasons','line_number':3473,'multiline':False]
['text':' Extend the modules to not convert to keys that are supposed to be offloaded to `cpu` or `disk`','line_number':3484,'multiline':False]
['text':' training in 8-bit is only available in 0.37.0+','line_number':3508,'multiline':False]
['text':' We store the original dtype for quantized models as we cannot easily retrieve it','line_number':3552,'multiline':False]
['text':' once the weights have been quantized','line_number':3553,'multiline':False]
['text':' Note that once you have loaded a quantized model, you can't change its dtype so this will','line_number':3554,'multiline':False]
['text':' remain a single source of truth','line_number':3555,'multiline':False]
['text':' need more space for buffers that are created during quantization','line_number':3620,'multiline':False]
['text':' Make sure tied weights are tied before creating the device map.','line_number':3624,'multiline':False]
['text':' The LM head / tied weights or any last module can stay on disk / CPU','line_number':3629,'multiline':False]
['text':' check if we don't have tied param in different devices','line_number':3649,'multiline':False]
['text':' Load from a TensorFlow 1.X checkpoint - provided by original authors','line_number':3654,'multiline':False]
['text':' Remove the '.index'','line_number':3655,'multiline':False]
['text':' Load from our TensorFlow 2.0 checkpoints','line_number':3657,'multiline':False]
['text':' restore default dtype','line_number':3684,'multiline':False]
['text':' XXX: rename?','line_number':3697,'multiline':False]
['text':' make sure token embedding weights are still tied if needed','line_number':3715,'multiline':False]
['text':' Set model in evaluation mode to deactivate DropOut modules by default','line_number':3718,'multiline':False]
['text':' If it is a model with generation capabilities, attempt to load the generation config','line_number':3721,'multiline':False]
['text':' Dispatch model with hooks on all devices if necessary','line_number':3752,'multiline':False]
['text':' tie the model weights before retrieving the state_dict','line_number':3835,'multiline':False]
['text':' Retrieve missing & unexpected_keys','line_number':3838,'multiline':False]
['text':' key re-naming operations are never done on the keys','line_number':3860,'multiline':False]
['text':' that are loaded, but always on the keys of the newly initialized model','line_number':3861,'multiline':False]
['text':' Remove nonpersistent buffers from unexpected keys: they are not in the state dict but will be in the model','line_number':3874,'multiline':False]
['text':' buffers','line_number':3875,'multiline':False]
['text':' These are all the pointers of shared tensors.','line_number':3890,'multiline':False]
['text':' id function doesn't work for meta tensor so we need this function','line_number':3893,'multiline':False]
['text':' Some models may have keys that are not in the state by design, removing them before needlessly warning','line_number':3905,'multiline':False]
['text':' the user.','line_number':3906,'multiline':False]
['text':' retrieve weights on meta device and put them back on CPU.','line_number':3915,'multiline':False]
['text':' This is not ideal in terms of memory, but if we don't do that not, we can't initialize them in the next step','line_number':3916,'multiline':False]
['text':' upcast in fp32 if any','line_number':3927,'multiline':False]
['text':' retrieve unintialized modules and initialize before maybe overriding that with the pretrained weights.','line_number':3946,'multiline':False]
['text':' This will only initialize submodules that are not marked as initialized by the line above.','line_number':3955,'multiline':False]
['text':' Set some modules to fp32 if any','line_number':3958,'multiline':False]
['text':' param = param.to(torch.float32) does not work here as only in the local scope.','line_number':3962,'multiline':False]
['text':' Make sure we are able to load base models as well as derived models (with heads)','line_number':3965,'multiline':False]
['text':' If the checkpoint is sharded, we may not have the key here.','line_number':3992,'multiline':False]
['text':' The model key starts with `prefix` but `checkpoint_key` doesn't so we add it.','line_number':3997,'multiline':False]
['text':' The model key doesn't start with `prefix` but `checkpoint_key` does so we remove it.','line_number':4000,'multiline':False]
['text':' Whole checkpoint','line_number':4036,'multiline':False]
['text':' Sharded checkpoint or whole but low_cpu_mem_usage==True','line_number':4048,'multiline':False]
['text':' This should always be a list but, just to be sure.','line_number':4050,'multiline':False]
['text':' Skip the load for shards that only contain disk-offloaded weights when using safetensors for the offload.','line_number':4076,'multiline':False]
['text':' Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not','line_number':4081,'multiline':False]
['text':' matching the weights in the model.','line_number':4082,'multiline':False]
['text':' force memory release','line_number':4124,'multiline':False]
['text':' We need to add the prefix of the base model','line_number':4130,'multiline':False]
['text':' Load back temporarily offloaded state dict','line_number':4144,'multiline':False]
['text':' torch.nn.ParameterList is a special case where two parameter keywords','line_number':4206,'multiline':False]
['text':' are appended to the module name, *e.g.* bert.special_embeddings.0','line_number':4207,'multiline':False]
['text':' retrieve all modules that has at least one missing weight name','line_number':4213,'multiline':False]
['text':' Skip the check during tracing.','line_number':4331,'multiline':False]
['text':' Check only the first and last input IDs to reduce overhead.','line_number':4338,'multiline':False]
['text':' If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an','line_number':4346,'multiline':False]
['text':' attention_mask or not. In this case, we should still show a warning because this is a rare case.','line_number':4347,'multiline':False]
['text':' shape (bsz, 1, hsz)','line_number':4458,'multiline':False]
['text':' shape (bsz, 1, hsz)','line_number':4459,'multiline':False]
['text':' shape (bsz, slen, hsz)','line_number':4460,'multiline':False]
['text':' No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.','line_number':4519,'multiline':False]
['text':' shape (bsz, 1, hsz)','line_number':4525,'multiline':False]
['text':' shape (bsz, hsz)','line_number':4526,'multiline':False]
['text':' shape (bsz, 1, hsz)','line_number':4529,'multiline':False]
['text':' shape (bsz, hsz)','line_number':4530,'multiline':False]
['text':' shape (bsz, hsz)','line_number':4532,'multiline':False]
['text':' If we are on multi-GPU, let's remove the dimension added by batch splitting','line_number':4625,'multiline':False]
['text':' during training, compute the end logits based on the ground truth of the start position','line_number':4630,'multiline':False]
['text':' Predict answerability from the representation of CLS and START','line_number':4639,'multiline':False]
['text':' note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss','line_number':4644,'multiline':False]
['text':' during inference, compute the end logits based on beam search','line_number':4650,'multiline':False]
['text':' shape (bsz, slen)','line_number':4652,'multiline':False]
['text':' shape (bsz, start_n_top)','line_number':4656,'multiline':False]
['text':' shape (bsz, start_n_top, hsz)','line_number':4657,'multiline':False]
['text':' shape (bsz, start_n_top, hsz)','line_number':4658,'multiline':False]
['text':' shape (bsz, slen, start_n_top, hsz)','line_number':4659,'multiline':False]
['text':' shape (bsz, slen, start_n_top, hsz)','line_number':4663,'multiline':False]
['text':' shape (bsz, slen, start_n_top)','line_number':4666,'multiline':False]
['text':' shape (bsz, end_n_top, start_n_top)','line_number':4670,'multiline':False]
['text':' We should use a standard multi-head attention module with absolute positional embedding for that.','line_number':4720,'multiline':False]
['text':' Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276','line_number':4721,'multiline':False]
['text':' We can probably just use the multi-head attention module of PyTorch >=1.1.0','line_number':4722,'multiline':False]
['text':' shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states','line_number':4775,'multiline':False]
['text':' shape (bsz, XX, hidden_size)','line_number':4776,'multiline':False]
['text':' since there could be multiple levels of wrapping, unwrap recursively','line_number':4795,'multiline':False]
