['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2019 HuggingFace Inc.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Use the first letter of the name to get a value and go from a <> -13 to z <> 12','line_number':137,'multiline':False]
['text':' Prune heads if needed','line_number':143,'multiline':False]
['text':' Initialize weights','line_number':152,'multiline':False]
['text':' Tie weights should be skipped when not initializing all weights','line_number':155,'multiline':False]
['text':' since from_pretrained(...) calls tie weights anyways','line_number':156,'multiline':False]
['text':' make sure we don't have nans','line_number':240,'multiline':False]
['text':' the config file (and the generation config file, if it can generate) should be saved','line_number':259,'multiline':False]
['text':' check the keys are in the original state_dict','line_number':315,'multiline':False]
['text':' check that certain keys didn't get saved with the model','line_number':319,'multiline':False]
['text':' Test we can load the state dict in the model, necessary for the checkpointing API in Trainer.','line_number':328,'multiline':False]
['text':' at init model should have gradient checkpointing disabled','line_number':356,'multiline':False]
['text':' check enable works','line_number':360,'multiline':False]
['text':' Loop over all modules and check that relevant modules have gradient_checkpointing set to True','line_number':364,'multiline':False]
['text':' check disable works','line_number':371,'multiline':False]
['text':' Loop over all modules and check that relevant modules have gradient_checkpointing set to False','line_number':375,'multiline':False]
['text':' make a copy of model class to not break future tests','line_number':395,'multiline':False]
['text':' from https://stackoverflow.com/questions/9541025/how-to-copy-a-python-class','line_number':396,'multiline':False]
['text':' make sure that all keys are expected for test','line_number':402,'multiline':False]
['text':' make init deterministic, but make sure that','line_number':405,'multiline':False]
['text':' non-initialized weights throw errors nevertheless','line_number':406,'multiline':False]
['text':' this will often delete a single weight of a multi-weight module','line_number':413,'multiline':False]
['text':' to test an edge case','line_number':414,'multiline':False]
['text':' check that certain keys didn't get saved with the model','line_number':418,'multiline':False]
['text':' Before we test anything','line_number':425,'multiline':False]
['text':' 1. Create a dummy class. Should have buffers as well? To make sure we test __init__','line_number':435,'multiline':False]
['text':' 2. Make sure a linear layer's reset params is properly skipped:','line_number':451,'multiline':False]
['text':' 3. Make sure weights that are not present use init_weight_ and get expected values','line_number':467,'multiline':False]
['text':' make a copy of model class to not break future tests','line_number':497,'multiline':False]
['text':' from https://stackoverflow.com/questions/9541025/how-to-copy-a-python-class','line_number':498,'multiline':False]
['text':' make sure that all keys are expected for test','line_number':504,'multiline':False]
['text':' make init deterministic, but make sure that','line_number':507,'multiline':False]
['text':' non-initialized weights throw errors nevertheless','line_number':508,'multiline':False]
['text':' this will often delete a single weight of a multi-weight module','line_number':515,'multiline':False]
['text':' to test an edge case','line_number':516,'multiline':False]
['text':' check that certain keys didn't get saved with the model','line_number':520,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':584,'multiline':False]
['text':' unfreeze additional layers','line_number':631,'multiline':False]
['text':' Scenario - 1 default behaviour','line_number':668,'multiline':False]
['text':' Scenario - 2 with `use_reentrant=True` - this is the default value that is used in pytorch's','line_number':672,'multiline':False]
['text':' torch.utils.checkpoint.checkpoint','line_number':673,'multiline':False]
['text':' Scenario - 3 with `use_reentrant=False` pytorch suggests users to use this value for','line_number':677,'multiline':False]
['text':' future releases: https://pytorch.org/docs/stable/checkpoint.html','line_number':678,'multiline':False]
['text':' check that output_attentions also work using config','line_number':709,'multiline':False]
['text':' loss is at first position','line_number':735,'multiline':False]
['text':' loss is added to beginning','line_number':737,'multiline':False]
['text':' Question Answering model returns start_logits and end_logits','line_number':738,'multiline':False]
['text':' start_logits and end_logits instead of only 1 output','line_number':743,'multiline':False]
['text':' past_key_values have been returned','line_number':745,'multiline':False]
['text':' decoder attentions','line_number':749,'multiline':False]
['text':' cross attentions','line_number':758,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':771,'multiline':False]
['text':' This is copied from `torch/testing/_internal/jit_utils.py::clear_class_registry`','line_number':819,'multiline':False]
['text':' torch 1.8 has no `_clear_class_state` in `torch.jit._state`','line_number':823,'multiline':False]
['text':' To be sure we have no Nan','line_number':831,'multiline':False]
['text':' FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward','line_number':848,'multiline':False]
['text':' LayoutLMv2 requires additional inputs','line_number':857,'multiline':False]
['text':' when traced model is checked, an error is produced due to name mangling','line_number':864,'multiline':False]
['text':' Bros requires additional inputs (bbox)','line_number':865,'multiline':False]
['text':' when traced model is checked, an error is produced due to name mangling','line_number':871,'multiline':False]
['text':' example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.','line_number':884,'multiline':False]
['text':' Avoid memory leak. Without this, each call increase RAM usage by ~20MB.','line_number':945,'multiline':False]
['text':' (Even with this call, there are still memory leak by ~0.04MB)','line_number':946,'multiline':False]
['text':' To be sure we have no Nan','line_number':961,'multiline':False]
['text':' FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward','line_number':972,'multiline':False]
['text':' Test that the model can be serialized and restored properly','line_number':1051,'multiline':False]
['text':' Avoid memory leak. Without this, each call increase RAM usage by ~20MB.','line_number':1071,'multiline':False]
['text':' (Even with this call, there are still memory leak by ~0.04MB)','line_number':1072,'multiline':False]
['text':' To be sure we have no Nan','line_number':1085,'multiline':False]
['text':' Prepare head_mask','line_number':1091,'multiline':False]
['text':' Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior)','line_number':1092,'multiline':False]
['text':' necessary diferentiation because of T5 model','line_number':1106,'multiline':False]
['text':' Test that we can get a gradient back for importance score computation','line_number':1112,'multiline':False]
['text':' Remove Nan','line_number':1122,'multiline':False]
['text':' Check we don't have more than 25% nans (arbitrary)','line_number':1126,'multiline':False]
['text':' remove them (the test is less complete)','line_number':1129,'multiline':False]
['text':' encoder-decoder models have only 2 layers in each module','line_number':1133,'multiline':False]
['text':' TODO: To have this check, we will need at least 3 layers. Do we really need it?','line_number':1174,'multiline':False]
['text':' self.assertEqual(attentions[1].shape[-3], self.model_tester.num_attention_heads)','line_number':1175,'multiline':False]
['text':' TODO: To have this check, we will need at least 3 layers. Do we really need it?','line_number':1211,'multiline':False]
['text':' self.assertEqual(attentions[1].shape[-3], self.model_tester.num_attention_heads)','line_number':1212,'multiline':False]
['text':' TODO: To have this check, we will need at least 3 layers. Do we really need it?','line_number':1246,'multiline':False]
['text':' self.assertEqual(attentions[1].shape[-3], self.model_tester.num_attention_heads)','line_number':1247,'multiline':False]
['text':' check that output_hidden_states also work using config','line_number':1351,'multiline':False]
['text':' no need to test all models as different heads yield the same functionality','line_number':1362,'multiline':False]
['text':' Seq2Seq models','line_number':1374,'multiline':False]
['text':' Encoder-/Decoder-only models','line_number':1401,'multiline':False]
['text':' Retrieve the embeddings and clone theme','line_number':1458,'multiline':False]
['text':' Check that resizing the position embeddings with a larger max_position_embeddings increases','line_number':1467,'multiline':False]
['text':' the model's postion embeddings size','line_number':1468,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':1472,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1481,'multiline':False]
['text':' Check that resizing the position embeddings with a smaller max_position_embeddings decreases','line_number':1484,'multiline':False]
['text':' the model's max_position_embeddings','line_number':1485,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':1489,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1498,'multiline':False]
['text':' Check that adding and removing tokens has not modified the first part of the embedding matrix.','line_number':1501,'multiline':False]
['text':' Retrieve the embeddings and clone theme','line_number':1535,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':1539,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':1542,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1544,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':1547,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':1550,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1553,'multiline':False]
['text':' Input ids should be clamped to the maximum size of the vocabulary','line_number':1554,'multiline':False]
['text':' make sure that decoder_input_ids are resized as well','line_number':1557,'multiline':False]
['text':' Check that adding and removing tokens has not modified the first part of the embedding matrix.','line_number':1562,'multiline':False]
['text':' Check that resizing a model to a multiple of pad_to_multiple leads to a model of exactly that size','line_number':1587,'multiline':False]
['text':' if model cannot untied embeddings -> leave test','line_number':1608,'multiline':False]
['text':' if no output embeddings -> leave test','line_number':1616,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':1620,'multiline':False]
['text':' Check bias if present','line_number':1626,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1629,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':1632,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':1635,'multiline':False]
['text':' Check bias if present','line_number':1638,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1641,'multiline':False]
['text':' Input ids should be clamped to the maximum size of the vocabulary','line_number':1642,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':1646,'multiline':False]
['text':' The main input is the name of the argument after `self`','line_number':1662,'multiline':False]
['text':' Some models define this as None','line_number':1678,'multiline':False]
['text':' In that case, we *are* on a head model, but every','line_number':1684,'multiline':False]
['text':' single key is not actual parameters and this is','line_number':1685,'multiline':False]
['text':' tested in `test_tied_model_weights_key_ignore` test.','line_number':1686,'multiline':False]
['text':' Check that the embedding layer and decoding layer are the same in size and in value','line_number':1717,'multiline':False]
['text':' self.assertTrue(check_same_values(embeddings, decoding))','line_number':1718,'multiline':False]
['text':' # Check that after modification, they remain the same.','line_number':1720,'multiline':False]
['text':' embeddings.weight.data.div_(2)','line_number':1721,'multiline':False]
['text':' # Check that the embedding layer and decoding layer are the same in size and in value','line_number':1722,'multiline':False]
['text':' self.assertTrue(embeddings.weight.shape, decoding.weight.shape)','line_number':1723,'multiline':False]
['text':' self.assertTrue(check_same_values(embeddings, decoding))','line_number':1724,'multiline':False]
['text':' # Check that after modification, they remain the same.','line_number':1726,'multiline':False]
['text':' decoding.weight.data.div_(4)','line_number':1727,'multiline':False]
['text':' # Check that the embedding layer and decoding layer are the same in size and in value','line_number':1728,'multiline':False]
['text':' self.assertTrue(embeddings.weight.shape, decoding.weight.shape)','line_number':1729,'multiline':False]
['text':' self.assertTrue(check_same_values(embeddings, decoding))','line_number':1730,'multiline':False]
['text':' Check that after resize they remain tied.','line_number':1732,'multiline':False]
['text':' decoding.weight.data.mul_(20)','line_number':1737,'multiline':False]
['text':' # Check that the embedding layer and decoding layer are the same in size and in value','line_number':1738,'multiline':False]
['text':' self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)','line_number':1739,'multiline':False]
['text':' self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))','line_number':1740,'multiline':False]
['text':' Checking the state dicts are correct','line_number':1754,'multiline':False]
['text':' Checking there was no complain of missing weights','line_number':1761,'multiline':False]
['text':' Checking the tensor sharing are correct','line_number':1764,'multiline':False]
['text':' Checking the state dicts are correct','line_number':1788,'multiline':False]
['text':' Checking there was no complain of missing weights','line_number':1795,'multiline':False]
['text':' These are all the pointers of shared tensors.','line_number':1808,'multiline':False]
['text':' Detect we get a hit for each key','line_number':1812,'multiline':False]
['text':' Removed tied weights found from tied params -> there should only be one left after','line_number':1817,'multiline':False]
['text':' We are nuking ALL weights on file, so every parameter should','line_number':1836,'multiline':False]
['text':' yell on load. We're going to detect if we yell too much, or too little.','line_number':1837,'multiline':False]
['text':' Remove tied weights from extra missing: they are normally not warned as missing if their tied','line_number':1850,'multiline':False]
['text':' counterpart is present but here there are no weights at all so we do get the warning.','line_number':1851,'multiline':False]
['text':' We remove the group from extra_missing if not all weights from group are in it','line_number':1858,'multiline':False]
['text':' Remove nonpersistent buffers from missed_missing','line_number':1870,'multiline':False]
['text':' Don't copy this method to model specific test file!','line_number':1964,'multiline':False]
['text':' TODO: remove this method once the issues are all fixed!','line_number':1965,'multiline':False]
['text':' Make sure no all 0s attention masks - to avoid failure at this moment.','line_number':1973,'multiline':False]
['text':' Put `1` at the beginning of sequences to make it still work when combining causal attention masks.','line_number':1974,'multiline':False]
['text':' TODO: remove this line once a fix regarding large negative values for attention mask is done.','line_number':1975,'multiline':False]
['text':' Here we make the first sequence with all 0s as attention mask.','line_number':1980,'multiline':False]
['text':' Currently, this will fail for `TFWav2Vec2Model`. This is caused by the different large negative','line_number':1981,'multiline':False]
['text':' values, like `1e-4`, `1e-9`, `1e-30` and `-inf` for attention mask across models/frameworks.','line_number':1982,'multiline':False]
['text':' TODO: enable this block once the large negative values thing is cleaned up.','line_number':1983,'multiline':False]
['text':' (see https://github.com/huggingface/transformers/issues/14859)','line_number':1984,'multiline':False]
['text':' attention_mask = torch.cat(','line_number':1985,'multiline':False]
['text':'     [torch.zeros_like(attention_mask[:1], dtype=attention_mask.dtype), attention_mask[1:]],','line_number':1986,'multiline':False]
['text':'     dim=0','line_number':1987,'multiline':False]
['text':' )','line_number':1988,'multiline':False]
['text':' Don't copy this method to model specific test file!','line_number':1992,'multiline':False]
['text':' TODO: remove this method once the issues are all fixed!','line_number':1993,'multiline':False]
['text':' `TFGPT2` has `past_key_values` as a tensor while `GPT2` has it as a tuple.','line_number':2013,'multiline':False]
['text':' create new outputs from the remaining fields','line_number':2017,'multiline':False]
['text':' Copied from tests.test_modeling_tf_common.TFModelTesterMixin.check_pt_tf_outputs','line_number':2023,'multiline':False]
['text':' Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).','line_number':2040,'multiline':False]
['text':' Don't copy this block to model specific test file!','line_number':2047,'multiline':False]
['text':' TODO: remove this method and this line after issues are fixed','line_number':2048,'multiline':False]
['text':' convert to the case of `tuple`','line_number':2056,'multiline':False]
['text':' appending each key to the current (string) `name`','line_number':2057,'multiline':False]
['text':' Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)','line_number':2063,'multiline':False]
['text':' case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)','line_number':2069,'multiline':False]
['text':' case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`','line_number':2076,'multiline':False]
['text':' deal with NumPy's scalars to make replacing nan values by 0 work.','line_number':2094,'multiline':False]
['text':' skip key that does not exist in tf','line_number':2118,'multiline':False]
['text':' other general float inputs','line_number':2127,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':2138,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2143,'multiline':False]
['text':' Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences','line_number':2146,'multiline':False]
['text':' tf models returned loss is usually a tensor rather than a scalar.','line_number':2153,'multiline':False]
['text':' (see `hf_compute_loss`: it uses `tf.keras.losses.Reduction.NONE`)','line_number':2154,'multiline':False]
['text':' Change it here to a scalar to match PyTorch models' loss','line_number':2155,'multiline':False]
['text':' Add the "TF" at the beginning','line_number':2169,'multiline':False]
['text':' transformers does not have this model in TF version yet','line_number':2171,'multiline':False]
['text':' Output all for aggressive testing','line_number':2174,'multiline':False]
['text':' Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency','line_number':2178,'multiline':False]
['text':' of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.','line_number':2179,'multiline':False]
['text':' TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.','line_number':2180,'multiline':False]
['text':' Not all models accept "labels" in the forward pass (yet :) )','line_number':2192,'multiline':False]
['text':' make sure only tf inputs are forward that actually exist in function args','line_number':2196,'multiline':False]
['text':' remove all head masks','line_number':2199,'multiline':False]
['text':' For some models (e.g. base models), there is no label returned.','line_number':2207,'multiline':False]
['text':' Set the input dict to `None` to avoid check outputs twice for the same input dicts.','line_number':2208,'multiline':False]
['text':' Check we can load pt model in tf and vice-versa with model => model functions','line_number':2212,'multiline':False]
['text':' Here requires `tf_inputs_dict` to build `tf_model`','line_number':2213,'multiline':False]
['text':' Original test: check without `labels`','line_number':2222,'multiline':False]
['text':' check with `labels`','line_number':2224,'multiline':False]
['text':' Check we can load pt model in tf and vice-versa with checkpoint => model functions','line_number':2228,'multiline':False]
['text':' Original test: check without `labels`','line_number':2242,'multiline':False]
['text':' check with `labels`','line_number':2244,'multiline':False]
['text':' Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).','line_number':2267,'multiline':False]
['text':' convert to the case of `tuple`','line_number':2279,'multiline':False]
['text':' appending each key to the current (string) `name`','line_number':2280,'multiline':False]
['text':' Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)','line_number':2286,'multiline':False]
['text':' case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)','line_number':2296,'multiline':False]
['text':' case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `name`','line_number':2303,'multiline':False]
['text':' Using `np.asarray` gives `ValueError: assignment destination is read-only` at the line `fx_outputs[fx_nans] = 0`.','line_number':2314,'multiline':False]
['text':' deal with NumPy's scalars to make replacing nan values by 0 work.','line_number':2322,'multiline':False]
['text':' no flax model exists for this class','line_number':2354,'multiline':False]
['text':' Output all for aggressive testing','line_number':2357,'multiline':False]
['text':' load PyTorch class','line_number':2363,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':2365,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':2366,'multiline':False]
['text':' load Flax class','line_number':2369,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':2372,'multiline':False]
['text':' prepare inputs','line_number':2375,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':2378,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':2381,'multiline':False]
['text':' convert inputs to Flax','line_number':2386,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2392,'multiline':False]
['text':' no flax model exists for this class','line_number':2426,'multiline':False]
['text':' Output all for aggressive testing','line_number':2429,'multiline':False]
['text':' load PyTorch class','line_number':2435,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':2437,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':2438,'multiline':False]
['text':' load Flax class','line_number':2441,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':2444,'multiline':False]
['text':' prepare inputs','line_number':2447,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':2450,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':2453,'multiline':False]
['text':' convert inputs to Flax','line_number':2458,'multiline':False]
['text':' make sure weights are tied in PyTorch','line_number':2463,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2466,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2483,'multiline':False]
['text':' some params shouldn't be scattered by nn.DataParallel','line_number':2529,'multiline':False]
['text':' so just remove them if they are present.','line_number':2530,'multiline':False]
['text':' move input tensors to cuda:O','line_number':2535,'multiline':False]
['text':' Wrap model in nn.DataParallel','line_number':2545,'multiline':False]
['text':' a candidate for testing_utils','line_number':2555,'multiline':False]
['text':' Needs a large model to see the difference.','line_number':2566,'multiline':False]
['text':' 1. single gpu memory load + unload + memory measurements','line_number':2572,'multiline':False]
['text':' Retrieve initial memory usage (can easily be ~0.6-1.5GB if cuda-kernels have been preloaded by previous tests)','line_number':2573,'multiline':False]
['text':' Put model on device 0 and take a memory snapshot','line_number':2576,'multiline':False]
['text':' The memory use on device 0 should be higher than it was initially.','line_number':2581,'multiline':False]
['text':' 2. MP test','line_number':2588,'multiline':False]
['text':' it's essential to re-calibrate the usage before the next stage','line_number':2589,'multiline':False]
['text':' Spread model layers over multiple devices','line_number':2592,'multiline':False]
['text':' Assert that the memory use on all devices is higher than it was when loaded only on CPU','line_number':2597,'multiline':False]
['text':' Assert that the memory use of device 0 is lower than it was when the entire model was loaded on it','line_number':2601,'multiline':False]
['text':' Assert that the memory use of device 1 is higher than it was when the entire model was loaded','line_number':2604,'multiline':False]
['text':' on device 0 and device 1 wasn't used at all','line_number':2605,'multiline':False]
['text':' Find device in device_map','line_number':2648,'multiline':False]
['text':' This errors out cause it's missing an offload folder','line_number':2683,'multiline':False]
['text':' This doesn't error out as it's in safetensors and doesn't need an offload folder','line_number':2721,'multiline':False]
['text':' We test several splits of sizes to make sure it works.','line_number':2748,'multiline':False]
['text':' Making sure part of the model will actually end up offloaded','line_number':2756,'multiline':False]
['text':' We test several splits of sizes to make sure it works.','line_number':2784,'multiline':False]
['text':' Making sure part of the model will actually end up offloaded','line_number':2792,'multiline':False]
['text':' This tests that we do not trigger the warning form PyTorch "Using a target size that is different','line_number':2834,'multiline':False]
['text':' to the input size. This will likely lead to incorrect results due to broadcasting. Please ensure','line_number':2835,'multiline':False]
['text':' they have the same size." which is a symptom something in wrong for the regression problem.','line_number':2836,'multiline':False]
['text':' See https://github.com/huggingface/transformers/issues/11780','line_number':2837,'multiline':False]
['text':' Fails when we don't set ignore_mismatched_sizes=True','line_number':2862,'multiline':False]
['text':' Just a consistency check to make sure we are not running tests on 80M parameter models.','line_number':2893,'multiline':False]
['text':' check with inference + dropout','line_number':3019,'multiline':False]
['text':' make sure we do left padding','line_number':3137,'multiline':False]
['text':' make sure we do right padding','line_number':3181,'multiline':False]
['text':' Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.','line_number':3212,'multiline':False]
['text':' We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,','line_number':3282,'multiline':False]
['text':' but it would be nicer to have an efficient way to use parameterized.expand','line_number':3283,'multiline':False]
['text':' TODO: never an `attention_mask` arg here?','line_number':3358,'multiline':False]
['text':' Otherwise fails for e.g. WhisperEncoderModel','line_number':3369,'multiline':False]
['text':' TODO: test gradients as well (& for FA2 as well!)','line_number':3373,'multiline':False]
['text':' Masked tokens output slightly deviates - we don't mind that.','line_number':3401,'multiline':False]
['text':' Testing the padding tokens is not really meaningful but anyway','line_number':3418,'multiline':False]
['text':' sub_sdpa = logits_sdpa[-1, -4:]','line_number':3419,'multiline':False]
['text':' sub_eager = logits_eager[-1, -4:]','line_number':3420,'multiline':False]
['text':' if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):','line_number':3421,'multiline':False]
['text':'     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))','line_number':3422,'multiline':False]
['text':' Testing the padding tokens is not really meaningful but anyway','line_number':3438,'multiline':False]
['text':' sub_sdpa = logits_sdpa[-1, :3]','line_number':3439,'multiline':False]
['text':' sub_eager = logits_eager[-1, :3]','line_number':3440,'multiline':False]
['text':' if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):','line_number':3441,'multiline':False]
['text':'     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))','line_number':3442,'multiline':False]
['text':' make sure that all models have enough positions for generation','line_number':3470,'multiline':False]
['text':' Just test that a large cache works as expected','line_number':3510,'multiline':False]
['text':' make sure that all models have enough positions for generation','line_number':3538,'multiline':False]
['text':' Just test that a large cache works as expected','line_number':3556,'multiline':False]
['text':' upcast only layer norms','line_number':3597,'multiline':False]
['text':' with attention mask','line_number':3603,'multiline':False]
['text':' with attention mask','line_number':3612,'multiline':False]
['text':' Add the "TF" at the beginning','line_number':3620,'multiline':False]
['text':' transformers does not have this model in TF version yet','line_number':3622,'multiline':False]
['text':' Check models are equal','line_number':3636,'multiline':False]
['text':' Add the "Flax at the beginning','line_number':3645,'multiline':False]
['text':' transformers does not have this model in Flax version yet','line_number':3647,'multiline':False]
['text':' Check models are equal','line_number':3661,'multiline':False]
['text':' TODO: to change it in the future with other relevant auto classes','line_number':3674,'multiline':False]
['text':'  Creates a random int32 tensor of the shape within the vocab size','line_number':3714,'multiline':False]
['text':' make sure that at least one token is attended to for each batch','line_number':3731,'multiline':False]
['text':' we choose the 1st token so this property of `at least one being non-zero` still holds after applying causal mask','line_number':3732,'multiline':False]
