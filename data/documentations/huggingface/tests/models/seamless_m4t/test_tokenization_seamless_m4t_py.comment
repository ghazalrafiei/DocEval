['text':' Copyright 2022 The HuggingFace Team. All rights reserved.','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':3,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':4,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':7,'multiline':False]
['text':'','line_number':8,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':9,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':10,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':11,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':12,'multiline':False]
['text':' limitations under the License.','line_number':13,'multiline':False]
['text':' We have a SentencePiece fixture for testing','line_number':65,'multiline':False]
['text':' Test with max model input length','line_number':157,'multiline':False]
['text':' Simple','line_number':170,'multiline':False]
['text':' Simple with no truncation','line_number':184,'multiline':False]
['text':' Reset warnings','line_number':185,'multiline':False]
['text':' Overflowing tokens','line_number':210,'multiline':False]
['text':' modify padding because it's activated by default in seamlessM4T','line_number':213,'multiline':False]
['text':' add_prefix_space=False,','line_number':222,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':225,'multiline':False]
['text':' default to padding=True so need to precise which padding is called','line_number':264,'multiline':False]
['text':' Should also work with truncation','line_number':269,'multiline':False]
['text':' truncation to something which is not a multiple of pad_to_multiple_of raises an error','line_number':274,'multiline':False]
['text':' Longer text that will definitely require truncation.','line_number':293,'multiline':False]
['text':' TODO: not working for tgt_text','line_number':322,'multiline':False]
['text':' max_target_length will default to max_length if not specified','line_number':323,'multiline':False]
['text':' Copied from tests.models.nllb.test_tokenization_nllb.NllbTokenizationTest.test_special_tokens_initialization','line_number':349,'multiline':False]
['text':' , from_slow=True <- unfortunately too slow to convert','line_number':368,'multiline':False]
['text':' This feature only exists for fast tokenizers','line_number':390,'multiline':False]
['text':' Test we can use the new tokenizer with something not seen during training','line_number':397,'multiline':False]
['text':' We check that the parameters of the tokenizer remained the same','line_number':407,'multiline':False]
['text':' Check we have the same number of added_tokens for both pair and non-pair inputs.','line_number':408,'multiline':False]
['text':' make sure it has the same prefix tokens first','line_number':409,'multiline':False]
['text':' Check we have the correct max_length for both pair and non-pair inputs.','line_number':415,'multiline':False]
['text':' Assert the set of special tokens match as we didn't ask to change them','line_number':419,'multiline':False]
['text':' fmt: skip','line_number':452,'multiline':False]
['text':' cls.pad_token_id = 1','line_number':459,'multiline':False]
['text':' Copied from tests.models.nllb.test_tokenization_nllb.NllbDistilledIntegrationTest.test_enro_tokenizer_decode_ignores_language_codes','line_number':481,'multiline':False]
['text':' fmt: skip','line_number':484,'multiline':False]
['text':' Copied from tests.models.nllb.test_tokenization_nllb.NllbDistilledIntegrationTest.test_special_tokens_unaffacted_by_save_load with fairseq_tokens_to_ids->additional_special_tokens, Nllb->SeamlessM4T, Dict->List','line_number':500,'multiline':False]
['text':' EOS','line_number':529,'multiline':False]
['text':' Test that special tokens are reset','line_number':530,'multiline':False]
['text':' A, test, EOS, en_XX','line_number':560,'multiline':False]
['text':' ar_AR','line_number':563,'multiline':False]
['text':' make sure `'▁'` is prepended, and outputs match sp_model's','line_number':584,'multiline':False]
['text':' `sentencepiece.NormalizerSpec.add_dummy_prefix` attribute','line_number':585,'multiline':False]
['text':' [bos, lang_id, _] + offset_sp_encode','line_number':590,'multiline':False]
['text':' make sure the extra spaces are eaten. Since the sample vocab does not have','line_number':608,'multiline':False]
['text':' `______`. sentencepiece.NormalizerSpec.remove_extra_whitespaces attribute is set to False','line_number':609,'multiline':False]
['text':' `'▁'` is also a whitespace','line_number':618,'multiline':False]
['text':' no extra space added','line_number':628,'multiline':False]
['text':' spaces are eaten by spm + our strip','line_number':633,'multiline':False]
['text':' make sure that the output after the extra id is the same as if','line_number':634,'multiline':False]
['text':' extra_id was not there','line_number':635,'multiline':False]
['text':' spaces are eaten by spm even if not start','line_number':639,'multiline':False]
['text':' Make sure that `tokenizer.tokenize` is similar to','line_number':642,'multiline':False]
['text':' adding the equivalent special token to the vocab','line_number':643,'multiline':False]
['text':' the last token besides eos should be 100 offset','line_number':648,'multiline':False]
['text':' spaces are eaten by rstrip / lstrip + spm sp_model.encode("  ") = []','line_number':662,'multiline':False]
['text':' spaces are eaten by rstrip / lstrip','line_number':668,'multiline':False]
