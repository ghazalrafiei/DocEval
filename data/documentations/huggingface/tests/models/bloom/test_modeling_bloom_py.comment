['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':'','line_number':15,'multiline':False]
['text':' first forward pass','line_number':154,'multiline':False]
['text':' create hypothetical next token and extent to next_input_ids','line_number':164,'multiline':False]
['text':' append to next input_ids and token_type_ids','line_number':167,'multiline':False]
['text':' select random slice','line_number':173,'multiline':False]
['text':' test that outputs are equal for slice','line_number':178,'multiline':False]
['text':' create attention mask','line_number':186,'multiline':False]
['text':' first forward pass','line_number':191,'multiline':False]
['text':' create hypothetical next token and extent to next_input_ids','line_number':194,'multiline':False]
['text':' change a random masked slice from input_ids','line_number':197,'multiline':False]
['text':' append to next input_ids and attn_mask','line_number':202,'multiline':False]
['text':' get two different outputs','line_number':209,'multiline':False]
['text':' select random slice','line_number':213,'multiline':False]
['text':' test that outputs are equal for slice','line_number':218,'multiline':False]
['text':' first forward pass','line_number':226,'multiline':False]
['text':' create hypothetical next token and extent to next_input_ids','line_number':231,'multiline':False]
['text':' append to next input_ids and token_type_ids','line_number':235,'multiline':False]
['text':' select random slice','line_number':245,'multiline':False]
['text':' test that outputs are equal for slice','line_number':250,'multiline':False]
['text':' torch.autograd functions seems to be not supported','line_number':348,'multiline':False]
['text':' This test is a bit flaky. For some GPU architectures, pytorch sets by default allow_fp16_reduced_precision_reduction = True and some operations','line_number':406,'multiline':False]
['text':' do not give the same results under this configuration, especially torch.baddmm and torch.bmm. https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200','line_number':407,'multiline':False]
['text':' As we leave the default value (True) for allow_fp16_reduced_precision_reduction , the tests failed when running in half-precision with smaller models (560m)','line_number':408,'multiline':False]
['text':' Please see: https://pytorch.org/docs/stable/notes/cuda.html#reduced-precision-reduction-in-fp16-gemms','line_number':409,'multiline':False]
['text':' This discrepancy is observed only when using small models and seems to be stable for larger models.','line_number':410,'multiline':False]
['text':' Our conclusion is that these operations are flaky for small inputs but seems to be stable for larger inputs (for the functions `baddmm` and `bmm`), and therefore for larger models.','line_number':411,'multiline':False]
['text':' Here is a summary of an ablation study of our observations','line_number':413,'multiline':False]
['text':' EXPECTED_OUTPUT = "I enjoy walking with my cute dog, and I love to watch the kids play. I am a very active person, and I am a very good listener. I am a very good person, and I am a very good person. I am a"','line_number':414,'multiline':False]
['text':' 560m + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS','line_number':415,'multiline':False]
['text':' 560m + allow_fp16_reduced_precision_reduction = False  + torch.baddm  ==> PASS','line_number':416,'multiline':False]
['text':' 560m + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS','line_number':417,'multiline':False]
['text':' 560m + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> FAIL','line_number':418,'multiline':False]
['text':' EXPECTED_OUTPUT = "I enjoy walking with my cute dog, but I also enjoy hiking, biking, and swimming. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love"','line_number':420,'multiline':False]
['text':' >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS  (for use_cache=True and use_cache=False)','line_number':421,'multiline':False]
['text':' >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> PASS','line_number':422,'multiline':False]
['text':' >=1b1 + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS','line_number':423,'multiline':False]
['text':' This output has been obtained using fp32 model on the huggingface DGX workstation - NVIDIA A100 GPU','line_number':431,'multiline':False]
['text':' test token values','line_number':482,'multiline':False]
['text':' test reconstructions','line_number':485,'multiline':False]
['text':' these generations match those of the PyTorch model','line_number':510,'multiline':False]
['text':' The config in this checkpoint has `bfloat16` as `torch_dtype` -> model in `bfloat16`','line_number':548,'multiline':False]
['text':' fmt: skip','line_number':702,'multiline':False]
['text':'','line_number':753,'multiline':False]
['text':' first check the embeddings before LN','line_number':754,'multiline':False]
['text':' This test does not pass when places = 2','line_number':770,'multiline':False]
['text':' fmt: skip','line_number':783,'multiline':False]
['text':' load in bf16','line_number':808,'multiline':False]
['text':' fmt: skip','line_number':811,'multiline':False]
['text':' 1e-06 precision!!','line_number':825,'multiline':False]
