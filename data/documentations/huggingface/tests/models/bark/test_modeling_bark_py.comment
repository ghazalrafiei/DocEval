['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' for now training is not supported','line_number':72,'multiline':False]
['text':' for BarkFineModel','line_number':84,'multiline':False]
['text':' for BarkFineModel','line_number':85,'multiline':False]
['text':' first forward pass','line_number':160,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':165,'multiline':False]
['text':' append to next input_ids and','line_number':169,'multiline':False]
['text':' select random slice','line_number':178,'multiline':False]
['text':' test that outputs are equal for slice','line_number':185,'multiline':False]
['text':' test no attention_mask works','line_number':188,'multiline':False]
['text':' test that outputs are equal for slice','line_number':198,'multiline':False]
['text':' for now training is not supported','line_number':208,'multiline':False]
['text':' for BarkFineModel','line_number':220,'multiline':False]
['text':' for BarkFineModel','line_number':221,'multiline':False]
['text':' first forward pass','line_number':296,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':301,'multiline':False]
['text':' append to next input_ids and','line_number':305,'multiline':False]
['text':' select random slice','line_number':314,'multiline':False]
['text':' test that outputs are equal for slice','line_number':321,'multiline':False]
['text':' test no attention_mask works','line_number':324,'multiline':False]
['text':' test that outputs are equal for slice','line_number':334,'multiline':False]
['text':' for now training is not supported','line_number':344,'multiline':False]
['text':' for BarkFineModel','line_number':356,'multiline':False]
['text':' for BarkFineModel','line_number':357,'multiline':False]
['text':' randint between self.n_codes_given - 1 and self.n_codes_total - 1','line_number':394,'multiline':False]
['text':' first forward pass','line_number':436,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':441,'multiline':False]
['text':' append to next input_ids and','line_number':445,'multiline':False]
['text':' select random slice','line_number':454,'multiline':False]
['text':' test that outputs are equal for slice','line_number':461,'multiline':False]
['text':' test no attention_mask works','line_number':464,'multiline':False]
['text':' test that outputs are equal for slice','line_number':474,'multiline':False]
['text':' for now training is not supported','line_number':486,'multiline':False]
['text':' follow the `get_pipeline_config` of the sub component models','line_number':516,'multiline':False]
['text':' no model_parallel for now','line_number':538,'multiline':False]
['text':' Same tester as BarkSemanticModelTest, except for model_class and config_class','line_number':595,'multiline':False]
['text':' no model_parallel for now','line_number':604,'multiline':False]
['text':' no model_parallel for now','line_number':667,'multiline':False]
['text':' torchscript disabled for now because forward with an int','line_number':670,'multiline':False]
['text':' take first codebook channel','line_number':716,'multiline':False]
['text':' toy generation_configs','line_number':721,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':757,'multiline':False]
['text':' one embedding layer per codebook','line_number':764,'multiline':False]
['text':' resizing tokens_embeddings of a ModuleList','line_number':777,'multiline':False]
['text':' Retrieve the embeddings and clone theme','line_number':791,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':795,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix for each codebook','line_number':799,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':803,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':806,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':812,'multiline':False]
['text':' Input ids should be clamped to the maximum size of the vocabulary','line_number':813,'multiline':False]
['text':' Check that adding and removing tokens has not modified the first part of the embedding matrix.','line_number':818,'multiline':False]
['text':' only check for the first embedding matrix','line_number':819,'multiline':False]
['text':' resizing tokens_embeddings of a ModuleList','line_number':828,'multiline':False]
['text':' if no output embeddings -> leave test','line_number':839,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':843,'multiline':False]
['text':' Check bias if present','line_number':852,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':856,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':859,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':862,'multiline':False]
['text':' Check bias if present','line_number':867,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':871,'multiline':False]
['text':' Input ids should be clamped to the maximum size of the vocabulary','line_number':872,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':875,'multiline':False]
['text':' check with inference + dropout','line_number':931,'multiline':False]
['text':' check first ids','line_number':1031,'multiline':False]
['text':' fmt: skip','line_number':1032,'multiline':False]
['text':' greedy decoding','line_number':1034,'multiline':False]
['text':' check first ids','line_number':1049,'multiline':False]
['text':' fmt: skip','line_number':1050,'multiline':False]
['text':' Should be able to read min_eos_p from kwargs','line_number':1052,'multiline':False]
['text':' Should be able to read min_eos_p from the semantic generation config','line_number':1072,'multiline':False]
['text':' check first ids','line_number':1093,'multiline':False]
['text':' fmt: skip','line_number':1094,'multiline':False]
['text':' fmt: off','line_number':1122,'multiline':False]
['text':' fmt: on','line_number':1133,'multiline':False]
['text':' greedy decoding','line_number':1153,'multiline':False]
['text':' generate in batch','line_number':1191,'multiline':False]
['text':' generate one-by-one','line_number':1194,'multiline':False]
['text':' up until the coarse acoustic model (included), results are the same','line_number':1200,'multiline':False]
['text':' the fine acoustic model introduces small differences','line_number':1201,'multiline':False]
['text':' first verify if same length (should be the same because it's decided in the coarse model)','line_number':1202,'multiline':False]
['text':' then assert almost equal','line_number':1205,'multiline':False]
['text':' now test single input with return_output_lengths = True','line_number':1209,'multiline':False]
['text':' standard generation','line_number':1249,'multiline':False]
['text':' activate cpu offload','line_number':1257,'multiline':False]
['text':' checks if the model have been offloaded','line_number':1262,'multiline':False]
['text':' CUDA memory usage after offload should be near 0, leaving room to small differences','line_number':1264,'multiline':False]
['text':' checks if device is the correct one','line_number':1270,'multiline':False]
['text':' checks if hooks exist','line_number':1273,'multiline':False]
['text':' output with cpu offload','line_number':1276,'multiline':False]
['text':' checks if same output','line_number':1279,'multiline':False]
