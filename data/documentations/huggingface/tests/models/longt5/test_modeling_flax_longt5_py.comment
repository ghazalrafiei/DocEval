['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 Google LongT5 Authors and HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' The slow tests are often failing with OOM error on GPU','line_number':39,'multiline':False]
['text':' This makes JAX allocate exactly what is needed on demand, and deallocate memory that is no longer needed','line_number':40,'multiline':False]
['text':' but will be slower as stated here https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html','line_number':41,'multiline':False]
['text':' For common tests','line_number':69,'multiline':False]
['text':' For common tests','line_number':94,'multiline':False]
['text':' prevent fully zero'd out attention mask','line_number':187,'multiline':False]
['text':' check that gated gelu feed forward and different word embeddings work','line_number':256,'multiline':False]
['text':' overwrite since special base model prefix is used','line_number':340,'multiline':False]
['text':' check that all base model weights are loaded correctly','line_number':352,'multiline':False]
['text':' overwrite since special base model prefix is used','line_number':363,'multiline':False]
['text':' check that all base model weights are loaded correctly','line_number':375,'multiline':False]
['text':' check that output_attentions also work using config','line_number':405,'multiline':False]
['text':' Question Answering model returns start_logits and end_logits','line_number':422,'multiline':False]
['text':' start_logits and end_logits instead of only 1 output','line_number':424,'multiline':False]
['text':' decoder attentions','line_number':428,'multiline':False]
['text':' cross attentions','line_number':437,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':450,'multiline':False]
['text':' overwrite since special base model prefix is used','line_number':472,'multiline':False]
['text':' convert Flax model to PyTorch model','line_number':485,'multiline':False]
['text':' Skip the "Flax" at the beginning','line_number':486,'multiline':False]
['text':' check that all base model weights are loaded correctly','line_number':490,'multiline':False]
['text':' save pt model','line_number':492,'multiline':False]
['text':' overwrite since special base model prefix is used','line_number':502,'multiline':False]
['text':' convert Flax model to PyTorch model','line_number':515,'multiline':False]
['text':' Skip the "Flax" at the beginning','line_number':516,'multiline':False]
['text':' check that all base model weights are loaded correctly','line_number':520,'multiline':False]
['text':' overwrite since special base model prefix is used','line_number':531,'multiline':False]
['text':' convert Flax model to PyTorch model','line_number':545,'multiline':False]
['text':' Skip the "Flax" at the beginning','line_number':546,'multiline':False]
['text':' check that all base model weights are loaded correctly','line_number':550,'multiline':False]
['text':' check that output_attentions also work using config','line_number':588,'multiline':False]
['text':' Question Answering model returns start_logits and end_logits','line_number':605,'multiline':False]
['text':' start_logits and end_logits instead of only 1 output','line_number':607,'multiline':False]
['text':' decoder attentions','line_number':611,'multiline':False]
['text':' cross attentions','line_number':620,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':633,'multiline':False]
