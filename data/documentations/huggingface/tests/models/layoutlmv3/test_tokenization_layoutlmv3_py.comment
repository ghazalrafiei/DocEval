['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' determined by the tokenization algortihm and the way it's decoded by the fast tokenizers','line_number':55,'multiline':False]
['text':' Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt','line_number':95,'multiline':False]
['text':' , add_prefix_space=True)','line_number':146,'multiline':False]
['text':' We usually have added tokens from the start in tests because our vocab fixtures are','line_number':195,'multiline':False]
['text':' smaller than the original vocabs - let's not assert this','line_number':196,'multiline':False]
['text':' self.assertEqual(vocab_size, all_size)','line_number':197,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':277,'multiline':False]
['text':' Test 'longest' and 'no_padding' don't do anything','line_number':288,'multiline':False]
['text':' Test right padding','line_number':321,'multiline':False]
['text':' Test left padding','line_number':340,'multiline':False]
['text':' test 1: single sequence','line_number':412,'multiline':False]
['text':' Method is implemented (e.g. not GPT-2)','line_number':418,'multiline':False]
['text':' test 2: two sequences','line_number':424,'multiline':False]
['text':' Method is implemented (e.g. not GPT-2)','line_number':430,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':444,'multiline':False]
['text':' Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':449,'multiline':False]
['text':' FIXME: the next line should be padding(max_length) to avoid warning','line_number':453,'multiline':False]
['text':' Check that nothing is done when a maximum length is not specified','line_number':461,'multiline':False]
['text':' Encode - Simple input','line_number':480,'multiline':False]
['text':' Encode - Pair input','line_number':493,'multiline':False]
['text':' Encode_plus - Simple input','line_number':509,'multiline':False]
['text':' Encode_plus - Pair input','line_number':528,'multiline':False]
['text':' Batch_encode_plus - Simple input','line_number':553,'multiline':False]
['text':' Batch_encode_plus - Pair input','line_number':602,'multiline':False]
['text':' Using pad on single examples after tokenization','line_number':637,'multiline':False]
['text':' Using pad on single examples after tokenization','line_number':649,'multiline':False]
['text':' Using pad after tokenization','line_number':658,'multiline':False]
['text':' Using pad after tokenization','line_number':674,'multiline':False]
['text':' We want to assert there are no warnings, but the 'assertLogs' method does not support that.','line_number':723,'multiline':False]
['text':' Therefore, we are adding a dummy warning, and then we will assert it is the only warning.','line_number':724,'multiline':False]
['text':' Tests that all call wrap to encode_plus and batch_encode_plus','line_number':734,'multiline':False]
['text':' Test not batched','line_number':738,'multiline':False]
['text':' Test not batched pairs','line_number':744,'multiline':False]
['text':' Test batched','line_number':750,'multiline':False]
['text':' Tests that all encoded values have the correct size','line_number':757,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':776,'multiline':False]
['text':' check 'longest' is unsensitive to a max length','line_number':794,'multiline':False]
['text':' check 'no_padding' is unsensitive to a max length','line_number':807,'multiline':False]
['text':' Test that padded sequences are equivalent between batch_encode_plus and encode_plus','line_number':825,'multiline':False]
['text':' Right padding tests','line_number':827,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':835,'multiline':False]
['text':' Left padding tests','line_number':851,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':860,'multiline':False]
['text':' empty_tokens = tokenizer([""], [[]], padding=True, pad_to_multiple_of=8)','line_number':885,'multiline':False]
['text':' for key, value in empty_tokens.items():','line_number':887,'multiline':False]
['text':'     self.assertEqual(len(value) % 8, 0, f"BatchEncoding.{key} is not multiple of 8")','line_number':888,'multiline':False]
['text':' Should also work with truncation','line_number':896,'multiline':False]
['text':' truncation to something which is not a multiple of pad_to_multiple_of raises an error','line_number':901,'multiline':False]
['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':923,'multiline':False]
['text':' Input tokens id','line_number':931,'multiline':False]
['text':' Generate output','line_number':936,'multiline':False]
['text':' Generate pair output','line_number':941,'multiline':False]
['text':' add_prefix_space=False,','line_number':957,'multiline':False]
['text':' Testing single inputs','line_number':974,'multiline':False]
['text':' safety check on max_len default value so we are sure the test works','line_number':987,'multiline':False]
['text':' Now let's start the test','line_number':993,'multiline':False]
['text':' Isolate this from the other tests because we save additional tokens/etc','line_number':997,'multiline':False]
['text':' check correct behaviour if no pad_token_id exists and add it eventually','line_number':1021,'multiline':False]
['text':' RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1026,'multiline':False]
['text':' LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True','line_number':1037,'multiline':False]
['text':' RIGHT & LEFT PADDING - Check that nothing is done for 'longest' and 'no_padding'','line_number':1048,'multiline':False]
['text':' test 1: single sequence','line_number':1080,'multiline':False]
['text':' Assert that the token type IDs have the same length as the input IDs','line_number':1085,'multiline':False]
['text':' Assert that the token type IDs have the same length as the attention mask','line_number':1088,'multiline':False]
['text':' test 2: two sequences (question + words)','line_number':1094,'multiline':False]
['text':' Assert that the token type IDs have the same length as the input IDs','line_number':1099,'multiline':False]
['text':' Assert that the token type IDs have the same length as the attention mask','line_number':1102,'multiline':False]
['text':' No pair','line_number':1115,'multiline':False]
['text':' Assert there is the same number of tokens and offsets','line_number':1126,'multiline':False]
['text':' Assert there is online added_tokens special_tokens','line_number':1129,'multiline':False]
['text':' Pairs','line_number':1132,'multiline':False]
['text':' Assert there is the same number of tokens and offsets','line_number':1147,'multiline':False]
['text':' Assert there is online added_tokens special_tokens','line_number':1150,'multiline':False]
['text':' Make sure the model contains at least the full vocabulary size in its embedding matrix','line_number':1176,'multiline':False]
['text':' Build sequence','line_number':1184,'multiline':False]
['text':' We add dummy pixel_values keys (as LayoutLMv3 actually also requires a feature extractor','line_number':1191,'multiline':False]
['text':' to prepare the image input)','line_number':1192,'multiline':False]
['text':' This should not fail','line_number':1196,'multiline':False]
['text':' saves some time','line_number':1197,'multiline':False]
['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1206,'multiline':False]
['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1224,'multiline':False]
['text':' Ensure basic input match','line_number':1234,'multiline':False]
['text':' Ensure truncation match','line_number':1254,'multiline':False]
['text':' Ensure truncation with stride match','line_number':1263,'multiline':False]
['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1278,'multiline':False]
['text':' tokenize()','line_number':1315,'multiline':False]
['text':' encode()','line_number':1320,'multiline':False]
['text':' encode_plus()','line_number':1325,'multiline':False]
['text':' # batch_encode_plus','line_number':1334,'multiline':False]
['text':' Ensure that the input IDs are less than the max length defined.','line_number':1352,'multiline':False]
['text':' Ensure that the input IDs are still truncated when no max_length is specified','line_number':1359,'multiline':False]
['text':' A Tensor cannot be build by sequences which are not the same size','line_number':1370,'multiline':False]
['text':' We want to have sequence 0 and sequence 1 are tagged','line_number':1415,'multiline':False]
['text':' respectively with 0 and 1 token_ids','line_number':1416,'multiline':False]
['text':' (regardless of whether the model use token type ids)','line_number':1417,'multiline':False]
['text':' We use this assumption in the QA pipeline among other place','line_number':1418,'multiline':False]
['text':' This feature only exists for fast tokenizers','line_number':1467,'multiline':False]
['text':' Test we can use the new tokenizer with something not seen during training','line_number':1474,'multiline':False]
['text':' We check that the parameters of the tokenizer remained the same','line_number':1486,'multiline':False]
['text':' Check we have the same number of added_tokens for both pair and non-pair inputs.','line_number':1487,'multiline':False]
['text':' Check we have the correct max_length for both pair and non-pair inputs.','line_number':1491,'multiline':False]
['text':' Assert the set of special tokens match as we didn't ask to change them','line_number':1495,'multiline':False]
['text':' This feature only exists for fast tokenizers','line_number':1504,'multiline':False]
['text':' Test with a special tokens map','line_number':1509,'multiline':False]
['text':' Create a new mapping from the special tokens defined in the original tokenizer','line_number':1519,'multiline':False]
['text':' Get the private one to avoid unnecessary warnings.','line_number':1524,'multiline':False]
['text':' Train new tokenizer','line_number':1529,'multiline':False]
['text':' Check the changes','line_number':1534,'multiline':False]
['text':' Get the private one to avoid unnecessary warnings.','line_number':1536,'multiline':False]
['text':' Check if the AddedToken / string format has been kept','line_number':1547,'multiline':False]
['text':' The special token must appear identically in the list of the new tokenizer.','line_number':1550,'multiline':False]
['text':' The special token must appear in the list of the new tokenizer as an object of type AddedToken with','line_number':1556,'multiline':False]
['text':' the same parameters as the old AddedToken except the content that the user has requested to change.','line_number':1557,'multiline':False]
['text':' The special token must appear identically in the list of the new tokenizer.','line_number':1580,'multiline':False]
['text':' The special token must appear in the list of the new tokenizer as an object of type string.','line_number':1587,'multiline':False]
['text':' Test we can use the new tokenizer with something not seen during training','line_number':1590,'multiline':False]
['text':' only test prepare_for_model for the slow tokenizer','line_number':1605,'multiline':False]
['text':' as we don't have a slow version, we can't compare the outputs between slow and fast versions','line_number':1618,'multiline':False]
['text':' rename encoded batch to "inputs"','line_number':1633,'multiline':False]
['text':' Renaming `input_ids` to `inputs`','line_number':1640,'multiline':False]
['text':' Single example','line_number':1674,'multiline':False]
['text':' Batch of examples','line_number':1693,'multiline':False]
['text':' For these 2 examples, 3 training examples will be created','line_number':1694,'multiline':False]
['text':' toks_str = [t[1] for t in toks]','line_number':1737,'multiline':False]
['text':' Ensure consistency','line_number':1740,'multiline':False]
['text':' Build a sequence from our model's vocabulary','line_number':1790,'multiline':False]
['text':' We are not using the special tokens - a bit too hard to test all the tokenizers with this','line_number':1818,'multiline':False]
['text':' TODO try this again later','line_number':1819,'multiline':False]
['text':' , add_prefix_space=False)','line_number':1822,'multiline':False]
['text':' Test with max model input length','line_number':1824,'multiline':False]
['text':' Simple','line_number':1841,'multiline':False]
['text':' Simple','line_number':1869,'multiline':False]
['text':' Simple with no truncation','line_number':1882,'multiline':False]
['text':' Reset warnings','line_number':1883,'multiline':False]
['text':' Check the order of Sequence of input ids, overflowing tokens and bbox sequence with truncation','line_number':1913,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':1967,'multiline':False]
['text':' add_prefix_space=False,','line_number':1978,'multiline':False]
['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':1996,'multiline':False]
['text':' add_prefix_space=False,','line_number':2007,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2018,'multiline':False]
['text':' add_prefix_space=False,','line_number':2029,'multiline':False]
['text':' No overflowing tokens when using 'longest' in python tokenizers','line_number':2045,'multiline':False]
['text':' add_prefix_space=False,','line_number':2056,'multiline':False]
['text':' add_prefix_space=False,','line_number':2076,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2078,'multiline':False]
['text':' add_prefix_space=False,','line_number':2116,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2118,'multiline':False]
['text':' Test with max model input length','line_number':2161,'multiline':False]
['text':' Simple','line_number':2174,'multiline':False]
['text':' Simple with no truncation','line_number':2201,'multiline':False]
['text':' Reset warnings','line_number':2202,'multiline':False]
['text':' Check the order of Sequence of input ids, overflowing tokens and bbox sequence with truncation','line_number':2228,'multiline':False]
['text':' add_prefix_space=False,','line_number':2238,'multiline':False]
['text':' Overflowing tokens are handled quite differently in slow and fast tokenizers','line_number':2241,'multiline':False]
['text':' bbox = information["bbox"][0]','line_number':2245,'multiline':False]
['text':' overflowing_bbox = information["bbox"][1]','line_number':2246,'multiline':False]
['text':' self.assertEqual(bbox, sequence["bbox"][:-2])','line_number':2255,'multiline':False]
['text':' self.assertEqual(overflowing_bbox, sequence["bbox"][-(2 + stride) :])','line_number':2256,'multiline':False]
['text':' bbox = information["bbox"]','line_number':2260,'multiline':False]
['text':' overflowing_bbox = information["overflowing_token_boxes"]','line_number':2261,'multiline':False]
['text':' self.assertEqual(bbox, sequence["bbox"][:-2])','line_number':2267,'multiline':False]
['text':' self.assertEqual(overflowing_bbox, sequence["bbox"][-(2 + stride) :])','line_number':2268,'multiline':False]
['text':' test slow tokenizer','line_number':2288,'multiline':False]
['text':' test fast tokenizer','line_number':2301,'multiline':False]
['text':' There are 3 cases:','line_number':2319,'multiline':False]
['text':' CASE 1: document image classification (training + inference), document image token classification (inference),','line_number':2320,'multiline':False]
['text':' in which case only words and normalized bounding boxes are provided to the tokenizer','line_number':2321,'multiline':False]
['text':' CASE 2: document image token classification (training),','line_number':2322,'multiline':False]
['text':' in which case one also provides word labels to the tokenizer','line_number':2323,'multiline':False]
['text':' CASE 3: document image visual question answering (inference),','line_number':2324,'multiline':False]
['text':' in which case one also provides a question to the tokenizer','line_number':2325,'multiline':False]
['text':' We need to test all 3 cases both on batched and non-batched inputs.','line_number':2327,'multiline':False]
['text':' CASE 1: not batched','line_number':2329,'multiline':False]
['text':' fmt: skip','line_number':2332,'multiline':False]
['text':' CASE 1: batched','line_number':2339,'multiline':False]
['text':' fmt: skip','line_number':2342,'multiline':False]
['text':' CASE 2: not batched','line_number':2349,'multiline':False]
['text':' fmt: skip','line_number':2353,'multiline':False]
['text':' # CASE 2: batched','line_number':2360,'multiline':False]
['text':' fmt: skip','line_number':2364,'multiline':False]
['text':' # CASE 3: not batched','line_number':2371,'multiline':False]
['text':' fmt: skip','line_number':2374,'multiline':False]
['text':' # CASE 3: batched','line_number':2381,'multiline':False]
['text':' fmt: skip','line_number':2384,'multiline':False]
['text':' Make sure the model contains at least the full vocabulary size in its embedding matrix','line_number':2416,'multiline':False]
['text':' Build sequence','line_number':2419,'multiline':False]
['text':' This should not fail','line_number':2427,'multiline':False]
