['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2021 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' "input_ids": input_features,','line_number':70,'multiline':False]
['text':' first forward pass','line_number':196,'multiline':False]
['text':' first forward pass','line_number':206,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':211,'multiline':False]
['text':' append to next input_ids and','line_number':215,'multiline':False]
['text':' select random slice','line_number':224,'multiline':False]
['text':' test that outputs are equal for slice','line_number':231,'multiline':False]
['text':' not implemented currently','line_number':317,'multiline':False]
['text':' training is not supported yet','line_number':321,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':357,'multiline':False]
['text':' check that output_hidden_states also work using config','line_number':420,'multiline':False]
['text':' check that output_attentions also work using config','line_number':452,'multiline':False]
['text':' loss is at first position','line_number':471,'multiline':False]
['text':' loss is added to beginning','line_number':473,'multiline':False]
['text':' past_key_values have been returned','line_number':475,'multiline':False]
['text':' decoder attentions','line_number':479,'multiline':False]
['text':' cross attentions','line_number':488,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':501,'multiline':False]
['text':' Retrieve the embeddings and clone theme','line_number':538,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':542,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':545,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':547,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':550,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':553,'multiline':False]
['text':' make sure that decoder_input_ids are resized','line_number':556,'multiline':False]
['text':' Check that adding and removing tokens has not modified the first part of the embedding matrix.','line_number':561,'multiline':False]
['text':' if model cannot untied embeddings -> leave test','line_number':579,'multiline':False]
['text':' if no output embeddings -> leave test','line_number':587,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':591,'multiline':False]
['text':' Check bias if present','line_number':597,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':600,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':603,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':606,'multiline':False]
['text':' Check bias if present','line_number':609,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':612,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':615,'multiline':False]
['text':' scores','line_number':648,'multiline':False]
['text':' Attentions','line_number':651,'multiline':False]
['text':' encoder','line_number':652,'multiline':False]
['text':' decoder','line_number':656,'multiline':False]
['text':' Hidden States','line_number':666,'multiline':False]
['text':' encoder','line_number':667,'multiline':False]
['text':' decoder','line_number':672,'multiline':False]
['text':' To be sure we have no Nan','line_number':686,'multiline':False]
['text':' FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward','line_number':695,'multiline':False]
['text':' Allow missing keys since TF doesn't cache the sinusoidal embeddings in an attribute','line_number':759,'multiline':False]
['text':' automatic decoding with librispeech','line_number':781,'multiline':False]
