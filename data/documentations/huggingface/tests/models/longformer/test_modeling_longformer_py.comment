['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' `ModelTesterMixin.test_attention_outputs` is expecting attention tensors to be of size','line_number':92,'multiline':False]
['text':' [num_attention_heads, encoder_seq_length, encoder_key_length], but LongformerSelfAttention','line_number':93,'multiline':False]
['text':' returns attention of shape [num_attention_heads, encoder_seq_length, self.attention_window + 1]','line_number':94,'multiline':False]
['text':' because its local attention only attends to `self.attention_window + 1` locations','line_number':95,'multiline':False]
['text':' (assuming no token with global attention, otherwise the last dimension of attentions','line_number':96,'multiline':False]
['text':' is x + self.attention_window + 1, where x is the number of tokens with global attention)','line_number':97,'multiline':False]
['text':' Replace sep_token_id by some random id','line_number':290,'multiline':False]
['text':' Make sure there are exactly three sep_token_id','line_number':292,'multiline':False]
['text':' pruning is not supported','line_number':301,'multiline':False]
['text':' Need to use `0.6` instead of `0.5` for `test_disk_offload`','line_number':329,'multiline':False]
['text':' TODO: Fix the failed tests','line_number':332,'multiline':False]
['text':' `QAPipelineTests` fails for a few models when the slower tokenizer are used.','line_number':341,'multiline':False]
['text':' (The slower tokenizers were never used for pipeline tests before the pipeline testing rework)','line_number':342,'multiline':False]
['text':' TODO: check (and possibly fix) the `QAPipelineTests` with slower tokenizer','line_number':343,'multiline':False]
['text':' longformer cannot keep gradients in attentions or hidden states','line_number':388,'multiline':False]
['text':' set seq length = 8, hidden dim = 4','line_number':448,'multiline':False]
['text':' first row => [0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000]','line_number':457,'multiline':False]
['text':' last row => [0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629]','line_number':466,'multiline':False]
['text':' expected slices across chunk and seq length dim','line_number':497,'multiline':False]
['text':' create attn mask','line_number':576,'multiline':False]
['text':' create attn mask','line_number':627,'multiline':False]
['text':' All tokens with global attention have weight 0 in local attentions.','line_number':648,'multiline':False]
['text':' The weight of all tokens with local attention must sum to 1.','line_number':652,'multiline':False]
['text':' All the global attention weights must sum to 1.','line_number':680,'multiline':False]
['text':' 'Hello world!'','line_number':712,'multiline':False]
['text':' 'Hello world! ' repeated 1000 times','line_number':728,'multiline':False]
['text':' long input','line_number':731,'multiline':False]
['text':' Set global attention on a few random positions','line_number':735,'multiline':False]
['text':' 'Hello world! ' repeated 1000 times','line_number':749,'multiline':False]
['text':' long input','line_number':752,'multiline':False]
