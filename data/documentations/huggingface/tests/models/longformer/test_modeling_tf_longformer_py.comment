['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' `ModelTesterMixin.test_attention_outputs` is expecting attention tensors to be of size','line_number':74,'multiline':False]
['text':' [num_attention_heads, encoder_seq_length, encoder_key_length], but TFLongformerSelfAttention','line_number':75,'multiline':False]
['text':' returns attention of shape [num_attention_heads, encoder_seq_length, self.attention_window + 1]','line_number':76,'multiline':False]
['text':' because its local attention only attends to `self.attention_window` and one before and one after','line_number':77,'multiline':False]
['text':' global attention mask has to be partly defined','line_number':239,'multiline':False]
['text':' to trace all weights','line_number':240,'multiline':False]
['text':' Replace sep_token_id by some random id','line_number':266,'multiline':False]
['text':' Make sure there are exactly three sep_token_id','line_number':268,'multiline':False]
['text':' TODO: Fix the failed tests','line_number':304,'multiline':False]
['text':' `QAPipelineTests` fails for a few models when the slower tokenizer are used.','line_number':313,'multiline':False]
['text':' (The slower tokenizers were never used for pipeline tests before the pipeline testing rework)','line_number':314,'multiline':False]
['text':' TODO: check (and possibly fix) the `QAPipelineTests` with slower tokenizer','line_number':315,'multiline':False]
['text':' set seq length = 8, hidden dim = 4','line_number':423,'multiline':False]
['text':' first row => [0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000]','line_number':434,'multiline':False]
['text':' last row => [0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629]','line_number':438,'multiline':False]
['text':' pad along seq length dim','line_number':446,'multiline':False]
['text':' expected slices across chunk and seq length dim','line_number':486,'multiline':False]
['text':' create attn mask','line_number':530,'multiline':False]
['text':' create attn mask','line_number':574,'multiline':False]
['text':'','line_number':606,'multiline':False]
['text':' The weight of all tokens with local attention must sum to 1.','line_number':607,'multiline':False]
['text':' All the global attention weights must sum to 1.','line_number':629,'multiline':False]
['text':' 'Hello world!'','line_number':649,'multiline':False]
['text':' 'Hello world! ' repeated 1000 times','line_number':665,'multiline':False]
['text':' Set global attention on a few random positions','line_number':670,'multiline':False]
['text':' assert close','line_number':682,'multiline':False]
['text':' 'Hello world! ' repeated 1000 times','line_number':690,'multiline':False]
['text':' assert close','line_number':701,'multiline':False]
