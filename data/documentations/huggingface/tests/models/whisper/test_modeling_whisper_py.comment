['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2022 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' we don't want to randomely sample timestamp tokens','line_number':95,'multiline':False]
['text':' make sure to use correct index if a batch was removed','line_number':101,'multiline':False]
['text':' produce timestamp with 30%','line_number':114,'multiline':False]
['text':' force a timestamp','line_number':119,'multiline':False]
['text':' force the same as before','line_number':128,'multiline':False]
['text':' "input_ids": input_features,','line_number':168,'multiline':False]
['text':' first forward pass','line_number':293,'multiline':False]
['text':' first forward pass','line_number':303,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':308,'multiline':False]
['text':' append to next input_ids and','line_number':312,'multiline':False]
['text':' select random slice','line_number':321,'multiline':False]
['text':' test that outputs are equal for slice','line_number':328,'multiline':False]
['text':' Needs higher percentages after model tester's vocab_size is changed to 200 (PR #21222)','line_number':379,'multiline':False]
['text':' `0.5` is for `test_disk_offload` (which also works for `test_model_parallelism`)','line_number':380,'multiline':False]
['text':' TODO: Fix the failed tests','line_number':385,'multiline':False]
['text':' RuntimeError: The size of tensor a (1500) must match the size of tensor b (30) at non-singleton','line_number':393,'multiline':False]
['text':' dimension 1','line_number':394,'multiline':False]
['text':' cut to half length & take max batch_size=batch_size','line_number':467,'multiline':False]
['text':' generate max 3 tokens','line_number':470,'multiline':False]
['text':' hack to allow generate for models such as GPT2 as is done in `generate()`','line_number':473,'multiline':False]
['text':' training is not supported yet','line_number':497,'multiline':False]
['text':' Hack to keep the test fast and not require downloading a model with a generation_config','line_number':534,'multiline':False]
['text':' test language code','line_number':538,'multiline':False]
['text':' test tokenizer code','line_number':540,'multiline':False]
['text':' test language name','line_number':542,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':551,'multiline':False]
['text':' check that output_hidden_states also work using config','line_number':614,'multiline':False]
['text':' check that output_attentions also work using config','line_number':646,'multiline':False]
['text':' loss is at first position','line_number':665,'multiline':False]
['text':' loss is added to beginning','line_number':667,'multiline':False]
['text':' past_key_values have been returned','line_number':669,'multiline':False]
['text':' decoder attentions','line_number':673,'multiline':False]
['text':' cross attentions','line_number':682,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':695,'multiline':False]
['text':' Retrieve the embeddings and clone theme','line_number':732,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':736,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':739,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':741,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':744,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':747,'multiline':False]
['text':' make sure that decoder_input_ids are resized','line_number':750,'multiline':False]
['text':' Check that adding and removing tokens has not modified the first part of the embedding matrix.','line_number':755,'multiline':False]
['text':' if model cannot untied embeddings -> leave test','line_number':773,'multiline':False]
['text':' if no output embeddings -> leave test','line_number':781,'multiline':False]
['text':' Check that resizing the token embeddings with a larger vocab size increases the model's vocab size','line_number':785,'multiline':False]
['text':' Check bias if present','line_number':791,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':794,'multiline':False]
['text':' Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size','line_number':797,'multiline':False]
['text':' Check that it actually resizes the embeddings matrix','line_number':800,'multiline':False]
['text':' Check bias if present','line_number':803,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':806,'multiline':False]
['text':' Check that the model can still do a forward pass successfully (every parameter should be resized)','line_number':809,'multiline':False]
['text':' scores','line_number':843,'multiline':False]
['text':' Attentions','line_number':846,'multiline':False]
['text':' encoder','line_number':847,'multiline':False]
['text':' decoder','line_number':851,'multiline':False]
['text':' Hidden States','line_number':861,'multiline':False]
['text':' encoder','line_number':862,'multiline':False]
['text':' decoder','line_number':867,'multiline':False]
['text':' whisper FA2 needs very high tolerance','line_number':916,'multiline':False]
['text':' check with inference + dropout','line_number':919,'multiline':False]
['text':' whisper FA2 needs very high tolerance','line_number':961,'multiline':False]
['text':' whisper FA2 needs very high tolerance','line_number':976,'multiline':False]
['text':' To be sure we have no Nan','line_number':983,'multiline':False]
['text':' FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward','line_number':993,'multiline':False]
['text':' prepare `attention_mask` with shape (batch_size, sequence_length)','line_number':997,'multiline':False]
['text':' We override with a slightly higher tol value, as test recently became flaky','line_number':1064,'multiline':False]
['text':' We override with a slightly higher tol value, as test recently became flaky','line_number':1068,'multiline':False]
['text':' no flax model exists for this class','line_number':1081,'multiline':False]
['text':' Output all for aggressive testing','line_number':1084,'multiline':False]
['text':' load PyTorch class','line_number':1090,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':1092,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':1093,'multiline':False]
['text':' load Flax class','line_number':1096,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':1099,'multiline':False]
['text':' prepare inputs','line_number':1102,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':1105,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':1108,'multiline':False]
['text':' convert inputs to Flax','line_number':1113,'multiline':False]
['text':' send pytorch model to the correct device','line_number':1119,'multiline':False]
['text':' no flax model exists for this class','line_number':1154,'multiline':False]
['text':' Output all for aggressive testing','line_number':1157,'multiline':False]
['text':' load PyTorch class','line_number':1163,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':1165,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':1166,'multiline':False]
['text':' load Flax class','line_number':1169,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':1172,'multiline':False]
['text':' prepare inputs','line_number':1175,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':1178,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':1181,'multiline':False]
['text':' convert inputs to Flax','line_number':1186,'multiline':False]
['text':' make sure weights are tied in PyTorch','line_number':1191,'multiline':False]
['text':' send pytorch model to the correct device','line_number':1194,'multiline':False]
['text':' send pytorch model to the correct device','line_number':1211,'multiline':False]
['text':' forward pass','line_number':1234,'multiline':False]
['text':' forward pass','line_number':1248,'multiline':False]
['text':' len = 250 with num_input_frames = 60','line_number':1323,'multiline':False]
['text':' force bsz=1','line_number':1326,'multiline':False]
['text':' each chunk should not be longer than 10','line_number':1343,'multiline':False]
['text':' if input features are long can't set return_timestamps to False','line_number':1346,'multiline':False]
['text':' if input features are long need to set generation config','line_number':1350,'multiline':False]
['text':' make sure that we only have the same begin token','line_number':1358,'multiline':False]
['text':' len = 250 with num_input_frames = 60','line_number':1384,'multiline':False]
['text':' force bsz=1','line_number':1393,'multiline':False]
['text':' make sure that we only have the same begin token','line_number':1403,'multiline':False]
['text':' automatic decoding with librispeech','line_number':1454,'multiline':False]
['text':' fmt: off','line_number':1479,'multiline':False]
['text':' fmt: on','line_number':1488,'multiline':False]
['text':' fmt: off','line_number':1491,'multiline':False]
['text':' fmt: on','line_number':1500,'multiline':False]
['text':' fmt: off','line_number':1527,'multiline':False]
['text':' fmt: on','line_number':1537,'multiline':False]
['text':' fmt: off','line_number':1567,'multiline':False]
['text':' fmt: on','line_number':1576,'multiline':False]
['text':' fmt: off','line_number':1695,'multiline':False]
['text':' fmt: on','line_number':1704,'multiline':False]
['text':' fmt: off','line_number':1708,'multiline':False]
['text':' fmt: on','line_number':1715,'multiline':False]
['text':' fmt: off','line_number':1733,'multiline':False]
['text':' fmt: on','line_number':1743,'multiline':False]
['text':' fmt: off','line_number':1747,'multiline':False]
['text':' fmt: on','line_number':1754,'multiline':False]
['text':' fmt: skip','line_number':1773,'multiline':False]
['text':' fmt: off','line_number':1842,'multiline':False]
['text':' fmt: on','line_number':1849,'multiline':False]
['text':' Apply SpecAugment','line_number':1857,'multiline':False]
['text':' Set model to training mode to enable SpecAugment','line_number':1859,'multiline':False]
['text':' fmt: off','line_number':1876,'multiline':False]
['text':' fmt: on','line_number':1885,'multiline':False]
['text':' warm up assisted decoding','line_number':1965,'multiline':False]
['text':' warm up non-assisted decoding','line_number':1967,'multiline':False]
['text':' assisted decoding','line_number':1970,'multiline':False]
['text':' non-assisted decoding','line_number':1977,'multiline':False]
['text':' warm up assisted decoding','line_number':2013,'multiline':False]
['text':' warm up non-assisted decoding','line_number':2015,'multiline':False]
['text':' assisted decoding','line_number':2018,'multiline':False]
['text':' non-assisted decoding','line_number':2025,'multiline':False]
['text':' fmt: off','line_number':2040,'multiline':False]
['text':' fmt: on','line_number':2042,'multiline':False]
['text':' fmt: off','line_number':2063,'multiline':False]
['text':' fmt: on','line_number':2068,'multiline':False]
['text':' make sure single & batch is exactly the same','line_number':2098,'multiline':False]
['text':' exact match','line_number':2104,'multiline':False]
['text':' fmt: off','line_number':2112,'multiline':False]
['text':' fmt: on','line_number':2123,'multiline':False]
['text':' first forward pass','line_number':2272,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':2302,'multiline':False]
['text':' input embeds is meaningless for an encoder-only acoustic model','line_number':2324,'multiline':False]
['text':' the equivalent test is passing the encoder outputs directly to the model','line_number':2328,'multiline':False]
['text':' Needs to override as the encoder input embedding is a Conv1d','line_number':2360,'multiline':False]
['text':' WhisperEncoder cannot resize token embeddings since it has no tokens embeddings','line_number':2371,'multiline':False]
['text':' no flax model exists for this class','line_number':2385,'multiline':False]
['text':' Output all for aggressive testing','line_number':2388,'multiline':False]
['text':' load PyTorch class','line_number':2394,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':2396,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':2397,'multiline':False]
['text':' load Flax class','line_number':2400,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':2403,'multiline':False]
['text':' prepare inputs','line_number':2406,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':2409,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':2412,'multiline':False]
['text':' convert inputs to Flax','line_number':2417,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2423,'multiline':False]
['text':' no flax model exists for this class','line_number':2458,'multiline':False]
['text':' Output all for aggressive testing','line_number':2461,'multiline':False]
['text':' load PyTorch class','line_number':2467,'multiline':False]
['text':' Flax models don't use the `use_cache` option and cache is not returned as a default.','line_number':2469,'multiline':False]
['text':' So we disable `use_cache` here for PyTorch model.','line_number':2470,'multiline':False]
['text':' load Flax class','line_number':2473,'multiline':False]
['text':' make sure only flax inputs are forward that actually exist in function args','line_number':2476,'multiline':False]
['text':' prepare inputs','line_number':2479,'multiline':False]
['text':' remove function args that don't exist in Flax','line_number':2482,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':2485,'multiline':False]
['text':' convert inputs to Flax','line_number':2490,'multiline':False]
['text':' make sure weights are tied in PyTorch','line_number':2495,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2498,'multiline':False]
['text':' send pytorch model to the correct device','line_number':2515,'multiline':False]
['text':' first forward pass','line_number':2654,'multiline':False]
['text':' create hypothetical next token and extent to next_input_ids','line_number':2664,'multiline':False]
['text':' append to next input_ids and','line_number':2667,'multiline':False]
['text':' select random slice','line_number':2673,'multiline':False]
['text':' test that outputs are equal for slice','line_number':2678,'multiline':False]
['text':' create attention mask','line_number':2684,'multiline':False]
['text':' first forward pass','line_number':2690,'multiline':False]
['text':' create hypothetical next token and extent to next_input_ids','line_number':2693,'multiline':False]
['text':' change a random masked slice from input_ids','line_number':2696,'multiline':False]
['text':' append to next input_ids and attn_mask','line_number':2701,'multiline':False]
['text':' get two different outputs','line_number':2708,'multiline':False]
['text':' select random slice','line_number':2714,'multiline':False]
['text':' test that outputs are equal for slice','line_number':2719,'multiline':False]
['text':' generate only works with input ids for whisper','line_number':2755,'multiline':False]
['text':' decoder cannot keep gradients','line_number':2760,'multiline':False]
['text':' and it's not used enough to be worth fixing :)','line_number':2767,'multiline':False]
