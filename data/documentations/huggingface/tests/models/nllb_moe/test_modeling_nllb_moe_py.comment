['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2023 The HuggingFace Inc. team. All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' Eos Token','line_number':134,'multiline':False]
['text':' we need to clamp the input ids here to avoid having pad token in between','line_number':137,'multiline':False]
['text':' this is because for NllbMoe the position_ids are prepared such that','line_number':138,'multiline':False]
['text':' all pad tokens have pos id = 2 and rest are between 2..seq_length','line_number':139,'multiline':False]
['text':' and the seq_length here is seq_length - num_pad_tokens','line_number':140,'multiline':False]
['text':' but when using past, there is no way of knowing if the past input ids had','line_number':141,'multiline':False]
['text':' pad tokens in them, which results in incorrect seq_lenth and which in turn results in','line_number':142,'multiline':False]
['text':' position_ids being off by num_pad_tokens in past input','line_number':143,'multiline':False]
['text':' first forward pass','line_number':187,'multiline':False]
['text':' create hypothetical multiple next token and extent to next_input_ids','line_number':192,'multiline':False]
['text':' append to next input_ids and','line_number':196,'multiline':False]
['text':' select random slice','line_number':205,'multiline':False]
['text':' test that outputs are equal for slice','line_number':212,'multiline':False]
['text':' TODO: Fix the failed tests when this model gets more usage','line_number':269,'multiline':False]
['text':' Saving the slow tokenizer after saving the fast tokenizer causes the loading of the later hanging forever.','line_number':273,'multiline':False]
['text':' fmt: off','line_number':385,'multiline':False]
['text':' fmt: on','line_number':388,'multiline':False]
['text':' fmt: skip','line_number':407,'multiline':False]
['text':' fmt: off','line_number':416,'multiline':False]
['text':' fmt: on','line_number':420,'multiline':False]
['text':' first 6 samples of load_dataset("facebook/flores", "eng_Latn-fra_Latn"), devtest. Truth are very similar to the fairseq translation files','line_number':435,'multiline':False]
['text':' test routing with minimal reproduction','line_number':482,'multiline':False]
['text':' fmt: skip','line_number':514,'multiline':False]
['text':' check that the routing is batch first. One of the last token is routed while expert capacity is very small','line_number':527,'multiline':False]
['text':' this means that it had a greater probability of being routed','line_number':528,'multiline':False]
['text':' fmt: off','line_number':557,'multiline':False]
['text':' `sampling` and `random` do not affect the mask of the top_1 router','line_number':564,'multiline':False]
['text':' fmt: on','line_number':565,'multiline':False]
