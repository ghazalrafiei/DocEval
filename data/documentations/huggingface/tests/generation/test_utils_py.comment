['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2020 The HuggingFace Team Inc.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a clone of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' cut to half length & take max batch_size 3','line_number':98,'multiline':False]
['text':' generate max 3 tokens','line_number':102,'multiline':False]
['text':' hack to allow generate for models such as GPT2 as is done in `generate()`','line_number':105,'multiline':False]
['text':' NoRepeatNGramLogitsProcessor + forced tokens may result in no valid continuations','line_number':128,'multiline':False]
['text':' NOTE: the order of operations here should match `generate` for accurate testing','line_number':132,'multiline':False]
['text':' prevent flaky generation test failures','line_number':167,'multiline':False]
['text':' one beam per group','line_number':209,'multiline':False]
['text':' beam_search does not automatically interleave `batch_size` dim for `num_beams`','line_number':415,'multiline':False]
['text':' beam_search does not automatically interleave `batch_size` dim for `num_beams`','line_number':475,'multiline':False]
['text':' prevent flaky generation test failures','line_number':491,'multiline':False]
['text':' group_beam_search does not automatically interleave `batch_size` dim for `num_beams`','line_number':542,'multiline':False]
['text':' group_beam_search does not automatically interleave `batch_size` dim for `num_beams`','line_number':604,'multiline':False]
['text':' check `generate()` and `greedy_search()` are equal','line_number':705,'multiline':False]
['text':' test old generation output for backwards compatibility','line_number':708,'multiline':False]
['text':' disable cache','line_number':717,'multiline':False]
['text':' enable cache','line_number':746,'multiline':False]
['text':' check `generate()` and `sample()` are equal','line_number':788,'multiline':False]
['text':' check `generate()` and `sample()` yield equal results for `num_return_sequences`','line_number':802,'multiline':False]
['text':' disable cache','line_number':818,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':866,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':867,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':868,'multiline':False]
['text':' check `generate()` and `beam_search()` are equal','line_number':885,'multiline':False]
['text':' disable cache','line_number':919,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':922,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':923,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':924,'multiline':False]
['text':' enable cache','line_number':973,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':976,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':977,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':978,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1049,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1050,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1051,'multiline':False]
['text':' check `generate()` and `beam_search()` are equal','line_number':1059,'multiline':False]
['text':' disable cache','line_number':1080,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1083,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1084,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1085,'multiline':False]
['text':' if no bos token id => cannot generate from None','line_number':1131,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1146,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1147,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1148,'multiline':False]
['text':' check `generate()` and `group_beam_search()` are equal','line_number':1165,'multiline':False]
['text':' check `generate()` and `group_beam_search()` are equal for `num_return_sequences`','line_number':1179,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1203,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1204,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1205,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1265,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1266,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1267,'multiline':False]
['text':' check `generate()` and `constrained_beam_search()` are equal','line_number':1282,'multiline':False]
['text':' Sample constraints','line_number':1283,'multiline':False]
['text':' check `generate()` and `constrained_beam_search()` are equal for `num_return_sequences`','line_number':1310,'multiline':False]
['text':' Sample constraints','line_number':1311,'multiline':False]
['text':' disable cache','line_number':1344,'multiline':False]
['text':' It is important set set the eos_token_id to None to ensure that no sequences','line_number':1347,'multiline':False]
['text':' shorter than `max_length` can be generated which could lead to flaky circle ci','line_number':1348,'multiline':False]
['text':' failures if the top `num_return_sequences` beams are all shorter than the longest beam','line_number':1349,'multiline':False]
['text':' Sample constraints','line_number':1365,'multiline':False]
['text':' check `generate()` and `contrastive_search()` are equal','line_number':1410,'multiline':False]
['text':' won't fix: FSMT and Reformer have a different cache variable type (and format).','line_number':1412,'multiline':False]
['text':' NOTE: contrastive search only works with cache on at the moment.','line_number':1418,'multiline':False]
['text':' test old generation output for backwards compatibility','line_number':1424,'multiline':False]
['text':' won't fix: FSMT and Reformer have a different cache variable type (and format).','line_number':1433,'multiline':False]
['text':' enable cache','line_number':1437,'multiline':False]
['text':' NOTE: contrastive search only works with cache on at the moment.','line_number':1440,'multiline':False]
['text':' Check that choosing 'low_memory' does not change the model output','line_number':1464,'multiline':False]
['text':' NOTE: contrastive search only works with cache on at the moment.','line_number':1473,'multiline':False]
['text':' test output equality of low versus high memory','line_number':1480,'multiline':False]
['text':' Read NOTE (1) below. If there are API issues, all attempts will fail.','line_number':1502,'multiline':False]
['text':' This test ensures that the assisted generation does not introduce output changes over greedy search.','line_number':1504,'multiline':False]
['text':' NOTE (1): The sentence above is true most of the time, there is a tiny difference in the logits due to matmul','line_number':1505,'multiline':False]
['text':' shape differences -- and it may result in a different output. The input shape difference happens in the','line_number':1506,'multiline':False]
['text':' main model, that runs the forward pass with several candidates at once (as opposed to generating one token at','line_number':1507,'multiline':False]
['text':' a time). See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535 for more info.','line_number':1508,'multiline':False]
['text':' NOTE (2): It breaks the pattern in the tests above, for multiple reasons:','line_number':1509,'multiline':False]
['text':' - assisted_decoding, contrarily to the other methods, can't be called on its own (e.g. needs to','line_number':1510,'multiline':False]
['text':' prepare the assistant encoder outputs in the main generate body);','line_number':1511,'multiline':False]
['text':' - assisted_decoding does not support `use_cache = False`','line_number':1512,'multiline':False]
['text':' - assisted_decoding does not support `batch_size > 1`','line_number':1513,'multiline':False]
['text':' enable cache','line_number':1533,'multiline':False]
['text':' NOTE: assisted generation only works with cache on at the moment.','line_number':1536,'multiline':False]
['text':' Sets assisted generation arguments such that:','line_number':1543,'multiline':False]
['text':' a) no EOS is generated, to ensure generation doesn't break early','line_number':1544,'multiline':False]
['text':' b) the assistant model always generates two tokens when it is called, to ensure the input preparation of','line_number':1545,'multiline':False]
['text':'    the assistant model is correct','line_number':1546,'multiline':False]
['text':' c) there are at least two forward passes in the main model, to ensure the input preparation of','line_number':1547,'multiline':False]
['text':'    the main model is correct','line_number':1548,'multiline':False]
['text':' see a)','line_number':1550,'multiline':False]
['text':' see c)','line_number':1551,'multiline':False]
['text':' see b)','line_number':1562,'multiline':False]
['text':' see b)','line_number':1563,'multiline':False]
['text':' The two outputs must match and their shape must be as expected','line_number':1567,'multiline':False]
['text':' In this test we don't check assisted vs non-assisted output -- seeded assisted decoding with sample will not','line_number':1573,'multiline':False]
['text':' match sample for the same seed, as the forward pass does not return the exact same logits (due to matmul with','line_number':1574,'multiline':False]
['text':' different shapes, see https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).','line_number':1575,'multiline':False]
['text':' enable cache','line_number':1594,'multiline':False]
['text':' NOTE: assisted generation only works with cache on at the moment.','line_number':1597,'multiline':False]
['text':' Sets assisted generation arguments such that:','line_number':1604,'multiline':False]
['text':' a) no EOS is generated, to ensure generation doesn't break early','line_number':1605,'multiline':False]
['text':' b) the assistant model always generates two tokens when it is called, to ensure the input preparation of','line_number':1606,'multiline':False]
['text':'    the assistant model is correct','line_number':1607,'multiline':False]
['text':' c) there are at least two forward passes in the main model, to ensure the input preparation of','line_number':1608,'multiline':False]
['text':'    the main model is correct','line_number':1609,'multiline':False]
['text':' see b)','line_number':1611,'multiline':False]
['text':' see b)','line_number':1612,'multiline':False]
['text':' see a)','line_number':1614,'multiline':False]
['text':' see c)','line_number':1615,'multiline':False]
['text':' We want to test only encoder-decoder models','line_number':1633,'multiline':False]
['text':' We want to test only models where encoder/decoder head masking is implemented','line_number':1649,'multiline':False]
['text':' We check the state of decoder_attentions and cross_attentions just from the last step','line_number':1663,'multiline':False]
['text':' The check done in this test is fairly difficult -- depending on the model architecture, passing the right','line_number':1668,'multiline':False]
['text':' position index for the position embeddings can still result in a different output, due to numerical masking.','line_number':1669,'multiline':False]
['text':' On the other hand, for some types of position embeddings, an incorrect position index can have a minimal','line_number':1670,'multiline':False]
['text':' impact on the output.','line_number':1671,'multiline':False]
['text':' There are two tricks employed to check whether left-padding compatibility is in place:','line_number':1672,'multiline':False]
['text':' 1 - To reduce the negative impact of the numerical attention mask on a correct position index, we set the','line_number':1673,'multiline':False]
['text':' padding size to 1.','line_number':1674,'multiline':False]
['text':' 2 - To reduce the chance of false positives (i.e. passing when it should be failing), we run the check','line_number':1675,'multiline':False]
['text':' multiple times with random inputs, and it has to pass with all of them.','line_number':1676,'multiline':False]
['text':' NOTE: because of 2), there is some chance of false positives in this test.','line_number':1677,'multiline':False]
['text':' skip for encoder-decoder models -- they don't need left-padding compatibility','line_number':1682,'multiline':False]
['text':' there may be false positives with 10 runs, we rely on the CI to catch the flakiness','line_number':1687,'multiline':False]
['text':' Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a','line_number':1713,'multiline':False]
['text':' standard KV cache format is important for a consistent API (and for advanced generation methods).','line_number':1714,'multiline':False]
['text':' If it doesn't support cache, pass the test','line_number':1718,'multiline':False]
['text':' If "past_key_values" is not returned, pass the test (e.g. RWKV uses a different cache name and format)','line_number':1727,'multiline':False]
['text':' Encoder-Decoder checks','line_number':1743,'multiline':False]
['text':' K V for the decoder + K V for the encoder = 4','line_number':1749,'multiline':False]
['text':' The sequence length for the encoder K V depends on the model. Since it is not manipulated in','line_number':1756,'multiline':False]
['text':' autoregressive generation, I'm keeping the test general and not checking the 3rd dim','line_number':1757,'multiline':False]
['text':' Decoder-only checks','line_number':1767,'multiline':False]
['text':' TODO: this line is only needed because of imagegpt, where "pixel_values" = "input_ids". Fix the','line_number':1769,'multiline':False]
['text':' tests in imagegpt such that `prepare_config_and_inputs_for_common` returns the later (and the other','line_number':1770,'multiline':False]
['text':' tests use it)','line_number':1771,'multiline':False]
['text':' K V for the decoder = 2','line_number':1775,'multiline':False]
['text':' When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`','line_number':1784,'multiline':False]
['text':' if fails, you should probably update the `prepare_inputs_for_generation` function','line_number':1785,'multiline':False]
['text':' Ignore:','line_number':1789,'multiline':False]
['text':' a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,','line_number':1790,'multiline':False]
['text':'   which would cause a mismatch),','line_number':1791,'multiline':False]
['text':' b) embedding scaling, the scaling factor applied after embeding from input_ids (requires knowledge of the','line_number':1793,'multiline':False]
['text':'   variable that holds the scaling factor, which is model-dependent)','line_number':1794,'multiline':False]
['text':' This test is for decoder-only models (encoder-decoder models have native input embeddings support in the','line_number':1798,'multiline':False]
['text':' decoder)','line_number':1799,'multiline':False]
['text':' Skip models without explicit support','line_number':1803,'multiline':False]
['text':' Traditional way of generating text','line_number':1808,'multiline':False]
['text':' Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)','line_number':1812,'multiline':False]
['text':' But if we pass different inputs_embeds, we should get different outputs','line_number':1817,'multiline':False]
['text':' input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same','line_number':1824,'multiline':False]
['text':' Tests that we can continue generating from past key values, returned from a previous `generate` call','line_number':1834,'multiline':False]
['text':' Let's make it always:','line_number':1846,'multiline':False]
['text':' 1. use cache (for obvious reasons)','line_number':1847,'multiline':False]
['text':' 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which','line_number':1848,'multiline':False]
['text':'    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the','line_number':1849,'multiline':False]
['text':'    continuation would force it to generate beyond an EOS token)','line_number':1850,'multiline':False]
['text':' 3. ignore `token_type_ids` for simplicity','line_number':1851,'multiline':False]
['text':' 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is','line_number':1852,'multiline':False]
['text':'    active by default on some models','line_number':1853,'multiline':False]
['text':' If "past_key_values" is not returned, skip the test (e.g. RWKV uses a different cache name and format)','line_number':1863,'multiline':False]
['text':' Traditional way of generating text, with `return_dict_in_generate` to return the past key values','line_number':1868,'multiline':False]
['text':' Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the','line_number':1871,'multiline':False]
['text':' inputs may need to be tweaked across `generate` calls (like the attention mask).','line_number':1872,'multiline':False]
['text':' Continue from the tokens generated above, preparing the inputs accordingly','line_number':1875,'multiline':False]
['text':' The two sets of generated text and past kv should be equal to each other','line_number':1898,'multiline':False]
['text':' Tests that generating with the new format is exactly the same as the legacy one (for models that support it).','line_number':1911,'multiline':False]
['text':' 👉 tests with and without beam search so that we can test with and without cache reordering.','line_number':1912,'multiline':False]
['text':' 👉 tests with and without sampling so we can cover the most common use cases.','line_number':1913,'multiline':False]
['text':' Required to return `past_key_values`','line_number':1928,'multiline':False]
['text':' Sets seed before calling `generate` for the case with do_sample=True','line_number':1931,'multiline':False]
['text':' The two sets of generated sequences must match, despite the cache format between forward passes being','line_number':1940,'multiline':False]
['text':' different','line_number':1941,'multiline':False]
['text':' The contents of the two caches, when converted to the same format (in both directions!), must match','line_number':1946,'multiline':False]
['text':' scores','line_number':1976,'multiline':False]
['text':' Attentions','line_number':1979,'multiline':False]
['text':' encoder','line_number':1981,'multiline':False]
['text':' decoder','line_number':1983,'multiline':False]
['text':' if use_cache first input is equal to no use_cache, so skip here','line_number':1993,'multiline':False]
['text':' Hidden States','line_number':2005,'multiline':False]
['text':' encoder','line_number':2007,'multiline':False]
['text':' decoder','line_number':2012,'multiline':False]
['text':' if use_cache first input is equal to no use_cache, so skip here','line_number':2022,'multiline':False]
['text':' Past Key Value States -- two notes here:','line_number':2034,'multiline':False]
['text':' 1. Its inner sequence length is with respect to the inputs of the latest forward pass, hence the "-1"','line_number':2035,'multiline':False]
['text':' 2. Some old models still return `output.past_key_values` even without `use_cache=True`','line_number':2036,'multiline':False]
['text':' 3. TODO (joao): A few models have different formats, skipping those until the cache refactor is complete','line_number':2037,'multiline':False]
['text':' check attn size','line_number':2077,'multiline':False]
['text':' check hidden size','line_number':2103,'multiline':False]
['text':' (batch, head, seq_length, head_features)','line_number':2124,'multiline':False]
['text':' check shape key, value','line_number':2131,'multiline':False]
['text':' check if tensor_1 inside tensor_2 or tensor_2 inside tensor_1.','line_number':2142,'multiline':False]
['text':' set to same device. we don't care what device.','line_number':2143,'multiline':False]
['text':' tests whether the top_k_top_p function behaves as expected','line_number':2167,'multiline':False]
['text':' 3rd highest value; idx. 0','line_number':2172,'multiline':False]
['text':' 2nd highest value; idx. 10','line_number':2182,'multiline':False]
['text':' 4th highest value; idx. 25','line_number':2197,'multiline':False]
['text':' 1st highest value; idx. 26','line_number':2198,'multiline':False]
['text':' cummulative prob of 4 highest values <= 0.6','line_number':2202,'multiline':False]
['text':' 4th highest value; idx. 13','line_number':2217,'multiline':False]
['text':' 2nd highest value; idx. 17','line_number':2221,'multiline':False]
['text':' 3rd highest value; idx. 20','line_number':2224,'multiline':False]
['text':' 1st highest value; idx. 27','line_number':2231,'multiline':False]
['text':' cummulative prob of 4 highest values <= 0.6','line_number':2234,'multiline':False]
['text':' expected non filtered idx as noted above','line_number':2244,'multiline':False]
['text':' expected non filtered values as noted above','line_number':2256,'multiline':False]
['text':' tests whether the function uses filter_value instead of default -inf','line_number':2268,'multiline':False]
['text':' get filtered by top-p filtering','line_number':2276,'multiline':False]
['text':' get filtered by top-k filtering','line_number':2277,'multiline':False]
['text':' setting framework_dependent_parameters needs to be gated, just like its contents' imports','line_number':2297,'multiline':False]
['text':' PT-only test: TF doesn't have a diverse beam search implementation','line_number':2313,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2347,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2376,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2405,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria & group beam search','line_number':2438,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2475,'multiline':False]
['text':' Greedy','line_number':2492,'multiline':False]
['text':' Sample','line_number':2513,'multiline':False]
['text':' Beam','line_number':2525,'multiline':False]
['text':' Grouped beam search','line_number':2542,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2561,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2575,'multiline':False]
['text':' PT-only test: TF doesn't have StoppingCriteria','line_number':2598,'multiline':False]
['text':' PT-only test: AFAIK there's no non-NLP model architecture in TF that supports `input_ids` as its only input','line_number':2617,'multiline':False]
['text':' PT-only test: AFAIK there's no generate-capable architecture in TF that supports `input_values` as its input','line_number':2630,'multiline':False]
['text':' PT-only test: TF doesn't have group beam search','line_number':2641,'multiline':False]
['text':' PT-only test: TF doesn't have a BeamSearchScorer','line_number':2671,'multiline':False]
['text':' exactly the example provided in the docstrings of beam search, which previously','line_number':2672,'multiline':False]
['text':' failed after directly copying from it. Refer to PR #15555','line_number':2673,'multiline':False]
['text':' lets run beam search using 3 beams','line_number':2680,'multiline':False]
['text':' define decoder start token ids','line_number':2682,'multiline':False]
['text':' add encoder_outputs to model keyword arguments','line_number':2686,'multiline':False]
['text':' instantiate beam scorer','line_number':2693,'multiline':False]
['text':' instantiate logits processors','line_number':2700,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2714,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2752,'multiline':False]
['text':' max_length=20,','line_number':2776,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2793,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2871,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2896,'multiline':False]
['text':' lets run beam search using 5 beams','line_number':2903,'multiline':False]
['text':' define decoder start token ids','line_number':2905,'multiline':False]
['text':' add encoder_outputs to model keyword arguments','line_number':2909,'multiline':False]
['text':' remove eos token','line_number':2917,'multiline':False]
['text':' instantiate beam scorer','line_number':2920,'multiline':False]
['text':' instantiate logits processors','line_number':2925,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2940,'multiline':False]
['text':' PT-only test: TF doesn't have constrained beam search','line_number':2981,'multiline':False]
['text':' Tests that contrastive search works with batched inputs (i.e. has the same output as for non-batched inputs)','line_number':2982,'multiline':False]
['text':' output_sequences_batched.scores[0][1] -> 1st set of logits, 2nd sequence','line_number':3002,'multiline':False]
['text':' Has TF equivalent: this test relies on random sampling','line_number':3007,'multiline':False]
['text':' Only some seeds will work both on CPU/GPU for a fixed `expectation` value.','line_number':3022,'multiline':False]
['text':' The selected seed is not guaranteed to work on all torch versions.','line_number':3023,'multiline':False]
['text':' Has TF equivalent: ample use of framework-specific code','line_number':3035,'multiline':False]
['text':' Let's create a fake model that has a different signature. In particular, this fake model accepts "foo" as an','line_number':3044,'multiline':False]
['text':' argument. Because "foo" is not in the encoder signature and doesn't start with "decoder_", it will be part of','line_number':3045,'multiline':False]
['text':' the encoder kwargs prior to signature filtering, which would lead to an exception. But filtering kicks in and','line_number':3046,'multiline':False]
['text':' saves the day.','line_number':3047,'multiline':False]
['text':' Encoder signature filtering only kicks in if it doesn't accept wildcard kwargs. The following test will fail','line_number':3056,'multiline':False]
['text':' because it doesn't do signature filtering.','line_number':3057,'multiline':False]
['text':' Normal generation still works (the output will be different because the encoder weights are different)','line_number':3065,'multiline':False]
['text':' FakeEncoder.forward() accepts **kwargs -> no filtering -> type error due to unexpected input "foo"','line_number':3068,'multiline':False]
['text':' Default generation config value of 20 -> emits warning','line_number':3080,'multiline':False]
['text':' Explicitly setting max_length to 20 -> no warning','line_number':3084,'multiline':False]
['text':' Generation config max_length != 20 -> no warning','line_number':3089,'multiline':False]
['text':' generation_config is modified -> legacy mode is disabled = generation_config takes precedence','line_number':3091,'multiline':False]
['text':' PT-only test: TF doesn't support assisted decoding yet.','line_number':3097,'multiline':False]
['text':' Traditional way of generating text','line_number':3106,'multiline':False]
['text':' Should be different with token_type_ids','line_number':3110,'multiline':False]
['text':' Assistant model','line_number':3118,'multiline':False]
['text':' If assisted generation passes model_kwargs correctly, should be same as previous','line_number':3122,'multiline':False]
['text':' PT-only test: TF doesn't support assisted decoding yet.','line_number':3139,'multiline':False]
['text':' Bart subclass with a kwarg that distorts the output','line_number':3140,'multiline':False]
['text':' Traditional way of generating text','line_number':3163,'multiline':False]
['text':' Should be different with foo','line_number':3167,'multiline':False]
['text':' Assistant model','line_number':3172,'multiline':False]
['text':' If assisted generation passes model_kwargs correctly, should be same as previous','line_number':3177,'multiline':False]
['text':' Check that passing encoder_outputs directly also works as expected','line_number':3185,'multiline':False]
['text':' PT-only test: TF doesn't support assisted decoding yet.','line_number':3205,'multiline':False]
['text':' Bart subclass with a kwarg called foo that distorts the output','line_number':3206,'multiline':False]
['text':' Traditional way of generating text','line_number':3242,'multiline':False]
['text':' Should be different with foo','line_number':3246,'multiline':False]
['text':' Assistant model','line_number':3251,'multiline':False]
['text':' If assisted generation passes model_kwargs correctly, should be same as previous','line_number':3256,'multiline':False]
['text':' Check that passing encoder_outputs directly also works as expected','line_number':3264,'multiline':False]
