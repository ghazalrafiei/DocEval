['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2019 HuggingFace Inc.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' noqa: F401','line_number':34,'multiline':False]
['text':' Restrict TensorFlow to only allocate x GB of memory on the GPUs','line_number':89,'multiline':False]
['text':' Virtual devices must be set before GPUs have been initialized','line_number':97,'multiline':False]
['text':' When we have enough CTC models for an AutoClass, we should use their mapping instead of name checks','line_number':170,'multiline':False]
['text':' the config file (and the generation config file, if it can generate) should be saved','line_number':190,'multiline':False]
['text':' make sure that returned config is jsonifiable, which is required by keras','line_number':208,'multiline':False]
['text':' make sure it also accepts a normal config','line_number':211,'multiline':False]
['text':' Build model','line_number':213,'multiline':False]
['text':' Check that we have one of three possible outputs: None, tuple of tensors or a tensor','line_number':252,'multiline':False]
['text':' signature.parameters is an OrderedDict => so arg_names order is deterministic','line_number':266,'multiline':False]
['text':' `tf2onnx` issue page: https://github.com/onnx/tensorflow-onnx/issues/2172','line_number':333,'multiline':False]
['text':' TODO: undo skip once a fix is done in `tf2onnx`','line_number':334,'multiline':False]
['text':' This condition is required, since `modeling_tf_clip.py` has 3 classes whose names end with `MainLayer`.','line_number':364,'multiline':False]
['text':' T5MainLayer needs an embed_tokens parameter when called without the inputs_embeds parameter','line_number':372,'multiline':False]
['text':' Take the same values than in TFT5ModelTester for this shared layer','line_number':374,'multiline':False]
['text':' Make sure we don't have nans','line_number':408,'multiline':False]
['text':' Don't copy this method to model specific test file!','line_number':422,'multiline':False]
['text':' TODO: remove this method once the issues are all fixed!','line_number':423,'multiline':False]
['text':' Make sure no all 0s attention masks - to avoid failure at this moment.','line_number':431,'multiline':False]
['text':' Put `1` at the beginning of sequences to make it still work when combining causal attention masks.','line_number':432,'multiline':False]
['text':' TODO: remove this line once a fix regarding large negative values for attention mask is done.','line_number':433,'multiline':False]
['text':' Here we make the first sequence with all 0s as attention mask.','line_number':438,'multiline':False]
['text':' Currently, this will fail for `TFWav2Vec2Model`. This is caused by the different large negative','line_number':439,'multiline':False]
['text':' values, like `1e-4`, `1e-9`, `1e-30` and `-inf` for attention mask across models/frameworks.','line_number':440,'multiline':False]
['text':' TODO: enable this block once the large negative values thing is cleaned up.','line_number':441,'multiline':False]
['text':' (see https://github.com/huggingface/transformers/issues/14859)','line_number':442,'multiline':False]
['text':' attention_mask = tf.concat(','line_number':443,'multiline':False]
['text':'     [','line_number':444,'multiline':False]
['text':'         tf.zeros_like(attention_mask[:1], dtype=tf.int32),','line_number':445,'multiline':False]
['text':'         tf.cast(attention_mask[1:], dtype=tf.int32)','line_number':446,'multiline':False]
['text':'     ],','line_number':447,'multiline':False]
['text':'     axis=0','line_number':448,'multiline':False]
['text':' )','line_number':449,'multiline':False]
['text':' Don't copy this method to model specific test file!','line_number':453,'multiline':False]
['text':' TODO: remove this method once the issues are all fixed!','line_number':454,'multiline':False]
['text':' `TFGPT2` has `past_key_values` as a tensor while `GPT2` has it as a tuple.','line_number':474,'multiline':False]
['text':' create new outputs from the remaining fields','line_number':478,'multiline':False]
['text':' Allow `ModelOutput` (e.g. `CLIPOutput` has `text_model_output` and `vision_model_output`).','line_number':500,'multiline':False]
['text':' Don't copy this block to model specific test file!','line_number':507,'multiline':False]
['text':' TODO: remove this method and this line after issues are fixed','line_number':508,'multiline':False]
['text':' convert to the case of `tuple`','line_number':516,'multiline':False]
['text':' appending each key to the current (string) `names`','line_number':517,'multiline':False]
['text':' Allow `list` (e.g. `TransfoXLModelOutput.mems` is a list of tensors.)','line_number':523,'multiline':False]
['text':' case 1: each output has assigned name (e.g. a tuple form of a `ModelOutput`)','line_number':529,'multiline':False]
['text':' case 2: each output has no assigned name (e.g. hidden states of each layer) -> add an index to `names`','line_number':536,'multiline':False]
['text':' deal with NumPy's scalars to make replacing nan values by 0 work.','line_number':554,'multiline':False]
['text':' other general float inputs','line_number':586,'multiline':False]
['text':' send pytorch inputs to the correct device','line_number':597,'multiline':False]
['text':' send pytorch model to the correct device','line_number':602,'multiline':False]
['text':' Check predictions on first output (logits/hidden-states) are close enough given low-level computational differences','line_number':605,'multiline':False]
['text':' tf models returned loss is usually a tensor rather than a scalar.','line_number':612,'multiline':False]
['text':' (see `hf_compute_loss`: it uses `tf.keras.losses.Reduction.NONE`)','line_number':613,'multiline':False]
['text':' Change it here to a scalar to match PyTorch models' loss','line_number':614,'multiline':False]
['text':' Output all for aggressive testing','line_number':628,'multiline':False]
['text':' Make sure no sequence has all zeros as attention mask, otherwise some tests fail due to the inconsistency','line_number':632,'multiline':False]
['text':' of the usage `1e-4`, `1e-9`, `1e-30`, `-inf`.','line_number':633,'multiline':False]
['text':' TODO: Use a uniform value for all models, make sure all tests pass without this processing, and remove it.','line_number':634,'multiline':False]
['text':' Skip the "TF" at the beginning','line_number':637,'multiline':False]
['text':' Not all models accept "labels" in the forward pass (yet :) )','line_number':647,'multiline':False]
['text':' For some models (e.g. base models), there is no label returned.','line_number':651,'multiline':False]
['text':' Set the input dict to `None` to avoid check outputs twice for the same input dicts.','line_number':652,'multiline':False]
['text':' Check we can load pt model in tf and vice-versa with model => model functions','line_number':656,'multiline':False]
['text':' Original test: check without `labels`','line_number':664,'multiline':False]
['text':' check with `labels`','line_number':666,'multiline':False]
['text':' Check we can load pt model in tf and vice-versa with checkpoint => model functions','line_number':670,'multiline':False]
['text':' Original test: check without `labels`','line_number':684,'multiline':False]
['text':' check with `labels`','line_number':686,'multiline':False]
['text':' Prepare our model','line_number':695,'multiline':False]
['text':' These are maximally general inputs for the model, with multiple None dimensions','line_number':697,'multiline':False]
['text':' Hopefully this will catch any conditionals that fail for flexible shapes','line_number':698,'multiline':False]
['text':' Compile extended model','line_number':708,'multiline':False]
['text':' Check we can pass inputs with the Keras API','line_number':710,'multiline':False]
['text':' Ensure we can save/export the whole functional model','line_number':713,'multiline':False]
['text':' differentiation due to newly added cross_attentions','line_number':744,'multiline':False]
['text':' Check that output attentions can also be changed via the config','line_number':777,'multiline':False]
['text':' Check attention is always last and order is fine','line_number':785,'multiline':False]
['text':' To be sure we have no Nan','line_number':805,'multiline':False]
['text':' Prepare head_mask','line_number':809,'multiline':False]
['text':' necessary diferentiation because of T5 model','line_number':835,'multiline':False]
['text':' Remove Nan','line_number':843,'multiline':False]
['text':' Check we don't have more than 25% nans (arbitrary)','line_number':847,'multiline':False]
['text':' remove them (the test is less complete)','line_number':851,'multiline':False]
['text':' encoder-decodere models have only 2 layers in each modules','line_number':855,'multiline':False]
['text':' Not all models accept "labels" in the forward pass (yet :) )','line_number':999,'multiline':False]
['text':' TODO (joao): after the embeddings refactor is complete, rework this test so as to rely exclusively on','line_number':1081,'multiline':False]
['text':' tf.keras.layers.Embedding','line_number':1082,'multiline':False]
['text':' builds the embeddings layer','line_number':1090,'multiline':False]
['text':' build the embeddings','line_number':1098,'multiline':False]
['text':' `resize_token_embeddings` mutates `config`','line_number':1099,'multiline':False]
['text':' reshape the embeddings','line_number':1103,'multiline':False]
['text':' check that the resized embeddings size matches the desired size.','line_number':1109,'multiline':False]
['text':' check that weights remain the same after resizing','line_number':1113,'multiline':False]
['text':' TODO (Joao): this test is not slow, but it's tagged as such to keep track of failures on the scheduled CI runs,','line_number':1140,'multiline':False]
['text':' while passing push CI. Fix the underlying issues and remove the tag.','line_number':1141,'multiline':False]
['text':' create a model with resized (expended) embeddings','line_number':1149,'multiline':False]
['text':' `resize_token_embeddings` mutates `config`','line_number':1153,'multiline':False]
['text':' fetch the output for an input exclusively made of new members of the vocabulary','line_number':1157,'multiline':False]
['text':' save and load the model','line_number':1177,'multiline':False]
['text':' check that the output for the restored model is the same','line_number':1183,'multiline':False]
['text':' TF embeddings layers don't raise an exception when an index is out of bounds on GPU, so we manually raise it.','line_number':1191,'multiline':False]
['text':' This test should only fail on GPU for models where we haven't added the safety check.','line_number':1192,'multiline':False]
['text':' iterate over all generative models','line_number':1212,'multiline':False]
['text':' if bos token id is not defined model needs input_ids','line_number':1217,'multiline':False]
['text':' num_return_sequences = 1','line_number':1220,'multiline':False]
['text':' Models with non-text inputs won't work here; num_return_sequences = 1','line_number':1223,'multiline':False]
['text':' generating multiple sequences when no beam search generation','line_number':1227,'multiline':False]
['text':' is not allowed as it would always generate the same sequences','line_number':1228,'multiline':False]
['text':' num_return_sequences > 1, sample','line_number':1231,'multiline':False]
['text':' check bad words tokens language generation','line_number':1234,'multiline':False]
['text':' create list of 1-seq bad token and list of 2-seq of bad tokens','line_number':1235,'multiline':False]
['text':' only count generated tokens','line_number':1240,'multiline':False]
['text':' iterate over all generative models','line_number':1250,'multiline':False]
['text':' if bos token id is not defined model needs input_ids, num_return_sequences = 1','line_number':1285,'multiline':False]
['text':' num_return_sequences = 1','line_number':1288,'multiline':False]
['text':' generating more sequences than having beams leads is not possible','line_number':1292,'multiline':False]
['text':' num_return_sequences > 1, sample','line_number':1295,'multiline':False]
['text':' num_return_sequences > 1, greedy','line_number':1304,'multiline':False]
['text':' check bad words tokens language generation','line_number':1307,'multiline':False]
['text':' create list of 1-seq bad token and list of 2-seq of bad tokens','line_number':1308,'multiline':False]
['text':' only count generated tokens','line_number':1313,'multiline':False]
['text':' iterate over all generative models','line_number':1323,'multiline':False]
['text':' The number of elements in the loss should be the same as the number of elements in the label','line_number':1356,'multiline':False]
['text':' This test is only for models with easily-separable labels','line_number':1360,'multiline':False]
['text':' Test that model correctly compute the loss with kwargs','line_number':1364,'multiline':False]
['text':' Test that model correctly compute the loss when we mask some positions','line_number':1377,'multiline':False]
['text':' Test that model correctly compute the loss with a dict','line_number':1391,'multiline':False]
['text':' Test that model correctly compute the loss with a tuple','line_number':1396,'multiline':False]
['text':' Get keys that were added with the _prepare_for_class function','line_number':1399,'multiline':False]
['text':' Create a dictionary holding the location of the tensors in the tuple','line_number':1404,'multiline':False]
['text':' Initialize a list with their default values, update the values and convert to a tuple','line_number':1410,'multiline':False]
['text':' Send to model','line_number':1422,'multiline':False]
['text':' Test that model correctly compute the loss with kwargs','line_number':1435,'multiline':False]
['text':' We also remove "return_loss" as this is covered by the train_step when using fit()','line_number':1437,'multiline':False]
['text':' Build the model so we can get some constant weights and check outputs','line_number':1468,'multiline':False]
['text':' Run eagerly to save some expensive compilation times','line_number':1474,'multiline':False]
['text':' Make sure the model fits without crashing regardless of where we pass the labels','line_number':1476,'multiline':False]
['text':' The next tests only make sense for models with separate inputs and labels, and do not make','line_number':1501,'multiline':False]
['text':' sense for models that don't clearly distinguish between the two (e.g. CLIP)','line_number':1502,'multiline':False]
['text':' We reinitialize the model here even though our learning rate was zero','line_number':1508,'multiline':False]
['text':' because BatchNorm updates weights by means other than gradient descent.','line_number':1509,'multiline':False]
['text':' No integer inputs means no need for this test','line_number':1543,'multiline':False]
['text':' No assertion, we're just checking this doesn't throw an error','line_number':1550,'multiline':False]
['text':' No assertion, we're just checking this doesn't throw an error','line_number':1555,'multiline':False]
['text':' After testing that the model accepts all int inputs, confirm that its dummies are int32','line_number':1557,'multiline':False]
['text':' Also confirm that the input_signature uses int32','line_number':1566,'multiline':False]
['text':' We want to test only encoder-decoder models','line_number':1578,'multiline':False]
['text':' We check the state of decoder_attentions and cross_attentions just from the last step','line_number':1601,'multiline':False]
['text':' Fails when we don't set ignore_mismatched_sizes=True','line_number':1621,'multiline':False]
['text':' Although Tf models always have a prefix pointing to `MainLayer`,','line_number':1643,'multiline':False]
['text':' we still add this "without prefix" test to keep a consistency between tf and pt tests.','line_number':1644,'multiline':False]
['text':' The main input is the name of the argument after `self`','line_number':1654,'multiline':False]
['text':' This is some kinda funky decoder model that needs labels in its forward pass','line_number':1664,'multiline':False]
['text':' Use a random other tensor','line_number':1670,'multiline':False]
['text':' Assert we didn't lose any data','line_number':1677,'multiline':False]
['text':' Assert we discarded the unwanted extra column but kept everything else','line_number':1679,'multiline':False]
['text':' Assert we didn't lose any data','line_number':1684,'multiline':False]
['text':' This model isn't giving us labels after all, don't try training with it','line_number':1690,'multiline':False]
['text':' Use a random other tensor','line_number':1692,'multiline':False]
['text':' Assert the labels are present','line_number':1698,'multiline':False]
['text':' Assert we discarded the unwanted extra column but kept everything else','line_number':1701,'multiline':False]
['text':' make sure there are no pad tokens in prompt, which may trigger unwanted behavior','line_number':1714,'multiline':False]
['text':' Due to numerical instability, let's fail the test only if there are more than 10% of input sequences give','line_number':1732,'multiline':False]
['text':' different outputs between XLA and non-XLA versions. If there are less than 10 examples, let's be strict','line_number':1733,'multiline':False]
['text':' and not allow any difference.','line_number':1734,'multiline':False]
['text':' Generate until max length','line_number':1746,'multiline':False]
['text':' fix config for models with additional sequence-length limiting settings','line_number':1749,'multiline':False]
['text':' xlnet will raise an exception when trying to set','line_number':1756,'multiline':False]
['text':' max_position_embeddings.','line_number':1757,'multiline':False]
['text':' special tokens cannot be bad tokens','line_number':1800,'multiline':False]
['text':' create random bad tokens that are not special tokens','line_number':1809,'multiline':False]
['text':' for all bad word tokens','line_number':1823,'multiline':False]
['text':' for all slices in batch','line_number':1825,'multiline':False]
['text':' for all word idx','line_number':1827,'multiline':False]
['text':' if tokens match','line_number':1829,'multiline':False]
['text':' make sure that at least one token is attended to for each batch','line_number':1855,'multiline':False]
