['text':' coding=utf-8','line_number':1,'multiline':False]
['text':' Copyright 2018 the HuggingFace Inc. team.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' Licensed under the Apache License, Version 2.0 (the "License");','line_number':4,'multiline':False]
['text':' you may not use this file except in compliance with the License.','line_number':5,'multiline':False]
['text':' You may obtain a copy of the License at','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':'     http://www.apache.org/licenses/LICENSE-2.0','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Unless required by applicable law or agreed to in writing, software','line_number':10,'multiline':False]
['text':' distributed under the License is distributed on an "AS IS" BASIS,','line_number':11,'multiline':False]
['text':' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.','line_number':12,'multiline':False]
['text':' See the License for the specific language governing permissions and','line_number':13,'multiline':False]
['text':' limitations under the License.','line_number':14,'multiline':False]
['text':' save resources not dealing with reporting unless specified (also avoids the warning when it's not set)','line_number':152,'multiline':False]
['text':' can be explicitly disabled via `keep_report_to`','line_number':153,'multiline':False]
['text':' For easy batching, we make every batch_size consecutive samples the same size.','line_number':175,'multiline':False]
['text':' We infer the correct model class if one uses gradient_checkpointing or not','line_number':378,'multiline':False]
['text':' We'll pop things so operate on copies.','line_number':448,'multiline':False]
['text':' Log history main contain different logs for the time metrics (after resuming a training).','line_number':451,'multiline':False]
['text':' Converts a checkpoint of a regression model to a sharded checkpoint.','line_number':463,'multiline':False]
['text':' Checks a training seeded with learning_rate = 0.1','line_number':526,'multiline':False]
['text':' Checks that training worked, model trained and seed made a reproducible training.','line_number':532,'multiline':False]
['text':' Checks that a different seed gets different (reproducible) results.','line_number':537,'multiline':False]
['text':' Base training. Should have the same results as test_reproducible_training','line_number':550,'multiline':False]
['text':' Can return tensors.','line_number':557,'multiline':False]
['text':' Adding one column not used by the model should have no impact','line_number':564,'multiline':False]
['text':' Re-training should restart from scratch, thus lead the same results.','line_number':579,'multiline':False]
['text':' Re-training should restart from scratch, thus lead the same results and new seed should be used.','line_number':583,'multiline':False]
['text':' Training with half the batch size but accumulation steps as 2 should give the same results.','line_number':589,'multiline':False]
['text':' Check if model weights have been updated','line_number':607,'multiline':False]
['text':' With even logs','line_number':617,'multiline':False]
['text':' With uneven logs','line_number':626,'multiline':False]
['text':' Training loss should be the same as before','line_number':631,'multiline':False]
['text':' test scheduler kwargs passed via TrainingArguments','line_number':650,'multiline':False]
['text':' Non-default arguments','line_number':654,'multiline':False]
['text':' Checking that the scheduler was created','line_number':665,'multiline':False]
['text':' Checking that the correct args were passed','line_number':668,'multiline':False]
['text':' test passed arguments for a custom ReduceLROnPlateau scheduler','line_number':677,'multiline':False]
['text':' test the ReduceLROnPlateau scheduler','line_number':699,'multiline':False]
['text':' the LR is computed after metrics and does not exist for the first epoch','line_number':703,'multiline':False]
['text':' Compare learning rate to next epoch's','line_number':729,'multiline':False]
['text':' test the special case where lr=None, since Trainer can't not have lr_scheduler','line_number':745,'multiline':False]
['text':' very basic test','line_number':765,'multiline':False]
['text':' --bf16 --half_precision_backend apex can't be used together','line_number':770,'multiline':False]
['text':' will add more specific tests once there are some bugs to fix','line_number':774,'multiline':False]
['text':' very basic test','line_number':779,'multiline':False]
['text':' Edge case because Apex with mode O2 will change our models to return dicts. This test checks it doesn't break','line_number':796,'multiline':False]
['text':' anything.','line_number':797,'multiline':False]
['text':' By default the past_key_values are removed','line_number':814,'multiline':False]
['text':' We can still get them by setting ignore_keys to []','line_number':817,'multiline':False]
['text':' Logging dir can be slightly different as they default to something with the time.','line_number':828,'multiline':False]
['text':' Regular training has n_epochs * len(train_dl) steps','line_number':833,'multiline':False]
['text':' Check passing num_train_epochs works (and a float version too):','line_number':838,'multiline':False]
['text':' If we pass a max_steps, num_train_epochs is ignored','line_number':843,'multiline':False]
['text':' Regular training has n_epochs * len(train_dl) steps','line_number':852,'multiline':False]
['text':' Check passing num_train_epochs works (and a float version too):','line_number':857,'multiline':False]
['text':' If we pass a max_steps, num_train_epochs is ignored','line_number':864,'multiline':False]
['text':' Trainer without inf/nan filter','line_number':877,'multiline':False]
['text':' redefine the model','line_number':892,'multiline':False]
['text':' Trainer without inf/nan filter','line_number':894,'multiline':False]
['text':' Check that it trains without errors','line_number':900,'multiline':False]
['text':' Make sure forward pass works fine','line_number':903,'multiline':False]
['text':' Check that we get identical embeddings just in case','line_number':909,'multiline':False]
['text':' Trainer without inf/nan filter','line_number':921,'multiline':False]
['text':' Trainer with inf/nan filter','line_number':927,'multiline':False]
['text':' Check drop_last works','line_number':947,'multiline':False]
['text':' Check passing a new dataset for evaluation works','line_number':965,'multiline':False]
['text':' tests that we do not require dataloader to have a .dataset attribute','line_number':969,'multiline':False]
['text':' Make the Trainer believe it's a parallelized model','line_number':981,'multiline':False]
['text':' Check the Trainer was fooled','line_number':986,'multiline':False]
['text':' The batch size of the training and evaluation dataloaders should be 16, not 16 * n_gpu','line_number':990,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1007,'multiline':False]
['text':' With logits preprocess','line_number':1018,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1045,'multiline':False]
['text':' With logits preprocess','line_number':1058,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1091,'multiline':False]
['text':' With logits preprocess','line_number':1110,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1135,'multiline':False]
['text':' With more than one output of the model','line_number':1141,'multiline':False]
['text':' With more than one output/label of the model','line_number':1149,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1167,'multiline':False]
['text':' With more than one output of the model','line_number':1173,'multiline':False]
['text':' With more than one output/label of the model','line_number':1181,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1204,'multiline':False]
['text':' With more than one output of the model','line_number':1210,'multiline':False]
['text':' With more than one output/label of the model','line_number':1220,'multiline':False]
['text':' Check evaluation can run to completion','line_number':1246,'multiline':False]
['text':' Check predictions','line_number':1249,'multiline':False]
['text':' Same tests with eval accumulation','line_number':1259,'multiline':False]
['text':' Check evaluation can run to completion','line_number':1263,'multiline':False]
['text':' Check predictions','line_number':1266,'multiline':False]
['text':' testing only --log_level (--log_level_replica requires multiple gpus and DDP and is tested elsewhere)','line_number':1277,'multiline':False]
['text':' test with the default log_level - should be the same as before and thus we test depending on is_info','line_number':1281,'multiline':False]
['text':' test with low log_level - lower than info','line_number':1291,'multiline':False]
['text':' test with high log_level - should be quiet','line_number':1297,'multiline':False]
['text':' With a regular model that is not a PreTrainedModel','line_number':1309,'multiline':False]
['text':' Attach unsaveable tokenizer to partially fail checkpointing','line_number':1322,'multiline':False]
['text':' With a regular model that is not a PreTrainedModel','line_number':1338,'multiline':False]
['text':' test that we don't wrap the model more than once','line_number':1350,'multiline':False]
['text':' since wrapping primarily happens on multi-gpu setup we want multiple gpus to test for','line_number':1351,'multiline':False]
['text':' example DataParallel(DataParallel(model))','line_number':1352,'multiline':False]
['text':' This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of','line_number':1363,'multiline':False]
['text':' save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model','line_number':1364,'multiline':False]
['text':' won't be the same since the training dataloader is shuffled).','line_number':1365,'multiline':False]
['text':' Reinitialize trainer','line_number':1382,'multiline':False]
['text':' Now check with a later checkpoint that it also works when we span over one epoch','line_number':1392,'multiline':False]
['text':' Reinitialize trainer and load model','line_number':1395,'multiline':False]
['text':' With a regular model that is not a PreTrainedModel','line_number':1405,'multiline':False]
['text':' Reinitialize trainer and load model','line_number':1422,'multiline':False]
['text':' Now check with a later checkpoint that it also works when we span over one epoch','line_number':1432,'multiline':False]
['text':' Reinitialize trainer and load model','line_number':1435,'multiline':False]
['text':' Now check failures','line_number':1445,'multiline':False]
['text':' 1. fail to find a bogus checkpoint','line_number':1447,'multiline':False]
['text':' 2. fail to find any checkpoint - due a fresh output_dir','line_number':1453,'multiline':False]
['text':' For more than 1 GPUs, since the randomness is introduced in the model and with DataParallel (which is used','line_number':1461,'multiline':False]
['text':' in this test for more than 2 GPUs), the calls to the torch RNG will happen in a random order (sometimes','line_number':1462,'multiline':False]
['text':' GPU 0 will call first and sometimes GPU 1).','line_number':1463,'multiline':False]
['text':' There should be one checkpoint per epoch.','line_number':1505,'multiline':False]
['text':' simulate OOM on the first step','line_number':1560,'multiline':False]
['text':' After `auto_find_batch_size` is ran we should now be at 8','line_number':1574,'multiline':False]
['text':' We can then make a new Trainer','line_number':1577,'multiline':False]
['text':' Check we are at 16 to start','line_number':1579,'multiline':False]
['text':' We should be back to 8 again, picking up based upon the last ran Trainer','line_number':1582,'multiline':False]
['text':' regression for this issue: https://github.com/huggingface/transformers/issues/12970','line_number':1585,'multiline':False]
['text':' This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of','line_number':1601,'multiline':False]
['text':' save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model','line_number':1602,'multiline':False]
['text':' won't be the same since the training dataloader is shuffled).','line_number':1603,'multiline':False]
['text':' Reinitialize trainer','line_number':1614,'multiline':False]
['text':' This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of','line_number':1627,'multiline':False]
['text':' save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model','line_number':1628,'multiline':False]
['text':' won't be the same since the training dataloader is shuffled).','line_number':1629,'multiline':False]
['text':' Reinitialize trainer','line_number':1648,'multiline':False]
['text':' This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of','line_number':1662,'multiline':False]
['text':' save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model','line_number':1663,'multiline':False]
['text':' won't be the same since the training dataloader is shuffled).','line_number':1664,'multiline':False]
['text':' Reinitialize trainer','line_number':1681,'multiline':False]
['text':' This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of','line_number':1700,'multiline':False]
['text':' save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model','line_number':1701,'multiline':False]
['text':' won't be the same since the training dataloader is shuffled).','line_number':1702,'multiline':False]
['text':' Reinitialize trainer','line_number':1719,'multiline':False]
['text':' Test this works with a non PreTrainedModel','line_number':1793,'multiline':False]
['text':' Adding one column not used by the model should have no impact','line_number':1862,'multiline':False]
['text':' Adding one column not used by the model should have no impact','line_number':1877,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1891,'multiline':False]
['text':' With a number of elements not a round multiple of the batch size','line_number':1914,'multiline':False]
['text':' Adding one column not used by the model should have no impact','line_number':1915,'multiline':False]
['text':' len(train_dl) < gradient_accumulation_steps shouldn't give ``ZeroDivisionError`` when ``max_steps`` is given.','line_number':1922,'multiline':False]
['text':' It should give 1 update step for each epoch.','line_number':1923,'multiline':False]
['text':' Even ``max_steps`` is not specified, we still expect 1 update step for each epoch if','line_number':1930,'multiline':False]
['text':' len(train_dl) < gradient_accumulation_steps.','line_number':1931,'multiline':False]
['text':' early stopping stops training before num_training_epochs','line_number':1937,'multiline':False]
['text':' Invalid inputs to trainer with early stopping callback result in assertion error','line_number':1954,'multiline':False]
['text':' with plain model','line_number':1979,'multiline':False]
['text':' with enforced DataParallel','line_number':1982,'multiline':False]
['text':' Make fake checkpoints','line_number':1989,'multiline':False]
['text':' Without best model at end','line_number':1999,'multiline':False]
['text':' With best model at end','line_number':2003,'multiline':False]
['text':' Edge case: we don't always honor save_total_limit=1 if load_best_model_at_end=True to be able to resume','line_number':2010,'multiline':False]
['text':' from checkpoint','line_number':2011,'multiline':False]
['text':' with mem metrics enabled','line_number':2040,'multiline':False]
['text':' with mem metrics disabled','line_number':2044,'multiline':False]
['text':' this is a sensitive test so let's keep debugging printouts in place for quick diagnosis.','line_number':2050,'multiline':False]
['text':' it's using pretty large safety margins, but small enough to detect broken functionality.','line_number':2051,'multiline':False]
['text':' make the params somewhat big so that there will be enough RAM consumed to be able to','line_number':2057,'multiline':False]
['text':' measure things. We should get about 64KB for a+b in fp32','line_number':2058,'multiline':False]
['text':' 1. with fp16_full_eval disabled','line_number':2062,'multiline':False]
['text':' here we expect the model to be preloaded in trainer.__init__ and consume around 64K gpu ram.','line_number':2075,'multiline':False]
['text':' perfect world: fp32_init == 64<<10','line_number':2076,'multiline':False]
['text':' after eval should be no extra memory allocated - with a small margin (other than the peak','line_number':2078,'multiline':False]
['text':' memory consumption for the forward calculation that gets recovered)','line_number':2079,'multiline':False]
['text':' perfect world: fp32_eval == close to zero','line_number':2080,'multiline':False]
['text':' 2. with fp16_full_eval enabled','line_number':2083,'multiline':False]
['text':' here we expect the model to not be preloaded in trainer.__init__, so with a small margin it should be close to 0','line_number':2093,'multiline':False]
['text':' perfect world: fp16_init == close to zero','line_number':2094,'multiline':False]
['text':' here we put the model on device in eval and only `half()` of it, i.e. about 32K,(again we ignore the peak margin which gets returned back)','line_number':2096,'multiline':False]
['text':' perfect world: fp32_init == 32<<10','line_number':2097,'multiline':False]
['text':' 3. relative comparison fp32 vs full fp16','line_number':2100,'multiline':False]
['text':' should be about half of fp16_init','line_number':2101,'multiline':False]
['text':' perfect world: fp32_init/2 == fp16_eval','line_number':2102,'multiline':False]
['text':' torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu','line_number':2111,'multiline':False]
['text':' make the params are somewhat big so that there will be enough RAM consumed to be able to','line_number':2116,'multiline':False]
['text':' measure things. We should get about 64KB for a+b in fp32','line_number':2117,'multiline':False]
['text':' 1. Default - without TorchDynamo','line_number':2121,'multiline':False]
['text':' 2. TorchDynamo eager','line_number':2127,'multiline':False]
['text':' 3. TorchDynamo nvfuser','line_number':2134,'multiline':False]
['text':' 4. TorchDynamo fx2trt','line_number':2140,'multiline':False]
['text':' torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu','line_number':2150,'multiline':False]
['text':' 1. without TorchDynamo (eager baseline)','line_number':2174,'multiline':False]
['text':' warmup','line_number':2178,'multiline':False]
['text':' resets','line_number':2182,'multiline':False]
['text':' 2. TorchDynamo nvfuser','line_number':2192,'multiline':False]
['text':' warmup','line_number':2197,'multiline':False]
['text':' resets','line_number':2201,'multiline':False]
['text':' Functional check','line_number':2211,'multiline':False]
['text':' AOT Autograd recomputaion and nvfuser recomputation optimization','line_number':2214,'multiline':False]
['text':' aggressively fuses the operations and reduce the memory footprint.','line_number':2215,'multiline':False]
['text':' note: most of the logic is the same as test_fp16_full_eval','line_number':2221,'multiline':False]
['text':' this is a sensitive test so let's keep debugging printouts in place for quick diagnosis.','line_number':2223,'multiline':False]
['text':' it's using pretty large safety margins, but small enough to detect broken functionality.','line_number':2224,'multiline':False]
['text':' make the params somewhat big so that there will be enough RAM consumed to be able to','line_number':2230,'multiline':False]
['text':' measure things. We should get about 64KB for a+b in fp32','line_number':2231,'multiline':False]
['text':' 1. with bf16_full_eval disabled','line_number':2235,'multiline':False]
['text':' here we expect the model to be preloaded in trainer.__init__ and consume around 64K gpu ram.','line_number':2248,'multiline':False]
['text':' perfect world: fp32_init == 64<<10','line_number':2249,'multiline':False]
['text':' after eval should be no extra memory allocated - with a small margin (other than the peak','line_number':2251,'multiline':False]
['text':' memory consumption for the forward calculation that gets recovered)','line_number':2252,'multiline':False]
['text':' perfect world: fp32_eval == close to zero','line_number':2253,'multiline':False]
['text':' 2. with bf16_full_eval enabled','line_number':2256,'multiline':False]
['text':' here we expect the model to not be preloaded in trainer.__init__, so with a small margin it should be close to 0','line_number':2266,'multiline':False]
['text':' perfect world: bf16_init == close to zero','line_number':2267,'multiline':False]
['text':' here we put the model on device in eval and only `half()` of it, i.e. about 32K,(again we ignore the peak margin which gets returned back)','line_number':2269,'multiline':False]
['text':' perfect world: fp32_init == 32<<10','line_number':2270,'multiline':False]
['text':' 3. relative comparison fp32 vs full bf16','line_number':2273,'multiline':False]
['text':' should be about half of bf16_init','line_number':2274,'multiline':False]
['text':' perfect world: fp32_init/2 == bf16_eval','line_number':2275,'multiline':False]
['text':' fmt: skip','line_number':2282,'multiline':False]
['text':' Tests that `translation.py` will run without issues','line_number':2291,'multiline':False]
['text':' successful return here == success - any errors would have caused an error or a timeout in the sub-call','line_number':2331,'multiline':False]
['text':' Extract repo_name from the url','line_number':2364,'multiline':False]
['text':' Extract repo_name from the url','line_number':2387,'multiline':False]
['text':' To avoid any flakiness if the training goes faster than the uploads.','line_number':2415,'multiline':False]
['text':' To avoid any flakiness if the training goes faster than the uploads.','line_number':2437,'multiline':False]
['text':' max_steps depend on the number of available GPUs','line_number':2448,'multiline':False]
['text':' Push the runs via `push_to_hub()`','line_number':2464,'multiline':False]
['text':' exercises all the valid --optim options','line_number':2837,'multiline':False]
['text':' Pretend that apex is installed and mock apex.optimizers.FusedAdam exists.','line_number':2844,'multiline':False]
['text':' Trainer.get_optimizer_cls_and_kwargs does not use FusedAdam. It only has to return the','line_number':2845,'multiline':False]
['text':' class given, so mocking apex.optimizers.FusedAdam should be fine for testing and allow','line_number':2846,'multiline':False]
['text':' the test to run without requiring an apex installation.','line_number':2847,'multiline':False]
['text':' Pretend that apex does not exist, even if installed. By setting apex to None, importing','line_number':2864,'multiline':False]
['text':' apex will fail even if apex is installed.','line_number':2865,'multiline':False]
['text':' Pretend that Bits and Bytes is installed and mock bnb.optim.Adam8bit exists.','line_number':2871,'multiline':False]
['text':' Trainer.get_optimizer_cls_and_kwargs does not use Adam8bit. It only has to return the','line_number':2872,'multiline':False]
['text':' class given, so mocking bnb.optim.Adam8bit should be fine for testing and allow','line_number':2873,'multiline':False]
['text':' the test to run without requiring a bnb installation.','line_number':2874,'multiline':False]
['text':' Pretend that bnb does not exist, even if installed. By setting bnb to None, importing','line_number':2989,'multiline':False]
['text':' bnb will fail even if bnb is installed.','line_number':2990,'multiline':False]
['text':' Pretend that bnb does not exist, even if installed. By setting bnb to None, importing','line_number':2998,'multiline':False]
['text':' bnb will fail even if bnb is installed.','line_number':2999,'multiline':False]
['text':' Pretend that bnb does not exist, even if installed. By setting bnb to None, importing','line_number':3007,'multiline':False]
['text':' bnb will fail even if bnb is installed.','line_number':3008,'multiline':False]
['text':' Pretend that bnb does not exist, even if installed. By setting bnb to None, importing','line_number':3016,'multiline':False]
['text':' bnb will fail even if bnb is installed.','line_number':3017,'multiline':False]
['text':' Pretend that bnb does not exist, even if installed. By setting bnb to None, importing','line_number':3025,'multiline':False]
['text':' bnb will fail even if bnb is installed.','line_number':3026,'multiline':False]
['text':' Pretend that torchdistx is installed and mock torchdistx.optimizers.AnyPrecisionAdamW exists.','line_number':3032,'multiline':False]
['text':' Trainer.get_optimizer_cls_and_kwargs does not use AnyPrecisioinAdamW. It only has to return the','line_number':3033,'multiline':False]
['text':' class given, so mocking torchdistx.optimizers.AnyPrecisionAdamW should be fine for testing and allow','line_number':3034,'multiline':False]
['text':' the test to run without requiring a bnb installation.','line_number':3035,'multiline':False]
['text':' Pretend that torchdistx does not exist, even if installed. By setting torchdistx to None, importing','line_number':3052,'multiline':False]
['text':' torchdistx.optimizers will fail even if torchdistx is installed.','line_number':3053,'multiline':False]
