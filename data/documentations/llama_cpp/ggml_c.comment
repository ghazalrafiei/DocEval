['text':' Disables ridiculous "unsafe" warnings on Windows','line_number':1,'multiline':False]
['text':' For M_PI on MSVC','line_number':2,'multiline':False]
['text':' using malloc.h with MSC/MINGW','line_number':8,'multiline':False]
['text':' disable "possible loss of data" to avoid hundreds of casts','line_number':32,'multiline':False]
['text':' we should just be careful :)','line_number':33,'multiline':False]
['text':' disable POSIX deprecation warnings','line_number':36,'multiline':False]
['text':' these functions are never going away, anyway','line_number':37,'multiline':False]
['text':'
    #include <execinfo.h>
    #include <dlfcn.h>

    void * trace[100];

    int nptrs = backtrace(trace, sizeof(trace)/sizeof(trace[0]));

    backtrace_symbols_fd(trace, nptrs, STDERR_FILENO);
    ','line_number':113,'multiline':True]
['text':' backtrack_symbols does not show line numbers, use gdb instead','line_number':124,'multiline':False]
['text':' platform not supported','line_number':142,'multiline':False]
['text':'#define GGML_PERF','line_number':146,'multiline':True]
['text':' #define GGML_CROSS_ENTROPY_EXP_FP16','line_number':151,'multiline':False]
['text':' #define GGML_FLASH_ATTN_EXP_FP16','line_number':152,'multiline':False]
['text':'','line_number':158,'multiline':False]
['text':' logging','line_number':159,'multiline':False]
['text':'','line_number':160,'multiline':False]
['text':'','line_number':182,'multiline':False]
['text':' end of logging block','line_number':183,'multiline':False]
['text':'','line_number':184,'multiline':False]
['text':' uncomment to use vDSP for soft max computation','line_number':187,'multiline':False]
['text':' note: not sure if it is actually faster','line_number':188,'multiline':False]
['text':'#define GGML_SOFT_MAX_ACCELERATE','line_number':189,'multiline':False]
['text':' Handle allocation failure','line_number':210,'multiline':False]
['text':' allow usage of CLBlast alongside Accelerate functions','line_number':238,'multiline':False]
['text':' floating point type used to accumulate sums','line_number':253,'multiline':False]
['text':'','line_number':262,'multiline':False]
['text':' global data','line_number':263,'multiline':False]
['text':'','line_number':264,'multiline':False]
['text':' precomputed gelu table for f16 (128 KB)','line_number':266,'multiline':False]
['text':' precomputed quick gelu table for f16 (128 KB)','line_number':269,'multiline':False]
['text':' precomputed silu table for f16 (128 KB)','line_number':272,'multiline':False]
['text':' precomputed exp table for f16 (128 KB)','line_number':275,'multiline':False]
['text':' precomputed f32 table for f16 (256 KB) (ggml-impl.h)','line_number':278,'multiline':False]
['text':' note: do not use these inside ggml.c','line_number':281,'multiline':False]
['text':' these are meant to be used via the ggml.h API','line_number':282,'multiline':False]
['text':'','line_number':316,'multiline':False]
['text':' timing','line_number':317,'multiline':False]
['text':'','line_number':318,'multiline':False]
['text':' The multiplication by 1000 or 1000000 below can cause an overflow if timer_freq','line_number':327,'multiline':False]
['text':' and the uptime is high enough.','line_number':328,'multiline':False]
['text':' We subtract the program start time to reduce the likelihood of that happening.','line_number':329,'multiline':False]
['text':'','line_number':378,'multiline':False]
['text':' cache line','line_number':379,'multiline':False]
['text':'','line_number':380,'multiline':False]
['text':' GGML_TYPE_Q4_2','line_number':457,'multiline':False]
['text':' GGML_TYPE_Q4_3','line_number':468,'multiline':False]
['text':' For internal test use','line_number':585,'multiline':False]
['text':'','line_number':591,'multiline':False]
['text':' simd mappings','line_number':592,'multiline':False]
['text':'','line_number':593,'multiline':False]
['text':' 64-bit compatibility','line_number':598,'multiline':False]
['text':' we define a common set of C macros which map to specific intrinsics based on the current architecture','line_number':607,'multiline':False]
['text':' we then implement the fundamental computation operations below using only these macros','line_number':608,'multiline':False]
['text':' adding support for new architectures requires to define the corresponding SIMD macros','line_number':609,'multiline':False]
['text':'','line_number':610,'multiline':False]
['text':' GGML_F32_STEP / GGML_F16_STEP','line_number':611,'multiline':False]
['text':'   number of elements to process in a single step','line_number':612,'multiline':False]
['text':'','line_number':613,'multiline':False]
['text':' GGML_F32_EPR / GGML_F16_EPR','line_number':614,'multiline':False]
['text':'   number of elements to fit in a single register','line_number':615,'multiline':False]
['text':'','line_number':616,'multiline':False]
['text':' F32 NEON','line_number':622,'multiline':False]
['text':' F16 NEON','line_number':663,'multiline':False]
['text':' if FP16 vector arithmetic is not supported, we use FP32 instead','line_number':706,'multiline':False]
['text':' and take advantage of the vcvt_ functions to convert to/from FP16','line_number':707,'multiline':False]
['text':' F32 AVX','line_number':737,'multiline':False]
['text':' TODO: is this optimal ?','line_number':773,'multiline':False]
['text':' F16 AVX','line_number':785,'multiline':False]
['text':' F16 arithmetic is not supported by AVX, so we use F32 instead','line_number':790,'multiline':False]
['text':' the  _mm256_cvt intrinsics require F16C','line_number':797,'multiline':False]
['text':' F32 POWER9','line_number':841,'multiline':False]
['text':' F16 POWER9','line_number':884,'multiline':False]
['text':' Use vec_xl, not vec_ld, in case the load address is not aligned.','line_number':892,'multiline':False]
['text':' F32 WASM','line_number':907,'multiline':False]
['text':' F16 WASM','line_number':950,'multiline':False]
['text':' F32 SSE','line_number':1019,'multiline':False]
['text':' TODO: Does this work?','line_number':1030,'multiline':False]
['text':' TODO: is this optimal ?','line_number':1054,'multiline':False]
['text':' F16 SSE','line_number':1066,'multiline':False]
['text':' GGML_F32_ARR / GGML_F16_ARR','line_number':1115,'multiline':False]
['text':'   number of registers to use per step','line_number':1116,'multiline':False]
['text':'','line_number':1122,'multiline':False]
['text':' fundamental operations','line_number':1123,'multiline':False]
['text':'','line_number':1124,'multiline':False]
['text':' reduce sum0..sum3 to sum0','line_number':1164,'multiline':False]
['text':' leftovers','line_number':1167,'multiline':False]
['text':' scalar','line_number':1172,'multiline':False]
['text':' reduce sum0..sum3 to sum0','line_number':1202,'multiline':False]
['text':' leftovers','line_number':1205,'multiline':False]
['text':' compute GGML_VEC_DOT_UNROLL dot products at once','line_number':1218,'multiline':False]
['text':' xs - x row stride in bytes','line_number':1219,'multiline':False]
['text':' reduce sum0..sum3 to sum0','line_number':1249,'multiline':False]
['text':' leftovers','line_number':1254,'multiline':False]
['text':' leftovers','line_number':1292,'multiline':False]
['text':' scalar','line_number':1297,'multiline':False]
['text':' xs and vs are byte strides of x and v','line_number':1304,'multiline':False]
['text':' leftovers','line_number':1340,'multiline':False]
['text':' scalar','line_number':1347,'multiline':False]
['text':'inline static void ggml_vec_scale_f32(const int n, float * y, const float   v) { for (int i = 0; i < n; ++i) y[i] *= v;          }','line_number':1356,'multiline':False]
['text':' leftovers','line_number':1376,'multiline':False]
['text':' scalar','line_number':1381,'multiline':False]
['text':'inline static void ggml_vec_gelu_quick_f16(const int n, ggml_fp16_t * y, const ggml_fp16_t * x) {','line_number':1436,'multiline':False]
['text':'    const uint16_t * i16 = (const uint16_t *) x;','line_number':1437,'multiline':False]
['text':'    for (int i = 0; i < n; ++i) {','line_number':1438,'multiline':False]
['text':'        y[i] = ggml_table_gelu_quick_f16[i16[i]];','line_number':1439,'multiline':False]
['text':'    }','line_number':1440,'multiline':False]
['text':'}','line_number':1441,'multiline':False]
['text':' Sigmoid Linear Unit (SiLU) function','line_number':1460,'multiline':False]
['text':'inline static void ggml_vec_silu_f16(const int n, ggml_fp16_t * y, const ggml_fp16_t * x) {','line_number':1465,'multiline':False]
['text':'    const uint16_t * i16 = (const uint16_t *) x;','line_number':1466,'multiline':False]
['text':'    for (int i = 0; i < n; ++i) {','line_number':1467,'multiline':False]
['text':'        y[i] = ggml_table_silu_f16[i16[i]];','line_number':1468,'multiline':False]
['text':'    }','line_number':1469,'multiline':False]
['text':'}','line_number':1470,'multiline':False]
['text':' we did not use x[i] to compute forward silu but its f16 equivalent','line_number':1497,'multiline':False]
['text':' take derivative at f16 of x[i]:','line_number':1498,'multiline':False]
['text':'','line_number':1567,'multiline':False]
['text':' data types','line_number':1568,'multiline':False]
['text':'','line_number':1569,'multiline':False]
['text':' WARN:','line_number':1765,'multiline':False]
['text':' Mis-configuration can lead to problem that's hard to reason about:','line_number':1766,'multiline':False]
['text':' * At best  it crash or talks nosense.','line_number':1767,'multiline':False]
['text':' * At worst it talks slightly difference but hard to perceive.','line_number':1768,'multiline':False]
['text':'','line_number':1769,'multiline':False]
['text':' An op has to enable INIT or FINALIZE when any of it's branch needs that pass.','line_number':1770,'multiline':False]
['text':' Take care about compile options (e.g., GGML_USE_xxx).','line_number':1771,'multiline':False]
['text':' INIT','line_number':1776,'multiline':False]
['text':' FINALIZE','line_number':1794,'multiline':False]
['text':'','line_number':1801,'multiline':False]
['text':' ggml context','line_number':1802,'multiline':False]
['text':'','line_number':1803,'multiline':False]
['text':' this is used to save the no_alloc state when using scratch buffers','line_number':1810,'multiline':False]
['text':'','line_number':1827,'multiline':False]
['text':' NUMA support','line_number':1828,'multiline':False]
['text':'','line_number':1829,'multiline':False]
['text':' hardware threads on this node','line_number':1835,'multiline':False]
['text':' hardware threads on system','line_number':1842,'multiline':False]
['text':'','line_number':1845,'multiline':False]
['text':' ggml state','line_number':1846,'multiline':False]
['text':'','line_number':1847,'multiline':False]
['text':' global state','line_number':1854,'multiline':False]
['text':' barrier via spin lock','line_number':1858,'multiline':False]
['text':' wait for other threads to finish','line_number':1863,'multiline':False]
['text':' TODO: reconsider this','line_number':1865,'multiline':False]
['text':' TODO: make this somehow automatically executed','line_number':1870,'multiline':False]
['text':'       some sort of "sentry" mechanism','line_number':1871,'multiline':False]
['text':' enumerate nodes','line_number':1888,'multiline':False]
['text':' enumerate CPUs','line_number':1896,'multiline':False]
['text':' TODO','line_number':1937,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1945,'multiline':False]
['text':' verify t0 is broadcastable','line_number':2092,'multiline':False]
['text':' verify t0 is broadcastable','line_number':2100,'multiline':False]
['text':' check if t1 can be represented as a repeatition of t0','line_number':2181,'multiline':False]
['text':'static inline int ggml_up64(int n) {','line_number':2202,'multiline':False]
['text':'    return (n + 63) & ~63;','line_number':2203,'multiline':False]
['text':'}','line_number':2204,'multiline':False]
['text':' assert m is a power of 2','line_number':2207,'multiline':False]
['text':' assert that pointer is aligned to GGML_MEM_ALIGN','line_number':2212,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':2216,'multiline':False]
['text':' make this function thread safe','line_number':2219,'multiline':False]
['text':' initialize time system (required on Windows)','line_number':2225,'multiline':False]
['text':' initialize GELU, Quick GELU, SILU and EXP F32 tables','line_number':2228,'multiline':False]
['text':' initialize g_state','line_number':2248,'multiline':False]
['text':'.contexts =','line_number':2253,'multiline':True]
['text':'.numa =','line_number':2254,'multiline':True]
['text':' find non-used context in g_state','line_number':2280,'multiline':False]
['text':' allow to call ggml_init with 0 size','line_number':2301,'multiline':False]
['text':'.mem_size           =','line_number':2309,'multiline':True]
['text':'.mem_buffer         =','line_number':2310,'multiline':True]
['text':'.mem_buffer_owned   =','line_number':2311,'multiline':True]
['text':'.no_alloc           =','line_number':2312,'multiline':True]
['text':'.no_alloc_save      =','line_number':2313,'multiline':True]
['text':'.n_objects          =','line_number':2314,'multiline':True]
['text':'.objects_begin      =','line_number':2315,'multiline':True]
['text':'.objects_end        =','line_number':2316,'multiline':True]
['text':'.scratch            =','line_number':2317,'multiline':True]
['text':'.scratch_save       =','line_number':2318,'multiline':True]
['text':' make this function thread safe','line_number':2333,'multiline':False]
['text':' IMPORTANT:','line_number':2411,'multiline':False]
['text':' when creating "opt" tensors, always save and load the scratch buffer','line_number':2412,'multiline':False]
['text':' this is an error prone process, but it is necessary to support inplace','line_number':2413,'multiline':False]
['text':' operators when using scratch buffers','line_number':2414,'multiline':False]
['text':' TODO: implement a better way','line_number':2415,'multiline':False]
['text':' this is needed to allow opt tensors to store their data','line_number':2417,'multiline':False]
['text':' TODO: again, need to find a better way','line_number':2418,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':2432,'multiline':False]
['text':' always insert objects at the end of the context's memory pool','line_number':2435,'multiline':False]
['text':' align to GGML_MEM_ALIGN','line_number':2442,'multiline':False]
['text':' this is the first object in this context','line_number':2467,'multiline':False]
['text':'printf("%s: inserted new object at %zu, size = %zu\n", __func__, cur_end, obj_new->size);','line_number':2473,'multiline':False]
['text':' find the base tensor and absolute offset','line_number':2488,'multiline':False]
['text':' allocate tensor data in the scratch buffer','line_number':2510,'multiline':False]
['text':' allocate tensor data in the context's memory pool','line_number':2522,'multiline':False]
['text':' TODO: for recoverable errors, we would need to free the data allocated from the scratch buffer here','line_number':2529,'multiline':False]
['text':'.type         =','line_number':2534,'multiline':True]
['text':'.backend      =','line_number':2535,'multiline':True]
['text':'.buffer       =','line_number':2536,'multiline':True]
['text':'.ne           =','line_number':2537,'multiline':True]
['text':'.nb           =','line_number':2538,'multiline':True]
['text':'.op           =','line_number':2539,'multiline':True]
['text':'.op_params    =','line_number':2540,'multiline':True]
['text':'.is_param     =','line_number':2541,'multiline':True]
['text':'.grad         =','line_number':2542,'multiline':True]
['text':'.src          =','line_number':2543,'multiline':True]
['text':'.perf_runs    =','line_number':2544,'multiline':True]
['text':'.perf_cycles  =','line_number':2545,'multiline':True]
['text':'.perf_time_us =','line_number':2546,'multiline':True]
['text':'.view_src     =','line_number':2547,'multiline':True]
['text':'.view_offs    =','line_number':2548,'multiline':True]
['text':'.data         =','line_number':2549,'multiline':True]
['text':'.name         =','line_number':2550,'multiline':True]
['text':'.extra        =','line_number':2551,'multiline':True]
['text':'.padding      =','line_number':2552,'multiline':True]
['text':' TODO: this should not be needed as long as we don't rely on aligned SIMD loads','line_number':2555,'multiline':False]
['text':'ggml_assert_aligned(result->data);','line_number':2556,'multiline':False]
['text':' silence -Warray-bounds warnings','line_number':2647,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':3154,'multiline':False]
['text':' ggml_dup','line_number':3156,'multiline':False]
['text':' ggml_add','line_number':3189,'multiline':False]
['text':' TODO: support backward pass for broadcasting','line_number':3201,'multiline':False]
['text':' ggml_add_cast','line_number':3230,'multiline':False]
['text':' TODO: support less-strict constraint','line_number':3237,'multiline':False]
['text':'       GGML_ASSERT(ggml_can_repeat(b, a));','line_number':3238,'multiline':False]
['text':' currently only supported for quantized input and f16','line_number':3240,'multiline':False]
['text':' TODO: support backward pass for broadcasting','line_number':3245,'multiline':False]
['text':' ggml_add1','line_number':3268,'multiline':False]
['text':' ggml_acc','line_number':3308,'multiline':False]
['text':' ggml_sub','line_number':3365,'multiline':False]
['text':' ggml_mul','line_number':3404,'multiline':False]
['text':' TODO: support backward pass for broadcasting','line_number':3416,'multiline':False]
['text':' ggml_div','line_number':3449,'multiline':False]
['text':' ggml_sqr','line_number':3492,'multiline':False]
['text':' ggml_sqrt','line_number':3525,'multiline':False]
['text':' ggml_log','line_number':3558,'multiline':False]
['text':' ggml_sum','line_number':3591,'multiline':False]
['text':' ggml_sum_rows','line_number':3611,'multiline':False]
['text':' ggml_mean','line_number':3636,'multiline':False]
['text':' TODO: implement','line_number':3644,'multiline':False]
['text':' ggml_argmax','line_number':3658,'multiline':False]
['text':' ggml_repeat','line_number':3680,'multiline':False]
['text':' ggml_repeat_back','line_number':3703,'multiline':False]
['text':' ggml_concat','line_number':3730,'multiline':False]
['text':' ggml_abs','line_number':3754,'multiline':False]
['text':' ggml_sgn','line_number':3768,'multiline':False]
['text':' ggml_neg','line_number':3782,'multiline':False]
['text':' ggml_step','line_number':3796,'multiline':False]
['text':' ggml_tanh','line_number':3810,'multiline':False]
['text':' ggml_elu','line_number':3824,'multiline':False]
['text':' ggml_relu','line_number':3838,'multiline':False]
['text':' ggml_leaky_relu','line_number':3852,'multiline':False]
['text':' ggml_gelu','line_number':3873,'multiline':False]
['text':' ggml_gelu_quick','line_number':3887,'multiline':False]
['text':' ggml_silu','line_number':3901,'multiline':False]
['text':' ggml_silu_back','line_number':3915,'multiline':False]
['text':' TODO: implement backward','line_number':3924,'multiline':False]
['text':' ggml_norm','line_number':3938,'multiline':False]
['text':' TODO: implement backward','line_number':3948,'multiline':False]
['text':' ggml_rms_norm','line_number':3977,'multiline':False]
['text':' ggml_rms_norm_back','line_number':4015,'multiline':False]
['text':' TODO: implement backward','line_number':4025,'multiline':False]
['text':' ggml_group_norm','line_number':4041,'multiline':False]
['text':' TODO: implement backward','line_number':4051,'multiline':False]
['text':' TODO: maybe store epsilon here?','line_number':4062,'multiline':False]
['text':' ggml_mul_mat','line_number':4081,'multiline':False]
['text':' ggml_mul_mat_id','line_number':4107,'multiline':False]
['text':' ggml_out_prod','line_number':4152,'multiline':False]
['text':' a is broadcastable to b for ne[2] and ne[3] -> use b->ne[2] and b->ne[3]','line_number':4167,'multiline':False]
['text':' ggml_scale','line_number':4179,'multiline':False]
['text':' ggml_set','line_number':4219,'multiline':False]
['text':' make a view of the destination','line_number':4238,'multiline':False]
['text':' ggml_cpy','line_number':4308,'multiline':False]
['text':' make a view of the destination','line_number':4323,'multiline':False]
['text':' ggml_cont','line_number':4353,'multiline':False]
['text':' make contiguous, with new shape','line_number':4387,'multiline':False]
['text':' ggml_reshape','line_number':4433,'multiline':False]
['text':' as only the shape of b is relevant, and not its memory layout, b is allowed to be non contiguous.','line_number':4440,'multiline':False]
['text':' gradient propagation is not supported','line_number':4450,'multiline':False]
['text':'GGML_ASSERT(false);','line_number':4451,'multiline':False]
['text':' ggml_view_1d','line_number':4591,'multiline':False]
['text':' ggml_view_2d','line_number':4604,'multiline':False]
['text':' ggml_view_3d','line_number':4625,'multiline':False]
['text':' ggml_view_4d','line_number':4648,'multiline':False]
['text':' ggml_permute','line_number':4673,'multiline':False]
['text':' ggml_transpose','line_number':4736,'multiline':False]
['text':' ggml_get_rows','line_number':4763,'multiline':False]
['text':' TODO: implement non F32 return','line_number':4779,'multiline':False]
['text':'struct ggml_tensor * result = ggml_new_tensor_2d(ctx, a->type, a->ne[0], b->ne[0]);','line_number':4780,'multiline':False]
['text':' ggml_get_rows_back','line_number':4791,'multiline':False]
['text':' TODO: implement non F32 return','line_number':4807,'multiline':False]
['text':'struct ggml_tensor * result = ggml_new_tensor_2d(ctx, a->type, a->ne[0], b->ne[0]);','line_number':4808,'multiline':False]
['text':' ggml_diag','line_number':4819,'multiline':False]
['text':' ggml_diag_mask_inf','line_number':4841,'multiline':False]
['text':' ggml_diag_mask_zero','line_number':4880,'multiline':False]
['text':' ggml_soft_max','line_number':4919,'multiline':False]
['text':' ggml_soft_max_back','line_number':4974,'multiline':False]
['text':' TODO : implement backward pass','line_number':4984,'multiline':False]
['text':' ggml_rope','line_number':5011,'multiline':False]
['text':'n_past','line_number':5042,'multiline':True]
['text':' ggml_rope_back','line_number':5135,'multiline':False]
['text':' TODO: implement backward','line_number':5162,'multiline':False]
['text':'n_past','line_number':5167,'multiline':True]
['text':' ggml_alibi','line_number':5186,'multiline':False]
['text':' TODO: implement backward','line_number':5198,'multiline':False]
['text':' TODO: when implement backward, fix this:','line_number':5202,'multiline':False]
['text':'struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);','line_number':5203,'multiline':False]
['text':' ggml_clamp','line_number':5217,'multiline':False]
['text':' TODO: implement backward','line_number':5227,'multiline':False]
['text':' TODO: when implement backward, fix this:','line_number':5231,'multiline':False]
['text':' ggml_conv_1d','line_number':5244,'multiline':False]
['text':' [N, OL, IC * K]','line_number':5257,'multiline':False]
['text':' [N, OL, IC * K] => [N*OL, IC * K]','line_number':5261,'multiline':False]
['text':' [OC，IC, K] => [OC, IC * K]','line_number':5262,'multiline':False]
['text':' [N, OC, OL]','line_number':5264,'multiline':False]
['text':' ggml_conv_1d_ph','line_number':5269,'multiline':False]
['text':' ggml_conv_transpose_1d','line_number':5280,'multiline':False]
['text':' TODO: implement backward','line_number':5303,'multiline':False]
['text':'p0','line_number':5308,'multiline':True]
['text':'d0','line_number':5308,'multiline':True]
['text':' ggml_conv_2d','line_number':5324,'multiline':False]
['text':' im2col: [N, IC, IH, IW] => [N, OH, OW, IC*KH*KW]','line_number':5326,'multiline':False]
['text':' a: [OC，IC, KH, KW]','line_number':5327,'multiline':False]
['text':' b: [N, IC, IH, IW]','line_number':5328,'multiline':False]
['text':' result: [N, OH, OW, IC*KH*KW]','line_number':5329,'multiline':False]
['text':' TODO: implement backward','line_number':5350,'multiline':False]
['text':' a: [OC，IC, KH, KW]','line_number':5376,'multiline':False]
['text':' b: [N, IC, IH, IW]','line_number':5377,'multiline':False]
['text':' result: [N, OC, OH, OW]','line_number':5378,'multiline':False]
['text':' [N, OH, OW, IC * KH * KW]','line_number':5389,'multiline':False]
['text':' [N, OH, OW, IC * KH * KW] => [N*OH*OW, IC * KH * KW]','line_number':5393,'multiline':False]
['text':' [OC，IC, KH, KW] => [OC, IC * KH * KW]','line_number':5394,'multiline':False]
['text':' [N, OC, OH, OW]','line_number':5396,'multiline':False]
['text':' ggml_conv_2d_sk_p0','line_number':5401,'multiline':False]
['text':' ggml_conv_2d_s1_ph','line_number':5409,'multiline':False]
['text':' ggml_conv_transpose_2d_p0','line_number':5418,'multiline':False]
['text':' TODO: implement backward','line_number':5434,'multiline':False]
['text':'p0','line_number':5439,'multiline':True]
['text':'p1','line_number':5440,'multiline':True]
['text':' ggml_pool_*','line_number':5456,'multiline':False]
['text':' ggml_pool_1d','line_number':5462,'multiline':False]
['text':' TODO: implement backward','line_number':5475,'multiline':False]
['text':' ggml_pool_2d','line_number':5495,'multiline':False]
['text':' TODO: implement backward','line_number':5511,'multiline':False]
['text':' ggml_upscale','line_number':5532,'multiline':False]
['text':' TODO: implement backward','line_number':5541,'multiline':False]
['text':' TODO: implement backward','line_number':5566,'multiline':False]
['text':' ggml_argsort','line_number':5590,'multiline':False]
['text':' ggml_top_k','line_number':5609,'multiline':False]
['text':' ggml_flash_attn','line_number':5627,'multiline':False]
['text':' TODO: check if vT can be multiplied by (k*qT)','line_number':5636,'multiline':False]
['text':'struct ggml_tensor * result = ggml_dup_tensor(ctx, q);','line_number':5644,'multiline':False]
['text':' ggml_flash_ff','line_number':5659,'multiline':False]
['text':' TODO: more checks','line_number':5669,'multiline':False]
['text':'struct ggml_tensor * result = ggml_dup_tensor(ctx, a);','line_number':5677,'multiline':False]
['text':' ggml_flash_attn_back','line_number':5691,'multiline':False]
['text':' TODO: check if vT can be multiplied by (k*qT)','line_number':5701,'multiline':False]
['text':' d shape [D,N,ne2,ne3]','line_number':5703,'multiline':False]
['text':' q shape [D,N,ne2,ne3]','line_number':5704,'multiline':False]
['text':' k shape [D,M,kvne2,ne3]','line_number':5705,'multiline':False]
['text':' v shape [M,D,kvne2,ne3]','line_number':5706,'multiline':False]
['text':' when using this operation (in backwards pass) these grads are set.','line_number':5732,'multiline':False]
['text':' we don't want to create (big) grad of our result, so is_node is false.','line_number':5733,'multiline':False]
['text':' store gradients of q, k and v as continuous tensors concatenated in result.','line_number':5737,'multiline':False]
['text':' note: v and gradv are actually transposed, i.e. v->ne[0] != D.','line_number':5738,'multiline':False]
['text':' ggml_win_part','line_number':5769,'multiline':False]
['text':' TODO: implement backward','line_number':5781,'multiline':False]
['text':' padding','line_number':5785,'multiline':False]
['text':' ggml_win_unpart','line_number':5806,'multiline':False]
['text':' TODO: implement backward','line_number':5819,'multiline':False]
['text':' ggml_get_rel_pos','line_number':5836,'multiline':False]
['text':' TODO: implement backward','line_number':5849,'multiline':False]
['text':' ggml_add_rel_pos','line_number':5864,'multiline':False]
['text':' gmml_unary','line_number':5916,'multiline':False]
['text':' ggml_map_unary','line_number':5954,'multiline':False]
['text':' ggml_map_binary','line_number':5992,'multiline':False]
['text':' ggml_map_custom1_f32','line_number':6036,'multiline':False]
['text':' ggml_map_custom2_f32','line_number':6074,'multiline':False]
['text':' ggml_map_custom3_f32','line_number':6116,'multiline':False]
['text':' ggml_map_custom1','line_number':6162,'multiline':False]
['text':'.fun      =','line_number':6187,'multiline':True]
['text':'.n_tasks  =','line_number':6188,'multiline':True]
['text':'.userdata =','line_number':6189,'multiline':True]
['text':' ggml_map_custom2','line_number':6218,'multiline':False]
['text':'.fun      =','line_number':6245,'multiline':True]
['text':'.n_tasks  =','line_number':6246,'multiline':True]
['text':'.userdata =','line_number':6247,'multiline':True]
['text':' ggml_map_custom3','line_number':6279,'multiline':False]
['text':'.fun      =','line_number':6307,'multiline':True]
['text':'.n_tasks  =','line_number':6308,'multiline':True]
['text':'.userdata =','line_number':6309,'multiline':True]
['text':' ggml_cross_entropy_loss','line_number':6344,'multiline':False]
['text':' ggml_cross_entropy_loss_back','line_number':6367,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':6388,'multiline':False]
['text':' ggml_compute_forward_dup','line_number':6400,'multiline':False]
['text':' thread index','line_number':6417,'multiline':False]
['text':' number of threads','line_number':6418,'multiline':False]
['text':' parallelize by elements','line_number':6420,'multiline':False]
['text':' thread index','line_number':6446,'multiline':False]
['text':' number of threads','line_number':6447,'multiline':False]
['text':' parallelize by rows','line_number':6454,'multiline':False]
['text':' number of rows per thread','line_number':6456,'multiline':False]
['text':' row range for this thread','line_number':6458,'multiline':False]
['text':' copy by rows','line_number':6465,'multiline':False]
['text':' TODO: add more special-case implementations for tensor shapes/strides that can benefit from memcpy','line_number':6480,'multiline':False]
['text':' TODO: implement','line_number':6542,'multiline':False]
['text':'printf("%s: this is not optimal - fix me\n", __func__);','line_number':6545,'multiline':False]
['text':' TODO: implement','line_number':6584,'multiline':False]
['text':' dst counters','line_number':6590,'multiline':False]
['text':' TODO: implement','line_number':6701,'multiline':False]
['text':' thread index','line_number':6717,'multiline':False]
['text':' number of threads','line_number':6718,'multiline':False]
['text':' parallelize by rows','line_number':6725,'multiline':False]
['text':' number of rows per thread','line_number':6727,'multiline':False]
['text':' row range for this thread','line_number':6729,'multiline':False]
['text':' copy by rows','line_number':6736,'multiline':False]
['text':' TODO: simplify','line_number':6752,'multiline':False]
['text':' TODO: implement','line_number':6789,'multiline':False]
['text':'printf("%s: this is not optimal - fix me\n", __func__);','line_number':6792,'multiline':False]
['text':' TODO: implement','line_number':6831,'multiline':False]
['text':' dst counters','line_number':6838,'multiline':False]
['text':' TODO: implement','line_number':6950,'multiline':False]
['text':' ggml_compute_forward_add','line_number':6978,'multiline':False]
['text':' rows per thread','line_number':7001,'multiline':False]
['text':' row range for this thread','line_number':7004,'multiline':False]
['text':' src1 is broadcastable across src0 and dst in i1, i2, i3','line_number':7010,'multiline':False]
['text':' src1 is not contiguous','line_number':7033,'multiline':False]
['text':' src1 is broadcastable across src0 and dst in i1, i2, i3','line_number':7035,'multiline':False]
['text':' rows per thread','line_number':7088,'multiline':False]
['text':' row range for this thread','line_number':7091,'multiline':False]
['text':' src0, src1 and dst are same shape => same indices','line_number':7098,'multiline':False]
['text':' src0, src1 and dst are same shape => same indices','line_number':7113,'multiline':False]
['text':' src1 is not contiguous','line_number':7129,'multiline':False]
['text':' rows per thread','line_number':7159,'multiline':False]
['text':' row range for this thread','line_number':7162,'multiline':False]
['text':' src0, src1 and dst are same shape => same indices','line_number':7168,'multiline':False]
['text':' src1 is not contiguous','line_number':7183,'multiline':False]
['text':' we don't support permuted src0 or src1','line_number':7211,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':7215,'multiline':False]
['text':' rows per thread','line_number':7223,'multiline':False]
['text':' row range for this thread','line_number':7226,'multiline':False]
['text':' src0 indices','line_number':7233,'multiline':False]
['text':' src1 and dst are same shape as src0 => same indices','line_number':7238,'multiline':False]
['text':' unquantize row from src0 to temp buffer','line_number':7253,'multiline':False]
['text':' add src1','line_number':7255,'multiline':False]
['text':' quantize row to dst','line_number':7257,'multiline':False]
['text':' ggml_compute_forward_add1','line_number':7308,'multiline':False]
['text':' rows per thread','line_number':7332,'multiline':False]
['text':' row range for this thread','line_number':7335,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7340,'multiline':False]
['text':' scalar to add','line_number':7374,'multiline':False]
['text':' rows per thread','line_number':7391,'multiline':False]
['text':' row range for this thread','line_number':7394,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7399,'multiline':False]
['text':' scalar to add','line_number':7424,'multiline':False]
['text':' rows per thread','line_number':7441,'multiline':False]
['text':' row range for this thread','line_number':7444,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7449,'multiline':False]
['text':' scalar to add','line_number':7474,'multiline':False]
['text':' we don't support permuted src0','line_number':7488,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':7491,'multiline':False]
['text':' rows per thread','line_number':7500,'multiline':False]
['text':' row range for this thread','line_number':7503,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7510,'multiline':False]
['text':' unquantize row from src0 to temp buffer','line_number':7520,'multiline':False]
['text':' add src1','line_number':7522,'multiline':False]
['text':' quantize row to dst','line_number':7524,'multiline':False]
['text':' ggml_compute_forward_acc','line_number':7572,'multiline':False]
['text':' view src0 and dst with these strides and data offset inbytes during acc','line_number':7582,'multiline':False]
['text':' nb0 is implicitly element_size because src0 and dst are contiguous','line_number':7583,'multiline':False]
['text':' memcpy needs to be synchronized across threads to avoid race conditions.','line_number':7591,'multiline':False]
['text':' => do it in INIT phase','line_number':7592,'multiline':False]
['text':' src0 and dst as viewed during acc','line_number':7612,'multiline':False]
['text':' rows per thread','line_number':7625,'multiline':False]
['text':' row range for this thread','line_number':7628,'multiline':False]
['text':' src0 and dst are viewed with shape of src1 and offset','line_number':7633,'multiline':False]
['text':' => same indices','line_number':7634,'multiline':False]
['text':' ggml_compute_forward_sub','line_number':7683,'multiline':False]
['text':' src0, src1 and dst are same shape => same indices','line_number':7706,'multiline':False]
['text':' }','line_number':7723,'multiline':False]
['text':' }','line_number':7724,'multiline':False]
['text':' src1 is not contiguous','line_number':7727,'multiline':False]
['text':' src0, src1 and dst are same shape => same indices','line_number':7729,'multiline':False]
['text':' ggml_compute_forward_mul','line_number':7762,'multiline':False]
['text':' TODO: OpenCL kernel support full broadcast','line_number':7779,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7797,'multiline':False]
['text':' src1 is not contiguous','line_number':7822,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7824,'multiline':False]
['text':' src1 is broadcastable across src0 and dst in i1, i2, i3','line_number':7825,'multiline':False]
['text':' ggml_compute_forward_div','line_number':7866,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7891,'multiline':False]
['text':' src1 is not contiguous','line_number':7916,'multiline':False]
['text':' src0 and dst are same shape => same indices','line_number':7918,'multiline':False]
['text':' src1 is broadcastable across src0 and dst in i1, i2, i3','line_number':7919,'multiline':False]
['text':' ggml_compute_forward_sqr','line_number':7958,'multiline':False]
['text':' ggml_compute_forward_sqrt','line_number':8000,'multiline':False]
['text':' ggml_compute_forward_log','line_number':8042,'multiline':False]
['text':' ggml_compute_forward_sum','line_number':8084,'multiline':False]
['text':' ggml_compute_forward_sum_rows','line_number':8171,'multiline':False]
['text':' ggml_compute_forward_mean','line_number':8222,'multiline':False]
['text':' ggml_compute_forward_argmax','line_number':8277,'multiline':False]
['text':' ggml_compute_forward_repeat','line_number':8323,'multiline':False]
['text':' guaranteed to be an integer due to the check in ggml_can_repeat','line_number':8338,'multiline':False]
['text':' TODO: support for transposed / permuted tensors','line_number':8344,'multiline':False]
['text':' TODO: maybe this is not optimal?','line_number':8348,'multiline':False]
['text':' guaranteed to be an integer due to the check in ggml_can_repeat','line_number':8381,'multiline':False]
['text':' TODO: support for transposed / permuted tensors','line_number':8387,'multiline':False]
['text':' TODO: maybe this is not optimal?','line_number':8391,'multiline':False]
['text':' ggml_vec_cpy_f16(ne00, y, x)','line_number':8401,'multiline':False]
['text':' ggml_compute_forward_repeat_back','line_number':8434,'multiline':False]
['text':' guaranteed to be an integer due to the check in ggml_can_repeat','line_number':8449,'multiline':False]
['text':' TODO: support for transposed / permuted tensors','line_number':8455,'multiline':False]
['text':' TODO: maybe this is not optimal?','line_number':8473,'multiline':False]
['text':' ggml_compute_forward_concat','line_number':8509,'multiline':False]
['text':' TODO: support for transposed / permuted tensors','line_number':8528,'multiline':False]
['text':' src0','line_number':8535,'multiline':False]
['text':' src1','line_number':8544,'multiline':False]
['text':' ggml_compute_forward_abs','line_number':8576,'multiline':False]
['text':' ggml_compute_forward_sgn','line_number':8618,'multiline':False]
['text':' ggml_compute_forward_neg','line_number':8660,'multiline':False]
['text':' ggml_compute_forward_step','line_number':8702,'multiline':False]
['text':' ggml_compute_forward_tanh','line_number':8744,'multiline':False]
['text':' ggml_compute_forward_elu','line_number':8786,'multiline':False]
['text':' ggml_compute_forward_relu','line_number':8828,'multiline':False]
['text':' ggml_compute_forward_gelu','line_number':8870,'multiline':False]
['text':' rows per thread','line_number':8890,'multiline':False]
['text':' row range for this thread','line_number':8893,'multiline':False]
['text':' ggml_compute_forward_gelu_quick','line_number':8929,'multiline':False]
['text':' rows per thread','line_number':8949,'multiline':False]
['text':' row range for this thread','line_number':8952,'multiline':False]
['text':' ggml_compute_forward_silu','line_number':8988,'multiline':False]
['text':' rows per thread','line_number':9008,'multiline':False]
['text':' row range for this thread','line_number':9011,'multiline':False]
['text':' ggml_compute_forward_leaky_relu','line_number':9046,'multiline':False]
['text':' ggml_compute_forward_silu_back','line_number':9091,'multiline':False]
['text':' rows per thread','line_number':9114,'multiline':False]
['text':' row range for this thread','line_number':9117,'multiline':False]
['text':' ggml_compute_forward_norm','line_number':9155,'multiline':False]
['text':' TODO: optimize','line_number':9177,'multiline':False]
['text':' ggml_compute_forward_group_rms_norm','line_number':9224,'multiline':False]
['text':' TODO: optimize','line_number':9246,'multiline':False]
['text':' for (int i00 = 0; i00 < ne00; i00++) {','line_number':9262,'multiline':False]
['text':'     y[i00] = x[i00];','line_number':9263,'multiline':False]
['text':' }','line_number':9264,'multiline':False]
['text':' TODO: optimize','line_number':9311,'multiline':False]
['text':' src1 is same shape as src0 => same indices','line_number':9315,'multiline':False]
['text':'const float mean     = (float)(sum_xx)/ne00;','line_number':9331,'multiline':False]
['text':'const float mean_xdz = (float)(sum_xdz)/ne00;','line_number':9334,'multiline':False]
['text':' we could cache rms from forward pass to improve performance.','line_number':9335,'multiline':False]
['text':' to do this implement ggml_rms and compose ggml_rms_norm using ggml_rms.','line_number':9336,'multiline':False]
['text':'const float rms      = sqrtf(mean_eps);','line_number':9337,'multiline':False]
['text':'const float scale    = -rrms/(ne00 * mean_eps); // -1/(n*rms**3)','line_number':9339,'multiline':False]
['text':' z = rms_norm(x)','line_number':9342,'multiline':False]
['text':'','line_number':9343,'multiline':False]
['text':' rms_norm(src0) =','line_number':9344,'multiline':False]
['text':'     scale(','line_number':9345,'multiline':False]
['text':'         src0,','line_number':9346,'multiline':False]
['text':'         div(','line_number':9347,'multiline':False]
['text':'             1,','line_number':9348,'multiline':False]
['text':'             sqrt(','line_number':9349,'multiline':False]
['text':'                 add(','line_number':9350,'multiline':False]
['text':'                     scale(','line_number':9351,'multiline':False]
['text':'                         sum(','line_number':9352,'multiline':False]
['text':'                             sqr(','line_number':9353,'multiline':False]
['text':'                                 src0)),','line_number':9354,'multiline':False]
['text':'                         (1.0/N)),','line_number':9355,'multiline':False]
['text':'                     eps))));','line_number':9356,'multiline':False]
['text':' postorder:','line_number':9358,'multiline':False]
['text':' ## op    args         grad','line_number':9359,'multiline':False]
['text':' 00 param src0         grad[#00]','line_number':9360,'multiline':False]
['text':' 01 const 1','line_number':9361,'multiline':False]
['text':' 02 sqr   (#00)        grad[#02]','line_number':9362,'multiline':False]
['text':' 03 sum   (#02)        grad[#03]','line_number':9363,'multiline':False]
['text':' 04 const 1/N','line_number':9364,'multiline':False]
['text':' 05 scale (#03, #04)   grad[#05]','line_number':9365,'multiline':False]
['text':' 06 const eps','line_number':9366,'multiline':False]
['text':' 07 add   (#05, #06)   grad[#07]','line_number':9367,'multiline':False]
['text':' 08 sqrt  (#07)        grad[#08]','line_number':9368,'multiline':False]
['text':' 09 div   (#01,#08)    grad[#09]','line_number':9369,'multiline':False]
['text':' 10 scale (#00,#09)    grad[#10]','line_number':9370,'multiline':False]
['text':'','line_number':9371,'multiline':False]
['text':' backward pass, given grad[#10]','line_number':9372,'multiline':False]
['text':' #10: scale','line_number':9373,'multiline':False]
['text':' grad[#00] += scale(grad[#10],#09)','line_number':9374,'multiline':False]
['text':' grad[#09] += sum(mul(grad[#10],#00))','line_number':9375,'multiline':False]
['text':' #09: div','line_number':9376,'multiline':False]
['text':' grad[#08] += neg(mul(grad[#09], div(#09,#08)))','line_number':9377,'multiline':False]
['text':' #08: sqrt','line_number':9378,'multiline':False]
['text':' grad[#07] += mul(grad[#08], div(0.5, #08))','line_number':9379,'multiline':False]
['text':' #07: add','line_number':9380,'multiline':False]
['text':' grad[#05] += grad[#07]','line_number':9381,'multiline':False]
['text':' #05: scale','line_number':9382,'multiline':False]
['text':' grad[#03] += scale(grad[#05],#04)','line_number':9383,'multiline':False]
['text':' #03: sum','line_number':9384,'multiline':False]
['text':' grad[#02] += repeat(grad[#03], #02)','line_number':9385,'multiline':False]
['text':' #02:','line_number':9386,'multiline':False]
['text':' grad[#00] += scale(mul(#00, grad[#02]), 2.0)','line_number':9387,'multiline':False]
['text':'','line_number':9388,'multiline':False]
['text':' substitute and simplify:','line_number':9389,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(mul(#00, grad[#02]), 2.0)','line_number':9390,'multiline':False]
['text':' grad[#02] = repeat(grad[#03], #02)','line_number':9391,'multiline':False]
['text':' grad[#02] = repeat(scale(grad[#05],#04), #02)','line_number':9392,'multiline':False]
['text':' grad[#02] = repeat(scale(grad[#07],#04), #02)','line_number':9393,'multiline':False]
['text':' grad[#02] = repeat(scale(mul(grad[#08], div(0.5, #08)),#04), #02)','line_number':9394,'multiline':False]
['text':' grad[#02] = repeat(scale(mul(neg(mul(grad[#09], div(#09,#08))), div(0.5, #08)),#04), #02)','line_number':9395,'multiline':False]
['text':' grad[#02] = repeat(scale(mul(neg(mul(sum(mul(grad[#10],#00)), div(#09,#08))), div(0.5, #08)),#04), #02)','line_number':9396,'multiline':False]
['text':' grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(#09,#08) * div(0.5, #08) * (1/N)), #02)','line_number':9397,'multiline':False]
['text':' grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(div(#01,#08),#08) * div(0.5, #08) * (1/N)), #02)','line_number':9398,'multiline':False]
['text':' grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(1,#08*#08) * div(0.5, #08) * (1/N)), #02)','line_number':9399,'multiline':False]
['text':' grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N)), #02)','line_number':9400,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(mul(#00, grad[#02]), 2.0)','line_number':9401,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(mul(#00, repeat(-(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N)), #02)), 2.0)','line_number':9402,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(scale(#00, -(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N))), 2.0)','line_number':9403,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, -(sum(mul(grad[#10],#00)) * div(1,#07) * div(1,#08) * (1/N)))','line_number':9404,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,#07*#08) * (-1/N))','line_number':9405,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,#07*#08) * (-1/N))','line_number':9406,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,mean_eps*rms) * (-1/N))','line_number':9407,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*mean_eps))','line_number':9408,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*(sum_xx/N+eps)))','line_number':9409,'multiline':False]
['text':' grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*sum_xx+rms*N*eps))','line_number':9410,'multiline':False]
['text':' grad[#00] = scale(dz, rrms) + scale(x, sum(mul(dz,x)) * div(-1,rms*N*mean_eps))','line_number':9411,'multiline':False]
['text':' grad[#00] = scale(dz, rrms) + scale(x, sum_xdz * div(-1,rms*N*mean_eps))','line_number':9412,'multiline':False]
['text':' a = b*c + d*e','line_number':9413,'multiline':False]
['text':' a = b*c*f/f + d*e*f/f','line_number':9414,'multiline':False]
['text':' a = (b*c*f + d*e*f)*(1/f)','line_number':9415,'multiline':False]
['text':' a = (b*c*(1/c) + d*e*(1/c))*(1/(1/c))','line_number':9416,'multiline':False]
['text':' a = (b + d*e/c)*c','line_number':9417,'multiline':False]
['text':' b = dz, c = rrms, d = x, e = sum_xdz * div(-1,rms*N*mean_eps)','line_number':9418,'multiline':False]
['text':' a = (dz + x*sum_xdz * div(-1,rms*N*mean_eps)/rrms)*rrms','line_number':9419,'multiline':False]
['text':' a = (dz + x*sum_xdz * div(-1,rms*N*mean_eps)*rms)*rrms','line_number':9420,'multiline':False]
['text':' a = (dz + x*sum_xdz * div(-rms,rms*N*mean_eps))*rrms','line_number':9421,'multiline':False]
['text':' a = (dz + x*sum_xdz * div(-1,N*mean_eps))*rrms','line_number':9422,'multiline':False]
['text':' a = (dz + x*div(-sum_xdz,N*mean_eps))*rrms','line_number':9423,'multiline':False]
['text':' a = (dz + x*div(-mean_xdz,mean_eps))*rrms','line_number':9424,'multiline':False]
['text':' grad[#00] = scale(dz + scale(x, div(-mean_xdz,mean_eps)),rrms)','line_number':9425,'multiline':False]
['text':' grad[#00] = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)','line_number':9426,'multiline':False]
['text':' dx = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)','line_number':9427,'multiline':False]
['text':' dx = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)','line_number':9429,'multiline':False]
['text':' post-order:','line_number':9430,'multiline':False]
['text':' dx := x','line_number':9431,'multiline':False]
['text':' dx := scale(dx,-mean_xdz/mean_eps)','line_number':9432,'multiline':False]
['text':' dx := add(dx, dz)','line_number':9433,'multiline':False]
['text':' dx := scale(dx, rrms)','line_number':9434,'multiline':False]
['text':' ggml_vec_scale_f32(ne00, dx, -mean_xdz/mean_eps);','line_number':9438,'multiline':False]
['text':' ggml_compute_forward_group_norm','line_number':9464,'multiline':False]
['text':' TODO: make this a parameter','line_number':9483,'multiline':False]
['text':' TODO: optimize','line_number':9485,'multiline':False]
['text':' ggml_compute_forward_mul_mat','line_number':9554,'multiline':False]
['text':' helper function to determine if it is better to use BLAS or not','line_number':9557,'multiline':False]
['text':' for large matrices, BLAS is faster','line_number':9558,'multiline':False]
['text':'const int64_t ne00 = src0->ne[0];','line_number':9563,'multiline':False]
['text':'const int64_t ne01 = src0->ne[1];','line_number':9564,'multiline':False]
['text':' NOTE: with GGML_OP_MUL_MAT_ID we don't want to go through the BLAS branch because it will dequantize (to_float)','line_number':9571,'multiline':False]
['text':'       all the experts for each batch element and the processing would become incredibly slow','line_number':9572,'multiline':False]
['text':' TODO: find the optimal values for these','line_number':9573,'multiline':False]
['text':'src0->type == GGML_TYPE_F32 &&','line_number':9577,'multiline':False]
['text':'printf("BLAS: %d %d %d %d %d\n", ne0, ne1, ne10, ne00, ne01);','line_number':9581,'multiline':True]
['text':' off1 = offset in i11 and i1','line_number':9589,'multiline':False]
['text':' cne1 = ne11 and ne1','line_number':9590,'multiline':False]
['text':' in a normal matrix multiplication, off1 = 0 and cne1 = ne1','line_number':9591,'multiline':False]
['text':' during GGML_TASK_INIT, the full src1 is converted regardless of off1 and cne1','line_number':9592,'multiline':False]
['text':' we don't support permuted src0 or src1','line_number':9620,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':9624,'multiline':False]
['text':' broadcast factors','line_number':9630,'multiline':False]
['text':' nb01 >= nb00 - src0 is not transposed','line_number':9634,'multiline':False]
['text':'   compute by src0 rows','line_number':9635,'multiline':False]
['text':' broadcast src0 into src1 across 2nd,3rd dimension','line_number':9662,'multiline':False]
['text':'printf("CBLAS = %f ms, %d x %d x %d x %d\n", (ggml_perf_time_us() - t0)/1000.0, ne0, ne1, ne2, ne3);','line_number':9692,'multiline':False]
['text':' src0 rows','line_number':9726,'multiline':False]
['text':' src1 rows','line_number':9727,'multiline':False]
['text':'printf("nr0 = %lld, nr1 = %lld\n", nr0, nr1);','line_number':9729,'multiline':False]
['text':' distribute the thread work across the inner or outer loop based on which one is larger','line_number':9731,'multiline':False]
['text':' parallelize by src0 rows','line_number':9733,'multiline':False]
['text':' parallelize by src1 rows','line_number':9734,'multiline':False]
['text':'printf("ir010 = %6lld, ir011 = %6lld, ir110 = %6lld, ir111 = %6lld\n", ir010, ir011, ir110, ir111);','line_number':9748,'multiline':False]
['text':' threads with no work simply yield (not sure if it helps)','line_number':9750,'multiline':False]
['text':' block-tiling attempt','line_number':9759,'multiline':False]
['text':' attempt to reduce false-sharing (does not seem to make a difference)','line_number':9763,'multiline':False]
['text':' broadcast src0 into src1','line_number':9773,'multiline':False]
['text':' desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides','line_number':9783,'multiline':False]
['text':'       if it is, then we have either copied the data to params->wdata and made it contiguous or we are using','line_number':9784,'multiline':False]
['text':'       the original src1 data pointer, so we should index using the indices directly','line_number':9785,'multiline':False]
['text':' TODO: this is a bit of a hack, we should probably have a better way to handle this','line_number':9786,'multiline':False]
['text':'for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir011; ++ir0) {','line_number':9794,'multiline':False]
['text':'    vec_dot(ne00, &dst_col[ir0], src0_row + ir0*nb01, src1_col);','line_number':9795,'multiline':False]
['text':'}','line_number':9796,'multiline':False]
['text':' ggml_compute_forward_mul_mat_id','line_number':9807,'multiline':False]
['text':' during GGML_TASK_INIT the entire src1 is converted to vec_dot_type','line_number':9816,'multiline':False]
['text':' ggml_compute_forward_out_prod','line_number':9835,'multiline':False]
['text':' int64_t t0 = ggml_perf_time_us();','line_number':9842,'multiline':False]
['text':' UNUSED(t0);','line_number':9843,'multiline':False]
['text':' we don't support permuted src0 or src1','line_number':9857,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':9860,'multiline':False]
['text':' GGML_ASSERT(nb0 <= nb1);','line_number':9862,'multiline':False]
['text':' GGML_ASSERT(nb1 <= nb2);','line_number':9863,'multiline':False]
['text':' GGML_ASSERT(nb2 <= nb3);','line_number':9864,'multiline':False]
['text':' nb01 >= nb00 - src0 is not transposed','line_number':9866,'multiline':False]
['text':'   compute by src0 rows','line_number':9867,'multiline':False]
['text':' TODO: #if defined(GGML_USE_CUBLAS) ggml_cuda_out_prod','line_number':9869,'multiline':False]
['text':' TODO: #if defined(GGML_USE_CLBLAST)','line_number':9870,'multiline':False]
['text':' gemm beta will zero dst','line_number':9880,'multiline':False]
['text':' All threads other than the first do no work.','line_number':9895,'multiline':False]
['text':' Arguments to ggml_compute_forward_out_prod (expressed as major,minor)','line_number':9898,'multiline':False]
['text':' src0: (k,n)','line_number':9899,'multiline':False]
['text':' src1: (k,m)','line_number':9900,'multiline':False]
['text':' dst:  (m,n)','line_number':9901,'multiline':False]
['text':'','line_number':9902,'multiline':False]
['text':' Arguments to sgemm (see https://github.com/Reference-LAPACK/lapack/blob/master/BLAS/SRC/sgemm.f)','line_number':9903,'multiline':False]
['text':' Also expressed as (major,minor)','line_number':9904,'multiline':False]
['text':' a: (m,k): so src1 transposed','line_number':9905,'multiline':False]
['text':' b: (k,n): so src0','line_number':9906,'multiline':False]
['text':' c: (m,n)','line_number':9907,'multiline':False]
['text':'','line_number':9908,'multiline':False]
['text':' However, if ggml_is_transposed(src1) is true, then','line_number':9909,'multiline':False]
['text':' src1->data already contains a transposed version, so sgemm mustn't','line_number':9910,'multiline':False]
['text':' transpose it further.','line_number':9911,'multiline':False]
['text':' dst[:,:,:,:] = 0','line_number':9937,'multiline':False]
['text':' for i2,i3:','line_number':9938,'multiline':False]
['text':'   for i1:','line_number':9939,'multiline':False]
['text':'     for i01:','line_number':9940,'multiline':False]
['text':'       for i0:','line_number':9941,'multiline':False]
['text':'         dst[i0,i1,i2,i3] += src0[i0,i01,i2,i3] * src1[i1,i01,i2,i3]','line_number':9942,'multiline':False]
['text':' parallelize by last three dimensions','line_number':9944,'multiline':False]
['text':' total rows in dst','line_number':9946,'multiline':False]
['text':' rows per thread','line_number':9949,'multiline':False]
['text':' row range for this thread','line_number':9952,'multiline':False]
['text':' block-tiling attempt','line_number':9956,'multiline':False]
['text':' dst indices','line_number':9965,'multiline':False]
['text':'const int64_t i10 = i1;','line_number':9973,'multiline':False]
['text':'int64_t t1 = ggml_perf_time_us();','line_number':10012,'multiline':False]
['text':'static int64_t acc = 0;','line_number':10013,'multiline':False]
['text':'acc += t1 - t0;','line_number':10014,'multiline':False]
['text':'if (t1 - t0 > 10) {','line_number':10015,'multiline':False]
['text':'    printf("\n");','line_number':10016,'multiline':False]
['text':'    printf("ne00 = %5d, ne01 = %5d, ne02 = %5d, ne03 = %5d\n", ne00, ne01, ne02, ne03);','line_number':10017,'multiline':False]
['text':'    printf("nb00 = %5d, nb01 = %5d, nb02 = %5d, nb03 = %5d\n", nb00, nb01, nb02, nb03);','line_number':10018,'multiline':False]
['text':'    printf("ne10 = %5d, ne11 = %5d, ne12 = %5d, ne13 = %5d\n", ne10, ne11, ne12, ne13);','line_number':10019,'multiline':False]
['text':'    printf("nb10 = %5d, nb11 = %5d, nb12 = %5d, nb13 = %5d\n", nb10, nb11, nb12, nb13);','line_number':10020,'multiline':False]
['text':'    printf("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX task %d/%d: %d us, acc = %d\n", ith, nth, (int) (t1 - t0), (int) acc);','line_number':10022,'multiline':False]
['text':'}','line_number':10023,'multiline':False]
['text':' int64_t t0 = ggml_perf_time_us();','line_number':10031,'multiline':False]
['text':' UNUSED(t0);','line_number':10032,'multiline':False]
['text':' we don't support permuted src0 dim0','line_number':10047,'multiline':False]
['text':' dst dim0 cannot be transposed or permuted','line_number':10050,'multiline':False]
['text':' GGML_ASSERT(nb0 <= nb1);','line_number':10052,'multiline':False]
['text':' GGML_ASSERT(nb1 <= nb2);','line_number':10053,'multiline':False]
['text':' GGML_ASSERT(nb2 <= nb3);','line_number':10054,'multiline':False]
['text':' nb01 >= nb00 - src0 is not transposed','line_number':10061,'multiline':False]
['text':'   compute by src0 rows','line_number':10062,'multiline':False]
['text':' TODO: #if defined(GGML_USE_CUBLAS) ggml_cuda_out_prod','line_number':10064,'multiline':False]
['text':' TODO: #if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS) || defined(GGML_USE_CLBLAST)','line_number':10065,'multiline':False]
['text':' parallelize by last three dimensions','line_number':10076,'multiline':False]
['text':' total rows in dst','line_number':10078,'multiline':False]
['text':' rows per thread','line_number':10081,'multiline':False]
['text':' row range for this thread','line_number':10084,'multiline':False]
['text':' dst[:,:,:,:] = 0','line_number':10088,'multiline':False]
['text':' for i2,i3:','line_number':10089,'multiline':False]
['text':'   for i1:','line_number':10090,'multiline':False]
['text':'     for i01:','line_number':10091,'multiline':False]
['text':'       for i0:','line_number':10092,'multiline':False]
['text':'         dst[i0,i1,i2,i3] += src0[i0,i01,i2,i3] * src1[i1,i01,i2,i3]','line_number':10093,'multiline':False]
['text':' dst indices','line_number':10098,'multiline':False]
['text':'const int64_t i10 = i1;','line_number':10106,'multiline':False]
['text':'int64_t t1 = ggml_perf_time_us();','line_number':10122,'multiline':False]
['text':'static int64_t acc = 0;','line_number':10123,'multiline':False]
['text':'acc += t1 - t0;','line_number':10124,'multiline':False]
['text':'if (t1 - t0 > 10) {','line_number':10125,'multiline':False]
['text':'    printf("\n");','line_number':10126,'multiline':False]
['text':'    printf("ne00 = %5d, ne01 = %5d, ne02 = %5d, ne03 = %5d\n", ne00, ne01, ne02, ne03);','line_number':10127,'multiline':False]
['text':'    printf("nb00 = %5d, nb01 = %5d, nb02 = %5d, nb03 = %5d\n", nb00, nb01, nb02, nb03);','line_number':10128,'multiline':False]
['text':'    printf("ne10 = %5d, ne11 = %5d, ne12 = %5d, ne13 = %5d\n", ne10, ne11, ne12, ne13);','line_number':10129,'multiline':False]
['text':'    printf("nb10 = %5d, nb11 = %5d, nb12 = %5d, nb13 = %5d\n", nb10, nb11, nb12, nb13);','line_number':10130,'multiline':False]
['text':'    printf("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX task %d/%d: %d us, acc = %d\n", ith, nth, (int) (t1 - t0), (int) acc);','line_number':10132,'multiline':False]
['text':'}','line_number':10133,'multiline':False]
['text':' todo','line_number':10157,'multiline':False]
['text':' ggml_compute_forward_out_prod_f16_f32(params, src0, src1, dst);','line_number':10158,'multiline':False]
['text':' ggml_compute_forward_scale','line_number':10171,'multiline':False]
['text':' scale factor','line_number':10187,'multiline':False]
['text':' rows per thread','line_number':10196,'multiline':False]
['text':' row range for this thread','line_number':10199,'multiline':False]
['text':' src0 is same shape as dst => same indices','line_number':10209,'multiline':False]
['text':' ggml_compute_forward_set','line_number':10233,'multiline':False]
['text':' view src0 and dst with these strides and data offset inbytes during set','line_number':10243,'multiline':False]
['text':' nb0 is implicitly element_size because src0 and dst are contiguous','line_number':10244,'multiline':False]
['text':' memcpy needs to be synchronized across threads to avoid race conditions.','line_number':10252,'multiline':False]
['text':' => do it in INIT phase','line_number':10253,'multiline':False]
['text':' src0 and dst as viewed during set','line_number':10273,'multiline':False]
['text':' rows per thread','line_number':10285,'multiline':False]
['text':' row range for this thread','line_number':10288,'multiline':False]
['text':' src0 and dst are viewed with shape of src1 and offset','line_number':10293,'multiline':False]
['text':' => same indices','line_number':10294,'multiline':False]
['text':' ggml_compute_forward_cpy','line_number':10335,'multiline':False]
['text':' ggml_compute_forward_cont','line_number':10344,'multiline':False]
['text':' ggml_compute_forward_reshape','line_number':10353,'multiline':False]
['text':' NOP','line_number':10359,'multiline':False]
['text':' ggml_compute_forward_view','line_number':10365,'multiline':False]
['text':' NOP','line_number':10370,'multiline':False]
['text':' ggml_compute_forward_permute','line_number':10375,'multiline':False]
['text':' NOP','line_number':10380,'multiline':False]
['text':' ggml_compute_forward_transpose','line_number':10385,'multiline':False]
['text':' NOP','line_number':10390,'multiline':False]
['text':' ggml_compute_forward_get_rows','line_number':10395,'multiline':False]
['text':' TODO: multi-thread','line_number':10421,'multiline':False]
['text':' TODO: multi-thread','line_number':10456,'multiline':False]
['text':' TODO: multi-thread','line_number':10491,'multiline':False]
['text':'static bool first = true;','line_number':10539,'multiline':False]
['text':'printf("ne0 = %d, ne1 = %d, ne2 = %d\n", dst->ne[0], dst->ne[1], dst->ne[2]);','line_number':10540,'multiline':False]
['text':'if (first) {','line_number':10541,'multiline':False]
['text':'    first = false;','line_number':10542,'multiline':False]
['text':'} else {','line_number':10543,'multiline':False]
['text':'    for (int k = 0; k < dst->ne[1]; ++k) {','line_number':10544,'multiline':False]
['text':'        for (int j = 0; j < dst->ne[0]/16; ++j) {','line_number':10545,'multiline':False]
['text':'            for (int i = 0; i < 16; ++i) {','line_number':10546,'multiline':False]
['text':'                printf("%8.4f ", ((float *) dst->data)[k*dst->ne[0] + j*16 + i]);','line_number':10547,'multiline':False]
['text':'            }','line_number':10548,'multiline':False]
['text':'            printf("\n");','line_number':10549,'multiline':False]
['text':'        }','line_number':10550,'multiline':False]
['text':'        printf("\n");','line_number':10551,'multiline':False]
['text':'    }','line_number':10552,'multiline':False]
['text':'    printf("\n");','line_number':10553,'multiline':False]
['text':'    exit(0);','line_number':10554,'multiline':False]
['text':'}','line_number':10555,'multiline':False]
['text':' ggml_compute_forward_get_rows_back','line_number':10558,'multiline':False]
['text':' ggml_compute_forward_dup_same_cont(params, opt0, dst);','line_number':10568,'multiline':False]
['text':' ggml_compute_forward_dup_same_cont(params, opt0, dst);','line_number':10602,'multiline':False]
['text':'static bool first = true;','line_number':10648,'multiline':False]
['text':'printf("ne0 = %d, ne1 = %d, ne2 = %d\n", dst->ne[0], dst->ne[1], dst->ne[2]);','line_number':10649,'multiline':False]
['text':'if (first) {','line_number':10650,'multiline':False]
['text':'    first = false;','line_number':10651,'multiline':False]
['text':'} else {','line_number':10652,'multiline':False]
['text':'    for (int k = 0; k < dst->ne[1]; ++k) {','line_number':10653,'multiline':False]
['text':'        for (int j = 0; j < dst->ne[0]/16; ++j) {','line_number':10654,'multiline':False]
['text':'            for (int i = 0; i < 16; ++i) {','line_number':10655,'multiline':False]
['text':'                printf("%8.4f ", ((float *) dst->data)[k*dst->ne[0] + j*16 + i]);','line_number':10656,'multiline':False]
['text':'            }','line_number':10657,'multiline':False]
['text':'            printf("\n");','line_number':10658,'multiline':False]
['text':'        }','line_number':10659,'multiline':False]
['text':'        printf("\n");','line_number':10660,'multiline':False]
['text':'    }','line_number':10661,'multiline':False]
['text':'    printf("\n");','line_number':10662,'multiline':False]
['text':'    exit(0);','line_number':10663,'multiline':False]
['text':'}','line_number':10664,'multiline':False]
['text':' ggml_compute_forward_diag','line_number':10667,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':10679,'multiline':False]
['text':' ggml_compute_forward_diag_mask_inf','line_number':10725,'multiline':False]
['text':' memcpy needs to be synchronized across threads to avoid race conditions.','line_number':10742,'multiline':False]
['text':' => do it in INIT phase','line_number':10743,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':10756,'multiline':False]
['text':' ggml_compute_forward_soft_max','line_number':10809,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':10826,'multiline':False]
['text':' rows per thread','line_number':10836,'multiline':False]
['text':' row range for this thread','line_number':10839,'multiline':False]
['text':' broadcast the mask across rows','line_number':10849,'multiline':False]
['text':'printf("p[%d] = %f\n", i, p[i]);','line_number':10860,'multiline':False]
['text':' const float val = (wp[i] == -INFINITY) ? 0.0 : exp(wp[i] - max);','line_number':10875,'multiline':False]
['text':' ggml_compute_forward_soft_max_back','line_number':10915,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':10932,'multiline':False]
['text':' rows per thread','line_number':10940,'multiline':False]
['text':' row range for this thread','line_number':10943,'multiline':False]
['text':'printf("p[%d] = %f\n", i, p[i]);','line_number':10954,'multiline':False]
['text':' Jii = yi - yi*yi','line_number':10959,'multiline':False]
['text':' Jij = -yi*yj','line_number':10960,'multiline':False]
['text':' J = diag(y)-y.T*y','line_number':10961,'multiline':False]
['text':' dx = J * dy','line_number':10962,'multiline':False]
['text':' dxk = sum_i(Jki * dyi)','line_number':10963,'multiline':False]
['text':' dxk = sum_i(-yk*yi * dyi) - (-yk*yk)*dyk + (yk - yk*yk)*dyk','line_number':10964,'multiline':False]
['text':' dxk = sum_i(-yk*yi * dyi) + yk*yk*dyk + yk*dyk - yk*yk*dyk','line_number':10965,'multiline':False]
['text':' dxk = sum_i(-yk*yi * dyi) + yk*dyk','line_number':10966,'multiline':False]
['text':' dxk = -yk * sum_i(yi * dyi) + yk*dyk','line_number':10967,'multiline':False]
['text':' dxk = -yk * dot(y, dy) + yk*dyk','line_number':10968,'multiline':False]
['text':' dxk = yk * (- dot(y, dy) + dyk)','line_number':10969,'multiline':False]
['text':' dxk = yk * (dyk - dot(y, dy))','line_number':10970,'multiline':False]
['text':'','line_number':10971,'multiline':False]
['text':' post-order:','line_number':10972,'multiline':False]
['text':' dot_y_dy := dot(y, dy)','line_number':10973,'multiline':False]
['text':' dx := dy','line_number':10974,'multiline':False]
['text':' dx := dx - dot_y_dy','line_number':10975,'multiline':False]
['text':' dx := dx * y','line_number':10976,'multiline':False]
['text':' linear runtime, no additional memory','line_number':10978,'multiline':False]
['text':' ggml_compute_forward_alibi','line_number':11011,'multiline':False]
['text':'const int n_past = ((int32_t *) dst->op_params)[0];','line_number':11023,'multiline':False]
['text':' all_seq_len = n_past + ne1','line_number':11028,'multiline':False]
['text':' seq_len_without_past','line_number':11029,'multiline':False]
['text':' n_head -> this is k','line_number':11030,'multiline':False]
['text':'const int64_t ne3 = src0->ne[3]; // 1 -> bsz','line_number':11031,'multiline':False]
['text':' ne2*ne3','line_number':11034,'multiline':False]
['text':'const int nb3 = src0->nb[3];','line_number':11039,'multiline':False]
['text':' add alibi to src0 (KQ_scaled)','line_number':11044,'multiline':False]
['text':' TODO: k*nb2 or k*nb3','line_number':11056,'multiline':False]
['text':'const int n_past = ((int32_t *) dst->op_params)[0];','line_number':11082,'multiline':False]
['text':' all_seq_len = n_past + ne1','line_number':11087,'multiline':False]
['text':' seq_len_without_past','line_number':11088,'multiline':False]
['text':' n_head -> this is k','line_number':11089,'multiline':False]
['text':'const int ne3 = src0->ne[3]; // 1 -> bsz','line_number':11090,'multiline':False]
['text':' ne2*ne3','line_number':11093,'multiline':False]
['text':'const int nb3 = src0->nb[3];','line_number':11098,'multiline':False]
['text':'GGML_ASSERT(ne1 + n_past == ne0); (void) n_past;','line_number':11101,'multiline':False]
['text':' add alibi to src0 (KQ_scaled)','line_number':11104,'multiline':False]
['text':' TODO: k*nb2 or k*nb3','line_number':11116,'multiline':False]
['text':' we return F32','line_number':11126,'multiline':False]
['text':' ggml_compute_forward_clamp','line_number':11168,'multiline':False]
['text':' ggml_compute_forward_rope','line_number':11242,'multiline':False]
['text':' YaRN algorithm based on LlamaYaRNScaledRotaryEmbedding.py from https://github.com/jquesnelle/yarn','line_number':11249,'multiline':False]
['text':' MIT licensed. Copyright (c) 2023 Jeffrey Quesnelle and Bowen Peng.','line_number':11250,'multiline':False]
['text':' Get n-d rotational scaling corrected for extrapolation','line_number':11255,'multiline':False]
['text':' Get n-d magnitude scaling corrected for interpolation','line_number':11262,'multiline':False]
['text':' Apparently solving `n_rot = 2pi * x * base^((2 * max_pos_emb) / n_dims)` for x, we get','line_number':11269,'multiline':False]
['text':' `corr_dim(n_rot) = n_dims * log(max_pos_emb / (n_rot * 2pi)) / (2 * log(base))`','line_number':11270,'multiline':False]
['text':' start and end correction dims','line_number':11278,'multiline':False]
['text':' these two only relevant for xPos RoPE:','line_number':11295,'multiline':False]
['text':'const int n_past     = ((int32_t *) dst->op_params)[0];','line_number':11299,'multiline':False]
['text':'printf("ne0: %d, ne1: %d, ne2: %d, ne3: %d\n", ne0, ne1, ne2, ne3);','line_number':11316,'multiline':False]
['text':'printf("n_past = %d, ne2 = %d\n", n_past, ne2);','line_number':11317,'multiline':False]
['text':' rows per thread','line_number':11329,'multiline':False]
['text':' row range for this thread','line_number':11332,'multiline':False]
['text':' row index used to determine which thread to use','line_number':11336,'multiline':False]
['text':' backward process uses inverse rotation by cos and sin.','line_number':11347,'multiline':False]
['text':' cos and sin build a rotation matrix, where the inverse is the transpose.','line_number':11348,'multiline':False]
['text':' this essentially just switches the sign of sin.','line_number':11349,'multiline':False]
['text':' zeta scaling for xPos only:','line_number':11396,'multiline':False]
['text':' TODO: this might be wrong for ne0 != n_dims - need double check','line_number':11412,'multiline':False]
['text':' ref:  https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py#LL251C1-L294C28','line_number':11413,'multiline':False]
['text':' simplified from `(ib * n_dims + ic) * inv_ndims`','line_number':11417,'multiline':False]
['text':'const int n_past     = ((int32_t *) dst->op_params)[0];','line_number':11459,'multiline':False]
['text':'printf("ne0: %d, ne1: %d, ne2: %d, ne3: %d\n", ne0, ne1, ne2, ne3);','line_number':11473,'multiline':False]
['text':'printf("n_past = %d, ne2 = %d\n", n_past, ne2);','line_number':11474,'multiline':False]
['text':' rows per thread','line_number':11486,'multiline':False]
['text':' row range for this thread','line_number':11489,'multiline':False]
['text':' row index used to determine which thread to use','line_number':11493,'multiline':False]
['text':' backward process uses inverse rotation by cos and sin.','line_number':11504,'multiline':False]
['text':' cos and sin build a rotation matrix, where the inverse is the transpose.','line_number':11505,'multiline':False]
['text':' this essentially just switches the sign of sin.','line_number':11506,'multiline':False]
['text':' TODO: this might be wrong for ne0 != n_dims - need double check','line_number':11565,'multiline':False]
['text':' ref:  https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py#LL251C1-L294C28','line_number':11566,'multiline':False]
['text':' simplified from `(ib * n_dims + ic) * inv_ndims`','line_number':11570,'multiline':False]
['text':' ggml_compute_forward_rope_back','line_number':11621,'multiline':False]
['text':' ggml_compute_forward_conv_transpose_1d','line_number':11644,'multiline':False]
['text':' permute kernel data (src0) from (K x Cout x Cin) to (Cin x K x Cout)','line_number':11671,'multiline':False]
['text':' permute source data (src1) from (L x Cin) to (Cin x L)','line_number':11686,'multiline':False]
['text':' need to zero dst since we are accumulating into it','line_number':11699,'multiline':False]
['text':' total rows in dst','line_number':11711,'multiline':False]
['text':' rows per thread','line_number':11714,'multiline':False]
['text':' row range for this thread','line_number':11717,'multiline':False]
['text':' prepare kernel data (src0) from (K x Cout x Cin) to (Cin x K x Cout)','line_number':11765,'multiline':False]
['text':' prepare source data (src1)','line_number':11780,'multiline':False]
['text':' need to zero dst since we are accumulating into it','line_number':11793,'multiline':False]
['text':' total rows in dst','line_number':11805,'multiline':False]
['text':' rows per thread','line_number':11808,'multiline':False]
['text':' row range for this thread','line_number':11811,'multiline':False]
['text':' src0: kernel [OC, IC, KH, KW]','line_number':11855,'multiline':False]
['text':' src1: image [N, IC, IH, IW]','line_number':11856,'multiline':False]
['text':' dst:  result [N, OH, OW, IC*KH*KW]','line_number':11857,'multiline':False]
['text':' im2col: [N, IC, IH, IW] => [N, OH, OW, IC*KH*KW]','line_number':11908,'multiline':False]
['text':' 1','line_number':11913,'multiline':False]
['text':' micro kernel','line_number':11917,'multiline':False]
['text':' [IC, KH, KW]','line_number':11918,'multiline':False]
['text':' [IH, IW]','line_number':11919,'multiline':False]
['text':' 1','line_number':11921,'multiline':False]
['text':' ggml_compute_forward_conv_transpose_2d','line_number':11961,'multiline':False]
['text':' permute kernel data (src0) from (Kw x Kh x Cout x Cin) to (Cin x Kw x Kh x Cout)','line_number':11988,'multiline':False]
['text':' permute source data (src1) from (Sw x Sh x Cin) to (Cin x Sw x Sh)','line_number':12005,'multiline':False]
['text':' total patches in dst','line_number':12030,'multiline':False]
['text':' patches per thread','line_number':12033,'multiline':False]
['text':' patch range for this thread','line_number':12036,'multiline':False]
['text':' Cout','line_number':12043,'multiline':False]
['text':' ggml_compute_forward_pool_1d_sk_p0','line_number':12063,'multiline':False]
['text':' ggml_compute_forward_pool_1d','line_number':12115,'multiline':False]
['text':' padding not supported','line_number':12127,'multiline':False]
['text':' only s = k supported','line_number':12128,'multiline':False]
['text':' ggml_compute_forward_pool_2d','line_number':12133,'multiline':False]
['text':' ggml_compute_forward_upscale','line_number':12207,'multiline':False]
['text':' TODO: optimize','line_number':12227,'multiline':False]
['text':' ggml_compute_forward_pad','line_number':12264,'multiline':False]
['text':' TODO: optimize','line_number':12285,'multiline':False]
['text':' ggml_compute_forward_argsort','line_number':12322,'multiline':False]
['text':' C doesn't have a functional sort, so we do a bubble sort instead','line_number':12352,'multiline':False]
['text':' ggml_compute_forward_flash_attn','line_number':12383,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':12430,'multiline':False]
['text':' parallelize by q rows using ggml_vec_dot_f32','line_number':12444,'multiline':False]
['text':' total rows in q','line_number':12446,'multiline':False]
['text':' rows per thread','line_number':12449,'multiline':False]
['text':' row range for this thread','line_number':12452,'multiline':False]
['text':'printf("P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\n", P, N, D, ir0, ir1, scale);','line_number':12458,'multiline':False]
['text':' q indices','line_number':12461,'multiline':False]
['text':' k indices','line_number':12474,'multiline':False]
['text':' S indices','line_number':12479,'multiline':False]
['text':' scale','line_number':12488,'multiline':False]
['text':' softmax','line_number':12495,'multiline':False]
['text':' exclude known -INF S[..] values from max and loop','line_number':12496,'multiline':False]
['text':' dont forget to set their SW values to zero','line_number':12497,'multiline':False]
['text':' dst indices','line_number':12558,'multiline':False]
['text':' v indices','line_number':12563,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':12620,'multiline':False]
['text':' parallelize by q rows using ggml_vec_dot_f32','line_number':12634,'multiline':False]
['text':' total rows in q','line_number':12636,'multiline':False]
['text':' rows per thread','line_number':12639,'multiline':False]
['text':' row range for this thread','line_number':12642,'multiline':False]
['text':'printf("P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\n", P, N, D, ir0, ir1, scale);','line_number':12648,'multiline':False]
['text':' q indices','line_number':12651,'multiline':False]
['text':' k indices','line_number':12664,'multiline':False]
['text':' S indices','line_number':12669,'multiline':False]
['text':' k indices','line_number':12679,'multiline':False]
['text':' S indices','line_number':12684,'multiline':False]
['text':' scale','line_number':12694,'multiline':False]
['text':' softmax','line_number':12705,'multiline':False]
['text':' todo: exclude known -INF S[..] values from max and loop, assuming their results to be zero.','line_number':12706,'multiline':False]
['text':' dont forget to set their S values to zero','line_number':12707,'multiline':False]
['text':' todo: exclude known zero S[..] values from dot (reducing nev0 and increasing begin of v and S16).','line_number':12764,'multiline':False]
['text':' dst indices','line_number':12767,'multiline':False]
['text':' v indices','line_number':12772,'multiline':False]
['text':' dst indices','line_number':12783,'multiline':False]
['text':' v indices','line_number':12788,'multiline':False]
['text':' ggml_compute_forward_flash_ff','line_number':12824,'multiline':False]
['text':' F16','line_number':12828,'multiline':False]
['text':' F16 fc_w','line_number':12829,'multiline':False]
['text':' F32 fc_b','line_number':12830,'multiline':False]
['text':' F16 proj_w','line_number':12831,'multiline':False]
['text':' F32 proj_b','line_number':12832,'multiline':False]
['text':'const int64_t N = nea1;','line_number':12854,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':12877,'multiline':False]
['text':' parallelize by a rows using ggml_vec_dot_f32','line_number':12891,'multiline':False]
['text':' total rows in a','line_number':12893,'multiline':False]
['text':' rows per thread','line_number':12896,'multiline':False]
['text':' row range for this thread','line_number':12899,'multiline':False]
['text':' a indices','line_number':12904,'multiline':False]
['text':' b0 indices','line_number':12912,'multiline':False]
['text':' S indices','line_number':12917,'multiline':False]
['text':'ggml_vec_gelu_f32(neb01, S, S);','line_number':12927,'multiline':False]
['text':' dst indices','line_number':12938,'multiline':False]
['text':' TODO','line_number':12974,'multiline':False]
['text':' ggml_compute_forward_flash_attn_back','line_number':12983,'multiline':False]
['text':' GGML_ASSERT(ne0 == D);','line_number':13018,'multiline':False]
['text':' GGML_ASSERT(ne1 == N);','line_number':13019,'multiline':False]
['text':' dst cannot be transposed or permuted','line_number':13036,'multiline':False]
['text':' parallelize by k rows using ggml_vec_dot_f32','line_number':13080,'multiline':False]
['text':' total rows in k','line_number':13082,'multiline':False]
['text':' rows per thread','line_number':13085,'multiline':False]
['text':' row range for this thread','line_number':13088,'multiline':False]
['text':'printf("P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\n", P, N, D, ir0, ir1, scale);','line_number':13094,'multiline':False]
['text':' how often k2 (and v2) is repeated in q2','line_number':13096,'multiline':False]
['text':' q indices','line_number':13100,'multiline':False]
['text':' (ik2 + irep*nek2) % nek2 == ik2','line_number':13113,'multiline':False]
['text':' not sure about CACHE_LINE_SIZE_F32..','line_number':13117,'multiline':False]
['text':' - maybe it must not be multiplied by 2 and excluded from .. in SM 1*(..) offset?','line_number':13118,'multiline':False]
['text':' k indices','line_number':13128,'multiline':False]
['text':' S indices','line_number':13131,'multiline':False]
['text':' scale','line_number':13140,'multiline':False]
['text':' softmax','line_number':13147,'multiline':False]
['text':' exclude known -INF S[..] values from max and loop','line_number':13148,'multiline':False]
['text':' dont forget to set their SM values to zero','line_number':13149,'multiline':False]
['text':' step-by-step explanation','line_number':13204,'multiline':False]
['text':' forward-process                    shape      grads from backward process','line_number':13206,'multiline':False]
['text':' parallel_for ik2,ik3:','line_number':13207,'multiline':False]
['text':'  for irep:','line_number':13208,'multiline':False]
['text':'   iq2 = ik2 + irep*nek2','line_number':13209,'multiline':False]
['text':'   k[:D,:M,:,:]                     [D,M,:,:]  grad[k][:D,:M,ik2,ik3]  += grad[kcur]','line_number':13210,'multiline':False]
['text':'   q[:D,:N,:,:]                     [D,N,:,:]  grad[q][:D,iq1,iq2,iq3] += grad[qcur]','line_number':13211,'multiline':False]
['text':'   v[:M,:D,:,:]                     [M,D,:,:]  grad[v][:M,:D,iv2,iv3]  += grad[vcur]','line_number':13212,'multiline':False]
['text':'   for iq1:','line_number':13213,'multiline':False]
['text':'    kcur   = k[:D,:M,ik2,ik3]       [D,M,1,1]  grad[kcur] = grad[S1].T @ qcur','line_number':13214,'multiline':False]
['text':'    qcur   = q[:D,iq1,iq2,iq3]      [D,1,1,1]  grad[qcur] = grad[S1]   @ kcur','line_number':13215,'multiline':False]
['text':'    vcur   = v[:M,:D,iv2,iv3]       [M,D,1,1]  grad[vcur] = grad[S5].T @ S4','line_number':13216,'multiline':False]
['text':'    S0     = -Inf                   [D,1,1,1]','line_number':13217,'multiline':False]
['text':'   ~S1[i]  = dot(kcur[:D,i], qcur)','line_number':13218,'multiline':False]
['text':'    S1     = qcur @ kcur.T          [M,1,1,1]  grad[S1]   = grad[S2] * scale','line_number':13219,'multiline':False]
['text':'    S2     = S1 * scale             [M,1,1,1]  grad[S2]   = diag_mask_zero(grad[S3], P)','line_number':13220,'multiline':False]
['text':'    S3     = diag_mask_inf(S2, P)   [M,1,1,1]  grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))','line_number':13221,'multiline':False]
['text':'    S4     = softmax(S3)            [M,1,1,1]  grad[S4]   = grad[S5] @ vcur','line_number':13222,'multiline':False]
['text':'   ~S5[i]  = dot(vcur[:,i], S4)','line_number':13223,'multiline':False]
['text':'    S5     = S4 @ vcur.T            [D,1,1,1]  grad[S5]   = d[:D,id1,id2,id3]','line_number':13224,'multiline':False]
['text':'   ~dst[i,iq1,iq2,iq3]  = S5[i]              ^','line_number':13225,'multiline':False]
['text':'    dst[:D,iq1,iq2,iq3] = S5                 | grad[dst[:D,iq1,iq2,iq3]] = d[:D,id1,id2,id3]','line_number':13226,'multiline':False]
['text':' dst                               backward-/ grad[dst]                 = d','line_number':13227,'multiline':False]
['text':'','line_number':13228,'multiline':False]
['text':' output gradients with their dependencies:','line_number':13229,'multiline':False]
['text':'','line_number':13230,'multiline':False]
['text':' grad[kcur] = grad[S1].T @ qcur','line_number':13231,'multiline':False]
['text':' grad[S1]   = diag_mask_zero(grad[S3], P) * scale','line_number':13232,'multiline':False]
['text':' grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))','line_number':13233,'multiline':False]
['text':' grad[S4]   = grad[S5] @ vcur','line_number':13234,'multiline':False]
['text':' grad[S4]   = d[:D,id1,id2,id3] @ vcur','line_number':13235,'multiline':False]
['text':' grad[qcur] = grad[S1]   @ kcur','line_number':13236,'multiline':False]
['text':' grad[vcur] = grad[S5].T @ S4','line_number':13237,'multiline':False]
['text':' grad[vcur] = d[:D,id1,id2,id3].T @ S4','line_number':13238,'multiline':False]
['text':'','line_number':13239,'multiline':False]
['text':' in post-order:','line_number':13240,'multiline':False]
['text':'','line_number':13241,'multiline':False]
['text':' S1         = qcur @ kcur.T','line_number':13242,'multiline':False]
['text':' S2         = S1 * scale','line_number':13243,'multiline':False]
['text':' S3         = diag_mask_inf(S2, P)','line_number':13244,'multiline':False]
['text':' S4         = softmax(S3)','line_number':13245,'multiline':False]
['text':' grad[S4]   = d[:D,id1,id2,id3] @ vcur','line_number':13246,'multiline':False]
['text':' grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))','line_number':13247,'multiline':False]
['text':' grad[S1]   = diag_mask_zero(grad[S3], P) * scale','line_number':13248,'multiline':False]
['text':' grad[qcur] = grad[S1]   @ kcur','line_number':13249,'multiline':False]
['text':' grad[kcur] = grad[S1].T @ qcur','line_number':13250,'multiline':False]
['text':' grad[vcur] = d[:D,id1,id2,id3].T @ S4','line_number':13251,'multiline':False]
['text':'','line_number':13252,'multiline':False]
['text':' using less variables (SM=S4):','line_number':13253,'multiline':False]
['text':'','line_number':13254,'multiline':False]
['text':' S             = diag_mask_inf(qcur @ kcur.T * scale, P)','line_number':13255,'multiline':False]
['text':' SM            = softmax(S)','line_number':13256,'multiline':False]
['text':' S             = d[:D,iq1,iq2,iq3] @ vcur','line_number':13257,'multiline':False]
['text':' dot_SM_gradSM = dot(SM, S)','line_number':13258,'multiline':False]
['text':' S             = SM * (S - dot(SM, S))','line_number':13259,'multiline':False]
['text':' S             = diag_mask_zero(S, P) * scale','line_number':13260,'multiline':False]
['text':'','line_number':13261,'multiline':False]
['text':' grad[q][:D,iq1,iq2,iq3] += S   @ kcur','line_number':13262,'multiline':False]
['text':' grad[k][:D,:M,ik2,ik3]  += S.T @ qcur','line_number':13263,'multiline':False]
['text':' grad[v][:M,:D,iv2,iv3]  += d[:D,id1,id2,id3].T @ SM','line_number':13264,'multiline':False]
['text':' S = gradSM = d[:D,id1,id2,id3] @ vcur[:,:,iv2,iv3]','line_number':13267,'multiline':False]
['text':' S = d[:D,id1,id2,id3] @ vcur[:,:,iv2,iv3]','line_number':13268,'multiline':False]
['text':' for ic:','line_number':13269,'multiline':False]
['text':'   S[:M] += vcur[:M,ic,iv2,iv3] * d[ic,id1,id2,id3]','line_number':13270,'multiline':False]
['text':' exclude known future zero S[..] values from operation','line_number':13271,'multiline':False]
['text':' S = SM * (S - dot(SM, S))','line_number':13280,'multiline':False]
['text':' S = diag_mask_zero(S, P) * scale','line_number':13286,'multiline':False]
['text':' already done by above ggml_vec_set_f32','line_number':13287,'multiline':False]
['text':' exclude known zero S[..] values from operation','line_number':13289,'multiline':False]
['text':' S    shape [M,1]','line_number':13292,'multiline':False]
['text':' SM   shape [M,1]','line_number':13293,'multiline':False]
['text':' kcur shape [D,M]','line_number':13294,'multiline':False]
['text':' qcur shape [D,1]','line_number':13295,'multiline':False]
['text':' vcur shape [M,D]','line_number':13296,'multiline':False]
['text':' grad[q][:D,iq1,iq2,iq3] += S @ kcur','line_number':13298,'multiline':False]
['text':' grad[q][:D,iq1,iq2,iq3] += shape[M,1] @ shape[D,M]','line_number':13299,'multiline':False]
['text':' for ic:','line_number':13300,'multiline':False]
['text':'  grad[q][:D,iq1,iq2,iq3] += S[ic] * kcur[:D,ic,ik2,ik3]','line_number':13301,'multiline':False]
['text':' exclude known zero S[..] values from loop','line_number':13302,'multiline':False]
['text':' grad[k][:D,:M,iq2,iq3] += S.T @ qcur','line_number':13310,'multiline':False]
['text':' for ic:','line_number':13311,'multiline':False]
['text':'  grad[k][:D,ic,iq2,iq3] += S.T[0,ic] * qcur[:D,0]','line_number':13312,'multiline':False]
['text':'  grad[k][:D,ic,iq2,iq3] += S[ic]     * qcur[:D,0]','line_number':13313,'multiline':False]
['text':' exclude known zero S[..] values from loop','line_number':13314,'multiline':False]
['text':' grad[v][:M,:D,iv2,iv3] += d[:D,id1,id2,id3].T       @ SM','line_number':13322,'multiline':False]
['text':' for ic:','line_number':13323,'multiline':False]
['text':'  grad[v][:M,ic,iv2,iv3] += d[:D,id1,id2,id3].T[0,ic] * SM[:M]','line_number':13324,'multiline':False]
['text':'  grad[v][:M,ic,iv2,iv3] += d[ic,id1,id2,id3]         * SM[:M]','line_number':13325,'multiline':False]
['text':' exclude known zero SM[..] values from mad','line_number':13326,'multiline':False]
['text':' ggml_compute_forward_win_part','line_number':13358,'multiline':False]
['text':' TODO: optimize / multi-thread','line_number':13378,'multiline':False]
['text':' ggml_compute_forward_win_unpart','line_number':13420,'multiline':False]
['text':' padding','line_number':13435,'multiline':False]
['text':'const int py = (w - ne2%w)%w;','line_number':13437,'multiline':False]
['text':'const int npy = (py + ne2)/w;','line_number':13440,'multiline':False]
['text':' TODO: optimize / multi-thread','line_number':13444,'multiline':False]
['text':'gmml_compute_forward_unary','line_number':13480,'multiline':False]
['text':' ggml_compute_forward_get_rel_pos','line_number':13536,'multiline':False]
['text':' ref: https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py#L292-L322','line_number':13546,'multiline':False]
['text':' ggml_compute_forward_add_rel_pos','line_number':13581,'multiline':False]
['text':' ref: https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py#L357-L359','line_number':13602,'multiline':False]
['text':' total patches in dst','line_number':13616,'multiline':False]
['text':' patches per thread','line_number':13619,'multiline':False]
['text':' patch range for this thread','line_number':13622,'multiline':False]
['text':' ggml_compute_forward_map_unary','line_number':13666,'multiline':False]
['text':' ggml_compute_forward_map_binary','line_number':13709,'multiline':False]
['text':' ggml_compute_forward_map_custom1','line_number':13757,'multiline':False]
['text':' ggml_compute_forward_map_custom2','line_number':13773,'multiline':False]
['text':' ggml_compute_forward_map_custom3','line_number':13790,'multiline':False]
['text':' ggml_compute_forward_map_custom1','line_number':13808,'multiline':False]
['text':' ggml_compute_forward_map_custom2','line_number':13823,'multiline':False]
['text':' ggml_compute_forward_map_custom3','line_number':13839,'multiline':False]
['text':' ggml_compute_forward_cross_entropy_loss','line_number':13856,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':13873,'multiline':False]
['text':' rows per thread','line_number':13897,'multiline':False]
['text':' row range for this thread','line_number':13900,'multiline':False]
['text':'printf("p[%d] = %f\n", i, p[i]);','line_number':13911,'multiline':False]
['text':' soft_max','line_number':13916,'multiline':False]
['text':' sum = 1.0/sum;','line_number':13941,'multiline':False]
['text':' avoid log(0) by rescaling from [0..1] to [eps..1]','line_number':13943,'multiline':False]
['text':' ggml_compute_forward_cross_entropy_loss_back','line_number':13981,'multiline':False]
['text':' TODO: handle transposed/permuted matrices','line_number':14004,'multiline':False]
['text':' rows per thread','line_number':14008,'multiline':False]
['text':' row range for this thread','line_number':14011,'multiline':False]
['text':'printf("p[%d] = %f\n", i, p[i]);','line_number':14024,'multiline':False]
['text':' soft_max','line_number':14030,'multiline':False]
['text':' grad(src0) = (softmax(src0) - src1) * grad(cross_entropy_loss(src0, src1)) / nr','line_number':14058,'multiline':False]
['text':'///////////////////////////////','line_number':14091,'multiline':False]
['text':' GGML_USE_CUBLAS','line_number':14107,'multiline':False]
['text':' nop','line_number':14422,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':14431,'multiline':False]
['text':' next primes after powers of two','line_number':14434,'multiline':False]
['text':' find the smallest prime that is larger or equal to min_sz','line_number':14444,'multiline':False]
['text':' linear probing','line_number':14466,'multiline':False]
['text':' visited all hash table entries -> not found','line_number':14471,'multiline':False]
['text':' insert','line_number':14492,'multiline':False]
['text':' gradient checkpointing','line_number':14539,'multiline':False]
['text':' assert that not full','line_number':14571,'multiline':False]
['text':' insert clone into replacements','line_number':14578,'multiline':False]
['text':' assert that we don't overwrite','line_number':14579,'multiline':False]
['text':' view_src not yet allocated','line_number':14595,'multiline':False]
['text':' view_src already allocated','line_number':14596,'multiline':False]
['text':' insert checkpoints in replacements','line_number':14627,'multiline':False]
['text':' assert that not full','line_number':14630,'multiline':False]
['text':' assert that we don't overwrite','line_number':14631,'multiline':False]
['text':' rewrite gb_tmp->nodes[gf->n_nodes:gb_tmp->n_nodes],','line_number':14637,'multiline':False]
['text':' replacing references to gb_tmp->nodes[0:gf->n_nodes] ( == gf->nodes[0:gf->n_nodes]),','line_number':14638,'multiline':False]
['text':' by recomputing them from checkpoints','line_number':14639,'multiline':False]
['text':' insert new tensors recomputing src, reusing already made replacements,','line_number':14643,'multiline':False]
['text':' remember replacements: remember new tensors with mapping from corresponding gf nodes','line_number':14644,'multiline':False]
['text':' recurse for input tensors,','line_number':14645,'multiline':False]
['text':' unless (i.e. terminating when) input tensors are replacements (like checkpoints)','line_number':14646,'multiline':False]
['text':' insert rewritten backward node with replacements made into resulting backward graph gb','line_number':14649,'multiline':False]
['text':' functions to change gradients considering the case that input a might be initial gradient with zero value','line_number':14656,'multiline':False]
['text':' TODO: should probably be sum instead of mean','line_number':14719,'multiline':False]
['text':' TODO: implement','line_number':14859,'multiline':False]
['text':' necessary for llama','line_number':14863,'multiline':False]
['text':' TODO: test this','line_number':14874,'multiline':False]
['text':' TODO: implement','line_number':14883,'multiline':False]
['text':' TODO: not implemented','line_number':14887,'multiline':False]
['text':' TODO: not implemented','line_number':14891,'multiline':False]
['text':' necessary for llama','line_number':14895,'multiline':False]
['text':' TODO: not implemented','line_number':14908,'multiline':False]
['text':' TODO: not implemented','line_number':14912,'multiline':False]
['text':' https://cs231n.github.io/optimization-2/#staged','line_number':14916,'multiline':False]
['text':' # forward pass','line_number':14917,'multiline':False]
['text':' s0 = np.random.randn(5, 10)','line_number':14918,'multiline':False]
['text':' s1 = np.random.randn(10, 3)','line_number':14919,'multiline':False]
['text':' t = s0.dot(s1)','line_number':14920,'multiline':False]
['text':' # now suppose we had the gradient on t from above in the circuit','line_number':14922,'multiline':False]
['text':' dt = np.random.randn(*t.shape) # same shape as t','line_number':14923,'multiline':False]
['text':' ds0 = dt.dot(s1.T) #.T gives the transpose of the matrix','line_number':14924,'multiline':False]
['text':' ds1 = t.T.dot(dt)','line_number':14925,'multiline':False]
['text':' tensor.shape [m,p,qq,rr]','line_number':14927,'multiline':False]
['text':' src0.shape   [n,m,q1,r1]','line_number':14928,'multiline':False]
['text':' src1.shape   [n,p,qq,rr]','line_number':14929,'multiline':False]
['text':' necessary for llama','line_number':14931,'multiline':False]
['text':' [n,m,qq,rr]','line_number':14934,'multiline':False]
['text':' [n,p,qq,rr]','line_number':14935,'multiline':False]
['text':' [m,p,qq,rr]','line_number':14936,'multiline':False]
['text':' sum broadcast repetitions of s1_tg into shape of src0','line_number':14944,'multiline':False]
['text':' [n,m,q1,r1]','line_number':14949,'multiline':False]
['text':' [n,m,q1,r1]','line_number':14950,'multiline':False]
['text':' [n,p,qq,rr]','line_number':14956,'multiline':False]
['text':' ggml_mul_mat(ctx,                   // [n,p,qq,rr]','line_number':14957,'multiline':False]
['text':'     ggml_cont(ctx,                  // [m,n,q1,r1]','line_number':14958,'multiline':False]
['text':'         ggml_transpose(ctx, src0)), // [m,n,q1,r1]','line_number':14959,'multiline':False]
['text':'     tensor->grad),                  // [m,p,qq,rr]','line_number':14960,'multiline':False]
['text':' // when src0 is bigger than tensor->grad (this is mostly the case in llama),','line_number':14962,'multiline':False]
['text':' // avoid transpose of src0, rather transpose smaller tensor->grad','line_number':14963,'multiline':False]
['text':' // and then use ggml_out_prod','line_number':14964,'multiline':False]
['text':' [n,p,qq,rr]','line_number':14965,'multiline':False]
['text':' [n,m,q1,r1]','line_number':14966,'multiline':False]
['text':' [p,m,qq,rr]','line_number':14967,'multiline':False]
['text':' [m,p,qq,rr]','line_number':14968,'multiline':False]
['text':' TODO: not implemented','line_number':14974,'multiline':False]
['text':' TODO: not implemented','line_number':14978,'multiline':False]
['text':' necessary for llama','line_number':14982,'multiline':False]
['text':' necessary for llama','line_number':15043,'multiline':False]
['text':' cpy overwrites value of src1 by src0 and returns view(src1)','line_number':15044,'multiline':False]
['text':' the overwriting is mathematically equivalent to:','line_number':15045,'multiline':False]
['text':' tensor = src0 * 1 + src1 * 0','line_number':15046,'multiline':False]
['text':' dsrc0 = dtensor * 1','line_number':15048,'multiline':False]
['text':' dsrc1 = dtensor * 0 -> noop','line_number':15052,'multiline':False]
['text':' same as cpy','line_number':15057,'multiline':False]
['text':' necessary for llama','line_number':15066,'multiline':False]
['text':' necessary for llama','line_number':15080,'multiline':False]
['text':' gradient is typically F32, but src0 could be other type','line_number':15091,'multiline':False]
['text':' necessary for llama','line_number':15109,'multiline':False]
['text':' necessary for llama','line_number':15134,'multiline':False]
['text':' necessary for llama (only for tokenizer)','line_number':15144,'multiline':False]
['text':' last ggml_get_rows_back argument src0->grad is only','line_number':15148,'multiline':False]
['text':' necessary to setup correct output shape','line_number':15149,'multiline':False]
['text':' noop','line_number':15154,'multiline':False]
['text':' TODO: not implemented','line_number':15159,'multiline':False]
['text':' TODO: not implemented','line_number':15163,'multiline':False]
['text':' necessary for llama','line_number':15167,'multiline':False]
['text':' necessary for llama','line_number':15178,'multiline':False]
['text':' necessary for llama','line_number':15189,'multiline':False]
['text':' TODO: not implemented','line_number':15200,'multiline':False]
['text':' necessary for llama','line_number':15204,'multiline':False]
['text':'const int n_past = ((int32_t *) tensor->op_params)[0];','line_number':15206,'multiline':False]
['text':'const int n_past = ((int32_t *) tensor->op_params)[0];','line_number':15245,'multiline':False]
['text':' TODO: not implemented','line_number':15284,'multiline':False]
['text':' TODO: not implemented','line_number':15288,'multiline':False]
['text':' TODO: not implemented','line_number':15292,'multiline':False]
['text':' TODO: not implemented','line_number':15296,'multiline':False]
['text':' TODO: not implemented','line_number':15300,'multiline':False]
['text':' TODO: not implemented','line_number':15304,'multiline':False]
['text':' TODO: not implemented','line_number':15308,'multiline':False]
['text':' TODO: not implemented','line_number':15312,'multiline':False]
['text':' TODO: not implemented','line_number':15316,'multiline':False]
['text':' TODO: not implemented','line_number':15320,'multiline':False]
['text':' TODO: not implemented','line_number':15324,'multiline':False]
['text':' not supported','line_number':15382,'multiline':False]
['text':' not supported','line_number':15386,'multiline':False]
['text':' noop','line_number':15408,'multiline':False]
['text':' noop','line_number':15420,'multiline':False]
['text':' TODO: not implemented','line_number':15425,'multiline':False]
['text':' TODO: not implemented','line_number':15429,'multiline':False]
['text':' TODO: not implemented','line_number':15444,'multiline':False]
['text':' TODO: not implemented','line_number':15448,'multiline':False]
['text':' necessary for llama','line_number':15452,'multiline':False]
['text':' not supported','line_number':15475,'multiline':False]
['text':' not supported','line_number':15491,'multiline':False]
['text':' nop','line_number':15495,'multiline':False]
['text':' this usually happens when we generate intermediate nodes from constants in the backward pass','line_number':15512,'multiline':False]
['text':' it can also happen during forward pass, if the user performs computations with constants','line_number':15513,'multiline':False]
['text':'GGML_PRINT_DEBUG("%s: warning: node %p has no grad, but op %d\n", __func__, (void *) node, node->op);','line_number':15515,'multiline':False]
['text':' check if already visited','line_number':15519,'multiline':False]
['text':' unknown order, just fall back to using i','line_number':15528,'multiline':True]
['text':' reached a leaf node, not part of the gradient graph (e.g. a constant)','line_number':15535,'multiline':False]
['text':' TODO: this branch isn't accessible anymore, maybe move this to ggml_build_forward_expand','line_number':15561,'multiline':False]
['text':' the last added node should always be starting point','line_number':15574,'multiline':False]
['text':' if we are keeping the gradient graph, we have to detach the gradient nodes from the original graph','line_number':15586,'multiline':False]
['text':' remember original gradients which start with zero values','line_number':15598,'multiline':False]
['text':' inplace operations to add gradients are not created by ggml_compute_backward','line_number':15609,'multiline':False]
['text':' use allocator to automatically make inplace operations','line_number':15610,'multiline':False]
['text':' leafs + nodes','line_number':15630,'multiline':False]
['text':' grads','line_number':15632,'multiline':False]
['text':' hash set','line_number':15634,'multiline':False]
['text':' check that we allocated the correct amount of memory','line_number':15659,'multiline':False]
['text':'.size         =','line_number':15666,'multiline':True]
['text':'.n_nodes      =','line_number':15667,'multiline':True]
['text':'.n_leafs      =','line_number':15668,'multiline':True]
['text':'.nodes        =','line_number':15669,'multiline':True]
['text':'.grads        =','line_number':15670,'multiline':True]
['text':'.leafs        =','line_number':15671,'multiline':True]
['text':'.hash_table   =','line_number':15672,'multiline':True]
['text':'.order        =','line_number':15673,'multiline':True]
['text':'.perf_runs    =','line_number':15674,'multiline':True]
['text':'.perf_cycles  =','line_number':15675,'multiline':True]
['text':'.perf_time_us =','line_number':15676,'multiline':True]
['text':'.size         =','line_number':15688,'multiline':True]
['text':'.n_nodes      =','line_number':15689,'multiline':True]
['text':'.n_leafs      =','line_number':15690,'multiline':True]
['text':'.nodes        =','line_number':15691,'multiline':True]
['text':'.grads        =','line_number':15692,'multiline':True]
['text':'.leafs        =','line_number':15693,'multiline':True]
['text':'.hash_table   =','line_number':15694,'multiline':True]
['text':'.order        =','line_number':15695,'multiline':True]
['text':'.perf_runs    =','line_number':15696,'multiline':True]
['text':'.perf_cycles  =','line_number':15697,'multiline':True]
['text':'.perf_time_us =','line_number':15698,'multiline':True]
['text':'','line_number':15759,'multiline':False]
['text':' thread data','line_number':15760,'multiline':False]
['text':'','line_number':15761,'multiline':False]
['text':' synchronization is done via busy loops','line_number':15762,'multiline':False]
['text':' I tried using spin locks, but not sure how to use them correctly - the things I tried were slower than busy loops','line_number':15763,'multiline':False]
['text':'','line_number':15764,'multiline':False]
['text':'#include <os/lock.h>','line_number':15768,'multiline':False]
['text':'','line_number':15769,'multiline':False]
['text':'typedef os_unfair_lock ggml_lock_t;','line_number':15770,'multiline':False]
['text':'','line_number':15771,'multiline':False]
['text':'#define ggml_lock_init(x)    UNUSED(x)','line_number':15772,'multiline':False]
['text':'#define ggml_lock_destroy(x) UNUSED(x)','line_number':15773,'multiline':False]
['text':'#define ggml_lock_lock       os_unfair_lock_lock','line_number':15774,'multiline':False]
['text':'#define ggml_lock_unlock     os_unfair_lock_unlock','line_number':15775,'multiline':False]
['text':'','line_number':15776,'multiline':False]
['text':'#define GGML_LOCK_INITIALIZER OS_UNFAIR_LOCK_INIT','line_number':15777,'multiline':False]
['text':'typedef pthread_spinlock_t ggml_lock_t;','line_number':15795,'multiline':False]
['text':'#define ggml_lock_init(x) pthread_spin_init(x, PTHREAD_PROCESS_PRIVATE)','line_number':15797,'multiline':False]
['text':'#define ggml_lock_destroy pthread_spin_destroy','line_number':15798,'multiline':False]
['text':'#define ggml_lock_lock    pthread_spin_lock','line_number':15799,'multiline':False]
['text':'#define ggml_lock_unlock  pthread_spin_unlock','line_number':15800,'multiline':False]
['text':' Android's libc implementation "bionic" does not support setting affinity','line_number':15822,'multiline':False]
['text':' run thread on node_num thread_n / (threads per node)','line_number':15829,'multiline':False]
['text':' TODO: Windows etc.','line_number':15871,'multiline':False]
['text':' (the linux implementation may also work on BSD, someone should test)','line_number':15872,'multiline':False]
['text':' synchronization primitives','line_number':15886,'multiline':False]
['text':' num active threads','line_number':15887,'multiline':False]
['text':' active graph node','line_number':15888,'multiline':False]
['text':' abort ggml_graph_compute when true','line_number':15890,'multiline':False]
['text':' TODO: use different scheduling for different matrix sizes','line_number':15973,'multiline':False]
['text':'const int nr0 = ggml_nrows(node->src[0]);','line_number':15974,'multiline':False]
['text':'const int nr1 = ggml_nrows(node->src[1]);','line_number':15975,'multiline':False]
['text':'n_tasks = MIN(n_threads, MAX(1, nr0/128));','line_number':15977,'multiline':False]
['text':'printf("nr0 = %8d, nr1 = %8d, nr0*nr1 = %8d, n_tasks%d\n", nr0, nr1, nr0*nr1, n_tasks);','line_number':15978,'multiline':False]
['text':' TODO: this actually is doing nothing','line_number':15982,'multiline':False]
['text':'       the threads are still spinning','line_number':15983,'multiline':False]
['text':' TODO: this actually is doing nothing','line_number':15987,'multiline':False]
['text':'       the threads are still spinning','line_number':15988,'multiline':False]
['text':' TODO: this actually is doing nothing','line_number':15993,'multiline':False]
['text':'       the threads are still spinning','line_number':15994,'multiline':False]
['text':' FIXME: blas','line_number':16000,'multiline':False]
['text':'TODO','line_number':16031,'multiline':False]
['text':'TODO','line_number':16035,'multiline':False]
['text':' all other threads are finished and spinning','line_number':16171,'multiline':False]
['text':' do finalize and init here so we don't have synchronize again','line_number':16172,'multiline':False]
['text':'.type  =','line_number':16174,'multiline':True]
['text':'.ith   =','line_number':16175,'multiline':True]
['text':'.nth   =','line_number':16176,'multiline':True]
['text':'.wsize =','line_number':16177,'multiline':True]
['text':'.wdata =','line_number':16178,'multiline':True]
['text':' FINALIZE ','line_number':16182,'multiline':True]
['text':' distribute new work or execute it direct if 1T','line_number':16191,'multiline':False]
['text':' INIT ','line_number':16203,'multiline':True]
['text':' TODO: maybe push node_n to the atomic but if other threads see n_tasks is 1,','line_number':16210,'multiline':False]
['text':' they do something more efficient than spinning (?)','line_number':16211,'multiline':False]
['text':' wait for other threads to finish','line_number':16233,'multiline':False]
['text':' TODO: this sched_yield can have significant impact on the performance - either positive or negative','line_number':16236,'multiline':False]
['text':'       depending on the workload and the operating system.','line_number':16237,'multiline':False]
['text':'       since it is not clear what is the best approach, it should potentially become user-configurable','line_number':16238,'multiline':False]
['text':'       ref: https://github.com/ggerganov/ggml/issues/291','line_number':16239,'multiline':False]
['text':' check if we should stop','line_number':16249,'multiline':False]
['text':' COMPUTE ','line_number':16252,'multiline':True]
['text':'.type  =','line_number':16257,'multiline':True]
['text':'.ith   =','line_number':16258,'multiline':True]
['text':'.nth   =','line_number':16259,'multiline':True]
['text':'.wsize =','line_number':16260,'multiline':True]
['text':'.wdata =','line_number':16261,'multiline':True]
['text':' thread scheduling for the different operations + work buffer size estimation','line_number':16282,'multiline':False]
['text':' here we need memory just for single 2D matrix from src0','line_number':16323,'multiline':False]
['text':' here we need memory just for single 2D matrix from src0','line_number':16340,'multiline':False]
['text':' K','line_number':16365,'multiline':False]
['text':' Cout','line_number':16366,'multiline':False]
['text':' Cin','line_number':16367,'multiline':False]
['text':' L','line_number':16369,'multiline':False]
['text':' Cin','line_number':16370,'multiline':False]
['text':' W','line_number':16386,'multiline':False]
['text':' H','line_number':16387,'multiline':False]
['text':' Channels Out','line_number':16388,'multiline':False]
['text':' Channels In','line_number':16389,'multiline':False]
['text':' W','line_number':16391,'multiline':False]
['text':' H','line_number':16392,'multiline':False]
['text':' Channels In','line_number':16393,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16403,'multiline':False]
['text':' this is overestimated by x2','line_number':16404,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16406,'multiline':False]
['text':' this is overestimated by x2','line_number':16407,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16413,'multiline':False]
['text':' this is overestimated by x2','line_number':16414,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16416,'multiline':False]
['text':' this is overestimated by x2','line_number':16417,'multiline':False]
['text':' *2 because of S and SM in ggml_compute_forward_flash_attn_back','line_number':16424,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16426,'multiline':False]
['text':' this is overestimated by x2','line_number':16427,'multiline':False]
['text':' TODO: this can become (n_tasks-1)','line_number':16429,'multiline':False]
['text':' this is overestimated by x2','line_number':16430,'multiline':False]
['text':'.cgraph                  =','line_number':16473,'multiline':True]
['text':'.cgraph_plan             =','line_number':16474,'multiline':True]
['text':'.perf_node_start_cycles  =','line_number':16475,'multiline':True]
['text':'.perf_node_start_time_us =','line_number':16476,'multiline':True]
['text':'.n_threads               =','line_number':16477,'multiline':True]
['text':'.n_active                =','line_number':16478,'multiline':True]
['text':'.node_n                  =','line_number':16479,'multiline':True]
['text':'.abort_callback          =','line_number':16480,'multiline':True]
['text':'.abort_callback_data     =','line_number':16481,'multiline':True]
['text':' create thread pool','line_number':16485,'multiline':False]
['text':' this is a work thread too','line_number':16506,'multiline':False]
['text':' don't leave affinity set on the main thread','line_number':16509,'multiline':False]
['text':' join or kill thread pool','line_number':16512,'multiline':False]
['text':' performance stats (graph)','line_number':16520,'multiline':False]
['text':' compute size of intermediate results','line_number':16602,'multiline':False]
['text':' TODO: does not take into account scratch buffers !!!!','line_number':16603,'multiline':False]
['text':' print','line_number':16608,'multiline':False]
['text':' header','line_number':16619,'multiline':False]
['text':' header','line_number':16632,'multiline':False]
['text':' write binary data','line_number':16652,'multiline':False]
['text':' header','line_number':16661,'multiline':False]
['text':' leafs','line_number':16675,'multiline':False]
['text':' dump the data','line_number':16697,'multiline':False]
['text':' TODO: pad this to 32 byte boundary','line_number':16698,'multiline':False]
['text':' nodes','line_number':16707,'multiline':False]
['text':' output the op arguments','line_number':16729,'multiline':False]
['text':' check if leaf','line_number':16741,'multiline':False]
['text':' check if node','line_number':16751,'multiline':False]
['text':' read file into data','line_number':16790,'multiline':False]
['text':' create the data context','line_number':16804,'multiline':False]
['text':' populate result','line_number':16837,'multiline':False]
['text':' create the data context','line_number':16860,'multiline':False]
['text':' leafs','line_number':16884,'multiline':False]
['text':' nodes','line_number':16930,'multiline':False]
['text':' parse args','line_number':16962,'multiline':False]
['text':' create the tensor','line_number':16977,'multiline':False]
['text':' "view" operations are handled differently','line_number':16978,'multiline':False]
['text':' TODO: handle inplace ops - currently a copy is always made','line_number':16979,'multiline':False]
['text':' TODO: implement other view ops','line_number':16984,'multiline':False]
['text':' check if node is part of the graph','line_number':17078,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':17247,'multiline':False]
['text':' TODO: add function to set tensor from array','line_number':17253,'multiline':False]
['text':' TODO: add function to get all elements at once','line_number':17264,'multiline':False]
['text':' TODO: add function to get all elements at once','line_number':17275,'multiline':False]
['text':' TODO: add function to get all elements at once','line_number':17286,'multiline':False]
['text':'','line_number':17293,'multiline':False]
['text':' ADAM','line_number':17294,'multiline':False]
['text':'','line_number':17295,'multiline':False]
['text':'   ref: https://arxiv.org/pdf/1412.6980.pdf','line_number':17296,'multiline':False]
['text':'','line_number':17297,'multiline':False]
['text':' these will store the parameters we want to optimize','line_number':17310,'multiline':False]
['text':' constants','line_number':17332,'multiline':False]
['text':' gradients','line_number':17344,'multiline':False]
['text':' first moment','line_number':17345,'multiline':False]
['text':' second moment','line_number':17346,'multiline':False]
['text':' past function values','line_number':17348,'multiline':False]
['text':' compute the function value','line_number':17356,'multiline':False]
['text':' ggml_graph_reset  (gf);','line_number':17366,'multiline':False]
['text':' initialize','line_number':17383,'multiline':False]
['text':' run the optimizer','line_number':17395,'multiline':False]
['text':' gradient clipping','line_number':17417,'multiline':False]
['text':' ggml_graph_reset  (gf);','line_number':17457,'multiline':False]
['text':' check convergence','line_number':17467,'multiline':False]
['text':' delta-based convergence test','line_number':17474,'multiline':False]
['text':' need at least params.past iterations to start checking for convergence','line_number':17476,'multiline':False]
['text':' check for improvement','line_number':17488,'multiline':False]
['text':'','line_number':17518,'multiline':False]
['text':' L-BFGS','line_number':17519,'multiline':False]
['text':'','line_number':17520,'multiline':False]
['text':' the L-BFGS implementation below is based on the following implementation:','line_number':17521,'multiline':False]
['text':'','line_number':17522,'multiline':False]
['text':'   https://github.com/chokkan/liblbfgs','line_number':17523,'multiline':False]
['text':'','line_number':17524,'multiline':False]
['text':' compute the initial gradient in the search direction','line_number':17568,'multiline':False]
['text':' make sure that d points to a descent direction','line_number':17571,'multiline':False]
['text':' initialize local variables','line_number':17576,'multiline':False]
['text':' evaluate the function and gradient values','line_number':17584,'multiline':False]
['text':' LBFG-S does not support learning rate -> ignore learning schedule','line_number':17592,'multiline':False]
['text':' ggml_graph_reset  (gf);','line_number':17599,'multiline':False]
['text':' Armijo condition is satisfied','line_number':17614,'multiline':False]
['text':' check the Wolfe condition','line_number':17621,'multiline':False]
['text':' regular Wolfe conditions','line_number':17626,'multiline':False]
['text':' strong Wolfe condition (GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE)','line_number':17633,'multiline':False]
['text':' these will store the parameters we want to optimize','line_number':17673,'multiline':False]
['text':' current parameters','line_number':17699,'multiline':False]
['text':' previous parameters','line_number':17700,'multiline':False]
['text':' current gradient','line_number':17701,'multiline':False]
['text':' previous gradient','line_number':17702,'multiline':False]
['text':' search direction','line_number':17703,'multiline':False]
['text':' past function values','line_number':17705,'multiline':False]
['text':' cost function value','line_number':17710,'multiline':False]
['text':' ||x||','line_number':17711,'multiline':False]
['text':' ||g||','line_number':17712,'multiline':False]
['text':' initialize x from the graph nodes','line_number':17714,'multiline':False]
['text':' the L-BFGS memory','line_number':17717,'multiline':False]
['text':' evaluate the function value and its gradient','line_number':17725,'multiline':False]
['text':' LBFG-S does not support learning rate -> ignore learning schedule','line_number':17733,'multiline':False]
['text':' ggml_graph_reset  (gf);','line_number':17740,'multiline':False]
['text':' search direction = -gradient','line_number':17752,'multiline':False]
['text':' ||x||, ||g||','line_number':17755,'multiline':False]
['text':' already optimized','line_number':17763,'multiline':False]
['text':' initial step','line_number':17774,'multiline':False]
['text':' store the current position and gradient vectors','line_number':17800,'multiline':False]
['text':' TODO: instead of passing &cancel here, use the return code of the linesearch','line_number':17804,'multiline':False]
['text':'       to determine if the optimization should be cancelled','line_number':17805,'multiline':False]
['text':'       this is a simple change, but not doing this atm, since I don't have a nice','line_number':17806,'multiline':False]
['text':'       way to test and don't want to break something with so many changes lined up','line_number':17807,'multiline':False]
['text':' linesearch failed - go back to the previous point and return','line_number':17814,'multiline':False]
['text':' converged','line_number':17832,'multiline':False]
['text':' delta-based convergence test','line_number':17836,'multiline':False]
['text':' need at least params.past iterations to start checking for convergence','line_number':17838,'multiline':False]
['text':' check for improvement','line_number':17850,'multiline':False]
['text':' reached the maximum number of iterations','line_number':17865,'multiline':False]
['text':' update vectors s and y:','line_number':17869,'multiline':False]
['text':'   s_{k+1} = x_{k+1} - x_{k} = \step * d_{k}.','line_number':17870,'multiline':False]
['text':'   y_{k+1} = g_{k+1} - g_{k}.','line_number':17871,'multiline':False]
['text':'','line_number':17872,'multiline':False]
['text':' compute scalars ys and yy:','line_number':17876,'multiline':False]
['text':'     ys = y^t \cdot s    -> 1 / \rho.','line_number':17877,'multiline':False]
['text':'     yy = y^t \cdot y.','line_number':17878,'multiline':False]
['text':'','line_number':17879,'multiline':False]
['text':' find new search direction','line_number':17885,'multiline':False]
['text':'   ref: https://en.wikipedia.org/wiki/Limited-memory_BFGS','line_number':17886,'multiline':False]
['text':' initialize search direction with -g','line_number':17893,'multiline':False]
['text':' \alpha_{j} = \rho_{j} s^{t}_{j} \cdot q_{k+1}','line_number':17899,'multiline':False]
['text':' q_{i} = q_{i+1} - \alpha_{i} y_{i}','line_number':17902,'multiline':False]
['text':' \beta_{j} = \rho_{j} y^t_{j} \cdot \gamma_{i}','line_number':17909,'multiline':False]
['text':' \gamma_{i+1} = \gamma_{i} + (\alpha_{j} - \beta_{j}) s_{j}','line_number':17912,'multiline':False]
['text':' FIXME: GGML_DEFAULT_N_THREADS ?','line_number':17932,'multiline':False]
['text':' build forward + backward compute graphs','line_number':18106,'multiline':False]
['text':' build forward + backward compute graphs','line_number':18125,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':18152,'multiline':False]
['text':' cast to 16 bins','line_number':18217,'multiline':False]
['text':' cast to 16 bins','line_number':18247,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':18362,'multiline':False]
['text':' GGUFv2','line_number':18365,'multiline':False]
['text':' undefined','line_number':18382,'multiline':False]
['text':' GGUFv2','line_number':18421,'multiline':False]
['text':' GGUFv2','line_number':18437,'multiline':False]
['text':' GGUFv2','line_number':18438,'multiline':False]
['text':' offset from start of `data`, must be a multiple of `ALIGNMENT`','line_number':18449,'multiline':False]
['text':' for writing API','line_number':18451,'multiline':False]
['text':' offset of `data` from beginning of file','line_number':18463,'multiline':False]
['text':' size of `data` in bytes','line_number':18464,'multiline':False]
['text':'uint8_t * padding;','line_number':18466,'multiline':False]
['text':' offset from start of file','line_number':18514,'multiline':False]
['text':' check the magic before making allocations','line_number':18519,'multiline':False]
['text':' read the header','line_number':18536,'multiline':False]
['text':' read the kv pairs','line_number':18563,'multiline':False]
['text':'fprintf(stderr, "%s: reading kv %d\n", __func__, i);','line_number':18570,'multiline':False]
['text':'fprintf(stderr, "%s: reading kv with key %s\n", __func__, kv->key.data);','line_number':18575,'multiline':False]
['text':' read the tensor infos','line_number':18638,'multiline':False]
['text':' we require the data section to be aligned, so take into account any padding','line_number':18673,'multiline':False]
['text':' store the current file offset - this is where the data section starts','line_number':18683,'multiline':False]
['text':' compute the total size of the data section, taking into account the alignment','line_number':18686,'multiline':False]
['text':' load the tensor data only if requested','line_number':18712,'multiline':False]
['text':' if the provided gguf_context is no_alloc, then we create "empty" tensors and do not read the binary blob','line_number':18714,'multiline':False]
['text':' otherwise, we load the binary blob into the created ggml_context as well, and point the "data" members of','line_number':18715,'multiline':False]
['text':' the ggml_tensor structs to the appropriate locations in the binary blob','line_number':18716,'multiline':False]
['text':' compute the exact size needed for the new ggml_context','line_number':18718,'multiline':False]
['text':' read the binary blob with the tensor data','line_number':18741,'multiline':False]
['text':' create the tensors','line_number':18757,'multiline':False]
['text':' point the data member to the appropriate location in the binary blob using the tensor infos','line_number':18776,'multiline':False]
['text':'cur->data = (char *) data->data + ctx->infos[i].offset - ctx->offset; // offset from start of file','line_number':18778,'multiline':False]
['text':' offset from data','line_number':18779,'multiline':False]
['text':' free string memory - not great..','line_number':18805,'multiline':False]
['text':' return -1 if key not found','line_number':18877,'multiline':False]
['text':' return -1 if tensor not found','line_number':19012,'multiline':False]
['text':' returns the index','line_number':19035,'multiline':False]
['text':' set or add KV pairs from another context','line_number':19161,'multiline':False]
['text':' update offsets','line_number':19245,'multiline':False]
['text':'static void gguf_fwrite_str(FILE * file, const struct gguf_str * val) {','line_number':19251,'multiline':False]
['text':'    fwrite(&val->n,   sizeof(val->n),    1, file);','line_number':19252,'multiline':False]
['text':'    fwrite(val->data, sizeof(char), val->n, file);','line_number':19253,'multiline':False]
['text':'}','line_number':19254,'multiline':False]
['text':'','line_number':19255,'multiline':False]
['text':'static void gguf_fwrite_el(FILE * file, const void * val, size_t size) {','line_number':19256,'multiline':False]
['text':'    fwrite(val, sizeof(char), size, file);','line_number':19257,'multiline':False]
['text':'}','line_number':19258,'multiline':False]
['text':'buf.data   =','line_number':19268,'multiline':True]
['text':'buf.size   =','line_number':19269,'multiline':True]
['text':'buf.offset =','line_number':19270,'multiline':True]
['text':' write header','line_number':19315,'multiline':False]
['text':' write key-value pairs','line_number':19321,'multiline':False]
['text':' write tensor infos','line_number':19375,'multiline':False]
['text':' we require the data section to be aligned, so take into account any padding','line_number':19388,'multiline':False]
['text':' write tensor data','line_number':19407,'multiline':False]
['text':' no allocs - only compute size','line_number':19447,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':19465,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':19615,'multiline':False]
