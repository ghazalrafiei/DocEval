['text':' backend buffer type','line_number':16,'multiline':False]
['text':' get_alloc_size is optional, defaults to ggml_nbytes','line_number':27,'multiline':False]
['text':' backend buffer','line_number':38,'multiline':False]
['text':' .interface = ','line_number':50,'multiline':True]
['text':' .buft      = ','line_number':51,'multiline':True]
['text':' .context   = ','line_number':52,'multiline':True]
['text':' .size      = ','line_number':53,'multiline':True]
['text':' init_tensor is optional','line_number':83,'multiline':False]
['text':' backend','line_number':101,'multiline':False]
['text':' TODO: optional sync','line_number':179,'multiline':False]
['text':' TODO: optional sync','line_number':186,'multiline':False]
['text':' backend copy','line_number':194,'multiline':False]
['text':'printf("src: %s ne: [%d %d %d %d] nb: [%d %d %d %d]\n", src->name, (int)src->ne[0], (int)src->ne[1], (int)src->ne[2], (int)src->ne[3], (int)src->nb[0], (int)src->nb[1], (int)src->nb[2], (int)src->nb[3]);','line_number':212,'multiline':False]
['text':'printf("dst: %s ne: [%d %d %d %d] nb: [%d %d %d %d]\n", dst->name, (int)dst->ne[0], (int)dst->ne[1], (int)dst->ne[2], (int)dst->ne[3], (int)dst->nb[0], (int)dst->nb[1], (int)dst->nb[2], (int)dst->nb[3]);','line_number':213,'multiline':False]
['text':' fprintf(stderr, "cpy tensor %s from %s to %s (%lu bytes)\n", src->name, ggml_backend_name(src->backend), ggml_backend_name(dst->backend), ggml_nbytes(src));','line_number':216,'multiline':False]
['text':' TODO: allow backends to support copy to/from same backend','line_number':222,'multiline':False]
['text':' shouldn't be hit when copying from/to CPU','line_number':229,'multiline':False]
['text':' backend registry','line_number':242,'multiline':False]
['text':' add forward decls here to avoid including the backend headers','line_number':269,'multiline':False]
['text':' .name                = ','line_number':288,'multiline':True]
['text':' .fn                  = ','line_number':289,'multiline':True]
['text':' .default_buffer_type = ','line_number':290,'multiline':True]
['text':' .user_data           = ','line_number':291,'multiline':True]
['text':' TODO: case insensitive in a portable way','line_number':313,'multiline':False]
['text':' init from backend:params string','line_number':321,'multiline':False]
['text':' backend CPU','line_number':373,'multiline':False]
['text':' .free_buffer     = ','line_number':415,'multiline':True]
['text':' .get_base        = ','line_number':416,'multiline':True]
['text':' .init_tensor     = ','line_number':417,'multiline':True]
['text':' no initialization required','line_number':417,'multiline':False]
['text':' .set_tensor      = ','line_number':418,'multiline':True]
['text':' .get_tensor      = ','line_number':419,'multiline':True]
['text':' .cpy_tensor_from = ','line_number':420,'multiline':True]
['text':' .cpy_tensor_to   = ','line_number':421,'multiline':True]
['text':' for buffers from ptr, free is not called','line_number':424,'multiline':False]
['text':' .free_buffer     = ','line_number':426,'multiline':True]
['text':' ptr is not owned by the buffer, so it does not need to be freed','line_number':426,'multiline':False]
['text':' .get_base        = ','line_number':427,'multiline':True]
['text':' .init_tensor     = ','line_number':428,'multiline':True]
['text':' no initialization required','line_number':428,'multiline':False]
['text':' .set_tensor      = ','line_number':429,'multiline':True]
['text':' .get_tensor      = ','line_number':430,'multiline':True]
['text':' .cpy_tensor_from = ','line_number':431,'multiline':True]
['text':' .cpy_tensor_to   = ','line_number':432,'multiline':True]
['text':' should be enough for AVX 512','line_number':435,'multiline':False]
['text':' malloc may return an address that is not aligned','line_number':438,'multiline':False]
['text':' TODO: maybe use GGML_ALIGNED_MALLOC?','line_number':439,'multiline':False]
['text':' .iface = ','line_number':460,'multiline':True]
['text':' .alloc_buffer     = ','line_number':461,'multiline':True]
['text':' .get_alignment    = ','line_number':462,'multiline':True]
['text':' .get_alloc_size   = ','line_number':463,'multiline':True]
['text':' defaults to ggml_nbytes','line_number':463,'multiline':False]
['text':' .supports_backend = ','line_number':464,'multiline':True]
['text':' .context = ','line_number':466,'multiline':True]
['text':' TODO: may be faster to free and use malloc to avoid the copy','line_number':540,'multiline':False]
['text':' .get_name                = ','line_number':558,'multiline':True]
['text':' .free                    = ','line_number':559,'multiline':True]
['text':' .get_default_buffer_type = ','line_number':560,'multiline':True]
['text':' .set_tensor_async        = ','line_number':561,'multiline':True]
['text':' .get_tensor_async        = ','line_number':562,'multiline':True]
['text':' .cpy_tensor_from_async   = ','line_number':563,'multiline':True]
['text':' .cpy_tensor_to_async     = ','line_number':564,'multiline':True]
['text':' .synchronize             = ','line_number':565,'multiline':True]
['text':' .graph_plan_create       = ','line_number':566,'multiline':True]
['text':' .graph_plan_free         = ','line_number':567,'multiline':True]
['text':' .graph_plan_compute      = ','line_number':568,'multiline':True]
['text':' .graph_compute           = ','line_number':569,'multiline':True]
['text':' .supports_op             = ','line_number':570,'multiline':True]
['text':' .interface = ','line_number':583,'multiline':True]
['text':' .context   = ','line_number':584,'multiline':True]
['text':' scheduler','line_number':612,'multiline':False]
['text':' [hash_set.size]','line_number':635,'multiline':False]
['text':' [hash_set.size][GGML_MAX_BACKENDS]','line_number':636,'multiline':False]
['text':' align context_buffer to GGML_MEM_ALIGN','line_number':644,'multiline':False]
['text':' returns the priority of the backend, lower is better','line_number':660,'multiline':False]
['text':' find highest prio backend that supports the buffer type','line_number':683,'multiline':False]
['text':' find highest prio backend that supports the buffer type','line_number':696,'multiline':False]
['text':' debug, remove','line_number':706,'multiline':False]
['text':' returns the backend that should be used for the node based on the current locations','line_number':714,'multiline':False]
['text':' if the dst tensor is already allocated in a buffer, we must assume that it is critical to keep it there','line_number':716,'multiline':False]
['text':' ie. kv cache updates','line_number':717,'multiline':False]
['text':' note that this doesn't allow fallback to CPU. need to add output tensors to the splits to copy the data back to the original backend.','line_number':718,'multiline':False]
['text':' dst','line_number':719,'multiline':False]
['text':' view_src','line_number':726,'multiline':False]
['text':' src','line_number':732,'multiline':False]
['text':' FIXME:','line_number':785,'multiline':False]
['text':' creates a copy of the tensor with the same memory layout','line_number':802,'multiline':False]
['text':' assigns backends to ops and splits the graph into subgraphs that can be computed on the same backend','line_number':811,'multiline':False]
['text':' TODO: merge passes','line_number':812,'multiline':False]
['text':' reset state','line_number':814,'multiline':False]
['text':' .mem_size =   ','line_number':822,'multiline':True]
['text':' .mem_buffer = ','line_number':823,'multiline':True]
['text':' .no_alloc =   ','line_number':824,'multiline':True]
['text':' pass 1: assign backends to ops with allocated inputs','line_number':833,'multiline':False]
['text':' do not overwrite user assignments','line_number':837,'multiline':False]
['text':' do not overwrite user assignments','line_number':852,'multiline':False]
['text':'printf("PASS 1 ASSIGNMENTS\n"); sched_print_assignments(sched, graph);','line_number':860,'multiline':False]
['text':' pass 2: assign backends to ops from current assignments','line_number':862,'multiline':False]
['text':' TODO:','line_number':863,'multiline':False]
['text':'  - reuse sched_backend_from_cur','line_number':864,'multiline':False]
['text':'printf("PASS 2 ASSIGNMENTS\n"); sched_print_assignments(sched, graph);','line_number':893,'multiline':False]
['text':' pass 3: assign backends to remaining src from dst (should only be leafs)','line_number':895,'multiline':False]
['text':'printf("PASS 3 ASSIGNMENTS\n"); sched_print_assignments(sched, graph);','line_number':910,'multiline':False]
['text':' pass 4: split graph, find tensors that need to be copied','line_number':912,'multiline':False]
['text':' TODO:','line_number':913,'multiline':False]
['text':'  - when switching from a less preferred backend to a more preferred backend, check if it is possible to move the switch to an earlier point for the same cost','line_number':914,'multiline':False]
['text':' find first backend','line_number':915,'multiline':False]
['text':'HACK','line_number':926,'multiline':False]
['text':'HACK','line_number':945,'multiline':False]
['text':' find inputs that are not on the same backend','line_number':950,'multiline':False]
['text':' create copies','line_number':962,'multiline':False]
['text':'fprintf(stderr, "PASS 4 ASSIGNMENTS\n"); sched_print_assignments(sched, graph); fflush(stdout);','line_number':978,'multiline':False]
['text':' sanity check: all sources should have the same backend as the node','line_number':981,'multiline':False]
['text':' && src_backend != NULL ','line_number':994,'multiline':True]
['text':' ignore nulls for now','line_number':994,'multiline':False]
['text':' create copies of the graph for each split','line_number':1003,'multiline':False]
['text':' FIXME: avoid this copy, pass split inputs to ggml_gallocr_alloc_graph_n in some other way','line_number':1004,'multiline':False]
['text':' add inputs to the graph copy so that they are allocated by ggml-alloc at the start of the split','line_number':1010,'multiline':False]
['text':' copy the input tensors to the split backend','line_number':1044,'multiline':False]
['text':' FIXME: may need to use the sched buffer instead','line_number':1054,'multiline':False]
['text':'GGML_ASSERT(input->buffer->backend != input_cpy->buffer->backend);','line_number':1061,'multiline':False]
['text':'GGML_ASSERT(input_cpy->buffer->backend == split_backend);','line_number':1062,'multiline':False]
['text':' ggml_backend_synchronize(split_backend);','line_number':1065,'multiline':False]
['text':' ggml_backend_synchronize(split_backend);','line_number':1077,'multiline':False]
['text':' per-backend timings','line_number':1083,'multiline':False]
['text':' init measure allocs for each backend','line_number':1112,'multiline':False]
['text':' initialize hash tables','line_number':1135,'multiline':False]
['text':' allocate buffers and reset allocators','line_number':1145,'multiline':False]
['text':' utils','line_number':1180,'multiline':False]
['text':' copy src','line_number':1227,'multiline':False]
['text':' init src','line_number':1255,'multiline':False]
['text':' .size = ','line_number':1267,'multiline':True]
['text':' .keys = ','line_number':1268,'multiline':True]
['text':' .mem_size   = ','line_number':1274,'multiline':True]
['text':' .mem_buffer = ','line_number':1275,'multiline':True]
['text':' .no_alloc   = ','line_number':1276,'multiline':True]
['text':' dup nodes','line_number':1282,'multiline':False]
['text':' allocate nodes','line_number':1288,'multiline':False]
['text':'printf("copy buffer size: %zu MB\n", ggml_backend_buffer_get_size(buffer) / 1024 / 1024);','line_number':1291,'multiline':False]
['text':' copy data and init views','line_number':1293,'multiline':False]
['text':' build graph copy','line_number':1299,'multiline':False]
['text':' .buffer           = ','line_number':1313,'multiline':True]
['text':' .ctx_allocated    = ','line_number':1314,'multiline':True]
['text':' .ctx_unallocated  = ','line_number':1315,'multiline':True]
['text':' .graph            = ','line_number':1316,'multiline':True]
['text':'printf("eval %d/%d\n", i, g1->n_nodes);','line_number':1334,'multiline':False]
['text':' compare results, calculate rms etc','line_number':1350,'multiline':False]
