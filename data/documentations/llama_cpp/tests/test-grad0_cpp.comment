['text':' Disables ridiculous "unsafe" warnings on Windows','line_number':1,'multiline':False]
['text':' possible loss of data','line_number':10,'multiline':False]
['text':'','line_number':26,'multiline':False]
['text':' logging','line_number':27,'multiline':False]
['text':'','line_number':28,'multiline':False]
['text':' ggml_graph_dump_dot(gf, NULL, "test-grad0-forward.dot");','line_number':247,'multiline':False]
['text':' ggml_graph_dump_dot(gb, gf,  "test-grad0-backward.dot");','line_number':248,'multiline':False]
['text':' compute gradient using finite differences','line_number':253,'multiline':False]
['text':' compute gradient using backward graph','line_number':272,'multiline':False]
['text':'assert(false);','line_number':286,'multiline':False]
['text':' TODO: clean-up this ..','line_number':295,'multiline':False]
['text':' .mem_size   = ','line_number':359,'multiline':True]
['text':' .mem_buffer = ','line_number':360,'multiline':True]
['text':' .no_alloc   = ','line_number':361,'multiline':True]
['text':' original loop: 1000','line_number':393,'multiline':False]
['text':' add f32','line_number':414,'multiline':False]
['text':' add f16','line_number':431,'multiline':False]
['text':' sub','line_number':448,'multiline':False]
['text':' mul','line_number':465,'multiline':False]
['text':' div','line_number':482,'multiline':False]
['text':' sqr','line_number':499,'multiline':False]
['text':' sqrt','line_number':516,'multiline':False]
['text':' log','line_number':533,'multiline':False]
['text':' sum','line_number':550,'multiline':False]
['text':' sum_rows','line_number':568,'multiline':False]
['text':' mean, not yet fully implemented','line_number':585,'multiline':False]
['text':' argmax','line_number':603,'multiline':False]
['text':' repeat','line_number':621,'multiline':False]
['text':' repeat back','line_number':644,'multiline':False]
['text':' abs (finite differences do not work)','line_number':667,'multiline':False]
['text':'{','line_number':668,'multiline':False]
['text':'    const int nargs = 1;','line_number':669,'multiline':False]
['text':'    for (int ndims = 1; ndims <= 2; ++ndims) {','line_number':671,'multiline':False]
['text':'        for (int i = 0; i < nargs; ++i) {','line_number':672,'multiline':False]
['text':'            x[i] = get_random_tensor_f32(ctx0, ndims, ne, -1.0f, 1.0f);','line_number':673,'multiline':False]
['text':'            ggml_set_param(ctx0, x[i]);','line_number':674,'multiline':False]
['text':'        }','line_number':675,'multiline':False]
['text':'        struct ggml_tensor * f = ggml_sum(ctx0, ggml_abs(ctx0, x[0]));','line_number':677,'multiline':False]
['text':'        check_gradient("abs", ctx0, x, f, ndims, nargs, 1e-3f, INFINITY, 1e-3f);','line_number':679,'multiline':False]
['text':'    }','line_number':680,'multiline':False]
['text':'}','line_number':681,'multiline':False]
['text':' sgn','line_number':683,'multiline':False]
['text':' neg','line_number':700,'multiline':False]
['text':' step','line_number':717,'multiline':False]
['text':' tanh, not yet fully implemented','line_number':734,'multiline':False]
['text':' mul_mat','line_number':752,'multiline':False]
['text':' check_mat_mul does not support ndims > 2','line_number':781,'multiline':False]
['text':' elu, not yet fully implemented','line_number':789,'multiline':False]
['text':' relu','line_number':807,'multiline':False]
['text':' gelu, not yet fully implemented','line_number':824,'multiline':False]
['text':' silu','line_number':842,'multiline':False]
['text':' due to GGML_SILU_FP16 the finite difference method will be slightly wrong -> increase error bounds.','line_number':856,'multiline':False]
['text':' rms_norm','line_number':864,'multiline':False]
['text':' scale','line_number':881,'multiline':False]
['text':' cpy f32','line_number':902,'multiline':False]
['text':' x[1] is overwritten by x[0], so the gradients don't propagate to x[1]','line_number':912,'multiline':False]
['text':' cpy f16','line_number':920,'multiline':False]
['text':' x[1] is overwritten by x[0], so the gradients don't propagate to x[1]','line_number':930,'multiline':False]
['text':' reshape (1d->nd)','line_number':938,'multiline':False]
['text':' reshape (nd->1d)','line_number':962,'multiline':False]
['text':' acc 1d','line_number':986,'multiline':False]
['text':' acc 2d','line_number':1014,'multiline':False]
['text':' acc 3d','line_number':1047,'multiline':False]
['text':' acc 4d','line_number':1082,'multiline':False]
['text':' set_1d','line_number':1119,'multiline':False]
['text':' set_2d','line_number':1147,'multiline':False]
['text':' view_1d','line_number':1180,'multiline':False]
['text':' view_2d','line_number':1204,'multiline':False]
['text':' view_3d','line_number':1235,'multiline':False]
['text':' permute','line_number':1267,'multiline':False]
['text':' ggml_permute will set axes of dimensions below n_dims to 1.','line_number':1275,'multiline':False]
['text':' to make ggml_permute work correctly on all axes,','line_number':1276,'multiline':False]
['text':' the input tensor needs maximal n_dim of 4.','line_number':1277,'multiline':False]
['text':' sum requires contiguous tensor rows','line_number':1294,'multiline':False]
['text':' transpose','line_number':1301,'multiline':False]
['text':' ggml_transpose will set axes of dimensions below n_dims to 1.','line_number':1309,'multiline':False]
['text':' to make ggml_transpose work correctly on all axes,','line_number':1310,'multiline':False]
['text':' the input tensor needs maximal n_dim of 4.','line_number':1311,'multiline':False]
['text':' sum requires contiguous tensor rows','line_number':1322,'multiline':False]
['text':' get_rows','line_number':1329,'multiline':False]
['text':' diag_mask_inf','line_number':1346,'multiline':False]
['text':' diag_mask_zero','line_number':1362,'multiline':False]
['text':' softmax','line_number':1378,'multiline':False]
['text':' dont use only sum as aggregation, because sum of softmax is always 1 -> finite differences should not work','line_number':1391,'multiline':False]
['text':' instead use sum(log(soft_max()*(1-eps)+eps)); use eps to avoid log(0)','line_number':1392,'multiline':False]
['text':' NOTE: softmax forward is computed using f16 table lookup instead of using actual expf, but backward assumes actual expf.','line_number':1402,'multiline':False]
['text':' this may result in different gradients too finite differences.','line_number':1403,'multiline':False]
['text':' when this test reports errors, first try to replace the table lookup with actual expf and test again to see if just that was the cause.','line_number':1404,'multiline':False]
['text':' if only the table lookup causes gradients to differ this is acceptable.','line_number':1405,'multiline':False]
['text':' cross_entropy_loss','line_number':1409,'multiline':False]
['text':' the second argument to cross_entropy_loss must sum up to 1 for each row','line_number':1420,'multiline':False]
['text':' rope f32','line_number':1440,'multiline':False]
['text':' we have no past, so this would have to work on uninitialized memory.','line_number':1464,'multiline':False]
['text':' we only test the gradients here;','line_number':1465,'multiline':False]
['text':' skip_past should have no influence on gradient computation.','line_number':1466,'multiline':False]
['text':' so when other modes work, we assume that this does as well.','line_number':1467,'multiline':False]
['text':' rope f16','line_number':1480,'multiline':False]
['text':' we have no past, so this would have to work on uninitialized memory.','line_number':1504,'multiline':False]
['text':' we only test the gradients here;','line_number':1505,'multiline':False]
['text':' skip_past should have no influence on gradient computation.','line_number':1506,'multiline':False]
['text':' so when other modes work, we assume that this does as well.','line_number':1507,'multiline':False]
['text':' flash_attn f32','line_number':1520,'multiline':False]
['text':' flash_attn f16, not yet fully implemented','line_number':1564,'multiline':False]
