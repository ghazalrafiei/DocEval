['text':' max number of parallel drafting sequences (i.e. tree branches)','line_number':37,'multiline':False]
['text':' probability threshold for accepting a token from the draft model','line_number':40,'multiline':False]
['text':' probability threshold for splitting a draft branch (only for n_seq_dft > 1)','line_number':43,'multiline':False]
['text':' LOG_DISABLE_LOGS','line_number':50,'multiline':False]
['text':' init llama.cpp','line_number':52,'multiline':False]
['text':' load the target model','line_number':61,'multiline':False]
['text':' load the draft model','line_number':65,'multiline':False]
['text':' Tokenize the prompt','line_number':98,'multiline':False]
['text':' eval the prompt with both models','line_number':134,'multiline':False]
['text':' the 2 models should have the same vocab','line_number':141,'multiline':False]
['text':'GGML_ASSERT(n_vocab == llama_n_vocab(model_dft));','line_number':142,'multiline':False]
['text':' how many tokens to draft each time','line_number':144,'multiline':False]
['text':' used to determine end of generation','line_number':154,'multiline':False]
['text':' target model sampling context','line_number':157,'multiline':False]
['text':' draft sequence data','line_number':160,'multiline':False]
['text':' the draft samplers will copy the target sampler's grammar','line_number':163,'multiline':False]
['text':' force greedy sampling with probs for the draft model','line_number':164,'multiline':False]
['text':' sample from the last token of the prompt','line_number':175,'multiline':False]
['text':' print current draft sequences','line_number':180,'multiline':False]
['text':' sample from the target model','line_number':197,'multiline':False]
['text':'LOG("last: %s\n", LOG_TOKENS_TOSTR_PRETTY(ctx_tgt, ctx_sampling->prev).c_str());','line_number':202,'multiline':False]
['text':' check if the target token matches any of the drafts','line_number':216,'multiline':False]
['text':' Color token according to its origin sequence','line_number':241,'multiline':False]
['text':' TODO: simplify','line_number':255,'multiline':False]
['text':' note: will be erased after the speculation phase','line_number':274,'multiline':False]
['text':' LOG("dft batch: %s\n", LOG_BATCH_TOSTR_PRETTY(ctx_dft, batch_dft).c_str());','line_number':282,'multiline':False]
['text':' sample n_draft tokens from the draft model using tree-based sampling','line_number':310,'multiline':False]
['text':' attempt to split the branch if the probability is high enough','line_number':340,'multiline':False]
['text':' all previous tokens from this branch are now also part of the new branch','line_number':348,'multiline':False]
['text':' copy the draft state','line_number':359,'multiline':False]
['text':' add drafted token for each sequence','line_number':378,'multiline':False]
['text':' add unique drafted tokens to the target batch','line_number':388,'multiline':False]
['text':' add the token to the batch for batched decoding with the draft model','line_number':393,'multiline':False]
['text':' no sequence is drafting anymore','line_number':404,'multiline':False]
['text':' evaluate the drafted tokens on the draft model','line_number':409,'multiline':False]
['text':' evaluate the target model on the drafted tokens','line_number':419,'multiline':False]
['text':' LOG("target batch: %s\n", LOG_BATCH_TOSTR_PRETTY(ctx_tgt, batch_tgt).c_str());','line_number':426,'multiline':False]
['text':' the first token is always proposed by the target model before the speculation loop so we erase it here','line_number':431,'multiline':False]
