['text':' Used for debugging to print out beam tokens.','line_number':27,'multiline':False]
['text':' Put here anything you want back in beam_search_callback().','line_number':41,'multiline':False]
['text':' In this case, end-of-beam (eob) is equivalent to end-of-sentence (eos) but this need not always be the same.','line_number':47,'multiline':False]
['text':' For example, eob can be flagged due to maximum token length, stop words, etc.','line_number':48,'multiline':False]
['text':' Function matching type llama_beam_search_callback_fn_t.','line_number':53,'multiline':False]
['text':' Custom callback example is called each time the beams lengths increase:','line_number':54,'multiline':False]
['text':'  * Show progress by printing ',' following by number of convergent beam tokens if any.','line_number':55,'multiline':False]
['text':'  * When all beams converge to a common prefix, they are made available in beams_state.beams[0].','line_number':56,'multiline':False]
['text':'    This is also called when the stop condition is met.','line_number':57,'multiline':False]
['text':'    Collect tokens into std::vector<llama_token> response which is pointed to by callback_data.','line_number':58,'multiline':False]
['text':' Mark beams as EOS as needed.','line_number':61,'multiline':False]
['text':' Show progress','line_number':68,'multiline':False]
['text':' DEBUG: print current beams for this iteration','line_number':77,'multiline':False]
['text':'params.n_gpu_layers = 200;','line_number':88,'multiline':False]
['text':'---------------------------------','line_number':90,'multiline':False]
['text':' Print help :','line_number':91,'multiline':False]
['text':'---------------------------------','line_number':92,'multiline':False]
['text':'---------------------------------','line_number':100,'multiline':False]
['text':' Load parameters :','line_number':101,'multiline':False]
['text':'---------------------------------','line_number':102,'multiline':False]
['text':'---------------------------------','line_number':118,'multiline':False]
['text':' Init LLM :','line_number':119,'multiline':False]
['text':'---------------------------------','line_number':120,'multiline':False]
['text':'---------------------------------','line_number':135,'multiline':False]
['text':' Tokenize the prompt :','line_number':136,'multiline':False]
['text':'---------------------------------','line_number':137,'multiline':False]
['text':' Print the tokens from the prompt :','line_number':153,'multiline':False]
