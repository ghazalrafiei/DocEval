['text':' GGUF keys & tensor names.','line_number':18,'multiline':False]
['text':' possible loss of data','line_number':57,'multiline':False]
['text':' 'ggjt'','line_number':60,'multiline':False]
['text':'////////////////////////////////////// llama2.c model structs and functions to load models, alloc memory etc.','line_number':68,'multiline':False]
['text':' transformer dimension','line_number':70,'multiline':False]
['text':' for ffn layers','line_number':71,'multiline':False]
['text':' number of layers','line_number':72,'multiline':False]
['text':' number of query heads','line_number':73,'multiline':False]
['text':' number of key/value heads (can be < query heads because of multiquery)','line_number':74,'multiline':False]
['text':' vocabulary size, usually 256 (byte-level)','line_number':75,'multiline':False]
['text':' max sequence length','line_number':76,'multiline':False]
['text':' token embedding table','line_number':80,'multiline':False]
['text':' (vocab_size, dim)','line_number':81,'multiline':False]
['text':' weights for rmsnorms','line_number':82,'multiline':False]
['text':' (layer, dim) rmsnorm weights','line_number':83,'multiline':False]
['text':' (layer, dim)','line_number':84,'multiline':False]
['text':' weights for matmuls','line_number':85,'multiline':False]
['text':' (layer, dim, dim)','line_number':86,'multiline':False]
['text':' (layer, dim, dim)','line_number':87,'multiline':False]
['text':' (layer, dim, dim)','line_number':88,'multiline':False]
['text':' (layer, dim, dim)','line_number':89,'multiline':False]
['text':' weights for ffn','line_number':90,'multiline':False]
['text':' (layer, hidden_dim, dim)','line_number':91,'multiline':False]
['text':' (layer, dim, hidden_dim)','line_number':92,'multiline':False]
['text':' (layer, hidden_dim, dim)','line_number':93,'multiline':False]
['text':' final rmsnorm','line_number':94,'multiline':False]
['text':' (dim,)','line_number':95,'multiline':False]
['text':' freq_cis for RoPE relatively positional embeddings','line_number':96,'multiline':False]
['text':' float* freq_cis_real; // (seq_len, dim/2)','line_number':97,'multiline':False]
['text':' float* freq_cis_imag; // (seq_len, dim/2)','line_number':98,'multiline':False]
['text':' (optional) classifier weights for the logits, on the last layer','line_number':99,'multiline':False]
['text':' we calloc instead of malloc to keep valgrind happy','line_number':119,'multiline':False]
['text':' Skip freq_cis_real & freq_cis_imag','line_number':174,'multiline':False]
['text':' Check we didn't forget to read anything','line_number':180,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////////////////////////////////','line_number':208,'multiline':False]
['text':'////////////////////////////////////// ggml structs and functions required to load models, configs and save the model.','line_number':210,'multiline':False]
['text':' this is provided as user input?','line_number':229,'multiline':False]
['text':' normalization','line_number':242,'multiline':False]
['text':' attention','line_number':245,'multiline':False]
['text':' normalization','line_number':251,'multiline':False]
['text':' ff','line_number':254,'multiline':False]
['text':' only adam','line_number':310,'multiline':False]
['text':' printing the per-layer allocations here so we dont print in the for loop.','line_number':361,'multiline':False]
['text':' use FILE * so we don't have to re-open the file to mmap','line_number':462,'multiline':False]
['text':' this really shouldn't fail','line_number':483,'multiline':False]
['text':' same','line_number':493,'multiline':False]
['text':'.no_alloc = ','line_number':557,'multiline':True]
['text':'.ctx      = ','line_number':558,'multiline':True]
['text':' assume llama2.c vocabulary','line_number':597,'multiline':False]
['text':' uint32_t max_token_length =  ','line_number':604,'multiline':True]
['text':' unused','line_number':604,'multiline':False]
['text':' Text of byte tokens is already in the expected format.','line_number':625,'multiline':False]
['text':' convert AK weights into GG weights one by one.','line_number':679,'multiline':False]
['text':' w->token_embedding_table -> model->tok_embeddings','line_number':680,'multiline':False]
['text':' float*                   -> struct ggml_tensor','line_number':681,'multiline':False]
['text':'print_row(model->norm, 0);','line_number':686,'multiline':False]
['text':' for rms-att-weight','line_number':688,'multiline':False]
['text':' 1d','line_number':694,'multiline':False]
['text':' from 3d matrix layer x dim x dim to 2d matrix dim x dim','line_number':698,'multiline':False]
['text':' special tokens','line_number':728,'multiline':False]
['text':' n_head_kv is optional, default to n_head','line_number':739,'multiline':False]
['text':' gguf_set_val_u32(ctx, KV_ATTENTION_HEAD_COUNT_KV, ...);','line_number':740,'multiline':False]
['text':' write tensors','line_number':745,'multiline':False]
['text':' only adam','line_number':821,'multiline':False]
['text':'argc','line_number':840,'multiline':True]
['text':' read in the config header','line_number':924,'multiline':False]
['text':' read in the Transformer weights','line_number':929,'multiline':False]
['text':'llama_n_vocab(lctx);','line_number':939,'multiline':False]
['text':'params.n_embd;','line_number':941,'multiline':False]
['text':'params.n_mult;','line_number':943,'multiline':False]
['text':'params.n_head;','line_number':944,'multiline':False]
['text':'params.n_layer;','line_number':945,'multiline':False]
