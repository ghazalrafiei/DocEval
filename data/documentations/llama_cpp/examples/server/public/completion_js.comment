['text':' Completes the prompt as a generator. Recommended for most use cases.','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':' Example:','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':'    import { llama } from '/completion.js'','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':'    const request = llama("Tell me a joke", {n_predict: 800})','line_number':17,'multiline':False]
['text':'    for await (const chunk of request) {','line_number':18,'multiline':False]
['text':'      document.write(chunk.data.content)','line_number':19,'multiline':False]
['text':'    }','line_number':20,'multiline':False]
['text':'','line_number':21,'multiline':False]
['text':' Buffer for partially read lines','line_number':46,'multiline':False]
['text':' Add any leftover data to the current chunk of data','line_number':57,'multiline':False]
['text':' Check if the last character is a line break','line_number':60,'multiline':False]
['text':' Split the text into lines','line_number':63,'multiline':False]
['text':' If the text doesn't end with a line break, then the last line is incomplete','line_number':66,'multiline':False]
['text':' Store it in leftover to be added to the next chunk of data','line_number':67,'multiline':False]
['text':' Reset leftover if we have a line break at the end','line_number':71,'multiline':False]
['text':' Parse all sse events and add them to result','line_number':74,'multiline':False]
['text':' since we know this is llama.cpp, let's just decode the json in data','line_number':80,'multiline':False]
['text':' yield','line_number':85,'multiline':False]
['text':' if we got a stop token from server, we will break here','line_number':88,'multiline':False]
['text':' Call llama, return an event target that you can subscribe to','line_number':117,'multiline':False]
['text':'','line_number':118,'multiline':False]
['text':' Example:','line_number':119,'multiline':False]
['text':'','line_number':120,'multiline':False]
['text':'    import { llamaEventTarget } from '/completion.js'','line_number':121,'multiline':False]
['text':'','line_number':122,'multiline':False]
['text':'    const conn = llamaEventTarget(prompt)','line_number':123,'multiline':False]
['text':'    conn.addEventListener("message", (chunk) => {','line_number':124,'multiline':False]
['text':'      document.write(chunk.detail.content)','line_number':125,'multiline':False]
['text':'    })','line_number':126,'multiline':False]
['text':'','line_number':127,'multiline':False]
['text':' Call llama, return a promise that resolves to the completed text. This does not support streaming','line_number':149,'multiline':False]
['text':'','line_number':150,'multiline':False]
['text':' Example:','line_number':151,'multiline':False]
['text':'','line_number':152,'multiline':False]
['text':'     llamaPromise(prompt).then((content) => {','line_number':153,'multiline':False]
['text':'       document.write(content)','line_number':154,'multiline':False]
['text':'     })','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':'     or','line_number':157,'multiline':False]
['text':'','line_number':158,'multiline':False]
['text':'     const content = await llamaPromise(prompt)','line_number':159,'multiline':False]
['text':'     document.write(content)','line_number':160,'multiline':False]
['text':'','line_number':161,'multiline':False]
['text':'*
 * (deprecated)
 ','line_number':176,'multiline':True]
['text':' Get the model info from the server. This is useful for getting the context window and so on.','line_number':185,'multiline':False]
