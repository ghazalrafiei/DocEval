['text':' possible loss of data','line_number':19,'multiline':False]
['text':' float f_norm_eps     = 1e-5f; // falcon','line_number':33,'multiline':False]
['text':' llama','line_number':34,'multiline':False]
['text':' normalization','line_number':57,'multiline':False]
['text':' attention','line_number':60,'multiline':False]
['text':' normalization','line_number':66,'multiline':False]
['text':' ff','line_number':69,'multiline':False]
['text':' normalization','line_number':108,'multiline':False]
['text':' attention','line_number':112,'multiline':False]
['text':' normalization','line_number':122,'multiline':False]
['text':' ff','line_number':126,'multiline':False]
['text':' gguf constants','line_number':152,'multiline':False]
['text':' gguf constants (sync with gguf.py)','line_number':169,'multiline':False]
['text':' TODO load in llama.cpp','line_number':182,'multiline':False]
['text':' n_head_kv is optional, default to n_head','line_number':265,'multiline':False]
['text':' get parameters directly from gguf file','line_number':295,'multiline':False]
['text':'.no_alloc = ','line_number':298,'multiline':True]
['text':'.ctx      = ','line_number':299,'multiline':True]
['text':' get tensors from llama_model (possibly mmapped)','line_number':310,'multiline':False]
['text':' context for lora tensors without their data','line_number':460,'multiline':False]
['text':' measure data size','line_number':531,'multiline':False]
['text':' allocate data','line_number':537,'multiline':False]
['text':' KQ_pos - contains the positions','line_number':628,'multiline':False]
['text':' rope has so much parameters that we make a custom function for it','line_number':638,'multiline':False]
['text':' not capturing these, to silcence warnings','line_number':641,'multiline':False]
['text':' make sure some tensors are not reallocated by inserting new temporary nodes depending on them','line_number':781,'multiline':False]
['text':' output tensors','line_number':785,'multiline':False]
['text':' input gradient','line_number':788,'multiline':False]
['text':' KQ_pos','line_number':792,'multiline':False]
['text':' make sure base model tensors data cannot be used in viewable operations','line_number':795,'multiline':False]
['text':' allocating checkpoints in one block to reduce memory fragmentation','line_number':812,'multiline':False]
['text':' note: they will be freed in reverse order','line_number':813,'multiline':False]
['text':' remove the additional nodes and leafs','line_number':822,'multiline':False]
['text':' NOTE: gguf_context must be initialized with f_ggml_ctx and no_alloc=false, otherwise tensor data can not be read','line_number':837,'multiline':False]
['text':' parameters that define tensor shapes must match','line_number':854,'multiline':False]
['text':' write file','line_number':1012,'multiline':False]
['text':' use FILE * so we don't have to re-open the file to mmap','line_number':1019,'multiline':False]
['text':' this really shouldn't fail','line_number':1040,'multiline':False]
['text':' same','line_number':1050,'multiline':False]
['text':' 'ggla'','line_number':1149,'multiline':False]
['text':' write_magic','line_number':1150,'multiline':False]
['text':' magic','line_number':1151,'multiline':False]
['text':' version','line_number':1152,'multiline':False]
['text':' write_hparams','line_number':1153,'multiline':False]
['text':' write tensors','line_number':1156,'multiline':False]
['text':' set params from command line','line_number':1566,'multiline':False]
['text':' set opt params from command line','line_number':1603,'multiline':False]
['text':' overwrite last n_ctx with user provided n_ctx','line_number':1629,'multiline':False]
['text':' need to discard previous optimizer gradient statistics and opt_init with new shapes','line_number':1654,'multiline':False]
['text':' TODO','line_number':1655,'multiline':False]
['text':' need to discard previous optimizer past function value statistics and opt_init with new shapes','line_number':1659,'multiline':False]
['text':' TODO','line_number':1660,'multiline':False]
['text':' existed == false','line_number':1662,'multiline':False]
['text':' context for input tensors without their data','line_number':1708,'multiline':False]
['text':' mem_size','line_number':1710,'multiline':False]
['text':' mem_buffer','line_number':1711,'multiline':False]
['text':' no_alloc','line_number':1712,'multiline':False]
['text':' the input tensors','line_number':1716,'multiline':False]
['text':' measure required memory for input tensors','line_number':1720,'multiline':False]
['text':' allocate input tensors','line_number':1726,'multiline':False]
['text':' context for compute tensors without their data','line_number':1733,'multiline':False]
['text':' mem_size','line_number':1739,'multiline':False]
['text':' mem_buffer','line_number':1740,'multiline':False]
['text':' no_alloc','line_number':1741,'multiline':False]
['text':' measure required memory for compute tensors','line_number':1752,'multiline':False]
['text':' find best evaluation order','line_number':1755,'multiline':False]
['text':' allocate compute tensors','line_number':1788,'multiline':False]
['text':' tokenize data','line_number':1808,'multiline':False]
['text':' measure required memory for work buffer','line_number':1900,'multiline':False]
['text':' context for work buffer','line_number':1904,'multiline':False]
['text':' mem_size','line_number':1906,'multiline':False]
['text':' mem_buffer','line_number':1907,'multiline':False]
['text':' no_alloc','line_number':1908,'multiline':False]
