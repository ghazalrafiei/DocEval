['text':' number of parallel batches','line_number':18,'multiline':False]
['text':' total length of the sequences including the prompt','line_number':21,'multiline':False]
['text':' number of layers to offload to the GPU','line_number':24,'multiline':False]
['text':' init LLM','line_number':51,'multiline':False]
['text':' initialize the model','line_number':55,'multiline':False]
['text':' tokenize the prompt','line_number':68,'multiline':False]
['text':' initialize the context','line_number':74,'multiline':False]
['text':' make sure the KV cache is big enough to hold all the prompt and generated tokens','line_number':95,'multiline':False]
['text':' print the prompt token-by-token','line_number':102,'multiline':False]
['text':' create a llama_batch','line_number':112,'multiline':False]
['text':' we use this object to submit token data for decoding','line_number':113,'multiline':False]
['text':' evaluate the initial prompt','line_number':116,'multiline':False]
['text':' llama_decode will output logits only for the last token of the prompt','line_number':122,'multiline':False]
['text':' assign the system KV cache to all parallel sequences','line_number':130,'multiline':False]
['text':' this way, the parallel sequences will "reuse" the prompt tokens without having to copy them','line_number':131,'multiline':False]
['text':' main loop','line_number':140,'multiline':False]
['text':' we will store the parallel decoded sequences in this vector','line_number':142,'multiline':False]
['text':' remember the batch index of the last token for each parallel sequence','line_number':145,'multiline':False]
['text':' we need this to determine which logits to sample from','line_number':146,'multiline':False]
['text':' prepare the next batch','line_number':155,'multiline':False]
['text':' sample the next token for each parallel sequence / stream','line_number':158,'multiline':False]
['text':' the stream has already finished','line_number':161,'multiline':False]
['text':'const llama_token new_token_id = llama_sample_token_greedy(ctx, &candidates_p);','line_number':187,'multiline':False]
['text':' is it an end of stream? -> mark the stream as finished','line_number':189,'multiline':False]
['text':' if there is only one stream, we print immediately to stdout','line_number':200,'multiline':False]
['text':' push this new token for next evaluation','line_number':210,'multiline':False]
['text':' all streams are finished','line_number':216,'multiline':False]
['text':' evaluate the current batch with the transformer model','line_number':223,'multiline':False]
