['text':' possible loss of data','line_number':12,'multiline':False]
['text':' this is provided as user input?','line_number':77,'multiline':False]
['text':' this is provided as user input?','line_number':96,'multiline':False]
['text':' normalization','line_number':110,'multiline':False]
['text':' attention','line_number':113,'multiline':False]
['text':' normalization','line_number':119,'multiline':False]
['text':' ff','line_number':122,'multiline':False]
['text':' normalization','line_number':129,'multiline':False]
['text':' attention','line_number':132,'multiline':False]
['text':' normalization','line_number':142,'multiline':False]
['text':' ff','line_number':145,'multiline':False]
['text':' llama_ctx_buffer buf;','line_number':158,'multiline':False]
['text':' number of tokens currently in the cache','line_number':160,'multiline':False]
['text':' ("tok_embeddings.weight", {n_embd, n_vocab});','line_number':201,'multiline':False]
['text':' ("norm.weight",           {n_embd});','line_number':202,'multiline':False]
['text':' ("output.weight",         {n_embd, n_vocab});','line_number':203,'multiline':False]
['text':' std::string layers_i = "layers." + std::to_string(i);','line_number':209,'multiline':False]
['text':' (layers_i + ".attention_norm.weight", {n_embd});','line_number':211,'multiline':False]
['text':' (layers_i + ".attention.wq.weight", {n_embd, n_embd});','line_number':213,'multiline':False]
['text':' (layers_i + ".attention.wk.weight", {n_embd, n_embd});','line_number':214,'multiline':False]
['text':' (layers_i + ".attention.wv.weight", {n_embd, n_embd});','line_number':215,'multiline':False]
['text':' (layers_i + ".attention.wo.weight", {n_embd, n_embd});','line_number':216,'multiline':False]
['text':' (layers_i + ".ffn_norm.weight", {n_embd});','line_number':218,'multiline':False]
['text':' (layers_i + ".feed_forward.w1.weight", {n_embd,   n_ff});','line_number':220,'multiline':False]
['text':' (layers_i + ".feed_forward.w2.weight", {  n_ff,   n_embd});','line_number':221,'multiline':False]
['text':' (layers_i + ".feed_forward.w3.weight", {n_embd,   n_ff});','line_number':222,'multiline':False]
['text':' ("tok_embeddings.weight", {n_embd, n_vocab});','line_number':240,'multiline':False]
['text':' ("norm.weight",           {n_embd});','line_number':241,'multiline':False]
['text':' ("output.weight",         {n_embd, n_vocab});','line_number':242,'multiline':False]
['text':' ("output.weight",         {n_embd, n_vocab});','line_number':243,'multiline':False]
['text':' std::string layers_i = "layers." + std::to_string(i);','line_number':249,'multiline':False]
['text':' (layers_i + ".attention_norm.weight", {n_embd});','line_number':251,'multiline':False]
['text':' (layers_i + ".attention.wq.weight", {n_embd, n_embd});','line_number':253,'multiline':False]
['text':' (layers_i + ".attention.wq.weight", {n_embd, n_embd});','line_number':254,'multiline':False]
['text':' (layers_i + ".attention.wk.weight", {n_embd, n_embd});','line_number':255,'multiline':False]
['text':' (layers_i + ".attention.wk.weight", {n_embd, n_embd});','line_number':256,'multiline':False]
['text':' (layers_i + ".attention.wv.weight", {n_embd, n_embd});','line_number':257,'multiline':False]
['text':' (layers_i + ".attention.wv.weight", {n_embd, n_embd});','line_number':258,'multiline':False]
['text':' (layers_i + ".attention.wo.weight", {n_embd, n_embd});','line_number':259,'multiline':False]
['text':' (layers_i + ".attention.wo.weight", {n_embd, n_embd});','line_number':260,'multiline':False]
['text':' (layers_i + ".ffn_norm.weight", {n_embd});','line_number':262,'multiline':False]
['text':' (layers_i + ".feed_forward.w1.weight", {n_embd,   n_ff});','line_number':264,'multiline':False]
['text':' (layers_i + ".feed_forward.w2.weight", {  n_ff,   n_embd});','line_number':265,'multiline':False]
['text':' (layers_i + ".feed_forward.w3.weight", {n_embd,   n_ff});','line_number':266,'multiline':False]
['text':' cache.buf.resize(2u*n_elements*ggml_type_size(wtype) + 2u*MB);','line_number':405,'multiline':False]
['text':' struct ggml_init_params params;','line_number':407,'multiline':False]
['text':' params.mem_size   = cache.buf.size;','line_number':408,'multiline':False]
['text':' params.mem_buffer = cache.buf.addr;','line_number':409,'multiline':False]
['text':' params.no_alloc   = false;','line_number':410,'multiline':False]
['text':' cache.buf.resize(2u*n_elements*ggml_type_size(wtype) + 2u*MB);','line_number':439,'multiline':False]
['text':' struct ggml_init_params params;','line_number':441,'multiline':False]
['text':' params.mem_size   = cache.buf.size;','line_number':442,'multiline':False]
['text':' params.mem_buffer = cache.buf.addr;','line_number':443,'multiline':False]
['text':' params.no_alloc   = false;','line_number':444,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':498,'multiline':False]
['text':' lctx.use_buf(ctx0, 0);','line_number':505,'multiline':False]
['text':' norm','line_number':507,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':509,'multiline':False]
['text':' cur = attention_norm*cur','line_number':512,'multiline':False]
['text':' self-attention','line_number':518,'multiline':False]
['text':' compute Q and K and RoPE them','line_number':520,'multiline':False]
['text':' wq   shape [n_embd, n_embd, 1, 1]','line_number':521,'multiline':False]
['text':' wk   shape [n_embd, n_embd, 1, 1]','line_number':522,'multiline':False]
['text':' Qcur shape [n_embd/n_head, n_head, N, 1]','line_number':523,'multiline':False]
['text':' Kcur shape [n_embd/n_head, n_head, N, 1]','line_number':524,'multiline':False]
['text':' store key and value to memory','line_number':528,'multiline':False]
['text':' compute the transposed [N, n_embd] V matrix','line_number':530,'multiline':False]
['text':' wv   shape [n_embd, n_embd, 1, 1]','line_number':531,'multiline':False]
['text':' Vcur shape [n_embd, N, 1, 1]','line_number':532,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_layer, 1]','line_number':535,'multiline':False]
['text':' kv_self.v shape [n_embd * n_ctx * n_layer, 1]','line_number':536,'multiline':False]
['text':' k         shape [n_embd * N, 1]   == kv_self.k[:,n_past:n_past+N,il,0]','line_number':537,'multiline':False]
['text':' v         shape [N, n_embd, 1, 1] == kv_self.v[:,n_past:n_past+N,il,0]','line_number':538,'multiline':False]
['text':' {
                    struct ggml_tensor * k = ggml_view_1d(ctx0, kv_self.k, N*n_embd, (ggml_element_size(kv_self.k)*n_embd)*(il*n_ctx + n_past));
                    struct ggml_tensor * v = ggml_view_2d(ctx0, kv_self.v, N, n_embd,
                            (   n_ctx)*ggml_element_size(kv_self.v),
                            (il*n_ctx)*ggml_element_size(kv_self.v)*n_embd + n_past*ggml_element_size(kv_self.v));

                    // important: storing RoPE-ed version of K in the KV cache!
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
                } //','line_number':540,'multiline':True]
['text':' Qcur shape [n_embd/n_head, n_head, N, 1]','line_number':556,'multiline':False]
['text':' Q shape    [n_embd/n_head, N, n_head, 1]','line_number':557,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_layer, 1]','line_number':563,'multiline':False]
['text':' K shape [n_embd/n_head, n_past + N, n_head, 1]','line_number':564,'multiline':False]
['text':' K * Q','line_number':572,'multiline':False]
['text':' KQ shape [n_past + N, N, n_head, 1]','line_number':573,'multiline':False]
['text':' KQ_scaled = KQ / sqrt(n_embd/n_head)','line_number':576,'multiline':False]
['text':' KQ_scaled shape [n_past + N, N, n_head, 1]','line_number':577,'multiline':False]
['text':' KQ_masked = mask_past(KQ_scaled)','line_number':583,'multiline':False]
['text':' KQ_masked shape [n_past + N, N, n_head, 1]','line_number':584,'multiline':False]
['text':' KQ = soft_max(KQ_masked)','line_number':587,'multiline':False]
['text':' KQ_soft_max shape [n_past + N, N, n_head, 1]','line_number':588,'multiline':False]
['text':' split cached V into n_head heads','line_number':591,'multiline':False]
['text':'// V shape [n_past + N, n_embd/n_head, n_head, 1]','line_number':592,'multiline':False]
['text':' V shape [n_past + N, n_embd/n_head, n_head, 1] == kv_self.v[:,:(n_past+N),il,1]','line_number':593,'multiline':False]
['text':' KQV shape [n_embd/n_head, N, n_head, 1]','line_number':601,'multiline':False]
['text':' KQV_merged = KQV.permute(0, 2, 1, 3)','line_number':604,'multiline':False]
['text':' KQV_merged shape [n_embd/n_head, n_head, N, 1]','line_number':605,'multiline':False]
['text':' KQV_merged shape','line_number':607,'multiline':False]
['text':' cur = KQV_merged.contiguous().view(n_embd, N)','line_number':609,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':610,'multiline':False]
['text':' cur = ggml_cpy(ctx0,','line_number':612,'multiline':False]
['text':'         KQV_merged,','line_number':613,'multiline':False]
['text':'         ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_embd, N));','line_number':614,'multiline':False]
['text':' projection (no bias)','line_number':616,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':617,'multiline':False]
['text':' lctx.use_buf(ctx0, 1);','line_number':623,'multiline':False]
['text':' inpFF shape [n_embd,N,1,1]','line_number':625,'multiline':False]
['text':' feed-forward network','line_number':628,'multiline':False]
['text':' norm','line_number':630,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':632,'multiline':False]
['text':' cur = ffn_norm*cur','line_number':635,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':636,'multiline':False]
['text':' tmp shape [n_ff,N,1,1]','line_number':642,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':647,'multiline':False]
['text':' SILU activation','line_number':652,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':653,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':656,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':659,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':665,'multiline':False]
['text':' input for next layer','line_number':668,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':669,'multiline':False]
['text':' norm','line_number':673,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':676,'multiline':False]
['text':' inpL = norm*inpL','line_number':679,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':680,'multiline':False]
['text':'embeddings = inpL;','line_number':685,'multiline':False]
['text':' lm_head','line_number':688,'multiline':False]
['text':' inpL shape [n_vocab,N,1,1]','line_number':689,'multiline':False]
['text':' run the computation','line_number':692,'multiline':False]
['text':' inpL shape [n_embd,N*n_batch,1]','line_number':734,'multiline':False]
['text':' lctx.use_buf(ctx0, 0);','line_number':743,'multiline':False]
['text':' norm','line_number':745,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':747,'multiline':False]
['text':' cur = attention_norm*cur','line_number':751,'multiline':False]
['text':' self-attention','line_number':758,'multiline':False]
['text':' compute Q and K and RoPE them','line_number':760,'multiline':False]
['text':' wq   shape [n_embd, n_embd, 1, 1]','line_number':761,'multiline':False]
['text':' wk   shape [n_embd, n_embd, 1, 1]','line_number':762,'multiline':False]
['text':' Qcur shape [n_embd/n_head, n_head, N, n_batch]','line_number':763,'multiline':False]
['text':' Kcur shape [n_embd/n_head, n_head, N, n_batch]','line_number':764,'multiline':False]
['text':' store key and value to memory','line_number':770,'multiline':False]
['text':' compute the transposed [N, n_embd] V matrix','line_number':772,'multiline':False]
['text':' wv   shape [n_embd, n_embd, 1, 1]','line_number':773,'multiline':False]
['text':' Vcur shape [N, n_embd, n_batch, 1]','line_number':774,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_batch * n_layer]','line_number':786,'multiline':False]
['text':' kv_self.v shape [n_ctx * n_embd * n_batch * n_layer]','line_number':787,'multiline':False]
['text':' k         shape [n_embd * N, n_batch]   == kv_self.k[:,n_past:n_past+N,:,il]','line_number':788,'multiline':False]
['text':' v         shape [N, n_embd, n_batch, 1] == kv_self.v[:,n_past:n_past+N,:,il]','line_number':789,'multiline':False]
['text':' {
                    struct ggml_tensor * k = ggml_view_1d(ctx0, kv_self.k, N*n_embd, (ggml_element_size(kv_self.k)*n_embd)*(il*n_ctx + n_past));
                    struct ggml_tensor * v = ggml_view_2d(ctx0, kv_self.v, N, n_embd,
                            (   n_ctx)*ggml_element_size(kv_self.v),
                            (il*n_ctx)*ggml_element_size(kv_self.v)*n_embd + n_past*ggml_element_size(kv_self.v));

                    // important: storing RoPE-ed version of K in the KV cache!
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
                } //','line_number':791,'multiline':True]
['text':' Qcur shape [n_embd/n_head, n_head, N, n_batch]','line_number':815,'multiline':False]
['text':' Q shape    [n_embd/n_head, N, n_head, n_batch]','line_number':816,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_batch * n_layer]','line_number':823,'multiline':False]
['text':' K shape [n_embd/n_head, n_past + N, n_head, n_batch]','line_number':824,'multiline':False]
['text':' K * Q','line_number':840,'multiline':False]
['text':' KQ shape [n_past + N, N, n_head, n_batch]','line_number':841,'multiline':False]
['text':' KQ_scaled = KQ / sqrt(n_embd/n_head)','line_number':845,'multiline':False]
['text':' KQ_scaled shape [n_past + N, N, n_head, n_batch]','line_number':846,'multiline':False]
['text':' KQ_masked = mask_past(KQ_scaled)','line_number':853,'multiline':False]
['text':' KQ_masked shape [n_past + N, N, n_head, n_batch]','line_number':854,'multiline':False]
['text':' KQ = soft_max(KQ_masked)','line_number':858,'multiline':False]
['text':' KQ_soft_max shape [n_past + N, N, n_head, n_batch]','line_number':859,'multiline':False]
['text':' split cached V into n_head heads','line_number':863,'multiline':False]
['text':' kv_self.v shape [n_ctx * n_embd * n_batch * n_layer]','line_number':864,'multiline':False]
['text':' V shape [n_past + N, n_embd/n_head, n_head, n_batch] == kv_self.v[:(n_past+N),:,:,il]','line_number':865,'multiline':False]
['text':' KQV shape [n_embd/n_head, N, n_head, n_batch]','line_number':875,'multiline':False]
['text':' KQV_merged = KQV.permute(0, 2, 1, 3)','line_number':879,'multiline':False]
['text':' KQV_merged shape [n_embd/n_head, n_head, N, n_batch]','line_number':880,'multiline':False]
['text':' KQV_merged shape','line_number':883,'multiline':False]
['text':' cur = KQV_merged.contiguous().view(n_embd, N)','line_number':885,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':886,'multiline':False]
['text':' cur = ggml_cpy(ctx0,','line_number':889,'multiline':False]
['text':'         KQV_merged,','line_number':890,'multiline':False]
['text':'         ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_embd, N));','line_number':891,'multiline':False]
['text':' projection (no bias)','line_number':893,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':894,'multiline':False]
['text':' lctx.use_buf(ctx0, 1);','line_number':901,'multiline':False]
['text':' inpFF shape [n_embd,N*n_batch,1,1]','line_number':903,'multiline':False]
['text':' feed-forward network','line_number':907,'multiline':False]
['text':' norm','line_number':909,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':911,'multiline':False]
['text':' cur = ffn_norm*cur','line_number':915,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':916,'multiline':False]
['text':' tmp shape [n_ff,N*n_batch,1,1]','line_number':923,'multiline':False]
['text':' cur shape [n_ff,N*n_batch,1,1]','line_number':929,'multiline':False]
['text':' SILU activation','line_number':935,'multiline':False]
['text':' cur shape [n_ff,N*n_batch,1,1]','line_number':936,'multiline':False]
['text':' cur shape [n_ff,N*n_batch,1,1]','line_number':940,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':944,'multiline':False]
['text':' cur shape [n_embd,N*n_batch,1,1]','line_number':951,'multiline':False]
['text':' input for next layer','line_number':955,'multiline':False]
['text':' inpL shape [n_embd,N*n_batch,1,1]','line_number':956,'multiline':False]
['text':' norm','line_number':961,'multiline':False]
['text':' inpL shape [n_embd,N*n_batch,1,1]','line_number':964,'multiline':False]
['text':' inpL = norm*inpL','line_number':968,'multiline':False]
['text':' inpL shape [n_embd,N*n_batch,1,1]','line_number':969,'multiline':False]
['text':'embeddings = inpL;','line_number':976,'multiline':False]
['text':' lm_head','line_number':979,'multiline':False]
['text':' inpL shape [n_vocab,N*n_batch,1,1]','line_number':980,'multiline':False]
['text':' inpL shape [n_vocab,N,n_batch,1]','line_number':985,'multiline':False]
['text':' run the computation','line_number':992,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':1032,'multiline':False]
['text':' norm','line_number':1039,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1041,'multiline':False]
['text':' cur = attention_norm*cur','line_number':1044,'multiline':False]
['text':' self-attention','line_number':1050,'multiline':False]
['text':' compute Q and K and RoPE them','line_number':1052,'multiline':False]
['text':' wq   shape [n_embd, n_embd, 1, 1]','line_number':1053,'multiline':False]
['text':' wk   shape [n_embd, n_embd, 1, 1]','line_number':1054,'multiline':False]
['text':' Qcur shape [n_embd/n_head, n_head, N, 1]','line_number':1055,'multiline':False]
['text':' Kcur shape [n_embd/n_head, n_head, N, 1]','line_number':1056,'multiline':False]
['text':' store key and value to memory','line_number':1076,'multiline':False]
['text':' compute the transposed [N, n_embd] V matrix','line_number':1078,'multiline':False]
['text':' wv   shape [n_embd, n_embd, 1, 1]','line_number':1079,'multiline':False]
['text':' Vcur shape [n_embd, N, 1, 1]','line_number':1080,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_layer, 1]','line_number':1091,'multiline':False]
['text':' kv_self.v shape [n_embd * n_ctx * n_layer, 1]','line_number':1092,'multiline':False]
['text':' k         shape [n_embd * N, 1]   == kv_self.k[:,n_past:n_past+N,il,0]','line_number':1093,'multiline':False]
['text':' v         shape [N, n_embd, 1, 1] == kv_self.v[:,n_past:n_past+N,il,0]','line_number':1094,'multiline':False]
['text':' {
                    struct ggml_tensor * k = ggml_view_1d(ctx0, kv_self.k, N*n_embd, (ggml_element_size(kv_self.k)*n_embd)*(il*n_ctx + n_past));
                    struct ggml_tensor * v = ggml_view_2d(ctx0, kv_self.v, N, n_embd,
                            (   n_ctx)*ggml_element_size(kv_self.v),
                            (il*n_ctx)*ggml_element_size(kv_self.v)*n_embd + n_past*ggml_element_size(kv_self.v));

                    // important: storing RoPE-ed version of K in the KV cache!
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Kcur, k));
                    ggml_build_forward_expand(gf, ggml_cpy(ctx0, Vcur, v));
                } //','line_number':1096,'multiline':True]
['text':' Qcur shape [n_embd/n_head, n_head, N, 1]','line_number':1112,'multiline':False]
['text':' Q shape    [n_embd/n_head, N, n_head, 1]','line_number':1113,'multiline':False]
['text':' kv_self.k shape [n_embd * n_ctx * n_layer, 1]','line_number':1119,'multiline':False]
['text':' K shape [n_embd/n_head, n_past + N, n_head, 1]','line_number':1120,'multiline':False]
['text':' K * Q','line_number':1128,'multiline':False]
['text':' KQ shape [n_past + N, N, n_head, 1]','line_number':1129,'multiline':False]
['text':' KQ_scaled = KQ / sqrt(n_embd/n_head)','line_number':1132,'multiline':False]
['text':' KQ_scaled shape [n_past + N, N, n_head, 1]','line_number':1133,'multiline':False]
['text':' KQ_masked = mask_past(KQ_scaled)','line_number':1139,'multiline':False]
['text':' KQ_masked shape [n_past + N, N, n_head, 1]','line_number':1140,'multiline':False]
['text':' KQ = soft_max(KQ_masked)','line_number':1143,'multiline':False]
['text':' KQ_soft_max shape [n_past + N, N, n_head, 1]','line_number':1144,'multiline':False]
['text':' split cached V into n_head heads','line_number':1147,'multiline':False]
['text':'// V shape [n_past + N, n_embd/n_head, n_head, 1]','line_number':1148,'multiline':False]
['text':' V shape [n_past + N, n_embd/n_head, n_head, 1] == kv_self.v[:,:(n_past+N),il,1]','line_number':1149,'multiline':False]
['text':' KQV shape [n_embd/n_head, N, n_head, 1]','line_number':1157,'multiline':False]
['text':' KQV_merged = KQV.permute(0, 2, 1, 3)','line_number':1160,'multiline':False]
['text':' KQV_merged shape [n_embd/n_head, n_head, N, 1]','line_number':1161,'multiline':False]
['text':' KQV_merged shape','line_number':1163,'multiline':False]
['text':' cur = KQV_merged.contiguous().view(n_embd, N)','line_number':1165,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1166,'multiline':False]
['text':' cur = ggml_cpy(ctx0,','line_number':1168,'multiline':False]
['text':'         KQV_merged,','line_number':1169,'multiline':False]
['text':'         ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_embd, N));','line_number':1170,'multiline':False]
['text':' projection (no bias)','line_number':1172,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1173,'multiline':False]
['text':' inpFF shape [n_embd,N,1,1]','line_number':1181,'multiline':False]
['text':' feed-forward network','line_number':1184,'multiline':False]
['text':' norm','line_number':1186,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1188,'multiline':False]
['text':' cur = ffn_norm*cur','line_number':1191,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1192,'multiline':False]
['text':' tmp shape [n_ff,N,1,1]','line_number':1198,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':1203,'multiline':False]
['text':' SILU activation','line_number':1208,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':1209,'multiline':False]
['text':' cur shape [n_ff,N,1,1]','line_number':1212,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1215,'multiline':False]
['text':' cur shape [n_embd,N,1,1]','line_number':1221,'multiline':False]
['text':' input for next layer','line_number':1224,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':1225,'multiline':False]
['text':' norm','line_number':1229,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':1232,'multiline':False]
['text':' inpL = norm*inpL','line_number':1235,'multiline':False]
['text':' inpL shape [n_embd,N,1,1]','line_number':1236,'multiline':False]
['text':'embeddings = inpL;','line_number':1241,'multiline':False]
['text':' lm_head','line_number':1245,'multiline':False]
['text':' inpL shape [n_vocab,N,1,1]','line_number':1246,'multiline':False]
['text':' ggml_set_scratch(ctx0, { 0, 0, nullptr, });','line_number':1253,'multiline':False]
['text':' run the computation','line_number':1254,'multiline':False]
['text':' ggml_set_zero(targets);','line_number':1369,'multiline':False]
['text':'*cosf(x*1.1f+1.0f);','line_number':1374,'multiline':False]
['text':' scale to [0..1]','line_number':1375,'multiline':False]
['text':' clamp to [0..1]','line_number':1377,'multiline':False]
['text':' todo: instead of a-b: a[1:]-b[:-1]','line_number':1425,'multiline':False]
['text':' model.hparams.n_embd  = 32;','line_number':1466,'multiline':False]
['text':' model.hparams.n_mult  = 2;','line_number':1467,'multiline':False]
['text':' model.hparams.n_head  = 4;','line_number':1468,'multiline':False]
['text':' model.hparams.n_layer = 8;','line_number':1469,'multiline':False]
['text':' model.hparams.n_rot   = 8;','line_number':1470,'multiline':False]
['text':'
    struct llama_model_lora model_lora;
    // model.hparams.n_vocab = 6;
    // model.hparams.n_ctx   = 64;
    // model.hparams.n_embd  = 128;
    // model.hparams.n_mult  = 2;
    // model.hparams.n_head  = 8;
    // model.hparams.n_layer = 6;
    // model.hparams.n_rot   = model.hparams.n_embd / model.hparams.n_head;

    model_lora.hparams.n_vocab = 16;
    model_lora.hparams.n_ctx   = 32;
    model_lora.hparams.n_embd  = 256;
    model_lora.hparams.n_mult  = 2;
    model_lora.hparams.n_head  = 16;
    model_lora.hparams.n_layer = 1;
    model_lora.hparams.n_lora  = 64;
    model_lora.hparams.n_rot   = MIN(16, model_lora.hparams.n_embd / model_lora.hparams.n_head);
    // model.hparams.n_rot   = (model.hparams.n_embd / model.hparams.n_head) / 2;

    // model.hparams.n_embd  = 32;
    // model.hparams.n_mult  = 2;
    // model.hparams.n_head  = 4;
    // model.hparams.n_layer = 8;
    // model.hparams.n_rot   = 8;

    model_lora.ctx = ggml_init(lcparams);
    printf("init model_lora\n");
    init_model_lora(&model_lora);
    set_param_model_lora(&model_lora);

    randomize_model_lora(&model_lora, 1337, 0.0f, 1.0f, -1.0f, +1.0f);
','line_number':1479,'multiline':True]
['text':' key + value cache for the self attention','line_number':1513,'multiline':False]
['text':'init_kv_cache_lora(&kv_self, &model_lora);','line_number':1518,'multiline':False]
['text':'.mem_size   =','line_number':1531,'multiline':True]
['text':'.mem_buffer =','line_number':1532,'multiline':True]
['text':'.no_alloc   =','line_number':1533,'multiline':True]
['text':' struct ggml_tensor * e = cross_entropy_loss(ctx0, targets, logits);','line_number':1550,'multiline':False]
['text':'n_threads','line_number':1554,'multiline':True]
['text':'','line_number':1563,'multiline':False]
['text':'n_threads','line_number':1565,'multiline':True]
['text':' printf("probabilities after optimization:\n");','line_number':1577,'multiline':False]
['text':' print_matrix(after_opt_probs);','line_number':1578,'multiline':False]
['text':'.mem_size   =','line_number':1606,'multiline':True]
['text':'.mem_buffer =','line_number':1607,'multiline':True]
['text':'.no_alloc   =','line_number':1608,'multiline':True]
['text':'n_threads','line_number':1618,'multiline':True]
['text':' int sample_at = n_tokens-1;','line_number':1625,'multiline':False]
['text':' print_row(probs, sample_at);','line_number':1628,'multiline':False]
['text':' ggml_free(kv_self.ctx);','line_number':1642,'multiline':False]
['text':' ggml_free(model_lora.ctx);','line_number':1643,'multiline':False]
