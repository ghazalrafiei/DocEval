['text':' GGML_USE_CUBLAS','line_number':10,'multiline':False]
['text':' 'ggsn'','line_number':42,'multiline':False]
['text':' Defined when llama.cpp is compiled with support for offloading model layers to GPU.','line_number':48,'multiline':False]
['text':'','line_number':56,'multiline':False]
['text':' C interface','line_number':57,'multiline':False]
['text':'','line_number':58,'multiline':False]
['text':' TODO: show sample usage','line_number':59,'multiline':False]
['text':'','line_number':60,'multiline':False]
['text':' SentencePiece','line_number':70,'multiline':False]
['text':' Byte Pair Encoding','line_number':71,'multiline':False]
['text':' model file types','line_number':84,'multiline':False]
['text':' except 1d tensors','line_number':87,'multiline':False]
['text':' except 1d tensors','line_number':88,'multiline':False]
['text':' except 1d tensors','line_number':89,'multiline':False]
['text':' tok_embeddings.weight and output.weight are F16','line_number':90,'multiline':False]
['text':' LLAMA_FTYPE_MOSTLY_Q4_2       = 5,  // support has been removed','line_number':91,'multiline':False]
['text':' LLAMA_FTYPE_MOSTLY_Q4_3       = 6,  // support has been removed','line_number':92,'multiline':False]
['text':' except 1d tensors','line_number':93,'multiline':False]
['text':' except 1d tensors','line_number':94,'multiline':False]
['text':' except 1d tensors','line_number':95,'multiline':False]
['text':' except 1d tensors','line_number':96,'multiline':False]
['text':' except 1d tensors','line_number':97,'multiline':False]
['text':' except 1d tensors','line_number':98,'multiline':False]
['text':' except 1d tensors','line_number':99,'multiline':False]
['text':' except 1d tensors','line_number':100,'multiline':False]
['text':' except 1d tensors','line_number':101,'multiline':False]
['text':' except 1d tensors','line_number':102,'multiline':False]
['text':' except 1d tensors','line_number':103,'multiline':False]
['text':' except 1d tensors','line_number':104,'multiline':False]
['text':' not specified in the model file','line_number':106,'multiline':False]
['text':' token id','line_number':118,'multiline':False]
['text':' log-odds of the token','line_number':119,'multiline':False]
['text':' probability of the token','line_number':120,'multiline':False]
['text':' Input data for llama_decode','line_number':131,'multiline':False]
['text':' A llama_batch object can contain input about one or many sequences','line_number':132,'multiline':False]
['text':' The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens','line_number':133,'multiline':False]
['text':'','line_number':134,'multiline':False]
['text':' - token  : the token ids of the input (used when embd is NULL)','line_number':135,'multiline':False]
['text':' - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)','line_number':136,'multiline':False]
['text':' - pos    : the positions of the respective token in the sequence','line_number':137,'multiline':False]
['text':' - seq_id : the sequence to which the respective token belongs','line_number':138,'multiline':False]
['text':' - logits : if zero, the logits for the respective token will not be output','line_number':139,'multiline':False]
['text':'','line_number':140,'multiline':False]
['text':' NOTE: helpers for smooth API transition - can be deprecated in the future','line_number':151,'multiline':False]
['text':'       for future-proof code, use the above fields instead and ignore everything below','line_number':152,'multiline':False]
['text':'','line_number':153,'multiline':False]
['text':' pos[i] = all_pos_0 + i*all_pos_1','line_number':154,'multiline':False]
['text':'','line_number':155,'multiline':False]
['text':' used if pos == NULL','line_number':156,'multiline':False]
['text':' used if pos == NULL','line_number':157,'multiline':False]
['text':' used if seq_id == NULL','line_number':158,'multiline':False]
['text':' number of layers to store in VRAM','line_number':178,'multiline':False]
['text':' the GPU that is used for scratch and small tensors','line_number':179,'multiline':False]
['text':' how to split layers across multiple GPUs (size: LLAMA_MAX_DEVICES)','line_number':180,'multiline':False]
['text':' called with a progress value between 0 and 1, pass NULL to disable','line_number':182,'multiline':False]
['text':' context pointer passed to the progress callback','line_number':185,'multiline':False]
['text':' override key-value pairs of the model meta data','line_number':188,'multiline':False]
['text':' Keep the booleans together to avoid misalignment during copy-by-value.','line_number':191,'multiline':False]
['text':' only load the vocabulary, no weights','line_number':192,'multiline':False]
['text':' use mmap if possible','line_number':193,'multiline':False]
['text':' force system to keep model in RAM','line_number':194,'multiline':False]
['text':' RNG seed, -1 for random','line_number':198,'multiline':False]
['text':' text context, 0 = from model','line_number':199,'multiline':False]
['text':' prompt processing maximum batch size','line_number':200,'multiline':False]
['text':' number of threads to use for generation','line_number':201,'multiline':False]
['text':' number of threads to use for batch processing','line_number':202,'multiline':False]
['text':' RoPE scaling type, from `enum llama_rope_scaling_type`','line_number':203,'multiline':False]
['text':' ref: https://github.com/ggerganov/llama.cpp/pull/2054','line_number':205,'multiline':False]
['text':' RoPE base frequency, 0 = from model','line_number':206,'multiline':False]
['text':' RoPE frequency scaling factor, 0 = from model','line_number':207,'multiline':False]
['text':' YaRN extrapolation mix factor, negative = from model','line_number':208,'multiline':False]
['text':' YaRN magnitude scaling factor','line_number':209,'multiline':False]
['text':' YaRN low correction dim','line_number':210,'multiline':False]
['text':' YaRN high correction dim','line_number':211,'multiline':False]
['text':' YaRN original context size','line_number':212,'multiline':False]
['text':' data type for K cache','line_number':214,'multiline':False]
['text':' data type for V cache','line_number':215,'multiline':False]
['text':' Keep the booleans together to avoid misalignment during copy-by-value.','line_number':217,'multiline':False]
['text':' if true, use experimental mul_mat_q kernels (DEPRECATED - always true)','line_number':218,'multiline':False]
['text':' the llama_eval() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)','line_number':219,'multiline':False]
['text':' embedding mode only','line_number':220,'multiline':False]
['text':' whether to offload the KQV ops (including the KV cache) to GPU','line_number':221,'multiline':False]
['text':' model quantization parameters','line_number':224,'multiline':False]
['text':' number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()','line_number':226,'multiline':False]
['text':' quantize to this llama_ftype','line_number':227,'multiline':False]
['text':' allow quantizing non-f32/f16 tensors','line_number':228,'multiline':False]
['text':' quantize output.weight','line_number':229,'multiline':False]
['text':' only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored','line_number':230,'multiline':False]
['text':' disable k-quant mixtures and quantize all tensors to the same type','line_number':231,'multiline':False]
['text':' grammar types','line_number':234,'multiline':False]
['text':' grammar element type','line_number':237,'multiline':False]
['text':' end of rule definition','line_number':239,'multiline':False]
['text':' start of alternate definition for rule','line_number':242,'multiline':False]
['text':' non-terminal element: reference to rule','line_number':245,'multiline':False]
['text':' terminal element: character (code point)','line_number':248,'multiline':False]
['text':' inverse char(s) ([^a], [^a-b] [^abc])','line_number':251,'multiline':False]
['text':' modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to','line_number':254,'multiline':False]
['text':' be an inclusive range ([a-z])','line_number':255,'multiline':False]
['text':' modifies a preceding LLAMA_GRETYPE_CHAR or','line_number':258,'multiline':False]
['text':' LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])','line_number':259,'multiline':False]
['text':' Unicode code point or rule ID','line_number':265,'multiline':False]
['text':' performance timing information','line_number':268,'multiline':False]
['text':' Helpers for getting default parameters','line_number':282,'multiline':False]
['text':' Initialize the llama + ggml backend','line_number':287,'multiline':False]
['text':' If numa is true, use NUMA optimizations','line_number':288,'multiline':False]
['text':' Call once at the start of the program','line_number':289,'multiline':False]
['text':' Call once at the end of the program - currently only used for MPI','line_number':292,'multiline':False]
['text':' Frees all allocated memory','line_number':305,'multiline':False]
['text':' Get the model's RoPE frequency scaling factor','line_number':324,'multiline':False]
['text':' Functions to access the model's GGUF metadata scalar values','line_number':327,'multiline':False]
['text':' - The functions return the length of the string on success, or -1 on failure','line_number':328,'multiline':False]
['text':' - The output string is always null-terminated and cleared on failure','line_number':329,'multiline':False]
['text':' - GGUF array values are not supported by these functions','line_number':330,'multiline':False]
['text':' Get metadata value as a string by key name','line_number':332,'multiline':False]
['text':' Get the number of metadata key/value pairs','line_number':335,'multiline':False]
['text':' Get metadata key name by index','line_number':338,'multiline':False]
['text':' Get metadata value as a string by index','line_number':341,'multiline':False]
['text':' Get a string describing the model type','line_number':344,'multiline':False]
['text':' Returns the total size of all the tensors in the model in bytes','line_number':347,'multiline':False]
['text':' Returns the total number of parameters in the model','line_number':350,'multiline':False]
['text':' Get a llama model tensor','line_number':353,'multiline':False]
['text':' Returns 0 on success','line_number':356,'multiline':False]
['text':' Apply a LoRA adapter to a loaded model','line_number':362,'multiline':False]
['text':' path_base_model is the path to a higher quality model to use as a base for','line_number':363,'multiline':False]
['text':' the layers modified by the adapter. Can be NULL to use the current loaded model.','line_number':364,'multiline':False]
['text':' The model needs to be reloaded before applying a new adapter, otherwise the adapter','line_number':365,'multiline':False]
['text':' will be applied on top of the previous one','line_number':366,'multiline':False]
['text':' Returns 0 on success','line_number':367,'multiline':False]
['text':'','line_number':383,'multiline':False]
['text':' KV cache','line_number':384,'multiline':False]
['text':'','line_number':385,'multiline':False]
['text':' Information associated with an individual cell in the KV cache view.','line_number':387,'multiline':False]
['text':' The position for this cell. Takes KV cache shifts into account.','line_number':389,'multiline':False]
['text':' May be negative if the cell is not populated.','line_number':390,'multiline':False]
['text':' An updateable view of the KV cache.','line_number':394,'multiline':False]
['text':' Number of KV cache cells. This will be the same as the context size.','line_number':396,'multiline':False]
['text':' Maximum number of sequences that can exist in a cell. It's not an error','line_number':399,'multiline':False]
['text':' if there are more sequences in a cell than this value, however they will','line_number':400,'multiline':False]
['text':' not be visible in the view cells_sequences.','line_number':401,'multiline':False]
['text':' Number of tokens in the cache. For example, if there are two populated','line_number':404,'multiline':False]
['text':' cells, the first with 1 sequence id in it and the second with 2 sequence','line_number':405,'multiline':False]
['text':' ids then you'll have 3 tokens.','line_number':406,'multiline':False]
['text':' Number of populated cache cells.','line_number':409,'multiline':False]
['text':' Maximum contiguous empty slots in the cache.','line_number':412,'multiline':False]
['text':' Index to the start of the max_contiguous slot range. Can be negative','line_number':415,'multiline':False]
['text':' when cache is full.','line_number':416,'multiline':False]
['text':' Information for an individual cell.','line_number':419,'multiline':False]
['text':' The sequences for each cell. There will be n_max_seq items per cell.','line_number':422,'multiline':False]
['text':' Create an empty KV cache view. (use only for debugging purposes)','line_number':426,'multiline':False]
['text':' Free a KV cache view. (use only for debugging purposes)','line_number':429,'multiline':False]
['text':' Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)','line_number':432,'multiline':False]
['text':' Returns the number of tokens in the KV cache (slow, use only for debug)','line_number':435,'multiline':False]
['text':' If a KV cell has multiple sequences assigned to it, it will be counted multiple times','line_number':436,'multiline':False]
['text':' Returns the number of used KV cells (i.e. have at least one sequence assigned to them)','line_number':439,'multiline':False]
['text':' Clear the KV cache','line_number':442,'multiline':False]
['text':' Removes all tokens that belong to the specified sequence and have positions in [p0, p1)','line_number':446,'multiline':False]
['text':' seq_id < 0 : match any sequence','line_number':447,'multiline':False]
['text':' p0 < 0     : [0,  p1]','line_number':448,'multiline':False]
['text':' p1 < 0     : [p0, inf)','line_number':449,'multiline':False]
['text':' Copy all tokens that belong to the specified sequence to another sequence','line_number':456,'multiline':False]
['text':' Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence','line_number':457,'multiline':False]
['text':' p0 < 0 : [0,  p1]','line_number':458,'multiline':False]
['text':' p1 < 0 : [p0, inf)','line_number':459,'multiline':False]
['text':' Removes all tokens that do not belong to the specified sequence','line_number':467,'multiline':False]
['text':' Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)','line_number':472,'multiline':False]
['text':' If the KV cache is RoPEd, the KV data is updated accordingly','line_number':473,'multiline':False]
['text':' p0 < 0 : [0,  p1]','line_number':474,'multiline':False]
['text':' p1 < 0 : [p0, inf)','line_number':475,'multiline':False]
['text':'','line_number':483,'multiline':False]
['text':' State / sessions','line_number':484,'multiline':False]
['text':'','line_number':485,'multiline':False]
['text':' Returns the maximum size in bytes of the state (rng, logits, embedding','line_number':487,'multiline':False]
['text':' and kv_cache) - will often be smaller after compacting tokens','line_number':488,'multiline':False]
['text':' Copies the state to the specified destination address.','line_number':491,'multiline':False]
['text':' Destination needs to have allocated enough memory.','line_number':492,'multiline':False]
['text':' Returns the number of bytes copied','line_number':493,'multiline':False]
['text':' Set the state reading from the specified address','line_number':498,'multiline':False]
['text':' Returns the number of bytes read','line_number':499,'multiline':False]
['text':' Save/load session file','line_number':504,'multiline':False]
['text':'','line_number':518,'multiline':False]
['text':' Decoding','line_number':519,'multiline':False]
['text':'','line_number':520,'multiline':False]
['text':' Run the llama inference to obtain the logits and probabilities for the next token(s).','line_number':522,'multiline':False]
['text':' tokens + n_tokens is the provided batch of new tokens to process','line_number':523,'multiline':False]
['text':' n_past is the number of tokens to use from previous eval calls','line_number':524,'multiline':False]
['text':' Returns 0 on success','line_number':525,'multiline':False]
['text':' DEPRECATED: use llama_decode() instead','line_number':526,'multiline':False]
['text':' Same as llama_eval, but use float matrix input directly.','line_number':534,'multiline':False]
['text':' DEPRECATED: use llama_decode() instead','line_number':535,'multiline':False]
['text':' Return batch for single sequence of tokens starting at pos_0','line_number':543,'multiline':False]
['text':'','line_number':544,'multiline':False]
['text':' NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it','line_number':545,'multiline':False]
['text':'','line_number':546,'multiline':False]
['text':' Allocates a batch of tokens on the heap that can hold a maximum of n_tokens','line_number':553,'multiline':False]
['text':' Each token can be assigned up to n_seq_max sequence ids','line_number':554,'multiline':False]
['text':' The batch has to be freed with llama_batch_free()','line_number':555,'multiline':False]
['text':' If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)','line_number':556,'multiline':False]
['text':' Otherwise, llama_batch.token will be allocated to store n_tokens llama_token','line_number':557,'multiline':False]
['text':' The rest of the llama_batch members are allocated with size n_tokens','line_number':558,'multiline':False]
['text':' All members are left uninitialized','line_number':559,'multiline':False]
['text':' Frees a batch of tokens allocated with llama_batch_init()','line_number':565,'multiline':False]
['text':' Positive return values does not mean a fatal error, but rather a warning.','line_number':568,'multiline':False]
['text':'   0 - success','line_number':569,'multiline':False]
['text':'   1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)','line_number':570,'multiline':False]
['text':' < 0 - error','line_number':571,'multiline':False]
['text':' Set the number of threads used for decoding','line_number':576,'multiline':False]
['text':' n_threads is the number of threads used for generation (single token)','line_number':577,'multiline':False]
['text':' n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)','line_number':578,'multiline':False]
['text':' Token logits obtained from the last call to llama_eval()','line_number':581,'multiline':False]
['text':' The logits for the last token are stored in the last row','line_number':582,'multiline':False]
['text':' Logits for which llama_batch.logits[i] == 0 are undefined','line_number':583,'multiline':False]
['text':' Rows: n_tokens provided with llama_batch','line_number':584,'multiline':False]
['text':' Cols: n_vocab','line_number':585,'multiline':False]
['text':' Logits for the ith token. Equivalent to:','line_number':588,'multiline':False]
['text':' llama_get_logits(ctx) + i*n_vocab','line_number':589,'multiline':False]
['text':' Get the embeddings for the input','line_number':592,'multiline':False]
['text':' shape: [n_embd] (1-dimensional)','line_number':593,'multiline':False]
['text':'','line_number':596,'multiline':False]
['text':' Vocab','line_number':597,'multiline':False]
['text':'','line_number':598,'multiline':False]
['text':' Special tokens','line_number':606,'multiline':False]
['text':' beginning-of-sentence','line_number':607,'multiline':False]
['text':' end-of-sentence','line_number':608,'multiline':False]
['text':' next-line','line_number':609,'multiline':False]
['text':' Returns -1 if unknown, 1 for true or 0 for false.','line_number':611,'multiline':False]
['text':' Returns -1 if unknown, 1 for true or 0 for false.','line_number':614,'multiline':False]
['text':' codellama infill tokens','line_number':617,'multiline':False]
['text':' Beginning of infill prefix','line_number':618,'multiline':False]
['text':' Beginning of infill middle','line_number':619,'multiline':False]
['text':' Beginning of infill suffix','line_number':620,'multiline':False]
['text':' End of infill middle','line_number':621,'multiline':False]
['text':'','line_number':623,'multiline':False]
['text':' Tokenization','line_number':624,'multiline':False]
['text':'','line_number':625,'multiline':False]
['text':'/ @details Convert the provided text into tokens.','line_number':627,'multiline':False]
['text':'/ @param tokens The tokens pointer must be large enough to hold the resulting tokens.','line_number':628,'multiline':False]
['text':'/ @return Returns the number of tokens on success, no more than n_max_tokens','line_number':629,'multiline':False]
['text':'/ @return Returns a negative number on failure - the number of tokens that would have been returned','line_number':630,'multiline':False]
['text':'/ @param special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.','line_number':631,'multiline':False]
['text':'/                Does not insert a leading space.','line_number':632,'multiline':False]
['text':' Token Id -> Piece.','line_number':642,'multiline':False]
['text':' Uses the vocabulary in the provided context.','line_number':643,'multiline':False]
['text':' Does not write null terminator to the buffer.','line_number':644,'multiline':False]
['text':' User code is responsible to remove the leading whitespace of the first non-BOS token when decoding multiple tokens.','line_number':645,'multiline':False]
['text':'','line_number':652,'multiline':False]
['text':' Grammar','line_number':653,'multiline':False]
['text':'','line_number':654,'multiline':False]
['text':'','line_number':665,'multiline':False]
['text':' Sampling functions','line_number':666,'multiline':False]
['text':'','line_number':667,'multiline':False]
['text':' Sets the current rng seed.','line_number':669,'multiline':False]
['text':'/ @details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.','line_number':672,'multiline':False]
['text':'/ @details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.','line_number':673,'multiline':False]
['text':'/ @details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806','line_number':683,'multiline':False]
['text':'/ @param candidates A vector of `llama_token_data` containing the candidate tokens, the logits must be directly extracted from the original generation context without being sorted.','line_number':684,'multiline':False]
['text':'/ @params guidance_ctx A separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.','line_number':685,'multiline':False]
['text':'/ @params scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.','line_number':686,'multiline':False]
['text':'/ @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.','line_number':693,'multiline':False]
['text':'/ @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751','line_number':698,'multiline':False]
['text':'/ @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751','line_number':705,'multiline':False]
['text':'/ @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841','line_number':712,'multiline':False]
['text':'/ @details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.','line_number':719,'multiline':False]
['text':'/ @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.','line_number':726,'multiline':False]
['text':'/ @details Apply constraints from grammar','line_number':744,'multiline':False]
['text':'/ @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.','line_number':750,'multiline':False]
['text':'/ @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.','line_number':751,'multiline':False]
['text':'/ @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.','line_number':752,'multiline':False]
['text':'/ @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.','line_number':753,'multiline':False]
['text':'/ @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.','line_number':754,'multiline':False]
['text':'/ @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.','line_number':755,'multiline':False]
['text':'/ @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.','line_number':764,'multiline':False]
['text':'/ @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.','line_number':765,'multiline':False]
['text':'/ @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.','line_number':766,'multiline':False]
['text':'/ @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.','line_number':767,'multiline':False]
['text':'/ @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.','line_number':768,'multiline':False]
['text':'/ @details Selects the token with the highest probability.','line_number':776,'multiline':False]
['text':'/          Does not compute the token probabilities. Use llama_sample_softmax() instead.','line_number':777,'multiline':False]
['text':'/ @details Randomly selects a token from the candidates based on their probabilities.','line_number':782,'multiline':False]
['text':'/ @details Accepts the sampled token into the grammar','line_number':787,'multiline':False]
['text':'','line_number':793,'multiline':False]
['text':' Beam search','line_number':794,'multiline':False]
['text':'','line_number':795,'multiline':False]
['text':' Cumulative beam probability (renormalized relative to all beams)','line_number':801,'multiline':False]
['text':' Callback should set this to true when a beam is at end-of-beam.','line_number':802,'multiline':False]
['text':' Passed to beam_search_callback function.','line_number':805,'multiline':False]
['text':' Whenever 0 < common_prefix_length, this number of tokens should be copied from any of the beams','line_number':806,'multiline':False]
['text':' (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.','line_number':807,'multiline':False]
['text':' These pointers are valid only during the synchronous callback, so should not be saved.','line_number':808,'multiline':False]
['text':' Number of elements in beam_views[].','line_number':812,'multiline':False]
['text':' Current max length of prefix tokens shared by all beams.','line_number':813,'multiline':False]
['text':' True iff this is the last callback invocation.','line_number':814,'multiline':False]
['text':' Type of pointer to the beam_search_callback function.','line_number':817,'multiline':False]
['text':' void* callback_data is any custom data passed to llama_beam_search, that is subsequently','line_number':818,'multiline':False]
['text':' passed back to beam_search_callback. This avoids having to use global variables in the callback.','line_number':819,'multiline':False]
['text':'/ @details Deterministically returns entire sentence constructed by a beam search.','line_number':822,'multiline':False]
['text':'/ @param ctx Pointer to the llama_context.','line_number':823,'multiline':False]
['text':'/ @param callback Invoked for each iteration of the beam_search loop, passing in beams_state.','line_number':824,'multiline':False]
['text':'/ @param callback_data A pointer that is simply passed back to callback.','line_number':825,'multiline':False]
['text':'/ @param n_beams Number of beams to use.','line_number':826,'multiline':False]
['text':'/ @param n_past Number of tokens already evaluated.','line_number':827,'multiline':False]
['text':'/ @param n_predict Maximum number of tokens to predict. EOS may occur earlier.','line_number':828,'multiline':False]
['text':' Performance information','line_number':837,'multiline':False]
['text':' Print system information','line_number':843,'multiline':False]
['text':' Set callback for all future logging events.','line_number':846,'multiline':False]
['text':' If this is not called, or NULL is supplied, everything is output on stderr.','line_number':847,'multiline':False]
['text':' Internal API to be implemented by llama.cpp and used by tests/benchmarks only','line_number':856,'multiline':False]
['text':' LLAMA_API_INTERNAL','line_number':868,'multiline':False]
['text':' LLAMA_H','line_number':870,'multiline':False]
