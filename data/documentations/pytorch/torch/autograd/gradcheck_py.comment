['text':' Note: `get_*_jacobian` functions are added here even though we didn't intend to make them public','line_number':13,'multiline':False]
['text':' since they have been exposed from before we added `__all__`  and we already maintain BC for them','line_number':14,'multiline':False]
['text':' We should eventually deprecate them and remove them from `__all__`','line_number':15,'multiline':False]
['text':' Makes zero-filled tensors from inputs. If `numel_output` is not None, for','line_number':52,'multiline':False]
['text':' each tensor in `input_tensors`, returns a new zero-filled tensor with height','line_number':53,'multiline':False]
['text':' of `t.numel` and width of `numel_output`. Otherwise, for each tensor, returns','line_number':54,'multiline':False]
['text':' a 1-d tensor with size `(t.numel,)`. Each new tensor will be strided and have','line_number':55,'multiline':False]
['text':' the same dtype and device as those of the corresponding input.','line_number':56,'multiline':False]
['text':' Makes zero-filled tensors from outputs. If `dim` is not None, for each tensor','line_number':67,'multiline':False]
['text':' in `output_tensors`, returns a new zero-filled tensor with height of `dim` and','line_number':68,'multiline':False]
['text':' width of `t.numel`. Otherwise, for each tensor, returns a 1-d tensor with size','line_number':69,'multiline':False]
['text':' (t.numel,).','line_number':70,'multiline':False]
['text':' mypy doesn't narrow type of `x` to torch.Tensor','line_number':83,'multiline':False]
['text':' type: ignore[union-attr]','line_number':84,'multiline':False]
['text':' type: ignore[misc]','line_number':85,'multiline':False]
['text':' return a copy of sparse x with all unspecified elements','line_number':92,'multiline':False]
['text':' "replaced" with zero-valued elements','line_number':93,'multiline':False]
['text':' type: ignore[attr-defined] # no attr _mkldnn','line_number':96,'multiline':False]
['text':' We'll use intermediate sparse COO for simplicity','line_number':135,'multiline':False]
['text':' Check that all elements are specified also after `to_sparse` op:','line_number':139,'multiline':False]
['text':' (Only used for slow gradcheck) Returns a generator that yields the following','line_number':154,'multiline':False]
['text':' elements at each iteration:','line_number':155,'multiline':False]
['text':'  1) a tensor: the same tensor is returned across all iterations. The tensor','line_number':156,'multiline':False]
['text':'     is not the same as the original x_tensor as given as input - it is','line_number':157,'multiline':False]
['text':'     prepared so that it can be modified in-place. Depending on whether the','line_number':158,'multiline':False]
['text':'     input tensor is strided, sparse, or dense, the returned tensor may or may','line_number':159,'multiline':False]
['text':'     not share storage with x_tensor.','line_number':160,'multiline':False]
['text':'  2) a tuple of indices that can be used with advanced indexing (yielded in','line_number':161,'multiline':False]
['text':'     dictionary order)','line_number':162,'multiline':False]
['text':'  3) flattened index that will be used to index into the Jacobian tensor','line_number':163,'multiline':False]
['text':'','line_number':164,'multiline':False]
['text':' For a tensor t with size (2, 2), _iter_tensor yields:','line_number':165,'multiline':False]
['text':'     `x, (0, 0), 0`, `x, (0, 1), 1`, `x, (1, 0), 2`, `x, (1, 1), 3`','line_number':166,'multiline':False]
['text':'','line_number':167,'multiline':False]
['text':' where x is the t.data of the original tensor. Perturbing the entry of x','line_number':168,'multiline':False]
['text':' at index (1, 1) yields the 3rd column of the overall Jacobian matrix.','line_number':169,'multiline':False]
['text':' Use .data here to get around the version check','line_number':235,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':243,'multiline':False]
['text':' this is really inefficient, but without indexing implemented, there's','line_number':245,'multiline':False]
['text':' not really a better way than converting back and forth','line_number':246,'multiline':False]
['text':' Use .data here to get around the version check','line_number':250,'multiline':False]
['text':' grad_out param is only kept for backward compatibility reasons','line_number':337,'multiline':False]
['text':' Performs finite differencing by perturbing `entry` in-place by `v` and','line_number':352,'multiline':False]
['text':' returns the gradient of each of the outputs wrt to x at idx.','line_number':353,'multiline':False]
['text':' sparse compressed tensors don't implement sub/add/copy_','line_number':355,'multiline':False]
['text':' yet. However, in non-masked semantics context entry and v','line_number':356,'multiline':False]
['text':' have the same sparse indices ...','line_number':357,'multiline':False]
['text':' ... the finite differencing can be performed on values only:','line_number':360,'multiline':False]
['text':' we'll detach to avoid backward computations that sparse','line_number':363,'multiline':False]
['text':' tensors have limited support for.','line_number':364,'multiline':False]
['text':' Computing the jacobian only works for real delta','line_number':385,'multiline':False]
['text':' For details on the algorithm used here, refer:','line_number':386,'multiline':False]
['text':' Section 3.5.3 https://arxiv.org/pdf/1701.00392.pdf','line_number':387,'multiline':False]
['text':' s = fn(z) where z = x for real valued input','line_number':388,'multiline':False]
['text':' and z = x + yj for complex valued input','line_number':389,'multiline':False]
['text':' C -> R','line_number':393,'multiline':False]
['text':' conjugate wirtinger derivative','line_number':399,'multiline':False]
['text':' R -> R or (R -> C for the forward AD case)','line_number':403,'multiline':False]
['text':' jacobian_cols maps column_idx -> output_idx -> single column of jacobian Tensor','line_number':412,'multiline':False]
['text':' we return a list that maps output_idx -> full jacobian Tensor','line_number':413,'multiline':False]
['text':' Prepares the inputs to be passed into the function while including the new','line_number':426,'multiline':False]
['text':' modified input.','line_number':427,'multiline':False]
['text':' type: ignore[attr-defined] # no attr _mkldnn','line_number':428,'multiline':False]
['text':' Convert back to mkldnn','line_number':429,'multiline':False]
['text':' entry is already a "cloned" version of the original tensor','line_number':436,'multiline':False]
['text':' thus changes to entry are not reflected in the input','line_number':437,'multiline':False]
['text':' We cannot use entry (input.data) if we want gradgrad to work because','line_number':442,'multiline':False]
['text':' fn (in the gradgrad case) needs to compute grad wrt input','line_number':443,'multiline':False]
['text':' Check that the returned outputs don't have different dtype or shape when you','line_number':448,'multiline':False]
['text':' perturb the input','line_number':449,'multiline':False]
['text':' Computes the numerical jacobians wrt to a single input. Returns N jacobian','line_number':466,'multiline':False]
['text':' tensors, where N is the number of outputs. We use a dictionary for','line_number':467,'multiline':False]
['text':' jacobian_cols because indices aren't necessarily consecutive for sparse inputs','line_number':468,'multiline':False]
['text':' When we perturb only a single element of the input tensor at a time, the jvp','line_number':469,'multiline':False]
['text':' is equivalent to a single col of the Jacobian matrix of fn.','line_number':470,'multiline':False]
['text':' To avoid early import issues','line_number':510,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':534,'multiline':False]
['text':' If inp is a differentiable view, the dual might not be the tangent given to','line_number':540,'multiline':False]
['text':' make_dual, so read it explicitly from the dual tensor','line_number':541,'multiline':False]
['text':' Do the full reduction in one pass','line_number':546,'multiline':False]
['text':' To be consistent with numerical evaluation, we actually compute one reduction per input','line_number':547,'multiline':False]
['text':' Remove extra dimension of size 1 corresponding to the reduced input','line_number':561,'multiline':False]
['text':' Reconstruct the full Jacobian column by column','line_number':569,'multiline':False]
['text':' Prepare the input so that it can be modified in-place and do certain','line_number':598,'multiline':False]
['text':' operations that require the tensor to have strides. If fast_mode=False,','line_number':599,'multiline':False]
['text':' _iter_tensor would handle the below cases:','line_number':600,'multiline':False]
['text':' type: ignore[attr-defined] # no attr _mkldnn','line_number':601,'multiline':False]
['text':' Convert to dense so we can perform operations that require strided tensors','line_number':602,'multiline':False]
['text':' Clone because input may require grad, and copy_ calls resize_,','line_number':605,'multiline':False]
['text':' which is not allowed for .data','line_number':606,'multiline':False]
['text':' Wraps `fn` so that its inputs are already supplied','line_number':614,'multiline':False]
['text':' Wraps jvp_fn so that certain arguments are already supplied','line_number':628,'multiline':False]
['text':' We don't need to reshape when input corresponding to u is sparse','line_number':638,'multiline':False]
['text':' Note that all_v can also be None, in that case, this function only computes Ju.','line_number':673,'multiline':False]
['text':' Filter out the Ju for non floating point outputs','line_number':679,'multiline':False]
['text':' TODO: handle the other Ju','line_number':687,'multiline':False]
['text':' Check whether the max difference between two Jacobian tensors are within some','line_number':700,'multiline':False]
['text':' tolerance `atol`.','line_number':701,'multiline':False]
['text':' For the ith tensor in the inner list checks whether it has the same size and','line_number':711,'multiline':False]
['text':' dtype as the ith differentiable input.','line_number':712,'multiline':False]
['text':' This is used by both fast and slow mode:','line_number':756,'multiline':False]
['text':'  - For slow mode, vjps[i][j] is the jth row the Jacobian wrt the ith','line_number':757,'multiline':False]
['text':'    input.','line_number':758,'multiline':False]
['text':'  - For fast mode, vjps[i][0] is a linear combination of the rows','line_number':759,'multiline':False]
['text':'    of the Jacobian wrt the ith input','line_number':760,'multiline':False]
['text':' Compute everything twice to check for nondeterminism (which we call reentrancy)','line_number':768,'multiline':False]
['text':' Why do we need squeeze here? vJ is a 2-d tensor so that we can reuse','line_number':807,'multiline':False]
['text':' the error checking logic from slow mode','line_number':808,'multiline':False]
['text':' C -> R','line_number':810,'multiline':False]
['text':' R -> R','line_number':815,'multiline':False]
['text':' Replicates the behavior of the old get_analytical_jacobian before the refactor','line_number':822,'multiline':False]
['text':' This shares much of its code with _check_analytical_jacobian_attributes','line_number':823,'multiline':False]
['text':' grad_out param is only kept for backward compatibility reasons','line_number':833,'multiline':False]
['text':' Compute everything twice to check for nondeterminism (which we call reentrancy)','line_number':850,'multiline':False]
['text':' Computes the analytical Jacobian in slow mode for a single input-output pair.','line_number':865,'multiline':False]
['text':' Forgoes performing checks on dtype, shape, and reentrancy.','line_number':866,'multiline':False]
['text':' Computes Jacobian row-by-row using backward function `vjp_fn` = v^T J','line_number':876,'multiline':False]
['text':' NB: this function does not assume vjp_fn(v) to return tensors with the same','line_number':877,'multiline':False]
['text':' number of elements for different v. This is checked when we later combine the','line_number':878,'multiline':False]
['text':' rows into a single tensor.','line_number':879,'multiline':False]
['text':' jacobians_rows[i][j] represents the jth row of the ith input','line_number':884,'multiline':False]
['text':' Make sure that gradients are saved for at least one input','line_number':910,'multiline':False]
['text':' TODO: To cover more problematic cases, replace stride = 0 check with','line_number':927,'multiline':False]
['text':' "any overlap in memory" once we have a proper function to check it.','line_number':928,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':929,'multiline':False]
['text':' it is easier to call to_dense() on the sparse output than','line_number':952,'multiline':False]
['text':' to modify analytical jacobian','line_number':953,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':958,'multiline':False]
['text':' When there are no differentiable outputs, numerical gradient for a function is','line_number':968,'multiline':False]
['text':' expected to be zero.','line_number':969,'multiline':False]
['text':' To avoid early import issues (do we need this?)','line_number':1055,'multiline':False]
['text':' Rethrow to provide a better error message','line_number':1097,'multiline':False]
['text':' NB: _test_batched_grad compares two autograd.grad invocations with a single','line_number':1114,'multiline':False]
['text':' vmap(autograd.grad) invocation. It's not exactly a "gradcheck" in the','line_number':1115,'multiline':False]
['text':' sense that we're not comparing an analytical jacobian with a numeric one,','line_number':1116,'multiline':False]
['text':' but it is morally similar (we could have computed a full analytic jac','line_number':1117,'multiline':False]
['text':' via vmap, but that is potentially slow)','line_number':1118,'multiline':False]
['text':' Squash warnings since these are expected to happen in most cases','line_number':1143,'multiline':False]
['text':' NB: this doesn't work for CUDA tests: https://github.com/pytorch/pytorch/issues/50209','line_number':1144,'multiline':False]
['text':' It's OK that we're not raising the error at the correct callsite.','line_number':1151,'multiline':False]
['text':' That's because the callsite is always going to inside the Python','line_number':1152,'multiline':False]
['text':' autograd.grad instead of the C++ traceback of what line in the','line_number':1153,'multiline':False]
['text':' backward formula','line_number':1154,'multiline':False]
['text':' Tests that backward is multiplied by grad_output','line_number':1169,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1236,'multiline':False]
['text':' If inp is a differentiable view, the dual might not be the tangent given to','line_number':1242,'multiline':False]
['text':' make_dual, so read it explicitly from the dual tensor','line_number':1243,'multiline':False]
['text':' case 1 (Materialized Zero Tensor Tangent)','line_number':1256,'multiline':False]
['text':' case 2 (Efficient Zero Tensor Tangent since we don't make a dual object and pass a regular tensor)','line_number':1261,'multiline':False]
['text':' reset','line_number':1266,'multiline':False]
['text':' All backward functions must work properly if all output grads are undefined','line_number':1329,'multiline':False]
['text':' This check filters out Tensor-likes that aren't instances of Tensor.','line_number':1334,'multiline':False]
['text':' If there are multiple output grads, we should be able to undef one at a time without error','line_number':1339,'multiline':False]
['text':' returns list of tuples','line_number':1397,'multiline':False]
['text':' returns new functions real(fn), and imag(fn) where real(fn) and imag(fn) behave the same as','line_number':1402,'multiline':False]
['text':' the original fn, except torch.real or torch.imag are applied to the complex outputs','line_number':1403,'multiline':False]
['text':' returns new functions that take real inputs instead of complex inputs as','line_number':1415,'multiline':False]
['text':' (x, y) -> fn(x + y * 1j). And it computes: inp -> fn(inp + y * 1j) and inp -> fn(x + inp * 1j).','line_number':1416,'multiline':False]
['text':' In each case, the other part is considered constant.','line_number':1417,'multiline':False]
['text':' We do not use 0 for the constant here to make sure we always call the user function with a valid input.','line_number':1418,'multiline':False]
['text':' Note: [numerical vs analytical output length]','line_number':1602,'multiline':False]
['text':' The numerical path returns jacobian quantity for all outputs, even if requires_grad of that','line_number':1603,'multiline':False]
['text':' output is False. This behavior is necessary for _check_no_differentiable_outputs to work.','line_number':1604,'multiline':False]
['text':' Create a random vector with the same number of elements as x and the same','line_number':1657,'multiline':False]
['text':' dtype/device. If x is complex and downcast_complex is False, we create a','line_number':1658,'multiline':False]
['text':' complex tensor with only real component.','line_number':1659,'multiline':False]
['text':' For sparse, create a random sparse vec with random values in the same','line_number':1661,'multiline':False]
['text':' indices. Make sure size is set so that it isn't inferred to be smaller.','line_number':1662,'multiline':False]
['text':' In slow gradcheck, we compare A and B element-wise, i.e., for some a, b we','line_number':1712,'multiline':False]
['text':' allow: |a - b| < atol + rtol * b. But since we now compare q1 = v^T A u and','line_number':1713,'multiline':False]
['text':' q2 = v^T B u, we must allow |q1 - q2| < v^T E u + rtol * v^T B u, where E is','line_number':1714,'multiline':False]
['text':' the correctly sized matrix in which each entry is atol.','line_number':1715,'multiline':False]
['text':'','line_number':1716,'multiline':False]
['text':' We see that atol needs to be scaled by v^T M u (where M is an all-ones M x N','line_number':1717,'multiline':False]
['text':' matrix): v^T M u = \sum_{i} \sum_{j} u_i * v_j = (\sum_{i} u_i)(\sum_{i} v_i)','line_number':1718,'multiline':False]
['text':' TODO: properly handle case when u is tuple instead of only taking first element','line_number':1719,'multiline':False]
['text':' Compute jacobians in slow mode for better error message','line_number':1746,'multiline':False]
['text':' Assume jacobians are non-empty and have the same shape','line_number':1765,'multiline':False]
['text':' Slow gradcheck would've passed!','line_number':1778,'multiline':False]
['text':' Use our own generator to avoid messing with the user's RNG state','line_number':1791,'multiline':False]
['text':' Default allocate all tensors on CPU, so they are on the same device as the generator','line_number':1795,'multiline':False]
['text':' even if the user specified a default device','line_number':1796,'multiline':False]
['text':' Forward AD generates the transpose of what this function expects','line_number':1839,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/53876 for details','line_number':1874,'multiline':False]
['text':' Backward mode computes v^T * J (VJP)','line_number':1876,'multiline':False]
['text':' Since we computed J * u (JVP) through finite difference method, we perform an equality check','line_number':1877,'multiline':False]
['text':' between VJP * u, v * JVP','line_number':1878,'multiline':False]
['text':' ----','line_number':1879,'multiline':False]
['text':' Forward mode computes J * u (JVP)','line_number':1880,'multiline':False]
['text':' Since we already compute JVP through finite difference method,','line_number':1881,'multiline':False]
['text':' we don't need v for correctness check here as asserted below','line_number':1882,'multiline':False]
['text':' TODO: replicate https://github.com/pytorch/pytorch/pull/77743 for fast gradcheck as well','line_number':1901,'multiline':False]
['text':' Note [VarArg of Tensors]','line_number':1940,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1941,'multiline':False]
['text':' 'func' accepts a vararg of tensors, which isn't expressable in the type system at the moment.','line_number':1942,'multiline':False]
['text':' If https://mypy.readthedocs.io/en/latest/additional_features.html?highlight=callable#extended-callable-types is accepted,','line_number':1943,'multiline':False]
['text':' the '...' first argument of Callable can be replaced with VarArg(Tensor).','line_number':1944,'multiline':False]
['text':' For now, we permit any input.','line_number':1945,'multiline':False]
['text':' See Note [VarArg of Tensors]','line_number':1947,'multiline':False]
['text':' noqa: D400,D205','line_number':1963,'multiline':False]
['text':' Short circuit because remaining tests rely on backward AD to be implemented','line_number':2096,'multiline':False]
['text':' See Note [VarArg of Tensors]','line_number':2112,'multiline':False]
['text':' noqa: D400,D205','line_number':2129,'multiline':False]
['text':' TODO: do we want to test this too?','line_number':2193,'multiline':False]
['text':' assert not (check_batched_forward_grad and not check_fwd_over_rev), (','line_number':2194,'multiline':False]
['text':'     "Setting check_batched_forward_grad=True requires check_fwd_over_rev to be True")','line_number':2195,'multiline':False]
['text':' If grad_outputs is not specified, create random Tensors of the same shape, type, and device as the outputs','line_number':2199,'multiline':False]
['text':' NB: We need to save the requires_grad information about the inputs here because gradcheck detaches inputs','line_number':2221,'multiline':False]
['text':'     before running forward mode AD','line_number':2222,'multiline':False]
['text':' Restore the requires_grad information','line_number':2231,'multiline':False]
