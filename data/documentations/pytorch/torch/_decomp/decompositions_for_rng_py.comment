['text':' TODO - We have to register many more distributions here, and also higher level','line_number':27,'multiline':False]
['text':' ops like dropout which have fused implementation and can hide the rand inside.','line_number':28,'multiline':False]
['text':' Only needed because we override get_rng_state.','line_number':97,'multiline':False]
['text':' Only needed because we override set_rng_state.','line_number':102,'multiline':False]
['text':' Tells the tracker to use fwd_state as the running state','line_number':137,'multiline':False]
['text':' Tells the tracker to use bwd_state as the running state','line_number':142,'multiline':False]
['text':' Records the seed and offset tensors. These tensors are used to invoke','line_number':147,'multiline':False]
['text':' the philox_rand functional primitives.','line_number':148,'multiline':False]
['text':' The only reason this exists is because we override get_rng_state and','line_number':158,'multiline':False]
['text':' set_rng_state during tracing. get_rng_state expects a tensor output,','line_number':159,'multiline':False]
['text':' so return (seed, offset) tuple upset other parts of the program like','line_number':160,'multiline':False]
['text':' ctx.saved_tensors.','line_number':161,'multiline':False]
['text':' A bad consequence is that if user saves and restores rng state, we','line_number':163,'multiline':False]
['text':' have little bit of ugliness in the generated code, where we first','line_number':164,'multiline':False]
['text':' concat the (seed, offset) to create a tensor for get_rng_state, and','line_number':165,'multiline':False]
['text':' then split it back to get (seed, offset) tuple in set_rng_state.','line_number':166,'multiline':False]
['text':' TODO: Investigate if there is be a better way to wrap the tuple in a','line_number':168,'multiline':False]
['text':' false Tensor object, and then desugar it later on.','line_number':169,'multiline':False]
['text':' This is only needed because we override set_rng_state. Look at the','line_number':178,'multiline':False]
['text':' comment in get_state_from_tensor method.','line_number':179,'multiline':False]
['text':' torch cuda rng state offset must be a multiple of 4. For inductor, as','line_number':192,'multiline':False]
['text':' we sum up all the numel, the result might not be a multiple of 4. This','line_number':193,'multiline':False]
['text':' method achieves that.','line_number':194,'multiline':False]
['text':' Short circuit if no rand ops were observed','line_number':199,'multiline':False]
['text':' Short circuit if no rand ops were observed','line_number':208,'multiline':False]
['text':' Adding more decompositions which eventually use rand_like inside decomps.','line_number':216,'multiline':False]
['text':' Adding these in rng_decompositions ensures the functionalization of rand_like','line_number':217,'multiline':False]
['text':' ops used in these decomps. The list is copied from inductor codebase, which','line_number':218,'multiline':False]
['text':' uses it for similar purpose.','line_number':219,'multiline':False]
['text':'','line_number':220,'multiline':False]
['text':' Caution - These decomps do not have same accuracy as that of eager. However,','line_number':221,'multiline':False]
['text':' we can't just disable them with a config flag like fallback_random, because','line_number':222,'multiline':False]
['text':' for functionalization of rng ops, we have to decompose these ops.','line_number':223,'multiline':False]
['text':' type: ignore[arg-type]','line_number':263,'multiline':False]
