['text':' type: ignore[attr-defined]','line_number':27,'multiline':False]
['text':' None of these functions are publicly accessible; get at them','line_number':29,'multiline':False]
['text':' from torch._decomps','line_number':30,'multiline':False]
['text':' This wraps a decomposition and performs various type promotion logic within it, depending on the strategy provided','line_number':42,'multiline':False]
['text':' We're currently re-using ELEMENTWISE_TYPE_PROMOTION_KIND, although some of the usages are on non-elementwise ops','line_number':43,'multiline':False]
['text':' Will need to validate the non-elementwise uses','line_number':44,'multiline':False]
['text':' TODO: pretty sure this is not quite right','line_number':59,'multiline':False]
['text':' This expands x until x.dim() == dim. Might be useful as an operator','line_number':94,'multiline':False]
['text':' CPU has a special formula that uses buffer, but disabled for convenience sake','line_number':365,'multiline':False]
['text':' return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output','line_number':366,'multiline':False]
['text':' TODO: None of these loss castings are quite correct, see','line_number':387,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/76870. Also, the ATen kernels','line_number':388,'multiline':False]
['text':' perform the pointwise portion in opmath, but don't maintain it between the','line_number':389,'multiline':False]
['text':' pointwise portion and the reduction','line_number':390,'multiline':False]
['text':' We cannot use @out_wrapper() here, because the output tensor is not named 'out', it's 'grad_input'','line_number':472,'multiline':False]
['text':' We cannot currently model this without introducing data-dependent control flow','line_number':629,'multiline':False]
['text':' TORCH_CHECK(','line_number':630,'multiline':False]
['text':'     (input_val >= 0) && (input_val <= 1),','line_number':631,'multiline':False]
['text':'     "all elements of input should be between 0 and 1"','line_number':632,'multiline':False]
['text':' )','line_number':633,'multiline':False]
['text':' Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1','line_number':723,'multiline':False]
['text':' 2^63 â€“ 1','line_number':741,'multiline':False]
['text':' CPU kernel doesn't respect input_dtype, but following check doesn't work for meta tensor','line_number':807,'multiline':False]
['text':' if grad_output.device == torch.device("cpu"):','line_number':808,'multiline':False]
['text':'     return grad_input.contiguous()','line_number':809,'multiline':False]
['text':' Stride kernel over input and find starting indices along dim d','line_number':834,'multiline':False]
['text':' Apply dilation on kernel and find its indices along dim d','line_number':837,'multiline':False]
['text':' Broadcast and add kernel starting positions (indices) with','line_number':840,'multiline':False]
['text':' kernel_grid along dim d, to get block indices along dim d','line_number':841,'multiline':False]
['text':' Note that F.pad takes (padding_left, padding_right, padding_top, padding_bottom)','line_number':910,'multiline':False]
['text':' ugh','line_number':911,'multiline':False]
['text':' col2im is defined as the backwards of im2col, so we differentiate its decomposition by hand','line_number':1002,'multiline':False]
['text':' According to the CUDA kernel implementation we should have this test;','line_number':1030,'multiline':False]
['text':' but it seems to fail tests!','line_number':1031,'multiline':False]
['text':' torch._check(mask.dtype == torch.bool, lambda: f"Mask should be Bool Scalar Type {mask.dtype}")','line_number':1032,'multiline':False]
['text':' Mimicking CUDA kernel's behavior for output stride: output follow input's memory format','line_number':1034,'multiline':False]
['text':' This different from TensorIterator's behavior','line_number':1035,'multiline':False]
['text':' nb. At the moment this generates two kernels in triton','line_number':1053,'multiline':False]
['text':' It could potentially be fused into one call to scatter_reduce,','line_number':1054,'multiline':False]
['text':' in the case step <= size provided scatter_reduce generates 1 kernel','line_number':1055,'multiline':False]
['text':' eager softmax returns a contiguous tensor. Ensure that decomp also returns','line_number':1112,'multiline':False]
['text':' a contiguous tensor.','line_number':1113,'multiline':False]
['text':' eager log_softmax returns a contiguous tensor. Ensure that decomp also','line_number':1135,'multiline':False]
['text':' returns a contiguous tensor.','line_number':1136,'multiline':False]
['text':' Nb. scale_grad_by_freq is not used in the forward','line_number':1176,'multiline':False]
['text':' We need this one as weight[indices] calls item() in these cases','line_number':1178,'multiline':False]
['text':' type: ignore[assignment]','line_number':1200,'multiline':False]
['text':' Avoid importing sympy at a module level','line_number':1238,'multiline':False]
['text':' We know this is true thanks to the sum, but this assertion helps','line_number':1247,'multiline':False]
['text':' out our internal reasoning','line_number':1248,'multiline':False]
['text':' Avoid importing sympy at a module level','line_number':1276,'multiline':False]
['text':' TODO: this doesn't appear to have enough precision in bfloat16','line_number':1310,'multiline':False]
['text':' The output of aten.addmm is contiguous, we need to match this behavior in the decomposition.','line_number':1322,'multiline':False]
['text':' The original implementation 'beta * self + out' would return a strided tensor if `self` is strided.','line_number':1323,'multiline':False]
['text':' We thus use `out`, the output of torch.mm, which is always contiguous, as the first argument for addition.','line_number':1324,'multiline':False]
['text':' This is relying on TensorIterator's behavior that it takes higher precedence on the stride of first input.','line_number':1325,'multiline':False]
['text':' Alternative, we can write `(beta * self + out).contiguous()`, but it introduces another copy in some cases.','line_number':1326,'multiline':False]
['text':' This implementation is not ideal, and we should revisit this when we have a better solution.','line_number':1327,'multiline':False]
['text':' Compute Internal gradients','line_number':1402,'multiline':False]
['text':' out_wrapper currently does not allow optional outputs','line_number':1452,'multiline':False]
['text':' TODO: Take a closer look at the type promotion semantics','line_number':1488,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1520,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1521,'multiline':False]
['text':' type: ignore[union-attr]','line_number':1528,'multiline':False]
['text':' type: ignore[union-attr]','line_number':1529,'multiline':False]
['text':' out_wrapper currently does not allow optional outputs','line_number':1567,'multiline':False]
['text':' This doesn't strictly match eager's numerics, which accumulates var sum and then directly applies the correction','line_number':1628,'multiline':False]
['text':' But... that would require re-implementing var here, for negligible numerics gain on a tensor whose','line_number':1629,'multiline':False]
['text':' numerics probably don't matter.','line_number':1630,'multiline':False]
['text':' Very annoying inconsistency where CPU and CUDA give different shapes','line_number':1644,'multiline':False]
['text':' TODO: this decomposition is NOT here to stay. We would much prefer replacing native_batch_norm','line_number':1695,'multiline':False]
['text':' with our new correctly schema'd _native_batch_norm_legit and its variants, but','line_number':1696,'multiline':False]
['text':' we cannot do that immediately in the C++ because it would be forwards incompatible','line_number':1697,'multiline':False]
['text':' with some mobile use cases.','line_number':1698,'multiline':False]
['text':'','line_number':1699,'multiline':False]
['text':' Since this change is most impactful for aot autograd/functionalization, we simply','line_number':1700,'multiline':False]
['text':' register this decomposition on the Autograd key for the python dispatcher (which is','line_number':1701,'multiline':False]
['text':' currently only used by aot autograd/functionalization and no one else, really).','line_number':1702,'multiline':False]
['text':' In two weeks or so, we should remove this decomposition and phase out the current native_batch_norm','line_number':1703,'multiline':False]
['text':' to be _native_batch_norm_legit and have the right schema (stating that there are input mutations).','line_number':1704,'multiline':False]
['text':' HACK: batch norm consolidation should clean this up so this op doesn't take in a training arg.','line_number':1732,'multiline':False]
['text':' training','line_number':1770,'multiline':False]
['text':' wrap meta tensor','line_number':1851,'multiline':False]
['text':' avoid conversions on cpu','line_number':1882,'multiline':False]
['text':' In case of dtype promotion, faketensor converted into tensor.','line_number':1890,'multiline':False]
['text':' Need to convert into faketensor if input was a faketensor.','line_number':1891,'multiline':False]
['text':' no ref/prim for memory format','line_number':1894,'multiline':False]
['text':' Questionable decompositions','line_number':1899,'multiline':False]
['text':' This is only valid if we're running the graph without autograd, such as if the backward pass has been traced.','line_number':1900,'multiline':False]
['text':' Note that this decomposition causes issues with in-place ops','line_number':1901,'multiline':False]
['text':' Also register to the Autograd dispatch key, so this decomp can run above autograd.','line_number':1908,'multiline':False]
['text':' native_batch_norm needs to decompose into other ops before autograd.','line_number':1909,'multiline':False]
['text':' Cudnn return running mean and variance when training is True','line_number':1933,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2013,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2015,'multiline':False]
['text':' type: ignore[operator]','line_number':2016,'multiline':False]
['text':' type: ignore[operator]','line_number':2019,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2022,'multiline':False]
['text':' type: ignore[operator]','line_number':2029,'multiline':False]
['text':' "None" doesn't work with vjp, should use zeros for vjp','line_number':2037,'multiline':False]
['text':' "None" doesn't work with vjp, should use zeros for vjp','line_number':2042,'multiline':False]
['text':' out_wrapper currently does not allow optional outputs','line_number':2051,'multiline':False]
['text':' Preconditions','line_number':2121,'multiline':False]
['text':' Optimisation (we should also do this in the kernel implementation)','line_number':2136,'multiline':False]
['text':' Let length = end_index - start_index, i.e. the length of the pooling kernels','line_number':2153,'multiline':False]
['text':' length.max() can be computed analytically as follows:','line_number':2154,'multiline':False]
['text':' adaptive = True iff there are kernels with different lengths','line_number':2157,'multiline':False]
['text':' Need to clamp to avoid accessing out-of-bounds memory','line_number':2167,'multiline':False]
['text':' TODO make minimum accept scalars','line_number':2168,'multiline':False]
['text':' Compute the length','line_number':2174,'multiline':False]
['text':' length is not None if it's constant, otherwise we'll need to compute it','line_number':2181,'multiline':False]
['text':' Shortcut for the simpler case','line_number':2186,'multiline':False]
['text':' zero-out the things we didn't really want to select','line_number':2194,'multiline':False]
['text':' hack','line_number':2196,'multiline':False]
['text':' Compute the length of each window','line_number':2201,'multiline':False]
['text':' We unroll the sum as we assume that the kernels are going to be small','line_number':2212,'multiline':False]
['text':' Treat scalars as elements of \R^1','line_number':2275,'multiline':False]
['text':' Treat scalars as elements of \R^1','line_number':2306,'multiline':False]
['text':' nb: Should use acc_t, not op_math','line_number':2318,'multiline':False]
['text':' aten/src/ATen/native/UpSample.cpp compute_output_size','line_number':2355,'multiline':False]
['text':' NB: this isn't necessary lol','line_number':2366,'multiline':False]
['text':' For each dim in output_size, compute the set of input indices used','line_number':2459,'multiline':False]
['text':' to produce the upsampled output.','line_number':2460,'multiline':False]
['text':' Math matches aten/src/ATen/native/cpu/UpSampleKernel.cpp','line_number':2466,'multiline':False]
['text':'','line_number':2467,'multiline':False]
['text':' Indices are computed as following:','line_number':2468,'multiline':False]
['text':' scale = isize / osize','line_number':2469,'multiline':False]
['text':' Case: exact=False','line_number':2470,'multiline':False]
['text':' input_index = floor(output_index * scale)','line_number':2471,'multiline':False]
['text':' Same as OpenCV INTER_NEAREST','line_number':2472,'multiline':False]
['text':'','line_number':2473,'multiline':False]
['text':' Case: exact=False','line_number':2474,'multiline':False]
['text':' index_f32 = (output_index + 0.5) * scale - 0.5','line_number':2475,'multiline':False]
['text':' input_index = round(index_f32)','line_number':2476,'multiline':False]
['text':' Same as Pillow and Scikit-Image/Scipy ndi.zoom','line_number':2477,'multiline':False]
['text':' convert output to correct memory format, if necessary','line_number':2519,'multiline':False]
['text':' following "heuristic: only use channels_last path when it's faster than the contiguous path"','line_number':2522,'multiline':False]
['text':' don't update cur_hidden','line_number':2664,'multiline':False]
['text':' this will only happen when reverse=False, since batch sizes are sorted largest -> smallest','line_number':2665,'multiline':False]
['text':' third_party/ideep/include/ideep/abstract_types.hpp: ideep::rnn_kind::LSTM = 2','line_number':2741,'multiline':False]
['text':' _rnn_helper already handles bidirectional and batch_first so we hard-code them to False here','line_number':2745,'multiline':False]
['text':' If batch_first, inp has been permuted in _rnn_helper. Convert to contiguous here.','line_number':2750,'multiline':False]
['text':' Same as aten/src/ATen/native/mkldnn/RNN.cpp: mkldnn_rnn: input = input.contiguous();','line_number':2751,'multiline':False]
['text':' this will only happen when reverse=False, since batch sizes are sorted largest -> smallest','line_number':3018,'multiline':False]
['text':' this will only happen when reverse=True','line_number':3028,'multiline':False]
['text':' With autocast, possible to have mixed dtype here','line_number':3081,'multiline':False]
['text':' mkldnn_one_layer_lstm does not depend on seq_len while one_layer_lstm','line_number':3096,'multiline':False]
['text':' will expand over the seq_len dim','line_number':3097,'multiline':False]
['text':' get dimensions of original image','line_number':3295,'multiline':False]
['text':' Calculate horizontal and vertical scaling factor','line_number':3298,'multiline':False]
['text':' We have to create arange with int64 dtype and use .to in order to avoid','line_number':3305,'multiline':False]
['text':' additional kernels creation in inductor and get a perf slowdown','line_number':3306,'multiline':False]
['text':' We are using torch.where instead of torch.clamp below due to an expected failure','line_number':3317,'multiline':False]
['text':' in test_aot_autograd_symbolic_exhaustive_nn_functional_interpolate_bilinear_cpu_float32 test','line_number':3318,'multiline':False]
['text':' torch.ops.aten.clamp.default(add, None, sub) on int64 input tensor is returning float32 and','line_number':3319,'multiline':False]
['text':' fails with torch.ops.aten._unsafe_index.Tensor(primals_1, [None, None, _to_copy_1, clamp_2])','line_number':3320,'multiline':False]
['text':' RuntimeError: _unsafe_index found unexpected index type Float','line_number':3321,'multiline':False]
['text':' xp1 = (x + 1).clamp(max=in_w - 1); yp1 = (y + 1).clamp(max=in_h - 1)','line_number':3322,'multiline':False]
['text':' x1 * (1 - alpha) + x2 * alpha == x1 + (x2 - x1) * alpha','line_number':3341,'multiline':False]
['text':' convert output to correct memory format, if necessary','line_number':3346,'multiline':False]
['text':' following "heuristic: only use channels_last path when it's faster than the contiguous path"','line_number':3349,'multiline':False]
['text':' We should be applying decompositions after all transformations','line_number':3361,'multiline':False]
['text':' self can be [N, C] or [C]','line_number':3385,'multiline':False]
['text':' target can be [N] or []','line_number':3386,'multiline':False]
['text':' target can be [N, 1] or [1]','line_number':3405,'multiline':False]
['text':' noqa: B950','line_number':3454,'multiline':False]
['text':' These are adapted from aten/src/ATen/native/UpSample.h, wich is based on','line_number':3471,'multiline':False]
['text':' https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm','line_number':3472,'multiline':False]
['text':' Need this instead of just sum() to keep mypy happy','line_number':3496,'multiline':False]
['text':' Using padding and summation generates a single kernel vs using torch.stack where 3 kernels generated','line_number':3515,'multiline':False]
['text':' corresponding to each individual tensor: grid_x, grid_y, grid_one','line_number':3516,'multiline':False]
['text':' this is just a temporary hack and we should use torch.stack here once #104480 is merged','line_number':3521,'multiline':False]
['text':' this is just a temporary hack and we should use torch.stack here once #104480 is merged','line_number':3537,'multiline':False]
['text':' base_grid shape is (h, w, 3) and theta shape is (n, 2, 3)','line_number':3548,'multiline':False]
['text':' We do manually a matrix multiplication which is faster than mm()','line_number':3549,'multiline':False]
['text':' (h * w, 3, 1) * (n, 1, 3, 2) -> (n, h * w, 2)','line_number':3550,'multiline':False]
['text':' base_grid shape is (d, h, w, 4) and theta shape is (n, 3, 4)','line_number':3558,'multiline':False]
['text':' We do manually a matrix multiplication which is faster than mm()','line_number':3559,'multiline':False]
['text':' (d * h * w, 4, 1) * (n, 1, 4, 3) -> (n, h * w, 3)','line_number':3560,'multiline':False]
['text':' This method is a copy of grid_sampler_2d implementation and introduced with additional arg _expand_grid to','line_number':3587,'multiline':False]
['text':' optionally expand the input grid for performance reasons.','line_number':3588,'multiline':False]
['text':' Experimenting locally it was found that compiled CUDA code is accelerated by ~5x','line_number':3589,'multiline':False]
['text':' and CPU code by ~2x on bicubic mode, if we expand the grid from (N, H, W, 2) into (N, C, H, W, 2)','line_number':3590,'multiline':False]
['text':' However, this leads to a slowdown around ~0.8x on CPU bilinear mode, channels first.','line_number':3591,'multiline':False]
['text':' Thus we apply this hack to not expand the grid for this case.','line_number':3592,'multiline':False]
['text':' Rescale coordinates from [-1, 1] to:','line_number':3603,'multiline':False]
['text':'   [0, size - 1] if align_corners is True','line_number':3604,'multiline':False]
['text':'   [-.5, size -.5] if align_corners is False','line_number':3605,'multiline':False]
['text':' Reflects coordinates until they fall between low and high (inclusive).','line_number':3610,'multiline':False]
['text':' The bounds are passed as twice their value so that half-integer values','line_number':3611,'multiline':False]
['text':' can be represented as ints.','line_number':3612,'multiline':False]
['text':' Zero','line_number':3626,'multiline':False]
['text':' Borders','line_number':3628,'multiline':False]
['text':' padding_mode == 2, Reflection','line_number':3630,'multiline':False]
['text':' Let's expand grid to [N, C, oH, oW, 2]','line_number':3646,'multiline':False]
['text':' This allows to generate a single triton cuda kernel instead of two kernels.','line_number':3647,'multiline':False]
['text':' Two kernels are due source indices, weights have shape (N, 1, oH, oW), xnumel=N*oH*oW','line_number':3648,'multiline':False]
['text':' and output has shape (N, C, oH, oW), xnumel=N*C*oH*oW','line_number':3649,'multiline':False]
['text':' Expanding grid to (N, C, oH, oW, two) unifies xnumel to N*C*oH*oW','line_number':3650,'multiline':False]
['text':' To clip to inside valid coordinates, we map the coordinates','line_number':3663,'multiline':False]
['text':' to (x, y) = (0, 0) and also set the weight to 0','line_number':3664,'multiline':False]
['text':' We also change the shape of the tensor to the appropriate one for','line_number':3665,'multiline':False]
['text':' broadcasting with N_idx, C_idx for the purposes of advanced indexing','line_number':3666,'multiline':False]
['text':' Perform clipping, index into input tensor and multiply by weight','line_number':3674,'multiline':False]
['text':' Bilinear','line_number':3681,'multiline':False]
['text':' Nearest','line_number':3704,'multiline':False]
['text':' interpolation_mode == 2, Bicubic','line_number':3712,'multiline':False]
['text':' For comments of the logic of this function see eager in /native/LinearAlgebra.cpp','line_number':3804,'multiline':False]
['text':' dim_tensor1 >=3 && (dim_tensor2 == 1 || dim_tensor2 == 2) ||','line_number':3840,'multiline':False]
['text':' dim_tensor2 >=3 && (dim_tensor1 == 1 || dim_tensor1 == 2)','line_number':3841,'multiline':False]
['text':' and some condition on the strides is fulfilled','line_number':3842,'multiline':False]
['text':' optimization: use mm instead of bmm by folding the batch of the larger tensor','line_number':3844,'multiline':False]
['text':' into its leading matrix dimension','line_number':3845,'multiline':False]
['text':' Invariant: t1.dim() >= 3 && (t2.dim() == 1 || t2.dim() == 2)','line_number':3851,'multiline':False]
['text':'            and t1 and t2 are matmul-compatible','line_number':3852,'multiline':False]
['text':' Why not t1.view(-1, sizes_1[-1])?','line_number':3854,'multiline':False]
['text':' If the last dim is 0, then view(-1, 0) won't work because the -1 becomes ambiguous.','line_number':3855,'multiline':False]
['text':' This can happen in e.g. [3, 5, 0] @ [0, 0].','line_number':3856,'multiline':False]
['text':' Readjust output_shape if we are multiplying by a matrix','line_number':3861,'multiline':False]
['text':' This will almost always be a view.','line_number':3866,'multiline':False]
['text':' It may not be a view if t2->requires_grad(). See should_fold in aten/ for an explanation','line_number':3867,'multiline':False]
['text':' This copies if we perform a 2D @ 3D and the first tensor requires_grad','line_number':3870,'multiline':False]
['text':' See should_fold native/LinearAlgebra.cpp for why.','line_number':3871,'multiline':False]
['text':' We are multiplying b1 x n x m1 by x2 x m2 x p (where b1 can be a list);','line_number':3878,'multiline':False]
['text':' we track m1 vs m2 separately even though they must match for nicer error messages','line_number':3879,'multiline':False]
['text':' TODO: handling of slice','line_number':3887,'multiline':False]
['text':' Same optimization for the gradients as that in should_fold','line_number':3891,'multiline':False]
['text':' If we're going to broadcast, we force it to go through the should_fold branch','line_number':3892,'multiline':False]
['text':' expand the batch portion (i.e. cut off matrix dimensions and expand rest)','line_number':3903,'multiline':False]
['text':' HACK: We need reshape with symint support','line_number':3912,'multiline':False]
['text':' convert output to correct memory format, if necessary','line_number':4003,'multiline':False]
['text':' convert output to correct memory format, if necessary','line_number':4093,'multiline':False]
['text':' type: ignore[union-attr]','line_number':4177,'multiline':False]
['text':' type: ignore[union-attr]','line_number':4178,'multiline':False]
['text':' ignores labels after the first -1, detects when -1 is not present','line_number':4218,'multiline':False]
['text':' target indices','line_number':4222,'multiline':False]
['text':' masks target to be able to use gather, which doesn't allow -1','line_number':4224,'multiline':False]
['text':' is_target','line_number':4227,'multiline':False]
['text':' loss','line_number':4230,'multiline':False]
['text':' masks loss','line_number':4234,'multiline':False]
['text':' reduction','line_number':4236,'multiline':False]
['text':' result','line_number':4243,'multiline':False]
['text':' scaled_dot_product_attention used to be decomposed in pre-autograd, given that','line_number':4248,'multiline':False]
['text':' it calls _scaled_dot_product_attention_math and','line_number':4249,'multiline':False]
['text':' _scaled_dot_product_attention_math only has a CompositeImplicitAutograd','line_number':4250,'multiline':False]
['text':' kernel. As a result it's decomposed into ops with finer granularity.','line_number':4251,'multiline':False]
['text':' However recent PRs (#103826 #105131) added new logic in','line_number':4252,'multiline':False]
['text':' scaled_dot_product_attention and now it calls','line_number':4253,'multiline':False]
['text':' _scaled_dot_product_flash_attention which contains a CPU kernel. This results','line_number':4254,'multiline':False]
['text':' in _scaled_dot_product_flash_attention showing up in torch.export().','line_number':4255,'multiline':False]
['text':' This decomposition ensures scaled_dot_product_attention is still decomposed','line_number':4256,'multiline':False]
['text':' the same way as before, i.e., going through','line_number':4257,'multiline':False]
['text':' _scaled_dot_product_attention_math. Notice that this decomp rule should be','line_number':4258,'multiline':False]
['text':' excluded by inductor.','line_number':4259,'multiline':False]
['text':' Why this change?','line_number':4315,'multiline':False]
['text':' In pre-dispatch export scaled_dot_product_attention is executed via','line_number':4316,'multiline':False]
['text':' * flash_attention.','line_number':4317,'multiline':False]
['text':' flash_attention allocates output tensor as (N, L, H, E)','line_number':4318,'multiline':False]
['text':'   it then transposes that to get (N, H, L, E) which is supposed to be the return','line_number':4319,'multiline':False]
['text':' tensor dim for scaled_dot_product_attention','line_number':4320,'multiline':False]
['text':' assume x: [N, H, L, E] is the output sdpa','line_number':4321,'multiline':False]
['text':' In MHA code, this output is then permuted via (2, 0, 1, 3) to get','line_number':4322,'multiline':False]
['text':' (L, N, H, E) dim tensor','line_number':4323,'multiline':False]
['text':' x = x.permute(2, 0, 1, 3).contiguous() and the viewed via','line_number':4324,'multiline':False]
['text':' x = x.view(L * N, H * E)','line_number':4325,'multiline':False]
['text':' During pre autograd dispatch call to contiguous is not traced because','line_number':4326,'multiline':False]
['text':' flash_attention output after the x.permute is already contiguous','line_number':4327,'multiline':False]
['text':' on which the view is valid','line_number':4328,'multiline':False]
['text':' However, during 2nd stage export, post-dispatch, we run _match variant','line_number':4329,'multiline':False]
['text':' instead of flash* to get the decomposition. _match variant returns','line_number':4330,'multiline':False]
['text':' x: [N, H, L, E] applying x.permute(2, 0, 1, 3) returns','line_number':4331,'multiline':False]
['text':' x: [L, N, H, E] and without converting this to contiguous tensor','line_number':4332,'multiline':False]
['text':' subsequent view is not valid and the export fails','line_number':4333,'multiline':False]
['text':' solution is to maintain the return tensor view from the decomp to be','line_number':4334,'multiline':False]
['text':' exactly same as *flash* variant.','line_number':4335,'multiline':False]
['text':' flash variants output is contiguous as [N, L, H, E]','line_number':4336,'multiline':False]
['text':' _match variant out is contiguous as [N, H, L, E]','line_number':4337,'multiline':False]
['text':' out = out.transpose(1, 2).contiguous gets output as contiguous','line_number':4338,'multiline':False]
['text':' in [N, L, H, E].','line_number':4339,'multiline':False]
['text':' Subsrequent transpose(1, 2) then returns a view on which','line_number':4340,'multiline':False]
['text':' aforementioned code snippet, as showm below, is valid','line_number':4341,'multiline':False]
['text':' x = x.permute(2, 0, 1, 3).contiguous() and the viewed via','line_number':4342,'multiline':False]
['text':' x = x.view(L * N, H * E)','line_number':4343,'multiline':False]
['text':' Really the invariant you want to maintain is:','line_number':4345,'multiline':False]
['text':' pre-dispatch op-output and its decomposed representation must','line_number':4346,'multiline':False]
['text':' return tensor with same view and dims','line_number':4347,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/852f8526c52190125446adc9a6ecbcc28fb66182/aten/src/ATen/native/WeightNorm.cpp#L58','line_number':4417,'multiline':False]
