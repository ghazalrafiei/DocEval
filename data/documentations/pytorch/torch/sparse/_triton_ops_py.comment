['text':' Triton loads only blocks which are at least 16 and powers of 2.','line_number':68,'multiline':False]
['text':' TODO: investigate if contiguity along other axes than the','line_number':91,'multiline':False]
['text':' last one can be beneficial for performance','line_number':92,'multiline':False]
['text':' grid_points are iterated in a "contiguous" order, i.e.','line_number':144,'multiline':False]
['text':' left dimensions traversed slower than right dimensions.','line_number':145,'multiline':False]
['text':' This order is reversed for CUDA grids.','line_number':146,'multiline':False]
['text':' cuda_max_grid = (2 ** 31 - 1, 2 ** 16 - 1, 2 ** 16 - 1)','line_number':151,'multiline':False]
['text':' grid must be at least 1 and no greater than mg','line_number':161,'multiline':False]
['text':' type: ignore[assignment]','line_number':166,'multiline':False]
['text':' Introduce fake batch dimension if not present for convenience.','line_number':173,'multiline':False]
['text':' Compute broadcasted batch dimension','line_number':179,'multiline':False]
['text':' Broadcast batch dimensions and squash.','line_number':182,'multiline':False]
['text':' The result can be either a view or a copy.','line_number':183,'multiline':False]
['text':' NOTE: this function will ALWAYS create a view','line_number':216,'multiline':False]
['text':' using .view instead of .reshape to ensure that the result is','line_number':225,'multiline':False]
['text':' indeed a view:','line_number':226,'multiline':False]
['text':' The following parameters are optimized for the performance','line_number':467,'multiline':False]
['text':' equilibrium points of bsr-dense and dense-dense matrix','line_number':468,'multiline':False]
['text':' multiplications when using GPU card NVIDIA GeForce RTX 2060','line_number':469,'multiline':False]
['text':' SUPER. For points far from the performance equilibrium','line_number':470,'multiline':False]
['text':' points as well as for other GPU cards, the optimal','line_number':471,'multiline':False]
['text':' parameters are likely different from what specified below.','line_number':472,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':475,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':477,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':479,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':481,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':484,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':486,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':488,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':490,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':493,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':495,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':497,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':499,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':501,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':504,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':506,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':508,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':510,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':512,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':515,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':517,'multiline':False]
['text':' noqa: E225,E231,E702','line_number':519,'multiline':False]
['text':' Assume NVIDIA GeForce RTX 2060 SUPER:','line_number':522,'multiline':False]
['text':' With the probality of 92% (99.9% when N > 512), the','line_number':523,'multiline':False]
['text':' performance will not be worse more than 2% from the','line_number':524,'multiline':False]
['text':' performance when using an optimal value.  Otherwise, when N','line_number':525,'multiline':False]
['text':' <= 512, using the following heuristics may give upto 15%','line_number':526,'multiline':False]
['text':' lower performance.','line_number':527,'multiline':False]
['text':' Warning: TensorAsKey does not track negative nor','line_number':608,'multiline':False]
['text':' conjugate bits of its input object because in the use','line_number':609,'multiline':False]
['text':' case of wrapping compressed/plain indices of compressed','line_number':610,'multiline':False]
['text':' sparse tensors (that are always integer tensors with','line_number':611,'multiline':False]
['text':' non-negative items) these bits are never set. However,','line_number':612,'multiline':False]
['text':' when extending the use of TensorAsKey to float or','line_number':613,'multiline':False]
['text':' complex tensors, the values of these bits (see is_neg','line_number':614,'multiline':False]
['text':' and is_conj methods) must be included in the key as','line_number':615,'multiline':False]
['text':' well.','line_number':616,'multiline':False]
['text':' dead objects always compare unequal unless these are','line_number':638,'multiline':False]
['text':' same objects','line_number':639,'multiline':False]
['text':' swizzle operation: mm elements with longer sums are computed first:','line_number':673,'multiline':False]
['text':' todo: eliminate inner for-loops for efficiency','line_number':705,'multiline':False]
['text':' no batch dims','line_number':730,'multiline':False]
['text':' equivalent to .transpose(-3, -2).transpose(-2, -1).transpose(-4, -3)','line_number':789,'multiline':False]
['text':' equivalent to .transpose(-4, -3).transpose(-2, -1).transpose(-3, -2)','line_number':795,'multiline':False]
['text':' todo: implement checks','line_number':824,'multiline':False]
['text':' prepare_inputs has made a copy of out, copy its content back','line_number':898,'multiline':False]
['text':' to out_backup:','line_number':899,'multiline':False]
['text':' Compute nnz for the row with number row_block_pid.','line_number':955,'multiline':False]
['text':' If it is zero, skip the row.','line_number':956,'multiline':False]
['text':' Pointers are set to the first block of the current row.','line_number':964,'multiline':False]
['text':' Advance mat1 to the current tiled row, ignore columns.','line_number':979,'multiline':False]
['text':' Advance mat2 in batch and block col dimension.','line_number':987,'multiline':False]
['text':' find column block index','line_number':998,'multiline':False]
['text':' write result','line_number':1025,'multiline':False]
['text':' advance val/col_index ptrs to the next block in the row.','line_number':1028,'multiline':False]
['text':' values prologue','line_number':1034,'multiline':False]
['text':' values epilogue','line_number':1040,'multiline':False]
['text':' crow_indices prologue','line_number':1041,'multiline':False]
['text':' crow_indices epilogue','line_number':1045,'multiline':False]
['text':' col_indices prologue','line_number':1046,'multiline':False]
['text':' col_indices epilogue','line_number':1050,'multiline':False]
['text':' dense prologue','line_number':1051,'multiline':False]
['text':' dense epilogue','line_number':1058,'multiline':False]
['text':' output prologue','line_number':1059,'multiline':False]
['text':' output epilogue','line_number':1066,'multiline':False]
['text':'','line_number':1067,'multiline':False]
['text':' gh-113754: Always keep all constexpr arguments at the end of','line_number':1068,'multiline':False]
['text':' triton kernel arguments list because with triton 2.1 or','line_number':1069,'multiline':False]
['text':' earlier non-contiguous outputs will corrupt CUDA state due','line_number':1070,'multiline':False]
['text':' to a triton bug (fixed in openai/triton#2262).','line_number':1071,'multiline':False]
['text':' Compute nnz for the row with number row_block_pid.','line_number':1096,'multiline':False]
['text':' If it is zero, skip the row.','line_number':1097,'multiline':False]
['text':' Pointers are set to the first block of the current row.','line_number':1105,'multiline':False]
['text':' NOTE: dense is advanced into all dimensions but the tiled row one.','line_number':1114,'multiline':False]
['text':' That will be advanced in the loop according to values in col_indices.','line_number':1115,'multiline':False]
['text':' Pointers are set to exact write-to locations','line_number':1124,'multiline':False]
['text':' Set pointer to the first nonzero element in the current row','line_number':1134,'multiline':False]
['text':' find which row of dense needs to get loaded','line_number':1145,'multiline':False]
['text':' for multiplication with values_block.','line_number':1146,'multiline':False]
['text':' do block mm','line_number':1150,'multiline':False]
['text':' move val/col_index ptrs to the next block in the row','line_number':1153,'multiline':False]
['text':' write back the result','line_number':1157,'multiline':False]
['text':' NOTE: (m, 0) @ (0, n) == zeros(m, n)','line_number':1259,'multiline':False]
['text':' prepare inputs by reshaping them to be kernel-compatible','line_number':1264,'multiline':False]
['text':' If nnz x block strides are not the same in out_backup.values and values,','line_number':1280,'multiline':False]
['text':' it means that out_backup.values and values are not the views of each other,','line_number':1281,'multiline':False]
['text':' so we have to copy.','line_number':1282,'multiline':False]
['text':' Allocate out','line_number':1331,'multiline':False]
['text':' Short circuit if lhs is zero','line_number':1335,'multiline':False]
['text':' with beta==0, addmm ignores input content, so we can use out','line_number':1339,'multiline':False]
['text':' as a placeholder for input because their shapes match:','line_number':1340,'multiline':False]
['text':' Compute nnz for the row with number row_block_pid.','line_number':1369,'multiline':False]
['text':' If it is zero, skip the row.','line_number':1370,'multiline':False]
['text':' find max in the row','line_number':1385,'multiline':False]
['text':' find denominator for stable softmax','line_number':1395,'multiline':False]
['text':' populate output','line_number':1405,'multiline':False]
['text':' reshape values from','line_number':1434,'multiline':False]
['text':' (b1, ..., bn, nnz, row_block, col_block) to','line_number':1435,'multiline':False]
['text':' (b1 * ... * bn, row_block, nnz * col_block).','line_number':1436,'multiline':False]
['text':' This simplifies batch dim manipulation and unlocks','line_number':1437,'multiline':False]
['text':' the possibility to access all nnzs in any given row.','line_number':1438,'multiline':False]
['text':' Need to clone to avoid `contiguous` returning a view.','line_number':1440,'multiline':False]
['text':' We span nnz number of blocks, not nnz + 1,','line_number':1448,'multiline':False]
['text':' hence crow_indices[..., :-1]','line_number':1449,'multiline':False]
['text':' Triton's max numel is bounded by 2 ** 17.','line_number':1459,'multiline':False]
['text':' When is_compressed is True, r is the only variable that','line_number':1643,'multiline':False]
['text':' depends on pid_t. This property allows sorting r values','line_number':1644,'multiline':False]
['text':' before calling the kernel. The sorting of r is equivalent to','line_number':1645,'multiline':False]
['text':' defining swizzle operator outside of the kernel.','line_number':1646,'multiline':False]
['text':' Re non-contiguous tensor arguments. Sometimes triton kernel','line_number':1722,'multiline':False]
['text':' launches may fail with','line_number':1723,'multiline':False]
['text':'','line_number':1724,'multiline':False]
['text':'   RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered','line_number':1725,'multiline':False]
['text':'','line_number':1726,'multiline':False]
['text':' that appears to be case when the size of a non-contiguous','line_number':1727,'multiline':False]
['text':' tensor argument is larger than a certain threshold. Could','line_number':1728,'multiline':False]
['text':' this be related to shared memory or L1 cache size of a GPU','line_number':1729,'multiline':False]
['text':' card? In anycase, ensuring that tensor arguments are','line_number':1730,'multiline':False]
['text':' contiguous seems to avoid the above exception. So, in the','line_number':1731,'multiline':False]
['text':' following we'll always convert tensor arguments to','line_number':1732,'multiline':False]
['text':' C-contiguous tensors.','line_number':1733,'multiline':False]
['text':' values prologue','line_number':1763,'multiline':False]
['text':' values epilogue','line_number':1769,'multiline':False]
['text':' crow_indices prologue','line_number':1770,'multiline':False]
['text':' crow_indices epilogue','line_number':1774,'multiline':False]
['text':' col_indices prologue','line_number':1775,'multiline':False]
['text':' col_indices epilogue','line_number':1779,'multiline':False]
['text':' input prologue','line_number':1780,'multiline':False]
['text':' input epilogue','line_number':1787,'multiline':False]
['text':' dense prologue','line_number':1788,'multiline':False]
['text':' dense epilogue','line_number':1795,'multiline':False]
['text':' output prologue','line_number':1796,'multiline':False]
['text':' output epilogue','line_number':1803,'multiline':False]
['text':' Compute nnz for the row with number row_block_pid.','line_number':1836,'multiline':False]
['text':' Pointers are set to exact write-to locations','line_number':1844,'multiline':False]
['text':' Pointers are set to the first block of the current row.','line_number':1854,'multiline':False]
['text':' NOTE: dense is advanced into all dimensions but the tiled row one.','line_number':1863,'multiline':False]
['text':' That will be advanced in the loop according to values in col_indices.','line_number':1864,'multiline':False]
['text':' Pointers are set to exact write-to locations','line_number':1873,'multiline':False]
['text':' Set pointer to the first nonzero element in the current row','line_number':1883,'multiline':False]
['text':' alpha is never 0','line_number':1890,'multiline':False]
['text':' find which row of dense needs to get loaded','line_number':1902,'multiline':False]
['text':' for multiplication with values_block.','line_number':1903,'multiline':False]
['text':' do block mm','line_number':1907,'multiline':False]
['text':' move val/col_index ptrs to the next block in the row','line_number':1910,'multiline':False]
['text':' write back the result','line_number':1917,'multiline':False]
['text':' type: ignore[assignment]','line_number':1922,'multiline':False]
['text':' type: ignore[assignment]','line_number':1923,'multiline':False]
['text':' type: ignore[assignment]','line_number':1924,'multiline':False]
['text':' type: ignore[assignment]','line_number':1925,'multiline':False]
['text':' type: ignore[assignment]','line_number':1926,'multiline':False]
['text':' type: ignore[assignment]','line_number':1927,'multiline':False]
['text':' type: ignore[assignment]','line_number':1928,'multiline':False]
