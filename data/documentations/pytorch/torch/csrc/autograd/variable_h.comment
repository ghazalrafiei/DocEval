['text':'/ `Variable` is exactly the same as `Tensor` (i.e. we have `using Variable =','line_number':27,'multiline':False]
['text':'/ at::Tensor`). This means you can perform all the usual mathematical and','line_number':28,'multiline':False]
['text':'/ other operations you can perform on `Tensor`s also on `Variable`s.','line_number':29,'multiline':False]
['text':'/','line_number':30,'multiline':False]
['text':'/ The only reason we are keeping the `Variable` class is backward','line_number':31,'multiline':False]
['text':'/ compatibility with external user's legacy C++ frontend code. Our intention','line_number':32,'multiline':False]
['text':'/ is to eliminate the `Variable` class in the near future.','line_number':33,'multiline':False]
['text':' namespace autograd','line_number':36,'multiline':False]
['text':' namespace torch','line_number':37,'multiline':False]
['text':' The following are all internal APIs and should not be shown in libtorch docs.','line_number':39,'multiline':False]
['text':' Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS','line_number':40,'multiline':False]
['text':' ... #endif`','line_number':41,'multiline':False]
['text':'/ Check if this type is supported by the autograd engine.','line_number':48,'multiline':False]
['text':'/ If you change this, update the doc at the top of the','line_number':49,'multiline':False]
['text':'/ torch/autograd/__init__.py file and','line_number':50,'multiline':False]
['text':'/ "test_set_requires_grad_only_for_continuous_types" in test/test_autograd.py','line_number':51,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':58,'multiline':False]
['text':'/                                Variable','line_number':59,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':60,'multiline':False]
['text':'/ A `Variable` augments a `Tensor` with the ability to interact in our','line_number':61,'multiline':False]
['text':'/ autograd machinery. Conceptually, `Variable`s travel along `Edge`s between','line_number':62,'multiline':False]
['text':'/ `Node`s in the autograd graph. A `Variable` can either be a leaf, like a','line_number':63,'multiline':False]
['text':'/ weight in a neural network, or an interior variable, when it is the result','line_number':64,'multiline':False]
['text':'/ of an operation between variables. Every `Variable` also stores another','line_number':65,'multiline':False]
['text':'/ `Variable` called its `grad` (gradient). If the variable is a leaf, its','line_number':66,'multiline':False]
['text':'/ gradient will be accumulated into this variable.','line_number':67,'multiline':False]
['text':'/','line_number':68,'multiline':False]
['text':'/ Every Tensor is a Variable, but sometimes we colloquially refer to Variables','line_number':69,'multiline':False]
['text':'/ that don't require gradients as Tensors (since none of the autograd','line_number':70,'multiline':False]
['text':'/ machinery for Variables applies).  Historically, Variables and Tensors','line_number':71,'multiline':False]
['text':'/ were separate concepts, but now they are exactly the same (i.e. we have','line_number':72,'multiline':False]
['text':'/ `using Variable = at::Tensor`).','line_number':73,'multiline':False]
['text':'/','line_number':74,'multiline':False]
['text':'/                              Gradient Edges','line_number':75,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':76,'multiline':False]
['text':'/ Furthermore, `Variable`s have the notion of a `gradient_edge`, which is the','line_number':77,'multiline':False]
['text':'/ edge in the autograd graph that connects the variable to a particular input','line_number':78,'multiline':False]
['text':'/ of the gradient function that will be invoked with the variable during the','line_number':79,'multiline':False]
['text':'/ backward pass. More precisely, this gradient function can be one of two','line_number':80,'multiline':False]
['text':'/ things:','line_number':81,'multiline':False]
['text':'/ 1. A `grad_fn`, if the variable is in the interior of the graph. This is the','line_number':82,'multiline':False]
['text':'/    gradient of the function that produced the variable.','line_number':83,'multiline':False]
['text':'/ 2. A `grad_accumulator`, if the variable is a leaf, which accumulates a','line_number':84,'multiline':False]
['text':'/    scalar gradient value into its `grad` variable.','line_number':85,'multiline':False]
['text':'/','line_number':86,'multiline':False]
['text':'/                               Versioning','line_number':87,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':88,'multiline':False]
['text':'/ Another major feature of `Variable`s are *versions*. Versions are','line_number':89,'multiline':False]
['text':'/ incremented when an in-place mutation of a variable occurs. Versions are','line_number':90,'multiline':False]
['text':'/ useful when constructing `SavedVariable`s, which take a snapshot of a','line_number':91,'multiline':False]
['text':'/ `Variable` at a certain version. You can retrieve a `Variable`'s version','line_number':92,'multiline':False]
['text':'/ through its `current_version()` method.','line_number':93,'multiline':False]
['text':'/','line_number':94,'multiline':False]
['text':'/                                 Views','line_number':95,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':96,'multiline':False]
['text':'/ It is possible for a  `Variable` to be a *view* of another `Variable`, in','line_number':97,'multiline':False]
['text':'/ which case it tracks that `Variable`'s data and autograd history. Beyond','line_number':98,'multiline':False]
['text':'/ construction, the interface of a view is identical to that of a regular','line_number':99,'multiline':False]
['text':'/ `Variable`. You can determine whether `Variable` is in fact a view by','line_number':100,'multiline':False]
['text':'/ probing its `is_view()` method. Note that the *view* semantics are only','line_number':101,'multiline':False]
['text':'/ meaningful for `Variable` relations that are relevant to autograd.','line_number':102,'multiline':False]
['text':'/ See NOTE [ Autograd View Variables ] for more details.','line_number':103,'multiline':False]
['text':'/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':104,'multiline':False]
['text':' Private-ish functions for manipulating variables; we don't want to put them','line_number':109,'multiline':False]
['text':' on Tensor proper','line_number':110,'multiline':False]
['text':' WARNING: This may return a nullptr.  If you require AutogradMeta to return','line_number':113,'multiline':False]
['text':' a materialized structure, use materialize_autograd_meta instead.','line_number':114,'multiline':False]
['text':' WARNING: This will return a nullptr if the Tensor is not a view.','line_number':117,'multiline':False]
['text':' Returns the current autograd meta, materializing it if it was previously','line_number':120,'multiline':False]
['text':' none.  This counts as a *mutating* operation, so do not call it on','line_number':121,'multiline':False]
['text':' "read-only" operators; in particular, this is NOT thread safe','line_number':122,'multiline':False]
['text':'/ Set the gradient accumulator of the `Variable`. This is only applicable to','line_number':125,'multiline':False]
['text':'/ leaf variables. Interior variables should call `set_gradient_edge()`.','line_number':126,'multiline':False]
['text':'/ Attempts to get a pointer to the gradient accumulator of the `Variable`,','line_number':131,'multiline':False]
['text':'/ if it still exists. If the gradient accumulator function has been','line_number':132,'multiline':False]
['text':'/ destroyed, returns a `nullptr`.','line_number':133,'multiline':False]
['text':'/ Gets the gradient accumulator of the `Variable` if it has one, or else','line_number':136,'multiline':False]
['text':'/ create one on the fly and return it.','line_number':137,'multiline':False]
['text':'/ Returns the "canonical" gradient edge of this `Variable`, i.e. either the','line_number':140,'multiline':False]
['text':'/ gradient function if this is an interior `Variable`, or the gradient','line_number':141,'multiline':False]
['text':'/ accumulator otherwise. If the `Variable` is interior, the returned `Edge`','line_number':142,'multiline':False]
['text':'/ will store the input index of the `Node` to which this variable is','line_number':143,'multiline':False]
['text':'/ connected in its `input_nr` field. For leaves, the `input_nr` is always','line_number':144,'multiline':False]
['text':'/ zero. Note that `set_gradient_edge` and `gradient_edge` are not','line_number':145,'multiline':False]
['text':'/ symmetric. You must use `set_gradient_edge` to set the `grad_fn` and','line_number':146,'multiline':False]
['text':'/ `set_grad_accumulator` to set the accumulator.','line_number':147,'multiline':False]
['text':'/ Set the gradient edge -- i.e. `grad_fn` and `input_nr` -- of the','line_number':150,'multiline':False]
['text':'/ `Variable`.','line_number':151,'multiline':False]
['text':'/ NOTE: This will always set the `grad_fn`, even if this is a leaf variable,','line_number':152,'multiline':False]
['text':'/ and never the `grad_accumulator`. For the latter, use','line_number':153,'multiline':False]
['text':'/ `set_grad_accumulator`. This allows late construction of an interior','line_number':154,'multiline':False]
['text':'/ `Variable`.','line_number':155,'multiline':False]
['text':' Autograd Graph Interaction','line_number':158,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':159,'multiline':False]
['text':'/ Update the `grad_fn` of an existing Variable. Called after in-place','line_number':161,'multiline':False]
['text':'/ modifications.','line_number':162,'multiline':False]
['text':'/','line_number':163,'multiline':False]
['text':'/ For View Variables:','line_number':164,'multiline':False]
['text':'/ Called after in-place modifications. Modifies the grad_fn of the base','line_number':165,'multiline':False]
['text':'/ Variable.','line_number':166,'multiline':False]
['text':'/ Gets the raw gradient function pointer, whatever it currently is.','line_number':169,'multiline':False]
['text':'/ Increments the version count of this `Variable`.','line_number':172,'multiline':False]
['text':'/ Retrieves this `Variable`s version counter.','line_number':178,'multiline':False]
['text':' namespace impl','line_number':198,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':200,'multiline':False]
['text':'                            AutogradMeta','line_number':201,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':202,'multiline':False]
['text':'/ Each `Variable` has one unique `AutogradMeta` struct, which stores autograd','line_number':204,'multiline':False]
['text':'/ metadata fields that are necessary for tracking the Variable's autograd','line_number':205,'multiline':False]
['text':'/ history. As an optimization, a Variable may store a nullptr, in lieu of a','line_number':206,'multiline':False]
['text':'/ default constructed AutogradMeta.','line_number':207,'multiline':False]
['text':' This field is used to store all the forward AD gradients','line_number':216,'multiline':False]
['text':' associated with this AutogradMeta (and the Tensor it corresponds to)','line_number':217,'multiline':False]
['text':' There is a semantic 1:1 correspondence between AutogradMeta and','line_number':218,'multiline':False]
['text':' ForwardGrad but:','line_number':219,'multiline':False]
['text':'   - This field is lazily populated.','line_number':220,'multiline':False]
['text':'   - This field is a shared_ptr but it must never be','line_number':221,'multiline':False]
['text':'     shared by multiple Tensors. See Note [ Using ForwardGrad ]','line_number':222,'multiline':False]
['text':' Any transition from not_initialized to initialized','line_number':223,'multiline':False]
['text':' must be protected by mutex_','line_number':224,'multiline':False]
['text':' The hooks_ field is actually reused by both python and cpp logic','line_number':227,'multiline':False]
['text':' For both cases, we have a data structure, cpp_hooks_list_ (cpp)','line_number':228,'multiline':False]
['text':' or dict (python) which is the canonical copy.','line_number':229,'multiline':False]
['text':' Then, for both cases, we always register a single hook to','line_number':230,'multiline':False]
['text':' hooks_ which wraps all the hooks in the list/dict.','line_number':231,'multiline':False]
['text':' And, again in both cases, if the grad_fn exists on that tensor','line_number':232,'multiline':False]
['text':' we will additionally register a single hook to the grad_fn.','line_number':233,'multiline':False]
['text':'','line_number':234,'multiline':False]
['text':' Note that the cpp and python use cases aren't actually aware of','line_number':235,'multiline':False]
['text':' each other, so using both is not defined behavior.','line_number':236,'multiline':False]
['text':' The post_acc_grad_hooks_ field stores only Python hooks','line_number':240,'multiline':False]
['text':' (PyFunctionTensorPostAccGradHooks) that are called after the','line_number':241,'multiline':False]
['text':' .grad field has been accumulated into. This is less complicated','line_number':242,'multiline':False]
['text':' than the hooks_ field, which encapsulates a lot more.','line_number':243,'multiline':False]
['text':' Only meaningful on leaf variables (must be false otherwise)','line_number':246,'multiline':False]
['text':' Only meaningful on non-leaf variables (must be false otherwise)','line_number':249,'multiline':False]
['text':' The "output number" of this variable; e.g., if this variable','line_number':254,'multiline':False]
['text':' was the second output of a function, then output_nr == 1.','line_number':255,'multiline':False]
['text':' We use this to make sure we can setup the backwards trace','line_number':256,'multiline':False]
['text':' correctly when this variable is passed to another function.','line_number':257,'multiline':False]
['text':' Mutex to ensure that concurrent read operations that modify internal','line_number':260,'multiline':False]
['text':' state are still thread-safe. Used by grad_fn(), grad_accumulator(),','line_number':261,'multiline':False]
['text':' fw_grad() and set_fw_grad()','line_number':262,'multiline':False]
['text':' This is mutable because we need to be able to acquire this from const','line_number':263,'multiline':False]
['text':' version of this class for the functions above','line_number':264,'multiline':False]
['text':'/ Sets the `requires_grad` property of `Variable`. This should be true for','line_number':267,'multiline':False]
['text':'/ leaf variables that want to accumulate gradients, and false for all other','line_number':268,'multiline':False]
['text':'/ variables.','line_number':269,'multiline':False]
['text':'/ Accesses the gradient `Variable` of this `Variable`.','line_number':282,'multiline':False]
['text':' set_requires_grad also checks error conditions.','line_number':307,'multiline':False]
['text':' If AutogradMeta is being destroyed, it means that there is no other','line_number':318,'multiline':False]
['text':' reference to its corresponding Tensor. It implies that no other thread','line_number':319,'multiline':False]
['text':' can be using this object and so there is no need to lock mutex_ here to','line_number':320,'multiline':False]
['text':' guard the check if fw_grad_ is populated.','line_number':321,'multiline':False]
['text':' See note [ Using ForwardGrad ]','line_number':323,'multiline':False]
['text':'/ The base `Variable`','line_number':330,'multiline':False]
['text':'/ If this ViewInfo represents a forward (respectively backward) AD gradient,','line_number':331,'multiline':False]
['text':'/ then this Tensor cannot be a forward (respectively backward) view.','line_number':332,'multiline':False]
['text':'/ By default we use as_strided to recover views which is more efficient.','line_number':335,'multiline':False]
['text':'/ view_fn is only saved when as_strided is not supported.','line_number':336,'multiline':False]
['text':'/ If view_fn has value, we use it to recover views in backward.','line_number':337,'multiline':False]
['text':'/ Accessors for the view function','line_number':340,'multiline':False]
['text':'/ The chain function can be used to build a new ViewInfo for a','line_number':351,'multiline':False]
['text':'/ differentiable view function. It will return a new view info that','line_number':352,'multiline':False]
['text':'/ accurately represents how "tensor" is a view of this instance's "base_".','line_number':353,'multiline':False]
['text':'/ The "base" and "tensor" are respectively the input and output of the','line_number':354,'multiline':False]
['text':'/ differentiable view function that happened. They are required to properly','line_number':355,'multiline':False]
['text':'/ set the optional view_fn_ when it is not provided. The "view_func", if','line_number':356,'multiline':False]
['text':'/ provided, should be a function that allows to re-do the view between','line_number':357,'multiline':False]
['text':'/ "base" and "tensor".','line_number':358,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':370,'multiline':False]
['text':'                     DifferentiableViewMeta','line_number':371,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':372,'multiline':False]
['text':'/ NOTE [ Autograd View Variables ]','line_number':374,'multiline':False]
['text':'/','line_number':375,'multiline':False]
['text':'/ Many operations return Variable that shares storage with an input Variable.','line_number':376,'multiline':False]
['text':'/ The returned Variable is called a **view** Variable on the input **base**','line_number':377,'multiline':False]
['text':'/ Variable.','line_number':378,'multiline':False]
['text':'/','line_number':379,'multiline':False]
['text':'/ In PyTorch, we have two types of views: differentiable views, and','line_number':380,'multiline':False]
['text':'/ non-differentiable views. In either type, to support proper version','line_number':381,'multiline':False]
['text':'/ checking, the base and view Variables must always share the same','line_number':382,'multiline':False]
['text':'/ version_counter.','line_number':383,'multiline':False]
['text':'/','line_number':384,'multiline':False]
['text':'/','line_number':385,'multiline':False]
['text':'/ Differentiable Views','line_number':386,'multiline':False]
['text':'/ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':387,'multiline':False]
['text':'/ This class allows to track both forward and backward AD differentiable','line_number':388,'multiline':False]
['text':'/ views. These views can have different base as non-differentiable view for','line_number':389,'multiline':False]
['text':'/ forward and backward mode AD are not the same.','line_number':390,'multiline':False]
['text':'/','line_number':391,'multiline':False]
['text':'/ Most function are either both forward and backward differentiable views (for','line_number':392,'multiline':False]
['text':'/ example: view, select, narrow, transpose, etc) or both not forward and not','line_number':393,'multiline':False]
['text':'/ backward differentiable views (for example: indices, values, eq, lt, etc).','line_number':394,'multiline':False]
['text':'/ But there are also functions that are forward but not backward','line_number':395,'multiline':False]
['text':'/ differentiable views (only detach for now) or functions that are backward','line_number':396,'multiline':False]
['text':'/ but not forward differentiable view (only make_dual and unpack dual for','line_number':397,'multiline':False]
['text':'/ now).','line_number':398,'multiline':False]
['text':'/','line_number':399,'multiline':False]
['text':'/ A concrete example of two views with different bases is as follow:','line_number':400,'multiline':False]
['text':'/','line_number':401,'multiline':False]
['text':'/     # Have:','line_number':402,'multiline':False]
['text':'/     #   dual is a dual Tensor that is neither a forward or backward view','line_number':403,'multiline':False]
['text':'/     detached_dual = dual.detach()','line_number':404,'multiline':False]
['text':'/     view = detached_dual.view_as(dual)','line_number':405,'multiline':False]
['text':'/     # The forward base of view is dual','line_number':406,'multiline':False]
['text':'/     # The backward base of view is detached_dual','line_number':407,'multiline':False]
['text':'/','line_number':408,'multiline':False]
['text':'/ - Backward Mode View','line_number':409,'multiline':False]
['text':'/ Differentiable views are the view variables where you want gradients to flow','line_number':410,'multiline':False]
['text':'/ back to the base variables. Out-of-place operations on views are quite','line_number':411,'multiline':False]
['text':'/ straightforward, but in-place ones are very tricky. Even if the base','line_number':412,'multiline':False]
['text':'/ variable may not require grad when we create the view, we still need to','line_number':413,'multiline':False]
['text':'/ track the view relation because future in-place ops may require back-proping','line_number':414,'multiline':False]
['text':'/ through it. For example, we need to support','line_number':415,'multiline':False]
['text':'/','line_number':416,'multiline':False]
['text':'/   (1) in-place operation on view, e.g.,','line_number':417,'multiline':False]
['text':'/','line_number':418,'multiline':False]
['text':'/     # Have:','line_number':419,'multiline':False]
['text':'/     #   base.requires_grad = False','line_number':420,'multiline':False]
['text':'/     #   var.requires_grad = True','line_number':421,'multiline':False]
['text':'/     base[1] = var  # i.e., base[1].copy_(var)','line_number':422,'multiline':False]
['text':'/     torch.autograd.grad(base.sum(), var)  <- should return an all ones','line_number':423,'multiline':False]
['text':'/     tensor','line_number':424,'multiline':False]
['text':'/','line_number':425,'multiline':False]
['text':'/   (2) in-place operation on base after view is created, e.g.,','line_number':426,'multiline':False]
['text':'/','line_number':427,'multiline':False]
['text':'/     # Have:','line_number':428,'multiline':False]
['text':'/     #   base.requires_grad = False','line_number':429,'multiline':False]
['text':'/     #   var.requires_grad = True','line_number':430,'multiline':False]
['text':'/     view = base[1]','line_number':431,'multiline':False]
['text':'/     base.copy_(var)','line_number':432,'multiline':False]
['text':'/     torch.autograd.grad(view.sum(), var)  <- should return a tensor with','line_number':433,'multiline':False]
['text':'/                                              var[1] filled with all ones and','line_number':434,'multiline':False]
['text':'/                                              zeros everywhere else','line_number':435,'multiline':False]
['text':'/','line_number':436,'multiline':False]
['text':'/ - Forward Mode View','line_number':437,'multiline':False]
['text':'/ Forward differentiable views follow the same semantic as backward ones but','line_number':438,'multiline':False]
['text':'/ show up differently as they are computed along with the forward evaluation.','line_number':439,'multiline':False]
['text':'/ The hard examples above are thus very similar','line_number':440,'multiline':False]
['text':'/','line_number':441,'multiline':False]
['text':'/   (1) in-place operation on view, e.g.,','line_number':442,'multiline':False]
['text':'/','line_number':443,'multiline':False]
['text':'/     # Have:','line_number':444,'multiline':False]
['text':'/     #   base is a regular Tensor','line_number':445,'multiline':False]
['text':'/     #   var is a dual Tensor whose tangent is all ones','line_number':446,'multiline':False]
['text':'/     base[1] = var  # i.e., base[1].copy_(var)','line_number':447,'multiline':False]
['text':'/     # Now, base is a dual Tensor','line_number':448,'multiline':False]
['text':'/     _, fw_grad = fwAD.unpack_dual(base) <- fw_grad should be a tensor with','line_number':449,'multiline':False]
['text':'/                                              fw_grad[1] filled with all ones','line_number':450,'multiline':False]
['text':'/                                              and zeros everywhere else','line_number':451,'multiline':False]
['text':'/','line_number':452,'multiline':False]
['text':'/   (2) in-place operation on base after view is created, e.g.,','line_number':453,'multiline':False]
['text':'/','line_number':454,'multiline':False]
['text':'/     # Have:','line_number':455,'multiline':False]
['text':'/     #   base is a regular Tensor','line_number':456,'multiline':False]
['text':'/     #   var is a dual Tensor whose tangent is all ones','line_number':457,'multiline':False]
['text':'/     view = base[1]','line_number':458,'multiline':False]
['text':'/     base.copy_(var)','line_number':459,'multiline':False]
['text':'/     _, fw_grad = fwAD.unpack_dual(view) <- fw_grad should be an all ones','line_number':460,'multiline':False]
['text':'/     tensor','line_number':461,'multiline':False]
['text':'/','line_number':462,'multiline':False]
['text':'/ See Note [Forward Grad View/inplace] for more details on how we handle these','line_number':463,'multiline':False]
['text':'/ hard cases.','line_number':464,'multiline':False]
['text':'/','line_number':465,'multiline':False]
['text':'/','line_number':466,'multiline':False]
['text':'/ DifferentiableViewMeta is created to support gradient tracking of','line_number':467,'multiline':False]
['text':'/ such **in-place** operations. In particular,','line_number':468,'multiline':False]
['text':'/   + if an in-place op is done on base, the grad_fn field of the view may','line_number':469,'multiline':False]
['text':'/     become stale. So accesses should always go through grad_fn(), which','line_number':470,'multiline':False]
['text':'/     reconstructs an updated grad_fn if the version_counter has incremented.','line_number':471,'multiline':False]
['text':'/     All other fields are always valid.','line_number':472,'multiline':False]
['text':'/   + if an in-place op is done on view, in rebase_history() of view, which is','line_number':473,'multiline':False]
['text':'/     called after every in-place op in VariableType.cpp, the grad_fn of base','line_number':474,'multiline':False]
['text':'/     is updated.','line_number':475,'multiline':False]
['text':'/   + if a single autograd Node returns multiple differentiable views, if any','line_number':476,'multiline':False]
['text':'/     output is modified by an inplace operation, the autograd engine will','line_number':477,'multiline':False]
['text':'/     make an equivalent graph (corresponding to the view operations) without','line_number':478,'multiline':False]
['text':'/     using equivalent graph, where each output is treated as if it were','line_number':479,'multiline':False]
['text':'/     produced by a distinct view operation. This discards the original (e.g.,','line_number':480,'multiline':False]
['text':'/     user provided) grad_fn. If the provided grad_fn does more than the','line_number':481,'multiline':False]
['text':'/     backward of the view, then the DifferentiableViewMeta must be created','line_number':482,'multiline':False]
['text':'/     with creation_meta= CreationMeta::MULTI_OUTPUT_NODE to prevent the','line_number':483,'multiline':False]
['text':'/     engine from ignoring the provided grad_fn.','line_number':484,'multiline':False]
['text':'/','line_number':485,'multiline':False]
['text':'/ Interaction with GradMode:','line_number':486,'multiline':False]
['text':'/ The particular case that we consider here is:','line_number':487,'multiline':False]
['text':'/','line_number':488,'multiline':False]
['text':'/     # Have:','line_number':489,'multiline':False]
['text':'/     #   base.requires_grad = True or False','line_number':490,'multiline':False]
['text':'/     with torch.no_grad():','line_number':491,'multiline':False]
['text':'/         view = base[1]','line_number':492,'multiline':False]
['text':'/     base.requires_grad_()','line_number':493,'multiline':False]
['text':'/     view.copy_(var)','line_number':494,'multiline':False]
['text':'/     torch.autograd.grad(base.sum(), var)  <- what should it return?','line_number':495,'multiline':False]
['text':'/','line_number':496,'multiline':False]
['text':'/ Given that this particular code example is ambiguous and can easily be','line_number':497,'multiline':False]
['text':'/ replace by either moving both inside the no_grad block or both outside, we','line_number':498,'multiline':False]
['text':'/ explicitly forbid it. For now, it is deprecated by a warning. This is','line_number':499,'multiline':False]
['text':'/ achieved by setting creation_meta=CreationMeta::NO_GRAD_MODE for all','line_number':500,'multiline':False]
['text':'/ differentiable views created in no_grad mode.','line_number':501,'multiline':False]
['text':'/','line_number':502,'multiline':False]
['text':'/ See Note [View + Inplace update for base tensor]','line_number':503,'multiline':False]
['text':'/ and Note [View + Inplace update for view tensor] for the details how','line_number':504,'multiline':False]
['text':'/ autograd handles inplace update with view ops.','line_number':505,'multiline':False]
['text':'/','line_number':506,'multiline':False]
['text':'/ Non-Differentiable Views','line_number':507,'multiline':False]
['text':'/ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':508,'multiline':False]
['text':'/ In certain cases, although function outputs share storage with inputs, they','line_number':509,'multiline':False]
['text':'/ will **never** require gradient history tracking. Instead of registering the','line_number':510,'multiline':False]
['text':'/ view relation via DifferentiableViewMeta in autograd, the views will be','line_number':511,'multiline':False]
['text':'/ using usual AutogradMeta and just share the version counters with the base','line_number':512,'multiline':False]
['text':'/ Variables.','line_number':513,'multiline':False]
['text':'/ Such views include:','line_number':514,'multiline':False]
['text':'/   1. Views created from .detach()','line_number':515,'multiline':False]
['text':'/   2. Views that are non-differentiable by its nature.','line_number':516,'multiline':False]
['text':'/      E.g., `sparse_tensor.indices()` is a integral view on a (possibly)','line_number':517,'multiline':False]
['text':'/      floating point tensor.','line_number':518,'multiline':False]
['text':'/      See top of `derivatives.yaml` on how to specify that outputs of a','line_number':519,'multiline':False]
['text':'/      function are non-differentiable.','line_number':520,'multiline':False]
['text':'/ These are called non-differentiable views as the gradients do not flow','line_number':521,'multiline':False]
['text':'/ through the view relation.','line_number':522,'multiline':False]
['text':'/','line_number':523,'multiline':False]
['text':'/ Relevant logic for both differentiable and non-differentiable views is','line_number':524,'multiline':False]
['text':'/ implemented in make_variable_(non_)differentiable_view below, and','line_number':525,'multiline':False]
['text':'/ wrap_output of gen_variable_type.py.','line_number':526,'multiline':False]
['text':'/ NOTE [ View + Inplace detection ]','line_number':528,'multiline':False]
['text':'/','line_number':529,'multiline':False]
['text':'/ We want to detect views followed by inplace as they are often forbidden to','line_number':530,'multiline':False]
['text':'/ ensure correctness of the computed gradients. But since we want to only','line_number':531,'multiline':False]
['text':'/ notify the user when both happen, we tag the DifferentiableViewMeta when the','line_number':532,'multiline':False]
['text':'/ view is created via the `make_variable_*_view()` functions. This tag is then','line_number':533,'multiline':False]
['text':'/ checked by the `check_inplace()` function from `VariableTypeUtils.h` that','line_number':534,'multiline':False]
['text':'/ should be called before every inplace operation and to detect cases where','line_number':535,'multiline':False]
['text':'/ other views are modified and this one is rebased by side effect, we also','line_number':536,'multiline':False]
['text':'/ check in the `VariableHooks::grad_fn()`.','line_number':537,'multiline':False]
['text':'/ Flag that gives more information about when this view was created:','line_number':539,'multiline':False]
['text':'/ - IN_CUSTOM_FUNCTION should be set when the view is created inside a custom','line_number':540,'multiline':False]
['text':'/   autograd Function is returned.','line_number':541,'multiline':False]
['text':'/ - NO_GRAD_MODE should be set when a view in created when GradMode is','line_number':542,'multiline':False]
['text':'/ disabled','line_number':543,'multiline':False]
['text':'/ - MULTI_OUTPUT_NODE should be set when a Node created by codegen code','line_number':544,'multiline':False]
['text':'/ returns','line_number':545,'multiline':False]
['text':'/   multiple differentiable views','line_number':546,'multiline':False]
['text':'/ - Inference_MODE should be set when a view of normal tensor is created in','line_number':547,'multiline':False]
['text':'/ InferenceMode.','line_number':548,'multiline':False]
['text':'/ - DEFAULT is for all other cases','line_number':549,'multiline':False]
['text':'/ Handles correctly propagating CreationMeta when a new view is created from a','line_number':558,'multiline':False]
['text':'/ previous view. In general, we don't want the new view to be _less_','line_number':559,'multiline':False]
['text':'/ restrictive than the previous view (it's okay to be _more_ restrictive). A','line_number':560,'multiline':False]
['text':'/ CreationMeta value of DEFAULT is currently the least restrictive, as the','line_number':561,'multiline':False]
['text':'/ behavior for all other CreationMeta values is to error out for in-place ops.','line_number':562,'multiline':False]
['text':'/ A CreationMeta value of INFERENCE_MODE is currently the most restrictive, so','line_number':563,'multiline':False]
['text':'/ it takes precedence in propagation. If this changes, the logic here will','line_number':564,'multiline':False]
['text':'/ need to be updated to properly handle the new semantics.','line_number':565,'multiline':False]
['text':'/ Unified function to handle error checking when rebase happens','line_number':576,'multiline':False]
['text':'/ indirect=true means that the caller is not doing the inplace, but the','line_number':577,'multiline':False]
['text':'/ inplace happened somewhere else.','line_number':578,'multiline':False]
['text':'/ Informations about the views','line_number':585,'multiline':False]
['text':' Optimization to reduce the number of ViewInfo we create.','line_number':589,'multiline':False]
['text':' In the (very common) case where backward_info_ == forward_info_, we only','line_number':590,'multiline':False]
['text':' populate backward_info_ (that should be used as both the forward and','line_number':591,'multiline':False]
['text':' backward view information) and set shared_view_info_ = true. Invariants:','line_number':592,'multiline':False]
['text':'   - If shared_view_info_ is false, there is no special constraints on','line_number':593,'multiline':False]
['text':'     backward_info_ and forward_info_','line_number':594,'multiline':False]
['text':'   - If shared_view_info_ is true, we must have:','line_number':595,'multiline':False]
['text':'      - backward_info_.has_value() == true','line_number':596,'multiline':False]
['text':'      - forward_info_.has_value() == false','line_number':597,'multiline':False]
['text':'/ The two following fields are extra information that we track to ensure','line_number':600,'multiline':False]
['text':'/ that any operation on this backward view is valid.','line_number':601,'multiline':False]
['text':'/ The value of the version_counter at the time grad_fn was created. The','line_number':603,'multiline':False]
['text':'/ grad_fn field is stale if attr_version_ !=','line_number':604,'multiline':False]
['text':'/ version_counter.current_version().','line_number':605,'multiline':False]
['text':'/ requires_grad is a backward AD field so we only use the view specific','line_number':610,'multiline':False]
['text':'/ logic for backward differentiable views','line_number':611,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':676,'multiline':False]
['text':'                        Variable Implementation','line_number':677,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':678,'multiline':False]
['text':' Factory Functions','line_number':680,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':681,'multiline':False]
['text':'/ Creates a `Variable` that is a *view* of another (*base*) variable.','line_number':683,'multiline':False]
['text':'/ The `gradient_edge` is an optional (gradient_function, input_number) pair.','line_number':684,'multiline':False]
['text':'/ `is_differentiable` is a bool that specifies whether this view is','line_number':685,'multiline':False]
['text':'/ differentiable, i.e., whether the relation should be tracked by autograd.','line_number':686,'multiline':False]
['text':'/ See NOTE [ Autograd View Variables ] for details.','line_number':687,'multiline':False]
['text':'/ NOTE: `allow_tensor_metadata_change` is set to true by default, because','line_number':689,'multiline':False]
['text':'/ there are a lot of call sites to these factory functions that need to change','line_number':690,'multiline':False]
['text':'/ the variable's size or storage afterwards, and they don't expect the','line_number':691,'multiline':False]
['text':'/ original tensor (where the variable is created from) to be updated. Setting','line_number':692,'multiline':False]
['text':'/ `allow_tensor_metadata_change_` to false by default would unnecessarily','line_number':693,'multiline':False]
['text':'/ prevent those changes from happening and is undesirable.','line_number':694,'multiline':False]
['text':' See NOTE [ Autograd View Variables ] for details.','line_number':696,'multiline':False]
['text':' Differentiable view. Track history with DifferentiableViewMeta.','line_number':697,'multiline':False]
['text':' See NOTE [ Autograd View Variables ] for details.','line_number':730,'multiline':False]
['text':' Non-differentiable view. Just share version counter.','line_number':731,'multiline':False]
['text':' Currently all of non-differentiable view ops(detach/_indices/_values)','line_number':737,'multiline':False]
['text':' share the same TensorImpl as their base Tensor. Thus a new TensorImpl','line_number':738,'multiline':False]
['text':' allocation here is required.','line_number':739,'multiline':False]
['text':'version_counter=','line_number':741,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':742,'multiline':True]
['text':'/ Creates a `Variable` from the given `Tensor`, copying its underlying','line_number':749,'multiline':False]
['text':'/ `TensorImpl`. `requires_grad` should be set only for leaves, and determines','line_number':750,'multiline':False]
['text':'/ whether the `Variable` will accumulate gradients. NOTE: `data` must *not* be','line_number':751,'multiline':False]
['text':'/ a `Variable` already. Its dynamic type *must* be `Tensor`.','line_number':752,'multiline':False]
['text':'/','line_number':753,'multiline':False]
['text':'/ TODO: Eliminate this function as much as possible, as it can be expressed','line_number':754,'multiline':False]
['text':'/ more clearly as detach() or a no-op in most call sites (especially when','line_number':755,'multiline':False]
['text':'/ there is only one use of the variable).','line_number':756,'multiline':False]
['text':'version_counter=','line_number':775,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':776,'multiline':True]
['text':'/ Creates a `Variable` from the given `Tensor`, copying its underlying','line_number':789,'multiline':False]
['text':'/ `TensorImpl`. `gradient_edge` should be a (function, input_nr) pair','line_number':790,'multiline':False]
['text':'/ specifying the function in the autograd graph, and what particular input of','line_number':791,'multiline':False]
['text':'/ that function, this variable is connected to.','line_number':792,'multiline':False]
['text':'version_counter=','line_number':799,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':800,'multiline':True]
['text':' namespace utils','line_number':846,'multiline':False]
['text':' namespace autograd','line_number':847,'multiline':False]
['text':' namespace torch','line_number':848,'multiline':False]
['text':' DOXYGEN_SHOULD_SKIP_THIS ','line_number':850,'multiline':True]
