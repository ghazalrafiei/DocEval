['text':' namespace torch','line_number':23,'multiline':False]
['text':' A Function which is implemented by a Python object (i.e., a THPFunction).','line_number':27,'multiline':False]
['text':' Calls to 'apply' are forwarded to the Python method implementation.','line_number':28,'multiline':False]
['text':' THPFunction this Function is wrapping.  Owning!','line_number':43,'multiline':False]
['text':' Can't use THPObjectPtr as a field in this class; destructor won't take','line_number':47,'multiline':False]
['text':' out GIL!  When I forgot to do this by hand','line_number':48,'multiline':False]
['text':' TestAutograd.test_inplace_view_python called me out about it.','line_number':49,'multiline':False]
['text':' If python is already dead, leak the wrapped python objects','line_number':50,'multiline':False]
['text':'*
 * Cast an object into a tuple, if it is not a tuple already. Returns true
 * if the original object was not a tuple.
 ','line_number':58,'multiline':True]
['text':' namespace autograd','line_number':74,'multiline':False]
['text':' namespace torch','line_number':75,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':77,'multiline':False]
['text':' Python tuple of tensors whose variables we should save.  Set','line_number':83,'multiline':False]
['text':' by Python with 'save_for_backward'.  If nullptr, no tensors were','line_number':84,'multiline':False]
['text':' saved.','line_number':85,'multiline':False]
['text':' Python tuple of tensors which are not differentiable.  Set by','line_number':87,'multiline':False]
['text':' Python with 'mark_non_differentiable'.  If nullptr, no tensors were','line_number':88,'multiline':False]
['text':' non-differentiable.','line_number':89,'multiline':False]
['text':' Python tuple of tensors which had inplace updates in the forward()','line_number':91,'multiline':False]
['text':' pass.  Set by Python with 'mark_dirty'.  If nullptr, no tensors were','line_number':92,'multiline':False]
['text':' modified inplace.','line_number':93,'multiline':False]
['text':' boolean indicating whether to materialize undefined output grad tensors','line_number':96,'multiline':False]
['text':' into tensors full of zeros. Set by Python with 'set_materialize_grads'.','line_number':97,'multiline':False]
['text':' Default is true.','line_number':98,'multiline':False]
['text':' boolean indicating whether to materialize output grad tensors','line_number':101,'multiline':False]
['text':' corresponding to non-differentiable outputs. Normally, someone would','line_number':102,'multiline':False]
['text':' already get this behavior by switching off materialize_grads,','line_number':103,'multiline':False]
['text':' but there are certain use cases where that is not feasible:','line_number':104,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/98659#pullrequestreview-1376822560','line_number':105,'multiline':False]
['text':' This is enabled by compiled autograd as a way to signal to AotAutograd it','line_number':108,'multiline':False]
['text':' should call the original FX graph rather than compiling.','line_number':109,'multiline':False]
['text':' For each input, true if the input is a THPVariable','line_number':116,'multiline':False]
['text':' The actual PyNode (in the autograd graph) that this data was','line_number':121,'multiline':False]
['text':' saved for.  This field may be NULL (because a user can construct','line_number':122,'multiline':False]
['text':' a THPFunction directly from Python), but when this field is non-NULL,','line_number':123,'multiline':False]
['text':' it is guaranteed that cdata.lock()->obj == this','line_number':124,'multiline':False]
['text':'','line_number':125,'multiline':False]
['text':' In most ordinary use, this field should always be non-NULL; e.g.,','line_number':126,'multiline':False]
['text':' when we allocate a THPFunction because we are running Node.apply,','line_number':127,'multiline':False]
['text':' after constructing a THPFunction, we immediately allocate a PyNode','line_number':128,'multiline':False]
['text':' for it.  We can't enforce this directly in the constructor of','line_number':129,'multiline':False]
['text':' THPFunction though, because there's no way to keep it live long enough','line_number':130,'multiline':False]
['text':' to save an owning reference to PyNode into the grad_fn of a Variable.','line_number':131,'multiline':False]
