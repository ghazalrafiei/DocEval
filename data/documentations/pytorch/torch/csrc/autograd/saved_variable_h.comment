['text':'/ A snapshot of a variable at a certain version. A `SavedVariable` stores','line_number':20,'multiline':False]
['text':'/ enough information to reconstruct a variable from a certain point in time.','line_number':21,'multiline':False]
['text':' See note [ Using ForwardGrad ]','line_number':37,'multiline':False]
['text':'/ Reconstructs the saved variable. Pass `saved_for` as the gradient','line_number':42,'multiline':False]
['text':'/ function if constructing the `SavedVariable` with it would have caused a','line_number':43,'multiline':False]
['text':'/ circular reference.','line_number':44,'multiline':False]
['text':' This field contains either:','line_number':56,'multiline':False]
['text':' 1. the variable to save','line_number':57,'multiline':False]
['text':' 2. or its tensor_data.','line_number':58,'multiline':False]
['text':' If storing the variable itself would create a circular reference,','line_number':59,'multiline':False]
['text':' we fall into the second case and its metadata is also saved separately.','line_number':60,'multiline':False]
['text':' In that case, the grad_fn must be passed in to the unpack function when','line_number':61,'multiline':False]
['text':' reconstructing the Variable (except when we are doing an inplace operation','line_number':62,'multiline':False]
['text':' on a view, see below). The field saved_original_ below reflects the two','line_number':63,'multiline':False]
['text':' cases: its value is true in the first case and false in the second case.','line_number':64,'multiline':False]
['text':' The value data_.defined() can be false in three cases:','line_number':65,'multiline':False]
['text':' 1. SavedVariable was constructed without a Tensor (the value to save is','line_number':66,'multiline':False]
['text':' None), in that case was_default_constructed_ will be kept at true','line_number':67,'multiline':False]
['text':' 2. The saved variable has been released by calling','line_number':68,'multiline':False]
['text':' SavedVariable::reset_data(), typically during the backward pass','line_number':69,'multiline':False]
['text':' 3. Hooks have been registered. In that case, hooks_ will be defined','line_number':70,'multiline':False]
['text':' instead. Note that the value of saved_original_ only reflects what happened','line_number':71,'multiline':False]
['text':' during the construction of the SavedVariable. If saved_original_ is true,','line_number':72,'multiline':False]
['text':' we saved the original tensor in data_, but if the user registers hooks, we','line_number':73,'multiline':False]
['text':' will no longer have it (despite the saved_original_ still being true)','line_number':74,'multiline':False]
['text':' This field is used to store the forward AD gradients associated with','line_number':77,'multiline':False]
['text':' the saved Tensor. Note that this shared_ptr must never be shared with','line_number':78,'multiline':False]
['text':' either the saved Tensor or the unpacked Tensor. See note [ Using','line_number':79,'multiline':False]
['text':' ForwardGrad ]','line_number':80,'multiline':False]
['text':' Weak version of grad_fn_ that prevents leaks in rebase_history() for','line_number':83,'multiline':False]
['text':' inplace views.','line_number':84,'multiline':False]
['text':' This variable is used when the user chooses to create a SavedVariable with','line_number':85,'multiline':False]
['text':' is_inplace_on_view = true.','line_number':86,'multiline':False]
['text':' In that case, the grad_fn passed in to the unpack function at unwrapping','line_number':87,'multiline':False]
['text':' time is unused.','line_number':88,'multiline':False]
['text':' Hooks are a pair of functions pack_hook/unpack_hook that provides','line_number':100,'multiline':False]
['text':' fine-grained control over how the SavedVariable should save its data.','line_number':101,'multiline':False]
['text':' pack_hook is called upon registration, while unpack_hook is called when','line_number':102,'multiline':False]
['text':' unpacking.','line_number':103,'multiline':False]
['text':' Fields grad_fn_, grad_accumulator_, and requires_grad_ are only used if','line_number':105,'multiline':False]
['text':' hooks are defined. They are set before pack_hook is called and used after','line_number':106,'multiline':False]
['text':' unpack_hook is called.','line_number':107,'multiline':False]
['text':' For the usual case where leaf tensors are the input, we expect its','line_number':109,'multiline':False]
['text':' grad_acc to be kept alive by the graph. The reason SavedVariable holds','line_number':110,'multiline':False]
['text':' a owning reference is to support the case where a custom autograd Function','line_number':111,'multiline':False]
['text':' saves an intermediate.','line_number':112,'multiline':False]
['text':' namespace autograd','line_number':122,'multiline':False]
['text':' namespace torch','line_number':123,'multiline':False]
