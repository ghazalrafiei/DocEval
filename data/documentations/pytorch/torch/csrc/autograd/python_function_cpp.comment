['text':' Anonymous namespace for helpful functions used in this file','line_number':56,'multiline':False]
['text':' TODO: We shouldn't need to call this function because the engine','line_number':59,'multiline':False]
['text':' can already persist the errors for us. This still seems to be','line_number':60,'multiline':False]
['text':' needed for the DistEngine however.','line_number':61,'multiline':False]
['text':'','line_number':62,'multiline':False]
['text':' python test/distributed/rpc/test_tensorpipe_agent.py -k','line_number':63,'multiline':False]
['text':' test_backward_autograd_engine_error','line_number':64,'multiline':False]
['text':'','line_number':65,'multiline':False]
['text':' See Note [ Persisting PyErr state across autograd engine threads ]','line_number':66,'multiline':False]
['text':' namespace','line_number':73,'multiline':False]
['text':' NOTE: this function is written in a way that assumes it's only called for','line_number':78,'multiline':False]
['text':' backward; it's used by engine.cpp.  This is responsible for forwarding a call','line_number':79,'multiline':False]
['text':' from C++'s Node::apply to a Python method "apply".','line_number':80,'multiline':False]
['text':' Massage a C++ variable_list into a Python arguments tuple','line_number':86,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':93,'multiline':False]
['text':' Returning too many results is ok, but only as long as they're all None.','line_number':124,'multiline':False]
['text':' Truncate the result tuple in that case.','line_number':125,'multiline':False]
['text':' Now the number of gradients should match','line_number':139,'multiline':False]
['text':' Massage the Python results tuple back into a C++ variable_list','line_number':148,'multiline':False]
['text':' This function is called as part of the Node destructor!','line_number':193,'multiline':False]
['text':' Since this object might be kept alive by C++, it is possible','line_number':194,'multiline':False]
['text':' that the python interpreter is already dead here. In that case','line_number':195,'multiline':False]
['text':' we just leak the saved objects.','line_number':196,'multiline':False]
['text':' first value is unique ID of the AotAutograd graph','line_number':223,'multiline':False]
['text':' AotAutograd symints are all dynamic','line_number':242,'multiline':False]
['text':' namespace autograd','line_number':280,'multiline':False]
['text':' namespace torch','line_number':281,'multiline':False]
['text':' Traverse and clear are required for supporting Python's GC cycle handling.','line_number':283,'multiline':False]
['text':' NB: We should not traverse PyObbject stored on PyNode, since we only hold','line_number':285,'multiline':False]
['text':' as weak reference to the PyNode.','line_number':286,'multiline':False]
['text':' Note that the cdata might not be expired yet in the case where this','line_number':295,'multiline':False]
['text':' object is part of a cycle and the GC happens to tp_clear this PyObject','line_number':296,'multiline':False]
['text':' before the other ones that trigger the de-allocation of the cdata','line_number':297,'multiline':False]
['text':' Why is this guaranteed to be true?  Suppose that self->cdata is non-null','line_number':315,'multiline':False]
['text':' (otherwise the condition is trivially true).  Then there is a PyNode','line_number':316,'multiline':False]
['text':' which contains an owning reference to this object.  But we are only','line_number':317,'multiline':False]
['text':' allowed to clear if all owning references are gone!  Contradiction.','line_number':318,'multiline':False]
['text':'','line_number':319,'multiline':False]
['text':' However, note that THPFunction_clear is typically called in the shared_ptr','line_number':320,'multiline':False]
['text':' destructor of PyNode; in that case, per','line_number':321,'multiline':False]
['text':' https://cplusplus.github.io/LWG/lwg-active.html#2751 it's not currently','line_number':322,'multiline':False]
['text':' specified in the standard that this is guaranteed.  If you see this','line_number':323,'multiline':False]
['text':' assert triggering in the wild, feel free to comment it out.  They're','line_number':324,'multiline':False]
['text':' likely to standardize that you ARE guaranteed to see the weak pointers','line_number':325,'multiline':False]
['text':' as expired in the destructor in the future, so we'll keep this for now.','line_number':326,'multiline':False]
['text':' Python zero-initializes the object memory, so there's no need to initialize','line_number':346,'multiline':False]
['text':' most fields','line_number':347,'multiline':False]
['text':' Setup the PyNode later; we can't keep it live here','line_number':349,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':361,'multiline':False]
['text':' Forward','line_number':362,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':363,'multiline':False]
['text':' Bump the counters of all recorded dirty input tensors, adding each of them','line_number':365,'multiline':False]
['text':' into dirty_inputs.  Also does some sanity checking.','line_number':366,'multiline':False]
['text':' Increase versions of modified tensors','line_number':368,'multiline':False]
['text':' We're not going to ever need this so let's remove references now','line_number':394,'multiline':False]
['text':' Given a Python tuple of raw output tensors (raw_output), set each of','line_number':402,'multiline':False]
['text':' the corresponding entries in a different Python tuple (outputs) with','line_number':403,'multiline':False]
['text':' these tensors wrapped with variables.  We save the gradient function (self)','line_number':404,'multiline':False]
['text':' to the variable if the output requires grad.','line_number':405,'multiline':False]
['text':'','line_number':406,'multiline':False]
['text':' There is a considerable amount of complexity to handle if the operation','line_number':407,'multiline':False]
['text':' that produced these output tensors is inplace.  A mapping of *input*','line_number':408,'multiline':False]
['text':' tensors to variables (t2var) is used to test if this occurred, and','line_number':409,'multiline':False]
['text':' the set of dirty tensors (dirty_inputs) is used to figure out what to','line_number':410,'multiline':False]
['text':' do in this case.  After this method is run, t2var is extended with','line_number':411,'multiline':False]
['text':' mappings for output tensors as well.','line_number':412,'multiline':False]
['text':' Only process tensors as outputs for autograd purposes.','line_number':435,'multiline':False]
['text':' Massage a C++ variable_list into a Python arguments tuple','line_number':448,'multiline':False]
['text':' Making sure to introduce the proper None for non-Tensor inputs','line_number':449,'multiline':False]
['text':' Massage the Python results tuple back into a C++ variable_list','line_number':484,'multiline':False]
['text':' Don't do any check on the number of results here as','line_number':485,'multiline':False]
['text':' it is handled by the caller','line_number':486,'multiline':False]
['text':' Wrap only the tensor outputs.','line_number':524,'multiline':False]
['text':' Keep the non-tensor outputs as is.','line_number':537,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':546,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':549,'multiline':False]
['text':' We look at saved_for_forward here purely for the purpose of populating','line_number':562,'multiline':False]
['text':' to_save_if_setup_context, the actual saving is not done here.','line_number':563,'multiline':False]
['text':' TODO: We should really just ALWAYS throw an error here, but','line_number':602,'multiline':False]
['text':' doing so will break some internal tests. We should fix those.','line_number':603,'multiline':False]
['text':' Save any variables that requested by to_save','line_number':614,'multiline':False]
['text':' Free .to_save','line_number':632,'multiline':False]
['text':' Mark requires_grad = 0 on non-differentiable variables (as per','line_number':636,'multiline':False]
['text':' non_differentiable)','line_number':637,'multiline':False]
['text':' TODO: remove this code path once Variable and Tensor are merged in','line_number':691,'multiline':False]
['text':' Python','line_number':692,'multiline':False]
['text':' Given a prim::PythonOp node, _append_subgraph creates a subgraph such that:','line_number':719,'multiline':False]
['text':' (1) It has the same inputs as the prim::PythonOp node','line_number':720,'multiline':False]
['text':' (2) The intermediate nodes used in the PythonOp are cloned and stored in the','line_number':721,'multiline':False]
['text':' subgraph (3) trace_outputs stores the Value* objects, before a new trace','line_number':722,'multiline':False]
['text':' value is assigned by the prim::PythonOp node and helps to eventually route','line_number':723,'multiline':False]
['text':' the outputs of the subgraph correctly This newly created subgraph is then','line_number':724,'multiline':False]
['text':' added to the prim::PythonOp node as a subgraph attribute','line_number':725,'multiline':False]
['text':' Find node position in owning block, all subsequent nodes after are added to','line_number':744,'multiline':False]
['text':' subgraph','line_number':745,'multiline':False]
['text':' Skip TupleUnpack node if created','line_number':749,'multiline':False]
['text':' Save scalar args and the calling convention','line_number':776,'multiline':False]
['text':' Isolate C variable ptrs in a vector','line_number':818,'multiline':False]
['text':' Original type is tuple of tensors "without" element type and shape.','line_number':826,'multiline':False]
['text':' The missed parts will be added below.','line_number':827,'multiline':False]
['text':' If TupleUnpack operator is created, we copy its output type back','line_number':857,'multiline':False]
['text':' to the original tuple type.','line_number':858,'multiline':False]
['text':' The i-th tuple element receives a new tensor type with element type and','line_number':866,'multiline':False]
['text':' shape.','line_number':867,'multiline':False]
['text':' Record type, device, and size information about inputs','line_number':892,'multiline':False]
['text':' It is important that creating the SavedVariables happen after the output','line_number':922,'multiline':False]
['text':' wrapping as the outputs must have their grad_fn/fw_grad properly set before','line_number':923,'multiline':False]
['text':' we save them.','line_number':924,'multiline':False]
['text':' Remove unnecessary attributes','line_number':928,'multiline':False]
['text':' Unpack the output, unless .forward() returned a tuple','line_number':938,'multiline':False]
['text':' namespace','line_number':1024,'multiline':False]
['text':' setup_context gets "leaked" - we return a new reference and hold onto it','line_number':1042,'multiline':False]
['text':' forever.','line_number':1043,'multiline':False]
['text':' save a local copy of seq_id before it gets incremented','line_number':1054,'multiline':False]
['text':' Call record function after all the inputs have been decoded, but','line_number':1060,'multiline':False]
['text':' before context has been allocated.','line_number':1061,'multiline':False]
['text':' autograd.Function support for functorch is handled in Python.','line_number':1070,'multiline':False]
['text':' If we have gotten here, then either we are dealing with a','line_number':1071,'multiline':False]
['text':' torch.autograd.function._SingleLevelFunction, or something in','line_number':1072,'multiline':False]
['text':' the implementation went wrong.','line_number':1073,'multiline':False]
['text':' The following code is useful for debugging when something goes wrong','line_number':1074,'multiline':False]
['text':' because it'll raise a loud error (instead of being silently incorrect).','line_number':1075,'multiline':False]
['text':' Record input nodes if tracing','line_number':1091,'multiline':False]
['text':' Initialize backward function (and ctx)','line_number':1094,'multiline':False]
['text':' autograd.Function may optionally override a setup_context staticmethod.','line_number':1100,'multiline':False]
['text':' In this case, autograd.Function.forward does NOT accept a ctx object.','line_number':1101,'multiline':False]
['text':' Determine if this is the case.','line_number':1102,'multiline':False]
['text':' Call forward','line_number':1116,'multiline':False]
['text':' call forward followed by setup_context','line_number':1125,'multiline':False]
['text':' signature is setup_context(ctx, inputs, output)','line_number':1130,'multiline':False]
['text':' call forward','line_number':1144,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1169,'multiline':False]
['text':' Other methods / attributes','line_number':1170,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1171,'multiline':False]
['text':' This is really a true assert, because we've already tested for the','line_number':1280,'multiline':False]
['text':' self->has_freed_buffers case at the beginning of this function:','line_number':1281,'multiline':False]
['text':' buffers are freed when PyNode dies; if the buffers are not freed,','line_number':1282,'multiline':False]
['text':' PyNode must be live.  (Note that the buffers could be freed','line_number':1283,'multiline':False]
['text':' even though the PyNode is live, but that doesn't matter here','line_number':1284,'multiline':False]
['text':' because we will never hit this line of code if the buffers are freed--','line_number':1285,'multiline':False]
['text':' and in any case saved_for will be non-NULL.)','line_number':1286,'multiline':False]
['text':' User tries to access saved variables after they have been freed','line_number':1361,'multiline':False]
['text':' The correct way to solve this problem is to stop exposing grad_fn','line_number':1413,'multiline':False]
['text':' of PyFunctions as THPFunction; instead, we should use THPCppFunction','line_number':1414,'multiline':False]
['text':' like everyone else.  But this is a BC-breaking change as it would','line_number':1415,'multiline':False]
['text':' mean that you no longer get the property that grad_fn is a subclass','line_number':1416,'multiline':False]
['text':' of the autograd function class that you defined in the custom case,','line_number':1417,'multiline':False]
['text':' so I didn't fix it here.','line_number':1418,'multiline':False]
['text':' namespace','line_number':1478,'multiline':False]
['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)','line_number':1480,'multiline':False]
['text':' NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)','line_number':1541,'multiline':False]
['text':' tp_name ','line_number':1568,'multiline':True]
['text':' tp_basicsize ','line_number':1569,'multiline':True]
['text':' tp_itemsize ','line_number':1570,'multiline':True]
['text':' tp_dealloc ','line_number':1571,'multiline':True]
['text':' tp_vectorcall_offset ','line_number':1572,'multiline':True]
['text':' tp_getattr ','line_number':1573,'multiline':True]
['text':' tp_setattr ','line_number':1574,'multiline':True]
['text':' tp_reserved ','line_number':1575,'multiline':True]
['text':' tp_repr ','line_number':1576,'multiline':True]
['text':' tp_as_number ','line_number':1577,'multiline':True]
['text':' tp_as_sequence ','line_number':1578,'multiline':True]
['text':' tp_as_mapping ','line_number':1579,'multiline':True]
['text':' tp_hash  ','line_number':1580,'multiline':True]
['text':' tp_call ','line_number':1581,'multiline':True]
['text':' tp_str ','line_number':1582,'multiline':True]
['text':' tp_getattro ','line_number':1583,'multiline':True]
['text':' tp_setattro ','line_number':1584,'multiline':True]
['text':' tp_as_buffer ','line_number':1585,'multiline':True]
['text':' NOLINTNEXTLINE(misc-redundant-expression)','line_number':1586,'multiline':False]
['text':' tp_flags ','line_number':1588,'multiline':True]
['text':' tp_doc ','line_number':1589,'multiline':True]
['text':' tp_traverse ','line_number':1590,'multiline':True]
['text':' tp_clear ','line_number':1591,'multiline':True]
['text':' tp_richcompare ','line_number':1592,'multiline':True]
['text':' tp_weaklistoffset ','line_number':1593,'multiline':True]
['text':' tp_iter ','line_number':1594,'multiline':True]
['text':' tp_iternext ','line_number':1595,'multiline':True]
['text':' tp_methods ','line_number':1596,'multiline':True]
['text':' tp_members ','line_number':1597,'multiline':True]
['text':' tp_getset ','line_number':1598,'multiline':True]
['text':' tp_base ','line_number':1599,'multiline':True]
['text':' tp_dict ','line_number':1600,'multiline':True]
['text':' tp_descr_get ','line_number':1601,'multiline':True]
['text':' tp_descr_set ','line_number':1602,'multiline':True]
['text':' tp_dictoffset ','line_number':1603,'multiline':True]
['text':' tp_init ','line_number':1604,'multiline':True]
['text':' tp_alloc ','line_number':1605,'multiline':True]
['text':' tp_new ','line_number':1606,'multiline':True]
