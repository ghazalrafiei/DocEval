['text':' namespace','line_number':90,'multiline':False]
['text':' NOTE: "leaks" THPVariableClass','line_number':99,'multiline':False]
['text':' NOTE: "leaks" Function','line_number':108,'multiline':False]
['text':' NOTE: "leaks" GradientEdge','line_number':113,'multiline':False]
['text':' NOTE: "leaks" ParameterClass','line_number':132,'multiline':False]
['text':' Initialize the Kineto profilers, if they have not already.','line_number':138,'multiline':False]
['text':' DO NOT REMOVE, this is needed for on-demand profiling.','line_number':139,'multiline':False]
['text':'cpuOnly=','line_number':142,'multiline':True]
['text':'logOnError=','line_number':143,'multiline':True]
['text':' Used for unit test to check profiler was initialized.','line_number':148,'multiline':False]
['text':' name of the event','line_number':216,'multiline':False]
['text':' PyTorch thread id of the start callback','line_number':218,'multiline':False]
['text':' PyTorch thread id of the end callback','line_number':222,'multiline':False]
['text':' for events of scope BACKWARD_FUNCTION - PyTorch thread id','line_number':225,'multiline':False]
['text':' of the corresponding forward op','line_number':226,'multiline':False]
['text':' together with fwd_thread_id, used to uniquely identify','line_number':229,'multiline':False]
['text':' the forward op','line_number':230,'multiline':False]
['text':' absolute start time (since unix epoch) in us','line_number':232,'multiline':False]
['text':' duration in us','line_number':234,'multiline':False]
['text':' used for correlation between high-level PyTorch events','line_number':236,'multiline':False]
['text':' and low-level device events','line_number':237,'multiline':False]
['text':' shapes of input tensors','line_number':241,'multiline':False]
['text':' stack traces of the PyTorch CPU events','line_number':257,'multiline':False]
['text':' type of the RecordFunction that generated a PyTorch CPU event','line_number':259,'multiline':False]
['text':' (op, torchscript function, user label, etc)','line_number':260,'multiline':False]
['text':' device number, for CPU - process id','line_number':262,'multiline':False]
['text':' for CUDA - stream id, for CPU - start thread id','line_number':264,'multiline':False]
['text':' device type','line_number':268,'multiline':False]
['text':' correlation id of a linked event','line_number':270,'multiline':False]
['text':' compute flops','line_number':274,'multiline':False]
['text':' Whether this is async event or not','line_number':276,'multiline':False]
['text':' USE_KINETO','line_number':291,'multiline':False]
['text':' Only if `USE_KINETO` is set','line_number':302,'multiline':False]
['text':' Only if `USE_KINETO` is set','line_number':303,'multiline':False]
['text':' NOTICE: These record functions are not torch operators and may not show up','line_number':306,'multiline':False]
['text':' in TorchScript tracing, FX transforms, or operator serialization. For these','line_number':307,'multiline':False]
['text':' use cases, please use `torch.profiler.record_function`.','line_number':308,'multiline':False]
['text':' Creates a new profiling scope using RecordFunction and invokes its starting','line_number':309,'multiline':False]
['text':' callbacks.','line_number':310,'multiline':False]
['text':' Ends the profiling scope created with record_function_with_param_enter.','line_number':335,'multiline':False]
['text':' We don't actually need to do anything with handle just need to persist','line_number':340,'multiline':False]
['text':' the lifetime until now.','line_number':341,'multiline':False]
['text':' Because we use a py::object, pybind will increment the refcount','line_number':491,'multiline':False]
['text':' of the hook functions for us','line_number':492,'multiline':False]
['text':' It is unlikely that the depth of forward nesting will overflow int64_t so','line_number':897,'multiline':False]
['text':' we just static cast here.','line_number':898,'multiline':False]
['text':' Make sure the given index is valid before casting it','line_number':914,'multiline':False]
['text':' When we push a mode onto the mode stack, we need to','line_number':990,'multiline':False]
['text':' check if it's an "infra" mode, by checking its _mode_key attribute.','line_number':991,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':1081,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':1097,'multiline':False]
['text':' autograd methods on torch._C','line_number':1120,'multiline':False]
['text':' NOLINT','line_number':1121,'multiline':False]
['text':' namespace autograd','line_number':1235,'multiline':False]
['text':' namespace torch','line_number':1236,'multiline':False]
