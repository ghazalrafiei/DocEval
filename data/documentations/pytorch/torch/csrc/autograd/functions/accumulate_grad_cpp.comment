['text':' AccumulateGrad sets sequence_nr to the max value so it's always called','line_number':18,'multiline':False]
['text':' ASAP during backwards.','line_number':19,'multiline':False]
['text':'sequence_nr=','line_number':21,'multiline':True]
['text':' std::move(grads[0]) to avoid bumping up refcount','line_number':36,'multiline':False]
['text':' Acquire lock to here protect thread safety on variable, this ensures','line_number':39,'multiline':False]
['text':' AccumulateGrad does not race to shared variable from different threads','line_number':40,'multiline':False]
['text':' when updating the gradients. We don't ensure thread safety on hooks','line_number':41,'multiline':False]
['text':' and rely on user to provide thread safe hooks','line_number':42,'multiline':False]
['text':' see Note [Thread Safety on Autograd Node]','line_number':43,'multiline':False]
['text':' If the function has post hooks (for example, a DDP allreduce hook),','line_number':48,'multiline':False]
['text':' call_function in Engine.cpp will temporarily bump the expected refcount','line_number':49,'multiline':False]
['text':' by one, hence the addition of !post_hooks().empty() for 'num_expected_refs'','line_number':50,'multiline':False]
['text':' in addition to the one reference that we're holding.','line_number':51,'multiline':False]
['text':' 'num_expected_refs' is used to determine whether or not we should clone','line_number':52,'multiline':False]
['text':' the grad or can steal the grad.','line_number':53,'multiline':False]
['text':' num_expected_refs ','line_number':58,'multiline':True]
['text':' op is intentionally static','line_number':92,'multiline':False]
['text':' namespace autograd','line_number':107,'multiline':False]
['text':' namespace torch','line_number':108,'multiline':False]
