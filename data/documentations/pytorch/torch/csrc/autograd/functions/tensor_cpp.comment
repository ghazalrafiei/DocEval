['text':' Handle R->C copies without raising a warning','line_number':30,'multiline':False]
['text':' Take the next_edges of fn as our own, except for index 0 which goes','line_number':65,'multiline':False]
['text':' to base instead of the view.','line_number':66,'multiline':False]
['text':' common code between apply/apply_with_saved','line_number':76,'multiline':False]
['text':' Acquire lock to here protect thread safety on fn','line_number':87,'multiline':False]
['text':' see Note [Thread Safety on Autograd Node]','line_number':88,'multiline':False]
['text':' See Note [View + Inplace update for view tensor] For more details on this','line_number':108,'multiline':False]
['text':' block Since the gradient edge for the 0th input is different between `this`','line_number':109,'multiline':False]
['text':' and `fn`, make sure that the one from `fn` has the same metadata in the','line_number':110,'multiline':False]
['text':' current GraphTask's exec_info as the one on `this`.','line_number':111,'multiline':False]
['text':' Node is not in the exec_info already','line_number':121,'multiline':False]
['text':' And we need gradient for the corresponding output','line_number':123,'multiline':False]
['text':' There is no need to remove this after execution because we are','line_number':125,'multiline':False]
['text':' guaranteed that this->next_edge(0) must be in the history of','line_number':126,'multiline':False]
['text':' fn->next_edge(0) (we cannot easily assert this as it might be far','line_number':127,'multiline':False]
['text':' away if there were many chained views). This means that, since','line_number':128,'multiline':False]
['text':' fn->next_edge(0) was not needed (no exec_info entry for it), we','line_number':129,'multiline':False]
['text':' know that nothing downstream of fn->next_edge(0) is needed either','line_number':130,'multiline':False]
['text':' (otherwise the whole path from that Node to this->next_edge(0)','line_number':131,'multiline':False]
['text':' would be needed as well). This means that no other Node will ever','line_number':132,'multiline':False]
['text':' look at fn->next_edge(0) metadata and thus there is no need to','line_number':133,'multiline':False]
['text':' clean them up.','line_number':134,'multiline':False]
['text':' Sanity check that the graph was never modified after the fact (it is','line_number':143,'multiline':False]
['text':' read-only!)','line_number':144,'multiline':False]
['text':' TODO: We clone grad_slice because we modify it below and "fn" might save','line_number':151,'multiline':False]
['text':' it for the backward of res. We might be able to avoid the clone() if','line_number':152,'multiline':False]
['text':' double-backprop is disabled.','line_number':153,'multiline':False]
['text':' If the output is not defined, treat it as if it was a zero tensor.','line_number':160,'multiline':False]
['text':' This can happen if users define a custom Function.','line_number':161,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-cplusplus.Move)','line_number':166,'multiline':False]
['text':' NOLINT(bugprone-use-after-move)','line_number':167,'multiline':False]
['text':' Acquire lock to here protect thread safety on fn','line_number':178,'multiline':False]
['text':' namespace autograd','line_number':216,'multiline':False]
['text':' namespace torch','line_number':217,'multiline':False]
