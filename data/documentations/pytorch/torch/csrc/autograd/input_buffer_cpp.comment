['text':' look what you made me do >.<','line_number':24,'multiline':False]
['text':' Divergent paths for per-Impl stream recording that leak implementation','line_number':25,'multiline':False]
['text':' details of the impls should not be needed here.','line_number':26,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/60306','line_number':27,'multiline':False]
['text':' TODO: clean this up when https://github.com/pytorch/pytorch/issues/60306 is','line_number':28,'multiline':False]
['text':' improved','line_number':29,'multiline':False]
['text':' `v` is a "vanilla" Tensor','line_number':75,'multiline':False]
['text':' with a favorable memory layout','line_number':78,'multiline':False]
['text':' and we hold the last reference','line_number':81,'multiline':False]
['text':' anonymous namespace','line_number':85,'multiline':False]
['text':' If we hold the last reference to `old_var` AND its storage we will try to','line_number':93,'multiline':False]
['text':' repurpose it to store the output. (Or, if `old_var` is sparse then `var`','line_number':94,'multiline':False]
['text':' becomes the candidate output Tensor.) We only do this if:','line_number':95,'multiline':False]
['text':'  1) GradMode is disabled since Autograd has special handling for inplace','line_number':96,'multiline':False]
['text':'     mutation which we don't want to trigger.','line_number':97,'multiline':False]
['text':'','line_number':98,'multiline':False]
['text':'  2) We hold the last reference.','line_number':99,'multiline':False]
['text':'     (Both `.use_count` and `.storage().use_count()` are one)','line_number':100,'multiline':False]
['text':'','line_number':101,'multiline':False]
['text':'  3) The candidate tensor is a contiguous, non-overlapping, dense, and','line_number':102,'multiline':False]
['text':'     otherwise stock standard Tensor.','line_number':103,'multiline':False]
['text':'','line_number':104,'multiline':False]
['text':'  4) The candidate is mutable. Currently only ZeroTensors are immutable.','line_number':105,'multiline':False]
['text':'','line_number':106,'multiline':False]
['text':'  5) The other Tensor is not a Tensor subclass (except sparse), since','line_number':107,'multiline':False]
['text':'     it's hard to predict the semantics of arbitrary subclass behavior.','line_number':108,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':110,'multiline':False]
['text':' ATen doesn't route sparse additions correctly...','line_number':114,'multiline':False]
['text':' Switches to accumulate device','line_number':139,'multiline':False]
['text':' The device (and stream) chosen for accumulation is:','line_number':140,'multiline':False]
['text':'  (1) var is not a CUDA variable. Accumulation happens on var's device.','line_number':141,'multiline':False]
['text':'  (2) var is a CUDA variable and it, the consumer, and the producer share','line_number':142,'multiline':False]
['text':'  the same device:','line_number':143,'multiline':False]
['text':'       (2a) Uses the consumer's stream as the accumulation stream','line_number':144,'multiline':False]
['text':'       (2b) Syncs the accumulation stream with the producer's stream (if','line_number':145,'multiline':False]
['text':'       different) (2c) Accumulates.','line_number':146,'multiline':False]
['text':'  (3) var is a CUDA variable and it shares a device with the consumer but','line_number':147,'multiline':False]
['text':'  not the producer:','line_number':148,'multiline':False]
['text':'       (3a) Uses the consumer's stream as the accumulation stream','line_number':149,'multiline':False]
['text':'       (3b) Syncs the accumulation stream with the consumer device's default','line_number':150,'multiline':False]
['text':'       stream (3c) Accumulates.','line_number':151,'multiline':False]
['text':'  (4) var is a CUDA variable and it shares a device with the producer but','line_number':152,'multiline':False]
['text':'  not the consumer:','line_number':153,'multiline':False]
['text':'       (4a) Uses the producer device's default stream as the accumulation','line_number':154,'multiline':False]
['text':'       stream (4b) Syncs the accumulation stream with the producer's','line_number':155,'multiline':False]
['text':'       stream (4c) Accumulates.','line_number':156,'multiline':False]
['text':'  (5) var is a CUDA variable and it does not share a device with the','line_number':157,'multiline':False]
['text':'  consumer or producer.','line_number':158,'multiline':False]
['text':'      Accumulation happens on the var device's default stream.','line_number':159,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':163,'multiline':False]
['text':' (2a)','line_number':171,'multiline':False]
['text':' (2b)','line_number':174,'multiline':False]
['text':' (3a)','line_number':184,'multiline':False]
['text':' (4a)','line_number':188,'multiline':False]
['text':' (5)','line_number':193,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':194,'multiline':False]
['text':' (3b), (4b)','line_number':198,'multiline':False]
['text':' (1) non-CUDA variable','line_number':217,'multiline':False]
['text':'     Accumulation happens on variable's device','line_number':218,'multiline':False]
['text':' Since we pick the first non-CPU tensor, this won't work with','line_number':226,'multiline':False]
['text':' mixed device-type operations (e.g., an op that is both CUDA','line_number':227,'multiline':False]
['text':' and XLA).  This is *incredibly* unlikely, so we don't worry','line_number':228,'multiline':False]
['text':' about it.','line_number':229,'multiline':False]
['text':' Only report to the CPU thread if there really were no tensors','line_number':238,'multiline':False]
['text':' from other devices.','line_number':239,'multiline':False]
['text':' namespace autograd','line_number':248,'multiline':False]
['text':' namespace torch','line_number':249,'multiline':False]
