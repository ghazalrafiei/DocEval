['text':' Enumerate over tensors in a stack, including ones in TensorLists','line_number':31,'multiline':False]
['text':' true for optional tensor that has value','line_number':35,'multiline':False]
['text':' namespace','line_number':51,'multiline':False]
['text':' Optimization: TLS access can be slow. So we only check if it necessary','line_number':126,'multiline':False]
['text':' by putting it after the requires_grad checks.','line_number':127,'multiline':False]
['text':' NB: It is standard to collect edges from all tensors','line_number':132,'multiline':False]
['text':' (see generated/VariableTypeEverything.cpp for examples)','line_number':133,'multiline':False]
['text':' NB: if the operator mutates any inputs in-place and does not return them','line_number':151,'multiline':False]
['text':' as outputs, we are unable to lazily raise a warning. This is OK because','line_number':152,'multiline':False]
['text':' we don't expect many existing operators to do this because of the amount','line_number':153,'multiline':False]
['text':' of technical expertise necessary (you would need to manually register an','line_number':154,'multiline':False]
['text':' autograd kernel without using autograd.Function)','line_number':155,'multiline':False]
['text':' If the post-autograd implementation returns Tensors that require','line_number':165,'multiline':False]
['text':' grad, then we install a hook that will warn during the backwards.','line_number':166,'multiline':False]
['text':'','line_number':167,'multiline':False]
['text':' NB: If the operation is inplace and the inputs were views,','line_number':168,'multiline':False]
['text':' it is possible that the history was rebased and the hook will','line_number':169,'multiline':False]
['text':' not warn in all places where it should. That is, the following','line_number':170,'multiline':False]
['text':' won't warn:','line_number':171,'multiline':False]
['text':' >>> x = torch.randn(3, 3, requires_grad=True)','line_number':172,'multiline':False]
['text':' >>> z = x.clone()','line_number':173,'multiline':False]
['text':' >>> w = z[0]','line_number':174,'multiline':False]
['text':' >>> k = w[0]','line_number':175,'multiline':False]
['text':' >>> y = op(k)','line_number':176,'multiline':False]
['text':' >>> torch.autograd.grad(z.sum(), w)','line_number':177,'multiline':False]
['text':' If history is rebased, then we will attempt to warn','line_number':182,'multiline':False]
['text':' on the view's base. This will catch most cases (because','line_number':183,'multiline':False]
['text':' users typically call .backward() and backprop through','line_number':184,'multiline':False]
['text':' the entire program).','line_number':185,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':187,'multiline':False]
['text':' Can only register_hook on tensors that require grad.','line_number':190,'multiline':False]
['text':' If the post-autograd implementation returns any Tensors that','line_number':199,'multiline':False]
['text':' don't require grad, then we install the WarnNotImplemented grad_fn.','line_number':200,'multiline':False]
['text':' This grad_fn warns in backward and returns undefined tensor','line_number':201,'multiline':False]
['text':' gradients.','line_number':202,'multiline':False]
['text':'','line_number':203,'multiline':False]
['text':' NOTE [autograd fallback and in-place operations]','line_number':204,'multiline':False]
['text':' If the schema says the output is mutable, and the output','line_number':205,'multiline':False]
['text':' is an input, and the input is a view Tensor, then...','line_number':206,'multiline':False]
['text':' we're not sure if set_history is OK to do, so we just skip','line_number':207,'multiline':False]
['text':' adding the grad_fn. Builtin operators do rebase_history here,','line_number':208,'multiline':False]
['text':' but custom operators may have multiple Tensor(a!) returns,','line_number':209,'multiline':False]
['text':' rebase_history assumes single Tensor(a!) return, and in general','line_number':210,'multiline':False]
['text':' custom ops don't have a good in-place story.','line_number':211,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':213,'multiline':False]
['text':' Mimics a subset of the logic of a VariableType NotImplemented kernel','line_number':239,'multiline':False]
['text':' See gen_variable_type.py','line_number':240,'multiline':False]
['text':' Keep track of which outputs are output of in-place modification','line_number':249,'multiline':False]
['text':' so we can rebase_history if necessary','line_number':250,'multiline':False]
['text':' Only used for DEBUG-only checks','line_number':286,'multiline':False]
['text':' See NOTE [ TensorImpl and Storage Pointer Sanity Checks ]','line_number':324,'multiline':False]
['text':' If neither in-place nor view','line_number':346,'multiline':False]
['text':' Okay to return undefined tensor','line_number':372,'multiline':False]
['text':' note(crcrpar): `_foreach_norm` returns a list of scalar Tensors and','line_number':373,'multiline':False]
['text':' each Tensor shares a storage of a hidden, intermediate 1D Tensor','line_number':374,'multiline':False]
['text':' created inside the CUDA implementation. This is because the','line_number':375,'multiline':False]
['text':' reference implementation of nvidia/apex repo returns this 1D Tensor','line_number':376,'multiline':False]
['text':' where each element represents the norm of corresponding input Tensor,','line_number':377,'multiline':False]
['text':' here I want to return the same number of Tensors as the input','line_number':378,'multiline':False]
['text':' TensorList, see https://github.com/pytorch/pytorch/issues/93940','line_number':379,'multiline':False]
['text':' There should be only a single base-view pair, make sure their storage is','line_number':387,'multiline':False]
['text':' aliased.','line_number':388,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':421,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':424,'multiline':False]
['text':' Mimics a subset of the logic from ADInplaceOrViewType kernel:','line_number':444,'multiline':False]
['text':' - see gen_inplace_or_view_type.py','line_number':445,'multiline':False]
['text':' - this should only be used with autogradNotImplementedFallback above','line_number':446,'multiline':False]
['text':' - For more information see','line_number':447,'multiline':False]
['text':' https://pytorch.org/tutorials/advanced/dispatcher','line_number':448,'multiline':False]
['text':'','line_number':449,'multiline':False]
['text':' NOTE [ Limitations of ADInplaceOrView boxed kernel ]','line_number':450,'multiline':False]
['text':'','line_number':451,'multiline':False]
['text':' This op should only be used with autogradNotImplementedFallback kernel','line_number':452,'multiline':False]
['text':' because there is some logic we need specifically to enforce that even','line_number':453,'multiline':False]
['text':' if we do in-place on view's created in this kernel, the proper "derivative','line_number':454,'multiline':False]
['text':' is not implemented" error is still raised.','line_number':455,'multiline':False]
['text':'','line_number':456,'multiline':False]
['text':' Just like the codegened kernel, we try to enforce some things:','line_number':457,'multiline':False]
['text':' - For views: we enforce that the view relationship is between the first','line_number':458,'multiline':False]
['text':' input','line_number':459,'multiline':False]
['text':'   and the first output (which may be either Tensor or vec of Tensors','line_number':460,'multiline':False]
['text':' - For inplace (TODO?): enforce that the same op cannot be both a view and','line_number':461,'multiline':False]
['text':' inplace','line_number':462,'multiline':False]
['text':'   that is not allowed in the gen_inplace_or_view logic','line_number':463,'multiline':False]
['text':' get a reference to an ivalue on the','line_number':498,'multiline':False]
['text':' stack','line_number':499,'multiline':False]
['text':' TODO: Can we avoid saving this tensor','line_number':502,'multiline':False]
['text':' and incurring the refcount bump?','line_number':503,'multiline':False]
['text':' See NOTE [ Limitations of ADInplaceOrView boxed kernel ] above','line_number':506,'multiline':False]
['text':' Only allow rebasing of the history if we return a single Tensor that is','line_number':533,'multiline':False]
['text':' why we don't have to care about the view_func logic below.','line_number':534,'multiline':False]
['text':' See NOTE [ View + Inplace detection ] for more details about this logic','line_number':535,'multiline':False]
['text':' base=','line_number':537,'multiline':True]
['text':' tensors=','line_number':538,'multiline':True]
['text':' is_bw_differentiable=','line_number':539,'multiline':True]
['text':' is_fw_differentiable=','line_number':540,'multiline':True]
['text':' creation_meta=','line_number':541,'multiline':True]
['text':' ^ pass in creation meta unnecessarily even if not isDifferentiableType,','line_number':546,'multiline':False]
['text':' but we don't have that','line_number':547,'multiline':False]
['text':'   information here anyway.','line_number':548,'multiline':False]
['text':' base=','line_number':553,'multiline':True]
['text':' tensor=','line_number':554,'multiline':True]
['text':' is_bw_differentiable=','line_number':555,'multiline':True]
['text':' is_fw_differentiable=','line_number':556,'multiline':True]
['text':' view_func=','line_number':557,'multiline':True]
['text':' We always need this view_func because otherwise if we do in-place','line_number':559,'multiline':False]
['text':' on this view, we would implicitly use AsStridedBackward instead','line_number':560,'multiline':False]
['text':' of the NotImplemented node. For the cross-dtype/non-strided','line_number':561,'multiline':False]
['text':' cases, we would create something like this anyway','line_number':562,'multiline':False]
['text':' creation_meta=','line_number':570,'multiline':True]
['text':' namespace autograd','line_number':586,'multiline':False]
['text':' namespace torch','line_number':587,'multiline':False]
