['text':' Some operations can be performed more efficiently if we're handling tensors','line_number':25,'multiline':False]
['text':' of a single type only. Adding this logic directly in the loop makes it a bit','line_number':26,'multiline':False]
['text':' ugly, so here's a helper for it.','line_number':27,'multiline':False]
['text':' ***************** Broadcast *******************','line_number':44,'multiline':False]
['text':'','line_number':45,'multiline':False]
['text':' Broadcast a source tensor (CPU or CUDA) to a list of CUDA devices, or CUDA','line_number':46,'multiline':False]
['text':' tensors on one or more devices.','line_number':47,'multiline':False]
['text':' no checks','line_number':49,'multiline':False]
['text':'non_blocking=','line_number':67,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':97,'multiline':False]
['text':' preserve memory format','line_number':107,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':111,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':116,'multiline':False]
['text':' NOTE [ Version Counter in comm.*_coalesced ]','line_number':127,'multiline':False]
['text':'','line_number':128,'multiline':False]
['text':' broadcast_coalesced','line_number':129,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~','line_number':130,'multiline':False]
['text':'','line_number':131,'multiline':False]
['text':' In broadcast_coalesced, multiple variables may be coalesced into a single','line_number':132,'multiline':False]
['text':' large one, broadcast to other devices, and the get split according to the','line_number':133,'multiline':False]
['text':' original shapes.','line_number':134,'multiline':False]
['text':'','line_number':135,'multiline':False]
['text':' When splitting, the view operations will make all Variables broadcast','line_number':136,'multiline':False]
['text':' together to share a single version counter, because they are all views of the','line_number':137,'multiline':False]
['text':' large Variable. However, that large Variable is immediately discarded and all','line_number':138,'multiline':False]
['text':' these Variables do not share storage at all.','line_number':139,'multiline':False]
['text':'','line_number':140,'multiline':False]
['text':' For example, when two buffers are broadcast together in `DataParallel` and','line_number':141,'multiline':False]
['text':' one of them is modified in-place during `forward` but the other is needed in','line_number':142,'multiline':False]
['text':' backward, autograd engine will complain.','line_number':143,'multiline':False]
['text':'','line_number':144,'multiline':False]
['text':' We thus re-wrap these Variables after broadcasting (i.e., effectively doing','line_number':145,'multiline':False]
['text':' what is equivalent to .data in Python), and give them individual version','line_number':146,'multiline':False]
['text':' counters.','line_number':147,'multiline':False]
['text':'','line_number':148,'multiline':False]
['text':' NB: Just calling detach() on the variables is not sufficient','line_number':149,'multiline':False]
['text':'','line_number':150,'multiline':False]
['text':' NB: For `device[0]` in broadcast_coalesced, the input Variables are always','line_number':151,'multiline':False]
['text':'     returned as-is, so **do not** re-wrap them.','line_number':152,'multiline':False]
['text':'','line_number':153,'multiline':False]
['text':' reduce_add_coalesced','line_number':154,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':' Similarly for reduce_add_coalesced, when the output are newly created','line_number':157,'multiline':False]
['text':' Variables.','line_number':158,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':174,'multiline':False]
['text':' See NOTE [ Version Counter in comm.*_coalesced ]','line_number':198,'multiline':False]
['text':' See NOTE [ Version Counter in comm.*_coalesced ]','line_number':210,'multiline':False]
['text':' If we only saw a single tensor type, then we can skip expensive reordering','line_number':217,'multiline':False]
['text':' ***************** Scatter *******************','line_number':225,'multiline':False]
['text':'','line_number':226,'multiline':False]
['text':' Scatter a source tensor (CPU or CUDA) to a list of CUDA tensors on one or','line_number':227,'multiline':False]
['text':' more devices.','line_number':228,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':241,'multiline':False]
['text':'split_sizes=','line_number':284,'multiline':True]
['text':'dim=','line_number':284,'multiline':True]
['text':' NB: We don't detect the case where `out_tensor` is already the correct','line_number':303,'multiline':False]
['text':'     view of `tensor` since that would be nontrivial and involve checking','line_number':304,'multiline':False]
['text':'     ptr, offset, and strides. So `scatter_out(src, src.chunk(...))` does','line_number':305,'multiline':False]
['text':'     more copying than `scatter(src)`.','line_number':306,'multiline':False]
['text':'non_blocking=','line_number':307,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':330,'multiline':False]
['text':'split_sizes=','line_number':332,'multiline':True]
['text':'dim=','line_number':332,'multiline':True]
['text':'chunks=','line_number':333,'multiline':True]
['text':'dim=','line_number':333,'multiline':True]
['text':'non_blocking=','line_number':358,'multiline':True]
['text':'copy=','line_number':359,'multiline':True]
['text':'memory_format=','line_number':360,'multiline':True]
['text':' ***************** Gather *******************','line_number':366,'multiline':False]
['text':'','line_number':367,'multiline':False]
['text':' Gather a list of CUDA tensors on one or more devices to a target tensor or','line_number':368,'multiline':False]
['text':' device, either CPU or CUDA.','line_number':369,'multiline':False]
['text':' no checks','line_number':371,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':376,'multiline':False]
['text':'split_sizes=','line_number':383,'multiline':True]
['text':'dim=','line_number':383,'multiline':True]
['text':'non_blocking=','line_number':385,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':399,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':454,'multiline':False]
['text':' namespace torch::cuda','line_number':505,'multiline':False]
