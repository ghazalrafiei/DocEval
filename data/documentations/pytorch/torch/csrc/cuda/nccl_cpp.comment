['text':' TODO(eqy): can this duplication be avoided from NCCLUtils.cpp?','line_number':137,'multiline':False]
['text':' fall through to failed case ','line_number':217,'multiline':True]
['text':' there are cases when this destructor is called after the
           CUDA driver is already unloaded from the process.
           In these cases, skip ncclCommDestroy ','line_number':257,'multiline':True]
['text':' accesses to this object have to be guarded by THC's CudaFreeMutex','line_number':272,'multiline':False]
['text':' all inputs must be same size','line_number':312,'multiline':False]
['text':' inputs and outputs must be on same device respectively','line_number':321,'multiline':False]
['text':' len(inputs) == len(outputs)','line_number':338,'multiline':False]
['text':' inputs must be on unique devices','line_number':364,'multiline':False]
['text':' inputs must be on unique devices','line_number':402,'multiline':False]
['text':' namespace detail','line_number':410,'multiline':False]
['text':' nccl < 2.0 cannot be called concurrently with cudaFree','line_number':414,'multiline':False]
['text':' nccl < 2.0 cannot be called concurrently with cudaFree','line_number':427,'multiline':False]
['text':' TODO(eqy): can we make comms_ reference?','line_number':430,'multiline':False]
['text':' return major version "1"','line_number':476,'multiline':False]
['text':'
   * TODO(T30279827) Temporarily disable calling ncclCommDestroy
   * Calling ncclCommDestroy while program exiting is undefined
   * according to Nvidia, and lead to segfault in NCCL 2
   * (whether it is called before or after the CUDA runtime destructor).
   * Temporarily disable it in destructor to avoid segfault.
   * Following up with Nvidia for long term solution.
   ','line_number':514,'multiline':True]
['text':' NCCL changed the numerical type used for count between NCCL1 and NCCL2.','line_number':531,'multiline':False]
['text':' So we use the following struct, which gets the type of the second argument','line_number':532,'multiline':False]
['text':' of T, if T is a function type, with ncclBcast, to get that type statically','line_number':533,'multiline':False]
['text':' and programmatically.','line_number':534,'multiline':False]
['text':' Since NCCL 2.12.10, NCCL supports send/recv 0 byte:','line_number':547,'multiline':False]
['text':' https://github.com/NVIDIA/nccl/issues/696. The issue of skipping send/recv','line_number':548,'multiline':False]
['text':' is that it can cause deadlock when a rank send and recv 0 bytes so it's','line_number':549,'multiline':False]
['text':' completely skipping the collective, causing mismatch across ranks','line_number':550,'multiline':False]
['text':' old NCCL uses 0 byte message for synchronization','line_number':558,'multiline':False]
['text':' Avoid send/recv when message size is zero','line_number':559,'multiline':False]
['text':' namespace','line_number':565,'multiline':False]
['text':' Default to the current stream','line_number':589,'multiline':False]
['text':' Default to the current stream','line_number':641,'multiline':False]
['text':'output=','line_number':670,'multiline':True]
['text':' Default to the current stream','line_number':695,'multiline':False]
['text':' Default to the current stream','line_number':737,'multiline':False]
['text':' Default to the current stream','line_number':778,'multiline':False]
['text':' on its own rank, simply copy from the input','line_number':1054,'multiline':False]
['text':' on its own rank, simply copy it to the output','line_number':1104,'multiline':False]
['text':' namespace torch::cuda::nccl','line_number':1127,'multiline':False]
