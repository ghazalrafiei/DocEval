['text':' WARNING: Be careful when adding new includes here. This header will be used','line_number':9,'multiline':False]
['text':' in model.so, and should not refer to any aten/c10 headers except the stable','line_number':10,'multiline':False]
['text':' C ABI defined in torch/csrc/inductor/aoti_torch/c/shim.h. The same rule','line_number':11,'multiline':False]
['text':' applies to other files under torch/csrc/inductor/aoti_runtime/.','line_number':12,'multiline':False]
['text':' Note that the all following fields (input_names_, output_names,','line_number':35,'multiline':False]
['text':' etc) can be filled in by the AOT','line_number':36,'multiline':False]
['text':' codegen. However, we choose to query such information from','line_number':37,'multiline':False]
['text':' the owned AOTInductorModel for a couple of reasons:','line_number':38,'multiline':False]
['text':'   * simplify the codegen templates','line_number':39,'multiline':False]
['text':'   * reduce information fragmentation and duplication','line_number':40,'multiline':False]
['text':'   * the initialization process below is done only once when the container','line_number':41,'multiline':False]
['text':'     is constructed, so it would have little performance impact','line_number':42,'multiline':False]
['text':' array of input AtenTensorHandle; handles','line_number':73,'multiline':False]
['text':' are stolen; the array itself is borrowed','line_number':74,'multiline':False]
['text':' array for writing output AtenTensorHandle; handles','line_number':76,'multiline':False]
['text':' will be stolen by the caller; the array itself is','line_number':77,'multiline':False]
['text':' borrowed','line_number':78,'multiline':False]
['text':' This function updates the inactive buffer for storing constants.','line_number':98,'multiline':False]
['text':' It will update the buffer, the mapping and the array mapping.','line_number':99,'multiline':False]
['text':' We can later change the inactive buffer to active with corresponding','line_number':100,'multiline':False]
['text':' function calls (swap_constant_buffer)','line_number':101,'multiline':False]
['text':' Move the data to container handled blob.','line_number':122,'multiline':False]
['text':' Generate Tensor from container handled blob.','line_number':136,'multiline':False]
['text':' We extract stride and offset from provided Tensor since we do not','line_number':137,'multiline':False]
['text':' guarantee that the tensor is contiguous.','line_number':138,'multiline':False]
['text':' Now place the tensor to constants_map. Note at this point the ownership','line_number':158,'multiline':False]
['text':' of the tensor_handle will be taken over.','line_number':159,'multiline':False]
['text':' Update the inactive constant array.','line_number':163,'multiline':False]
['text':' USE_CUDA','line_number':165,'multiline':False]
['text':' remap_constants_array = ','line_number':186,'multiline':True]
['text':' Holds the blob storage for constants' at::Tensor for CUDA.','line_number':228,'multiline':False]
['text':' Let's place this within USE_CUDA at the moment before we fully support','line_number':232,'multiline':False]
['text':' update for CPU cases.','line_number':233,'multiline':False]
['text':' USE_CUDA','line_number':236,'multiline':False]
['text':' Determine which constants is being used for the model.','line_number':238,'multiline':False]
['text':' If true,','line_number':239,'multiline':False]
['text':' constants_map_secondary/constant_blob_secondary/constants_array_secondary','line_number':240,'multiline':False]
['text':' is being used.','line_number':241,'multiline':False]
['text':' Holds the mapping of constants to at::Tensor.','line_number':244,'multiline':False]
['text':' The underlying data of at::Tensor is in either constant_blob_ (for CUDA).','line_number':245,'multiline':False]
['text':' or _binary_constants_bin_start (for CPU).','line_number':246,'multiline':False]
['text':' Holds the indexed array of constant for faster lookup during runtime.','line_number':250,'multiline':False]
['text':' Holds all the AOTInductorModel instances owned by this container.','line_number':254,'multiline':False]
['text':' Holds the AOTInductorModel instances available for inference.','line_number':257,'multiline':False]
['text':' Holds the AOTInductorModel instances that have started running','line_number':260,'multiline':False]
['text':' inference and can be placed onto available_models_ upon their','line_number':261,'multiline':False]
['text':' completion.','line_number':262,'multiline':False]
['text':' Protects available_models_ and pending_models_.','line_number':265,'multiline':False]
['text':' Notified whenever a model is placed onto pending_models_.','line_number':268,'multiline':False]
['text':' This mutex is used to protect execution of model.','line_number':281,'multiline':False]
['text':' We acquire the mutex in shared mode if we allow concurrent execution.','line_number':282,'multiline':False]
['text':' We acquire the mutex in unique mode when we want exclusive access of the','line_number':283,'multiline':False]
['text':' model. One such case is when we want to do a weight swapping. We want to','line_number':284,'multiline':False]
['text':' make sure no one is executing the model.','line_number':285,'multiline':False]
['text':' USE_CUDA','line_number':299,'multiline':False]
['text':' push finished model instances to the end of pending_models_','line_number':326,'multiline':False]
['text':' We have finished model instances that can be pushed into','line_number':333,'multiline':False]
['text':' available_models_ so that we don't have to be blocked on waiting','line_number':334,'multiline':False]
['text':' the pending_models_available_ condition.','line_number':335,'multiline':False]
['text':' Let's make the schedule simple first. We always wait on the first','line_number':344,'multiline':False]
['text':' pending_models_ to be complete.','line_number':345,'multiline':False]
['text':' namespace aot_inductor','line_number':361,'multiline':False]
['text':' namespace torch','line_number':362,'multiline':False]
