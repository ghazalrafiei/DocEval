['text':' NB: I'd like to index this on OperatorHandle, but I can't, as I can't','line_number':36,'multiline':False]
['text':' guarantee that the main interpreter has finish doing all registrations before','line_number':37,'multiline':False]
['text':' the other interpreters start banging on it','line_number':38,'multiline':False]
['text':' default','line_number':59,'multiline':False]
['text':' Figure out if we can handle it hermetically, or if we have','line_number':122,'multiline':False]
['text':' to double dispatch','line_number':123,'multiline':False]
['text':' If Torch Dispatch Mode is active, use its PyInterpreter for dispatch','line_number':125,'multiline':False]
['text':' Otherwise, find a PyInterpreter on a Tensor IF if has Python key (which','line_number':138,'multiline':False]
['text':' means it's a nontrivial tensor subclass)','line_number':139,'multiline':False]
['text':' NB: use toListRef as it doesn't induce refcount bumps','line_number':152,'multiline':False]
['text':' (toTensorListRef is not a thing)','line_number':153,'multiline':False]
['text':' Nothing requires the operator to be homed to a specific interpreter, so','line_number':170,'multiline':False]
['text':' run it on the current interpreter','line_number':171,'multiline':False]
['text':'self=','line_number':204,'multiline':True]
['text':' A small RAII guard that lets you explicitly *remove* a key from the TLS','line_number':212,'multiline':False]
['text':' exclude set.','line_number':213,'multiline':False]
['text':' TODO: figure out how to do chaining','line_number':242,'multiline':False]
['text':' Some of these APIs are only for testing and do not work in multipy','line_number':244,'multiline':False]
['text':' environment','line_number':245,'multiline':False]
['text':' Simulated "legacy" def where alias analysis kind is not set.','line_number':257,'multiline':False]
['text':' Ordinarily this can only be exercised from RegisterOperators() API','line_number':258,'multiline':False]
['text':' but I am not going to bind that here','line_number':259,'multiline':False]
['text':' We can't conveniently turn Python functions into valid functions','line_number':269,'multiline':False]
['text':' in the dispatcher.  So instead we provide a bunch of precanned','line_number':270,'multiline':False]
['text':' functions for testing purposes.  You're NOT intended to actually','line_number':271,'multiline':False]
['text':' call these functions; they're just here so we can actually register','line_number':272,'multiline':False]
['text':' something','line_number':273,'multiline':False]
['text':'','line_number':274,'multiline':False]
['text':' Mangling scheme: args_rets.  One character per.','line_number':275,'multiline':False]
['text':'  t = Tensor','line_number':276,'multiline':False]
['text':' TODO: maybe consider deduplicating the definitions here, it's getting','line_number':314,'multiline':False]
['text':' pretty long','line_number':315,'multiline':False]
['text':' TODO: empty string no longer works','line_number':337,'multiline':False]
['text':' TODO: this is dumb, had to make a second copy','line_number':378,'multiline':False]
['text':' temporary workaround','line_number':411,'multiline':False]
['text':' Returns whether or not a direct kernel registration exists','line_number':465,'multiline':False]
['text':' for this <op_name, dispatch_key> pair.','line_number':466,'multiline':False]
['text':' Returns whether or not there is an entry in the runtime computed','line_number':485,'multiline':False]
['text':' dispatch table, for this <op_name, dispatch_key> pair. For example, if','line_number':486,'multiline':False]
['text':' "op" has a `CompositeImplicitAutograd` kernel, Then','line_number':487,'multiline':False]
['text':' _dispatch_has_computed_kernel_for_dispatch_key(op, backend) will return','line_number':488,'multiline':False]
['text':' true for all backends that are part of the alias set for','line_number':489,'multiline':False]
['text':' CompositeImplicitAutograd.','line_number':490,'multiline':False]
['text':' E.g. given `DispatchKey::AutogradFunctionality`, returns a keyset of:','line_number':561,'multiline':False]
['text':'  AutogradCPU','line_number':562,'multiline':False]
['text':'  AutogradCUDA','line_number':563,'multiline':False]
['text':'  ...','line_number':564,'multiline':False]
['text':'  AutogradPrivateUse3','line_number':565,'multiline':False]
['text':' clang-format off','line_number':584,'multiline':False]
['text':' NestedTensor is not a backend key','line_number':590,'multiline':False]
['text':' clang-format on','line_number':616,'multiline':False]
['text':' clang-format off','line_number':625,'multiline':False]
['text':' clang-format on','line_number':627,'multiline':False]
['text':' DEPRECATED, please don't use this. Instead use','line_number':699,'multiline':False]
['text':' torch._C._ExcludeDispatchKeyGuard','line_number':700,'multiline':False]
['text':' Prints out the name of every operator that has a kernel registered to the','line_number':719,'multiline':False]
['text':' Dispatcher under [dispatch_key]. If no arguments are specified, it'll print','line_number':720,'multiline':False]
['text':' out the name of every operator that the Dispatcher knows of. This can be','line_number':721,'multiline':False]
['text':' useful to answer questions like "list all operators that do not have a CPU','line_number':722,'multiline':False]
['text':' kernel".','line_number':723,'multiline':False]
['text':' NB: NOT sym_size','line_number':823,'multiline':False]
['text':' TODO: dedupe with the kernel','line_number':833,'multiline':False]
['text':' namespace dispatch','line_number':853,'multiline':False]
['text':' namespace impl','line_number':854,'multiline':False]
['text':' namespace torch','line_number':855,'multiline':False]
