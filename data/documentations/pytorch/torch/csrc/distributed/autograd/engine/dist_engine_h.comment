['text':' Forward declaration.','line_number':15,'multiline':False]
['text':' This is a singleton class responsible for running distributed backward','line_number':18,'multiline':False]
['text':' passes. This engine relies heavily on the vanilla autograd engine and tries','line_number':19,'multiline':False]
['text':' to re-use it as much as possible. This class is mostly responsible for the','line_number':20,'multiline':False]
['text':' distributed aspects of autograd and tries to hook into the autograd engine','line_number':21,'multiline':False]
['text':' where convenient.','line_number':22,'multiline':False]
['text':' Unlike the vanilla autograd engine, the distributed autograd engine','line_number':24,'multiline':False]
['text':' accumulates the gradients in the appropriate DistAutogradContext. This avoids','line_number':25,'multiline':False]
['text':' multiple trainer nodes stomping on each others gradients.','line_number':26,'multiline':False]
['text':' Retrieve the singleton instance.','line_number':29,'multiline':False]
['text':' Given a list of root variables, start the distributed backwards pass from','line_number':32,'multiline':False]
['text':' these variables and accumulate all the gradients in the current autograd','line_number':33,'multiline':False]
['text':' context on each node. This method is used to kickoff distributed autograd','line_number':34,'multiline':False]
['text':' on a single node.','line_number':35,'multiline':False]
['text':' Given a send function to execute in the autograd engine, ensures we compute','line_number':41,'multiline':False]
['text':' dependencies once for this node and enqueues the send function for execute','line_number':42,'multiline':False]
['text':' in the engine.','line_number':43,'multiline':False]
['text':' This method is used to kick off the autograd computation on a node when it','line_number':44,'multiline':False]
['text':' receives gradients from the corresponding 'recv' method on another node.','line_number':45,'multiline':False]
['text':' The gradients are accumulated in the provided autograd context.','line_number':46,'multiline':False]
['text':' Number of backward passes currently running for the Distributed Engine.','line_number':52,'multiline':False]
['text':' Returns key-value pairs consisting of useful debugging information related','line_number':55,'multiline':False]
['text':' to distributed autograd.','line_number':56,'multiline':False]
['text':' Make sure this is a singleton.','line_number':65,'multiline':False]
['text':' Validates the input roots for the backward computations and retrieves the','line_number':69,'multiline':False]
['text':' appropriate root edges and corresponding gradients. Populates root_edges','line_number':70,'multiline':False]
['text':' with the appropriate gradient edges and grads with the gradients for each','line_number':71,'multiline':False]
['text':' edge.','line_number':72,'multiline':False]
['text':' Given the autograd context, root edges and grads, we compute dependencies','line_number':78,'multiline':False]
['text':' for the local node and fill out the provided GraphTask and GraphRoot with','line_number':79,'multiline':False]
['text':' appropriate information for the local autograd engine.','line_number':80,'multiline':False]
['text':' We also determine all leaf nodes(functions) in the graph and accumulate','line_number':81,'multiline':False]
['text':' them in outputEdges.','line_number':82,'multiline':False]
['text':' Given a pre-populated GraphTask and a root node, compute the backward pass','line_number':91,'multiline':False]
['text':' for the autograd graph until the graph task ready queue is empty.','line_number':92,'multiline':False]
['text':'','line_number':93,'multiline':False]
['text':' This method assumes that the appropriate GraphTask has already been','line_number':94,'multiline':False]
['text':' initialized appropriately. It will construct a local ready queue to','line_number':95,'multiline':False]
['text':' traverse the GraphTask instead of using the GraphTask embedded','line_number':96,'multiline':False]
['text':' cpu_ready_queue, this is because dist engine might run the same GraphTask','line_number':97,'multiline':False]
['text':' from different SendFunctions concurrently in different threads. The method','line_number':98,'multiline':False]
['text':' will only mark the GraphTask as completed when it needs to, which means it','line_number':99,'multiline':False]
['text':' might not mark as completed for every call as dist engine would like to','line_number':100,'multiline':False]
['text':' keep the GraphTask alive when it not receives all gradients.','line_number':101,'multiline':False]
['text':'','line_number':102,'multiline':False]
['text':' When `incrementOutstandingTasks=false`, the function does not increment','line_number':103,'multiline':False]
['text':' 'outstanding_tasks_' in the appropriate GraphTask. It is assumed we've','line_number':104,'multiline':False]
['text':' already done this before hand for this task (to ensure we don't pre-mark','line_number':105,'multiline':False]
['text':' this graph_task as completed). This is useful in the distributed autograd','line_number':106,'multiline':False]
['text':' case where we need to increment 'outstanding_tasks_' first to indicate the','line_number':107,'multiline':False]
['text':' local autograd engine the graph task is not completed until it receives the','line_number':108,'multiline':False]
['text':' signals from other workers over the network.','line_number':109,'multiline':False]
['text':'','line_number':110,'multiline':False]
['text':' XXX: calling this function assumes that we will have NO GPU nodetasks be','line_number':111,'multiline':False]
['text':' executed for the graph_task, the caller of this function need to ensure','line_number':112,'multiline':False]
['text':' this otherwise there will be undefined behaviors. A correct way to fix this','line_number':113,'multiline':False]
['text':' is to re-design the autograd engine so that GPU worker thread to behave the','line_number':114,'multiline':False]
['text':' same as CPU caller thread, record the operation/thread for the device, and','line_number':115,'multiline':False]
['text':' reuse it in backward.','line_number':116,'multiline':False]
['text':' TODO: 1. Add assert in the dist engine to ensure no GPU NodeTasks during','line_number':117,'multiline':False]
['text':' backward','line_number':118,'multiline':False]
['text':'       2. properly setup the thread local ready queue to enable reentrant','line_number':119,'multiline':False]
['text':'       backwards','line_number':120,'multiline':False]
['text':' Run the local autograd engine using the provided graphTask and graphRoot','line_number':125,'multiline':False]
['text':' and accumulate the gradients part 'outputEdges' in the provided autograd','line_number':126,'multiline':False]
['text':' context.','line_number':127,'multiline':False]
['text':' Run after the backward pass is done to appropriately cleanup structures.','line_number':134,'multiline':False]
['text':' Global thread to execute CPU continuations.','line_number':137,'multiline':False]
['text':' Set of autograd context_ids, which we have already initialized for','line_number':141,'multiline':False]
['text':' distributed autograd on this node (e.g.: already computed dependencies)','line_number':142,'multiline':False]
['text':' Reference to local autograd engine.','line_number':147,'multiline':False]
['text':' Ready queue used by the CPU thread in distributed engine.','line_number':150,'multiline':False]
['text':' See Note [GPU to CPU continuations]','line_number':151,'multiline':False]
['text':' See Note [GPU to CPU continuations]','line_number':154,'multiline':False]
['text':' Guard to clean up resources once the backward pass is done.','line_number':160,'multiline':False]
['text':' namespace autograd','line_number':174,'multiline':False]
['text':' namespace distributed','line_number':175,'multiline':False]
['text':' namespace torch','line_number':176,'multiline':False]
