['text':' Forward declarations.','line_number':12,'multiline':False]
['text':' As part of our distributed autograd implementation, whenever we receive an','line_number':15,'multiline':False]
['text':' RPC from a node, we add a 'RecvRpcBackward' autograd function to the','line_number':16,'multiline':False]
['text':' autograd graph. This is more or less a placeholder function that is used to','line_number':17,'multiline':False]
['text':' pass gradients to the remote host during the backward pass. The inputs to the','line_number':18,'multiline':False]
['text':' RPC function are the inputs to this autograd function.','line_number':19,'multiline':False]
['text':' Hold a weak reference to the autograd context to avoid circular','line_number':34,'multiline':False]
['text':' dependencies with the context (since it holds a reference to','line_number':35,'multiline':False]
['text':' RecvRpcBackward).','line_number':36,'multiline':False]
['text':' The worker id from which the RPC was received. During the backward pass,','line_number':39,'multiline':False]
['text':' we need to propagate the gradients to this workerId.','line_number':40,'multiline':False]
['text':' Device mapping for tensors sent over RPC.','line_number':43,'multiline':False]
['text':' namespace autograd','line_number':47,'multiline':False]
['text':' namespace distributed','line_number':48,'multiline':False]
['text':' namespace torch','line_number':49,'multiline':False]
