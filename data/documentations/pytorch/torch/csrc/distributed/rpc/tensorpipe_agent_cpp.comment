['text':' An environment variable along the lines of GLOO_ and NCCL_SOCKET_IFNAME that','line_number':27,'multiline':False]
['text':' allows the user to specify a device to bind to, instead of binding to the','line_number':28,'multiline':False]
['text':' address that the hostname resolves to.','line_number':29,'multiline':False]
['text':' If the deviceMap is overridden, use that instead.','line_number':44,'multiline':False]
['text':' namespace','line_number':154,'multiline':False]
['text':' The UV transport is implemented using standard TCP connections. It leverages','line_number':198,'multiline':False]
['text':' libuv (https://github.com/libuv/libuv) in order to be cross-platform.','line_number':199,'multiline':False]
['text':' The SHM implements connections using ringbuffers residing in anonymous shared','line_number':210,'multiline':False]
['text':' memory (plus UNIX domain sockets to bootstrap the connection and exchange','line_number':211,'multiline':False]
['text':' file descriptors). It is Linux-only due to some advanced features (O_TMPFILE,','line_number':212,'multiline':False]
['text':' eventfd, ...).','line_number':213,'multiline':False]
['text':' TENSORPIPE_HAS_SHM_TRANSPORT','line_number':216,'multiline':False]
['text':' The IBV transport sends data across using an InfiniBand queue pair, locally','line_number':227,'multiline':False]
['text':' copying data to and from a staging buffer (registered with libibverbs) and','line_number':228,'multiline':False]
['text':' issuing a RDMA write for transferring data across machines (plus a send for','line_number':229,'multiline':False]
['text':' acknowledging it). It bootstraps using a standard TCP connection to exchange','line_number':230,'multiline':False]
['text':' setup information. It is Linux-only.','line_number':231,'multiline':False]
['text':' TENSORPIPE_HAS_IBV_TRANSPORT','line_number':234,'multiline':False]
['text':' The basic channel is just a straightforward adapter wrapper that allows any','line_number':242,'multiline':False]
['text':' transport to be used as a channel.','line_number':243,'multiline':False]
['text':' The CMA channel uses the Linux cross-memory attach syscalls (process_vm_readv','line_number':254,'multiline':False]
['text':' and _writev), which allow one process to access the private memory of another','line_number':255,'multiline':False]
['text':' process (as long as they belong to the same user and other security','line_number':256,'multiline':False]
['text':' constraints are satisfied). It does, more or less, what GDB does when it's','line_number':257,'multiline':False]
['text':' attached to a running process.','line_number':258,'multiline':False]
['text':' TENSORPIPE_HAS_CMA_CHANNEL','line_number':261,'multiline':False]
['text':' The multiplexed UV channel encapsulates multiple UV transports (each with its','line_number':280,'multiline':False]
['text':' own event loop thread). Each channel will, in turn, contain multiple UV','line_number':281,'multiline':False]
['text':' connections, one for each of those contexts. When sending a tensor, its data','line_number':282,'multiline':False]
['text':' is split in equal chunks and each chunks is sent on a different connection','line_number':283,'multiline':False]
['text':' and thus driven by a different thread. This is needed to reach very high','line_number':284,'multiline':False]
['text':' bandwidths.','line_number':285,'multiline':False]
['text':' namespace','line_number':291,'multiline':False]
['text':'////////////////////////  MetricsTracker  /////////////////////////////////','line_number':293,'multiline':False]
['text':'//////////////////////  TensorpipeRpcAgent  /////////////////////////////////','line_number':309,'multiline':False]
['text':' Remove entry from timeoutMap_.','line_number':312,'multiline':False]
['text':' Already removed from the map by pollTimeoutRpcs(), no need to','line_number':317,'multiline':False]
['text':' process further.','line_number':318,'multiline':False]
['text':' Remove from messageId to timeout map as well.','line_number':338,'multiline':False]
['text':' In both cases, the returned value should be the value of isStaticGroupStr,','line_number':373,'multiline':False]
['text':' otherwise there is a discrepency with initialization among one of the','line_number':374,'multiline':False]
['text':' members','line_number':375,'multiline':False]
['text':' check the static group attribute against store','line_number':414,'multiline':False]
['text':' collect worker names','line_number':417,'multiline':False]
['text':' Initialize the time-series metrics tracking map','line_number':420,'multiline':False]
['text':' Register transports','line_number':436,'multiline':False]
['text':' Assign priorities in reverse order of occurrence in the vector, so that','line_number':445,'multiline':False]
['text':' a transport that comes before another receives a higher priority.','line_number':446,'multiline':False]
['text':' Register channels','line_number':467,'multiline':False]
['text':' Assign priorities in reverse order of occurrence in the vector, so','line_number':476,'multiline':False]
['text':' that a channel that comes before another receives a higher priority.','line_number':477,'multiline':False]
['text':' Store our own url.','line_number':494,'multiline':False]
['text':' Start the Timeout Thread','line_number':509,'multiline':False]
['text':' This is expected.','line_number':525,'multiline':False]
['text':' Accept the next connection request','line_number':534,'multiline':False]
['text':' Arm for server read','line_number':544,'multiline':False]
['text':' FIXME This does some unpickling, which could be a bit expensive:','line_number':584,'multiline':False]
['text':' perhaps it would be best to perform it inside the worker threads?','line_number':585,'multiline':False]
['text':' devices ','line_number':693,'multiline':True]
['text':' This is expected.','line_number':721,'multiline':False]
['text':' Arm for next read','line_number':731,'multiline':False]
['text':' Defer user RPC UDF run to thread pool','line_number':741,'multiline':False]
['text':' Instead of creating a MultiStreamGuard here, the ctx is passed','line_number':753,'multiline':False]
['text':' to the callback and the MultiStreamGuard is created there,','line_number':754,'multiline':False]
['text':' because subsequent processing can switch threads due to 1)','line_number':755,'multiline':False]
['text':' waiting for RRef arguments to become ready 2) async_execution.','line_number':756,'multiline':False]
['text':' Besides, the `ctx` also needs to be propagated to','line_number':757,'multiline':False]
['text':' `process***Call` methods to synchronize CUDA streams there','line_number':758,'multiline':False]
['text':' to make sure that we fetch the correct value from `to_here()`','line_number':759,'multiline':False]
['text':' call.','line_number':760,'multiline':False]
['text':' unused ','line_number':763,'multiline':True]
['text':' See if we already have a connection to this address or not','line_number':813,'multiline':False]
['text':' An instance of ClientPipe cannot be copied or moved as it contains a','line_number':816,'multiline':False]
['text':' mutex, and to force in-place construction in GCC 5 we need piecewise','line_number':817,'multiline':False]
['text':' construction in order to work around an issue.','line_number':818,'multiline':False]
['text':' Get devices for tensors in the request message. This can throw if device','line_number':842,'multiline':False]
['text':' maps are not configured properly for this request.','line_number':843,'multiline':False]
['text':' If deviceMap is specified, use that instead.','line_number':849,'multiline':False]
['text':' unused ','line_number':857,'multiline':True]
['text':' Use the default RPC timeout if no timeout is specified for this send call','line_number':864,'multiline':False]
['text':' We only add to the timeoutMap_ if the timeout is not 0. Per our','line_number':870,'multiline':False]
['text':' documentation, a user-provided timeout of 0 indicates the RPC should never','line_number':871,'multiline':False]
['text':' expire (infinite timeout), so there is no need to track it in the','line_number':872,'multiline':False]
['text':' timeoutMap_.','line_number':873,'multiline':False]
['text':' Compute the expiration time for this message based on the timeout','line_number':876,'multiline':False]
['text':' Add the Future to the right vector in the timeoutMap_','line_number':879,'multiline':False]
['text':' This is expected.','line_number':911,'multiline':False]
['text':' This is expected.','line_number':935,'multiline':False]
['text':' Identify future response message by message ID','line_number':947,'multiline':False]
['text':' A read error will lead all following callbacks to be','line_number':957,'multiline':False]
['text':' invoked with error, and shouldn't reach here.','line_number':958,'multiline':False]
['text':' Remove entry from timeoutMap_.','line_number':971,'multiline':False]
['text':' When an error occurs on a pipe all pending operations will be aborted and','line_number':995,'multiline':False]
['text':' all callbacks invoked with error, hence we immediately flush all future','line_number':996,'multiline':False]
['text':' messages belonging to this pipe.','line_number':997,'multiline':False]
['text':' Remove entry from timeoutMap_.','line_number':1008,'multiline':False]
['text':' We sleep until the earliest expiring RPC in the timeoutMap_. We must','line_number':1017,'multiline':False]
['text':' also ensure that we sleep while the map is empty, and we exit sleeping','line_number':1018,'multiline':False]
['text':' if the RPC Agent has been shutdown.','line_number':1019,'multiline':False]
['text':' Move all these futures to a separate vector so we can process them','line_number':1036,'multiline':False]
['text':' outside the lock.','line_number':1037,'multiline':False]
['text':' We can safely remove this key from the timeoutMap_ since all these','line_number':1041,'multiline':False]
['text':' futures will be processed.','line_number':1042,'multiline':False]
['text':' Remove from messageIdToTimeout map.','line_number':1046,'multiline':False]
['text':' Set an error on futures added to the timedOutFutures vector. We do this','line_number':1051,'multiline':False]
['text':' outside the lock to prevent potential lock-order-inversions by callbacks','line_number':1052,'multiline':False]
['text':' triggered by the setError call.','line_number':1053,'multiline':False]
['text':' local worker ActiveCallCount is 0 at this point and we will shutdown','line_number':1066,'multiline':False]
['text':' (any future calls will be dropped)','line_number':1067,'multiline':False]
['text':' Remove this agent's WorkerInfo from store','line_number':1070,'multiline':False]
['text':' Set internal variable to be used during destructor','line_number':1073,'multiline':False]
['text':' TODO: Remove join()','line_number':1077,'multiline':False]
['text':' unused ','line_number':1078,'multiline':True]
['text':' This method behaves like a barrier, as it can only return once all workers','line_number':1085,'multiline':False]
['text':' have no more requests pending, including "nested" requests (triggered from','line_number':1086,'multiline':False]
['text':' within the remote code of another call) and "follow-up" requests (triggered','line_number':1087,'multiline':False]
['text':' from the callback of a future).','line_number':1088,'multiline':False]
['text':' It is enough to wait for there to be no more active client calls, since','line_number':1092,'multiline':False]
['text':' each server call corresponds to a client call for some other worker.','line_number':1093,'multiline':False]
['text':' We'd like to immediately proceed with the allreduce, but it's a call','line_number':1096,'multiline':False]
['text':' that may block for some time, as it waits for other workers to also','line_number':1097,'multiline':False]
['text':' complete all their active client calls. While we call allreduce we must','line_number':1098,'multiline':False]
['text':' hold the mutex, or else the count we send to other workers may get','line_number':1099,'multiline':False]
['text':' stale (e.g., if some nested call happens in the meantime). But we can't','line_number':1100,'multiline':False]
['text':' hold the lock for an indeterminately long time, as that would block','line_number':1101,'multiline':False]
['text':' other operations (e.g., send). Thus we must release the lock and only','line_number':1102,'multiline':False]
['text':' re-acquire it when all workers are ready to proceed with the allreduce.','line_number':1103,'multiline':False]
['text':' We perform this synchronization using a barrier.','line_number':1104,'multiline':False]
['text':' At this point, the count may have become non-zero again. We can't wait','line_number':1111,'multiline':False]
['text':' for those calls to complete as other workers are waiting for us in the','line_number':1112,'multiline':False]
['text':' allreduce and we would block them. Thus we send our count even if it is','line_number':1113,'multiline':False]
['text':' non-zero and if anyone (be it us or another worker) has a non-zero','line_number':1114,'multiline':False]
['text':' count we'll just do another round.','line_number':1115,'multiline':False]
['text':' FIXME Isn't it too verbose for a library to print logs in normal operation?','line_number':1138,'multiline':False]
['text':' Join the Timeout Thread','line_number':1141,'multiline':False]
['text':' This will close all the pipes and listeners, invoke all callbacks with','line_number':1149,'multiline':False]
['text':' errors, turn down the I/O event loops and wait for everything to terminate.','line_number':1150,'multiline':False]
['text':' NOTE: We need to call waitWorkComplete in the end after we have shutdown','line_number':1155,'multiline':False]
['text':' all listeners for Tensorpipe. This is to drain any already accepted work','line_number':1156,'multiline':False]
['text':' in the ThreadPool. If this is done before we shutdown the listeners,','line_number':1157,'multiline':False]
['text':' additional work could be added after this call and before we shutdown','line_number':1158,'multiline':False]
['text':' listeners. This work would continue executing in the threadpool and might','line_number':1159,'multiline':False]
['text':' cause issues during shutdown of the system.','line_number':1160,'multiline':False]
['text':' Rank with workerInfo is joining the group, update internal mappings','line_number':1232,'multiline':False]
['text':' TODO: we should get nodeAddrStr in the joining process, then pass in as','line_number':1238,'multiline':False]
['text':' an argument rather than getting from store each time','line_number':1239,'multiline':False]
['text':' TODO: clean up mutex for devices_ usage','line_number':1250,'multiline':False]
['text':' Add devices that have not been added yet','line_number':1251,'multiline':False]
['text':' remove reverse device maps that are no longer used','line_number':1262,'multiline':False]
['text':' remove devices that are no longer used','line_number':1272,'multiline':False]
['text':' Include the averages for each time series metric. This is just the GIL','line_number':1295,'multiline':False]
['text':' Wait Time for now.','line_number':1296,'multiline':False]
['text':' Completing the future will run its callbacks, which could execute','line_number':1366,'multiline':False]
['text':' arbitrary user code. To prevent blocking or stalling the TensorPipe event','line_number':1367,'multiline':False]
['text':' loops, we defer this to a worker thread.','line_number':1368,'multiline':False]
['text':' The future's callbacks may schedule further RPCs, increasing the count.','line_number':1378,'multiline':False]
['text':' Thus we must decrease it after completing the future, otherwise it may','line_number':1379,'multiline':False]
['text':' briefly dip to zero and trick join into thinking all work is done.','line_number':1380,'multiline':False]
['text':' Completing the future will run its callbacks, which could execute','line_number':1390,'multiline':False]
['text':' arbitrary user code. To prevent blocking or stalling the TensorPipe event','line_number':1391,'multiline':False]
['text':' loops, we defer this to a worker thread.','line_number':1392,'multiline':False]
['text':' The future's callbacks may schedule further RPCs, increasing the count.','line_number':1398,'multiline':False]
['text':' Thus we must decrease it after completing the future, otherwise it may','line_number':1399,'multiline':False]
['text':' briefly dip to zero and trick join into thinking all work is done.','line_number':1400,'multiline':False]
['text':' namespace rpc','line_number':1475,'multiline':False]
['text':' namespace distributed','line_number':1476,'multiline':False]
['text':' namespace torch','line_number':1477,'multiline':False]
['text':' USE_TENSORPIPE','line_number':1479,'multiline':False]
