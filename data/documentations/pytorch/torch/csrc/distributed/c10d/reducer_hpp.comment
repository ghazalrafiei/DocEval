['text':' Collect runtime stats once for every kDDPRuntimeLoggingSampleRate iterations.','line_number':31,'multiline':False]
['text':' Forward declaration','line_number':34,'multiline':False]
['text':' Local accumulator type for a single bucket.','line_number':37,'multiline':False]
['text':' The constructor takes a list of variables (i.e. parameters) for this','line_number':46,'multiline':False]
['text':' process's single model replica (as DDP assumes single-process','line_number':47,'multiline':False]
['text':' single-device). The bucket assignment for this reducer, `bucket_indices`,','line_number':48,'multiline':False]
['text':' is specified as a list of buckets, each of which is specified as a list of','line_number':49,'multiline':False]
['text':' indices into the bucket's `variables` list.','line_number':50,'multiline':False]
['text':' To (re-)initialize bucket assignment, pass a list of buckets, each of','line_number':65,'multiline':False]
['text':' which is specified by a list of indices in the bucket's `variables` list.','line_number':66,'multiline':False]
['text':' This function performs validation that the variables within a bucket','line_number':67,'multiline':False]
['text':' all live on the same device and have the same dimensionality.','line_number':68,'multiline':False]
['text':' This function is called when the forward function has produced an output,','line_number':73,'multiline':False]
['text':' and the user wishes to reduce gradients in the backwards pass.','line_number':74,'multiline':False]
['text':' If they don't, and wish to accumulate gradients before reducing them,','line_number':75,'multiline':False]
['text':' a call to this function can simply be omitted.','line_number':76,'multiline':False]
['text':' Called at the beginning of forward() inside DistributedDataParallel,','line_number':79,'multiline':False]
['text':' right now it captures the starting time of forward in each iteration.','line_number':80,'multiline':False]
['text':' Returns the relative time in nanoseconds when gradients were ready,','line_number':83,'multiline':False]
['text':' with respect to the time `prepare_for_backward` was called. The','line_number':84,'multiline':False]
['text':' vector is for parameters for a single model replica.','line_number':85,'multiline':False]
['text':' Registers a hook to the reducer. The hook is `CommHookInterface`','line_number':90,'multiline':False]
['text':' type to allow both Python and CPP hooks. This function can only','line_number':91,'multiline':False]
['text':' be called once before calling backward.','line_number':92,'multiline':False]
['text':' Cannot combine with the call of `register_builtin_comm_hook`.','line_number':93,'multiline':False]
['text':' Registers a built-in C++ comm hook to the reducer. This function can only','line_number':96,'multiline':False]
['text':' be called once before calling backward.','line_number':97,'multiline':False]
['text':' Cannot combine with the call of `register_comm_hook`.','line_number':98,'multiline':False]
['text':' Informs reducer that optimizer is running in backward, so gradients','line_number':101,'multiline':False]
['text':' don't need to be copied from buckets as the optimizer would've already','line_number':102,'multiline':False]
['text':' been applied.','line_number':103,'multiline':False]
['text':' Runs allreduce or installed communication hook given GradBucket instance.','line_number':108,'multiline':False]
['text':' Runs default allreduce hook.','line_number':112,'multiline':False]
['text':' Returns gradient buckets in sequential order of buckets_. This is the order','line_number':116,'multiline':False]
['text':' in which buckets are reduced across processes. If return_zero_tensors=true,','line_number':117,'multiline':False]
['text':' will return zero tensors of the same shape instead of the true tensors.','line_number':118,'multiline':False]
['text':' Rebuild buckets based on rebuilt_params_ and rebuilt_param_indices_','line_number':122,'multiline':False]
['text':' according to when tensors received grads in the backward pass.','line_number':123,'multiline':False]
['text':' TODO this function makes broadcast communication call and','line_number':124,'multiline':False]
['text':' could be overlapped with next forward() call, thus','line_number':125,'multiline':False]
['text':' it could be async. Will make it async when rebuilding buckets for','line_number':126,'multiline':False]
['text':' find_unused_parameters = true case, as we could rebuild buckets more than','line_number':127,'multiline':False]
['text':' once for find_unused_parameters = true case, where subgraphs are trained','line_number':128,'multiline':False]
['text':' and parameter indices order may change more frequently.','line_number':129,'multiline':False]
['text':' For find_unused_parameters = false case, buckets are only rebuilt once,','line_number':130,'multiline':False]
['text':' the performance cost is negligible. Returns true if the buckets were','line_number':131,'multiline':False]
['text':' rebuilt.','line_number':132,'multiline':False]
['text':' Install futures that should be awaited at end of backwards. Currently these','line_number':137,'multiline':False]
['text':' are only used by user-defined custom buffer reduction hooks, but can be','line_number':138,'multiline':False]
['text':' generalized to any user-originating futures that need to be awaited.','line_number':139,'multiline':False]
['text':' Returns true if we should rebuild buckets, else false. We only rebuild','line_number':142,'multiline':False]
['text':' buckets once after the first iteration and never rebuild them if','line_number':143,'multiline':False]
['text':' find_unused_parameters_.','line_number':144,'multiline':False]
['text':' Pushes all parameters to be rebuilt.','line_number':149,'multiline':False]
['text':' Creates and sets ForwardPassWorkHandle given a Work and the','line_number':152,'multiline':False]
['text':' corresponding tensor being reduced.','line_number':153,'multiline':False]
['text':' Retrieve on-device tensors used to track locally unused parameters. It is','line_number':158,'multiline':False]
['text':' a tensor where index i = 1 if the Variable with that index has been used.','line_number':159,'multiline':False]
['text':' An function for users to set sample_rate of collecting','line_number':162,'multiline':False]
['text':' runtime stats. The time stats will be recorded for the','line_number':163,'multiline':False]
['text':' first 10 iterations, after 10 iterations time stats will be','line_number':164,'multiline':False]
['text':' recorded once every "sample_rate" training iterations.','line_number':165,'multiline':False]
['text':' Specify the training graph is static.','line_number':168,'multiline':False]
['text':' Delay all reduce to be after all gradients' calculation is complete.','line_number':171,'multiline':False]
['text':' Weak reference to associated DDP logger. The reference is weak to avoid','line_number':176,'multiline':False]
['text':' refcycle between reducer and logger.','line_number':177,'multiline':False]
['text':' When graph is not explicitly set by user as static and has unused','line_number':180,'multiline':False]
['text':' parameters, this will return whether the graph has been static until the','line_number':181,'multiline':False]
['text':' current iteration, which means unused params set has not changed.','line_number':182,'multiline':False]
['text':' Removes autograd hooks registered by the Reducer on the model parameters.','line_number':185,'multiline':False]
['text':' Checks whether or not the reducer has finalized the current backward','line_number':188,'multiline':False]
['text':' iteration.','line_number':189,'multiline':False]
['text':' Updates the underlying process group used by DDP with the new process','line_number':192,'multiline':False]
['text':' group.','line_number':193,'multiline':False]
['text':' Resets reducer state.','line_number':197,'multiline':False]
['text':' Forward declaration.','line_number':201,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':206,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':208,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':210,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':212,'multiline':False]
['text':' NOLINT(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':216,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':217,'multiline':False]
['text':' NOLINT(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':220,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':222,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':224,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':226,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':229,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':231,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':233,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':235,'multiline':False]
['text':' Previous iteration's unused params, used for checking if unused parameters','line_number':237,'multiline':False]
['text':' change between iterations. Only filled during the first backwards call.','line_number':238,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':239,'multiline':False]
['text':' Whether graph is static or not. When user does not explicitly set static','line_number':241,'multiline':False]
['text':' graph, the only possible dynamism is set of unused parameters changing','line_number':242,'multiline':False]
['text':' between iterations which is tracked by this flag.','line_number':243,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':244,'multiline':False]
['text':' Locally used parameter maps indicating if parameters are used locally','line_number':246,'multiline':False]
['text':' during the current iteration or no_sync session if no_sync is on.','line_number':247,'multiline':False]
['text':' Each map is a one-dim int32 tensor of number of parameters. These tensors','line_number':248,'multiline':False]
['text':' are marked in autograd_hook to indicate the corresponding param has been','line_number':249,'multiline':False]
['text':' used, and get allreduced in the end of backward step of current iteration','line_number':250,'multiline':False]
['text':' or no_sync session for figuring out the globally unused parameters.','line_number':251,'multiline':False]
['text':'','line_number':252,'multiline':False]
['text':' local_used_map_:     CPU tensor for bookkeeping locally used params','line_number':253,'multiline':False]
['text':' local_used_map_dev_: dev tensor for reducing globally unused params','line_number':254,'multiline':False]
['text':' Indicate that reduction is done and D2H copy is done as well.','line_number':257,'multiline':False]
['text':' Weak pointer to associated DDP logger.','line_number':260,'multiline':False]
['text':' List of futures installed by Reducer::install_futures that should be','line_number':262,'multiline':False]
['text':' awaited at the end of backwards pass.','line_number':263,'multiline':False]
['text':' Mixed precision parameter dtype for bucket type checking.','line_number':266,'multiline':False]
['text':' Work handle for allreduce on local_used_map_','line_number':269,'multiline':False]
['text':' Returns list of model parameters corresponding to the given bucket.','line_number':284,'multiline':False]
['text':' bucket_index is a key to cache after buckets are rebuilt, after which this','line_number':285,'multiline':False]
['text':' mapping never changes.','line_number':286,'multiline':False]
['text':' Asserts that the reduction for the previous iteration has finished before','line_number':291,'multiline':False]
['text':' rebuilding buckets or kicking off the next one.','line_number':292,'multiline':False]
['text':' Broadcast rebuilt buckets from rank 0 to other ranks before initializing','line_number':295,'multiline':False]
['text':' the buckets','line_number':296,'multiline':False]
['text':' We'd like to use DistAutogradContext::GradCallback here but dist autograd','line_number':299,'multiline':False]
['text':' doesn't exist under Windows. So we just directly use the concrete type but','line_number':300,'multiline':False]
['text':' to preserve and enforce our original intent we do a static assert when dist','line_number':301,'multiline':False]
['text':' autograd is available.','line_number':302,'multiline':False]
['text':' This function is called inside `initialize_buckets()`. It initializes both','line_number':314,'multiline':False]
['text':' `bucket_views_in` and `bucket_views_out` with views for each variable's','line_number':315,'multiline':False]
['text':' gradient into the bucket's flattened `gradients` tensor. Views serve as','line_number':316,'multiline':False]
['text':' entry points to `copy_()` each grad's data in/out of the flattened','line_number':317,'multiline':False]
['text':' `gradients` tensor.','line_number':318,'multiline':False]
['text':' This function is called inside `finalize_backward`, it happens only if','line_number':321,'multiline':False]
['text':' DDP communication hook was registered to recreate just bucket_views_out','line_number':322,'multiline':False]
['text':' with the result of `future_work`.','line_number':323,'multiline':False]
['text':' If gradient_as_bucket_view_ is false, after allreduce buckets,','line_number':326,'multiline':False]
['text':' copy bucket results back to grads.','line_number':327,'multiline':False]
['text':' Check layout of grad and bucket_view before copying the grad to bucket.','line_number':333,'multiline':False]
['text':' A bucket contains [1..N] gradients to be reduced, where the gradients','line_number':336,'multiline':False]
['text':' have the same dtype and device.','line_number':337,'multiline':False]
['text':' Coalescing gradients together before reducing can result in lower overhead','line_number':338,'multiline':False]
['text':' and/or faster time to completion. Coalescing requires the constituent','line_number':339,'multiline':False]
['text':' gradients to have the same dtype and device, and the resulting flattened','line_number':340,'multiline':False]
['text':' tensor uses that common dtype and device. The flattened tensor is filled','line_number':341,'multiline':False]
['text':' as the corresponding gradients are computed (triggered by autograd hooks),','line_number':342,'multiline':False]
['text':' and the buckets are reduced in a predetermined order consistent across','line_number':343,'multiline':False]
['text':' processes.','line_number':344,'multiline':False]
['text':' Gradients of the bucket flattened into a 1-dimensional tensor','line_number':346,'multiline':False]
['text':' Views into the `gradients` tensor for each individual gradient','line_number':349,'multiline':False]
['text':' Each view is created with layout (size and stride) matching the','line_number':350,'multiline':False]
['text':' gradient's expected layout (see the "Gradient Layout Contract" in','line_number':351,'multiline':False]
['text':' torch/csrc/autograd/functions/accumulate_grad.h).','line_number':352,'multiline':False]
['text':' `bucket_views_in[i].copy_(grad)` and `grad.copy_(bucket_views_out[i])`','line_number':353,'multiline':False]
['text':' provide convenient ways to copy gradient data in/out of `gradients`,','line_number':354,'multiline':False]
['text':' respectively.','line_number':355,'multiline':False]
['text':' We keep both `bucket_views_in` and `bucket_views_out` because','line_number':356,'multiline':False]
['text':' registering a DDP communication hook may re-initialize','line_number':357,'multiline':False]
['text':' `bucket_views_out` with the value of the hook's `future_work` but we','line_number':358,'multiline':False]
['text':' still need separate views into the bucket's original flattened gradient','line_number':359,'multiline':False]
['text':' to copy in gradient data.','line_number':360,'multiline':False]
['text':' Variables whose gradients are held in this bucket','line_number':364,'multiline':False]
['text':' We use refcounted tensors here so that we can easily unflatten the','line_number':365,'multiline':False]
['text':' bucket's flattened `gradients` tensor into the participating variables','line_number':366,'multiline':False]
['text':' after reduction has completed.','line_number':367,'multiline':False]
['text':' Per-variable offset/length into the flattened `gradients` tensor and','line_number':370,'multiline':False]
['text':' the corresponding `GradBucket` instance for communication hooks','line_number':371,'multiline':False]
['text':' Per-variable sizes slicing into the bucket's `gradients` tensor','line_number':375,'multiline':False]
['text':' Number of gradients left to be computed before the bucket is ready to','line_number':378,'multiline':False]
['text':' be reduced','line_number':379,'multiline':False]
['text':' Global indices of participating variables in the bucket','line_number':382,'multiline':False]
['text':' Future work handle for DDP communication hook','line_number':385,'multiline':False]
['text':' If no hook is registered, a temporary vanilla allreduce hook is used.','line_number':386,'multiline':False]
['text':' If this bucket should expect a single sparse gradient','line_number':389,'multiline':False]
['text':' If `true`, then this implies that `bucket.variables.size() == 1`.','line_number':390,'multiline':False]
['text':' Sparse indices tensor','line_number':393,'multiline':False]
['text':' TODO(@pietern)','line_number':396,'multiline':False]
['text':' Memory copies from gradient tensors into the bucket are potentially','line_number':397,'multiline':False]
['text':' done on different CUDA streams. We record an event for every copy','line_number':398,'multiline':False]
['text':' so that we can synchronize with them prior to kicking off the reduction.','line_number':399,'multiline':False]
['text':' std::vector<at::cuda::CUDAEvent> events;','line_number':400,'multiline':False]
['text':' A variable locator locates a particular variable in the reducer's buckets','line_number':405,'multiline':False]
['text':' Index of the bucket containing the variable in the `buckets_` vector','line_number':407,'multiline':False]
['text':' Index of the variable in the bucket, which may be used consistently','line_number':409,'multiline':False]
['text':' across `bucket_views_in`, `bucket_views_out`, `variables`, `offsets`,','line_number':410,'multiline':False]
['text':' `lengths`, `sizes_vec`, and `variable_indices` in `Bucket`','line_number':411,'multiline':False]
['text':' Map the index of a variable to its location in the bucket structure.','line_number':421,'multiline':False]
['text':' track the number of iterations to synchronize grads in training so far.','line_number':424,'multiline':False]
['text':' track distinct iteration of backward call. This is distinct from','line_number':426,'multiline':False]
['text':' num_iterations_, for example in the case of multiple forward before','line_number':427,'multiline':False]
['text':' backward.','line_number':428,'multiline':False]
['text':' whether the first autograd hook for a distinct backward pass has been','line_number':430,'multiline':False]
['text':' called.','line_number':431,'multiline':False]
['text':' track the number of buckets that have been ready for','line_number':433,'multiline':False]
['text':' communication calls like allReduce or communication hooks.','line_number':434,'multiline':False]
['text':' Timing information.','line_number':437,'multiline':False]
['text':' We collect the relative timestamp of every gradient being ready','line_number':441,'multiline':False]
['text':' when executing autograd. This can be used to derive a timeline of','line_number':442,'multiline':False]
['text':' the point in time buckets were ready, or ideal bucket assignment/ordering.','line_number':443,'multiline':False]
['text':' Following variables are to help build dynamic bucket order','line_number':458,'multiline':False]
['text':' The shared_ptr is to hold the context instance.','line_number':467,'multiline':False]
['text':' A struct containing work handle and tensor for allreduce scheduled in','line_number':476,'multiline':False]
['text':' forward pass, if applicable.','line_number':477,'multiline':False]
['text':' whether we should divide by the initial world_size or the no. of','line_number':481,'multiline':False]
['text':' remaining DDP ranks.','line_number':482,'multiline':False]
['text':' Handle for the currently scheduled allreduce in the forward pass, if','line_number':486,'multiline':False]
['text':' applicable.','line_number':487,'multiline':False]
['text':' Division factor for reduction of gradients.','line_number':490,'multiline':False]
['text':' Equal to the process group size, with an exception of handling uneven','line_number':491,'multiline':False]
['text':' input.','line_number':492,'multiline':False]
['text':' Key: size_t (index), Value: the number of times that a variable's','line_number':497,'multiline':False]
['text':' autograd_hook() should be triggered before marking this variable's grad as','line_number':498,'multiline':False]
['text':' ready for communication. Map will not change after 1st iteration.','line_number':499,'multiline':False]
['text':' Key: size_t (index), Value: the number of times that a variable's','line_number':501,'multiline':False]
['text':' autograd_hook() are left to be triggered before marking this variable's','line_number':502,'multiline':False]
['text':' grad as ready for communication. Map will change after 1st iteration to','line_number':503,'multiline':False]
['text':' track a grad is ready for communication or not.','line_number':504,'multiline':False]
['text':' reset counting for buckets before backward starts','line_number':508,'multiline':False]
['text':' search unused parameters beore backward starts','line_number':510,'multiline':False]
['text':' kick off all reduce for the ready bucket','line_number':514,'multiline':False]
['text':' kick off all reduce to local used map, it can help find global unused','line_number':516,'multiline':False]
['text':' parameters','line_number':517,'multiline':False]
['text':' initialize locally used parameter maps','line_number':519,'multiline':False]
['text':' get current cuda stream','line_number':521,'multiline':False]
['text':' comm_hook_ is used to access the DDP communication hook if registered.','line_number':527,'multiline':False]
['text':' Sparse metadata contains the indices that will be used','line_number':530,'multiline':False]
['text':' when calling into sparse allreduce.','line_number':531,'multiline':False]
['text':' This is only used in the sparse allreduce collective calls','line_number':532,'multiline':False]
['text':' Debug level setting. It is parsed once when Reducer is constructed, and','line_number':535,'multiline':False]
['text':' remains the same across a single invocation of DDP training.','line_number':536,'multiline':False]
['text':' Mapping of variable index to fully qualified name of model to notify users','line_number':538,'multiline':False]
['text':' about errors when certain parameters do not get gradient.','line_number':539,'multiline':False]
['text':' Variable indices stored sequentially in order of when the gradient is ready','line_number':541,'multiline':False]
['text':' for the current backwards pass.','line_number':542,'multiline':False]
['text':' Bytes capacity of first bucket, can be configured by user','line_number':544,'multiline':False]
['text':' Per iteration set of parameter indices that have been marked ready.','line_number':546,'multiline':False]
['text':' Retrieves parameter names that have not been marked as ready as part of','line_number':548,'multiline':False]
['text':' previous iteration.','line_number':549,'multiline':False]
['text':' Retrieves parameter indices that have not been marked as ready as part of','line_number':551,'multiline':False]
['text':' previous iteration.','line_number':552,'multiline':False]
['text':' Raises appropriate error if mark_variable_ready is called on the same','line_number':554,'multiline':False]
['text':' variable twice, which is unexpected.','line_number':555,'multiline':False]
['text':' Retrieves parameter corresponding to the given VariableIndex.','line_number':557,'multiline':False]
['text':' Cached bucket index to model parameter mapping. Populated after buckets','line_number':560,'multiline':False]
['text':' are rebuilt after which this mapping is static.','line_number':561,'multiline':False]
['text':' This is equivalent to take_tensors but returns indices into the','line_number':569,'multiline':False]
['text':' tensor list argument for bucket assignment. Also, it is aware','line_number':570,'multiline':False]
['text':' of device placement and will not allow buckets to span devices.','line_number':571,'multiline':False]
['text':' The index of tensors[i] assigned to bucket is tensor_indices[i],','line_number':572,'multiline':False]
['text':' when tensor_indices is empty, the index of tensors[i] assigned to','line_number':573,'multiline':False]
['text':' bucket is i.','line_number':574,'multiline':False]
['text':' Verify models across all processes are the same as model on rank 0 with','line_number':583,'multiline':False]
['text':' respect to no. of params and matching dtype/size/layout.','line_number':584,'multiline':False]
['text':' namespace c10d','line_number':589,'multiline':False]
