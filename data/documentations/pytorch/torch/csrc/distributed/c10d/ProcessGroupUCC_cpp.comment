['text':' bitwise or','line_number':68,'multiline':False]
['text':' bitwise and','line_number':71,'multiline':False]
['text':' Sharing UCC communicator among multiple PGs to save resource.','line_number':90,'multiline':False]
['text':' Using allgatherv to achieve allgather, without flattening the list of','line_number':92,'multiline':False]
['text':' (potentially non-contiguous) tensors.','line_number':93,'multiline':False]
['text':' TORCH_UCC_BLOCKING_WAIT allowed syntax:','line_number':99,'multiline':False]
['text':' - TORCH_UCC_BLOCKING_WAIT=none --> blocking wait completely disabled','line_number':100,'multiline':False]
['text':' - TORCH_UCC_BLOCKING_WAIT=all --> blocking wait completely enabled','line_number':101,'multiline':False]
['text':' - TORCH_UCC_BLOCKING_WAIT=allreduce,send,recv --> blocking wait enabled','line_number':102,'multiline':False]
['text':'                                                   on selected operations','line_number':103,'multiline':False]
['text':' Supported operations:','line_number':104,'multiline':False]
['text':' [allgather,allgather_base,allreduce,alltoall,broadcast,','line_number':105,'multiline':False]
['text':'  gather,reduce,reduce_scatter,scatter,send,recv]','line_number':106,'multiline':False]
['text':' namespace','line_number':148,'multiline':False]
['text':' default configuration','line_number':151,'multiline':False]
['text':' read all torch_ucc env. variables and update the map','line_number':159,'multiline':False]
['text':' barrier is always blocking','line_number':172,'multiline':False]
['text':' TODO: check cuda case','line_number':206,'multiline':False]
['text':' status_ <= 0 to avoid listing all possible status codes.  The main thread','line_number':237,'multiline':False]
['text':' needs to be unblocked when UCC (in progress thread) returns success (== 0)','line_number':238,'multiline':False]
['text':' or any error code (< 0).','line_number':239,'multiline':False]
['text':' unused ','line_number':250,'multiline':True]
['text':' block user stream','line_number':256,'multiline':False]
['text':' wait for complete.  For blocking case, the main thread will be blocked in','line_number':262,'multiline':False]
['text':' this loop until the progress thread changes the status of this request.','line_number':263,'multiline':False]
['text':' If timeout occurs, UCC will return UCC_ERR_TIMEOUT as the status.  The','line_number':264,'multiline':False]
['text':' main thread will throw out the exception then. There is no "abort"','line_number':265,'multiline':False]
['text':' function in UCC currently.','line_number':266,'multiline':False]
['text':' manually call profiling end callbacks if they are set,','line_number':270,'multiline':False]
['text':' since progress thread does not own WorkUCC','line_number':271,'multiline':False]
['text':' Throw an error','line_number':285,'multiline':False]
['text':' Find the highest id.','line_number':378,'multiline':False]
['text':' Prepare comm_id (static variable) to the next id.','line_number':388,'multiline':False]
['text':' TODO: report exact op type or','line_number':543,'multiline':False]
['text':' id?','line_number':544,'multiline':False]
['text':' Perform health check by initializing dummy communicators and destroying','line_number':597,'multiline':False]
['text':' them. This will help indicate any UCC/UCX-related issues prior to the','line_number':598,'multiline':False]
['text':' first collective. Run it in a separate thread and wait on CV to handle','line_number':599,'multiline':False]
['text':' timeouts so that if there are hangs, the main thread can still run','line_number':600,'multiline':False]
['text':' correctly.','line_number':601,'multiline':False]
['text':' Return CUDA device with ordinal given by input rank.','line_number':638,'multiline':False]
['text':' Run health check in a separate thread and wait on CV to handle timeouts.','line_number':648,'multiline':False]
['text':' This design allows us to handle hangs.','line_number':649,'multiline':False]
['text':' When size_ is 1, there is no need to do any communication at all.','line_number':651,'multiline':False]
['text':' Mark ucc health check as complete.','line_number':692,'multiline':False]
['text':' Notify main thread the health check is complete.','line_number':700,'multiline':False]
['text':' Populate exception ptr.','line_number':705,'multiline':False]
['text':' Unblock waiting main thread which will report exception.','line_number':707,'multiline':False]
['text':' Unknown exceptions will just cause the program to terminate.','line_number':709,'multiline':False]
['text':' We don't need to join the thread, just need to verify health check via the','line_number':712,'multiline':False]
['text':' CV. Hence we detach the thread here.','line_number':713,'multiline':False]
['text':' NOLINT','line_number':714,'multiline':False]
['text':' If there is no exception, the likely culprit is a timeout/hang','line_number':729,'multiline':False]
['text':' Store references to outputs to be used by result','line_number':785,'multiline':False]
['text':' Add a callback that runs profiling end callbacks','line_number':828,'multiline':False]
['text':' unused ','line_number':830,'multiline':True]
['text':' #ifdef USE_CUDA','line_number':839,'multiline':False]
['text':' unused ','line_number':851,'multiline':True]
['text':' unused ','line_number':1025,'multiline':True]
['text':' unused ','line_number':1026,'multiline':True]
['text':' unused ','line_number':1034,'multiline':True]
['text':' to avoid flatten the tensors, we use alltoallv to achieve Alltoall as
     follow.
      1. store addresses of each tensor directly in displacements, keep buffer
     to nullptr, i.e., 0
      2. convert datatype to UINT8, which is always 1 bytes, to avoid wrong size
     calculation in UCC layer
      3. post Alltoallv
  ','line_number':1048,'multiline':True]
['text':' unused ','line_number':1100,'multiline':True]
['text':' if current device is 0, likely the device is not set, use the best guess','line_number':1183,'multiline':False]
['text':' for non-root ranks, only src is valid ','line_number':1271,'multiline':True]
['text':' use gatherv and store non-contiguous addresses in displacements to avoid
     * flatten outputTensors ','line_number':1302,'multiline':True]
['text':' for non-root ranks, outputTensors should be an empty list','line_number':1312,'multiline':False]
['text':' append a empty tensor to the list to be used by future mark','line_number':1318,'multiline':False]
['text':' src is only valid at non-root rank ','line_number':1461,'multiline':True]
['text':' use scatter and store non-contiguous addresses in displacements to avoid
     * flatten inputTensors ','line_number':1484,'multiline':True]
['text':' for non-root ranks, inputTensors should be an empty list','line_number':1495,'multiline':False]
['text':' Create UCC execution engine.','line_number':1634,'multiline':False]
['text':' namespace c10d','line_number':1660,'multiline':False]
['text':' USE_C10D_UCC','line_number':1662,'multiline':False]
