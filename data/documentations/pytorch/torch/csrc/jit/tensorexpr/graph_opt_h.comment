['text':' Optimize aten::cat ops in the given subgraph.','line_number':9,'multiline':False]
['text':'','line_number':10,'multiline':False]
['text':' Moving users of cat to its inputs.','line_number':11,'multiline':False]
['text':'    Cat ops get lowered into multiple loops, one per input. When the result','line_number':12,'multiline':False]
['text':'    of cat is used by some other op, it results in a situation where inlining','line_number':13,'multiline':False]
['text':'    of cat does not happen. This in turn results in intermediate buffers','line_number':14,'multiline':False]
['text':'    being created for the result of cat, since it is not inlined.','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':'    For example, consider the following graph:','line_number':17,'multiline':False]
['text':'       graph(%x : Float(10, strides=[1], device=cpu),','line_number':18,'multiline':False]
['text':'             %y : Float(20, strides=[1], device=cpu)):','line_number':19,'multiline':False]
['text':'         %dim : int = prim::Constant[value=0]()','line_number':20,'multiline':False]
['text':'         %xy_list : Tensor[] = prim::ListConstruct(%x, %y)','line_number':21,'multiline':False]
['text':'         %cat : Float(60, strides=[1], device=cpu) = aten::cat(%xy_list, %dim)','line_number':22,'multiline':False]
['text':'         %5 : Float(60, strides=[1], device=cpu) = aten::log(%cat)','line_number':23,'multiline':False]
['text':'         return (%5))IR";','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':'     This will get lowered into:','line_number':26,'multiline':False]
['text':'         Allocate(aten_cat);','line_number':27,'multiline':False]
['text':'         for (...)','line_number':28,'multiline':False]
['text':'           aten_cat[...] = x[...]','line_number':29,'multiline':False]
['text':'         for (...)','line_number':30,'multiline':False]
['text':'           aten_cat[...] = y[...]','line_number':31,'multiline':False]
['text':'         for (...)','line_number':32,'multiline':False]
['text':'           aten_log[...] = log(aten_cat[...])','line_number':33,'multiline':False]
['text':'         Free(aten_cat);','line_number':34,'multiline':False]
['text':'     Note that aten_cat is not inlined into aten_log and it results in','line_number':35,'multiline':False]
['text':'     an intermediate buffer allocation as well.','line_number':36,'multiline':False]
['text':'','line_number':37,'multiline':False]
['text':'     Optimization:','line_number':38,'multiline':False]
['text':'        We move the ops that use the result of `cat` into its inputs whenever','line_number':39,'multiline':False]
['text':'     possible.','line_number':40,'multiline':False]
['text':'','line_number':41,'multiline':False]
['text':'     The graph above will be transformed to:','line_number':42,'multiline':False]
['text':'        graph(%x : Float(10, strides=[1], device=cpu),','line_number':43,'multiline':False]
['text':'              %y : Float(20, strides=[1], device=cpu)):','line_number':44,'multiline':False]
['text':'          %3 : int = prim::Constant[value=0]()','line_number':45,'multiline':False]
['text':'          %7 : Float(10, strides=[1], device=cpu) = aten::log(%x)','line_number':46,'multiline':False]
['text':'          %8 : Float(20, strides=[1], device=cpu) = aten::log(%y)','line_number':47,'multiline':False]
['text':'          %9 : Tensor[] = prim::ListConstruct(%7, %8)','line_number':48,'multiline':False]
['text':'          %10 : Float(60, strides=[1], device=cpu) = aten::cat(%9, %3)','line_number':49,'multiline':False]
['text':'          return (%10)','line_number':50,'multiline':False]
['text':'','line_number':51,'multiline':False]
['text':'     This will get lowered into:','line_number':52,'multiline':False]
['text':'         for (...)','line_number':53,'multiline':False]
['text':'           aten_cat[...] = log(x[...])','line_number':54,'multiline':False]
['text':'         for (...)','line_number':55,'multiline':False]
['text':'           aten_cat[...] = log(y[...])','line_number':56,'multiline':False]
['text':'     aten_cat is the output buffer here.','line_number':57,'multiline':False]
['text':' Perform \p ITERS rounds of "trimming" for the given \p GRAPH.','line_number':72,'multiline':False]
['text':'','line_number':73,'multiline':False]
['text':' Trimming means that we try to remove a small portion of the graph while','line_number':74,'multiline':False]
['text':' keeping it valid. This is useful for debugging when we try to find a minimal','line_number':75,'multiline':False]
['text':' example reproducing the issue at hand. When ITERS is 0, the graph remains','line_number':76,'multiline':False]
['text':' unchanged, when ITERS is a big number, the graph usually becomes empty.','line_number':77,'multiline':False]
['text':' Scan all values in the given graph and replace each dimension with a size Xi','line_number':82,'multiline':False]
['text':' present in \p SIZES with a symbolic shape Yi. Return a vector of symbol','line_number':83,'multiline':False]
['text':' values [Y0, Y1, .., Yn].','line_number':84,'multiline':False]
['text':'','line_number':85,'multiline':False]
['text':' For example:','line_number':86,'multiline':False]
['text':' Input:','line_number':87,'multiline':False]
['text':' graph(%x : Float(10, 20, 30, 40)):','line_number':88,'multiline':False]
['text':'   %y : Float(10, 20, 30, 40) = aten::relu(%x)','line_number':89,'multiline':False]
['text':'   return %y','line_number':90,'multiline':False]
['text':'','line_number':91,'multiline':False]
['text':' If we run makeShapesSymbolic(graph, {20, 40}), then we'll get:','line_number':92,'multiline':False]
['text':'','line_number':93,'multiline':False]
['text':' graph(%x : Float(10, SS(-3), 30, SS(-5))):','line_number':94,'multiline':False]
['text':'   %y : Float(10, SS(-3), 30, SS(-5)) = aten::relu(%x)','line_number':95,'multiline':False]
['text':'   return %y','line_number':96,'multiline':False]
['text':'','line_number':97,'multiline':False]
['text':' and get {-3, -5} as the return value.','line_number':98,'multiline':False]
['text':' Inspect the graph and report whether it can be converted to TE IR.','line_number':103,'multiline':False]
['text':' TODO: add error reporting for graphs that can't be converted.','line_number':104,'multiline':False]
['text':' Examine the graph and (hackily) fill in missing tensor type info, such as','line_number':107,'multiline':False]
['text':' scalar type, device, and strides. Ideally, this should be done by a proper','line_number':108,'multiline':False]
['text':' dtype/device/shape propagation passes, but until they are ready we can use','line_number':109,'multiline':False]
['text':' this, not always correct, workaround pass.','line_number':110,'multiline':False]
['text':' namespace tensorexpr','line_number':113,'multiline':False]
['text':' namespace jit','line_number':114,'multiline':False]
['text':' namespace torch','line_number':115,'multiline':False]
