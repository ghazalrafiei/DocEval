['text':' We currently only support rank 2 matmuls','line_number':24,'multiline':False]
['text':' For small sizes, where N*M*K < 1000, lower matmul to a naive 3-level','line_number':32,'multiline':False]
['text':' loopnest. The number is not tuned very carefully, and in future we should','line_number':33,'multiline':False]
['text':' fine-tune it as well as we should add more advanced native TE lowerings for','line_number':34,'multiline':False]
['text':' matmuls. For bigger sizes we generate a TE ExternalCall, which would call','line_number':35,'multiline':False]
['text':' an aten::matmul.','line_number':36,'multiline':False]
['text':' Native, even naive, lowering is beneficial when the sizes are small because','line_number':37,'multiline':False]
['text':' it allows to eliminate dispatch overhead.','line_number':38,'multiline':False]
['text':' TODO: handle other dtypes of alpha and beta','line_number':76,'multiline':False]
['text':' namespace tensorexpr','line_number':79,'multiline':False]
['text':' namespace jit','line_number':80,'multiline':False]
['text':' namespace torch','line_number':81,'multiline':False]
