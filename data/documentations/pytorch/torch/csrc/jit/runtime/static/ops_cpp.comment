['text':' Add new leading dimensions to the tensor if the','line_number':61,'multiline':False]
['text':' number of target dimensions is larger than the','line_number':62,'multiline':False]
['text':' number of source dimensions.','line_number':63,'multiline':False]
['text':' return an empty tensor if one of the repeat dimensions is zero','line_number':77,'multiline':False]
['text':' can't unfold with step 0, so make sure step is at least 1','line_number':86,'multiline':False]
['text':' (it doesn't matter what it is in that case, because the size is 0).','line_number':87,'multiline':False]
['text':' copy version of view ops','line_number':95,'multiline':False]
['text':' We don't want to infer_size on the entire shape, because that can give us','line_number':141,'multiline':False]
['text':' an extra degree of freedom we don't want; for example, consider shape [0,','line_number':142,'multiline':False]
['text':' 1, 3, 0], with start_dim=1, end_dim=2. It's clear we want result shape [0,','line_number':143,'multiline':False]
['text':' 3, 0] but passing [0, -1, 0] to infer_size means the -1 can take on any','line_number':144,'multiline':False]
['text':' value and satisfy the constraints.','line_number':145,'multiline':False]
['text':' NOLINTNEXTLINE(modernize-use-transparent-functors)','line_number':151,'multiline':False]
['text':' This is annoying and sily, but it's solving a real problem: the','line_number':168,'multiline':False]
['text':' _MSC_VER version causes an ICE on our old clang5 builds. The','line_number':169,'multiline':False]
['text':' non-_MSC_VER version is a syntax error according to MSVC. Use the','line_number':170,'multiline':False]
['text':' appropriate version depending on if we're MSVC or not.','line_number':171,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse) ','line_number':186,'multiline':True]
['text':' namespace','line_number':205,'multiline':False]
['text':' Fast path: can we just copy the data ourselves? Avoids creating a','line_number':231,'multiline':False]
['text':' TensorIterator in at::native::copy_, which is relatively','line_number':232,'multiline':False]
['text':' expensive.','line_number':233,'multiline':False]
['text':' Did the user request us to make a copy that isn't contiguous?','line_number':235,'multiline':False]
['text':' CopyKernel.cpp handles this case specially, so let's not mess','line_number':239,'multiline':False]
['text':' with it.','line_number':240,'multiline':False]
['text':' FBGEMM optimization might kick in, don't interfere with','line_number':244,'multiline':False]
['text':' that.','line_number':245,'multiline':False]
['text':' Fused op is marginally faster.','line_number':270,'multiline':False]
['text':' input is a [prev_size, n] tensor.','line_number':315,'multiline':False]
['text':' output is a [prev_size,] tensor.','line_number':316,'multiline':False]
['text':' Thus, access is contiguous/coalesced.','line_number':317,'multiline':False]
['text':' if a is nan, then a is *less* than b with LessOrNan','line_number':323,'multiline':False]
['text':' semantics','line_number':324,'multiline':False]
['text':' if a is not nan and b is nan, then a is not less than b','line_number':328,'multiline':False]
['text':' with LessOrNan semantics otherwise, act normally. If `b` is','line_number':329,'multiline':False]
['text':' NaN then a < b will always return false, so this is','line_number':330,'multiline':False]
['text':' equivalent to the first snippet.','line_number':331,'multiline':False]
['text':' fallback to dequantize_cpu equivalent case: make sure out is at::kFloat','line_number':371,'multiline':False]
['text':' namespace at::native','line_number':377,'multiline':False]
['text':' This list contains ops that use caffe2 math library or use NNC that does','line_number':392,'multiline':False]
['text':' not guarantee bit exactness vs the jit interpreter. Note aten::relu is not','line_number':393,'multiline':False]
['text':' included even though it uses NNC because the results of relu should always','line_number':394,'multiline':False]
['text':' match.','line_number':395,'multiline':False]
['text':' Returns true if the node represents an op with variadic arguments.','line_number':410,'multiline':False]
['text':' returns true if the producers of the inputs','line_number':428,'multiline':False]
['text':' to this operations are out of place.','line_number':429,'multiline':False]
['text':' This means the IValues will not change run to run','line_number':430,'multiline':False]
['text':' prepare inputs','line_number':513,'multiline':False]
['text':' Disable externally to avoid MSVC errors in open-source CI','line_number':624,'multiline':False]
['text':' This might be UB if in3_s is absurdly large, but most implementations','line_number':669,'multiline':False]
['text':' just turn it into `inf` in that case. The PyTorch core nan_to_num','line_number':670,'multiline':False]
['text':' kernel just static_cast()s the limits to the destination type, so','line_number':671,'multiline':False]
['text':' we'll ignore overflow issues here as well.','line_number':672,'multiline':False]
['text':' All stack inputs should have the same size, so we only check','line_number':846,'multiline':False]
['text':' the first one. If this isn't true, an exception will be thrown','line_number':847,'multiline':False]
['text':' in the VarStack implementation','line_number':848,'multiline':False]
['text':'skip_overlap_check','line_number':868,'multiline':True]
['text':' namespace','line_number':877,'multiline':False]
['text':' Split out into a function to appease MSVC's pre-processor','line_number':879,'multiline':False]
['text':'
      disable out_variant of clone for case with stride = 0 and
      memory formats other than preserve. Perform dynamic allocation
      instead of memory reuse for simpler implementation. We could,
      in principle, figure out copy of strides.
    ','line_number':1092,'multiline':True]
['text':' Copy all strides','line_number':1106,'multiline':False]
['text':' unused scale_grad_by_freq','line_number':1150,'multiline':False]
['text':' unused mode','line_number':1151,'multiline':False]
['text':' unused scale_grad_by_freq','line_number':1188,'multiline':False]
['text':' unused mode','line_number':1189,'multiline':False]
['text':' The out variant takes precedence over native','line_number':1218,'multiline':False]
['text':' self','line_number':1226,'multiline':False]
['text':' dim','line_number':1227,'multiline':False]
['text':' length','line_number':1235,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':1354,'multiline':False]
['text':' Static runtime only works with CPU tensors; don't need to read this.','line_number':1374,'multiline':False]
['text':' The to_maybe_copy_out operator functor should have detected a','line_number':1397,'multiline':False]
['text':' constant true `copy` argument and used to_copy instead.','line_number':1398,'multiline':False]
['text':' Special case! First, there is no memory format to check. Second,','line_number':1421,'multiline':False]
['text':' we know that the layout and device will match self, so we only','line_number':1422,'multiline':False]
['text':' need to check the dtype.','line_number':1423,'multiline':False]
['text':' !copy','line_number':1425,'multiline':False]
['text':' Force inlining so we don't have to branch on whether args is null','line_number':1431,'multiline':False]
['text':' at runtime.','line_number':1432,'multiline':False]
['text':' ignore input 3 (copy)','line_number':1438,'multiline':False]
['text':' non_blocking','line_number':1439,'multiline':False]
['text':' handle memory format','line_number':1440,'multiline':False]
['text':' See Note [Explicit nullopt MemoryFormat argument]','line_number':1480,'multiline':False]
['text':' Can't use size {0} if memory_format is ChannelLast','line_number':1481,'multiline':False]
['text':' It would be great if we could avoid checking our arguments every','line_number':1518,'multiline':False]
['text':' time. However, we need to make account for the possibility that','line_number':1519,'multiline':False]
['text':' the dtype (and layout, memory format, etc.) of self changed','line_number':1520,'multiline':False]
['text':' between iterations.','line_number':1521,'multiline':False]
['text':' Don't write our Tensor output. This leaves it None if it','line_number':1529,'multiline':False]
['text':' was never allocated (and there is a special case in the','line_number':1530,'multiline':False]
['text':' memory planner to not start managing in this case), but','line_number':1531,'multiline':False]
['text':' if we are oscillating between aliasing and needing to','line_number':1532,'multiline':False]
['text':' copy, we should just leave our output in place so as not','line_number':1533,'multiline':False]
['text':' to confuse the memory planner.','line_number':1534,'multiline':False]
['text':' Take advantage of CheckToWillAlias not requiring the args in this','line_number':1544,'multiline':False]
['text':' case.','line_number':1545,'multiline':False]
['text':' namespace','line_number':1584,'multiline':False]
['text':' support 4- or 5-arg for adindexer/adfinder models','line_number':1590,'multiline':False]
['text':' Keep TORCH_CHECK here because there is no alternative for fallback','line_number':1591,'multiline':False]
['text':' If we are going to copy always, just use the to_copy path so','line_number':1604,'multiline':False]
['text':' that the to_maybe_copy path can assume that won't happen.','line_number':1605,'multiline':False]
['text':' out variant takes precedence over native','line_number':1630,'multiline':False]
['text':' NB: This impl doesn't work for cpu->cuda copy/cast or vice versa.','line_number':1631,'multiline':False]
['text':' support 4- or 5-arg for adindexer/adfinder models','line_number':1636,'multiline':False]
['text':' Keep TORCH_CHECK here because there is no alternative for fallback','line_number':1637,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)','line_number':1653,'multiline':False]
['text':' please implement static runtime support for aten::dequantize with','line_number':1660,'multiline':False]
['text':' TensorList','line_number':1661,'multiline':False]
['text':' Out variants for view ops are registered to a separate registry because','line_number':1678,'multiline':False]
['text':' their outputs (views) can't participate in memory reuse.','line_number':1679,'multiline':False]
['text':' self','line_number':1691,'multiline':False]
['text':' shape','line_number':1692,'multiline':False]
['text':'dim=','line_number':1794,'multiline':True]
['text':'keepdim=','line_number':1794,'multiline':True]
['text':' trunc after div','line_number':1930,'multiline':False]
['text':' floor after div','line_number':1933,'multiline':False]
['text':' TODO: support clamp_min.Tensor(Tensor self, Tensor min) -> Tensor','line_number':2004,'multiline':False]
['text':' namespace','line_number':2083,'multiline':False]
['text':' ignore Input(5): `bool cudnn_enable=True`','line_number':2092,'multiline':False]
['text':' dtype ','line_number':2115,'multiline':True]
['text':' layout ','line_number':2116,'multiline':True]
['text':' device ','line_number':2117,'multiline':True]
['text':' pin_memory ','line_number':2118,'multiline':True]
['text':' dim','line_number':2164,'multiline':False]
['text':' keepdim','line_number':2165,'multiline':False]
['text':' dtype','line_number':2166,'multiline':False]
['text':' dim','line_number':2185,'multiline':False]
['text':' keepdim','line_number':2186,'multiline':False]
['text':' Weights could be quantized on the fly','line_number':2247,'multiline':False]
['text':' Weights could be quantized on the fly','line_number':2294,'multiline':False]
['text':' The implementation of PackedLinearWeightFp16::apply_dynamic_impl does not','line_number':2327,'multiline':False]
['text':' handle relu. Currently, it ignores the `ReluFused` template parameter.','line_number':2328,'multiline':False]
['text':' So, we explicitly do the relu here.','line_number':2329,'multiline':False]
['text':' Weights could be quantized on the fly','line_number':2359,'multiline':False]
['text':' namespace','line_number':2368,'multiline':False]
['text':' device & pin_memory matter only when CUDA is enabled.','line_number':2396,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)','line_number':2672,'multiline':False]
['text':' This template and its specialization help us avoid compiler warnings','line_number':2698,'multiline':False]
['text':' about taking the absolute value of an unsigned type in signed_log1p','line_number':2699,'multiline':False]
['text':' Computes f(x) = sign(x) * ln(|1 + x|) for each x in the input tensor','line_number':2710,'multiline':False]
['text':' namespace','line_number':2729,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)','line_number':2731,'multiline':False]
['text':' Unrecognized overload','line_number':2792,'multiline':False]
['text':' namespace torch::jit','line_number':2862,'multiline':False]
