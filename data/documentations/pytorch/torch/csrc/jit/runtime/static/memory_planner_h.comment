['text':' A StorageGroup represents a collection of tensors that share backing storage.','line_number':7,'multiline':False]
['text':' Every storage group must contain at least one tensor.','line_number':10,'multiline':False]
['text':' The size attribute represents the amount of memory that will be','line_number':34,'multiline':False]
['text':' allocated for all tensors in this storage group. Initially it','line_number':35,'multiline':False]
['text':' is zero, eventually it gets updated by the MemoryPlanner.','line_number':36,'multiline':False]
['text':' A contiguous buffer of `StorageImpl`s','line_number':41,'multiline':False]
['text':' Append a new StorageImpl to the buffer. The new StorageImpl is given the','line_number':56,'multiline':False]
['text':' same size and allocator as `storageImpl` argument','line_number':57,'multiline':False]
['text':' We will use placement-new to add new storages to this buffer','line_number':83,'multiline':False]
['text':' Current number of storages that have been placed into the storage buffer','line_number':86,'multiline':False]
['text':' Total allocated capacity of the storage buffer','line_number':89,'multiline':False]
['text':' There are three types of ops in a processed graph in Static Runtime:','line_number':98,'multiline':False]
['text':'   1. op with _out variant','line_number':99,'multiline':False]
['text':'   2. view-producing op','line_number':100,'multiline':False]
['text':'   3. tensor-producing op (could be replaced with type 1 by adding the _out','line_number':101,'multiline':False]
['text':'      variant to Static Runtime)','line_number':102,'multiline':False]
['text':' In Static Runtime, type 2 ops are replaced with their corresponding copy','line_number':103,'multiline':False]
['text':' versions when enable_out_variant is enabled and become type 1 ops.The memory','line_number':104,'multiline':False]
['text':' planner only manages tensors that are outputs of type 1 ops. For type 3, the','line_number':105,'multiline':False]
['text':' output tensors are allocated inside the operator and can't be directly','line_number':106,'multiline':False]
['text':' managed by memory planner.','line_number':107,'multiline':False]
['text':'','line_number':108,'multiline':False]
['text':' Memory planner tries to minimize the number of memory allocations by','line_number':109,'multiline':False]
['text':' tracking the output tensors of ops with _out variants with unique DataPtr','line_number':110,'multiline':False]
['text':' (part of StorageImpl). It tries to do this in several steps:','line_number':111,'multiline':False]
['text':'   1. record the max memory usage for each Tensor with unique DataPtr at the','line_number':112,'multiline':False]
['text':'      end of each iteration','line_number':113,'multiline':False]
['text':'   2. in the next iteration, allocate the buffer for the max total usage and','line_number':114,'multiline':False]
['text':'      compute the offset of each allocation with regard to the single memory','line_number':115,'multiline':False]
['text':'      buffer, optionally reusing memory. In the first iteration, we rely on','line_number':116,'multiline':False]
['text':'      the default allocator for memory allocation.','line_number':117,'multiline':False]
['text':'   3. free the buffer at the end of each iteration','line_number':118,'multiline':False]
['text':' Steps 1 and 3 are handled by `deallocate()`, and step 2 by `allocate()`.','line_number':119,'multiline':False]
['text':' Only models with simple output types are supported, i.e. None, Tensor or','line_number':120,'multiline':False]
['text':' List/Tuple/Dict of Tensors. Complex output types such as List of Lists are','line_number':121,'multiline':False]
['text':' not supported.','line_number':122,'multiline':False]
['text':'','line_number':123,'multiline':False]
['text':' Additional Optimizations:','line_number':124,'multiline':False]
['text':'','line_number':125,'multiline':False]
['text':' [Borrowed IValue Outputs]','line_number':126,'multiline':False]
['text':' A few native ops (notably, `static_runtime::dict_unpack` and','line_number':127,'multiline':False]
['text':' `static_runtime::VarTupleUnpack`) simply unpack IValues to a bunch of','line_number':128,'multiline':False]
['text':' outputs without modification. For example, `dict_unpack` does the following:','line_number':129,'multiline':False]
['text':' for each key in inputs:','line_number':130,'multiline':False]
['text':'     output[i] = dict_input[key]','line_number':131,'multiline':False]
['text':' To avoid refcount bumps, the outputs of these ops are non-owning references.','line_number':132,'multiline':False]
['text':' This requires special logic in the memory planner - when adding an op that','line_number':133,'multiline':False]
['text':' borrows outputs, be sure that the memory planner is updated accordingly!','line_number':134,'multiline':False]
['text':'','line_number':135,'multiline':False]
['text':' [Managed Output Tensors]','line_number':136,'multiline':False]
['text':' The memory planner is able to manage output tensors if the appropriate','line_number':137,'multiline':False]
['text':' `StaticModuleOptions` are set. However, the memory planner handles output','line_number':138,'multiline':False]
['text':' tensors separately from regular intermediate tensors:','line_number':139,'multiline':False]
['text':' 1. They don't participate in memory reuse.','line_number':140,'multiline':False]
['text':' 2. The memory planner cannot reclaim their backing storage until they have','line_number':141,'multiline':False]
['text':'    been explicitly freed by the client.','line_number':142,'multiline':False]
['text':' disable copying and moving','line_number':152,'multiline':False]
['text':' Check if `ivalue` is contained as a managed tensor. Only used in DCHECK().','line_number':195,'multiline':False]
['text':' output buffer got already deallocated.','line_number':197,'multiline':False]
['text':' memory planning is not yet initialized.','line_number':198,'multiline':False]
['text':' a non-tensor is never managed','line_number':199,'multiline':False]
['text':' TODO: Improve this once D31357486 is landed.','line_number':207,'multiline':False]
['text':' Comparing pointers that aren't within the same array is','line_number':219,'multiline':False]
['text':' UB. We're doing fancy memory allocation stuff, so we cast to an','line_number':220,'multiline':False]
['text':' integer type and carry on.','line_number':221,'multiline':False]
['text':' We allocate StorageImpls ourselves so that 1) we don't have to do','line_number':239,'multiline':False]
['text':' an extra two loads per Tensor (which will likely miss in the CPU','line_number':240,'multiline':False]
['text':' data cache) first reading the Storage (i.e., StorageImpl pointer)','line_number':241,'multiline':False]
['text':' from the TensorImpl object and then second dereferencing it and','line_number':242,'multiline':False]
['text':' 2) our memory access pattern during allocate() has high locality.','line_number':243,'multiline':False]
['text':' We don't have any guarantee that the model doesn't change the','line_number':244,'multiline':False]
['text':' Storage for managed tensors out from under us during execution,','line_number':245,'multiline':False]
['text':' so we have to check the StorageImpls each time we deallocate.','line_number':246,'multiline':False]
['text':' Contains the size (in bytes) of the data to be allocated for each storage','line_number':249,'multiline':False]
['text':' ivalues created in one run but not managed by MemoryPlanner','line_number':253,'multiline':False]
['text':' Special class of unmanaged values: some native ops create IValues','line_number':256,'multiline':False]
['text':' in a "borrowed" state that can and must be cleaned up without a','line_number':257,'multiline':False]
['text':' reference count decrement.','line_number':258,'multiline':False]
['text':' Even more special class of unmanaged values: if select_tensor','line_number':261,'multiline':False]
['text':' outputs are outputs of the graph, then they need to be restored','line_number':262,'multiline':False]
['text':' to an ordinary "strong reference" state.','line_number':263,'multiline':False]
['text':' allocated each time we call Run()','line_number':267,'multiline':False]
['text':' namespace torch::jit','line_number':298,'multiline':False]
