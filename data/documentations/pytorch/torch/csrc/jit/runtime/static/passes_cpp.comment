['text':' TODO:: check restrictions for inputs; outputs not used elsewhere','line_number':42,'multiline':False]
['text':' TODO:: check restrictions for inputs; outputs not used elsewhere','line_number':96,'multiline':False]
['text':' this pattern found in several models has a redundant second `flatten`','line_number':142,'multiline':False]
['text':' TODO:: check restrictions for inputs; outputs not used elsewhere','line_number':176,'multiline':False]
['text':' TODO:: check restrictions for inputs; outputs not used elsewhere','line_number':193,'multiline':False]
['text':' fuse without lengths-to-offsets','line_number':194,'multiline':False]
['text':' TODO:: check restrictions for inputs; outputs not used elsewhere','line_number':286,'multiline':False]
['text':' Placeholder is a dummy op used to capture the first subgraph','line_number':321,'multiline':False]
['text':' the second gather_ranges can be eliminated because the `lengths` is','line_number':333,'multiline':False]
['text':' produces is identical to the lengths produced by','line_number':334,'multiline':False]
['text':' clip_ranges_gather_sigrid_hash_v3 (caveat, the fused ops makes some','line_number':335,'multiline':False]
['text':' simplifying assumptions about the ranges input)','line_number':336,'multiline':False]
['text':' reverse the ops that got fused in step 1 but not in step2','line_number':355,'multiline':False]
['text':' namespace','line_number':368,'multiline':False]
['text':' prioritize clip_ranges+gather_ranges+sigrid_hash fusion over','line_number':387,'multiline':False]
['text':' clip_ranges+gather_ranges','line_number':388,'multiline':False]
['text':' num_outputs ','line_number':511,'multiline':True]
['text':' namespace','line_number':527,'multiline':False]
['text':' This macro makes maps from c10::Symbol -> c10::Symbol a lot easier to read.','line_number':535,'multiline':False]
['text':' Out variants of ops cannot participate in memory planning if they','line_number':539,'multiline':False]
['text':' have outputs that alias inputs. For ops that either return their','line_number':540,'multiline':False]
['text':' input directly or copy it (most notably aten::to), we adopt the','line_number':541,'multiline':False]
['text':' following strategy instead of directly making them out variants so','line_number':542,'multiline':False]
['text':' that they can participate in memory planning anyway. Let `a` denote','line_number':543,'multiline':False]
['text':' the input Tensor to the op.','line_number':544,'multiline':False]
['text':'','line_number':545,'multiline':False]
['text':' 1) Pass `a` (and the other operator inputs) to a special','line_number':546,'multiline':False]
['text':' `static_runtime::$OP_maybe_copy_out` variant of the op. This op','line_number':547,'multiline':False]
['text':' returns a normal output Tensor (call it `b_out` as well as a','line_number':548,'multiline':False]
['text':' `did_copy` flag indicating whether the output should be used. If','line_number':549,'multiline':False]
['text':' `did_copy` is false, the value of `b_out` is unspecified. Note that','line_number':550,'multiline':False]
['text':' this operator is an ordinary out variant that is perfectly amenable','line_number':551,'multiline':False]
['text':' to memory planning.','line_number':552,'multiline':False]
['text':'','line_number':553,'multiline':False]
['text':' 2) Pass `a`, `b_out`, and `did_copy` to a special','line_number':554,'multiline':False]
['text':' `static_runtime::select_tensor` op, which returns `b_out` if','line_number':555,'multiline':False]
['text':' `did_copy` is true and `a` otherwise. Note that this operator does','line_number':556,'multiline':False]
['text':' not need to participate in memory planning because its output','line_number':557,'multiline':False]
['text':' always aliases one of its inputs.','line_number':558,'multiline':False]
['text':'','line_number':559,'multiline':False]
['text':' Here is an illustration:','line_number':560,'multiline':False]
['text':'','line_number':561,'multiline':False]
['text':'                        |','line_number':562,'multiline':False]
['text':' |----------------------+ a','line_number':563,'multiline':False]
['text':' |                      v','line_number':564,'multiline':False]
['text':' |    +------------------------------------+','line_number':565,'multiline':False]
['text':' |    |                                    |','line_number':566,'multiline':False]
['text':' |    | static_runtime::$OP_maybe_copy_out |','line_number':567,'multiline':False]
['text':' |    |                                    |','line_number':568,'multiline':False]
['text':' |    +------------------+--------+--------+','line_number':569,'multiline':False]
['text':' |                       |        |','line_number':570,'multiline':False]
['text':' +--------------+        | b_out  | did_copy','line_number':571,'multiline':False]
['text':'                | a      |        |','line_number':572,'multiline':False]
['text':'                v        v        v','line_number':573,'multiline':False]
['text':'      +------------------------------------+','line_number':574,'multiline':False]
['text':'      |                                    |','line_number':575,'multiline':False]
['text':'      |    static_runtime::select_tensor   |','line_number':576,'multiline':False]
['text':'      |                                    |','line_number':577,'multiline':False]
['text':'      +------------------+-----------------+','line_number':578,'multiline':False]
['text':'                         |','line_number':579,'multiline':False]
['text':'                         |','line_number':580,'multiline':False]
['text':'                         | either a or b_out','line_number':581,'multiline':False]
['text':'                         |','line_number':582,'multiline':False]
['text':'                         v','line_number':583,'multiline':False]
['text':' for ops that have overloads, match the schema','line_number':589,'multiline':False]
['text':' old node, new node, select_tensor node','line_number':611,'multiline':False]
['text':' Duplicate input writers guard from ReplaceWithCopy below.','line_number':621,'multiline':False]
['text':' Add the did_copy flag to outputs.','line_number':631,'multiline':False]
['text':' We do not want to replace operators with their copy variant when the','line_number':699,'multiline':False]
['text':' inputs to the operators have writers (can be updated). With an output','line_number':700,'multiline':False]
['text':' that aliases to the input, updates to the input will be visible to the','line_number':701,'multiline':False]
['text':' operator's output as well. For example:','line_number':702,'multiline':False]
['text':'','line_number':703,'multiline':False]
['text':' def forward(self, inp: Tensor, shape: List[int]):','line_number':704,'multiline':False]
['text':'   a = inp + inp','line_number':705,'multiline':False]
['text':'   b = a.reshape(shape)','line_number':706,'multiline':False]
['text':'   c = b.sigmoid_()','line_number':707,'multiline':False]
['text':'   d = c + c','line_number':708,'multiline':False]
['text':'   e = a + a','line_number':709,'multiline':False]
['text':'   f = b + b','line_number':710,'multiline':False]
['text':'   return (d, e, f)','line_number':711,'multiline':False]
['text':'','line_number':712,'multiline':False]
['text':' b and c are aliases of a, sigmoid_ changes b, c, as well as a. e should','line_number':713,'multiline':False]
['text':' equal to d in this case. If we replace reshape with the copy version, b','line_number':714,'multiline':False]
['text':' and c are no longer aliases of a, the value of e would change as a','line_number':715,'multiline':False]
['text':' result. To keep static runtime consistent with the jit interpreter, here','line_number':716,'multiline':False]
['text':' we choose not to replace reshape with the copy version','line_number':717,'multiline':False]
['text':' replace aten::permute with copy version only when it's followed by','line_number':751,'multiline':False]
['text':' reshape/flatten. It's only enabled when ReplaceWithCopy is off.','line_number':752,'multiline':False]
['text':' To fuse with sigrid transforms, we must be able to statically determine','line_number':850,'multiline':False]
['text':' `instance` and `use_offsets` - these two together let us statically','line_number':851,'multiline':False]
['text':' determine the types of the outputs. Rationale: it is a huge pain to write','line_number':852,'multiline':False]
['text':' fused sigrid transforms without static type information, and these two','line_number':853,'multiline':False]
['text':' arguments are indeed statically known in every model we've seen.','line_number':854,'multiline':False]
['text':' The reason why trying to fuse the outputs is annoying without static type','line_number':855,'multiline':False]
['text':' information is that, if one of the outputs is not managed, you need to','line_number':856,'multiline':False]
['text':' reset to an empty tensor of the correct type each iteration. So, if we','line_number':857,'multiline':False]
['text':' can't collect types ahead of time, we would have to do it lazily on the','line_number':858,'multiline':False]
['text':' first iteration, which would could be wasteful in terms of time/memory','line_number':859,'multiline':False]
['text':' - either each thread would have its own set of output types, or we would','line_number':860,'multiline':False]
['text':' need a lock to prevent data races.','line_number':861,'multiline':False]
['text':' namespace','line_number':867,'multiline':False]
['text':' replacement contains (old_node, new_node, list_unpack_node)','line_number':899,'multiline':False]
['text':' namespace jit','line_number':951,'multiline':False]
['text':' Gather all dict -> getitems where dict is immutable and getitems use','line_number':957,'multiline':False]
['text':' constant keys.','line_number':958,'multiline':False]
['text':' Find aten::__getitem__(%dict, %constant_key).','line_number':962,'multiline':False]
['text':' Mutable dict. Skip this optimization.','line_number':969,'multiline':False]
['text':' Move all keys to the beginning of the graph and insert new dict_unpack','line_number':992,'multiline':False]
['text':' nodes after that.','line_number':993,'multiline':False]
['text':' Create owned refs for inputs. Otherwise, the input cleanup process','line_number':1045,'multiline':False]
['text':' will destroy our outputs before we return.','line_number':1046,'multiline':False]
['text':' No need to create owned refs of NoneType since moving','line_number':1054,'multiline':False]
['text':' from None will have no effect','line_number':1055,'multiline':False]
['text':' If the output's owning block is not this one, it's from an outer','line_number':1060,'multiline':False]
['text':' scope','line_number':1061,'multiline':False]
['text':' Loop sub-blocks should always return at least one output (the new loop','line_number':1087,'multiline':False]
['text':' condition)','line_number':1088,'multiline':False]
['text':' Only search the top-level block','line_number':1097,'multiline':False]
['text':' namespace','line_number':1115,'multiline':False]
['text':' SubgraphRewriter can't pattern-match on constants, so we use this','line_number':1151,'multiline':False]
['text':' extra filter to make sure the values of the `dim` arguments are','line_number':1152,'multiline':False]
['text':' correct.','line_number':1153,'multiline':False]
['text':' Get the nodes in the real graph from the nodes in the template','line_number':1157,'multiline':False]
['text':' pattern graph','line_number':1158,'multiline':False]
['text':' Check that permute_dim is (0, 2, 1) and softmax_dim is 2','line_number':1199,'multiline':False]
['text':' namespace','line_number':1218,'multiline':False]
['text':' namespace','line_number':1239,'multiline':False]
['text':' Could also look at list length, but most models that have this pattern are','line_number':1341,'multiline':False]
['text':' just doing list[0:], so it's not needed for now.','line_number':1342,'multiline':False]
['text':' namespace','line_number':1345,'multiline':False]
['text':' Get the nodes in the real graph from the nodes in the template','line_number':1420,'multiline':False]
['text':' pattern graph','line_number':1421,'multiline':False]
['text':' Constant propagation should be called after this pass + others.','line_number':1453,'multiline':False]
['text':' namespace torch::jit','line_number':1456,'multiline':False]
