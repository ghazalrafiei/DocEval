['text':' need_trim_grad_ops contains functions that return multiple outputs in','line_number':22,'multiline':False]
['text':' forward, but only the first one requires grad.','line_number':23,'multiline':False]
['text':' Example:','line_number':24,'multiline':False]
['text':' kthvalue returns (kthvalue, index of kthvalue), currently autodiff only','line_number':25,'multiline':False]
['text':' supports at most one output that requires grad. Thus we need to remove','line_number':26,'multiline':False]
['text':' the grad for index that doesn't require grad.','line_number':27,'multiline':False]
['text':' TODO: scalar-tensor ops should be canonicalized','line_number':41,'multiline':False]
['text':' TODO: add support for the following fusible operators.','line_number':47,'multiline':False]
['text':' They're a little tricky to implement; max/min require mutability for best','line_number':48,'multiline':False]
['text':' perf "aten::atan2(Tensor self) -> Tensor", "aten::max(Tensor self) ->','line_number':49,'multiline':False]
['text':' Tensor", "aten::min(Tensor self) -> Tensor"','line_number':50,'multiline':False]
['text':' linear blocks may appear as inputs to graph executors, but they are removed','line_number':77,'multiline':False]
['text':' before differentiation occurs','line_number':78,'multiline':False]
['text':' formulas are only defined with floating point scalars,','line_number':87,'multiline':False]
['text':' so we fallback to autograd for other cases.','line_number':88,'multiline':False]
['text':' NB: Write gradient using torchscript','line_number':105,'multiline':False]
['text':' For example, node aten::mul() should be defined as follows','line_number':106,'multiline':False]
['text':' def forward(x, y):','line_number':107,'multiline':False]
['text':'     return x*y, (x, y)','line_number':108,'multiline':False]
['text':' def backward(ctx, grad_output):','line_number':109,'multiline':False]
['text':'     x, y = ctx','line_number':110,'multiline':False]
['text':'     return (y * grad_output).sum_to_size(x), (x * grad_output).sum_to_size(y)','line_number':111,'multiline':False]
['text':'','line_number':112,'multiline':False]
['text':' Here ctx is a tuple that carries all input/intermediate results needed in','line_number':113,'multiline':False]
['text':' backward from forward pass.','line_number':114,'multiline':False]
['text':'','line_number':115,'multiline':False]
['text':' This python code is compiled into a GradientPair which includes a forward','line_number':116,'multiline':False]
['text':' graph and a backward graph. Forward graph will be used to replace the node in','line_number':117,'multiline':False]
['text':' grad_desc.f, and backward graph will be used to construct GradOf(node) in','line_number':118,'multiline':False]
['text':' reverse_block. Grad_values(a.k.a gradOutputs) propagated through','line_number':119,'multiline':False]
['text':' node->owningGraph() in **reversed** order, thus GradientPair.forward should','line_number':120,'multiline':False]
['text':' be inserted **after** the node being replaced, so that we don't traverse the','line_number':121,'multiline':False]
['text':' graph infinite times.','line_number':122,'multiline':False]
['text':'','line_number':123,'multiline':False]
['text':' The output of compiled forward graph is [real_outputs, ctx]','line_number':124,'multiline':False]
['text':' The input of compiled backward graph is [ctx, grad_values]','line_number':125,'multiline':False]
['text':' We run LowerSimpleTuples afterwards to eliminate all tuples generated in','line_number':126,'multiline':False]
['text':' this process. The original node and TupleConstruct nodes in forward graph','line_number':127,'multiline':False]
['text':' will be cleaned up later using EliminateDeadCode(block). TupleUnPack node in','line_number':128,'multiline':False]
['text':' backward graph will be removed in eliminateDeadcode(ReverseDetails) defined','line_number':129,'multiline':False]
['text':' in this file.','line_number':130,'multiline':False]
['text':' Use forward graph to replace node in grad_desc.f','line_number':143,'multiline':False]
['text':' Use backward graph to construct reverse_block','line_number':158,'multiline':False]
['text':' If AD is defined using torchscript, use it instead of symbolic','line_number':183,'multiline':False]
['text':' Definition not found in torchscript, look up in the buildSymbolicGradient','line_number':188,'multiline':False]
['text':' TODO: migrate all to using torchscript','line_number':189,'multiline':False]
['text':' NB: AutogradAdds don't broadcast','line_number':202,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':209,'multiline':False]
['text':' graph->insert returns a tuple automatically if multiple outputs are','line_number':241,'multiline':False]
['text':' returned. So unpack them again.','line_number':242,'multiline':False]
['text':' graph->insert returns a tuple automatically if multiple outputs are','line_number':271,'multiline':False]
['text':' returned. So unpack them again.','line_number':272,'multiline':False]
['text':' namespace','line_number':293,'multiline':False]
['text':' If we have a function y = f(x) with jacobian J, the backwards of f is dx =','line_number':295,'multiline':False]
['text':' J^t dy. Note that because the backwards always implements this matrix','line_number':296,'multiline':False]
['text':' multiply, we know that it maps an input vector of zeros to an output vector','line_number':297,'multiline':False]
['text':' of zero regardless of what operations it chooses to do inside to actually','line_number':298,'multiline':False]
['text':' implement the matrix multiply (most use some optimized form and never','line_number':299,'multiline':False]
['text':' generate J^t). More generally, we know that all of the backward computations','line_number':300,'multiline':False]
['text':' are linear and can use this property to do more aggressive optimizations','line_number':301,'multiline':False]
['text':' later. It is ok to replace any backward function with known-zero inputs with','line_number':302,'multiline':False]
['text':' something that produces known-zero outputs. This function encloses each','line_number':303,'multiline':False]
['text':' know-linear backward function in a 'GradOf' sub-block so that we can perform','line_number':304,'multiline':False]
['text':' optimizations using this information. In particular, specializeAutogradZero','line_number':305,'multiline':False]
['text':' will observe if all the inputs to the linear block are AutogradZeroTensor,','line_number':306,'multiline':False]
['text':' which the autograd uses to represent zeros, and then propagate the zeros to','line_number':307,'multiline':False]
['text':' the outputs of the block.','line_number':308,'multiline':False]
['text':' FIXME: In case forward has multi outputs, we only support one requires grad','line_number':314,'multiline':False]
['text':' to make reading gradient graphs easier, remember the name of the forward op','line_number':319,'multiline':False]
['text':' AutogradAdd is a special addition function that handles Undef','line_number':340,'multiline':False]
['text':' AutogradAdd(a, b) == a + b if defined(a) and defined(b)','line_number':341,'multiline':False]
['text':' AutogradAdd(Undef, b) == b','line_number':342,'multiline':False]
['text':' AutogradAdd(a, Undef) == a','line_number':343,'multiline':False]
['text':' AutogradAdd(Undef, Undef) == Undef','line_number':344,'multiline':False]
['text':' namespace','line_number':370,'multiline':False]
['text':' Before:','line_number':372,'multiline':False]
['text':'   - grad_desc has field f initialized to the original 0-stage graph','line_number':373,'multiline':False]
['text':' After:','line_number':374,'multiline':False]
['text':'   - the last node of f (f->nodes().reverse()[0]) is a gradient node','line_number':375,'multiline':False]
['text':'     whose block has vjp inputs for all outputs that require_grad','line_number':376,'multiline':False]
['text':'     and vjp outputs for all primal inputs that require_grad','line_number':377,'multiline':False]
['text':'   - grad_desc has df_input_vjps and df_output_vjps set','line_number':378,'multiline':False]
['text':'     (but df_input_vjps will be modified later as well)','line_number':379,'multiline':False]
['text':' note: reverse_node is intentionally not inserted to avoid','line_number':382,'multiline':False]
['text':' accidentally acting on it (e.g. in eliminate dead code),','line_number':383,'multiline':False]
['text':' std::cout << *reverse_node << to view its state.','line_number':384,'multiline':False]
['text':' x -> dx mapping','line_number':389,'multiline':False]
['text':' NB: Not returning a gradient w.r.t. a value that requires grad is','line_number':442,'multiline':False]
['text':' normal if the input is non-differentiable. This happens e.g. in the','line_number':443,'multiline':False]
['text':' aten::type_as case.','line_number':444,'multiline':False]
['text':' NB: Not having a gradient defined w.r.t. an input to the graph which','line_number':456,'multiline':False]
['text':' requires grad can happen and is not an error. It might have been used','line_number':457,'multiline':False]
['text':' only in non-differentiable contexts (e.g. as second input to','line_number':458,'multiline':False]
['text':' aten::type_as). In that case we simply ignore it as an output, because it','line_number':459,'multiline':False]
['text':' won't ever produce any meaningful values.','line_number':460,'multiline':False]
['text':' Returns a topologically-sorted list of values produced in f, and used in its','line_number':471,'multiline':False]
['text':' reverse program.','line_number':472,'multiline':False]
['text':' Invariant: topo sorted','line_number':478,'multiline':False]
['text':' bool unseen = ','line_number':483,'multiline':True]
['text':' Any temporary value from the primal graphs needs to be captured for later use','line_number':498,'multiline':False]
['text':' in the reverse graph, to avoid costly recomputations. However, a lot of the','line_number':499,'multiline':False]
['text':' nodes we have in our graphs are simply constants, which are cheap to execute','line_number':500,'multiline':False]
['text':' and replicate, and so it's better to just copy them into the reverse graph,','line_number':501,'multiline':False]
['text':' without polluting the output lists unnecessarily.','line_number':502,'multiline':False]
['text':' is node defined inside container?','line_number':505,'multiline':False]
['text':' if this constant is _already_ defined in the backward pass','line_number':525,'multiline':False]
['text':' block, we do not need to duplicate and move it because','line_number':526,'multiline':False]
['text':' it already won't be part of the capture set','line_number':527,'multiline':False]
['text':' we need to fold aten::_size_if_not_equal at the differentiation time','line_number':552,'multiline':False]
['text':' while we know the shapes of aten::_size_if_not_equal's arguments','line_number':553,'multiline':False]
['text':' Otherwise, they will become inputs to a reverse Graph, and we will','line_number':554,'multiline':False]
['text':' lose this information and we don't profile Scalars, or Lists yet.','line_number':555,'multiline':False]
['text':' insert in front of _grad_sum_to_size','line_number':575,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':578,'multiline':False]
['text':' we need to fold aten::_size_if_not_equal at the differentiation time','line_number':593,'multiline':False]
['text':' while we know the shapes of aten::_size_if_not_equal's arguments','line_number':594,'multiline':False]
['text':' Otherwise, they will become inputs to a reverse Graph, and we will','line_number':595,'multiline':False]
['text':' lose this information and we don't profile Scalars, or Lists yet.','line_number':596,'multiline':False]
['text':' addReverseInline has to call gradientForNode if *any* of the inputs','line_number':637,'multiline':False]
['text':' require grad, but it will emit vjps for *all* inputs. Use DCE to remove','line_number':638,'multiline':False]
['text':' unnecessary nodes. Additionally, requires_grad() on intermediates is an','line_number':639,'multiline':False]
['text':' overapproximation of the real state, so we might have emitted some','line_number':640,'multiline':False]
['text':' gradients, only to realize that they were unnecessary once we reach a','line_number':641,'multiline':False]
['text':' point that doesn't require grad.','line_number':642,'multiline':False]
['text':' Of course, we need to filter out corresponding entries of grad_map, because','line_number':643,'multiline':False]
['text':' we don't want to accidentally access freed pointers later.','line_number':644,'multiline':False]
['text':' TODO: we are sometimes emitting expressions like','line_number':663,'multiline':False]
['text':' _grad_sum_to_size(_grad_sum_so_size(x, s1), s2), which are equivalent to','line_number':664,'multiline':False]
['text':' _grad_sum_to_size(x, s2), and could save us some','line_number':665,'multiline':False]
['text':' captures, but I'm not 100% sure how to optimize this at this stage, since','line_number':666,'multiline':False]
['text':' we don't know which GradOf blocks will be stitched together to form the','line_number':667,'multiline':False]
['text':' derivative. I guess a smart analysis could implement this, but I didn't','line_number':668,'multiline':False]
['text':' have time before the 1.0 release, so I put this only as a peephole','line_number':669,'multiline':False]
['text':' optimization.','line_number':670,'multiline':False]
['text':' TODO: see if this pass can be replaced with peephole pass','line_number':672,'multiline':False]
['text':' We generally add a lot of aten::size calls (for derivatives of broadcasting','line_number':674,'multiline':False]
['text':' operators), and they often end up duplicated, and would get captured','line_number':675,'multiline':False]
['text':' multiple times. Make sure we deduplicate them before lifting.','line_number':676,'multiline':False]
['text':' Takes a grad_desc.f returned from `addReverseInline` and splits off the','line_number':682,'multiline':False]
['text':' reverse_block into its own graph, storing it in df.','line_number':683,'multiline':False]
['text':' All intermediates needed in the second stage are added to','line_number':684,'multiline':False]
['text':' outputs of f, and taken as inputs in df. For a more','line_number':685,'multiline':False]
['text':' detailed description see Note [Gradient graphs] in autodiff.h.','line_number':686,'multiline':False]
['text':' This function also initializes the fields in grad_desc that were undefined','line_number':687,'multiline':False]
['text':' after `addReverseInline` (and extends `df_input_vjps` with vjps for captured','line_number':688,'multiline':False]
['text':' temporaries).','line_number':689,'multiline':False]
['text':' --------------------------------------------------------------------------','line_number':694,'multiline':False]
['text':' 1. Find values of f that need to be captured.','line_number':695,'multiline':False]
['text':' --------------------------------------------------------------------------','line_number':696,'multiline':False]
['text':' First, we need to find all values that are produced in f,','line_number':697,'multiline':False]
['text':' and used in df. They will need to be added as inputs of the df','line_number':698,'multiline':False]
['text':' and some of them may also need to be appended as outputs of f if','line_number':699,'multiline':False]
['text':' they are not already an input or an output of f','line_number':700,'multiline':False]
['text':' Invariant: topo sorted','line_number':701,'multiline':False]
['text':' --------------------------------------------------------------------------','line_number':704,'multiline':False]
['text':' 2. Prepare input/outputs lists for f and df','line_number':705,'multiline':False]
['text':' --------------------------------------------------------------------------','line_number':706,'multiline':False]
['text':' It's simple to construct primal_inputs/reverse_outputs,','line_number':707,'multiline':False]
['text':' but primal_outputs/reverse_inputs are much more subtle.','line_number':708,'multiline':False]
['text':' Here's a summary of how they are supposed to look like:','line_number':709,'multiline':False]
['text':'','line_number':710,'multiline':False]
['text':' Primal outputs:','line_number':711,'multiline':False]
['text':'   [original outputs], [temporaries]','line_number':712,'multiline':False]
['text':'','line_number':713,'multiline':False]
['text':' Reverse inputs:','line_number':714,'multiline':False]
['text':'   [output vjps (aka grad_outputs)], [temporary vjps]','line_number':715,'multiline':False]
['text':'   [captured primal values, in topological order],','line_number':716,'multiline':False]
['text':' -- Construct primal_outputs, df_input_captures, f_real_outputs ----','line_number':718,'multiline':False]
['text':' NOTE: we use emplace to avoid replacing an existing index if an output is','line_number':723,'multiline':False]
['text':' repeated','line_number':724,'multiline':False]
['text':' NB: reverse_captures are already deduplicated, and in topo order','line_number':730,'multiline':False]
['text':' If it's already an output we don't have to add anything,','line_number':732,'multiline':False]
['text':' but register the fact that it needs to be captured.','line_number':733,'multiline':False]
['text':' If it's an input, we could add it as an output but in fact it's','line_number':737,'multiline':False]
['text':' more efficient to use a special kind of capture.','line_number':738,'multiline':False]
['text':' Otherwise it's just a regular intermediate value that we need to add as','line_number':742,'multiline':False]
['text':' an output','line_number':743,'multiline':False]
['text':' we need to create a new temporary output for this capture because it','line_number':745,'multiline':False]
['text':' wasn't available.','line_number':746,'multiline':False]
['text':' -- Add VJPs for temporaries, adjust df_input_vjps -------------------------','line_number':760,'multiline':False]
['text':' NB [possible optimization]: use the newly added vjp input as soon as the','line_number':761,'multiline':False]
['text':' first vjp for that value is generated, to reduce the lifespan of this input','line_number':762,'multiline':False]
['text':' (currently we add it to the final vjp after all adds).','line_number':763,'multiline':False]
['text':' Add VJP inputs only for intermediates that actually required grad.','line_number':766,'multiline':False]
['text':' Note that we check the contents of the grad_map instead of','line_number':767,'multiline':False]
['text':' tmp->requires_grad(), because it's actually a more faithful source.','line_number':768,'multiline':False]
['text':' tmp->requires_grad() is really an overapproximation (i.e. it can have','line_number':769,'multiline':False]
['text':' false positives), while the gradients we will emit for this value can get','line_number':770,'multiline':False]
['text':' DCE-d in the optimization pass (because it has no influence on the real','line_number':771,'multiline':False]
['text':' f's outputs that we differentiate).','line_number':772,'multiline':False]
['text':' This is quite weird because we can't first make a sum and then replace','line_number':778,'multiline':False]
['text':' all uses of tmp_vjp_prev (that would replace its use in the sum too!), so','line_number':779,'multiline':False]
['text':' we create an incorrect sum that doesn't use prev vjp, replace uses, and','line_number':780,'multiline':False]
['text':' fix the sum.','line_number':781,'multiline':False]
['text':' can't move a node after a block param node','line_number':784,'multiline':False]
['text':' add the captures as formal arguments to the reverse_block','line_number':797,'multiline':False]
['text':' afterward inputs: [output vjps][temporary vjps][captures]','line_number':798,'multiline':False]
['text':' construct a map from captured 'value' to the index in the input list','line_number':799,'multiline':False]
['text':' used to extract this block into its own function','line_number':800,'multiline':False]
['text':' reverse_node was just to hold onto reverse_block in a debuggable way','line_number':824,'multiline':False]
['text':' we can remove it now.','line_number':825,'multiline':False]
['text':' Take ownership of the graph','line_number':839,'multiline':False]
['text':' XXX: Take care when handling outputs - they can be duplicated!','line_number':846,'multiline':False]
['text':' Fills in df_input_vjps and df_output_vjps','line_number':850,'multiline':False]
['text':' Clean up old nodes which has been replaced by forward graphs in torchscript','line_number':853,'multiline':False]
['text':' Fills in f, df, f_real_outputs, df_input_captures,','line_number':856,'multiline':False]
['text':' modifies df_input_vjps (new vjps are added for temporaries)','line_number':857,'multiline':False]
['text':' we have created a differentiable forward graph','line_number':861,'multiline':False]
['text':' which will be run with tensors that have their gradients detached,','line_number':862,'multiline':False]
['text':' so profiled types will have outdated requires_grad=True, update the','line_number':863,'multiline':False]
['text':' requires_grad property','line_number':864,'multiline':False]
['text':' namespace torch::jit','line_number':868,'multiline':False]
