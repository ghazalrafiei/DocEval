['text':' namespace','line_number':75,'multiline':False]
['text':' for debugging it is helpful to be able to force autodiff subgraphs','line_number':77,'multiline':False]
['text':' to be created, to check their correctness, even when the','line_number':78,'multiline':False]
['text':' size of the of the subgraph is too small to be profitable.','line_number':79,'multiline':False]
['text':' for debugging it is helpful to be able to force fusion groups','line_number':89,'multiline':False]
['text':' to be created','line_number':90,'multiline':False]
['text':' var_captures_.size() might be','line_number':113,'multiline':False]
['text':' greater than capture_size','line_number':114,'multiline':False]
['text':'  For TensorList, we have to flatten it to Tensors during saving and','line_number':127,'multiline':False]
['text':'  unflatten it back to TensorList when using it in backward apply().','line_number':128,'multiline':False]
['text':'  This is to avoid any implicit mutation to TensorList happened','line_number':129,'multiline':False]
['text':'  between forward & backward.','line_number':130,'multiline':False]
['text':' how do we turn a flattened list of tensors back into the ivalues that','line_number':194,'multiline':False]
['text':' the DifferentiableGraphBackward expects','line_number':195,'multiline':False]
['text':' consumes one size','line_number':233,'multiline':False]
['text':' unpack values packed by `packReturnValuesIntoTuple`','line_number':240,'multiline':False]
['text':' NB: stack.size() == num_outputs() is not always true','line_number':265,'multiline':False]
['text':' after we added TensorList support.','line_number':266,'multiline':False]
['text':' Example: aten::stack(Tensor[] tensors, int) where','line_number':267,'multiline':False]
['text':' tensors = [x, x]','line_number':268,'multiline':False]
['text':' Here stack.size()[=1] with a TensorList IValue of','line_number':269,'multiline':False]
['text':' backward graph output.','line_number':270,'multiline':False]
['text':' num_outputs()[=2], however, is the number of outputs of','line_number':271,'multiline':False]
['text':' grad_fn (an autograd::Node). grad_fn's outputs are','line_number':272,'multiline':False]
['text':' grads with regard to Tensor/Variables `x`, but not','line_number':273,'multiline':False]
['text':' graph input TensorList [x, x]. These two grads will','line_number':274,'multiline':False]
['text':' be accumulated to x.grad later using autograd::InputBuffer.','line_number':275,'multiline':False]
['text':' this undefined gradient actually corresponds to a tensor list','line_number':286,'multiline':False]
['text':' Input grad can also be None even if it requires grad','line_number':301,'multiline':False]
['text':' Example: `other` in expand_as(self, other)','line_number':302,'multiline':False]
['text':' We could have None passed here via `Optional[Tensor]`','line_number':336,'multiline':False]
['text':' NB: since our requires_grad setting is only a heuristic we might end','line_number':343,'multiline':False]
['text':' up wanting to differentiate through integral tensors, which is','line_number':344,'multiline':False]
['text':' generally a hard error in autograd.','line_number':345,'multiline':False]
['text':' we need to track input lists to fwd graph','line_number':396,'multiline':False]
['text':' since in backward graphs these will become','line_number':397,'multiline':False]
['text':' an undefined tensors if gradients are zeros','line_number':398,'multiline':False]
['text':' we will need to convert an undefined tensor','line_number':399,'multiline':False]
['text':' back to a list','line_number':400,'multiline':False]
['text':' TODO: switch to using UnpackInstructions','line_number':401,'multiline':False]
['text':' an optimized way of executing the subgraph computed directly on','line_number':406,'multiline':False]
['text':' tensors rather than Variables.','line_number':407,'multiline':False]
['text':' This will unwrap Variables, run the plan, and re-wrap them.','line_number':408,'multiline':False]
['text':' It can optionally also have a gradient which is hooked up','line_number':409,'multiline':False]
['text':' to the output Variables if present.','line_number':410,'multiline':False]
['text':' XXX: keep in mind that stack can be larger than the inputs we need!','line_number':420,'multiline':False]
['text':' hook up the outputs of df to the gradient functions of the inputs that','line_number':430,'multiline':False]
['text':' require gradients','line_number':431,'multiline':False]
['text':' hookup the gradients for the output tensors that require gradients','line_number':448,'multiline':False]
['text':' to the inputs to our gradient function df','line_number':449,'multiline':False]
['text':' TODO - XXX - if any output is the same tensor multiple times, views','line_number':450,'multiline':False]
['text':' have to be setup here. We need to refactor autograd until it is safe','line_number':451,'multiline':False]
['text':' for tensors to be constructed without all the viewing infrastructure.','line_number':452,'multiline':False]
['text':' this is currently intentionally not done here so we can get an idea of','line_number':453,'multiline':False]
['text':' our perf before introducing overhead for correctness','line_number':454,'multiline':False]
['text':' drop the temporary outputs so that we return the same number of','line_number':459,'multiline':False]
['text':' outputs as if we were not also calculating gradient','line_number':460,'multiline':False]
['text':' NOLINTNEXTLINE(performance-move-const-arg)','line_number':485,'multiline':False]
['text':' It would be nice to use an ArrayRef here, but unfortunately those can','line_number':491,'multiline':False]
['text':' only return const references, so we need to do a bunch of indexing','line_number':492,'multiline':False]
['text':' ourselves.','line_number':493,'multiline':False]
['text':' Capture (save) inputs that would be required to subsequently run backwards','line_number':500,'multiline':False]
['text':'is_output','line_number':505,'multiline':True]
['text':'is_output','line_number':512,'multiline':True]
['text':' anonymous namespace','line_number':539,'multiline':False]
['text':' namespace detail','line_number':567,'multiline':False]
['text':' If not completed, persist the Frame until complete.','line_number':611,'multiline':False]
['text':' unused ','line_number':612,'multiline':True]
['text':' a Graph can be created via tracing, or via a language-based frontend','line_number':617,'multiline':False]
['text':' GraphExecutor runs it. It can run the same graph on many different sizes','line_number':618,'multiline':False]
['text':' and different requires_grad states, and handles specializations for each','line_number':619,'multiline':False]
['text':' situation. GraphExecutor is completely unaware of tracing or module','line_number':620,'multiline':False]
['text':' parameters to keep the tracing concerns separated.','line_number':621,'multiline':False]
['text':' outside lock guard, to minimize the time holding the lock on the fast','line_number':665,'multiline':False]
['text':' path ArgumentSpec even computes its hashCode here.','line_number':666,'multiline':False]
['text':' Phase 0. Inline functions, then clean up any artifacts that the inliner','line_number':690,'multiline':False]
['text':'          left in that may inhibit optimization','line_number':691,'multiline':False]
['text':' Phase 1. Specialize to input definedness (this is very important for','line_number':707,'multiline':False]
['text':'          gradient graphs), and run required passes to bring the graph','line_number':708,'multiline':False]
['text':'          to an executable form.','line_number':709,'multiline':False]
['text':' Phase 2. Propagate detailed information about the spec through the','line_number':714,'multiline':False]
['text':'          graph (enabled more specializations in later passes).','line_number':715,'multiline':False]
['text':'          Shape propagation sometimes depends on certain arguments being','line_number':716,'multiline':False]
['text':'          constants, and constant propagation doesn't need shape','line_number':717,'multiline':False]
['text':'          information anyway, so it's better to run it first.','line_number':718,'multiline':False]
['text':' Phase 3. Run differentiable optimizations (i.e. simple graph rewrites','line_number':730,'multiline':False]
['text':'          that we can still execute using autograd).','line_number':731,'multiline':False]
['text':' Phase 4. If this graph will be differentiated, we need to slice out the','line_number':734,'multiline':False]
['text':'          symbolically differentiable subgraphs for further optimizations.','line_number':735,'multiline':False]
['text':' Phase 5. Apply non-differentiable optimizations to the graphs we've found','line_number':736,'multiline':False]
['text':'          (or the whole graph if we know we won't need its derivative).','line_number':737,'multiline':False]
['text':' Run post differentiation optimizations, Autodiff will replace some','line_number':750,'multiline':False]
['text':' parts of graph with new graph, these new graphs usually consists of','line_number':751,'multiline':False]
['text':' control flows and miss shape information on nodes, so we run shape','line_number':752,'multiline':False]
['text':' prop and differentiable optimizations to ensure the graph is','line_number':753,'multiline':False]
['text':' optimized','line_number':754,'multiline':False]
['text':' run non diff optimization on the forward graph','line_number':758,'multiline':False]
['text':' Make sure there are no leftovers from any passes.','line_number':770,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':778,'multiline':False]
['text':' Populated only when optimize is false (and in that case plan_cache will be','line_number':780,'multiline':False]
['text':' unused). The compiled version of graph.','line_number':781,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':782,'multiline':False]
['text':' Mapping from argument configurations to optimized versions of the graph','line_number':785,'multiline':False]
['text':' that are specialized to the spec.','line_number':786,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)','line_number':787,'multiline':False]
['text':' we are deprecating legacy executor','line_number':849,'multiline':False]
['text':' implicit inserted expand nodes are not necessarily always valid','line_number':866,'multiline':False]
['text':' when used inside script methods that might have unstable shapes','line_number':867,'multiline':False]
['text':' we remove the implicitly created ones, and have shape analysis','line_number':868,'multiline':False]
['text':' add valid expand nodes when the shapes are stable','line_number':869,'multiline':False]
['text':' Run custom passes that different backends can register.','line_number':925,'multiline':False]
['text':' decomposition pass, decompose certain ops that will be used in the','line_number':931,'multiline':False]
['text':' following passes (like batchmm and jit fusion)','line_number':932,'multiline':False]
['text':' TupleConstruct / TupleUnpack pairs can still be present at this point','line_number':936,'multiline':False]
['text':' and must be removed for fusion.','line_number':937,'multiline':False]
['text':' Rewrite subgraphs with many MMs into expressions that batch them.','line_number':941,'multiline':False]
['text':'composed_op','line_number':949,'multiline':True]
['text':' Run custom post-fusion passes','line_number':956,'multiline':False]
['text':' Basic graph preprocessing to eliminate noise.','line_number':968,'multiline':False]
['text':' Unroll small loops, and eliminate expressions that are the same at every','line_number':991,'multiline':False]
['text':' iteration.','line_number':992,'multiline':False]
['text':' run again with unrolled loops','line_number':1004,'multiline':False]
['text':' we are copying the block inside If or prim::Loop otherwise we are copying','line_number':1023,'multiline':False]
['text':' the whole graph we need to differentiate the two cases  because cloneFrom','line_number':1024,'multiline':False]
['text':' automatically adds inputs if we are copying graph's block and we will','line_number':1025,'multiline':False]
['text':'  need the inputs from a user otherwise','line_number':1026,'multiline':False]
['text':' namespace torch::jit','line_number':1064,'multiline':False]
