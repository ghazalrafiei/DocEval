['text':' Template for computing the offset into the tensor to access a value','line_number':25,'multiline':False]
['text':'printf("tensor ${tensor} sizes[${d}] = %d, strides[${d}] = %d\n", ${tensor}.sizes[${d}],${tensor}.strides[${d}]);','line_number':27,'multiline':False]
['text':' Note: The NAN, NEG_INFINITY and POS_INFINITY strings map to device-specific','line_number':44,'multiline':False]
['text':' implementations of these special values. These macros are found in the','line_number':45,'multiline':False]
['text':' resource strings for each device.','line_number':46,'multiline':False]
['text':' Note: Half is special-cased to avoid returning at::Half','line_number':63,'multiline':False]
['text':' something went wrong with the type analysis during shape propagation','line_number':103,'multiline':False]
['text':'includeBool=','line_number':113,'multiline':True]
['text':' We don't guard this on anything because in our type system for scalars,','line_number':118,'multiline':False]
['text':' there is not a distinction between `float` and `double`, however there','line_number':119,'multiline':False]
['text':' *is* a distinction in tensor scalar types. We conservatively insert a','line_number':120,'multiline':False]
['text':' cast here, which may end up being a no-op if the tensor's scalar type','line_number':121,'multiline':False]
['text':' is `double`.','line_number':122,'multiline':False]
['text':' Support None value for optional arguments like memory format','line_number':125,'multiline':False]
['text':' something went wrong with the type analysis during shape propagation','line_number':133,'multiline':False]
['text':' Writes RHS of special handling "simple mappable" ops','line_number':138,'multiline':False]
['text':' special case for clamp fusion on missing min/max inputs','line_number':140,'multiline':False]
['text':' Note: It may seem unusual to have the bounds as the first case below,','line_number':141,'multiline':False]
['text':' this is so that if min or max is NaN, they are "ignored"','line_number':142,'multiline':False]
['text':' and when the input is NaN, the output is, too','line_number':143,'multiline':False]
['text':' This struct specifies a template for dispatching specific aten:: operators.','line_number':168,'multiline':False]
['text':' The current variants of RHS code selection we support are for double and','line_number':169,'multiline':False]
['text':' float output values. For example, an aten::log operator which is assigned','line_number':170,'multiline':False]
['text':' to a float value would emit logf(), whereas an aten::log operator which is','line_number':171,'multiline':False]
['text':' assigned to a double would emit log().','line_number':172,'multiline':False]
['text':' Common case: float and double dispatch are identical','line_number':174,'multiline':False]
['text':' Writes "simple mappable" ops','line_number':185,'multiline':False]
['text':' unary','line_number':188,'multiline':False]
['text':' simple binary','line_number':222,'multiline':False]
['text':' binary with other','line_number':229,'multiline':False]
['text':' TODO: some of these ops will not get generated because','line_number':230,'multiline':False]
['text':' we only work on float inputs/outputs, but they are here to record','line_number':231,'multiline':False]
['text':' that they are valid mappable ops once we handle more type','line_number':232,'multiline':False]
['text':' alpha','line_number':254,'multiline':False]
['text':' where','line_number':259,'multiline':False]
['text':' PyTorch converts (scalar) argument types to result before applying the','line_number':274,'multiline':False]
['text':' operator e.g. 1.4-torch.tensor(3) = -2','line_number':275,'multiline':False]
['text':' Uncasted operands only used for comparison operators','line_number':279,'multiline':False]
['text':' allocate buffer to load 4','line_number':329,'multiline':False]
['text':' check if last dim is contiguous','line_number':332,'multiline':False]
['text':' disable on dtype > 4 bytes for performance','line_number':338,'multiline':False]
['text':' last dim size multiple of 4, other dim stride multiple of 4','line_number':344,'multiline':False]
['text':' last dim stride already checked above at compile time','line_number':348,'multiline':False]
['text':' pointer aligned','line_number':357,'multiline':False]
['text':' TODO: handle cases where we need to generate > 2^32 element tensors','line_number':363,'multiline':False]
['text':' Note: not uint32_t to avoid including cstdint','line_number':375,'multiline':False]
['text':' Lambda for writing arguments','line_number':386,'multiline':False]
['text':' + 1 because the first argument is the linearIndex','line_number':391,'multiline':False]
['text':' can't be unique() because Param may be an output','line_number':395,'multiline':False]
['text':' + 1 because the first argument is the linearIndex','line_number':413,'multiline':False]
['text':' can't be unique() because Param may be an output','line_number':417,'multiline':False]
['text':' + 1 because the first argument is the linearIndex','line_number':421,'multiline':False]
['text':' Writes input parameters','line_number':429,'multiline':False]
['text':' Writes output parameters','line_number':438,'multiline':False]
['text':' Acquires input values','line_number':443,'multiline':False]
['text':' Acquires and converts (if needed) inputs','line_number':452,'multiline':False]
['text':' Note: conversion from half is only supported for CUDA kernels.','line_number':453,'multiline':False]
['text':'  The conversion immediately converts fp16 inputs to float.','line_number':454,'multiline':False]
['text':'  Access for other types is common to CUDA and CPU kernels.','line_number':455,'multiline':False]
['text':' No __ldg overload for bool','line_number':480,'multiline':False]
['text':' load input in vectorized code path','line_number':495,'multiline':False]
['text':' Generates code for intermediate nodes','line_number':534,'multiline':False]
['text':' Note: Concat and Chunk are implicitly generated','line_number':535,'multiline':False]
['text':' Note: Random number generation is only supported for CUDA kernels.','line_number':536,'multiline':False]
['text':' Note: Constant None node is ignored and we will handle it in the','line_number':537,'multiline':False]
['text':'       places where the constant None node is used','line_number':538,'multiline':False]
['text':' Note: No need to iterate over reference as n is a pointer','line_number':539,'multiline':False]
['text':' Note: FusedConcat nodes work by narrowing the output Tensors before the','line_number':542,'multiline':False]
['text':' kernel runs','line_number':543,'multiline':False]
['text':' Always emit double for prim::Constant. This will be narrowed later based','line_number':554,'multiline':False]
['text':' on either:','line_number':555,'multiline':False]
['text':'  - Tensor-Scalar operator type rules','line_number':556,'multiline':False]
['text':'  - Math function rules','line_number':557,'multiline':False]
['text':' Generates writes to output tensors','line_number':582,'multiline':False]
['text':' Acquires and converts (if needed) outputs','line_number':589,'multiline':False]
['text':' Note: conversion to half is only supported for CUDA kernels.','line_number':590,'multiline':False]
['text':' store output in vectorized code path','line_number':609,'multiline':False]
['text':' Includes headers','line_number':639,'multiline':False]
['text':' Note: CUDA kernels support halfs and random generation, CPU kernels do not','line_number':640,'multiline':False]
['text':' HIP headers must be included until precompiled header feature is available','line_number':662,'multiline':False]
['text':' clang-format off','line_number':663,'multiline':False]
['text':' Still need the key defined, but empty.','line_number':677,'multiline':False]
['text':' clang-format on','line_number':681,'multiline':False]
['text':' Instantiates the CUDA or CPU-specific templates','line_number':683,'multiline':False]
['text':' namespace fuser','line_number':707,'multiline':False]
['text':' namespace jit','line_number':708,'multiline':False]
['text':' namespace torch','line_number':709,'multiline':False]
