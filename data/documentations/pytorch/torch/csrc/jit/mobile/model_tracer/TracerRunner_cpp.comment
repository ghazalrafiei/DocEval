['text':' Fetched from caffe2/aten/src/ATen/native/metal/MetalAten.mm','line_number':21,'multiline':False]
['text':' Diffusion Link: https://fburl.com/diffusion/atwwmax2','line_number':22,'multiline':False]
['text':'*
 * These are a collection of some common ATen methods that are usually
 * called outside of the Model's forward() run, and they need to be
 * traced to ensure that the used operators are included in the build.
 * If/When this list becomes too long, we can consider making it a
 * per-model list.
 ','line_number':45,'multiline':True]
['text':' TODO investigate how this is different from normal empty_strided','line_number':60,'multiline':False]
['text':' Create a byte tensor and copy it','line_number':66,'multiline':False]
['text':' Typically, failures show up in CopyKernel.cpp, so enumerating','line_number':72,'multiline':False]
['text':' common dtypes that may show up.','line_number':73,'multiline':False]
['text':'*
 * Similar to setup methods there are a suite a functions that often appear
 * under certain conditions but may avoid getting called in the trace due to the
 * narrow nature of bundled inputs
 ','line_number':96,'multiline':True]
['text':'*
 * Call methods on the Tensor object that we expect to be called
 * in production on this Tensor.
 ','line_number':134,'multiline':True]
['text':' Grab the jit operators','line_number':147,'multiline':False]
['text':' Grab the dispatcher operators','line_number':158,'multiline':False]
['text':' grab schema','line_number':161,'multiline':False]
['text':'*
 * For the vast majority of usecases the instrumentation in getCustomClass will
 * catch any custom classes referenced by a model. There are however, niche
 * situations that avoid the getCustomClass instrumentation due to some nuances
 * of mobile model deserialization. To get around that we can search through all
 * the used ops, and inspect their schemas to search for any referenced classes.
 * Example schema: prepacked::linear_clamp_prepack(Tensor W, Tensor? B=None,
 *   Scalar? output_min=None, Scalar? output_max=None) ->
 *   __torch__.torch.classes.xnnpack.LinearOpContext"
 ','line_number':175,'multiline':True]
['text':' All custom class types start with __torch__ not sure if this is by','line_number':195,'multiline':False]
['text':' chance or guaranteed','line_number':196,'multiline':False]
['text':' The name of a customClassType here is its fully qualified name, but','line_number':198,'multiline':False]
['text':' in registration only the class name is used so only record that','line_number':199,'multiline':False]
['text':' Function schemas can include other type indicators such as [] so we','line_number':201,'multiline':False]
['text':' need to trim to just alphanumeric + '_' characters as well','line_number':202,'multiline':False]
['text':' This check is only necessary because of GPU models.','line_number':212,'multiline':False]
['text':' Certain models can only run on a specific backend say metal.','line_number':213,'multiline':False]
['text':' Those ops will be present in the models root ops, but likely','line_number':214,'multiline':False]
['text':' not the tracer on linux','line_number':215,'multiline':False]
['text':' Load the module on CPU with the flag to skip the operator exists check.','line_number':233,'multiline':False]
['text':' This is needed so that we can load any TorchBind objects (custom classes)','line_number':234,'multiline':False]
['text':' that this model refers to so that any operators being called from those','line_number':235,'multiline':False]
['text':' TorchBind objects can be traced by the model tracer.','line_number':236,'multiline':False]
['text':' When we encounter a GPU model, we should call .cpu().copy_() on the','line_number':248,'multiline':False]
['text':' tensors in the bundled inputs, since this is what will happen when','line_number':249,'multiline':False]
['text':' such a model is executed on an iOS device (to copy the Tensor to Metal','line_number':250,'multiline':False]
['text':' memory via a call to .metal()).','line_number':251,'multiline':False]
['text':' When we encounter a CPU model, we should call .cpu().copy_() on the','line_number':259,'multiline':False]
['text':' tensors in the bundled inputs, since this is what will happen when','line_number':260,'multiline':False]
['text':' such a model is executed on an Android device since the PyTorch JNI','line_number':261,'multiline':False]
['text':' bindings call .cpu() in JIValue::newJIValueFromAtIValue().','line_number':262,'multiline':False]
['text':' If a user has bundled inputs since that api was updated to accept','line_number':265,'multiline':False]
['text':' bundled inputs for multiple methods They should go down this route.','line_number':266,'multiline':False]
['text':' Even if they only bundle inputs for forward they will have the new','line_number':267,'multiline':False]
['text':' style bundled inputs. Since at this time in tracer.cpp we do not know','line_number':268,'multiline':False]
['text':' what functions have bundled inputs we must call','line_number':269,'multiline':False]
['text':' get_bundled_inputs_functions_and_info if it exists to get the set.','line_number':270,'multiline':False]
['text':' Consume the result Tensor(s) when tracing on CPU since the','line_number':283,'multiline':False]
['text':' Android/Java JNI bindings will do the same.','line_number':284,'multiline':False]
['text':' If get_bundled_inputs_functions_and_info does not exists we default','line_number':288,'multiline':False]
['text':' to assuming they bundled before that change was made. If no bundled','line_number':289,'multiline':False]
['text':' inputs are found here either an error will be thrown','line_number':290,'multiline':False]
['text':' Consume the result Tensor(s) when tracing on CPU since the','line_number':299,'multiline':False]
['text':' Android/Java JNI bindings will do the same.','line_number':300,'multiline':False]
['text':' run with QNNPACK','line_number':329,'multiline':False]
['text':' Not every model can be successfully run with fbgemm,','line_number':334,'multiline':False]
['text':' but for those that can this can help broaden the tracers scope around','line_number':335,'multiline':False]
['text':' hyper optimized QNNPack paths','line_number':336,'multiline':False]
['text':' namespace mobile','line_number':396,'multiline':False]
['text':' namespace jit','line_number':397,'multiline':False]
['text':' namespace torch','line_number':398,'multiline':False]
