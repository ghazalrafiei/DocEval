['text':' This allocates a new chunk of memory every time (just FYI). It should be','line_number':18,'multiline':False]
['text':' amortized over the entire epoch hopefully.','line_number':19,'multiline':False]
['text':'dim=','line_number':32,'multiline':True]
['text':' You may want to store your indices with 32-bit or less, but here we need','line_number':33,'multiline':False]
['text':' to upcast to 64-bit. A batch itself won't hold too many indices, so that','line_number':34,'multiline':False]
['text':' should be ok. Note that if this indeed results in a type promotion, there','line_number':35,'multiline':False]
['text':' will be two allocations: one for the upcast slice, and one for the','line_number':36,'multiline':False]
['text':' returned `index_batch` vector.','line_number':37,'multiline':False]
['text':' namespace mobile','line_number':57,'multiline':False]
['text':' namespace jit','line_number':58,'multiline':False]
['text':' namespace torch','line_number':59,'multiline':False]
