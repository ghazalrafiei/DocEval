['text':' Takes in a TensorExprGraph of static shapes and generalizes the input shapes','line_number':12,'multiline':False]
['text':' to symbolic dimensions. Dimensions of value 1 will be preserved, otherwise','line_number':13,'multiline':False]
['text':' dimensions with the same value will be bucketed to the same symbolic shape.','line_number':14,'multiline':False]
['text':' E.g. Tensor(5, 3), Tensor(3, 1) -> Tensor(SS(-1), SS(-2)), Tensor(SS(-2), 1)','line_number':15,'multiline':False]
['text':' From there, runs symbolic shape inference on the graph, and creates a','line_number':16,'multiline':False]
['text':' versioning if in the graph with prim::TensorExprDynamicGuard checking if','line_number':17,'multiline':False]
['text':' the inputs at runtime match the Generalized Symbolic Shapes that are inputs','line_number':18,'multiline':False]
['text':' to the TE Kernel. The computate to calculate all symbolic dimensions is','line_number':19,'multiline':False]
['text':' inlined in to the if block with the TE Kernel. All Sym Dim Value* are','line_number':20,'multiline':False]
['text':' appended to the end of the TE Kernel Graph/Node inputs, and the Node is','line_number':21,'multiline':False]
['text':' augmented with a integer list attr `symbolic_shape_inputs` that gives the','line_number':22,'multiline':False]
['text':' mapping from Value * -> Symbolic Shape int64_t value. For more lengthy IR','line_number':23,'multiline':False]
['text':' examples and walkthrough look at ShapeAnalysisTest.DynamicShapesFusion in','line_number':24,'multiline':False]
['text':' `test_shape_analysis` Returns True on Success, False on Failure, can fail if','line_number':25,'multiline':False]
['text':' shape propagation fails to propagate # of dims or if complete shapes on','line_number':26,'multiline':False]
['text':' inputs not set','line_number':27,'multiline':False]
['text':' Tensors natively store whether they are contiguous or not as a property','line_number':36,'multiline':False]
['text':' this makes it faster to query `is_contiguous` or','line_number':37,'multiline':False]
['text':' `is_contiguous(memory_format=channels_last)`','line_number':38,'multiline':False]
['text':' than looping through the sizes/strides yourself','line_number':39,'multiline':False]
['text':' For tensors with these properties, we only store one value:','line_number':40,'multiline':False]
['text':' now, we describe other cases, where there is one stride enum','line_number':43,'multiline':False]
['text':' per dimension','line_number':44,'multiline':False]
['text':' STRIDE_ONE: packed','line_number':45,'multiline':False]
['text':' STRIDE_CONTIGUOUS: stride[i + 1] * sizes[i + 1]','line_number':46,'multiline':False]
['text':' STRIDE_TRANSPOSED_CONTIGUOUS: stride[i-1] * sizes[i-1]','line_number':47,'multiline':False]
['text':' STRIDE_AS_ARG: stride passed in as runtime value','line_number':48,'multiline':False]
['text':' namespace jit','line_number':54,'multiline':False]
['text':' namespace torch','line_number':55,'multiline':False]
