['text':' Conservatively compare two optionals. If both are undefined, assume','line_number':19,'multiline':False]
['text':' they aren't equal','line_number':20,'multiline':False]
['text':' NOLINTNEXTLINE(modernize-pass-by-value)','line_number':28,'multiline':False]
['text':' The intent for this optimization pass is to catch all of the small, easy to','line_number':43,'multiline':False]
['text':' catch peephole optimizations you might be interested in doing.','line_number':44,'multiline':False]
['text':'','line_number':45,'multiline':False]
['text':' TODO: Decide what kind of fixed point strategy we will have','line_number':46,'multiline':False]
['text':' XXX: remember that if you want to simplify an expression by combining','line_number':56,'multiline':False]
['text':' multiple nodes into a different one, then you need to check that they','line_number':57,'multiline':False]
['text':' all belong to the given block','line_number':58,'multiline':False]
['text':' TODO: this doesn't work with Scalar-Tensor ops! We should','line_number':59,'multiline':False]
['text':' canonicalize those','line_number':60,'multiline':False]
['text':' Eliminate no-op _grad_sum_to_size.','line_number':63,'multiline':False]
['text':' TODO: this doesn't work with Scalar-Tensor ops! We should','line_number':64,'multiline':False]
['text':' canonicalize those','line_number':65,'multiline':False]
['text':'const_inputs=','line_number':91,'multiline':True]
['text':' x.expand(x.size()) == x','line_number':92,'multiline':False]
['text':' x.t().t() == x','line_number':109,'multiline':False]
['text':' x.type_as(y) == x iff x.type() == y.type()','line_number':122,'multiline':False]
['text':' only handle one use case for now to avoid modifying mutated lists','line_number':169,'multiline':False]
['text':' TODO: canonicalize as aten::dim ?','line_number':170,'multiline':False]
['text':' XXX: remember that if you want to simplify an expression by combining','line_number':337,'multiline':False]
['text':' multiple nodes into a different one, then you need to check that they','line_number':338,'multiline':False]
['text':' all belong to the given block','line_number':339,'multiline':False]
['text':'const_inputs=','line_number':342,'multiline':True]
['text':' z + x.mm(y) == z.addmm(x, y) == x.mm(y) + z','line_number':343,'multiline':False]
['text':' Look for mm from both sides of the add','line_number':345,'multiline':False]
['text':' Add will accept tensors of mismatched scalar types, as long as','line_number':347,'multiline':False]
['text':' one of them is a scalar, but addmm will throw in that case, so we','line_number':348,'multiline':False]
['text':' can only perform this fusion if we're sure that it is correct,','line_number':349,'multiline':False]
['text':' and for that we need the add_mat_type. An alternative would be to','line_number':350,'multiline':False]
['text':' insert a type_as conditional on the tensor shape being a scalar,','line_number':351,'multiline':False]
['text':' but that might add overhead, and make analysis harder.','line_number':352,'multiline':False]
['text':' if we don't have the rank, we can't tell if the bias is a scalar','line_number':355,'multiline':False]
['text':' Attempts to find a matrix with a defined scalar type to type as','line_number':370,'multiline':False]
['text':' we can't use type_as if we don't know the target type (mm), the','line_number':378,'multiline':False]
['text':' bias needs to be coerced to','line_number':379,'multiline':False]
['text':' We insert the type_as if we're sure that the added element is a','line_number':384,'multiline':False]
['text':' scalar, and we either don't know the type of the scalar, or','line_number':385,'multiline':False]
['text':' know that it's mismatched.','line_number':386,'multiline':False]
['text':' Copy shape information from output node','line_number':411,'multiline':False]
['text':' FuseAddMM is a separate pass from peephole optimize because it is currently','line_number':436,'multiline':False]
['text':' used for exporting to ONNX.','line_number':437,'multiline':False]
['text':' Today, fusing add + MM has no benefit within PyTorch running ATen','line_number':438,'multiline':False]
['text':' ops. However, we rely on seeing the fused version of AddMM for ONNX export,','line_number':439,'multiline':False]
['text':' since otherwise after ONNX translation we would see redundant Gemm ops with','line_number':440,'multiline':False]
['text':' sub-optimal inputs.','line_number':441,'multiline':False]
['text':' It won't be helpful for ATen until we're able to represent','line_number':442,'multiline':False]
['text':'   torch.addmm(a, b, c, out=a).','line_number':443,'multiline':False]
['text':' That's because addmm dispatches internally to gemm, which computes:','line_number':444,'multiline':False]
['text':'   C = beta * C + alpha * A @ B','line_number':445,'multiline':False]
['text':' but aten::addmm(a, b, c, 1, 1) is really:','line_number':446,'multiline':False]
['text':'   D = beta * C + alpha * A @ B','line_number':447,'multiline':False]
['text':' and because it works out of place on C, we're only trading off an','line_number':448,'multiline':False]
['text':' explicit add for a copy inside the addmm function. Note that it','line_number':449,'multiline':False]
['text':' doesn't even result in fewer reads, because mm won't even load C','line_number':450,'multiline':False]
['text':' (because beta == 0 for it).','line_number':451,'multiline':False]
['text':' Eliminate dead code created by any peephole passes we've just done','line_number':464,'multiline':False]
['text':' namespace jit','line_number':471,'multiline':False]
['text':' namespace torch','line_number':472,'multiline':False]
