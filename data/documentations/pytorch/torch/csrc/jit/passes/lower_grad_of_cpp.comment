['text':' if any_defined(inputs):','line_number':11,'multiline':False]
['text':'  outputs = <original_computation>','line_number':12,'multiline':False]
['text':' else:','line_number':13,'multiline':False]
['text':'  outputs = autograd zero tensors','line_number':14,'multiline':False]
['text':' the else block returns a tensor for each of the outputs of the GradOf','line_number':28,'multiline':False]
['text':' i.e. assuming that all the outputs are tensors. This might not be','line_number':29,'multiline':False]
['text':' true, e.g. backward for cat() returns a list of gradient tensors.','line_number':30,'multiline':False]
['text':' This is fixed in DifferentiableGraphBackward, where the list sizes','line_number':31,'multiline':False]
['text':' are stored during the forward pass, and then undefined tensors are','line_number':32,'multiline':False]
['text':' turned into lists of undefined tensors where necessary.','line_number':33,'multiline':False]
['text':' namespace jit','line_number':44,'multiline':False]
['text':' namespace torch','line_number':45,'multiline':False]
