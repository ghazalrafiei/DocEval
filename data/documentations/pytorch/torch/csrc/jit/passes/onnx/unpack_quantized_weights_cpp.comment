['text':' TODO: Switch to per operator headers after','line_number':12,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/68693 is merged','line_number':13,'multiline':False]
['text':' Get the scale of the input to quantized op. There are two cases here','line_number':26,'multiline':False]
['text':' 1. For ops with output_scale specified in op signature, we get the output','line_number':27,'multiline':False]
['text':' scale','line_number':28,'multiline':False]
['text':' 2. For ops with no output scale in op signature (like quantized::relu)','line_number':29,'multiline':False]
['text':' we traverse up the graph to get the scale from its input until we hit a node','line_number':30,'multiline':False]
['text':' where scale is explicitly specified.','line_number':31,'multiline':False]
['text':' %r = quantized::linear(%input, %packed_weight, %w_scale, %w_zero_point)','line_number':57,'multiline':False]
['text':' %r = quantized::conv2d(%input, %packed_weight, %w_scale, %w_zero_point)','line_number':64,'multiline':False]
['text':' %r = quantized::conv2d_relu(%input, %packed_weight, %w_scale,','line_number':72,'multiline':False]
['text':' %w_zero_point)','line_number':73,'multiline':False]
['text':' %r = quantized::add(%input_a, %input_b, %w_scale, %w_zero_point)','line_number':81,'multiline':False]
['text':' For the _caffe2::Int8Sigmoid op output scale is 1.0/256','line_number':88,'multiline':False]
['text':' And output zero_point is set to 0 (quint8 type).','line_number':89,'multiline':False]
['text':' For the ops below the scale is not part of the op signature, so we traverse','line_number':92,'multiline':False]
['text':' up the graph to get the scale from its input when defined in the graph.','line_number':93,'multiline':False]
['text':' Retrieve scales and zero_points. Their formats are different depending on','line_number':140,'multiline':False]
['text':' different weight qscheme.','line_number':141,'multiline':False]
['text':' Cast to float since ONNX (De)QuantizeLinear only supports float scale.','line_number':149,'multiline':False]
['text':' Cast to float since ONNX (De)QuantizeLinear only supports float scale.','line_number':165,'multiline':False]
['text':' Need clone because at::from_blob does not take ownership of data.','line_number':193,'multiline':False]
['text':' Permute weights','line_number':261,'multiline':False]
['text':' Remove packed_params','line_number':266,'multiline':False]
['text':' Convert from int8 to uint8','line_number':273,'multiline':False]
['text':' Create caffe2::Int8GivenTensorFill node','line_number':276,'multiline':False]
['text':' CONV1D needs a different unpacking from CONV, since it's','line_number':300,'multiline':False]
['text':' packed as CONV2D intentionally at the first place.','line_number':301,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/pull/38248','line_number':302,'multiline':False]
['text':' This is called before the onnx pass. Using pattern matching we','line_number':305,'multiline':False]
['text':' find the relevant nodes and extract the packed_params. The packed_params are','line_number':306,'multiline':False]
['text':' passed to the appropriate unpack function using c10::Dispatcher. We insert','line_number':307,'multiline':False]
['text':' the unpacked weights and bias into the graph using','line_number':308,'multiline':False]
['text':' caffe2::Int8GivenTensorFill nodes.','line_number':309,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':356,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':358,'multiline':False]
['text':' Pre-unpacked weights. Comes from Conv/Linear weights which are','line_number':362,'multiline':False]
['text':' stored as bound C++ classes.','line_number':363,'multiline':False]
['text':' skip kSpatialDim','line_number':383,'multiline':False]
['text':' Suppress unused variable warning','line_number':386,'multiline':False]
['text':' Suppress unused variable warning','line_number':391,'multiline':False]
['text':' Suppress unused variable warning','line_number':396,'multiline':False]
['text':' Suppress unused variable warning','line_number':401,'multiline':False]
['text':' skip kSpatialDim','line_number':442,'multiline':False]
['text':' kSpatialDim = 2 even it's for Conv1D from torch.op to adopt Conv2D,','line_number':444,'multiline':False]
['text':' so we need a special unpack for Conv1D which has Conv2D dim.','line_number':445,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/pull/38248','line_number':446,'multiline':False]
['text':' Legacy','line_number':497,'multiline':False]
['text':' conv only parameters','line_number':500,'multiline':False]
['text':' Add bias','line_number':544,'multiline':False]
['text':' For quantized_linear inputs, the order is input, weight, bias, ....','line_number':592,'multiline':False]
['text':' Therefore bias is at location 2.','line_number':593,'multiline':False]
['text':' add conv arguments: stride, padding, dilation, groups, output_padding','line_number':597,'multiline':False]
['text':' skip (input, weight, bias)','line_number':608,'multiline':False]
['text':' Unpack quantized tensor inputs into {value, scale, zero_point},','line_number':635,'multiline':False]
['text':' Then create a prim::TupleConstruct node based on these three values.','line_number':636,'multiline':False]
['text':' scale and zero_point type can be found at torch/include/ATen/Operators.h','line_number':654,'multiline':False]
['text':'requires_grad=','line_number':658,'multiline':True]
['text':'requires_grad=','line_number':662,'multiline':True]
['text':' Erase the original quantized tensor input.','line_number':667,'multiline':False]
['text':' https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter#quantized-model-export','line_number':673,'multiline':False]
['text':' Caffe2 expects quantized ops to be in NHWC format while pytorch inputs are in','line_number':808,'multiline':False]
['text':' NCHW. This pass inserts permutes to convert from NCHW to NHWC before each','line_number':809,'multiline':False]
['text':' conv op and add another permute from NHWC to NCHW after the conv op.','line_number':810,'multiline':False]
['text':' namespace jit','line_number':865,'multiline':False]
['text':' namespace torch','line_number':866,'multiline':False]
