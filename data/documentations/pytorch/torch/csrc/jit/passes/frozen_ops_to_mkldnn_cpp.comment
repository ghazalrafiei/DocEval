['text':' clang-format off','line_number':28,'multiline':False]
['text':' moving ConvUtils include induces import cycle','line_number':29,'multiline':False]
['text':' clang-format on','line_number':45,'multiline':False]
['text':' no uses of tensors in container types','line_number':88,'multiline':False]
['text':' This function first calculates aliasing sets,','line_number':99,'multiline':False]
['text':' then calculates the last node each aliasing set is alive for.','line_number':100,'multiline':False]
['text':' Then we go through each node, if it's a node which has an equivalent','line_number':101,'multiline':False]
['text':' inplace node and the aliasing set for its input is dead afer this node, we','line_number':102,'multiline':False]
['text':' inplace it. Then we merge the aliasing sets for the input and output of the','line_number':103,'multiline':False]
['text':' node and extend the liveness of the set. To inplace a node you need to','line_number':104,'multiline':False]
['text':' prove device and dtype of the input and output are the same, which we've','line_number':105,'multiline':False]
['text':' already done, and prove that the output size is the same as the input size,','line_number':106,'multiline':False]
['text':' which is achieved by explicit Broadcast nodes (which we inserted for other','line_number':107,'multiline':False]
['text':' reasons).','line_number':108,'multiline':False]
['text':' The graphs here are simple subgraphs without uses of Tensors in','line_number':109,'multiline':False]
['text':' containers (Lists, GetAttrs, etc)','line_number':110,'multiline':False]
['text':' CALCULATE ALIASING SETS','line_number':112,'multiline':False]
['text':' map from Value to its Aliasing Set','line_number':116,'multiline':False]
['text':' CALCULATE ALIASING SET LIVENESS','line_number':146,'multiline':False]
['text':' map from aliased set -> last use of set','line_number':148,'multiline':False]
['text':' REUSING MEMORY BY REINPLACING NODES','line_number':171,'multiline':False]
['text':' defer making the inplacing change because that would invalidate the old','line_number':175,'multiline':False]
['text':' Node output Value*','line_number':176,'multiline':False]
['text':' the binary operators (add/mul) are commutative and only take tensor','line_number':197,'multiline':False]
['text':' inputs, so we can inplace either the first or second input','line_number':198,'multiline':False]
['text':' This is a factory function that creates an Operation that takes','line_number':227,'multiline':False]
['text':' MKLDNN tensors and unpacks them into 1D contiguous tensors that we can','line_number':228,'multiline':False]
['text':' run aten operations on. The precondition for using this function is that the','line_number':229,'multiline':False]
['text':' aten operations in `aten_op` should be an identity for zero inputs. In other','line_number':230,'multiline':False]
['text':' words, this should: `aten_op(0) = 0` The reason for this precondition has to','line_number':231,'multiline':False]
['text':' do with blocked formats MKLDNN uses to lay tensor elements (nChw8c, nChw16c,','line_number':232,'multiline':False]
['text':' etc). It splits the channel dimension into chunks of 8/16 makes it the','line_number':233,'multiline':False]
['text':' innermost dimension. Whenever the channel dim isn't divisible by 8/16 the','line_number':234,'multiline':False]
['text':' innermost dimension is padded with 0s. The precondition, `aten_op(0) == 0`','line_number':235,'multiline':False]
['text':' allows us to avoid any special casing of padded elements.','line_number':236,'multiline':False]
['text':' we cast `a` to an `ideep::tensor`, so we can get at its descriptor','line_number':243,'multiline':False]
['text':' which we then use to set up `out` tensor w/ the same props as a','line_number':244,'multiline':False]
['text':' tensor's physical size could be bigger than a logical one','line_number':249,'multiline':False]
['text':' `a_it.get_desc().get_size()` returns the real physical size (in bytes)','line_number':250,'multiline':False]
['text':' we use it to compute `nelem` for `aten` ops','line_number':251,'multiline':False]
['text':' we also wrap `a` storage into an aten tensor','line_number':254,'multiline':False]
['text':' `a_it.get_desc()` will allocate a tensor','line_number':261,'multiline':False]
['text':' of the right physical size.','line_number':262,'multiline':False]
['text':' enable_cudnn not used','line_number':286,'multiline':False]
['text':' TODO: follow up with MKLDNN what the best way is','line_number':317,'multiline':False]
['text':' to handle perf incompatible formats','line_number':318,'multiline':False]
['text':' mkldnn tensors only support reshape, not expand or view operators','line_number':331,'multiline':False]
['text':' TODO: consider to initializing to a blocked layout','line_number':338,'multiline':False]
['text':' directly if needed','line_number':339,'multiline':False]
['text':' If one of the inputs was expanded and converted to nchw/nhwc','line_number':356,'multiline':False]
['text':' we might end up in a very bad spot if the second argument','line_number':357,'multiline':False]
['text':' is in a blocked format. In this case, MKLDNN uses its','line_number':358,'multiline':False]
['text':' reference implementation for a binary operation that follows','line_number':359,'multiline':False]
['text':' these broadcasts and it could be up to ~100x slower.','line_number':360,'multiline':False]
['text':' We use a very simple heuristic to convert an arg in nchw','line_number':361,'multiline':False]
['text':' to the blocked format of the other argument.','line_number':362,'multiline':False]
['text':' `is_public_format` means a tensor's physical layout isn't in MKLDNN','line_number':367,'multiline':False]
['text':' blocked layout e.g. nchw or nhwc but not nChw8c','line_number':368,'multiline':False]
['text':' any op added to this registry needs to meet','line_number':414,'multiline':False]
['text':' the precondition: `aten_op(0) == 0`','line_number':415,'multiline':False]
['text':'alpha','line_number':509,'multiline':True]
['text':' aten::convolution does a lot of precomputation and dispatching before','line_number':513,'multiline':False]
['text':' mkldnn_convolution is called. registering here we can directly invoke the op','line_number':514,'multiline':False]
['text':' and avoid overhead. avoiding dispatch overhead for other operators - relu,','line_number':515,'multiline':False]
['text':' add, etc - did not benchmark as speeding up models noticeably. the additional','line_number':516,'multiline':False]
['text':' overhead of `convolution` warrants the custom operator.','line_number':517,'multiline':False]
['text':' XXX: this follows the schema convention of conv2d/conv3d, not','line_number':520,'multiline':False]
['text':' aten::mkldnn_convolution, which is different for some reason!','line_number':521,'multiline':False]
['text':' aten::convolution takes care of 0 dim case before calls into','line_number':538,'multiline':False]
['text':' backends','line_number':539,'multiline':False]
['text':' aten::convolution also checks dtype mismatches','line_number':553,'multiline':False]
['text':' registering as custom operators avoids Scalar->Tensor->Scalar conversion','line_number':568,'multiline':False]
['text':' in default bindings','line_number':569,'multiline':False]
['text':' This is registered as its own op instead of as prim::Constant bc it does not','line_number':601,'multiline':False]
['text':' serialize which is an invariant of prim::Constant','line_number':602,'multiline':False]
['text':' TODO: make mkldnn tensor serialize...','line_number':603,'multiline':False]
['text':' conv goes through special pathway so we can call mkldnn reorder conv','line_number':681,'multiline':False]
['text':' primitive','line_number':682,'multiline':False]
['text':' N.B. we can't use `insert` as it calls `getOperation` (via','line_number':704,'multiline':False]
['text':' `emitBuiltinCall`) which uses `min_val` and `max_val` attrs which we','line_number':705,'multiline':False]
['text':' haven't set yet.','line_number':706,'multiline':False]
['text':' this node doesnt handle string padding yet...','line_number':817,'multiline':False]
['text':' mkldnn does not support conv1d','line_number':853,'multiline':False]
['text':' _convolution is rewritten before this pass is invoked','line_number':854,'multiline':False]
['text':' [mkldnn perf strategy]','line_number':863,'multiline':False]
['text':' Certain ops - aten::linear, aten::conv2d, aten::conv3d - provide a huge speed','line_number':864,'multiline':False]
['text':' up just by converting the constant weights to MKLDNN AOT, and then at runtime','line_number':865,'multiline':False]
['text':' converting the non-constant input to_mkldnn before the op, and then back to','line_number':866,'multiline':False]
['text':' its original layout after the op. The speed up holds even if you end up','line_number':867,'multiline':False]
['text':' converting the input to_mkldnn and output back to_dense. We start groups of','line_number':868,'multiline':False]
['text':' ops to compute in MKLDNN only from these ops that are a strict speedup. Then,','line_number':869,'multiline':False]
['text':' we expand the groups to include operators which are computable in MKLDNN &','line_number':870,'multiline':False]
['text':' are roughly perf equal to eager. We do this in the hopes of joining multiple','line_number':871,'multiline':False]
['text':' fast nodes together, saving to_mkldnn and to_dense conversions.','line_number':872,'multiline':False]
['text':'','line_number':873,'multiline':False]
['text':' MKLDNN only supports float32 inputs for aten::linear, aten::conv2d &','line_number':874,'multiline':False]
['text':' aten::conv3d. We only fuse these nodes if the weights are float32, and then','line_number':875,'multiline':False]
['text':' we only include operators which we can prove will execute in float32. By','line_number':876,'multiline':False]
['text':' fusing topologically we can maintain the invariant that all tensor types in','line_number':877,'multiline':False]
['text':' the graph are floating point. In fusing Conv-> Add -> Relu -> Conv we start','line_number':878,'multiline':False]
['text':' with the first Conv, know that the output is float, and can then safely merge','line_number':879,'multiline':False]
['text':' Add and Relu. If we started with the last Conv, it would be difficult to','line_number':880,'multiline':False]
['text':' prove in our first pass that the Add's inputs were both float32 without first','line_number':881,'multiline':False]
['text':' fusing the first conv.','line_number':882,'multiline':False]
['text':' We maintain alias db correctness in-place while building up the autodiff','line_number':893,'multiline':False]
['text':' subgraphs, however it is difficult to preserve correctness when','line_number':894,'multiline':False]
['text':' un-inlining autodiff subgraphs. We first recursively construct all','line_number':895,'multiline':False]
['text':' subgraphs and then unmerge them into the graph','line_number':896,'multiline':False]
['text':' Run CSE globally onceto eliminate duplicates that may have occurred','line_number':899,'multiline':False]
['text':' while inlining subgraphs.','line_number':900,'multiline':False]
['text':' We need to run the slicer multiple times in order to get all merge','line_number':905,'multiline':False]
['text':' opportunities. This is because moveBeforeTopologicalValid may reorder','line_number':906,'multiline':False]
['text':' nodes to be AFTER the current iteration point. In order to properly','line_number':907,'multiline':False]
['text':' consider those nodes for merging, we need run the pass until no changes','line_number':908,'multiline':False]
['text':' have been made.','line_number':909,'multiline':False]
['text':'','line_number':910,'multiline':False]
['text':' Example:','line_number':911,'multiline':False]
['text':'   c = f(a, b)','line_number':912,'multiline':False]
['text':'   d = f(c)','line_number':913,'multiline':False]
['text':'   e = f(d)  <- iter is here, moving upward','line_number':914,'multiline':False]
['text':' After c.moveBeforeTopologicallyValid(e), we have:','line_number':915,'multiline':False]
['text':'   c = f(a, b)','line_number':916,'multiline':False]
['text':'   e = f(d)  <- iter still here','line_number':917,'multiline':False]
['text':'   d = f(c)  <- this was node moved on the other side.','line_number':918,'multiline':False]
['text':' Construct Subgraphs Recursively','line_number':930,'multiline':False]
['text':' if we're already in the process of merging','line_number':939,'multiline':False]
['text':' see [mkldnn perf strategy]','line_number':943,'multiline':False]
['text':' MKLDNN only supports floats of dimension > 0, so we only support','line_number':948,'multiline':False]
['text':' Tensors who have a known type or were previously verified','line_number':949,'multiline':False]
['text':' to be usable in an MKLDNN Group','line_number':950,'multiline':False]
['text':' We include ops here which are roughly perf-equivalent in mkldnn as with','line_number':970,'multiline':False]
['text':' aten (single & multithreaded) and whose inputs & outputs are float32.','line_number':971,'multiline':False]
['text':' unary ops we dont need to prove anything else than','line_number':989,'multiline':False]
['text':' the input is mkldnn supported','line_number':990,'multiline':False]
['text':' case aten::adaptive_max_pool2d: // return tuples which break fusion','line_number':1006,'multiline':False]
['text':' case aten::adaptive_max_pool3d: // return tuples which break fusion','line_number':1007,'multiline':False]
['text':' case aten::adaptive_avg_pool3d: // no ideep binding','line_number':1008,'multiline':False]
['text':' we need to maintain the following invariant `pointwise_func(0) == 0`,','line_number':1017,'multiline':False]
['text':' see `createUnaryOp`','line_number':1018,'multiline':False]
['text':' mkldnn doesn't currently support Tensor-Scalar add','line_number':1025,'multiline':False]
['text':' if we're already in the process of merging','line_number':1065,'multiline':False]
['text':' we successfully merged, so the new group's `outputs` may have','line_number':1091,'multiline':False]
['text':' changed. So rescan the new group for more merging opportunities.','line_number':1092,'multiline':False]
['text':' Try to merge `consumer` into `producer`. If successful, this destroys','line_number':1101,'multiline':False]
['text':' `consumer` and returns the `producer` group.','line_number':1102,'multiline':False]
['text':' namespace','line_number':1137,'multiline':False]
['text':' TODO: replace conv1d with conv2d ?','line_number':1141,'multiline':False]
['text':' Only remove tensor mutation if we know we're going to create speedups','line_number':1144,'multiline':False]
['text':' with mkldnn. Only supporting functional ops simplifies this pass bc','line_number':1145,'multiline':False]
['text':' running an op in mkldnn removes the aliasing relationships that','line_number':1146,'multiline':False]
['text':' previously existed between input and output.','line_number':1147,'multiline':False]
['text':' namespace jit','line_number':1183,'multiline':False]
['text':' namespace torch','line_number':1184,'multiline':False]
