['text':'
XXX: this is still in prototype phase and has much work left to do, including
but not limited to:
- Refactor APIs
- Add decent coverage of common ops
- Add shape analysis pass on Graph that handles Loops
- Allow concurrent reads to the operator map
- Supporting returning partially evaluated shape compute graph
','line_number':33,'multiline':True]
['text':' This is similar to c10::SymbolicShape, but instead of either having','line_number':48,'multiline':False]
['text':' a concrete dimension or a symbolic dimension, an argument may be:','line_number':49,'multiline':False]
['text':' - A Symbolic Dimension','line_number':50,'multiline':False]
['text':' - A Constant Integer','line_number':51,'multiline':False]
['text':' - Neither of the above. The third case can occur due to inputs to','line_number':52,'multiline':False]
['text':' ops like view that accept negative values. Maintaining the distinction','line_number':53,'multiline':False]
['text':' between an unknown symbolic dimension and an unknown integer allows','line_number':54,'multiline':False]
['text':' us to optimize out comparisons to values < 0 (symbolic shapes are always >=','line_number':55,'multiline':False]
['text':' 0) For example, a call like graph(%y: Tensor(SS(-1), 10, 10), %inp: int):','line_number':56,'multiline':False]
['text':'   %five: int = prim::Constant[value=5]()','line_number':57,'multiline':False]
['text':'   %zero: int = prim::Constant[value=0]()','line_number':58,'multiline':False]
['text':'   %1 : int = aten::size(%y, %zero)','line_number':59,'multiline':False]
['text':'   %2 : int[] = prim::ListConstruct(%five, %1, %inp)','line_number':60,'multiline':False]
['text':'   %y.2: Tensor(5, SS(-1), (New Symbolic Shape)) = aten::view(%y, %2)','line_number':61,'multiline':False]
['text':'','line_number':62,'multiline':False]
['text':' x.view([5, y.size(0), inp])','line_number':63,'multiline':False]
['text':' will have inputs equal to [5, SS(-1), c10::nullopt]','line_number':64,'multiline':False]
['text':' Superset of SymbolicShape, with additional support for unknown, nonsymbolic','line_number':117,'multiline':False]
['text':' vals','line_number':118,'multiline':False]
['text':' Allows List[Optional[List[Int]]]','line_number':195,'multiline':False]
['text':' TODO: lower simple tuples ?','line_number':223,'multiline':False]
['text':'refine_list_len','line_number':228,'multiline':True]
['text':' todo add return change for constant pooling','line_number':231,'multiline':False]
['text':' We need a list construct or a constant output','line_number':251,'multiline':False]
['text':' that is not written to in order to analyze the output shape','line_number':252,'multiline':False]
['text':' Symbolic Shape Analysis works through iteratively partially evaluating','line_number':269,'multiline':False]
['text':' a TorchScript shape compute graph by inputting properties from input','line_number':270,'multiline':False]
['text':' Tensors. We can substitute in properties like `len(x)` and `x[1]`','line_number':271,'multiline':False]
['text':' if they are statically on the input Tensors. We can also use','line_number':272,'multiline':False]
['text':' assertions like `assert len(x) == 4` in order to refine the input','line_number':273,'multiline':False]
['text':' length and unroll loops over its elements. We iteratively optimize and','line_number':274,'multiline':False]
['text':' substitute in properties until we are unable to make any further','line_number':275,'multiline':False]
['text':' optimizations. Finally, we try to extract Tensor properties from the output.','line_number':276,'multiline':False]
['text':' For instance `return [1, 2, inp[2] + 1, inp[3]]` we know that the output','line_number':277,'multiline':False]
['text':' will be length 4 with first two dimensions equal to 1 and 2. We can also','line_number':278,'multiline':False]
['text':' deduce that the 4th dimension has the same symbolic shape as inp[3], which','line_number':279,'multiline':False]
['text':' means that we do know its concrete value statically but we can assign sets','line_number':280,'multiline':False]
['text':' of tensor dimensions which must be equal at runtime.','line_number':281,'multiline':False]
['text':' For the case where we have a JIT graph,','line_number':288,'multiline':False]
['text':' substitute optional types for their component types','line_number':289,'multiline':False]
['text':' if the type is known. This doesn't need to be done','line_number':290,'multiline':False]
['text':' for known IValues.','line_number':291,'multiline':False]
['text':' None will get handled with constant substitution later','line_number':301,'multiline':False]
['text':' We handle non-constant values in the shape propagation step','line_number':317,'multiline':False]
['text':' Modifying the graph where _node is part of to not use the tensor','line_number':333,'multiline':False]
['text':' construct','line_number':334,'multiline':False]
['text':' When we have partially evaluate a list of Tensors like cat(tensor[])','line_number':336,'multiline':False]
['text':' We have a few problems:','line_number':337,'multiline':False]
['text':' - optimizing out calls to the length of the list: len(tensors)','line_number':338,'multiline':False]
['text':' - resolving accesses of the list to the tensor symbolic sizes the','line_number':339,'multiline':False]
['text':' corresponding list element We can solve both of these problems by','line_number':340,'multiline':False]
['text':' replacing the partial evaluation of cat([x, y]) def cat(tensors:','line_number':341,'multiline':False]
['text':' List[List[int]], dim: int)','line_number':342,'multiline':False]
['text':'    body','line_number':343,'multiline':False]
['text':' with','line_number':344,'multiline':False]
['text':' def cat(x, y, dim: int)','line_number':345,'multiline':False]
['text':'     tensors = [x, y]','line_number':346,'multiline':False]
['text':'     body','line_number':347,'multiline':False]
['text':'?','line_number':372,'multiline':False]
['text':' clang-format off','line_number':394,'multiline':False]
['text':' here we iteratively substitute properties of the node's input tensors','line_number':395,'multiline':False]
['text':' into the shape compute graph. we can substitute constants into the','line_number':396,'multiline':False]
['text':' like len(inp) or inp[0] if the tensor has a fixed length or a fixed','line_number':397,'multiline':False]
['text':' first dimension. we also try to resolve symbolic shapes of the same','line_number':398,'multiline':False]
['text':' symbolic value to the same Value * in the shape compute graph.','line_number':399,'multiline':False]
['text':' for the shape logic:','line_number':400,'multiline':False]
['text':' dim1 = inp1[0]','line_number':401,'multiline':False]
['text':' dim2 = inp2[0]','line_number':402,'multiline':False]
['text':' return dim1 if dim2 == 1 else dim2','line_number':403,'multiline':False]
['text':' if we see that inp1[0] and inp2[0] both have the same symbolic shape','line_number':404,'multiline':False]
['text':' value, then it is a valid transformation to replace dim2 with dim1 or','line_number':405,'multiline':False]
['text':' vice versa. to do this we collect all Value * for a particular symbolic','line_number':406,'multiline':False]
['text':' shape. Then, we replace all Value * within that set with their dominator.','line_number':407,'multiline':False]
['text':' In the example above, this allows us to infer  that the output will be the','line_number':408,'multiline':False]
['text':' symbolic dimension value of dim1.','line_number':409,'multiline':False]
['text':' if `symbolic_shape_values` is not null, record list accesses','line_number':411,'multiline':False]
['text':' which resolve to symbolic dimension values with their concrete symbolic','line_number':412,'multiline':False]
['text':' shape value. Because symbolic dimensions are represented as negative numbers and','line_number':413,'multiline':False]
['text':' are not real values, inserting them as constants in the graph would invalidate','line_number':414,'multiline':False]
['text':' the graph for further use. Instead, we keep track of what their value would be','line_number':415,'multiline':False]
['text':' for extracting output shapes.','line_number':416,'multiline':False]
['text':' clang-format on','line_number':417,'multiline':False]
['text':' Add support for testing symbolic shapes with dynamic dims','line_number':430,'multiline':False]
['text':' TODO: either decompose composite ops like slice or add handling here','line_number':433,'multiline':False]
['text':' check for dim >= 0, 0 <= dim','line_number':478,'multiline':False]
['text':' dim >= 0','line_number':479,'multiline':False]
['text':' 0 <= dim','line_number':484,'multiline':False]
['text':' check for dim comparisons to negative number','line_number':490,'multiline':False]
['text':' True if:','line_number':495,'multiline':False]
['text':' -2 != {Positive}','line_number':496,'multiline':False]
['text':' True if:','line_number':499,'multiline':False]
['text':' -2 <= / < {Positive}','line_number':500,'multiline':False]
['text':' {Positive} >= / > {-2}','line_number':501,'multiline':False]
['text':' `symbolic_set` represents a set of Value * which are all equal','line_number':519,'multiline':False]
['text':' to each other. Here, we optimize the graph by replacing values','line_number':520,'multiline':False]
['text':' in the set with other dominating values.','line_number':521,'multiline':False]
['text':' in the following example, where a, b and c are all in the same','line_number':522,'multiline':False]
['text':' symbolic set:','line_number':523,'multiline':False]
['text':' if cond:','line_number':524,'multiline':False]
['text':'    a = li[0]','line_number':525,'multiline':False]
['text':'    b = li[1]','line_number':526,'multiline':False]
['text':'    return [a, b]','line_number':527,'multiline':False]
['text':' else:','line_number':528,'multiline':False]
['text':'    c = li[0]','line_number':529,'multiline':False]
['text':'    return [c, c]','line_number':530,'multiline':False]
['text':' we can replace `b` with `a` because it is dominated by `a`,','line_number':531,'multiline':False]
['text':' but we cannot replace `c` with another dominating value','line_number':532,'multiline':False]
['text':' there are ways to compute this more efficiently but typically number of','line_number':534,'multiline':False]
['text':' Values for each symbolic set is low and this is cheap to run','line_number':535,'multiline':False]
['text':' symbolic shape concrete values are only used in final shape extraction','line_number':555,'multiline':False]
['text':'symbolic_shape_values','line_number':557,'multiline':True]
['text':' TODO: would be nice if there were easy facility to look at uses and see','line_number':572,'multiline':False]
['text':' if they are all pure instead of instantiating db.','line_number':573,'multiline':False]
['text':' for testing, we don't insert complete tensor shapes and rely on our','line_number':628,'multiline':False]
['text':' partial evaluation pipeline to propagate information.','line_number':629,'multiline':False]
['text':' this is a good proxy for our ability to propagate non-complete shape','line_number':630,'multiline':False]
['text':' information.','line_number':631,'multiline':False]
['text':' TODO: fix the List of integers implementation, and','line_number':647,'multiline':False]
['text':' extract out the shape changes, otherwise this is complete','line_number':648,'multiline':False]
['text':' NB: shape compute graphs may have less inputs than their node','line_number':649,'multiline':False]
['text':' counterparts to allow e.g. sharing one single unary definition','line_number':650,'multiline':False]
['text':' so iterate on # of shape inputs','line_number':651,'multiline':False]
['text':' We make lists of Tensor inputs variadic, which results in','line_number':652,'multiline':False]
['text':' offset between a node index and its corresponding graph index','line_number':653,'multiline':False]
['text':' waiting for more use cases to decide on best generalization','line_number':664,'multiline':False]
['text':' it is a very common in graphs to see patterns like:','line_number':691,'multiline':False]
['text':' z = x.view(y.size())','line_number':692,'multiline':False]
['text':' or:','line_number':693,'multiline':False]
['text':' z = x.view(1, 10, y.size(0), y.size(1))','line_number':694,'multiline':False]
['text':' We want to propagate symbolic dimensions and concrete sizes','line_number':695,'multiline':False]
['text':' from y to z. To do this we try to associate symbolic dimensions','line_number':696,'multiline':False]
['text':' or concrete sizes with the integer list inputs that have a','line_number':697,'multiline':False]
['text':' constructor taken from constants or y.size() or y.size(0)','line_number':698,'multiline':False]
['text':' if we are getting a size of a tensor, it is an unknown','line_number':709,'multiline':False]
['text':' symbolic dimension instead of an unknown integer (must be','line_number':710,'multiline':False]
['text':' >=0)','line_number':711,'multiline':False]
['text':' TODO: At some point we might want to add support for dynamic dims','line_number':789,'multiline':False]
['text':' TODO: Merge equivalent expressions (not needed for current use case)','line_number':796,'multiline':False]
['text':' We want to build up a computational graph which computes all shapes','line_number':822,'multiline':False]
['text':' we dont know statically - that is, all symbolic shapes within','line_number':823,'multiline':False]
['text':' the region [beg, end). it must be executable before beg.','line_number':824,'multiline':False]
['text':' TODO: dont require dimensions of tensors to be set AOT ?','line_number':825,'multiline':False]
['text':' TODO: generalize logic to for other tensor input ops when they are','line_number':832,'multiline':False]
['text':' added','line_number':833,'multiline':False]
['text':' TODO: dont require # of dimensions of tensors set ?','line_number':858,'multiline':False]
['text':' for any output that is duplicated, the symbolic shape must be equal','line_number':877,'multiline':False]
['text':' take the symbolic shape that is generated first and get equivalent ones','line_number':878,'multiline':False]
['text':' this Value is already contained, so the symbolic shape for i must be','line_number':886,'multiline':False]
['text':' equal to the symbolic shape at the existing index','line_number':887,'multiline':False]
['text':' we are building up the large shape compute graph by iteratively','line_number':963,'multiline':False]
['text':' combining partially evaluated individual node shape graphs.','line_number':964,'multiline':False]
['text':' We need to maintain two mappings, one from non-Tensor inputs in the','line_number':966,'multiline':False]
['text':' enclosing graph to their equivalent mappings within the large shape','line_number':967,'multiline':False]
['text':' compute graph, and one from symbolic shape dimension to new node output','line_number':968,'multiline':False]
['text':' When we add a new tensor node, we do two things:','line_number':970,'multiline':False]
['text':' 1: record a mapping from the tensor node output to its shape in the','line_number':971,'multiline':False]
['text':' partial eval graph 2: add each symbolic shape dimension that we have','line_number':972,'multiline':False]
['text':' not already added as a output to the large shape compute graph','line_number':973,'multiline':False]
['text':' Once we are done stitching together all partial eval'd graphs, we can','line_number':975,'multiline':False]
['text':' cleanup the graph and remove the unneeded complete shapes as outputs,','line_number':976,'multiline':False]
['text':' leaving us only compute for calculating the runtime value of symbolic','line_number':977,'multiline':False]
['text':' dimensions','line_number':978,'multiline':False]
['text':' leaving us only compute for calculating the runtime value of symbolic','line_number':979,'multiline':False]
['text':' dimensions','line_number':980,'multiline':False]
['text':' TODO: generalize logic','line_number':983,'multiline':False]
['text':' make sure all symbolic dimensions in the graph we are creating are','line_number':1013,'multiline':False]
['text':' computed in the partial eval graph','line_number':1014,'multiline':False]
['text':' TODO: handle loop','line_number':1100,'multiline':False]
['text':' namespace','line_number':1116,'multiline':False]
['text':' Avoid doing all this work for functions that don't have a','line_number':1138,'multiline':False]
['text':' supported schema','line_number':1139,'multiline':False]
['text':' Handle bounded shape option','line_number':1156,'multiline':False]
['text':' Stitch together the values','line_number':1164,'multiline':False]
['text':' namespace jit','line_number':1186,'multiline':False]
['text':' namespace torch','line_number':1187,'multiline':False]
