['text':' First input of quantize node is FP32 weight','line_number':36,'multiline':False]
['text':' namespace','line_number':44,'multiline':False]
['text':' Must run this pass after constant folding.','line_number':53,'multiline':False]
['text':' int + method name gives unique identifier','line_number':59,'multiline':False]
['text':' Now register attribute for this packed param but dont set it to any','line_number':84,'multiline':False]
['text':' value. No value because we dont know what the value is at this point.','line_number':85,'multiline':False]
['text':' Only when we run on-device ptq workflow, e.g. run quantize_forward','line_number':86,'multiline':False]
['text':' method, is when the linear_prepack op will be executed and at that','line_number':87,'multiline':False]
['text':' point we will have the actual value for this attribute.','line_number':88,'multiline':False]
['text':' In order to add the output of linear_prepack, we now have to do','line_number':90,'multiline':False]
['text':' setAttr Thus when quantize_forward is actually called the attribute','line_number':91,'multiline':False]
['text':' is appropriately set.','line_number':92,'multiline':False]
['text':' Now let's add GetAttr for the same attribute.','line_number':96,'multiline':False]
['text':' Why?','line_number':97,'multiline':False]
['text':' Because eventually the method being modified will be cloned into','line_number':98,'multiline':False]
['text':' quantize_forward and quantized_forward.','line_number':99,'multiline':False]
['text':' quantize_forward will only have, for example, linear_prepack and','line_number':100,'multiline':False]
['text':' SetAttr Thus when quantize_forward is run attributes on the module','line_number':101,'multiline':False]
['text':' are set. Then in quantized_forward we will actually get','line_number':102,'multiline':False]
['text':' packed_params, via GetAttr and supply it to, for example,','line_number':103,'multiline':False]
['text':' dynamic_linear At the end quantize_forward will not have any ops like','line_number':104,'multiline':False]
['text':' dynamic_linear and quantized_forward will not have any linear_prepack','line_number':105,'multiline':False]
['text':' or SetAttr','line_number':106,'multiline':False]
['text':' We must replace this specific usage and we cannot doe','line_number':110,'multiline':False]
['text':' replaceAllUsesWith This is because we first had to insert SetAttr','line_number':111,'multiline':False]
['text':' node. This also takes as input packed_param_value, similar to the','line_number':112,'multiline':False]
['text':' actual op. But only the use of the actual op must be replaced by','line_number':113,'multiline':False]
['text':' output of GetAttr. Input of SetAttr still must use the','line_number':114,'multiline':False]
['text':' packed_param_value','line_number':115,'multiline':False]
['text':' Record the name of the attribute so that we can delete the SetAttr','line_number':117,'multiline':False]
['text':' for it','line_number':118,'multiline':False]
['text':' Now make sure that original weight is reset such that the module','line_number':121,'multiline':False]
['text':' does not have weight attribute set anymore','line_number':122,'multiline':False]
['text':' none_node->output()->setType(TensorType::create(at::kFloat,','line_number':130,'multiline':False]
['text':' c10::kCPU, 1, false));','line_number':131,'multiline':False]
['text':' namespace jit','line_number':148,'multiline':False]
['text':' namespace torch','line_number':149,'multiline':False]
