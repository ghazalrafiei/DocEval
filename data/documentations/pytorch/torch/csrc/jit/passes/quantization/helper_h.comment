['text':' Vector of a module and the name of its method','line_number':16,'multiline':False]
['text':' Map of quantization parameter name and value','line_number':18,'multiline':False]
['text':' for example _scale, _zero_point,','line_number':19,'multiline':False]
['text':' _scalar_type and _axis(for per channel quantization)','line_number':20,'multiline':False]
['text':' =========== helper functions for Value =========','line_number':23,'multiline':False]
['text':' Check if a value is weight, since we need to use weight observer','line_number':24,'multiline':False]
['text':' for weight','line_number':25,'multiline':False]
['text':' Check if a value is bias for conv and linear, which we do not','line_number':28,'multiline':False]
['text':' quantize','line_number':29,'multiline':False]
['text':' Get the use as scalar input of clamp ops for the input value','line_number':34,'multiline':False]
['text':' For a given value `v`, get the list of values that we need to check','line_number':37,'multiline':False]
['text':' if they are observed/quantized or not, if so, we can say the','line_number':38,'multiline':False]
['text':' `v` is also observed/quantized, since we can derive','line_number':39,'multiline':False]
['text':' the quantization parameters for `v` given the list of values','line_number':40,'multiline':False]
['text':' Clones the method by the name of orig_method_name into new_method_name method','line_number':43,'multiline':False]
['text':' Check if a value in the graph is a Scalar value','line_number':49,'multiline':False]
['text':' Check if value is the input of the graph','line_number':52,'multiline':False]
['text':' Converts a mangled name, such as','line_number':55,'multiline':False]
['text':'   __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_7.Conv2d','line_number':56,'multiline':False]
['text':' into an unmangled name, such as','line_number':57,'multiline':False]
['text':'   __torch__.torch.ao.nn.quantized.modules.conv.Conv2d','line_number':58,'multiline':False]
['text':' Return the module name that corresponds to the value.','line_number':61,'multiline':False]
['text':' =========== helper functions for Node =========','line_number':64,'multiline':False]
['text':' Check if the node will produce the same result regardless of whether','line_number':75,'multiline':False]
['text':' the input tensor is quantized or not, example: aten::size','line_number':76,'multiline':False]
['text':' Check if this the propagate op that has single input, e.g. aten::cat','line_number':79,'multiline':False]
['text':' Check if this is the propagate op that has two inputs, e.g. aten::add','line_number':82,'multiline':False]
['text':' Check if this is the node that we'll quantize or not quantize depending on','line_number':85,'multiline':False]
['text':' whether the input of the node is quantized, example: aten::cat','line_number':86,'multiline':False]
['text':' Check if the node is a binary op like aten::add and aten::mul and','line_number':89,'multiline':False]
['text':' if the input 1 is a scalar, these ops will be quantized to','line_number':90,'multiline':False]
['text':' quantized::{op}_scalar','line_number':91,'multiline':False]
['text':' We don't want to analyze the graph for some `builtin` CallFunctions','line_number':97,'multiline':False]
['text':' like `linear` because we want to preserve the op boundary','line_number':98,'multiline':False]
['text':' Check if the node has scalar input','line_number':101,'multiline':False]
['text':' Check if a node is quantizable','line_number':104,'multiline':False]
['text':' Nodes which only require quantization of weight value, eg. embedding_bag','line_number':109,'multiline':False]
['text':' Check if a use of the value is quantizable, this depends on','line_number':112,'multiline':False]
['text':' both the use node and the offset','line_number':113,'multiline':False]
['text':' Given a CallFunction node, extract the graph of the called function','line_number':116,'multiline':False]
['text':' Check if `use` is a CallFunction of name `func_name` and if value','line_number':119,'multiline':False]
['text':' `v` is the nth argument (if provided) of the function','line_number':120,'multiline':False]
['text':' Check if `use` is a AtenFunction of name `func_name` and if value','line_number':126,'multiline':False]
['text':' `v` is the nth argument (if provided) of the function','line_number':127,'multiline':False]
['text':' =========== helper functions for Block =========','line_number':133,'multiline':False]
['text':' checks if a block will always raise an Exception','line_number':134,'multiline':False]
['text':' =========== helper functions for Module  ==========','line_number':137,'multiline':False]
['text':' TODO: remove','line_number':138,'multiline':False]
['text':' TODO: remove','line_number':142,'multiline':False]
['text':' Given an CallMethod node, get the module instance corresponding','line_number':146,'multiline':False]
['text':' to the instance Value','line_number':147,'multiline':False]
['text':' TODO: refactor all current uses of this function to the Opt one','line_number':148,'multiline':False]
['text':' Given an CallMethod node, get the module instance corresponding','line_number':151,'multiline':False]
['text':' to the instance Value if the instance is a module, otherwise return','line_number':152,'multiline':False]
['text':' c10::nullopt','line_number':153,'multiline':False]
['text':' ==================== filter functions for matches ==============','line_number':159,'multiline':False]
['text':' filter to check Value `vname` is a constant of int value `value`','line_number':160,'multiline':False]
['text':' filter to check if the %alpha argument of aten::add is constant 1','line_number':167,'multiline':False]
['text':' filter to check if the functional in CallFunction is relu','line_number':172,'multiline':False]
['text':' filter to check if the module is torch.nn.ReLU','line_number':177,'multiline':False]
['text':' TODO: add a macro to declare the filters','line_number':186,'multiline':False]
['text':' namespace jit','line_number':215,'multiline':False]
['text':' namespace torch','line_number':216,'multiline':False]
