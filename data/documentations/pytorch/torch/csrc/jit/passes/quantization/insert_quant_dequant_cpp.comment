['text':' dynamic quantization ops for activation: choose_qparams, quant, dequant','line_number':24,'multiline':False]
['text':' This is only so that insertQuantizationOps can be templatized','line_number':32,'multiline':False]
['text':' and subsequently significant portion of that code can be reused.','line_number':33,'multiline':False]
['text':' Go through the CallMethod graph to check if the value is Weight.','line_number':57,'multiline':False]
['text':' Check to make sure all the CallMethods in the graph produce the same','line_number':75,'multiline':False]
['text':' output.','line_number':76,'multiline':False]
['text':' Set the reduce range to default to true, since qnnpack backend ignores this','line_number':92,'multiline':False]
['text':' argument.','line_number':93,'multiline':False]
['text':' choose_qparams_per_tensor has 2 outputs, (scale, zero_point).','line_number':96,'multiline':False]
['text':' num_outputs = ','line_number':100,'multiline':True]
['text':' copy uses to vector since value->uses() is a reference','line_number':139,'multiline':False]
['text':' and changing the graph will also change the uses() list','line_number':140,'multiline':False]
['text':' Insert dequantize node right before use node, because','line_number':145,'multiline':False]
['text':' we want to make sure use node and dequantize node reside','line_number':146,'multiline':False]
['text':' in the same block so that quant fusion can happen','line_number':147,'multiline':False]
['text':' replace original_output with tensor','line_number':181,'multiline':False]
['text':' If the weight value is outside of the range for FP16 range, i.e. [5.96e-8,','line_number':213,'multiline':False]
['text':' 65504], we saturate the values to the min/max of this range.','line_number':214,'multiline':False]
['text':' find the observer for Value `v` and return the name of the observer','line_number':223,'multiline':False]
['text':' Note that here we just check for the name of observer, but the ideally','line_number':225,'multiline':False]
['text':' we should be comparing the type of observer, this is a temporary','line_number':226,'multiline':False]
['text':' work around until data only clone of module.clone is supported.','line_number':227,'multiline':False]
['text':' if PlaceholderObserver is (anywhere) in name','line_number':243,'multiline':False]
['text':' Insert quant and dequant nodes into the graph for both static and dynamic','line_number':288,'multiline':False]
['text':' quant.','line_number':289,'multiline':False]
['text':' Insert GetAttr nodes for quantization parameters','line_number':300,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':341,'multiline':False]
['text':' We expect that the output of the weight observer will be consumed by the','line_number':343,'multiline':False]
['text':' embedding_bag operator.','line_number':344,'multiline':False]
['text':' Insert prepack op','line_number':352,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)','line_number':357,'multiline':False]
['text':' Create and insert quantized embedding op.','line_number':363,'multiline':False]
['text':' input 0 is the output of prepack op.','line_number':373,'multiline':False]
['text':' Last input is added after we account for extra input in 4-bit case.','line_number':374,'multiline':False]
['text':' The sparse field in the float operator denotes sparse gradients.','line_number':378,'multiline':False]
['text':' For inference this stands for pruned weights. We currently don't support','line_number':379,'multiline':False]
['text':' pruning in graph mode API so we set the field to 0 for inference.','line_number':380,'multiline':False]
['text':' indices','line_number':386,'multiline':False]
['text':' offsets','line_number':387,'multiline':False]
['text':' scale_grad_by_freq','line_number':389,'multiline':False]
['text':' mode','line_number':390,'multiline':False]
['text':' pruned_weights','line_number':391,'multiline':False]
['text':' per_sample_weights','line_number':393,'multiline':False]
['text':' compressed_indices_mapping','line_number':396,'multiline':False]
['text':' Verify that the outputs (apart from index 0) have no uses in the graph.','line_number':408,'multiline':False]
['text':' Observer output','line_number':435,'multiline':False]
['text':' Inserting before insert point','line_number':437,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':447,'multiline':False]
['text':' Temporary solution to quantize embedding_bag operators. Will be re-written','line_number':449,'multiline':False]
['text':' once we support quantization of embedding_bag weights.','line_number':450,'multiline':False]
['text':' Special case for embedding bag operators indices input - we don't','line_number':459,'multiline':False]
['text':' quantize the input but we still need to insert observers for it because','line_number':460,'multiline':False]
['text':' the order of input and weight can be changed in the module code.','line_number':461,'multiline':False]
['text':' For activation tensors we insert choose_qparams, quant, dequant ops.','line_number':473,'multiline':False]
['text':' dtype does not require quantization, e.g. float32','line_number':479,'multiline':False]
['text':' will just remove the observer call','line_number':480,'multiline':False]
['text':' For weight tensors we insert quant-dequant ops.','line_number':485,'multiline':False]
['text':' Static quant','line_number':488,'multiline':False]
['text':' get input of quantize call.','line_number':533,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':546,'multiline':False]
['text':' Values can be used multiple times in a single node','line_number':575,'multiline':False]
['text':' Values can be used multiple times in a single node','line_number':602,'multiline':False]
['text':' Expect first two elements of the tuple to be Tensor','line_number':666,'multiline':False]
['text':' Given a list of nodes, build a graph corresponding to these nodes.','line_number':679,'multiline':False]
['text':' User should make sure to run this graph with expected input.','line_number':680,'multiline':False]
['text':' Given a list of nodes in src, produce a Graph with these nodes.','line_number':685,'multiline':False]
['text':' Clone node in the destination Graph g.','line_number':691,'multiline':False]
['text':' Cleanup observer nodes from graph and observer modules','line_number':707,'multiline':False]
['text':' from module object and ClassType','line_number':708,'multiline':False]
['text':' Cleanup observer nodes only but not modules','line_number':711,'multiline':False]
['text':' This is for ondevice PTQ','line_number':712,'multiline':False]
['text':' In order to propagate quantization ops through the ops that doesn't','line_number':715,'multiline':False]
['text':' require observation, we'll first inline the graph, and call the','line_number':716,'multiline':False]
['text':' PropagateQuantizationOps pass','line_number':717,'multiline':False]
['text':' Used for dynamic quantization to selectively run the weight observers.','line_number':720,'multiline':False]
['text':' It extracts the subgraph corresponding to the weight and runs it with','line_number':721,'multiline':False]
['text':' the module instance.','line_number':722,'multiline':False]
['text':' Get quantization parameter map of the given Value in Graph','line_number':730,'multiline':False]
['text':' by searching for observer module of the value and extract the','line_number':731,'multiline':False]
['text':' quantization parameters from the observer module','line_number':732,'multiline':False]
['text':' FIXME[T110786721]: This check was broken before nevery failing.','line_number':743,'multiline':False]
['text':' Once fixed, this check triggers and fails tests.','line_number':744,'multiline':False]
['text':' Fix the tests that enabling this check produce!','line_number':745,'multiline':False]
['text':'
      TORCH_CHECK(
          qscheme_for_graph_.at(g) == qscheme,
          "Quantizing same graph with different types of "
          "QSchemes is not supported.\n",
          " Expecting:",
          c10::toString(qscheme_for_graph_.at(g)),
          " Got:",
          c10::toString(qscheme));
      ','line_number':746,'multiline':True]
['text':' Function that extracts and runs the weight observer in a separate','line_number':770,'multiline':False]
['text':' subgraph.','line_number':771,'multiline':False]
['text':' Recursively find the nodes that produce the value and add to subgraph.','line_number':777,'multiline':False]
['text':' Quantizes two types of general ops(ops that works both for floating point','line_number':780,'multiline':False]
['text':' and quantized Tensors) in this pass','line_number':781,'multiline':False]
['text':' for ops that only manipulates shape, e.g. flatten, quantization','line_number':782,'multiline':False]
['text':' is done by swapping with previous dequantize op','line_number':783,'multiline':False]
['text':' for ops that manipulates values of Tensor, e.g. average pool, quantization','line_number':784,'multiline':False]
['text':' is done by inserting quant/dequant ops after the op','line_number':785,'multiline':False]
['text':' also has a special handling of clamp/hardtanh','line_number':786,'multiline':False]
['text':' Propagate quantization parameters from other quantized tensors','line_number':789,'multiline':False]
['text':' We only remove observer module attributes from type in the','line_number':803,'multiline':False]
['text':' first encounter of the graph, after that since the attributes','line_number':804,'multiline':False]
['text':' is already removed from the ClassType, we'll use the list of slot index to','line_number':805,'multiline':False]
['text':' replay this removal','line_number':806,'multiline':False]
['text':' Map from Graph to observer node, we can use observer node to','line_number':809,'multiline':False]
['text':' get the information of original value that's been observed and','line_number':810,'multiline':False]
['text':' the quantization parameters','line_number':811,'multiline':False]
['text':' A map from qparam name (e.g. _scale) to the attribute name in','line_number':813,'multiline':False]
['text':' the module(e.g. weight_scale_0)','line_number':814,'multiline':False]
['text':' Record qscheme for every graph, this is for checking','line_number':817,'multiline':False]
['text':' each graph is only quantized with one type of QScheme','line_number':818,'multiline':False]
['text':' Set of quantized values, so that we quantize each value only','line_number':821,'multiline':False]
['text':' once','line_number':822,'multiline':False]
['text':' Map from original weight value to GraphFunction corresponding to the','line_number':825,'multiline':False]
['text':' subgraph that includes the weight observer and dependent nodes.','line_number':826,'multiline':False]
['text':' Observer forward call node','line_number':851,'multiline':False]
['text':' GetAttr node for observer module','line_number':853,'multiline':False]
['text':' 1. If we have seen this graph before, this means the observer','line_number':892,'multiline':False]
['text':' attributes has been removed from the type(see step 2) but the slot','line_number':893,'multiline':False]
['text':' index of these attributes are kept in the list, we'll replay the observer','line_number':894,'multiline':False]
['text':' slots removal using these slot indexes','line_number':895,'multiline':False]
['text':' 2. Remove observer modules from last one to first one in order to','line_number':902,'multiline':False]
['text':' reduce the time complexity, assuming all the observer modules','line_number':903,'multiline':False]
['text':' are added after the existing modules, we'll have complexity of','line_number':904,'multiline':False]
['text':' O(N) where N is number of observer modules with this optimization','line_number':905,'multiline':False]
['text':' We record the slot index here in order to replay the','line_number':912,'multiline':False]
['text':' slot removal in other objects that's sharing the ClassType','line_number':913,'multiline':False]
['text':' since we're going to remove attribute in the ClassType here','line_number':914,'multiline':False]
['text':' Build weight subgraph','line_number':954,'multiline':False]
['text':' Add last node output value as subgraph output.','line_number':960,'multiline':False]
['text':' If the graph was already visited, return the GraphFunction directly.','line_number':1005,'multiline':False]
['text':' Multiple module instances can share the same graph code, so we don't need','line_number':1006,'multiline':False]
['text':' to re-run the extraction process.','line_number':1007,'multiline':False]
['text':' Extract the subgraph nodes.','line_number':1009,'multiline':False]
['text':' Reverse to traverse subgraph in correct direction','line_number':1012,'multiline':False]
['text':' Build the graph using the nodes found from the weight observer.','line_number':1015,'multiline':False]
['text':' Run the graph with the module input.','line_number':1022,'multiline':False]
['text':' TODO: refactor findObserverName to take Node* as input','line_number':1061,'multiline':False]
['text':' get compute_dtype for dynamic quantization','line_number':1078,'multiline':False]
['text':' quantization parameters should appear in the same order as','line_number':1096,'multiline':False]
['text':' the argument for quantize_per_tensor/quantize_per_channel function','line_number':1097,'multiline':False]
['text':' calling method on self','line_number':1129,'multiline':False]
['text':' convert Scalar to Tensor','line_number':1159,'multiline':False]
['text':' for ops like average pool, we'll insert quant dequant after the op','line_number':1163,'multiline':False]
['text':' We'll assume the tensor is a PerTensorAffine quantized Tensor for','line_number':1164,'multiline':False]
['text':' now, and may generalize later if this becomes an issue','line_number':1165,'multiline':False]
['text':' input of the dequantize node','line_number':1168,'multiline':False]
['text':' insert ops after the general op','line_number':1170,'multiline':False]
['text':' Insert after the node that is later in topological order','line_number':1172,'multiline':False]
['text':' Only per tensor affine quantized tensor is supported in this case','line_number':1191,'multiline':False]
['text':' get quantization parameters from previous quantized op','line_number':1192,'multiline':False]
['text':' replace uses of original output of the general op with quantized','line_number':1216,'multiline':False]
['text':' output','line_number':1217,'multiline':False]
['text':' Convert the dequantized Tensor back to Scalar','line_number':1223,'multiline':False]
['text':' Delete dequantize node, we have one dequantize','line_number':1234,'multiline':False]
['text':' for each use of the value','line_number':1235,'multiline':False]
['text':' Replace useses of dequantized_val with the input of','line_number':1241,'multiline':False]
['text':' dequantize node','line_number':1242,'multiline':False]
['text':' Check if we need to propagate the quantization ops from input to','line_number':1249,'multiline':False]
['text':' output','line_number':1250,'multiline':False]
['text':' note that we don't need to recursively check for prim::If','line_number':1254,'multiline':False]
['text':' here because if all inputs of a prim::If is dequantized','line_number':1255,'multiline':False]
['text':' the dequantize will be factored out before we get to this','line_number':1256,'multiline':False]
['text':' point','line_number':1257,'multiline':False]
['text':' Factoring out dequantize for if blocks with multiple outputs','line_number':1285,'multiline':False]
['text':' is not supported right now','line_number':1286,'multiline':False]
['text':' propagate qparams for min and max scalar arguments','line_number':1299,'multiline':False]
['text':' for aten::clamp/aten::hardtanh','line_number':1300,'multiline':False]
['text':' is_scalar ','line_number':1301,'multiline':True]
['text':' is_scalar ','line_number':1312,'multiline':True]
['text':' For ops that are quantized by propagating dequantize ops,','line_number':1316,'multiline':False]
['text':' e.g. flatten we need to','line_number':1317,'multiline':False]
['text':' 1. check if we need to propagate dequantize op','line_number':1318,'multiline':False]
['text':' 2. remove the dequantize ops from inputs','line_number':1319,'multiline':False]
['text':' 3. insert dequantize for all outputs','line_number':1320,'multiline':False]
['text':' to make sure it works for ops with multiple outputs','line_number':1321,'multiline':False]
['text':' since removing dequantize from inputs is mutating the graph','line_number':1322,'multiline':False]
['text':' and it will affect future checks for whether all the inputs','line_number':1323,'multiline':False]
['text':' has been quantized or not(since currently we just check if','line_number':1324,'multiline':False]
['text':' the value is produced by dequantize op to decide if the value','line_number':1325,'multiline':False]
['text':' is quantized or not','line_number':1326,'multiline':False]
['text':' list of dequantized input values','line_number':1327,'multiline':False]
['text':' 1. collect dequantized inputs and outputs we need to dequantize','line_number':1330,'multiline':False]
['text':' 2. remove the dequantize ops from inputs','line_number':1343,'multiline':False]
['text':' 3. insert dequantize op for outputs','line_number':1345,'multiline':False]
['text':' Print warning for add_scalar/mul_scalar when debug is enabled','line_number':1352,'multiline':False]
['text':' since the quantization parameter for these ops depends on','line_number':1353,'multiline':False]
['text':' input and it's too complicated to encode the equations in','line_number':1354,'multiline':False]
['text':' the IR:','line_number':1355,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp#L64-L74','line_number':1356,'multiline':False]
['text':' Visit all blocks in the current graph to find weight values.','line_number':1383,'multiline':False]
['text':' For all the observed weight values, find the corresponding subgraph that','line_number':1404,'multiline':False]
['text':' contributes to the weight tensor, and run that subgraph to observe the','line_number':1405,'multiline':False]
['text':' weight.','line_number':1406,'multiline':False]
['text':' We only need to register new parameters if the graph has','line_number':1423,'multiline':False]
['text':' been quantized before','line_number':1424,'multiline':False]
['text':' TODO: dedup this part with code in quantizeTensors','line_number':1425,'multiline':False]
['text':' We check the size here because for some observers (like','line_number':1431,'multiline':False]
['text':' PlaceholderObserver) the qparams might be empty.','line_number':1432,'multiline':False]
['text':' prim::Param nodes do not belong to the graph. Hence the Insert','line_number':1449,'multiline':False]
['text':' point is the beginning of graph node. This also safe guards against','line_number':1450,'multiline':False]
['text':' observing a potentially mutated value due to some in-place operation','line_number':1451,'multiline':False]
['text':' TODO: add filter to the clamp patterns and remove this pass','line_number':1498,'multiline':False]
['text':' Insert quant and dequant nodes into the graph for both static and dynamic','line_number':1504,'multiline':False]
['text':' quant.','line_number':1505,'multiline':False]
['text':' + 1 for tensor to be quantized','line_number':1517,'multiline':False]
['text':' Have to make sure that quant node appears after the values it depends on.','line_number':1528,'multiline':False]
['text':' TODO: refactor findObserverName to take Node* as input','line_number':1554,'multiline':False]
['text':' Not sure if we need to support this for on device PTQ.','line_number':1578,'multiline':False]
['text':' scale Value*','line_number':1595,'multiline':False]
['text':' zero_point Value*','line_number':1597,'multiline':False]
['text':' Observer output','line_number':1616,'multiline':False]
['text':' Inserting before insert point','line_number':1618,'multiline':False]
['text':' In all likelihood this really wont do anything because we expect that','line_number':1634,'multiline':False]
['text':' the input method for quantization's prepare step will be inlined. Thus','line_number':1635,'multiline':False]
['text':' only call methods we will see will belong to observer's forward calls.','line_number':1636,'multiline':False]
['text':' Unliked the run method we dont need to extract new qparam values for the','line_number':1645,'multiline':False]
['text':' the same graph used in different call site.','line_number':1646,'multiline':False]
['text':' Reason is that for on device PTQ we dont:','line_number':1647,'multiline':False]
['text':' 1. Run calculate_qparams','line_number':1648,'multiline':False]
['text':' 2. Get the scale and zero point','line_number':1649,'multiline':False]
['text':' 3. get axis and dtype','line_number':1650,'multiline':False]
['text':' 4. register values from 2 and 3 as attributes on the parent module.','line_number':1651,'multiline':False]
['text':' Instead we insert call to calculate_qparams (1) via insertCalculateQParams','line_number':1652,'multiline':False]
['text':' in the graph itself. Then instead of 2 and 3, we get the output Value*','line_number':1653,'multiline':False]
['text':' and for 3, we insert GetAttr for axis and dtype and use those Value*','line_number':1654,'multiline':False]
['text':' with insterQuantizationOps','line_number':1655,'multiline':False]
['text':' prim::Param nodes do not belong to the graph. Hence the Insert','line_number':1657,'multiline':False]
['text':' point is the beginning of graph node. This also safe guards against','line_number':1658,'multiline':False]
['text':' observing a potentially mutated value due to some in-place operation','line_number':1659,'multiline':False]
['text':' namespace','line_number':1698,'multiline':False]
['text':' find quantize node that quantizes the output of if','line_number':1708,'multiline':False]
['text':' move the nodes that produces the quantization parameters before','line_number':1721,'multiline':False]
['text':' prim::If','line_number':1722,'multiline':False]
['text':' replace all uses of the quantized node with the output of if node','line_number':1726,'multiline':False]
['text':' add quantize nodes to the end of all blocks','line_number':1728,'multiline':False]
['text':' the original return value of the block','line_number':1733,'multiline':False]
['text':'
 *
 * Assumption: method_name method has observer placed
 * Objective: modify that method to insert calls to:
 * 1. calculate_qparams
 * 2. GetAttr for axis and dtype values
 * 3. Use Values from above two to insert calls to quant + dequant
 * Thus after this step you have a graph of, e.g., observe_forward,
 * that has observer nodes, calculate_qparams run on those observer nodes,
 * output of which is used by quant-dequant nodes. output of dequant is used
 * by the actual op.
 * Later on we will replace dequant + op (e.g. linear) with
 * 1. prepacked_op context
 * 2. unpack
 * 3. dequantize
 * 4. linear
 *
 * Of the above pattern 2, 3, and 4 can be replaced by linear_run op
 ','line_number':1800,'multiline':True]
['text':' Module InsertQuantDeQuantForOnDevicePTQ(','line_number':1819,'multiline':False]
['text':' Dont need:','line_number':1842,'multiline':False]
['text':' ReplicateChooseQParamsQuantDequant: This is propagating dynamic quant's','line_number':1843,'multiline':False]
['text':' quant dequant RemoveRedundantQuantizationOps: THis is removing activation','line_number':1844,'multiline':False]
['text':' observers for dynamic quant when the op related to it is not dynamically','line_number':1845,'multiline':False]
['text':' quantizable. Doesnt really make sense. In our case we wont have those','line_number':1846,'multiline':False]
['text':' anyway since for dynamic quant activations wont be observed We can still','line_number':1847,'multiline':False]
['text':' use this function because the above two methods should really be a noop','line_number':1848,'multiline':False]
['text':' namespace jit','line_number':1852,'multiline':False]
['text':' namespace torch','line_number':1853,'multiline':False]
