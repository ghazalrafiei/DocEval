['text':' Get the pattern we want to replace the match with','line_number':32,'multiline':False]
['text':' generate ops for quantize pattern for a scalar value','line_number':64,'multiline':False]
['text':' 6 is `torch.float` ScalarType, we are creating a float scalar','line_number':66,'multiline':False]
['text':' tensor from a scalar value','line_number':67,'multiline':False]
['text':' Suppress unused variable warning','line_number':79,'multiline':False]
['text':' Patterns for the ops that inherit parameters from input','line_number':105,'multiline':False]
['text':' QuantFusionInfo for the ops that inherit parameters from input','line_number':122,'multiline':False]
['text':' quant fusion for ops like `quantized::add_scalar`, `quantized::mul_scalar`','line_number':136,'multiline':False]
['text':' IR pattern common to all ops that inherit qparam from input','line_number':185,'multiline':False]
['text':' Patterns for the ops that has fixed quantization parameters','line_number':196,'multiline':False]
['text':' IR pattern common to all ops with fixed quantization parameters for','line_number':208,'multiline':False]
['text':' asymetric quantization','line_number':209,'multiline':False]
['text':' filter that checks %b_scalar is a scalar','line_number':232,'multiline':False]
['text':' Patterns for ops that require observation for output quantization parameters','line_number':241,'multiline':False]
['text':' Example:','line_number':242,'multiline':False]
['text':'','line_number':243,'multiline':False]
['text':' before fusion:','line_number':244,'multiline':False]
['text':'','line_number':245,'multiline':False]
['text':' graph(%a_quant, %r_scale, %r_zero_point, %r_dtype):','line_number':246,'multiline':False]
['text':'     %a_dequant = aten::dequantize(%a_quant)','line_number':247,'multiline':False]
['text':'     %r = {op_name}(%a_dequant, {extra_args})','line_number':248,'multiline':False]
['text':'     %r_quant = aten::quantize_per_tensor(%r, %r_scale, %r_zero_point,','line_number':249,'multiline':False]
['text':'     %r_dtype) return (%r_quant)','line_number':250,'multiline':False]
['text':'','line_number':251,'multiline':False]
['text':' after fusion:','line_number':252,'multiline':False]
['text':'','line_number':253,'multiline':False]
['text':' graph(%a_quant, %r_scale, %r_zero_point, %r_dtype):','line_number':254,'multiline':False]
['text':'     %r_quant = {quantized_op_name}(%a_quant, {extra_args}, %r_scale,','line_number':255,'multiline':False]
['text':'     %r_zero_point) return (%r_quant)','line_number':256,'multiline':False]
['text':' namespace','line_number':283,'multiline':False]
['text':' aten::conv1d','line_number':286,'multiline':False]
['text':' aten::conv1d - aten::relu','line_number':296,'multiline':False]
['text':' aten::conv1d - aten::relu_','line_number':307,'multiline':False]
['text':' quantized::conv1d','line_number':318,'multiline':False]
['text':' quantized::conv1d_relu','line_number':324,'multiline':False]
['text':' aten::conv2d','line_number':330,'multiline':False]
['text':' aten::conv2d - aten::relu','line_number':340,'multiline':False]
['text':' aten::conv2d - aten::relu_','line_number':351,'multiline':False]
['text':' quantized::conv2d','line_number':362,'multiline':False]
['text':' quantized::conv2d_relu','line_number':368,'multiline':False]
['text':' aten::conv3d','line_number':374,'multiline':False]
['text':' aten::conv3d - aten::relu','line_number':384,'multiline':False]
['text':' aten::conv3d - aten::relu_','line_number':395,'multiline':False]
['text':' quantized::conv3d','line_number':406,'multiline':False]
['text':' quantized::conv3d_relu','line_number':412,'multiline':False]
['text':' aten::conv_transpose1d','line_number':418,'multiline':False]
['text':' quantized::conv_transpose1d','line_number':428,'multiline':False]
['text':' aten::conv_transpose2d','line_number':434,'multiline':False]
['text':' quantized::conv_transpose1d','line_number':444,'multiline':False]
['text':' aten::linear','line_number':491,'multiline':False]
['text':' quantized::linear','line_number':521,'multiline':False]
['text':' aten::add','line_number':544,'multiline':False]
['text':' TODO: add %dtype after when https://github.com/pytorch/pytorch/issues/34351','line_number':553,'multiline':False]
['text':' is fixed','line_number':554,'multiline':False]
['text':' quantized::add','line_number':555,'multiline':False]
['text':' aten::add_','line_number':561,'multiline':False]
['text':' quantized::add_scalar_relu -- fusing quantized::add_scalar','line_number':584,'multiline':False]
['text':' and aten::relu','line_number':585,'multiline':False]
['text':' quantized::add_scalar_relu_out -- fusing quantized::add_scalarOut','line_number':603,'multiline':False]
['text':' and aten::relu','line_number':604,'multiline':False]
['text':' quantized::batch_norm','line_number':622,'multiline':False]
['text':' aten::mul','line_number':654,'multiline':False]
['text':' aten::mul_','line_number':663,'multiline':False]
['text':' quantized::mul','line_number':672,'multiline':False]
['text':' quantized::mul_relu','line_number':692,'multiline':False]
['text':' quantized::mul_scalar_relu -- fusing quantized::mul_scalar','line_number':734,'multiline':False]
['text':' and aten::relu','line_number':735,'multiline':False]
['text':' quantized::mul_scalar_relu_out -- fusing quantized::mul_scalarOut','line_number':753,'multiline':False]
['text':' and aten::relu','line_number':754,'multiline':False]
['text':' quantized::elu','line_number':772,'multiline':False]
['text':' ============= General Ops that inherit quantization parameters from input','line_number':792,'multiline':False]
['text':' tensor =============','line_number':793,'multiline':False]
['text':' Ops with fixed quantization parameters','line_number':895,'multiline':False]
['text':' note that these must come after quantized::add_scalar and','line_number':985,'multiline':False]
['text':' quantized::add_scalar_out patterns','line_number':986,'multiline':False]
['text':' note that these must come after quantized::mul_scalar and','line_number':1019,'multiline':False]
['text':' quantized::mul_scalar_out patterns','line_number':1020,'multiline':False]
['text':' fixed qparam ops','line_number':1073,'multiline':False]
['text':' This pattern ignores reduce range','line_number':1092,'multiline':False]
['text':' Set the reduce range to default to true, since qnnpack backend ignores this','line_number':1093,'multiline':False]
['text':' argument.','line_number':1094,'multiline':False]
['text':' namespace jit','line_number':1271,'multiline':False]
['text':' namespace torch','line_number':1272,'multiline':False]
