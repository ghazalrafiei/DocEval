['text':' Lists of allowed quantizable operators','line_number':21,'multiline':False]
['text':' These are the prim::CallFunctions that doesn't require observation and','line_number':70,'multiline':False]
['text':' have a single input Tensor','line_number':71,'multiline':False]
['text':' example: `prim::CallFunction(%dropout, %input_tensor, ...)','line_number':72,'multiline':False]
['text':' so we propagate observed property from %input_tensor to the','line_number':73,'multiline':False]
['text':' output of the `prim::CallFunction`','line_number':74,'multiline':False]
['text':' Also these ops doesn't do computation on the value of Tensor, the','line_number':75,'multiline':False]
['text':' operation only depends on the shape of the Tensor','line_number':76,'multiline':False]
['text':' Similar to prim::CallFunctions, there are aten ops that doesn't','line_number':85,'multiline':False]
['text':' require observation and have a single input Tensor','line_number':86,'multiline':False]
['text':' Also these ops doesn't do computation on the value of Tensor, the','line_number':87,'multiline':False]
['text':' operation only depends on the shape of the Tensor','line_number':88,'multiline':False]
['text':' e.g. `aten::flatten(%input_tensor, ...)`','line_number':89,'multiline':False]
['text':' Non-inplace resize is deprecated','line_number':99,'multiline':False]
['text':' Theses are prim::CallFunctions for ops that doesn't require observation and','line_number':120,'multiline':False]
['text':' have a single input Tensor','line_number':121,'multiline':False]
['text':' Also these ops do computation on the value of Tensor','line_number':122,'multiline':False]
['text':' TODO: [Need verify] looks like we can quantize simple functionals that just','line_number':123,'multiline':False]
['text':' call into aten functions','line_number':124,'multiline':False]
['text':' Theses are aten functions for ops that doesn't require observation and','line_number':140,'multiline':False]
['text':' have a single input Tensor','line_number':141,'multiline':False]
['text':' Also these ops do computation on the value of Tensor','line_number':142,'multiline':False]
['text':' e.g. `aten::avg_pool2d(%input_tensor, ...)`','line_number':143,'multiline':False]
['text':' "clamp_",  // Enable when quantized `clamp_` is ready','line_number':160,'multiline':False]
['text':' "clamp_",  // Enable when quantized `clamp_` is ready','line_number':171,'multiline':False]
['text':' quantization parameters for ops with range 0 to 1','line_number':178,'multiline':False]
['text':' for example: aten/src/ATen/native/quantized/cpu/qsigmoid.cpp','line_number':179,'multiline':False]
['text':' quantization parameters for ops with range -1 to 1','line_number':188,'multiline':False]
['text':' for example: aten/src/ATen/native/quantized/cpu/qtanh.cpp','line_number':189,'multiline':False]
['text':' Map from aten op symbol to the quantization parameters','line_number':197,'multiline':False]
['text':' for the ops with fixed quantization parameters','line_number':198,'multiline':False]
['text':' Special checks for ops that do not require observers for all input tensors.','line_number':209,'multiline':False]
['text':' For each operator in this list observers are inserted for the input based','line_number':210,'multiline':False]
['text':' on the index specified.','line_number':211,'multiline':False]
['text':' Aten functions for getting tensor information','line_number':215,'multiline':False]
['text':' Aten functions whose output will be quantized or not quantized depending','line_number':218,'multiline':False]
['text':' on input tensor','line_number':219,'multiline':False]
['text':' Rules are slightly different for binary ops like `aten::add`, for these ops,','line_number':222,'multiline':False]
['text':' if both of the inputs are Tensor, we'll quantize the output only if both of','line_number':223,'multiline':False]
['text':' the inputs are quantized','line_number':224,'multiline':False]
['text':' if the second input is a Scalar, we'll only look at the first input to decide','line_number':225,'multiline':False]
['text':' if we need to quantize the output','line_number':226,'multiline':False]
['text':' Check if `use` is an aten function of name `func_name` and if value','line_number':233,'multiline':False]
['text':' `v` is the nth argument (if provided) of the function.','line_number':234,'multiline':False]
['text':' Check any use of `v` matches the aten function call','line_number':254,'multiline':False]
['text':' or CallFunction patterns','line_number':255,'multiline':False]
['text':' TODO add other op signatures.','line_number':276,'multiline':False]
['text':' ate::embedding_bag(%weight, %input, %offsets, %scale_grad_by_freq,','line_number':280,'multiline':False]
['text':' %mode_enum, %sparse, %per_sample_weights, %include_last_offset)','line_number':281,'multiline':False]
['text':' embedding_bag - prim::CallFunction(%func, %input.1, %weight,','line_number':290,'multiline':False]
['text':' %offsets.1, %max_norm, %norm_type, %scale_grad_by_freq, %mode, %sparse,','line_number':291,'multiline':False]
['text':' %per_sample_weights.1, %include_last_offset)','line_number':292,'multiline':False]
['text':' only propagate dequantize for Tensor','line_number':365,'multiline':False]
['text':' TODO: factor out isCallFunc','line_number':419,'multiline':False]
['text':' call_funcs = ','line_number':455,'multiline':True]
['text':' aten_funcs = ','line_number':456,'multiline':True]
['text':' call_funcs = ','line_number':526,'multiline':True]
['text':' aten_funcs = ','line_number':529,'multiline':True]
['text':' Block helper functions','line_number':560,'multiline':False]
['text':' Check if a value in the graph is a Scalar value','line_number':579,'multiline':False]
['text':' =================== Graph/Module analysis helper functions ============','line_number':587,'multiline':False]
['text':' Check if value is the input of the graph','line_number':588,'multiline':False]
['text':' Get the module access path for a Value representing a module instance','line_number':595,'multiline':False]
['text':' by tracing back the GetAttr nodes and recording all the attribute','line_number':596,'multiline':False]
['text':' names along the way.','line_number':597,'multiline':False]
['text':' Assuming 'self.sub.basic_block.conv1',','line_number':598,'multiline':False]
['text':' Input1: Value instance of conv1','line_number':599,'multiline':False]
['text':' Input2: Value instance of self','line_number':600,'multiline':False]
['text':' Output: ['sub', 'basic_block', 'conv1']','line_number':601,'multiline':False]
['text':' Iterator to traverse back the GetAttr calls','line_number':604,'multiline':False]
['text':' trace back the instance to recover the path of the submodule','line_number':606,'multiline':False]
['text':' record the name of GetAttr','line_number':609,'multiline':False]
['text':' trace back the chain of GetAttr','line_number':611,'multiline':False]
['text':' Assuming self.foo.bar.conv1,','line_number':625,'multiline':False]
['text':' Input1: Module instance of self','line_number':626,'multiline':False]
['text':' Input2: ['foo', 'bar', 'conv1']','line_number':627,'multiline':False]
['text':' Output: Module instance of conv1','line_number':628,'multiline':False]
['text':' ==================== filter functions for matches ==============','line_number':662,'multiline':False]
['text':' namespace jit','line_number':798,'multiline':False]
['text':' namespace torch','line_number':799,'multiline':False]
