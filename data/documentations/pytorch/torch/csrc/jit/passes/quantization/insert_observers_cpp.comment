['text':' helper functions','line_number':46,'multiline':False]
['text':' helper classes','line_number':78,'multiline':False]
['text':'* Clone according to module qconfig map, this is for handling the case
   *  where we have two module instances sharing the same ClassType
   *  but configured with different QConfig
   *  code is copied and modified from
   * https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/api/module.cpp
   * inplace option means if the copy of the Tensor is deepcopy or not
   * if inplace is true, the cloned module will share the tensors with
   * original model instead of deepcopy them
   ','line_number':81,'multiline':True]
['text':' Create a new _ivalue in the same compilation unit.','line_number':109,'multiline':False]
['text':' Since now we have shared ClassType, we need to preserve the shared','line_number':110,'multiline':False]
['text':' ClassType during cloning, so we first use type and qconfig to check if','line_number':111,'multiline':False]
['text':' the type is already cloned, if so, we'll create a new module with the','line_number':112,'multiline':False]
['text':' cloned ClassType, if not, we'll create a new module and a new ClassType.','line_number':113,'multiline':False]
['text':' if we cloned the class type before, we'll reuse it','line_number':118,'multiline':False]
['text':' Copy slots. If a slot is a module - recursively clone it.','line_number':129,'multiline':False]
['text':' NOTE: why do we need to manually setattr on object instead of using','line_number':140,'multiline':False]
['text':' register_module here? because the attr can be a module interface','line_number':141,'multiline':False]
['text':' type and hold a Module object still. register_module will not let us','line_number':142,'multiline':False]
['text':' correctly set up the type for this attr, so we had to do this','line_number':143,'multiline':False]
['text':' manually. In the case it's an interface type, the type will be shared','line_number':144,'multiline':False]
['text':' by the new cloned instance in the same compilation unit bc it only','line_number':145,'multiline':False]
['text':' contains a list of functionSchema','line_number':146,'multiline':False]
['text':' we'll deepcopy the IValue in non inplace option','line_number':152,'multiline':False]
['text':' only clone the methods and constants if the ClassType is not cloned','line_number':162,'multiline':False]
['text':' before','line_number':163,'multiline':False]
['text':' Clone methods remapping the types to the cloned ones.','line_number':168,'multiline':False]
['text':' Execute __setstate__(__getstate__()) to initialize custom class','line_number':172,'multiline':False]
['text':' members.','line_number':173,'multiline':False]
['text':' remap of %self will be done outside of the function','line_number':192,'multiline':False]
['text':' and we don't support the case when people pass in','line_number':193,'multiline':False]
['text':' module as argument of the method because in that case','line_number':194,'multiline':False]
['text':' we need to do more comprehensive analysis to decide the','line_number':195,'multiline':False]
['text':' QConfig for the module','line_number':196,'multiline':False]
['text':' remapping type for module instance','line_number':203,'multiline':False]
['text':' We don't remap output and the remapping of module type','line_number':212,'multiline':False]
['text':' will be done in CallMethod, we don't support type remapping','line_number':213,'multiline':False]
['text':' for modules returned from methods or functions','line_number':214,'multiline':False]
['text':' remap self','line_number':271,'multiline':False]
['text':' we only support %self being Module in the arguments of function','line_number':273,'multiline':False]
['text':' TODO: replace (module, method_name) with graph?','line_number':296,'multiline':False]
['text':' preprocess to clean up the graph from tracing','line_number':297,'multiline':False]
['text':' Fill the map between the caller input/output to input/output','line_number':300,'multiline':False]
['text':' of called graph, this is used to navigate through the graph','line_number':301,'multiline':False]
['text':' to find the observer for a given value','line_number':302,'multiline':False]
['text':' analyze the graph and record necessary information that can','line_number':305,'multiline':False]
['text':' be used in insert observers','line_number':306,'multiline':False]
['text':'*
   * Recursively insert observers for the method, also we'll process
   * the nodes in the graph in the order of execution of these nodes
   * since we need the context information to decide whether we want to
   * observe/quantize a value a not, we don't want to observe a value multiple
   * times.
   *
   * argument: is_entry_point means whether the current method is the forward
   * method of the top level module.
   *
   * Since we want to insert observers in the call site instead of in the called
   * graph, we'll postpone inserting observer to caller as much as possible, if
   * we know the current method is the outer most method, then
   * we will insert all observers in the graph instead of postpone this to the
   * parent, note that this assumes we don't have recursive method
   * calls
   *
   * returns a tuple of vectors of observer modules for input and output, these
   * are used for inserting observers for the input/output values
   * since we need to insert these values at call site.
   * And a vector of indexes of outputs that indicates whether the output value
   * is already observed or not, this is used for propagating the observed
   * property of a value through CallMethods, because we should skip inserting
   * observers for ops that don't require observation
   ','line_number':311,'multiline':True]
['text':' this is a reference because when we insert observer for a value','line_number':356,'multiline':False]
['text':' in one block it is also observed in another block, we don't want to','line_number':357,'multiline':False]
['text':' insert multiple observers for the same value','line_number':358,'multiline':False]
['text':' Record v as "ready for observation" by storing it in values_to_observe.','line_number':363,'multiline':False]
['text':' If v is a part of a delayed observation pattern, record v's descendant','line_number':364,'multiline':False]
['text':' (per delay rules) instead. The observers are inserted at a later stage','line_number':365,'multiline':False]
['text':' by reading the state created by this function.','line_number':366,'multiline':False]
['text':' Fill the map from value to the corresponding observer module','line_number':385,'multiline':False]
['text':' this map is used in insertObservers to actually insert','line_number':386,'multiline':False]
['text':' observers to the module','line_number':387,'multiline':False]
['text':' Clone observer module and add it to the original module,','line_number':390,'multiline':False]
['text':' and insert a call to observer forward function','line_number':391,'multiline':False]
['text':' Uses the state created by fillBoundaryValueMap and fillValueObserverMap','line_number':402,'multiline':False]
['text':' to return an observer configured for a value, if it is needed.','line_number':403,'multiline':False]
['text':' Uses the state created by fillPassThroughValueMap to propagage observed','line_number':406,'multiline':False]
['text':' property which should pass through from inputs to outputs.','line_number':407,'multiline':False]
['text':' for cat/add/mul we will only observe their output if their input','line_number':412,'multiline':False]
['text':' are observed','line_number':413,'multiline':False]
['text':' Check whether node output uses can be quantized, eg cat followed by','line_number':418,'multiline':False]
['text':' linear op','line_number':419,'multiline':False]
['text':' This checks both of the input should be tensor and observed.','line_number':430,'multiline':False]
['text':' There is one check that we didn't do here, which is','line_number':431,'multiline':False]
['text':' !isScalar(isObserved(n->input(1), block_observed_values)','line_number':432,'multiline':False]
['text':' to make sure input 1 is not a scalar, because scalar tensor input','line_number':433,'multiline':False]
['text':' for add/mul won't be observed with current rule, we can omit','line_number':434,'multiline':False]
['text':' this check here','line_number':435,'multiline':False]
['text':' Find and mark known patterns such as conv-relu (and others) where','line_number':444,'multiline':False]
['text':' we should not insert observers in the middle of the pattern.','line_number':445,'multiline':False]
['text':' Fill the map from values to the list of values that can pass the observed','line_number':450,'multiline':False]
['text':' property to it','line_number':451,'multiline':False]
['text':' Values we want to delay observation, used to delay the observation for','line_number':460,'multiline':False]
['text':' values in the middle of the ops that are supposed to be fused, e.g.','line_number':461,'multiline':False]
['text':' the output value of conv in the conv - relu pattern','line_number':462,'multiline':False]
['text':' the key is the intermediate output, e.g. output of conv','line_number':463,'multiline':False]
['text':' the value is the value we want to observe, e.g. output of relu','line_number':464,'multiline':False]
['text':'','line_number':465,'multiline':False]
['text':' example, assuming we want to delay conv-relu:','line_number':466,'multiline':False]
['text':'   %x1 = conv(%x0)','line_number':467,'multiline':False]
['text':'   %x2 = relu(%x1)','line_number':468,'multiline':False]
['text':'','line_number':469,'multiline':False]
['text':' delay_observation_map_ = {','line_number':470,'multiline':False]
['text':'   %x1: %x2,','line_number':471,'multiline':False]
['text':' }','line_number':472,'multiline':False]
['text':' Map of value to observer module configured for that value.','line_number':477,'multiline':False]
['text':' Map from values from callsite into the values in the CallMethod graph','line_number':480,'multiline':False]
['text':' key of the map is the value from caller graph, and the value of the map','line_number':481,'multiline':False]
['text':' is the list of values in the callee graph (the graph','line_number':482,'multiline':False]
['text':' corresponding to the called method),','line_number':483,'multiline':False]
['text':' the reason it is a set is that a value in the caller graph','line_number':484,'multiline':False]
['text':' can both correspond to the output of one callee graph and input of another','line_number':485,'multiline':False]
['text':' callee graph.','line_number':486,'multiline':False]
['text':'','line_number':487,'multiline':False]
['text':' example:','line_number':488,'multiline':False]
['text':'   // top level module','line_number':489,'multiline':False]
['text':'   %x1 = conv(%x0)','line_number':490,'multiline':False]
['text':'   %x2 = prim::CallFunction(%foo, %x1)','line_number':491,'multiline':False]
['text':'','line_number':492,'multiline':False]
['text':'   // graph of %foo','line_number':493,'multiline':False]
['text':'   %y2 = conv(%y1)','line_number':494,'multiline':False]
['text':'   return %y2','line_number':495,'multiline':False]
['text':'','line_number':496,'multiline':False]
['text':' boundary_value_map = {','line_number':497,'multiline':False]
['text':'   // current module's output values to corresponding return values from','line_number':498,'multiline':False]
['text':'   subgraph %x2: %y2,','line_number':499,'multiline':False]
['text':'   // current module's input values to corresponding input value to subgraph','line_number':500,'multiline':False]
['text':'   %x1: %y1,','line_number':501,'multiline':False]
['text':' }','line_number':502,'multiline':False]
['text':' This is used for the observed values to pass through the ops like flatten,','line_number':507,'multiline':False]
['text':' so that output value of flatten does not need to be observed','line_number':508,'multiline':False]
['text':' key is the output of the op, value is a vector of values that need','line_number':509,'multiline':False]
['text':' to be observed in order to pass the observed property to the output','line_number':510,'multiline':False]
['text':'','line_number':511,'multiline':False]
['text':' example:','line_number':512,'multiline':False]
['text':'   %x1 = flatten(%x0) // pass_through','line_number':513,'multiline':False]
['text':'   %x2 = conv(%x1) // not pass_through','line_number':514,'multiline':False]
['text':'','line_number':515,'multiline':False]
['text':' pass_through_value_map_ = {','line_number':516,'multiline':False]
['text':'   %x1: [%x0],','line_number':517,'multiline':False]
['text':' }','line_number':518,'multiline':False]
['text':' Unique id generator for observer module, used for generating','line_number':521,'multiline':False]
['text':' unique observer names when we insert observer module, we','line_number':522,'multiline':False]
['text':' record the current unique id used to avoid incrementing from 0','line_number':523,'multiline':False]
['text':' every time to find a unique id.','line_number':524,'multiline':False]
['text':' Set of observer forward call nodes','line_number':526,'multiline':False]
['text':' Map from block to a vector of observer name and observer modules we','line_number':528,'multiline':False]
['text':' want to add to the module instance that has the block','line_number':529,'multiline':False]
['text':' Type of quantization for this pass.','line_number':532,'multiline':False]
['text':' These are the IR patterns we match to skip inserting observers.','line_number':534,'multiline':False]
['text':' They are compiled once on construction and used repeatedly within','line_number':535,'multiline':False]
['text':' the pass.','line_number':536,'multiline':False]
['text':' nn.Linear + nn.ReLU','line_number':538,'multiline':False]
['text':' nn.Linear + F.relu','line_number':547,'multiline':False]
['text':' nn.Linear + aten::relu','line_number':556,'multiline':False]
['text':' nn.Linear + aten::relu_','line_number':565,'multiline':False]
['text':' aten::linear + nn.ReLU','line_number':574,'multiline':False]
['text':' aten::linear + F.relu','line_number':583,'multiline':False]
['text':' aten::linear + aten::relu','line_number':592,'multiline':False]
['text':' aten::linear + aten::relu_','line_number':600,'multiline':False]
['text':' Skip observer nodes','line_number':927,'multiline':False]
['text':' Get handle of observer module','line_number':964,'multiline':False]
['text':' Match arguments to types of observer's arguments','line_number':971,'multiline':False]
['text':' Insert call to observer's forward','line_number':978,'multiline':False]
['text':' Replace v with the output of observer','line_number':982,'multiline':False]
['text':' The above also replaced the input to `call`, so switch it back to','line_number':984,'multiline':False]
['text':' the correct value','line_number':985,'multiline':False]
['text':' offset of input for the caller node, since the first','line_number':1120,'multiline':False]
['text':' input of CallFunction is the function node and the graph','line_number':1121,'multiline':False]
['text':' for CallFunction start with actual input','line_number':1122,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1123,'multiline':False]
['text':' add mapping from callsite value to value in called graph','line_number':1137,'multiline':False]
['text':' run preprocess for child module before parent, since preprocess','line_number':1183,'multiline':False]
['text':' mutates the graph and it might affect passes like fillBoundaryValueMap','line_number':1184,'multiline':False]
['text':' Inline fork-wait calls','line_number':1193,'multiline':False]
['text':' fuse decomposed linear into aten::linear','line_number':1195,'multiline':False]
['text':' fill out various internal state which will be later used in','line_number':1210,'multiline':False]
['text':' insertObservers to insert the correct observer','line_number':1211,'multiline':False]
['text':' For dynamic quantization we only insert observers at the input','line_number':1228,'multiline':False]
['text':' of the quantizable function.','line_number':1229,'multiline':False]
['text':' Check whether producer is quantizable','line_number':1231,'multiline':False]
['text':' Check the dtype of the observer module.','line_number':1238,'multiline':False]
['text':' For inputs with Fp16 type that are not-weights we don't observer them for','line_number':1241,'multiline':False]
['text':' dynamic quantization.','line_number':1242,'multiline':False]
['text':' Check whether node input value is quantizable','line_number':1247,'multiline':False]
['text':' Need to make sure all values are','line_number':1333,'multiline':False]
['text':' configured with same observer','line_number':1334,'multiline':False]
['text':' input/output values, used to skip inserting observers','line_number':1381,'multiline':False]
['text':' for input and output of the block and the owning graph,','line_number':1382,'multiline':False]
['text':' we have to insert the observers at call site because','line_number':1383,'multiline':False]
['text':' the graph itself can be shared','line_number':1384,'multiline':False]
['text':' list of observer modules for input values','line_number':1386,'multiline':False]
['text':' list of observer modules for output values','line_number':1388,'multiline':False]
['text':' if the current block is the block for entry point graph(the forward graph','line_number':1391,'multiline':False]
['text':' of the top level module), we can insert observers in the block directly','line_number':1392,'multiline':False]
['text':' graph inputs/outputs','line_number':1395,'multiline':False]
['text':' block outputs','line_number':1401,'multiline':False]
['text':' we need explicitly skip the values that are already observed','line_number':1411,'multiline':False]
['text':' this might happen in subblocks for `if` since','line_number':1412,'multiline':False]
['text':' these subblock has access to all values before the `if` node','line_number':1413,'multiline':False]
['text':' This means the block is been processed before, we just','line_number':1422,'multiline':False]
['text':' need to attach observer modules and construct the information','line_number':1423,'multiline':False]
['text':' needed by call site here','line_number':1424,'multiline':False]
['text':' instance clone of observer module and setAttr','line_number':1427,'multiline':False]
['text':' NB: Why do we need to process the graph even if it's visited?','line_number':1434,'multiline':False]
['text':' Reason is `block_observed_values` can','line_number':1435,'multiline':False]
['text':' change depending on where the method is called, and','line_number':1436,'multiline':False]
['text':' outputs that's been observed(third item of the returned result)','line_number':1437,'multiline':False]
['text':' can change depending on that, so for each graph we'll need to go through','line_number':1438,'multiline':False]
['text':' the whole process of inserting observers, the observers inserted in this','line_number':1439,'multiline':False]
['text':' block won't change, but the information we return to the caller will change','line_number':1440,'multiline':False]
['text':' based on `block_observed_values`','line_number':1441,'multiline':False]
['text':' We first construct a map from value to the module, then','line_number':1446,'multiline':False]
['text':' insert observers for them later, this is to avoid interference','line_number':1447,'multiline':False]
['text':' of the inserted observers with the analysis to decide where','line_number':1448,'multiline':False]
['text':' to insert observers, also we only insert observers for','line_number':1449,'multiline':False]
['text':' "intermediate values" that is not the input/output of the','line_number':1450,'multiline':False]
['text':' graph','line_number':1451,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1472,'multiline':False]
['text':' CallFunction','line_number':1483,'multiline':False]
['text':' a vector recoding whether each output is observed or not','line_number':1528,'multiline':False]
['text':' subblock has access to all the values in the scope of prim::If,','line_number':1534,'multiline':False]
['text':' so subblock_observed_values == block_observed_values','line_number':1535,'multiline':False]
['text':' subblock for prim::If doesn't have inputs','line_number':1538,'multiline':False]
['text':' We'll insert output observer for each subblock, and in the end','line_number':1542,'multiline':False]
['text':' we will check if output of subblocks are quantized consistently','line_number':1543,'multiline':False]
['text':' mark the output of if as observed','line_number':1575,'multiline':False]
['text':' If the node is one of the propagate quant node, e.g.','line_number':1591,'multiline':False]
['text':' aten::cat, we should observe its output only','line_number':1592,'multiline':False]
['text':' if the input of the node is observed','line_number':1593,'multiline':False]
['text':' since the vector is always non-empty, we will','line_number':1633,'multiline':False]
['text':' not return the initial value','line_number':1634,'multiline':False]
['text':' This is to propagate observed property through','line_number':1642,'multiline':False]
['text':' all ops that doesn't require observation','line_number':1643,'multiline':False]
['text':' namespace','line_number':1649,'multiline':False]
['text':' Since the types are changed after clone, we need to fill','line_number':1663,'multiline':False]
['text':' the qconfig map again','line_number':1664,'multiline':False]
['text':' analyze needs to run after fillBoundaryValueMap','line_number':1670,'multiline':False]
['text':' since we need to know the boundary value mapping to trace','line_number':1671,'multiline':False]
['text':' through the calls','line_number':1672,'multiline':False]
['text':' is_entry_point ','line_number':1674,'multiline':True]
['text':' Since the types are changed after clone, we need to fill','line_number':1693,'multiline':False]
['text':' the qconfig map again','line_number':1694,'multiline':False]
['text':' Removes list mutation part is not clear. Is it needed','line_number':1698,'multiline':False]
['text':' Since we expect the graph to be inlined this should not have any use','line_number':1700,'multiline':False]
['text':' However, this function does handle if blocks','line_number':1701,'multiline':False]
['text':' Although as far as I understood If blocks are not really handled','line_number':1702,'multiline':False]
['text':' in JIT quantization. Should we just protect against this. That is if we','line_number':1703,'multiline':False]
['text':' find observable value inside If block? Also side effect of inlining is that','line_number':1704,'multiline':False]
['text':' you will have multiple getattrs for the same attribute and thus potentially','line_number':1705,'multiline':False]
['text':' multiple observers observing the same value. This will also lead to','line_number':1706,'multiline':False]
['text':' increased size of the packed param struct. I dont expect this to be a','line_number':1707,'multiline':False]
['text':' common pattern but something to be aware fo Note that current quant','line_number':1708,'multiline':False]
['text':' workflow does not prevent this anyway since during inset quant dequant','line_number':1709,'multiline':False]
['text':' things are inlined anyway','line_number':1710,'multiline':False]
['text':' analyze needs to run after fillBoundaryValueMap','line_number':1712,'multiline':False]
['text':' since we need to know the boundary value mapping to trace','line_number':1713,'multiline':False]
['text':' through the calls','line_number':1714,'multiline':False]
['text':' Remove activation observer if quant_type is dynamic','line_number':1716,'multiline':False]
['text':' is_entry_point ','line_number':1722,'multiline':True]
['text':' namespace jit','line_number':1725,'multiline':False]
['text':' namespace torch','line_number':1726,'multiline':False]
