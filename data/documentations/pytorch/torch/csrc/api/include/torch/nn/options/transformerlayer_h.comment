['text':'/ Options for the `TransformerEncoderLayer`','line_number':16,'multiline':False]
['text':'/','line_number':17,'multiline':False]
['text':'/ Example:','line_number':18,'multiline':False]
['text':'/ ```','line_number':19,'multiline':False]
['text':'/ auto options = TransformerEncoderLayer(512, 8).dropout(0.2);','line_number':20,'multiline':False]
['text':'/ ```','line_number':21,'multiline':False]
['text':' implicit ','line_number':23,'multiline':True]
['text':'/ the number of expected features in the input','line_number':25,'multiline':False]
['text':'/ the number of heads in the multiheadattention models','line_number':28,'multiline':False]
['text':'/ the dimension of the feedforward network model, default is 2048','line_number':31,'multiline':False]
['text':'/ the dropout value, default is 0.1','line_number':34,'multiline':False]
['text':'/ the activation function of intermediate layer, can be ``torch::kReLU``,','line_number':37,'multiline':False]
['text':'/ ``torch::GELU``, or a unary callable. Default: ``torch::kReLU``','line_number':38,'multiline':False]
['text':' ============================================================================','line_number':42,'multiline':False]
['text':'/ Options for the `TransformerDecoderLayer` module.','line_number':44,'multiline':False]
['text':'/','line_number':45,'multiline':False]
['text':'/ Example:','line_number':46,'multiline':False]
['text':'/ ```','line_number':47,'multiline':False]
['text':'/ TransformerDecoderLayer model(TransformerDecoderLayerOptions(512,','line_number':48,'multiline':False]
['text':'/ 8).dropout(0.2));','line_number':49,'multiline':False]
['text':'/ ```','line_number':50,'multiline':False]
['text':'/ number of expected features in the input','line_number':54,'multiline':False]
['text':'/ number of heads in the multiheadattention models','line_number':57,'multiline':False]
['text':'/ dimension of the feedforward network model. Default: 2048','line_number':60,'multiline':False]
['text':'/ dropout value. Default: 1','line_number':63,'multiline':False]
['text':'/ activation function of intermediate layer, can be ``torch::kGELU``,','line_number':66,'multiline':False]
['text':'/ ``torch::kReLU``, or a unary callable. Default: ``torch::kReLU``','line_number':67,'multiline':False]
['text':' namespace nn','line_number':71,'multiline':False]
['text':' namespace torch','line_number':72,'multiline':False]
