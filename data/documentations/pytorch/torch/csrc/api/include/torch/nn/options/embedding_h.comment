['text':'/ Options for the `Embedding` module.','line_number':11,'multiline':False]
['text':'/','line_number':12,'multiline':False]
['text':'/ Example:','line_number':13,'multiline':False]
['text':'/ ```','line_number':14,'multiline':False]
['text':'/ Embedding model(EmbeddingOptions(10,','line_number':15,'multiline':False]
['text':'/ 2).padding_idx(3).max_norm(2).norm_type(2.5).scale_grad_by_freq(true).sparse(true));','line_number':16,'multiline':False]
['text':'/ ```','line_number':17,'multiline':False]
['text':'/ The size of the dictionary of embeddings.','line_number':21,'multiline':False]
['text':'/ The size of each embedding vector.','line_number':23,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':25,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at `padding_idx` is not updated','line_number':26,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad". For a newly constructed','line_number':27,'multiline':False]
['text':'/ Embedding, the embedding vector at `padding_idx` will default to all','line_number':28,'multiline':False]
['text':'/ zeros, but can be updated to another value to be used as the padding','line_number':29,'multiline':False]
['text':'/ vector.','line_number':30,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':32,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':33,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':35,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':37,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``.','line_number':38,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':40,'multiline':False]
['text':'/ The learnable weights of the module of shape (num_embeddings,','line_number':42,'multiline':False]
['text':'/ embedding_dim)','line_number':43,'multiline':False]
['text':' ============================================================================','line_number':47,'multiline':False]
['text':'/ Options for the `Embedding::from_pretrained` function.','line_number':49,'multiline':False]
['text':'/ If ``true``, the tensor does not get updated in the learning process.','line_number':51,'multiline':False]
['text':'/ Equivalent to ``embedding.weight.requires_grad_(false)``. Default:','line_number':52,'multiline':False]
['text':'/ ``true``','line_number':53,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':55,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at `padding_idx` is not updated','line_number':56,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad".','line_number':57,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':59,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':60,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':62,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':64,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``.','line_number':65,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':67,'multiline':False]
['text':' ============================================================================','line_number':71,'multiline':False]
['text':'/ Options for `torch::nn::functional::embedding`.','line_number':75,'multiline':False]
['text':'/','line_number':76,'multiline':False]
['text':'/ Example:','line_number':77,'multiline':False]
['text':'/ ```','line_number':78,'multiline':False]
['text':'/ namespace F = torch::nn::functional;','line_number':79,'multiline':False]
['text':'/ F::embedding(input, weight,','line_number':80,'multiline':False]
['text':'/ F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));','line_number':81,'multiline':False]
['text':'/ ```','line_number':82,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':84,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at `padding_idx` is not updated','line_number':85,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad".','line_number':86,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':88,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':89,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':91,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':93,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``.','line_number':94,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':96,'multiline':False]
['text':' namespace functional','line_number':100,'multiline':False]
['text':' ============================================================================','line_number':102,'multiline':False]
['text':'/ Options for the `EmbeddingBag` module.','line_number':107,'multiline':False]
['text':'/','line_number':108,'multiline':False]
['text':'/ Example:','line_number':109,'multiline':False]
['text':'/ ```','line_number':110,'multiline':False]
['text':'/ EmbeddingBag model(EmbeddingBagOptions(10,','line_number':111,'multiline':False]
['text':'/ 2).max_norm(2).norm_type(2.5).scale_grad_by_freq(true).sparse(true).mode(torch::kSum));','line_number':112,'multiline':False]
['text':'/ ```','line_number':113,'multiline':False]
['text':'/ The size of the dictionary of embeddings.','line_number':117,'multiline':False]
['text':'/ The size of each embedding vector.','line_number':119,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':121,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':122,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':124,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':126,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``. Note: this option is not','line_number':127,'multiline':False]
['text':'/ supported when ``mode="kMax"``.','line_number':128,'multiline':False]
['text':'/ ``"kSum"``, ``"kMean"`` or ``"kMax"``. Specifies the way to reduce the','line_number':130,'multiline':False]
['text':'/ bag. ``"kSum"`` computes the weighted sum, taking `per_sample_weights`','line_number':131,'multiline':False]
['text':'/ into consideration. ``"kMean"`` computes the average of the values in the','line_number':132,'multiline':False]
['text':'/ bag, ``"kMax"`` computes the max value over each bag.','line_number':133,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':135,'multiline':False]
['text':'/ Note: this option is not supported when ``mode="kMax"``.','line_number':136,'multiline':False]
['text':'/ The learnable weights of the module of shape (num_embeddings,','line_number':138,'multiline':False]
['text':'/ embedding_dim)','line_number':139,'multiline':False]
['text':'/ If ``true``, `offsets` has one additional element, where the last element','line_number':141,'multiline':False]
['text':'/ is equivalent to the size of `indices`. This matches the CSR format.','line_number':142,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':144,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at padding_idx is not updated','line_number':145,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad". For a newly constructed','line_number':146,'multiline':False]
['text':'/ EmbeddingBag, the embedding vector at `padding_idx` will default to all','line_number':147,'multiline':False]
['text':'/ zeros, but can be updated to another value to be used as the padding','line_number':148,'multiline':False]
['text':'/ vector. Note that the embedding vector at `padding_idx` is excluded from','line_number':149,'multiline':False]
['text':'/ the reduction.','line_number':150,'multiline':False]
['text':' ============================================================================','line_number':154,'multiline':False]
['text':'/ Options for the `EmbeddingBag::from_pretrained` function.','line_number':156,'multiline':False]
['text':'/ If ``true``, the tensor does not get updated in the learning process.','line_number':158,'multiline':False]
['text':'/ Equivalent to ``embeddingbag.weight.requires_grad_(false)``. Default:','line_number':159,'multiline':False]
['text':'/ ``true``','line_number':160,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':162,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':163,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':165,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':167,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``. Note: this option is not','line_number':168,'multiline':False]
['text':'/ supported when ``mode="kMax"``.','line_number':169,'multiline':False]
['text':'/ ``"kSum"``, ``"kMean"`` or ``"kMax"``. Specifies the way to reduce the','line_number':171,'multiline':False]
['text':'/ bag. ``"kSum"`` computes the weighted sum, taking `per_sample_weights`','line_number':172,'multiline':False]
['text':'/ into consideration. ``"kMean"`` computes the average of the values in the','line_number':173,'multiline':False]
['text':'/ bag, ``"kMax"`` computes the max value over each bag.','line_number':174,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':176,'multiline':False]
['text':'/ Note: this option is not supported when ``mode="kMax"``.','line_number':177,'multiline':False]
['text':'/ If ``true``, `offsets` has one additional element, where the last element','line_number':179,'multiline':False]
['text':'/ is equivalent to the size of `indices`. This matches the CSR format. Note:','line_number':180,'multiline':False]
['text':'/ this option is currently only supported when ``mode="sum"``.','line_number':181,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':183,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at padding_idx is not updated','line_number':184,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad". Note that the embedding','line_number':185,'multiline':False]
['text':'/ vector at `padding_idx` is excluded from the reduction.','line_number':186,'multiline':False]
['text':' ============================================================================','line_number':190,'multiline':False]
['text':'/ Options for `torch::nn::functional::embedding_bag`.','line_number':194,'multiline':False]
['text':'/','line_number':195,'multiline':False]
['text':'/ Example:','line_number':196,'multiline':False]
['text':'/ ```','line_number':197,'multiline':False]
['text':'/ namespace F = torch::nn::functional;','line_number':198,'multiline':False]
['text':'/ F::embedding_bag(input, weight,','line_number':199,'multiline':False]
['text':'/ F::EmbeddingBagFuncOptions().mode(torch::kSum).offsets(offsets));','line_number':200,'multiline':False]
['text':'/ ```','line_number':201,'multiline':False]
['text':'/ Only used when `input` is 1D. `offsets` determines','line_number':203,'multiline':False]
['text':'/ the starting index position of each bag (sequence) in `input`.','line_number':204,'multiline':False]
['text':'/ If given, each embedding vector with norm larger than `max_norm` is','line_number':206,'multiline':False]
['text':'/ renormalized to have norm `max_norm`.','line_number':207,'multiline':False]
['text':'/ The p of the p-norm to compute for the `max_norm` option. Default ``2``.','line_number':209,'multiline':False]
['text':'/ If given, this will scale gradients by the inverse of frequency of the','line_number':211,'multiline':False]
['text':'/ words in the mini-batch. Default ``false``. Note: this option is not','line_number':212,'multiline':False]
['text':'/ supported when ``mode="kMax"``.','line_number':213,'multiline':False]
['text':'/ ``"kSum"``, ``"kMean"`` or ``"kMax"``. Specifies the way to reduce the','line_number':215,'multiline':False]
['text':'/ bag. ``"kSum"`` computes the weighted sum, taking `per_sample_weights`','line_number':216,'multiline':False]
['text':'/ into consideration. ``"kMean"`` computes the average of the values in the','line_number':217,'multiline':False]
['text':'/ bag, ``"kMax"`` computes the max value over each bag.','line_number':218,'multiline':False]
['text':'/ If ``true``, gradient w.r.t. `weight` matrix will be a sparse tensor.','line_number':220,'multiline':False]
['text':'/ Note: this option is not supported when ``mode="kMax"``.','line_number':221,'multiline':False]
['text':'/ a tensor of float / double weights, or None to indicate all weights should','line_number':223,'multiline':False]
['text':'/ be taken to be 1. If specified, `per_sample_weights` must have exactly the','line_number':224,'multiline':False]
['text':'/ same shape as input and is treated as having the same `offsets`, if those','line_number':225,'multiline':False]
['text':'/ are not None.','line_number':226,'multiline':False]
['text':'/ If ``true``, `offsets` has one additional element, where the last element','line_number':228,'multiline':False]
['text':'/ is equivalent to the size of `indices`. This matches the CSR format. Note:','line_number':229,'multiline':False]
['text':'/ this option is currently only supported when ``mode="sum"``.','line_number':230,'multiline':False]
['text':'/ If specified, the entries at `padding_idx` do not contribute to the','line_number':232,'multiline':False]
['text':'/ gradient; therefore, the embedding vector at padding_idx is not updated','line_number':233,'multiline':False]
['text':'/ during training, i.e. it remains as a fixed "pad". Note that the embedding','line_number':234,'multiline':False]
['text':'/ vector at `padding_idx` is excluded from the reduction.','line_number':235,'multiline':False]
['text':' namespace functional','line_number':239,'multiline':False]
['text':' namespace nn','line_number':241,'multiline':False]
['text':' namespace torch','line_number':242,'multiline':False]
