['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TransformerEncoderLayer','line_number':20,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':21,'multiline':False]
['text':'/ TransformerEncoderLayer module.','line_number':23,'multiline':False]
['text':'/ See','line_number':24,'multiline':False]
['text':'/ https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html','line_number':25,'multiline':False]
['text':'/ to learn abouut the exact behavior of this encoder layer model','line_number':26,'multiline':False]
['text':'/','line_number':27,'multiline':False]
['text':'/ See the documentation for `torch::nn::TransformerEncoderLayer` class to','line_number':28,'multiline':False]
['text':'/ learn what constructor arguments are supported for this encoder layer model','line_number':29,'multiline':False]
['text':'/','line_number':30,'multiline':False]
['text':'/ Example:','line_number':31,'multiline':False]
['text':'/ ```','line_number':32,'multiline':False]
['text':'/ TransformerEncoderLayer encoderLayer(TransformerEncoderLayerOptions(512,','line_number':33,'multiline':False]
['text':'/ 8).dropout(0.1));','line_number':34,'multiline':False]
['text':'/ ```','line_number':35,'multiline':False]
['text':'/ options with which this `TransformerEncoderLayer` was constructed','line_number':57,'multiline':False]
['text':'/ self attention','line_number':60,'multiline':False]
['text':'/ feedforward first linear layer','line_number':63,'multiline':False]
['text':'/ feedforward dropout layer','line_number':66,'multiline':False]
['text':'/ feedforward second linear layer','line_number':69,'multiline':False]
['text':'/ pre feedforward, normalization layer','line_number':72,'multiline':False]
['text':'/ post feedfastward, normalization layer','line_number':74,'multiline':False]
['text':'/ pre feedfastward, dropout layer','line_number':77,'multiline':False]
['text':'/ post feedfastward, dropout layer','line_number':79,'multiline':False]
['text':'/ A `ModuleHolder` subclass for `TransformerEncoderLayerImpl``.','line_number':83,'multiline':False]
['text':'/ See the documentation for `TransformerEncoderLayerImpl` class to learn what','line_number':84,'multiline':False]
['text':'/ methods it provides, and examples of how to use `TransformerEncoderLayer`','line_number':85,'multiline':False]
['text':'/ with `torch::nn::TransformerEncoderLayerOptions`. See the documentation for','line_number':86,'multiline':False]
['text':'/ `ModuleHolder` to learn about PyTorch's module storage semantics.','line_number':87,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TransformerDecoderLayer','line_number':90,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':91,'multiline':False]
['text':'/ TransformerDecoderLayer is made up of self-attn, multi-head-attn and','line_number':93,'multiline':False]
['text':'/ feedforward network. This standard decoder layer is based on the paper','line_number':94,'multiline':False]
['text':'/ "Attention Is All You Need". Ashish Vaswani, Noam Shazeer, Niki Parmar,','line_number':95,'multiline':False]
['text':'/ Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia','line_number':96,'multiline':False]
['text':'/ Polosukhin. 2017. Attention is all you need. In Advances in Neural','line_number':97,'multiline':False]
['text':'/ Information Processing Systems, pages 6000-6010. Users may modify or','line_number':98,'multiline':False]
['text':'/ implement in a different way during application. See','line_number':99,'multiline':False]
['text':'/ https://pytorch.org/docs/master/nn.html#transformer-layers to learn about','line_number':100,'multiline':False]
['text':'/ the exact behavior of this module.','line_number':101,'multiline':False]
['text':'/','line_number':102,'multiline':False]
['text':'/ See the documentation for `torch::nn::TransformerDecoderLayerOptions` class','line_number':103,'multiline':False]
['text':'/ to learn what constructor arguments are supported for this module.','line_number':104,'multiline':False]
['text':'/','line_number':105,'multiline':False]
['text':'/ Example:','line_number':106,'multiline':False]
['text':'/ ```','line_number':107,'multiline':False]
['text':'/ TransformerDecoderLayer model(TransformerDecoderLayerOptions(512,','line_number':108,'multiline':False]
['text':'/ 8).dropout(0.2));','line_number':109,'multiline':False]
['text':'/ ```','line_number':110,'multiline':False]
['text':'/ Pass the inputs (and mask) through the decoder layer.','line_number':123,'multiline':False]
['text':'/ Args:','line_number':124,'multiline':False]
['text':'/       tgt: the sequence to the decoder layer (required).','line_number':125,'multiline':False]
['text':'/       memory: the sequence from the last layer of the encoder (required).','line_number':126,'multiline':False]
['text':'/       tgt_mask: the mask for the tgt sequence (optional).','line_number':127,'multiline':False]
['text':'/       memory_mask: the mask for the memory sequence (optional).','line_number':128,'multiline':False]
['text':'/       tgt_key_padding_mask: the mask for the tgt keys per batch','line_number':129,'multiline':False]
['text':'/       (optional). memory_key_padding_mask: the mask for the memory keys','line_number':130,'multiline':False]
['text':'/       per batch (optional).','line_number':131,'multiline':False]
['text':'/ The options used to configure this module.','line_number':140,'multiline':False]
['text':'/ self attention','line_number':143,'multiline':False]
['text':'/ Dropout, post self attention','line_number':146,'multiline':False]
['text':'/ Normalization, post self attention','line_number':149,'multiline':False]
['text':'/ Multi-headed attention','line_number':152,'multiline':False]
['text':'/ Dropout, post multi-headed attention','line_number':155,'multiline':False]
['text':'/ Normalization, post multi-headed attention','line_number':158,'multiline':False]
['text':'/ Feed forward first linear layer','line_number':161,'multiline':False]
['text':'/ Feed forward dropout layer','line_number':164,'multiline':False]
['text':'/ Feed forward second linear layer','line_number':167,'multiline':False]
['text':'/ Dropout, post feed forward','line_number':170,'multiline':False]
['text':'/ Normalization, post feed forward','line_number':173,'multiline':False]
['text':'/ Apply activation based on configuration','line_number':183,'multiline':False]
['text':'/ A `ModuleHolder` subclass for `TransformerDecoderLayerImpl`.','line_number':187,'multiline':False]
['text':'/ See the documentation for `TransformerDecoderLayerImpl` class to learn what','line_number':188,'multiline':False]
['text':'/ methods it provides, and examples of how to use `TransformerDecoderLayer`','line_number':189,'multiline':False]
['text':'/ with `torch::nn::TransformerDecoderLayerOptions`. See the documentation for','line_number':190,'multiline':False]
['text':'/ `ModuleHolder` to learn about PyTorch's module storage semantics.','line_number':191,'multiline':False]
['text':' namespace nn','line_number':194,'multiline':False]
['text':' namespace torch','line_number':195,'multiline':False]
