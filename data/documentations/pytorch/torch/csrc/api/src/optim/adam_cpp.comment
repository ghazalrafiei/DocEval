['text':', please consider SparseAdam instead','line_number':86,'multiline':True]
['text':' State initialization','line_number':90,'multiline':False]
['text':' Exponential moving average of gradient values','line_number':94,'multiline':False]
['text':' Exponential moving average of squared gradient values','line_number':96,'multiline':False]
['text':' Maintains max of all exp. moving avg. of sq. grad. values','line_number':99,'multiline':False]
['text':' Decay the first and second moment running average coefficient','line_number':122,'multiline':False]
['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':128,'multiline':False]
['text':' Use the max. for normalizing running avg. of gradient','line_number':130,'multiline':False]
['text':' deserializing archives saved in old format (prior to','line_number':153,'multiline':False]
['text':' version 1.5.0)','line_number':154,'multiline':False]
['text':' since there were no param_groups prior to version 1.5.0, assuming all','line_number':169,'multiline':False]
['text':' tensors are now in one param_group','line_number':170,'multiline':False]
['text':' namespace optim','line_number':184,'multiline':False]
['text':' namespace torch','line_number':185,'multiline':False]
