['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' This file contains functorch's Python bindings.','line_number':23,'multiline':False]
['text':' Ensure that the input is up to date by committing any pending updates to','line_number':72,'multiline':False]
['text':' the alias.','line_number':73,'multiline':False]
['text':' It would probably be more reasonable to check that the two tensors are','line_number':76,'multiline':False]
['text':' aliased, but we can't do that unless we give BatchedTensorImpl a notion of','line_number':77,'multiline':False]
['text':' storage.','line_number':78,'multiline':False]
['text':' Functions might resize zero-sized inputs, which we need to reflect','line_number':82,'multiline':False]
['text':' ehre.','line_number':83,'multiline':False]
['text':' If the input tensor's metadata was mutated, then use as_strided_()','line_number':86,'multiline':False]
['text':' to propagate the metadata change.','line_number':87,'multiline':False]
['text':' Poor man's version of np.moveaxis. Moves the dimension at `dst` to `src`','line_number':103,'multiline':False]
['text':' while preserving the order of other existing dimensions.','line_number':104,'multiline':False]
['text':' We should probably add np.moveaxis (it is more general) to PyTorch. (#36048)','line_number':105,'multiline':False]
['text':' When we do, replace the following with it.','line_number':106,'multiline':False]
['text':' Removes the batch dim with level `level` from `self`. If this causes the','line_number':126,'multiline':False]
['text':' last batch dim to be removed from a BatchedTensor, then this returns a','line_number':127,'multiline':False]
['text':' regular Tensor.','line_number':128,'multiline':False]
['text':'','line_number':129,'multiline':False]
['text':' If the `level` of the batch dim to remove does not exist in `self`, then we','line_number':130,'multiline':False]
['text':' add the batch dim in. This can happen if `self` didn't interact with a tensor','line_number':131,'multiline':False]
['text':' inside the vmap level, for example,','line_number':132,'multiline':False]
['text':'     self = torch.randn(3)','line_number':133,'multiline':False]
['text':'     y = torch.randn(5)','line_number':134,'multiline':False]
['text':'     out = vmap(lambda x: vmap(lambda y: x)(y))(self)','line_number':135,'multiline':False]
['text':'     assert out.shape == (3, 5)','line_number':136,'multiline':False]
['text':' Inside the inner vmap, `x` is a BatchedTensor with a single batch dimension','line_number':137,'multiline':False]
['text':' corresponding to the *outer* vmap level and it doesn't have any dimensions','line_number':138,'multiline':False]
['text':' that correspond to the inner vmap level so we need to create one for the','line_number':139,'multiline':False]
['text':' user.','line_number':140,'multiline':False]
['text':'','line_number':141,'multiline':False]
['text':' `out_dim` controls where we should put the batch dimension in the output','line_number':142,'multiline':False]
['text':' tensor.','line_number':143,'multiline':False]
['text':' Must be batched if has_level(self, /*any_level*/)','line_number':161,'multiline':False]
['text':' We only ever call that after popping out of a functionalize() call, in','line_number':172,'multiline':False]
['text':' which case the current tensors should always be wrapped in a','line_number':173,'multiline':False]
['text':' FunctionalTensorWrapper.','line_number':174,'multiline':False]
['text':' when regenerating the (potentially mutated) input tensors, the','line_number':179,'multiline':False]
['text':' functionalization pass regenerates them through a series of view_copy() op','line_number':180,'multiline':False]
['text':' calls. Functorch wants to turn those back into view ops though. Ensure that','line_number':181,'multiline':False]
['text':' the input is up to date by committing any pending updates to the alias.','line_number':182,'multiline':False]
['text':' NB: different behavior inside??','line_number':193,'multiline':False]
['text':' return self;','line_number':194,'multiline':False]
['text':' TORCH_INTERNAL_ASSERT(!maybeGetTensorWrapper(self));','line_number':195,'multiline':False]
['text':' TORCH_INTERNAL_ASSERT(self.has_storage());','line_number':196,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':220,'multiline':False]
['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':243,'multiline':False]
['text':' See NOTE [grad and vjp interaction with no_grad]','line_number':256,'multiline':False]
['text':'functionalize_add_back_views=','line_number':295,'multiline':True]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':344,'multiline':False]
['text':' TODO: this is a weird special case...','line_number':347,'multiline':False]
['text':' various debugging things. Maybe we should offer these as first-class APIs','line_number':469,'multiline':False]
['text':' on Tensors?','line_number':470,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unused-raii)','line_number':497,'multiline':False]
['text':' namespace impl','line_number':540,'multiline':False]
['text':' namespace functorch','line_number':541,'multiline':False]
['text':' namespace torch','line_number':542,'multiline':False]
