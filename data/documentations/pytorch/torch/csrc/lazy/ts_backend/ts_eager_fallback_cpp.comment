['text':'non_blocking','line_number':32,'multiline':True]
['text':'copy','line_number':33,'multiline':True]
['text':' convenience helper for converting tensors to cpu','line_number':41,'multiline':False]
['text':' We can't just call _to_eager() on the entire list of Tensors because it','line_number':46,'multiline':False]
['text':' will break on undefined tensors. Separate out undefined tensors first.','line_number':47,'multiline':False]
['text':' Explicitly handling undefined tensors here instead of letting `_to_eager`','line_number':53,'multiline':False]
['text':' handle it. Otherwise, we'd need to require all backends with their own','line_number':54,'multiline':False]
['text':' implementation of _to_eager to properly handle undefined tensors.','line_number':55,'multiline':False]
['text':' We can't just call _to_eager() on the entire list of Tensors because it','line_number':75,'multiline':False]
['text':' will break on undefined tensors. Separate out undefined tensors first.','line_number':76,'multiline':False]
['text':' Explicitly handling undefined tensors here instead of letting `_to_eager`','line_number':82,'multiline':False]
['text':' handle it. Otherwise, we'd need to require all backends with their own','line_number':83,'multiline':False]
['text':' implementation of _to_eager to properly handle undefined tensors.','line_number':84,'multiline':False]
['text':' Decide what device to move the output tensor(s) to.','line_number':119,'multiline':False]
['text':' The current convention is that we use the first tensor arg to pick the','line_number':120,'multiline':False]
['text':' device Barring that, we take the first tensor from a TensorList arg.','line_number':121,'multiline':False]
['text':' We need to loop through all of the (potentially multiple) TensorList','line_number':125,'multiline':False]
['text':' arguments In case, e.g. the first one is empty but the second is not.','line_number':126,'multiline':False]
['text':' namespace','line_number':143,'multiline':False]
['text':' When symbolic shape mode is not enabled, the nonzero shape function','line_number':157,'multiline':False]
['text':' returns an incorrect result.','line_number':158,'multiline':False]
['text':' TODO(whc) this FN_TRACK thing hasn't been used so far in LTC iirc but could','line_number':168,'multiline':False]
['text':' land/re-enable it LTC_FN_TRACK(3);;','line_number':169,'multiline':False]
['text':' Manually applying the TORCH_LAZY_COUNTER macro.','line_number':172,'multiline':False]
['text':' We need to do it ourselves and explicitly keep a mapping of counters','line_number':173,'multiline':False]
['text':' because this boxed fallback kernel is used by multiple operators,','line_number':174,'multiline':False]
['text':' and the macro stamps out a static Counter object with a fixed name','line_number':175,'multiline':False]
['text':' at the code location that it was called.','line_number':176,'multiline':False]
['text':' Log each tensor argument.','line_number':185,'multiline':False]
['text':' Call the actual boxed CPU fallback.','line_number':192,'multiline':False]
['text':' Most backends use TORCH_LIBRARY_* macros which perform their dispatcher','line_number':199,'multiline':False]
['text':' registrations at static library init time, but the lazy Torchscript backend','line_number':200,'multiline':False]
['text':' does not since it is built in the main torch lib but not always used.','line_number':201,'multiline':False]
['text':' In particular, if another external backend wants to register itself to the','line_number':202,'multiline':False]
['text':' same key (Lazy), Torchscript backend must not be initialized.','line_number':203,'multiline':False]
['text':' Step 1: Convert all non-eager tensor inputs into eager tensors and put them','line_number':222,'multiline':False]
['text':' on the stack at the correct indices.','line_number':223,'multiline':False]
['text':' Note: we copy each TensorList argument to eager individually out of','line_number':230,'multiline':False]
['text':' convenience, but XLA would benefit from materializing all tensor and','line_number':231,'multiline':False]
['text':' TensorList args onto the CPU at the same time. We can improve this if','line_number':232,'multiline':False]
['text':' we need better perf for XLA's CPU fallbacks.','line_number':233,'multiline':False]
['text':' XLA requires all of the tensor arguments to be gathered up and converted to','line_number':245,'multiline':False]
['text':' CPU together.','line_number':246,'multiline':False]
['text':' Step 2: Call the underlying eager implementation of the operator','line_number':254,'multiline':False]
['text':' Step 3: We need to take special care to handle mutable aliases properly:','line_number':257,'multiline':False]
['text':' If any input tensors are mutable aliases, we need to directly copy the','line_number':258,'multiline':False]
['text':' updated data on the eager tensors back to the original inputs.','line_number':259,'multiline':False]
['text':' Step 4: Convert any eager output tensors back to the original input device.','line_number':268,'multiline':False]
['text':' For mutable alias'd outputs, we also need to take special care','line_number':269,'multiline':False]
['text':' to move the ORIGINAL input tensor back onto the stack, in place of','line_number':270,'multiline':False]
['text':' the temporary eager output tensor that we created.','line_number':271,'multiline':False]
['text':'','line_number':272,'multiline':False]
['text':' Note [Eager Fallback Does Not Handle View Operators]','line_number':273,'multiline':False]
['text':' Also note that we are incapable of handling immutable alises properly.','line_number':274,'multiline':False]
['text':' Why?','line_number':275,'multiline':False]
['text':' Schemas with an immutable alias'd tensor outputs correspond to view','line_number':276,'multiline':False]
['text':' operators. For example, the `view_as` schema from native_functions.yaml:','line_number':277,'multiline':False]
['text':' `view_as(Tensor(a) self, Tensor other) -> Tensor(a)`','line_number':278,'multiline':False]
['text':' We can't handle these ops properly, because view ops are supposed to return','line_number':279,'multiline':False]
['text':' a NEW tensor that shares the SAME storage as the original tensor.','line_number':280,'multiline':False]
['text':' However, the new tensor that we created cannot share the same storage,','line_number':281,'multiline':False]
['text':' since it lives on the eager CPU / CUDA device and the original tensor lives','line_number':282,'multiline':False]
['text':' on a different device. Because of that, we warn if someone attempts to call','line_number':283,'multiline':False]
['text':' the eager fallback on a view operator (this is to maintain BC for view ops','line_number':284,'multiline':False]
['text':' for XLA that fall back to CPU).','line_number':285,'multiline':False]
['text':' Case (1): mutable alias case. Move the input ivalue directly onto','line_number':297,'multiline':False]
['text':' the stack in place of the existing eager output tensor.','line_number':298,'multiline':False]
['text':' We could store some extra metadata on the function schema to avoid','line_number':300,'multiline':False]
['text':' the loop here if we need to improve perf.','line_number':301,'multiline':False]
['text':' We've found the original input tensor that aliases with the','line_number':309,'multiline':False]
['text':' current output. Wrap it in an IValue and put it directly on the','line_number':310,'multiline':False]
['text':' stack.','line_number':311,'multiline':False]
['text':' immutable alias (view) case: Warn here, since we're copying and','line_number':329,'multiline':False]
['text':' not creating a view.','line_number':330,'multiline':False]
['text':' If this operator is needed, the backend should provide a kernel','line_number':331,'multiline':False]
['text':' for it.','line_number':332,'multiline':False]
['text':' See Note [Eager Fallback Does Not Handle View Operators]','line_number':333,'multiline':False]
['text':' We should never hit this for a view op,','line_number':340,'multiline':False]
['text':' because LazyTensor should provide a lowering for the','line_number':341,'multiline':False]
['text':' corresponding view_copy operator. The functionalization pass will','line_number':342,'multiline':False]
['text':' take care of calling the view_copy operator intead of the view.','line_number':343,'multiline':False]
['text':' Case (2): copy case. Copy the eager output tensor to the original','line_number':355,'multiline':False]
['text':' device.','line_number':356,'multiline':False]
['text':' We technically  might not have a target device, e.g. if you call','line_number':358,'multiline':False]
['text':' torch.cat() with an empty list In that case, we shouldn't have any','line_number':359,'multiline':False]
['text':' tensors to schlep across devices anyway.','line_number':360,'multiline':False]
['text':' namespace lazy','line_number':371,'multiline':False]
['text':' namespace torch','line_number':372,'multiline':False]
