['text':' Register a lazy graph executor instance that can be retrieved using Get()','line_number':24,'multiline':False]
['text':' Override these methods to perform custom tensor registration and','line_number':30,'multiline':False]
['text':' unregistration Note: It is vital that the parent implementations are also','line_number':31,'multiline':False]
['text':' called in order for the tensors to show up in the live tensor list','line_number':32,'multiline':False]
['text':' Seed for random generator.','line_number':36,'multiline':False]
['text':' Override to supply your own DeviceContextArena.','line_number':37,'multiline':False]
['text':' Retrieves the set of lazy tensors which are currently live in the system,','line_number':53,'multiline':False]
['text':' for the given device. If device is nullptr, the live tensors for all','line_number':54,'multiline':False]
['text':' devices will be returned. Returned tensors are sorted by device as primary','line_number':55,'multiline':False]
['text':' key, and by unique ID as secondary key.','line_number':56,'multiline':False]
['text':' Makes sure that any outstanding IR operation accumulated over live tensors,','line_number':59,'multiline':False]
['text':' gets turned into device data. If wait is true, the sync operation will be','line_number':60,'multiline':False]
['text':' run synchronously. The devices argument, if not empty, tells the devices','line_number':61,'multiline':False]
['text':' which should be partecipating into the replicated computation.','line_number':62,'multiline':False]
['text':' Applies all the pending IR operations queued over the input tensors. All','line_number':68,'multiline':False]
['text':' the tensors must be on the same device. If wait is true, the sync operation','line_number':69,'multiline':False]
['text':' will be run synchronously. The devices argument, if not empty, tells the','line_number':70,'multiline':False]
['text':' devices which should be partecipating into the replicated computation.','line_number':71,'multiline':False]
['text':' Marks an execution step, which allows the tensor framework to understand','line_number':78,'multiline':False]
['text':' the computation boundaries.','line_number':79,'multiline':False]
['text':' Override to supply your own DeviceContextArena.','line_number':80,'multiline':False]
['text':' Waits for all the outstanding operations on all the supplied devices.','line_number':83,'multiline':False]
['text':' If devices is empty, the wait will happen for all local devices.','line_number':84,'multiline':False]
['text':' Retrieves the PyTorch CPU tensors behind the lazy tensors IR operations.','line_number':87,'multiline':False]
['text':' All the tensors must be on the same device.','line_number':88,'multiline':False]
['text':' Dumps the backend specific text of the computation accumulated in the graph','line_number':93,'multiline':False]
['text':' which is attached the tensors.','line_number':94,'multiline':False]
['text':' TODO: even though this API is currently used **only** in codegen to','line_number':109,'multiline':False]
['text':' generate real scalar IR values vs scalar tensors, we would like to','line_number':110,'multiline':False]
['text':' use it in other cases where `GetIrValueForXXXScalar` is used, as well','line_number':111,'multiline':False]
['text':' In order to do that, we need to untangle the cases where we don't need','line_number':112,'multiline':False]
['text':' `expand` and where we don't expect a scalar tensor','line_number':113,'multiline':False]
['text':' TODO(alanwaketan): Revisit if all of them need to be accessible to','line_number':136,'multiline':False]
['text':' derived classes.','line_number':137,'multiline':False]
['text':' Whether we want to force data on the target tensors (hence trimming','line_number':140,'multiline':False]
['text':' the IR graph above them).','line_number':141,'multiline':False]
['text':' Whether when setting the data, the other properties of the tensor','line_number':143,'multiline':False]
['text':' state should be reset.','line_number':144,'multiline':False]
['text':' Locking:','line_number':165,'multiline':False]
['text':' We perform two kinds of operations of tensors, synchronous and','line_number':166,'multiline':False]
['text':' asynchronous. The ApplyPendingGraph() are synchronous, as we need the','line_number':167,'multiline':False]
['text':' device data result immediately. Before the synchronous operations can','line_number':168,'multiline':False]
['text':' start, they need to wait that the pending asynchronous operations have','line_number':169,'multiline':False]
['text':' completed. Synchronous operations do not hold device locks, since they are','line_number':170,'multiline':False]
['text':' strictly sequential, dictated by the PyTorch execution order. The','line_number':171,'multiline':False]
['text':' SyncTensorsGraph() is asynchronous, and returns immediately after having','line_number':172,'multiline':False]
['text':' scheduled the asynchronous operation. While executing, the asynchronous','line_number':173,'multiline':False]
['text':' operations will hold locks on all the participating devices (in most common','line_number':174,'multiline':False]
['text':' cases there will be only one device).','line_number':175,'multiline':False]
['text':' Since asynchronous operations capture device locks, only one asynchronous','line_number':176,'multiline':False]
['text':' operation can execute at the same time, on a given device. Tensor','line_number':177,'multiline':False]
['text':' operations which send data to device do not need to hold any device locks','line_number':178,'multiline':False]
['text':' while doing so. Only operations which _use_ device data (computations, and','line_number':179,'multiline':False]
['text':' transfer from server) need to wait for asynchronous operations to complete','line_number':180,'multiline':False]
['text':' (barrier).','line_number':181,'multiline':False]
['text':' Use a set to impose an order on the device locking sequence (ABBA','line_number':213,'multiline':False]
['text':' prevention).','line_number':214,'multiline':False]
['text':' The DeviceContextArena holds per device live information and statistics,','line_number':259,'multiline':False]
['text':' among which the lazy tensors which are currently alive in the system. This','line_number':260,'multiline':False]
['text':' is used to create computation "barriers" in order to flush pending','line_number':261,'multiline':False]
['text':' operations and ensure the same computations are created during the training','line_number':262,'multiline':False]
['text':' loops.','line_number':263,'multiline':False]
['text':' TODO(alanwaketan): Add a registry such that we don't need to make all','line_number':264,'multiline':False]
['text':' related methods virtual.','line_number':265,'multiline':False]
['text':' Overriding it allow derived class to use their own IRs for Value.','line_number':285,'multiline':False]
['text':' Overriding it allow derived class to use their own conversions.','line_number':301,'multiline':False]
['text':' Waits for this SyncTensorCollection's device barrier and acquire the lock.','line_number':335,'multiline':False]
['text':' One can override to insert your own profiler.','line_number':338,'multiline':False]
['text':' Schedules the execution of a sync tensors operation in background. The','line_number':394,'multiline':False]
['text':' asynchronous operation will hold the device locks by capturing the ones','line_number':395,'multiline':False]
['text':' present within the coll structure.','line_number':396,'multiline':False]
['text':' Gathers the device data for all the input tensors, after an','line_number':417,'multiline':False]
['text':' asynchronous operation.','line_number':418,'multiline':False]
['text':' namespace lazy','line_number':425,'multiline':False]
['text':' namespace torch','line_number':426,'multiline':False]
