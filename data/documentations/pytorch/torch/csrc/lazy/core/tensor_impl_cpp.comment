['text':' LTCGuardImpl is used by CompositeExplicitAutograd ops or eager fallbacks to','line_number':15,'multiline':False]
['text':' make sure that some particular tensors within the life scope of the guard are','line_number':16,'multiline':False]
['text':' on the same device. For example, in RegisterCompositeExplicitAutograd.cpp,','line_number':17,'multiline':False]
['text':' outputs of each op are examined if they are on same device as the supplied','line_number':18,'multiline':False]
['text':' TensorOptions. For more information, see DeviceGuard.h. For ops that have LTC','line_number':19,'multiline':False]
['text':' native function implementations, this guard is omitted.','line_number':20,'multiline':False]
['text':' This will get called when autograd initializes its device pool','line_number':59,'multiline':False]
['text':' regardless whether we have a backend registered aforehand.','line_number':60,'multiline':False]
['text':' namespace','line_number':71,'multiline':False]
['text':' TODO(whc) when do we want to clone vs share?','line_number':73,'multiline':False]
['text':'src_impl=','line_number':101,'multiline':True]
['text':'dest_impl=','line_number':102,'multiline':True]
['text':'version_counter=','line_number':103,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':104,'multiline':True]
['text':'src_impl=','line_number':113,'multiline':True]
['text':'dest_impl=','line_number':114,'multiline':True]
['text':'version_counter=','line_number':115,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':116,'multiline':True]
['text':'src_impl=','line_number':125,'multiline':True]
['text':'dest_impl=','line_number':126,'multiline':True]
['text':'version_counter=','line_number':127,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':128,'multiline':True]
['text':' Fill up the basic dimension data members which the base class','line_number':148,'multiline':False]
['text':' implementation uses in its APIs.','line_number':149,'multiline':False]
['text':' We can't call refresh_numel() given we override sizes() too.','line_number':151,'multiline':False]
['text':' We can't call empty_tensor_restride(c10::MemoryFormat::Contiguous) given','line_number':154,'multiline':False]
['text':' we override sizes() too.','line_number':155,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':166,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':172,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':178,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)','line_number':184,'multiline':False]
['text':' This should be true, but false as a temporary fix for a PyTorch core issue,','line_number':200,'multiline':False]
['text':' according to https://github.com/pytorch/xla/pull/2682.','line_number':201,'multiline':False]
['text':' TODO(ezyang): I don't think this branch is actually necessary','line_number':206,'multiline':False]
['text':' TODO(ezyang): I don't think this logic is right, shouldn't we pass on','line_number':207,'multiline':False]
['text':' the memory format?','line_number':208,'multiline':False]
['text':' Only check that the storage is already contiguous.','line_number':212,'multiline':False]
['text':' TODO: I don't think logic is right, we should check the requested memory','line_number':214,'multiline':False]
['text':' format before returning true','line_number':215,'multiline':False]
['text':' namespace lazy','line_number':219,'multiline':False]
['text':' namespace torch','line_number':220,'multiline':False]
