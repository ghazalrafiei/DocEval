['text':' Indicates to our torch_dispatch dispatching infra that','line_number':28,'multiline':False]
['text':' this is an "infra" mode with lower dispatching precedence.','line_number':29,'multiline':False]
['text':' Note: The reason we add these extra keys to our FunctionalTensor subclass','line_number':32,'multiline':False]
['text':' is to mirror the behavior of C++ functionalization (we can choose to change this','line_number':33,'multiline':False]
['text':' later, as long as it doesn't break anything).','line_number':34,'multiline':False]
['text':' FunctionalTensorWrapper copies **all** dispatch keys from the inner tensor','line_number':35,'multiline':False]
['text':' to the wrapper, excluding functorch and python dispatch keys.','line_number':36,'multiline':False]
['text':' Here I'm trying to re-use the keyset the functorch wrapper subclasses copy,','line_number':37,'multiline':False]
['text':' except that they don't include ZeroTensor so I'm manually adding it in.','line_number':38,'multiline':False]
['text':' These are all aten ops that correspond to metadata queries.','line_number':43,'multiline':False]
['text':' We want FunctionalTensor to be able to handle them directly.','line_number':44,'multiline':False]
['text':' type: ignore[has-type]','line_number':46,'multiline':False]
['text':' type: ignore[has-type]','line_number':47,'multiline':False]
['text':' type: ignore[has-type]','line_number':48,'multiline':False]
['text':' type: ignore[has-type]','line_number':49,'multiline':False]
['text':' type: ignore[has-type]','line_number':50,'multiline':False]
['text':' type: ignore[has-type]','line_number':51,'multiline':False]
['text':' type: ignore[has-type]','line_number':52,'multiline':False]
['text':' type: ignore[has-type]','line_number':53,'multiline':False]
['text':' type: ignore[has-type]','line_number':54,'multiline':False]
['text':' type: ignore[has-type]','line_number':55,'multiline':False]
['text':' type: ignore[has-type]','line_number':56,'multiline':False]
['text':' type: ignore[has-type]','line_number':57,'multiline':False]
['text':' type: ignore[has-type]','line_number':58,'multiline':False]
['text':' In general, we'd like our functional tensor subclass to only be in charge of functionalization,','line_number':64,'multiline':False]
['text':' and defer to the inner subclass for all other functionality.','line_number':65,'multiline':False]
['text':' Example: If our inner tensor is a ZeroTensor, we would want to defer running the ZeroTensor fallback','line_number':66,'multiline':False]
['text':' until after we redispatch to our inner ZeroTensor.','line_number':67,'multiline':False]
['text':' However, there are a few keys that we need to mirror between the inner and outer tensors.','line_number':68,'multiline':False]
['text':'   Conjugate','line_number':69,'multiline':False]
['text':'   Negative','line_number':70,'multiline':False]
['text':' Why? These keys are used to test metadata queries, like `.is_conj()` and `.is_neg()`.','line_number':71,'multiline':False]
['text':' We **need** calls to is_conj() to return the same thing on the outer and inner tensors,','line_number':72,'multiline':False]
['text':' Because user code / framework code that branches like so needs to do the same thing','line_number':73,'multiline':False]
['text':' when it sees the outer FunctionalTensor:','line_number':74,'multiline':False]
['text':'     if (x.is_conj()) {','line_number':75,'multiline':False]
['text':'         return at::view_as_real(x.resolve_conj());','line_number':76,'multiline':False]
['text':'     } else {','line_number':77,'multiline':False]
['text':'         return at::view_as_real(x);','line_number':78,'multiline':False]
['text':'     }','line_number':79,'multiline':False]
['text':' type: ignore[arg-type, attr-defined]','line_number':84,'multiline':False]
['text':' TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.','line_number':85,'multiline':False]
['text':' Calling the overload that has kwargs causes us to go down the first overload path,','line_number':86,'multiline':False]
['text':' which will **always** specialize sizes.','line_number':87,'multiline':False]
['text':' We should probably eventually fix this so that the first overload can just handle dynamic shapes.','line_number':88,'multiline':False]
['text':' sizes','line_number':90,'multiline':False]
['text':' strides','line_number':91,'multiline':False]
['text':' storage_offset','line_number':92,'multiline':False]
['text':' memory_format','line_number':93,'multiline':False]
['text':' dtype','line_number':94,'multiline':False]
['text':' layout','line_number':95,'multiline':False]
['text':' device','line_number':96,'multiline':False]
['text':' pin_memory','line_number':97,'multiline':False]
['text':' requires_grad','line_number':98,'multiline':False]
['text':' dispatch_sizes_strides_policy','line_number':99,'multiline':False]
['text':' dispatch_device','line_number':100,'multiline':False]
['text':' dispatch_layout','line_number':101,'multiline':False]
['text':' _extra_dispatch_keys','line_number':102,'multiline':False]
['text':' Need to disable default torch_function. Why?','line_number':107,'multiline':False]
['text':' Default torch_function will always wrap outputs into a subclass if they aren't already a subclass.','line_number':108,'multiline':False]
['text':' We actually.. don't want to do this sometimes, see Note [FunctionalTensorMode inputs are sometimes plain tensors]','line_number':109,'multiline':False]
['text':' FunctionalTensor needs to plumb all metadata requests to the inner tensor.','line_number':127,'multiline':False]
['text':' In theory we don't have to do this - but if we want to service metadata requests here,','line_number':128,'multiline':False]
['text':' we need to carefully make sure all metadata is accurate (including metadata mutations)','line_number':129,'multiline':False]
['text':' All metadata accesses should be plumbed to the inner tensor, that way we don't have to worry','line_number':137,'multiline':False]
['text':' about the problem of keeping metadata in sync between the wrapper and inner tensor.','line_number':138,'multiline':False]
['text':' This also alleviates us from having to manually handle metadata mutations on the wrapper.','line_number':139,'multiline':False]
['text':' Originally I tried to implement my subclass without giving it a torch_dispatch, but I gave up:','line_number':141,'multiline':False]
['text':' - _make_wrapper_subclass requires a __torch_dispatch__','line_number':142,'multiline':False]
['text':' - If we want to use _make_subclass(), we have a problem: the subclass will share a TensorImpl with the inner tensor,','line_number':143,'multiline':False]
['text':'   which is of type FunctionalTensorWrapper! We explicitly do not want our wrapper to be a FunctionalTensorWrapper.','line_number':144,'multiline':False]
['text':' - If we use the default tensor.__new__(), we have another problem: it returns inner_tensor.alias(),','line_number':145,'multiline':False]
['text':'   which causes every subclass created above autograd to have autograd view metadata','line_number':146,'multiline':False]
['text':'   (in addition to also being a FunctionalTensorWrapper).','line_number':147,'multiline':False]
['text':' We will do the wrapping for the user.','line_number':157,'multiline':False]
['text':' The only autograd metadata we care about on the FunctionalTensor is:','line_number':159,'multiline':False]
['text':' - requires_grad (so autograd runs)','line_number':160,'multiline':False]
['text':' - is_leaf (so that mutations on graph inputs that are not leaves are allowed by the autograd engine)','line_number':161,'multiline':False]
['text':'   this is handled by FunctionalTensor.to_functional','line_number':162,'multiline':False]
['text':' Technically the FunctionalTensormode here is unnecessary,','line_number':164,'multiline':False]
['text':' but it avoids spurious NotImplemented logs during `ProxyTorchDispatchMode` tracing.','line_number':165,'multiline':False]
['text':' _mirror_autograd_meta_to queries tensor sizes,','line_number':166,'multiline':False]
['text':' and otherwise the sym_size() call will go to the proxy mode before hitting','line_number':167,'multiline':False]
['text':' FunctionalTensor.__torch_dispatch__','line_number':168,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':170,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':172,'multiline':False]
['text':' Indicates to our torch_dispatch dispatching infra that','line_number':196,'multiline':False]
['text':' this is an "infra" mode with lower dispatching precedence.','line_number':197,'multiline':False]
['text':' This will be turned off later for pre-dispatch functionalization','line_number':199,'multiline':False]
['text':' No-op if FunctionalTensorMode is already in use','line_number':202,'multiline':False]
['text':' Not all funcs from __torch_dispatch__ are actual dispatcher ops,','line_number':239,'multiline':False]
['text':' e.g. prim.device','line_number':240,'multiline':False]
['text':' Decomposes CompositeImplicitAutograd ops','line_number':244,'multiline':False]
['text':' Only wrap our outputs in subclasses if the inner functionalization call','line_number':253,'multiline':False]
['text':' also wrapped outputs into FunctionalTensorWrappers.','line_number':254,'multiline':False]
['text':' When can this happen? e.g. `torch.div(2, 2)`','line_number':255,'multiline':False]
['text':' Expectation: functionalization should not **already** be enabled above our mode.','line_number':283,'multiline':False]
['text':' Why would that be bad? when we return a FunctionalTensor here, we don't want functionalization','line_number':284,'multiline':False]
['text':' to run above this mode and further wrap that output in **another** C++ FunctionalTensorWrapper.','line_number':285,'multiline':False]
['text':' All we want to do here is re-use the existing C++ functionalization logic.','line_number':303,'multiline':False]
['text':' This requires swizzling our TLS dispatch keys so that the Functionalize key is active.','line_number':304,'multiline':False]
['text':' By default for python functionalization (for AOTAutograd), we reapply views.','line_number':307,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':308,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':313,'multiline':False]
['text':' If no outputs are our functional subclass, then don't try to fix up aliasing','line_number':324,'multiline':False]
['text':' Since lift_fresh lifts its argument into a functional tensor, we can skip the','line_number':329,'multiline':False]
['text':' aliasing correction step. Otherwise, we would be setting the storage of a','line_number':330,'multiline':False]
['text':' lifted tensor to that of an unlifted tensor.','line_number':331,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/issues/111506','line_number':332,'multiline':False]
['text':' Wrapper tensor subclasses do not have correct aliasing info! Use this util to manually correct the output aliasing.','line_number':336,'multiline':False]
['text':' inplace ops like `aten.add_()` are expected to return inputs **directly**, instead of creating fresh tensor objects.','line_number':337,'multiline':False]
['text':' Use this util to figure out the right thing to return.','line_number':338,'multiline':False]
['text':' If none of our inputs were wrapped, then we have no FunctionalTensor outputs that we need to fix up storages for.','line_number':339,'multiline':False]
['text':' TODO: clean up the redundancy here,','line_number':355,'multiline':False]
['text':' unify on a single context manager for all mode keys.','line_number':356,'multiline':False]
['text':' This is similar to torch.func.functionalize, but:','line_number':367,'multiline':False]
['text':' - It uses FunctionalTensorMode, and FunctionalTensor (a python subclass).','line_number':368,'multiline':False]
['text':'   One important advantage to using this mode is that it will let us','line_number':369,'multiline':False]
['text':'   run functionalization underneath __torch_dispatch__,','line_number':370,'multiline':False]
['text':'   which we need in AOTAutograd.','line_number':371,'multiline':False]
['text':' - Doing so means that it does not automatically compose with other','line_number':372,'multiline':False]
['text':'   functorch transforms, since these transforms always run above __torch_dispatch__.','line_number':373,'multiline':False]
['text':'   That's why this util lives here, and not in functorch.','line_number':374,'multiline':False]
['text':' TODO: pull these from aot autograd','line_number':376,'multiline':False]
['text':' quick sanity assert','line_number':384,'multiline':False]
