['text':' Perform type promotion, as this is expected from prim_metafunction','line_number':60,'multiline':False]
['text':' Broadcast','line_number':67,'multiline':False]
['text':' Perform prim checks','line_number':70,'multiline':False]
['text':' steps does not participate in the computation of the dtype','line_number':132,'multiline':False]
['text':' for mypy','line_number':138,'multiline':False]
['text':' type: ignore[arg-type]','line_number':142,'multiline':False]
['text':' Type and device checks','line_number':154,'multiline':False]
['text':' Index checks','line_number':159,'multiline':False]
['text':' Checks that dim is within bounds','line_number':203,'multiline':False]
['text':' Checks that dim is within bounds','line_number':211,'multiline':False]
['text':' Stride-related code from _exec_fft in aten/src/ATen/native/cuda/SpectralOps.cpp','line_number':216,'multiline':False]
['text':' Permute dimensions so batch dimensions come first, and in stride order','line_number':222,'multiline':False]
['text':' std::partition','line_number':229,'multiline':False]
['text':' Collapse batch dimensions into a single dimension','line_number':245,'multiline':False]
['text':' Reshaping to original batch shape and inverting the dimension permutation','line_number':256,'multiline':False]
['text':' See _fft_c2c_cufft in aten/src/ATen/native/cuda/SpectralOps.cpp','line_number':269,'multiline':False]
['text':' and _fft_c2c_mkl in aten/src/ATen/native/mkl/SpectralOps.cpp','line_number':270,'multiline':False]
['text':' This code simulates the original decomp from inductor,','line_number':363,'multiline':False]
['text':' which runs most of the meta checks that we care about.','line_number':364,'multiline':False]
['text':' In theory, we should make this more robust by carefully','line_number':365,'multiline':False]
['text':' auditing our C++ copy_() kernel and copying the checks here.','line_number':366,'multiline':False]
['text':' 1 == MemOverlap::Yes','line_number':368,'multiline':False]
['text':' see: https://github.com/pytorch/pytorch/pull/114477#issuecomment-1830121375','line_number':411,'multiline':False]
['text':' We assume that we have already squashed the inputs into a 2-D tensor','line_number':412,'multiline':False]
['text':' Then, as the output is transposed, we need to propagate the transposed','line_number':413,'multiline':False]
['text':' stride information to the output tensor','line_number':414,'multiline':False]
['text':' Implementations below are taken from https://github.com/albanD/subclass_zoo/blob/main/python_meta_tensor.py','line_number':486,'multiline':False]
['text':' FIXME should probably check that lengths and offset aren't both set, but','line_number':523,'multiline':False]
['text':' the ATen implementation neglects this too','line_number':524,'multiline':False]
['text':' lengths == torch.diff(offsets)','line_number':526,'multiline':False]
['text':' Avoid importing sympy at a module level','line_number':606,'multiline':False]
['text':' Avoid importing sympy at a module level','line_number':622,'multiline':False]
['text':' From aten/src/ATen/native/LinearAlgebraUtils.h','line_number':641,'multiline':False]
['text':' Validates input shapes and devices','line_number':651,'multiline':False]
['text':' for linear solve methods (solve, cholesky_solve, lu_solve, triangular_solve)','line_number':652,'multiline':False]
['text':' From aten/src/ATen/native/LinearAlgebraUtils.h','line_number':653,'multiline':False]
['text':' From aten/src/ATen/native/LinearAlgebraUtils.h','line_number':693,'multiline':False]
['text':' From aten/src/ATen/native/LinearAlgebraUtils.h','line_number':709,'multiline':False]
['text':' From aten/src/ATen/native/BatchLinearAlgebra.cpp','line_number':821,'multiline':False]
['text':' L','line_number':830,'multiline':False]
['text':' infos','line_number':835,'multiline':False]
['text':' From aten/src/ATen/native/BatchLinearAlgebra.cpp','line_number':893,'multiline':False]
['text':' Sets sizes to the size of pivots','line_number':1016,'multiline':False]
['text':' Sets sizes to the size of info','line_number':1021,'multiline':False]
['text':' dtype','line_number':1038,'multiline':False]
['text':' matrix shapes','line_number':1052,'multiline':False]
['text':' batches','line_number':1060,'multiline':False]
['text':' parse the "mode" param in linalg_qr: return a tuple of bools (compute_q, reduced)','line_number':1126,'multiline':False]
['text':' this is actually irrelevant in this mode','line_number':1136,'multiline':False]
['text':' For readability','line_number':1171,'multiline':False]
['text':' From aten/src/ATen/native/BatchLinearAlgebra.cpp','line_number':1197,'multiline':False]
['text':' NOTE: matching defaults in aten/src/ATen/native/native_functions.yaml','line_number':1198,'multiline':False]
['text':' NB: This checks for CUDA since there is no way to check for cuSolver.','line_number':1221,'multiline':False]
['text':' Also, this might not work correctly on CPU when fake_device is not','line_number':1222,'multiline':False]
['text':' available as device_hint just defaults to CUDA in that case. See','line_number':1223,'multiline':False]
['text':' _linalg_svd meta in core.','line_number':1224,'multiline':False]
['text':' doesn't matter','line_number':1228,'multiline':False]
['text':' S is always real, even when A is complex.','line_number':1232,'multiline':False]
['text':' broadcast the batch dimensions of arg1 and arg2.','line_number':1240,'multiline':False]
['text':' If there's no name we assume we don't want to check the errors','line_number':1256,'multiline':False]
['text':' resize and copy operations are done in-place','line_number':1331,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1332,'multiline':False]
['text':' strides are not copied in out_wrapper','line_number':1333,'multiline':False]
['text':' type: ignore[union-attr]','line_number':1334,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1335,'multiline':False]
['text':' reimplementation of resize_output with result F-contig','line_number':1358,'multiline':False]
['text':' type: ignore[return-value]','line_number':1362,'multiline':False]
['text':' From aten/src/ATen/native/LinearAlgebra.cpp','line_number':1413,'multiline':False]
['text':' allow batch size of 0-dim.','line_number':1534,'multiline':False]
['text':' allow empty batch size but not other dimensions.','line_number':1541,'multiline':False]
['text':' type: ignore[call-overload]','line_number':1880,'multiline':False]
['text':' type: ignore[call-overload]','line_number':1884,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/88612','line_number':1929,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/88612','line_number':1940,'multiline':False]
['text':' FakeTensors (meta tensors with a device) will report device as meta','line_number':1989,'multiline':False]
['text':' when running meta kernels. Here, access the "fake device" of FakeTensor if it','line_number':1990,'multiline':False]
['text':' exists so meta kernels which have diverge per device will be more','line_number':1991,'multiline':False]
['text':' accurate when run with FakeTensors','line_number':1992,'multiline':False]
['text':' default to cuda','line_number':1997,'multiline':False]
['text':' If output_padding is present, we are dealing with a transposed convolution','line_number':2081,'multiline':False]
['text':' type: ignore[call-overload]','line_number':2141,'multiline':False]
['text':' type: ignore[call-overload]','line_number':2168,'multiline':False]
['text':' prepacked_weight','line_number':2203,'multiline':False]
['text':' The weight has been transposed during the qlinear weight prepack process.','line_number':2250,'multiline':False]
['text':' from check_dim_size() in aten/src/ATen/TensorUtils.cpp.','line_number':2290,'multiline':False]
['text':' from avg_pool2d_backward_shape_check() in aten/src/ATen/native/Pool.h.','line_number':2376,'multiline':False]
['text':' Don't override the C++ registration.','line_number':2420,'multiline':False]
['text':' From aten/src/ATen/native/AveragePool2d.cpp structured kernel meta func.','line_number':2432,'multiline':False]
['text':' need to set memory_format to preserve the memory format of the input','line_number':2668,'multiline':False]
['text':' channel last input should have channel last output','line_number':2669,'multiline':False]
['text':' type: ignore[assignment]','line_number':2768,'multiline':False]
['text':' type: ignore[assignment]','line_number':2833,'multiline':False]
['text':' aten::index is the internal advanced indexing implementation','line_number':2873,'multiline':False]
['text':' checkIndexTensorTypes and expandTensors','line_number':2874,'multiline':False]
['text':' expand_outplace','line_number':2905,'multiline':False]
['text':' avoid import cycle in mypy','line_number':2906,'multiline':False]
['text':' add missing null tensors','line_number':2909,'multiline':False]
['text':' hasContiguousSubspace','line_number':2913,'multiline':False]
['text':'   true if all non-null tensors are adjacent','line_number':2914,'multiline':False]
['text':' See:','line_number':2915,'multiline':False]
['text':' https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing','line_number':2916,'multiline':False]
['text':' https://stackoverflow.com/questions/53841497/why-does-numpy-mixed-basic-advanced-indexing-depend-on-slice-adjacency','line_number':2917,'multiline':False]
['text':' transposeToFront','line_number':2933,'multiline':False]
['text':' This is the logic that causes the newly inserted dimensions to show up','line_number':2934,'multiline':False]
['text':' at the beginning of the tensor, if they're not contiguous','line_number':2935,'multiline':False]
['text':' AdvancedIndex::AdvancedIndex','line_number':2950,'multiline':False]
['text':' Now we can assume the indices have contiguous subspace','line_number':2951,'multiline':False]
['text':' This is simplified from AdvancedIndex which goes to more effort','line_number':2952,'multiline':False]
['text':' to put the input and indices in a form so that TensorIterator can','line_number':2953,'multiline':False]
['text':' take them.  If we write a ref for this, probably that logic should','line_number':2954,'multiline':False]
['text':' get implemented','line_number':2955,'multiline':False]
['text':' High level logic taken from slow_conv3d_backward_cpu which should','line_number':2984,'multiline':False]
['text':' be representative of all convolution_backward impls','line_number':2985,'multiline':False]
['text':' Only foreach_pow has a ScalarAndTensor method and needs special','line_number':3179,'multiline':False]
['text':' handling because it does not work with _meta_foreach_out_of_place.','line_number':3180,'multiline':False]
['text':' aten.maximum(Tensor, Scalar) does not exist.','line_number':3212,'multiline':False]
['text':' aten.maximum(Tensor, Scalar) does not exist','line_number':3223,'multiline':False]
['text':' forach_addcdiv and addcdiv have different signatures and','line_number':3235,'multiline':False]
['text':' cannot use _meta_foreach_out_of_place.','line_number':3236,'multiline':False]
['text':' NB: This meta function accepts non-meta arguments!  When this behavior','line_number':3467,'multiline':False]
['text':' was originally introduced this was accidental, but it is now load bearing','line_number':3468,'multiline':False]
['text':' as people are using this so that they can conveniently test code involving','line_number':3469,'multiline':False]
['text':' embeddings (feeding CPU tensor inputs with meta device EmbeddingBag module)','line_number':3470,'multiline':False]
['text':' This part of the logic comes from make_max_indices_out in EmbeddingBag.cpp','line_number':3561,'multiline':False]
['text':' if specified, dtype takes precedence','line_number':3587,'multiline':False]
['text':' Add new leading dimensions to the tensor if the','line_number':3649,'multiline':False]
['text':' number of target dimensions is larger than the','line_number':3650,'multiline':False]
['text':' number of source dimensions.','line_number':3651,'multiline':False]
['text':' TODO: handle out','line_number':3820,'multiline':False]
['text':' WARNING: explicit bool conversion here is necessary;','line_number':3842,'multiline':False]
['text':' would be fixed by SymBool','line_number':3843,'multiline':False]
['text':' size of batch-dim can be 0.','line_number':3999,'multiline':False]
['text':' AveragePool3d','line_number':4010,'multiline':False]
['text':' Reference: aten/src/ATen/native/DilatedMaxPool2d.cpp','line_number':4156,'multiline':False]
['text':' type: ignore[assignment]','line_number':4523,'multiline':False]
['text':' zeros_like is special cased to work for sparse','line_number':4773,'multiline':False]
['text':' device can be not "meta"','line_number':4814,'multiline':False]
['text':' TODO: Deduplicate this with canonicalize_dim','line_number':4858,'multiline':False]
['text':' From aten/src/ATen/native/ScatterGatherChecks.h','line_number':4875,'multiline':False]
['text':' From aten/src/ATen/native/TensorAdvancedIndexing.cpp','line_number':4905,'multiline':False]
['text':' From aten/src/ATen/native/ScatterGatherChecks.h','line_number':4932,'multiline':False]
['text':' From aten/src/ATen/native/ScatterGatherChecks.h','line_number':4951,'multiline':False]
['text':' Check: index.size(d) <= self.size(d) for all d != dim','line_number':4963,'multiline':False]
['text':' Check: index.size(d) <= src.size(d) for all d if src is Tensor','line_number':4972,'multiline':False]
['text':' From aten/src/ATen/native/TensorAdvancedIndexing.cpp','line_number':4998,'multiline':False]
['text':' Check if we have a valid reduce operator.','line_number':5004,'multiline':False]
['text':' Cuda Path','line_number':5097,'multiline':False]
['text':' Note [Seed and Offset]: device for seed and offset below depends on whether we are','line_number':5121,'multiline':False]
['text':' capturing or not, but at the time of tracing we don't know if we','line_number':5122,'multiline':False]
['text':' are going to use cudagraphs or not, so we return meta tensors here','line_number':5123,'multiline':False]
['text':' it's possible we'll need to have some special handling in inductor for sdpa','line_number':5124,'multiline':False]
['text':' See Note [Seed and Offset]:','line_number':5232,'multiline':False]
['text':' Cuda Path','line_number':5324,'multiline':False]
['text':' See Note [Seed and Offset]:','line_number':5347,'multiline':False]
['text':' See Note [Seed and Offset]:','line_number':5423,'multiline':False]
['text':' N, C, ...','line_number':5560,'multiline':False]
['text':' convert output to correct memory format, if necessary','line_number':5605,'multiline':False]
['text':' following "heuristic: only use channels_last path when it's faster than the contiguous path"','line_number':5608,'multiline':False]
['text':' type: ignore[call-overload]','line_number':5650,'multiline':False]
['text':' Makes sure values and indices have the same strides. For cases where','line_number':5682,'multiline':False]
['text':' these have different shapes, like (5, 10, 5) and (0) in msort.','line_number':5683,'multiline':False]
['text':' type: ignore[arg-type]','line_number':5690,'multiline':False]
['text':' type: ignore[arg-type]','line_number':5691,'multiline':False]
['text':' TODO: Query cudnnGetRNNTrainingReserveSize (expose to python)','line_number':5795,'multiline':False]
['text':' From aten/src/ATen/native/ReduceOps.cpp','line_number':5855,'multiline':False]
['text':' From aten/src/ATen/native/Sorting.cpp','line_number':5884,'multiline':False]
['text':' From aten/src/ATen/native/cuda/RNN.cu','line_number':5902,'multiline':False]
['text':' From aten/src/ATen/native/cuda/RNN.cu','line_number':5917,'multiline':False]
['text':' From aten/src/ATen/native/mps/operations/Linear.mm','line_number':5931,'multiline':False]
['text':' type: ignore[call-overload]','line_number':5971,'multiline':False]
['text':' From aten/src/ATen/native/cuda/AmpKernels.cu','line_number':6034,'multiline':False]
['text':' From aten/src/ATen/native/UnaryOps.cpp','line_number':6053,'multiline':False]
['text':' noqa: B950','line_number':6097,'multiline':False]
['text':' Scalar','line_number':6114,'multiline':False]
['text':' We must also trigger meta registrations from PrimTorch ref','line_number':6169,'multiline':False]
['text':' decompositions','line_number':6170,'multiline':False]
['text':' For a given op, we pick the most specific decomp function from','line_number':6179,'multiline':False]
['text':' global_decomp_table in the precedence order of meta > post_autograd > pre_autograd','line_number':6180,'multiline':False]
['text':' Don't register meta for HigherOrderOp's decomp.','line_number':6189,'multiline':False]
['text':' We can reconsider this in the future, but in general,','line_number':6190,'multiline':False]
['text':' the way you do a meta for a HigherOrderOp is different from','line_number':6191,'multiline':False]
['text':' OpOverload.','line_number':6192,'multiline':False]
['text':' Internally, we shouldn't be registering meta kernels for any operators that','line_number':6202,'multiline':False]
['text':' have CompositeImplicitAutograd kernels.','line_number':6203,'multiline':False]
['text':' Instead, we should be letting those decompositions run, and writing meta kernels','line_number':6204,'multiline':False]
['text':' only for the base operators.','line_number':6205,'multiline':False]
['text':' Attempting to register a python meta kernel for a view operator.','line_number':6214,'multiline':False]
['text':' We shouldn't do this, because the output will report as not having aliased storages.','line_number':6215,'multiline':False]
['text':' All view ops have meta kernels in C++ today, so we should use those instead.','line_number':6216,'multiline':False]
['text':' causing infinite recursion, test_meta.py','line_number':6219,'multiline':False]
['text':' causing infinite recursion','line_number':6220,'multiline':False]
['text':' causing infinite recursion, test_serialization.py -k test_tensor_subclass_getstate_overwrite  # noqa: B950','line_number':6221,'multiline':False]
['text':' Exception not raised, test_torch.py -k test_storage_meta_errors_cpu_int64  # noqa: B950','line_number':6222,'multiline':False]
['text':' requires_grad mismatch, test_ops.py -k test_fake_crossref_backward_amp_istft_cuda_float32  # noqa: B950','line_number':6223,'multiline':False]
['text':' requires_grad mismatch! test_ops.py -k test_fake_crossref_backward_amp_rot90_cuda_float32  # noqa: B950','line_number':6224,'multiline':False]
['text':' requires_grad mismatch, test_ops.py -k test_fake_crossref_backward_no_amp_as_strided_scatter_cuda_float32  # noqa: B950','line_number':6225,'multiline':False]
