['text':' We will not let users register CustomOps with anything that could look like','line_number':33,'multiline':False]
['text':' PyTorch internals to avoid confusion.','line_number':34,'multiline':False]
['text':' Global dictionary holding references to all CustomOp objects','line_number':161,'multiline':False]
['text':' Yes, it keeps all CustomOps alive (see NOTE [CustomOp lifetime])','line_number':162,'multiline':False]
['text':' Used to query the CustomOp associated with a specific C++ dispatcher operator.','line_number':163,'multiline':False]
['text':' An example usage is FakeTensor: FakeTensor checks if a specific operator','line_number':164,'multiline':False]
['text':' has an implementation registered via the CustomOp API.','line_number':165,'multiline':False]
['text':' Indexed by qualname (e.g. aten::foo)','line_number':166,'multiline':False]
['text':' Has the name of the op, e.g. "foo". We cache here for convenience.','line_number':192,'multiline':False]
['text':' this is _opname but with namespace. e.g. "custom::foo"','line_number':194,'multiline':False]
['text':' mypy requires this','line_number':196,'multiline':False]
['text':' NB: Some of these impls are registered as kernels to DispatchKeys.','line_number':197,'multiline':False]
['text':' Modifying the _impls dict directly won't do anything in that case.','line_number':198,'multiline':False]
['text':' See NOTE [CustomOp autograd kernel indirection]','line_number':200,'multiline':False]
['text':' Records the impl and the source location in self._impls','line_number':210,'multiline':False]
['text':' Note that this doesn't cause torch.library to use the impl, that','line_number':211,'multiline':False]
['text':' needs to be done in a separate self._lib.impl call.','line_number':212,'multiline':False]
['text':' Pacify mypy','line_number':216,'multiline':False]
['text':' NOTE: [CustomOp lifetime]','line_number':234,'multiline':False]
['text':' A CustomOp, once created, lives forever. The mechanism is that the','line_number':235,'multiline':False]
['text':' global registry holds a reference to it. However, to make testing','line_number':236,'multiline':False]
['text':' easier, we want to be able to destroy CustomOp objects.','line_number':237,'multiline':False]
['text':' CustomOp._destroy does the job, though it leaves the CustomOp','line_number':238,'multiline':False]
['text':' in a garbage state.','line_number':239,'multiline':False]
['text':' Bypass torch.ops.* and directly do OperatorHandle::callBoxed.','line_number':252,'multiline':False]
['text':' Using torch.ops.* is a bit of a pain (it can be slow and it has lifetime','line_number':253,'multiline':False]
['text':' issues from caching operators that make testing CustomOp difficult).','line_number':254,'multiline':False]
['text':' Handle DispatchKey.Meta registration','line_number':418,'multiline':False]
['text':' We make assumptions about the schema's return types.','line_number':464,'multiline':False]
['text':' We can improve this by adding "all Autograd<BACKEND> keys", but','line_number':491,'multiline':False]
['text':' realistically people will just be using this API for CPU/CUDA for now.','line_number':492,'multiline':False]
['text':' If the user's operator is CompositeExplicitAutograd,','line_number':506,'multiline':False]
['text':' allow them to impl_abstract. This is being pragmatic','line_number':507,'multiline':False]
['text':' (existing custom ops may have CompositeExplicitAutograd','line_number':508,'multiline':False]
['text':' registration that don't work with Meta kernels, so this','line_number':509,'multiline':False]
['text':' gives them an escape hatch).','line_number':510,'multiline':False]
['text':' Otherwise, if the user's already has a Meta kernel or their','line_number':517,'multiline':False]
['text':' op is CompositeImplicitAutograd or some other alias dispatch key,','line_number':518,'multiline':False]
['text':' raise.','line_number':519,'multiline':False]
['text':' Special case for CompositeImplicitAutograd','line_number':521,'multiline':False]
['text':' NOTE ["backward", "save_for_backward", and "autograd"]','line_number':538,'multiline':False]
['text':' As a part of the explicit autograd API, a user must provide us','line_number':539,'multiline':False]
['text':' a "save_for_backward" function and a "backward" function.','line_number':540,'multiline':False]
['text':' When both of these have been provided, then we automatically','line_number':541,'multiline':False]
['text':' construct the "autograd" kernel.','line_number':542,'multiline':False]
['text':' For simplicity: don't allow self arguments','line_number':671,'multiline':False]
['text':' type: ignore[valid-type]','line_number':818,'multiline':False]
['text':' type: ignore[valid-type]','line_number':820,'multiline':False]
['text':' type: ignore[valid-type]','line_number':822,'multiline':False]
['text':' (python type, schema type, type[] variant, type?[] variant, type[]? variant','line_number':828,'multiline':False]
['text':' CustomOp expects the schema string without the namespace','line_number':913,'multiline':False]
