['text':' Import the following modules during type checking to enable code intelligence features,','line_number':26,'multiline':False]
['text':' such as auto-completion in tools like pylance, even when these modules are not explicitly','line_number':27,'multiline':False]
['text':' imported in user code.','line_number':28,'multiline':False]
['text':' TODO: Type[torch.SymInt], Type[torch.SymFloat]','line_number':40,'multiline':False]
['text':' TODO: This needs a lot more type annotations','line_number':42,'multiline':False]
['text':' NumberType = Union[bool, int, float, complex, torch.SymInt, torch.SymFloat]','line_number':43,'multiline':False]
['text':' I don't call it Integral because numbers.Integral includes bool, but IntLike','line_number':48,'multiline':False]
['text':' does not','line_number':49,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':69,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':74,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':75,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':76,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':77,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':78,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':79,'multiline':False]
['text':' For TorchRefsMode only','line_number':81,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':84,'multiline':False]
['text':' TODO: We should check that the symbols are consistent','line_number':102,'multiline':False]
['text':' with each other','line_number':103,'multiline':False]
['text':' TODO: look at using torch.testing.assert_close instead with an option','line_number':123,'multiline':False]
['text':'   to just compare metadata','line_number':124,'multiline':False]
['text':' Handles special cuda:0 vs cuda case','line_number':151,'multiline':False]
['text':' TODO: we should review why this happens and see about fixing it','line_number':152,'multiline':False]
['text':' Stride checking is currently disabled, see https://github.com/pytorch/pytorch/issues/78050','line_number':161,'multiline':False]
['text':' NOTE: only on CUDA because CPU elementwise strides are incorrect in PyTorch','line_number':186,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/77553','line_number':187,'multiline':False]
['text':' Only compares strides that are "meaningful" -- strides for dimensions with length > 1','line_number':188,'multiline':False]
['text':' and for tensors with more than one element','line_number':189,'multiline':False]
['text':' This function is equivalent to compute_contiguous() from TensorImpl.cpp','line_number':213,'multiline':False]
['text':' Skips checking strides when a dimension has length 1','line_number':226,'multiline':False]
['text':' This function is equivalent to compute_channels_last_contiguous_2d() in TensorImpl.cpp','line_number':237,'multiline':False]
['text':' NHWC or not channels last 2D contiguous','line_number':239,'multiline':False]
['text':' NDHWC or not channels last 3D contiguous','line_number':259,'multiline':False]
['text':' type: ignore[return]','line_number':293,'multiline':False]
['text':' NOTE: that tensors with no elements and channels last is ???','line_number':311,'multiline':False]
['text':' Short-circuits if the tensor is already contiguous or channels-last contiguous','line_number':339,'multiline':False]
['text':' The following is equivalent to compute_non_overlapping_and_dense in TensorImpl.cpp','line_number':343,'multiline':False]
['text':' Short-circuits for tensors of rank one, which are','line_number':345,'multiline':False]
['text':' non-overlapping and "dense" if their stride is one','line_number':346,'multiline':False]
['text':' Checks that there exists a permutation of the strides s.t. the tensor would be contiguous','line_number':350,'multiline':False]
['text':' Sorts (length, stride) pairs by stride','line_number':351,'multiline':False]
['text':' NOTE: Based on the implementation in TensorIterator.cpp, but note that','line_number':367,'multiline':False]
['text':' the note [Computing output strides] is incorrect, because it','line_number':368,'multiline':False]
['text':' says that strides will be preserved even if they are not','line_number':369,'multiline':False]
['text':' "non overlapping and dense", but this is incorrect. The','line_number':370,'multiline':False]
['text':' output of elementwise operations are always given','line_number':371,'multiline':False]
['text':' non overlapping and dense strides.','line_number':372,'multiline':False]
['text':' This is also INCORRECT because it does not model TensorIterator's','line_number':373,'multiline':False]
['text':' short-circuit, which can cause different strides.','line_number':374,'multiline':False]
['text':' Filters the tensors to actual tensors','line_number':385,'multiline':False]
['text':' Short-circuits for CPU scalar case','line_number':393,'multiline':False]
['text':' Short-circuits for shapes with zero or one dimensions','line_number':397,'multiline':False]
['text':' TODO: are these necessary?','line_number':398,'multiline':False]
['text':' Short-circuits if contiguous, following the fake fast path.','line_number':405,'multiline':False]
['text':' This reduces the number of guards we end up making','line_number':406,'multiline':False]
['text':' TODO: do channels last too','line_number':407,'multiline':False]
['text':' stride_a == stride_b','line_number':433,'multiline':False]
['text':' Note: this case is hit if all strides are zero,','line_number':437,'multiline':False]
['text':' or all strides are equal and all dimensions have the same length','line_number':438,'multiline':False]
['text':' The "sort" order for the permutation is back-to-front, but','line_number':441,'multiline':False]
['text':' the natural order for permutations is front-to-back.  Do the','line_number':442,'multiline':False]
['text':' sorting back-to-front and then reverse it on output.','line_number':443,'multiline':False]
['text':'','line_number':444,'multiline':False]
['text':' also, note this returns the logical to physical shape permutation','line_number':445,'multiline':False]
['text':' insertion sort with support for ambiguous comparisons','line_number':448,'multiline':False]
['text':' Filters the tensors to actual tensors','line_number':472,'multiline':False]
['text':' Short-circuits for CPU scalar case','line_number':477,'multiline':False]
['text':' to physical','line_number':492,'multiline':False]
['text':' to logical','line_number':497,'multiline':False]
['text':' Identity permutation is [0, 1, 2]','line_number':502,'multiline':False]
['text':'','line_number':519,'multiline':False]
['text':' Common helper functions','line_number':520,'multiline':False]
['text':'','line_number':521,'multiline':False]
['text':' sometimes called with sympy expression by inductor','line_number':533,'multiline':False]
['text':' "Wraps" a dim (up to one time) for the given rank, allowing dims to be','line_number':585,'multiline':False]
['text':' specified using negative indices. If `wrap_scalar` is true then scalar','line_number':586,'multiline':False]
['text':' tensors of rank 0 will allow dimensions in the range [-1, 0]. Otherwise,','line_number':587,'multiline':False]
['text':' idx should be in the range [-rank, rank-1].','line_number':588,'multiline':False]
['text':' Same error message as in aten/src/ATen/WrapDimUtils.h:49','line_number':609,'multiline':False]
['text':' Takes a dimension or sequence of dimensions and "wraps" them,','line_number':616,'multiline':False]
['text':' mapping negative offsets to positive ones','line_number':617,'multiline':False]
['text':' Short-circuits if all (one or fewer) arguments are trivially on the same device','line_number':672,'multiline':False]
['text':' Note: cannot initialize device to the first arg's device (it may not have one)','line_number':676,'multiline':False]
['text':' Asserts if any of the following are true:','line_number':712,'multiline':False]
['text':'   - a non-scalar or non-Tensor is given','line_number':713,'multiline':False]
['text':'   - the shape of any tensors is distinct','line_number':714,'multiline':False]
['text':' Acquires a common shape, if it exists, from one or more tensor arguments,','line_number':745,'multiline':False]
['text':' filtering number arguments','line_number':746,'multiline':False]
['text':' Extracts dimensions that might be passed either as a list/tuple or as varargs.','line_number':770,'multiline':False]
['text':' A typical case is Tensor.permute .','line_number':771,'multiline':False]
['text':' Handles tuple unwrapping','line_number':805,'multiline':False]
['text':' type: ignore[arg-type]','line_number':810,'multiline':False]
['text':' type: ignore[return-value]','line_number':811,'multiline':False]
['text':' 1s map to the other size (even 0)','line_number':833,'multiline':False]
['text':' Convert to list to produce a compatible error message with core','line_number':875,'multiline':False]
['text':' PyTorch, which prints sequences in square brackets.','line_number':876,'multiline':False]
['text':' NB: This is pretty important when you have unbacked SymInts.','line_number':879,'multiline':False]
['text':' Suppose you have (i0, 12) resizing into (2, -1, 12).  The old','line_number':880,'multiline':False]
['text':' range for i0 is typically [2, inf], which means if you divide','line_number':881,'multiline':False]
['text':' by two the new range should be [1, inf].  But this is bad news','line_number':882,'multiline':False]
['text':' if you have an unbacked SymInt: we need to reapply the unsound','line_number':883,'multiline':False]
['text':' assumption that the size is >= 2.','line_number':884,'multiline':False]
['text':' TODO: type error here is real, replace with sym_complex','line_number':981,'multiline':False]
['text':' type: ignore[arg-type]','line_number':982,'multiline':False]
['text':' TODO: sym_complex_float?','line_number':1000,'multiline':False]
['text':' Type checking','line_number':1048,'multiline':False]
['text':' Returns the higher of two torch datatypes a and b or, if the two','line_number':1064,'multiline':False]
['text':'   are not ordered relative to each other, the next','line_number':1065,'multiline':False]
['text':'   higher datatype','line_number':1066,'multiline':False]
['text':' Type checking','line_number':1076,'multiline':False]
['text':' TODO: maybe unify with can_cast_to?','line_number':1142,'multiline':False]
['text':' Scalar type checking is disabled (and may be removed in the future)','line_number':1191,'multiline':False]
['text':' if scalar_type is None:','line_number':1193,'multiline':False]
['text':'     scalar_type = type(arg)','line_number':1194,'multiline':False]
['text':' if scalar_type is not type(arg):','line_number':1196,'multiline':False]
['text':'     msg = (','line_number':1197,'multiline':False]
['text':'         "Scalar of type "','line_number':1198,'multiline':False]
['text':'         + str(type(arg))','line_number':1199,'multiline':False]
['text':'         + " is not the expected type of "','line_number':1200,'multiline':False]
['text':'         + str(scalar_type)','line_number':1201,'multiline':False]
['text':'         + "!"','line_number':1202,'multiline':False]
['text':'     )','line_number':1203,'multiline':False]
['text':'     raise RuntimeError(msg)','line_number':1204,'multiline':False]
['text':' Maps datatypes to their computation types for elementwise operations','line_number':1238,'multiline':False]
['text':' Equivalent to at::toAccumulateType, prefer computation_dtype where possible','line_number':1260,'multiline':False]
['text':' for complex types outputs corresponding real type','line_number':1278,'multiline':False]
['text':' keep output in opmath type, needed for mean','line_number':1279,'multiline':False]
['text':' Describes the return type of the primitive:','line_number':1283,'multiline':False]
['text':'','line_number':1284,'multiline':False]
['text':'   - NEW, a new tensor is created','line_number':1285,'multiline':False]
['text':'   - VIEW, a view of an input tensor is returned','line_number':1286,'multiline':False]
['text':'   - INPLACE, one or more input tensors is modified','line_number':1287,'multiline':False]
['text':'','line_number':1288,'multiline':False]
['text':' these descriptors are mututally exclusive and exhaustive.','line_number':1289,'multiline':False]
['text':' TODO: when NumberType contains the sym types, can simplify this','line_number':1296,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1307,'multiline':False]
['text':' NB: Not strictly correct, but we don't support SymPy complex or bool.','line_number':1310,'multiline':False]
['text':' TODO: document type promotion kinds','line_number':1314,'multiline':False]
['text':' Import sympy locally, as importing it eagerly at a module level is too slow','line_number':1409,'multiline':False]
['text':' See https://dev-discuss.pytorch.org/t/delving-into-what-happens-when-you-import-torch/1589','line_number':1410,'multiline':False]
['text':' x is a TensorLike','line_number':1423,'multiline':False]
['text':' x.ndim > 0','line_number':1443,'multiline':False]
['text':' Prefers dtype of tensors with one or more dimensions','line_number':1448,'multiline':False]
['text':' highest_type is bool','line_number':1471,'multiline':False]
['text':' NOTE: computation can still occur in a complex dtype','line_number':1483,'multiline':False]
['text':' even though some reductions, like amin or amax, don't strictly require type promotion,','line_number':1503,'multiline':False]
['text':' all the math ops (including comparisons) are still defined only for a computation type,','line_number':1504,'multiline':False]
['text':' so promotion will still happen. We are doing it explicitly here','line_number':1505,'multiline':False]
['text':' ALWAYS_BOOL','line_number':1520,'multiline':False]
['text':' This function's logic is borrowed from the following functions defined in C++:','line_number':1525,'multiline':False]
['text':' batched_matrix_contiguous_strides and contiguous_strides','line_number':1526,'multiline':False]
['text':' contiguous_strides from c10/util/strides.h','line_number':1535,'multiline':False]
['text':' batched_matrix_contiguous_strides from aten/src/ATen/native/LinearAlgebraUtils.h','line_number':1550,'multiline':False]
['text':' NOTE: intentionally divergence from make_contiguous_strides_for','line_number':1568,'multiline':False]
['text':' This is consistent with eager','line_number':1569,'multiline':False]
['text':' TODO: maybe inform the user of channels_last_3d if rank of the tensor is 5?','line_number':1577,'multiline':False]
['text':' NOTE: intentionally divergence from make_contiguous_strides_for','line_number':1586,'multiline':False]
['text':' This is consistent with eager','line_number':1587,'multiline':False]
['text':' NOTE: intentionally divergence from make_contiguous_strides_for','line_number':1603,'multiline':False]
['text':' This is consistent with eager','line_number':1604,'multiline':False]
['text':' NB: we don't actually support symint here, but it's harmless to accept','line_number':1664,'multiline':False]
['text':' Short-circuits if the shape has no elements','line_number':1702,'multiline':False]
['text':' +1 to account for the first element which offsets are taken from','line_number':1707,'multiline':False]
['text':' NOTE: This function should ideally be removed, but some Meta internal models','line_number':1729,'multiline':False]
['text':' packaged with `torch.package` are using it, so it will have to be removed','line_number':1730,'multiline':False]
['text':' at some point in the future when those models no longer use this function.','line_number':1731,'multiline':False]
['text':' This combines is_channels_last_strides_2d and is_channels_last_strides_3d in','line_number':1753,'multiline':False]
['text':' c10/core/MemoryFormat.h into one function','line_number':1754,'multiline':False]
['text':' Check for channels_last_2d','line_number':1761,'multiline':False]
['text':' Check for channels_last_3d','line_number':1764,'multiline':False]
['text':' This is a Python implementation of','line_number':1805,'multiline':False]
['text':' aten/src/ATen/ExpandUtils.h:is_expandable_to','line_number':1806,'multiline':False]
['text':' torch.where(mask, t, False) is equivalent','line_number':1820,'multiline':False]
['text':' but feels hacky and might break in the future','line_number':1821,'multiline':False]
['text':' We want to go from .special / .nn.functional','line_number':1840,'multiline':False]
['text':' to special and special_ / nn_functional_','line_number':1841,'multiline':False]
['text':' Our eager implementations for *_scatter ops are all primitives w.r.t autograd,','line_number':1865,'multiline':False]
['text':' so these as_strided() calls are not seen by autograd.','line_number':1866,'multiline':False]
['text':' We need to mimic this behavior in our ref/prim implementations.','line_number':1867,'multiline':False]
['text':' TODO: a better way to handle this would be with a new op, "_unsafe_as_strided"','line_number':1868,'multiline':False]
['text':' We should revisit this when we add a compositional as_strided op,','line_number':1869,'multiline':False]
['text':' and also as part of https://github.com/pytorch/pytorch/issues/90507','line_number':1870,'multiline':False]
['text':' Rng state is [64-bit seed, 64-bit offset]','line_number':1922,'multiline':False]
