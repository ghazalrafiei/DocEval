['text':' We can't know if the run_fn will internally move some args to different devices,','line_number':157,'multiline':False]
['text':' which would require logic to preserve rng states for those devices as well.','line_number':158,'multiline':False]
['text':' We could paranoically stash and restore ALL the rng states for all visible devices,','line_number':159,'multiline':False]
['text':' but that seems very wasteful for most cases.  Compromise:  Stash the RNG state for','line_number':160,'multiline':False]
['text':' the device of all Tensor args.','line_number':161,'multiline':False]
['text':'','line_number':162,'multiline':False]
['text':' To consider:  maybe get_device_states and set_device_states should reside in torch/random.py?','line_number':163,'multiline':False]
['text':' This will not error out if "arg" is a CPU tensor or a non-tensor type because','line_number':165,'multiline':False]
['text':' the conditionals short-circuit.','line_number':166,'multiline':False]
['text':' Accommodates the (remote) possibility that autocast is enabled for cpu AND gpu.','line_number':228,'multiline':False]
['text':' Don't eagerly initialize the cuda context by accident.','line_number':235,'multiline':False]
['text':' (If the user intends that the context is initialized later, within their','line_number':236,'multiline':False]
['text':' run_function, we SHOULD actually stash the cuda state here.  Unfortunately,','line_number':237,'multiline':False]
['text':' we have no way to anticipate this will happen before we run the function.)','line_number':238,'multiline':False]
['text':' Save non-tensor inputs in ctx, keep a placeholder None for tensors','line_number':245,'multiline':False]
['text':' to be filled out during the backward.','line_number':246,'multiline':False]
['text':' Copy the list to avoid modifying original list.','line_number':272,'multiline':False]
['text':' Fill in inputs with appropriate saved tensors.','line_number':278,'multiline':False]
['text':' Stash the surrounding rng state, and mimic the state that was','line_number':282,'multiline':False]
['text':' present at this time during forward.  Restore the surrounding state','line_number':283,'multiline':False]
['text':' when we're done.','line_number':284,'multiline':False]
['text':' run backward() with only tensor that requires grad','line_number':307,'multiline':False]
['text':' TorchDynamo does not step inside utils.checkpoint function.  The flow','line_number':331,'multiline':False]
['text':' looks likes this','line_number':332,'multiline':False]
['text':'  1) TorchDynamo tries to wrap utils.checkpoint in a HigherOrderOp by','line_number':333,'multiline':False]
['text':'     speculatively checking if the forward function is safe to trace.','line_number':334,'multiline':False]
['text':'  2) If yes, then Dynamo-generated Fx graph has the wrapped higher','line_number':335,'multiline':False]
['text':'     order op. As a result, TorchDynamo does not look inside utils.checkpoint.','line_number':336,'multiline':False]
['text':'  3) If not, then TorchDynamo falls back to eager by performing a graph','line_number':337,'multiline':False]
['text':'     break. And here, the following disable wrapper ensures that','line_number':338,'multiline':False]
['text':'     TorchDynamo does not trigger again on the frames created by','line_number':339,'multiline':False]
['text':'     utils.checkpoint innards.','line_number':340,'multiline':False]
['text':' Hack to mix *args with **kwargs in a python 2.7-compliant way','line_number':470,'multiline':False]
['text':' Runs pre-forward logic','line_number':488,'multiline':False]
['text':' Runs post-forward logic','line_number':491,'multiline':False]
['text':' Hack for keyword-only parameter in a python 2.7-compliant way','line_number':553,'multiline':False]
['text':' the last chunk has to be non-volatile','line_number':572,'multiline':False]
['text':' NOTE [ Nestable Checkpoint ]','line_number':593,'multiline':False]
['text':'','line_number':594,'multiline':False]
['text':' The semantics of nested checkpoint can be defined by two basic rules.','line_number':595,'multiline':False]
['text':' Following the two rules leads to an important implication that is central','line_number':596,'multiline':False]
['text':' to motivating the design.','line_number':597,'multiline':False]
['text':'','line_number':598,'multiline':False]
['text':' Rule 1. Saved tensors are managed by inner-most checkpoint only and hidden','line_number':599,'multiline':False]
['text':'         from any outer layers of checkpoint.','line_number':600,'multiline':False]
['text':'','line_number':601,'multiline':False]
['text':' Rule 2. The inputs of inner checkpoints are treated as tensors saved to its','line_number':602,'multiline':False]
['text':'         parent checkpoint.','line_number':603,'multiline':False]
['text':'','line_number':604,'multiline':False]
['text':' Implication: To recompute any given saved tensor, we need to recompute all of','line_number':605,'multiline':False]
['text':'              the checkpoints wrapping it.','line_number':606,'multiline':False]
['text':'','line_number':607,'multiline':False]
['text':' Why is this implied? To unpack a saved tensor X during backward we need to','line_number':608,'multiline':False]
['text':' recompute the inner-most checkpoint (#1), and in order to recompute that','line_number':609,'multiline':False]
['text':' checkpoint I need to have its inputs, which are managed by that checkpoint's','line_number':610,'multiline':False]
['text':' parent (#2), which thus also needs to be recomputed first. Continue this line','line_number':611,'multiline':False]
['text':' of reasoning and we realize that in order to unpack X, all checkpoints that','line_number':612,'multiline':False]
['text':' were active at the time X was saved need to be recomputed. (unless we have','line_number':613,'multiline':False]
['text':' already done so in that backward for some other saved tensor).','line_number':614,'multiline':False]
['text':'','line_number':615,'multiline':False]
['text':' In practice, we use a noop autograd Function to save inputs as saved tensors.','line_number':616,'multiline':False]
['text':' During unpack calling ctx.saved_tensor triggers the parent checkpoint to','line_number':617,'multiline':False]
['text':' recompute.','line_number':618,'multiline':False]
['text':'','line_number':619,'multiline':False]
['text':' Rule 3. We should start recomputation as if there are no checkpoints currently','line_number':620,'multiline':False]
['text':'         active. Checkpoints encountered during recomputation are still','line_number':621,'multiline':False]
['text':'         respected.','line_number':622,'multiline':False]
['text':'','line_number':623,'multiline':False]
['text':' When we start recomputation, we push the saved variable hook meant for','line_number':624,'multiline':False]
['text':' recomputation on the stack. See examples in Rule 6 for more context.','line_number':625,'multiline':False]
['text':'','line_number':626,'multiline':False]
['text':'                                  * * * *','line_number':627,'multiline':False]
['text':'','line_number':628,'multiline':False]
['text':' Beyond the basic semantics specific to nested checkpoint, we impose several','line_number':629,'multiline':False]
['text':' more constraints that may apply to checkpointing in general.','line_number':630,'multiline':False]
['text':'','line_number':631,'multiline':False]
['text':' Rule 4. Lifetime of recomputed tensors','line_number':632,'multiline':False]
['text':'','line_number':633,'multiline':False]
['text':'         Recomputed tensors are considered specific to particular invocations','line_number':634,'multiline':False]
['text':'         of backward and are always cleared immediately as they are unpacked','line_number':635,'multiline':False]
['text':'         Particularly, we require this to happen even if retain_graph=True.','line_number':636,'multiline':False]
['text':'','line_number':637,'multiline':False]
['text':' [ Implementation details of Rule 4 ]','line_number':638,'multiline':False]
['text':'','line_number':639,'multiline':False]
['text':' If we were okay with recomputed tensors staying alive after backward is run','line_number':640,'multiline':False]
['text':' with retain_graph=True, we would store recomputed variables as the values of a','line_number':641,'multiline':False]
['text':' WeakKeyDictionary and pack strong references to the keys, so that as we','line_number':642,'multiline':False]
['text':' backward, those packed keys would be cleared as long as retain_graph=False.','line_number':643,'multiline':False]
['text':' Clearing the packed key clears the corresponding entry in the WKD.','line_number':644,'multiline':False]
['text':'','line_number':645,'multiline':False]
['text':' If we wish recomputed variables to be immediately cleared as we unpack them in','line_number':646,'multiline':False]
['text':' the retain_graph=True case, we cannot rely on the packed keys to be cleared by','line_number':647,'multiline':False]
['text':' backward automatically. Instead of packing the strong reference to the key','line_number':648,'multiline':False]
['text':' directly, we pack a container object, which we manually clear as we unpack.','line_number':649,'multiline':False]
['text':'','line_number':650,'multiline':False]
['text':' An important detail is that if a second backward happens, the second','line_number':651,'multiline':False]
['text':' recomputation needs to reset the container with a newly created key.','line_number':652,'multiline':False]
['text':'','line_number':653,'multiline':False]
['text':' Rule 5. Stop recomputation as soon as we've recomputed the saved tensors we','line_number':654,'multiline':False]
['text':'         know we need.','line_number':655,'multiline':False]
['text':'','line_number':656,'multiline':False]
['text':' [ Implementation details of Rule 5 ]','line_number':657,'multiline':False]
['text':'','line_number':658,'multiline':False]
['text':' During recomputation, raise an exception if the number of recomputed tensors','line_number':659,'multiline':False]
['text':' matches the number of tensors that we expected to recompute. We wrap the','line_number':660,'multiline':False]
['text':' recomputation call with a try-catch to catch this specific exception. See','line_number':661,'multiline':False]
['text':' Rule #6 below for some examples.','line_number':662,'multiline':False]
['text':'','line_number':663,'multiline':False]
['text':' Rule 6. We support doing backward inside checkpoint context','line_number':664,'multiline':False]
['text':'','line_number':665,'multiline':False]
['text':' [ retain_graph is True]','line_number':666,'multiline':False]
['text':'','line_number':667,'multiline':False]
['text':' def fn(x):','line_number':668,'multiline':False]
['text':'   y = x.sin()','line_number':669,'multiline':False]
['text':'   z = y.cos()','line_number':670,'multiline':False]
['text':'   gx, = torch.autograd.grad(z, x, retains_grad=True)','line_number':671,'multiline':False]
['text':'   return gx, z','line_number':672,'multiline':False]
['text':'','line_number':673,'multiline':False]
['text':' out = checkpoint(fn)(inp)','line_number':674,'multiline':False]
['text':' out.backward()','line_number':675,'multiline':False]
['text':'','line_number':676,'multiline':False]
['text':' Because z is saved by cos while checkpoint is enabled, it would not be','line_number':677,'multiline':False]
['text':' actually saved, and so the .grad() call inside must trigger a recomputation.','line_number':678,'multiline':False]
['text':'','line_number':679,'multiline':False]
['text':' During recomputation the "inner pack hook" has two responsibilities:','line_number':680,'multiline':False]
['text':'','line_number':681,'multiline':False]
['text':' 1) As usual, populating the WeakKeyDictionary storing recomputed tensors','line_number':682,'multiline':False]
['text':' 2) Pack the actual tensor (detached) so that one may perform backward on the','line_number':683,'multiline':False]
['text':'    recomputed graph. The tensors saved to this graph will live until the end','line_number':684,'multiline':False]
['text':'    of recomputation, or die earlier if someone performs backward with','line_number':685,'multiline':False]
['text':'    retain_graph=False.','line_number':686,'multiline':False]
['text':'','line_number':687,'multiline':False]
['text':' More generally performing backward on the recomputed graph occurs in the','line_number':688,'multiline':False]
['text':' following cases:','line_number':689,'multiline':False]
['text':' - If backward is performed inside forward,','line_number':690,'multiline':False]
['text':'   - During the original forward IF early-stop is disabled','line_number':691,'multiline':False]
['text':'   - During the original backward','line_number':692,'multiline':False]
['text':' - If there are multiple .grad()/.backward() calls, we would perform backward','line_number':693,'multiline':False]
['text':'   on the recomputed graph even if early-stop is enabled (see the example below)','line_number':694,'multiline':False]
['text':'','line_number':695,'multiline':False]
['text':' [ retain_graph is False ]','line_number':696,'multiline':False]
['text':'','line_number':697,'multiline':False]
['text':' The example below shows what happens if during recomputation we find that some','line_number':698,'multiline':False]
['text':' of the tensors we are trying to recompute have already been cleared.','line_number':699,'multiline':False]
['text':'','line_number':700,'multiline':False]
['text':' Spoiler: we don't do anything special, we just skip over them!','line_number':701,'multiline':False]
['text':'','line_number':702,'multiline':False]
['text':' def fn(x):','line_number':703,'multiline':False]
['text':'   y = x.sin()                           # (1)','line_number':704,'multiline':False]
['text':'   z = y.cos()                           # (2)','line_number':705,'multiline':False]
['text':'   gx, = torch.autograd.grad(z, x)       # (3)','line_number':706,'multiline':False]
['text':'   return x.cos() * gx                   # (4)','line_number':707,'multiline':False]
['text':'','line_number':708,'multiline':False]
['text':' out = checkpoint(fn)(inp)','line_number':709,'multiline':False]
['text':' out.backward()                          # (5)','line_number':710,'multiline':False]
['text':'','line_number':711,'multiline':False]
['text':' 1, 2. Don't save x and y since we are inside a checkpoint.','line_number':712,'multiline':False]
['text':' 3. Trigger a recompute of fn since x and y weren't saved.','line_number':713,'multiline':False]
['text':'    And depending on whether early stop is enabled, either stop at (2) or','line_number':714,'multiline':False]
['text':'    continue running the function.','line_number':715,'multiline':False]
['text':'    Because we are running backward with retain_graph=False, we clear x and y's','line_number':716,'multiline':False]
['text':'    holders.','line_number':717,'multiline':False]
['text':' 4. Don't save x since we are inside a checkpoint.','line_number':718,'multiline':False]
['text':' 5. Calling backward triggers another recompute of fn. During recompute, we see','line_number':719,'multiline':False]
['text':'    that x and y have already been cleared in the original graph as indicated','line_number':720,'multiline':False]
['text':'    by holder=None. We skip over them. We still save x at (4) (since its holder','line_number':721,'multiline':False]
['text':'    is still alive.)','line_number':722,'multiline':False]
['text':' Only tensors can be saved with ctx.save_for_backward, everything else','line_number':774,'multiline':False]
['text':' is captured by get_args, which is saved directly on ctx','line_number':775,'multiline':False]
['text':' args but with tensors replaced with None as placeholders','line_number':780,'multiline':False]
['text':' restore the placeholders with the original tensors grabbed from','line_number':784,'multiline':False]
['text':' ctx.saved_tensors (which may be saved on a parent checkpoint if','line_number':785,'multiline':False]
['text':' this checkpoint is nested, and that would trigger a recursive','line_number':786,'multiline':False]
['text':' unpack!)','line_number':787,'multiline':False]
['text':' grab the tail since we also saved the dummy to avoid having to explicitly','line_number':792,'multiline':False]
['text':' handle the case where there are no tensor inputs','line_number':793,'multiline':False]
['text':' We store this as a weakkeydictionary so that in the case of a partial','line_number':809,'multiline':False]
['text':' backward, the entries in the dict are cleared alongside the Holder','line_number':810,'multiline':False]
['text':' which will be removed when the SavedVariable is cleared.','line_number':811,'multiline':False]
['text':' We need both recomp_counter and recomputed since they can diverge','line_number':815,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/90105#discussion_r1135889885','line_number':816,'multiline':False]
['text':' See Rule 5','line_number':820,'multiline':False]
['text':' Debugging','line_number':823,'multiline':False]
['text':' TODO: we can probably make this check stricter by checking that','line_number':832,'multiline':False]
['text':'       the metadata of the first tensors still match.','line_number':833,'multiline':False]
['text':' NOTE [ Error handling for checkpoint ]','line_number':835,'multiline':False]
['text':'','line_number':836,'multiline':False]
['text':' At a high level, we need to check that the tensors saved','line_number':837,'multiline':False]
['text':' during original forward matches tensors saved during recompute','line_number':838,'multiline':False]
['text':' This means handling 3 cases:','line_number':839,'multiline':False]
['text':'','line_number':840,'multiline':False]
['text':' 1. During recompute, more tensors were saved.','line_number':841,'multiline':False]
['text':'','line_number':842,'multiline':False]
['text':'    Usually this is hidden due to the StopRecomputationError','line_number':843,'multiline':False]
['text':'    but if early stop is not enabled, or we would have errored','line_number':844,'multiline':False]
['text':'    anyway because there aren't enough weak_holders. But we','line_number':845,'multiline':False]
['text':'    do want to have a nice error. See the _recomputation_hook','line_number':846,'multiline':False]
['text':'    for details.','line_number':847,'multiline':False]
['text':' 2. During recompute, fewer tensors were saved','line_number':849,'multiline':False]
['text':'','line_number':850,'multiline':False]
['text':' We know that everytime we save something do original forward','line_number':851,'multiline':False]
['text':' we append to weak_holder, and every time we save a tensor','line_number':852,'multiline':False]
['text':' during recompute we increment recompute_counter.','line_number':853,'multiline':False]
['text':' 3. During recompute, the same tensors were saved, but they','line_number':861,'multiline':False]
['text':'    have different metadata','line_number':862,'multiline':False]
['text':' We've seen all holders since we iterate over them in order','line_number':868,'multiline':False]
['text':' For every holder that is still alive now, it must've been','line_number':869,'multiline':False]
['text':' alive when we saw it during recompute, therefore, the','line_number':870,'multiline':False]
['text':' gid must be set.','line_number':871,'multiline':False]
['text':' We know this is the first unpack, so it couldn't have been set','line_number':873,'multiline':False]
['text':' to None yet.','line_number':874,'multiline':False]
['text':' We always set these together in the recomputation hook','line_number':876,'multiline':False]
['text':' see pack hook, x_metadata is 1:1 with weak_holders.','line_number':878,'multiline':False]
['text':' This function returns the context_fn and error_cb to be used by the','line_number':964,'multiline':False]
['text':' checkpointing mechanism. error_cb is invoked when an error is detected','line_number':965,'multiline':False]
['text':' during unpack.','line_number':966,'multiline':False]
['text':' record_context_cpp is not support on non-linux non-x86_64 platforms','line_number':968,'multiline':False]
['text':' Start printing stack trace only after __torch_dispatch__ is found','line_number':996,'multiline':False]
['text':' These properties are fast to check, easy to understand','line_number':1023,'multiline':False]
['text':' See Rule 5','line_number':1035,'multiline':False]
['text':' appease mypy','line_number':1044,'multiline':False]
['text':' We run into this case when early stop is not enabled and do','line_number':1051,'multiline':False]
['text':' grad within checkpoint.','line_number':1052,'multiline':False]
['text':' We need to set this flag, so we don't error out later when','line_number':1053,'multiline':False]
['text':' we check if the number of tensors saved during forward and','line_number':1054,'multiline':False]
['text':' recomputation match.','line_number':1055,'multiline':False]
['text':' This holder may have been cleared because someone may have called','line_number':1065,'multiline':False]
['text':' backward within forward. If so, we don't need to save.','line_number':1066,'multiline':False]
['text':' See Rule 6: [ retain_graph is True ] above','line_number':1076,'multiline':False]
['text':' See Rule 6: [ retain_graph is True ] above for an example of when','line_number':1080,'multiline':False]
['text':' the graph created during recomputation could be backwarded.','line_number':1081,'multiline':False]
['text':' See Rule 4 above','line_number':1090,'multiline':False]
['text':' Save metadata to detect non-determinism','line_number':1093,'multiline':False]
['text':' generate a temporary id if we trigger unpack outside of a backward call','line_number':1102,'multiline':False]
['text':' Check if we are under AOTAutograd tracing','line_number':1144,'multiline':False]
['text':' There should probably be a better way to do this...','line_number':1145,'multiline':False]
['text':' NOTE: torch.utils.checkpoint internal logic will call these two functions unknown number of times','line_number':1164,'multiline':False]
['text':' (i.e. there could be _CachedTorchDispatchMode calls that doesn't map to a _CachingTorchDispatchMode call),','line_number':1165,'multiline':False]
['text':' so we ignore these ops and just always recompute them.','line_number':1166,'multiline':False]
['text':' NOTE: Here we just store and reuse output of all ops, since in torch.compile mode','line_number':1191,'multiline':False]
['text':' we decide and handle recomputation in the partitioner.','line_number':1192,'multiline':False]
['text':' NB: this helper wraps fn before calling checkpoint_impl. kwargs and','line_number':1301,'multiline':False]
['text':'     saving/restoring of global state is handled here.','line_number':1302,'multiline':False]
['text':' Accommodates the (remote) possibility that autocast is enabled for cpu AND gpu.','line_number':1367,'multiline':False]
['text':' Don't eagerly initialize the cuda context by accident.','line_number':1372,'multiline':False]
['text':' (If the user intends that the context is initialized later, within their','line_number':1373,'multiline':False]
['text':' run_function, we SHOULD actually stash the cuda state here.  Unfortunately,','line_number':1374,'multiline':False]
['text':' we have no way to anticipate this will happen before we run the function.','line_number':1375,'multiline':False]
['text':' If they do so, we raise an error.)','line_number':1376,'multiline':False]
['text':' This will be called later during recomputation. This wrapping enables','line_number':1384,'multiline':False]
['text':' the necessary global state to be captured.','line_number':1385,'multiline':False]
['text':' When ambient grad_mode is False','line_number':1413,'multiline':False]
['text':' Device was not initialized before running the forward, so we didn't','line_number':1424,'multiline':False]
['text':' stash the device state.','line_number':1425,'multiline':False]
