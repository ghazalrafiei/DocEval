['text':' This module provides a FAST (on GPU) content addressable store for storages','line_number':1,'multiline':False]
['text':' (and tensors on top of them) with VERY WEAK portability guarantees (e.g.,','line_number':2,'multiline':False]
['text':' don't expect CPU/CUDA to address to the same hash, don't expect it to be','line_number':3,'multiline':False]
['text':' portable across devices) that is NOT cryptographically secure.  In return,','line_number':4,'multiline':False]
['text':' we are able to hash 40G of tensor data on GPU in less than a second,','line_number':5,'multiline':False]
['text':' compared to running SHA-1 in CPU which would a minute or so.  The primary','line_number':6,'multiline':False]
['text':' use case is for efficiently snapshotting intermediate tensor data for','line_number':7,'multiline':False]
['text':' offline debugging, but it's been put in this module in case you think of','line_number':8,'multiline':False]
['text':' another use case for it.  The hash function could be replaced with a','line_number':9,'multiline':False]
['text':' straight reimplementation of SHA-1, which would give us much stronger','line_number':10,'multiline':False]
['text':' portability guarantees.','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':' WARNING: THERE IS NO BC/FC GUARANTEE FOR THIS FORMAT!  If you need to format','line_number':13,'multiline':False]
['text':' shift the result, consider packing it into a single torch.save object','line_number':14,'multiline':False]
['text':' with traditional view sharing.','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':' Because of the weak portability guarantees, you can only write to the','line_number':17,'multiline':False]
['text':' content store from a single process; we don't provide any capability','line_number':18,'multiline':False]
['text':' of "reopening" a content store to add more things to it.  But we don't','line_number':19,'multiline':False]
['text':' assume that you can keep all of the tensors you want to add to the store','line_number':20,'multiline':False]
['text':' in memory at once, because you probably can't!  Nor do we assume that','line_number':21,'multiline':False]
['text':' you know a priori whether or not two storages can be deduplicated or not.','line_number':22,'multiline':False]
['text':'','line_number':23,'multiline':False]
['text':' Note: only storages are content-addressed; tensors are name addressed','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':' Note: our padding strategy means that [1, 0] and [1] int16 tensors would','line_number':26,'multiline':False]
['text':' map to the same (padded) storage.  We think this will be immaterial for most','line_number':27,'multiline':False]
['text':' users.','line_number':28,'multiline':False]
['text':' Use of torch.compile is mandatory for (1) good memory usage','line_number':65,'multiline':False]
['text':' and (2) xor_sum implementation.  This is our first instance of','line_number':66,'multiline':False]
['text':' using PT2 to implement a kernel in PyTorch; if we get AOT capabilities','line_number':67,'multiline':False]
['text':' it would be good to apply it here.','line_number':68,'multiline':False]
['text':' The randint calls are carefully written to hit things we','line_number':71,'multiline':False]
['text':' have lowerings for in inductor.  Lack of unsigned 32-bit integer','line_number':72,'multiline':False]
['text':' is a pain.','line_number':73,'multiline':False]
['text':' This is a standard shift-multiply universal hash family','line_number':83,'multiline':False]
['text':' plus xor sum hash, using Philox to generate random numbers.','line_number':84,'multiline':False]
['text':' Our Philox RNG is not deterministic across devices so','line_number':85,'multiline':False]
['text':' don't use this for stable hashing.','line_number':86,'multiline':False]
['text':'','line_number':87,'multiline':False]
['text':' This assumes fixed length so you're also obligated to bucket','line_number':88,'multiline':False]
['text':' by the length of tensor as well','line_number':89,'multiline':False]
['text':' Returns a hex digest of the data in the storage.  Guaranteed to be','line_number':93,'multiline':False]
['text':' SHA-1 if stable_hash=True, otherwise it will consistent for a single','line_number':94,'multiline':False]
['text':' process run but not necessarily across processes.','line_number':95,'multiline':False]
['text':' TODO: make storage support buffer protocol so this isn't','line_number':103,'multiline':False]
['text':' necessary','line_number':104,'multiline':False]
['text':' TODO: factor this into a random utility','line_number':112,'multiline':False]
['text':' type: ignore[call-overload]','line_number':124,'multiline':False]
['text':' The dtype-casting view cannot be compiled, and so the','line_number':125,'multiline':False]
['text':' padding/reshaping also needs to be done externally even','line_number':126,'multiline':False]
['text':' though it could be profitably fused','line_number':127,'multiline':False]
['text':' We run the 32-bit hash five times with differing parameters to','line_number':132,'multiline':False]
['text':' reduce chance of collision','line_number':133,'multiline':False]
['text':' Structure:','line_number':142,'multiline':False]
['text':'   storages/','line_number':143,'multiline':False]
['text':'     00/','line_number':144,'multiline':False]
['text':'       0000..00','line_number':145,'multiline':False]
['text':'   tensors/','line_number':146,'multiline':False]
['text':'     name','line_number':147,'multiline':False]
['text':' TODO: offer some sort of non-blocking API to speed things up','line_number':153,'multiline':False]
['text':' TODO: consider not using torch.save for this; we don't actually','line_number':158,'multiline':False]
['text':' need any metadata for the storage','line_number':159,'multiline':False]
['text':' TODO: Support more advanced snapshotting of requires_grad/grad/etc','line_number':184,'multiline':False]
