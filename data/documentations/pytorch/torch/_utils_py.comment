['text':' Note [Don't serialize hooks]','line_number':140,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':141,'multiline':False]
['text':' Since time immemorial, we have serialized the backward hooks associated with','line_number':142,'multiline':False]
['text':' variables.  This kind of half-worked--Python can pickle global functions','line_number':143,'multiline':False]
['text':' (but not closures!)--but there were problems.','line_number':144,'multiline':False]
['text':'','line_number':145,'multiline':False]
['text':'   - It's fragile.  If you serialize a backward hook into a saved','line_number':146,'multiline':False]
['text':'     model, and then you rename the function associated with the hook,','line_number':147,'multiline':False]
['text':'     now your saved model is broken and you can't load it anymore.','line_number':148,'multiline':False]
['text':'','line_number':149,'multiline':False]
['text':'   - It's not actually used.  The standard recommendation is to','line_number':150,'multiline':False]
['text':'     serialize the *state_dict* of a model, not the model itself','line_number':151,'multiline':False]
['text':'     (since this is more stable to code changes affecting the model','line_number':152,'multiline':False]
['text':'     serialization), and the state dict saves "data" only, thus','line_number':153,'multiline':False]
['text':'     stripping the backward hooks.  In some cases, hooks are','line_number':154,'multiline':False]
['text':'     essential to the well-functioning of a model (e.g., DDP),','line_number':155,'multiline':False]
['text':'     but DDP already manages readding the hooks!','line_number':156,'multiline':False]
['text':'','line_number':157,'multiline':False]
['text':'   - We didn't serialize them in many cases.  Prior to #10220, we','line_number':158,'multiline':False]
['text':'     were dropping backward hooks in ForkingPickler.  We "fixed" this','line_number':159,'multiline':False]
['text':'     to be convenient with other serialization sites, but lack of','line_number':160,'multiline':False]
['text':'     serializing backward hooks wasn't actually the root cause of','line_number':161,'multiline':False]
['text':'     the bug.','line_number':162,'multiline':False]
['text':'','line_number':163,'multiline':False]
['text':' With these cases in mind, we have decided that a better strategy','line_number':164,'multiline':False]
['text':' is to just NOT serialize hooks at all.','line_number':165,'multiline':False]
['text':'','line_number':166,'multiline':False]
['text':' Since this is a BC-breaking change, we should warn when we previously','line_number':167,'multiline':False]
['text':' serialized a hook, but no longer do so. This will be done by adding a special','line_number':168,'multiline':False]
['text':' sentinel property to hooks will be used to suppress this warning. If a hook','line_number':169,'multiline':False]
['text':' has the property _torch_serialize_ignore, we will not emit a warning if we','line_number':170,'multiline':False]
['text':' attempt to serialize a Tensor with this hook attached to it.','line_number':171,'multiline':False]
['text':'','line_number':172,'multiline':False]
['text':' By the way, when _backward_hooks is skipped, we must give an EMPTY','line_number':173,'multiline':False]
['text':' OrderedDict(), if you pass a None you'll run afoul #12219.','line_number':174,'multiline':False]
['text':' TODO: Once we decide to break serialization FC, `storage` no longer needs to','line_number':177,'multiline':False]
['text':' be a TypedStorage','line_number':178,'multiline':False]
['text':' first construct a tensor with the correct dtype/device','line_number':180,'multiline':False]
['text':' Tensor's Metadata for serializing.','line_number':186,'multiline':False]
['text':' Currently, this only returns a dict[string, bool] specifing whether','line_number':187,'multiline':False]
['text':' `conj` or `neg` bit is set.','line_number':188,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':190,'multiline':False]
['text':' See `get_tensor_metadata` above','line_number':194,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':197,'multiline':False]
['text':' NB: This line exists only for backwards compatibility; the','line_number':208,'multiline':False]
['text':' general expectation is that backward_hooks is an empty','line_number':209,'multiline':False]
['text':' OrderedDict.  See Note [Don't serialize hooks]','line_number':210,'multiline':False]
['text':' In _legacy_load() in serialization.py we unpickle storages after the sparse','line_number':241,'multiline':False]
['text':' tensors have been already unpickled. Those storages contain data necessary for','line_number':242,'multiline':False]
['text':' validating sparse tensors: indices and values. That's why sparse tensors are','line_number':243,'multiline':False]
['text':' first unpickled without any validation, and then this function is called just','line_number':244,'multiline':False]
['text':' before _legacy_load() returns, so that all the sparse tensors can be validated','line_number':245,'multiline':False]
['text':' in bulk.','line_number':246,'multiline':False]
['text':'','line_number':247,'multiline':False]
['text':' The same procedure must be followed by _load() in serialization.py because due','line_number':248,'multiline':False]
['text':' to Pickler semantics, we have to use the same (non-validating) function for','line_number':249,'multiline':False]
['text':' unpickling sparse tensors, regardless of the caller.','line_number':250,'multiline':False]
['text':' TODO: Validation currently involves an expensive traversal','line_number':264,'multiline':False]
['text':' on CPU, which may include a device transfer.','line_number':265,'multiline':False]
['text':' For BC:','line_number':298,'multiline':False]
['text':' Should not be used, only here to be able to load Tensors serialized with older versions of pytorch','line_number':340,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':353,'multiline':False]
['text':' TODO: Once we decide to break serialization FC, `storage` no longer needs to','line_number':364,'multiline':False]
['text':' be a TypedStorage','line_number':365,'multiline':False]
['text':' NB: This line exists only for backwards compatibility; the','line_number':410,'multiline':False]
['text':' general expectation is that backward_hooks is an empty','line_number':411,'multiline':False]
['text':' OrderedDict.  See Note [Don't serialize hooks]','line_number':412,'multiline':False]
['text':' NB: This line exists only for backwards compatibility; the','line_number':419,'multiline':False]
['text':' general expectation is that backward_hooks is an empty','line_number':420,'multiline':False]
['text':' OrderedDict.  See Note [Don't serialize hooks]','line_number':421,'multiline':False]
['text':' NB: This line exists only for backwards compatibility; the','line_number':429,'multiline':False]
['text':' general expectation is that backward_hooks is an empty','line_number':430,'multiline':False]
['text':' OrderedDict.  See Note [Don't serialize hooks]','line_number':431,'multiline':False]
['text':' Restore state on Parameter like python attr.','line_number':434,'multiline':False]
['text':' Get the state of the python subclass','line_number':440,'multiline':False]
['text':' This loosely mimicks the function on the object class but since Tensor do not inherit','line_number':441,'multiline':False]
['text':' from it, we cannot call that function directly','line_number':442,'multiline':False]
['text':' https://github.com/python/cpython/blob/c83919bd635f4433f1c6ae8504996a9fe3c215e5/Objects/typeobject.c#L4891','line_number':443,'multiline':False]
['text':' Note that starting with Python 3.11, this `__getstate__` is always defined and thus','line_number':444,'multiline':False]
['text':' the else branch will never be taken.','line_number':445,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':450,'multiline':False]
['text':' Starting with Python 3.11, the __dict__ attribute is lazily created','line_number':476,'multiline':False]
['text':' and is serialized as None when not needed.','line_number':477,'multiline':False]
['text':' Taken from python 3.5 docs','line_number':496,'multiline':False]
['text':' _accumulate([1,2,3,4,5]) --> 1 3 6 10 15','line_number':499,'multiline':False]
['text':' _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120','line_number':500,'multiline':False]
['text':' annotation decorator to get annotations in a way that is compatible','line_number':650,'multiline':False]
['text':' with both Python 2 and 3','line_number':651,'multiline':False]
['text':' NOTE [ Python Traceback Reference Cycle Problem ]','line_number':674,'multiline':False]
['text':'','line_number':675,'multiline':False]
['text':' When using sys.exc_info(), it is important to **not** store the exc_info[2],','line_number':676,'multiline':False]
['text':' which is the traceback, because otherwise you will run into the traceback','line_number':677,'multiline':False]
['text':' reference cycle problem, i.e., the traceback holding reference to the frame,','line_number':678,'multiline':False]
['text':' and the frame (which holds reference to all the object in its temporary scope)','line_number':679,'multiline':False]
['text':' holding reference the traceback.','line_number':680,'multiline':False]
['text':' It is important that we don't store exc_info, see','line_number':694,'multiline':False]
['text':' NOTE [ Python Traceback Reference Cycle Problem ]','line_number':695,'multiline':False]
['text':' Format a message such as: "Caught ValueError in DataLoader worker','line_number':704,'multiline':False]
['text':' process 2. Original Traceback:", followed by the traceback.','line_number':705,'multiline':False]
['text':' KeyError calls repr() on its argument (usually a dict key). This','line_number':708,'multiline':False]
['text':' makes stack traces unreadable. It will not be changed in Python','line_number':709,'multiline':False]
['text':' (https://bugs.python.org/issue2651), so we work around it.','line_number':710,'multiline':False]
['text':' Some exceptions have first argument as non-str but explicitly','line_number':713,'multiline':False]
['text':' have message field','line_number':714,'multiline':False]
['text':' If the exception takes multiple arguments, don't try to','line_number':719,'multiline':False]
['text':' instantiate since we don't know how to','line_number':720,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':728,'multiline':False]
['text':' add more available device types here','line_number':734,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':743,'multiline':False]
['text':' add more available device types here','line_number':746,'multiline':False]
['text':' current device index','line_number':751,'multiline':False]
['text':' all device index','line_number':756,'multiline':False]
['text':' all device properties','line_number':761,'multiline':False]
['text':' The eager API _get_current_device_index uses `lambda` functions which are','line_number':805,'multiline':False]
['text':' not supported in JIT and hence not scriptable. The JIT equivalent API to get','line_number':806,'multiline':False]
['text':' the current device index is `get_current_device_index()` which can','line_number':807,'multiline':False]
['text':' be scripted. We use is_scripting to check the mode we are in and call the','line_number':808,'multiline':False]
['text':' appropriate API.','line_number':809,'multiline':False]
['text':' NOTE: torch.bool is not supported in torch.iinfo()','line_number':846,'multiline':False]
['text':' Whether we are compiling with torch.compile or not','line_number':868,'multiline':False]
['text':' This code lives in python instead of C++ since conditioning on a certain python subclass','line_number':874,'multiline':False]
['text':' is much more of a pain in C++.','line_number':875,'multiline':False]
['text':' If a FunctionalTensorMode is active while syncing, we don't want it to intercept any ops that get called','line_number':887,'multiline':False]
['text':' when we sync our inner tensor.','line_number':888,'multiline':False]
['text':' Why?','line_number':889,'multiline':False]
['text':' (1) If there are input mutations in the graph, then they will be re-applied during','line_number':890,'multiline':False]
['text':'     AOTAutograd when we call _sync() from inside of our functionalization kernels.','line_number':891,'multiline':False]
['text':' (2) _sync() causes us to regenerate our updated the tensor from the updated base,','line_number':892,'multiline':False]
['text':'     which dispatches to a bunch of view ops','line_number':893,'multiline':False]
['text':' (3) The input to these view ops is our inner FunctionalTensorWrapper','line_number':894,'multiline':False]
['text':'     (since the sync was called from C++), not the python FunctionalTensor','line_number':895,'multiline':False]
['text':' (4) if a python FunctionalTensorMode is active, it will complain when it intercepts','line_number':896,'multiline':False]
['text':'     the view op, since it will see an input that is a C++ FunctionalTensorWrapper','line_number':897,'multiline':False]
['text':'     (aka a normal torch.Tensor) instead of a python `FunctionalTensor).','line_number':898,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':903,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':908,'multiline':False]
