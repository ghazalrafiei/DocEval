['text':' flake8: noqa','line_number':5,'multiline':False]
['text':'##','line_number':7,'multiline':False]
['text':' There are generated files that depend on this file','line_number':8,'multiline':False]
['text':' To re-generate, please run from the root of the repo:','line_number':9,'multiline':False]
['text':' python torchgen/shape_functions/gen_jit_shape_functions.py','line_number':10,'multiline':False]
['text':' How to test:','line_number':12,'multiline':False]
['text':' After regenerating files, compile PyTorch.','line_number':13,'multiline':False]
['text':' Then run: ./build/bin/test_jit --gtest_filter=TestShapeGraphLinting.Basic','line_number':14,'multiline':False]
['text':' If you have enabled opinfo testing for the op, also run:','line_number':15,'multiline':False]
['text':' python test/test_ops_jit.py TestJitCPU.test_variant_consistency_jit_[FAILING_OP]_cpu_float32','line_number':16,'multiline':False]
['text':' to reproduce errors from opinfo tests.','line_number':17,'multiline':False]
['text':' Example PR: https://github.com/pytorch/pytorch/pull/80860/files','line_number':19,'multiline':False]
['text':'###','line_number':20,'multiline':False]
['text':' TODO: only assertion error is bound in C++ compilation right now','line_number':39,'multiline':False]
['text':' TODO: only assertion error is bound in C++ compilation right now','line_number':94,'multiline':False]
['text':' note: python already rounds down towards negative infinity on integer division, special arithmetic not needed','line_number':193,'multiline':False]
['text':' TODO: return self','line_number':405,'multiline':False]
['text':' We are multiplying b1 x n x m1 by x2 x m2 x p (where b1 can be a list);','line_number':606,'multiline':False]
['text':' we track m1 vs m2 separately even though they must match for nicer error messages','line_number':607,'multiline':False]
['text':' TODO: handling of slice','line_number':611,'multiline':False]
['text':' TODO: handling of slice','line_number':617,'multiline':False]
['text':' expand the batch portion (i.e. cut off matrix dimensions and expand rest)','line_number':621,'multiline':False]
['text':' todo: copy ?','line_number':624,'multiline':False]
['text':' TODO: look into rewriting with early return and getting loop unrolling to fire','line_number':678,'multiline':False]
['text':' TODO: assertions could be expanded with the error messages','line_number':698,'multiline':False]
['text':' only handling not transposed','line_number':705,'multiline':False]
['text':' this is not handling transposed convolution yet','line_number':714,'multiline':False]
['text':' Bias gradient is always generated regardess of if biases is supplied','line_number':781,'multiline':False]
['text':' TODO: return self','line_number':1029,'multiline':False]
['text':' TODO: use slicing when slice optimization has landed','line_number':1037,'multiline':False]
['text':' slice_numel = multiply_integers(input[start_dim:end_dim - start_dim + 1])','line_number':1038,'multiline':False]
['text':' This is taken shamelessly from the meta function in LossNLL.cpp','line_number':1103,'multiline':False]
['text':' Adds a shape compute function for both upper and lower bounds','line_number':1208,'multiline':False]
['text':' add_shape_compute_mapping("aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor", index_Tensor)','line_number':1437,'multiline':False]
['text':' TODO: migrate over all of symbolic_shape_registry_util.cpp','line_number':1439,'multiline':False]
['text':' These are duplicated here so that the functions will be serialiazed','line_number':1440,'multiline':False]
['text':' quantized_conv_prepack TODO','line_number':1454,'multiline':False]
['text':' Shape Compute Fn with upper and lower bounds','line_number':1456,'multiline':False]
