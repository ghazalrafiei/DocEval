['text':' Experimental module containing prototype "primitive" operations.','line_number':42,'multiline':False]
['text':'','line_number':45,'multiline':False]
['text':' Common datastructures and helpers','line_number':46,'multiline':False]
['text':'','line_number':47,'multiline':False]
['text':'','line_number':49,'multiline':False]
['text':' Elementwise unary prims','line_number':50,'multiline':False]
['text':'','line_number':51,'multiline':False]
['text':'','line_number':102,'multiline':False]
['text':' Elementwise binary prims','line_number':103,'multiline':False]
['text':'','line_number':104,'multiline':False]
['text':' 'complex',  # needs custom meta','line_number':110,'multiline':False]
['text':' not implemented','line_number':134,'multiline':False]
['text':'','line_number':137,'multiline':False]
['text':' View prims','line_number':138,'multiline':False]
['text':'','line_number':139,'multiline':False]
['text':' implemented using slice -- make this a ref?','line_number':146,'multiline':False]
['text':'','line_number':152,'multiline':False]
['text':' Functionalized view mutations','line_number':153,'multiline':False]
['text':'','line_number':154,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':' Shape prims','line_number':157,'multiline':False]
['text':'','line_number':158,'multiline':False]
['text':'','line_number':163,'multiline':False]
['text':' Conditional prims','line_number':164,'multiline':False]
['text':'','line_number':165,'multiline':False]
['text':'','line_number':167,'multiline':False]
['text':' Data conversion and movement prims','line_number':168,'multiline':False]
['text':'','line_number':169,'multiline':False]
['text':'','line_number':177,'multiline':False]
['text':' Inplace prims','line_number':178,'multiline':False]
['text':'','line_number':179,'multiline':False]
['text':' "_set",  # Commented out, see note below','line_number':182,'multiline':False]
['text':'','line_number':183,'multiline':False]
['text':' Reduction prims','line_number':184,'multiline':False]
['text':'','line_number':185,'multiline':False]
['text':'','line_number':192,'multiline':False]
['text':' Tensor Creation Prims','line_number':193,'multiline':False]
['text':'','line_number':194,'multiline':False]
['text':'','line_number':199,'multiline':False]
['text':' Linear algebra (linalg) Prims','line_number':200,'multiline':False]
['text':'','line_number':201,'multiline':False]
['text':'','line_number':203,'multiline':False]
['text':' Randomness Prims','line_number':204,'multiline':False]
['text':'','line_number':205,'multiline':False]
['text':'','line_number':208,'multiline':False]
['text':' FFT prims','line_number':209,'multiline':False]
['text':'','line_number':210,'multiline':False]
['text':' TODO: This looks wrong, a number that is wrapped into a tensor','line_number':232,'multiline':False]
['text':' needs to behave differently than a scalar tensor for type','line_number':233,'multiline':False]
['text':' promotion purposes','line_number':234,'multiline':False]
['text':' If no tensorlike "example" is given then all metadata','line_number':242,'multiline':False]
['text':' must be provided explicitly','line_number':243,'multiline':False]
['text':' always run the meta function because aten implementation will','line_number':277,'multiline':False]
['text':' typically accept more inputs (e.g., it will do promotion and','line_number':278,'multiline':False]
['text':' broadcasting) which we want to reject','line_number':279,'multiline':False]
['text':' Right now prims don't support autograd (we can and should add an','line_number':283,'multiline':False]
['text':' argument that provides an implementation for backward here.)  Because we','line_number':284,'multiline':False]
['text':' don't have derivative formulas, we must setup a custom autograd function','line_number':285,'multiline':False]
['text':' that raises an error if backwards is invoked','line_number':286,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/103532','line_number':313,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':320,'multiline':False]
['text':' TODO: implement dtype validation here, too, or on the corresponding refs','line_number':337,'multiline':False]
['text':' Acquires the dtype','line_number':364,'multiline':False]
['text':' Acquires the device (if it exists) or number','line_number':380,'multiline':False]
['text':' keep going, in case there is a cuda tensor later','line_number':388,'multiline':False]
['text':' NOTE: type promotion behavior here is mostly hidden from tests because','line_number':397,'multiline':False]
['text':' references will typically handle the type promotion properly even if this doesn't','line_number':398,'multiline':False]
['text':' (but getting it wrong will cause too many casts to be inserted in traces!)','line_number':399,'multiline':False]
['text':' type: ignore[return-value]','line_number':416,'multiline':False]
['text':' Number case','line_number':418,'multiline':False]
['text':' TODO: fix number type promotion (bool, complex->float)','line_number':419,'multiline':False]
['text':' For now for symint/float, just implementing the common / simple cases of (int,float,symint,symfloat)','line_number':421,'multiline':False]
['text':' type: ignore[arg-type]','line_number':430,'multiline':False]
['text':'','line_number':474,'multiline':False]
['text':' Elementwise unary operations','line_number':475,'multiline':False]
['text':'','line_number':476,'multiline':False]
['text':' Returns the real cubic root of the number.','line_number':597,'multiline':False]
['text':' Note that if a < 0, pow(a, (1. / 3.)) returns th complex number','line_number':598,'multiline':False]
['text':' exp(1/3 * log(a)) = exp(1/3 * (log(abs(a)) + pi*i)) = cbrt(abs(a)) * e^{pi/3*i}','line_number':599,'multiline':False]
['text':' which is a complex number.','line_number':600,'multiline':False]
['text':' For more info see the section Note in','line_number':601,'multiline':False]
['text':' https://en.cppreference.com/w/cpp/numeric/math/cbrt','line_number':602,'multiline':False]
['text':' memory_format == torch.preserve_format','line_number':650,'multiline':False]
['text':' NOTE: fill uses _make_prim directly because it has a value parameter','line_number':732,'multiline':False]
['text':'','line_number':910,'multiline':False]
['text':' Elementwise binary operations','line_number':911,'multiline':False]
['text':'','line_number':912,'multiline':False]
['text':' TODO: complex needs a special meta to account for its float -> complex behavior','line_number':949,'multiline':False]
['text':' complex = _make_elementwise_binary_prim(','line_number':950,'multiline':False]
['text':'   impl_aten=torch.complex,','line_number':951,'multiline':False]
['text':'   doc="",','line_number':952,'multiline':False]
['text':' )','line_number':953,'multiline':False]
['text':' div prim performs truncation division on integer inputs','line_number':956,'multiline':False]
['text':'   and true division for floating and complex inputs','line_number':957,'multiline':False]
['text':' Note: the following impls are because torch.maximum and torch.minimum do not support scalar inputs','line_number':1063,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1072,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1091,'multiline':False]
['text':'','line_number':1168,'multiline':False]
['text':' View operations','line_number':1169,'multiline':False]
['text':' NOTE: This special case is to avoid having to acquire the storage below','line_number':1179,'multiline':False]
['text':' as_strided to shapes with no elements are trivially valid, so it's OK','line_number':1180,'multiline':False]
['text':' Type checks','line_number':1213,'multiline':False]
['text':' every dimension must be accounted for','line_number':1218,'multiline':False]
['text':' broadcast shape must have weakly more dimensions','line_number':1221,'multiline':False]
['text':' broadcast_dimensions must be an ascending sequence','line_number':1224,'multiline':False]
['text':' (no relative reordering of dims) of integers and','line_number':1225,'multiline':False]
['text':' each dimension must be within the new shape','line_number':1226,'multiline':False]
['text':' shape must be broadcastable to','line_number':1236,'multiline':False]
['text':' Assigns a stride of zero to dimensions','line_number':1244,'multiline':False]
['text':' which were actually broadcast','line_number':1245,'multiline':False]
['text':' Special-case for zero dimensional tensors','line_number':1296,'multiline':False]
['text':' Verifies end is strictly greater than start','line_number':1301,'multiline':False]
['text':' (Collapse requires a non-empty interval)','line_number':1302,'multiline':False]
['text':' Special-case for zero dimensional tensors','line_number':1313,'multiline':False]
['text':' Special-case for zero dimensional tensors','line_number':1330,'multiline':False]
['text':' type: ignore[assignment]','line_number':1335,'multiline':False]
['text':' type: ignore[assignment]','line_number':1336,'multiline':False]
['text':' NOTE: when the input has no elements it's restrided as if it were contiguous','line_number':1365,'multiline':False]
['text':' TODO: this is only here to support the unsqueeze ref','line_number':1443,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1444,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1446,'multiline':False]
['text':' Note: saves the Python slice object because we're about to clobber its name with the slice prim','line_number':1461,'multiline':False]
['text':' type: ignore[has-type]','line_number':1462,'multiline':False]
['text':' type: ignore[call-overload]','line_number':1540,'multiline':False]
['text':' TODO: make stride SymInt','line_number':1631,'multiline':False]
['text':' Verifies the dim can be split with the specified lhs_length','line_number':1646,'multiline':False]
['text':' TODO: consider renaming split_dim_view','line_number':1682,'multiline':False]
['text':' Note: allows dimensions to be specified redundantly','line_number':1692,'multiline':False]
['text':'','line_number':1809,'multiline':False]
['text':' Functionalized view mutations','line_number':1810,'multiline':False]
['text':'','line_number':1811,'multiline':False]
['text':'','line_number':1856,'multiline':False]
['text':' Shape operations','line_number':1857,'multiline':False]
['text':'','line_number':1858,'multiline':False]
['text':' Special-case for zero dimensional tensors','line_number':1862,'multiline':False]
['text':' TODO: review stride logic','line_number':1890,'multiline':False]
['text':' Verifies same shape (except in the concat dimension)','line_number':1892,'multiline':False]
['text':' Validates the tensor and the requested shape have the','line_number':1939,'multiline':False]
['text':' same number of elements','line_number':1940,'multiline':False]
['text':'','line_number':1983,'multiline':False]
['text':' Conditional prims','line_number':1984,'multiline':False]
['text':'','line_number':1985,'multiline':False]
['text':'','line_number':2015,'multiline':False]
['text':' Type conversions','line_number':2016,'multiline':False]
['text':'','line_number':2017,'multiline':False]
['text':' Type checks','line_number':2019,'multiline':False]
['text':' dtype conversion preserves dense strides','line_number':2023,'multiline':False]
['text':' Propagates requires grad when possible','line_number':2033,'multiline':False]
['text':' TODO: update meta objects so this can be acquired directly','line_number':2037,'multiline':False]
['text':' NOTE: need to model meta scalars','line_number':2090,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/78070','line_number':2091,'multiline':False]
['text':' TODO: create a new return type for scalars?','line_number':2101,'multiline':False]
['text':' FIXME: currently returns integers for boolean tensors','line_number':2102,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/78071','line_number':2103,'multiline':False]
['text':' NOTE: need to model meta scalars','line_number':2113,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/78070','line_number':2114,'multiline':False]
['text':' TODO: create a new return type for scalars?','line_number':2133,'multiline':False]
['text':' FIXME: currently returns integers for boolean tensors','line_number':2134,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/78071','line_number':2135,'multiline':False]
['text':' NOTE: need to model meta scalars','line_number':2145,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/78070','line_number':2146,'multiline':False]
['text':' TODO: create a new return type for scalars?','line_number':2165,'multiline':False]
['text':' FIXME: currently returns integers for boolean tensors','line_number':2166,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/78071','line_number':2167,'multiline':False]
['text':'','line_number':2176,'multiline':False]
['text':' Inplace operators','line_number':2177,'multiline':False]
['text':'','line_number':2178,'multiline':False]
['text':' Validates the cast is safe','line_number':2185,'multiline':False]
['text':' TODO: move this as an option on the reference','line_number':2186,'multiline':False]
['text':' a_typ = utils.dtype_to_type(a.dtype)','line_number':2187,'multiline':False]
['text':' b_typ = utils.dtype_to_type(b.dtype)','line_number':2188,'multiline':False]
['text':' if a_typ is not utils.get_higher_type(a_typ, b_typ):','line_number':2189,'multiline':False]
['text':'     raise RuntimeError(str(b.dtype), " can't be cast safely to ", str(a.dtype), "!")','line_number':2190,'multiline':False]
['text':' Validates the tensors have the same number of elements','line_number':2192,'multiline':False]
['text':' TODO: Remove safe casting and implement on reference instead','line_number':2208,'multiline':False]
['text':' TODO: review support arbitrary resizes','line_number':2271,'multiline':False]
['text':' TODO: layout, pin_memory, memory_format','line_number':2421,'multiline':False]
['text':' TODO: model requires_grad on TensorMeta','line_number':2422,'multiline':False]
['text':' noqa: B950','line_number':2461,'multiline':False]
['text':' TODO: layout, pin_memory, memory_format','line_number':2469,'multiline':False]
['text':' TODO: model requires_grad on TensorMeta','line_number':2470,'multiline':False]
['text':' TODO: add layout, pin_memory','line_number':2512,'multiline':False]
['text':' TODO: add layout, pin_memory','line_number':2567,'multiline':False]
['text':' noqa: B950','line_number':2569,'multiline':False]
['text':' Note that Mypy thinks torch.full can't accept a complex fill_value','line_number':2597,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2599,'multiline':False]
['text':' TODO: add layout','line_number':2607,'multiline':False]
['text':' Note that Mypy thinks torch.full can't accept a complex fill_value','line_number':2640,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2642,'multiline':False]
['text':' Note that Mypy thinks torch.scalar can't accept a complex scalar','line_number':2682,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2683,'multiline':False]
['text':' TODO: add layout and pin_memory support','line_number':2690,'multiline':False]
['text':'','line_number':2700,'multiline':False]
['text':' Linear algebra (linalg) prims','line_number':2701,'multiline':False]
['text':'','line_number':2702,'multiline':False]
['text':' The CPU backend returns V, but the cuSolver backend returns V^H','line_number':2730,'multiline':False]
['text':' TODO The MAGMA backend returns V, so this is wrong if used with the MAGMA backend','line_number':2731,'multiline':False]
['text':' Also makes sure this is CUDA or HIP:','line_number':2735,'multiline':False]
['text':' https://pytorch.org/docs/stable/notes/hip.html#checking-for-hip','line_number':2736,'multiline':False]
['text':'','line_number':2763,'multiline':False]
['text':' Randomness Prims','line_number':2764,'multiline':False]
['text':'','line_number':2765,'multiline':False]
['text':' NOTE: normal_ is incorrectly annotated to expect mean to be a float','line_number':2804,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2805,'multiline':False]
['text':' noqa: B950','line_number':2818,'multiline':False]
['text':' TODO: we should more seriously review randomness modeling and prims','line_number':2858,'multiline':False]
['text':'','line_number':2869,'multiline':False]
['text':' FFT prims','line_number':2870,'multiline':False]
['text':'','line_number':2871,'multiline':False]
['text':' No normalization','line_number':2899,'multiline':False]
['text':' No normalization','line_number':2939,'multiline':False]
['text':' No normalization','line_number':2979,'multiline':False]
