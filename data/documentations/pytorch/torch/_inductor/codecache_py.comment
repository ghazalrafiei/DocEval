['text':' timing metrics for time spent in the compilation','line_number':91,'multiline':False]
['text':' print("CUMULATIVE COMPILE TIME", _cumulative_compile_time)','line_number':108,'multiline':False]
['text':' If cuda is not installed, none of the above config is relevant.','line_number':157,'multiline':False]
['text':' cache hit','line_number':272,'multiline':False]
['text':' cache miss','line_number':275,'multiline':False]
['text':' check local cache first since it is data specific to the current machine','line_number':284,'multiline':False]
['text':' re-benchmark everything to try to get consistent numbers from the same machine','line_number':290,'multiline':False]
['text':' catch and log autotuning failures','line_number':298,'multiline':False]
['text':' only check global cache, not local one','line_number':309,'multiline':False]
['text':' may have a partial cache hit, where not everything is benchmarked','line_number':311,'multiline':False]
['text':' [:51] to strip off the "Q====" suffix common to every hash value.','line_number':324,'multiline':False]
['text':' use striped content to compute hash so we don't end up with different','line_number':364,'multiline':False]
['text':' hashes just because the content begins/ends with differnet number of','line_number':365,'multiline':False]
['text':' spaces.','line_number':366,'multiline':False]
['text':' Write into temporary file first to avoid conflicts between threads','line_number':377,'multiline':False]
['text':' Avoid using a named temporary file, as those have restricted permissions','line_number':378,'multiline':False]
['text':' If we see tensors, we know they're contstants stored as attributes on','line_number':465,'multiline':False]
['text':' the GraphModule. See tensor lowering; small constants are inlined. If','line_number':466,'multiline':False]
['text':' we see a small tensor, therefore, no reference will ultimately remain','line_number':467,'multiline':False]
['text':' in the generated code. So we need to include its value in the cache key.','line_number':468,'multiline':False]
['text':' Large constannts are effectively treated as inputs and we consider only','line_number':469,'multiline':False]
['text':' their metadata.','line_number':470,'multiline':False]
['text':' For hashing purposes, we only care about the name of the symbol and','line_number':482,'multiline':False]
['text':' not the backed value. We evaluate guards stored with a cached graph','line_number':483,'multiline':False]
['text':' to ensure a cached entity with SymInt args is safe to reuse.','line_number':484,'multiline':False]
['text':' Excluded kwargs param that are not stable between runs','line_number':557,'multiline':False]
['text':' Order kwargs so hashing is stable to changes in kwarg order.','line_number':569,'multiline':False]
['text':' Special case to handle set params. Python sets can't be','line_number':574,'multiline':False]
['text':' ordered, so sort the elements and store them in a proxy.','line_number':575,'multiline':False]
['text':' Also hash on various system info (including the triton compiler version), as','line_number':580,'multiline':False]
['text':' well as the inductor configuration and code.','line_number':581,'multiline':False]
['text':' The prefix distinguishes among the other kinds of objects we','line_number':628,'multiline':False]
['text':' cache in this module.','line_number':629,'multiline':False]
['text':' TODO(masnesral): Investigate whether it's beneficial to store compiled graphs','line_number':663,'multiline':False]
['text':' in an in-memory cache after loading from disk.','line_number':664,'multiline':False]
['text':' Iterate over any entries in the subdir for this key and evaluate','line_number':706,'multiline':False]
['text':' their guards to determine whether there's a hit.','line_number':707,'multiline':False]
['text':' No guards to evaluate','line_number':714,'multiline':False]
['text':' Evaluate the guard expression in the current context.','line_number':717,'multiline':False]
['text':' If there's not a cache hit, we don't want the evaluation to','line_number':721,'multiline':False]
['text':' affect the current env, e.g., cause the creation of new guards,','line_number':722,'multiline':False]
['text':' so we evaluate with the hints instead of the symbols.','line_number':723,'multiline':False]
['text':' Now re-evaluate with the symints to add any guards to the current env.','line_number':735,'multiline':False]
['text':' Important as compiled models are not pickleable:','line_number':755,'multiline':False]
['text':' Before serializing, compute the guard expression that will be used to','line_number':758,'multiline':False]
['text':' ensure that a CompiledFxGraph is valid when loaded from the cache. It's','line_number':759,'multiline':False]
['text':' sufficient to consider only the SymInt args to the fx graph since the','line_number':760,'multiline':False]
['text':' Tensor shapes are already captured in the hash for the cache key. Any','line_number':761,'multiline':False]
['text':' Tensor arg with a symbolic shape will have a SymInt arg for the graph.','line_number':762,'multiline':False]
['text':' Use a hash of the serialized CompiledFxGraph to get a unique file','line_number':773,'multiline':False]
['text':' name. The specific name doesn't matter since a lookup involves','line_number':774,'multiline':False]
['text':' iterating over all entries in the parent subdir.','line_number':775,'multiline':False]
['text':' This is a string representation of an expression we serialize','line_number':835,'multiline':False]
['text':' with the object so the guards can be evaluated in a different','line_number':836,'multiline':False]
['text':' context in order to verify the validity of serving a cached','line_number':837,'multiline':False]
['text':' fx graph. The expression must be generated by:','line_number':838,'multiline':False]
['text':' ShapeEnv.produce_guards_expression()','line_number':839,'multiline':False]
['text':' This prevents a circular reference that makes CompiledFxGraph','line_number':867,'multiline':False]
['text':' get stuck without getting garbage collected','line_number':868,'multiline':False]
['text':' We can't really serialize callables that may be C++/Triton/etc.,','line_number':875,'multiline':False]
['text':' so we serialize their disk cache location instead','line_number':876,'multiline':False]
['text':' TODO: When making an API that can save compiled models e2e to disk','line_number':877,'multiline':False]
['text':' this will need to be better','line_number':878,'multiline':False]
['text':' gxx package is only available for Linux','line_number':909,'multiline':False]
['text':' according to https://anaconda.org/conda-forge/gxx/','line_number':910,'multiline':False]
['text':' Do not install GXX by default','line_number':913,'multiline':False]
['text':' Note [Checking for Vectorized Support in Inductor]','line_number':978,'multiline':False]
['text':' TorchInductor CPU vectorization reuses PyTorch vectorization utility functions','line_number':979,'multiline':False]
['text':' Hence, TorchInductor would depend on Sleef* to accelerate mathematical functions','line_number':980,'multiline':False]
['text':' like exp, pow, sin, cos and etc.','line_number':981,'multiline':False]
['text':' But PyTorch and TorchInductor might use different compilers to build code. If','line_number':982,'multiline':False]
['text':' PyTorch uses gcc-7/g++-7 to build the release package, the libtorch_cpu.so','line_number':983,'multiline':False]
['text':' will not expose the Sleef* AVX512 symbols since gcc-7/g++-7 cannot pass','line_number':984,'multiline':False]
['text':' avx512 check in CMake - FindAVX.cmake. But TorchInductor install the latest','line_number':985,'multiline':False]
['text':' gcc/g++ compiler by default while it could support the AVX512 compilation.','line_number':986,'multiline':False]
['text':' Therefore, there would be a conflict sleef version between PyTorch and','line_number':987,'multiline':False]
['text':' TorchInductor. Hence, we dry-compile the following code to check whether current','line_number':988,'multiline':False]
['text':' HW platform and PyTorch both could support AVX512 or AVX2. And suppose ARM','line_number':989,'multiline':False]
['text':' also needs the logic','line_number':990,'multiline':False]
['text':' In fbcode however, we are using the same compiler for pytorch and for inductor codegen,','line_number':991,'multiline':False]
['text':' making the runtime check unnecessary.','line_number':992,'multiline':False]
['text':' Check build result','line_number':1050,'multiline':False]
['text':' type: ignore[override]','line_number':1115,'multiline':False]
['text':' Cache the cpuinfo to avoid I/O overhead. Meanwhile, the cpuinfo content','line_number':1125,'multiline':False]
['text':' might have too much redundant content that is useless for ISA check. Hence,','line_number':1126,'multiline':False]
['text':' we only cache some key isa information.','line_number':1127,'multiline':False]
['text':' If the simdlen is None, it indicates determin the vectorization length automatically','line_number':1153,'multiline':False]
['text':' FIXME: passing `-fopenmp` adds libgomp.so to the generated shared library's dependencies.','line_number':1199,'multiline':False]
['text':' This causes `ldopen` to fail in fbcode, because libgomp does not exist in the default paths.','line_number':1200,'multiline':False]
['text':' We will fix it later by exposing the lib path.','line_number':1201,'multiline':False]
['text':' Per https://mac.r-project.org/openmp/ right way to pass `openmp` flags to MacOS is via `-Xclang`','line_number':1205,'multiline':False]
['text':' Also, `-march=native` is unrecognized option on M1','line_number':1206,'multiline':False]
['text':' Internal cannot find libgomp.so','line_number':1214,'multiline':False]
['text':' check if `brew` is installed','line_number':1259,'multiline':False]
['text':' get the location of `libomp` if it is installed','line_number':1261,'multiline':False]
['text':' this is the location that `libomp` **would** be installed','line_number':1262,'multiline':False]
['text':' see https://github.com/Homebrew/brew/issues/10261#issuecomment-756563567 for details','line_number':1263,'multiline':False]
['text':' check if `libomp` is installed','line_number':1269,'multiline':False]
['text':' Note - We include pytorch only on linux right now. There is more work','line_number':1298,'multiline':False]
['text':' to do to enable OMP build on darwin where PyTorch is built with IOMP','line_number':1299,'multiline':False]
['text':' and we need a way to link to what PyTorch links.','line_number':1300,'multiline':False]
['text':' No need to manually specify libraries in fbcode.','line_number':1308,'multiline':False]
['text':' internal remote execution is able to find omp, but not gomp','line_number':1315,'multiline':False]
['text':' This is a special treatment for Meta internal cuda-12 where all libs','line_number':1320,'multiline':False]
['text':' are in lib/cuda-12 and lib/cuda-12/stubs','line_number':1321,'multiline':False]
['text':' Note - this is effectively a header only inclusion. Usage of some header files may result in','line_number':1359,'multiline':False]
['text':' symbol not found, if those header files require a library.','line_number':1360,'multiline':False]
['text':' For those cases, include the lpath and libs command as we do for pytorch above.','line_number':1361,'multiline':False]
['text':' This approach allows us to only pay for what we use.','line_number':1362,'multiline':False]
['text':' only Apple builtin compilers (Apple Clang++) require openmp','line_number':1368,'multiline':False]
['text':' check the `OMP_PREFIX` environment first','line_number':1371,'multiline':False]
['text':' prefer to use openmp from `conda install llvm-openmp`','line_number':1384,'multiline':False]
['text':' Prefer Intel OpenMP on x86 machine','line_number':1391,'multiline':False]
['text':' next, try to use openmp from `brew install libomp`','line_number':1397,'multiline':False]
['text':' if openmp is still not available, we let the compiler to have a try,','line_number':1404,'multiline':False]
['text':' and raise error together with instructions at compilation error later','line_number':1405,'multiline':False]
['text':' Unconditionally import c10 for non-abi-compatible mode to use TORCH_CHECK - See PyTorch #108690','line_number':1409,'multiline':False]
['text':' third party libs','line_number':1414,'multiline':False]
['text':' We also need to bundle includes with absolute paths into a remote directory','line_number':1425,'multiline':False]
['text':' (later on, we copy the include paths from cpp_extensions into our remote dir)','line_number':1426,'multiline':False]
['text':' For Meta internal cuda-12, it is recommended to static link cudart','line_number':1431,'multiline':False]
['text':' We need to copy any absolute-path torch includes','line_number':1463,'multiline':False]
['text':' Use clang runtime instead of libgcc','line_number':1467,'multiline':False]
['text':' let the compiler pick','line_number':1474,'multiline':False]
['text':' Meta internal AOTInductor CPU','line_number':1559,'multiline':False]
['text':' Currently, this only support serializing extern nodes in fbcode','line_number':1591,'multiline':False]
['text':' Eventually, we should also have a serializer for OSS.','line_number':1592,'multiline':False]
['text':' This serializes the tensor's untyped_storage to bytes by accessing','line_number':1623,'multiline':False]
['text':' the raw data of the underlying structure.','line_number':1624,'multiline':False]
['text':' Putting this fn in cpp.py (unfortunately) causes a deadlock, which is why it's in codecache.py.','line_number':1715,'multiline':False]
['text':' Why? importing from cpp.py invokes codecache.pick_vec_isa(), which takes out a lock.','line_number':1716,'multiline':False]
['text':' Cycle goes:','line_number':1717,'multiline':False]
['text':' - CppCodeCache.load()','line_number':1718,'multiline':False]
['text':' - pick_vec_isa()','line_number':1719,'multiline':False]
['text':' - valid_vec_isa_list()','line_number':1720,'multiline':False]
['text':' - VecISA.__bool__() <-- takes out a lock','line_number':1721,'multiline':False]
['text':' - compile_file() <-- imports cpp_prefix_path from cpp, which causes us to try to take out the same lock.','line_number':1722,'multiline':False]
['text':' We need relative paths, since we bundle up','line_number':1738,'multiline':False]
['text':' everything that we compile into a folder for remote compilation.','line_number':1739,'multiline':False]
['text':' Given a path to an input cpp file and an output path,','line_number':1745,'multiline':False]
['text':' Attempts to compile the file, storing the output in "output_path"','line_number':1746,'multiline':False]
['text':' Need to copy our header into the same folder as the sourcecode.','line_number':1756,'multiline':False]
['text':' When we build remotely, we need to make sure to carefully copy any files','line_number':1760,'multiline':False]
['text':' that are required during the compilation process into our build directly.','line_number':1761,'multiline':False]
['text':' This is where all of the ATen/c10/Torch includes come from.','line_number':1762,'multiline':False]
['text':' Copy everything to tmp compilation folder','line_number':1767,'multiline':False]
['text':' Run the build','line_number':1773,'multiline':False]
['text':' Copy output from the build','line_number':1775,'multiline':False]
['text':' hacky workaround for fbcode/buck','line_number':1811,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1844,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1889,'multiline':False]
['text':' another thread might set this first','line_number':1892,'multiline':False]
['text':' unzip into separate lines/nodes lists','line_number':1894,'multiline':False]
['text':' [(starting_line, <fx node>), ...]','line_number':1910,'multiline':False]
['text':' ideally fx stores stack traces as data rather than a string','line_number':1920,'multiline':False]
['text':' but this is not along a performance critical path','line_number':1921,'multiline':False]
['text':' For CPP wrapper, add -ffast-math during linking to make CPU flush denormals.','line_number':1976,'multiline':False]
['text':' CPP wrapper leverages cpp_extension which will do the compilation and linking in two stages.','line_number':1977,'multiline':False]
['text':' We need to explicitly add -ffast-math as a linking flag.','line_number':1978,'multiline':False]
['text':' For the default python wrapper, the compilation and linking are done in one command thus -ffast-math','line_number':1979,'multiline':False]
['text':' will take effect in both compilation and linking.','line_number':1980,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1996,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1998,'multiline':False]
['text':' 64-bit CUDA may be installed in "lib"','line_number':2044,'multiline':False]
['text':' Note that it's also possible both don't exist (see _find_cuda_home) - in that case we stay with "lib64"','line_number':2045,'multiline':False]
['text':' Required by cutlass compilation.','line_number':2072,'multiline':False]
['text':' Keep the intermediate files for debugging (including ptx, sass, cubin etc.)','line_number':2091,'multiline':False]
['text':' warn us if local memory is used in CUDA Kernels','line_number':2092,'multiline':False]
['text':' warn us if register spilling happens in CUDA Kernels','line_number':2093,'multiline':False]
['text':' Report on CUDA resource usage (shared mem, registers etc.)','line_number':2094,'multiline':False]
['text':' Annotate the ptx file with source information','line_number':2097,'multiline':False]
['text':' Apline Linux','line_number':2161,'multiline':False]
['text':' @dynamo_utils.dynamo_timed','line_number':2306,'multiline':False]
['text':' If the worker failed this will throw an exception.','line_number':2311,'multiline':False]
['text':' If this process dies abnormally (e.g. segfault)','line_number':2324,'multiline':False]
['text':' it will not shut down the workers. Instead','line_number':2325,'multiline':False]
['text':' the workers will have their parent reassigned to the','line_number':2326,'multiline':False]
['text':' init process. This launches a separate thread to','line_number':2327,'multiline':False]
['text':' watch for the worker getting reassigned,','line_number':2328,'multiline':False]
['text':' and cleans it up in this case.','line_number':2329,'multiline':False]
['text':'','line_number':2330,'multiline':False]
['text':' This function cannot be an inner function since otherwise mp_context="spawn" would','line_number':2331,'multiline':False]
['text':' not work for ProcessPoolExecutor since inner functions cannot be pickled.','line_number':2332,'multiline':False]
['text':' ensure properties have been calculated before processes','line_number':2361,'multiline':False]
['text':' are forked','line_number':2362,'multiline':False]
['text':' when this pool is created in a subprocess object, the normal exit handler','line_number':2373,'multiline':False]
['text':' doesn't run, and we need to register our own handler.','line_number':2374,'multiline':False]
['text':' exitpriority has to be high, because another one of the finalizers will','line_number':2375,'multiline':False]
['text':' kill the worker thread that sends the shutdown message to the workers...','line_number':2376,'multiline':False]
['text':' We have to fork processes for compiler workers, but the more memory and other resources that are loaded, the','line_number':2387,'multiline':False]
['text':' slower the os.fork time is, quite drastically. It also holds the GIL so we can't put it on another thread.','line_number':2388,'multiline':False]
['text':' Examples:','line_number':2390,'multiline':False]
['text':' A simple x + x + x script: 10ms seconds in the middle of the program, 2ms at startup','line_number':2391,'multiline':False]
['text':' tf_efficientnet_b0 benchmark: 50ms! in the middle of the program , 3ms at startup','line_number':2392,'multiline':False]
['text':' So we want to start the workers early when it is still cheap, and also to allow the workers to get','line_number':2394,'multiline':False]
['text':' ready before we have work for them.','line_number':2395,'multiline':False]
['text':' ProcessPoolExecutor also does not launch the workers until it finds a point when all the workers are idle.','line_number':2397,'multiline':False]
['text':' But if we waited until then fork time will be long and we will be waiting for the processes to initialize.','line_number':2398,'multiline':False]
['text':' We force them to start here with some YOLOing of the internal methods.','line_number':2400,'multiline':False]
