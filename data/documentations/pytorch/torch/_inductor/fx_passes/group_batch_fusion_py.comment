['text':' importing this will register fbgemm lowerings for inductor','line_number':18,'multiline':False]
['text':' noqa: F401','line_number':19,'multiline':False]
['text':' The maximum tensor size that can go into the fusion group','line_number':33,'multiline':False]
['text':' exclude these nodes from BFS','line_number':36,'multiline':False]
['text':' excluding get item improves optimizer compilation time by 60s','line_number':37,'multiline':False]
['text':' only handle the cases where inputs are 2D tensors','line_number':152,'multiline':False]
['text':' type: ignore[assignment]','line_number':263,'multiline':False]
['text':' note: we only consider the case where the inputs are tensors','line_number':291,'multiline':False]
['text':' for mixed precision training, we need to make sure the inputs','line_number':292,'multiline':False]
['text':' of the aten.cat when do the stack should be the same dtype','line_number':293,'multiline':False]
['text':' otherwise, the output of the aten.cat may be not the same as','line_number':294,'multiline':False]
['text':' its inputs, and cause dtype not same error in mm or addmm','line_number':295,'multiline':False]
['text':' type: ignore[assignment]','line_number':580,'multiline':False]
['text':' type: ignore[assignment]','line_number':583,'multiline':False]
['text':' for relu op, we also use the inplace to construct the key','line_number':654,'multiline':False]
['text':' noqa: G004','line_number':850,'multiline':False]
['text':' type: ignore[operator]','line_number':860,'multiline':False]
['text':' we keep all current pre grad fusions to keep','line_number':867,'multiline':False]
['text':' current implementation, will remove this later','line_number':868,'multiline':False]
