['text':' x_scale','line_number':114,'multiline':False]
['text':' x_zp','line_number':115,'multiline':False]
['text':' packed_weight','line_number':116,'multiline':False]
['text':' w_scale','line_number':117,'multiline':False]
['text':' w_zp','line_number':118,'multiline':False]
['text':' bias','line_number':119,'multiline':False]
['text':' inv_output_scale = 1.0','line_number':124,'multiline':False]
['text':' output_zero_point = 0','line_number':125,'multiline':False]
['text':' output_dtype = None','line_number':126,'multiline':False]
['text':' attr = "none"','line_number':127,'multiline':False]
['text':' scalars','line_number':128,'multiline':False]
['text':' algorithm','line_number':129,'multiline':False]
['text':' Only keep matched pattern with same output_dtype','line_number':252,'multiline':False]
['text':' Activation QParams','line_number':278,'multiline':False]
['text':' Weight QParams','line_number':284,'multiline':False]
['text':' Conv Params','line_number':290,'multiline':False]
['text':' Output QParams','line_number':299,'multiline':False]
['text':' Expected int8-in fp32-out qconv in weight prepack phase','line_number':304,'multiline':False]
['text':' Expected no post op fused in weight prepack phase','line_number':307,'multiline':False]
['text':' Only keep matched pattern with same output_dtype','line_number':342,'multiline':False]
['text':' Activation QParams','line_number':368,'multiline':False]
['text':' Weight QParams','line_number':374,'multiline':False]
['text':' bias','line_number':381,'multiline':False]
['text':' Output QParams','line_number':384,'multiline':False]
['text':' Expected int8-in fp32/bf16-out qlinear in weight prepack phase','line_number':389,'multiline':False]
['text':' Expected no post op fused in weight prepack phase','line_number':392,'multiline':False]
['text':' Check if it's a valid Conv Binary Pattern:','line_number':417,'multiline':False]
['text':' * qconv2d_pointwise should only has one users','line_number':418,'multiline':False]
['text':' * Extra input of binary node comes from dequant pattern','line_number':419,'multiline':False]
['text':' Output QParams','line_number':477,'multiline':False]
['text':' QConv2d','line_number':520,'multiline':False]
['text':' Priority 1 to match: QConv2d Unary pattern with int8 output','line_number':521,'multiline':False]
['text':' If a pattern1 is a sub-set of pattern2, we should try to match pattern2 firstly.','line_number':522,'multiline':False]
['text':' For example: pattern1 is qconv_fp32 -> relu, pattern2 is qconv_fp32 -> relu -> quant','line_number':523,'multiline':False]
['text':' Register qconv2d pattern for ExternKernel Lowering','line_number':544,'multiline':False]
['text':' pass_number','line_number':547,'multiline':False]
['text':' computation_op','line_number':548,'multiline':False]
['text':' output_dtype, None is the default value for int8 output','line_number':549,'multiline':False]
['text':' unary_attr','line_number':550,'multiline':False]
['text':' Priority 2 to match: QConv2d Unary pattern with fp32/bfloat16 output','line_number':554,'multiline':False]
['text':' Register qconv2d pattern for ExternKernel Lowering','line_number':565,'multiline':False]
['text':' pass_number','line_number':568,'multiline':False]
['text':' computation_op','line_number':569,'multiline':False]
['text':' output_dtype','line_number':570,'multiline':False]
['text':' unary_attr','line_number':571,'multiline':False]
['text':' QLinear','line_number':575,'multiline':False]
['text':' Priority 1 to match: QLinear Unary pattern with int8 output','line_number':576,'multiline':False]
['text':' pass_number','line_number':591,'multiline':False]
['text':' computation_op','line_number':592,'multiline':False]
['text':' output_dtype','line_number':593,'multiline':False]
['text':' unary_attr','line_number':594,'multiline':False]
['text':' Priority 2 to match: QLinear Unary pattern with FP32/BF16 output','line_number':598,'multiline':False]
['text':' pass_number','line_number':608,'multiline':False]
['text':' computation_op','line_number':609,'multiline':False]
['text':' output_dtype','line_number':610,'multiline':False]
['text':' unary_attr','line_number':611,'multiline':False]
['text':' Priority 1 to match: QConv2d Binary or Binary-Unary pattern with int8 output','line_number':633,'multiline':False]
['text':' pass_number','line_number':669,'multiline':False]
['text':' computation_op','line_number':670,'multiline':False]
['text':' output_dtype','line_number':671,'multiline':False]
['text':' binary_unary_attr','line_number':672,'multiline':False]
['text':' Priority 2 to match: QConv2d Binary-Unary pattern with fp32/bfloat16 output','line_number':675,'multiline':False]
['text':' pass_number','line_number':695,'multiline':False]
['text':' computation_op','line_number':696,'multiline':False]
['text':' Note that for int8-mixed-bf16 and non-inplace add, because we have','line_number':697,'multiline':False]
['text':' q-dq inserted at extra input of add, so the non-inplace add has bf16 and fp32 inputs,','line_number':698,'multiline':False]
['text':' the output dtype will be float32.','line_number':699,'multiline':False]
['text':' For inplace add, there is a extra to_bf16 node at add output, so the fusion pattern has bfloat16 output.','line_number':700,'multiline':False]
['text':' binary_unary_attr','line_number':702,'multiline':False]
['text':' pass_number','line_number':707,'multiline':False]
['text':' computation_op','line_number':708,'multiline':False]
['text':' binary_unary_attr','line_number':710,'multiline':False]
['text':' Priority 3: QConv2d Binary pattern with fp32/bfloat16 output','line_number':713,'multiline':False]
['text':' pass_number','line_number':729,'multiline':False]
['text':' computation_op','line_number':730,'multiline':False]
['text':' Same output dtype setting as conv-add-relu pattern','line_number':731,'multiline':False]
['text':' binary_unary_attr','line_number':733,'multiline':False]
['text':' Only match the pattern which max_pool2d_with_indices returns value','line_number':739,'multiline':False]
['text':' instead of indices.','line_number':740,'multiline':False]
['text':' Currently, the default parameters are not in FX Graph generated by Dynamo export.','line_number':794,'multiline':False]
['text':' So, if user defines nn.MaxPool2d with different assignment of default parameter,','line_number':795,'multiline':False]
['text':' it will generate graph with different number of input nodes and hence','line_number':796,'multiline':False]
['text':' different pattern to be matched.','line_number':797,'multiline':False]
['text':' Refer to the issue: https://github.com/pytorch/pytorch/issues/105901','line_number':798,'multiline':False]
['text':' Ensure all the inputs and output has same scale and zero point','line_number':840,'multiline':False]
['text':' Step 1: Check inputs/output zero point','line_number':841,'multiline':False]
['text':' Step 2: Check inputs/output scale','line_number':850,'multiline':False]
['text':' We need to find mul node at output since the scale value is reciprocal to input scale.','line_number':852,'multiline':False]
['text':' Mul node at output should connect to cat node directly.','line_number':853,'multiline':False]
['text':' inputs is with format: [[x1, x1_dq_dtype, x1_zp, x1_scale], ...]','line_number':879,'multiline':False]
['text':' pattern: linear <- reshape <- mul','line_number':964,'multiline':False]
['text':' pattern: linear <- reshape <- to_bf16 <- mul','line_number':968,'multiline':False]
['text':' pattern: linear <- mul','line_number':972,'multiline':False]
['text':' pattern: linear <- to_bf16 <- mul','line_number':976,'multiline':False]
['text':' If dequant pattern has more than 1 users, then do dequant promoted','line_number':987,'multiline':False]
['text':' Dequant_promotion will transform','line_number':1001,'multiline':False]
['text':' graph 1:','line_number':1002,'multiline':False]
['text':'            quant','line_number':1003,'multiline':False]
['text':'      + - - - | - - - +','line_number':1004,'multiline':False]
['text':'      |    dequant    |','line_number':1005,'multiline':False]
['text':'      |    /     \    |','line_number':1006,'multiline':False]
['text':'      |  node1  node2 |','line_number':1007,'multiline':False]
['text':'      + - | - - - | - +','line_number':1008,'multiline':False]
['text':'        quant   quant','line_number':1009,'multiline':False]
['text':' into:','line_number':1010,'multiline':False]
['text':' graph 2:','line_number':1011,'multiline':False]
['text':'            quant','line_number':1012,'multiline':False]
['text':'      + - - / - \ - - +','line_number':1013,'multiline':False]
['text':'      |dequant dequant|','line_number':1014,'multiline':False]
['text':'      |    |      |   |','line_number':1015,'multiline':False]
['text':'      | node1 node2   |','line_number':1016,'multiline':False]
['text':'      + - | - - - | - +','line_number':1017,'multiline':False]
['text':'        quant   quant','line_number':1018,'multiline':False]
['text':' In graph 1, the dequant node is shared by node1 and node2,','line_number':1019,'multiline':False]
['text':' as a result, neither node1 nor node2 could form an int8','line_number':1020,'multiline':False]
['text':' fusion pattern.','line_number':1021,'multiline':False]
['text':' After this transformation, the graph 2 could hit the int8','line_number':1022,'multiline':False]
['text':' fusion pattern: dequant-node-quant, respectively for','line_number':1023,'multiline':False]
['text':' node1 and node2.','line_number':1024,'multiline':False]
['text':' Clone the source_node to a new node','line_number':1028,'multiline':False]
['text':' Replace user_node's input from source_node to new_node','line_number':1029,'multiline':False]
['text':' Find the start node and end node of a dequant pattern','line_number':1043,'multiline':False]
['text':' * End node should be the match.output_node()','line_number':1044,'multiline':False]
['text':' * Start node should be the node of dtype convert to float32','line_number':1045,'multiline':False]
['text':' For a dequant pattern, we should expect see the node list as:','line_number':1053,'multiline':False]
['text':' * OPT(aten.reshape.default)','line_number':1054,'multiline':False]
['text':' * OPT(prims.convert_element_type.default) (to_bf16)','line_number':1055,'multiline':False]
['text':' * aten.mul','line_number':1056,'multiline':False]
['text':' * aten.sub','line_number':1057,'multiline':False]
['text':' * prims.convert_element_type.default (to_fp32)','line_number':1058,'multiline':False]
['text':' For a dequant pattern, we expect the start node is a to_fp32 node','line_number':1064,'multiline':False]
['text':' Clone the dequant pattern for each user node','line_number':1076,'multiline':False]
['text':' Here we do some further check to ensure:','line_number':1092,'multiline':False]
['text':' 1. It's a conv2d node with dim of 4, since we only support lowering of conv2d now.','line_number':1093,'multiline':False]
['text':' 2. The dequant pattern has only 1 user of conv2d node.','line_number':1094,'multiline':False]
['text':' If these conditions don't meet, we will not','line_number':1095,'multiline':False]
['text':' insert weight prepack node into the matched pattern.','line_number':1096,'multiline':False]
['text':' Only support conv2d now','line_number':1107,'multiline':False]
['text':' Ensure the dequant pattern only has 1 user','line_number':1127,'multiline':False]
['text':' since we will delete the dequant pattern here','line_number':1128,'multiline':False]
['text':' Activation QParams','line_number':1191,'multiline':False]
['text':' Weight QParams','line_number':1198,'multiline':False]
['text':' Conv Params','line_number':1205,'multiline':False]
['text':' For dynamic shape case, we can't get activation shape ahead of runtime.','line_number':1216,'multiline':False]
['text':' Insert weight prepack node and the QConv node','line_number':1220,'multiline':False]
['text':' inv_output_scale','line_number':1249,'multiline':False]
['text':' output_zero_point','line_number':1250,'multiline':False]
['text':' output_dtype','line_number':1251,'multiline':False]
['text':' attr','line_number':1252,'multiline':False]
['text':' scalars','line_number':1253,'multiline':False]
['text':' algorithm','line_number':1254,'multiline':False]
['text':' Erase the original conv node','line_number':1262,'multiline':False]
['text':' Erase the dequant pattern','line_number':1264,'multiline':False]
['text':' Erase the dequant pattern','line_number':1267,'multiline':False]
['text':' Erase the dequant per channel pattern','line_number':1271,'multiline':False]
['text':' There is another pattern due to the pass of convert_conv_weights_to_channels_last','line_number':1315,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/07107919297db3f8ab37f11c12666b6d6d5f692e/torch/_inductor/freezing.py#L338-L362.','line_number':1316,'multiline':False]
['text':' Depend on some heuristics, it may or may not insert to(channel_last) node','line_number':1317,'multiline':False]
['text':' between convolution and dequant_per_channel node','line_number':1318,'multiline':False]
['text':' Check dequant pattern has only 1 user.','line_number':1330,'multiline':False]
['text':' pattern: linear -> reshape -> mul','line_number':1346,'multiline':False]
['text':' pattern: linear -> reshape -> to_bf16 -> mul','line_number':1350,'multiline':False]
['text':' pattern: linear -> mul','line_number':1354,'multiline':False]
['text':' pattern: linear -> to_fb16 -> mul','line_number':1358,'multiline':False]
['text':' Ensure the dequant pattern only has 1 user','line_number':1372,'multiline':False]
['text':' since we will delete the dequant pattern here','line_number':1373,'multiline':False]
['text':' pattern: linear -> reshape -> mul','line_number':1421,'multiline':False]
['text':' pattern: linear -> reshape -> to_bf16 -> mul','line_number':1424,'multiline':False]
['text':' pattern: linear -> mul','line_number':1429,'multiline':False]
['text':' pattern: linear -> to_bf16 -> mul','line_number':1432,'multiline':False]
['text':' Activation QParams','line_number':1449,'multiline':False]
['text':' Weight QParams','line_number':1456,'multiline':False]
['text':' Params','line_number':1463,'multiline':False]
['text':' For dynamic shape case, we can't get activation shape ahead of runtime.','line_number':1468,'multiline':False]
['text':' Insert weight prepack node and the qlinear node','line_number':1472,'multiline':False]
['text':' output_scale','line_number':1490,'multiline':False]
['text':' output_zero_point','line_number':1491,'multiline':False]
['text':' output_dtype','line_number':1492,'multiline':False]
['text':' post op name','line_number':1493,'multiline':False]
['text':' post op args','line_number':1494,'multiline':False]
['text':' post op algorithm','line_number':1495,'multiline':False]
['text':' Erase the original linear node','line_number':1507,'multiline':False]
['text':' Erase the dequant pattern','line_number':1515,'multiline':False]
['text':' Erase the dequant per channel pattern','line_number':1519,'multiline':False]
['text':' 4 dequantization patterns will be matched based on the dtype and input dimension size.','line_number':1595,'multiline':False]
['text':' Case 1: int8-mixed-fp32, input dim size is 2','line_number':1596,'multiline':False]
['text':' Case 2: int8-mixed-fp32, input dim size exceeds 2','line_number':1597,'multiline':False]
['text':' Case 3: int8-mixed-bf16, input dim size is 2','line_number':1598,'multiline':False]
['text':' Case 4: int8-mixed-bf16, input dim size exceeds 2','line_number':1599,'multiline':False]
['text':'           quant','line_number':1600,'multiline':False]
['text':'   + - - - - | - - - - +','line_number':1601,'multiline':False]
['text':'   |      dequant      |','line_number':1602,'multiline':False]
['text':'   |         |         |','line_number':1603,'multiline':False]
['text':'   |    OPT(to_bf16)   |','line_number':1604,'multiline':False]
['text':'   |         |         |','line_number':1605,'multiline':False]
['text':'   |    OPT(reshape)   |','line_number':1606,'multiline':False]
['text':'   |      /     \      |','line_number':1607,'multiline':False]
['text':'   |    node1  node2   |','line_number':1608,'multiline':False]
['text':'   + - - | - - - | - - +','line_number':1609,'multiline':False]
['text':'  OPT(reshape) OPT(reshape)','line_number':1610,'multiline':False]
['text':'   + - - | - - - | - - +','line_number':1611,'multiline':False]
['text':'  OPT(to_fp32) OPT(to_fp32)','line_number':1612,'multiline':False]
['text':'   + - - | - - - | - - +','line_number':1613,'multiline':False]
['text':'       quant   quant','line_number':1614,'multiline':False]
['text':' pass_number=0 to run before weight prepack','line_number':1627,'multiline':False]
['text':' Register to pass_number 1, so we can do dequant promotion in pass_number 0.','line_number':1634,'multiline':False]
['text':' Register to pass_number 1, so we can do dequant promotion in pass_number 0.','line_number':1649,'multiline':False]
['text':' Step 1: Dequant promotion for int8-mixed-fp32/bf16','line_number':1660,'multiline':False]
['text':' Step 2: QConv weight prepack','line_number':1663,'multiline':False]
['text':' Step 3: QLinear weight prepack','line_number':1666,'multiline':False]
