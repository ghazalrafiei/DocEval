['text':' addmm decomp with padding will go through pad_addmm multiple times if multiple dimensions are needed to be padded','line_number':128,'multiline':False]
['text':' the add broadcasts, so we only pad if the dimension != 1','line_number':137,'multiline':False]
['text':' Fails with AMD','line_number':165,'multiline':False]
['text':' dram_gbps might be underestimating bandwidth because of cache.','line_number':173,'multiline':False]
['text':' if we estimate machine balance too low we might miss some speedups,','line_number':174,'multiline':False]
['text':' if we extimate too high there will be unnecessary compilation time increase.','line_number':175,'multiline':False]
['text':' TODO - finetune coefficient here. As a reference point, Triton mm model assumes','line_number':176,'multiline':False]
['text':' 80% of reads are in cache and cache is 4x faster than dram_gbps','line_number':177,'multiline':False]
['text':' We don't want to look up the cache for cases that are trivially false','line_number':253,'multiline':False]
['text':' since it does file io','line_number':254,'multiline':False]
['text':' Shape padding introduces additional memory ops. Based on microbenchmarks, 1.1x represents a reasonable','line_number':312,'multiline':False]
['text':' tradeoff between performance improvement from shape padding and overhead from additional memory ops','line_number':313,'multiline':False]
['text':' TODO: Build a learned model which would be better than this heuristic','line_number':314,'multiline':False]
['text':' mm_replace will go through pad_mm multiple times if multiple dimensions are needed to be padded','line_number':347,'multiline':False]
['text':' bmm_replace will go through pad_bmm multiple times if multiple dimensions are needed to be padded','line_number':389,'multiline':False]
['text':' workaround https://github.com/pytorch/pytorch/issues/97894','line_number':408,'multiline':False]
['text':' sizes/values dont actually matter for initial trace','line_number':413,'multiline':False]
['text':' once we get a possible match we re-trace with the actual values and verify the match still holds','line_number':414,'multiline':False]
['text':' workaround https://github.com/pytorch/pytorch/issues/97894','line_number':424,'multiline':False]
['text':' 0.113377 is a "magic" value that lets us recover the lost input arg relationship','line_number':425,'multiline':False]
['text':' mypy is unable to infer the type properly','line_number':451,'multiline':False]
