['text':' NB: this is symbolic so that we don't try to reuse a buffer','line_number':46,'multiline':False]
['text':' for s0 for s1, just because they happen to share the same','line_number':47,'multiline':False]
['text':' size hint','line_number':48,'multiline':False]
['text':' Cpp code gen adds L at the end of ints','line_number':54,'multiline':False]
['text':' Lets remove it for checking whether we have an int or not','line_number':55,'multiline':False]
['text':' Conversions rules follow https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native#func','line_number':79,'multiline':False]
['text':' Convert args of container types e.g. Optional[*]','line_number':85,'multiline':False]
['text':' TODO: support alias','line_number':100,'multiline':False]
['text':' use x.real_type instead of x.type so that we get ScalarType instead of int','line_number':112,'multiline':False]
['text':' the original symbolic expression represented by inner','line_number':153,'multiline':False]
['text':' Default thread stack sizes vary by platform:','line_number':160,'multiline':False]
['text':' - Linux: 8 MB','line_number':161,'multiline':False]
['text':' - macOS: 512 KB','line_number':162,'multiline':False]
['text':' - Windows: 1 MB','line_number':163,'multiline':False]
['text':' Just pick something comfortably smaller than the smallest for now.','line_number':164,'multiline':False]
['text':' In AOT mode, we have a stream provided as a param. A stream is','line_number':198,'multiline':False]
['text':' associated with a device, so we never expect the device to change.','line_number':199,'multiline':False]
['text':' CUDAStreamGuard sets the stream and the device.','line_number':200,'multiline':False]
['text':' Note _DeviceGuard has less overhead than device, but only accepts','line_number':223,'multiline':False]
['text':' integers','line_number':224,'multiline':False]
['text':' try to reuse a recently freed buffer','line_number':273,'multiline':False]
['text':' include a hash so our code cache puts different constants into different files','line_number':379,'multiline':False]
['text':' maps from reusing buffer to reused buffer','line_number':385,'multiline':False]
['text':' type: ignore[assignment]','line_number':388,'multiline':False]
['text':' comparing strides for 0 size tensor is tricky. Ignore them for now.','line_number':461,'multiline':False]
['text':' view operation fallbacks cause issues since inductor','line_number':529,'multiline':False]
['text':' doesn't know the memory is still needed and might reuse it.','line_number':530,'multiline':False]
['text':' Must happen after free symbols are already codegened','line_number':557,'multiline':False]
['text':' TODO: Add check for python too.','line_number':593,'multiline':False]
['text':' We disable planning during training because it presently increases peak memory consumption.','line_number':610,'multiline':False]
['text':' TODO: integrate memory planning & stack allocation?','line_number':613,'multiline':False]
['text':' TODO: this seems legit, NullLine has no node','line_number':670,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':671,'multiline':False]
['text':' these lines will be pointless','line_number':673,'multiline':False]
['text':' codegen allocations in two passes','line_number':676,'multiline':False]
['text':' Assign all symbolic shapes needed to local variables','line_number':711,'multiline':False]
['text':' bytes not numel','line_number':783,'multiline':False]
['text':' No one should ever use this buffer, but for uniformity','line_number':809,'multiline':False]
['text':' define the variable and assign it None','line_number':810,'multiline':False]
['text':' all the constants are global variables, that's why we need','line_number':838,'multiline':False]
['text':' these 'global var_name' lines','line_number':839,'multiline':False]
['text':' Inductor should only work with dense -> dense graph, and','line_number':849,'multiline':False]
['text':' SingletonInts belong to metadata that should only live on','line_number':850,'multiline':False]
['text':' the subclass.','line_number':851,'multiline':False]
['text':' Don't need to add symbolic','line_number':853,'multiline':False]
['text':' Distinguish between different functions using function id','line_number':893,'multiline':False]
['text':' We need to key on non tensor arg only in autotune mode','line_number':899,'multiline':False]
['text':' Add to the cache for the next use','line_number':907,'multiline':False]
['text':' For ReinterpretView, we do not want to check alignment','line_number':939,'multiline':False]
['text':' Also include any possible kernel being called indirectly','line_number':977,'multiline':False]
['text':' We can get symbolic expressions here, like s0*64','line_number':1021,'multiline':False]
['text':' It is fine to have them here, but we need to handle them correctly as their own type','line_number':1022,'multiline':False]
['text':' This is tricky to do, so we wrap in a custom type, distinct from scalars, but also from sympy*','line_number':1023,'multiline':False]
['text':' scalars as well.','line_number':1024,'multiline':False]
['text':' This is handled in `generate_args_decl` which has a correct comment of: TODO: only works for','line_number':1025,'multiline':False]
['text':' constant now, need type info. I agree, this needs type info, and while this is not true type info','line_number':1026,'multiline':False]
['text':' it suffices as a type hint for the purposes of producing the correct code for this type.','line_number':1027,'multiline':False]
['text':' The following methods are for memory management','line_number':1114,'multiline':False]
['text':' cannot determine truth value of Relational','line_number':1125,'multiline':False]
['text':' can be freed but not reused','line_number':1218,'multiline':False]
['text':' Check whether a given buffer was reused by a possible reuser in the wrapper codegen','line_number':1243,'multiline':False]
['text':' Can be consulted from inside ir codegen, e.g. to determine whether a copy is needed','line_number':1244,'multiline':False]
['text':' When in CppWrapperCodeGen, we should only generate the declaration once','line_number':1263,'multiline':False]
['text':' for int array local variable declarations','line_number':1322,'multiline':False]
['text':' for tmp tensor local variable declarations','line_number':1324,'multiline':False]
['text':' CppPrinter sometimes calls at::native functions which causes problems in','line_number':1333,'multiline':False]
['text':' the ABI-compatible mode. Currently we are hitting this problem when codegen','line_number':1334,'multiline':False]
['text':' Grid computation expressions, but we my need to fix other size computation','line_number':1335,'multiline':False]
['text':' as well.','line_number':1336,'multiline':False]
['text':' include a hash so our code cache gives different constants different files','line_number':1385,'multiline':False]
['text':' Round up to the nearest multiple of ALIGN_BYTES','line_number':1428,'multiline':False]
['text':' ALIGN_BYTES must be a power of 2','line_number':1429,'multiline':False]
['text':' mark output type to unwrap tensor back to python scalar','line_number':1439,'multiline':False]
['text':' assign inputs and outputs in both cases so the later codegen can be simplified','line_number':1563,'multiline':False]
['text':' This looks dumb, but can avoid creating two versions of code in the AOTInductor runtime.','line_number':1573,'multiline':False]
['text':' unwrap input tensor back to scalar','line_number':1593,'multiline':False]
['text':' Weights are stored in constants_ and owned by RAIIAtenTensorHandle there.','line_number':1621,'multiline':False]
['text':' Don't call std::move here because it will cause constants_ to lose the ownership.','line_number':1622,'multiline':False]
['text':' Append constants as inputs to the graph','line_number':1633,'multiline':False]
['text':' TODO: input shape checking for regular tensor interface as well?','line_number':1643,'multiline':False]
['text':' comparing strides for 0 size tensor is tricky. Ignore them for now.','line_number':1656,'multiline':False]
['text':' For brevity.','line_number':1807,'multiline':False]
['text':' NOTE(return_constant): In some rare cases where we return','line_number':1864,'multiline':False]
['text':' a constant, we have to return a copy of this constant,','line_number':1865,'multiline':False]
['text':' because (1) constants are not owned by the Model instance','line_number':1866,'multiline':False]
['text':' (2) constants remain the same cross inference runs,','line_number':1867,'multiline':False]
['text':' assuming they are not updated at runtime Basically, we','line_number':1868,'multiline':False]
['text':' cannot release or transfer the ownership of any original','line_number':1869,'multiline':False]
['text':' constant to the user.','line_number':1870,'multiline':False]
['text':' See NOTE(return_constant) above.','line_number':1890,'multiline':False]
['text':' See NOTE(return_constant) above.','line_number':1909,'multiline':False]
['text':' get the hash of the wrapper code to name the extension','line_number':1933,'multiline':False]
['text':' unwrap output tensor back to python scalar','line_number':1941,'multiline':False]
['text':' If no ShapeAsConstantBuffer in the output, directly return the output as tensors','line_number':1943,'multiline':False]
['text':' Append constants to the input args for cpp wrapper.','line_number':1958,'multiline':False]
['text':' Python wrapper directly gets the value inside the wrapper call','line_number':1959,'multiline':False]
['text':' as a global variable passed when calling exec(code, mod.__dict__, mod.__dict__).','line_number':1960,'multiline':False]
['text':' For cpp wrapper, we need to pass this python value to the inductor_entry_cpp function explicitly.','line_number':1961,'multiline':False]
['text':' Wrap the func to support setting result._boxed_call = True','line_number':1971,'multiline':False]
['text':' In the abi_compatible mode, we call fallback aten ops through a C shim layer','line_number':1984,'multiline':False]
['text':' HACK: val_to_arg_str jams multiple arguments together using a comma. If that','line_number':1991,'multiline':False]
['text':' ever breaks, it needs to be reworked to be able to return multiple arguments,','line_number':1992,'multiline':False]
['text':' and the split-on-comma code here needs to be removed.','line_number':1993,'multiline':False]
['text':' We only really *need* convert_arrayref_tensor_to_tensor for','line_number':1998,'multiline':False]
['text':' ArrayRefTensors. The code flowing into here uses `0` for nullptr,','line_number':1999,'multiline':False]
['text':' which convert_arrayref_tensor_to_tensor would blindly coerce to int,','line_number':2000,'multiline':False]
['text':' so just avoid wrapping integers.','line_number':2001,'multiline':False]
['text':' registered output buffer name','line_number':2010,'multiline':False]
['text':' TODO: support other overload for cpp wrapper and remove the below assertions','line_number':2108,'multiline':False]
['text':' call the ABI shim function instead of the ATen one','line_number':2110,'multiline':False]
['text':' in the abi_compatible mode, outputs are returned via arguments','line_number':2136,'multiline':False]
['text':' TODO: Add buf name directly into check_inf_and_nan.','line_number':2209,'multiline':False]
['text':' Because the memory planning is done in two passes (see the implementation','line_number':2240,'multiline':False]
['text':' of self.generate), the writeline behavior is different in the two passes.','line_number':2241,'multiline':False]
['text':' As a result, the emitted int array declarations may appear in a later','line_number':2242,'multiline':False]
['text':' position of the generated code, so the second pass codegen should not','line_number':2243,'multiline':False]
['text':' reuse int array declarations generated in the first pass','line_number':2244,'multiline':False]
['text':' The first pass codegen uses `self` as the writer','line_number':2246,'multiline':False]
['text':' Note: we don't zero storage because empty_strided doesn't zero either.','line_number':2295,'multiline':False]
['text':' bytes not numel','line_number':2340,'multiline':False]
['text':' bytes not numel','line_number':2357,'multiline':False]
['text':' Because the memory planning is done in two passes (see the implementation','line_number':2375,'multiline':False]
['text':' of self.generate), the writeline behavior is different in the two passes.','line_number':2376,'multiline':False]
['text':' NB, the return handle here represents a temporary tensor, which will be automatically','line_number':2412,'multiline':False]
['text':' released.','line_number':2413,'multiline':False]
['text':' Here's a sample usage in the cpp wrapper code:','line_number':2414,'multiline':False]
['text':' ```','line_number':2415,'multiline':False]
['text':' aoti_torch_addmm_out(','line_number':2416,'multiline':False]
['text':'     buf1,','line_number':2417,'multiline':False]
['text':'     arg1_1,','line_number':2418,'multiline':False]
['text':'     RAIIAtenTensorHandle(tmp_tensor_handle_0),','line_number':2419,'multiline':False]
['text':'     buf0,','line_number':2420,'multiline':False]
['text':'     1L,','line_number':2421,'multiline':False]
['text':'     1L));','line_number':2422,'multiline':False]
['text':' ```','line_number':2423,'multiline':False]
['text':' RAIIAtenTensorHandle(tmp_tensor_handle_0) will be released after the call to addmm_out.','line_number':2424,'multiline':False]
['text':' This could be problematic when it's used in a different pattern, for example:','line_number':2425,'multiline':False]
['text':' ````','line_number':2426,'multiline':False]
['text':' AtenTensorHandle tensor_args[] = {RAIIAtenTensorHandle(tmp_tensor_handle_2), buf5, buf6};','line_number':2427,'multiline':False]
['text':' aoti_torch_proxy_executor_call_function(..., tensor_args);','line_number':2428,'multiline':False]
['text':' ````','line_number':2429,'multiline':False]
['text':' RAIIAtenTensorHandle(tmp_tensor_handle_2) will be invalid when it's used in the latter','line_number':2430,'multiline':False]
['text':' kernel call.','line_number':2431,'multiline':False]
['text':'','line_number':2432,'multiline':False]
['text':' This is solved by updating the proxy_executor invocation to','line_number':2433,'multiline':False]
['text':' ```','line_number':2434,'multiline':False]
['text':' aoti_torch_proxy_executor_call_function(...,','line_number':2435,'multiline':False]
['text':'     std::vector<AtenTensorHandle>{','line_number':2436,'multiline':False]
['text':'         RAIIAtenTensorHandle(tmp_tensor_handle_2), buf5, buf6','line_number':2437,'multiline':False]
['text':'     }.data()','line_number':2438,'multiline':False]
['text':' );','line_number':2439,'multiline':False]
['text':' ```','line_number':2440,'multiline':False]
['text':' in the abi_compatible mode, outputs are retrieved by passing','line_number':2455,'multiline':False]
['text':' output pointers, so we skip its codegen here.','line_number':2456,'multiline':False]
['text':' int','line_number':2486,'multiline':False]
['text':' SymInt','line_number':2489,'multiline':False]
['text':' Scalar of type int','line_number':2492,'multiline':False]
['text':' Only treat int Scalar as dynamic','line_number':2494,'multiline':False]
['text':' List[Tensor]','line_number':2500,'multiline':False]
['text':' List[Optional[Tensor]]','line_number':2503,'multiline':False]
['text':' List [int] or List[SymInt]','line_number':2512,'multiline':False]
['text':' List[Scalar]','line_number':2517,'multiline':False]
['text':' Only treat int Scalar as dynamic','line_number':2519,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2528,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2532,'multiline':False]
['text':' TODO: Only support tensor(s) returns for now, SymInt is not implemented yet','line_number':2559,'multiline':False]
['text':' contains both args and flatten kwargs','line_number':2644,'multiline':False]
['text':' output_args has the same pytree structure as outputs','line_number':2656,'multiline':False]
['text':' nullptr is not available in C','line_number':2691,'multiline':False]
['text':' When None is passed as an argument, it represents an optional that does not contain a value.','line_number':2703,'multiline':False]
['text':' nullptr is not available in C','line_number':2705,'multiline':False]
['text':' FIXME handle embedded optional types?','line_number':2728,'multiline':False]
['text':' Need to pass the array length because we can't use std::vector','line_number':2732,'multiline':False]
['text':' TODO: only works for constant now, need type info','line_number':2875,'multiline':False]
['text':' Even in CudaWrapperCodeGen, we may see cpp kernels','line_number':2932,'multiline':False]
