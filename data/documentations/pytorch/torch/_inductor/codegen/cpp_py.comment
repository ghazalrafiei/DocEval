['text':' Since load promotes all half-precision inputs to float, the initial','line_number':153,'multiline':False]
['text':' constant for reduction must be promoted as well','line_number':154,'multiline':False]
['text':' When reading a value from Inductor IR we have a tuple of variable names','line_number':244,'multiline':False]
['text':' When combining intermediate accumulators we have a Welford<T> struct','line_number':247,'multiline':False]
['text':' A small annoyance, due to it being a little cumbersome to just throw {} into strings','line_number':270,'multiline':False]
['text':' Uses float constants to perform FP div','line_number':350,'multiline':False]
['text':' exp == 0','line_number':362,'multiline':False]
['text':' Uses float constants to perform FP div','line_number':368,'multiline':False]
['text':' Initializer list overload','line_number':385,'multiline':False]
['text':' Initializer list overload','line_number':394,'multiline':False]
['text':' A function to print, useful for printing sympy symbols.','line_number':403,'multiline':False]
['text':' args[1] is index','line_number':462,'multiline':False]
['text':' propagate relevant itervars and is_vec from args','line_number':465,'multiline':False]
['text':' c10::bit_cast requires the source and target have the bitwidth.','line_number':525,'multiline':False]
['text':' Because the input tensor's dtype could be promoted, e.g. from float16 to','line_number':526,'multiline':False]
['text':' float, we have to cast the tensor to its original source dtype before','line_number':527,'multiline':False]
['text':' invoking bit_cast. We also need to convert the bit-casted tensor','line_number':528,'multiline':False]
['text':' back to float to make sure we keep using higher precision values','line_number':529,'multiline':False]
['text':' for the rest of the computation.','line_number':530,'multiline':False]
['text':' return f"Sleef_expf_u10({x})"','line_number':555,'multiline':False]
['text':' a and b are integer type','line_number':628,'multiline':False]
['text':' a and b are integer type','line_number':643,'multiline':False]
['text':' Since load promotes all half-precision inputs to float, constants','line_number':752,'multiline':False]
['text':' must be promoted as well','line_number':753,'multiline':False]
['text':' Write masked operation into a lambda','line_number':768,'multiline':False]
['text':' Use the lambda's return type as the type of other','line_number':777,'multiline':False]
['text':' auto tmp5 = tmp4 < 0 ? -1 : 1;','line_number':840,'multiline':False]
['text':' `CppVecKernel` generates both scalar ops and vector ops according to','line_number':860,'multiline':False]
['text':' whether the inputs are scalars or vectors while all ops in `CppVecOverrides`','line_number':861,'multiline':False]
['text':' (except for "masked") assume the inputs are vectors. We wrap the ops in','line_number':862,'multiline':False]
['text':' `CppVecOverrides` to broadcast scalar inputs to vectors if needed or fallback to','line_number':863,'multiline':False]
['text':' `CppOverrides` when all inputs are scalars.','line_number':864,'multiline':False]
['text':'','line_number':865,'multiline':False]
['text':' Inputs to ops.masked are handled separately in its own function due to','line_number':866,'multiline':False]
['text':' the need of recurive handling of masked body.','line_number':867,'multiline':False]
['text':' broadcast scalar args to vector if needed','line_number':877,'multiline':False]
['text':' fallback to scalar ops','line_number':889,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':892,'multiline':False]
['text':' decompose for a better performance','line_number':942,'multiline':False]
['text':' For real x, atanh(x) = 1/2 * log((1+x)/(1-x))','line_number':1099,'multiline':False]
['text':' For real x, asinh(x) = log(x + sqrt(1 + x**2))','line_number':1106,'multiline':False]
['text':' For real x, acosh(x) = log(x + sqrt(x**2 -1))','line_number':1112,'multiline':False]
['text':' TODO: this seems to be dead','line_number':1132,'multiline':False]
['text':' a and b are integer type','line_number':1143,'multiline':False]
['text':' a and b are integer type','line_number':1151,'multiline':False]
['text':' auto tmp5 = tmp4 < 0 ? -1 : 1;','line_number':1173,'multiline':False]
['text':' auto tmp6 = tmp4 == 0 ? 0 : tmp5;','line_number':1180,'multiline':False]
['text':' Note: this function only convert inputs number of elements equal to at::vec::Vectorized<float>.size()','line_number':1211,'multiline':False]
['text':' TODO(Leslie): Add fast path to at::vec::convert_float_to_uint8,','line_number':1214,'multiline':False]
['text':' if we already handle the saturation previously.','line_number':1215,'multiline':False]
['text':' * Pattern match of quantization op in the loop body.','line_number':1216,'multiline':False]
['text':' * Skip the explicit saturation and clamp inside at::vec::convert_float_to_uint8.','line_number':1217,'multiline':False]
['text':' TODO(jgong5): support conversion for other types','line_number':1219,'multiline':False]
['text':' currently we only allow load/store torch.uint8 and handle conversion there','line_number':1220,'multiline':False]
['text':' `result` is explicitly added to the args for correct propagation','line_number':1261,'multiline':False]
['text':' of relevant itervars and vectorization status.','line_number':1262,'multiline':False]
['text':' type: ignore[assignment]','line_number':1268,'multiline':False]
['text':' num_threads the kernel specialized for','line_number':1285,'multiline':False]
['text':' Scalar reduction for other reductions are declared by default','line_number':1373,'multiline':False]
['text':' need to close the worksharing scope to define reduction vars outside it','line_number':1434,'multiline':False]
['text':' generate inner loops or loop body','line_number':1498,'multiline':False]
['text':' not enough work','line_number':1529,'multiline':False]
['text':' if we assume thread number is dynamic, make sure we','line_number':1534,'multiline':False]
['text':' have at least one parallel scope and let OMP runtime','line_number':1535,'multiline':False]
['text':' to manage the serial vs. parallel.','line_number':1536,'multiline':False]
['text':' type: ignore[assignment]','line_number':1559,'multiline':False]
['text':' TODO: support masked_load for non_contiguous path?','line_number':1629,'multiline':False]
['text':' when value's type is str (e.g., welford reduction), caller should make sure','line_number':1661,'multiline':False]
['text':' it is a vector','line_number':1662,'multiline':False]
['text':' this happens when we store a scalar into a vectorized buffer like "fill"','line_number':1696,'multiline':False]
['text':' Scalar reduction for other reductions are declared by default','line_number':1729,'multiline':False]
['text':' Horizontal reduction','line_number':1777,'multiline':False]
['text':' Only float reductions are vectorized currently','line_number':1804,'multiline':False]
['text':' Horizontal reduction','line_number':1807,'multiline':False]
['text':' Vertical reduction','line_number':1815,'multiline':False]
['text':' transposed tile load/store outside the kernel inner loop','line_number':1914,'multiline':False]
['text':' vector load inside the kernel inner loop','line_number':1957,'multiline':False]
['text':' vector store inside the kernel inner loop','line_number':1994,'multiline':False]
['text':' do vertical reduction as the tail loop','line_number':2019,'multiline':False]
['text':' Since this kernel is only for checker but does not generate any','line_number':2032,'multiline':False]
['text':' code, so we need to decrease the kernel count.','line_number':2033,'multiline':False]
['text':' Used to record the graph wrapper code as the wrapper_code status could be','line_number':2037,'multiline':False]
['text':' changed during graph run.','line_number':2038,'multiline':False]
['text':' Cache all the load result','line_number':2049,'multiline':False]
['text':' Cache the dtypes of the store operation. If the store is mixing dtypes, the','line_number':2063,'multiline':False]
['text':' vectorization would not support it as it is hard to determine the vec dtype','line_number':2064,'multiline':False]
['text':' The dtype is used for vectorization','line_number':2066,'multiline':False]
['text':' cmp(x, y): y is a magic value like x >= 1','line_number':2244,'multiline':False]
['text':' cmp(x, y): x is a magic value like 1 >= y','line_number':2247,'multiline':False]
['text':' TODO(Eikan): To record, deduce and propagate the data type of every expression.','line_number':2253,'multiline':False]
['text':' Restore the wrapper_code','line_number':2260,'multiline':False]
['text':' Record the graph wrapper code. The wrapper_code status could be','line_number':2265,'multiline':False]
['text':' changed during graph run. Regarding this checker, we also need to','line_number':2266,'multiline':False]
['text':' run the graph but we don't expect to change any status that would','line_number':2267,'multiline':False]
['text':' impact the code generation. Hence, we record the graph wrapper code','line_number':2268,'multiline':False]
['text':' and replace it with a dummy wrapper_code and then restore to the','line_number':2269,'multiline':False]
['text':' original one as long as the checker is finished.','line_number':2270,'multiline':False]
['text':' type: ignore[misc]','line_number':2285,'multiline':False]
['text':' VecKernel override dtype for constant','line_number':2317,'multiline':False]
['text':' Vectorization only support int32/fp32 now','line_number':2318,'multiline':False]
['text':' So if dtype = int64/fp64, we will cast it to int32/fp32 if possible','line_number':2319,'multiline':False]
['text':' if the range value is sympy.Expr, we might could not deduce the accurate loop interval.','line_number':2361,'multiline':False]
['text':' Trivial case: Range empty','line_number':2372,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2385,'multiline':False]
['text':' If something takes the values 0..7, we will compare in the loop','line_number':2387,'multiline':False]
['text':' x < 8. As such, for the loop not to overflow in the last iteration, we want','line_number':2388,'multiline':False]
['text':' to check that expr_ranges.upper + 1 is representable as well','line_number':2389,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2392,'multiline':False]
['text':' Support masked_load for BF16/FP16. Because the legalization will','line_number':2445,'multiline':False]
['text':' insert to_dtype to convert the BF16/FP16 input to FP32.','line_number':2446,'multiline':False]
['text':' Convert from dtype to torch.float','line_number':2458,'multiline':False]
['text':' Check if load of a scalar tensor of integer','line_number':2465,'multiline':False]
['text':' Only allow below 2 cases:','line_number':2492,'multiline':False]
['text':' Case 1: to_uint8 and store which corresponding to the single quant node','line_number':2493,'multiline':False]
['text':' at last of fusion pattern.','line_number':2494,'multiline':False]
['text':' Case 2: to_uint8 and to_float which corresponding to pair of quant/dequant node','line_number':2498,'multiline':False]
['text':' at middle of fusion pattern.','line_number':2499,'multiline':False]
['text':' Check if all the nodes of a given fx graph can support BF16/FP16','line_number':2531,'multiline':False]
['text':' Propagate the dtype to check if all the fx node is bf16/fp16','line_number':2538,'multiline':False]
['text':' TODO(Eikan): Regarding get_index and index_expr, we should conclude the','line_number':2546,'multiline':False]
['text':' the data type as well.','line_number':2547,'multiline':False]
['text':' Fast path if all operations can support bf16/fp16 without converting to fp32','line_number':2554,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2578,'multiline':False]
['text':' No need to promote to float if all users are direct stores','line_number':2601,'multiline':False]
['text':' No need to promote to float if it is a user of a load which are all directly stored','line_number':2615,'multiline':False]
['text':' Since we always convert the load/store value to float if the tensor is bfloat16/float16.','line_number':2636,'multiline':False]
['text':' Therefore, the reduction should never work with bfloat16/float16 value. Hence, we update','line_number':2637,'multiline':False]
['text':' the bfloat16/float16 reduction by','line_number':2638,'multiline':False]
['text':'     1) updating the src_dtype to float','line_number':2639,'multiline':False]
['text':' and 2) updating the dtype to float if it is bfloat16/float16.','line_number':2640,'multiline':False]
['text':' The legalization always loads the BF16/FP16 tensor as FP32 for computation','line_number':2656,'multiline':False]
['text':' and converts back to BF16/FP16 after the computation.','line_number':2657,'multiline':False]
['text':' Hence, there should be no computation w/ BF16/FP16.','line_number':2658,'multiline':False]
['text':' Therefore, we update the to_dtype by replacing the bf16/fp16 dtype with fp32.','line_number':2659,'multiline':False]
['text':' Save the legalized to_dtype node for the elimination(eliminate_to_dtype step):','line_number':2660,'multiline':False]
['text':'  1) Eliminate the redundant to_dtype node if we have a pattern as follows:','line_number':2661,'multiline':False]
['text':'     graph():','line_number':2662,'multiline':False]
['text':'       %lowp_fp_legalized = call_method[target=to_dtype](args = (%ops, %input, torch.float))','line_number':2663,'multiline':False]
['text':'       %to_dtype2 = call_method[target=to_dtype](args = (%ops, %lowp_fp_legalized, torch.bfloat16/float16))','line_number':2664,'multiline':False]
['text':' Regarding the first to_dtype, it is redundant because','line_number':2665,'multiline':False]
['text':' the second to_type also converts to the torch.bfloat16/torch.float16.','line_number':2666,'multiline':False]
['text':' Hence, we remove the first to_type.','line_number':2667,'multiline':False]
['text':' Eliminate the redundant to_dtype node. Let's consider a pattern as follows:','line_number':2675,'multiline':False]
['text':'   graph():','line_number':2676,'multiline':False]
['text':'     %to_dtype1 = call_method[target=to_dtype](args = (%ops, %input, torch.float), kwargs = {})','line_number':2677,'multiline':False]
['text':'     %to_dtype2 = call_method[target=to_dtype](args = (%ops, %to_dtype1, torch.float), kwargs = {})','line_number':2678,'multiline':False]
['text':' Regarding the first to_dtype, it is redundant because the second to_type also converts to the','line_number':2679,'multiline':False]
['text':' torch.float. Hence, we remove the first to_type','line_number':2680,'multiline':False]
['text':' For debug mode, the graph of LoopBody will attach a new GraphModule as','line_number':2705,'multiline':False]
['text':' owning_module for debugging while the release mode will not. The lint will','line_number':2706,'multiline':False]
['text':' check whether the graph has owning_module to decide if it needs to check','line_number':2707,'multiline':False]
['text':' call_module. LoopBody might contain get_index as a module call. But it','line_number':2708,'multiline':False]
['text':' is just a function. Hence, it cannot pass the lint check for debug mode.','line_number':2709,'multiline':False]
['text':' We bypass the check if the owning_module is None. Eventually, we should call','line_number':2710,'multiline':False]
['text':' get_index via call_function but not call_module.','line_number':2711,'multiline':False]
['text':' Mark the load node to load bf16/fp16','line_number':2728,'multiline':False]
['text':' Bypass the legalization as the kernel can run with bf16/fp16 directly','line_number':2743,'multiline':False]
['text':' Legalize BF16 node by adding to_dtype explicitly','line_number':2763,'multiline':False]
['text':' Ugly hack to maintain the metrics kernel count since','line_number':2790,'multiline':False]
['text':' we only count in CppKernelProxy, not those contained in it','line_number':2791,'multiline':False]
['text':' we can fuse in some extra pointwise into the suffix','line_number':2812,'multiline':False]
['text':' no contiguous vars','line_number':2849,'multiline':False]
['text':' TODO(jgong5): support alternative tiling factors and data types','line_number':2866,'multiline':False]
['text':' Kernels share the same global contexts like V.graph.wrapper_code, V.kernel.args.','line_number':2889,'multiline':False]
['text':' But the generated scalar kernel has updated these global contexts. Hence, the other kernels','line_number':2890,'multiline':False]
['text':' should not do this again to avoid context conflict. By now, we only control the','line_number':2891,'multiline':False]
['text':' config.inplace_buffers. In the future, we could maintain more contexts.','line_number':2892,'multiline':False]
['text':' We chop the loop into two cubes by the nelements - main loop and tail loop.','line_number':2908,'multiline':False]
['text':' Regarding the main loop, it is straightforward that it could be vectorized with','line_number':2909,'multiline':False]
['text':' nelements. But for the tail loop, it still could be vectorized. For example,','line_number':2910,'multiline':False]
['text':' if the nelements is 8(256bits), then the tail loop still could be vectorized','line_number':2911,'multiline':False]
['text':' as 4(128bits).','line_number':2912,'multiline':False]
['text':' ctypes limits the number of args to 1024, refer to:','line_number':2942,'multiline':False]
['text':' https://github.com/python/cpython/commit/a285af7e626d1b81cf09f8b2bf7656f100bc1237','line_number':2943,'multiline':False]
['text':' We set a conservative threshold here.','line_number':2944,'multiline':False]
['text':' TODO(jansel): allow fusion pointwise (vars1, ()) suffix?','line_number':2974,'multiline':False]
['text':' TODO: support kernel profile on other platforms','line_number':3058,'multiline':False]
['text':' TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does','line_number':3089,'multiline':False]
['text':' not use BracesBuffer, so we have no good indicator of a C++ buffer atm.','line_number':3090,'multiline':False]
['text':' generate the code to call this','line_number':3093,'multiline':False]
['text':' wrong number of threads','line_number':3112,'multiline':False]
['text':' the next inner level of the loop, empty if it is inner-most','line_number':3152,'multiline':False]
['text':' contains >1 LoopLevel if the inner level of loop is split','line_number':3153,'multiline':False]
['text':' kernel assigned to this loop level, only valid when it is a leaf','line_number':3155,'multiline':False]
['text':' Regarding the C++/OpenMP backend, `codecache.pick_vec_isa()` to check','line_number':3159,'multiline':False]
['text':' vectorization ISA is a time-consuming and one-shot operation. It leads','line_number':3160,'multiline':False]
['text':' to taking a longer time to import `codegen.cpp` package because the','line_number':3161,'multiline':False]
['text':' `LoopLevel` of the package is decorated by `@dataclasses.dataclass` while','line_number':3162,'multiline':False]
['text':' the decorator will invoke `codecache.pick_vec_isa()` to initialize the','line_number':3163,'multiline':False]
['text':' `simd_nelements` of the `LoopLevel`. It might introduce additional compilation','line_number':3164,'multiline':False]
['text':' overhead to the Triton backend. Therefore, we moved the `simd_nelements` to','line_number':3165,'multiline':False]
['text':' `__post_init__`','line_number':3166,'multiline':False]
['text':' TODO(jansel): look into chunk size and other schedules','line_number':3286,'multiline':False]
