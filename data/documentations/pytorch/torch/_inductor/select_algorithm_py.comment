['text':' correctness checks struggle with fp16/tf32','line_number':32,'multiline':False]
['text':' these objects are imported from the generated wrapper code','line_number':42,'multiline':False]
['text':' for templates with fixed epilogues','line_number':103,'multiline':False]
['text':' get args in correct order','line_number':158,'multiline':False]
['text':' The args may be duplicated, so renaming must be after args are de-duplicated.','line_number':166,'multiline':False]
['text':' get args in correct order','line_number':177,'multiline':False]
['text':' python_argdefs() cannot be run until after the rest of the template lazily adds more args','line_number':181,'multiline':False]
['text':' glue to make generated code use same indexing from template','line_number':243,'multiline':False]
['text':' more stuff might have been added since the codegen_body above','line_number':279,'multiline':False]
['text':' ignore default codegen','line_number':346,'multiline':False]
['text':' In the cpp_wrapper case, we have to compute CUDA launch grid at runtime','line_number':362,'multiline':False]
['text':' if any dynamic dimension is involved. We rely on the Python version','line_number':363,'multiline':False]
['text':' of the grid function to generate those grid configs, which may contain','line_number':364,'multiline':False]
['text':' symbolic values. The wrapper will use cexpr to print out C++ code','line_number':365,'multiline':False]
['text':' appropriately for the grid configs.','line_number':366,'multiline':False]
['text':' TODO(nmacchioni): fix sympy division by zero','line_number':470,'multiline':False]
['text':' create the BenchmarkRequest','line_number':517,'multiline':False]
['text':' for correctness checking','line_number':657,'multiline':False]
['text':' optional dict mapping arg indices to the functions','line_number':711,'multiline':False]
['text':' generating a torch.Tensor for that input from the','line_number':712,'multiline':False]
['text':' corresponding ir.Buffer. if passed for a given','line_number':713,'multiline':False]
['text':' arg, the function will be called instead of','line_number':714,'multiline':False]
['text':' generating a random torch.Tensor for benchmarking.','line_number':715,'multiline':False]
['text':' TODO(nmacchioni): remove once CI tests are fixed','line_number':720,'multiline':False]
['text':' CUDATemplateCaller still needs to go through autotuning process to retrieve workspace size.','line_number':731,'multiline':False]
['text':' do the optional warmup','line_number':744,'multiline':False]
['text':' de-duplicate args','line_number':780,'multiline':False]
['text':' aten kernels want the offset baked in for sliced tensors','line_number':834,'multiline':False]
['text':' triton templates want the base pointer for sliced tensors','line_number':837,'multiline':False]
['text':' shake out any CUDA errors','line_number':841,'multiline':False]
['text':' noqa: TRY200','line_number':863,'multiline':False]
['text':' noqa: TRY200','line_number':865,'multiline':False]
['text':' only benchmark triton kernel in sub process for now.','line_number':876,'multiline':False]
['text':' ATen/Extern kernel are still benchmarked in the current process.','line_number':877,'multiline':False]
['text':' triton templates want the base tensor.','line_number':939,'multiline':False]
['text':' preserve rng states to avoid the rand_strided call below changes','line_number':942,'multiline':False]
['text':' the rng states for the real model code.','line_number':943,'multiline':False]
['text':' ensure lowering is imported so that `extern_kernels.*` is populated','line_number':1000,'multiline':False]
['text':' noqa: F401','line_number':1001,'multiline':False]
