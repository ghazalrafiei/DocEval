['text':' no-op decorator','line_number':60,'multiline':False]
['text':' copy_ fails when trying to write to tensors with memory overlap,','line_number':95,'multiline':False]
['text':' for expanded dimensions (a dimension which used to have size 1 -> ?)','line_number':96,'multiline':False]
['text':' we can select one element from that dimension and write to it','line_number':97,'multiline':False]
['text':' to achieve writing to all values of that dimension of the input tensor','line_number':98,'multiline':False]
['text':' if torch._debug_has_internal_overlap thinks this tensor potentially has','line_number':112,'multiline':False]
['text':' memory overlap internally, let's dig deeper to find out whether it's true.','line_number':113,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':236,'multiline':False]
['text':' Inputs to fx_codegen_and_compile','line_number':294,'multiline':False]
['text':' Anything that affects codegen should go here, so if the signature','line_number':295,'multiline':False]
['text':' of fx_codegen_and_compile changes, the dict should be updated accordingly','line_number':296,'multiline':False]
['text':' type: ignore[arg-type]','line_number':318,'multiline':False]
['text':' Return the output strides to the caller via TracingContext','line_number':323,'multiline':False]
['text':' output args are tuple of first argument','line_number':333,'multiline':False]
['text':' doesnt work for non-trees because the warmup run would apply mutation twice','line_number':347,'multiline':False]
['text':' checking if mutation is only on parameters/static inputs','line_number':349,'multiline':False]
['text':' Force specialize all inputs so that CUDA graphs will work','line_number':379,'multiline':False]
['text':' guard','line_number':382,'multiline':False]
['text':' See [Backward Generation Handling]','line_number':404,'multiline':False]
['text':' if cudagraph'd the forward and set the device, we need to let the cudagraph manager','line_number':405,'multiline':False]
['text':' know we are we running the backward even if we will not run it in cudagraphs','line_number':406,'multiline':False]
['text':' should already exist from forward','line_number':415,'multiline':False]
['text':' cudagraphs does its own aligning of inputs','line_number':429,'multiline':False]
['text':' aot autograd needs to know to pass in inputs as a list','line_number':444,'multiline':False]
['text':' lift the maximum depth of the Python interpreter stack','line_number':466,'multiline':False]
['text':' to adapt large/deep models','line_number':467,'multiline':False]
['text':' Convert view to reshape in the graph. This is necessary primarily for','line_number':480,'multiline':False]
['text':' layout optimization. Do it unconditionally for uniformity.','line_number':481,'multiline':False]
['text':'','line_number':482,'multiline':False]
['text':' It's needed because when we do layout optimization, an contiguous tensor','line_number':483,'multiline':False]
['text':' in eager mode may becomes a channels last tensor. A view op previously','line_number':484,'multiline':False]
['text':' can be applied to the contiguous tensor may not be able to be applied','line_number':485,'multiline':False]
['text':' on the channels tensor any more. An error like','line_number':486,'multiline':False]
['text':'   RuntimeError: view size is not compatible with input tensor's size and stride','line_number':487,'multiline':False]
['text':'   (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.','line_number':488,'multiline':False]
['text':' will be printed.','line_number':489,'multiline':False]
['text':'','line_number':490,'multiline':False]
['text':' Replace view op to reshape op in this case.','line_number':491,'multiline':False]
['text':' As an example, timm_resnest/botnet26t_256/convnext_base etc. will fail if we don't do this.','line_number':492,'multiline':False]
['text':'','line_number':493,'multiline':False]
['text':' Also this has to be done before FakeTensorProp below to avoid the failed','line_number':494,'multiline':False]
['text':' .view() call.','line_number':495,'multiline':False]
['text':' It is safe to run FakeTensorProp under no_grad because by the time','line_number':498,'multiline':False]
['text':' we're in inductor, we assume that AOTAutograd has already "taken care"','line_number':499,'multiline':False]
['text':' of autograd, so there should be no more autograd-related API's in the','line_number':500,'multiline':False]
['text':' graph.','line_number':501,'multiline':False]
['text':' pattern matcher passes might not preserve striding information','line_number':505,'multiline':False]
['text':' on node.meta["val"]. if in the future we rely on these being','line_number':506,'multiline':False]
['text':' correct we will need to fix.','line_number':507,'multiline':False]
['text':' has some issues with memory in training','line_number':510,'multiline':False]
['text':' example_inputs will be used by AOTInductor to dry-run the generated code for Triton kernel tuning.','line_number':518,'multiline':False]
['text':' For the forward pass, we have the real inputs to be used as example_inputs. For the backward pass,','line_number':519,'multiline':False]
['text':' we currently use fake tensors and defake them later.','line_number':520,'multiline':False]
['text':' We'll put the output strides in the compiled graph so we','line_number':535,'multiline':False]
['text':' can later return them to the caller via TracingContext','line_number':536,'multiline':False]
['text':' if using fake tensors, defer cudagraphs until we get real inputs at runtime','line_number':651,'multiline':False]
['text':' TODO(jansel): figure out why this version doesn't work:','line_number':688,'multiline':False]
['text':' return torch.empty_strided(x.size(), x.stride(), dtype=x.dtype, device=x.device)','line_number':689,'multiline':False]
['text':' allocate static tensor inputs','line_number':727,'multiline':False]
['text':' copy over input values for fresh allocations','line_number':737,'multiline':False]
['text':' warmup','line_number':742,'multiline':False]
['text':' copy static_inputs because it will be cleared in model','line_number':746,'multiline':False]
['text':' record','line_number':753,'multiline':False]
['text':' TODO - could make one single op of multiple slices','line_number':772,'multiline':False]
['text':' and avoid dispatch.','line_number':773,'multiline':False]
['text':' Could also pre-index the `dst` tensors','line_number':774,'multiline':False]
['text':' partition_fn won't be called','line_number':875,'multiline':False]
['text':' make sure meta['val'] is properly setup','line_number':880,'multiline':False]
['text':' type: ignore[arg-type]','line_number':887,'multiline':False]
['text':' for freezing, all graph outputs should be user visible','line_number':895,'multiline':False]
['text':' constant params will be real tensors, not fake','line_number':902,'multiline':False]
['text':' aot_inductor codegens a call that takes in just the inputs, so we don't return a wrapper','line_number':924,'multiline':False]
['text':' that drops constant-ified params','line_number':925,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':934,'multiline':False]
['text':' need extra layer of patching as backwards is compiled out of scope','line_number':952,'multiline':False]
['text':' Validate devices before switching to fake tensors.','line_number':974,'multiline':False]
['text':' this graph is the result of dynamo.export()','line_number':1005,'multiline':False]
['text':' partition_fn won't be called','line_number':1039,'multiline':False]
['text':' See Note [User Outputs in the inductor graph]','line_number':1053,'multiline':False]
['text':' Note [User Outputs in the inductor graph]','line_number':1073,'multiline':False]
['text':' We makes the following assumption','line_number':1074,'multiline':False]
['text':' For inference','line_number':1075,'multiline':False]
['text':'   len(orig_model_outputs) == len(model_outputs)','line_number':1076,'multiline':False]
['text':' For training','line_number':1077,'multiline':False]
['text':'   len(orig_model_outputs) <= len(model_outputs)','line_number':1078,'multiline':False]
['text':' During training, most of the time the model_outputs starts with','line_number':1079,'multiline':False]
['text':' original module's outputs followed by saved activations.','line_number':1080,'multiline':False]
['text':' But this can be not true if the model have inplace updated tensors.','line_number':1081,'multiline':False]
['text':' AOTAutograd will make those tensors being returned before the original','line_number':1082,'multiline':False]
['text':' module's output.','line_number':1083,'multiline':False]
['text':' To make things safe, we'll use original_output_start_index field','line_number':1084,'multiline':False]
['text':' set by AOTAutograd to decide where the original module outputs start.','line_number':1085,'multiline':False]
['text':' Sanity chec: we are about to splice out the "user" outputs from the full set','line_number':1087,'multiline':False]
['text':' of "graph" outputs. Make sure we're within bounds.','line_number':1088,'multiline':False]
['text':' TODO: can add logging before/after the call to create_aot_dispatcher_function','line_number':1142,'multiline':False]
['text':' in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func','line_number':1143,'multiline':False]
['text':' once torchdynamo is merged into pytorch','line_number':1144,'multiline':False]
['text':' pass config dict back to user','line_number':1175,'multiline':False]
['text':' TODO(voz): It would be nice to enable this assert, but there are lots of tests that','line_number':1185,'multiline':False]
['text':' pass in real inputs for now.','line_number':1186,'multiline':False]
['text':' if len(inputs) > 0:','line_number':1187,'multiline':False]
['text':' assert fake_mode is not None, breakpoint()','line_number':1188,'multiline':False]
['text':' When there are no tensor inputs, get shape_env from the first SymInt.','line_number':1193,'multiline':False]
['text':' TODO(voz): Should we always have one anyway?','line_number':1198,'multiline':False]
['text':' can't check this, assume true','line_number':1212,'multiline':False]
['text':' for graphs whose result is one node with multiple outputs','line_number':1222,'multiline':False]
['text':' note this doesn't check the spec, assuming it is the same','line_number':1274,'multiline':False]
