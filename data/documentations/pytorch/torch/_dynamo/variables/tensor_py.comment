['text':' this is a fully static shape, and the keys on props here inform specialization.','line_number':149,'multiline':False]
['text':' We have to cast to int here, because these might get accessed as ConstantVariable, which has','line_number':150,'multiline':False]
['text':' a strict no-symint policy. If we got here due to not having free symbols, this is a known constant','line_number':151,'multiline':False]
['text':' already. We could remove the discrepancy here, by having ConstantVariable be more permissive for','line_number':152,'multiline':False]
['text':' constant backed SymInts, but that assert being strict has led to some good signal in hunting bugs, and','line_number':153,'multiline':False]
['text':' I'd like to keep it around for now.','line_number':154,'multiline':False]
['text':' the non is_symbolic case applies to the jagged layout','line_number':156,'multiline':False]
['text':' NestedTensor case as singleton ints are not symbolic','line_number':157,'multiline':False]
['text':' For local source, we associate the real value. We use this real value','line_number':174,'multiline':False]
['text':' for implementing getattr fallthrough on the variable tracker base class.','line_number':175,'multiline':False]
['text':' Note - this scope construction is mirrored in guards','line_number':177,'multiline':False]
['text':' A subsequent PR will introduce a util.','line_number':178,'multiline':False]
['text':' We raise in case we get a typerror bug w/ SuperSource.','line_number':181,'multiline':False]
['text':' SuperSource has bugs in it atm, and can produce code like','line_number':182,'multiline':False]
['text':' eval("super(L['mod'].model.model.encoder.embed_positions.forward__class__,','line_number':183,'multiline':False]
['text':' L['mod'].model.model.encoder.embed_positions)", scope)','line_number':184,'multiline':False]
['text':' Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.','line_number':185,'multiline':False]
['text':' Callables have more nuanced handling, and we should let the existing system delegate here.','line_number':201,'multiline':False]
['text':' Raising was past behavior and so should always be sound to fall back.','line_number':202,'multiline':False]
['text':' Note - at a certain point we may want to handle','line_number':203,'multiline':False]
['text':' Add a guard for type matching, these guards are checked before tensor guards','line_number':249,'multiline':False]
['text':' In some cases, a <tensor>.<attr> guard can be evaluated first, and break if','line_number':250,'multiline':False]
['text':' <tensor> is later changed to another type','line_number':251,'multiline':False]
['text':' It's hard to get inplace view (metadata mutation) on graph input work properly across','line_number':256,'multiline':False]
['text':' dynamo/aot/inductor, just fall back.','line_number':257,'multiline':False]
['text':' Delay the graph break to the actual call of unsqueeze_/resize_/resize_as_ etc.','line_number':265,'multiline':False]
['text':' For attributes (not methods) that were not caught in the special handling above,','line_number':270,'multiline':False]
['text':' (e.g. tensor.real), we handle these generically, assuming that the output type is','line_number':271,'multiline':False]
['text':' a tensor.','line_number':272,'multiline':False]
['text':' Make sure this is an attribute, not a method.','line_number':284,'multiline':False]
['text':' type(torch.Tensor.H) should be "getset_descriptor"','line_number':285,'multiline':False]
['text':' This is a because of CPython implementation, see THPVariableType:','line_number':286,'multiline':False]
['text':' these attributes are implemented under tp_getset, which appear','line_number':287,'multiline':False]
['text':' as `getset_descriptor`s, (compared to, say, methods which appear','line_number':288,'multiline':False]
['text':' as `method_descriptor`s)','line_number':289,'multiline':False]
['text':' SymNodeVariable for symbolic sizes, ConstantVariable for constants OR values produced through','line_number':323,'multiline':False]
['text':' symbolic_shapes, but that end up as int/sympy.Integer','line_number':324,'multiline':False]
['text':' Technically, this should not be necessary, but I'm including it','line_number':375,'multiline':False]
['text':' for enhanced BC, in case example_value is sometimes not set','line_number':376,'multiline':False]
['text':' (it really should always be set though!)','line_number':377,'multiline':False]
['text':' It might still be constant!  Consult the fake tensor and see','line_number':384,'multiline':False]
['text':' int conversion for safety, in case a SymInt refined','line_number':389,'multiline':False]
['text':' to constant','line_number':390,'multiline':False]
['text':' Oops, it's not constant.  Do the dynamic shapes path.','line_number':397,'multiline':False]
['text':' It might still be constant!  Consult the fake tensor and see','line_number':411,'multiline':False]
['text':' Oops, it's not constant.  Do the dynamic shapes path.','line_number':419,'multiline':False]
['text':' torch.FloatTensor, etc. are all of type "torch.tensortype".','line_number':471,'multiline':False]
['text':' torch.fx's tracer fails on these types, because it doesn't support arguments of torch.tensortype type.','line_number':472,'multiline':False]
['text':' So, we pass it in as a string (which is also supported, see above implementation for .type() with 0 args)','line_number':473,'multiline':False]
['text':' [Note: __torch_function__] coerce this tensor variable into a TensorWithTFOverrideVariable','line_number':492,'multiline':False]
['text':' in eager, this is just a type change. This isn't sound if a __torch_function__ tensor subclass','line_number':493,'multiline':False]
['text':' defines a constructor, but if only a __torch_function__ impl is defined, this is okay to call.','line_number':494,'multiline':False]
['text':' It is up to the user whether this is correct behavior or not.','line_number':495,'multiline':False]
['text':' TODO: I think this branch is dead','line_number':515,'multiline':False]
['text':' We don't check that the tensor is on CPU when force is False, as this','line_number':531,'multiline':False]
['text':' allows us to execute NumPy code on CUDA. Same for requires_grad=True','line_number':532,'multiline':False]
['text':' If the user set force=True we try to preserve the semantics (no gradients, move to CPU...)','line_number':535,'multiline':False]
['text':' Hacky way to create a view of self that will be marked as NumpyNdarrayVariable','line_number':541,'multiline':False]
['text':' Handling resizing in its full generality is difficult.','line_number':611,'multiline':False]
['text':' torch.Tensor.set_() has several overloads.','line_number':614,'multiline':False]
['text':' aten::set_.source_Tensor(Tensor) gets special handling','line_number':615,'multiline':False]
['text':' in AOTAutograd and functionalization, because it is the most common','line_number':616,'multiline':False]
['text':' overload and is used by FSDP.','line_number':617,'multiline':False]
['text':' graph-breaking on aten::set_source_Tensor_storage_offset for now,','line_number':618,'multiline':False]
['text':' unless we find that we need to make it work.','line_number':619,'multiline':False]
['text':' Rewrite __contains__ here so that downstream passes can trace through','line_number':640,'multiline':False]
['text':' without dealing with unbacked symbool. Roughly the code we translate is:','line_number':641,'multiline':False]
['text':' def __contains__(self, x):','line_number':642,'multiline':False]
['text':'     return (x == self).any().item()','line_number':643,'multiline':False]
['text':' rewrite non-primitive args/kwargs to be included in the on-the-fly prim function','line_number':648,'multiline':False]
['text':' and rewrite args to have only proxyable args, then insert call_function','line_number':649,'multiline':False]
['text':' attach the same function name for better debugging','line_number':656,'multiline':False]
['text':' Note - do not arbitrarily add hooks here - make sure they match the same contract','line_number':668,'multiline':False]
['text':' see [On tensor.register_hook]','line_number':669,'multiline':False]
['text':' NestedUserFunctionVariable don't carry their fn, but reconstruction builds it','line_number':684,'multiline':False]
['text':' This should not be onerous to support when needed.','line_number':685,'multiline':False]
['text':' Intermediary','line_number':697,'multiline':False]
['text':' TODO(voz):','line_number':712,'multiline':False]
['text':' We can relax this by speculating the callable and ensuring that it doesn't modify arbitrary','line_number':713,'multiline':False]
['text':' python state.','line_number':714,'multiline':False]
['text':' We *Must* be in compiled_autograd here because backward hooks can contain anything, and it is unsafe to run','line_number':715,'multiline':False]
['text':' them in a compiled bwd without re-entering dynamo as compiled_autograd does.','line_number':716,'multiline':False]
['text':'','line_number':717,'multiline':False]
['text':' Discussion point 1 - Should we bypass this if nopython/fullgraph = True?','line_number':718,'multiline':False]
['text':'   No. Because this was going to be a graph break anyway - this check does not','line_number':719,'multiline':False]
['text':' introduce new graph breaks where there were none.','line_number':720,'multiline':False]
['text':'','line_number':721,'multiline':False]
['text':' Discussion point 2 - Should we defer this check to backwards?','line_number':722,'multiline':False]
['text':'   No. Because compiled autograd is not yet ready for prime time. As such, if we defer, a user','line_number':723,'multiline':False]
['text':' would have no recourse - their forward traces just fine, but will fail at backwards unless','line_number':724,'multiline':False]
['text':' compiled_autograd is enabled. If compiled_autograd fails (there are a lot of failures today)','line_number':725,'multiline':False]
['text':' then they have nothing they can do except disable compile.','line_number':726,'multiline':False]
['text':' This wraps our user provided fn with a function that intercedes and','line_number':731,'multiline':False]
['text':' uses our `invoke` higher order op to record a hook invocation in bwd graph.','line_number':732,'multiline':False]
['text':' Convert x.new(torch.Size) into x.new_empty(torch.Size),','line_number':758,'multiline':False]
['text':' as Tensor.new acts differently with a Size input versus a tuple input.','line_number':759,'multiline':False]
['text':' TODO: Should we allow non SymTypes here?  Today it is allowed','line_number':798,'multiline':False]
['text':' noqa: TRY200','line_number':814,'multiline':False]
['text':' NB: This INTENTIONALLY does not call super(), because there is','line_number':857,'multiline':False]
['text':' no intrinsic reason ndarray properties are related to Tensor','line_number':858,'multiline':False]
['text':' properties.  The inheritance here is for implementation sharing.','line_number':859,'multiline':False]
['text':' These are awkward to implement.  The standard playbook for torch._numpy','line_number':886,'multiline':False]
['text':' interop is to trace a call into the torch._numpy wrapper which works for','line_number':887,'multiline':False]
['text':' Tensor operations.  However, we don't want to do this for calls','line_number':888,'multiline':False]
['text':' that don't return Tensors, because in those cases we may not want','line_number':889,'multiline':False]
['text':' to trace the attribute access into the graph at all (it is sort','line_number':890,'multiline':False]
['text':' of harmless to do so, because AOTAutograd will eliminate them,','line_number':891,'multiline':False]
['text':' but it's best not to trace them in to begin with.)  But in any','line_number':892,'multiline':False]
['text':' case, tracing these into the graph is like trying to fit a square','line_number':893,'multiline':False]
['text':' peg into a round hole; best not to do it.  So instead we','line_number':894,'multiline':False]
['text':' painstakingly implement these by hand','line_number':895,'multiline':False]
['text':'','line_number':896,'multiline':False]
['text':' NB: only ALWAYS specialized attributes can go here; notably,','line_number':897,'multiline':False]
['text':' size/shape not allowed!','line_number':898,'multiline':False]
['text':' delegate back to TensorVariable','line_number':927,'multiline':False]
['text':' Convert a `TensorVariable` instance into an `UnspecializedPythonVariable` instance.','line_number':956,'multiline':False]
