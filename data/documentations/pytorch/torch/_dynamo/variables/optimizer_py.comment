['text':' stash a weak_ptr to optimizer to invalidate code','line_number':63,'multiline':False]
['text':' if the optimizer object dies','line_number':64,'multiline':False]
['text':' This is currently safe only because the only actual `ret_val`s returned','line_number':68,'multiline':False]
['text':' by the `_init_group` of existing optimizers are properties that are invariant','line_number':69,'multiline':False]
['text':' to the input tensors (e.g. dtype, layout). Changing these would trigger a','line_number':70,'multiline':False]
['text':' recompilation and hence never result in the wrong specialization of `ret_val`.','line_number':71,'multiline':False]
['text':' trace normally if we can't map args or install guards correctly','line_number':74,'multiline':False]
['text':' state guards take a long time to generate','line_number':127,'multiline':False]
['text':' so we manually generate them here','line_number':128,'multiline':False]
['text':' this next line has the side effect of installing guards','line_number':151,'multiline':False]
['text':' If we have a source for a tensor already use it,','line_number':160,'multiline':False]
['text':' if we have not seen a tensor before, stash and use a','line_number':161,'multiline':False]
['text':' global weak ref source, since it must be an optimizer tensor','line_number':162,'multiline':False]
['text':' that we have missed','line_number':163,'multiline':False]
['text':' mark these tensors as static for cudagraphs','line_number':166,'multiline':False]
['text':' mark these tensors as static for cudagraphs','line_number':173,'multiline':False]
