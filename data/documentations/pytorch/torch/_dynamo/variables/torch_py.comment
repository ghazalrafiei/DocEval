['text':' constant fold','line_number':229,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/110765','line_number':238,'multiline':False]
['text':' [Note: __torch_function__] we return empty here because we restrict','line_number':254,'multiline':False]
['text':' the set of functions that we trace __torch_function__ on to','line_number':255,'multiline':False]
['text':' functions outside of the actual set. Implementing this properly will require implementing','line_number':256,'multiline':False]
['text':' some variable types to track and compare tensor getset descriptors','line_number':257,'multiline':False]
['text':' Use polyfill to convert math.radians(x) into math.pi * x / 180.0','line_number':264,'multiline':False]
['text':' is_acceptable(tensor) returns true if','line_number':375,'multiline':False]
['text':'   (a) tensor dtype/device are supported by cudnn','line_number':376,'multiline':False]
['text':'   (b) cudnn is available','line_number':377,'multiline':False]
['text':'   (c) some initialization has completed','line_number':378,'multiline':False]
['text':' technically, it depends on some global state from (c) (torch.backends.cudnn.__cudnn_version)','line_number':379,'multiline':False]
['text':' TODO(voz): This is rewritten as a call_method because','line_number':399,'multiline':False]
['text':' torch.numel(x) w/ sym shapes raises a RuntimeError and x.numel() does not','line_number':400,'multiline':False]
['text':' TODO: These special cases shouldn't be necessary; we should','line_number':409,'multiline':False]
['text':' generically support torch.ops that return int','line_number':410,'multiline':False]
['text':' we see this when retracing already traced code','line_number':417,'multiline':False]
['text':' decompose addcdiv into constituent ops, prevents a graph break due to converting','line_number':432,'multiline':False]
['text':' value to a scalar','line_number':433,'multiline':False]
['text':' becuase the input is a "ProcessGroupVariable", we'll be guarding on its','line_number':456,'multiline':False]
['text':' ID_MATCH based on how it was constructed.','line_number':457,'multiline':False]
['text':' We desugar it at trace-time into ranks by directly calling util','line_number':459,'multiline':False]
['text':' bake the result into the trace','line_number':460,'multiline':False]
['text':' Note - while we *could* cook up sources around invocations, like a FunctionSource','line_number':465,'multiline':False]
['text':' the space of invoking functions in the middle of the guard chain is very iffy. As such,','line_number':466,'multiline':False]
['text':' guard propagation via options is the best we can do.','line_number':467,'multiline':False]
['text':' rewrite non-primitive args/kwargs to be included in the on-the-fly prim function','line_number':472,'multiline':False]
['text':' and rewrite args to have only proxyable args, then insert call_function','line_number':473,'multiline':False]
['text':' attach the same function name for better debugging','line_number':480,'multiline':False]
['text':' TODO(voz): Replace w/ dynamic shape rewrite table.','line_number':517,'multiline':False]
['text':' Ideally, we would be able to do this at ctor time, but alas we need a combination','line_number':518,'multiline':False]
['text':' of value + args to determine this.','line_number':519,'multiline':False]
['text':' NB: This includes UnspecializedPythonVariable','line_number':530,'multiline':False]
['text':' TODO: there maybe other recursive structures you need to','line_number':535,'multiline':False]
['text':' check','line_number':536,'multiline':False]
['text':' NB: OK to pass torch.tensor(tensor), this will trace fine','line_number':546,'multiline':False]
['text':' This is slower and less canonical, so only use it if we','line_number':550,'multiline':False]
['text':' have to','line_number':551,'multiline':False]
['text':' out variants of torch operators like torch.sort and','line_number':577,'multiline':False]
['text':' torch.sigmoid mutate the tensors in the out field. Track such','line_number':578,'multiline':False]
['text':' tensors and rewrite the symbolic locals.','line_number':579,'multiline':False]
['text':' It's hard to get out variants with resizing on graph inputs work','line_number':595,'multiline':False]
['text':' properly across dynamo/aot/inductor, just fall back.','line_number':596,'multiline':False]
['text':' It's difficult to handle strides correctly in functionalization','line_number':602,'multiline':False]
['text':' when calling an out= op with a non-contiguous out argument','line_number':603,'multiline':False]
['text':' constant prop through it','line_number':630,'multiline':False]
['text':' the remainder of this is just optional debug checks','line_number':653,'multiline':False]
['text':' weird ones like torch.nn.functional.avg_pool2d have __self__','line_number':666,'multiline':False]
['text':' some _C functions have __self__ as a null capsule','line_number':672,'multiline':False]
['text':' constant fold','line_number':700,'multiline':False]
['text':' torch.LongTensor cannot accept a list of FakeTensors.','line_number':717,'multiline':False]
['text':' So we stack the list of FakeTensors instead.','line_number':718,'multiline':False]
['text':' Stack FakeTensor','line_number':727,'multiline':False]
