['text':' TODO: Figure out whether masks are actually supported for this layout or not','line_number':65,'multiline':False]
['text':' This is expected to be called after check_tensor_shapes ensuring that the','line_number':75,'multiline':False]
['text':' size() calls won't error since the inputs are all 4 dimensional','line_number':76,'multiline':False]
['text':' num_heads logic for nested input is checked in','line_number':81,'multiline':False]
['text':' check_for_seq_len_0_nested_tensor as there is handling there to make sure','line_number':82,'multiline':False]
['text':' num_heads is not ragged','line_number':83,'multiline':False]
['text':' num_head_dims is ragged','line_number':119,'multiline':False]
['text':' This is being called inside sdp with shape [batch, heads, {seq_len}, dim]','line_number':127,'multiline':False]
['text':' When this function is called we are assured that the nt is dim==4','line_number':163,'multiline':False]
['text':' short circuit if any is unsafe','line_number':171,'multiline':False]
['text':' short circuit if any is unsafe','line_number':182,'multiline':False]
['text':' short circuit if any is unsafe','line_number':193,'multiline':False]
['text':' We now know none of the inputs have ragged num_heads, so we can safely','line_number':197,'multiline':False]
['text':' access .size(1)','line_number':198,'multiline':False]
['text':' This function is used to calculate two pieces of metadata that are needed','line_number':305,'multiline':False]
['text':' for use with flash-attention and efficient_attention kernels. They are the','line_number':306,'multiline':False]
['text':' cumulative sequence_length over a batch of sequences and the maximum','line_number':307,'multiline':False]
['text':' sequence length.','line_number':308,'multiline':False]
['text':' It returns a tuple of cumulative sequence lengths and the maximum sequence','line_number':310,'multiline':False]
['text':' length, and the last element in the cumulative_sequence_lengths','line_number':311,'multiline':False]
['text':' TODO: Explore performance impact of copying','line_number':316,'multiline':False]
['text':' TODO: Explore performance impact of copying','line_number':321,'multiline':False]
['text':' TODO: Explore performance impact when compiling','line_number':327,'multiline':False]
['text':' This function checks if a nested tensor is valid for','line_number':333,'multiline':False]
['text':' use with the flash-attention and efficient_attention kernels without','line_number':334,'multiline':False]
['text':' needing to call contiguous on the nested tensor input.','line_number':335,'multiline':False]
['text':' It checks that the storage offsets' adjacent_differences are a constant','line_number':336,'multiline':False]
['text':' mutiple of the previous tensor in the nested tensor and that the strides','line_number':337,'multiline':False]
['text':' are monitonically decreasing. This check is done after calling transpose on','line_number':338,'multiline':False]
['text':' the nested tensor resulting in a Nt of shape [bsz, {seq_len}, num_heads, dim]','line_number':339,'multiline':False]
['text':' Returns a boolean indicating if contiguous needs to be called for input','line_number':341,'multiline':False]
['text':' Check initially that the tensor strides are in strictly descending order','line_number':350,'multiline':False]
['text':' This would mean that the last stride is greater than the seq_len','line_number':354,'multiline':False]
['text':' stride','line_number':355,'multiline':False]
['text':' Congrats you made it!','line_number':359,'multiline':False]
['text':' TODO: Next iteration should add test cases and check it works','line_number':371,'multiline':False]
['text':' def _sdpa_nested_preprocessing_with_broadcast(query, key, value):','line_number':372,'multiline':False]
['text':'     # Query (Batch x Num_heads x {Q_seq_len}  x Dim_per_head)','line_number':373,'multiline':False]
['text':'     # Key   (Batch x Num_heads x {KV_seq_len} x Dim_per_head)','line_number':374,'multiline':False]
['text':'     # Value (Batch x Num_heads x {KV_seq_len} x Dim_per_head)','line_number':375,'multiline':False]
['text':'     q_batch_size = query.size(0)','line_number':376,'multiline':False]
['text':'     k_batch_size = key.size(0)','line_number':377,'multiline':False]
['text':'     v_batch_size = value.size(0)','line_number':378,'multiline':False]
['text':'     output_batch_size = max(q_batch_size, k_batch_size, v_batch_size)','line_number':380,'multiline':False]
['text':'     q_num_heads = query.size(1)','line_number':382,'multiline':False]
['text':'     k_num_heads = key.size(1)','line_number':383,'multiline':False]
['text':'     v_num_heads = value.size(1)','line_number':384,'multiline':False]
['text':'     output_num_heads = max(q_num_heads, k_num_heads, v_num_heads)','line_number':386,'multiline':False]
['text':'     head_dim_qk = query.size(3)','line_number':388,'multiline':False]
['text':'     head_dim_v = value.size(3)','line_number':389,'multiline':False]
['text':'     q_t = query.transpose(1, 2)','line_number':391,'multiline':False]
['text':'     k_t = key.transpose(1, 2)','line_number':392,'multiline':False]
['text':'     v_t = value.transpose(1, 2)','line_number':393,'multiline':False]
['text':'     # Checks in sdp_utils ensure that if {*}_batch_size/{*}_num_heads !=','line_number':395,'multiline':False]
['text':'     # output_batch_size/num_heads then they are 1','line_number':396,'multiline':False]
['text':'     q_batch_size_needs_broadcast = q_batch_size != output_batch_size','line_number':397,'multiline':False]
['text':'     k_batch_size_needs_broadcast = k_batch_size != output_batch_size','line_number':398,'multiline':False]
['text':'     v_batch_size_needs_broadcast = v_batch_size != output_batch_size','line_number':399,'multiline':False]
['text':'     # If {*}_batch_size_needs_broadcast, then','line_number':401,'multiline':False]
['text':'     # (1) max_seqlen_batch_{*} is given by {*}_t.size(1)','line_number':402,'multiline':False]
['text':'     #     this is because needs_broadcast indicates that the batch_size is 1','line_number':403,'multiline':False]
['text':'     #     and hence there is only 1 value for seq_len','line_number':404,'multiline':False]
['text':'     # (2) The cum_seq_lens are given by [0, {*}_t.size(1), 2 * {*}_t.size(1),','line_number':405,'multiline':False]
['text':'     # ..., outut_batch_size * {*}_t.size(1)]','line_number':406,'multiline':False]
['text':'     # (3) Nnz_{*} is given by output_batch_size * {*}_t.size(1)','line_number':407,'multiline':False]
['text':'     if q_batch_size_needs_broadcast or not q_t.is_nested:','line_number':409,'multiline':False]
['text':'         max_seqlen_batch_q = q_t.size(1)','line_number':410,'multiline':False]
['text':'         cumulative_sequence_length_q = torch.arange(','line_number':411,'multiline':False]
['text':'             0,','line_number':412,'multiline':False]
['text':'             (output_batch_size + 1) * max_seqlen_batch_q,','line_number':413,'multiline':False]
['text':'             max_seqlen_batch_q,','line_number':414,'multiline':False]
['text':'             device=q_t.device,','line_number':415,'multiline':False]
['text':'             dtype=torch.int32,','line_number':416,'multiline':False]
['text':'         )','line_number':417,'multiline':False]
['text':'         Nnz_q = output_batch_size * max_seqlen_batch_q','line_number':418,'multiline':False]
['text':'     else:','line_number':419,'multiline':False]
['text':'         (','line_number':420,'multiline':False]
['text':'             cumulative_sequence_length_q,','line_number':421,'multiline':False]
['text':'             max_seqlen_batch_q,','line_number':422,'multiline':False]
['text':'             Nnz_q,','line_number':423,'multiline':False]
['text':'         ) = _cumulative_and_max_seq_len_nnz(q_t)','line_number':424,'multiline':False]
['text':'     if k_batch_size_needs_broadcast and v_batch_size_needs_broadcast:','line_number':426,'multiline':False]
['text':'         assert k_t.size(1) == v_t.size(1)','line_number':427,'multiline':False]
['text':'         max_seqlen_batch_kv = k_t.size(1)','line_number':428,'multiline':False]
['text':'         cumulative_sequence_length_kv = torch.arange(','line_number':429,'multiline':False]
['text':'             0,','line_number':430,'multiline':False]
['text':'             (output_batch_size + 1) * max_seqlen_batch_kv,','line_number':431,'multiline':False]
['text':'             max_seqlen_batch_kv,','line_number':432,'multiline':False]
['text':'             device=k_t.device,','line_number':433,'multiline':False]
['text':'             dtype=torch.int32,','line_number':434,'multiline':False]
['text':'         )','line_number':435,'multiline':False]
['text':'         Nnz_kv = output_batch_size * max_seqlen_batch_kv','line_number':436,'multiline':False]
['text':'     else:','line_number':437,'multiline':False]
['text':'         cumulative_sequence_length_kv, max_seqlen_batch_kv, Nnz_kv = (','line_number':438,'multiline':False]
['text':'             _cumulative_and_max_seq_len_nnz(v_t)','line_number':439,'multiline':False]
['text':'             if k_batch_size_needs_broadcast','line_number':440,'multiline':False]
['text':'             else _cumulative_and_max_seq_len_nnz(k_t)','line_number':441,'multiline':False]
['text':'         )','line_number':442,'multiline':False]
['text':'     q_num_heads_needs_broadcast = q_num_heads != output_num_heads','line_number':444,'multiline':False]
['text':'     k_num_heads_needs_broadcast = k_num_heads != output_num_heads','line_number':445,'multiline':False]
['text':'     v_num_heads_needs_broadcast = v_num_heads != output_num_heads','line_number':446,'multiline':False]
['text':'     if not q_t.is_nested:','line_number':448,'multiline':False]
['text':'         query_buffer_reshaped = q_t.expand(','line_number':449,'multiline':False]
['text':'             output_batch_size, q_t.size(1), output_num_heads, head_dim_qk','line_number':450,'multiline':False]
['text':'         )','line_number':451,'multiline':False]
['text':'         query_buffer_reshaped = query_buffer_reshaped.reshape(','line_number':452,'multiline':False]
['text':'             Nnz_q, output_num_heads, head_dim_qk','line_number':453,'multiline':False]
['text':'         )','line_number':454,'multiline':False]
['text':'     else:','line_number':455,'multiline':False]
['text':'         if not q_t.is_contiguous() and not _is_safe_to_get_storage_as_tensor(q_t):','line_number':456,'multiline':False]
['text':'             q_t = q_t.contiguous()','line_number':457,'multiline':False]
['text':'         # If we are broadcasting then Nnz_q will be the output_batch_size since','line_number':458,'multiline':False]
['text':'         # seq_len is 1','line_number':459,'multiline':False]
['text':'         effective_batch_size_q = (','line_number':460,'multiline':False]
['text':'             output_batch_size if q_batch_size_needs_broadcast else Nnz_q','line_number':461,'multiline':False]
['text':'         )','line_number':462,'multiline':False]
['text':'         query_buffer_reshaped = _view_as_dense(','line_number':463,'multiline':False]
['text':'             q_t, effective_batch_size_q, output_num_heads, head_dim_qk','line_number':464,'multiline':False]
['text':'         )','line_number':465,'multiline':False]
['text':'     # If the physical layout of the NestedTensor's storage','line_number':467,'multiline':False]
['text':'     # is not: batch, {seq_len}, num_heads, head_dim then we need','line_number':468,'multiline':False]
['text':'     # to call contiguous','line_number':469,'multiline':False]
['text':'     if not k_t.is_contiguous() and not _is_safe_to_get_storage_as_tensor(k_t):','line_number':470,'multiline':False]
['text':'         k_t = k_t.contiguous()','line_number':471,'multiline':False]
['text':'     if not v_t.is_contiguous() and not _is_safe_to_get_storage_as_tensor(v_t):','line_number':472,'multiline':False]
['text':'         v_t = v_t.contiguous()','line_number':473,'multiline':False]
['text':'     effective_batch_size_k = (','line_number':475,'multiline':False]
['text':'         output_batch_size if k_batch_size_needs_broadcast else Nnz_kv','line_number':476,'multiline':False]
['text':'     )','line_number':477,'multiline':False]
['text':'     key_buffer_reshaped = _view_as_dense(','line_number':478,'multiline':False]
['text':'         k_t, effective_batch_size_k, output_num_heads, head_dim_qk','line_number':479,'multiline':False]
['text':'     )','line_number':480,'multiline':False]
['text':'     effective_batch_size_v = (','line_number':482,'multiline':False]
['text':'         output_batch_size if v_batch_size_needs_broadcast else Nnz_kv','line_number':483,'multiline':False]
['text':'     )','line_number':484,'multiline':False]
['text':'     value_buffer_reshaped = _view_as_dense(','line_number':485,'multiline':False]
['text':'         v_t, effective_batch_size_v, output_num_heads, head_dim_v','line_number':486,'multiline':False]
['text':'     )','line_number':487,'multiline':False]
['text':'     if not q_batch_size_needs_broadcast:','line_number':489,'multiline':False]
['text':'         output_shape = q_t._size','line_number':490,'multiline':False]
['text':'         if head_dim_v != head_dim_qk:','line_number':491,'multiline':False]
['text':'             output_shape[-1] = head_dim_v','line_number':492,'multiline':False]
['text':'         if q_num_heads_needs_broadcast:','line_number':493,'multiline':False]
['text':'             output_shape[1] = output_num_heads','line_number':494,'multiline':False]
['text':'     else:','line_number':495,'multiline':False]
['text':'         output_shape = torch.empty(3, dtype=torch.int64, device=torch.device("cpu"))','line_number':496,'multiline':False]
['text':'         output_shape[0] = q_t.size(1)','line_number':497,'multiline':False]
['text':'         output_shape[1] = output_num_heads','line_number':498,'multiline':False]
['text':'         output_shape[2] = head_dim_v','line_number':499,'multiline':False]
['text':'     return (','line_number':501,'multiline':False]
['text':'         query_buffer_reshaped,','line_number':502,'multiline':False]
['text':'         key_buffer_reshaped,','line_number':503,'multiline':False]
['text':'         value_buffer_reshaped,','line_number':504,'multiline':False]
['text':'         cumulative_sequence_length_q,','line_number':505,'multiline':False]
['text':'         cumulative_sequence_length_kv,','line_number':506,'multiline':False]
['text':'         max_seqlen_batch_q,','line_number':507,'multiline':False]
['text':'         max_seqlen_batch_kv,','line_number':508,'multiline':False]
['text':'         output_shape,','line_number':509,'multiline':False]
['text':'     )','line_number':510,'multiline':False]
['text':' Query (Batch x Num_heads x {Q_seq_len}  x Dim_per_head)','line_number':514,'multiline':False]
['text':' Key   (Batch x Num_heads x {KV_seq_len} x Dim_per_head)','line_number':515,'multiline':False]
['text':' Value (Batch x Num_heads x {KV_seq_len} x Dim_per_head)','line_number':516,'multiline':False]
['text':' return _sdpa_nested_preprocessing_with_broadcast(query, key, value)','line_number':531,'multiline':False]
['text':' [TODO] K and V have to have the same Nnz, should probably torch_check','line_number':551,'multiline':False]
['text':' assume in order to not iterate over v','line_number':552,'multiline':False]
['text':' If the physical layout of the NestedTensor's storage','line_number':554,'multiline':False]
['text':' is not: batch, {seq_len}, num_heads, head_dim then we need','line_number':555,'multiline':False]
['text':' to call contiguous','line_number':556,'multiline':False]
['text':' FlashAttentionV2 requires that head dimension be a multiple of 8','line_number':589,'multiline':False]
['text':' This was previously done within the kernel, however','line_number':590,'multiline':False]
['text':' This causes the kernel to maybe alias query, key, value','line_number':591,'multiline':False]
['text':' So instead we pad the head_dimensions to be a multiple of 8','line_number':592,'multiline':False]
['text':' in the composite region','line_number':593,'multiline':False]
['text':' TODO: coalesce with torch/nn/utils/attention.py','line_number':604,'multiline':False]
['text':' for mypy, ugh','line_number':626,'multiline':False]
['text':' Special path for non-ragged sequence length (e.g. for SAM where we have a ragged','line_number':633,'multiline':False]
['text':' second batch dim instead). For this case, we can just send the dense buffers through','line_number':634,'multiline':False]
['text':' vanilla SDPA.','line_number':635,'multiline':False]
['text':' We need to calculate the scale based off the OG head dim size','line_number':664,'multiline':False]
['text':' Reshape output to convert nnz to batch_size and seq_len','line_number':696,'multiline':False]
['text':' Reshape output to convert nnz to batch_size and seq_len','line_number':734,'multiline':False]
