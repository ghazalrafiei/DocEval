['text':' noqa: F403','line_number':8,'multiline':False]
['text':' SDPA metadata; max / min seqlens are needed for e.g. flash','line_number':24,'multiline':False]
['text':' type: ignore[assignment]','line_number':30,'multiline':False]
['text':' NOTE [ Singleton ints for ragged sizes and strides ]','line_number':33,'multiline':False]
['text':'','line_number':34,'multiline':False]
['text':' Jagged layout tensors are tensors that represent a n-dim tensor with a','line_number':35,'multiline':False]
['text':' ragged dimension, but are backed by an (n-1)-dim tensor underneath, e.g.,','line_number':36,'multiline':False]
['text':' a jagged tensor with outer shape [B, x, D] is represented internally by a','line_number':37,'multiline':False]
['text':' tensor with shape [sum(x), D] where we introduce what we call a singleton','line_number':38,'multiline':False]
['text':' (or skolem) denoted as "x" here (but sometimes denoted with "*" to','line_number':39,'multiline':False]
['text':' represent the ragged dimension, and sum(x) represents the dim of the inner','line_number':40,'multiline':False]
['text':' tensor or equivalently the sum of all the sizes of the constituent','line_number':41,'multiline':False]
['text':' tensors' varying lengths.','line_number':42,'multiline':False]
['text':'','line_number':43,'multiline':False]
['text':' We also use singleton ints to represent the strides of this tensor.','line_number':44,'multiline':False]
['text':' For example, a jagged tensor with shape [B, x, D] can be strided in two','line_number':45,'multiline':False]
['text':' ways: [xD, D, 1] and [x, 1, sum(x)], where xD represents x multiplied by D','line_number':46,'multiline':False]
['text':' Indicates that the nth dimension is ragged','line_number':49,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':64,'multiline':False]
['text':' dispatch_layout','line_number':77,'multiline':False]
['text':' Only support jagged for now.','line_number':84,'multiline':False]
['text':' Query cache for the symint associated with offsets or lengths','line_number':89,'multiline':False]
['text':' (create a new one if needed).','line_number':90,'multiline':False]
['text':' holds properties that are computed lazily','line_number':115,'multiline':False]
['text':' collapsed ragged dim must always be dynamic','line_number':118,'multiline':False]
['text':' compute & cache','line_number':134,'multiline':False]
['text':' compute & cache','line_number':144,'multiline':False]
['text':' We should implement this in torch/_tensor_str.py instead','line_number':152,'multiline':False]
['text':' SymNodes are not serializable','line_number':163,'multiline':False]
['text':' TODO: Don't guard on this!','line_number':176,'multiline':False]
['text':' Note that we cannot simply check if is_fake(values) because','line_number':193,'multiline':False]
['text':' during aot autograd, FunctionalTensors are not fake but hold','line_number':194,'multiline':False]
['text':' symbolic sizes.','line_number':195,'multiline':False]
['text':' Associate offsets or lengths (possibly fake, possibly functionalized)','line_number':198,'multiline':False]
['text':' with the ragged_size.','line_number':199,'multiline':False]
['text':' Lazy import to avoid circular dependency','line_number':216,'multiline':False]
['text':' Not actually a view!','line_number':240,'multiline':False]
['text':' type: ignore[override]','line_number':243,'multiline':False]
['text':' type: ignore[override]','line_number':250,'multiline':False]
['text':' Not actually a view!','line_number':260,'multiline':False]
['text':' type: ignore[override]','line_number':263,'multiline':False]
['text':' type: ignore[override]','line_number':270,'multiline':False]
['text':' Not actually a view!','line_number':274,'multiline':False]
['text':' NOTE: @jbschlosser is working on making it a view','line_number':275,'multiline':False]
['text':' type: ignore[override]','line_number':278,'multiline':False]
['text':' type: ignore[override]','line_number':286,'multiline':False]
['text':' Need to make it obvious that users should be passing in offsets','line_number':290,'multiline':False]
['text':' noqa: C401','line_number':299,'multiline':False]
['text':' noqa: C401','line_number':303,'multiline':False]
['text':' Check that the NT is representable by the jagged layout.','line_number':308,'multiline':False]
['text':' Jagged layout represents (B, *, D_0, D_1, ..., D_N), where the only','line_number':309,'multiline':False]
['text':' raggedness allowed is for the single dim immediately adjacent to the batch dim.','line_number':310,'multiline':False]
['text':' Set properties appropriately.','line_number':321,'multiline':False]
['text':' Calculate jagged offsets if not provided.','line_number':330,'multiline':False]
['text':' Jagged layout specifies that offsets are stored as int64 on the same device as values.','line_number':332,'multiline':False]
['text':' compute this now since it's easy','line_number':342,'multiline':False]
['text':' type: ignore[return-value]','line_number':346,'multiline':False]
['text':' Calculate jagged offsets','line_number':365,'multiline':False]
['text':' Jagged layout specifies that offsets are stored as int64 on the same device as values.','line_number':373,'multiline':False]
['text':' Reshape buffer to flatten the 1st and 2nd dimension (view used to enforce non-copy)','line_number':381,'multiline':False]
['text':' Check if offsets and lengths make it possibly contiguous and return a regular NT','line_number':387,'multiline':False]
['text':' populate metadata cache with computed seqlen extremes','line_number':408,'multiline':False]
