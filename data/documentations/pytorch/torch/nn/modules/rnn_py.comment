['text':' Second bias vector included for CuDNN compatibility. Only one','line_number':120,'multiline':False]
['text':' bias vector is needed in standard definition.','line_number':121,'multiline':False]
['text':' keep self._flat_weights up to date if you do self.weight = ...','line_number':162,'multiline':False]
['text':' Short-circuits if _flat_weights is only partially instantiated','line_number':173,'multiline':False]
['text':' Short-circuits if any tensor in self._flat_weights is not acceptable to cuDNN','line_number':180,'multiline':False]
['text':' or the tensors in _flat_weights are of different dtypes','line_number':181,'multiline':False]
['text':' If any parameters alias, we fall back to the slower, copying code path. This is','line_number':191,'multiline':False]
['text':' a sufficient check, because overlapping parameter buffers that don't completely','line_number':192,'multiline':False]
['text':' alias would break the assumptions of the uniqueness check in','line_number':193,'multiline':False]
['text':' Module.named_parameters().','line_number':194,'multiline':False]
['text':' Note: no_grad() is necessary since _cudnn_rnn_flatten_weight is','line_number':202,'multiline':False]
['text':' an inplace operation on self._flat_weights','line_number':203,'multiline':False]
['text':' Resets _flat_weights','line_number':218,'multiline':False]
['text':' Note: be v. careful before removing this, as 3rd party device types','line_number':219,'multiline':False]
['text':' likely rely on this behavior to properly .to() modules like LSTM.','line_number':220,'multiline':False]
['text':' Returns True if the weight tensors have changed since the last forward pass.','line_number':262,'multiline':False]
['text':' This is the case when used with torch.func.functional_call(), for example.','line_number':263,'multiline':False]
['text':' If weights have been changed, update the _flat_weights in __getstate__ here.','line_number':306,'multiline':False]
['text':' Don't serialize the weight references.','line_number':308,'multiline':False]
['text':' In PyTorch 1.8 we added a proj_size member variable to LSTM.','line_number':317,'multiline':False]
['text':' LSTMs that were serialized via torch.save(module) before PyTorch 1.8','line_number':318,'multiline':False]
['text':' don't have it, so to preserve compatibility we set proj_size here.','line_number':319,'multiline':False]
['text':' Need to copy these caches, otherwise the replica will share the same','line_number':360,'multiline':False]
['text':' flat weights list.','line_number':361,'multiline':False]
['text':' noqa: F811','line_number':493,'multiline':False]
['text':' noqa: F811','line_number':498,'multiline':False]
['text':' noqa: F811','line_number':502,'multiline':False]
['text':' script() is unhappy when max_batch_size is different type in cond branches, so we duplicate','line_number':511,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':517,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':518,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':545,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':546,'multiline':False]
['text':' XXX: LSTM and GRU implementation is different from RNNBase, this is because:','line_number':584,'multiline':False]
['text':' 1. we want to support nn.LSTM and nn.GRU in TorchScript and TorchScript in','line_number':585,'multiline':False]
['text':'    its current state could not support the python Union Type or Any Type','line_number':586,'multiline':False]
['text':' 2. TorchScript static typing does not allow a Function or Callable type in','line_number':587,'multiline':False]
['text':'    Dict values, so we have to separately call _VF instead of using _rnn_impls','line_number':588,'multiline':False]
['text':' 3. This is temporary only and in the transition state that we want to make it','line_number':589,'multiline':False]
['text':'    on time for the release','line_number':590,'multiline':False]
['text':'','line_number':591,'multiline':False]
['text':' More discussion details in https://github.com/pytorch/pytorch/pull/23266','line_number':592,'multiline':False]
['text':'','line_number':593,'multiline':False]
['text':' TODO: remove the overriding implementations for LSTM and GRU when TorchScript','line_number':594,'multiline':False]
['text':' support expressing these two modules generally.','line_number':595,'multiline':False]
['text':' In the future, we should prevent mypy from applying contravariance rules here.','line_number':782,'multiline':False]
['text':' See torch/nn/modules/module.py::_forward_unimplemented','line_number':783,'multiline':False]
['text':' type: ignore[override]','line_number':784,'multiline':False]
['text':' Same as above, see torch/nn/modules/module.py::_forward_unimplemented','line_number':795,'multiline':False]
['text':' type: ignore[override]','line_number':796,'multiline':False]
['text':' Same as above, see torch/nn/modules/module.py::_forward_unimplemented','line_number':804,'multiline':False]
['text':' type: ignore[override]','line_number':805,'multiline':False]
['text':' noqa: F811','line_number':806,'multiline':False]
['text':' noqa: F811','line_number':808,'multiline':False]
['text':' Same as above, see torch/nn/modules/module.py::_forward_unimplemented','line_number':811,'multiline':False]
['text':' noqa: F811','line_number':813,'multiline':False]
['text':' noqa: F811','line_number':815,'multiline':False]
['text':' noqa: F811','line_number':818,'multiline':False]
['text':' xxx: isinstance check needs to be in conditional for TorchScript to compile','line_number':822,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':839,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':840,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':872,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':873,'multiline':False]
['text':' xxx: isinstance check needs to be in conditional for TorchScript to compile','line_number':885,'multiline':False]
['text':' type: ignore[override]','line_number':1041,'multiline':False]
['text':' noqa: F811','line_number':1042,'multiline':False]
['text':' noqa: F811','line_number':1043,'multiline':False]
['text':' noqa: F811','line_number':1047,'multiline':False]
['text':' noqa: F811','line_number':1048,'multiline':False]
['text':' noqa: F811','line_number':1051,'multiline':False]
['text':' xxx: isinstance check needs to be in conditional for TorchScript to compile','line_number':1055,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':1065,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':1066,'multiline':False]
['text':' Each batch of the hidden state should match the input sequence that','line_number':1094,'multiline':False]
['text':' the user believes he/she is passing in.','line_number':1095,'multiline':False]
['text':' xxx: isinstance check needs to be in conditional for TorchScript to compile','line_number':1108,'multiline':False]
['text':' WARNING: bias_ih and bias_hh purposely not defined here.','line_number':1128,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/39670','line_number':1129,'multiline':False]
['text':' TODO: remove when jit supports exception flow','line_number':1254,'multiline':False]
