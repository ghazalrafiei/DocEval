['text':' noqa: F401','line_number':45,'multiline':False]
['text':' TODO (rohan-varma): keep_low_precision_grads: bool = False','line_number':89,'multiline':False]
['text':' TODO (rohan-varma): APIs to allow users to run batchnorm and layernorm','line_number':90,'multiline':False]
['text':' in full precision. For DDP, this can be implemented by not performing the','line_number':91,'multiline':False]
['text':' parameter cast for BN and LN units.','line_number':92,'multiline':False]
['text':' Do not setup mixed precision for DDP ignored parameters.','line_number':107,'multiline':False]
['text':' _fp_param will point to the full precision param so it can be switched','line_number':119,'multiline':False]
['text':' back to at the end of forward / backward.','line_number':120,'multiline':False]
['text':' Need to return flattened tensors, spec to re-pack them, as well','line_number':130,'multiline':False]
['text':' as if the return type was actually an RRef to reconstruct.','line_number':131,'multiline':False]
['text':' If the current node is the owner of the RRef, unwrap it and try to','line_number':145,'multiline':False]
['text':' find Tensors.','line_number':146,'multiline':False]
['text':' TODO: Expand to remote RRefs.','line_number':147,'multiline':False]
['text':' More NCCL env vars:','line_number':179,'multiline':False]
['text':' Add a DDPSink to run various functions when backwards starts, such as','line_number':234,'multiline':False]
['text':' queueing call back of out-most backward/graph task,','line_number':235,'multiline':False]
['text':' this helps call back is fired after all gradients' calculation','line_number':236,'multiline':False]
['text':' is completed.','line_number':237,'multiline':False]
['text':' set_materialize_grads(False) will ensure that None gradients stay as','line_number':241,'multiline':False]
['text':' None and are not filled with zeros.','line_number':242,'multiline':False]
['text':' Enqueue delay allreduce for static graph training on the first','line_number':252,'multiline':False]
['text':' iteration.','line_number':253,'multiline':False]
['text':' type: ignore[call-arg,misc]','line_number':261,'multiline':False]
['text':' Buckets are rebuilt only once during a training period','line_number':285,'multiline':False]
['text':' Schedule a broadcast if we are syncing module buffers in the','line_number':288,'multiline':False]
['text':' forward pass','line_number':289,'multiline':False]
['text':' TODO: make DDP uneven inputs context manager support buffer','line_number':290,'multiline':False]
['text':' comm hook (https://github.com/pytorch/pytorch/issues/65436)','line_number':291,'multiline':False]
['text':' Check if need to sync in the backward pass','line_number':294,'multiline':False]
['text':' Forward parameter sync is disabled in the next iteration if we','line_number':298,'multiline':False]
['text':' are skipping gradient sync this iteration, so set','line_number':299,'multiline':False]
['text':' `require_forward_param_sync` accordingly','line_number':300,'multiline':False]
['text':' Schedule one allreduce per gradient bucket to match the backward','line_number':305,'multiline':False]
['text':' pass allreduce','line_number':306,'multiline':False]
['text':' Check if we need to allreduce locally unused parameters','line_number':309,'multiline':False]
['text':' Rebuilt parameters are pushed only once during a training period','line_number':313,'multiline':False]
['text':' used to track whether the given thread is inside ddp forward for torchdynamo purposes','line_number':619,'multiline':False]
['text':' For backward compatibility.','line_number':700,'multiline':False]
['text':' This argument is no longer used since the reducer','line_number':756,'multiline':False]
['text':' will ensure reduction completes even if some parameters','line_number':757,'multiline':False]
['text':' do not receive gradients.','line_number':758,'multiline':False]
['text':' Check that a module does not have Uninitialized parameters','line_number':764,'multiline':False]
['text':' used for intra-node param sync and inter-node sync as well','line_number':772,'multiline':False]
['text':' reduction bucket size','line_number':775,'multiline':False]
['text':' Whether to perform input tensor CPU to GPU copies on a side-stream','line_number':777,'multiline':False]
['text':' Initialize gradient buffers and register all reduce hook','line_number':782,'multiline':False]
['text':' Build parameters for reducer.','line_number':795,'multiline':False]
['text':' Verify model equivalence.','line_number':797,'multiline':False]
['text':' Sync params and buffers. Ensures all DDP models start off at the same value.','line_number':799,'multiline':False]
['text':' In debug mode, build a mapping of parameter index -> parameter.','line_number':808,'multiline':False]
['text':' Builds reducer.','line_number':811,'multiline':False]
['text':' Stream used for async low precision copies.','line_number':821,'multiline':False]
['text':' type: ignore[var-annotated]','line_number':823,'multiline':False]
['text':' Add forward pre-hook to root module to kick off copies to lower','line_number':824,'multiline':False]
['text':' precision.','line_number':825,'multiline':False]
['text':' Add forward pre hook to all submodules to wait for copy events','line_number':829,'multiline':False]
['text':' before running computation.','line_number':830,'multiline':False]
['text':' Set up callbacks in backward to upcast and use full precision','line_number':837,'multiline':False]
['text':' params. TODO (rohan-varma): Make this compose with general','line_number':838,'multiline':False]
['text':' comm hooks and apply_optimizer_in_backward. Importing inline to','line_number':839,'multiline':False]
['text':' avoid circular import issue.','line_number':840,'multiline':False]
['text':' Inform reducer of reduced precision param dtype for correctness','line_number':854,'multiline':False]
['text':' of type checks between gradient and bucket.','line_number':855,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':856,'multiline':False]
['text':' type: ignore[union-attr]','line_number':870,'multiline':False]
['text':' 1. Create gradient buffer','line_number':882,'multiline':False]
['text':' 2. Broadcast the parameters','line_number':889,'multiline':False]
['text':' 3. Hook all reduce to the specified parameter','line_number':893,'multiline':False]
['text':' 4. Build tensor views for gradients','line_number':896,'multiline':False]
['text':' 5. Check whether the all reduce of all params requiring grad is delayed.','line_number':905,'multiline':False]
['text':' There is at least a param whose all reduce will not be delayed.','line_number':911,'multiline':False]
['text':' In this case, we should not set self._delay_all_reduce_all_params','line_number':912,'multiline':False]
['text':' to True.','line_number':913,'multiline':False]
['text':' Check if user has used apply_optim_in_backward to overlap optimizer','line_number':918,'multiline':False]
['text':' step + DDP backward. Current constraints:','line_number':919,'multiline':False]
['text':' 1. Only allreduce is supported at the moment, no custom communication.','line_number':920,'multiline':False]
['text':' 2. For DDP-managed parameters that have their optimizer run in','line_number':921,'multiline':False]
['text':' backward, their gradients are set to ``None``. If your use case','line_number':922,'multiline':False]
['text':' requires DDP parameters grad not to be set to ``None`` after their','line_number':923,'multiline':False]
['text':' in-backward optimizer runs, please ping','line_number':924,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/90052.','line_number':925,'multiline':False]
['text':' NOTE: we use self._module_parameters instead of .parameters() since','line_number':926,'multiline':False]
['text':' the former excludes ignored (non-DDP managed) parameters.','line_number':927,'multiline':False]
['text':' Remove hooks that apply_optim_in_backward had registered because','line_number':930,'multiline':False]
['text':' DDP customizes how optimizer is overlapped with backward due to','line_number':931,'multiline':False]
['text':' the allreduce.','line_number':932,'multiline':False]
['text':' Need a weakref to DDP instance to run all_reduce (from reducer)','line_number':940,'multiline':False]
['text':' and get managed DDP parameters.','line_number':941,'multiline':False]
['text':' Note: importing in function, otherwise this will cause a circular','line_number':943,'multiline':False]
['text':' import.','line_number':944,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':956,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':966,'multiline':False]
['text':' Clear out previous iteration submodule to event. This is because we','line_number':976,'multiline':False]
['text':' may have populated some events for modules that didn't end up being','line_number':977,'multiline':False]
['text':' used.','line_number':978,'multiline':False]
['text':' type: ignore[var-annotated]','line_number':979,'multiline':False]
['text':' Do not cast DDP ignored parameters.','line_number':983,'multiline':False]
['text':' copy() implicitly casts to low precision','line_number':987,'multiline':False]
['text':' TODO: when zero_grad(set_to_none=False) or in grad','line_number':990,'multiline':False]
['text':' accumulation case, accumulated grads can be in fp32','line_number':991,'multiline':False]
['text':' which can cause errors when running DDP backwards due','line_number':992,'multiline':False]
['text':' to mismatched incoming and accumulated gradient types.','line_number':993,'multiline':False]
['text':' So we manually cast the accumulated grad down for now,','line_number':994,'multiline':False]
['text':' in the future we may shift to FSDP style gradient','line_number':995,'multiline':False]
['text':' accumulation management where the accumulated gradient','line_number':996,'multiline':False]
['text':' is saved and .grad field is set to None, bypassing','line_number':997,'multiline':False]
['text':' this issue.','line_number':998,'multiline':False]
['text':' type: ignore[union-attr]','line_number':1001,'multiline':False]
['text':' copy event has already been waited on','line_number':1018,'multiline':False]
['text':' Don't register hooks if param does not require grad','line_number':1023,'multiline':False]
['text':' We need to register autograd hook here instead of DDP's ctor','line_number':1026,'multiline':False]
['text':' since we're working with the low precision param. Register them','line_number':1027,'multiline':False]
['text':' via obtaining the gradient accumulator.','line_number':1028,'multiline':False]
['text':' Notice, the parameters order is not in the order in which they are used,','line_number':1059,'multiline':False]
['text':' especially in models with control flow.','line_number':1060,'multiline':False]
['text':'','line_number':1061,'multiline':False]
['text':' Alongside parameters are not presented in the real execution order,','line_number':1062,'multiline':False]
['text':' if a certain model happens to also','line_number':1063,'multiline':False]
['text':'   1) have other collectives comm ops in its backward graph.','line_number':1064,'multiline':False]
['text':'   2) have unused parameter in subset ranks of the whole world.','line_number':1065,'multiline':False]
['text':' bucketing could insert ALL-REDUCE comm op too early on the rank with unused parameter,','line_number':1066,'multiline':False]
['text':' matching up with other collectives comm ops on other ranks unexpectedly.','line_number':1067,'multiline':False]
['text':'','line_number':1068,'multiline':False]
['text':' In order to handle this corner case, when the parameters are not in the real execution order,','line_number':1069,'multiline':False]
['text':' we don't do bucketing, thus only one ALL-REDUCE is inserted after all the gradients','line_number':1070,'multiline':False]
['text':' of the whole graph are computed.','line_number':1071,'multiline':False]
['text':'','line_number':1072,'multiline':False]
['text':' Notice, here we only disable bucketing for the first iteration.','line_number':1073,'multiline':False]
['text':' After the first iteration, it's OK to rebuild buckets,','line_number':1074,'multiline':False]
['text':' because "bucket rebuild" bucketizes parameters based on its real execution order in backward graph.','line_number':1075,'multiline':False]
['text':' Can remove this branching once #73732 is landed.','line_number':1077,'multiline':False]
['text':' Remember index for parameters if we are in mixed precision, as we','line_number':1094,'multiline':False]
['text':' need to pass in index to Reducer's autograd hook via python.','line_number':1095,'multiline':False]
['text':' Note: reverse list of buckets because we want to approximate the','line_number':1100,'multiline':False]
['text':' order in which their gradients are produced, and assume they','line_number':1101,'multiline':False]
['text':' are used in the forward pass in the order they are defined.','line_number':1102,'multiline':False]
['text':' The bucket size limit is specified in the constructor.','line_number':1109,'multiline':False]
['text':' Additionally, we allow for a single small bucket for parameters','line_number':1110,'multiline':False]
['text':' that are defined first, such that their gradients don't spill into','line_number':1111,'multiline':False]
['text':' a much larger bucket, adding unnecessary latency after gradient','line_number':1112,'multiline':False]
['text':' computation finishes. Experiments showed 1MB is a reasonable value.','line_number':1113,'multiline':False]
['text':' User can set dist._DEFAULT_FIRST_BUCKET_BYTES to tune DDP first','line_number':1118,'multiline':False]
['text':' bucket.','line_number':1119,'multiline':False]
['text':' Set as a weak reference to avoid reference cycle between','line_number':1124,'multiline':False]
['text':' logger and reducer.','line_number':1125,'multiline':False]
['text':' Set logging data that can be got during construction time.','line_number':1134,'multiline':False]
['text':' passing a handle to torch.nn.SyncBatchNorm layer','line_number':1144,'multiline':False]
['text':' If serializable, then the process group should be the default one','line_number':1156,'multiline':False]
['text':' In debug mode, build a mapping of parameter index -> parameter.','line_number':1162,'multiline':False]
['text':' Builds reducer.','line_number':1164,'multiline':False]
['text':' Build tuple of (module, parameter) for all parameters that require grads.','line_number':1177,'multiline':False]
['text':' Note that we access module.named_parameters instead of','line_number':1183,'multiline':False]
['text':' parameters(module). parameters(module) is only needed in the','line_number':1184,'multiline':False]
['text':' single-process multi device case, where it accesses replicated','line_number':1185,'multiline':False]
['text':' parameters through _former_parameters.','line_number':1186,'multiline':False]
['text':' Deduplicate any parameters that might be shared across child modules.','line_number':1193,'multiline':False]
['text':' "p not in memo" is the deduplication check.','line_number':1196,'multiline':False]
['text':' "not memo.add(p)" is always True, and it's only there to cause "add(p)" if needed.','line_number':1197,'multiline':False]
['text':' type: ignore[func-returns-value]','line_number':1200,'multiline':False]
['text':' Build list of parameters.','line_number':1203,'multiline':False]
['text':' Checks if a module will produce a sparse gradient.','line_number':1206,'multiline':False]
['text':' Build list of booleans indicating whether or not to expect sparse','line_number':1212,'multiline':False]
['text':' gradients for the corresponding parameters.','line_number':1213,'multiline':False]
['text':' Collect buffers for modules, filtering out buffers that should be ignored.','line_number':1232,'multiline':False]
['text':' Dict[str, tensor] representing module buffers not ignored by DDP.','line_number':1241,'multiline':False]
['text':' Bypass ignored parameters since those are not reduced by DDP','line_number':1253,'multiline':False]
['text':' to begin with.','line_number':1254,'multiline':False]
['text':' Ensure we covered all parameters','line_number':1265,'multiline':False]
['text':' note, this ctxmgr function is marked 'skip' in torchdynamo, so dynamo only kicks in','line_number':1345,'multiline':False]
['text':' for the 'module_to_run' underneath','line_number':1346,'multiline':False]
['text':' see torch._dynamo/eval_frame.py TorchPatcher.patch for more details','line_number':1347,'multiline':False]
['text':' type: ignore[index]','line_number':1359,'multiline':False]
['text':' Making param.grad points to the grad buffers before backward is based on the','line_number':1362,'multiline':False]
['text':' assumption that the grad accumulation is done in place in autograd engine,','line_number':1363,'multiline':False]
['text':' for some edge cases, if the grad accumulation in autograd engine is not in','line_number':1364,'multiline':False]
['text':' place, then the param.grad and grad buffers are detached.','line_number':1365,'multiline':False]
['text':' We batch zero_grad for all params by resetting the whole grad','line_number':1367,'multiline':False]
['text':' buffer when the grad of all params is set to None.','line_number':1368,'multiline':False]
['text':' Initialization for DDP that occurs after construction, but lazily','line_number':1383,'multiline':False]
['text':' before the first forward pass.','line_number':1384,'multiline':False]
['text':' Notify the join context that this process has not joined, if','line_number':1399,'multiline':False]
['text':' needed','line_number':1400,'multiline':False]
['text':' type: ignore[arg-type]','line_number':1404,'multiline':False]
['text':' Calling _rebuild_buckets before forward computation,','line_number':1407,'multiline':False]
['text':' It may allocate new buckets before deallocating old buckets','line_number':1408,'multiline':False]
['text':' inside _rebuild_buckets. To save peak memory usage,','line_number':1409,'multiline':False]
['text':' call _rebuild_buckets before the peak memory usage increases','line_number':1410,'multiline':False]
['text':' during forward computation.','line_number':1411,'multiline':False]
['text':' This should be called only once during whole training period.','line_number':1412,'multiline':False]
['text':' sync params according to location (before/after forward) user','line_number':1417,'multiline':False]
['text':' specified as part of hook, if hook was specified.','line_number':1418,'multiline':False]
['text':' Notify joined ranks whether they should sync in backwards pass or not.','line_number':1423,'multiline':False]
['text':' Cast inputs to reduced precision if needed.','line_number':1434,'multiline':False]
['text':' Cast inputs to reduced precision if needed.','line_number':1443,'multiline':False]
['text':' TODO (rohan-varma) test this codepath.','line_number':1444,'multiline':False]
['text':' sync params according to location (before/after forward) user','line_number':1458,'multiline':False]
['text':' specified as part of hook, if hook was specified.','line_number':1459,'multiline':False]
['text':' We'll return the output object verbatim since it is a freeform','line_number':1465,'multiline':False]
['text':' object. We need to find any tensors in this object, though,','line_number':1466,'multiline':False]
['text':' because we need to figure out which parameters were used during','line_number':1467,'multiline':False]
['text':' this forward pass, to ensure we short circuit reduction for any','line_number':1468,'multiline':False]
['text':' unused parameters. Only if `find_unused_parameters` is set.','line_number':1469,'multiline':False]
['text':' Do not need to populate this for static graph.','line_number':1471,'multiline':False]
['text':' TODO: DDPSink is currently enabled for unused parameter detection and','line_number':1478,'multiline':False]
['text':' static graph training for first iteration.','line_number':1479,'multiline':False]
['text':' Do not touch tensors that have no grad_fn, which can cause issues','line_number':1489,'multiline':False]
['text':' such as https://github.com/pytorch/pytorch/issues/60733','line_number':1490,'multiline':False]
['text':' When find_unused_parameters=True, makes tensors which require grad','line_number':1495,'multiline':False]
['text':' run through the DDPSink backward pass. When not all outputs are','line_number':1496,'multiline':False]
['text':' used in loss, this makes those corresponding tensors receive','line_number':1497,'multiline':False]
['text':' undefined gradient which the reducer then handles to ensure','line_number':1498,'multiline':False]
['text':' param.grad field is not touched and we don't error out.','line_number':1499,'multiline':False]
['text':' Reconstruct output data structure.','line_number':1508,'multiline':False]
['text':' At the end of the forward pass, reset the grad buffer and grad views','line_number':1513,'multiline':False]
['text':' Kept for BC','line_number':1531,'multiline':False]
['text':' When running in join mode, schedules an allreduce to notify joined ranks','line_number':1546,'multiline':False]
['text':' of whether backwards pass synchronization will run this iteration or not.','line_number':1547,'multiline':False]
['text':' When running in join mode, checks and performs sync of module buffers if','line_number':1559,'multiline':False]
['text':' the models have buffers that should be synchronized in the forward pass.','line_number':1560,'multiline':False]
['text':' When running in join model, agrees upon a common rank and broadcast model','line_number':1566,'multiline':False]
['text':' parameters to all other ranks.','line_number':1567,'multiline':False]
['text':' Agree upon the process that will be the authoritative model copy.','line_number':1569,'multiline':False]
['text':' The current rank is a candidate for being the authoritative copy if','line_number':1570,'multiline':False]
['text':' is_last_joiner=True. We break ties via picking the larger rank.','line_number':1571,'multiline':False]
['text':' Schedule comm ops to match those scheduled in the reducer's backward','line_number':1584,'multiline':False]
['text':' pass.','line_number':1585,'multiline':False]
['text':' Schedule comm in the same order as Reducer schedules them, i.e.','line_number':1588,'multiline':False]
['text':' the order of the buckets. Retrieving the bucket order from the reducer','line_number':1589,'multiline':False]
['text':' ensures that we keep the same order in join mode, such as when bucket','line_number':1590,'multiline':False]
['text':' order is rebuilt dynamically.','line_number':1591,'multiline':False]
['text':' Returns grad_buckets in order, but real tensors are substituted with','line_number':1593,'multiline':False]
['text':' zero tensors of the same shape.','line_number':1594,'multiline':False]
['text':' Joined processes contribute zero gradient. In the case that','line_number':1597,'multiline':False]
['text':' divide_by_initial_world_size=True, we divide grads by the static','line_number':1598,'multiline':False]
['text':' world size, if not, the dividing factor is reduced by the number','line_number':1599,'multiline':False]
['text':' of joined processes.','line_number':1600,'multiline':False]
['text':' Allreduces the used parameter mapping across ranks.','line_number':1606,'multiline':False]
['text':' Note: importing in function, otherwise this will cause a circular','line_number':1966,'multiline':False]
['text':' import as optimizer_overlap module needs to import DistributedDataParallel.','line_number':1967,'multiline':False]
['text':' -1 indicates that this rank is not under consideration to be the','line_number':2008,'multiline':False]
['text':' common_rank','line_number':2009,'multiline':False]
['text':' module buffer sync','line_number':2025,'multiline':False]
['text':' Synchronize buffers across processes.','line_number':2026,'multiline':False]
['text':' If we are running DDP with the join manager, we have to agree','line_number':2027,'multiline':False]
['text':' upon a rank to sync module buffers from, since rank 0 may','line_number':2028,'multiline':False]
['text':' already have been joined and have stale module buffers.','line_number':2029,'multiline':False]
['text':' The process with rank 0 is considered the authoritative copy.','line_number':2035,'multiline':False]
['text':' Update self.modules_buffers incase any buffers were','line_number':2037,'multiline':False]
['text':' reassigned.','line_number':2038,'multiline':False]
['text':' This is a workaround to set parameters and buffers DDP should ignore','line_number':2153,'multiline':False]
['text':' during synchronization. It will be removed when the API is finalized','line_number':2154,'multiline':False]
['text':' as part of addressing https://github.com/pytorch/pytorch/issues/43690.','line_number':2155,'multiline':False]
['text':' If self.static_graph has been set, no need to set it again','line_number':2207,'multiline':False]
['text':' Force a rebuild of buckets for a new process group. This ensures all ranks','line_number':2252,'multiline':False]
['text':' are synchronized in terms of when they will rebuild buckets and also','line_number':2253,'multiline':False]
['text':' re-evaluates previous assumptions of buckets given the world size might have','line_number':2254,'multiline':False]
['text':' changed.','line_number':2255,'multiline':False]
