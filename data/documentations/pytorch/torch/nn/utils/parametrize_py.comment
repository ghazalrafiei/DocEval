['text':' We require this because we need to treat differently the first parametrization','line_number':99,'multiline':False]
['text':' This should never throw, unless this class is used from the outside','line_number':100,'multiline':False]
['text':' In plain words:','line_number':107,'multiline':False]
['text':' module.weight must keep its dtype and shape.','line_number':108,'multiline':False]
['text':' Furthermore, if there is no right_inverse or the right_inverse returns a tensor,','line_number':109,'multiline':False]
['text':' this should be of the same dtype as the original tensor','line_number':110,'multiline':False]
['text':'','line_number':111,'multiline':False]
['text':' We check that the following invariants hold:','line_number':112,'multiline':False]
['text':'    X = module.weight','line_number':113,'multiline':False]
['text':'    Y = param.right_inverse(X)','line_number':114,'multiline':False]
['text':'    assert isinstance(Y, Tensor) or','line_number':115,'multiline':False]
['text':'           (isinstance(Y, collections.abc.Sequence) and all(isinstance(t, Tensor) for t in Y))','line_number':116,'multiline':False]
['text':'    Z = param(Y) if isinstance(Y, Tensor) else param(*Y)','line_number':117,'multiline':False]
['text':'    # Consistency checks','line_number':118,'multiline':False]
['text':'    assert X.dtype == Z.dtype and X.shape == Z.shape','line_number':119,'multiline':False]
['text':'    # If it has one input, this allows to be able to use set_ to be able to','line_number':120,'multiline':False]
['text':'    # move data to/from the original tensor without changing its id (which is what the','line_number':121,'multiline':False]
['text':'    # optimizer uses to track parameters)','line_number':122,'multiline':False]
['text':'    if isinstance(Y, Tensor)','line_number':123,'multiline':False]
['text':'      assert X.dtype == Y.dtype','line_number':124,'multiline':False]
['text':' Below we use original = X, new = Y','line_number':125,'multiline':False]
['text':' Compute new','line_number':130,'multiline':False]
['text':' type: ignore[call-overload]','line_number':133,'multiline':False]
['text':' else, or if it throws, we assume that right_inverse is the identity','line_number':139,'multiline':False]
['text':' Set the number of original tensors','line_number':145,'multiline':False]
['text':' Register the tensor(s)','line_number':149,'multiline':False]
['text':' Set the original to original so that the user does not need to re-register the parameter','line_number':157,'multiline':False]
['text':' manually in the optimiser','line_number':158,'multiline':False]
['text':' type: ignore[call-overload]','line_number':160,'multiline':False]
['text':' If the original tensor was a Parameter that required grad, we expect the user to','line_number':169,'multiline':False]
['text':' add the new parameters to the optimizer after registering the parametrization','line_number':170,'multiline':False]
['text':' (this is documented)','line_number':171,'multiline':False]
['text':' Consistency checks:','line_number':178,'multiline':False]
['text':' Since f : A -> B, right_inverse : B -> A, Z and original should live in B','line_number':179,'multiline':False]
['text':' Z = forward(right_inverse(original))','line_number':180,'multiline':False]
['text':' All the exceptions in this function should almost never throw.','line_number':208,'multiline':False]
['text':' They could throw if, for example, right_inverse function returns a different','line_number':209,'multiline':False]
['text':' dtype when given a different input, which should most likely be caused by a','line_number':210,'multiline':False]
['text':' bug in the user's code','line_number':211,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/53103','line_number':214,'multiline':False]
['text':' type: ignore[call-overload]','line_number':215,'multiline':False]
['text':' These exceptions should only throw when a right_inverse function does not','line_number':222,'multiline':False]
['text':' return the same dtype for every input, which should most likely be caused by a bug','line_number':223,'multiline':False]
['text':' We know that the result is going to have the same dtype','line_number':233,'multiline':False]
['text':' type: ignore[call-overload]','line_number':234,'multiline':False]
['text':' Unpack the originals for the first parametrization','line_number':263,'multiline':False]
['text':' It's not possible to call self[1:] here, so we have to be a bit more cryptic','line_number':269,'multiline':False]
['text':' Also we want to skip all non-integer keys','line_number':270,'multiline':False]
['text':' Just emulate a standard deepcopy procedure when __deepcopy__ doesn't exist in the current class.','line_number':290,'multiline':False]
['text':' Also save all slots if they exist.','line_number':297,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':298,'multiline':False]
['text':' We don't allow serialization of parametrized modules but should still allow deepcopying.','line_number':313,'multiline':False]
['text':' Default 'deepcopy' function invokes __deepcopy__ method instead of __getstate__ when it exists.','line_number':314,'multiline':False]
['text':' type: ignore[assignment]','line_number':316,'multiline':False]
['text':' We check the precondition.','line_number':338,'multiline':False]
['text':' This should never fire if register_parametrization is correctly implemented','line_number':339,'multiline':False]
['text':' Scripting','line_number':358,'multiline':False]
['text':' Tracing','line_number':362,'multiline':False]
['text':' If caching is not active, this function just evaluates the parametrization','line_number':367,'multiline':False]
['text':' Correctness checks.','line_number':503,'multiline':False]
['text':' If A is the space of tensors with shape and dtype equal to module.weight','line_number':504,'multiline':False]
['text':' we check that parametrization.forward and parametrization.right_inverse are','line_number':505,'multiline':False]
['text':' functions from A to A','line_number':506,'multiline':False]
['text':' type: ignore[operator]','line_number':528,'multiline':False]
['text':' else right_inverse is assumed to be the identity','line_number':550,'multiline':False]
['text':' add the new parametrization to the parametrization list','line_number':552,'multiline':False]
['text':' Make mypy happy','line_number':553,'multiline':False]
['text':' If unsafe was True in previous parametrization, keep it enabled','line_number':555,'multiline':False]
['text':' type: ignore[index, union-attr]','line_number':556,'multiline':False]
['text':' Set the parametrization mechanism','line_number':558,'multiline':False]
['text':' Fetch the original buffer or parameter','line_number':559,'multiline':False]
['text':' We create this early to check for possible errors','line_number':561,'multiline':False]
['text':' Delete the previous parameter or buffer','line_number':563,'multiline':False]
['text':' If this is the first parametrization registered on the module,','line_number':565,'multiline':False]
['text':' we prepare the module to inject the property','line_number':566,'multiline':False]
['text':' Change the class','line_number':568,'multiline':False]
['text':' Inject a ``ModuleDict`` into the instance under module.parametrizations','line_number':570,'multiline':False]
['text':' Add a property into the class','line_number':572,'multiline':False]
['text':' Add a ParametrizationList','line_number':574,'multiline':False]
['text':' Make mypy happy','line_number':575,'multiline':False]
['text':' Check that there is at least one parametrized buffer or Parameter','line_number':601,'multiline':False]
['text':' Fetch the original tensor','line_number':634,'multiline':False]
['text':' Make mypy happy','line_number':635,'multiline':False]
['text':' We know they have the same dtype because we have checked this when registering the','line_number':642,'multiline':False]
['text':' parametrizations. As such, we can use set_','line_number':643,'multiline':False]
['text':' We do this so that the parameter does not to change the id()','line_number':644,'multiline':False]
['text':' This way the user does not need to update the optimizer','line_number':645,'multiline':False]
['text':' TODO: Fix this for tensor subclasses that are parameters:','line_number':653,'multiline':False]
['text':' RuntimeError: set_storage is not allowed on a Tensor created from .data or .detach().','line_number':654,'multiline':False]
['text':' We cannot use no_grad because we need to know whether one or more','line_number':662,'multiline':False]
['text':' original tensors required grad','line_number':663,'multiline':False]
['text':' We'll have to trust the user to add it to the optimizer','line_number':665,'multiline':False]
['text':' Delete the property that manages the parametrization','line_number':671,'multiline':False]
['text':' Delete the ParametrizationList','line_number':673,'multiline':False]
['text':' Restore the parameter / buffer into the main class','line_number':676,'multiline':False]
['text':' Roll back the parametrized class if no other buffer or parameter','line_number':679,'multiline':False]
['text':' is currently parametrized in this class','line_number':680,'multiline':False]
['text':' Restore class','line_number':683,'multiline':False]
['text':' for mypy','line_number':717,'multiline':False]
['text':' get list of all params or the single param to transfer','line_number':719,'multiline':False]
['text':' for mypy','line_number':724,'multiline':False]
['text':' initialize the to-be-transferred param in to_module if it doesn't exist already','line_number':727,'multiline':False]
['text':' apply the params's parametrizations to to_module','line_number':735,'multiline':False]
['text':' for mypy','line_number':738,'multiline':False]
['text':' make values match, original values can be stored in either original or','line_number':740,'multiline':False]
['text':' original0, original1..., need to check both cases','line_number':741,'multiline':False]
['text':' loop through each original# until all values have been set','line_number':748,'multiline':False]
