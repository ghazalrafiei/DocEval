['text':' A reasonable eps, but not too large','line_number':17,'multiline':False]
['text':' The diagonal of X is the diagonal of R (which is always real) so we normalise by its signs','line_number':29,'multiline':False]
['text':' Note [Householder complex]','line_number':50,'multiline':False]
['text':' For complex tensors, it is not possible to compute the tensor `tau` necessary for','line_number':51,'multiline':False]
['text':' linalg.householder_product from the reflectors.','line_number':52,'multiline':False]
['text':' To see this, note that the reflectors have a shape like:','line_number':53,'multiline':False]
['text':' 0 0 0','line_number':54,'multiline':False]
['text':' * 0 0','line_number':55,'multiline':False]
['text':' * * 0','line_number':56,'multiline':False]
['text':' which, for complex matrices, give n(n-1) (real) parameters. Now, you need n^2 parameters','line_number':57,'multiline':False]
['text':' to parametrize the unitary matrices. Saving tau on its own does not work either, because','line_number':58,'multiline':False]
['text':' not every combination of `(A, tau)` gives a unitary matrix, meaning that if we optimise','line_number':59,'multiline':False]
['text':' them as independent tensors we would not maintain the constraint','line_number':60,'multiline':False]
['text':' An equivalent reasoning holds for rectangular matrices','line_number':61,'multiline':False]
['text':' Here n > k and X is a tall matrix','line_number':76,'multiline':False]
['text':' We just need n x k - k(k-1)/2 parameters','line_number':78,'multiline':False]
['text':' Embed into a square matrix','line_number':81,'multiline':False]
['text':' A is skew-symmetric (or skew-hermitian)','line_number':84,'multiline':False]
['text':' Computes the Cayley retraction (I+A/2)(I-A/2)^{-1}','line_number':88,'multiline':False]
['text':' Q is now orthogonal (or unitary) of size (..., n, n)','line_number':91,'multiline':False]
['text':' Q is now the size of the X (albeit perhaps transposed)','line_number':94,'multiline':False]
['text':' X is real here, as we do not support householder with complex numbers','line_number':96,'multiline':False]
['text':' The diagonal of X is 1's and -1's','line_number':100,'multiline':False]
['text':' We do not want to differentiate through this or update the diagonal of X hence the casting','line_number':101,'multiline':False]
['text':' We always make sure to always copy Q in every path','line_number':123,'multiline':False]
['text':' Note [right_inverse expm cayley]','line_number':125,'multiline':False]
['text':' If we do not have use_trivialization=True, we just implement the inverse of the forward','line_number':126,'multiline':False]
['text':' map for the Householder. To see why, think that for the Cayley map,','line_number':127,'multiline':False]
['text':' we would need to find the matrix X \in R^{n x k} such that:','line_number':128,'multiline':False]
['text':' Y = torch.cat([X.tril(), X.new_zeros(n, n - k).expand(*X.shape[:-2], -1, -1)], dim=-1)','line_number':129,'multiline':False]
['text':' A = Y - Y.mH','line_number':130,'multiline':False]
['text':' cayley(A)[:, :k]','line_number':131,'multiline':False]
['text':' gives the original tensor. It is not clear how to do this.','line_number':132,'multiline':False]
['text':' Perhaps via some algebraic manipulation involving the QR like that of','line_number':133,'multiline':False]
['text':' Corollary 2.2 in Edelman, Arias and Smith?','line_number':134,'multiline':False]
['text':' If parametrization == _OrthMaps.householder, make Q orthogonal via the QR decomposition.','line_number':139,'multiline':False]
['text':' Here Q is always real because we do not support householder and complex matrices.','line_number':140,'multiline':False]
['text':' See note [Householder complex]','line_number':141,'multiline':False]
['text':' We want to have a decomposition X = QR with diag(R) > 0, as otherwise we could','line_number':143,'multiline':False]
['text':' decompose an orthogonal matrix Q as Q = (-Q)@(-Id), which is a valid QR decomposition','line_number':144,'multiline':False]
['text':' The diagonal of Q is the diagonal of R from the qr decomposition','line_number':145,'multiline':False]
['text':' Equality with zero is ok because LAPACK returns exactly zero when it does not want','line_number':147,'multiline':False]
['text':' to use a particular reflection','line_number':148,'multiline':False]
['text':' We check whether Q is orthogonal','line_number':153,'multiline':False]
['text':' Is orthogonal','line_number':156,'multiline':False]
['text':' Complete Q into a full n x n orthogonal matrix','line_number':159,'multiline':False]
['text':' It is necessary to return the -Id, as we use the diagonal for the','line_number':165,'multiline':False]
['text':' Householder parametrization. Using -Id makes:','line_number':166,'multiline':False]
['text':' householder(torch.zeros(m,n)) == torch.eye(m,n)','line_number':167,'multiline':False]
['text':' Poor man's version of eye_like','line_number':168,'multiline':False]
['text':' We could implement this for 1-dim tensors as the maps on the sphere','line_number':269,'multiline':False]
['text':' but I believe it'd bite more people than it'd help','line_number':270,'multiline':False]
['text':' For ndim == 1 we do not need to approximate anything (see _SpectralNorm.forward)','line_number':387,'multiline':False]
['text':' Start with u, v initialized to some reasonable values by performing a number','line_number':397,'multiline':False]
['text':' of iterations of the power method','line_number':398,'multiline':False]
['text':' Precondition','line_number':402,'multiline':False]
['text':' permute dim to front','line_number':406,'multiline':False]
['text':' See original note at torch/nn/utils/spectral_norm.py','line_number':413,'multiline':False]
['text':' NB: If `do_power_iteration` is set, the `u` and `v` vectors are','line_number':414,'multiline':False]
['text':'     updated in power iteration **in-place**. This is very important','line_number':415,'multiline':False]
['text':'     because in `DataParallel` forward, the vectors (being buffers) are','line_number':416,'multiline':False]
['text':'     broadcast from the parallelized module to each module replica,','line_number':417,'multiline':False]
['text':'     which is a new module object created on the fly. And each replica','line_number':418,'multiline':False]
['text':'     runs its own spectral norm power iteration. So simply assigning','line_number':419,'multiline':False]
['text':'     the updated vectors to the module this function runs on will cause','line_number':420,'multiline':False]
['text':'     the update to be lost forever. And the next time the parallelized','line_number':421,'multiline':False]
['text':'     module is replicated, the same randomly initialized vectors are','line_number':422,'multiline':False]
['text':'     broadcast and used!','line_number':423,'multiline':False]
['text':'','line_number':424,'multiline':False]
['text':'     Therefore, to make the change propagate back, we rely on two','line_number':425,'multiline':False]
['text':'     important behaviors (also enforced via tests):','line_number':426,'multiline':False]
['text':'       1. `DataParallel` doesn't clone storage if the broadcast tensor','line_number':427,'multiline':False]
['text':'          is already on correct device; and it makes sure that the','line_number':428,'multiline':False]
['text':'          parallelized module is already on `device[0]`.','line_number':429,'multiline':False]
['text':'       2. If the out tensor in `out=` kwarg has correct shape, it will','line_number':430,'multiline':False]
['text':'          just fill in the values.','line_number':431,'multiline':False]
['text':'     Therefore, since the same power iteration is performed on all','line_number':432,'multiline':False]
['text':'     devices, simply updating the tensors in-place will make sure that','line_number':433,'multiline':False]
['text':'     the module replica on `device[0]` will update the _u vector on the','line_number':434,'multiline':False]
['text':'     parallelized module (by shared storage).','line_number':435,'multiline':False]
['text':'','line_number':436,'multiline':False]
['text':'    However, after we update `u` and `v` in-place, we need to **clone**','line_number':437,'multiline':False]
['text':'    them before using them to normalize the weight. This is to support','line_number':438,'multiline':False]
['text':'    backproping through two forward passes, e.g., the common pattern in','line_number':439,'multiline':False]
['text':'    GAN training: loss = D(real) - D(fake). Otherwise, engine will','line_number':440,'multiline':False]
['text':'    complain that variables needed to do backward for the first forward','line_number':441,'multiline':False]
['text':'    (i.e., the `u` and `v` vectors) are changed in the second forward.','line_number':442,'multiline':False]
['text':' Precondition','line_number':444,'multiline':False]
['text':' Spectral norm of weight equals to `u^T W v`, where `u` and `v`','line_number':448,'multiline':False]
['text':' are the first left and right singular vectors.','line_number':449,'multiline':False]
['text':' This power iteration produces approximations of `u` and `v`.','line_number':450,'multiline':False]
['text':' type: ignore[has-type]','line_number':451,'multiline':False]
['text':' type: ignore[has-type]','line_number':452,'multiline':False]
['text':' type: ignore[has-type]','line_number':454,'multiline':False]
['text':' Faster and more exact path, no need to approximate anything','line_number':458,'multiline':False]
['text':' See above on why we need to clone','line_number':464,'multiline':False]
['text':' The proper way of computing this should be through F.bilinear, but','line_number':467,'multiline':False]
['text':' it seems to have some efficiency issues:','line_number':468,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/58093','line_number':469,'multiline':False]
['text':' we may want to assert here that the passed value already','line_number':474,'multiline':False]
['text':' satisfies constraints','line_number':475,'multiline':False]
