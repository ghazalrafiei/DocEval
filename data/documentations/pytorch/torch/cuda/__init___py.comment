['text':' type: ignore[attr-defined]','line_number':40,'multiline':False]
['text':' don't invoke these until initialization occurs','line_number':47,'multiline':False]
['text':' type: ignore[import]','line_number':54,'multiline':False]
['text':' sometimes a lib is installed but the import fails for some other reason, so we log the error for later','line_number':58,'multiline':False]
['text':' Since seeding is memory-less, only track the latest seed.','line_number':62,'multiline':False]
['text':' Note: `manual_seed_all` followed by `manual_seed` overwrites','line_number':63,'multiline':False]
['text':' the seed on current device. We track the order of **latest**','line_number':64,'multiline':False]
['text':' calls between these two API.','line_number':65,'multiline':False]
['text':' update seed_all to be latest','line_number':73,'multiline':False]
['text':' update seed to be latest','line_number':78,'multiline':False]
['text':' Define dummy _CudaDeviceProperties type if PyTorch was compiled without CUDA','line_number':87,'multiline':False]
['text':' type: ignore[assignment, misc]','line_number':91,'multiline':False]
['text':' Global variables dynamically populated by native code','line_number':113,'multiline':False]
['text':' type: ignore[assignment]','line_number':116,'multiline':False]
['text':' The user has set an env variable to request this availability check that attempts to avoid fork poisoning by','line_number':133,'multiline':False]
['text':' using NVML at the cost of a weaker CUDA availability assessment. Note that if NVML discovery/initialization','line_number':134,'multiline':False]
['text':' fails, this assessment falls back to the default CUDA Runtime API assessment (`cudaGetDeviceCount`)','line_number':135,'multiline':False]
['text':' The default availability inspection never throws and returns 0 if the driver is missing or can't','line_number':138,'multiline':False]
['text':' be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver','line_number':139,'multiline':False]
['text':' API via `cuInit`','line_number':140,'multiline':False]
['text':' Check for ROCm, if true return true, no ROCM_VERSION check required,','line_number':146,'multiline':False]
['text':' since it is supported on AMD GPU archs.','line_number':147,'multiline':False]
['text':' on ROCm we don't want this check','line_number':180,'multiline':False]
['text':' on ROCm we don't want this check','line_number':205,'multiline':False]
['text':' NVIDIA GPU compute architectures are backward compatible within major version','line_number':213,'multiline':False]
['text':' TODO(torch_deploy): this accesses linecache, which attempts to read the','line_number':234,'multiline':False]
['text':' file system to get traceback info. Patch linecache or do something','line_number':235,'multiline':False]
['text':' else here if this ends up being important.','line_number':236,'multiline':False]
['text':' Don't store the actual traceback to avoid memory cycle','line_number':243,'multiline':False]
['text':' We be double-checked locking, boys!  This is OK because','line_number':277,'multiline':False]
['text':' the above test was GIL protected anyway.  The inner test','line_number':278,'multiline':False]
['text':' is for when a thread blocked on some other thread which was','line_number':279,'multiline':False]
['text':' doing the initialization; when they get the lock, they will','line_number':280,'multiline':False]
['text':' find there is nothing left to do.','line_number':281,'multiline':False]
['text':' It is important to prevent other threads from entering _lazy_init','line_number':284,'multiline':False]
['text':' immediately, while we are still guaranteed to have the GIL, because some','line_number':285,'multiline':False]
['text':' of the C calls we make below will release the GIL','line_number':286,'multiline':False]
['text':' This function throws if there's a driver initialization error, no GPUs','line_number':298,'multiline':False]
['text':' are found or any other error occurs','line_number':299,'multiline':False]
['text':' Some of the queued calls may reentrantly call _lazy_init();','line_number':303,'multiline':False]
['text':' we need to just return without initializing in that case.','line_number':304,'multiline':False]
['text':' However, we must not let any *other* threads in!','line_number':305,'multiline':False]
['text':' will define _get_device_properties','line_number':453,'multiline':False]
['text':' type: ignore[name-defined]','line_number':457,'multiline':False]
['text':' Local cur_stream variable for type refinement','line_number':500,'multiline':False]
['text':' Return if stream is None or CUDA device not available','line_number':502,'multiline':False]
['text':' If the stream is not on the current device, then','line_number':507,'multiline':False]
['text':' set the current stream on the device','line_number':508,'multiline':False]
['text':' Local cur_stream variable for type refinement','line_number':515,'multiline':False]
['text':' If stream is None or no CUDA device available, return','line_number':517,'multiline':False]
['text':' Reset the stream on the original device','line_number':521,'multiline':False]
['text':' and destination device','line_number':522,'multiline':False]
['text':' type: ignore[union-attr]','line_number':523,'multiline':False]
['text':' type: ignore[arg-type]','line_number':524,'multiline':False]
['text':' type: ignore[arg-type]','line_number':525,'multiline':False]
['text':' Repeated id results in empty set','line_number':593,'multiline':False]
['text':' Anything other but prefix is ignored','line_number':596,'multiline':False]
['text':' CUDA_VISIBLE_DEVICES uses something like strtoul','line_number':606,'multiline':False]
['text':' which makes `1gpu2,2ampere` is equivalent to `1,2`','line_number':607,'multiline':False]
['text':' Repeated ordinal results in empty set','line_number':611,'multiline':False]
['text':' Negative value aborts the sequence','line_number':614,'multiline':False]
['text':' Ambiguous candidate','line_number':679,'multiline':False]
['text':' First invalid ordinal stops parsing','line_number':688,'multiline':False]
['text':' Duplicates result in empty set','line_number':691,'multiline':False]
['text':' Skip MIG parsing','line_number':708,'multiline':False]
['text':' Trim the list up to a maximum available device','line_number':721,'multiline':False]
['text':' bypass _device_count_nvml() if rocm (not supported)','line_number':756,'multiline':False]
['text':' 0 refers to the temperature sensor for the GPU die.','line_number':960,'multiline':False]
['text':' noqa: F403','line_number':1055,'multiline':False]
['text':' noqa: F403','line_number':1058,'multiline':False]
['text':'###############################################################################','line_number':1060,'multiline':False]
['text':' Define Storage and Tensor classes','line_number':1061,'multiline':False]
['text':'###############################################################################','line_number':1062,'multiline':False]
['text':' type: ignore[misc]','line_number':1065,'multiline':False]
['text':' We may need to call lazy init again if we are a forked child','line_number':1068,'multiline':False]
['text':' del _CudaBase.__new__','line_number':1069,'multiline':False]
['text':' We could use a Protocol here to tell mypy that self has `get_device` method','line_number':1078,'multiline':False]
['text':' but it is only available in the typing module on Python >= 3.8','line_number':1079,'multiline':False]
['text':' or on typing_extensions module on Python >= 3.6','line_number':1080,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1081,'multiline':False]
['text':' type: ignore[misc]','line_number':1082,'multiline':False]
['text':' Typed storage and tensors','line_number':1309,'multiline':False]
