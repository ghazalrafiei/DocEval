['text':' Defines default_factory for GradScaler's _per_optimizer_states defaultdict,','line_number':35,'multiline':False]
['text':' as well as associated "enum" values.  Prefers defining these at top level because','line_number':36,'multiline':False]
['text':' - Lambdas can't be pickled, so we don't want to supply a lambda as the factory.','line_number':37,'multiline':False]
['text':' - Defining READY, UNSCALED, STEPPED and _refresh_per_optimizer_state within GradScaler','line_number':38,'multiline':False]
['text':'   causes a circular reference, which we'd rather avoid.','line_number':39,'multiline':False]
['text':' self._scale will be lazily initialized during the first call to scale()','line_number':138,'multiline':False]
['text':' self._growth_tracker will be lazily initialized during the first call to scale()','line_number':144,'multiline':False]
['text':' Short-circuit for the common case.','line_number':201,'multiline':False]
['text':' Invoke the more complex machinery only if we're treating multiple outputs.','line_number':209,'multiline':False]
['text':' holds a reference that can be overwritten by apply_scale','line_number':212,'multiline':False]
['text':' To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.','line_number':242,'multiline':False]
['text':' There could be hundreds of grads, so we'd like to iterate through them just once.','line_number':243,'multiline':False]
['text':' However, we don't know their devices or dtypes in advance.','line_number':244,'multiline':False]
['text':' https://stackoverflow.com/questions/5029934/defaultdict-of-defaultdict','line_number':246,'multiline':False]
['text':' Google says mypy struggles with defaultdicts type annotations.','line_number':247,'multiline':False]
['text':' is_coalesced() == False means the sparse grad has values with duplicate indices.','line_number':260,'multiline':False]
['text':' coalesce() deduplicates indices and adds all values that have the same index.','line_number':261,'multiline':False]
['text':' For scaled fp16 values, there's a good chance coalescing will cause overflow,','line_number':262,'multiline':False]
['text':' so we should check the coalesced _values().','line_number':263,'multiline':False]
['text':' TODO: is there a way to split by device and dtype without appending in the inner loop?','line_number':270,'multiline':False]
['text':' FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.','line_number':331,'multiline':False]
['text':' This optimizer has customized scale-handling logic, so we can call optimizer.step() directly.','line_number':397,'multiline':False]
['text':' The contract with custom optimizers is that their step() should accept an additional,','line_number':398,'multiline':False]
['text':' optional grad_scaler kwarg.  We append self to the kwargs so the custom optimizer has full information:','line_number':399,'multiline':False]
['text':' it can query its own state, invoke unscale_ on itself, etc','line_number':400,'multiline':False]
['text':' The contract above is being deprecated to avoid introducing `grad_scaler: GradScaler` argument','line_number':401,'multiline':False]
['text':' to `Optimizer.step`. The new behavior is going to add two Tensor attributes of `grad_scale`','line_number':402,'multiline':False]
['text':' and `found_inf` to the passed optimizer so that the optimizer can utilize those','line_number':403,'multiline':False]
['text':' to skip the parameter updates or unscale gradients before updating parameters in','line_number':404,'multiline':False]
['text':' the fused kernel, e.g. `FusedAdamMathFunctor`.','line_number':405,'multiline':False]
['text':' In this behavior, `GradScaler._check_inf_per_device` is called if `OptState.READY`,','line_number':406,'multiline':False]
['text':' while the method is expected to be called by users side, i.e. their optimizers.','line_number':407,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':434,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':437,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':441,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':442,'multiline':False]
['text':' Accept a new user-defined scale.','line_number':490,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':495,'multiline':False]
['text':' Consume shared inf/nan data collected from optimizers to update the scale.','line_number':500,'multiline':False]
['text':' If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.','line_number':501,'multiline':False]
['text':' To prepare for next iteration, clear the data collected from optimizers this iteration.','line_number':524,'multiline':False]
['text':' Pickling _scale and _growth_tracker Tensors directly triggers','line_number':654,'multiline':False]
['text':' "warnings.warn("pickle support for Storage will be removed in 1.5..."','line_number':655,'multiline':False]
['text':' so instead, we set the unpickled instance up to reinitialize them lazily.','line_number':656,'multiline':False]
