['text':' noqa: F401','line_number':26,'multiline':False]
['text':' noqa: F401','line_number':39,'multiline':False]
['text':' noqa: F401','line_number':53,'multiline':False]
['text':' noqa: F401','line_number':67,'multiline':False]
['text':' noqa: F401','line_number':83,'multiline':False]
['text':' noqa: F401','line_number':90,'multiline':False]
['text':' noqa: F401','line_number':93,'multiline':False]
['text':' noqa: F401','line_number':100,'multiline':False]
['text':' noqa: F401','line_number':109,'multiline':False]
['text':' noqa: F401','line_number':117,'multiline':False]
['text':' noqa: F401','line_number':121,'multiline':False]
['text':' This global counter increments every time we compile a graph with','line_number':128,'multiline':False]
['text':' AOTAutograd.  You can use this to correlate runtime error messages','line_number':129,'multiline':False]
['text':' with compile time (e.g., if you get an error at runtime saying','line_number':130,'multiline':False]
['text':' compiled graph 3 failed, you can set a breakpoint at compile time','line_number':131,'multiline':False]
['text':' for this graph number to investigate further at compile time.)','line_number':132,'multiline':False]
['text':'','line_number':133,'multiline':False]
['text':' NB: this is different from get_aot_compilation_context, which tracks','line_number':134,'multiline':False]
['text':' each underlying graph that is compiled.  In contrast, AOT_COUNTER','line_number':135,'multiline':False]
['text':' corresponds to top-level invocations of aot_module/aot_function;','line_number':136,'multiline':False]
['text':' one counter is allocated per entire compiled block (but this block','line_number':137,'multiline':False]
['text':' may involve compiling multiple subgraphs; e.g., for forwards/backwards)','line_number':138,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':141,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':142,'multiline':False]
['text':'','line_number':143,'multiline':False]
['text':' AOT Autograd contains a pretty non-trivial amount of logic to handle edge cases around aliasing and mutation','line_number':144,'multiline':False]
['text':' that are external to the graph (they show up as side effects in some way when you run the graph).','line_number':145,'multiline':False]
['text':'','line_number':146,'multiline':False]
['text':' Take a look at `test_aotdispatch.py TestAOTAutograd.test_input_mutation*` tests for some examples functions','line_number':147,'multiline':False]
['text':' and what they're compiled graphs looks like.','line_number':148,'multiline':False]
['text':' Below is a very long comment detailing several edge cases, and showing how AOT Autograd handles them.','line_number':149,'multiline':False]
['text':'','line_number':150,'multiline':False]
['text':' Note [AOT Autograd: input data mutations]','line_number':151,'multiline':False]
['text':'','line_number':152,'multiline':False]
['text':' If we compile a function that mutates inputs, then those input mutations are real side effects','line_number':153,'multiline':False]
['text':' that a user expects to see after running the compiled graph.','line_number':154,'multiline':False]
['text':' However, the graph that we want to send to a backend needs to be *entirely* functional.','line_number':155,'multiline':False]
['text':' The way we reconcile this difference is that we remove the mutations completely from the graph that we compile','line_number':156,'multiline':False]
['text':' but we update the graph to return (updated_inputs, user_outputs).','line_number':157,'multiline':False]
['text':' In the epilogue that runs after the compiled graph is executed, we copy the updated inputs back to the originals.','line_number':158,'multiline':False]
['text':'','line_number':159,'multiline':False]
['text':' Example: original user code:','line_number':160,'multiline':False]
['text':' def f(x):','line_number':161,'multiline':False]
['text':'     x.mul_(2)','line_number':162,'multiline':False]
['text':'     out = x.mul(3)','line_number':163,'multiline':False]
['text':'     return out','line_number':164,'multiline':False]
['text':'','line_number':165,'multiline':False]
['text':' After AOT Autograd compiles, we end up with a:','line_number':166,'multiline':False]
['text':' (a) compiled graph','line_number':167,'multiline':False]
['text':' (b) autograd.Function.forward() method, that executes the compiled graph','line_number':168,'multiline':False]
['text':' (c) wrapper function, that calls the autograd.Function.forward() and performs the epilogue','line_number':169,'multiline':False]
['text':'','line_number':170,'multiline':False]
['text':' The output of (a, b, c) are all written below.','line_number':171,'multiline':False]
['text':'','line_number':172,'multiline':False]
['text':' def compiled_forward_graph(x):','line_number':173,'multiline':False]
['text':'     x_updated = x.mul(2)','line_number':174,'multiline':False]
['text':'     out = x_updated.mul(3)','line_number':175,'multiline':False]
['text':'     return x_updated, out','line_number':176,'multiline':False]
['text':'','line_number':177,'multiline':False]
['text':' # x_updated gets a gradient in the compiled backward','line_number':178,'multiline':False]
['text':' def compiled_backward_graph(grad_x_updated, grad_out):','line_number':179,'multiline':False]
['text':'     grad_x = ...','line_number':180,'multiline':False]
['text':'     return grad_x','line_number':181,'multiline':False]
['text':'','line_number':182,'multiline':False]
['text':' def autograd.Function.forward(x):','line_number':183,'multiline':False]
['text':'     x_updated, out = compiled_forward_graph(x)','line_number':184,'multiline':False]
['text':'     return x_updated, out','line_number':185,'multiline':False]
['text':'','line_number':186,'multiline':False]
['text':' def compiled_wrapper(x):','line_number':187,'multiline':False]
['text':'     x_updated, out = autograd.Function.apply(x)','line_number':188,'multiline':False]
['text':'     x.copy_(x_updated)','line_number':189,'multiline':False]
['text':'     return out','line_number':190,'multiline':False]
['text':'','line_number':191,'multiline':False]
['text':' Another important thing to note is that updated inputs (due to data mutations) *do* participate','line_number':192,'multiline':False]
['text':' in the compiled backward graph! Since the compiled forward graph gets N extra outputs','line_number':193,'multiline':False]
['text':' (due to updated inputs showing up as graph outputs),','line_number':194,'multiline':False]
['text':' The compiled backward gets an additional N inputs.','line_number':195,'multiline':False]
['text':' That way, during the x.copy_(x_updated) bit in the epilogue, gradients will flow from the updated input','line_number':196,'multiline':False]
['text':' back to the original input.','line_number':197,'multiline':False]
['text':' Note [AOT Autograd: input metadata mutations]','line_number':200,'multiline':False]
['text':'','line_number':201,'multiline':False]
['text':' For the same reason as input mutations, we also don't put input metadata mutations in the graph.','line_number':202,'multiline':False]
['text':' Instead, we return the updated version of the input (a view), and mutate the input's metadata outside of the graph','line_number':203,'multiline':False]
['text':'','line_number':204,'multiline':False]
['text':' Example: original user code:','line_number':205,'multiline':False]
['text':' def f(x):','line_number':206,'multiline':False]
['text':'     x.t_()','line_number':207,'multiline':False]
['text':'     out = x.mul(3)','line_number':208,'multiline':False]
['text':'     return out','line_number':209,'multiline':False]
['text':'','line_number':210,'multiline':False]
['text':' AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):','line_number':211,'multiline':False]
['text':' def compiled_forward_graph(x):','line_number':212,'multiline':False]
['text':'     x_updated = x.t()','line_number':213,'multiline':False]
['text':'     out = x_updated.mul(3)','line_number':214,'multiline':False]
['text':'     return x_updated, out','line_number':215,'multiline':False]
['text':'','line_number':216,'multiline':False]
['text':' # x_updated does *not* get a gradient in the compiled backward','line_number':217,'multiline':False]
['text':' def compiled_backward_graph(grad_out):','line_number':218,'multiline':False]
['text':'     grad_x = ...','line_number':219,'multiline':False]
['text':'     return grad_x','line_number':220,'multiline':False]
['text':'','line_number':221,'multiline':False]
['text':' def autograd.Function.forward(x):','line_number':222,'multiline':False]
['text':'     x_updated, out = compiled_forward_graph(x)','line_number':223,'multiline':False]
['text':'     return x_updated, out','line_number':224,'multiline':False]
['text':'','line_number':225,'multiline':False]
['text':' def compiled_wrapper(x):','line_number':226,'multiline':False]
['text':'     x_updated, out = autograd.Function.apply(x)','line_number':227,'multiline':False]
['text':'     x.as_strided_(x_updated)','line_number':228,'multiline':False]
['text':'     return out','line_number':229,'multiline':False]
['text':' Note [AOT Autograd: outputs aliasing inputs or intermediates!]','line_number':232,'multiline':False]
['text':'','line_number':233,'multiline':False]
['text':' AOT Autograd needs special handling for outputs that alias graph inputs or intermediates!','line_number':234,'multiline':False]
['text':' Why?','line_number':235,'multiline':False]
['text':' (1) autograd.Function.forward() has a limitation, where views that returned in the forward cannot later be mutated.','line_number':236,'multiline':False]
['text':' (2) views don't need to be compiled in the graph anyway - it's cheap to generate them outside of the compiled graph,','line_number':237,'multiline':False]
['text':'     in an epilogue.','line_number':238,'multiline':False]
['text':' For outputs that alias inputs, we do the following:','line_number':239,'multiline':False]
['text':' (a) *still* return the aliased output as a graph output','line_number':240,'multiline':False]
['text':' (b) In the AOT Autograd wrapper/epilogue, we don't return that aliased output. Instead, we use it to regenerate the output.','line_number':241,'multiline':False]
['text':'','line_number':242,'multiline':False]
['text':' For outputs that alias *intermediates*, we do the following:','line_number':243,'multiline':False]
['text':' (a) Return the output in the compiled forward, **and** return it's ._base (a graph intermediates) as an output in the forward','line_number':244,'multiline':False]
['text':' (b) Use (output, graph_intermediate) to regenerate the alias, and return that to the user (instead of the compiled fw output).','line_number':245,'multiline':False]
['text':' You might wonder why we return the aliased output directly in the graph (and making the graph compute it),','line_number':246,'multiline':False]
['text':' only to not return it and instead generate a fresh alias off of the intermediate,','line_number':247,'multiline':False]
['text':' instead of (say) just storing metadata about the size/stride of the output somewhere to generate the alias. There are two reasons:','line_number':248,'multiline':False]
['text':' (1) Getting the actual alias tensor allows us to use view-replay to generate the alias, instead of an as_strided() call','line_number':249,'multiline':False]
['text':' (2) Inductor (and other backends) are free to change the memory format of graph outputs, if it results in better performance.','line_number':250,'multiline':False]
['text':'     This can result in problems if a user later tries to .view() that output expecting it to have one set of strides,','line_number':251,'multiline':False]
['text':'     when it has a different set of strides.','line_number':252,'multiline':False]
['text':'     By including the view op directly in the graph, inductor takes that into account when deciding what memory format','line_number':253,'multiline':False]
['text':'     the graph intermediate should be.','line_number':254,'multiline':False]
['text':'','line_number':255,'multiline':False]
['text':' Another important thing to note is how our traced backward() graph handles aliases.','line_number':256,'multiline':False]
['text':' (this applies to outputs aliasing inputs, outputs aliasing intermediates,','line_number':257,'multiline':False]
['text':'  *and* updated inputs returned in the compiled forward due to metadata-only mutations).','line_number':258,'multiline':False]
['text':' Any outputs that alias (either inputs or intermediates) do NOT participate in the compiled backward graph','line_number':259,'multiline':False]
['text':' It would be wasteful to include them in the compiled backward(), because we regenerate them eagerly','line_number':260,'multiline':False]
['text':' at the end of the forward.','line_number':261,'multiline':False]
['text':'','line_number':262,'multiline':False]
['text':' Example: original user code:','line_number':263,'multiline':False]
['text':' def f(x):','line_number':264,'multiline':False]
['text':'     out1 = x.t()','line_number':265,'multiline':False]
['text':'     intermediate = x.mul(2)','line_number':266,'multiline':False]
['text':'     out2 = intermediate.view(-1)','line_number':267,'multiline':False]
['text':'     return out1, out2','line_number':268,'multiline':False]
['text':'','line_number':269,'multiline':False]
['text':' AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):','line_number':270,'multiline':False]
['text':' def compiled_forward_graph(x):','line_number':271,'multiline':False]
['text':'     out1 = x.t()','line_number':272,'multiline':False]
['text':'     intermediate = x.mul(2)','line_number':273,'multiline':False]
['text':'     out2 = intermediate.view(-1)','line_number':274,'multiline':False]
['text':'     # the compiled graph also returns the intermediate','line_number':275,'multiline':False]
['text':'     return out1, out2, intermediate','line_number':276,'multiline':False]
['text':'','line_number':277,'multiline':False]
['text':' # intermediate gets a gradient in the compiled backward.','line_number':278,'multiline':False]
['text':' # both output aliases (out1 and out2) do not.','line_number':279,'multiline':False]
['text':' def compiled_backward_graph(grad_intermediate):','line_number':280,'multiline':False]
['text':'     grad_x = ...','line_number':281,'multiline':False]
['text':'     return grad_x','line_number':282,'multiline':False]
['text':'','line_number':283,'multiline':False]
['text':' def autograd.Function.forward(x):','line_number':284,'multiline':False]
['text':'     out1, out2, intermediate = compiled_forward_graph(x)','line_number':285,'multiline':False]
['text':'     return out1, out2, intermediate','line_number':286,'multiline':False]
['text':'','line_number':287,'multiline':False]
['text':' def compiled_wrapper(x):','line_number':288,'multiline':False]
['text':'     out1, out2, intermediate = autograd.Function.apply(x)','line_number':289,'multiline':False]
['text':'     # regenerate out1 from the input','line_number':290,'multiline':False]
['text':'     out1_regenerated = out1._view_func(x)','line_number':291,'multiline':False]
['text':'     # regenerate out1 from the intermediate','line_number':292,'multiline':False]
['text':'     out2_regenerated = out2._view_func(intermediate)','line_number':293,'multiline':False]
['text':'     return out1_regenerated, out2_regenerated','line_number':294,'multiline':False]
['text':' Note [AOT Autograd: mutations to inputs that alias other inputs]','line_number':297,'multiline':False]
['text':'','line_number':298,'multiline':False]
['text':' Another edge case that is (only partially) handled today is when an input is mutated, but itself aliases another input.','line_number':299,'multiline':False]
['text':' AOT Autograd needs to **ensure** that functionalization knows that the two inputs are aliased to each other.','line_number':300,'multiline':False]
['text':' That way, when the aliased input is accessed later in the graph, functionalization knows to "update" the alias','line_number':301,'multiline':False]
['text':' given the mutation that occurred.','line_number':302,'multiline':False]
['text':'','line_number':303,'multiline':False]
['text':' This is handled by updating the calling convention: we create a "synthetic base" that becomes a new input','line_number':304,'multiline':False]
['text':' in the compiled function, and we regenerate the original (aliased) inputs directly off of the base','line_number':305,'multiline':False]
['text':' inside of the compiled function.','line_number':306,'multiline':False]
['text':'','line_number':307,'multiline':False]
['text':' This logic is fully encapsulated in aot_wrapper_synthetic_base()','line_number':308,'multiline':False]
['text':'','line_number':309,'multiline':False]
['text':' Example: original user code:','line_number':310,'multiline':False]
['text':' def f(x, x_view):','line_number':311,'multiline':False]
['text':'     x.mul_(2)','line_number':312,'multiline':False]
['text':'     out = x * x_view','line_number':313,'multiline':False]
['text':'     return out','line_number':314,'multiline':False]
['text':' f(x, x.view(-1))','line_number':315,'multiline':False]
['text':'','line_number':316,'multiline':False]
['text':' AOT Autograd output (compiled graph, autograd.Function.forward(), wrapper function):','line_number':317,'multiline':False]
['text':' def compiled_forward_graph(base)','line_number':318,'multiline':False]
['text':'     x = generate_x(base)','line_number':319,'multiline':False]
['text':'     x_view = generate_x_view(base)','line_number':320,'multiline':False]
['text':'     x_updated = x.mul(2)','line_number':321,'multiline':False]
['text':'     x_view_updated = x_updated.view(-1)','line_number':322,'multiline':False]
['text':'     out = x_updated * x_view_updated','line_number':323,'multiline':False]
['text':'     return x_updated, out','line_number':324,'multiline':False]
['text':'','line_number':325,'multiline':False]
['text':' # The calling convention change from (aliases) -> (base) happens','line_number':326,'multiline':False]
['text':' # *outside* of the autograd.Function.forward().','line_number':327,'multiline':False]
['text':' # That means the forward() only has 1 input (base),','line_number':328,'multiline':False]
['text':' # and the backward() only has 1 output (grad_base)','line_number':329,'multiline':False]
['text':' def compiled_backward_graph(grad_out):','line_number':330,'multiline':False]
['text':'     grad_base = ...','line_number':331,'multiline':False]
['text':'     return grad_base','line_number':332,'multiline':False]
['text':'','line_number':333,'multiline':False]
['text':' def autograd.Function.forward(base):','line_number':334,'multiline':False]
['text':'     x_updated, out = compiled_forward_graph(base)','line_number':335,'multiline':False]
['text':'     return x_updated, out','line_number':336,'multiline':False]
['text':'','line_number':337,'multiline':False]
['text':' # The compiled wrapper is where we create synthetic bases.','line_number':338,'multiline':False]
['text':' # The info on which inputs are mutated is also tracked *before* synthetic base creation.','line_number':339,'multiline':False]
['text':' def compiled_wrapper(x, x_view):','line_number':340,'multiline':False]
['text':'     base = merge_view_inputs(x, x_view)','line_number':341,'multiline':False]
['text':'     x_updated, out = autograd.Function.apply(base)','line_number':342,'multiline':False]
['text':'     # x and x_view are aliased in eager mode, so this mutation to x will automatically affect x_view.','line_number':343,'multiline':False]
['text':'     x.copy_(x_updated)','line_number':344,'multiline':False]
['text':'     return out','line_number':345,'multiline':False]
['text':' Note [AOT Autograd: Views to avoid tangents aliasing inputs]','line_number':348,'multiline':False]
['text':'','line_number':349,'multiline':False]
['text':' We view every forward output when creating out tangent tensors to handle the problematic','line_number':350,'multiline':False]
['text':' case in which a subclass does extra aliasing between graph outputs/inputs in a way that','line_number':351,'multiline':False]
['text':' is not visible above the sublass.','line_number':352,'multiline':False]
['text':'','line_number':353,'multiline':False]
['text':' Ordinarily, when constructing the joint function that we want to trace in AOTAutograd,','line_number':354,'multiline':False]
['text':' we're guaranteed that the tangent tensors that we pass','line_number':355,'multiline':False]
['text':' into the joint are distinct tensors from the primals. This is because when','line_number':356,'multiline':False]
['text':' decide which forward outputs to create tangents for, we only create tangents','line_number':357,'multiline':False]
['text':' for forward outputs that are not aliases of inputs (See Note','line_number':358,'multiline':False]
['text':' [AOT Autograd: outputs aliasing inputs or intermediates!]).','line_number':359,'multiline':False]
['text':'','line_number':360,'multiline':False]
['text':' However, when wrapper tensor subclasses enter the picture, it is possible','line_number':361,'multiline':False]
['text':' to have an output of the forward that is a subclass that is not an','line_number':362,'multiline':False]
['text':' input / alias of an input, but one of its inner tensors is an alias!','line_number':363,'multiline':False]
['text':' NestedTensor is an example: Performing an out-of-place pointwise op on a','line_number':364,'multiline':False]
['text':' NestedTensor constructs a fresh NestedTensor that holds onto the input's','line_number':365,'multiline':False]
['text':' offsets tensor directly.','line_number':366,'multiline':False]
['text':'','line_number':367,'multiline':False]
['text':' Having tangent tensors that are the same as the (primal) forward inputs,','line_number':368,'multiline':False]
['text':' can cause problems during tracing as make_fx() will specialize on our','line_number':369,'multiline':False]
['text':' duplicate inputs: If we passed in the same tensor for primals_1 and','line_number':370,'multiline':False]
['text':' tangents_1 during tracing, make_fx() will happily sub out all usages of','line_number':371,'multiline':False]
['text':' tangents_1 with primals_1 in the graph, which is not what we want.','line_number':372,'multiline':False]
['text':'','line_number':373,'multiline':False]
['text':' To work around this, we view every forward output when creating out tangent','line_number':374,'multiline':False]
['text':' tensors so that tangents can never be the same as forward inputs even if','line_number':375,'multiline':False]
['text':' forward inputs alias forward outputs.','line_number':376,'multiline':False]
['text':'','line_number':377,'multiline':False]
['text':'','line_number':378,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':379,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':380,'multiline':False]
['text':' This is the main entry point.','line_number':411,'multiline':False]
['text':' TODO: Chillee argues that dynamo itself should pass in fake tensors to','line_number':412,'multiline':False]
['text':' the list of arguments when compiling; at the moment we do not do this','line_number':413,'multiline':False]
['text':' Update the decompositions with functionalized random decompositions','line_number':425,'multiline':False]
['text':' Check flat_args to see if they're already fake.  If so, use that fake','line_number':431,'multiline':False]
['text':' mode instead.','line_number':432,'multiline':False]
['text':' We always specialize on scalar values in export.','line_number':454,'multiline':False]
['text':' see note [Tensor Fakification and Symbol Caching]','line_number':475,'multiline':False]
['text':' TODO: Ensure that this codepath is never exercised from','line_number':487,'multiline':False]
['text':' Dynamo','line_number':488,'multiline':False]
['text':' Patch set_rng_state as set_rng_state with fake tensors is','line_number':504,'multiline':False]
['text':' nonsensical. This does not affect the collection of metadata.','line_number':505,'multiline':False]
['text':' We realized that none of the outputs require grad,','line_number':516,'multiline':False]
['text':' so we actually have an inference graph.','line_number':517,'multiline':False]
['text':' A bit silly: right now in the subclass codepath, our ViewAndMutationMeta','line_number':519,'multiline':False]
['text':' changes depending on whether we pass in is_train / keep_input_mutations,','line_number':520,'multiline':False]
['text':' so we're forced to recompute the metadata.','line_number':521,'multiline':False]
['text':' TODO: refactor the subclass path of run_functionalized_fw_and_collect_metadata','line_number':522,'multiline':False]
['text':' so that this is unnecessary.','line_number':523,'multiline':False]
['text':' aot_export: ban input metadata mutations for now to keep shared code paths simpler.','line_number':552,'multiline':False]
['text':' Keeping .resize_() in the graph will require some work','line_number':553,'multiline':False]
['text':' Allowing it but keeping the graph functional will require some calling convention changes.','line_number':554,'multiline':False]
['text':' In export, banning data mutations on inputs that require grad for now.','line_number':561,'multiline':False]
['text':' This should be rare, and is tricky to get right. When we trace the backward,','line_number':562,'multiline':False]
['text':' we currently trace with autograd.grad instead of .backward(), which makes it difficult','line_number':563,'multiline':False]
['text':' to ensure that we run autograd all the way through the input **before** it saw the mutation.','line_number':564,'multiline':False]
['text':' Need to decide on a strategy for functionalized RNG: toggling via global config seems bad,','line_number':576,'multiline':False]
['text':' and turning it on will require a non-trivial calling convention change for any export runtime.','line_number':577,'multiline':False]
['text':' crappy version of dispatcher','line_number':583,'multiline':False]
['text':' TODO: Do this properly','line_number':584,'multiline':False]
['text':' For now, aot_dispatch_autograd knows to explicitly return a graph','line_number':586,'multiline':False]
['text':' when run with export, and an opaque callable otherwise.','line_number':587,'multiline':False]
['text':' In theory we could factor these out, but I wanted to let the dust','line_number':588,'multiline':False]
['text':' settle on how functionalized rng fits into export first.','line_number':589,'multiline':False]
['text':' aot_dispatch_base_graph contains only the "graph bits", while aot_dispatch_base','line_number':592,'multiline':False]
['text':' includes some extra work around handling a runtime epilogue.','line_number':593,'multiline':False]
['text':' You can put more passes here','line_number':598,'multiline':False]
['text':' During export, we don't get back a callable - we get back the raw fx graph','line_number':614,'multiline':False]
['text':' (either a joint or an inference-only graph)','line_number':615,'multiline':False]
['text':' Whether or not to trace with dynamic shapes','line_number':635,'multiline':False]
['text':' Now flatten the tensor args','line_number':715,'multiline':False]
['text':' Compile the function and save it in the cache','line_number':718,'multiline':False]
['text':' See Note: [Fake Modules and AOTAutograd]','line_number':758,'multiline':False]
['text':' First, the params','line_number':826,'multiline':False]
['text':' Then, the params 1:1 mapped sources, if relevant.','line_number':833,'multiline':False]
['text':' We now know this came from dynamo, and (1) we care about guards,','line_number':836,'multiline':False]
['text':' so setting up aot_autograd_arg_pos_to_source for downstream dedup guards','line_number':837,'multiline':False]
['text':' can now be done safely. (2) Dynamo logic protects the 1:1 sizing below.','line_number':838,'multiline':False]
['text':' Next, the input args','line_number':846,'multiline':False]
['text':' Non dynamo entrypoints can get to here...','line_number':850,'multiline':False]
['text':' ... but not here!','line_number':854,'multiline':False]
['text':' TODO: There is something deeply wrong here; compiled_fn running with','line_number':893,'multiline':False]
['text':' the boxed calling convention, but aot_module_simplified somehow','line_number':894,'multiline':False]
['text':' historically returned a function that was not the boxed calling','line_number':895,'multiline':False]
['text':' convention.  This should get fixed...','line_number':896,'multiline':False]
['text':' Just for convenience','line_number':903,'multiline':False]
['text':' If true, we'll return a joint forward-backward graph,','line_number':915,'multiline':False]
['text':' As well as metadata on the loss + gradients in the backward.','line_number':916,'multiline':False]
['text':' If trace_joint is True, we expect your module to return a scalar loss.','line_number':918,'multiline':False]
['text':' Your module can return multiple outputs, so you must specify which output the loss is.','line_number':919,'multiline':False]
['text':' This helper effectively just adds some extra asserts about what the backward will look like:','line_number':967,'multiline':False]
['text':' Outputs must include a scalar loss, that we compute gradients w.r.t.','line_number':968,'multiline':False]
['text':' We don't compute gradients w.r.t. anything else: so just in case we detach()','line_number':969,'multiline':False]
['text':' and other output tensors.','line_number':970,'multiline':False]
['text':' We only want to create a backward graph w.r.t. the loss that the user passed in.','line_number':984,'multiline':False]
['text':' This implies that every other output should not require gradients.','line_number':985,'multiline':False]
['text':' Instead of making this an error (and forcing the user to detach all other outputs','line_number':986,'multiline':False]
['text':' of their forward),','line_number':987,'multiline':False]
['text':' we'll automatically detach them here.','line_number':988,'multiline':False]
['text':' Run under no_grad, so our tracing machinery only traces an inference graph.','line_number':1008,'multiline':False]
['text':' First, the params','line_number':1013,'multiline':False]
['text':' NB: It is REQUIRED that parameters come first, Inductor infers "fixed"','line_number':1014,'multiline':False]
['text':' parameters by looking at the difference in parameter count outside','line_number':1015,'multiline':False]
['text':' and inside AOTAutograd, and assumes the prefix of arguments are fixed','line_number':1016,'multiline':False]
['text':' arguments','line_number':1017,'multiline':False]
['text':' Next, the input args','line_number':1019,'multiline':False]
['text':' The idea here is that the joint graph that AOTAutograd creates has some strict properties:','line_number':1032,'multiline':False]
['text':' (1) It accepts two arguments (primals, tangents), and pytree_flattens them','line_number':1033,'multiline':False]
['text':' (2) It returns a tuple of (fw_outs, gradients)','line_number':1034,'multiline':False]
['text':' This is a very useful convention for anyone who wants to partition the joint graph','line_number':1035,'multiline':False]
['text':' into a separate forward and backward graph.','line_number':1036,'multiline':False]
['text':' However,','line_number':1037,'multiline':False]
['text':' (1) for people exporting a single joint graph, it would be preferable not to have','line_number':1038,'multiline':False]
['text':'     any pytrees in the graph.','line_number':1039,'multiline':False]
['text':' (2) We are guaranteed in the aot_export_module case that the forward outputs a loss,','line_number':1040,'multiline':False]
['text':'     and there are therefore no tangents that are needed to run the joint graph.','line_number':1041,'multiline':False]
['text':' (3) AOTAutograd creates a grad_input for every input in the forward,','line_number':1042,'multiline':False]
['text':'     including None's for inputs that are not grad-requiring tensors.','line_number':1043,'multiline':False]
['text':'     we don't want these in our export graph.','line_number':1044,'multiline':False]
['text':'     and there are therefore no tangents that are needed to run the joint graph.','line_number':1045,'multiline':False]
['text':' This function "fixes" both of the above by removing any tangent inputs,','line_number':1046,'multiline':False]
['text':' and removing pytrees from the original FX graph.','line_number':1047,'multiline':False]
['text':' It looks like the main consequence of this API is that for dynamic shapes,','line_number':1085,'multiline':False]
['text':' it will assume that parms/buffers are static.','line_number':1086,'multiline':False]
['text':' With the new inferred dynamic shapes API, maybe this doesn't matter?','line_number':1087,'multiline':False]
['text':' Run under no_grad, so our tracing machinery only traces an inference graph.','line_number':1111,'multiline':False]
['text':' At this point, we can just directly return the (joint or inference graph) that we traced.','line_number':1120,'multiline':False]
['text':' First though: a bunch of assertions to make sure that our graph doesn't require','line_number':1121,'multiline':False]
['text':' any calling convention changes compared to the original function.','line_number':1122,'multiline':False]
['text':' These restrictions are *in addition to* the general restrictions on export.','line_number':1123,'multiline':False]
['text':' No input mutations','line_number':1125,'multiline':False]
['text':' No output aliasing','line_number':1128,'multiline':False]
['text':' No pytrees','line_number':1131,'multiline':False]
['text':' TODO: we might have to temporarily patch config.functionalize_rng','line_number':1140,'multiline':False]
['text':' so that it doesn't run when we're exporting a higher order op.','line_number':1141,'multiline':False]
['text':' Smoke test that after partitioning, we can run the forward without any calling convention changes.','line_number':1144,'multiline':False]
['text':' Attempt to run the fw_module with the original user inputs','line_number':1148,'multiline':False]
['text':' Private for now because we aren't providing a contract on what to return','line_number':1156,'multiline':False]
['text':' for joint graphs (we could when there's a clearer use case)','line_number':1157,'multiline':False]
['text':' In the future, we may need to add more export API's that provide their own strong guarantees.','line_number':1158,'multiline':False]
['text':' This is meant as a general helper function for handling various export-y use cases.','line_number':1159,'multiline':False]
['text':' If we're exporting a joint graph and we don't want any tangent inputs in the graph','line_number':1166,'multiline':False]
['text':' (because we are backpropping through a scalar 1 loss),','line_number':1167,'multiline':False]
['text':' we need to explicitly specify not to include tangents in the graph.','line_number':1168,'multiline':False]
['text':' It's not enough just to check that our tangent is a scalar, since we also','line_number':1169,'multiline':False]
['text':' need to know if it is a 1 (no need to make it a graph input), or something else','line_number':1170,'multiline':False]
['text':' (requiring it to be a graph input).','line_number':1171,'multiline':False]
['text':' We don't know this info at trace time though, so we need to make it an explicit config.','line_number':1172,'multiline':False]
['text':' The export use case doesn't care about several bits of AOTConfig','line_number':1184,'multiline':False]
['text':' (1) compilers (we just export the graph)','line_number':1185,'multiline':False]
['text':' (2) partitioners (export is only full graph, user can partition themselves)','line_number':1186,'multiline':False]
['text':' For now there's no use case involving keeping input mutations in the graph','line_number':1195,'multiline':False]
['text':' (which we can only do in the inference case anyway).','line_number':1196,'multiline':False]
['text':' We can add this later if we need to.','line_number':1197,'multiline':False]
