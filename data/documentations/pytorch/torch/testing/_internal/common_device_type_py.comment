['text':' type: ignore[import]','line_number':25,'multiline':False]
['text':' Note [Writing Test Templates]','line_number':30,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':31,'multiline':False]
['text':'','line_number':32,'multiline':False]
['text':' This note was written shortly after the PyTorch 1.9 release.','line_number':33,'multiline':False]
['text':' If you notice it's out-of-date or think it could be improved then please','line_number':34,'multiline':False]
['text':' file an issue.','line_number':35,'multiline':False]
['text':'','line_number':36,'multiline':False]
['text':' PyTorch has its own framework for instantiating test templates. That is, for','line_number':37,'multiline':False]
['text':'   taking test classes that look similar to unittest or pytest','line_number':38,'multiline':False]
['text':'   compatible test classes and optionally doing the following:','line_number':39,'multiline':False]
['text':'','line_number':40,'multiline':False]
['text':'     - instantiating a version of the test class for each available device type','line_number':41,'multiline':False]
['text':'         (often the CPU, CUDA, and META device types)','line_number':42,'multiline':False]
['text':'     - further instantiating a version of each test that's always specialized','line_number':43,'multiline':False]
['text':'         on the test class's device type, and optionally specialized further','line_number':44,'multiline':False]
['text':'         on datatypes or operators','line_number':45,'multiline':False]
['text':'','line_number':46,'multiline':False]
['text':' This functionality is similar to pytest's parametrize functionality','line_number':47,'multiline':False]
['text':'   (see https://docs.pytest.org/en/6.2.x/parametrize.html), but with considerable','line_number':48,'multiline':False]
['text':'   additional logic that specializes the instantiated test classes for their','line_number':49,'multiline':False]
['text':'   device types (see CPUTestBase and CUDATestBase below), supports a variety','line_number':50,'multiline':False]
['text':'   of composable decorators that allow for test filtering and setting','line_number':51,'multiline':False]
['text':'   tolerances, and allows tests parametrized by operators to instantiate','line_number':52,'multiline':False]
['text':'   only the subset of device type x dtype that operator supports.','line_number':53,'multiline':False]
['text':'','line_number':54,'multiline':False]
['text':' This framework was built to make it easier to write tests that run on','line_number':55,'multiline':False]
['text':'   multiple device types, multiple datatypes (dtypes), and for multiple','line_number':56,'multiline':False]
['text':'   operators. It's also useful for controlling which tests are run. For example,','line_number':57,'multiline':False]
['text':'   only tests that use a CUDA device can be run on platforms with CUDA.','line_number':58,'multiline':False]
['text':'   Let's dive in with an example to get an idea for how it works:','line_number':59,'multiline':False]
['text':'','line_number':60,'multiline':False]
['text':' --------------------------------------------------------','line_number':61,'multiline':False]
['text':' A template class (looks like a regular unittest TestCase)','line_number':62,'multiline':False]
['text':' class TestClassFoo(TestCase):','line_number':63,'multiline':False]
['text':'','line_number':64,'multiline':False]
['text':'   # A template test that can be specialized with a device','line_number':65,'multiline':False]
['text':'   # NOTE: this test case is not runnable by unittest or pytest because it','line_number':66,'multiline':False]
['text':'   #   accepts an extra positional argument, "device", that they do not understand','line_number':67,'multiline':False]
['text':'   def test_bar(self, device):','line_number':68,'multiline':False]
['text':'     pass','line_number':69,'multiline':False]
['text':'','line_number':70,'multiline':False]
['text':' # Function that instantiates a template class and its tests','line_number':71,'multiline':False]
['text':' instantiate_device_type_tests(TestCommon, globals())','line_number':72,'multiline':False]
['text':' --------------------------------------------------------','line_number':73,'multiline':False]
['text':'','line_number':74,'multiline':False]
['text':' In the above code example we see a template class and a single test template','line_number':75,'multiline':False]
['text':'   that can be instantiated with a device. The function','line_number':76,'multiline':False]
['text':'   instantiate_device_type_tests(), called at file scope, instantiates','line_number':77,'multiline':False]
['text':'   new test classes, one per available device type, and new tests in those','line_number':78,'multiline':False]
['text':'   classes from these templates. It actually does this by removing','line_number':79,'multiline':False]
['text':'   the class TestClassFoo and replacing it with classes like TestClassFooCPU','line_number':80,'multiline':False]
['text':'   and TestClassFooCUDA, instantiated test classes that inherit from CPUTestBase','line_number':81,'multiline':False]
['text':'   and CUDATestBase respectively. Additional device types, like XLA,','line_number':82,'multiline':False]
['text':'   (see https://github.com/pytorch/xla) can further extend the set of','line_number':83,'multiline':False]
['text':'   instantiated test classes to create classes like TestClassFooXLA.','line_number':84,'multiline':False]
['text':'','line_number':85,'multiline':False]
['text':' The test template, test_bar(), is also instantiated. In this case the template','line_number':86,'multiline':False]
['text':'   is only specialized on a device, so (depending on the available device','line_number':87,'multiline':False]
['text':'   types) it might become test_bar_cpu() in TestClassFooCPU and test_bar_cuda()','line_number':88,'multiline':False]
['text':'   in TestClassFooCUDA. We can think of the instantiated test classes as','line_number':89,'multiline':False]
['text':'   looking like this:','line_number':90,'multiline':False]
['text':'','line_number':91,'multiline':False]
['text':' --------------------------------------------------------','line_number':92,'multiline':False]
['text':' # An instantiated test class for the CPU device type','line_number':93,'multiline':False]
['text':' class TestClassFooCPU(CPUTestBase):','line_number':94,'multiline':False]
['text':'','line_number':95,'multiline':False]
['text':'   # An instantiated test that calls the template with the string representation','line_number':96,'multiline':False]
['text':'   #   of a device from the test class's device type','line_number':97,'multiline':False]
['text':'   def test_bar_cpu(self):','line_number':98,'multiline':False]
['text':'     test_bar(self, 'cpu')','line_number':99,'multiline':False]
['text':'','line_number':100,'multiline':False]
['text':' # An instantiated test class for the CUDA device type','line_number':101,'multiline':False]
['text':' class TestClassFooCUDA(CUDATestBase):','line_number':102,'multiline':False]
['text':'','line_number':103,'multiline':False]
['text':'   # An instantiated test that calls the template with the string representation','line_number':104,'multiline':False]
['text':'   #   of a device from the test class's device type','line_number':105,'multiline':False]
['text':'   def test_bar_cuda(self):','line_number':106,'multiline':False]
['text':'     test_bar(self, 'cuda:0')','line_number':107,'multiline':False]
['text':' --------------------------------------------------------','line_number':108,'multiline':False]
['text':'','line_number':109,'multiline':False]
['text':' These instantiated test classes ARE discoverable and runnable by both','line_number':110,'multiline':False]
['text':'   unittest and pytest. One thing that may be confusing, however, is that','line_number':111,'multiline':False]
['text':'   attempting to run "test_bar" will not work, despite it appearing in the','line_number':112,'multiline':False]
['text':'   original template code. This is because "test_bar" is no longer discoverable','line_number':113,'multiline':False]
['text':'   after instantiate_device_type_tests() runs, as the above snippet shows.','line_number':114,'multiline':False]
['text':'   Instead "test_bar_cpu" and "test_bar_cuda" may be run directly, or both','line_number':115,'multiline':False]
['text':'   can be run with the option "-k test_bar".','line_number':116,'multiline':False]
['text':'','line_number':117,'multiline':False]
['text':' Removing the template class and adding the instantiated classes requires','line_number':118,'multiline':False]
['text':'   passing "globals()" to instantiate_device_type_tests(), because it','line_number':119,'multiline':False]
['text':'   edits the file's Python objects.','line_number':120,'multiline':False]
['text':'','line_number':121,'multiline':False]
['text':' As mentioned, tests can be additionally parametrized on dtypes or','line_number':122,'multiline':False]
['text':'   operators. Datatype parametrization uses the @dtypes decorator and','line_number':123,'multiline':False]
['text':'   require a test template like this:','line_number':124,'multiline':False]
['text':'','line_number':125,'multiline':False]
['text':' --------------------------------------------------------','line_number':126,'multiline':False]
['text':' # A template test that can be specialized with a device and a datatype (dtype)','line_number':127,'multiline':False]
['text':' @dtypes(torch.float32, torch.int64)','line_number':128,'multiline':False]
['text':' def test_car(self, device, dtype)','line_number':129,'multiline':False]
['text':'   pass','line_number':130,'multiline':False]
['text':' --------------------------------------------------------','line_number':131,'multiline':False]
['text':'','line_number':132,'multiline':False]
['text':' If the CPU and CUDA device types are available this test would be','line_number':133,'multiline':False]
['text':'   instantiated as 4 tests that cover the cross-product of the two dtypes','line_number':134,'multiline':False]
['text':'   and two device types:','line_number':135,'multiline':False]
['text':'','line_number':136,'multiline':False]
['text':'     - test_car_cpu_float32','line_number':137,'multiline':False]
['text':'     - test_car_cpu_int64','line_number':138,'multiline':False]
['text':'     - test_car_cuda_float32','line_number':139,'multiline':False]
['text':'     - test_car_cuda_int64','line_number':140,'multiline':False]
['text':'','line_number':141,'multiline':False]
['text':' The dtype is passed as a torch.dtype object.','line_number':142,'multiline':False]
['text':'','line_number':143,'multiline':False]
['text':' Tests parametrized on operators (actually on OpInfos, more on that in a','line_number':144,'multiline':False]
['text':'   moment...) use the @ops decorator and require a test template like this:','line_number':145,'multiline':False]
['text':' --------------------------------------------------------','line_number':146,'multiline':False]
['text':' # A template test that can be specialized with a device, dtype, and OpInfo','line_number':147,'multiline':False]
['text':' @ops(op_db)','line_number':148,'multiline':False]
['text':' def test_car(self, device, dtype, op)','line_number':149,'multiline':False]
['text':'   pass','line_number':150,'multiline':False]
['text':' --------------------------------------------------------','line_number':151,'multiline':False]
['text':'','line_number':152,'multiline':False]
['text':' See the documentation for the @ops decorator below for additional details','line_number':153,'multiline':False]
['text':'   on how to use it and see the note [OpInfos] in','line_number':154,'multiline':False]
['text':'   common_methods_invocations.py for more details on OpInfos.','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':' A test parametrized over the entire "op_db", which contains hundreds of','line_number':157,'multiline':False]
['text':'   OpInfos, will likely have hundreds or thousands of instantiations. The','line_number':158,'multiline':False]
['text':'   test will be instantiated on the cross-product of device types, operators,','line_number':159,'multiline':False]
['text':'   and the dtypes the operator supports on that device type. The instantiated','line_number':160,'multiline':False]
['text':'   tests will have names like:','line_number':161,'multiline':False]
['text':'','line_number':162,'multiline':False]
['text':'     - test_car_add_cpu_float32','line_number':163,'multiline':False]
['text':'     - test_car_sub_cuda_int64','line_number':164,'multiline':False]
['text':'','line_number':165,'multiline':False]
['text':' The first instantiated test calls the original test_car() with the OpInfo','line_number':166,'multiline':False]
['text':'   for torch.add as its "op" argument, the string 'cpu' for its "device" argument,','line_number':167,'multiline':False]
['text':'   and the dtype torch.float32 for is "dtype" argument. The second instantiated','line_number':168,'multiline':False]
['text':'   test calls the test_car() with the OpInfo for torch.sub, a CUDA device string','line_number':169,'multiline':False]
['text':'   like 'cuda:0' or 'cuda:1' for its "device" argument, and the dtype','line_number':170,'multiline':False]
['text':'   torch.int64 for its "dtype argument."','line_number':171,'multiline':False]
['text':'','line_number':172,'multiline':False]
['text':' In addition to parametrizing over device, dtype, and ops via OpInfos, the','line_number':173,'multiline':False]
['text':'   @parametrize decorator is supported for arbitrary parametrizations:','line_number':174,'multiline':False]
['text':' --------------------------------------------------------','line_number':175,'multiline':False]
['text':' # A template test that can be specialized with a device, dtype, and value for x','line_number':176,'multiline':False]
['text':' @parametrize("x", range(5))','line_number':177,'multiline':False]
['text':' def test_car(self, device, dtype, x)','line_number':178,'multiline':False]
['text':'   pass','line_number':179,'multiline':False]
['text':' --------------------------------------------------------','line_number':180,'multiline':False]
['text':'','line_number':181,'multiline':False]
['text':' See the documentation for @parametrize in common_utils.py for additional details','line_number':182,'multiline':False]
['text':'   on this. Note that the instantiate_device_type_tests() function will handle','line_number':183,'multiline':False]
['text':'   such parametrizations; there is no need to additionally call','line_number':184,'multiline':False]
['text':'   instantiate_parametrized_tests().','line_number':185,'multiline':False]
['text':'','line_number':186,'multiline':False]
['text':' Clever test filtering can be very useful when working with parametrized','line_number':187,'multiline':False]
['text':'   tests. "-k test_car" would run every instantiated variant of the test_car()','line_number':188,'multiline':False]
['text':'   test template, and "-k test_car_add" runs every variant instantiated with','line_number':189,'multiline':False]
['text':'   torch.add.','line_number':190,'multiline':False]
['text':'','line_number':191,'multiline':False]
['text':' It is important to use the passed device and dtype as appropriate. Use','line_number':192,'multiline':False]
['text':'   helper functions like make_tensor() that require explicitly specifying','line_number':193,'multiline':False]
['text':'   the device and dtype so they're not forgotten.','line_number':194,'multiline':False]
['text':'','line_number':195,'multiline':False]
['text':' Test templates can use a variety of composable decorators to specify','line_number':196,'multiline':False]
['text':'   additional options and requirements, some are listed here:','line_number':197,'multiline':False]
['text':'','line_number':198,'multiline':False]
['text':'     - @deviceCountAtLeast(<minimum number of devices to run test with>)','line_number':199,'multiline':False]
['text':'         Passes a list of strings representing all available devices of','line_number':200,'multiline':False]
['text':'         the test class's device type as the test template's "device" argument.','line_number':201,'multiline':False]
['text':'         If there are fewer devices than the value passed to the decorator','line_number':202,'multiline':False]
['text':'         the test is skipped.','line_number':203,'multiline':False]
['text':'     - @dtypes(<list of tuples of dtypes>)','line_number':204,'multiline':False]
['text':'         In addition to accepting multiple dtypes, the @dtypes decorator','line_number':205,'multiline':False]
['text':'         can accept a sequence of tuple pairs of dtypes. The test template','line_number':206,'multiline':False]
['text':'         will be called with each tuple for its "dtype" argument.','line_number':207,'multiline':False]
['text':'     - @onlyNativeDeviceTypes','line_number':208,'multiline':False]
['text':'         Skips the test if the device is not a native device type (currently CPU, CUDA, Meta)','line_number':209,'multiline':False]
['text':'     - @onlyCPU','line_number':210,'multiline':False]
['text':'         Skips the test if the device is not a CPU device','line_number':211,'multiline':False]
['text':'     - @onlyCUDA','line_number':212,'multiline':False]
['text':'         Skips the test if the device is not a CUDA device','line_number':213,'multiline':False]
['text':'     - @onlyMPS','line_number':214,'multiline':False]
['text':'         Skips the test if the device is not a MPS device','line_number':215,'multiline':False]
['text':'     - @skipCPUIfNoLapack','line_number':216,'multiline':False]
['text':'         Skips the test if the device is a CPU device and LAPACK is not installed','line_number':217,'multiline':False]
['text':'     - @skipCPUIfNoMkl','line_number':218,'multiline':False]
['text':'         Skips the test if the device is a CPU device and MKL is not installed','line_number':219,'multiline':False]
['text':'     - @skipCUDAIfNoMagma','line_number':220,'multiline':False]
['text':'         Skips the test if the device is a CUDA device and MAGMA is not installed','line_number':221,'multiline':False]
['text':'     - @skipCUDAIfRocm','line_number':222,'multiline':False]
['text':'         Skips the test if the device is a CUDA device and ROCm is being used','line_number':223,'multiline':False]
['text':' Note [Adding a Device Type]','line_number':226,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':227,'multiline':False]
['text':'','line_number':228,'multiline':False]
['text':' To add a device type:','line_number':229,'multiline':False]
['text':'','line_number':230,'multiline':False]
['text':'   (1) Create a new "TestBase" extending DeviceTypeTestBase.','line_number':231,'multiline':False]
['text':'       See CPUTestBase and CUDATestBase below.','line_number':232,'multiline':False]
['text':'   (2) Define the "device_type" attribute of the base to be the','line_number':233,'multiline':False]
['text':'       appropriate string.','line_number':234,'multiline':False]
['text':'   (3) Add logic to this file that appends your base class to','line_number':235,'multiline':False]
['text':'       device_type_test_bases when your device type is available.','line_number':236,'multiline':False]
['text':'   (4) (Optional) Write setUpClass/tearDownClass class methods that','line_number':237,'multiline':False]
['text':'       instantiate dependencies (see MAGMA in CUDATestBase).','line_number':238,'multiline':False]
['text':'   (5) (Optional) Override the "instantiate_test" method for total','line_number':239,'multiline':False]
['text':'       control over how your class creates tests.','line_number':240,'multiline':False]
['text':'','line_number':241,'multiline':False]
['text':' setUpClass is called AFTER tests have been created and BEFORE and ONLY IF','line_number':242,'multiline':False]
['text':' they are run. This makes it useful for initializing devices and dependencies.','line_number':243,'multiline':False]
['text':' Note [Overriding methods in generic tests]','line_number':246,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':247,'multiline':False]
['text':'','line_number':248,'multiline':False]
['text':' Device generic tests look a lot like normal test classes, but they differ','line_number':249,'multiline':False]
['text':' from ordinary classes in some important ways.  In particular, overriding','line_number':250,'multiline':False]
['text':' methods in generic tests doesn't work quite the way you expect.','line_number':251,'multiline':False]
['text':'','line_number':252,'multiline':False]
['text':'     class TestFooDeviceType(TestCase):','line_number':253,'multiline':False]
['text':'         # Intention is to override','line_number':254,'multiline':False]
['text':'         def assertEqual(self, x, y):','line_number':255,'multiline':False]
['text':'             # This DOESN'T WORK!','line_number':256,'multiline':False]
['text':'             super().assertEqual(x, y)','line_number':257,'multiline':False]
['text':'','line_number':258,'multiline':False]
['text':' If you try to run this code, you'll get an error saying that TestFooDeviceType','line_number':259,'multiline':False]
['text':' is not in scope.  This is because after instantiating our classes, we delete','line_number':260,'multiline':False]
['text':' it from the parent scope.  Instead, you need to hardcode a direct invocation','line_number':261,'multiline':False]
['text':' of the desired subclass call, e.g.,','line_number':262,'multiline':False]
['text':'','line_number':263,'multiline':False]
['text':'     class TestFooDeviceType(TestCase):','line_number':264,'multiline':False]
['text':'         # Intention is to override','line_number':265,'multiline':False]
['text':'         def assertEqual(self, x, y):','line_number':266,'multiline':False]
['text':'             TestCase.assertEqual(x, y)','line_number':267,'multiline':False]
['text':'','line_number':268,'multiline':False]
['text':' However, a less error-prone way of customizing the behavior of TestCase','line_number':269,'multiline':False]
['text':' is to either (1) add your functionality to TestCase and make it toggled','line_number':270,'multiline':False]
['text':' by a class attribute, or (2) create your own subclass of TestCase, and','line_number':271,'multiline':False]
['text':' then inherit from it for your generic test.','line_number':272,'multiline':False]
['text':' Make name plural (e.g. devices / dtypes) if the value is composite.','line_number':289,'multiline':False]
['text':' Clear out old entries of the arg if any.','line_number':292,'multiline':False]
['text':' Leave param_kwargs as-is when value is None.','line_number':303,'multiline':False]
['text':' Flag to disable test suite early due to unrecoverable error such as CUDA error.','line_number':309,'multiline':False]
['text':' Precision is a thread-local setting since it may be overridden per test','line_number':312,'multiline':False]
['text':' Returns a string representing the device that single device tests should use.','line_number':333,'multiline':False]
['text':' Note: single device tests use this device exclusively.','line_number':334,'multiline':False]
['text':' For CUDATestBase, XLATestBase, and possibly others, the primary device won't be available','line_number':344,'multiline':False]
['text':' until setUpClass() sets it. Call that manually here if needed.','line_number':345,'multiline':False]
['text':' Returns a list of strings representing all available devices of this','line_number':350,'multiline':False]
['text':' device type. The primary device must be the first string in the list','line_number':351,'multiline':False]
['text':' and the list must contain no duplicates.','line_number':352,'multiline':False]
['text':' Note: UNSTABLE API. Will be replaced once PyTorch has a device generic','line_number':353,'multiline':False]
['text':'   mechanism of acquiring all available devices.','line_number':354,'multiline':False]
['text':' Returns the dtypes the test has requested.','line_number':359,'multiline':False]
['text':' Prefers device-specific dtype specifications over generic ones.','line_number':360,'multiline':False]
['text':' Creates device-specific tests.','line_number':389,'multiline':False]
['text':' Add the device param kwarg if the test needs device or devices.','line_number':394,'multiline':False]
['text':' Apply decorators based on param kwargs.','line_number':403,'multiline':False]
['text':' Constructs the test','line_number':407,'multiline':False]
['text':' Sets precision and runs test','line_number':410,'multiline':False]
['text':' Note: precision is reset after the test is run','line_number':411,'multiline':False]
['text':' check if rte should stop entire test suite.','line_number':418,'multiline':False]
['text':' Check if test has been decorated with `@expectedFailure`','line_number':420,'multiline':False]
['text':' Using `__unittest_expecting_failure__` attribute, see','line_number':421,'multiline':False]
['text':' https://github.com/python/cpython/blob/ffa505b580464/Lib/unittest/case.py#L164','line_number':422,'multiline':False]
['text':' In that case, make it fail with "unexpected success" by suppressing exception','line_number':423,'multiline':False]
['text':' raise the runtime error as is for the test suite to record.','line_number':428,'multiline':False]
['text':' By default, no parametrization is needed.','line_number':440,'multiline':False]
['text':' Parametrization decorators set the parametrize_fn attribute on the test.','line_number':443,'multiline':False]
['text':' If one of the @dtypes* decorators is present, also parametrize over the dtypes set by it.','line_number':446,'multiline':False]
['text':' Note that an empty test suffix is set here so that the dtype can be appended','line_number':455,'multiline':False]
['text':' later after the device.','line_number':456,'multiline':False]
['text':' Instantiate the parametrized tests.','line_number':461,'multiline':False]
['text':' noqa: B020','line_number':462,'multiline':False]
['text':' Note: device and dtype suffix placement','line_number':466,'multiline':False]
['text':' Special handling here to place dtype(s) after device according to test name convention.','line_number':467,'multiline':False]
['text':' Early terminate test if _stop_test_suite is set.','line_number':478,'multiline':False]
['text':' No critical error should stop CPU test suite','line_number':486,'multiline':False]
['text':' has_magma shows up after cuda is initialized','line_number':518,'multiline':False]
['text':' Determines if cuDNN is available and its version','line_number':522,'multiline':False]
['text':' Acquires the current device as the primary (test) device','line_number':526,'multiline':False]
['text':' See Note [Lazy Tensor tests in device agnostic testing]','line_number':529,'multiline':False]
['text':' Need to connect the TS backend to lazy key before running tests','line_number':544,'multiline':False]
['text':' currently only one device is supported on MPS backend','line_number':558,'multiline':False]
['text':' Adds available device-type-specific test base classes','line_number':595,'multiline':False]
['text':' set type to List[Any] due to mypy list-of-union issue:','line_number':597,'multiline':False]
['text':' https://github.com/python/mypy/issues/3351','line_number':598,'multiline':False]
['text':' Skip if sanitizer is enabled','line_number':603,'multiline':False]
['text':' Disable MPS testing in generic device testing temporarily while we're','line_number':616,'multiline':False]
['text':' ramping up support.','line_number':617,'multiline':False]
['text':' elif torch.backends.mps.is_available():','line_number':618,'multiline':False]
['text':'   test_bases.append(MPSTestBase)','line_number':619,'multiline':False]
['text':' device type cannot appear in both except_for and only_for','line_number':627,'multiline':False]
['text':' Note [How to extend DeviceTypeTestBase to add new test device]','line_number':641,'multiline':False]
['text':' The following logic optionally allows downstream projects like pytorch/xla to','line_number':642,'multiline':False]
['text':' add more test devices.','line_number':643,'multiline':False]
['text':' Instructions:','line_number':644,'multiline':False]
['text':'  - Add a python file (e.g. pytorch/xla/test/pytorch_test_base.py) in downstream project.','line_number':645,'multiline':False]
['text':'    - Inside the file, one should inherit from `DeviceTypeTestBase` class and define','line_number':646,'multiline':False]
['text':'      a new DeviceTypeTest class (e.g. `XLATestBase`) with proper implementation of','line_number':647,'multiline':False]
['text':'      `instantiate_test` method.','line_number':648,'multiline':False]
['text':'    - DO NOT import common_device_type inside the file.','line_number':649,'multiline':False]
['text':'      `runpy.run_path` with `globals()` already properly setup the context so that','line_number':650,'multiline':False]
['text':'      `DeviceTypeTestBase` is already available.','line_number':651,'multiline':False]
['text':'    - Set a top-level variable `TEST_CLASS` equal to your new class.','line_number':652,'multiline':False]
['text':'      E.g. TEST_CLASS = XLATensorBase','line_number':653,'multiline':False]
['text':'  - To run tests with new device type, set `TORCH_TEST_DEVICE` env variable to path','line_number':654,'multiline':False]
['text':'    to this file. Multiple paths can be separated by `:`.','line_number':655,'multiline':False]
['text':' See pytorch/xla/test/pytorch_test_base.py for a more detailed example.','line_number':656,'multiline':False]
['text':' runpy (a stdlib module) lacks annotations','line_number':660,'multiline':False]
['text':' type: ignore[func-returns-value]','line_number':661,'multiline':False]
['text':' allow callers to specifically opt tests into being tested on MPS, similar to `include_lazy`','line_number':672,'multiline':False]
['text':' Filter out the device types based on user inputs','line_number':676,'multiline':False]
['text':' Note [Lazy Tensor tests in device agnostic testing]','line_number':679,'multiline':False]
['text':' Right now, test_view_ops.py runs with LazyTensor.','line_number':680,'multiline':False]
['text':' We don't want to opt every device-agnostic test into using the lazy device,','line_number':681,'multiline':False]
['text':' because many of them will fail.','line_number':682,'multiline':False]
['text':' So instead, the only way to opt a specific device-agnostic test file into','line_number':683,'multiline':False]
['text':' lazy tensor testing is with include_lazy=True','line_number':684,'multiline':False]
['text':' Filter out the device types based on environment variables if available','line_number':693,'multiline':False]
['text':' Usage:','line_number':694,'multiline':False]
['text':' export PYTORCH_TESTING_DEVICE_ONLY_FOR=cuda,cpu','line_number':695,'multiline':False]
['text':' export PYTORCH_TESTING_DEVICE_EXCEPT_FOR=xla','line_number':696,'multiline':False]
['text':' Adds 'instantiated' device-specific test cases to the given scope.','line_number':704,'multiline':False]
['text':' The tests in these test cases are derived from the generic tests in','line_number':705,'multiline':False]
['text':' generic_test_class. This function should be used instead of','line_number':706,'multiline':False]
['text':' instantiate_parametrized_tests() if the test class contains','line_number':707,'multiline':False]
['text':' device-specific tests (NB: this supports additional @parametrize usage).','line_number':708,'multiline':False]
['text':'','line_number':709,'multiline':False]
['text':' See note "Writing Test Templates"','line_number':710,'multiline':False]
['text':' Removes the generic test class from its enclosing scope so its tests','line_number':712,'multiline':False]
['text':' are not discoverable.','line_number':713,'multiline':False]
['text':' Creates an 'empty' version of the generic_test_class','line_number':716,'multiline':False]
['text':' Note: we don't inherit from the generic_test_class directly because','line_number':717,'multiline':False]
['text':'   that would add its tests to our test classes and they would be','line_number':718,'multiline':False]
['text':'   discovered (despite not being runnable). Inherited methods also','line_number':719,'multiline':False]
['text':'   can't be removed later, and we can't rely on load_tests because','line_number':720,'multiline':False]
['text':'   pytest doesn't support it (as of this writing).','line_number':721,'multiline':False]
['text':' Acquires members names','line_number':725,'multiline':False]
['text':' See Note [Overriding methods in generic tests]','line_number':726,'multiline':False]
['text':' Creates device-specific test cases','line_number':730,'multiline':False]
['text':' type set to Any and suppressed due to unsupport runtime class:','line_number':734,'multiline':False]
['text':' https://github.com/python/mypy/wiki/Unsupported-Python-Features','line_number':735,'multiline':False]
['text':' Instantiates test member','line_number':739,'multiline':False]
['text':' XLA-compat shim (XLA's instantiate_test takes doesn't take generic_cls)','line_number':741,'multiline':False]
['text':' Instantiates the device-specific tests','line_number':744,'multiline':False]
['text':' Ports non-test member','line_number':748,'multiline':False]
['text':' Mimics defining the instantiated class in the caller's file','line_number':753,'multiline':False]
['text':' by setting its module to the given class's and adding','line_number':754,'multiline':False]
['text':' the module to the given scope.','line_number':755,'multiline':False]
['text':' This lets the instantiated class be discovered by unittest.','line_number':756,'multiline':False]
['text':' Category of dtypes to run an OpInfo-based test for','line_number':761,'multiline':False]
['text':' Example use: @ops(dtype=OpDTypes.supported)','line_number':762,'multiline':False]
['text':'','line_number':763,'multiline':False]
['text':' There are 5 categories:','line_number':764,'multiline':False]
['text':' - supported: Every dtype supported by the operator. Use for exhaustive','line_number':765,'multiline':False]
['text':'              testing of all dtypes.','line_number':766,'multiline':False]
['text':' - unsupported: Run tests on dtypes not supported by the operator. e.g. for','line_number':767,'multiline':False]
['text':'                testing the operator raises an error and doesn't crash.','line_number':768,'multiline':False]
['text':' - supported_backward: Every dtype supported by the operator's backward pass.','line_number':769,'multiline':False]
['text':' - unsupported_backward: Run tests on dtypes not supported by the operator's backward pass.','line_number':770,'multiline':False]
['text':' - any_one: Runs a test for one dtype the operator supports. Prioritizes dtypes the','line_number':771,'multiline':False]
['text':'     operator supports in both forward and backward.','line_number':772,'multiline':False]
['text':' - none: Useful for tests that are not dtype-specific. No dtype will be passed to the test','line_number':773,'multiline':False]
['text':'         when this is selected.','line_number':774,'multiline':False]
['text':' Test all supported dtypes (default)','line_number':776,'multiline':False]
['text':' Test only unsupported dtypes','line_number':777,'multiline':False]
['text':' Test all supported backward dtypes','line_number':778,'multiline':False]
['text':' Test only unsupported backward dtypes','line_number':779,'multiline':False]
['text':' Test precisely one supported dtype','line_number':780,'multiline':False]
['text':' Instantiate no dtype variants (no dtype kwarg needed)','line_number':781,'multiline':False]
['text':' Test precisely one supported dtype that is common to both cuda and cpu','line_number':782,'multiline':False]
['text':' Arbitrary order','line_number':785,'multiline':False]
['text':' NB: For OpInfos, SampleInput.summary() prints in a cleaner way.','line_number':802,'multiline':False]
['text':' Decorator that defines the OpInfos a test template should be instantiated for.','line_number':807,'multiline':False]
['text':'','line_number':808,'multiline':False]
['text':' Example usage:','line_number':809,'multiline':False]
['text':'','line_number':810,'multiline':False]
['text':' @ops(unary_ufuncs)','line_number':811,'multiline':False]
['text':' def test_numerics(self, device, dtype, op):','line_number':812,'multiline':False]
['text':'   <test_code>','line_number':813,'multiline':False]
['text':'','line_number':814,'multiline':False]
['text':' This will instantiate variants of test_numerics for each given OpInfo,','line_number':815,'multiline':False]
['text':' on each device the OpInfo's operator supports, and for every dtype supported by','line_number':816,'multiline':False]
['text':' that operator. There are a few caveats to the dtype rule, explained below.','line_number':817,'multiline':False]
['text':'','line_number':818,'multiline':False]
['text':' The @ops decorator can accept two','line_number':819,'multiline':False]
['text':' additional arguments, "dtypes" and "allowed_dtypes". If "dtypes" is specified','line_number':820,'multiline':False]
['text':' then the test variants are instantiated for those dtypes, regardless of','line_number':821,'multiline':False]
['text':' what the operator supports. If given "allowed_dtypes" then test variants','line_number':822,'multiline':False]
['text':' are instantiated only for the intersection of allowed_dtypes and the dtypes','line_number':823,'multiline':False]
['text':' they would otherwise be instantiated with. That is, allowed_dtypes composes','line_number':824,'multiline':False]
['text':' with the options listed above and below.','line_number':825,'multiline':False]
['text':'','line_number':826,'multiline':False]
['text':' The "dtypes" argument can also accept additional values (see OpDTypes above):','line_number':827,'multiline':False]
['text':'   OpDTypes.supported - the test is instantiated for all dtypes the operator','line_number':828,'multiline':False]
['text':'     supports','line_number':829,'multiline':False]
['text':'   OpDTypes.unsupported - the test is instantiated for all dtypes the operator','line_number':830,'multiline':False]
['text':'     doesn't support','line_number':831,'multiline':False]
['text':'   OpDTypes.supported_backward - the test is instantiated for all dtypes the','line_number':832,'multiline':False]
['text':'     operator's gradient formula supports','line_number':833,'multiline':False]
['text':'   OpDTypes.unsupported_backward - the test is instantiated for all dtypes the','line_number':834,'multiline':False]
['text':'     operator's gradient formula doesn't support','line_number':835,'multiline':False]
['text':'   OpDTypes.any_one - the test is instantiated for one dtype the','line_number':836,'multiline':False]
['text':'     operator supports. The dtype supports forward and backward if possible.','line_number':837,'multiline':False]
['text':'   OpDTypes.none - the test is instantiated without any dtype. The test signature','line_number':838,'multiline':False]
['text':'     should not include a dtype kwarg in this case.','line_number':839,'multiline':False]
['text':'','line_number':840,'multiline':False]
['text':' These options allow tests to have considerable control over the dtypes','line_number':841,'multiline':False]
['text':'   they're instantiated for.','line_number':842,'multiline':False]
['text':' Determine the set of dtypes to use.','line_number':860,'multiline':False]
['text':' Tries to pick a dtype that supports both forward or backward','line_number':873,'multiline':False]
['text':' Tries to pick a dtype that supports both CPU and CUDA','line_number':885,'multiline':False]
['text':' Construct the test name; device / dtype parts are handled outside.','line_number':900,'multiline':False]
['text':' See [Note: device and dtype suffix placement]','line_number':901,'multiline':False]
['text':' Construct parameter kwargs to pass to the test.','line_number':905,'multiline':False]
['text':' NOTE: test_wrapper exists because we don't want to apply','line_number':909,'multiline':False]
['text':'   op-specific decorators to the original test.','line_number':910,'multiline':False]
['text':'   Test-specific decorators are applied to the original test,','line_number':911,'multiline':False]
['text':'   however.','line_number':912,'multiline':False]
['text':' Initialize info for the last input seen. This is useful for tracking','line_number':931,'multiline':False]
['text':' down which inputs caused a test failure. Note that TrackedInputIter is','line_number':932,'multiline':False]
['text':' responsible for managing this.','line_number':933,'multiline':False]
['text':' Provides an error message for debugging before rethrowing the exception','line_number':941,'multiline':False]
['text':' Decorator that skips a test if the given condition is true.','line_number':948,'multiline':False]
['text':' Notes:','line_number':949,'multiline':False]
['text':'   (1) Skip conditions stack.','line_number':950,'multiline':False]
['text':'   (2) Skip conditions can be bools or strings. If a string the','line_number':951,'multiline':False]
['text':'       test base must have defined the corresponding attribute to be False','line_number':952,'multiline':False]
['text':'       for the test to run. If you want to use a string argument you should','line_number':953,'multiline':False]
['text':'       probably define a new decorator instead (see below).','line_number':954,'multiline':False]
['text':'   (3) Prefer the existing decorators to defining the 'device_type' kwarg.','line_number':955,'multiline':False]
['text':' Skips a test on CPU if the condition is true.','line_number':975,'multiline':False]
['text':' Skips a test on CUDA if the condition is true.','line_number':982,'multiline':False]
['text':' Skips a test on Lazy if the condition is true.','line_number':988,'multiline':False]
['text':' Skips a test on Meta if the condition is true.','line_number':994,'multiline':False]
['text':' Skips a test on MPS if the condition is true.','line_number':1000,'multiline':False]
['text':' Skips a test on XLA if the condition is true.','line_number':1006,'multiline':False]
['text':' torch.cuda.mem_get_info, aka cudaMemGetInfo, returns a tuple of (free memory, total memory) of a GPU','line_number':1024,'multiline':False]
['text':' CPU','line_number':1035,'multiline':False]
['text':' The sanitizers have significant memory overheads','line_number':1039,'multiline':False]
['text':' Decorator that provides all available devices of the device type to the test','line_number':1115,'multiline':False]
['text':' as a list of strings instead of providing a single device string.','line_number':1116,'multiline':False]
['text':' Skips the test if the number of available devices of the variant's device','line_number':1117,'multiline':False]
['text':' type is less than the 'num_required_devices' arg.','line_number':1118,'multiline':False]
['text':' Only runs the test on the native device type (currently CPU, CUDA, Meta and PRIVATEUSE1)','line_number':1138,'multiline':False]
['text':' Specifies per-dtype precision overrides.','line_number':1150,'multiline':False]
['text':' Ex.','line_number':1151,'multiline':False]
['text':'','line_number':1152,'multiline':False]
['text':' @precisionOverride({torch.half : 1e-2, torch.float : 1e-4})','line_number':1153,'multiline':False]
['text':' @dtypes(torch.half, torch.float, torch.double)','line_number':1154,'multiline':False]
['text':' def test_X(self, device, dtype):','line_number':1155,'multiline':False]
['text':'   ...','line_number':1156,'multiline':False]
['text':'','line_number':1157,'multiline':False]
['text':' When the test is instantiated its class's precision will be set to the','line_number':1158,'multiline':False]
['text':' corresponding override, if it exists.','line_number':1159,'multiline':False]
['text':' self.precision can be accessed directly, and it also controls the behavior of','line_number':1160,'multiline':False]
['text':' functions like self.assertEqual().','line_number':1161,'multiline':False]
['text':'','line_number':1162,'multiline':False]
['text':' Note that self.precision is a scalar value, so if you require multiple','line_number':1163,'multiline':False]
['text':' precisions (or are working with multiple dtypes) they should be specified','line_number':1164,'multiline':False]
['text':' explicitly and computed using self.precision (e.g.','line_number':1165,'multiline':False]
['text':' self.precision *2, max(1, self.precision)).','line_number':1166,'multiline':False]
['text':' Specifies per-dtype tolerance overrides tol(atol, rtol). It has priority over','line_number':1180,'multiline':False]
['text':' precisionOverride.','line_number':1181,'multiline':False]
['text':' Ex.','line_number':1182,'multiline':False]
['text':'','line_number':1183,'multiline':False]
['text':' @toleranceOverride({torch.float : tol(atol=1e-2, rtol=1e-3},','line_number':1184,'multiline':False]
['text':'                     torch.double : tol{atol=1e-4, rtol = 0})','line_number':1185,'multiline':False]
['text':' @dtypes(torch.half, torch.float, torch.double)','line_number':1186,'multiline':False]
['text':' def test_X(self, device, dtype):','line_number':1187,'multiline':False]
['text':'   ...','line_number':1188,'multiline':False]
['text':'','line_number':1189,'multiline':False]
['text':' When the test is instantiated its class's tolerance will be set to the','line_number':1190,'multiline':False]
['text':' corresponding override, if it exists.','line_number':1191,'multiline':False]
['text':' self.rtol and self.precision can be accessed directly, and they also control','line_number':1192,'multiline':False]
['text':' the behavior of functions like self.assertEqual().','line_number':1193,'multiline':False]
['text':'','line_number':1194,'multiline':False]
['text':' The above example sets atol = 1e-2 and rtol = 1e-3 for torch.float and','line_number':1195,'multiline':False]
['text':' atol = 1e-4 and rtol = 0 for torch.double.','line_number':1196,'multiline':False]
['text':' Decorator that instantiates a variant of the test for each given dtype.','line_number':1212,'multiline':False]
['text':' Notes:','line_number':1213,'multiline':False]
['text':'   (1) Tests that accept the dtype argument MUST use this decorator.','line_number':1214,'multiline':False]
['text':'   (2) Can be overridden for CPU or CUDA, respectively, using dtypesIfCPU','line_number':1215,'multiline':False]
['text':'       or dtypesIfCUDA.','line_number':1216,'multiline':False]
['text':'   (3) Can accept an iterable of dtypes or an iterable of tuples','line_number':1217,'multiline':False]
['text':'       of dtypes.','line_number':1218,'multiline':False]
['text':' Examples:','line_number':1219,'multiline':False]
['text':' @dtypes(torch.float32, torch.float64)','line_number':1220,'multiline':False]
['text':' @dtypes((torch.long, torch.float32), (torch.int, torch.float64))','line_number':1221,'multiline':False]
['text':' Overrides specified dtypes on the CPU.','line_number':1246,'multiline':False]
['text':' Overrides specified dtypes on CUDA.','line_number':1253,'multiline':False]
['text':' Skips a test on CPU if LAPACK is not available.','line_number':1331,'multiline':False]
['text':' Skips a test on CPU if FFT is not available.','line_number':1336,'multiline':False]
['text':' Skips a test on CPU if MKL is not available.','line_number':1341,'multiline':False]
['text':' Skips a test on CPU if MKL Sparse is not available (it's not linked on Windows).','line_number':1346,'multiline':False]
['text':' Skips a test on CPU if mkldnn is not available.','line_number':1351,'multiline':False]
['text':' Skips a test on CUDA if MAGMA is not available.','line_number':1356,'multiline':False]
['text':' hipSOLVER is disabled on ROCM < 5.3','line_number':1365,'multiline':False]
['text':' Skips a test on CUDA/ROCM if cuSOLVER/hipSOLVER is not available','line_number':1368,'multiline':False]
['text':' Skips a test if both cuSOLVER and MAGMA are not available','line_number':1373,'multiline':False]
['text':' cuSolver is disabled on cuda < 10.1.243, tests depend on MAGMA','line_number':1378,'multiline':False]
['text':' Skips a test if both cuSOLVER/hipSOLVER and MAGMA are not available','line_number':1381,'multiline':False]
['text':' cuSolver is disabled on cuda < 10.1.243, tests depend on MAGMA','line_number':1386,'multiline':False]
['text':' Skips a test on CUDA when using ROCm.','line_number':1389,'multiline':False]
['text':' Skips a test on CUDA when not using ROCm.','line_number':1398,'multiline':False]
['text':' Skips a test on CUDA if ROCm is unavailable or its version is lower than requested.','line_number':1402,'multiline':False]
['text':' Skips a test on CUDA when using ROCm.','line_number':1422,'multiline':False]
['text':' Skips a test for specified CUDA versions, given in the form of a list of [major, minor]s.','line_number':1426,'multiline':False]
['text':' cpu or rocm','line_number':1432,'multiline':False]
['text':' Skips a test for CUDA versions less than specified, given in the form of [major, minor].','line_number':1442,'multiline':False]
['text':' cpu or rocm','line_number':1448,'multiline':False]
['text':' Skips a test on CUDA if cuDNN is unavailable or its version is lower than requested.','line_number':1458,'multiline':False]
['text':' Skips a test on CUDA if cuSparse generic API is not available','line_number':1477,'multiline':False]
['text':' TODO: the "all" in the name isn't true anymore for quite some time as we have also have for example XLA and MPS now.','line_number':1511,'multiline':False]
['text':'  This should probably enumerate all available device type test base classes.','line_number':1512,'multiline':False]
