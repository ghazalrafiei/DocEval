['text':' orders with / without identity','line_number':62,'multiline':False]
['text':' We also return S to test','line_number':106,'multiline':False]
['text':' Each column of the matrix is getting multiplied many times leading to very large values for','line_number':172,'multiline':False]
['text':' the Jacobian matrix entries and making the finite-difference result of grad check less accurate.','line_number':173,'multiline':False]
['text':' That's why gradcheck with the default range [-9, 9] fails and [-2, 2] is used here.','line_number':174,'multiline':False]
['text':' m = n = S, k = S - 2','line_number':184,'multiline':False]
['text':' m = S, n = S -1, k = S - 2','line_number':186,'multiline':False]
['text':' (<matrix_size>, (<batch_sizes, ...>))','line_number':229,'multiline':False]
['text':' Need to make the matrices in A have positive determinant for autograd','line_number':262,'multiline':False]
['text':' To do so, we multiply A by its determinant to flip the sign of its determinant','line_number':263,'multiline':False]
['text':' we try all possible combinations of requires_grad for each input','line_number':295,'multiline':False]
['text':' when requires_grad == True, at least one input has to have requires_grad enabled','line_number':297,'multiline':False]
['text':' Each test case consists of the sizes in the chain of multiplications','line_number':313,'multiline':False]
['text':' e.g. [2, 3, 4, 5] generates matrices (2, 3) @ (3, 4) @ (4, 5)','line_number':314,'multiline':False]
['text':' svdvals not supported for low precision dtypes','line_number':343,'multiline':False]
['text':' svdvals not supported for low precision dtypes','line_number':376,'multiline':False]
['text':' IndexError: amax(): Expected reduction dim 0 to have non-zero size.','line_number':394,'multiline':False]
['text':' RuntimeError: linalg.vector_norm cannot compute the','line_number':409,'multiline':False]
['text':' {ord} norm on an empty tensor because the operation','line_number':410,'multiline':False]
['text':' does not have an identity','line_number':411,'multiline':False]
['text':' IndexError: amax(): Expected reduction dim {dim} to','line_number':425,'multiline':False]
['text':' have non-zero size.','line_number':426,'multiline':False]
['text':' default behavior, so skipped here so it's not tested 2 extra times','line_number':507,'multiline':False]
['text':' default kwargs','line_number':519,'multiline':False]
['text':' the size of at least 30 is required to cause failures for the previous implicit implementation','line_number':533,'multiline':False]
['text':' of the pinv's backward method, albeit it is slow.','line_number':534,'multiline':False]
['text':' Note that by making the columns of `a` and `b` orthonormal we make sure that','line_number':539,'multiline':False]
['text':' the product matrix `a @ b.t()` has condition number 1 when restricted to its image','line_number':540,'multiline':False]
['text':' autograd is not supported for inputs with zero number of elements','line_number':559,'multiline':False]
['text':' n-1, n, n+1','line_number':587,'multiline':False]
['text':' Wrapper around np.vander that supports batches of 1 dimension (enough for the tests)','line_number':595,'multiline':False]
['text':' Cholesky factorization is for positive-definite matrices','line_number':613,'multiline':False]
['text':' 0x0 matrix','line_number':624,'multiline':False]
['text':' zero batch of matrices','line_number':625,'multiline':False]
['text':' generated lower-triangular samples','line_number':631,'multiline':False]
['text':' upper=False by default','line_number':633,'multiline':False]
['text':' generate upper-triangular inputs','line_number':638,'multiline':False]
['text':' Symmetric inputs','line_number':653,'multiline':False]
['text':' single matrix','line_number':657,'multiline':False]
['text':' batch of matrices','line_number':661,'multiline':False]
['text':' 0x0 matrix','line_number':664,'multiline':False]
['text':' zero batch of matrices','line_number':667,'multiline':False]
['text':' Hermitian inputs','line_number':669,'multiline':False]
['text':' hermitian=True for complex inputs on CUDA is supported only with MAGMA 2.5.4+','line_number':670,'multiline':False]
['text':' single matrix','line_number':676,'multiline':False]
['text':' batch of matrices','line_number':680,'multiline':False]
['text':' Generate LDL factors of symmetric (and Hermitian on CPU) matrices','line_number':686,'multiline':False]
['text':' single matrix','line_number':694,'multiline':False]
['text':' batch of matrices','line_number':697,'multiline':False]
['text':' 0x0 matrix','line_number':698,'multiline':False]
['text':' zero batch of matrices','line_number':699,'multiline':False]
['text':' Symmetric case','line_number':716,'multiline':False]
['text':' Hermitian case','line_number':731,'multiline':False]
['text':' we generate matrices of shape (..., n + delta, n)','line_number':755,'multiline':False]
['text':' only square systems if Cusolver is not available','line_number':759,'multiline':False]
['text':' becase we solve a lstsq problem with a transposed matrix in the backward','line_number':760,'multiline':False]
['text':' Shapes for 2D Tensors','line_number':802,'multiline':False]
['text':' Shapes for 3D Tensors','line_number':805,'multiline':False]
['text':' dim1 == dim2 is not allowed','line_number':831,'multiline':False]
['text':' out of bounds dims are not allowed','line_number':833,'multiline':False]
['text':' these are valid inputs for diag_embed','line_number':854,'multiline':False]
['text':' eigh function','line_number':928,'multiline':False]
['text':' eigvalsh function','line_number':931,'multiline':False]
['text':' Samples do not need to be Hermitian, as we're using gradcheck_wrapper_hermitian_input','line_number':934,'multiline':False]
['text':' Note: we cannot use np.random.choice here as TorchDynamo','line_number':937,'multiline':False]
['text':' does not support tensors of strings.','line_number':938,'multiline':False]
['text':' requires_grad path for rtol tensor is not implemented','line_number':952,'multiline':False]
['text':' Not really necessary, but writing it for consistency','line_number':1034,'multiline':False]
['text':' Either A or B needs to have a gradient','line_number':1046,'multiline':False]
['text':' Reverses tensor order','line_number':1073,'multiline':False]
['text':' pivot=False only supported in CUDA','line_number':1097,'multiline':False]
['text':' Insanely annoying that make_fullrank_blablabla accepts a *shape and not a tuple!','line_number':1102,'multiline':False]
['text':' QR is just well defined when the matrix is full rank','line_number':1122,'multiline':False]
['text':' Zero-dim tensors are not supported in NumPy, so we skip them for now.','line_number':1138,'multiline':False]
['text':' NumPy is used in reference check tests.','line_number':1139,'multiline':False]
['text':' See https://github.com/numpy/numpy/pull/20482 for tracking NumPy bugfix.','line_number':1140,'multiline':False]
['text':' a_shapes += [(0, 0, 1, 2, 3, 0)]','line_number':1141,'multiline':False]
['text':' lhs / rhs shape can have any number of dimensions as long as their product equals 12','line_number':1159,'multiline':False]
['text':' Both Hessians are incorrect on complex inputs??','line_number':1237,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/78358','line_number':1303,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/78358','line_number':1315,'multiline':False]
['text':' Issue with conj and torch dispatch, see https://github.com/pytorch/pytorch/issues/82479','line_number':1331,'multiline':False]
['text':' AssertionError: Scalars are not equal!','line_number':1387,'multiline':False]
['text':' Pre-existing condition; Needs to be fixed','line_number':1500,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/80411','line_number':1530,'multiline':False]
['text':' TODO: backward uses in-place operations that vmap doesn't like','line_number':1532,'multiline':False]
['text':' we skip gradient checks for this suite as they are tested in','line_number':1594,'multiline':False]
['text':' variant_test_name='grad_oriented'','line_number':1595,'multiline':False]
['text':' The values for attribute 'shape' do not match','line_number':1598,'multiline':False]
['text':' gradchecks for forward AD fails with multi-Tensor outputs','line_number':1627,'multiline':False]
['text':' Runs very slowly on slow gradcheck - alternatively reduce input sizes','line_number':1633,'multiline':False]
['text':' tests do not work with passing lambda for op','line_number':1640,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/80411','line_number':1656,'multiline':False]
['text':' Need this lambda because gradcheck does not work with TensorList inputs','line_number':1667,'multiline':False]
['text':' Batched grad checks fail for empty input tensors (see https://github.com/pytorch/pytorch/issues/53407)','line_number':1672,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/66357','line_number':1677,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/67470','line_number':1682,'multiline':False]
['text':' Fails on XLA.','line_number':1686,'multiline':False]
['text':' AssertionError: False is not true : Tensors failed to compare as equal!','line_number':1687,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/71774','line_number':1694,'multiline':False]
['text':' NB: linalg.norm has two variants so that different skips can be used for different sample inputs','line_number':1704,'multiline':False]
['text':' torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got:','line_number':1748,'multiline':False]
['text':' Could not allocate memory to change Tensor SizesAndStrides!','line_number':1749,'multiline':False]
['text':' [NEW] Skips specifically for sample inputs at zero','line_number':1753,'multiline':False]
['text':' norm's vjp/jvp are not well-conditioned near zero','line_number':1754,'multiline':False]
['text':' In-place ops','line_number':1819,'multiline':False]
['text':' torch.autograd.gradcheck.GradcheckError: While computing batched gradients','line_number':1860,'multiline':False]
['text':' got: Could not allocate memory to change Tensor SizesAndStrides!','line_number':1861,'multiline':False]
['text':' FIXME: sum reduces all dimensions when dim=[]','line_number':1868,'multiline':False]
['text':' Runs very slowly on slow gradcheck - alternatively reduce input sizes','line_number':1880,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/80411','line_number':1881,'multiline':False]
['text':' linalg.lu_factor: LU without pivoting is not implemented on the CPU','line_number':1888,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/80411','line_number':1897,'multiline':False]
['text':' linalg.lu_factor: LU without pivoting is not implemented on the CPU','line_number':1904,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/80411','line_number':1913,'multiline':False]
['text':' Runs very slowly on slow-gradcheck - alternatively reduce input sizes','line_number':1914,'multiline':False]
['text':' linalg.lu_factor: LU without pivoting is not implemented on the CPU','line_number':1921,'multiline':False]
['text':' Runs very slowly on slow gradcheck - alternatively reduce input sizes','line_number':1930,'multiline':False]
['text':' Runs very slowly on slow gradcheck - alternatively reduce input sizes','line_number':2020,'multiline':False]
['text':' linalg.solve_triangular cannot be batched over because of a call to out.copy_(result);','line_number':2090,'multiline':False]
['text':' jit doesn't accept tensor inputs for matrix rank','line_number':2115,'multiline':False]
['text':' Runs very slowly on slow gradcheck - alternatively reduce input sizes','line_number':2154,'multiline':False]
['text':' errors with "leaked XXXX bytes CUDA memory on device 0"','line_number':2163,'multiline':False]
['text':' pinv is Frechet-differentiable in a rank-preserving neighborhood,','line_number':2176,'multiline':False]
['text':' so we feed inputs that are the products of two full-rank factors,','line_number':2177,'multiline':False]
['text':' to avoid any rank changes caused by the perturbations in the gradcheck','line_number':2178,'multiline':False]
['text':' Only large tensors show issues with implicit backward used prior to','line_number':2187,'multiline':False]
['text':' explicit backward implementation.','line_number':2188,'multiline':False]
['text':' CUDA runs out of memory','line_number':2194,'multiline':False]
['text':' This test takes almost 2 hours to run!','line_number':2202,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/78358','line_number':2221,'multiline':False]
['text':' This test is flaky under slow gradcheck, likely due to rounding issues','line_number':2254,'multiline':False]
['text':' Runs very slowly on slow-gradcheck - alternatively reduce input sizes','line_number':2269,'multiline':False]
['text':' We're using at::allclose, which does not have a batching rule','line_number':2274,'multiline':False]
['text':' We're using at::allclose, which does not have a batching rule','line_number':2328,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/78358','line_number':2358,'multiline':False]
['text':'','line_number':2397,'multiline':False]
['text':' torch.linalg','line_number':2398,'multiline':False]
['text':'','line_number':2399,'multiline':False]
['text':' FIXME: sum reduces all dimensions when dim=[]','line_number':2417,'multiline':False]
['text':' Uses vector_norm inside and vector_norm is affected by','line_number':2428,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/77216','line_number':2429,'multiline':False]
['text':' Uses vector_norm inside and vector_norm is affected by','line_number':2437,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/77216','line_number':2438,'multiline':False]
