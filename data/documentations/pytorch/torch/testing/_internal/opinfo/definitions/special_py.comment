['text':' TODO: Consolidate `i0e` with sample_inputs_unary when `make_tensor`,','line_number':40,'multiline':False]
['text':'       supports `exclude` argument.','line_number':41,'multiline':False]
['text':'       For more context: https://github.com/pytorch/pytorch/pull/56352#discussion_r633277617','line_number':42,'multiline':False]
['text':' Special Case for gradient','line_number':56,'multiline':False]
['text':' Sample with `0` in the input','line_number':57,'multiline':False]
['text':' WEIRD `scipy.special.polygamma` behavior','line_number':76,'multiline':False]
['text':' >>> scipy.special.polygamma(0, np.array(501, dtype=np.float32)).dtype','line_number':77,'multiline':False]
['text':' dtype('float64')','line_number':78,'multiline':False]
['text':' >>> scipy.special.polygamma(0, np.array([501], dtype=np.float32)).dtype','line_number':79,'multiline':False]
['text':' dtype('float32')','line_number':80,'multiline':False]
['text':'','line_number':81,'multiline':False]
['text':' Thus we cast output to the default torch dtype or preserve double','line_number':82,'multiline':False]
['text':' Dispatch stub: unsupported device typemeta','line_number':163,'multiline':False]
['text':' A separate OpInfo entry for special.polygamma is needed to reorder the arguments','line_number':172,'multiline':False]
['text':' for the alias. See the discussion here: https://github.com/pytorch/pytorch/pull/59691#discussion_r650261939','line_number':173,'multiline':False]
['text':' lambda impl','line_number':185,'multiline':False]
['text':' polygamma functions have multiple singularities at x <= 0','line_number':196,'multiline':False]
['text':' We don't test -1 as the gradient will be NaN and it'll break','line_number':209,'multiline':False]
['text':' Reference reference_inputs nans and infs on cuda and nan, inf, 0., -inf for cpu','line_number':220,'multiline':False]
['text':' TODO: FIXME','line_number':224,'multiline':False]
['text':' OpInfo entry to verify the gradient formula of `other`/`q`','line_number':225,'multiline':False]
['text':' BinaryUfuncInfo('special.zeta',','line_number':226,'multiline':False]
['text':'                 op=lambda q, x, **kwargs: torch.special.zeta(x, q, **kwargs),','line_number':227,'multiline':False]
['text':'                 aten_name='special_zeta',','line_number':228,'multiline':False]
['text':'                 variant_test_name='grad',','line_number':229,'multiline':False]
['text':'                 dtypes=all_types_and(torch.bool),','line_number':230,'multiline':False]
['text':'                 promotes_int_to_float=True,','line_number':231,'multiline':False]
['text':'                 supports_autograd=True,','line_number':232,'multiline':False]
['text':'                 supports_rhs_python_scalar=False,','line_number':233,'multiline':False]
['text':'                 decorators=[','line_number':234,'multiline':False]
['text':'                     # Derivative wrt first tensor not implemented','line_number':235,'multiline':False]
['text':'                     DecorateInfo(unittest.expectedFailure, "TestCommon",','line_number':236,'multiline':False]
['text':'                                  "test_floating_inputs_are_differentiable")','line_number':237,'multiline':False]
['text':'                 ],','line_number':238,'multiline':False]
['text':'                 skips=(','line_number':239,'multiline':False]
['text':'                     # Lambda doesn't work in JIT test','line_number':240,'multiline':False]
['text':'                     # AssertionError: JIT Test does not execute any logic','line_number':241,'multiline':False]
['text':'                     DecorateInfo(unittest.skip("Skipped!"), "TestJit", "test_variant_consistency_jit"),','line_number':242,'multiline':False]
['text':'                 )),','line_number':243,'multiline':False]
['text':' Greatest absolute difference: inf','line_number':443,'multiline':False]
['text':'','line_number':687,'multiline':False]
['text':' Elementwise Unary Special OpInfos','line_number':688,'multiline':False]
['text':'','line_number':689,'multiline':False]
['text':'','line_number':804,'multiline':False]
['text':' Elementwise Binary Special OpInfos','line_number':805,'multiline':False]
['text':'','line_number':806,'multiline':False]
['text':' Reference reference_inputs nans and infs on cuda and nan, inf, 0., -inf for cpu','line_number':813,'multiline':False]
