['text':' type: ignore[import]','line_number':63,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':71,'multiline':False]
['text':' Class to keep track of test flags configurable by environment variables.','line_number':99,'multiline':False]
['text':' Flags set here are intended to be read-only and should not be modified after','line_number':100,'multiline':False]
['text':' definition.','line_number':101,'multiline':False]
['text':' TODO: Expand this class to handle abritrary settings in addition to boolean flags?','line_number':102,'multiline':False]
['text':' Set of env vars to set for the repro command that is output on test failure.','line_number':104,'multiline':False]
['text':' Specifically, this includes env vars that are set to non-default values and','line_number':105,'multiline':False]
['text':' are not implied. Maps from env var name -> value (int)','line_number':106,'multiline':False]
['text':' Defines a flag usable throughout the test suite, determining its value by querying','line_number':109,'multiline':False]
['text':' the specified environment variable.','line_number':110,'multiline':False]
['text':'','line_number':111,'multiline':False]
['text':' Args:','line_number':112,'multiline':False]
['text':'     name (str): The name of the flag. A global variable with this name will be set','line_number':113,'multiline':False]
['text':'         for convenient access throughout the test suite.','line_number':114,'multiline':False]
['text':'     env_var (str): The name of the primary environment variable from which to','line_number':115,'multiline':False]
['text':'         determine the value of this flag. If this is None or the environment variable','line_number':116,'multiline':False]
['text':'         is unset, the default value will be used unless otherwise implied (see','line_number':117,'multiline':False]
['text':'         implied_by_fn). Default: None','line_number':118,'multiline':False]
['text':'     default (bool): The default value to use for the flag if unset by the environment','line_number':119,'multiline':False]
['text':'         variable and unimplied. Default: False','line_number':120,'multiline':False]
['text':'     include_in_repro (bool): Indicates whether this flag should be included in the','line_number':121,'multiline':False]
['text':'         repro command that is output on test failure (i.e. whether it is possibly','line_number':122,'multiline':False]
['text':'         relevant to reproducing the test failure). Default: True','line_number':123,'multiline':False]
['text':'     enabled_fn (Callable): Callable returning whether the flag should be enabled','line_number':124,'multiline':False]
['text':'         given the environment variable value and the default value. Default: Lambda','line_number':125,'multiline':False]
['text':'         requiring "0" to disable if on by default OR "1" to enable if off by default.','line_number':126,'multiline':False]
['text':'     implied_by_fn (Callable): Thunk returning a bool to imply this flag as enabled','line_number':127,'multiline':False]
['text':'         by something outside of its primary environment variable setting. For example,','line_number':128,'multiline':False]
['text':'         this can be useful if the value of another environment variable implies the flag','line_number':129,'multiline':False]
['text':'         as enabled. Default: Lambda returning False to indicate no implications.','line_number':130,'multiline':False]
['text':' export flag globally for convenience','line_number':150,'multiline':False]
['text':' Returns a string prefix usable to set environment variables for any test','line_number':154,'multiline':False]
['text':' settings that should be explicitly set to match this instantiation of the','line_number':155,'multiline':False]
['text':' test suite.','line_number':156,'multiline':False]
['text':' Example: "PYTORCH_TEST_WITH_ASAN=1 PYTORCH_TEST_WITH_ROCM=1"','line_number':157,'multiline':False]
['text':' NB: This flag differs semantically from others in that setting the env var to any','line_number':171,'multiline':False]
['text':' non-empty value will cause it to be true:','line_number':172,'multiline':False]
['text':'   CI=1, CI="true", CI=0, etc. all set the flag to be true.','line_number':173,'multiline':False]
['text':'   CI= and an unset CI set the flag to be false.','line_number':174,'multiline':False]
['text':' GitHub sets the value to CI="true" to enable it.','line_number':175,'multiline':False]
['text':' NB: enabled by default unless in an fbcode context.','line_number':199,'multiline':False]
['text':' set them here in case the tests are running in a subprocess that doesn't call run_tests','line_number':216,'multiline':False]
['text':' Irregular Jetson host/device memory setup requires cleanup to avoid tests being killed','line_number':228,'multiline':False]
['text':' Tries to extract the current test function by crawling the stack.','line_number':237,'multiline':False]
['text':' If unsuccessful, return None.','line_number':238,'multiline':False]
['text':' Contains tracked input data useful for debugging purposes','line_number':256,'multiline':False]
['text':' Attempt to pull out tracked input information from the test function.','line_number':263,'multiline':False]
['text':' A TrackedInputIter is used to insert this information.','line_number':264,'multiline':False]
['text':' Wraps an iterator and tracks the most recent value the iterator produces','line_number':281,'multiline':False]
['text':' for debugging purposes. Tracked values are stored on the test function.','line_number':282,'multiline':False]
['text':' Input type describes the things we're tracking (e.g. "sample input", "error input").','line_number':286,'multiline':False]
['text':' Callback is run on each iterated thing to get the thing to track.','line_number':288,'multiline':False]
['text':' allow StopIteration to bubble up','line_number':296,'multiline':False]
['text':' Do composition with the product of args.','line_number':355,'multiline':False]
['text':' Remove the generic test from the test class.','line_number':429,'multiline':False]
['text':' Add parametrized tests to the test class.','line_number':432,'multiline':False]
['text':' Apply decorators based on full param kwargs.','line_number':445,'multiline':False]
['text':' Can't use isinstance as it would cause a circular import','line_number':540,'multiline':False]
['text':' No additional parameters needed for the test.','line_number':562,'multiline':False]
['text':' Each "values" item is expected to be either:','line_number':566,'multiline':False]
['text':' * A tuple of values with one for each arg. For a single arg, a single item is expected.','line_number':567,'multiline':False]
['text':' * A subtest instance with arg_values matching the previous.','line_number':568,'multiline':False]
['text':' Leave test as-is and return the appropriate decorator_fn.','line_number':655,'multiline':False]
['text':' TODO fix when https://github.com/python/mypy/issues/2427 is address','line_number':738,'multiline':False]
['text':' type: ignore[assignment]','line_number':739,'multiline':False]
['text':' type: ignore[assignment]','line_number':740,'multiline':False]
['text':' allow users to override the test file location. We need this','line_number':743,'multiline':False]
['text':' because the distributed tests run the same test file multiple','line_number':744,'multiline':False]
['text':' times with different configurations.','line_number':745,'multiline':False]
['text':' Only run when -h or --help flag is active to display both unittest and parser help messages.','line_number':771,'multiline':False]
['text':' infer flags based on the default settings','line_number':788,'multiline':False]
['text':' CI Prefix path used only on CI environment','line_number':810,'multiline':False]
['text':' Give `p` a chance to handle KeyboardInterrupt. Without this,','line_number':819,'multiline':False]
['text':' `pytest` can't print errors it collected so far upon KeyboardInterrupt.','line_number':820,'multiline':False]
['text':' send SIGINT to give pytest a chance to make xml','line_number':828,'multiline':False]
['text':' try to handle the case where p.wait(timeout=5) times out as well as','line_number':833,'multiline':False]
['text':' otherwise the wait() call in the finally block can potentially hang','line_number':834,'multiline':False]
['text':' noqa: B001,E722, copied from python core library','line_number':842,'multiline':False]
['text':' Always call p.wait() to ensure exit','line_number':846,'multiline':False]
['text':' The following cool snippet is copied from Py3 core library subprocess.call','line_number':852,'multiline':False]
['text':' only the with','line_number':853,'multiline':False]
['text':'   1. `except KeyboardInterrupt` block added for SIGINT handling.','line_number':854,'multiline':False]
['text':'   2. In Py2, subprocess.Popen doesn't return a context manager, so we do','line_number':855,'multiline':False]
['text':'      `p.wait()` in a `final` block for the code to be portable.','line_number':856,'multiline':False]
['text':'','line_number':857,'multiline':False]
['text':' https://github.com/python/cpython/blob/71b6c1af727fbe13525fb734568057d78cea33f3/Lib/subprocess.py#L309-L323','line_number':858,'multiline':False]
['text':' Returns exicode + whether it was rerun','line_number':874,'multiline':False]
['text':' sanitize filename e.g., distributed/pipeline/sync/skip/test_api.py -> distributed.pipeline.sync.skip.test_api','line_number':935,'multiline':False]
['text':' inspect.getfile returns absolute path in some CI jobs, converting it to relative path if needed','line_number':937,'multiline':False]
['text':' pytext xml is different from unittext xml, this function makes pytest xml more similar to unittest xml','line_number':976,'multiline':False]
['text':' consider somehow modifying the XML logger in conftest to do this instead','line_number':977,'multiline':False]
['text':' The test prefix is optional','line_number':982,'multiline':False]
['text':' import test files.','line_number':1012,'multiline':False]
['text':' use env vars so pytest-xdist subprocesses can still access them','line_number':1018,'multiline':False]
['text':' Determine the test launch mechanism','line_number':1030,'multiline':False]
['text':' Before running the tests, lint to check that every test class extends from TestCase','line_number':1035,'multiline':False]
['text':' This is sort of hacky, but add on relevant env variables for distributed tests.','line_number':1074,'multiline':False]
['text':' Log the command to reproduce the failure.','line_number':1080,'multiline':False]
['text':' exitcode of 5 means no tests were found, which happens since some test configs don't','line_number':1114,'multiline':False]
['text':' run tests from certain files','line_number':1115,'multiline':False]
['text':' Only record the test report and always return a success code when running under rerun','line_number':1118,'multiline':False]
['text':' disabled tests mode','line_number':1119,'multiline':False]
['text':' import here so that non-CI doesn't need xmlrunner installed','line_number':1122,'multiline':False]
['text':' type: ignore[import]','line_number':1123,'multiline':False]
['text':' type: ignore[import]','line_number':1124,'multiline':False]
['text':' this message is printed in test summary;','line_number':1143,'multiline':False]
['text':' it stands for `verbose_str` captured in the closure','line_number':1144,'multiline':False]
['text':' Ideally we would like to not have to manually delete the file, but NamedTemporaryFile','line_number':1184,'multiline':False]
['text':' opens the file, and it cannot be opened multiple times in Windows. To support Windows,','line_number':1185,'multiline':False]
['text':' close the file after creation and try to remove it manually','line_number':1186,'multiline':False]
['text':' noqa: T484','line_number':1199,'multiline':False]
['text':' On Windows the directory created by TemporaryDirectory is likely to be removed prematurely,','line_number':1207,'multiline':False]
['text':' so we first create the directory using mkdtemp and then remove it manually','line_number':1208,'multiline':False]
['text':' noqa: T484','line_number':1215,'multiline':False]
['text':' Python 2.7 doesn't have spawn','line_number':1261,'multiline':False]
['text':' TODO: Remove PYTORCH_MIOPEN_SUGGEST_NHWC once ROCm officially supports NHWC in MIOpen','line_number':1269,'multiline':False]
['text':' See #64427','line_number':1270,'multiline':False]
['text':' Enables tests that are slow to run (disabled by default)','line_number':1272,'multiline':False]
['text':' Disables non-slow tests (these tests enabled by default)','line_number':1275,'multiline':False]
['text':' This is usually used in conjunction with TEST_WITH_SLOW to','line_number':1276,'multiline':False]
['text':' run *only* slow tests.  (I could have done an enum, but','line_number':1277,'multiline':False]
['text':' it felt a little awkward.','line_number':1278,'multiline':False]
['text':' Enables crossref tests, in addition to standard tests which','line_number':1281,'multiline':False]
['text':' are being run.  crossref tests work by installing a torch','line_number':1282,'multiline':False]
['text':' function mode that runs extra compute alongside the regular','line_number':1283,'multiline':False]
['text':' computation that happens with the test.  After both computations','line_number':1284,'multiline':False]
['text':' are done, we cross-reference them (thus the name) to check for','line_number':1285,'multiline':False]
['text':' correction, before throwing out the extra compute and proceeding','line_number':1286,'multiline':False]
['text':' as we had before.  By default, we don't run these tests.','line_number':1287,'multiline':False]
['text':' other libraries take up about 11% of space per process','line_number':1298,'multiline':False]
['text':' Run PyTorch tests with TorchDynamo','line_number':1317,'multiline':False]
['text':' AOT_EAGER not tested in ci, useful for debugging','line_number':1319,'multiline':False]
['text':' Do not spend time on helper functions that are called with different inputs','line_number':1326,'multiline':False]
['text':' Do not log compilation metrics from unit tests','line_number':1328,'multiline':False]
['text':' Run PyTorch tests with translation validation on.','line_number':1441,'multiline':False]
['text':' Some tests take too long when dynamic_shapes is combined with','line_number':1447,'multiline':False]
['text':' translation_validation. Whenever that happens, we solve that by','line_number':1448,'multiline':False]
['text':' disabling translation_validation.','line_number':1449,'multiline':False]
['text':' Turning TV off due to high latency on dynamic shapes.','line_number':1454,'multiline':False]
['text':' Determine whether to enable cuda memory leak check.','line_number':1460,'multiline':False]
['text':' CUDA mem leak check is expensive and thus we don't want to execute it on every','line_number':1461,'multiline':False]
['text':' test case / configuration.','line_number':1462,'multiline':False]
['text':' If this is True then CUDA memory leak checks are skipped. If this is false','line_number':1463,'multiline':False]
['text':'   then CUDA memory leak checks are performed.','line_number':1464,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/pull/59402#issuecomment-858811135','line_number':1465,'multiline':False]
['text':' True if CI is running TBB-enabled Pytorch','line_number':1468,'multiline':False]
['text':' Dict of NumPy dtype -> torch dtype (when the correspondence exists)','line_number':1471,'multiline':False]
['text':' numpy dtypes like np.float64 are not instances, but rather classes. This leads to rather absurd cases like','line_number':1487,'multiline':False]
['text':' np.float64 != np.dtype("float64") but np.float64 == np.dtype("float64").type.','line_number':1488,'multiline':False]
['text':' Especially when checking against a reference we can't be sure which variant we get, so we simply try both.','line_number':1489,'multiline':False]
['text':' Size of `np.intc` is platform defined.','line_number':1506,'multiline':False]
['text':' It is returned by functions like `bitwise_not`.','line_number':1507,'multiline':False]
['text':' On Windows `int` is 32-bit','line_number':1508,'multiline':False]
['text':' https://docs.microsoft.com/en-us/cpp/cpp/data-type-ranges?view=msvc-160','line_number':1509,'multiline':False]
['text':' Dict of torch dtype -> NumPy dtype','line_number':1512,'multiline':False]
['text':' Skips a test on CUDA if ROCm is available and its version is lower than requested.','line_number':1552,'multiline':False]
['text':' ignore git sha','line_number':1559,'multiline':False]
['text':' Reverts the linalg backend back to default to make sure potential failures in one','line_number':1578,'multiline':False]
['text':' test do not affect other tests','line_number':1579,'multiline':False]
['text':' Context manager for setting deterministic flag and automatically','line_number':1591,'multiline':False]
['text':' resetting it to its original value','line_number':1592,'multiline':False]
['text':' Context manager for setting cuda sync debug mode and reset it','line_number':1626,'multiline':False]
['text':' to original value','line_number':1627,'multiline':False]
['text':' we are not exposing it to the core because sync debug mode is','line_number':1628,'multiline':False]
['text':' global and thus not thread safe','line_number':1629,'multiline':False]
['text':' This decorator can be used for API tests that call','line_number':1641,'multiline':False]
['text':' torch.use_deterministic_algorithms().  When the test is finished, it will','line_number':1642,'multiline':False]
['text':' restore the previous deterministic flag setting.','line_number':1643,'multiline':False]
['text':'','line_number':1644,'multiline':False]
['text':' If CUDA >= 10.2, this will set the environment variable','line_number':1645,'multiline':False]
['text':' CUBLAS_WORKSPACE_CONFIG=:4096:8 so that the error associated with that','line_number':1646,'multiline':False]
['text':' setting is not thrown during the test unless the test changes that variable','line_number':1647,'multiline':False]
['text':' on purpose. The previous CUBLAS_WORKSPACE_CONFIG setting will also be','line_number':1648,'multiline':False]
['text':' restored once the test is finished.','line_number':1649,'multiline':False]
['text':'','line_number':1650,'multiline':False]
['text':' Note that if a test requires CUDA to actually register the changed','line_number':1651,'multiline':False]
['text':' CUBLAS_WORKSPACE_CONFIG variable, a new subprocess must be created, because','line_number':1652,'multiline':False]
['text':' CUDA only checks the variable when the runtime initializes. Tests can be','line_number':1653,'multiline':False]
['text':' run inside a subprocess like so:','line_number':1654,'multiline':False]
['text':'','line_number':1655,'multiline':False]
['text':'   import subprocess, sys, os','line_number':1656,'multiline':False]
['text':'   script = '''','line_number':1657,'multiline':False]
['text':'   # Test code should go here','line_number':1658,'multiline':False]
['text':'   '''','line_number':1659,'multiline':False]
['text':'   try:','line_number':1660,'multiline':False]
['text':'       subprocess.check_output(','line_number':1661,'multiline':False]
['text':'           [sys.executable, '-c', script],','line_number':1662,'multiline':False]
['text':'           stderr=subprocess.STDOUT,','line_number':1663,'multiline':False]
['text':'           cwd=os.path.dirname(os.path.realpath(__file__)),','line_number':1664,'multiline':False]
['text':'           env=os.environ.copy())','line_number':1665,'multiline':False]
['text':'   except subprocess.CalledProcessError as e:','line_number':1666,'multiline':False]
['text':'       error_message = e.output.decode('utf-8')','line_number':1667,'multiline':False]
['text':'       # Handle exceptions raised by the subprocess here','line_number':1668,'multiline':False]
['text':'','line_number':1669,'multiline':False]
['text':' Even if the numpy module is present, if `USE_NUMPY=0` is used during the','line_number':1700,'multiline':False]
['text':' build, numpy tests will fail','line_number':1701,'multiline':False]
['text':' The numpy module is present, verify that PyTorch is compiled with','line_number':1705,'multiline':False]
['text':' numpy support','line_number':1706,'multiline':False]
['text':' if current True','line_number':1815,'multiline':False]
['text':' if current True','line_number':1822,'multiline':False]
['text':' no_dispatch needed for test_composite_compliance','line_number':1872,'multiline':False]
['text':' Some OpInfos use freeze_rng_state for rng determinism, but','line_number':1873,'multiline':False]
['text':' test_composite_compliance overrides dispatch for all torch functions','line_number':1874,'multiline':False]
['text':' which we need to disable to get and set rng state','line_number':1875,'multiline':False]
['text':' Modes are not happy with torch.cuda.set_rng_state','line_number':1883,'multiline':False]
['text':' because it clones the state (which could produce a Tensor Subclass)','line_number':1884,'multiline':False]
['text':' and then grabs the new tensor's data pointer in generator.set_state.','line_number':1885,'multiline':False]
['text':'','line_number':1886,'multiline':False]
['text':' In the long run torch.cuda.set_rng_state should probably be','line_number':1887,'multiline':False]
['text':' an operator.','line_number':1888,'multiline':False]
['text':'','line_number':1889,'multiline':False]
['text':' NB: Mode disable is to avoid running cross-ref tests on thes seeding','line_number':1890,'multiline':False]
['text':' Tensor itself is iterable so we check this first','line_number':1935,'multiline':False]
['text':' Before starting CUDA test save currently active streams on all','line_number':1955,'multiline':False]
['text':' CUDA devices and set new non default streams to all CUDA devices','line_number':1956,'multiline':False]
['text':' to ensure CUDA tests do not use default stream by mistake.','line_number':1957,'multiline':False]
['text':' After completing CUDA test load previously active streams on all','line_number':1970,'multiline':False]
['text':' CUDA devices.','line_number':1971,'multiline':False]
['text':' initialize context & RNG to prevent false positive detections','line_number':1984,'multiline':False]
['text':' when the test is the first to initialize those','line_number':1985,'multiline':False]
['text':' Stores CUDA memory data provided by PyTorch's caching allocator and','line_number':1989,'multiline':False]
['text':'   the CUDA driver.','line_number':1990,'multiline':False]
['text':'','line_number':1991,'multiline':False]
['text':' NOTE: The undocumented torch.cuda.mem_get_info() returns','line_number':1992,'multiline':False]
['text':'   (#free bytes, #total bytes available) on the GPU','line_number':1993,'multiline':False]
['text':' Performs a gc if required (required if any CUDA memory is held)','line_number':1998,'multiline':False]
['text':' NOTE: gc is based exclusively on caching allocator memory','line_number':2002,'multiline':False]
['text':'   because the driver will always have some bytes in use (context size?)','line_number':2003,'multiline':False]
['text':' Acquires caching allocator and driver statistics before the test is run','line_number':2010,'multiline':False]
['text':' Don't check for leaks if an exception was thrown','line_number':2018,'multiline':False]
['text':' Compares caching allocator before/after statistics','line_number':2022,'multiline':False]
['text':' An increase in allocated memory is a discrepancy indicating a possible','line_number':2023,'multiline':False]
['text':'   memory leak','line_number':2024,'multiline':False]
['text':' avoid counting cublasWorkspace allocations','line_number':2028,'multiline':False]
['text':' Short-circuits if no discrepancy detected','line_number':2036,'multiline':False]
['text':' Validates the discrepancy persists after garbage collection and','line_number':2040,'multiline':False]
['text':'   is confirmed by the driver API','line_number':2041,'multiline':False]
['text':' NOTE: driver API iscrepancies alone are ignored because with the jiterator','line_number':2043,'multiline':False]
['text':'   some tests may permanently increase the CUDA context size and','line_number':2044,'multiline':False]
['text':'   that will appear as a driver memory leak but is the expected behavior.','line_number':2045,'multiline':False]
['text':' GCs and clears the cache','line_number':2047,'multiline':False]
['text':' Query memory multiple items to ensure leak was not transient','line_number':2055,'multiline':False]
['text':' Leak was false positive, exit loop','line_number':2071,'multiline':False]
['text':' Just raises a warning if the leak is not validated by the','line_number':2079,'multiline':False]
['text':'   driver API','line_number':2080,'multiline':False]
['text':' NOTE: this may be a problem with how the caching allocator collects its','line_number':2081,'multiline':False]
['text':'   statistics or a leak too small to trigger the allocation of an','line_number':2082,'multiline':False]
['text':'   additional block of memory by the CUDA driver','line_number':2083,'multiline':False]
['text':' A caching allocator discrepancy validated by the driver API is a','line_number':2097,'multiline':False]
['text':'   failure (except on ROCm, see below)','line_number':2098,'multiline':False]
['text':' NB: Hacking the exception args is the cleanest way I've found to append','line_number':2126,'multiline':False]
['text':' failure reproduction info without poisoning the stack trace.','line_number':2127,'multiline':False]
['text':'  "min_satisfying_examples" setting has been deprecated in hypothesis','line_number':2132,'multiline':False]
['text':'  3.56.0 and removed in hypothesis 4.x','line_number':2133,'multiline':False]
['text':' Used in check_if_enable to see if a test method should be disabled by an issue,','line_number':2172,'multiline':False]
['text':' sanitizes a test method name from appended suffixes by @dtypes parametrization.','line_number':2173,'multiline':False]
['text':' e.g., an issue with title "DISABLED test_bitwise_ops (__main__.TestBinaryUfuncs)" should','line_number':2174,'multiline':False]
['text':' disabled ALL parametrized test_bitwise_ops tests, such test_bitwise_ops_cuda_int32','line_number':2175,'multiline':False]
['text':' import statement is localized to avoid circular dependency issues with common_device_type.py','line_number':2177,'multiline':False]
['text':' poorly formed target test name','line_number':2197,'multiline':False]
['text':' if test method name or its sanitized version exactly matches the disabled','line_number':2201,'multiline':False]
['text':' test method name AND allow non-parametrized suite names to disable','line_number':2202,'multiline':False]
['text':' parametrized ones (TestSuite disables TestSuiteCPU)','line_number':2203,'multiline':False]
['text':' Sanitize the platforms list so that we continue to disable the test for any valid platforms given','line_number':2240,'multiline':False]
['text':' Skip the disabled test when not running under --rerun-disabled-tests verification mode','line_number':2252,'multiline':False]
['text':' `TestCase.assertEqual` is very permissive and coerced the inputs into a format that could be compared. This is very','line_number':2265,'multiline':False]
['text':' convenient when writing tests, but not so much while reviewing them. By default, the comparison `Pair` framework of','line_number':2266,'multiline':False]
['text':' `torch.testing._comparison.are_equal`, used for example by the public testing function','line_number':2267,'multiline':False]
['text':' `torch.testing.assert_close`, is more strict. In order to use the same framework and thus reduce the divergence','line_number':2268,'multiline':False]
['text':' between internal and external comparison logic as much as possible, we define some "relaxed" pairs here. They only','line_number':2269,'multiline':False]
['text':' change the supported inputs, but the comparison logic is the same.','line_number':2270,'multiline':False]
['text':' TODO: Revisit the relaxed pairs and check how much work it is to fix the tests that would fail without the relaxation.','line_number':2271,'multiline':False]
['text':' We require only one of the inputs of the inputs to be a boolean and the other can also be a boolean, a','line_number':2282,'multiline':False]
['text':' number, or a single element tensor or array, whereas in default BooleanPair both inputs have to be booleans.','line_number':2283,'multiline':False]
['text':' We require only one of the inputs of the inputs to be a number and the other can also be a number or a single','line_number':2339,'multiline':False]
['text':' element tensor or array, whereas in default NumberPair both inputs have to be numbers.','line_number':2340,'multiline':False]
['text':' type: ignore[call-overload]','line_number':2367,'multiline':False]
['text':' This implements a variant of assertRaises/assertRaisesRegex where we first test','line_number':2463,'multiline':False]
['text':' if the exception is NotImplementedError, and if so just skip the test instead','line_number':2464,'multiline':False]
['text':' of failing it.','line_number':2465,'multiline':False]
['text':'','line_number':2466,'multiline':False]
['text':' This is implemented by inheriting from the (private) implementation of','line_number':2467,'multiline':False]
['text':' assertRaises from unittest.case, and slightly tweaking it for this new','line_number':2468,'multiline':False]
['text':' behavior.  The year is 2021: this private class hierarchy hasn't changed since','line_number':2469,'multiline':False]
['text':' 2010, seems low risk to inherit from.','line_number':2470,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2474,'multiline':False]
['text':' causes pytest to not recognize this class as a test','line_number':2489,'multiline':False]
['text':' NOTE: "precision" lets classes and generated tests set minimum','line_number':2494,'multiline':False]
['text':' atol values when comparing tensors. Used by @precisionOverride and @toleranceOverride, for','line_number':2495,'multiline':False]
['text':' example.','line_number':2496,'multiline':False]
['text':' NOTE: "rel_tol" lets classes and generated tests set minimum','line_number':2497,'multiline':False]
['text':' rtol values when comparing tensors. Used by @toleranceOverride, for example.','line_number':2498,'multiline':False]
['text':' Toggles whether to assert that `torch.get_default_dtype()` returns','line_number':2502,'multiline':False]
['text':' `torch.float` when `setUp` and `tearDown` are called.','line_number':2503,'multiline':False]
['text':' Always use difflib to print diffs on multi line equality.','line_number':2506,'multiline':False]
['text':' Undocumented feature in unittest','line_number':2507,'multiline':False]
['text':' checker to early terminate test suite if unrecoverable failure occurs.','line_number':2511,'multiline':False]
['text':' CUDA device side error will cause subsequence test cases to fail.','line_number':2514,'multiline':False]
['text':' stop entire test suite if catches RuntimeError during torch.cuda.synchronize().','line_number':2515,'multiline':False]
['text':' When True, if a test case raises a NotImplementedError, instead of failing','line_number':2545,'multiline':False]
['text':' the test, skip it instead.','line_number':2546,'multiline':False]
['text':' Wraps the tested method if we should do CUDA memory check.','line_number':2554,'multiline':False]
['text':' FIXME: figure out the flaky -1024 anti-leaks on windows. See #8044','line_number':2557,'multiline':False]
['text':' Wraps the tested method if we should enforce non default CUDA stream.','line_number':2561,'multiline':False]
['text':' Attempt to get relative path based on the "test" dir.','line_number':2573,'multiline':False]
['text':' In CI, the working dir is not guaranteed to be the base repo dir so','line_number':2574,'multiline':False]
['text':' we can't just compute relative path from that.','line_number':2575,'multiline':False]
['text':' Can't determine containing dir; just return the test filename.','line_number':2582,'multiline':False]
['text':' The path isn't strictly correct but it's arguably better than nothing.','line_number':2583,'multiline':False]
['text':' Don't fail entirely if we can't get the test filename','line_number':2596,'multiline':False]
['text':' Munges exceptions that internally contain stack traces, using munge_exc','line_number':2609,'multiline':False]
['text':' the import below may initialize CUDA context, so we do it only if','line_number':2634,'multiline':False]
['text':' self._do_cuda_memory_leak_check or self._do_cuda_non_default_stream','line_number':2635,'multiline':False]
['text':' is True.','line_number':2636,'multiline':False]
['text':' TODO: sure looks like we unconditionally initialize the context here','line_number':2637,'multiline':False]
['text':' -- ezyang','line_number':2638,'multiline':False]
['text':' class_name.method_name','line_number':2640,'multiline':False]
['text':' A policy is a zero-argument function that returns a context manager.','line_number':2648,'multiline':False]
['text':' We don't take the context manager directly as it may be necessary to','line_number':2649,'multiline':False]
['text':' construct it once per test method','line_number':2650,'multiline':False]
['text':' Assumes that `method` is the tested function in `self`.','line_number':2652,'multiline':False]
['text':' NOTE: Python Exceptions (e.g., unittest.Skip) keeps objects in scope','line_number':2653,'multiline':False]
['text':'       alive, so this cannot be done in setUp and tearDown because','line_number':2654,'multiline':False]
['text':'       tearDown is run unconditionally no matter whether the test','line_number':2655,'multiline':False]
['text':'       passes or not. For the same reason, we can't wrap the `method`','line_number':2656,'multiline':False]
['text':'       call in try-finally and always do the check.','line_number':2657,'multiline':False]
['text':' Are we compiling?','line_number':2673,'multiline':False]
['text':' Is the class strict and compiling?','line_number':2675,'multiline':False]
['text':' TODO: Remove this; this is grandfathered in because we suppressed errors','line_number':2682,'multiline':False]
['text':' on test suite previously','line_number':2683,'multiline':False]
['text':' When strict mode is False, supress_errors is True','line_number':2684,'multiline':False]
['text':' TorchDynamo optimize annotation','line_number':2695,'multiline':False]
['text':' Early terminate test if necessary.  If using pytest, use the -x flag instead','line_number':2703,'multiline':False]
['text':' This is a big hacky, XMLRunner modifies expected type from TestCase to TestInfo','line_number':2708,'multiline':False]
['text':' Create dummy TestInfo to record results correctly','line_number':2709,'multiline':False]
['text':' type: ignore[import]','line_number':2710,'multiline':False]
['text':' This shouldn't really happen, but if does add fake failure','line_number':2715,'multiline':False]
['text':' For more details see https://github.com/pytorch/pytorch/issues/71973','line_number':2716,'multiline':False]
['text':' Save global check sparse tensor invariants state that can be','line_number':2734,'multiline':False]
['text':' restored from tearDown:','line_number':2735,'multiline':False]
['text':' Enable invariant checks for all sparse tensors constructions','line_number':2738,'multiline':False]
['text':' including the unsafe ones. If this is not desired for some','line_number':2739,'multiline':False]
['text':' test case, use check_invariants=False optional argument to','line_number':2740,'multiline':False]
['text':' sparse tensor constructors or','line_number':2741,'multiline':False]
['text':' @torch.sparse.check_sparse_tensor_invariants(False)','line_number':2742,'multiline':False]
['text':' decorator to disable the invariant checks.','line_number':2743,'multiline':False]
['text':' There exists test cases that override TestCase.setUp','line_number':2750,'multiline':False]
['text':' definition, so we cannot assume that _check_invariants','line_number':2751,'multiline':False]
['text':' attribute is defined in general.','line_number':2752,'multiline':False]
['text':' Restore the global check sparse tensor invariants state','line_number':2754,'multiline':False]
['text':' return the total number of counts in the sequence of','line_number':2820,'multiline':False]
['text':' sawteeth where n and m define a window in (n_rows+1,','line_number':2821,'multiline':False]
['text':' n_cols+1) rectangle where the sequence of sawteeth','line_number':2822,'multiline':False]
['text':' perfectly fit.','line_number':2823,'multiline':False]
['text':' Different from the original method description, here counts','line_number':2828,'multiline':False]
['text':' has leading 0 required by crow_indices:','line_number':2829,'multiline':False]
['text':' determine the width of the sawteeth window. We use bisection to solve','line_number':2835,'multiline':False]
['text':'   N(n, 0) == 0 or nnz - n * n_cols < max(N(n, 0), n_cols)','line_number':2836,'multiline':False]
['text':' for n','line_number':2837,'multiline':False]
['text':' fill the right rectangle with counts:','line_number':2849,'multiline':False]
['text':' determine the height of the sawteeth window. We use bisection to solve','line_number':2854,'multiline':False]
['text':'   N(n, m) == 0 or nnz - n * n_cols - m * (n_rows - n) < max(N(n, m), n_rows - n)','line_number':2855,'multiline':False]
['text':' for m.','line_number':2856,'multiline':False]
['text':' fill the bottom rectangle with counts:','line_number':2868,'multiline':False]
['text':' fill the sawteeth window with counts','line_number':2873,'multiline':False]
['text':' full sawteeth are never on top of a bottom rectangle','line_number':2881,'multiline':False]
['text':' sequence of full sawteeth:','line_number':2882,'multiline':False]
['text':' incomplete sawtooth:','line_number':2884,'multiline':False]
['text':' given input does not support sawteeth','line_number':2887,'multiline':False]
['text':' correction that will guarantee counts.sum() == nnz:','line_number':2891,'multiline':False]
['text':' randomize crow_indices by shuffling the sawteeth','line_number':2895,'multiline':False]
['text':' sequence:','line_number':2896,'multiline':False]
['text':' compute crow_indices:','line_number':2900,'multiline':False]
['text':' Assert not given impossible combination, where the sparse dims have','line_number':2970,'multiline':False]
['text':' empty numel, but nnz > 0 makes the indices containing values.','line_number':2971,'multiline':False]
['text':' FIXME: `x` is a sparse view of `v`. Currently rebase_history for','line_number':2988,'multiline':False]
['text':'        sparse views is not implemented, so this workaround is','line_number':2989,'multiline':False]
['text':'        needed for inplace operations done on `x`, e.g., copy_().','line_number':2990,'multiline':False]
['text':'        Remove after implementing something equivalent to CopySlice','line_number':2991,'multiline':False]
['text':'        for sparse views.','line_number':2992,'multiline':False]
['text':' NOTE: We do clone() after detach() here because we need to be able to change size/storage of x afterwards','line_number':2993,'multiline':False]
['text':' to ensure that a zero-sized tensor has the desired shape','line_number':3047,'multiline':False]
['text':' unreachable','line_number':3056,'multiline':False]
['text':' pattern is expected to be a matrix','line_number':3072,'multiline':False]
['text':' We cannot use `torch.sparse_xyz_tensor(pattern)` to','line_number':3074,'multiline':False]
['text':' compute the sparse layout indices and values because','line_number':3075,'multiline':False]
['text':' generate_simple_inputs is used to generate the inputs to','line_number':3076,'multiline':False]
['text':' test `torch.sparse_xyz_tensor` factory functions, so','line_number':3077,'multiline':False]
['text':' we'll compute the indices and values independently of','line_number':3078,'multiline':False]
['text':' the factory functions.','line_number':3079,'multiline':False]
['text':' the property of `values == range(1, 1+nnz)` is used in','line_number':3088,'multiline':False]
['text':' get_sparse_data_with_block to relate BSR and BSC values,','line_number':3089,'multiline':False]
['text':' so, don't change the following line:','line_number':3090,'multiline':False]
['text':' here we use the property `values == range(1, 1+nnz)` and','line_number':3117,'multiline':False]
['text':' `values` relation to `csc_values` (see get_sparse_data)','line_number':3118,'multiline':False]
['text':' to get BSC blocks via reordering the BSR blocks:','line_number':3119,'multiline':False]
['text':' non-batch','line_number':3128,'multiline':False]
['text':' batch data is created recursively:','line_number':3131,'multiline':False]
['text':' a "batch COO" means a COO with the leading','line_number':3137,'multiline':False]
['text':' sparse dimensions interpreted as batch','line_number':3138,'multiline':False]
['text':' dimensions','line_number':3139,'multiline':False]
['text':' A pattern is a 3-tuple with the following items:','line_number':3180,'multiline':False]
['text':'','line_number':3181,'multiline':False]
['text':' - a list of integers with the depth of two or more. The','line_number':3182,'multiline':False]
['text':'   integers define the sparsity patterns of the generated','line_number':3183,'multiline':False]
['text':'   inputs: zero values correspond to unspecified','line_number':3184,'multiline':False]
['text':'   elements/blocks, and non-zero values to the specified','line_number':3185,'multiline':False]
['text':'   elements.','line_number':3186,'multiline':False]
['text':'','line_number':3187,'multiline':False]
['text':'   For debugging convenience, the elements with the same','line_number':3188,'multiline':False]
['text':'   value typically belong to the same block. However, it','line_number':3189,'multiline':False]
['text':'   is not a hard requirement: as long as the shape of a','line_number':3190,'multiline':False]
['text':'   pattern divides with block sizes, the pattern will be','line_number':3191,'multiline':False]
['text':'   a valid one.','line_number':3192,'multiline':False]
['text':'','line_number':3193,'multiline':False]
['text':'   If the depth of the list is larger than two, inputs','line_number':3194,'multiline':False]
['text':'   with batch dimensions will be generated.','line_number':3195,'multiline':False]
['text':'','line_number':3196,'multiline':False]
['text':' - a list of 2-tuples of block sizes, used to generate','line_number':3197,'multiline':False]
['text':'   BSR/BSC tensors with various block size parameters','line_number':3198,'multiline':False]
['text':'','line_number':3199,'multiline':False]
['text':' - a list of tuples of dense dimensions, used to generate','line_number':3200,'multiline':False]
['text':'   hybrid tensors with various dense dimensions','line_number':3201,'multiline':False]
['text':'','line_number':3202,'multiline':False]
['text':' a simple 3 x 2 tensor: non-hybrid, hybrid with 1 and 2 dense dimensions','line_number':3204,'multiline':False]
['text':' 2 x 3 batch of 3 x 2 tensors: non-hybrid and hybrid with 2 dense dimensions','line_number':3207,'multiline':False]
['text':' tensor with non-trivial blocksize','line_number':3220,'multiline':False]
['text':' batch tensor with variable NSE','line_number':3229,'multiline':False]
['text':' Requires https://github.com/pytorch/pytorch/pull/84843 or similar.','line_number':3230,'multiline':False]
['text':' return a copy of t that is non-contiguous along the','line_number':3237,'multiline':False]
['text':' given dimension and with the given storage offset','line_number':3238,'multiline':False]
['text':' the main loop of the method:','line_number':3251,'multiline':False]
['text':' sparse compressed indices can be sliced only along batch dimensions','line_number':3269,'multiline':False]
['text':' zero-sized tensor inputs, non-batch, non-hybrid/hybrid','line_number':3285,'multiline':False]
['text':' unreachable','line_number':3321,'multiline':False]
['text':' coalesce is only implemented for COO','line_number':3325,'multiline':False]
['text':' Compares a torch function with a reference function for a given sample input (object of SampleInput)','line_number':3330,'multiline':False]
['text':' Note: only values are compared, type comparison is not done here','line_number':3331,'multiline':False]
['text':' Compares the given Torch and NumPy functions on the given tensor-like object.','line_number':3342,'multiline':False]
['text':' NOTE: both torch_fn and np_fn should be functions that take a single','line_number':3343,'multiline':False]
['text':'   tensor (array). If the torch and/or NumPy function require additional','line_number':3344,'multiline':False]
['text':'   arguments then wrap the function in a lambda or pass a partial function.','line_number':3345,'multiline':False]
['text':' TODO: add args/kwargs for passing to assertEqual (e.g. rtol, atol)','line_number':3346,'multiline':False]
['text':' Converts arrays to tensors','line_number':3368,'multiline':False]
['text':' NOTE: copying an array before conversion is necessary when,','line_number':3373,'multiline':False]
['text':'   for example, the array has negative strides.','line_number':3374,'multiline':False]
['text':' If you are seeing this function used, that means test is written wrongly','line_number':3382,'multiline':False]
['text':' and deserves detailed investigation','line_number':3383,'multiline':False]
['text':' int, float, etc. or different shape tensors','line_number':3390,'multiline':False]
['text':' iterable, but not a tensor','line_number':3393,'multiline':False]
['text':' TODO: default this to True','line_number':3407,'multiline':False]
['text':' Hide this function from `pytest`'s traceback','line_number':3413,'multiline':False]
['text':' numpy's dtypes are a superset of what PyTorch supports. In case we encounter an unsupported dtype, we fall','line_number':3416,'multiline':False]
['text':' back to an elementwise comparison. Note that this has to happen here and not for example in','line_number':3417,'multiline':False]
['text':' `TensorOrArrayPair`, since at that stage we can no longer split the array into its elements and perform','line_number':3418,'multiline':False]
['text':' multiple comparisons.','line_number':3419,'multiline':False]
['text':' When comparing a sequence of numbers to a tensor, we need to convert the sequence to a tensor here.','line_number':3428,'multiline':False]
['text':' Otherwise, the pair origination of `are_equal` will fail, because the sequence is recognized as container','line_number':3429,'multiline':False]
['text':' that should be checked elementwise while the tensor is not.','line_number':3430,'multiline':False]
['text':' If x or y are tensors and nested then we unbind them to a list of tensors this should allow us to compare','line_number':3436,'multiline':False]
['text':' a nested tensor to a nested tensor and a nested tensor to a list of expected tensors','line_number':3437,'multiline':False]
['text':' See [ErrorMeta Cycles]','line_number':3479,'multiline':False]
['text':' TODO: compose all metas into one AssertionError','line_number':3481,'multiline':False]
['text':' This emulates unittest.TestCase's behavior if a custom message passed and','line_number':3483,'multiline':False]
['text':' TestCase.longMessage (https://docs.python.org/3/library/unittest.html#unittest.TestCase.longMessage)','line_number':3484,'multiline':False]
['text':' is True (default)','line_number':3485,'multiline':False]
['text':' type: ignore[override]','line_number':3489,'multiline':False]
['text':' This API is used simulate deprecated x.type() == y.type()','line_number':3495,'multiline':False]
['text':' Reimplemented to provide special behavior when','line_number':3506,'multiline':False]
['text':' _ignore_not_implemented_error is True','line_number':3507,'multiline':False]
['text':' type: ignore[call-arg]','line_number':3511,'multiline':False]
['text':' type: ignore[union-attr]','line_number':3513,'multiline':False]
['text':' see https://bugs.python.org/issue23890','line_number':3515,'multiline':False]
['text':' Reimplemented to provide special behavior when','line_number':3520,'multiline':False]
['text':' _ignore_not_implemented_error is True','line_number':3521,'multiline':False]
['text':' Verifies that an exception with the type expected_exception and message','line_number':3523,'multiline':False]
['text':' matching the regular expression defined by expected_regex is thrown.','line_number':3524,'multiline':False]
['text':' If the test is instantiated for a non-native device type (like XLA)','line_number':3525,'multiline':False]
['text':' then the message is not validated.','line_number':3526,'multiline':False]
['text':' Checks whether the test is instantiated for a device type by testing','line_number':3528,'multiline':False]
['text':' if the test class has defined the device_type attribute and,','line_number':3529,'multiline':False]
['text':' if so, tests whether the instantiated device type is native or not','line_number':3530,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':3531,'multiline':False]
['text':' empty string matches any string','line_number':3532,'multiline':False]
['text':' type: ignore[call-arg]','line_number':3536,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':3538,'multiline':False]
['text':' Verifies that no unraisable exceptions are raised by callable.  Unlike regular','line_number':3542,'multiline':False]
['text':' exceptions, these do not actually propagate to the caller and are','line_number':3543,'multiline':False]
['text':' suppressed.  We must test for them specially.','line_number':3544,'multiline':False]
['text':' Disable GC when running the callable to prevent spurious flakiness','line_number':3552,'multiline':False]
['text':' from unlucky GCs inside the callable','line_number':3553,'multiline':False]
['text':' TODO: Support context manager interface','line_number':3565,'multiline':False]
['text':' NB: The kwargs forwarding to callable robs the 'subname' parameter.','line_number':3566,'multiline':False]
['text':' If you need it, manually apply your callable in a lambda instead.','line_number':3567,'multiline':False]
['text':' Don't put this in the try block; the AssertionError will catch it','line_number':3578,'multiline':False]
['text':' allow any warning to be raised','line_number':3586,'multiline':False]
['text':' allow any warning to be raised','line_number':3601,'multiline':False]
['text':' NB: we take __file__ from the module that defined the test','line_number':3629,'multiline':False]
['text':' class, so we place the expect directory where the test script','line_number':3630,'multiline':False]
['text':' lives, NOT where test/common_utils.py lives.  This doesn't matter in','line_number':3631,'multiline':False]
['text':' PyTorch where all test scripts are in the same directory as','line_number':3632,'multiline':False]
['text':' test/common_utils.py, but it matters in onnx-pytorch','line_number':3633,'multiline':False]
['text':' Adjust for producer_version, leave s unmodified','line_number':3651,'multiline':False]
['text':' a hack for JIT tests','line_number':3670,'multiline':False]
['text':' Adjust for producer_version','line_number':3675,'multiline':False]
['text':' Python 2.7 only','line_number':3685,'multiline':False]
['text':' NB: Python considers lhs "old" and rhs "new".','line_number':3686,'multiline':False]
['text':' Check that errors are thrown correctly','line_number':3757,'multiline':False]
['text':' If a nondeterministic error is not expected, make sure','line_number':3767,'multiline':False]
['text':' that it is not raised','line_number':3768,'multiline':False]
['text':' Reraise exceptions unrelated to nondeterminism','line_number':3776,'multiline':False]
['text':' Check that warnings are thrown correctly','line_number':3779,'multiline':False]
['text':' run code in subprocess and capture exceptions.','line_number':3794,'multiline':False]
['text':' returns captured stderr','line_number':3807,'multiline':False]
['text':' remove CI flag since this is a wrapped test process.','line_number':3812,'multiline':False]
['text':' CI flag should be set in the parent process only.','line_number':3813,'multiline':False]
['text':' Errors that we can get in c10d initialization for which we should retry tests for.','line_number':3855,'multiline':False]
['text':' This if block is executed when using this function as a decorator with arguments.','line_number':3862,'multiline':False]
['text':' Decorator to retry upon certain Exceptions.','line_number':3884,'multiline':False]
['text':' true decorator','line_number':3902,'multiline':False]
['text':' FIXME: modernize these to be consistent with make_tensor','line_number':3906,'multiline':False]
['text':'   and review including them in torch.testing','line_number':3907,'multiline':False]
['text':' Methods for matrix generation','line_number':3908,'multiline':False]
['text':' Returns a noncontiguous (tensor with the same shape and values as t','line_number':3943,'multiline':False]
['text':' The noncontiguous tensor is constructed such that elements in the innermost','line_number':3944,'multiline':False]
['text':'   dimension are separated by zeros or (whenever possible) nans','line_number':3945,'multiline':False]
['text':' TODO: consider more complicated noncontiguity schemes','line_number':3946,'multiline':False]
['text':' Short-circuits if t is already noncontiguous','line_number':3948,'multiline':False]
['text':' Choose a "weird" value that won't be accessed','line_number':3952,'multiline':False]
['text':' TODO: remove this (prefer make_symmetric_matrices below)','line_number':3967,'multiline':False]
['text':' Creates a symmetric matrix or batch of symmetric matrices','line_number':3975,'multiline':False]
['text':' Shape must be a square matrix or batch of square matrices','line_number':3976,'multiline':False]
['text':' TODO: remove this (prefer make_symmetric_pd_matrices below)','line_number':4017,'multiline':False]
['text':' Creates a symmetric positive-definite matrix or batch of','line_number':4027,'multiline':False]
['text':'   such matrices','line_number':4028,'multiline':False]
['text':' Creates a full rank matrix with distinct singular values or','line_number':4047,'multiline':False]
['text':'   a batch of such matrices','line_number':4048,'multiline':False]
['text':' We choose the singular values to be "around one"','line_number':4055,'multiline':False]
['text':' This is to make the matrix well conditioned','line_number':4056,'multiline':False]
['text':' s = [2, 3, ..., k+1]','line_number':4057,'multiline':False]
['text':' s = [2, -3, 4, ..., (-1)^k k+1]','line_number':4059,'multiline':False]
['text':' 1 + 1/s so that the singular values are in the range [2/3, 3/2]','line_number':4061,'multiline':False]
['text':' This gives a condition number of 9/4, which should be good enough','line_number':4062,'multiline':False]
['text':' Note that the singular values need not be ordered in an SVD so','line_number':4064,'multiline':False]
['text':' we don't need need to sort S','line_number':4065,'multiline':False]
['text':' make matrix singular','line_number':4092,'multiline':False]
['text':' increase the order of singularity so that the pivoting','line_number':4095,'multiline':False]
['text':' in LU factorization will be non-trivial','line_number':4096,'multiline':False]
['text':' ensure that the diagonal dominates','line_number':4130,'multiline':False]
['text':' FIXME: remove this by updating test suites using it','line_number':4193,'multiline':False]
['text':' FIXME: remove this by updating test suites using it','line_number':4202,'multiline':False]
['text':' FIXME: improve load_tests() documentation here','line_number':4256,'multiline':False]
['text':' skip if the running file is not a script','line_number':4262,'multiline':False]
['text':' FIXME: document this and move it to test_serialization','line_number':4286,'multiline':False]
['text':' Tentative value for nondet_tol for gradcheck when backward implementation','line_number':4294,'multiline':False]
['text':' relies on nondeterministic operations, i.e., those listed here:','line_number':4295,'multiline':False]
['text':' https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html','line_number':4296,'multiline':False]
['text':'','line_number':4297,'multiline':False]
['text':' For more information see https://github.com/pytorch/pytorch/issues/56202','line_number':4298,'multiline':False]
['text':' Wrapper around gradcheck that enables certain keys by default.','line_number':4309,'multiline':False]
['text':' Use this testing-internal gradcheck instead of autograd.gradcheck so that new features like vmap and','line_number':4310,'multiline':False]
['text':' forward-mode AD are tested by default. We create this wrapper because we'd like to keep new checks','line_number':4311,'multiline':False]
['text':' to be disabled to default for the public-facing api to avoid breaking user code.','line_number':4312,'multiline':False]
['text':'','line_number':4313,'multiline':False]
['text':' All PyTorch devs doing testing should use this wrapper instead of autograd.gradcheck.','line_number':4314,'multiline':False]
['text':' default value override values explicitly set to None','line_number':4324,'multiline':False]
['text':' Wrapper around gradgradcheck that enables certain keys by default','line_number':4331,'multiline':False]
['text':' See gradcheck above for an explanation of why we need something like this.','line_number':4332,'multiline':False]
['text':'','line_number':4333,'multiline':False]
['text':' All PyTorch devs doing testing should use this wrapper instead of autograd.gradgradcheck','line_number':4334,'multiline':False]
['text':' default value override values explicitly set to None','line_number':4344,'multiline':False]
['text':' call assert function rather than returning a bool since it's nicer','line_number':4352,'multiline':False]
['text':' if we get whether this failed on the gradcheck or the gradgradcheck.','line_number':4353,'multiline':False]
['text':' FIXME: delete this','line_number':4368,'multiline':False]
['text':' Using @toleranceOverride specific to your test is the recommended way','line_number':4369,'multiline':False]
['text':' of doing this. These are just some values that worked for test_nn.','line_number':4370,'multiline':False]
['text':' FIXME: move to test_sparse or sparse utils','line_number':4376,'multiline':False]
['text':' This is a wrapper that wraps a test to run this test twice, one with','line_number':4377,'multiline':False]
['text':' coalesced=True, another with coalesced=False for coalesced/uncoalesced sparse tensors.','line_number':4378,'multiline':False]
['text':' check if indices are sorted','line_number':4397,'multiline':False]
['text':' check if there are no repeated indices','line_number':4400,'multiline':False]
['text':' return the shared library file in the installed folder if exist,','line_number':4419,'multiline':False]
['text':' else the file in the build folder','line_number':4420,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':4459,'multiline':False]
['text':' Returns scalar tensor representation of a list of integer byte values','line_number':4468,'multiline':False]
['text':' NB: expectedFailure operates by mutating the method in question,','line_number':4525,'multiline':False]
['text':' which is why you have to copy the function first','line_number':4526,'multiline':False]
['text':' Get 10 values and remove the 2 max and 2 min and return the avg.','line_number':4609,'multiline':False]
['text':' This is to avoid system disturbance that skew the results, e.g.','line_number':4610,'multiline':False]
['text':' the very first cuda call likely does a bunch of init, which takes','line_number':4611,'multiline':False]
['text':' much longer than subsequent calls.','line_number':4612,'multiline':False]
['text':'','line_number':4613,'multiline':False]
['text':' Tested on both Tesla V100, Quadro GP100, Titan RTX, RTX 3090 GPUs','line_number':4614,'multiline':False]
['text':' and seems to return stable values. Therefore, we enable caching','line_number':4615,'multiline':False]
['text':' using lru_cache decorator above.','line_number':4616,'multiline':False]
['text':' OpInfo utils','line_number':4625,'multiline':False]
['text':' this helper method is to recursively','line_number':4638,'multiline':False]
['text':' clone the tensor-type input of operators tested by OpInfo','line_number':4639,'multiline':False]
['text':' Copies inputs to inplace operations to avoid inplace modifications','line_number':4679,'multiline':False]
['text':'   to leaves requiring gradient','line_number':4680,'multiline':False]
['text':' NB: check_backward_ad does not affect gradgradcheck (always True)','line_number':4691,'multiline':False]
['text':' Gradcheck expects tensors as its input, but autograd actually supports tensorlists','line_number':4711,'multiline':False]
['text':'   and tensors passed as kwargs. The following creates a function that accepts just','line_number':4712,'multiline':False]
['text':'   the tensors that require grad as varargs, and then recomposes them back into the','line_number':4713,'multiline':False]
['text':'   original input.','line_number':4714,'multiline':False]
['text':' Creates gradcheck inputs by identifying tensors requiring grad','line_number':4716,'multiline':False]
['text':' Verifies sample input tensors should have no grad','line_number':4724,'multiline':False]
['text':' This may happen if the same tensor is used in two different SampleInputs','line_number':4725,'multiline':False]
['text':' Puts inputs back into sample properly','line_number':4749,'multiline':False]
['text':' Recreates kwargs','line_number':4759,'multiline':False]
['text':' gradgrad check','line_number':4782,'multiline':False]
['text':' skip one frame','line_number':4853,'multiline':False]
['text':' Remove everything that looks like stack frames in NOT this file','line_number':4857,'multiline':False]
['text':' Don't accept top-level, even for this script, these will wobble','line_number':4861,'multiline':False]
['text':' depending on how the testing script was invoked','line_number':4862,'multiline':False]
['text':' for Windows','line_number':4872,'multiline':False]
