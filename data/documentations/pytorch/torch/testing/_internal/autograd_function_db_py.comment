['text':' Note: [autograd.Function db]','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':' This is a collection of autograd.Function test cases written as OpInfos','line_number':13,'multiline':False]
['text':' so they can easily be consumed by OpInfo-based tests to check if a subsystem','line_number':14,'multiline':False]
['text':' supports autograd.Function.','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':' Axes:','line_number':17,'multiline':False]
['text':' - saves {output, input, intermediate, non-tensor}','line_number':18,'multiline':False]
['text':' - {inputs, output} x {single tensor, tensors, arbitrary objects}','line_number':19,'multiline':False]
['text':' - Uses {mark_dirty, mark_non_differentiable, once_differentiable}','line_number':20,'multiline':False]
['text':' Broadcasting','line_number':140,'multiline':False]
['text':' Doesn't call numpy operations because I didn't want to write NumpyMul_','line_number':199,'multiline':False]
['text':' wrap dim','line_number':236,'multiline':False]
['text':' wrap dim','line_number':314,'multiline':False]
['text':' Intentionally returning torch.zeros instead of zeros_like or new_zeros.','line_number':449,'multiline':False]
['text':' Also intentionally not None.','line_number':450,'multiline':False]
['text':' Intentionally too-large gradient','line_number':452,'multiline':False]
['text':' Intentionally returning torch.zeros instead of zeros_like or new_zeros.','line_number':459,'multiline':False]
['text':' Also intentionally not None.','line_number':460,'multiline':False]
