['text':' min/max return a namedtuple','line_number':50,'multiline':False]
['text':' use rank0 as the device for sum','line_number':89,'multiline':False]
['text':' collect all data to the list and make them','line_number':91,'multiline':False]
['text':' all on rank 0 device','line_number':92,'multiline':False]
['text':' now mimic reduce across all ranks','line_number':96,'multiline':False]
['text':' copy all the reduced value to each rank','line_number':99,'multiline':False]
['text':' Can't handle all_gather with multiple tensors','line_number':109,'multiline':False]
['text':' Can't handle scatter with multiple input tensor list','line_number':125,'multiline':False]
['text':' Can't handle scatter with multiple output tensor','line_number':131,'multiline':False]
['text':' Can't handle gather with multiple tensor lists','line_number':143,'multiline':False]
['text':' Can't handle gather with multiple tensor lists','line_number':148,'multiline':False]
['text':' Can't handle reduce_scatter with multiple scatter list','line_number':163,'multiline':False]
['text':' Can't handle reduce_scatter with multiple output tensor','line_number':168,'multiline':False]
['text':' notify rank 0','line_number':209,'multiline':False]
['text':' SystemExit is not a subclass of Exception but BaseException','line_number':218,'multiline':False]
['text':' and can be distinguished from normal exception raised from program errors','line_number':219,'multiline':False]
['text':' so that we can hide it from the exception queue','line_number':220,'multiline':False]
['text':' wait for rank 0 to finish','line_number':225,'multiline':False]
['text':' copy data around','line_number':231,'multiline':False]
['text':' pg_name is unique, we use that to record the mapping between pg and collective','line_number':247,'multiline':False]
['text':' This is racily called by all ranks, so only one will work','line_number':254,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/103033 changed store based barrier to optional','line_number':368,'multiline':False]
['text':' When device mesh involves sub groups while store based barrier is not enabled in c10d,','line_number':369,'multiline':False]
['text':' even though threaded pg actual collectives are assumed to be single threaded,','line_number':370,'multiline':False]
['text':' different threads may be initializing different groups,','line_number':371,'multiline':False]
['text':' leading to race conditions.','line_number':372,'multiline':False]
['text':' For example, if we have a mesh of [[0, 1], [2, 3]], the sub groups','line_number':373,'multiline':False]
['text':' (dim 0 and 1) would be initialized in different threads independently.','line_number':374,'multiline':False]
['text':' In this case we can no longer rely on class or global variables','line_number':375,'multiline':False]
['text':' but have to rely on store based barrier to make sure each group','line_number':376,'multiline':False]
['text':' is ready separately before we can invoke collectives in any of the groups.','line_number':377,'multiline':False]
['text':' the prefix store is already per group so we pass an empty name here','line_number':379,'multiline':False]
