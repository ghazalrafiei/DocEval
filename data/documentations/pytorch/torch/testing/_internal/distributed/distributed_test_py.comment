['text':' Can be tensor or int','line_number':115,'multiline':False]
['text':' Allowlist of distributed backends where profiling collectives is supported.','line_number':145,'multiline':False]
['text':' Allowlist of distributed backends where profiling is supported with use_cuda=True','line_number':153,'multiline':False]
['text':' Allowlist of distributed backends where profiling is supported for p2p ops','line_number':161,'multiline':False]
['text':' Dummy NamedTuple data structures to test DDP support for NamedTuple types.','line_number':169,'multiline':False]
['text':' Base error message substring on unfinished reductions.','line_number':205,'multiline':False]
['text':' Error message substring when find_unused_parameters=True has not been passed','line_number':209,'multiline':False]
['text':' Error message substring when find_unused_parameters=True is enabled','line_number':213,'multiline':False]
['text':' Error message substring for possibility of not all model outputs being used','line_number':215,'multiline':False]
['text':' in loss computation','line_number':216,'multiline':False]
['text':' Error message substring suggesting to use TORCH_DISTRIBUTED_DEBUG','line_number':220,'multiline':False]
['text':' Second layer is used dependent on input x.','line_number':366,'multiline':False]
['text':' This test runs slowly and needs additional time to complete, otherwise can','line_number':391,'multiline':False]
['text':' be taken down by TORCH_NCCL_ASYNC_ERROR_HANDLING','line_number':392,'multiline':False]
['text':' This test has a short timeout since it tests being taken down by','line_number':394,'multiline':False]
['text':' TORCH_NCCL_ASYNC_ERROR_HANDLING which we want to happen quickly.','line_number':395,'multiline':False]
['text':' This test has a short timeout since it tests being taken down by','line_number':397,'multiline':False]
['text':' TORCH_NCCL_ASYNC_ERROR_HANDLING which we want to happen quickly.','line_number':398,'multiline':False]
['text':' Not setting MASTER_PORT and get a random free port','line_number':544,'multiline':False]
['text':' initialize temp directories','line_number':549,'multiline':False]
['text':' initialize Barrier','line_number':551,'multiline':False]
['text':' Skip return code checking for following tests as they are expected to','line_number':553,'multiline':False]
['text':' crash a process due to TORCH_NCCL_ASYNC_ERROR_HANDLING.','line_number':554,'multiline':False]
['text':' Execute barrier prior to running test to ensure that every process','line_number':593,'multiline':False]
['text':' has finished initialization and that the following test','line_number':594,'multiline':False]
['text':' immediately exiting due to a skip doesn't cause flakiness.','line_number':595,'multiline':False]
['text':' Needed since MultiProcessTestCase assumes a world_size of 4, but we','line_number':603,'multiline':False]
['text':' run these tests under other various world_sizes.','line_number':604,'multiline':False]
['text':' verify buffers across models','line_number':637,'multiline':False]
['text':' Verify buffers across ranks.','line_number':642,'multiline':False]
['text':' Check relevant env vars','line_number':667,'multiline':False]
['text':' N/A','line_number':672,'multiline':False]
['text':' Check irrelevant env vars','line_number':678,'multiline':False]
['text':' GET RANK','line_number':688,'multiline':False]
['text':' test parsing','line_number':729,'multiline':False]
['text':' Test destroy','line_number':740,'multiline':False]
['text':' Test get rank and size of group','line_number':750,'multiline':False]
['text':' Test destroy full groups','line_number':764,'multiline':False]
['text':' Test get rank and size of full group','line_number':770,'multiline':False]
['text':' Only execute barrier on rank == 0, causing it to timeout','line_number':779,'multiline':False]
['text':' In debug mode, we execute a monitored_barrier before the','line_number':782,'multiline':False]
['text':' collective, so assert on that.','line_number':783,'multiline':False]
['text':' Explicitly pass world size to the barrier because we've','line_number':810,'multiline':False]
['text':' just destroyed any state in torch.distributed.','line_number':811,'multiline':False]
['text':' Reinitialize global process group','line_number':814,'multiline':False]
['text':' This test helper can only be used when using the Gloo or NCCL backend','line_number':844,'multiline':False]
['text':' **and** both the Gloo and NCCL backends are available.','line_number':845,'multiline':False]
['text':' See the @skip annotations below.','line_number':846,'multiline':False]
['text':' Pin device (so we avoid NCCL race conditions/deadlocks).','line_number':867,'multiline':False]
['text':' Run broadcast of CUDA tensor (so it works for both Gloo and NCCL).','line_number':871,'multiline':False]
['text':' Test global model averaging','line_number':1018,'multiline':False]
['text':' Every element will be the same as the input.','line_number':1024,'multiline':False]
['text':' Test partial model averaging','line_number':1028,'multiline':False]
['text':' Every element on device 0 or 1 should be the average of 0 and 1, i.e., 0.5.','line_number':1036,'multiline':False]
['text':' Every element on device not in the subgroup should remain the same.','line_number':1040,'multiline':False]
['text':' Reset the parameters at every step.','line_number':1067,'multiline':False]
['text':' mock grad','line_number':1070,'multiline':False]
['text':' No model averaging, so the parameters are not updated.','line_number':1076,'multiline':False]
['text':' Reset the parameters at every step.','line_number':1096,'multiline':False]
['text':' mock grad','line_number':1099,'multiline':False]
['text':' No model averaging, so the parameters are not updated.','line_number':1115,'multiline':False]
['text':' Run the global averaging at a period of 4,','line_number':1146,'multiline':False]
['text':' which is equivalent to the above periodic model averaging test case.','line_number':1147,'multiline':False]
['text':' Reset the parameters at every step.','line_number':1156,'multiline':False]
['text':' mock grad','line_number':1159,'multiline':False]
['text':' No model averaging, so the parameters are not updated.','line_number':1165,'multiline':False]
['text':' Set up such a hierarchical model averaging as follows:','line_number':1183,'multiline':False]
['text':' after the first 10 warmup steps,','line_number':1184,'multiline':False]
['text':' run model averaging every 2 steps within each subgroup of size 2,','line_number':1185,'multiline':False]
['text':' run model averaging every 4 steps within each subgroup of size 3,','line_number':1186,'multiline':False]
['text':' and run the global model averaging every 8 steps.','line_number':1187,'multiline':False]
['text':' If there is a conflict in model averaging at a step, only run the highest-level model averaging.','line_number':1188,'multiline':False]
['text':' Reset the parameters at every step.','line_number':1235,'multiline':False]
['text':' mock grad','line_number':1238,'multiline':False]
['text':' Run global model averaging when `step` can be divided by 8.','line_number':1242,'multiline':False]
['text':' Run model averaging within subgroup when `step` can be divided by 4 but not by 8.','line_number':1245,'multiline':False]
['text':' Run model averaging within subgroup when `step` can be divided by 2 but not by 4 or 8.','line_number':1248,'multiline':False]
['text':' No model averaging, so the parameters are not updated.','line_number':1251,'multiline':False]
['text':' Coalescing manager (sync mode)','line_number':1254,'multiline':False]
['text':' Coalescing manager (async mode)','line_number':1288,'multiline':False]
['text':' NCCL Batch SEND RECV','line_number':1323,'multiline':False]
['text':' Ensure the process group has been fully initialized (needed by','line_number':1393,'multiline':False]
['text':' the first sub-group batch_isend_irecv call)','line_number':1394,'multiline':False]
['text':' Ensure the process group has been fully initialized (needed by','line_number':1421,'multiline':False]
['text':' the first sub-group batch_isend_irecv call)','line_number':1422,'multiline':False]
['text':' GLOO Batch SEND RECV CPU','line_number':1449,'multiline':False]
['text':' GLOO Batch SEND RECV CPU with provided tags','line_number':1472,'multiline':False]
['text':' NCCL Batch SEND RECV Op Error','line_number':1495,'multiline':False]
['text':' NCCL Batch SEND RECV p2p_op_list Error','line_number':1509,'multiline':False]
['text':' NCCL Batch SEND RECV Mixed Backend Error','line_number':1519,'multiline':False]
['text':' NCCL SEND RECV','line_number':1538,'multiline':False]
['text':' TODO: now that nccl send/recv is supported, there does not seem to','line_number':1543,'multiline':False]
['text':' be a need to have nccl send/recv be tested separately.','line_number':1544,'multiline':False]
['text':' Send mode','line_number':1556,'multiline':False]
['text':' Recv mode','line_number':1562,'multiline':False]
['text':' Event order is not deterministic, so simply assert their shape','line_number':1578,'multiline':False]
['text':' is found in the following list.','line_number':1579,'multiline':False]
['text':' SEND RECV','line_number':1617,'multiline':False]
['text':' Send mode','line_number':1626,'multiline':False]
['text':' Recv mode','line_number':1632,'multiline':False]
['text':' Each rank sends/recvs from all other ranks.','line_number':1644,'multiline':False]
['text':' Event order is not deterministic, so simply assert their shape','line_number':1648,'multiline':False]
['text':' is found in the following list.','line_number':1649,'multiline':False]
['text':' SEND RECV ANY SOURCE','line_number':1682,'multiline':False]
['text':' Recv mode','line_number':1694,'multiline':False]
['text':' Assert the scalar value "sender" that should be','line_number':1711,'multiline':False]
['text':' equal to the rank of the sender is equal to all','line_number':1712,'multiline':False]
['text':' values in the received tensor.','line_number':1713,'multiline':False]
['text':' Send mode','line_number':1716,'multiline':False]
['text':' recv','line_number':1717,'multiline':False]
['text':' irecv','line_number':1718,'multiline':False]
['text':' Each rank sends/recvs from other rank twice.','line_number':1725,'multiline':False]
['text':' Each rank would have 2 * (world_size - 1) sends, verify that','line_number':1734,'multiline':False]
['text':' globally we receive the same amount on the other end.','line_number':1735,'multiline':False]
['text':' SEND RECV WITH TAG','line_number':1788,'multiline':False]
['text':' Recv mode','line_number':1798,'multiline':False]
['text':' Send mode','line_number':1806,'multiline':False]
['text':' Each rank sends/recvs from all other ranks','line_number':1814,'multiline':False]
['text':' ISEND','line_number':1848,'multiline':False]
['text':' Event ordering is not guaranteed, so simply ensure the shapes are','line_number':1879,'multiline':False]
['text':' found in the following map.','line_number':1880,'multiline':False]
['text':' IRECV','line_number':1919,'multiline':False]
['text':' BROADCAST','line_number':1946,'multiline':False]
['text':' REDUCE','line_number':2076,'multiline':False]
['text':' REDUCE TWICE','line_number':2331,'multiline':False]
['text':' Test reduce_scatter_tensor accepting single tensor as input','line_number':2470,'multiline':False]
['text':' Concatenated input','line_number':2502,'multiline':False]
['text':' Check result','line_number':2507,'multiline':False]
['text':' Stacked input','line_number':2512,'multiline':False]
['text':' Check result','line_number':2517,'multiline':False]
['text':' Should be the same as the result in concatenated case','line_number':2518,'multiline':False]
['text':' Calling result right the work is finished should throw exception.','line_number':2543,'multiline':False]
['text':' Here we have a race condition, we may not assume the work is not','line_number':2544,'multiline':False]
['text':' finished by the time we run next lines.','line_number':2545,'multiline':False]
['text':' Exception was not raised, ensure is_completed()','line_number':2553,'multiline':False]
['text':' In case of NCCL we should be able to retrieve pointer to the result','line_number':2559,'multiline':False]
['text':' even before work is finished.','line_number':2560,'multiline':False]
['text':' TODO: move this test to use torch.profiler once kineto issues are','line_number':2588,'multiline':False]
['text':' fixed internally.','line_number':2589,'multiline':False]
['text':' We are only interested in the backend's implementation not the dispatcher wrapper.','line_number':2597,'multiline':False]
['text':' DETAIL debug mode can use a pg wrapper that issues more collectives','line_number':2601,'multiline':False]
['text':' under the hood','line_number':2602,'multiline':False]
['text':' Verify tensor shapes if given','line_number':2609,'multiline':False]
['text':' DETAIL debug mode can use a pg wrapper that issues more collectives','line_number':2610,'multiline':False]
['text':' under the hood','line_number':2611,'multiline':False]
['text':' ALL REDUCE','line_number':2622,'multiline':False]
['text':' Currently, only Gloo backend has profiling tested with CUDA enabled.','line_number':2657,'multiline':False]
['text':' Only run cuda profiling test for one rank to speed up since','line_number':2658,'multiline':False]
['text':' running with different src_rank does not affect the correctness.','line_number':2659,'multiline':False]
['text':' SPARSE ALL REDUCE','line_number':2945,'multiline':False]
['text':' ALL REDUCE - COALESCED','line_number':2970,'multiline':False]
['text':' SCATTER','line_number':3190,'multiline':False]
['text':' Specify scatter_list argument only on source rank.','line_number':3234,'multiline':False]
['text':' Don't specify src argument.','line_number':3243,'multiline':False]
['text':' GATHER','line_number':3313,'multiline':False]
['text':' Specify gather_list argument only on destination rank.','line_number':3353,'multiline':False]
['text':' Don't specify dst argument.','line_number':3362,'multiline':False]
['text':' ALL GATHER','line_number':3411,'multiline':False]
['text':' Test all_gather accepting single tensor as output','line_number':3537,'multiline':False]
['text':' Concatenated output','line_number':3570,'multiline':False]
['text':' Check result','line_number':3576,'multiline':False]
['text':' Concatenate all blocks into a bigger tensor','line_number':3577,'multiline':False]
['text':' Stacked output','line_number':3591,'multiline':False]
['text':' Check result','line_number':3597,'multiline':False]
['text':' Stack all blocks into a bigger tensor','line_number':3598,'multiline':False]
['text':' TODO: Instead we should probably go through _rank_not_in_group','line_number':3635,'multiline':False]
['text':' mechanism to disable sending tensors','line_number':3636,'multiline':False]
['text':' Make sure we create tensors of incompatible sizes, e.g.','line_number':3639,'multiline':False]
['text':' [1], [2x2], [3x3x3] ... to be sent in one batch','line_number':3640,'multiline':False]
['text':' AllToAll','line_number':3744,'multiline':False]
['text':' BARRIER','line_number':4079,'multiline':False]
['text':' seconds','line_number':4083,'multiline':False]
['text':' sleep a little bit longer','line_number':4092,'multiline':False]
['text':' Use higher timeout for the instance where the test runs','line_number':4104,'multiline':False]
['text':' against a subgroup and uses a CUDA tensor for expected time.','line_number':4105,'multiline':False]
['text':' The CUDA initialization for the participating processes can','line_number':4106,'multiline':False]
['text':' take long enough for the barrier timeout to trigger on the','line_number':4107,'multiline':False]
['text':' process that doesn't participate in the group.','line_number':4108,'multiline':False]
['text':' global_bs for DDP should be divisible by WORLD_SIZE','line_number':4184,'multiline':False]
['text':' END TO END TEST FOR DISTRIBUTEDDATAPARALLEL','line_number':4192,'multiline':False]
['text':' single cpu/gpu training','line_number':4226,'multiline':False]
['text':' DDP training, DDP scatters subsets of input_cpu to nodes/GPUs','line_number':4234,'multiline':False]
['text':' Update weights and run a second iteration to shake out errors','line_number':4244,'multiline':False]
['text':' Shuffle the input so that DDP input is different','line_number':4255,'multiline':False]
['text':' save the model in the middle and reload','line_number':4258,'multiline':False]
['text':' Run a simple end to end DDP model, use result of single node model','line_number':4285,'multiline':False]
['text':' as baseline','line_number':4286,'multiline':False]
['text':' cpu training setup','line_number':4288,'multiline':False]
['text':' single gpu training setup','line_number':4291,'multiline':False]
['text':' DDP training setup','line_number':4295,'multiline':False]
['text':' test serializable/unserializable','line_number':4308,'multiline':False]
['text':' dummy data initialization','line_number':4318,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':4322,'multiline':False]
['text':' Run a simple end to end DDP-CPU model, use result of single node','line_number':4337,'multiline':False]
['text':' model as baseline','line_number':4338,'multiline':False]
['text':' cpu training setup','line_number':4341,'multiline':False]
['text':' DDP-CPU training setup','line_number':4344,'multiline':False]
['text':' dummy data initialization','line_number':4350,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':4354,'multiline':False]
['text':' a module without gradients shouldn't be accepted','line_number':4388,'multiline':False]
['text':' Verify DDP doesn't throw when ran with create_graph=True.','line_number':4425,'multiline':False]
['text':' Although we do warn about potential issues, please see','line_number':4426,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/63929 for details.','line_number':4427,'multiline':False]
['text':' grad tensors should require grad.','line_number':4429,'multiline':False]
['text':' Clear gradients manually','line_number':4447,'multiline':False]
['text':' Forward + BW','line_number':4452,'multiline':False]
['text':' For each worker, the gradient on the weight should be worker_rank.','line_number':4456,'multiline':False]
['text':' All-reducing the gradient averages should give us the gradient','line_number':4459,'multiline':False]
['text':' average. If not, then one of the workers has not correctly','line_number':4460,'multiline':False]
['text':' written back the averaged gradient before this all-reduce call.','line_number':4461,'multiline':False]
['text':' Hook not registered yet, so should be empty','line_number':4498,'multiline':False]
['text':' Hook not registered yet, so should be empty','line_number':4510,'multiline':False]
['text':' No hook registered','line_number':4516,'multiline':False]
['text':' Hook not registered yet, so should be empty','line_number':4522,'multiline':False]
['text':' After second forward pass, hook should still be empty string','line_number':4524,'multiline':False]
['text':' Note: DETAIL debug mode logs DDP logging data to stdout and','line_number':4531,'multiline':False]
['text':' thus accesses std::map, which fills in a default value for the','line_number':4532,'multiline':False]
['text':' type if it didn't exist.','line_number':4533,'multiline':False]
['text':' Enable determinism in cudnn operators','line_number':4557,'multiline':False]
['text':' Create DDP model that runs optimizer in fused fashion.','line_number':4561,'multiline':False]
['text':' Create DDP model with no hook that does optimizer after','line_number':4571,'multiline':False]
['text':' backward.','line_number':4572,'multiline':False]
['text':' Register a fused optimizer that will run optimizer in step','line_number':4588,'multiline':False]
['text':' with allreduce.','line_number':4589,'multiline':False]
['text':' API where optim_params is specified.','line_number':4592,'multiline':False]
['text':' API where optim_params is omitted','line_number':4600,'multiline':False]
['text':' Verify parameters are equal initially.','line_number':4613,'multiline':False]
['text':' Save old parameters to later verify optimizer modified them.','line_number':4620,'multiline':False]
['text':' Run optimizer with hook model.','line_number':4625,'multiline':False]
['text':' Run regular model.','line_number':4634,'multiline':False]
['text':' Now verify parameters are equal.','line_number':4644,'multiline':False]
['text':' Verify optimizer modified appropriate parameter set,','line_number':4651,'multiline':False]
['text':' otherwise they'd be trivially equal above.','line_number':4652,'multiline':False]
['text':' Untouched params should be equal','line_number':4658,'multiline':False]
['text':' Parameters to ignore are in the format {module_name}.{param_name}','line_number':4751,'multiline':False]
['text':' test named_params=False, just check if returns the expected','line_number':4765,'multiline':False]
['text':' no of parameters.','line_number':4766,'multiline':False]
['text':' Need to seed to ensure inputs are unique across rank. Otherwise,','line_number':4781,'multiline':False]
['text':' allreduce won't have any effect.','line_number':4782,'multiline':False]
['text':' Test a simple linear as well as a ResNet model.','line_number':4787,'multiline':False]
['text':' Enable determinism in cudnn operators','line_number':4822,'multiline':False]
['text':' runs optimizer as well','line_number':4836,'multiline':False]
['text':' set_to_none for regular optimizer to match in backward','line_number':4848,'multiline':False]
['text':' case.','line_number':4849,'multiline':False]
['text':' Parameters to ignore are in the format {module_name}.{param_name}','line_number':4882,'multiline':False]
['text':' a.weight did not go through allreduce, so optimizer acted on local','line_number':4906,'multiline':False]
['text':' gradient, which should be different across ranks. Remaining params','line_number':4907,'multiline':False]
['text':' should be equal.','line_number':4908,'multiline':False]
['text':' Parameters to ignore are in the format {module_name}.{param_name}','line_number':4934,'multiline':False]
['text':' ignored params should not have _mp_param or _fp_param fields.','line_number':4949,'multiline':False]
['text':' noqa: B902','line_number':4980,'multiline':False]
['text':' Buffers are casted in constructor.','line_number':4998,'multiline':False]
['text':' Each param should have an mp_param in the lower precision, and','line_number':5000,'multiline':False]
['text':' an fp_param in the higher precision.','line_number':5001,'multiline':False]
['text':' Verify gradient synchronization and params and grads are fp32.','line_number':5009,'multiline':False]
['text':' Only param that doesn't require grad','line_number':5013,'multiline':False]
['text':' Clear gradients manually.','line_number':5074,'multiline':False]
['text':' Forward + BW','line_number':5082,'multiline':False]
['text':' For each worker, the gradient on the weight should be worker_rank.','line_number':5086,'multiline':False]
['text':' Verify hook grad with expected.','line_number':5098,'multiline':False]
['text':' Verify hook grad with vanilla allreduce','line_number':5104,'multiline':False]
['text':' process_group is passed in to both DDP and comm. hook','line_number':5125,'multiline':False]
['text':' Although we start run local SGD at iteration 10, since we still use the global process group to run it,','line_number':5160,'multiline':False]
['text':' the post-LocalSGD actually still allreduces gradients globally for the remaining iterations.','line_number':5161,'multiline':False]
['text':' Only validate the warmup iterations before local SGD is applied,','line_number':5168,'multiline':False]
['text':' because when `post_local_gradient_allreduce` is disabled, the gradients will not be synchronized at all.','line_number':5169,'multiline':False]
['text':' Note that in practice a model averager has to be applied to run model averaging,','line_number':5170,'multiline':False]
['text':' so local gradient averaging is not necessary.','line_number':5171,'multiline':False]
['text':' When `subgroup` is None, it is equivalent to the subgroup on the each node.','line_number':5185,'multiline':False]
['text':' For this single-node test environment, the intra-node process group is equivalent to','line_number':5186,'multiline':False]
['text':' the global process group.','line_number':5187,'multiline':False]
['text':' Since we start local SGD later than the total number of 100 iterations,','line_number':5196,'multiline':False]
['text':' no local SGD actually is executed, and we don't even need to provide a subgroup for this case.','line_number':5197,'multiline':False]
['text':' FIXME: Add testing for gloo/CUDA','line_number':5260,'multiline':False]
['text':' ensure accumulate grads works with no_grad => no grads are accumulated.','line_number':5292,'multiline':False]
['text':' check two model parameters over num_iters iterations','line_number':5298,'multiline':False]
['text':' accumulate grads locally','line_number':5310,'multiline':False]
['text':' sync grads','line_number':5314,'multiline':False]
['text':' Shuffle the input so that DDP input is different','line_number':5325,'multiline':False]
['text':' Multiply the result by 2.','line_number':5400,'multiline':False]
['text':' Divide the result by 2 * world_size.','line_number':5404,'multiline':False]
['text':' test set static graph twice','line_number':5457,'multiline':False]
['text':' test output_device','line_number':5466,'multiline':False]
['text':' test device_ids','line_number':5475,'multiline':False]
['text':' Creates model and optimizer in default precision','line_number':5487,'multiline':False]
['text':' Creates a GradScaler once at the beginning of training.','line_number':5491,'multiline':False]
['text':' verify grads are none before training','line_number':5502,'multiline':False]
['text':' Runs the forward pass with autocasting.','line_number':5509,'multiline':False]
['text':' Scales loss.  Calls backward() on scaled loss to create scaled gradients.','line_number':5514,'multiline':False]
['text':' Backward passes under autocast are not recommended.','line_number':5515,'multiline':False]
['text':' Backward ops run in the same dtype autocast chose for corresponding forward ops.','line_number':5516,'multiline':False]
['text':' verify grads are not none and are valid during training','line_number':5519,'multiline':False]
['text':' scaler.step() first unscales the gradients of the optimizer's assigned params.','line_number':5526,'multiline':False]
['text':' If these gradients do not contain infs or NaNs, optimizer.step() is then called,','line_number':5527,'multiline':False]
['text':' otherwise, optimizer.step() is skipped.','line_number':5528,'multiline':False]
['text':' Updates the scale for next iteration.','line_number':5531,'multiline':False]
['text':' Shuffle the input so that DDP input is different','line_number':5534,'multiline':False]
['text':' Run a simple end to end DDP model, use result of single node model','line_number':5569,'multiline':False]
['text':' as baseline','line_number':5570,'multiline':False]
['text':' cpu training setup','line_number':5572,'multiline':False]
['text':' single gpu training setup','line_number':5575,'multiline':False]
['text':' DDP training setup','line_number':5579,'multiline':False]
['text':' test serializable/unserializable','line_number':5586,'multiline':False]
['text':' data initialization','line_number':5596,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':5601,'multiline':False]
['text':' Process group cannot be pickled in some environments,','line_number':5634,'multiline':False]
['text':' so cannot deep copy an averager. See:','line_number':5635,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/74737#pullrequestreview-922487496','line_number':5636,'multiline':False]
['text':' Also check if the built-in step counters are the same to prevent a bug like #74737.','line_number':5662,'multiline':False]
['text':' Check that we didn't hit the trivial case','line_number':5723,'multiline':False]
['text':' Check if dummy averager was initialized to a correct value','line_number':5725,'multiline':False]
['text':' Remove 'step' entry from a checkpoint.','line_number':5728,'multiline':False]
['text':' And make sure it is not in the state dictionary','line_number':5729,'multiline':False]
['text':' Check if checkpoint without a 'step' entry invokes a warning','line_number':5733,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':5859,'multiline':False]
['text':' DDP does not support replicating BN layers within a process, hence','line_number':5884,'multiline':False]
['text':' testing with one module replica per process','line_number':5885,'multiline':False]
['text':' test output_device','line_number':5900,'multiline':False]
['text':' test device_ids','line_number':5910,'multiline':False]
['text':' DDP does not support replicating BN layers within a process, hence','line_number':5929,'multiline':False]
['text':' testing with one module replica per process','line_number':5930,'multiline':False]
['text':' DDP does not support replicating BN layers within a process, hence','line_number':5953,'multiline':False]
['text':' testing with one module replica per process','line_number':5954,'multiline':False]
['text':' single gpu training setup','line_number':5959,'multiline':False]
['text':' DDP training setup','line_number':5963,'multiline':False]
['text':' disabling cudnn.','line_number':5974,'multiline':False]
['text':' SyncBatchNorm goes through native_batch_norm kernel, this avoids the','line_number':5975,'multiline':False]
['text':' numerical issue created by the divergent code path.','line_number':5976,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':5978,'multiline':False]
['text':' DDP does not support replicating BN layers within a process, hence','line_number':6000,'multiline':False]
['text':' testing with one module replica per process','line_number':6001,'multiline':False]
['text':' single gpu training setup','line_number':6006,'multiline':False]
['text':' DDP training setup','line_number':6010,'multiline':False]
['text':' disabling cudnn.','line_number':6021,'multiline':False]
['text':' SyncBatchNorm goes through native_batch_norm kernel, this avoids the','line_number':6022,'multiline':False]
['text':' numerical issue created by the divergent code path.','line_number':6023,'multiline':False]
['text':' check two model parameters over 5 iterations','line_number':6025,'multiline':False]
['text':' only do single GPU per process','line_number':6089,'multiline':False]
['text':' cpu training setup','line_number':6092,'multiline':False]
['text':' Check that forward/backward do not error with dtype mismatch','line_number':6121,'multiline':False]
['text':' dummy data initialization','line_number':6138,'multiline':False]
['text':' DDP training, DDP scatters subsets of input to nodes/GPUs','line_number':6150,'multiline':False]
['text':' Verify DDP logging data is sampled as expected','line_number':6161,'multiline':False]
['text':' If it has ran more than 10 iterations and this is','line_number':6162,'multiline':False]
['text':' the sampled iteration for measuring run time stats,','line_number':6163,'multiline':False]
['text':' the run time stats for this idx-th iteration will not','line_number':6164,'multiline':False]
['text':' be zeros.','line_number':6165,'multiline':False]
['text':' if the idx-th iteration is not sampled to set runtime stats,','line_number':6187,'multiline':False]
['text':' ddp_logging_data.iteration will not be updated to current','line_number':6188,'multiline':False]
['text':' iteration.','line_number':6189,'multiline':False]
['text':' Shuffle the input so that DDP input is different','line_number':6192,'multiline':False]
['text':' output_device is -1 in default if it is not set, e.g.','line_number':6213,'multiline':False]
['text':' output_device of CPU training is -1.','line_number':6214,'multiline':False]
['text':' test runtime logging fields','line_number':6272,'multiline':False]
['text':' Note: DETAIL debug mode logs DDP logging data to stdout and','line_number':6273,'multiline':False]
['text':' thus accesses std::map, which fills in a default value for the','line_number':6274,'multiline':False]
['text':' type if it didn't exist.','line_number':6275,'multiline':False]
['text':' It is hard to test accurate latency, but it can test whether the latency is','line_number':6288,'multiline':False]
['text':' a valid value and in the expected range.','line_number':6289,'multiline':False]
['text':' Test host-side times are roughly in the order that we expect','line_number':6303,'multiline':False]
['text':' test larger net with mixed data types, verify multiple bucket sizes','line_number':6326,'multiline':False]
['text':' test runtime logging fields','line_number':6364,'multiline':False]
['text':' It is hard to test accurate latency, but it can test whether the latency is','line_number':6365,'multiline':False]
['text':' a valid value and in the expected range.','line_number':6366,'multiline':False]
['text':' Test host-side times are roughly in the order that we expect','line_number':6379,'multiline':False]
['text':' DDP training, DDP scatters subsets of input to nodes/GPUs','line_number':6413,'multiline':False]
['text':' Verify error was logged in ddp_logging_data.','line_number':6423,'multiline':False]
['text':' When adopting `convert_sync_batchnorm` to convert a `nn.modules`,','line_number':6428,'multiline':False]
['text':' it need to recursively pass the `process_group` in the module when the `SyncBatchNorm`','line_number':6429,'multiline':False]
['text':' is nested in a sub-module or sub-sub-module (e.g. resnet50 in torchvision.models).','line_number':6430,'multiline':False]
['text':' Only destination rank tensor is expected to have final result.','line_number':6448,'multiline':False]
['text':' Run all_reduce with PRODUCT','line_number':6459,'multiline':False]
['text':' Ensure that all ranks contributing True (cast to 1) results in the','line_number':6466,'multiline':False]
['text':' correct reduction.','line_number':6467,'multiline':False]
['text':' Run all_reduce with SUM','line_number':6472,'multiline':False]
['text':' TODO: NCCL backend does not work correctly for bitwise reduction ops','line_number':6478,'multiline':False]
['text':' (see https://github.com/pytorch/pytorch/issues/41362). Add tests for','line_number':6479,'multiline':False]
['text':' these once it is supported.','line_number':6480,'multiline':False]
['text':' Preserve a copy of the tensor to compare against after allgather.','line_number':6488,'multiline':False]
['text':' Ensure that the input tensor is not modified, since this collective','line_number':6500,'multiline':False]
['text':' does not modify its input.','line_number':6501,'multiline':False]
['text':' Run reduce() with product op','line_number':6509,'multiline':False]
['text':' Ensure that all ranks contributing True (cast to 1) results in the','line_number':6514,'multiline':False]
['text':' correct reduction.','line_number':6515,'multiline':False]
['text':' Now allgather and ensure the tensors are equal.','line_number':6542,'multiline':False]
['text':' Tests padding of distributed sampler.','line_number':6558,'multiline':False]
['text':' Simulates the 'casual' dataset size','line_number':6561,'multiline':False]
['text':' Simulates the 'tiny' dataset size','line_number':6565,'multiline':False]
['text':' Specifying drop_last=True will cause the tail of the data to be dropped.','line_number':6571,'multiline':False]
['text':' The effective dataset size should be the greatest integer that is <=','line_number':6577,'multiline':False]
['text':' dataset_size that is divisible by the world_size. This is to ensure each','line_number':6578,'multiline':False]
['text':' rank processes the same number of samples.','line_number':6579,'multiline':False]
['text':' Ensure that each rank processes the same number of samples.','line_number':6591,'multiline':False]
['text':' drop_last=False is the default and will add additional indices to be sampled,','line_number':6603,'multiline':False]
['text':' increasing the effective dataset size.','line_number':6604,'multiline':False]
['text':' The effective dataset size is the smallest integer that is >= dataset_size','line_number':6610,'multiline':False]
['text':' and divisible by the world size.','line_number':6611,'multiline':False]
['text':' Ensure that each rank processes the same number of samples.','line_number':6617,'multiline':False]
['text':' Ensure additional samples are padded even when','line_number':6620,'multiline':False]
['text':' the extremely small dataset is given.','line_number':6621,'multiline':False]
['text':' Only set device for NCCL backend since it must use GPUs.','line_number':6636,'multiline':False]
['text':' Case where rank != GPU device.','line_number':6642,'multiline':False]
['text':' If GPU test, add object with GPU tensor','line_number':6646,'multiline':False]
['text':' Ensure stateful objects can be gathered','line_number':6681,'multiline':False]
['text':' Case where rank != GPU device.','line_number':6687,'multiline':False]
['text':' If GPU test, add object with GPU tensor','line_number':6691,'multiline':False]
['text':' Validate errors when objects can't be pickled.','line_number':6714,'multiline':False]
['text':' Helper to validate synchronization of nets across ranks.','line_number':6747,'multiline':False]
['text':' Check that all tensors in module's state_dict() are equal.','line_number':6749,'multiline':False]
['text':' Test that after calling _sync_module_states, models across ranks','line_number':6764,'multiline':False]
['text':' are the same and are equal to the model on the input rank.','line_number':6765,'multiline':False]
['text':' Seed to ensure that ranks are initialized with different initial models.','line_number':6769,'multiline':False]
['text':' Assert params are different','line_number':6777,'multiline':False]
['text':' tensor from another rank should be different.','line_number':6788,'multiline':False]
['text':' Now all model params should be the same.','line_number':6798,'multiline':False]
['text':' Since the network params were broadcast from rank_to_broadcast, validate that','line_number':6800,'multiline':False]
['text':' they are the same as new_model on rank_to_broadcast.','line_number':6801,'multiline':False]
['text':' Test gradient division during training with join() API. If','line_number':6813,'multiline':False]
['text':' divide_by_initial_world_size=False, we scale by the effective world','line_number':6814,'multiline':False]
['text':' size when allreducing grads.','line_number':6815,'multiline':False]
['text':' The grad is always expected_grad, since we divide by the number','line_number':6833,'multiline':False]
['text':' of currently active processes and inactive processes contribute','line_number':6834,'multiline':False]
['text':' zero gradient. If we kept dividing by static initial world','line_number':6835,'multiline':False]
['text':' size as processes leave, the grad would be smaller.','line_number':6836,'multiline':False]
['text':' Avoid accumulating grads so that it's the same every iteration','line_number':6840,'multiline':False]
['text':' If divide_by_initial_world_size=True (default), we always scale grads','line_number':6844,'multiline':False]
['text':' by the initial world_size.','line_number':6845,'multiline':False]
['text':' Avoid accumulating grad so that it's the same every iteration.','line_number':6860,'multiline':False]
['text':' Broadcast is called during rebuild_buckets','line_number':6893,'multiline':False]
['text':' Run DDP with profiling for a few iterations, then enable profiling','line_number':6898,'multiline':False]
['text':' for a single pass, and ensure it is recorded. This tests that the','line_number':6899,'multiline':False]
['text':' thread local state is correctly updated.','line_number':6900,'multiline':False]
['text':' Now enable the profiler.','line_number':6909,'multiline':False]
['text':' Ensure searching unused parameters was profiled','line_number':6920,'multiline':False]
['text':' Verifies equivalence with model training locally and with DDP under','line_number':6950,'multiline':False]
['text':' the join context manager.','line_number':6951,'multiline':False]
['text':' run local model','line_number':6962,'multiline':False]
['text':' run DDP model with join API','line_number':6972,'multiline':False]
['text':' Validate model state dicts are equal','line_number':6989,'multiline':False]
['text':' Ensure all outstanding GPU work is completed so this test runs independently.','line_number':7006,'multiline':False]
['text':' Bucket_cap_mb is intentionally low to test allreduce scheduling when','line_number':7008,'multiline':False]
['text':' there are many buckets.','line_number':7009,'multiline':False]
['text':' Register hook if specified','line_number':7016,'multiline':False]
['text':' Determine num iters for this rank via the passed in mapping.','line_number':7021,'multiline':False]
['text':' If we throw when earliest rank terminates, we should ensure','line_number':7023,'multiline':False]
['text':' that we iterate for that minimum number of times.','line_number':7024,'multiline':False]
['text':' Early termination rank(s)','line_number':7033,'multiline':False]
['text':' Non early termination rank','line_number':7038,'multiline':False]
['text':' Use model.no_sync() to disable grad synchronization every','line_number':7050,'multiline':False]
['text':' sync_interval.','line_number':7051,'multiline':False]
['text':' Ensure completion of GPU kernels (including allreduce). If the','line_number':7063,'multiline':False]
['text':' join API is not properly implemented, then this should hang','line_number':7064,'multiline':False]
['text':' since the allreduce will hang.','line_number':7065,'multiline':False]
['text':' Ensure we iterated min_num_iters times.','line_number':7069,'multiline':False]
['text':' Ensure we iterated at least min_num_iters times.','line_number':7072,'multiline':False]
['text':' Ensure completion of all GPU kernels.','line_number':7075,'multiline':False]
['text':' When throwing on early rank termination, we do not','line_number':7077,'multiline':False]
['text':' broadcast model state from an authoritative rank. All models','line_number':7078,'multiline':False]
['text':' should already be in sync.','line_number':7079,'multiline':False]
['text':' All ranks should have agreed on the same authoritative_rank!','line_number':7082,'multiline':False]
['text':' Ensure that all models are the same across ranks after all have joined.','line_number':7095,'multiline':False]
['text':' Ensure that running with DDP uneven inputs was logged.','line_number':7097,'multiline':False]
['text':' Tests that uneven inputs join handler correctly throws StopIteration','line_number':7108,'multiline':False]
['text':' for models with SyncBN or general collective comm when','line_number':7109,'multiline':False]
['text':' throw_on_early_termination=True.','line_number':7110,'multiline':False]
['text':' Early termination rank(s)','line_number':7136,'multiline':False]
['text':' Non early termination rank','line_number':7142,'multiline':False]
['text':' Verify model equivalence','line_number':7158,'multiline':False]
['text':' Create a variety of models to run uneven input tests on.','line_number':7169,'multiline':False]
['text':' Network with batchnorm','line_number':7203,'multiline':False]
['text':' Unused parameter test where rank that does not join early has unused params','line_number':7222,'multiline':False]
['text':' Unused parameter test where rank that does join early has unused params','line_number':7229,'multiline':False]
['text':' Test models that have hook installed.','line_number':7238,'multiline':False]
['text':' Config so that powerSGD runs immediately instead of','line_number':7255,'multiline':False]
['text':' allreduce.','line_number':7256,'multiline':False]
['text':' Add resnet model if we have torchvision installed.','line_number':7267,'multiline':False]
['text':' Test with no_sync every 2, 3, 4, ... iterations.','line_number':7279,'multiline':False]
['text':' 0 iteration tests for when one process does not train model at all, so','line_number':7306,'multiline':False]
['text':' we must shadow the broadcast calls made when rebuilding buckets.','line_number':7307,'multiline':False]
['text':' Generate rank : num_iters mappings for various uneven input scenarios.','line_number':7314,'multiline':False]
['text':' This includes cases where rank 0 joins early and all other ranks join','line_number':7315,'multiline':False]
['text':' later, and scenarios where multiple ranks join early, but at different','line_number':7316,'multiline':False]
['text':' iterations, and later ranks join later.','line_number':7317,'multiline':False]
['text':' if num_early_join_ranks > 1, ranks > 0 that will join early','line_number':7325,'multiline':False]
['text':' iterate offset//2 more times than rank 0, to test nodes','line_number':7326,'multiline':False]
['text':' depleting inputs at different times.','line_number':7327,'multiline':False]
['text':' tests that if net.join() with enable=False is specified, DDP works as','line_number':7363,'multiline':False]
['text':' expected with even inputs.','line_number':7364,'multiline':False]
['text':' Clear grads','line_number':7374,'multiline':False]
['text':' Validate gradients to ensure that we divide by the correct','line_number':7382,'multiline':False]
['text':' world_size when join mode is disabled.','line_number':7383,'multiline':False]
['text':' Tests that exceptions during training are correctly propagated by the','line_number':7397,'multiline':False]
['text':' context manager.','line_number':7398,'multiline':False]
['text':' Only set device for NCCL backend since it must use GPUs.','line_number':7423,'multiline':False]
['text':' Case where rank != GPU device.','line_number':7424,'multiline':False]
['text':' If GPU test, add object with GPU tensor','line_number':7431,'multiline':False]
['text':' Create Tensor with > 2^31 Bytes storage requirements','line_number':7436,'multiline':False]
['text':' Only on FBCODE as testing OOMs in OSS','line_number':7437,'multiline':False]
['text':' Single object test with device specified. Backend="gloo", device=cpu','line_number':7445,'multiline':False]
['text':' Single object test with device specified. Backend="gloo", device=current_device+1','line_number':7455,'multiline':False]
['text':' The test is gated by the fact GPU count is the same as world size to avoid the case','line_number':7456,'multiline':False]
['text':' when backend is gloo but there is no multiple GPU devices.','line_number':7457,'multiline':False]
['text':' Single object test with device specified. Backend="nccl", device=current_device+1','line_number':7467,'multiline':False]
['text':' Single object test: backward compatibility with device unspecified','line_number':7477,'multiline':False]
['text':' Multiple input objects test','line_number':7484,'multiline':False]
['text':' Proxy that will be materialized to another architecture later.','line_number':7516,'multiline':False]
['text':' (after wrapping model with DDP)','line_number':7517,'multiline':False]
['text':' Ensure the test works for both find_unused_parameter and broadcast_buffer settings.','line_number':7529,'multiline':False]
['text':' Note that the model can have different shape buffers if we pass','line_number':7534,'multiline':False]
['text':' them in to be ignored as well.','line_number':7535,'multiline':False]
['text':' Specify that we should ignore proxy_params since it will be','line_number':7554,'multiline':False]
['text':' materialized later.','line_number':7555,'multiline':False]
['text':' Materialize new params. These are not registered in DDP and thus','line_number':7566,'multiline':False]
['text':' don't have autograd hooks installed on them.','line_number':7567,'multiline':False]
['text':' local model with the new materialized parameters.','line_number':7570,'multiline':False]
['text':' materialized param grad is not touched by DDP, so its grad should','line_number':7578,'multiline':False]
['text':' be the same as if running locally.','line_number':7579,'multiline':False]
['text':' fc1 parameter grad should still be different, due to allreduce.','line_number':7585,'multiline':False]
['text':' Proxy module grad should not be touched','line_number':7591,'multiline':False]
['text':' Synchronize since we run multiple iterations of this test, to','line_number':7595,'multiline':False]
['text':' isolate failure hangs.','line_number':7596,'multiline':False]
['text':' On 2nd iteration, this will fail during rebuild_buckets,','line_number':7624,'multiline':False]
['text':' but we should report an error regarding unused parameters','line_number':7625,'multiline':False]
['text':' since that is the underlying root cause.','line_number':7626,'multiline':False]
['text':' In debug mode, should show parameters that weren't reduced.','line_number':7637,'multiline':False]
['text':' Without debug mode, should show suggestion to use debug mode.','line_number':7638,'multiline':False]
['text':' When find_unused_parameters=True, ensure we mark unused parameters','line_number':7661,'multiline':False]
['text':' even if they share gradient accumulators.','line_number':7662,'multiline':False]
['text':' net1, bias, and net1.bias are all unused params.','line_number':7666,'multiline':False]
['text':' net1.bias and self.bias are names for the same underlying','line_number':7669,'multiline':False]
['text':' parameter, so they share the same grad acc. This caused','line_number':7670,'multiline':False]
['text':' the bug reported in https://github.com/pytorch/pytorch/issues/41324.','line_number':7671,'multiline':False]
['text':' To test https://github.com/pytorch/pytorch/issues/61982','line_number':7690,'multiline':False]
['text':' Handlers for specific types of validation we want to do based on','line_number':7707,'multiline':False]
['text':' the input type.','line_number':7708,'multiline':False]
['text':' noqa: B902','line_number':7745,'multiline':False]
['text':' noqa: B902','line_number':7749,'multiline':False]
['text':' Similar to scatter, the recursive to in the single-device','line_number':7750,'multiline':False]
['text':' case does not move tensors if they are in a custom type.','line_number':7751,'multiline':False]
['text':' CPU tuple input, should be moved to the proper device before call','line_number':7765,'multiline':False]
['text':' to forward.','line_number':7766,'multiline':False]
['text':' List CPU input, should be moved to proper device before call to','line_number':7770,'multiline':False]
['text':' forward.','line_number':7771,'multiline':False]
['text':' Custom type containing tensor. The type is maintained, but the','line_number':7774,'multiline':False]
['text':' device is not propagated (which is what happens with scatter too)','line_number':7775,'multiline':False]
['text':' NamedTuple input. The type should be maintained and tensor inputs','line_number':7778,'multiline':False]
['text':' should be moved to the correct device as in scatter.','line_number':7779,'multiline':False]
['text':' dictionary input.','line_number':7791,'multiline':False]
['text':' noqa: B902','line_number':7808,'multiline':False]
['text':' noqa: B902','line_number':7812,'multiline':False]
['text':' Without NamedTuple support, this would be of type tuple.','line_number':7813,'multiline':False]
['text':' The following would fail if DDP does not propagate NamedTuples correctly.','line_number':7827,'multiline':False]
['text':' Control flow that is the same across ranks.','line_number':7837,'multiline':False]
['text':' On even iterations, 2nd param goes unused, on odd iterations,','line_number':7857,'multiline':False]
['text':' it is used.','line_number':7858,'multiline':False]
['text':' Validate parameter usage.','line_number':7869,'multiline':False]
['text':' Validate appropriate error message when DDP is used with','line_number':7873,'multiline':False]
['text':' find_unused_parameters=False.','line_number':7874,'multiline':False]
['text':' 2nd linear layer is unused','line_number':7891,'multiline':False]
['text':' In debug mode, should show parameters that weren't reduced.','line_number':7899,'multiline':False]
['text':' Without debug mode, should show suggestion to use debug mode.','line_number':7900,'multiline':False]
['text':' unused parameter in the first iteration got used','line_number':7928,'multiline':False]
['text':' in second iteration.','line_number':7929,'multiline':False]
['text':' used parameter in the first iteration got unused','line_number':7942,'multiline':False]
['text':' in second iteration.','line_number':7943,'multiline':False]
['text':' Control flow that is different across ranks.','line_number':7968,'multiline':False]
['text':' Control-flow that is rank and input dependent for the','line_number':7980,'multiline':False]
['text':' model.','line_number':7981,'multiline':False]
['text':' On even iterations, 2nd param goes unused, on odd iterations,','line_number':8008,'multiline':False]
['text':' it is used only on rank 1.','line_number':8009,'multiline':False]
['text':' Validate parameter usage. On odd iterations, 2nd param is only','line_number':8022,'multiline':False]
['text':' used on rank 1.','line_number':8023,'multiline':False]
['text':' Validate appropriate error message when DDP is used with','line_number':8026,'multiline':False]
['text':' find_unused_parameters=False.','line_number':8027,'multiline':False]
['text':' In debug mode, should show parameters that weren't reduced.','line_number':8051,'multiline':False]
['text':' Without debug mode, should show suggestion to use debug mode.','line_number':8052,'multiline':False]
['text':' Ensure errors are raised upon incorrect arguments.','line_number':8091,'multiline':False]
['text':' Set TORCH_NCCL_BLOCKING_WAIT and use a new NCCL group to improve test','line_number':8115,'multiline':False]
['text':' determinism.','line_number':8116,'multiline':False]
['text':' Create a valid model. The constructor initializes the logger that we use later.','line_number':8123,'multiline':False]
['text':' We never actually use the rest of the model - we only need its logger.','line_number':8124,'multiline':False]
['text':' if we don't pass a logger then we can only check that an exception was thrown.','line_number':8132,'multiline':False]
['text':' Perform gloo-based barrier to ensure one rank doesn't exit test','line_number':8149,'multiline':False]
['text':' early which causes failure with Barrier.sync.','line_number':8150,'multiline':False]
['text':' When running with NCCL backend, we don't expect an error on rank 0,','line_number':8166,'multiline':False]
['text':' rather, it will be taken down by TORCH_NCCL_ASYNC_ERROR_HANDLING. When','line_number':8167,'multiline':False]
['text':' running with Gloo or with debug mode wrapper, we expect the error','line_number':8168,'multiline':False]
['text':' to be caught inline.','line_number':8169,'multiline':False]
['text':' All ranks report same error when there is a # of parameter','line_number':8170,'multiline':False]
['text':' mismatch since we use allgather in the impl.','line_number':8171,'multiline':False]
['text':' Set TORCH_NCCL_BLOCKING_WAIT and use a new NCCL group to improve test','line_number':8197,'multiline':False]
['text':' determinism.','line_number':8198,'multiline':False]
['text':' Create a valid model. The constructor initializes the logger that we use later.','line_number':8208,'multiline':False]
['text':' Modify the model so that the number of parameters are different for each rank.','line_number':8216,'multiline':False]
['text':' This will cause a RuntimeError to be thrown below in _verify_param_shape_across_processes,','line_number':8217,'multiline':False]
['text':' so we can check if the correct error is thrown and is logged.','line_number':8218,'multiline':False]
['text':' We can't do this in the constructor above otherwise the logger will','line_number':8219,'multiline':False]
['text':' not be properly initialized.','line_number':8220,'multiline':False]
['text':' if we pass a logger we can verify that it was logged','line_number':8223,'multiline':False]
['text':' Should only be run by rank 0, and blocking_wait catches and','line_number':8233,'multiline':False]
['text':' reports exception.','line_number':8234,'multiline':False]
['text':' We don't check when self.rank != 0 because the logger doesn't log','line_number':8237,'multiline':False]
['text':' the error "Caught collective operation" as that is not thrown in the reducer.','line_number':8238,'multiline':False]
['text':' Perform gloo-based barrier to ensure one rank doesn't exit test','line_number':8242,'multiline':False]
['text':' early which causes failure with Barrier.sync.','line_number':8243,'multiline':False]
['text':' Should only be run by rank 0, and blocking_wait catches and','line_number':8267,'multiline':False]
['text':' reports exception.','line_number':8268,'multiline':False]
['text':' can't use verify_ddp_error_logged here because net was never properly constructed','line_number':8271,'multiline':False]
['text':' Perform gloo-based barrier to ensure one rank doesn't exit test','line_number':8273,'multiline':False]
['text':' early which causes failure with Barrier.sync.','line_number':8274,'multiline':False]
['text':' Set TORCH_NCCL_BLOCKING_WAIT and use a new NCCL group to improve test','line_number':8286,'multiline':False]
['text':' determinism.','line_number':8287,'multiline':False]
['text':' Creates network with different sized embedding table on different','line_number':8296,'multiline':False]
['text':' ranks. This should throw an error during DDP init.','line_number':8297,'multiline':False]
['text':' Set TORCH_NCCL_BLOCKING_WAIT and use a new NCCL group to improve test','line_number':8312,'multiline':False]
['text':' determinism.','line_number':8313,'multiline':False]
['text':' Creates network with diff # of param across ranks, reducer should','line_number':8323,'multiline':False]
['text':' recognize this and throw appropriate error.','line_number':8324,'multiline':False]
['text':' Tests that certain parameters not getting gradient since the','line_number':8345,'multiline':False]
['text':' output is unused in loss computation is supported. Specifically,','line_number':8346,'multiline':False]
['text':' checks that the grads remain unchanged and are the same as local','line_number':8347,'multiline':False]
['text':' training.','line_number':8348,'multiline':False]
['text':' Ensure that if a param is not used in loss computation, its','line_number':8351,'multiline':False]
['text':' gradient is untouched, i.e. if it is None before it is None after,','line_number':8352,'multiline':False]
['text':' not zero.','line_number':8353,'multiline':False]
['text':' Ensure that gradient corresponding to parameter "a" was not','line_number':8364,'multiline':False]
['text':' touched, i.e. it is None and matches the local grad.','line_number':8365,'multiline':False]
['text':' Use both params in loss computation. Later, "a" will go','line_number':8387,'multiline':False]
['text':' unused and we check to ensure DDP supports this and','line_number':8388,'multiline':False]
['text':' gradients remain the same as local training.','line_number':8389,'multiline':False]
['text':' Model output "a" unused in loss.','line_number':8395,'multiline':False]
['text':' Save grads to compare with them in next iterations.','line_number':8401,'multiline':False]
['text':' parameter "a" of both models should be the same and not change','line_number':8410,'multiline':False]
['text':' Verify grads are the same','line_number':8422,'multiline':False]
['text':' TODO: enable this for general training use cases:','line_number':8458,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/58511.','line_number':8459,'multiline':False]
['text':' Simulates undefined gradients.','line_number':8470,'multiline':False]
['text':' Kick off some allreduce work on all ranks','line_number':8543,'multiline':False]
['text':' Run monitored barrier and ensure it passes','line_number':8546,'multiline':False]
['text':' Check monitored_barrier success with wait_all_ranks=True','line_number':8549,'multiline':False]
['text':' All ranks besides 1 call into barrier, rank 0 should report failure','line_number':8553,'multiline':False]
['text':' while others report gloo error.','line_number':8554,'multiline':False]
['text':' Other ranks should not pass barrier since rank 0 failed.','line_number':8563,'multiline':False]
['text':' We need a barrier since otherwise failed_rank exits too early','line_number':8572,'multiline':False]
['text':' and cause a timeout.','line_number':8573,'multiline':False]
['text':' Tests that monitored_barrier works as expected on non-default','line_number':8578,'multiline':False]
['text':' process groups.','line_number':8579,'multiline':False]
['text':' Other ranks call into monitored_barrier, but this should be a','line_number':8593,'multiline':False]
['text':' noop because they are not part of the subgroup. Verify that','line_number':8594,'multiline':False]
['text':' there are no errors here.','line_number':8595,'multiline':False]
['text':' tests expected behavior when nonzero rank hangs.','line_number':8599,'multiline':False]
['text':' provide sufficient timeout so communicators','line_number':8602,'multiline':False]
['text':' can be initialized in ctor.','line_number':8603,'multiline':False]
['text':' Let all ranks call allreduce first to set up communicators etc.','line_number':8612,'multiline':False]
['text':' Directly simulating error here will run into store issue described','line_number':8613,'multiline':False]
['text':' in https://github.com/pytorch/pytorch/issues/54524.','line_number':8614,'multiline':False]
['text':' All ranks besides 0 call into allreduce. This is to simulate a','line_number':8616,'multiline':False]
['text':' desync across the world, where some ranks call into','line_number':8617,'multiline':False]
['text':' monitored_barrier() and others are stuck in collective comm. In','line_number':8618,'multiline':False]
['text':' practice, we don't need TORCH_NCCL_BLOCKING_WAIT, but we use it in this','line_number':8619,'multiline':False]
['text':' test to ensure it exits cleanly.','line_number':8620,'multiline':False]
['text':' Can get different errors here depending on whether gloo-based','line_number':8622,'multiline':False]
['text':' wrapper PG is enabled or not, since with wrapper pg, it will','line_number':8623,'multiline':False]
['text':' fail in a collective synchronization check and not actually','line_number':8624,'multiline':False]
['text':' call into the nccl pg.','line_number':8625,'multiline':False]
['text':' Rank 0 should report first (in order) timed out rank or all ranks','line_number':8633,'multiline':False]
['text':' depending on wait_all_ranks flag passed into monitored_barrier.','line_number':8634,'multiline':False]
['text':' tests expected behavior when nonzero rank hangs and we want to','line_number':8655,'multiline':False]
['text':' report first timed out rank.','line_number':8656,'multiline':False]
['text':' tests expected behavior when nonzero rank hangs and we want to','line_number':8663,'multiline':False]
['text':' report all timed out ranks.','line_number':8664,'multiline':False]
['text':' tests error when rank 0 exhausts its given timeout.','line_number':8669,'multiline':False]
['text':' Ensure that the first (in sorted order) rank is reported when','line_number':8685,'multiline':False]
['text':' multiple ranks fail to pass the monitored_barrier.','line_number':8686,'multiline':False]
['text':' TODO(#54879): Provide ability to wait and report all failed ranks','line_number':8687,'multiline':False]
['text':' Tests simple case where > 1 rank does not call into monitored','line_number':8708,'multiline':False]
['text':' barrier and verifies all ranks are reported by rank 0.','line_number':8709,'multiline':False]
['text':' Test when DDP is used with ignored parameters.','line_number':8731,'multiline':False]
['text':' Parameters to ignore are in the format {module_name}.{param_name}','line_number':8733,'multiline':False]
['text':' Test errors are raised when DDP and module parameters mismatch.','line_number':8747,'multiline':False]
['text':' This generally indicates a bug with DDP and is not expected to','line_number':8748,'multiline':False]
['text':' happen in user applications.','line_number':8749,'multiline':False]
['text':' Is not tracked by DDP and should not show up in param to','line_number':8791,'multiline':False]
['text':' name mapping.','line_number':8792,'multiline':False]
['text':' self.lin.b param unused','line_number':8823,'multiline':False]
['text':' EmbeddingNetDifferentParams entirely unused: self.embedding_net.embedding and','line_number':8824,'multiline':False]
['text':' self.embedding_net.lin unused.','line_number':8825,'multiline':False]
['text':' Validate that these don't mistakenly show up.','line_number':8860,'multiline':False]
['text':' Validate that each unused param fully qualified name','line_number':8898,'multiline':False]
['text':' shows up in error logs. We do this instead of','line_number':8899,'multiline':False]
['text':' constructing a joined string since order of parameters','line_number':8900,'multiline':False]
['text':' can be different in Reducer. In addition, validate','line_number':8901,'multiline':False]
['text':' param indices show up as well.','line_number':8902,'multiline':False]
['text':' Validate that used param fqns don't show up in error','line_number':8914,'multiline':False]
['text':' logs.','line_number':8915,'multiline':False]
['text':' Validate that ignored param fqns don't show up as unused','line_number':8918,'multiline':False]
['text':' (since DDP does not track them)','line_number':8919,'multiline':False]
['text':' Tests unused parameter reporting when DDP is configured to ignore','line_number':8935,'multiline':False]
['text':' certain parameters.','line_number':8936,'multiline':False]
['text':' tests that DDP module can be run on a single node with no_grad','line_number':8945,'multiline':False]
['text':' or eval setting and there is no hang.','line_number':8946,'multiline':False]
['text':' Barrier since only rank 0 runs inference. Test should be','line_number':8978,'multiline':False]
['text':' much faster than 30s, but this is to avoid flakiness.','line_number':8979,'multiline':False]
['text':' Need to set track_running_stats=False, when track_running_stats=True,','line_number':8991,'multiline':False]
['text':' bn_training is False and sync could not occur in eval model.','line_number':8992,'multiline':False]
['text':' Test sync occurs in training mode.','line_number':8997,'multiline':False]
['text':' SyncBN allgathers stats across all ranks, so verify call to','line_number':9005,'multiline':False]
['text':' all_gather in profiler.','line_number':9006,'multiline':False]
['text':' Only do inference on one rank. If SyncBN did collective stats sync,','line_number':9013,'multiline':False]
['text':' this would hang/error.','line_number':9014,'multiline':False]
['text':' Ensure sync does not occur in eval() mode.','line_number':9025,'multiline':False]
['text':' Most python exceptions in DDP are raised during init before','line_number':9038,'multiline':False]
['text':' reducer is constructed, so we don't have a logger in those cases.','line_number':9039,'multiline':False]
['text':' However, the below is one example where a python error is thrown','line_number':9040,'multiline':False]
['text':' after reducer is constructed.','line_number':9041,'multiline':False]
['text':' Tests for static graph training when outputs are not just tensors','line_number':9059,'multiline':False]
['text':' but can be (nested) tuple, list, dict, etc.','line_number':9060,'multiline':False]
['text':' Tests case where module returns tensor that does not require grad.','line_number':9147,'multiline':False]
['text':' Set of unused parameters don't change across iterations','line_number':9202,'multiline':False]
['text':' Set of unused parameters dynamically change','line_number':9218,'multiline':False]
['text':' Test from https://github.com/pytorch/pytorch/issues/60733','line_number':9232,'multiline':False]
['text':' Test multiple tensors as well as newly created tensors','line_number':9253,'multiline':False]
['text':' within a struct.','line_number':9254,'multiline':False]
['text':' Since buffer reduction is done pre-forward, simulate it for','line_number':9343,'multiline':False]
['text':' no hook case here.','line_number':9344,'multiline':False]
['text':' Simulate allreduce appropriately depending on hook location.','line_number':9345,'multiline':False]
['text':' if return_futures, they are only awaited on by DDP','line_number':9358,'multiline':False]
['text':' at the end of the backwards pass for maximum overlap.','line_number':9359,'multiline':False]
['text':' Note that when custom hooks return futures, this','line_number':9364,'multiline':False]
['text':' comparison is not expected to work when hook run location','line_number':9365,'multiline':False]
['text':' is pre-forward pass. This is because the hook does async','line_number':9366,'multiline':False]
['text':' communication and forward pass modifies the buffer without','line_number':9367,'multiline':False]
['text':' appropriate synchronization. Therefore, if returning','line_number':9368,'multiline':False]
['text':' futures from custom buffer hooks, it is advised to set','line_number':9369,'multiline':False]
['text':' hook run location to post forward.','line_number':9370,'multiline':False]
['text':' test that _distributed_broadcast_coalesced via registered hook is','line_number':9397,'multiline':False]
['text':' equivalent to DDP's default broadcast coalesced.','line_number':9398,'multiline':False]
['text':' named_buffers is a Dict[str, Tensor] representing a mapping','line_number':9405,'multiline':False]
['text':' from buffer name to buffer.','line_number':9406,'multiline':False]
['text':' Run with error to trigger backward pass that marks fc1 as being marked','line_number':9457,'multiline':False]
['text':' ready. If we don't remove autograd hooks before running below it would','line_number':9458,'multiline':False]
['text':' fail on the old autograd hook.','line_number':9459,'multiline':False]
['text':' Remove autograd hooks on old instance.','line_number':9470,'multiline':False]
['text':' Try another DDP instance without error now.','line_number':9473,'multiline':False]
['text':' One bucket per parameter.','line_number':9512,'multiline':False]
['text':' Reinitialize global process group with TORCH_NCCL_USE_COMM_NONBLOCKING=1','line_number':9542,'multiline':False]
['text':' Abort pg in background thread.','line_number':9554,'multiline':False]
['text':' First collective triggers initialization via ncclCommInitRank.','line_number':9568,'multiline':False]
['text':' 4MB for multiple buckets.','line_number':9591,'multiline':False]
['text':' Run regular iteration.','line_number':9613,'multiline':False]
['text':' Run with error.','line_number':9618,'multiline':False]
['text':' Now reduce world_size and run iteration.','line_number':9628,'multiline':False]
['text':' Increase the world size and run iteration.','line_number':9634,'multiline':False]
['text':' Back to default size.','line_number':9640,'multiline':False]
['text':' Create default pg of smaller size.','line_number':9644,'multiline':False]
['text':' Need a barrier here to ensure ranks 1, 2 and 3 are done.','line_number':9659,'multiline':False]
['text':' Need to init pg again for "_barrier" to succeed.','line_number':9662,'multiline':False]
['text':' Validate no more recompiles.','line_number':9671,'multiline':False]
['text':' Ensure all buffers are synchronized.','line_number':9724,'multiline':False]
['text':' Grads should be equal to a local model that ran through inp twice and averaged grads','line_number':9764,'multiline':False]
['text':' single gpu training setup','line_number':9798,'multiline':False]
['text':' check that the weight remain unmodified','line_number':9851,'multiline':False]
['text':' run a backward pass and check the gradients','line_number':9856,'multiline':False]
['text':' Gradient was not calculated for the module stated and buffers','line_number':9860,'multiline':False]
['text':' Check that the logger has only one entry','line_number':9942,'multiline':False]
['text':' Check that the logger has an expected entry','line_number':9944,'multiline':False]
['text':' Check that the logger has only one entry','line_number':9955,'multiline':False]
['text':' Check that the logger has an expected entry','line_number':9957,'multiline':False]
['text':' Check that loaded function is correct','line_number':9971,'multiline':False]
['text':' Check that all slots' keys were restored correctly','line_number':9974,'multiline':False]
['text':' Check that all slots' attributes are restored correctly','line_number':9977,'multiline':False]
['text':' Excluding ``process_group`` and ``rng``.','line_number':9978,'multiline':False]
['text':' Check that ``process_group`` was set to default','line_number':9985,'multiline':False]
['text':' Check that a random state was restored properly:','line_number':9988,'multiline':False]
['text':' ``np.random.RandomState.get_state`` returns a tuple with entries:','line_number':9989,'multiline':False]
['text':' ``bit_generator`` - str,','line_number':9990,'multiline':False]
['text':' ``state.key`` - ndarray dtype[uint32],','line_number':9991,'multiline':False]
['text':' ``state.pos`` - int,','line_number':9992,'multiline':False]
['text':' ``has_gauss`` - int,','line_number':9993,'multiline':False]
['text':' ``gauss`` - float','line_number':9994,'multiline':False]
['text':'  (refer to https://github.com/numpy/numpy/blob/266aad7478bc7fbcc55eea7f942a0d373b838396/numpy/random/mtrand.pyi)','line_number':9995,'multiline':False]
['text':' To make sure random state was restored properly, all entries should equal the original','line_number':9996,'multiline':False]
['text':' Check that gradients after 10 epochs are the same','line_number':10017,'multiline':False]
['text':' verify output and gradient parity','line_number':10101,'multiline':False]
