['text':' CCT: CompositeCompliantTensor class which is generated using generate_cct','line_number':27,'multiline':False]
['text':' manually populated from native_functions that have inplace_view: True.','line_number':79,'multiline':False]
['text':' In the future we will probably be able to grab that list directly','line_number':80,'multiline':False]
['text':' Introspection please save us','line_number':94,'multiline':False]
['text':' This function returns a new class CompositeCompliantTensor','line_number':105,'multiline':False]
['text':' The two arguments control the behaviour described below.','line_number':106,'multiline':False]
['text':' autograd_view_consistency:','line_number':108,'multiline':False]
['text':'   If True, alias result using `set_` if func returns a view','line_number':109,'multiline':False]
['text':'   (See Note [Alias Result]).','line_number':110,'multiline':False]
['text':'   Since Forward AD doesn't work with `set_`','line_number':111,'multiline':False]
['text':'   we disable it by setting alias to False.','line_number':112,'multiline':False]
['text':' The storage of CompositeCompliantTensor should never be used directly','line_number':125,'multiline':False]
['text':' by a Composite operation; if the Composite','line_number':126,'multiline':False]
['text':' operator attempts to read from the storage without dispatching then it'll','line_number':127,'multiline':False]
['text':' raise a RuntimeError due to it being a meta storage.','line_number':128,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':129,'multiline':False]
['text':' CompositeCompliantTensor steals the "requires_grad"-ness.','line_number':136,'multiline':False]
['text':' Why a new copy of `elem`? Because sometimes OpInfo shares inputs between tests...','line_number':137,'multiline':False]
['text':' Propagate conjugate bits to the wrapper tensor','line_number':148,'multiline':False]
['text':' Ref: https://github.com/albanD/subclass_zoo/issues/24','line_number':149,'multiline':False]
['text':' Ref: https://github.com/albanD/subclass_zoo/issues/21','line_number':150,'multiline':False]
['text':' NB: We are making an assumption that if the function is in-place,','line_number':189,'multiline':False]
['text':' then the first argument is being written to. Introspection please save us!','line_number':190,'multiline':False]
['text':' Note [Alias Result]','line_number':206,'multiline':False]
['text':' Autograd asserts that for B = A.view_fn(...), B and A's storages','line_number':207,'multiline':False]
['text':' are the same. Here we try to make B alias A to avoid those asserts.','line_number':208,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/65339 for more information','line_number':209,'multiline':False]
['text':' about the issue.','line_number':210,'multiline':False]
['text':' Idea: this is a weird way of getting a storage that aliases the input.','line_number':212,'multiline':False]
['text':' This is a workaround for #65339.','line_number':213,'multiline':False]
['text':' 1. under no_dispatch, all of the wrapper tensors look like regular','line_number':214,'multiline':False]
['text':'    tensors with special storage (the storage is nullptr and','line_number':215,'multiline':False]
['text':'    advertises CPU/CUDA device.','line_number':216,'multiline':False]
['text':' 2. we run func, which ends up running the view operation','line_number':217,'multiline':False]
['text':' 3. All view operations reuse the input's storage and return','line_number':218,'multiline':False]
['text':'    result Tensor(s) with new sizes/strides/offset that alias','line_number':219,'multiline':False]
['text':'    the input.','line_number':220,'multiline':False]
['text':' 4. we set the storage (and sizes/strides/offset) of the wrapper','line_number':221,'multiline':False]
['text':'    tensor results to be that of the tensors that alias the input','line_number':222,'multiline':False]
['text':' Some operations are allowed to in-place modify the metadata of the','line_number':230,'multiline':False]
['text':' inputs. The only ones are the "inplace view functions"; when we','line_number':231,'multiline':False]
['text':' run into these, we manually modify the metadata of the input.','line_number':232,'multiline':False]
['text':' For each CompositeCompliantTensor t, we check that t and t.elem','line_number':237,'multiline':False]
['text':' have consistent metadata. If they don't have consistent metadata,','line_number':238,'multiline':False]
['text':' that means the operator did something fishy.','line_number':239,'multiline':False]
['text':' CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode','line_number':268,'multiline':False]
['text':' Given a list of flat arguments, some of which may be Tensors, return all','line_number':276,'multiline':False]
['text':' possible ways some of the arguments could be CompositeCompliantTensors (CCT).','line_number':277,'multiline':False]
['text':' For example, given Tensors A, B, C and flat_args = [A, 1, B],','line_number':278,'multiline':False]
['text':' We would return the following 4 options:','line_number':279,'multiline':False]
['text':' [CCT(A), 1, CCT(B)]','line_number':280,'multiline':False]
['text':' [CCT(A), 1, B]','line_number':281,'multiline':False]
['text':' [A, 1, CCT(B)]','line_number':282,'multiline':False]
['text':' [A, 1, B]','line_number':283,'multiline':False]
['text':' NB: Yes, this is exponential. No, we don't care too much because PyTorch ops','line_number':284,'multiline':False]
['text':' don't accept that many input Tensors.','line_number':285,'multiline':False]
['text':' CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode','line_number':287,'multiline':False]
['text':' For an operation f(*args, **kwargs), each Tensor argument may either be','line_number':298,'multiline':False]
['text':' a regular Tensor or a Tensor Subclass. This iterator iterates through','line_number':299,'multiline':False]
['text':' all of those options.','line_number':300,'multiline':False]
['text':' CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode','line_number':302,'multiline':False]
['text':' This test checks ALL possible permutations of calling `op` with arguments','line_number':327,'multiline':False]
['text':' that are individually either a regular Tensor or a Tensor subclass.','line_number':328,'multiline':False]
['text':'','line_number':329,'multiline':False]
['text':' The general strategy is to wrap some Tensor args and kwargs in','line_number':330,'multiline':False]
['text':' CompositeCompliantTensor wrappers and call the operation.','line_number':331,'multiline':False]
['text':' If some composite operation does any non-compliant behavior,','line_number':333,'multiline':False]
['text':' CompositeCompliantTensor will raise an error.','line_number':334,'multiline':False]
['text':' NOTE: [What errors are Composite Compliance trying to catch?]','line_number':343,'multiline':False]
['text':'','line_number':344,'multiline':False]
['text':' There's two things we want to catch:','line_number':345,'multiline':False]
['text':' - errors that would raise within the torch_dispatch impl','line_number':346,'multiline':False]
['text':' - data_ptr accesses','line_number':347,'multiline':False]
['text':' The first is easy to filter for (we could make the error a different','line_number':348,'multiline':False]
['text':' error class), the second is always going to be a RuntimeError due to','line_number':349,'multiline':False]
['text':' how it is implemented (if you try to access the data_ptr of thex','line_number':350,'multiline':False]
['text':' wrapper Tensor, it raises you some internal RuntimeError).','line_number':351,'multiline':False]
['text':'','line_number':352,'multiline':False]
['text':' So the most general thing to catch here was RuntimeError. If you','line_number':353,'multiline':False]
['text':' are here and debugging why your test failed, it's plausible that','line_number':354,'multiline':False]
['text':' the operator itself is broken and that there are other tests failing.','line_number':355,'multiline':False]
['text':' Checks via the usage of torch dispatch mode certain anti-patterns that','line_number':368,'multiline':False]
['text':' are not composite compliant.','line_number':369,'multiline':False]
['text':'','line_number':370,'multiline':False]
['text':' In particular, the anti-pattern we are trying to prevent is a user','line_number':371,'multiline':False]
['text':' creating an empty tensor and then resize_-ing it. Torch Dispatch Mode helps','line_number':372,'multiline':False]
['text':' here because all factory functions will create tensors that are','line_number':373,'multiline':False]
['text':' CompositeCompliantTensor.','line_number':374,'multiline':False]
['text':'','line_number':375,'multiline':False]
['text':' The general strategy is to wrap all Tensor args and kwargs in','line_number':376,'multiline':False]
['text':' CompositeCompliantTensor wrappers. If an operator that is','line_number':377,'multiline':False]
['text':' Composite does any non-compliant behavior,','line_number':378,'multiline':False]
['text':' CompositeCompliantTensor will raise an error.','line_number':379,'multiline':False]
['text':' see NOTE: [What errors are Composite Compliance trying to catch?]','line_number':393,'multiline':False]
['text':' Checks if the backward formula is composite compliant by testing','line_number':436,'multiline':False]
['text':' all possible permutations of {inputs, grad_outputs} being','line_number':437,'multiline':False]
['text':' CompositeCompliantTensor or regular Tensors.','line_number':438,'multiline':False]
['text':'','line_number':439,'multiline':False]
['text':' NB: it is important that op is accepted as a Callable and not an OpInfo,','line_number':440,'multiline':False]
['text':' this means we can apply check_backward_formula to things that aren't OpInfos','line_number':441,'multiline':False]
['text':' while debugging.','line_number':442,'multiline':False]
['text':' see NOTE: [What errors are Composite Compliance trying to catch?]','line_number':462,'multiline':False]
['text':' NB: ones, not ones_like, so we get a regular Tensor here','line_number':475,'multiline':False]
['text':' see NOTE: [What errors are Composite Compliance trying to catch?]','line_number':482,'multiline':False]
['text':' Checks if the forward AD formula is composite compliant by testing','line_number':496,'multiline':False]
['text':' all possible permutations of {primals, tangents} being','line_number':497,'multiline':False]
['text':' CompositeCompliantTensor or regular Tensors.','line_number':498,'multiline':False]
['text':'','line_number':499,'multiline':False]
['text':' NB: it is important that op is accepted as a Callable and not an OpInfo,','line_number':500,'multiline':False]
['text':' this means we can apply check_forward_ad_formula to things that aren't OpInfos','line_number':501,'multiline':False]
['text':' while debugging.','line_number':502,'multiline':False]
['text':' Generate `tangent` tensor','line_number':508,'multiline':False]
['text':' if given object is a Tensor and requires grad is set.','line_number':509,'multiline':False]
['text':' Returns dual tensor if primal is a tensor/tensor subclass','line_number':523,'multiline':False]
['text':' with requires_grad set.','line_number':524,'multiline':False]
['text':' Permutations of arg and kwargs in CCT.','line_number':546,'multiline':False]
['text':' Permutations tangent arg and tangent kwargs in CCT.','line_number':550,'multiline':False]
['text':' see NOTE: [What errors are Composite Compliance trying to catch?]','line_number':563,'multiline':False]
