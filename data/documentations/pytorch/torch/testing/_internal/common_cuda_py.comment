['text':' note: if ROCm is targeted, TEST_CUDNN is code for TEST_MIOPEN','line_number':17,'multiline':False]
['text':' This condition always evaluates to PLATFORM_SUPPORTS_MEM_EFF_ATTENTION but for logical clarity we keep it separate','line_number':50,'multiline':False]
['text':' Used below in `initialize_cuda_context_rng` to ensure that CUDA context and','line_number':65,'multiline':False]
['text':' RNG have been initialized.','line_number':66,'multiline':False]
['text':' after this call, CUDA context and RNG must have been initialized on each GPU','line_number':70,'multiline':False]
['text':' initialize cuda context and rng for memory tests','line_number':75,'multiline':False]
['text':' Test whether hardware TF32 math mode enabled. It is enabled only on:','line_number':81,'multiline':False]
['text':' - CUDA >= 11','line_number':82,'multiline':False]
['text':' - arch >= Ampere','line_number':83,'multiline':False]
['text':' This is a wrapper that wraps a test to run this test twice, one with','line_number':119,'multiline':False]
['text':' allow_tf32=True, another with allow_tf32=False. When running with','line_number':120,'multiline':False]
['text':' allow_tf32=True, it will use reduced precision as specified by the','line_number':121,'multiline':False]
['text':' argument. For example:','line_number':122,'multiline':False]
['text':'    @dtypes(torch.float32, torch.float64, torch.complex64, torch.complex128)','line_number':123,'multiline':False]
['text':'    @tf32_on_and_off(0.005)','line_number':124,'multiline':False]
['text':'    def test_matmul(self, device, dtype):','line_number':125,'multiline':False]
['text':'        a = ...; b = ...;','line_number':126,'multiline':False]
['text':'        c = torch.matmul(a, b)','line_number':127,'multiline':False]
['text':'        self.assertEqual(c, expected)','line_number':128,'multiline':False]
['text':' In the above example, when testing torch.float32 and torch.complex64 on CUDA','line_number':129,'multiline':False]
['text':' on a CUDA >= 11 build on an >=Ampere architecture, the matmul will be running at','line_number':130,'multiline':False]
['text':' TF32 mode and TF32 mode off, and on TF32 mode, the assertEqual will use reduced','line_number':131,'multiline':False]
['text':' precision to check values.','line_number':132,'multiline':False]
['text':'','line_number':133,'multiline':False]
['text':' This decorator can be used for function with or without device/dtype, such as','line_number':134,'multiline':False]
['text':' @tf32_on_and_off(0.005)','line_number':135,'multiline':False]
['text':' def test_my_op(self)','line_number':136,'multiline':False]
['text':' @tf32_on_and_off(0.005)','line_number':137,'multiline':False]
['text':' def test_my_op(self, device)','line_number':138,'multiline':False]
['text':' @tf32_on_and_off(0.005)','line_number':139,'multiline':False]
['text':' def test_my_op(self, device, dtype)','line_number':140,'multiline':False]
['text':' @tf32_on_and_off(0.005)','line_number':141,'multiline':False]
['text':' def test_my_op(self, dtype)','line_number':142,'multiline':False]
['text':' if neither device nor dtype is specified, it will check if the system has ampere device','line_number':143,'multiline':False]
['text':' if device is specified, it will check if device is cuda','line_number':144,'multiline':False]
['text':' if dtype is specified, it will check if dtype is float32 or complex64','line_number':145,'multiline':False]
['text':' tf32 and fp32 are different only when all the three checks pass','line_number':146,'multiline':False]
['text':' This is a wrapper that wraps a test to run it with TF32 turned off.','line_number':179,'multiline':False]
['text':' This wrapper is designed to be used when a test uses matmul or convolutions','line_number':180,'multiline':False]
['text':' but the purpose of that test is not testing matmul or convolutions.','line_number':181,'multiline':False]
['text':' Disabling TF32 will enforce torch.float tensors to be always computed','line_number':182,'multiline':False]
['text':' at full precision.','line_number':183,'multiline':False]
['text':' ignore git sha','line_number':209,'multiline':False]
['text':' ignore git sha','line_number':220,'multiline':False]
['text':' Shared by test_cuda.py and test_multigpu.py','line_number':228,'multiline':False]
['text':' Create a module+optimizer that will use scaling, and a control module+optimizer','line_number':230,'multiline':False]
['text':' that will not use scaling, against which the scaling-enabled module+optimizer can be compared.','line_number':231,'multiline':False]
['text':' Importing this module should NOT eagerly initialize CUDA','line_number':262,'multiline':False]
