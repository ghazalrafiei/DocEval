['text':' Torch','line_number':1,'multiline':False]
['text':' noqa: F401','line_number':2,'multiline':False]
['text':' noqa: F401','line_number':15,'multiline':False]
['text':' Testing utils','line_number':17,'multiline':False]
['text':' double check casting','line_number':60,'multiline':False]
['text':' NOTE: We do clone() after detach() here because we need to be able to change size/storage of v afterwards','line_number':69,'multiline':False]
['text':' NB: JIT script tests for all nn functional interfaces, script mode does','line_number':81,'multiline':False]
['text':' not support in_place operations yet, so no inplace operation tests added.','line_number':82,'multiline':False]
['text':' removed all the deprecated functions','line_number':83,'multiline':False]
['text':'','line_number':84,'multiline':False]
['text':' (','line_number':85,'multiline':False]
['text':'   method name,','line_number':86,'multiline':False]
['text':'   input size/constructing fn,','line_number':87,'multiline':False]
['text':'   args (tuple represents shape of a tensor arg),','line_number':88,'multiline':False]
['text':'   test variant name(will be used at test name suffix,','line_number':89,'multiline':False]
['text':'       'inplace' skips grad tests),                         // optional','line_number':90,'multiline':False]
['text':'   (True, nonfusible_nodes, fusible_nodes) for autodiff     // optional','line_number':91,'multiline':False]
['text':'   fn to determine if test should be skipped,               // optional','line_number':92,'multiline':False]
['text':'   fn mapping output to part that should be gradcheck'ed,   // optional','line_number':93,'multiline':False]
['text':'   kwargs for function,                                     // optional','line_number':94,'multiline':False]
['text':' )','line_number':95,'multiline':False]
['text':' Quotes string and escapes special characters','line_number':324,'multiline':False]
['text':' create a script function from (name, func_type, output_process_fn),','line_number':380,'multiline':False]
['text':' and returns the compiled function and example inputs','line_number':381,'multiline':False]
['text':' create a script function from (name, func_type),','line_number':389,'multiline':False]
['text':' returns a function takes in (args, kwargs) and runs the compiled function','line_number':390,'multiline':False]
['text':' function returns tuple containing original output and','line_number':392,'multiline':False]
['text':' filtered output to be used in checking gradients','line_number':393,'multiline':False]
['text':' skip type annotate function attributes for now, see: https://github.com/python/mypy/issues/2087','line_number':398,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':399,'multiline':False]
['text':' make a new function where all non-tensor arguments in 'args' have been partially','line_number':440,'multiline':False]
['text':' applied, and all tensor arguments remain.','line_number':441,'multiline':False]
['text':' used to trace functions when some arguments are not tensors','line_number':442,'multiline':False]
['text':' create a trace function from input fn','line_number':454,'multiline':False]
['text':' `check_trace` is set to False because check_trace is run with @no_grad','line_number':457,'multiline':False]
['text':' Also, `check_against_reference` already does all the checks','line_number':458,'multiline':False]
['text':' against python function','line_number':459,'multiline':False]
['text':' Guard to check that nontensor inputs are the same as during tracing','line_number':469,'multiline':False]
['text':' skip type annotate function attributes for now, see: https://github.com/python/mypy/issues/2087','line_number':473,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':474,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':475,'multiline':False]
['text':' known to be failing in script','line_number':479,'multiline':False]
['text':' aten op has additional cudnn argument','line_number':487,'multiline':False]
['text':' flaky test - TODO fix','line_number':490,'multiline':False]
['text':' unknown builtin op','line_number':493,'multiline':False]
['text':' jit doesn't support sparse tensors.','line_number':496,'multiline':False]
['text':' generates a script function and set of example inputs','line_number':501,'multiline':False]
['text':' from a specified test in the format of nn_functional_tests','line_number':502,'multiline':False]
['text':' need to record this because methods can change the size (e.g. unsqueeze)','line_number':514,'multiline':False]
['text':' additional modules test','line_number':527,'multiline':False]
['text':' TODO: delete this list once we make all nn_tests work','line_number':528,'multiline':False]
['text':' Doesn't use future division, so this is not supported','line_number':573,'multiline':False]
['text':' Derivative for aten::_scaled_dot_product_flash_attention_backward is not implemented','line_number':575,'multiline':False]
['text':' Create module to use the script method','line_number':601,'multiline':False]
['text':' check __repr__','line_number':611,'multiline':False]
['text':' skip type annotate function attributes for now, see: https://github.com/python/mypy/issues/2087','line_number':620,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':621,'multiline':False]
['text':' to clean up IR','line_number':630,'multiline':False]
['text':' eval() is not supported, so skip these tests','line_number':664,'multiline':False]
['text':' Set up inputs from tuple of sizes or constructor fn','line_number':687,'multiline':False]
['text':' Extra parameters to forward()','line_number':699,'multiline':False]
