['text':' TODO(crcrpar): [low prec params & their higher prec copy]','line_number':51,'multiline':False]
['text':' Support AMP with FP16/BF16 model params which would need','line_number':52,'multiline':False]
['text':' higher prec copy of params to do update math in higher prec to','line_number':53,'multiline':False]
['text':' alleviate the loss of information.','line_number':54,'multiline':False]
['text':' Lazy state initialization','line_number':100,'multiline':False]
['text':' note(crcrpar): [special device hosting for step]','line_number':102,'multiline':False]
['text':' Deliberately host `step` on CPU if both capturable and fused are off.','line_number':103,'multiline':False]
['text':' This is because kernel launches are costly on CUDA and XLA.','line_number':104,'multiline':False]
['text':' Exponential moving average of gradient values','line_number':110,'multiline':False]
['text':' Exponential moving average of squared gradient values','line_number':112,'multiline':False]
['text':' Maintains max of all exp. moving avg. of sq. grad. values','line_number':115,'multiline':False]
['text':' Foreach without capturable does not support a tensor lr','line_number':126,'multiline':False]
['text':' kwonly args with defaults are not supported by functions compiled with torchscript issue #70627','line_number':264,'multiline':False]
['text':' setting this as kwarg for now as functional API is compiled by torch/distributed/optim','line_number':265,'multiline':False]
['text':' Respect when the user inputs False/True for foreach or fused. We only want to change','line_number':285,'multiline':False]
['text':' the default when neither have been user-specified. Note that we default to foreach','line_number':286,'multiline':False]
['text':' and pass False to use_fused. This is not a mistake--we want to give the fused impl','line_number':287,'multiline':False]
['text':' bake-in time before making it the default, even if it is typically faster.','line_number':288,'multiline':False]
['text':' Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.','line_number':291,'multiline':False]
['text':' this check is slow during compilation, so we skip it','line_number':299,'multiline':False]
['text':' if it's strictly needed we can add this check back in dynamo','line_number':300,'multiline':False]
['text':' this assert is due to JIT being dumb and not realizing that the ops below','line_number':359,'multiline':False]
['text':' have overloads to handle both float and Tensor lrs, so we just assert it's','line_number':360,'multiline':False]
['text':' a float since most people using JIT are using floats','line_number':361,'multiline':False]
['text':' If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]','line_number':370,'multiline':False]
['text':' update step','line_number':376,'multiline':False]
['text':' Decay the first and second moment running average coefficient','line_number':390,'multiline':False]
['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':406,'multiline':False]
['text':' Uses the max. for normalizing running avg. of gradient','line_number':414,'multiline':False]
['text':' Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write','line_number':415,'multiline':False]
['text':' (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)','line_number':416,'multiline':False]
['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':433,'multiline':False]
['text':' Use the max. for normalizing running avg. of gradient','line_number':436,'multiline':False]
['text':' Lastly, switch back to complex view','line_number':443,'multiline':False]
['text':' If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]','line_number':473,'multiline':False]
['text':' Handle complex parameters','line_number':496,'multiline':False]
['text':' Update steps','line_number':503,'multiline':False]
['text':' If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over','line_number':504,'multiline':False]
['text':' and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just','line_number':505,'multiline':False]
['text':' wrapped it once now. The alpha is required to assure we go to the right overload.','line_number':506,'multiline':False]
['text':' Re-use the intermediate memory (device_grads) already allocated for maximize','line_number':513,'multiline':False]
['text':' Decay the first and second moment running average coefficient','line_number':519,'multiline':False]
['text':' Delete the local intermediate since it won't be used anymore to save on peak memory','line_number':525,'multiline':False]
['text':' foreach_sub doesn't allow a scalar as the first arg','line_number':531,'multiline':False]
['text':' we do not negate bias_correction1 as it'll need to be negated later anyway','line_number':534,'multiline':False]
['text':' foreach_div doesn't allow a scalar as the first arg','line_number':537,'multiline':False]
['text':' Re-assign for clarity as we maintain minimal intermediates: we'll have','line_number':543,'multiline':False]
['text':' step_size = - lr / (1 - beta1 ^ t) where t = num_steps','line_number':544,'multiline':False]
['text':' bias_correction2_sqrt = sqrt(1 - beta2 ^ t)','line_number':545,'multiline':False]
['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':550,'multiline':False]
['text':' type: ignore[assignment]','line_number':551,'multiline':False]
['text':' Set intermediate to the max. for normalizing running avg. of gradient when amsgrad','line_number':553,'multiline':False]
['text':' at this point, exp_avg_sq_sqrt = - (1 - beta^t) * [sqrt(exp_avg_sq / (1 - beta2^t)) + eps] / lr','line_number':562,'multiline':False]
['text':' Maintains the maximum of all 2nd moment running avg. till now','line_number':573,'multiline':False]
['text':' Use the max. for normalizing running avg. of gradient','line_number':576,'multiline':False]
['text':' Needed for consistency.','line_number':597,'multiline':False]
['text':' Needed for consistency.','line_number':604,'multiline':False]
['text':' We only shuffle around the lr when it is a Tensor and on CUDA, otherwise, we prefer','line_number':615,'multiline':False]
['text':' treating it as a scalar.','line_number':616,'multiline':False]
