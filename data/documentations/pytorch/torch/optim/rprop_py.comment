['text':' State initialization','line_number':58,'multiline':False]
['text':' Complex Number should be as if they are two independent real numbers.','line_number':65,'multiline':False]
['text':' Hence the step_size shouldn't be zero for imaginary part.','line_number':66,'multiline':False]
['text':' kwonly args with defaults are not supported by functions compiled with torchscript issue #70627','line_number':181,'multiline':False]
['text':' setting this as kwarg for now as functional API is compiled by torch/distributed/optim','line_number':182,'multiline':False]
['text':' update stepsizes with step size updates','line_number':258,'multiline':False]
['text':' for dir<0, dfdx=0','line_number':261,'multiline':False]
['text':' for dir>=0 dfdx=dfdx','line_number':262,'multiline':False]
['text':' update parameters','line_number':266,'multiline':False]
['text':' Handle complex params','line_number':293,'multiline':False]
['text':' At the end of the step, grouped_prevs will contain the current grads, so we reuse','line_number':301,'multiline':False]
['text':' grouped_prevs memory instead of creating a new buffer, but, for clarity, we reassign','line_number':302,'multiline':False]
['text':' to keep referring to the buffer as grouped_grads.','line_number':303,'multiline':False]
['text':' update stepsizes with step size updates','line_number':315,'multiline':False]
['text':' for dir<0, dfdx=0','line_number':320,'multiline':False]
['text':' for dir>=0 dfdx=dfdx','line_number':321,'multiline':False]
['text':' explicitly del signs as it's not used after here to save memory','line_number':326,'multiline':False]
['text':' update parameters','line_number':329,'multiline':False]
['text':' Logically, you may expect grouped_prevs to get updated to grouped_grads, but that's','line_number':333,'multiline':False]
['text':' basically already happened since we've been using grouped_prevs' memory to store','line_number':334,'multiline':False]
['text':' updated grouped_grads!','line_number':335,'multiline':False]
