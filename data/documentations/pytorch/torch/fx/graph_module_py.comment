['text':' Normal exec loses the source code, however we can work with','line_number':29,'multiline':False]
['text':' the linecache module to recover it.','line_number':30,'multiline':False]
['text':' Using _exec_with_source will add it to our local cache','line_number':31,'multiline':False]
['text':' and then tools like TorchScript will be able to get source info.','line_number':32,'multiline':False]
['text':' Don't mutate globals so that this loader is only used','line_number':55,'multiline':False]
['text':' to populate linecache, and doesn't interact with other modules','line_number':56,'multiline':False]
['text':' that might check `__loader__`','line_number':57,'multiline':False]
['text':' Part of the loader protocol (PEP 302)','line_number':66,'multiline':False]
['text':' linecache will use this method when trying to find source code','line_number':67,'multiline':False]
['text':' avoid mutating the passed in dict','line_number':96,'multiline':False]
['text':' Sort the imports so we have a stable import block that allows us to','line_number':117,'multiline':False]
['text':' hash the graph module and get a consistent key for use in a cache.','line_number':118,'multiline':False]
['text':' BC: attribute name was changed from `code` to `_code` to facilitate','line_number':124,'multiline':False]
['text':' making `code` into a property and adding a docstring to it','line_number':125,'multiline':False]
['text':' We create a dummy class here because symbolic_trace pulls the forward()','line_number':151,'multiline':False]
['text':' function off of the class, rather than the instance. This class is used','line_number':152,'multiline':False]
['text':' in _deserialize_graph_module() below.','line_number':153,'multiline':False]
['text':' Try to retrieve the forward source in a backward-compatible way','line_number':168,'multiline':False]
['text':' This is a workaround for a mypy linter issue related to','line_number':179,'multiline':False]
['text':' passing base class as an argument - https://github.com/python/mypy/issues/5865.','line_number':180,'multiline':False]
['text':' we shouldn't trace into any of the submodules,','line_number':184,'multiline':False]
['text':' because they were not traced in the original GraphModule','line_number':185,'multiline':False]
['text':' Manually set Tracer class on the reconstructed Graph, to avoid','line_number':194,'multiline':False]
['text':' referencing the private local subclass KeepModules.','line_number':195,'multiline':False]
['text':' The GraphModule constructor only retains attributes referenced by the graph.','line_number':201,'multiline':False]
['text':' In this case, our goal is return a GraphModule as close to identical as the one','line_number':202,'multiline':False]
['text':' put into the package. If any additional attributes were present in body,','line_number':203,'multiline':False]
['text':' we should keep them.','line_number':204,'multiline':False]
['text':' copy an attribute value with qualified name 'target' from 'from_module' to 'to_module'','line_number':211,'multiline':False]
['text':' This installs empty Modules where none exist yet if they are subpaths of target','line_number':212,'multiline':False]
['text':' we have already installed one of its parents','line_number':219,'multiline':False]
['text':' (e.g. target = root.linear.weight, but we have already installed root.linear)','line_number':220,'multiline':False]
['text':' once we install a parent, we no longer need to copy the children','line_number':221,'multiline':False]
['text':' since all the needed properties will already be present','line_number':222,'multiline':False]
['text':' If it is a tensor and not a parameter attribute of a module, it should be a named buffer.','line_number':231,'multiline':False]
['text':' So, we register it as a named buffer in the target module.','line_number':232,'multiline':False]
['text':' Assign attribute 'from_obj' to the qualified name 'target' on 'to_module','line_number':239,'multiline':False]
['text':' This installs empty Modules where none exist yet if they are subpaths of target','line_number':240,'multiline':False]
['text':' If it is a tensor and not a parameter attribute of a module, it should be a named buffer.','line_number':251,'multiline':False]
['text':' So, we register it as a named buffer in the target module.','line_number':252,'multiline':False]
['text':' Previously, if an error occurred when valid','line_number':266,'multiline':False]
['text':' symbolically-traced code was run with an invalid input, the','line_number':267,'multiline':False]
['text':' user would see the source of the error as coming from','line_number':268,'multiline':False]
['text':' `File "<eval_with_key_N">`, where N is some number. We use','line_number':269,'multiline':False]
['text':' this function to generate a more informative error message. We','line_number':270,'multiline':False]
['text':' return the traceback itself, a message explaining that the','line_number':271,'multiline':False]
['text':' error occurred in a traced Module's generated forward','line_number':272,'multiline':False]
['text':' function, and five lines of context surrounding the faulty','line_number':273,'multiline':False]
['text':' line','line_number':274,'multiline':False]
['text':' auxiliary variables (for readability)','line_number':277,'multiline':False]
['text':' constituent substrings of the error message','line_number':285,'multiline':False]
['text':' joined message','line_number':296,'multiline':False]
['text':' type: ignore[misc]','line_number':304,'multiline':False]
['text':' type: ignore[arg-type]','line_number':309,'multiline':False]
['text':' noqa: TRY200','line_number':315,'multiline':False]
['text':' each instance of a graph module needs its own forward method','line_number':336,'multiline':False]
['text':' so create a new singleton class for each instance.','line_number':337,'multiline':False]
['text':' it is a subclass of the user-defined class, the only difference','line_number':338,'multiline':False]
['text':' is an extra layer to install the forward method','line_number':339,'multiline':False]
['text':' address issue described at https://github.com/pytorch/pytorch/issues/63883','line_number':341,'multiline':False]
['text':' in other words, traverse class hierarchy to fix the redundant class definition problem','line_number':342,'multiline':False]
['text':' type: ignore[misc, valid-type]','line_number':349,'multiline':False]
['text':' When we pickle/unpickle graph module, we don't want to drop any module or attributes.','line_number':387,'multiline':False]
['text':' Sort targets in ascending order of the # of atoms.','line_number':416,'multiline':False]
['text':' This will ensure that less deeply nested attributes are assigned','line_number':417,'multiline':False]
['text':' before more deeply nested attributes. For example, foo.bar','line_number':418,'multiline':False]
['text':' will be assigned before foo.bar.baz. Otherwise, we might assign','line_number':419,'multiline':False]
['text':' the user-provided ``foo.bar`` and wipe out the previously-assigned','line_number':420,'multiline':False]
['text':' ``foo.bar.baz``','line_number':421,'multiline':False]
['text':' Store the Tracer class responsible for creating a Graph separately as part of the','line_number':430,'multiline':False]
['text':' GraphModule state, except when the Tracer is defined in a local namespace.','line_number':431,'multiline':False]
['text':' Locally defined Tracers are not pickleable. This is needed because torch.package will','line_number':432,'multiline':False]
['text':' serialize a GraphModule without retaining the Graph, and needs to use the correct Tracer','line_number':433,'multiline':False]
['text':' to re-create the Graph during deserialization.','line_number':434,'multiline':False]
['text':' Dictionary to store metadata','line_number':446,'multiline':False]
['text':' TorchScript breaks trying to compile the graph setter because of the','line_number':449,'multiline':False]
['text':' continued string literal. Issue here: https://github.com/pytorch/pytorch/issues/44842','line_number':450,'multiline':False]
['text':'','line_number':451,'multiline':False]
['text':' Shouldn't be an issue since these methods shouldn't be used in TorchScript anyway','line_number':452,'multiline':False]
['text':' Get the parent module','line_number':618,'multiline':False]
['text':' A list of strings representing the different parts','line_number':659,'multiline':False]
['text':' of the path. For example, `foo.bar.baz` gives us','line_number':660,'multiline':False]
['text':' ["foo", "bar", "baz"]','line_number':661,'multiline':False]
['text':' If we're looking at multiple parts of a path, join','line_number':664,'multiline':False]
['text':' join them with a dot. Otherwise, return that single','line_number':665,'multiline':False]
['text':' element without doing anything to it.','line_number':666,'multiline':False]
['text':' Progressively collect all the names of intermediate','line_number':670,'multiline':False]
['text':' modules. For example, if we have the target','line_number':671,'multiline':False]
['text':' `foo.bar.baz`, we'll add `foo`, `foo.bar`, and','line_number':672,'multiline':False]
['text':' `foo.bar.baz` to the list.','line_number':673,'multiline':False]
['text':' For a `call_module` node, also register all recursive submodules','line_number':677,'multiline':False]
['text':' as used','line_number':678,'multiline':False]
['text':' Node referenced nonexistent submodule, don't need to','line_number':687,'multiline':False]
['text':' worry about GCing anything','line_number':688,'multiline':False]
['text':' Determine whether this class explicitly defines a __call__ implementation','line_number':726,'multiline':False]
['text':' to wrap. If it does, save it in order to have wrapped_call invoke it.','line_number':727,'multiline':False]
['text':' If it does not, wrapped_call can use a dynamic call to super() instead.','line_number':728,'multiline':False]
['text':' In most cases, super().__call__ should be torch.nn.Module.__call__.','line_number':729,'multiline':False]
['text':' We do not want to hold a reference to Module.__call__ here; doing so will','line_number':730,'multiline':False]
['text':' bypass patching of torch.nn.Module.__call__ done while symbolic tracing.','line_number':731,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':735,'multiline':False]
['text':' type: ignore[method-assign]','line_number':740,'multiline':False]
['text':' Passing Tracer as argument allows subclasses extending fx.GraphModule','line_number':744,'multiline':False]
['text':' define their own Tracer (extending fx.Tracer).','line_number':745,'multiline':False]
['text':' because __reduce__ is defined for serialization,','line_number':787,'multiline':False]
['text':' we need to define deepcopy otherwise it will call __reduce__','line_number':788,'multiline':False]
['text':' and cause symbolic tracing to occur every time we try to copy the object','line_number':789,'multiline':False]
['text':' hooks are lost during `GraphModule.__init__`, so we need to copy over','line_number':795,'multiline':False]
['text':' them explicitly, note right now we are only copying state_dict related','line_number':796,'multiline':False]
['text':' hooks, to reduce bc-related issues, we can copy forward/backward related','line_number':797,'multiline':False]
['text':' hooks in the future as well if needed','line_number':798,'multiline':False]
['text':' workarounds for issues in __torch_function__','line_number':854,'multiline':False]
['text':' WAR for __torch_function__ not handling tensor lists,','line_number':856,'multiline':False]
['text':' fix is in https://github.com/pytorch/pytorch/pull/34725','line_number':857,'multiline':False]
['text':' orig_cat = torch.cat','line_number':858,'multiline':False]
['text':' def patched_cat(*args, **kwargs):','line_number':859,'multiline':False]
['text':'     tensors = args[0]','line_number':860,'multiline':False]
['text':'     for t in tensors:','line_number':861,'multiline':False]
['text':'         if isinstance(t, Proxy):','line_number':862,'multiline':False]
['text':'             return t.__torch_function__(patched_cat, (), args, kwargs)','line_number':863,'multiline':False]
['text':'     return orig_cat(*args, **kwargs)','line_number':864,'multiline':False]
['text':' patched_cat.__module__ = 'torch'','line_number':865,'multiline':False]
['text':' patched_cat.__name__ = 'cat'','line_number':866,'multiline':False]
['text':' torch.cat = patched_cat','line_number':867,'multiline':False]
