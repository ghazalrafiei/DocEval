['text':' TODO: Add type annotations','line_number':12,'multiline':False]
['text':' TODO: Check tensor types for ops','line_number':13,'multiline':False]
['text':' Scalar types that appear explicitly in models.','line_number':146,'multiline':False]
['text':' These must be kept in sync with','line_number':147,'multiline':False]
['text':' AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS.','line_number':148,'multiline':False]
['text':' TODO: Expose these directly to Python to avoid maintaining this list.','line_number':149,'multiline':False]
['text':' NNAPI operand type.  One of NNAPI_OperandCode.','line_number':204,'multiline':False]
['text':' TODO: Make this an enum.','line_number':205,'multiline':False]
['text':' This is always the PyTorch shape, which is NCHW for feature maps.','line_number':208,'multiline':False]
['text':' The actual NNAPI operand might have a transposed shape.','line_number':209,'multiline':False]
['text':' we use 0 for load time dynamic shapes & -1 for runtime dynamic shapes','line_number':210,'multiline':False]
['text':' Specifies how the shape of the operand that we define in NNAPI','line_number':213,'multiline':False]
['text':' relates to the shape we track above.','line_number':214,'multiline':False]
['text':' - PRESUMED_CONTIGUOUS: physical NNAPI operand will exactly match','line_number':215,'multiline':False]
['text':'   the shape of the PyTorch tensor.','line_number':216,'multiline':False]
['text':' - CHANNELS_LAST: The PyTorch tensor is expected to be NCHW, and','line_number':217,'multiline':False]
['text':'   the NNAPI operand will be represented explicitly as NHWC.','line_number':218,'multiline':False]
['text':' Quantization params','line_number':221,'multiline':False]
['text':' TODO: Support non-equal-rank broadcast where semantics match.','line_number':238,'multiline':False]
['text':' This can be tricky for NHWC tensors because dimension orders','line_number':239,'multiline':False]
['text':' don't match between PT and NNAPI, even though semantics match.','line_number':240,'multiline':False]
['text':' s2 = [1] * (len(s1) - len(s2)) + s2','line_number':242,'multiline':False]
['text':' s3 = [1] * (len(s2) - len(s1)) + s1','line_number':245,'multiline':False]
['text':' TODO: Handle dilation','line_number':263,'multiline':False]
['text':' Handle variable-sized tensors.','line_number':274,'multiline':False]
['text':' Return the actual shape that an operand should have in NNAPI,','line_number':285,'multiline':False]
['text':' given a PyTorch shape and dimension order.  This is where we','line_number':286,'multiline':False]
['text':' convert from PyTorch's "always NCHW" shape to explicit NHWC.','line_number':287,'multiline':False]
['text':' XXX think this through','line_number':296,'multiline':False]
['text':' Return the original PyTorch dimension position for a given dimension.','line_number':302,'multiline':False]
['text':' d should be the dimension that NNAPI will see.','line_number':303,'multiline':False]
['text':' reverse_map_dim(PRESUMED_CONTIGUOUS, x) == x','line_number':304,'multiline':False]
['text':' reverse_map_dim(CHANNELS_LAST, 3) == 1','line_number':305,'multiline':False]
['text':' Return the local variable name for the computed flexible size','line_number':313,'multiline':False]
['text':' for a given op and dimension.','line_number':314,'multiline':False]
['text':' Add a tensor operand corresponding to a JIT Value.','line_number':344,'multiline':False]
['text':' Returns the NNAPI operand ID.  Can be looked up later with','line_number':345,'multiline':False]
['text':' get_tensor_operand_by_jitval.','line_number':346,'multiline':False]
['text':' Add a tensor operand that does not correspond to a JIT Value.','line_number':357,'multiline':False]
['text':' Useful for cases where multiple NNAPI operands are required','line_number':358,'multiline':False]
['text':' to implement one JIT IR node.  Returns the NNAPI operand ID.','line_number':359,'multiline':False]
['text':' For NHWC NNAPI op, lay out data in the same dim order by permuting torch tensor','line_number':441,'multiline':False]
['text':' TODO: Improve this error message, possibly after converting','line_number':491,'multiline':False]
['text':' many callsites to support flexible size.','line_number':492,'multiline':False]
['text':' runtime flex','line_number':495,'multiline':False]
['text':' Fixed shape dimension: just add the value.','line_number':546,'multiline':False]
['text':' Load time flexible shape dimension: it should have been computed in a variable.','line_number':549,'multiline':False]
['text':' Runtime flexible shape','line_number':552,'multiline':False]
['text':' Transpose inputs as necessary to allow broadcasting.','line_number':608,'multiline':False]
['text':' Assume NHWC is preferred if there is a mismatch.','line_number':613,'multiline':False]
['text':' NNAPI uses 4 values for padding.','line_number':674,'multiline':False]
['text':' Compact the model so we can get its length so far.','line_number':753,'multiline':False]
['text':' Model offset is the index into the model (in 32-bit words, not bytes)','line_number':756,'multiline':False]
['text':' of the next dimension we're about to serialize.  If it's 0,','line_number':757,'multiline':False]
['text':' generate code to mutate it before passing to NNAPI.','line_number':758,'multiline':False]
['text':' convert runtime flex shape from -1 to 0','line_number':772,'multiline':False]
['text':' Pad with 0 bytes out to a multiple of 4 for alignment.','line_number':799,'multiline':False]
['text':' NOTE: Now that TorchScript supports list constants,','line_number':935,'multiline':False]
['text':' this code path might not be used anymore.','line_number':936,'multiline':False]
['text':' Handle to("cpu") / to("gpu") case','line_number':978,'multiline':False]
['text':' Bit of a hack here.  Use a real tensor to infer the output shape.','line_number':997,'multiline':False]
['text':' channels last with channels == 1 or (height & width both 1)','line_number':1021,'multiline':False]
['text':' flex inputs','line_number':1113,'multiline':False]
['text':' begin mask','line_number':1134,'multiline':False]
['text':' shrink axis mas','line_number':1136,'multiline':False]
['text':' TODO: Possibly check scale and zero point.','line_number':1174,'multiline':False]
['text':' TODO: Possibly support variable-sized inputs.','line_number':1176,'multiline':False]
['text':' Expect None for dtype','line_number':1215,'multiline':False]
['text':' NNAPI docs: For ANEURALNETWORKS_TENSOR_QUANT8_ASYMM, the scale','line_number':1316,'multiline':False]
['text':' must be 1.f / 256 and the zeroPoint must be 0.','line_number':1317,'multiline':False]
['text':' https://fburl.com/h52stoog','line_number':1318,'multiline':False]
['text':' noqa: D401','line_number':1336,'multiline':False]
['text':' NOTE: PyTorch and NNAPI have the same broadcast semantics.','line_number':1360,'multiline':False]
['text':' positive scaling factor of exponent, beta','line_number':1425,'multiline':False]
['text':' noqa: E201','line_number':1443,'multiline':False]
['text':' TODO: Support this by adding trailing 1 dims.','line_number':1471,'multiline':False]
['text':' TODO: Validate ceil_mode semantics.','line_number':1501,'multiline':False]
['text':' The only way for the 4-argument overload of upsample_nearest2d to','line_number':1646,'multiline':False]
['text':' have been added to the graph without error is if the scale_h and','line_number':1647,'multiline':False]
['text':' scale_w arguments are None','line_number':1648,'multiline':False]
['text':' Handle variable input size','line_number':1702,'multiline':False]
['text':' h, w indices','line_number':1703,'multiline':False]
['text':' TODO: Transform at load time to share weights with CPU model.','line_number':1758,'multiline':False]
['text':' TODO: Support automatic reshape','line_number':1798,'multiline':False]
['text':' TODO: Transform at load time to share weights with CPU model.','line_number':1837,'multiline':False]
['text':' transpose','line_number':1901,'multiline':False]
['text':' specifying 1 as the scaling factor for the exponent, beta','line_number':1956,'multiline':False]
['text':' Full convolution','line_number':2047,'multiline':False]
['text':' Depthwise convolution','line_number':2054,'multiline':False]
['text':' TODO: Transform at load time to share weights with CPU model.','line_number':2060,'multiline':False]
['text':' Depthwise convolution','line_number':2083,'multiline':False]
['text':' Don't support multiplier','line_number':2088,'multiline':False]
['text':' Full convolution','line_number':2091,'multiline':False]
['text':' H & W','line_number':2148,'multiline':False]
