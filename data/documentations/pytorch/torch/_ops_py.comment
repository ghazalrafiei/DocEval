['text':' Query `hasattr` only once.','line_number':14,'multiline':False]
['text':' The dispatch cache precomputes a mapping of dispatch key that the','line_number':43,'multiline':False]
['text':' dispatcher wants to dispatch to, to an actual implementation of the','line_number':44,'multiline':False]
['text':' dispatch key.  Confusingly, the actual implementation could *also* be a','line_number':45,'multiline':False]
['text':' dispatch key, but in this case, this refers to the C++ kernel that','line_number':46,'multiline':False]
['text':' was registered to some dispatch key.  Aliases are permitted in the','line_number':47,'multiline':False]
['text':' latter but not the former; for example, you might lookup the','line_number':48,'multiline':False]
['text':' entry for AutogradCPU, and this maps you to the Autograd key for','line_number':49,'multiline':False]
['text':' the generic autograd kernel that works for all devices.  Since this','line_number':50,'multiline':False]
['text':' is the Python dispatcher, you can also put an arbitrary Python','line_number':51,'multiline':False]
['text':' callable to call instead.  This handler gets precisely the','line_number':52,'multiline':False]
['text':' args/kwargs that the operator was __call__'ed with.','line_number':53,'multiline':False]
['text':' NB: This name is hard-coded in torch/csrc/autograd/python_variable.cpp','line_number':54,'multiline':False]
['text':' for use with OpOverload; cache lookup is done entirely from C++','line_number':55,'multiline':False]
['text':' for speed.','line_number':56,'multiline':False]
['text':' TODO: The cache is NOT currently used by HigherOrderOperator, but it should!','line_number':57,'multiline':False]
['text':' This table allows you to override the behavior of a particular','line_number':62,'multiline':False]
['text':' dispatch key to call a custom Python function, rather than the','line_number':63,'multiline':False]
['text':' ordinary C++ configured behavior.  This is the raison d'etre of','line_number':64,'multiline':False]
['text':' Python dispatcher: to let you program the dispatcher from Python','line_number':65,'multiline':False]
['text':' in case you need something unusual, and don't want to clobber','line_number':66,'multiline':False]
['text':' the existing registrations using the Python operator registration','line_number':67,'multiline':False]
['text':' API.','line_number':68,'multiline':False]
['text':' This table allows you to override the behavior of a particular','line_number':73,'multiline':False]
['text':' operator for a particular TorchDispatchMode.  In practice,','line_number':74,'multiline':False]
['text':' we are using this mostly for ProxyTensorMode.  Modes can be','line_number':75,'multiline':False]
['text':' thought of as an open world extension of dispatch keys, so it','line_number':76,'multiline':False]
['text':' makes sense that you should be able to register them, the same','line_number':77,'multiline':False]
['text':' way you can register dispatch keys.','line_number':78,'multiline':False]
['text':' This table allows you to override the behavior of functorch','line_number':83,'multiline':False]
['text':' transformations.  NB: this currently only does something for','line_number':84,'multiline':False]
['text':' HigherOrderOperator','line_number':85,'multiline':False]
['text':' TODO(voz): Should we replace setting torch._C.DispatchKey.Python entirely with setting mode keys?','line_number':106,'multiline':False]
['text':' Registers an implementation to all **3** variants of functionalization that we have:','line_number':131,'multiline':False]
['text':' - DispatchKey.Functionalize','line_number':132,'multiline':False]
['text':' - functorch.TransformType.Functionalize','line_number':133,'multiline':False]
['text':' - FunctionalTensorMode','line_number':134,'multiline':False]
['text':' Example:','line_number':135,'multiline':False]
['text':'   @py_functionalize_impl','line_number':136,'multiline':False]
['text':'   def functionalize_rule(ctx, inner_f, *args):','line_number':137,'multiline':False]
['text':'       args_unwrapped = ctx.unwrap_tensors(args)','line_number':138,'multiline':False]
['text':'       with ctx.redispatch_to_next():','line_number':139,'multiline':False]
['text':'           out = ctx.functionalize(inner_f)(*args_unwrapped)','line_number':140,'multiline':False]
['text':'           return ctx.wrap_tensors(out)','line_number':141,'multiline':False]
['text':' Construct our three flavors of functionalization,','line_number':149,'multiline':False]
['text':' each of which have slightly different wrap/unwrap/redispatch policies','line_number':150,'multiline':False]
['text':' Mode is unused (there's a global FunctionalTensorMode that we can access)','line_number':155,'multiline':False]
['text':' Equivalent to computeDispatchTableEntryWithDebug','line_number':180,'multiline':False]
['text':' type: ignore[valid-type]','line_number':181,'multiline':False]
['text':' 1. (Direct) operator registration','line_number':182,'multiline':False]
['text':' 2.1 Use CompositeExplicitAutogradNonFunctional kernel if available','line_number':185,'multiline':False]
['text':' 2.2 Use CompositeExplicitAutograd kernel if available','line_number':191,'multiline':False]
['text':' 2.3. Use CompositeImplicitAutograd kernel if available','line_number':200,'multiline':False]
['text':' 2.4. For autograd backend keys, use kernel from DispatchKey::Autograd if available','line_number':218,'multiline':False]
['text':' 2.5 Use kernel from DispatchKey::FuncTorchBatchedDecomposition if available','line_number':222,'multiline':False]
['text':' Backend fallback','line_number':226,'multiline':False]
['text':' The dispatch key itself will implicitly route to backend fallback.','line_number':228,'multiline':False]
['text':' This is probably not great for the pure Python implementation.','line_number':229,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':237,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':238,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':241,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':242,'multiline':False]
['text':' The HigherOrderOperator will appear as torch.ops.higher_order.{name}','line_number':247,'multiline':False]
['text':'','line_number':248,'multiline':False]
['text':' If you're creating a new HigherOrderOperator, please do not change the','line_number':249,'multiline':False]
['text':' default. Adding operators to the global torch.ops namespace is a bad','line_number':250,'multiline':False]
['text':' practice due to name collisions.','line_number':251,'multiline':False]
['text':' Make _OPNamespace not scream, this whole name based association needs a good hard look','line_number':256,'multiline':False]
['text':' For a normal HigherOrderOperator instance, we will change its __module__ from torch._ops to','line_number':261,'multiline':False]
['text':' torch._ops.higher_order.','line_number':262,'multiline':False]
['text':' For an instance of subclass of HigherOrderOperator (e.g. customized higher order op),','line_number':263,'multiline':False]
['text':' the __module__ attribute will be kept unchanged.','line_number':264,'multiline':False]
['text':' The place to handle ProxyTorchDispatchMode, FakeTensorMode, etc','line_number':297,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':311,'multiline':False]
['text':' The place to handle DispatchKey.PreDispatch','line_number':313,'multiline':False]
['text':' The check for Python in the exclude set is so we properly respect `with no_dispatch()`','line_number':315,'multiline':False]
['text':' calls inside of a mode.','line_number':316,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':324,'multiline':False]
['text':' This can current fail due to backend fallbacks.  You just have to','line_number':333,'multiline':False]
['text':' register them by hand for HigherOrderOperator.','line_number':334,'multiline':False]
['text':' It's illegal to register DispatchKey to py_kernels, since there's no','line_number':342,'multiline':False]
['text':' C++ kernel to call into','line_number':343,'multiline':False]
['text':' Dynamo already traces the body of HigherOrderOp beforehand when it','line_number':348,'multiline':False]
['text':' so no need to trace into it.','line_number':349,'multiline':False]
['text':' Note - this should maintain identical impl to the C++ dispatcher key extraction logic','line_number':390,'multiline':False]
['text':' at ATen/core/dispatch/DispatchKeyExtractor.h','line_number':391,'multiline':False]
['text':' Note [Per Dispatch Key Modes]','line_number':401,'multiline':False]
['text':' In ordinary eager mode, we have a Python dispatch key that we attach','line_number':402,'multiline':False]
['text':' a mode stack to.','line_number':403,'multiline':False]
['text':' However - when the PyDispatcher is enabled, we extend this functionality','line_number':404,'multiline':False]
['text':' such that every (functionality) dispatch key is allowed to have','line_number':405,'multiline':False]
['text':' its own mode stack.','line_number':406,'multiline':False]
['text':' This is controlled by passing a `torch._C.DispatchKey` into','line_number':407,'multiline':False]
['text':' the mode constructor.','line_number':408,'multiline':False]
['text':' Per-dispatch-key mode variant.','line_number':412,'multiline':False]
['text':' Temporarily pops the top of a given mode stack.','line_number':413,'multiline':False]
['text':' Per-dispatch-key mode variant of push_mode().','line_number':429,'multiline':False]
['text':' Per-dispatch-key mode variant of pop_mode().','line_number':438,'multiline':False]
['text':' Each OpOverload object contains pointer to a a specific operator overload, a pointer to the parent `OpOverloadPacket` object.','line_number':465,'multiline':False]
['text':' You can obtain an OpOverload object through attribute query on OpOverloadPacket.','line_number':466,'multiline':False]
['text':' If the OpOverload was constructed from a Library.def in Python.','line_number':487,'multiline':False]
['text':' Logic replicated from aten/src/ATen/native/MathBitsFallback.h','line_number':490,'multiline':False]
['text':' We will conservatively call mixed mutable/non-mutable','line_number':498,'multiline':False]
['text':' aliased inputs as NOT a view','line_number':499,'multiline':False]
['text':' it's a no-op since OpOverload object is immutable and must be unique for a given op overload.','line_number':503,'multiline':False]
['text':' `my_namespace.my_op_name.overload_name`','line_number':518,'multiline':False]
['text':' NB: This branch is not too necessary anymore, because we can','line_number':539,'multiline':False]
['text':' apply Python CompositeImplicitAutograd *before* tracing','line_number':540,'multiline':False]
['text':' using Python dispatcher (also taking advantage of the autograd','line_number':541,'multiline':False]
['text':' formula).  But it's included for completeness','line_number':542,'multiline':False]
['text':' Remove a dispatch key from the dispatch cache.  This will force it to get','line_number':549,'multiline':False]
['text':' recomputed the next time.  Does nothing','line_number':550,'multiline':False]
['text':' WARNING: if you register a dispatch key to py_kernels of an OpOverload,','line_number':551,'multiline':False]
['text':' calling _del_dispatch on that key is NOT sufficient to apply your change,','line_number':552,'multiline':False]
['text':' because a single registration may affect MULTIPLE dispatch keys (e.g.,','line_number':553,'multiline':False]
['text':' registering Autograd affects AutogradCPU).  del_dispatch is to be used','line_number':554,'multiline':False]
['text':' only if you are specifically modifying how get_dispatch handles a','line_number':555,'multiline':False]
['text':' particular input 'key'.','line_number':556,'multiline':False]
['text':' This implements the pre-computation logic for the Python dispatcher.','line_number':560,'multiline':False]
['text':' This is only called upon a cache miss','line_number':562,'multiline':False]
['text':' TODO: We also need to handle tensor subclasses here','line_number':574,'multiline':False]
['text':' TODO(voz): We should walk all the nodes here / turn it into a list, topmode is ok for now.','line_number':575,'multiline':False]
['text':' TODO: This path is slow, should generally encourage this','line_number':581,'multiline':False]
['text':' case to not happen','line_number':582,'multiline':False]
['text':' TODO(voz): The idea behind this is that we do not yet support dispatch by key + mode, only key.','line_number':584,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':592,'multiline':False]
['text':' The check for Python in the exclude set is so we properly respect `with no_dispatch()`','line_number':595,'multiline':False]
['text':' calls inside of a mode.','line_number':596,'multiline':False]
['text':' This logic is meant to be a python parallel of handle_torch_function_no_python_arg_parser.','line_number':604,'multiline':False]
['text':' TODO: need to double check the semantics of the "types" argument to torch_dispatch.','line_number':610,'multiline':False]
['text':' It's generated in PyInterpreter.cpp, but seems to be generated in two places,','line_number':611,'multiline':False]
['text':' where in one case we only include tensors with the python key, and in another','line_number':612,'multiline':False]
['text':' we include **all** tensors.','line_number':613,'multiline':False]
['text':' TODO: check that I got these args correct (in C++, we pass in "0000"??)','line_number':618,'multiline':False]
['text':' Note [Not Caching Per-Dispatch-Key Mode Handlers]','line_number':623,'multiline':False]
['text':' Note that we're not caching this handler.  There isn't really a point, since the slow bit','line_number':624,'multiline':False]
['text':' is the handler itself (in python).','line_number':625,'multiline':False]
['text':' Also, not caching means that we don't have to reset the cache when any existing','line_number':626,'multiline':False]
['text':' modes go out of scope (which in of itself takes time to loop through all operators).','line_number':627,'multiline':False]
['text':' See Note [Not Caching Per-Dispatch-Key Mode Handlers]','line_number':630,'multiline':False]
['text':' TODO: We could potentially have lots of debugging wrappers against','line_number':635,'multiline':False]
['text':' dispatch keys; design some general registration mechanism instead of','line_number':636,'multiline':False]
['text':' having if statement for each of them','line_number':637,'multiline':False]
['text':' print(self, key, final_key)','line_number':648,'multiline':False]
['text':' TODO: add more methods to expose information about input and output arguments','line_number':670,'multiline':False]
['text':' OpOverloadPacket class contains pointer to a base unresolved operator that doesn't correspond to a specific operator','line_number':673,'multiline':False]
['text':' You can obtain an OpOverload object through attribute query.','line_number':674,'multiline':False]
['text':' These attributes are accessible on the object through the properties','line_number':677,'multiline':False]
['text':' defined below but are immutable','line_number':678,'multiline':False]
['text':' it's a no-op since OpOverloadPacket object is immutable and must be unique for a given op.','line_number':685,'multiline':False]
['text':' It is not a valid op_name when __file__ is passed in','line_number':705,'multiline':False]
['text':' ensure that query for dunder attributes that does not exist on','line_number':709,'multiline':False]
['text':' opoverloadpacket but instead exists on the self._op object does not unnecessarily call','line_number':710,'multiline':False]
['text':' `_get_operation_overload` (which is an expensive operation).','line_number':711,'multiline':False]
['text':' This is done to prevent any potential slowdown. This list can be extended','line_number':712,'multiline':False]
['text':' if there exists other attributes like `__name__` that only exist on self._op and not on the','line_number':713,'multiline':False]
['text':' opoverloadpacket.','line_number':714,'multiline':False]
['text':' This is ok since we are guaranteed that an overload name for an aten op can't start with '__'','line_number':715,'multiline':False]
['text':' for consistency because it seems weird to','line_number':720,'multiline':False]
['text':' throw an attribute error with a message containing','line_number':721,'multiline':False]
['text':' an object name different from the one the attribute','line_number':722,'multiline':False]
['text':' query was performed on.','line_number':723,'multiline':False]
['text':' This is ok since we are guaranteed that an overload name for an aten op can't be 'default'','line_number':730,'multiline':False]
['text':' TODO: disallow access to overloads registered by JIT','line_number':732,'multiline':False]
['text':' cache the overload object','line_number':738,'multiline':False]
['text':' overloading __call__ to ensure torch.ops.foo.bar()','line_number':751,'multiline':False]
['text':' is still callable from JIT','line_number':752,'multiline':False]
['text':' We save the function ptr as the `op` attribute on','line_number':753,'multiline':False]
['text':' OpOverloadPacket to access it here.','line_number':754,'multiline':False]
['text':' TODO: use this to make a __dir__','line_number':757,'multiline':False]
['text':' Resolution of torch.fn is different from torch.ops.aten.fn','line_number':762,'multiline':False]
['text':' torch.fn uses the Python argparser, matches with the','line_number':763,'multiline':False]
['text':' appropriate schema, and calls into the unboxed version of the method','line_number':764,'multiline':False]
['text':' torch.ops.aten.fn resolution is done via the mechanism defined in JIT.','line_number':765,'multiline':False]
['text':' JIT creates a stack of all the overloads and then tries to match the','line_number':766,'multiline':False]
['text':' correct one at runtime and always calls into the boxed version of the method','line_number':767,'multiline':False]
['text':' Autograd codegen creates VariableType, TracerType,','line_number':768,'multiline':False]
['text':' inplace or view type and python bindings.','line_number':769,'multiline':False]
['text':' Aten codegen generates tensor methods for the tensor class.','line_number':770,'multiline':False]
['text':' _OpNamespace is a subclass of ModuleType because the torch script','line_number':772,'multiline':False]
['text':' allows attribute lookups on modules only. Since we want torch.ops.foo.bar()','line_number':773,'multiline':False]
['text':' to work from script, we need to ensure ops and foo are modules','line_number':774,'multiline':False]
['text':' It is not a valid op_name when __file__ is passed in','line_number':807,'multiline':False]
['text':' Get the op `my_namespace::my_op` if available. This will also check','line_number':815,'multiline':False]
['text':' for overloads and raise an exception if there are more than one.','line_number':816,'multiline':False]
['text':' Turn this into AttributeError so getattr(obj, key, default)','line_number':826,'multiline':False]
['text':' works (this is called by TorchScript with __origin__)','line_number':827,'multiline':False]
['text':' let the script frontend know that op is identical to the builtin op','line_number':832,'multiline':False]
['text':' with qualified_op_name','line_number':833,'multiline':False]
['text':' cache the opoverloadpacket to ensure that each op corresponds to','line_number':840,'multiline':False]
['text':' a unique OpOverloadPacket object','line_number':841,'multiline':False]
['text':' Following _OpNamespace.__getattr__, we cache the op on the _PyOpNamespace object.','line_number':853,'multiline':False]
['text':' Check if the name is a HigherOrderOperator','line_number':875,'multiline':False]
['text':' Here we are creating `torch.ops.my_namespace`','line_number':879,'multiline':False]
['text':' Import the shared library into the process, thus running its','line_number':930,'multiline':False]
['text':' static (global) initialization code in order to register custom','line_number':931,'multiline':False]
['text':' operators with the JIT.','line_number':932,'multiline':False]
['text':' The ops "namespace"','line_number':937,'multiline':False]
