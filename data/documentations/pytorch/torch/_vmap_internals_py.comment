['text':' Checks that all args-to-be-batched have the same batch dim size','line_number':13,'multiline':False]
['text':' If value is a tuple, check it has length `num_elements`.','line_number':36,'multiline':False]
['text':' If value is not a tuple, make a tuple with `value` repeated `num_elements` times','line_number':37,'multiline':False]
['text':' Creates BatchedTensors for every Tensor in arg that should be batched.','line_number':48,'multiline':False]
['text':' Returns the (potentially) batched arguments and the batch_size.','line_number':49,'multiline':False]
['text':' See NOTE [Ignored _remove_batch_dim, _add_batch_dim]','line_number':99,'multiline':False]
['text':' Undos the batching (and any batch dimensions) associated with the `vmap_level`.','line_number':107,'multiline':False]
['text':' NOTE [Ignored _remove_batch_dim, _add_batch_dim]','line_number':124,'multiline':False]
['text':' There is something wrong with our type bindings for functions that begin','line_number':125,'multiline':False]
['text':' with '_', see #40397.','line_number':126,'multiline':False]
['text':' type: ignore[return-value]','line_number':129,'multiline':False]
['text':' Checks that `fn` returned one or more Tensors and nothing else.','line_number':146,'multiline':False]
['text':' NB: A python function that return multiple arguments returns a single tuple,','line_number':147,'multiline':False]
['text':' so we are effectively checking that `outputs` is a single Tensor or a tuple of','line_number':148,'multiline':False]
['text':' Tensors.','line_number':149,'multiline':False]
['text':' Not all callables have __name__, in fact, only static functions/methods do.','line_number':184,'multiline':False]
['text':' A callable created via functools.partial or an nn.Module, to name some','line_number':185,'multiline':False]
['text':' examples, don't have a __name__.','line_number':186,'multiline':False]
['text':' vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,','line_number':190,'multiline':False]
['text':' sends those into func, and then unwraps the output BatchedTensors. Operations','line_number':191,'multiline':False]
['text':' on BatchedTensors perform the batched operations that the user is asking for.','line_number':192,'multiline':False]
['text':' A version of vmap but without the initial "experimental prototype" warning','line_number':204,'multiline':False]
['text':' The `allow_none_pass_through` argument is a temporary workaround may be removed.','line_number':211,'multiline':False]
['text':' Currently it enables us to wrap the call in `autograd.grad` to the autograd engine,','line_number':212,'multiline':False]
['text':' which may return None if any of the inputs are unused. See the issue discussing this:','line_number':213,'multiline':False]
['text':' https://github.com/facebookresearch/functorch/issues/159.','line_number':214,'multiline':False]
