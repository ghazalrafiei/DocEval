['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' NOTE [Autograd interaction between torch.Tensor]','line_number':36,'multiline':False]
['text':'','line_number':37,'multiline':False]
['text':' The autograd functions defined below are being used by the public','line_number':38,'multiline':False]
['text':' facing APIs (i.e. from_local, to_local) to ensure our DTensor','line_number':39,'multiline':False]
['text':' works together with torch.Tensor within autograd engine. This','line_number':40,'multiline':False]
['text':' allows DistributedTensor to exist on part of the module hierarchy','line_number':41,'multiline':False]
['text':' and still able to calculate gradients across the torch.Tensor and','line_number':42,'multiline':False]
['text':' DistributedTensor boundary.','line_number':43,'multiline':False]
['text':' As an example, we have the a module that consists of submodules','line_number':44,'multiline':False]
['text':' A, B, and C, the execution flow would be like:','line_number':45,'multiline':False]
['text':'  input(torch.Tensor) -> Module A -> Module B -> Module C -> output (torch.Tensor)','line_number':46,'multiline':False]
['text':'','line_number':47,'multiline':False]
['text':' Suppose I only want to make Module B be a sharded module with','line_number':48,'multiline':False]
['text':' DistributedTensor params, we would need to make the following','line_number':49,'multiline':False]
['text':' flow to work:','line_number':50,'multiline':False]
['text':'','line_number':51,'multiline':False]
['text':'  input(torch.Tensor) -> Module A','line_number':52,'multiline':False]
['text':'       -> DTensor input -> Sharded Module B -> DTensor output','line_number':53,'multiline':False]
['text':'           -> output (torch.Tensor) -> Module C -> output (torch.Tensor)','line_number':54,'multiline':False]
['text':'','line_number':55,'multiline':False]
['text':' We need the conversion from Module A to DTensor input, which is','line_number':56,'multiline':False]
['text':' `from_local`, and conversion from DTensor output to output, which','line_number':57,'multiline':False]
['text':' is `to_local`, thus these two functions must be Autograd functions.','line_number':58,'multiline':False]
['text':'','line_number':59,'multiline':False]
['text':' type: ignore[override]','line_number':62,'multiline':False]
['text':' synchronously wait for any pending collectives to get the result tensor','line_number':72,'multiline':False]
['text':' We need to return a fresh Tensor object there as autograd metadata','line_number':75,'multiline':False]
['text':' will be inplaced into it. So we don't want to pollute the Tensor','line_number':76,'multiline':False]
['text':' object stored in the _local_tensor of this DTensor.','line_number':77,'multiline':False]
['text':' type: ignore[override]','line_number':81,'multiline':False]
['text':' type: ignore[override]','line_number':113,'multiline':False]
['text':' pyre-ignore[2]: Parameter must be annotated.','line_number':114,'multiline':False]
['text':' if it's not by default run_check, we assume user is certain that each','line_number':128,'multiline':False]
['text':' rank has the same tensor shape, and we just use that to calculate the','line_number':129,'multiline':False]
['text':' global shape','line_number':130,'multiline':False]
['text':' if the global rank is not participating in the device mesh, we','line_number':142,'multiline':False]
['text':' simply set the local tensor to an empty tensor','line_number':143,'multiline':False]
['text':' TODO: by default check tensor metas across rank','line_number':146,'multiline':False]
['text':' TODO: See if we need to make this run_check logic','line_number':147,'multiline':False]
['text':' have a corresponding backward.','line_number':148,'multiline':False]
['text':' broadcast rank 0 tensor to all ranks','line_number':151,'multiline':False]
['text':' only broadcast if run_check is True','line_number':152,'multiline':False]
['text':' We want a fresh Tensor object that shares memory with the input tensor','line_number':156,'multiline':False]
['text':' requires_grad of the dist tensor depends on if input','line_number':163,'multiline':False]
['text':' requires_grad or not','line_number':164,'multiline':False]
['text':' type: ignore[override]','line_number':171,'multiline':False]
['text':' reshard to the placement when creating DistributedTensor','line_number':175,'multiline':False]
['text':' so that the gradient layout matches, and we could return','line_number':176,'multiline':False]
['text':' local gradients directly','line_number':177,'multiline':False]
['text':' pyre-fixme[16]: `Redistribute` has no attribute `apply`.','line_number':179,'multiline':False]
['text':' TODO: backward is also differentiable now, add a test','line_number':184,'multiline':False]
['text':' to test higher level gradients.','line_number':185,'multiline':False]
['text':' pyre-ignore[13]: pyre is bad at __new__','line_number':189,'multiline':False]
['text':' class attribute that handles operator placements propagation','line_number':194,'multiline':False]
['text':' rules, keyed by aten op name, value is propagation func','line_number':195,'multiline':False]
['text':' new method instruct wrapper tensor from local_tensor and add','line_number':226,'multiline':False]
['text':' placement spec, it does not do actual distribution','line_number':227,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':228,'multiline':False]
['text':' deepcopy and set spec','line_number':239,'multiline':False]
['text':' pyre-fixme[14]: `__repr__` overrides method defined in `DTensor` inconsistently.','line_number':244,'multiline':False]
['text':' pyre-fixme[3]: Return type must be annotated.','line_number':245,'multiline':False]
['text':' TODO: consider all_gather the local tensors for better debugging','line_number':247,'multiline':False]
['text':' pyre-fixme[3]: Return type must be annotated.','line_number':277,'multiline':False]
['text':' pyre-fixme[2]: Parameter must be annotated.','line_number':278,'multiline':False]
['text':' if same shape/dtype, no need to run_check, if not, must allgather','line_number':331,'multiline':False]
['text':' the metadatas to check the size/dtype across ranks','line_number':332,'multiline':False]
['text':' There should be no data communication unless there's replication','line_number':333,'multiline':False]
['text':' strategy, where we broadcast the replication from the first rank','line_number':334,'multiline':False]
['text':' in the mesh dimension','line_number':335,'multiline':False]
['text':' convert the local tensor to desired device base on device mesh's device_type','line_number':339,'multiline':False]
['text':' set default placements to replicated if not specified','line_number':343,'multiline':False]
['text':' normalize shard dim to be positive','line_number':349,'multiline':False]
['text':' `from_local` is differentiable, and the gradient of the dist tensor this function','line_number':355,'multiline':False]
['text':' created should flow back the gradients to the local_tensor, so we call an autograd','line_number':356,'multiline':False]
['text':' function to construct the dist tensor instead.','line_number':357,'multiline':False]
['text':' pyre-ignore[16]: autograd func','line_number':358,'multiline':False]
['text':' pyre-ignore[16]: autograd func','line_number':397,'multiline':False]
['text':' NOTE: This redistribute API currently only supports out','line_number':423,'multiline':False]
['text':' of place redistribution, i.e. it always create a new','line_number':424,'multiline':False]
['text':' DTensor object and leave the original one unchanged.','line_number':425,'multiline':False]
['text':' if device_mesh is not specified, use the current device_mesh','line_number':427,'multiline':False]
['text':' raise error if new placements not specified','line_number':429,'multiline':False]
['text':' normalize shard dim to be positive','line_number':440,'multiline':False]
['text':' Early return the original DTensor if the placements are the same.','line_number':444,'multiline':False]
['text':' pyre-fixme[16]: `Redistribute` has no attribute `apply`.','line_number':448,'multiline':False]
['text':' TODO: fix issue with full_tensor() for uneven-sharded tensor','line_number':477,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/115310','line_number':478,'multiline':False]
['text':' get default device mesh if there's nothing specified','line_number':536,'multiline':False]
['text':' call PyTorch/XLA SPMD for `xla` backend type device mesh.','line_number':540,'multiline':False]
['text':' This returns XLAShardedTensor','line_number':541,'multiline':False]
['text':' type:ignore[return-value]','line_number':546,'multiline':False]
['text':' instantiate a RNG tracker if haven't. By default DTensor uses an','line_number':548,'multiline':False]
['text':' OffsetBasedRNGTracker to perform random operators.','line_number':549,'multiline':False]
['text':' TODO: the value assignment to global variable is not the ideal solution','line_number':550,'multiline':False]
['text':' we can replace it in future.','line_number':551,'multiline':False]
['text':' convert tensor to the corresponding device type if it's not in that device type','line_number':560,'multiline':False]
['text':' set default placements to replicated if not specified','line_number':564,'multiline':False]
['text':' if the tensor is already a DTensor, we just need to check if the','line_number':574,'multiline':False]
['text':' device mesh and placements are the same','line_number':575,'multiline':False]
['text':' distribute the tensor according to the placements.','line_number':591,'multiline':False]
['text':' normalize shard placement dim','line_number':597,'multiline':False]
['text':' detach the local tensor passed to DTensor since after the construction','line_number':611,'multiline':False]
['text':' of DTensor, autograd would work on top of DTensor instead of local tensor','line_number':612,'multiline':False]
['text':' This function loop over the immediate module parameters and','line_number':658,'multiline':False]
['text':' buffers, replicate all non DTensor params/buffers to DTensor','line_number':659,'multiline':False]
['text':' parameters/buffers, if they have not been partitioned in the','line_number':660,'multiline':False]
['text':' partition_fn, we can't easily use `module._apply` here','line_number':661,'multiline':False]
['text':' because we don't know what happened inside partition_fn as','line_number':662,'multiline':False]
['text':' user could do anything, i.e. install hooks, and we want to','line_number':663,'multiline':False]
['text':' preserve those.','line_number':664,'multiline':False]
['text':' if partition_fn not specified, we by default replicate','line_number':677,'multiline':False]
['text':' all module params/buffers','line_number':678,'multiline':False]
['text':' apply partition_fun to submodules','line_number':682,'multiline':False]
['text':' register input_fn as module forward pre hook','line_number':687,'multiline':False]
['text':' type: ignore[misc]','line_number':689,'multiline':False]
['text':' register input_fn as module forward hook','line_number':690,'multiline':False]
['text':' type: ignore[misc]','line_number':693,'multiline':False]
