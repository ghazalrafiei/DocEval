['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' Default strategy by default just propagate the first input strategy','line_number':49,'multiline':False]
['text':' equal_strategy deals with ops that comparing two tensor, we need to make sure','line_number':67,'multiline':False]
['text':' sharding layout the same with two operands, we choose to follow the arg with max','line_number':68,'multiline':False]
['text':' num of shards, still keep is_same_size here for completeness as they share the','line_number':69,'multiline':False]
['text':' same strategy in theory.','line_number':70,'multiline':False]
['text':' if the arg_spec have partial, reshard to replicate','line_number':85,'multiline':False]
['text':' otherwise local shard tensor comparison would be invalid','line_number':86,'multiline':False]
['text':' create_like_strategy deals with ops that creating tensors with same','line_number':123,'multiline':False]
['text':' shape as input, but with specific content that does not depend on','line_number':124,'multiline':False]
['text':' the input, we can propagate sharding, but we have to make sure we','line_number':125,'multiline':False]
['text':' move from partial to replicated.','line_number':126,'multiline':False]
['text':' if the arg_spec have partial, accept partial','line_number':133,'multiline':False]
['text':' in the input_specs but output replicate for','line_number':134,'multiline':False]
['text':' those corresponding mesh dims','line_number':135,'multiline':False]
['text':' TODO: re-think new_empty_strided','line_number':159,'multiline':False]
['text':' TODO: maybe we should generate all possible shardings intead of just stay','line_number':164,'multiline':False]
['text':' replicated for new factory methods','line_number':165,'multiline':False]
['text':' normalize args','line_number':207,'multiline':False]
['text':' only add the strategy if the slice dim is not sharded','line_number':219,'multiline':False]
['text':' if all strategies are filtered out, unsharding all specs on slice dim','line_number':223,'multiline':False]
['text':' of the input strategy, and use that as the op strategy','line_number':224,'multiline':False]
['text':' Not using p.is_shard() to avoid mypy complain about Placement not having','line_number':250,'multiline':False]
['text':' attribute dim.','line_number':251,'multiline':False]
['text':' 1. number of dimensions in input and src need to match.','line_number':260,'multiline':False]
['text':' 2. number of elements on all non-dim need to match between input and src.','line_number':261,'multiline':False]
['text':' 3. numer of elements in src in dim need to match the slice size.','line_number':262,'multiline':False]
['text':' Given the above:','line_number':263,'multiline':False]
['text':' - We suggest for src to follow the sharding of input, except on the scatter dimension,','line_number':264,'multiline':False]
['text':'   where our best bet for now is to make them replicated as a fall-back.','line_number':265,'multiline':False]
['text':'   TODO: Ideally we'd like to make sure the output is re-sharded afterwards to keep input sharding.','line_number':266,'multiline':False]
['text':' by default follow the input strategy for both input and src','line_number':277,'multiline':False]
['text':' only add the strategy if the slice_scatter dim is not sharded or partial','line_number':284,'multiline':False]
['text':' if all strategies are filtered out, replicating all specs on slice_scatter dim','line_number':290,'multiline':False]
['text':' of the input strategy, and use that as the op strategy','line_number':291,'multiline':False]
['text':' Current sharding constraints:','line_number':348,'multiline':False]
['text':' For values:','line_number':349,'multiline':False]
['text':'   1. We currently require that the dimension of values_spec be replicated or partial','line_number':350,'multiline':False]
['text':'      if they are being indexed on.','line_number':351,'multiline':False]
['text':'   2. Other dimensions of values_spec can remain sharded if they are so.','line_number':352,'multiline':False]
['text':' For indices:','line_number':353,'multiline':False]
['text':'   Indices can be either sharded or replicated. All index tensors need to be sharded','line_number':354,'multiline':False]
['text':'   in a compatible way, following the pointwise rule (including resolving _Partial','line_number':355,'multiline':False]
['text':'   into either sharded or replicated)','line_number':356,'multiline':False]
['text':' 1. All indices have to be sharded equally. Moreover, indices can be broadcast.','line_number':366,'multiline':False]
['text':'    Here, we piggyback on the pointwise sharding rule for indices.','line_number':367,'multiline':False]
['text':' this means that our inputs are already sharded properly and we will use that as our indices_spec','line_number':378,'multiline':False]
['text':' we'll need to call pointwise_rule again to see what's our ideal indices_spec and then','line_number':386,'multiline':False]
['text':' use that to compute our ideal values_spec','line_number':387,'multiline':False]
['text':' if all index vectors are consecutives, insert at the dimension of the first index','line_number':407,'multiline':False]
['text':' else, insert on the first dimension','line_number':410,'multiline':False]
['text':' accounts for the offset in output dimensions','line_number':418,'multiline':False]
['text':' _Partial or Replicated','line_number':425,'multiline':False]
['text':' torch.cat requires all tensors must either have the same shape (except','line_number':469,'multiline':False]
['text':' in the concatenating dimension) or be "empty". "Empty" here strictly means','line_number':470,'multiline':False]
['text':' tensor.shape is torch.Size([0]). When tensor.ndim > 1, it will be treated','line_number':471,'multiline':False]
['text':' as a non-empty tensor and the shape must match on non-cat dimensions.','line_number':472,'multiline':False]
['text':' the first arg is a list of input tensor specs','line_number':476,'multiline':False]
['text':' all tensors are empty, we can return any output sharding','line_number':482,'multiline':False]
['text':' ndim will also be the result's ndim','line_number':497,'multiline':False]
['text':' default dim = 0','line_number':502,'multiline':False]
['text':' Make sure all tensors are replciated on cat dimension','line_number':507,'multiline':False]
['text':' align non-cat dimensions placements based on reshard cost','line_number':527,'multiline':False]
['text':' compute the minimum cost of resharding on this mesh_dim','line_number':533,'multiline':False]
['text':' only reshard if there is a mismatch','line_number':538,'multiline':False]
['text':' compute the cost of resharding on this shard_dim','line_number':542,'multiline':False]
['text':' found one tensor where the shard_dim is smaller than','line_number':547,'multiline':False]
['text':' mesh_dim. In this case, we cannot shard on this shard_dim,','line_number':548,'multiline':False]
['text':' and hence set cost to infinity.','line_number':549,'multiline':False]
['text':' no mismatch, keep the original placement','line_number':565,'multiline':False]
['text':' at this point, the cat dim is not sharded,','line_number':596,'multiline':False]
['text':' TODO: tensor to split cannot have _Partial','line_number':616,'multiline':False]
['text':' in its placements for now. Will need to','line_number':617,'multiline':False]
['text':' support in future.','line_number':618,'multiline':False]
['text':' TODO: just like slice op, split replicates before','line_number':626,'multiline':False]
['text':' splitting on a sharded dimension','line_number':627,'multiline':False]
['text':' Last chunk will be smaller if the tensor size N','line_number':650,'multiline':False]
['text':' along the given dimension dim is not divisible by i.','line_number':651,'multiline':False]
