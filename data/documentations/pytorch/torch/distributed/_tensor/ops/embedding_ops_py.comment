['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' implement matrix related ops for distributed tensor','line_number':2,'multiline':False]
['text':' TODO: Enable BWD for embedding op.','line_number':18,'multiline':False]
['text':' Embedding table is replicated, input ids are sharded along batch','line_number':28,'multiline':False]
['text':' dimension. Output lookups should match input sharding spec in this case.','line_number':29,'multiline':False]
['text':' The embedding table is replicated, and input/oupput activations are','line_number':74,'multiline':False]
['text':' sharded. In this case, gradients for the embedding table should be','line_number':75,'multiline':False]
['text':' Partial.','line_number':76,'multiline':False]
['text':' The embedding table is replicated and the indices is also replicated','line_number':81,'multiline':False]
['text':' (local is a more precise term). This is postional embedding. In this','line_number':82,'multiline':False]
['text':' case, gradients for the embmedding table should be Partial.','line_number':83,'multiline':False]
['text':' BWD for colwise sharding case','line_number':88,'multiline':False]
