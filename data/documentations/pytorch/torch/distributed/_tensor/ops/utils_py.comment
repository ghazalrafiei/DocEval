['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' convenient wrapper to register sharding propagation rules','line_number':19,'multiline':False]
['text':' pyre-fixme[3]: Return type must be annotated.','line_number':20,'multiline':False]
['text':' pyre-fixme[2]: Parameter must be annotated.','line_number':21,'multiline':False]
['text':' pyre-fixme[53]: Captured variable `func` is not annotated.','line_number':23,'multiline':False]
['text':' pyre-fixme[3]: Return type must be annotated.','line_number':24,'multiline':False]
['text':' pyre-fixme[2]: Parameter must be annotated.','line_number':25,'multiline':False]
['text':' pyre-fixme[53]: Captured variable `func` is not annotated.','line_number':38,'multiline':False]
['text':' pyre-fixme[3]: Return type must be annotated.','line_number':39,'multiline':False]
['text':' pyre-fixme[2]: Parameter must be annotated.','line_number':40,'multiline':False]
['text':' pyre-fixme[11]: Annotation `immutable_list` is not defined as a type.','line_number':54,'multiline':False]
['text':' type: ignore[valid-type]','line_number':55,'multiline':False]
['text':' During tracing, `aten.sum.dim_IntList` uses `immutable_list` for its args,','line_number':56,'multiline':False]
['text':' which is an object but treated as a list by the tracer. Therefore, keep','line_number':57,'multiline':False]
['text':' `immutable_list` intact here as well.','line_number':58,'multiline':False]
['text':' number of shards in each tensor dimension','line_number':105,'multiline':False]
['text':' TODO: maybe we should determine is_shardable based on','line_number':113,'multiline':False]
['text':'       whether it's evenly sharded or not','line_number':114,'multiline':False]
['text':' infer the broadcast dims map, where it maps from the common shape dim to the input shape dim','line_number':134,'multiline':False]
['text':' this is aligned with the broadcast semantics','line_number':135,'multiline':False]
['text':' there's a map from the common shape shard dim to','line_number':160,'multiline':False]
['text':' the input shape shard dim before broadcasting,','line_number':161,'multiline':False]
['text':' use that instead','line_number':162,'multiline':False]
['text':' there's no map between common shape shard dim and','line_number':165,'multiline':False]
['text':' the input shape shard dim before broadcasting,','line_number':166,'multiline':False]
['text':' in this case it means implicit broadcasting happen','line_number':167,'multiline':False]
['text':' in this dim, so we can just mark it as replicate','line_number':168,'multiline':False]
['text':' and implict broadcast will broadcast automatically','line_number':169,'multiline':False]
['text':' to the sharded shape','line_number':170,'multiline':False]
