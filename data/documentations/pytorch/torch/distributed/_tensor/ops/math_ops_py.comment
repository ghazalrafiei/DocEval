['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' if input dim in reduction dims, mark it as -1','line_number':54,'multiline':False]
['text':' otherwise mark it as the new dim','line_number':57,'multiline':False]
['text':' replicate the reduction dims if not reduction_linear','line_number':67,'multiline':False]
['text':' if new_shard_dim collapsed or its in the reduction dims','line_number':99,'multiline':False]
['text':' (i.e. for the case where keepdims=True), we generate partial','line_number':100,'multiline':False]
['text':' by default follow reduction input strategy','line_number':121,'multiline':False]
['text':' input placements for this strategy should clear out pending sum and sharding','line_number':126,'multiline':False]
['text':' on the reduction dimension','line_number':127,'multiline':False]
['text':' args must be: input, normalized_shape, weight, bias, eps','line_number':263,'multiline':False]
['text':' for None weight and bias, their corresponding objects will','line_number':264,'multiline':False]
['text':' be None as well. layer_norm_strategy returns one OpStrategy','line_number':265,'multiline':False]
['text':' for the triple return values (out, mean, rstd).','line_number':266,'multiline':False]
['text':' the current layer norm implementation requires that all','line_number':276,'multiline':False]
['text':' input DTensor's sharding must be in form of OpStrategy','line_number':277,'multiline':False]
['text':' we use OpStrategy because the output (out, mean, rstd)','line_number':285,'multiline':False]
['text':' should have the same placements','line_number':286,'multiline':False]
['text':' for the input tensor, we replicate it on the inner dims if necessary','line_number':293,'multiline':False]
['text':' TODO: we can avoid forcing the redistribution once we figure out','line_number':294,'multiline':False]
['text':' how to decompose layer norm','line_number':295,'multiline':False]
['text':' for the weight tensor, we replicate it on all dims if necessary','line_number':310,'multiline':False]
['text':' TODO: we can avoid forcing the redistribution once we figure out','line_number':311,'multiline':False]
['text':' how to decompose layer norm','line_number':312,'multiline':False]
['text':' for the bias tensor, we replicate it on all dims if necessary','line_number':327,'multiline':False]
['text':' TODO: we can avoid forcing the redistribution once we figure out','line_number':328,'multiline':False]
['text':' how to decompose layer norm','line_number':329,'multiline':False]
['text':' the output spec is the same as input spec','line_number':340,'multiline':False]
['text':' make it replicate','line_number':359,'multiline':False]
['text':' keep the placement','line_number':361,'multiline':False]
