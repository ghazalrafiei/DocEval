['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' parse einop equation and extract arg specs','line_number':73,'multiline':False]
['text':' NOTE: only support single output unless needed in future','line_number':77,'multiline':False]
['text':' record pending sum, key is mesh dimension, value is pending sum','line_number':82,'multiline':False]
['text':' counter across input specs','line_number':83,'multiline':False]
['text':' merge the sharding of inputs if it's able to merge, i.e. we can merge','line_number':89,'multiline':False]
['text':' replicate and shard to shard, but this will trigger an reshard operation','line_number':90,'multiline':False]
['text':' reshard the replicate to match the sharded one','line_number':93,'multiline':False]
['text':' TODO: further merge the sharding properly (i.e. reshard one input to replicate)','line_number':98,'multiline':False]
['text':' deal with partial sums','line_number':106,'multiline':False]
['text':' update pending sum counter for pending sum mesh','line_number':111,'multiline':False]
['text':' dimension with the occurrence from each input','line_number':112,'multiline':False]
['text':' after merging sharding, we check if there're multiple','line_number':130,'multiline':False]
['text':' sharding on the same mesh dim.','line_number':131,'multiline':False]
['text':' return reshard suggestion with no pending sum, because we already properly','line_number':144,'multiline':False]
['text':' merge the sharding, this reshard suggestion is legit to use','line_number':145,'multiline':False]
['text':' It's a op that support linearity, but not all input arguments are partial','line_number':150,'multiline':False]
['text':' we fail the sharding propagation with suggestion to make all inputs be','line_number':151,'multiline':False]
['text':' partial on the corresponding mesh dim (all inputs should be partial for','line_number':152,'multiline':False]
['text':' the mesh dims in order to execute locally and delay the sum reduction)','line_number':153,'multiline':False]
['text':' we found different input dims are being sharded on the same mesh dim','line_number':160,'multiline':False]
['text':' in order to perform local op computation, we need to reshard inputs','line_number':161,'multiline':False]
['text':' base on some simple heuristics, now we simply pick the one with least comm','line_number':162,'multiline':False]
['text':' volume. (i.e. the input with least size)','line_number':163,'multiline':False]
['text':' TODO: consider a more advanced heuristic to pick the best sharding','line_number':164,'multiline':False]
['text':' update dim_to_sharding to keep the sharding of the dim with','line_number':182,'multiline':False]
['text':' highest comm and make the rest of the dims to replicate','line_number':183,'multiline':False]
['text':' generate output pending sum if a dim is sharded, and it appears in input','line_number':193,'multiline':False]
['text':' but not output','line_number':194,'multiline':False]
['text':' if no need to reshard, we directly generate the output sharding','line_number':199,'multiline':False]
['text':' find output dim that is a singleton dimension, mark sharding and shape','line_number':204,'multiline':False]
['text':' XXX: since we still need to have intermediate shape calculation, we need','line_number':211,'multiline':False]
['text':' to pass in the shape here. We should remove this once sharding decomp works','line_number':212,'multiline':False]
['text':' for ops like addmm','line_number':213,'multiline':False]
['text':' find the max_dim first in case we need to broadcasting','line_number':239,'multiline':False]
['text':' handle the "broadcasting to a common shape case"','line_number':247,'multiline':False]
['text':' see https://pytorch.org/docs/stable/notes/broadcasting.html','line_number':248,'multiline':False]
['text':' If any of the dimensions is singleton dimension (i.e. 1).','line_number':249,'multiline':False]
['text':' we mark the dim char as a special "1" to distinguish with','line_number':250,'multiline':False]
['text':' the non-singleton dimension, so that sharding propagation','line_number':251,'multiline':False]
['text':' should just ignore the singleton dimension.','line_number':252,'multiline':False]
['text':' treat the leading miss dim chars as singleton','line_number':256,'multiline':False]
['text':' mark singleton dim char as a special "1" in einop rule','line_number':259,'multiline':False]
['text':' check if we replace the all inputs dim char with singleton dimension,','line_number':265,'multiline':False]
['text':' if we replace all inputs, we also need to replace the output dimension.','line_number':266,'multiline':False]
['text':' inplace op should keep the input sharding it writes to','line_number':276,'multiline':False]
