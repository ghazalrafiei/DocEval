['text':' type: ignore[no-redef, assignment]','line_number':12,'multiline':False]
['text':' Common type aliases','line_number':18,'multiline':False]
['text':' ATen op schemas could have Tensor, Tuple[Tensor] and List[Tensor], so output type sould','line_number':21,'multiline':False]
['text':' be the same set of possibilities.','line_number':22,'multiline':False]
['text':' simple analysis of function schema to determine','line_number':39,'multiline':False]
['text':' if this is an inplace variant, it might not','line_number':40,'multiline':False]
['text':' be entirely correct, but it's good enough for now.','line_number':41,'multiline':False]
['text':' simple analysis of function schema to determine','line_number':46,'multiline':False]
['text':' if this is an out variant, it might not','line_number':47,'multiline':False]
['text':' be entirely correct, but it's good enough for now.','line_number':48,'multiline':False]
['text':' redistribute costs for this op placement strategy','line_number':62,'multiline':False]
['text':' we need a nested list to record the cost for each','line_number':63,'multiline':False]
['text':' operand of this operator, and for each operand of','line_number':64,'multiline':False]
['text':' this operator it might have multiple placement strategies','line_number':65,'multiline':False]
['text':' This static_argnum records static arg "starting index" for ops that have non-tensor','line_number':157,'multiline':False]
['text':' args/kwargs which would affect sharding propagation results. All args after this','line_number':158,'multiline':False]
['text':' index would be hashed to our sharding cache.','line_number':159,'multiline':False]
['text':' Note that only a few ops need this information, e.g. view, transpose, var.dim, etc.','line_number':160,'multiline':False]
['text':' This static_kwargkey records static kwarg names which would affect sharding prop','line_number':162,'multiline':False]
['text':' each op can decide if it wants to use pytree flatten/unflatten during operator','line_number':164,'multiline':False]
['text':' eager execution, by default we don't need to do flatten/unflatten, only if the','line_number':165,'multiline':False]
['text':' op indicate it needs to, this is to accelate eager performance.','line_number':166,'multiline':False]
['text':' filter out non-relevant values from args schema to get a clean spec list','line_number':202,'multiline':False]
['text':' this would mainly be used by sharding propagation rules','line_number':203,'multiline':False]
['text':' all dispatch ops only return Tensor or Tuple[Tensor], so this check if enough','line_number':256,'multiline':False]
['text':' all dispatch ops only return Tensor or Tuple[Tensor] for tensor like','line_number':263,'multiline':False]
['text':' return types, so this check is enough for tensor like types','line_number':264,'multiline':False]
['text':' Only hash args and kwargs that op indicates to hash','line_number':268,'multiline':False]
['text':' early return checks','line_number':290,'multiline':False]
['text':' compare each element and early return if any of them is different','line_number':300,'multiline':False]
['text':' check kwarg equality when there's a static kwarg key','line_number':316,'multiline':False]
['text':' the output sharding info','line_number':391,'multiline':False]
