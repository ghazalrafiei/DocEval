['text':' XXX: we define a _ScriptModuleOptimizer here to explicitly','line_number':21,'multiline':False]
['text':' compile the FunctionalOptimizer class into TorchScript','line_number':22,'multiline':False]
['text':' This is because ScriptClass instance still lives in','line_number':23,'multiline':False]
['text':' python unless you explicitly compile it as an attribute','line_number':24,'multiline':False]
['text':' in ScriptModule or pass it to a ScriptFunction','line_number':25,'multiline':False]
['text':' _ScriptLocalOptimizerInterface serves as a common','line_number':26,'multiline':False]
['text':' interface type for Optimizer ScriptModules.','line_number':27,'multiline':False]
['text':'','line_number':28,'multiline':False]
['text':' TODO (wanchaol): remove this once we added TorchScript','line_number':29,'multiline':False]
['text':' class reference semantics','line_number':30,'multiline':False]
['text':' TorchScript does not support multithread concurrent compiling.','line_number':38,'multiline':False]
['text':' request_callback might invoke concurrent compiling, so we','line_number':39,'multiline':False]
['text':' serialize the compiling with a lock','line_number':40,'multiline':False]
['text':' apply functional optimizer step with a list of gradients','line_number':51,'multiline':False]
['text':' TODO (wanchaol): remove/merge this with ScriptLocalOptimizer once','line_number':60,'multiline':False]
['text':' we have converted all to functional optimizer in distributed.optim','line_number':61,'multiline':False]
['text':' Ideally we would only need to share a lock for instances of','line_number':63,'multiline':False]
['text':' _LocalOptimizer that deal with the same parameters. We are','line_number':64,'multiline':False]
['text':' making a simplifying assumption here that if there is more','line_number':65,'multiline':False]
['text':' than one instance of _LocalOptimizer per worker, they will','line_number':66,'multiline':False]
['text':' be optimizing the same parameters (e.g. each data parallel','line_number':67,'multiline':False]
['text':' trainer will create its own instance of _LocalOptimizer but','line_number':68,'multiline':False]
['text':' they will all optimize the same parameters on each worker)','line_number':69,'multiline':False]
['text':' new/step functions combined with _ScriptLocalOptimizer to provide GIL-free optimizer','line_number':94,'multiline':False]
['text':' TODO: improve error propagation','line_number':112,'multiline':False]
