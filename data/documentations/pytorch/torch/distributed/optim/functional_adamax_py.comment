['text':' Define a TorchScript compatible Functional Adamax Optimizer','line_number':10,'multiline':False]
['text':' where we use these optimizer in a functional way.','line_number':11,'multiline':False]
['text':' Instead of using the `param.grad` when updating parameters,','line_number':12,'multiline':False]
['text':' we explicitly allow the distributed optimizer pass gradients to','line_number':13,'multiline':False]
['text':' the `step` function. In this way, we could separate the gradients','line_number':14,'multiline':False]
['text':' and parameters and allow multithreaded trainer to update the','line_number':15,'multiline':False]
['text':' parameters without data traces on accumulating to the same .grad.','line_number':16,'multiline':False]
['text':' NOTE: This should be only used by distributed optimizer internals','line_number':17,'multiline':False]
['text':' and not meant to expose to the user.','line_number':18,'multiline':False]
['text':' NOTE: we only have one param_group and don't allow user to add additional','line_number':57,'multiline':False]
['text':' param group as it's not a common use case.','line_number':58,'multiline':False]
['text':' Lazy state initialization','line_number':82,'multiline':False]
['text':' Exponential moving average of gradient values','line_number':87,'multiline':False]
['text':' Exponential moving average of squared gradient values','line_number':91,'multiline':False]
