['text':' WeakTensorKeyDictionary to store relevant meta-data for the Tensor/Parameter','line_number':7,'multiline':False]
['text':' without changing it's life-time.','line_number':8,'multiline':False]
['text':' NOTE: Alternative is to add the meta-data as an attribute to the tensor,','line_number':9,'multiline':False]
['text':'       but that will serialize the meta-data if Tensor is serialized.','line_number':10,'multiline':False]
['text':' view_as creates a node in autograd graph that allows us access to the','line_number':57,'multiline':False]
['text':' parameter's AccumulateGrad autograd function object. We register a','line_number':58,'multiline':False]
['text':' hook on this object to fire the optimizer when the gradient for','line_number':59,'multiline':False]
['text':' this parameter is ready (has been accumulated into .grad field)','line_number':60,'multiline':False]
['text':' Don't create a new acc_grad if we already have one','line_number':62,'multiline':False]
['text':' i.e. for shared parameters or attaching multiple optimizers to a param.','line_number':63,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':70,'multiline':False]
['text':' TODO: Remove these attributes once we have a better way of accessing','line_number':71,'multiline':False]
['text':' optimizer classes and kwargs for a parameter.','line_number':72,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':73,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':74,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':76,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':77,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':78,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':84,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':89,'multiline':False]
