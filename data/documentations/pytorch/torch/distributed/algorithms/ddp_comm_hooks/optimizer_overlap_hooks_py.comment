['text':' Run original hook','line_number':60,'multiline':False]
['text':' Apply gradient division since C++ side only allreduces and does','line_number':68,'multiline':False]
['text':' not average. TODO: (rohan-varma) the div factor may be different','line_number':69,'multiline':False]
['text':' when running with join hook','line_number':70,'multiline':False]
['text':' TODO (rohan-varma): upcast as needed for DDP mixed precision,','line_number':74,'multiline':False]
['text':' once optimizer in backward + DDP mixed precision is supported.','line_number':75,'multiline':False]
['text':' Note: need to set grad to the bucket's grad, because','line_number':78,'multiline':False]
['text':' running allreduce results in the bucket's grad being','line_number':79,'multiline':False]
['text':' reduced, but not grad field.','line_number':80,'multiline':False]
['text':' Need to return a Future[Tensor] to obey comm hook API contract.','line_number':86,'multiline':False]
['text':' enqueue a callback to wait for this optimizer stream at the end of','line_number':90,'multiline':False]
['text':' backward and set all DDP managed grads to None.','line_number':91,'multiline':False]
['text':' Set DDP managed grads to None','line_number':96,'multiline':False]
['text':' reset for the next backwards pass','line_number':101,'multiline':False]
['text':' mark that the callback is enqueued','line_number':108,'multiline':False]
['text':' These are needed for DDP's logging of comm hooks','line_number':116,'multiline':False]
['text':' Run original hook','line_number':137,'multiline':False]
