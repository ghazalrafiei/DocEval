['text':' a custom type wrapping both inplace output tensor and work handle','line_number':22,'multiline':False]
['text':' This function is only used by tracing mode as a call_function node right','line_number':28,'multiline':False]
['text':' before consuming a collective result tensor.','line_number':29,'multiline':False]
['text':' E.g.,','line_number':43,'multiline':False]
['text':' allreduce_ returns ([tensor], work)','line_number':44,'multiline':False]
['text':' allgather_ returns ([[tensor1, tensor2]], work)','line_number':45,'multiline':False]
['text':' noop for eager mode','line_number':103,'multiline':False]
['text':' Use non-CommTensor to avoid nested CommTensor Wrapping','line_number':106,'multiline':False]
['text':' The tensor object wrapped by this CommTensor','line_number':108,'multiline':False]
['text':' NB: THIS CAN BE A CommTensor; see test_nested_comm_tensor_wrapping','line_number':109,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':110,'multiline':False]
['text':' Record the LAST `work` object returned by collective communication','line_number':111,'multiline':False]
['text':' operations. If this is None, it means no collectives have called','line_number':112,'multiline':False]
['text':' since last time a tensor is wrapped by CommTensor','line_number':113,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':114,'multiline':False]
['text':' disable __torch_function__ so that CommTensor can recursively dispatch','line_number':120,'multiline':False]
['text':' with ProxyTorchDispatchMode in make_fx','line_number':121,'multiline':False]
['text':' shared states when unwrapping args','line_number':130,'multiline':False]
['text':' wrapped ._tensor if this is a CommTensor, and insert/call wait()','line_number':134,'multiline':False]
['text':' if communication has been launched on this tensor.','line_number':135,'multiline':False]
['text':' TODO(ezyang): I don't really understand what's going on','line_number':141,'multiline':False]
['text':' here, but it seems that tracer doesn't reflect whether or','line_number':142,'multiline':False]
['text':' not there is ambient tracing going on, but rather, whether','line_number':143,'multiline':False]
['text':' or not we will trace THIS particular invocation.  If we','line_number':144,'multiline':False]
['text':' have a nested CommTensor, the outer layer doesn't actually','line_number':145,'multiline':False]
['text':' trace and we only trace the inner layer','line_number':146,'multiline':False]
['text':' insert a node to the traced graph.','line_number':152,'multiline':False]
['text':' type: ignore[union-attr]','line_number':153,'multiline':False]
['text':' HACK: update the proxy for the inplace output','line_number':160,'multiline':False]
['text':' For eager mode, simply wait.','line_number':162,'multiline':False]
['text':' During tracing, still need to wait here, to make sure the','line_number':163,'multiline':False]
['text':' execution during tracing is correct.','line_number':164,'multiline':False]
['text':' communication has been waited, stop propagating CommTensor','line_number':167,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':177,'multiline':False]
['text':' in tracing mode, get proxies for args','line_number':190,'multiline':False]
['text':' get proxy for output tuple','line_number':201,'multiline':False]
['text':' insert a node that wraps the output tuple into','line_number':204,'multiline':False]
['text':' _CommResult(tensor, work)','line_number':205,'multiline':False]
['text':' type: ignore[union-attr]','line_number':206,'multiline':False]
['text':' disable dispatch to avoid trigger ProxyTorchDispatchMode logic','line_number':215,'multiline':False]
['text':' wrap output with the proxy of _CommResult, so that subsequent','line_number':218,'multiline':False]
['text':' ops and link to it.','line_number':219,'multiline':False]
['text':' N.B.: we still need to remember the work handle here, and wait','line_number':222,'multiline':False]
['text':' for it later to make sure the execution during tracing is','line_number':223,'multiline':False]
['text':' correct. Also, remember comm is already launched','line_number':224,'multiline':False]
['text':' args[0] is always the collection of output tensors','line_number':225,'multiline':False]
['text':' HACK: update the proxy on the input argument as this is an','line_number':228,'multiline':False]
['text':' inplace collective communication.','line_number':229,'multiline':False]
['text':' in eager mode, simply remember work handle as an attribute','line_number':237,'multiline':False]
['text':' we need to propagate CommTensor wrapping until the first','line_number':245,'multiline':False]
['text':' subsequent operation has waited for it.','line_number':246,'multiline':False]
