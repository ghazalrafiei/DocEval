['text':' We need to import _functional_collectives to trigger op registration','line_number':13,'multiline':False]
['text':' pyre-ignore[3]','line_number':93,'multiline':False]
['text':' pyre-ignore[2, 3]','line_number':97,'multiline':False]
['text':' pyre-ignore[16]','line_number':114,'multiline':False]
['text':' pyre-ignore[6]','line_number':117,'multiline':False]
['text':' type: ignore[arg-type]','line_number':118,'multiline':False]
['text':' pyre-ignore[6]','line_number':119,'multiline':False]
['text':' type: ignore[arg-type]','line_number':120,'multiline':False]
['text':' pyre-ignore[16]','line_number':121,'multiline':False]
['text':' Use a dtensor expand mode for now to preserve the old behavior','line_number':129,'multiline':False]
['text':' and avoid breaking existing code','line_number':130,'multiline':False]
['text':' update opt.state with proxy tensors','line_number':147,'multiline':False]
['text':' opt.state's key type is string, but optimizer uses Parameter as keys','line_number':150,'multiline':False]
['text':' type: ignore[index]','line_number':151,'multiline':False]
['text':' FIXME: support multiple parameter groups','line_number':153,'multiline':False]
['text':' pyre-ignore','line_number':165,'multiline':False]
['text':' The return value of torch._utils.is_compiling changes optimizer behavior.','line_number':170,'multiline':False]
['text':' We need that function to return True to include optimizer in the graph.','line_number':171,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/blob/a524123c91ab399c9dd6882c1189596dd77e7734/torch/optim/optimizer.py#L41','line_number':172,'multiline':False]
['text':' skip gradient copying as we don't need to copy gradients back','line_number':253,'multiline':False]
['text':' replace all args with the results from the first unique comm op','line_number':306,'multiline':False]
['text':' first time seeing this combination, remember it','line_number':313,'multiline':False]
['text':' the current node is a duplicate, replace it','line_number':316,'multiline':False]
['text':' 1. Extract nn.Module and Optimizer from args and kwargs','line_number':340,'multiline':False]
['text':' FIXME(@mrshenli): support multiple nn.Module instances','line_number':341,'multiline':False]
['text':' FIXME(@mrshenli): support multiple Optiimzer instances','line_number':342,'multiline':False]
['text':' FIXME(@mrshenli): need to broadcast model to sync parameters','line_number':343,'multiline':False]
['text':' 2. Override target submodules (e.g., MoE) with dummy replacements','line_number':355,'multiline':False]
['text':' type: ignore[union-attr]','line_number':360,'multiline':False]
['text':' 3. Trace statelss version of the train_step','line_number':373,'multiline':False]
['text':' Pass named_states instead of opt.state to stateless_func, because','line_number':379,'multiline':False]
['text':' the later uses nn.Parameter as key. During tracing, we need to','line_number':380,'multiline':False]
['text':' make sure optimizers can find the states using proxy tensors.','line_number':381,'multiline':False]
['text':' opt.state's key type is string, but optimizer uses','line_number':384,'multiline':False]
['text':' Parameter as keys','line_number':385,'multiline':False]
['text':' type: ignore[index]','line_number':386,'multiline':False]
['text':' Lift states and parameters as function arguments so that make_fx','line_number':390,'multiline':False]
['text':' can trace operations applied to them.','line_number':391,'multiline':False]
['text':' For DataParallel mode, install hooks first to tag the gradients','line_number':398,'multiline':False]
['text':' make sure updated parameters are returned','line_number':402,'multiline':False]
['text':' type: ignore[union-attr]','line_number':403,'multiline':False]
['text':' FIXME: Using symbolic tracing to work around in DTensor expand mode.','line_number':405,'multiline':False]
['text':' Otherwise it hits shape mismatch error, as we use local inputs to','line_number':406,'multiline':False]
['text':' trace local graph and use DTensor to expand operators, where','line_number':407,'multiline':False]
['text':' DTensor's shape is the global shape.','line_number':408,'multiline':False]
['text':' since compilation happens in the first iteration and we','line_number':416,'multiline':False]
['text':' receives mini-batch input, convert them to full batch','line_number':417,'multiline':False]
['text':' fake tensor input first for data parallel sharding','line_number':418,'multiline':False]
['text':' propagations','line_number':419,'multiline':False]
['text':' expand the tensor to full batch size on its batch dim','line_number':422,'multiline':False]
['text':' FIXME(@mrshenli): functionalization does not work for our use','line_number':438,'multiline':False]
['text':' case yet. Use explicit decompositions for foreach ops.','line_number':439,'multiline':False]
['text':' Remove this when the following issue is addressed.','line_number':440,'multiline':False]
['text':' Issue: https://github.com/pytorch/pytorch/issues/97852','line_number':441,'multiline':False]
['text':' 4. parallel mode to expand a single device graph to a distributed graph','line_number':454,'multiline':False]
['text':' 5. Move the responsibility of flattening the input arguments from the','line_number':465,'multiline':False]
['text':' graph module to the caller. This serves two purposes:','line_number':466,'multiline':False]
['text':'   - Transformations that add/remove state need to manipulate a state','line_number':467,'multiline':False]
['text':'   container that maintains the state tensors in the same order as they','line_number':468,'multiline':False]
['text':'   appear in graph placeholders.','line_number':469,'multiline':False]
['text':'   - Reduced runtime cost. The state container is only flattened once upfront.','line_number':470,'multiline':False]
['text':' 6. dedup comm operators.','line_number':474,'multiline':False]
['text':' The duplication could come from DTensor args and kwargs redistribution.','line_number':475,'multiline':False]
['text':' Suppose one operator produces a Partial gradient tensor and model','line_number':476,'multiline':False]
['text':' parameters are replicated. In this case, every optimizer operation using','line_number':477,'multiline':False]
['text':' that Partial gradient tensor would trigger an allreduce. This is becuase','line_number':478,'multiline':False]
['text':' DTensor only has local information on individual tensor/operator, which is','line_number':479,'multiline':False]
['text':' not sufficient to detect duplications in the graph. This situation can','line_number':480,'multiline':False]
['text':' also happen when inserting FSDP allgather if a parameter is used multiple','line_number':481,'multiline':False]
['text':' times in the forward method.','line_number':482,'multiline':False]
['text':' TODO(@mrshenli): @yifuwang has a suggestion of conducting expansion and','line_number':483,'multiline':False]
['text':' dedup at tracer-level to avoid multiple graph passes.','line_number':484,'multiline':False]
['text':' 7. Replace previously inserted dummy ones with real graphs.','line_number':487,'multiline':False]
['text':' Note that the Python convention of __dict__ requires the key to be str.','line_number':495,'multiline':False]
['text':' TODO: ensure the key is unique.','line_number':496,'multiline':False]
['text':' Put the COMPILED_OBJECT_KEY in ``wrapper`` instead of ``func`` as','line_number':532,'multiline':False]
['text':' ``wrapper`` is the one that users will get.','line_number':533,'multiline':False]
['text':' N.B.: we don't need autograd as backward has already been','line_number':550,'multiline':False]
['text':' captured in the graph.','line_number':551,'multiline':False]
['text':' TODO: SPMD should provid a default and configurable','line_number':553,'multiline':False]
['text':' transformation.','line_number':554,'multiline':False]
['text':' This is the last train step. Call IterGraphModule.forward()','line_number':559,'multiline':False]
['text':' with the `last_iter` argument and catch the exception in','line_number':560,'multiline':False]
['text':' case the compiled_obj is not wrapped with IterGraphModule.','line_number':561,'multiline':False]
