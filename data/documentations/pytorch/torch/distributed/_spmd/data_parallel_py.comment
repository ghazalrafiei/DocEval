['text':' Dummy op used by data parallel to tag gradients.','line_number':35,'multiline':False]
['text':' NON_TENSOR is to tag non tensor node (i.e. graph output)','line_number':77,'multiline':False]
['text':' remove those hooks after tracing','line_number':127,'multiline':False]
['text':' NOTE: we use AVG by default, avg reduction is needed depending on','line_number':154,'multiline':False]
['text':' the loss function, for most loss function it should do','line_number':155,'multiline':False]
['text':' gradient averaging. There might be certain cases it should','line_number':156,'multiline':False]
['text':' not do gradient averaging (i.e. sum) but it's pretty rare.','line_number':157,'multiline':False]
['text':' TODO: Only NCCL supports AVG so using backend like Gloo would','line_number':158,'multiline':False]
['text':' crash, we should figure out a way to support avg reduction','line_number':159,'multiline':False]
['text':' for non-NCCL backend','line_number':160,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':161,'multiline':False]
['text':' first we backward propagate to mark the param gradients sharding','line_number':194,'multiline':False]
['text':' with tag_grad node helps and then delete the tag_grad nodes','line_number':195,'multiline':False]
['text':' find a param_grad node via the tagging','line_number':197,'multiline':False]
['text':' remove the tag_grad node from graph','line_number':207,'multiline':False]
['text':' early break if we have already processed all param_grads','line_number':212,'multiline':False]
['text':' next we forward propagate to mark all the sharding','line_number':215,'multiline':False]
['text':' NOTE: There're certain cases where the placeholder nodes do','line_number':219,'multiline':False]
['text':' not have real tensor values:','line_number':220,'multiline':False]
['text':' 1. optimizer states can be None sometimes, i.e. SGD with','line_number':221,'multiline':False]
['text':'    no momentum, optimizer states populate `momentum` state','line_number':222,'multiline':False]
['text':'    as None, the full graph we get from `compile` would have','line_number':223,'multiline':False]
['text':'    None as the placeholder value','line_number':224,'multiline':False]
['text':' 2. function args might not only contain params or activations,','line_number':225,'multiline':False]
['text':'    but also contain other non-tensor inputs, i.e. the model','line_number':226,'multiline':False]
['text':'    and optimizer instances baked in as a placeholder, there might','line_number':227,'multiline':False]
['text':'    also be some scalar argument which is not a tensor','line_number':228,'multiline':False]
['text':'','line_number':229,'multiline':False]
['text':' For the above cases, we create a NON_TENSOR stratgy so that we','line_number':230,'multiline':False]
['text':' know it's not a tensor and we don't need to shard it','line_number':231,'multiline':False]
['text':' during compilation there's an assumption that the first num_params','line_number':235,'multiline':False]
['text':' placeholders should be parameters','line_number':236,'multiline':False]
['text':' optimizer states follow the same strategy as','line_number':244,'multiline':False]
['text':' the corresponding parameters','line_number':245,'multiline':False]
['text':' find the first activation node and use its batch dim size','line_number':254,'multiline':False]
['text':' Annotate node types for the computation graph','line_number':265,'multiline':False]
['text':' Data Parallel node propagation logic:','line_number':266,'multiline':False]
['text':' param (non-compute) -> out: param','line_number':267,'multiline':False]
['text':' grad (non-compute before/after) -> out: grad','line_number':268,'multiline':False]
['text':' state -> output: state','line_number':269,'multiline':False]
['text':'','line_number':270,'multiline':False]
['text':' param + activation (param must be replicate, act be sharded) -> out: activation','line_number':271,'multiline':False]
['text':' param/state + grad (param/state/grad be the same spec) -> out: param/state','line_number':272,'multiline':False]
['text':' param + state -> out: param','line_number':273,'multiline':False]
['text':' At this point, we should have removed all the `tag_grad` nodes in the graph','line_number':276,'multiline':False]
['text':' for getitem call, just forward the strategy from the input','line_number':286,'multiline':False]
['text':' for tuple strategy, we need to get the child strategy from the tuple','line_number':289,'multiline':False]
['text':' if it's not a tuple strategy, we just forward the arg strategy','line_number':292,'multiline':False]
['text':' finished processing this non-compute node','line_number':325,'multiline':False]
['text':' for computatation nodes, we need to check all the inputs','line_number':328,'multiline':False]
['text':' found a param_grad node that already have output pre-filled spec','line_number':332,'multiline':False]
['text':' fill in the expected input specs for the pre-filled strategy','line_number':333,'multiline':False]
['text':' activation sharded','line_number':345,'multiline':False]
['text':' ops that need to build tuple strategy instead of normal strategy','line_number':355,'multiline':False]
['text':' This should happen rarely and only needed when we need to generate','line_number':356,'multiline':False]
['text':' different node strategy for multiple outputs (i.e. fused_adam op)','line_number':357,'multiline':False]
['text':' TODO: Currently this specializes to fused optimizer ops, but we need','line_number':358,'multiline':False]
['text':' to see how to generalize this strategy building logic','line_number':359,'multiline':False]
['text':' for list/tuple arg, use the first one to find out the node type','line_number':367,'multiline':False]
['text':' NOTE: This is the common region for all regular computation ops','line_number':387,'multiline':False]
['text':' param/state + grad, build up acceptable strategy','line_number':395,'multiline':False]
['text':' the strategy should be the same for all the inputs/outputs','line_number':396,'multiline':False]
['text':' TODO: optimizer parts should follow the dtensor prop logic','line_number':397,'multiline':False]
['text':' to support more general cases that allows optimizer states','line_number':398,'multiline':False]
['text':' to have different shardings compare to the params','line_number':399,'multiline':False]
['text':' either param + state or state + state','line_number':420,'multiline':False]
['text':' param + activation, build up acceptable strategy','line_number':434,'multiline':False]
['text':' param must be replicated, activation must be sharded','line_number':435,'multiline':False]
['text':' compute activation spec','line_number':441,'multiline':False]
['text':' param must be replicated','line_number':448,'multiline':False]
['text':' produce activation type sharding for output','line_number':456,'multiline':False]
['text':' If inputs only have parameters, the','line_number':467,'multiline':False]
['text':' strategy of this node should follow input','line_number':468,'multiline':False]
['text':' If input nodes does not have PARAM/GRAD/STATE, then','line_number':471,'multiline':False]
['text':' it should be a pure activation computation, it should','line_number':472,'multiline':False]
['text':' produce activation output.','line_number':473,'multiline':False]
['text':' Activations are usually sharded unless model creates','line_number':474,'multiline':False]
['text':' new tensors during computation, which depend on whether','line_number':475,'multiline':False]
['text':' the new tensor associate with a batch dim or not, it could','line_number':476,'multiline':False]
['text':' be shard/replicate/partial, batch dim analyzer should tell','line_number':477,'multiline':False]
['text':' us the correct sharding.','line_number':478,'multiline':False]
['text':' type: ignore[return-value]','line_number':499,'multiline':False]
['text':' set node sharding to None','line_number':519,'multiline':False]
['text':' set to replicate for replicate style','line_number':524,'multiline':False]
['text':' set to shard for fully shard style','line_number':527,'multiline':False]
['text':' only one strategy, use that instead','line_number':529,'multiline':False]
['text':' i.e. optimizer state steps can only be replicate','line_number':530,'multiline':False]
['text':' use the full sharding strategy','line_number':533,'multiline':False]
['text':' TODO: add support for default mode','line_number':536,'multiline':False]
['text':' default mode would generate either replicate or shard','line_number':537,'multiline':False]
['text':' mark activation as sharded on batch dim','line_number':541,'multiline':False]
['text':' For tuple strategy in the data parallel mode, it should have the same strategy','line_number':549,'multiline':False]
['text':' for all tuple elements, assert that then use the first element's strategy as sharding','line_number':550,'multiline':False]
['text':' set to replicate for replicate style','line_number':568,'multiline':False]
['text':' set to shard for fully shard style','line_number':571,'multiline':False]
['text':' If it's already a scalar tensor, it is already local, we don't','line_number':594,'multiline':False]
['text':' need to do anything','line_number':595,'multiline':False]
['text':' partition the graph to distributed','line_number':626,'multiline':False]
['text':' None sharding means this node don't need sharding','line_number':629,'multiline':False]
['text':' update node value','line_number':637,'multiline':False]
['text':' check if there's misaligned sharding, insert reshard if there is','line_number':642,'multiline':False]
['text':' insert reshard operation','line_number':658,'multiline':False]
['text':' for repeat op, we need to infer the repeat sizes','line_number':681,'multiline':False]
['text':' for view related op that needs shape, adjust shape to local shape if needed','line_number':704,'multiline':False]
['text':' convert output val to its local component','line_number':712,'multiline':False]
['text':' clean up the graph by removing sharding and partitioning related metadata','line_number':720,'multiline':False]
['text':' 1. First build up data parallel strategies for the whole graph','line_number':757,'multiline':False]
['text':' 2. Next we mark the data parallel strategy for each node base on','line_number':762,'multiline':False]
['text':'    the parallel_style','line_number':763,'multiline':False]
['text':' 3. Partition the single machine graph to the distribute graph','line_number':772,'multiline':False]
['text':' preserve node types for the expanded graph','line_number':775,'multiline':False]
['text':' if the nodes are expanded nodes (collectives), we mark them','line_number':786,'multiline':False]
['text':' the same type as the input node.','line_number':787,'multiline':False]
['text':' 4. Last, inplace partition the weights and optim states to','line_number':791,'multiline':False]
['text':'    DTensors base on the parallel style','line_number':792,'multiline':False]
['text':' update re-parameterized module param dict and optim states dict to DTensor','line_number':802,'multiline':False]
['text':' update module parameters to DTensor','line_number':804,'multiline':False]
['text':' update the optimizer state key and values to DTensor','line_number':807,'multiline':False]
['text':' shard/replicate non-scalar tensors, for scalar tensor, we','line_number':813,'multiline':False]
['text':' don't do anything','line_number':814,'multiline':False]
['text':' type: ignore[call-overload]','line_number':821,'multiline':False]
['text':' type: ignore[index]','line_number':822,'multiline':False]
