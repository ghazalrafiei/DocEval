['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' The key is the target pass and the value is the prerequisites of the pass.','line_number':44,'multiline':False]
['text':' The key is the target pass and the value is the passes that must applied before','line_number':46,'multiline':False]
['text':' the key.','line_number':47,'multiline':False]
['text':' TODO(@fegin): Support multiple runs of graph optimization','line_number':59,'multiline':False]
['text':' TODO(@fegin): With this design, circular imports will happen when a pass','line_number':60,'multiline':False]
['text':' developer accidentally create a pass dependency cycle. As a result, we need to','line_number':61,'multiline':False]
['text':' break this file into a finer granularity to avoid incorrect circular import.','line_number':62,'multiline':False]
['text':' We choose 5 to prevent some accidents that cause infinite loop. But','line_number':169,'multiline':False]
['text':' with functional collective, the distance is 1.','line_number':170,'multiline':False]
['text':' Identify all the outputs of this collective block.','line_number':201,'multiline':False]
['text':' TODO: populate all the tensor metadata and remove the default.','line_number':215,'multiline':False]
['text':' TODO: support symbolic shapes','line_number':218,'multiline':False]
['text':' TODO: fix the memory_format','line_number':242,'multiline':False]
['text':' type: ignore[arg-type]','line_number':263,'multiline':False]
['text':' TODO: fix these value','line_number':264,'multiline':False]
['text':' TODO(@fegin): support symbolic shapes','line_number':324,'multiline':False]
['text':' Scatter the split result.','line_number':329,'multiline':False]
['text':' Some users of the original allreduce and wait are scheduled','line_number':334,'multiline':False]
['text':' before the fused allreduce. We must move these users to a','line_number':335,'multiline':False]
['text':' correct topological sort order -- right after the last fused','line_number':336,'multiline':False]
['text':' allreduce result, the `last_split_reshape_node` variable.','line_number':337,'multiline':False]
['text':' Find the last input node.','line_number':370,'multiline':False]
['text':' If the input node is a clone, this is CommTensor based implementation.','line_number':376,'multiline':False]
['text':' Flatten all the inputs right after the last input is ready.','line_number':386,'multiline':False]
['text':' Create a new Comm node.','line_number':399,'multiline':False]
['text':' Create a new Wait node.','line_number':416,'multiline':False]
['text':' Move the fused_comm_node and its args to right after the source node','line_number':430,'multiline':False]
['text':' type: ignore[union-attr]','line_number':436,'multiline':False]
['text':' First ensure the allreduce are scheduled immediately right after the gradients.','line_number':475,'multiline':False]
['text':' Get the comm_blocks based on the new order.','line_number':477,'multiline':False]
['text':' TODO: determine the dtype','line_number':485,'multiline':False]
['text':' Find all the end users.','line_number':507,'multiline':False]
['text':' Find the earliest users.','line_number':515,'multiline':False]
['text':' Initialize the target_node to be the first user of the first output.','line_number':519,'multiline':False]
['text':' Move wait nodes and all the subsequent output nodes before the','line_number':528,'multiline':False]
['text':' earliest user.','line_number':529,'multiline':False]
['text':' We add all ancestors to the list and it is okay as not all of','line_number':581,'multiline':False]
['text':' them will be erased -- only those nodes with zero users will be','line_number':582,'multiline':False]
['text':' erased.','line_number':583,'multiline':False]
['text':' The args list of fused_adam function. We don't care about kwargs.','line_number':594,'multiline':False]
['text':' TODO(fegin): Have a template class for all Block class.','line_number':601,'multiline':False]
['text':' The output list of the copy nodes. The order follows the argument order.','line_number':606,'multiline':False]
['text':' TODO(fegin): populate/generate the max_exp_avg_sqs if exists','line_number':611,'multiline':False]
['text':' Iterate all the args and generate the corresponding output lists.','line_number':615,'multiline':False]
['text':' Assuming the corrsesponding output nodes are not created yet.','line_number':616,'multiline':False]
['text':' Do not generate gradient out list as it is not used.','line_number':633,'multiline':False]
['text':' Populate the existing output lists from the graph.','line_number':638,'multiline':False]
['text':' The output list of the copy nodes. The order follows the argument order.','line_number':686,'multiline':False]
['text':' Iterate all the args and generate the corresponding output lists','line_number':690,'multiline':False]
['text':' Assuming the corrsesponding output nodes are not created yet.','line_number':691,'multiline':False]
['text':' Populate the existing output lists from the graph.','line_number':702,'multiline':False]
['text':' Find the step (foreach_add)','line_number':738,'multiline':False]
['text':' The only hint we can use to split the optimizer is the order/indices.','line_number':796,'multiline':False]
['text':' Get the argument for idx-th gradient from orig_optim_args','line_number':803,'multiline':False]
['text':' Only add the argument to the list if the original argument list','line_number':805,'multiline':False]
['text':' is not empty. If the original argument list is empty, the new','line_number':806,'multiline':False]
['text':' one must be an empty list as well.','line_number':807,'multiline':False]
['text':' If argument order of step is the same as optimizer, nothing has to be','line_number':811,'multiline':False]
['text':' done. However, it is risky to rely on this assumption so we populate','line_number':812,'multiline':False]
['text':' the orig_step_indices.','line_number':813,'multiline':False]
['text':' Create the new step and optim nodes and blocks.','line_number':842,'multiline':False]
['text':' We have to create the new step node and block first because it is used','line_number':846,'multiline':False]
['text':' for the new optim node as the input.','line_number':847,'multiline':False]
['text':' Replace the original step output in the graph output node with','line_number':862,'multiline':False]
['text':' the new one.','line_number':863,'multiline':False]
['text':' Also need to replace the step output used for the new optimizer.','line_number':866,'multiline':False]
['text':' Insert the optimizer node after the first step output because its','line_number':873,'multiline':False]
['text':' topo sort order is the last.','line_number':874,'multiline':False]
['text':' Optimizer is used as the output of the train_step. Therefore, we have to','line_number':891,'multiline':False]
['text':' update the output node of the graph.','line_number':892,'multiline':False]
['text':' Remove the original copy_ nodes as they won't be DCE.','line_number':896,'multiline':False]
['text':' Call DCE once to get rid of the old optimizer. By doing so, we will be','line_number':903,'multiline':False]
['text':' able to erase the copy_ nodes of step later.','line_number':904,'multiline':False]
['text':' This is not required but calling this for consistency.','line_number':908,'multiline':False]
['text':' TODO(fegin): The API only support fused adam now. Should extend it to support','line_number':927,'multiline':False]
['text':' foreach as well.','line_number':928,'multiline':False]
