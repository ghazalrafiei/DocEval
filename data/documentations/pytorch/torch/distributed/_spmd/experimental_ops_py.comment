['text':' Copyright (c) Meta Platforms, Inc. and affiliates','line_number':1,'multiline':False]
['text':' pyre-ignore','line_number':18,'multiline':False]
['text':' pyre-ignore','line_number':21,'multiline':False]
['text':' FIXME(@mrshenli): for sqrt, this is only mathematically correct for','line_number':31,'multiline':False]
['text':' Replicate and Shard tensor.','line_number':32,'multiline':False]
['text':' pyre-ignore','line_number':36,'multiline':False]
['text':' If DTensorSpec for the two operand do not match, suggest using','line_number':58,'multiline':False]
['text':' self's DTensorSpec. This will trigger allreduce if other is partial','line_number':59,'multiline':False]
['text':' and self is replicated.','line_number':60,'multiline':False]
['text':' pyre-ignore','line_number':75,'multiline':False]
['text':' pyre-ignore','line_number':90,'multiline':False]
['text':' If DTensorSpec for the two operand do not match, suggest using','line_number':103,'multiline':False]
['text':' self's DTensorSpec. This will trigger allreduce if other is partial','line_number':104,'multiline':False]
['text':' and self is replicated.','line_number':105,'multiline':False]
['text':' pyre-ignore','line_number':122,'multiline':False]
['text':' pyre-ignore','line_number':131,'multiline':False]
['text':' type: ignore[assignment]','line_number':134,'multiline':False]
['text':' type: ignore[assignment]','line_number':141,'multiline':False]
['text':' type: ignore[assignment]','line_number':151,'multiline':False]
['text':' type: ignore[arg-type]','line_number':165,'multiline':False]
['text':' pyre-ignore','line_number':168,'multiline':False]
['text':' Self and target must match in placements, which should be shard along','line_number':174,'multiline':False]
['text':' batch dimension in data parallell use cases. Force redistribute.','line_number':175,'multiline':False]
['text':' need to create a new self instead return (target, target) as target','line_number':177,'multiline':False]
['text':' and self might not match in shape.','line_number':178,'multiline':False]
['text':' by default, nll_loss_forward conducts a reduction and returns','line_number':197,'multiline':False]
['text':' a scalar tensor, and hence the _Partial placements.','line_number':198,'multiline':False]
['text':' the 2nd output total_weight is always a scalar tensor','line_number':200,'multiline':False]
['text':' pyre-ignore','line_number':206,'multiline':False]
['text':' TODO: provide schema_suggestions when placements do not match','line_number':227,'multiline':False]
['text':' select will remove one dimension, decrement dim of Shard placements by 1','line_number':250,'multiline':False]
['text':' if they are larger than dim.','line_number':251,'multiline':False]
['text':' Using isinstance instead of is_shard so that mypy won't complain','line_number':254,'multiline':False]
['text':' about accessing dim attribute.','line_number':255,'multiline':False]
['text':' pyre-ignore','line_number':266,'multiline':False]
['text':' only the left-most (non-normalized) dimensions of the input can be sharded','line_number':277,'multiline':False]
['text':' pyre-ignore','line_number':290,'multiline':False]
['text':' ensure sharding on dim 0, which will trigger the "Partial" output on','line_number':310,'multiline':False]
['text':' weight and bias grads','line_number':311,'multiline':False]
['text':' NOTE: type errors below are legit. This is because DTensor currently','line_number':332,'multiline':False]
['text':' doesn't support Optional return values. Need to be fixed in DTensor repo.','line_number':333,'multiline':False]
['text':' consider the operating dimension as a singleton to prevent sharding on it','line_number':346,'multiline':False]
['text':' however, if active_dim is None, this means the input and output shapes are equal and','line_number':347,'multiline':False]
['text':' we'll apply exactly the pointwise rule.','line_number':348,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':355,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':356,'multiline':False]
['text':' type: ignore[arg-type]','line_number':371,'multiline':False]
['text':' pyre-ignore','line_number':385,'multiline':False]
['text':' 1. number of dimensions in input and src need to match.','line_number':387,'multiline':False]
['text':' 2. number of elements on all non-dim need to match between input and src.','line_number':388,'multiline':False]
['text':' 3. numer of elements in src in dim need to match the slice size.','line_number':389,'multiline':False]
['text':' Given the above:','line_number':390,'multiline':False]
['text':' - We suggest for src to follow the sharding of input, except on the scatter dimension,','line_number':391,'multiline':False]
['text':'   where our best bet for now is to make them replicated as a fall-back.','line_number':392,'multiline':False]
['text':'   TODO: Ideally we'd like to make sure the output is re-sharded afterwards to keep input sharding.','line_number':393,'multiline':False]
['text':' if the input shape and the output shape are the same on the operating dimension,','line_number':406,'multiline':False]
['text':' this is effectively a no-op, so we just propagate sharding as we would do for','line_number':407,'multiline':False]
['text':' pointwise, no exceptions.','line_number':408,'multiline':False]
['text':' type: ignore[operator]','line_number':411,'multiline':False]
['text':' apply sharding refinement as implemented in pointwise_rule','line_number':414,'multiline':False]
['text':' apply the exception -- disallow sharding on the operating dimension.','line_number':416,'multiline':False]
['text':' type: ignore[assignment]','line_number':420,'multiline':False]
['text':' if our sharding is correct, the output sharding will be the same as the input.','line_number':425,'multiline':False]
['text':' otherwise, return the suggestion.','line_number':433,'multiline':False]
