['text':' batch dim size is used to track the batch dim size of the input tensor','line_number':40,'multiline':False]
['text':' if batch dim already computed, simply return it','line_number':79,'multiline':False]
['text':' type: ignore[index]','line_number':83,'multiline':False]
['text':' update batch dim size before return','line_number':107,'multiline':False]
['text':' this is because batch dim size might change during the middle','line_number':108,'multiline':False]
['text':' if there's no hints from the output_dim_rules, we infer from output','line_number':112,'multiline':False]
['text':' shape to see if there's batch dim, and shard correspondingly','line_number':113,'multiline':False]
['text':' for reduction op that reduces over the sharded batch dim','line_number':120,'multiline':False]
['text':' we don't generate partial, but rather, we generate shard','line_number':121,'multiline':False]
['text':' This is because the intention of data parallel is to never','line_number':122,'multiline':False]
['text':' do full reduction across batch dimension, it would still','line_number':123,'multiline':False]
['text':' keep the reduction activation as sharded.','line_number':124,'multiline':False]
['text':' loop through the dim size to find the output batch dim','line_number':126,'multiline':False]
['text':' if there's no operands, it must be factory ops and it's a tensor','line_number':138,'multiline':False]
['text':' generated for computation and should be marked as replicated','line_number':139,'multiline':False]
['text':' -1 means replicated','line_number':141,'multiline':False]
['text':' if there's operand we see the operand have batch dim, if operand','line_number':144,'multiline':False]
['text':' have batch dim but output does not, it's either a full reduction,','line_number':145,'multiline':False]
['text':' where we should stay sharded, or it's a reduction on batch dim only','line_number':146,'multiline':False]
['text':' where we should produce partial','line_number':147,'multiline':False]
['text':' self.get_batch_dim(operands[0])','line_number':152,'multiline':False]
['text':' if operand does not have batch dim, we also don't have batch dim','line_number':154,'multiline':False]
['text':' if operand have batch dim but output does not, it should','line_number':161,'multiline':False]
['text':' produce partial, we use -2 to indicate partial','line_number':162,'multiline':False]
['text':' indicate this activation is replicated','line_number':170,'multiline':False]
['text':' indicate this activation is partial','line_number':173,'multiline':False]
['text':' indicate this activation is Shard','line_number':176,'multiline':False]
