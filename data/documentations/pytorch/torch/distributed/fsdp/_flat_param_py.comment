['text':' type: ignore[attr-defined]','line_number':39,'multiline':False]
['text':' Environment variable toggling whether to use unsafe `setattr()` for view','line_number':81,'multiline':False]
['text':' setting in `_use_sharded_views()` and `_use_unsharded_views()`','line_number':82,'multiline':False]
['text':' We should use 'safe' by default since it respects method overrides, but for','line_number':83,'multiline':False]
['text':' special cases such as for high CPU overhead or for intentionally bypassing','line_number':84,'multiline':False]
['text':' checks in the overrides, we may use 'unsafe'.','line_number':85,'multiline':False]
['text':' Environment variable toggling whether to check for parameter/gradient','line_number':88,'multiline':False]
['text':' writeback in case their storages change after FSDP initialization','line_number':89,'multiline':False]
['text':' We should check by default since it prevents silent correctness errors, but','line_number':90,'multiline':False]
['text':' since such changes are atypical, we may want to skip the check to save CPU','line_number':91,'multiline':False]
['text':' overhead, especially since the check happens in the pre-forward and','line_number':92,'multiline':False]
['text':' pre-backward each iteration.','line_number':93,'multiline':False]
['text':' Env var toggling whether when model is in .eval() mode, should we run in fp32','line_number':96,'multiline':False]
['text':' or the reduced precision.','line_number':97,'multiline':False]
['text':' Some value to set padding in tensors to for debuggability','line_number':100,'multiline':False]
['text':' Environment variables for disabling the all-gather and reduce-scatter','line_number':103,'multiline':False]
['text':' communication ops for ablation studies. Note that without these communication','line_number':104,'multiline':False]
['text':' ops the training won't converge, and you probably need to disable correctness','line_number':105,'multiline':False]
['text':' checks in your model.','line_number':106,'multiline':False]
['text':' TODO: Define this for now to avoid circular imports. See if we can remove.','line_number':111,'multiline':False]
['text':' unprefixed','line_number':133,'multiline':False]
['text':' unprefixed','line_number':148,'multiline':False]
['text':' unprefixed','line_number':151,'multiline':False]
['text':' Use to index into the sharded flat parameter, e.g.','line_number':160,'multiline':False]
['text':' `flat_param[offset_in_shard : offset_in_shard + numel_in_shard]`','line_number':161,'multiline':False]
['text':' Use to get part of the parameter in the local shard from a flattened','line_number':164,'multiline':False]
['text':' version of the unsharded parameter, e.g.','line_number':165,'multiline':False]
['text':' `param.flatten()[intra_param_start_idx : intra_param_end_idx + 1]`','line_number':166,'multiline':False]
['text':' inclusive','line_number':168,'multiline':False]
['text':' Make `isinstance(t, FlatParameter)` return True for custom tensor','line_number':194,'multiline':False]
['text':' instances that have the _is_flat_param flag for BC','line_number':195,'multiline':False]
['text':' NB: do NOT test the super implementation','line_number':197,'multiline':False]
['text':' Eager only','line_number':344,'multiline':False]
['text':' Compile only','line_number':346,'multiline':False]
['text':' type: ignore[call-arg]','line_number':360,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':361,'multiline':False]
['text':' NB: This is not a regular method, because FlatParameters are not actually','line_number':364,'multiline':False]
['text':' instances of this class (see __new__ above).  So you must indirectly','line_number':365,'multiline':False]
['text':' call this directly through the classmethod.','line_number':366,'multiline':False]
['text':' Mark the original parameters to avoid flattening them into','line_number':427,'multiline':False]
['text':' another `FlatParameter` during recursive construction','line_number':428,'multiline':False]
['text':' Tracks whether the `FlatParameter`'s post-backward hook has been','line_number':440,'multiline':False]
['text':' called to modify the behavior of the post-backward callback','line_number':441,'multiline':False]
['text':'#################','line_number':475,'multiline':False]
['text':' INITIALIZATION #','line_number':476,'multiline':False]
['text':'#################','line_number':477,'multiline':False]
['text':' Only align addresses for `use_orig_params=True` (for now)','line_number':529,'multiline':False]
['text':' For strategies that do not free after forward, we skip using sharded','line_number':548,'multiline':False]
['text':' views after forward since the unsharded data exists. We still switch','line_number':549,'multiline':False]
['text':' `self.flat_param` to point to the sharded flat parameter since what','line_number':550,'multiline':False]
['text':' it points to parameterizes behavior. We use the following attribute','line_number':551,'multiline':False]
['text':' to track which tensor data the parameters are unsharded views into.','line_number':552,'multiline':False]
['text':' The index in the state's `all_handles`, which must be the','line_number':554,'multiline':False]
['text':' same across ranks for the execution order validation to work','line_number':555,'multiline':False]
['text':' Index in handles_to_pre_forward_order','line_number':557,'multiline':False]
['text':' Index in `handles_post_forward_order`','line_number':559,'multiline':False]
['text':' Used for guarding against mistargeted forward prefetches','line_number':561,'multiline':False]
['text':' Used for guarding against mistargeted backward prefetches','line_number':563,'multiline':False]
['text':' Was the handle prefetched? Set on successful _prefetch_handle and unshard','line_number':565,'multiline':False]
['text':' Optimistically assume a valid input `params` and set dtype attributes','line_number':567,'multiline':False]
['text':' before `_init_flat_param()`, which performs the actual validation','line_number':568,'multiline':False]
['text':' mypy','line_number':571,'multiline':False]
['text':' type: ignore[arg-type]','line_number':579,'multiline':False]
['text':' For alignment padding, only `numels` gets strictly non-`None`','line_number':630,'multiline':False]
['text':' elements, and all other lists get `None` elements for padding.','line_number':631,'multiline':False]
['text':' shared reference','line_number':651,'multiline':False]
['text':' Pad to be divisible by world size to avoid a copy for the','line_number':715,'multiline':False]
['text':' post-backward reduce-scatter','line_number':716,'multiline':False]
['text':' Pass `aligned_numel=0` since we already included padding tensors','line_number':732,'multiline':False]
['text':' Return as the logical OR over each tensor's value','line_number':756,'multiline':False]
['text':' For `use_orig_params=True`, permit non-uniform `requires_grad`','line_number':759,'multiline':False]
['text':' Save whether these dtypes were specified so that we permit the','line_number':867,'multiline':False]
['text':' parameter dtype to change up until the lazy initialization','line_number':868,'multiline':False]
['text':' Special case: infer gradient reduction mixed precision','line_number':875,'multiline':False]
['text':'##################################','line_number':884,'multiline':False]
['text':' SHARD INITIALIZATION & METADATA #','line_number':885,'multiline':False]
['text':'##################################','line_number':886,'multiline':False]
['text':' type: ignore[call-overload]','line_number':911,'multiline':False]
['text':' inclusive','line_number':913,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':943,'multiline':False]
['text':' includes `numel_padded`','line_number':944,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':960,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':961,'multiline':False]
['text':' `unsharded_param_start_idx` and `unsharded_param_end_idx` are indices','line_number':980,'multiline':False]
['text':' into the unsharded flat parameter (inclusive) of the given parameter','line_number':981,'multiline':False]
['text':' This branch can only happen once since the rank's','line_number':996,'multiline':False]
['text':' unsharded start index can only intersect one parameter','line_number':997,'multiline':False]
['text':' This rank gets an empty chunk fully padded with zeros since there','line_number':1044,'multiline':False]
['text':' are not enough chunks across ranks','line_number':1045,'multiline':False]
['text':' inclusive','line_number':1099,'multiline':False]
['text':' Entering this branch means that the user changed the parameter','line_number':1157,'multiline':False]
['text':' dtype after FSDP initialization, in which case we may need to','line_number':1158,'multiline':False]
['text':' refresh some saved dtype attributes (dtypes specified as a part','line_number':1159,'multiline':False]
['text':' of mixed precision take precedence).','line_number':1160,'multiline':False]
['text':' For `reduce_dtype`, require `param_dtype` was not specified since','line_number':1163,'multiline':False]
['text':' then we infer the `reduce_dtype` from the specified `param_dtype`','line_number':1164,'multiline':False]
['text':' Pin the memory for faster H2D transfer','line_number':1182,'multiline':False]
['text':' Pre-allocate the sharded gradient on CPU to enable non-blocking','line_number':1184,'multiline':False]
['text':' D2H transfer during the backward pass','line_number':1185,'multiline':False]
['text':' For parameter mixed precision, we maintain a low precision','line_number':1190,'multiline':False]
['text':' sharded tensor on the compute device to be all-gathered (for','line_number':1191,'multiline':False]
['text':' sharded strategies) or directly used (for `NO_SHARD`) for','line_number':1192,'multiline':False]
['text':' computation.','line_number':1193,'multiline':False]
['text':' We maintain a padded unsharded tensor that serves as the','line_number':1201,'multiline':False]
['text':' all-gather destination and owns the original parameter storages.','line_number':1202,'multiline':False]
['text':' use low precision if parameter mixed precision is enabled','line_number':1207,'multiline':False]
['text':' For parameter mixed precision, we maintain a full precision','line_number':1218,'multiline':False]
['text':' padded unsharded tensor for when we force full precision.','line_number':1219,'multiline':False]
['text':' full precision','line_number':1223,'multiline':False]
['text':'##################','line_number':1227,'multiline':False]
['text':' UNSHARD/RESHARD #','line_number':1228,'multiline':False]
['text':'##################','line_number':1229,'multiline':False]
['text':' Since this path imposes special semantics for the unsharded flat','line_number':1242,'multiline':False]
['text':' parameter (e.g. forcing full precision), use sharded views to','line_number':1243,'multiline':False]
['text':' reuse the existing logic for that special handling','line_number':1244,'multiline':False]
['text':' no-op','line_number':1254,'multiline':False]
['text':' NOTE: This creates a new tensor distinct from any attributes.','line_number':1259,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1270,'multiline':False]
['text':' `copy_()` implicitly casts to the low precision','line_number':1272,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1273,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1274,'multiline':False]
['text':' Invariant: `_mp_shard` is always on the compute device.','line_number':1278,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1279,'multiline':False]
['text':' Even when not needing an unshard, we should switch to using','line_number':1294,'multiline':False]
['text':' the unsharded flat parameter','line_number':1295,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1331,'multiline':False]
['text':' When parameter mixed precision is enabled, we use a different','line_number':1343,'multiline':False]
['text':' tensor as the all-gather destination to preserve the invariant','line_number':1344,'multiline':False]
['text':' that  `_full_param_padded` is in the low precision','line_number':1345,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1346,'multiline':False]
['text':' For no-reshard-after-forward strategies, `_full_param_padded` may','line_number':1351,'multiline':False]
['text':' still be allocated from a previous forward. As we are forcing','line_number':1352,'multiline':False]
['text':' full precision here, the full-precision unsharded copy may be','line_number':1353,'multiline':False]
['text':' modified, invalidating the existing low-precision unsharded copy,','line_number':1354,'multiline':False]
['text':' so we should free it here to ensure a new all-gather for the next','line_number':1355,'multiline':False]
['text':' forward/backward computation to persist the modifications.','line_number':1356,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1360,'multiline':False]
['text':' HACK this should be handled by C10D','line_number':1389,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1390,'multiline':False]
['text':' In case of offloading, `flat_param.data` (i.e. sharded param) is','line_number':1403,'multiline':False]
['text':' created on the pre-unshard stream. We need to hand it over to the','line_number':1404,'multiline':False]
['text':' unshard stream for all-gather','line_number':1405,'multiline':False]
['text':' unshard_stream','line_number':1408,'multiline':False]
['text':' this `.view()` is not autograd visible','line_number':1426,'multiline':False]
['text':' This call corresponds to the complementary pre-backward','line_number':1431,'multiline':False]
['text':' `_use_unsharded_views()` to the skipped pre-forward','line_number':1432,'multiline':False]
['text':' `_use_sharded_views()`, so we should skip this one too.','line_number':1433,'multiline':False]
['text':' We use `Tensor` views in the forward so that they are tracked by','line_number':1435,'multiline':False]
['text':' autograd. We use them in the pre-backward as well to support','line_number':1436,'multiline':False]
['text':' reentrant activation checkpointing, which needs the views to be','line_number':1437,'multiline':False]
['text':' tracked by autograd in the backward pass's recomputed forward.','line_number':1438,'multiline':False]
['text':' `_mp_shard` is allocated in the pre-unshard stream, consumed in the','line_number':1458,'multiline':False]
['text':' unshard stream for sharded strategies, and consumed in both the','line_number':1459,'multiline':False]
['text':' unshard and default streams for `NO_SHARD`. For sharded strategies,','line_number':1460,'multiline':False]
['text':' the current stream here is the unshard stream, and for `NO_SHARD`,','line_number':1461,'multiline':False]
['text':' it is the default stream. For `NO_SHARD`, only recording for the','line_number':1462,'multiline':False]
['text':' default stream suffices since the default stream waits for the','line_number':1463,'multiline':False]
['text':' unshard stream.','line_number':1464,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1466,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1468,'multiline':False]
['text':' Check if all ranks have a `None` gradient','line_number':1490,'multiline':False]
['text':' type: ignore[assignment]','line_number':1495,'multiline':False]
['text':' In the case that only some ranks have `None` gradient, we use','line_number':1500,'multiline':False]
['text':' zeros to approximate as a best effort attempt','line_number':1501,'multiline':False]
['text':' type: ignore[assignment]','line_number':1508,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1509,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1512,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1513,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1515,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1533,'multiline':False]
['text':' grad on CPU','line_number':1551,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1562,'multiline':False]
['text':' TODO (awgu): Gradient accumulation outside `no_sync()`','line_number':1565,'multiline':False]
['text':' does not work with CPU offloading. The issue should be','line_number':1566,'multiline':False]
['text':' that, in the post-backward hook, we cannot do an addition','line_number':1567,'multiline':False]
['text':' between a CPU tensor (the existing sharded gradient) and','line_number':1568,'multiline':False]
['text':' a GPU tensor (the new sharded gradient).','line_number':1569,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1571,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1572,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1578,'multiline':False]
['text':' If user specified to keep the gradient in low precision, then','line_number':1579,'multiline':False]
['text':' the gradient may still be of the low precision dtype if the','line_number':1580,'multiline':False]
['text':' user did not set the gradient to `None` after the previous','line_number':1581,'multiline':False]
['text':' backward, in which case FSDP should cast back to the full','line_number':1582,'multiline':False]
['text':' precision dtype so that FSDP can accumulate in that dtype in','line_number':1583,'multiline':False]
['text':' the post-backward hook and assign to `.grad` in that dtype in','line_number':1584,'multiline':False]
['text':' the post-backward callback.','line_number':1585,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1586,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1593,'multiline':False]
['text':' TODO (rohan-varma): test for full precision with keep_low_precision_grads','line_number':1606,'multiline':False]
['text':' TODO (awgu): We should replace these conditional checks to encode','line_number':1615,'multiline':False]
['text':' the logical intention more directly.','line_number':1616,'multiline':False]
['text':' NOTE: This branch includes `NO_SHARD`.','line_number':1618,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1621,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1627,'multiline':False]
['text':' If no sharded gradient was computed this iteration, then there is','line_number':1628,'multiline':False]
['text':' no need to forward `_saved_grad_shard` to `grad`','line_number':1629,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1630,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1631,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1637,'multiline':False]
['text':' Delete `_saved_grad_shard` since its existence indicates a previous','line_number':1641,'multiline':False]
['text':' gradient to accumulate with in the post-backward hook','line_number':1642,'multiline':False]
['text':' Check that the unpadded unsharded flat parameter is a view into the','line_number':1666,'multiline':False]
['text':' padded unsharded flat parameter as expected','line_number':1667,'multiline':False]
['text':' NOTE: This check is not strictly needed for correctness but is a','line_number':1668,'multiline':False]
['text':' useful sanity check since the tensor should only be used internally.','line_number':1669,'multiline':False]
['text':' Copy from CPU to the compute device','line_number':1688,'multiline':False]
['text':' Switch to the sharded `FlatParameter` before freeing to prevent','line_number':1704,'multiline':False]
['text':' "use-after-free"-type bugs with external profiling tools, where for','line_number':1705,'multiline':False]
['text':' `use_orig_params=True`, the `param` does not point to valid memory','line_number':1706,'multiline':False]
['text':' when setting `param.data = ...` in `_use_sharded_views()`.','line_number':1707,'multiline':False]
['text':' For `NO_SHARD`, `_mp_shard` is not freed in the post-unshard since it','line_number':1723,'multiline':False]
['text':' is also the low precision *unsharded* flat parameter. Hence, we delay','line_number':1724,'multiline':False]
['text':' the free until the reshard.','line_number':1725,'multiline':False]
['text':' did not use the low precision shard','line_number':1729,'multiline':False]
['text':' Do not free the memory until all ops in the current stream finish','line_number':1745,'multiline':False]
['text':' Only incur the extra `.data` call if needed','line_number':1762,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1766,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1771,'multiline':False]
['text':' For the post-forward reshard, we may try to use sharded gradient','line_number':1777,'multiline':False]
['text':' views (or unsharded gradient views if a gradient was accumulated','line_number':1778,'multiline':False]
['text':' in `no_sync()`), but for the post-backward reshard, we delay the','line_number':1779,'multiline':False]
['text':' call to after the reduce-scatter.','line_number':1780,'multiline':False]
['text':' Skip using gradient views if skipped using sharded views','line_number':1783,'multiline':False]
['text':' since exposing unsharded parameters with sharded gradients','line_number':1784,'multiline':False]
['text':' may be confusing to the user','line_number':1785,'multiline':False]
['text':' TODO: Change `_unpadded_unsharded_size` if we change the','line_number':1788,'multiline':False]
['text':' gradient to be computed directly with padding.','line_number':1789,'multiline':False]
['text':'########','line_number':1800,'multiline':False]
['text':' VIEWS #','line_number':1801,'multiline':False]
['text':'########','line_number':1802,'multiline':False]
['text':' A `DTensor` `view` is not compatible with assigning','line_number':1889,'multiline':False]
['text':' `param.data = view`, so we cannot preserve the parameter','line_number':1890,'multiline':False]
['text':' variable.','line_number':1891,'multiline':False]
['text':' `as_params=False`','line_number':1907,'multiline':False]
['text':' Save the `Tensor` for the pre-backward','line_number':1911,'multiline':False]
['text':' save for pre-backward','line_number':1912,'multiline':False]
['text':' Use the saved `Tensor` variable from the forward to','line_number':1914,'multiline':False]
['text':' preserve the autograd graph so that the post-backward','line_number':1915,'multiline':False]
['text':' hook fires (e.g. for reentrant AC)','line_number':1916,'multiline':False]
['text':' Expects the gradient to be in `flat_param.grad`','line_number':1963,'multiline':False]
['text':' NOTE: This is a hack using `.data` to side step the check','line_number':1983,'multiline':False]
['text':' that parameter/gradient sizes/dtypes/devices match. From','line_number':1984,'multiline':False]
['text':' calling `reshard()`, `param` has the sharded size, has the','line_number':1985,'multiline':False]
['text':' full precision dtype, and if CPU offloading is enabled, is on','line_number':1986,'multiline':False]
['text':' CPU. Thus, one or more of the following cases can hold when','line_number':1987,'multiline':False]
['text':' in `no_sync()`, where `view` is the original parameter's','line_number':1988,'multiline':False]
['text':' gradient:','line_number':1989,'multiline':False]
['text':' 1. `view` can have the unsharded size.','line_number':1990,'multiline':False]
['text':' 2. `view` can have the parameter low precision dtype.','line_number':1991,'multiline':False]
['text':' 3. `view` can be on GPU.','line_number':1992,'multiline':False]
['text':' did not save FQN info in `_shared_param_infos`','line_number':2009,'multiline':False]
['text':' NOTE: This is the same hack to use `.data` to side step the','line_number':2017,'multiline':False]
['text':' size check.','line_number':2018,'multiline':False]
['text':' For `NO_SHARD`, use the *unflattened* unsharded views since we','line_number':2056,'multiline':False]
['text':' have the unsharded parameter','line_number':2057,'multiline':False]
['text':' Construct once and reuse for all parameters not in the local shard','line_number':2062,'multiline':False]
['text':' in case `flat_param` changed dtype','line_number':2065,'multiline':False]
['text':' Allow the original data to be freed via garbage collection','line_number':2074,'multiline':False]
['text':' could be both empty and non-empty','line_number':2089,'multiline':False]
['text':' Clear the saved `Tensor`s since they are unneeded now','line_number':2091,'multiline':False]
['text':' NOTE: This is a hack using `.data` to side step the','line_number':2129,'multiline':False]
['text':' check that parameter/gradient dtypes match. Here,','line_number':2130,'multiline':False]
['text':' `param` has full precision; `grad` has low precision.','line_number':2131,'multiline':False]
['text':' `.grad` must have the same shape as `param`','line_number':2133,'multiline':False]
['text':' share the same reference','line_number':2151,'multiline':False]
['text':' For `NO_SHARD`, we may still need to writeback','line_number':2176,'multiline':False]
['text':' NOTE: We must use the unsharded flat parameter from which the','line_number':2181,'multiline':False]
['text':' unsharded views were computed, not the one from the current','line_number':2182,'multiline':False]
['text':' calling context (`_get_padded_unsharded_flat_param()`) since that','line_number':2183,'multiline':False]
['text':' may be different (e.g. the model changed from train to eval).','line_number':2184,'multiline':False]
['text':' NOTE: Since this method is called in the pre-unshard, which is only','line_number':2195,'multiline':False]
['text':' called during computation in the pre-forward or pre-backward, the','line_number':2196,'multiline':False]
['text':' sharded gradient should be guaranteed to be in `.grad`, not in','line_number':2197,'multiline':False]
['text':' `._saved_grad_shard`.','line_number':2198,'multiline':False]
['text':' Do not writeback if original parameters are deregistered','line_number':2223,'multiline':False]
['text':' (e.g. during model checkpointing)','line_number':2224,'multiline':False]
['text':' Check for parameter writeback','line_number':2227,'multiline':False]
['text':' changed parameter variable itself','line_number':2236,'multiline':False]
['text':' changed `.data`','line_number':2239,'multiline':False]
['text':' NOTE: The gradient is not preserved after a parameter change.','line_number':2249,'multiline':False]
['text':' Check for gradient writeback','line_number':2259,'multiline':False]
['text':' Skip the writeback check because we do not expose gradients','line_number':2261,'multiline':False]
['text':' when we skipped using sharded views','line_number':2262,'multiline':False]
['text':' For `NO_SHARD` + CPU offloading, `_cpu_grad` is always in','line_number':2270,'multiline':False]
['text':' memory and owns the gradient storage, so it will never','line_number':2271,'multiline':False]
['text':' require gradient writeback.','line_number':2272,'multiline':False]
['text':' Explicitly continue to handle the case of `no_sync()`,','line_number':2274,'multiline':False]
['text':' where `param.grad` is a view into the GPU gradient','line_number':2275,'multiline':False]
['text':' referenced by `flat_param.grad`, while `flat_param_grad`','line_number':2276,'multiline':False]
['text':' is `flat_param._cpu_grad`, which is on CPU','line_number':2277,'multiline':False]
['text':' TODO: If we want to handle shared parameters, we need to re-generate','line_number':2302,'multiline':False]
['text':' the shared parameter data structures in case sharedness changed.','line_number':2303,'multiline':False]
['text':' else gradient','line_number':2325,'multiline':False]
['text':' NOTE: Gradient shape mismatch is not possible in practice since','line_number':2354,'multiline':False]
['text':' the gradient shape is enforced to match that of the parameter and','line_number':2355,'multiline':False]
['text':' we already check for parameter shape mismatch.','line_number':2356,'multiline':False]
['text':' mypy','line_number':2384,'multiline':False]
['text':' As long as one parameter requires gradient, then the flat parameter','line_number':2392,'multiline':False]
['text':' must require gradient','line_number':2393,'multiline':False]
['text':'##########','line_number':2405,'multiline':False]
['text':' HELPERS #','line_number':2406,'multiline':False]
['text':'##########','line_number':2407,'multiline':False]
['text':' Refresh the views because their storage may have changed','line_number':2412,'multiline':False]
['text':' `_sharded_size` is defined iff `handle.shard()` has been called','line_number':2434,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2436,'multiline':False]
['text':' type: ignore[misc]','line_number':2452,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2474,'multiline':False]
['text':' Priority for non-`None`: `_cpu_grad` > `_saved_grad_shard` > `grad`','line_number':2484,'multiline':False]
['text':' - CPU offloading: `_cpu_grad`','line_number':2485,'multiline':False]
['text':' - No CPU offloading + sharded strategies: `_saved_grad_shard`','line_number':2486,'multiline':False]
['text':' - No CPU offloading + `NO_SHARD`: `grad`','line_number':2487,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2490,'multiline':False]
['text':' In the post-backward hook, the sharded gradient is still in','line_number':2492,'multiline':False]
['text':' `_saved_grad_shard`.','line_number':2493,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2494,'multiline':False]
['text':' If in IDLE or in FORWARD states, then there may be an','line_number':2496,'multiline':False]
['text':' (accumulated) gradient. If accessed in IDLE, then this should','line_number':2497,'multiline':False]
['text':' be due to re-registering the original parameters (e.g. in state','line_number':2498,'multiline':False]
['text':' dict load).','line_number':2499,'multiline':False]
['text':' mypy','line_number':2527,'multiline':False]
['text':' type: ignore[arg-type]','line_number':2528,'multiline':False]
['text':' As long as the parameter requires gradient, it should receive a','line_number':2529,'multiline':False]
['text':' meaningful gradient (even if the gradient happens to be zeros)','line_number':2530,'multiline':False]
['text':' mypy','line_number':2532,'multiline':False]
['text':'######################','line_number':2535,'multiline':False]
['text':' CHECKS & INVARIANTS #','line_number':2536,'multiline':False]
['text':'######################','line_number':2537,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2575,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':2593,'multiline':False]
['text':'#############','line_number':2599,'multiline':False]
['text':' PROPERTIES #','line_number':2600,'multiline':False]
['text':'#############','line_number':2601,'multiline':False]
['text':' Also disable mixed precision in model eval mode, if configured','line_number':2621,'multiline':False]
['text':' NOTE: These are hacks to bypass `nn.Module.__setattr__` checks.','line_number':2637,'multiline':False]
['text':' This bypasses any overrides in case `module` is an instance of an','line_number':2642,'multiline':False]
['text':' `nn.Module` subclass','line_number':2643,'multiline':False]
['text':' This bypasses any overrides in case `module` is an instance of an','line_number':2649,'multiline':False]
['text':' `nn.Module` subclass','line_number':2650,'multiline':False]
['text':' Call `delattr()` and `setattr()` to go through `nn.Module` checks','line_number':2657,'multiline':False]
['text':' NOTE: This alignment constraint comes from TorchInductor.','line_number':2678,'multiline':False]
['text':' bytes','line_number':2679,'multiline':False]
['text':' NOTE: Set the padding value as a magic number for debuggability. The','line_number':2693,'multiline':False]
['text':' value itself should never be used in any user-facing computation.','line_number':2694,'multiline':False]
['text':' Use `lru_cache(1)` to only log the warning once (assuming the fixed warning','line_number':2703,'multiline':False]
['text':' messasge is passed in)','line_number':2704,'multiline':False]
['text':' Use `lru_cache(1)` to only log the warning once','line_number':2710,'multiline':False]
['text':' Use `lru_cache(1)` to only log the warning once','line_number':2716,'multiline':False]
