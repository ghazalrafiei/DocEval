['text':' Add module annotations for Dynamo support (see function for details)','line_number':439,'multiline':False]
['text':' Initializes self.process_group, along with rank and world size. This will','line_number':442,'multiline':False]
['text':' also set another attribute, _inter_node_pg, to control the process group','line_number':443,'multiline':False]
['text':' over which sharding occurs, if sharding_strategy is {HYBRID_SHARD, _HYBRID_SHARD_ZERO2}.','line_number':444,'multiline':False]
['text':' Note that this is done before auto_wrapping, so that child FSDP modules simply pick up','line_number':445,'multiline':False]
['text':' the same process group state as the root FSDP module.','line_number':446,'multiline':False]
['text':' Share root process groups with children to maintain','line_number':472,'multiline':False]
['text':' the invariant that all FSDP modules will have the same','line_number':473,'multiline':False]
['text':' process groups.','line_number':474,'multiline':False]
['text':' extension needs to be set before `_init_param_handle_from_module()`','line_number':501,'multiline':False]
['text':' `_state_dict_type` controls the `state_dict()` behavior, which is','line_number':515,'multiline':False]
['text':' implemented using post-save and pre-load hooks','line_number':516,'multiline':False]
['text':' FSDP's `.module` must refer to the innermost wrapped module when','line_number':523,'multiline':False]
['text':' composing with other module wrappers in order for state dict to work','line_number':524,'multiline':False]
['text':' defer to nn.Module's logic','line_number':541,'multiline':False]
['text':' type: ignore[operator]','line_number':548,'multiline':False]
['text':' Use `_unshard_params_recurse()` with `recurse=False` instead of','line_number':595,'multiline':False]
['text':' `_unshard_fsdp_state_params()` directly to perform lazy','line_number':596,'multiline':False]
['text':' initialization, which is needed to initialize `FlatParameter`','line_number':597,'multiline':False]
['text':' parameter attributes as required by the unshard logic','line_number':598,'multiline':False]
['text':' Reset lazy init called in `_unshard_params_recurse()` since `apply()`','line_number':610,'multiline':False]
['text':' may have been called on FSDP instance that is not truly a root, in','line_number':611,'multiline':False]
['text':' which case it will be incorrectly marked as one.','line_number':612,'multiline':False]
['text':' Use the default config if a state_dict config is not set.','line_number':695,'multiline':False]
['text':' Set the state_dict type and configurations.','line_number':713,'multiline':False]
['text':' When using the original parameters: Since (1) the `FlatParameter`s','line_number':952,'multiline':False]
['text':' own the storage and (2) `_apply()` is the subroutine underlying the','line_number':953,'multiline':False]
['text':' most common storage-changing ops like `to()` and `cuda()`, we','line_number':954,'multiline':False]
['text':' override `_apply()` to have the storage change directly performed on','line_number':955,'multiline':False]
['text':' the `FlatParameter`s instead of applying to the original parameters','line_number':956,'multiline':False]
['text':' and then writing back to the `FlatParameter`s.','line_number':957,'multiline':False]
['text':' Remove any instances of the FSDP-specific prefix; there can','line_number':979,'multiline':False]
['text':' be multiple in the case of nested FSDP modules','line_number':980,'multiline':False]
['text':' Remove any instances of the FSDP-specific prefix; there can','line_number':997,'multiline':False]
['text':' be multiple in the case of nested FSDP modules','line_number':998,'multiline':False]
['text':' Since assert can be turned off and this error checking','line_number':1004,'multiline':False]
['text':' is really important, we use explicit error checking','line_number':1005,'multiline':False]
['text':' and raise a ValueError if needed.','line_number':1006,'multiline':False]
['text':' In case we are failing in the context of autograd hook, asserting','line_number':1014,'multiline':False]
['text':' may not generate useful msg. So, let's print it to be sure.','line_number':1015,'multiline':False]
['text':' If every FSDP instance uses `NO_SHARD`, then we can directly use','line_number':1103,'multiline':False]
['text':' the normal `nn.utils` one targeting local gradients','line_number':1104,'multiline':False]
['text':' Otherwise, there exists some FSDP instance using a sharded strategy,','line_number':1112,'multiline':False]
['text':' where sharded and non-sharded parameters must be handled separately','line_number':1113,'multiline':False]
['text':' `NO_SHARD` or not FSDP-managed','line_number':1117,'multiline':False]
['text':' Compute local norms (forced to be in FP32)','line_number':1140,'multiline':False]
['text':' Reconstruct the total gradient norm depending on the norm type','line_number':1147,'multiline':False]
['text':' All-reducing the local non-sharded norm would count it an extra','line_number':1156,'multiline':False]
['text':' world-size-many times','line_number':1157,'multiline':False]
['text':' Multiplying by the clamped coefficient is meaningless when it is','line_number':1164,'multiline':False]
['text':' equal to 1, but it avoids the host-device sync that would result from','line_number':1165,'multiline':False]
['text':' `if clip_coef < 1`','line_number':1166,'multiline':False]
['text':' Use the "largest" dtype by type promotion semantics to use the same','line_number':1170,'multiline':False]
['text':' dtype as if we did not force local norm computation to be in FP32','line_number':1171,'multiline':False]
['text':' If this rank has no gradients, then we must default to FP32','line_number':1173,'multiline':False]
['text':' unless we use additional communication, which we prefer to avoid','line_number':1174,'multiline':False]
['text':' since `clip_grad_norm_()` is called in the training loop','line_number':1175,'multiline':False]
['text':' warn since this is generally unexpected','line_number':1180,'multiline':False]
['text':' Use the default behavior of `optim_input``','line_number':1199,'multiline':False]
['text':' Use the `optim_input` code path','line_number':1202,'multiline':False]
['text':' Use the `optim` code path','line_number':1204,'multiline':False]
['text':' alias','line_number':1681,'multiline':False]
['text':' Validate that the existing parameter keys are uniformly typed','line_number':1682,'multiline':False]
['text':' Return directly if the existing key type matches the target key type','line_number':1690,'multiline':False]
['text':' Otherwise, actually perform the re-keying','line_number':1699,'multiline':False]
['text':' ID -> name','line_number':1701,'multiline':False]
['text':' name -> ID','line_number':1724,'multiline':False]
['text':' Because not all model parameters may be passed as the optimizer','line_number':1731,'multiline':False]
['text':' input, we may need to drop some parameters from this mapping','line_number':1732,'multiline':False]
['text':' should never reach here','line_number':1751,'multiline':False]
['text':' Compute the gradient norm in FP32, where we treat the gradients as a','line_number':2015,'multiline':False]
['text':' single vector','line_number':2016,'multiline':False]
