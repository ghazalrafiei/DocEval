['text':' Validate src_rank and sharding_spec are same across all ranks.','line_number':54,'multiline':False]
['text':' type: ignore[index]','line_number':59,'multiline':False]
['text':' type: ignore[index]','line_number':61,'multiline':False]
['text':' type: ignore[index]','line_number':63,'multiline':False]
['text':' type: ignore[index]','line_number':65,'multiline':False]
['text':' Perform some validation first.','line_number':106,'multiline':False]
['text':' Replace param with ShardedTensor.','line_number':119,'multiline':False]
['text':' Tracks the current process group in the load context manager.','line_number':122,'multiline':False]
['text':' Squeeze the # of dimensions manually, only applicable to ChunkShardingSpec','line_number':195,'multiline':False]
['text':' type: ignore[attr-defined, arg-type]','line_number':198,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':200,'multiline':False]
['text':' record Sharder paths for sanity check on the plan to ensure items in the plan','line_number':235,'multiline':False]
['text':' does not conflict with the submodule tree that the Sharder is working with','line_number':236,'multiline':False]
['text':' shard the parameter according to the ShardingPlan','line_number':242,'multiline':False]
['text':' if found a sharding spec, try to shard the parameter','line_number':245,'multiline':False]
['text':' swap this submodule with the sharded module','line_number':270,'multiline':False]
['text':' reshard output if there's an entry in `reshard_output` for this module','line_number':275,'multiline':False]
['text':' convert the output back to data parallel for the modules appears in','line_number':283,'multiline':False]
['text':' `return_local_tensor` of the plan, we will call `_collect_local_shard`','line_number':284,'multiline':False]
['text':' to collect the local tensor for output of modules','line_number':285,'multiline':False]
