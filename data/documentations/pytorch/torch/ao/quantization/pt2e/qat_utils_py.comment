['text':' noqa: F401','line_number':13,'multiline':False]
['text':' type: ignore[var-annotated]','line_number':30,'multiline':False]
['text':' Example inputs for quantized and folded conv-bn1d patterns used in convert','line_number':33,'multiline':False]
['text':' x','line_number':35,'multiline':False]
['text':' conv_weight','line_number':36,'multiline':False]
['text':' bn_weight','line_number':37,'multiline':False]
['text':' bn_bias','line_number':38,'multiline':False]
['text':' bn_running_mean','line_number':39,'multiline':False]
['text':' bn_running_var','line_number':40,'multiline':False]
['text':' Example inputs for quantized and folded conv-bn2d patterns used in convert','line_number':43,'multiline':False]
['text':' x','line_number':45,'multiline':False]
['text':' conv_weight','line_number':46,'multiline':False]
['text':' bn_weight','line_number':47,'multiline':False]
['text':' bn_bias','line_number':48,'multiline':False]
['text':' bn_running_mean','line_number':49,'multiline':False]
['text':' bn_running_var','line_number':50,'multiline':False]
['text':' Per tensor quantization uses literals to represent scale and zero','line_number':63,'multiline':False]
['text':' point, so there is no need to include them here as kwargs','line_number':64,'multiline':False]
['text':' TODO: merge this with the `no_conv_bias` case','line_number':91,'multiline':False]
['text':' TODO: allow setting eps','line_number':107,'multiline':False]
['text':' Not used, only for matching convenience','line_number':128,'multiline':False]
['text':' TODO: allow setting eps','line_number':138,'multiline':False]
['text':' Dummy args to be passed into q-dq ops','line_number':161,'multiline':False]
['text':' TODO: allow setting eps','line_number':192,'multiline':False]
['text':' TODO: allow setting eps','line_number':236,'multiline':False]
['text':' type: ignore[name-defined]','line_number':261,'multiline':False]
['text':' type: ignore[name-defined]','line_number':275,'multiline':False]
['text':' Create the mapping from original node to replacement node','line_number':354,'multiline':False]
['text':' Extract conv input and weight','line_number':361,'multiline':False]
['text':' Note: here we extract the original nodes indirectly through the pattern nodes','line_number':362,'multiline':False]
['text':' because the args of the original nodes are no longer available after replacement','line_number':363,'multiline':False]
['text':' If conv weight is quantized, extract the q - dq nodes','line_number':374,'multiline':False]
['text':' Extract conv bias','line_number':386,'multiline':False]
['text':' If conv bias is quantized, extract the q - dq nodes','line_number':394,'multiline':False]
['text':' bias can be None','line_number':414,'multiline':False]
['text':' skip pattern placeholder nodes','line_number':417,'multiline':False]
['text':' TODO: this is error prone, use the replace_literals_with_placeholders hack instead','line_number':423,'multiline':False]
['text':' x, weight, bias, [stride, padding, dilation, transposed, output_padding, groups]','line_number':441,'multiline':False]
['text':' bias is optional, when it is not present, it means it is None','line_number':444,'multiline':False]
['text':' get the list of configs, it should be ordered as input, weight, bias','line_number':462,'multiline':False]
['text':' note: this is really hacky, we need a better solution, hopefully','line_number':463,'multiline':False]
['text':' in subgraph_rewriter, issue tracking the problem: https://github.com/pytorch/pytorch/issues/101820','line_number':464,'multiline':False]
['text':' input activation','line_number':466,'multiline':False]
['text':' weight','line_number':468,'multiline':False]
['text':' bias','line_number':470,'multiline':False]
['text':' Step (1): Replace patterns with conv bias','line_number':544,'multiline':False]
['text':'','line_number':545,'multiline':False]
['text':' Here we do replacement separately for cases with and without conv bias, since','line_number':546,'multiline':False]
['text':' the replacement patterns for these two cases are substantially different.','line_number':547,'multiline':False]
['text':' TODO: use the public replace_pattern API once it also returns replacement nodes','line_number':548,'multiline':False]
['text':' Step (2): Replace patterns without conv bias','line_number':565,'multiline':False]
['text':' Step (3): Post processing','line_number':582,'multiline':False]
['text':'','line_number':583,'multiline':False]
['text':' Due to limited functionality in the subgraph rewriter, here we manually','line_number':584,'multiline':False]
['text':' update the replacement graph as follows:','line_number':585,'multiline':False]
['text':'','line_number':586,'multiline':False]
['text':'   (a) Copy over metadata from original subgraph. This ensures the stack traces','line_number':587,'multiline':False]
['text':'       and annotations are preserved in the new subgraph','line_number':588,'multiline':False]
['text':'','line_number':589,'multiline':False]
['text':'   (b) Copy over literal args for conv from the original subgraph','line_number':590,'multiline':False]
['text':'       TODO: do this for literal args for batchnorm as well','line_number':591,'multiline':False]
['text':'','line_number':592,'multiline':False]
['text':'   (c) Update all references of the old nodes in the original subgraph to refer','line_number':593,'multiline':False]
['text':'       to the corresponding nodes in the new subgraph in the annotations','line_number':594,'multiline':False]
['text':'','line_number':595,'multiline':False]
['text':' In the future, we should try to push as much of this functionality into the','line_number':596,'multiline':False]
['text':' subgraph rewriter as possible, so we don't have to manually copy anything over.','line_number':597,'multiline':False]
['text':' For more detail, see https://github.com/pytorch/pytorch/issues/100419.','line_number':598,'multiline':False]
['text':' Step (3a): Copy over metadata for all nodes in [conv - bn - getitem]','line_number':603,'multiline':False]
['text':' Step (3b): Copy over conv literal args','line_number':606,'multiline':False]
['text':' Step (3c): Update old references in the conv node's input_qspec_map','line_number':608,'multiline':False]
['text':' Step (3c): Update old references in the special qspecs for all nodes in the graph','line_number':612,'multiline':False]
['text':' For quantize_per_tensor, scale and zp are literals and need to be copied','line_number':672,'multiline':False]
['text':' For quantize_per_channel, scale and zp are get_attr nodes and should be skipped','line_number':673,'multiline':False]
['text':' Args: input, [scale, zp, qmin, qmax, dtype]','line_number':679,'multiline':False]
['text':' Args: input, scale, zp, [axis, qmin, qmax, dtype]','line_number':685,'multiline':False]
['text':' Step (1): Replace QAT pattern with simple [conv - bn] pattern','line_number':714,'multiline':False]
['text':' is_per_channel','line_number':717,'multiline':False]
['text':' has_bias','line_number':718,'multiline':False]
['text':' bias_is_quantized','line_number':719,'multiline':False]
['text':' bn_is_training','line_number':720,'multiline':False]
['text':' For the cases without bias, `bias_is_quantized` is irrelevant, so here we arbitrarily','line_number':723,'multiline':False]
['text':' filter out one of the values for this flag to avoid having duplicate patterns','line_number':724,'multiline':False]
['text':' Step (2): Copy over metadata from original subgraph','line_number':750,'multiline':False]
['text':' Step (3): Copy over args for weight (and optionally bias) q - dq nodes','line_number':754,'multiline':False]
['text':' Step (4): Fold BN weights into conv','line_number':762,'multiline':False]
['text':' Copy over literal args for conv','line_number':771,'multiline':False]
