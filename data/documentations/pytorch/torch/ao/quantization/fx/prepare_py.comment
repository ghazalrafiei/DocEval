['text':' list of dtypes to not add observers to','line_number':131,'multiline':False]
['text':' note: the following default target dtype info dicts are temporary,','line_number':146,'multiline':False]
['text':' should be moved to the new programmable API class soon','line_number':147,'multiline':False]
['text':' type: ignore[assignment]','line_number':169,'multiline':False]
['text':' can't use asdict, so not calling get_observer_kwargs here','line_number':193,'multiline':False]
['text':' we will remove is_dynamic from QuantizationSpec because','line_number':218,'multiline':False]
['text':' it seems that dynamic range quantization','line_number':219,'multiline':False]
['text':' type: ignore[union-attr, assignment]','line_number':222,'multiline':False]
['text':' type: ignore[operator, union-attr]','line_number':223,'multiline':False]
['text':' need to insert placeholder observer for dynamic quantization so that it can','line_number':246,'multiline':False]
['text':' be converted to choose_qparams -> q -> dq in convert step','line_number':247,'multiline':False]
['text':' non dynamic quantization','line_number':255,'multiline':False]
['text':' lots of error checking are skipped here for now','line_number':259,'multiline':False]
['text':' TODO: instead of instantiating the instance, we can use inspect to get the default args','line_number':270,'multiline':False]
['text':' type: ignore[return-value]','line_number':274,'multiline':False]
['text':' TODO: support check for standalone module','line_number':292,'multiline':False]
['text':' TODO(future PR): remove the cast to bool below after figuring','line_number':300,'multiline':False]
['text':' out why backend_config has is_dynamic set to None in some cases.','line_number':301,'multiline':False]
['text':' TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well','line_number':308,'multiline':False]
['text':' bias','line_number':317,'multiline':False]
['text':' TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well','line_number':318,'multiline':False]
['text':' TODO: move dtype check into `_qconfig_satisfies_dtype_config_constraints` as well','line_number':333,'multiline':False]
['text':' TODO: we should check is_dynamic here as well, the code from _is_input_arg_dtype_supported_by_backend','line_number':335,'multiline':False]
['text':' from input activation check can be reused here','line_number':336,'multiline':False]
['text':' TODO: this is a hack because we can only specify one activation_obs_or_fq for','line_number':341,'multiline':False]
['text':' qconfig (qconfig.activation), and we are only supporting dynamically quantized','line_number':342,'multiline':False]
['text':' linear op which has fp32 output dtype, this should be removed if we generalize','line_number':343,'multiline':False]
['text':' the structure of qconfig in the future','line_number':344,'multiline':False]
['text':' check if arg dtype are supported','line_number':391,'multiline':False]
['text':' check if output dtype is supported','line_number':396,'multiline':False]
['text':' type: ignore[index]','line_number':416,'multiline':False]
['text':' name config has precedence over type config','line_number':417,'multiline':False]
['text':' fallback to use parent module's qconfig if user didn't specify qconfig dict','line_number':421,'multiline':False]
['text':' add obs_or_fq module as attribute','line_number':456,'multiline':False]
['text':' set target_dtype_info if matched_node_pattern is a Node','line_number':496,'multiline':False]
['text':' other types of matched object, e.g. int, float literals, are ignored','line_number':497,'multiline':False]
['text':' for pyre','line_number':499,'multiline':False]
['text':' TODO: refactor the following code in terms of apply a qconfig to a pattern','line_number':508,'multiline':False]
['text':' e.g. for a pattern with op1 -> op2 -> op3, and qconfig = QConfig(input_act=obs0, output_act=obs1)','line_number':509,'multiline':False]
['text':' we set the input_obs_or_fq_ctr for the arguments of op1 to based on qconfig.input_act,','line_number':510,'multiline':False]
['text':' and set output_obs_or_fq_ctr based on qconfig.output_act','line_number':511,'multiline':False]
['text':' this also requires we extend the structure of QConfig to support more fine','line_number':512,'multiline':False]
['text':' grained configurations','line_number':513,'multiline':False]
['text':' get qconfig to determine the eventual dtype of this node','line_number':566,'multiline':False]
['text':' Currently `QConfig` only has one `activation` field.','line_number':571,'multiline':False]
['text':' For static quantization, it is reused for both input','line_number':572,'multiline':False]
['text':' and output activation. For dynamic quantization, this','line_number':573,'multiline':False]
['text':' field is currently only used for the input activation,','line_number':574,'multiline':False]
['text':' with the output activation being in fp32.','line_number':575,'multiline':False]
['text':' In the future this may change as we add more fields','line_number':576,'multiline':False]
['text':' to the `QConfig` object.','line_number':577,'multiline':False]
['text':' Custom module LSTM output is a tuple that we broke down into the internal nodes in order','line_number':635,'multiline':False]
['text':' to insert DeQuantStubs (see `_insert_dequant_stubs_for_custom_module_lstm_output`).','line_number':636,'multiline':False]
['text':' Since we modified the graph in this case, we must trace back from the args through','line_number':637,'multiline':False]
['text':' the specific nodes we added in order to reach the original LSTM node. Otherwise, we would','line_number':638,'multiline':False]
['text':' not be able to accurately detect whether this node is a consumer of custom module LSTM.','line_number':639,'multiline':False]
['text':' "input_qspec_map" is the more general design we'll use for pt2e path','line_number':687,'multiline':False]
['text':' it is a map from input argument node to observer or fake quant constructor, for example','line_number':688,'multiline':False]
['text':' for the following graph:','line_number':689,'multiline':False]
['text':' x -> conv -> output','line_number':690,'multiline':False]
['text':'','line_number':691,'multiline':False]
['text':' we may annotate conv node like the following:','line_number':692,'multiline':False]
['text':' conv.meta[...] = QuantizationAnnotation("input_qspec_map": {x: MinMaxObserver.with_args(dtype=torch.qint8)}, ...)','line_number':693,'multiline':False]
['text':'','line_number':694,'multiline':False]
['text':' we can remove the following path in the future if fx graph mode quantization is','line_number':704,'multiline':False]
['text':' no longer used','line_number':705,'multiline':False]
['text':' for ops such as torch.cat([x0, x1]),','line_number':736,'multiline':False]
['text':' traverse through the list','line_number':737,'multiline':False]
['text':' default (no observer)','line_number':755,'multiline':False]
['text':' TODO: move this to a separate function','line_number':759,'multiline':False]
['text':' Note: qconfig can be None in this branch this we are getting act/fq from','line_number':761,'multiline':False]
['text':' node.meta now','line_number':762,'multiline':False]
['text':' regular flow for most nodes, except standalone modules','line_number':763,'multiline':False]
['text':' TODO: we are assuming "target_dtype_info" exists here, maybe','line_number':769,'multiline':False]
['text':' a default value also need to be provided here','line_number':770,'multiline':False]
['text':' for nodes that doesn't have `reuse_input_obs_or_fq` configured,','line_number':772,'multiline':False]
['text':' we'll default to False, this makes configuring this field optional for users','line_number':773,'multiline':False]
['text':' custom flow for standalone modules','line_number':793,'multiline':False]
['text':' for args, this is set to the index of the current arg','line_number':799,'multiline':False]
['text':' for kwargs, this is left at None','line_number':800,'multiline':False]
['text':' Before using the new observer, check if an observer','line_number':825,'multiline':False]
['text':' of the correct type already exists. If it does, use it.','line_number':826,'multiline':False]
['text':' This prevents duplicate observer insertions if a node is','line_number':827,'multiline':False]
['text':' used by multiple nodes.','line_number':828,'multiline':False]
['text':' TODO: this is looking into how the value is used in the future','line_number':829,'multiline':False]
['text':' we should remove this','line_number':830,'multiline':False]
['text':' removing this means we insert one observer for each use, even if they','line_number':831,'multiline':False]
['text':' have the same dtype, we can have an extra pass that removes the extra observers','line_number':832,'multiline':False]
['text':' type: ignore[index]','line_number':835,'multiline':False]
['text':' type: ignore[assignment]','line_number':840,'multiline':False]
['text':' override this arg to be the observed arg','line_number':849,'multiline':False]
['text':' Look through every input arg.  If that arg's target dtype does not','line_number':883,'multiline':False]
['text':' match the current node's target dtype, insert an observer.','line_number':884,'multiline':False]
['text':' assign the new args and kwargs to the node, inplace','line_number':907,'multiline':False]
['text':' assign the new args and kwargs to the node, inplace','line_number':951,'multiline':False]
['text':' uncomment after we support reuse_input_obs_or_fq properly by having separate','line_number':984,'multiline':False]
['text':' implemntations for this key instead of reusing the input_output_share_observers','line_number':985,'multiline':False]
['text':' code','line_number':986,'multiline':False]
['text':' reuse_input_obs_or_fq = node.meta["target_dtype_info"].get("reuse_input_obs_or_fq", False)','line_number':987,'multiline':False]
['text':' for now we set this to False since reuse_input_obs_or_fq for','line_number':988,'multiline':False]
['text':' the output of a node is implementation in the same code path as observer sharing,','line_number':989,'multiline':False]
['text':' we should refactor this part to make it clearer in the future','line_number':990,'multiline':False]
['text':' and we would be able to read this from config directly','line_number':991,'multiline':False]
['text':' Note: prev_output_dtype = torch.float and prev_output_is_dynamic=False','line_number':994,'multiline':False]
['text':' because the prev_output is the output of an fp32 op, althought technically','line_number':995,'multiline':False]
['text':' we should get the dtype of the output from node.meta["val"] in the future','line_number':996,'multiline':False]
['text':' if we deprecate fx graph mode quantization','line_number':997,'multiline':False]
['text':' currently the activation in QConfig(activation=...,) is for both input','line_number':999,'multiline':False]
['text':' and output, and when the activation is configured to be dynamic quantization','line_number':1000,'multiline':False]
['text':' e.g. PlaceholderObserver(dtype=torch.quint8, is_dynamic=True, ...), it means','line_number':1001,'multiline':False]
['text':' the input should by dynamically quantized, but output should not be quantized','line_number':1002,'multiline':False]
['text':'','line_number':1003,'multiline':False]
['text':' there is no way we can specify different observer/fq for input and output','line_number':1004,'multiline':False]
['text':' activation through QConfig today, this limitation is lifted in the','line_number':1005,'multiline':False]
['text':' quantizer/annotation API in pytorch 2.0 export quantization code path,','line_number':1006,'multiline':False]
['text':' but since this code is reused, annotating output to be dynamically quantized','line_number':1007,'multiline':False]
['text':' would not work either for that.','line_number':1008,'multiline':False]
['text':' we can change QConfig to support input/output activation if we want','line_number':1009,'multiline':False]
['text':' to remove the following check, or if we can deprecate fx graph mode quantization','line_number':1010,'multiline':False]
['text':' we never insert observers to output of standalone module, we assume','line_number':1014,'multiline':False]
['text':' if needed, they are inserted inside the standalone module','line_number':1015,'multiline':False]
['text':' check dtype of this node','line_number':1062,'multiline':False]
['text':' TODO: this does not handle dynamic quantization yet','line_number':1071,'multiline':False]
['text':' insert observer','line_number':1078,'multiline':False]
['text':' type: ignore[assignment]','line_number':1110,'multiline':False]
['text':' if this is a copy node, propagate to first arg','line_number':1125,'multiline':False]
['text':' TODO: probably need to remove `is_general_tensor_value_op`','line_number':1128,'multiline':False]
['text':' when an argument is a tuple, it does not show up as another node so we need to go through','line_number':1158,'multiline':False]
['text':' all elements of the tuple manually','line_number':1159,'multiline':False]
['text':' hard coded arguments show up but aren't `Node` typed and do not need dtype propagated','line_number':1166,'multiline':False]
['text':' find the first non-Tensor arg','line_number':1191,'multiline':False]
['text':' if there is no non-Tensor arg, return directly','line_number':1197,'multiline':False]
['text':' if we have a graph such as','line_number':1208,'multiline':False]
['text':'   observed_node -> non_observed_node -> cat','line_number':1209,'multiline':False]
['text':' we need to navigate up to the first observer','line_number':1210,'multiline':False]
['text':' did not find an activation_post_process for the op','line_number':1215,'multiline':False]
['text':' trace back the args until we found the first Tensor/Node','line_number':1218,'multiline':False]
['text':' set all other input observer nodes to use that module','line_number':1238,'multiline':False]
['text':' failed to trace back since no input arg for the current node','line_number':1244,'multiline':False]
['text':' set the output observer node to use that module','line_number':1255,'multiline':False]
['text':' TODO(future PR): delete the orphaned observer modules','line_number':1261,'multiline':False]
['text':' type: ignore[union-attr, operator]','line_number':1272,'multiline':False]
['text':' type: ignore[index]','line_number':1279,'multiline':False]
['text':' node.meta["target_dtype_info"] stores the target dtype information','line_number':1329,'multiline':False]
['text':' that's derived from qconfig for the Node, for example, if we have','line_number':1330,'multiline':False]
['text':' a conv2d node that has a qconfig','line_number':1331,'multiline':False]
['text':' qconfig = QConfig(activation=..., weight=...)','line_number':1332,'multiline':False]
['text':' # information for input and bias node omitted','line_number':1333,'multiline':False]
['text':' # for getattr node','line_number':1334,'multiline':False]
['text':' # weight = getattr(self, 'weight')','line_number':1335,'multiline':False]
['text':' weight.meta["target_dtype_info"] = {','line_number':1336,'multiline':False]
['text':'    'output_act_obs_or_fq_ctr': qconfig.weight,','line_number':1337,'multiline':False]
['text':' }','line_number':1338,'multiline':False]
['text':' # for conv2d node','line_number':1339,'multiline':False]
['text':' # conv2d = call_function[target=torch.nn.functional.conv2d](','line_number':1340,'multiline':False]
['text':' #            args=(input, weight, bias))','line_number':1341,'multiline':False]
['text':' conv2d.meta["target_dtype_info"] = {','line_number':1342,'multiline':False]
['text':'   'input_act_obs_or_fq_ctr': qconfig.activation','line_number':1343,'multiline':False]
['text':'   'weight_obs_or_fq_ctr': qconfig.weight,','line_number':1344,'multiline':False]
['text':'   'bias_obs_or_fq_ctr': PlaceholderObserver.with_args(dtype=torch.float32),','line_number':1345,'multiline':False]
['text':'   'output_act_obs_or_fq_ctr': qconfig.activation,','line_number':1346,'multiline':False]
['text':' }','line_number':1347,'multiline':False]
['text':'','line_number':1348,'multiline':False]
['text':' first, populate the dtype map based only on qconfig and qhandler','line_number':1351,'multiline':False]
['text':' this assumes:','line_number':1352,'multiline':False]
['text':' graph inputs are fp32 by default, and int8 where overriden','line_number':1353,'multiline':False]
['text':' other nodes output dtype is specified by the qconfig','line_number':1354,'multiline':False]
['text':' initialize target_dtype_info','line_number':1360,'multiline':False]
['text':' TODO: we probably don't need this counter since each graph will only have','line_number':1367,'multiline':False]
['text':' one output node?','line_number':1368,'multiline':False]
['text':' Step 1, set the observer or fake quantize module constructor for each node in the','line_number':1378,'multiline':False]
['text':' matched_node_pattern','line_number':1379,'multiline':False]
['text':' Step 2. Special cases for some operators, we might be able to remove them','line_number':1395,'multiline':False]
['text':' in the future if we know dtype information of each node better','line_number':1396,'multiline':False]
['text':' Step 2.1. some settings are not based on patterns, we need to process each node','line_number':1398,'multiline':False]
['text':' instead','line_number':1399,'multiline':False]
['text':' users are not supposed to call calculate_qparams on PlaceholderObserver, and','line_number':1402,'multiline':False]
['text':' this is OK because we are using this as a way to encode the dtypes of input','line_number':1403,'multiline':False]
['text':' tensor, we won't actually insert these observers in the graph and won't','line_number':1404,'multiline':False]
['text':' actually call calculate_qparams','line_number':1405,'multiline':False]
['text':' TODO(future PR): update the output_quantized_idxs API to match','line_number':1417,'multiline':False]
['text':' arbitrary data structures. There is always a single output, and','line_number':1418,'multiline':False]
['text':' that output can have arbitrary nesting of values. List[int] is','line_number':1419,'multiline':False]
['text':' not the right data type for this.','line_number':1420,'multiline':False]
['text':' TODO(future PR): support more dtypes in model outputs, if necessary','line_number':1422,'multiline':False]
['text':' Step 2.2, for nodes with known input dtypes, propagate them throughout the','line_number':1425,'multiline':False]
['text':' graph. For example, if there is a call such as','line_number':1426,'multiline':False]
['text':'   x1 = x0.masked_fill(mask, 1)','line_number':1427,'multiline':False]
['text':' we propagate the type of mask to be torch.bool','line_number':1428,'multiline':False]
['text':' Step 3, check if the requested target_dtype_info is supported by backend or not','line_number':1431,'multiline':False]
['text':' if not, we'll reset the target_dtye_info to use the default (float Tensor)','line_number':1432,'multiline':False]
['text':' reset the counters and set of processed_nodes','line_number':1434,'multiline':False]
['text':' get output_act_dtype so that we don't also reset the special typed nodes','line_number':1442,'multiline':False]
['text':' TODO: we might want to handle these more uniformly with the default path','line_number':1443,'multiline':False]
['text':' this can be improved if we can use node.meta["val"]','line_number':1444,'multiline':False]
['text':' restore target_dtype_info to default if it is not supported by backend','line_number':1449,'multiline':False]
['text':' After this point, the current node and all of its arguments','line_number':1461,'multiline':False]
['text':' have a target_dtype_info assigned. Now, we insert observers for inputs','line_number':1462,'multiline':False]
['text':' of this node (if needed for this node), and the output of this node','line_number':1463,'multiline':False]
['text':' (if needed for this node).','line_number':1464,'multiline':False]
['text':' Since we are mutating the graph as we go, we iterate over the original','line_number':1466,'multiline':False]
['text':' nodes before observer insertion, instead of model.graph.nodes.','line_number':1467,'multiline':False]
['text':' Avoid duplicates custom module swaps for multiple nodes with same target.','line_number':1470,'multiline':False]
['text':' TODO: reuse placeholder_node_to_input_index and output_node_to_output_index','line_number':1473,'multiline':False]
['text':' reset inputs/outputs counters','line_number':1474,'multiline':False]
['text':' TODO: change this to insert obs/fq by pattern instead of by node','line_number':1480,'multiline':False]
['text':' if a graph input is in fp32, it does not need observation','line_number':1484,'multiline':False]
['text':' if a graph input is in int8, we assume the observation happens','line_number':1485,'multiline':False]
['text':'   outside of the graph, and no additional observation is needed','line_number':1486,'multiline':False]
['text':' check for matches','line_number':1490,'multiline':False]
['text':' type: ignore[assignment]','line_number':1492,'multiline':False]
['text':' TODO: take a closer look to see if we can remove this check','line_number':1512,'multiline':False]
['text':' right now it is here because of `observed_node_names`, we are using','line_number':1513,'multiline':False]
['text':' it as an indicator for swapping the modules to reference modules in','line_number':1514,'multiline':False]
['text':' convert','line_number':1515,'multiline':False]
['text':' add matched nodes to the observed node name set','line_number':1523,'multiline':False]
['text':' This is currently only used for equalization.','line_number':1526,'multiline':False]
['text':' Checks if the current node is in a branch in which the two','line_number':1527,'multiline':False]
['text':' first layers are both being quantized.','line_number':1528,'multiline':False]
['text':'','line_number':1529,'multiline':False]
['text':' ex.       conv2','line_number':1530,'multiline':False]
['text':'         /','line_number':1531,'multiline':False]
['text':'      x -> conv1','line_number':1532,'multiline':False]
['text':'','line_number':1533,'multiline':False]
['text':' If this is the case, we will not apply equalization to the','line_number':1534,'multiline':False]
['text':' initial two layers.','line_number':1535,'multiline':False]
['text':' Checks if there exists another user being quantized','line_number':1543,'multiline':False]
['text':' this modifies node inplace','line_number':1556,'multiline':False]
['text':' insert equalization input observers if needed','line_number':1565,'multiline':False]
['text':' Currently custom module outputs are assumed to be already quantized,','line_number':1576,'multiline':False]
['text':' so we need to insert a DeQuantStub after the output. For custom module','line_number':1577,'multiline':False]
['text':' LSTM specifically, the outputs are also a nested tuple, so we must first','line_number':1578,'multiline':False]
['text':' break down the tuple to insert DeQuantStubs after the internal nodes.','line_number':1579,'multiline':False]
['text':' TODO: This currently diverges from how custom modules are handled today,','line_number':1581,'multiline':False]
['text':' where we insert observers after the output instead of DeQuantStubs, and','line_number':1582,'multiline':False]
['text':' replace these observers with "dequantize" nodes during convert. Conceptually,','line_number':1583,'multiline':False]
['text':' these output observers are the same as DeQuantStubs. In the future, we','line_number':1584,'multiline':False]
['text':' should resolve this inconsistency by inserting DeQuantStubs for all custom','line_number':1585,'multiline':False]
['text':' modules, not just for LSTM.','line_number':1586,'multiline':False]
['text':' this returns the new observer node if it was needed','line_number':1592,'multiline':False]
['text':' Update users of original node to use the output observer','line_number':1597,'multiline':False]
['text':' instead. For example, change','line_number':1598,'multiline':False]
['text':'','line_number':1599,'multiline':False]
['text':'           next_node','line_number':1600,'multiline':False]
['text':'          /','line_number':1601,'multiline':False]
['text':'   cur_node -> obs','line_number':1602,'multiline':False]
['text':'','line_number':1603,'multiline':False]
['text':' to','line_number':1604,'multiline':False]
['text':'','line_number':1605,'multiline':False]
['text':'                 next_node','line_number':1606,'multiline':False]
['text':'                 /','line_number':1607,'multiline':False]
['text':'   cur_node -> obs','line_number':1608,'multiline':False]
['text':'','line_number':1609,'multiline':False]
['text':' We need to save orig users before updating uses because','line_number':1610,'multiline':False]
['text':' the list of users will change as we update uses','line_number':1611,'multiline':False]
['text':' for ops whose inputs and outputs share observer/fqs, we modify the graph','line_number':1621,'multiline':False]
['text':' to make all inputs and outputs use the first input's','line_number':1622,'multiline':False]
['text':' observer/fq','line_number':1623,'multiline':False]
['text':' output','line_number':1634,'multiline':False]
['text':'','line_number':1637,'multiline':False]
['text':' After this point, the current node has input and output observers','line_number':1638,'multiline':False]
['text':' that it needs for itself inserted.','line_number':1639,'multiline':False]
['text':'','line_number':1640,'multiline':False]
['text':' increment the counters, so future inputs and outputs are assigned','line_number':1642,'multiline':False]
['text':' correct dtypes','line_number':1643,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':1677,'multiline':False]
['text':' mapping from a tuple of nodes in reverse order to uninitialized','line_number':1779,'multiline':False]
['text':'   QuantizeHandler subclass. For example,','line_number':1780,'multiline':False]
['text':' {','line_number':1781,'multiline':False]
['text':'   # match a single node','line_number':1782,'multiline':False]
['text':'   (<class 'torch.nn.modules.conv.Conv3d'>:','line_number':1783,'multiline':False]
['text':'     <class 'torch.ao.quantization.fx.quantize.ConvRelu'>),','line_number':1784,'multiline':False]
['text':'   # match multiple nodes in reverse order','line_number':1785,'multiline':False]
['text':'   ((<function relu at 0x7f766a7360d0>, <built-in function add>):','line_number':1786,'multiline':False]
['text':'     <class 'torch.ao.quantization.fx.quantize.Add'>),','line_number':1787,'multiline':False]
['text':' }','line_number':1788,'multiline':False]
['text':' TODO: support regex as well','line_number':1802,'multiline':False]
['text':' mapping from fully qualified module name to module instance','line_number':1810,'multiline':False]
['text':' for example,','line_number':1811,'multiline':False]
['text':' {','line_number':1812,'multiline':False]
['text':'   '': Model(...),','line_number':1813,'multiline':False]
['text':'   'linear': Linear(...),','line_number':1814,'multiline':False]
['text':'   'linear.weight_fake_quant': PerChannelMinMaxObserver(...),','line_number':1815,'multiline':False]
['text':' }','line_number':1816,'multiline':False]
['text':' fill node_name_to_qconfig, a map from node name to qconfig, used in _find_matches','line_number':1819,'multiline':False]
['text':' match the patterns that will get quantized','line_number':1824,'multiline':False]
['text':' map qconfig instances to matches','line_number':1833,'multiline':False]
['text':' record names for the set of observed node, so that in convert step','line_number':1842,'multiline':False]
['text':' we know whether we need to convert a floating point module to reference','line_number':1843,'multiline':False]
['text':' quantized module or not','line_number':1844,'multiline':False]
['text':' these inputs are observed in parent','line_number':1868,'multiline':False]
['text':' converting List[int] to Tensor since module attribute is','line_number':1869,'multiline':False]
['text':' Union[Tensor, Module]','line_number':1870,'multiline':False]
['text':' inplace modification','line_number':1874,'multiline':False]
