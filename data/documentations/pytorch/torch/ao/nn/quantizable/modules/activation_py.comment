['text':' this is needed to avoid a circular import','line_number':2,'multiline':False]
['text':' for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969','line_number':76,'multiline':False]
['text':' type: ignore[assignment]','line_number':77,'multiline':False]
['text':' Functionals','line_number':79,'multiline':False]
['text':' note: importing torch.ao.nn.quantized at top creates a circular import','line_number':81,'multiline':False]
['text':' Quant/Dequant','line_number':83,'multiline':False]
['text':' Setting the dropout to 0.0!','line_number':97,'multiline':False]
['text':' Set the linear weights','line_number':107,'multiline':False]
['text':' for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969','line_number':108,'multiline':False]
['text':' type: ignore[has-type]','line_number':109,'multiline':False]
['text':' type: ignore[has-type]','line_number':110,'multiline':False]
['text':' Use separate params','line_number':112,'multiline':False]
['text':' type: ignore[assignment]','line_number':146,'multiline':False]
['text':' type: ignore[assignment]','line_number':147,'multiline':False]
['text':' type: ignore[assignment]','line_number':148,'multiline':False]
['text':' Explicit prepare','line_number':154,'multiline':False]
['text':' Set the linear weights','line_number':176,'multiline':False]
['text':' Note: Because the linear layers are quantized, mypy does not nkow how','line_number':177,'multiline':False]
['text':' to deal with them -- might need to ignore the typing checks.','line_number':178,'multiline':False]
['text':' for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969','line_number':179,'multiline':False]
['text':' type: ignore[operator, has-type]','line_number':180,'multiline':False]
['text':' type: ignore[operator]','line_number':185,'multiline':False]
['text':' type: ignore[operator]','line_number':187,'multiline':False]
['text':' type: ignore[operator]','line_number':189,'multiline':False]
['text':' Use separate params','line_number':192,'multiline':False]
['text':' The whole flow is float -> observed -> quantized','line_number':230,'multiline':False]
['text':' This class does float -> observed only','line_number':231,'multiline':False]
['text':' See nn.quantized.MultiheadAttention','line_number':232,'multiline':False]
['text':' This version will not deal with the static key/value pairs.','line_number':305,'multiline':False]
['text':' Keeping it here for future changes.','line_number':306,'multiline':False]
['text':'','line_number':307,'multiline':False]
['text':' TODO: This method has some duplicate lines with the','line_number':308,'multiline':False]
['text':' `torch.nn.functional.multi_head_attention`. Will need to refactor.','line_number':309,'multiline':False]
['text':' allow MHA to have different sizes for the feature dimension','line_number':324,'multiline':False]
['text':' attn_mask's dim is 3 now.','line_number':353,'multiline':False]
['text':' convert ByteTensor key_padding_mask to bool','line_number':355,'multiline':False]
['text':' Explicitly assert that bias_k and bias_v are not None','line_number':362,'multiline':False]
['text':' in a way that TorchScript can understand.','line_number':363,'multiline':False]
['text':' Leaving the quantized zone here','line_number':420,'multiline':False]
['text':' Reentering the quantized zone','line_number':452,'multiline':False]
['text':' for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969','line_number':454,'multiline':False]
['text':' type: ignore[has-type]','line_number':455,'multiline':False]
['text':' average attention weights over heads','line_number':459,'multiline':False]
