['text':' type: ignore[operator]','line_number':28,'multiline':False]
['text':' type: ignore[index]','line_number':31,'multiline':False]
['text':' type: ignore[operator]','line_number':34,'multiline':False]
['text':' type: ignore[arg-type]','line_number':38,'multiline':False]
['text':' type: ignore[index]','line_number':40,'multiline':False]
['text':' type: ignore[union-attr]','line_number':46,'multiline':False]
['text':' type: ignore[operator]','line_number':61,'multiline':False]
['text':' type: ignore[operator]','line_number':69,'multiline':False]
['text':' TODO(future PR): make more generic, handle everything','line_number':72,'multiline':False]
['text':' traverse backwards from the weight arg, accounting for any observers','line_number':89,'multiline':False]
['text':' type: ignore[arg-type]','line_number':95,'multiline':False]
['text':' qconv state is arg 1','line_number':99,'multiline':False]
['text':' type: ignore[arg-type]','line_number':103,'multiline':False]
['text':' traverse backwards from the weight arg, accounting for any observers','line_number':107,'multiline':False]
['text':' supported patterns:','line_number':108,'multiline':False]
['text':' weight -> obs -> linear','line_number':109,'multiline':False]
['text':' weight -> to(torch.float16) -> dequantize -> linear','line_number':110,'multiline':False]
['text':' weight -> obs -> linear','line_number':115,'multiline':False]
['text':' type: ignore[arg-type]','line_number':121,'multiline':False]
['text':' weight -> to(torch.float16) -> dequantize -> linear','line_number':124,'multiline':False]
['text':' extract the dtype, so we can cast to it before returning','line_number':130,'multiline':False]
['text':' type: ignore[arg-type]','line_number':135,'multiline':False]
['text':' return the weight with fp16 cast','line_number':136,'multiline':False]
['text':' type: ignore[arg-type]','line_number':140,'multiline':False]
['text':' packed weight is arg 1','line_number':144,'multiline':False]
['text':' type: ignore[arg-type]','line_number':148,'multiline':False]
['text':' TODO(future PR): why does packed_weight.unpack() not work?','line_number':149,'multiline':False]
['text':' Conv1d','line_number':157,'multiline':False]
['text':' Conv2d','line_number':166,'multiline':False]
['text':' Conv3d','line_number':175,'multiline':False]
['text':' Linear','line_number':184,'multiline':False]
['text':' LSTM','line_number':194,'multiline':False]
['text':' Conv','line_number':199,'multiline':False]
['text':' Linear','line_number':209,'multiline':False]
['text':' Not all graphmodules have _node_name_to_scope, so only fill it','line_number':225,'multiline':False]
['text':' out if it exists.','line_number':226,'multiline':False]
['text':' type: ignore[index]','line_number':229,'multiline':False]
['text':' for extracting weights, these are always the same','line_number':235,'multiline':False]
['text':' for call_module, we need to look up the modules to do the type check','line_number':256,'multiline':False]
