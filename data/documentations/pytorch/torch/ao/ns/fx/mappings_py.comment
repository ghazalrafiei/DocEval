['text':' note: this set is modified below by items from backend_config','line_number':27,'multiline':False]
['text':' conv modules','line_number':29,'multiline':False]
['text':' conv functionals','line_number':39,'multiline':False]
['text':' linear modules','line_number':49,'multiline':False]
['text':' linear functionals','line_number':53,'multiline':False]
['text':' average pool','line_number':57,'multiline':False]
['text':' adaptive average pool','line_number':70,'multiline':False]
['text':' LSTM','line_number':83,'multiline':False]
['text':' add','line_number':87,'multiline':False]
['text':' x + y','line_number':90,'multiline':False]
['text':' cat','line_number':92,'multiline':False]
['text':' mul','line_number':96,'multiline':False]
['text':' relu','line_number':101,'multiline':False]
['text':' maxpool','line_number':109,'multiline':False]
['text':' sigmoid','line_number':122,'multiline':False]
['text':' BatchNorm','line_number':130,'multiline':False]
['text':' ConvTranspose','line_number':137,'multiline':False]
['text':' functional transposed conv','line_number':147,'multiline':False]
['text':' ELU','line_number':157,'multiline':False]
['text':' Embedding','line_number':161,'multiline':False]
['text':' EmbeddingBag','line_number':165,'multiline':False]
['text':' GroupNorm','line_number':169,'multiline':False]
['text':' Hardswish','line_number':173,'multiline':False]
['text':' InstanceNorm','line_number':177,'multiline':False]
['text':' LayerNorm','line_number':187,'multiline':False]
['text':' LeakyReLU','line_number':191,'multiline':False]
['text':' ReLU6','line_number':195,'multiline':False]
['text':' F.elu','line_number':200,'multiline':False]
['text':' F.hardswish','line_number':204,'multiline':False]
['text':' F.group_norm','line_number':208,'multiline':False]
['text':' F.instance_norm','line_number':212,'multiline':False]
['text':' F.layer_norm','line_number':216,'multiline':False]
['text':' F.leaky_relu','line_number':220,'multiline':False]
['text':' F.silu','line_number':224,'multiline':False]
['text':' F.mish','line_number':229,'multiline':False]
['text':' F.tanh','line_number':234,'multiline':False]
['text':' F.hardsigmoid','line_number':242,'multiline':False]
['text':' F.hardtanh','line_number':249,'multiline':False]
['text':' floordiv','line_number':255,'multiline':False]
['text':' unsqueeze','line_number':259,'multiline':False]
['text':' stack','line_number':263,'multiline':False]
['text':' squeeze','line_number':267,'multiline':False]
['text':' sort','line_number':271,'multiline':False]
['text':' repeat_interleave','line_number':275,'multiline':False]
['text':' min','line_number':279,'multiline':False]
['text':' mean','line_number':283,'multiline':False]
['text':' max','line_number':287,'multiline':False]
['text':' transpose','line_number':291,'multiline':False]
['text':' flatten','line_number':295,'multiline':False]
['text':' clamp','line_number':299,'multiline':False]
['text':' chunk','line_number':303,'multiline':False]
['text':' interpolate','line_number':307,'multiline':False]
['text':' dropout','line_number':311,'multiline':False]
['text':' F.dropout','line_number':315,'multiline':False]
['text':' matmul','line_number':319,'multiline':False]
['text':' Softmax','line_number':323,'multiline':False]
['text':' PReLU','line_number':327,'multiline':False]
['text':' F.prelu','line_number':332,'multiline':False]
['text':' pixel shuffle','line_number':337,'multiline':False]
['text':' pixel unshuffle','line_number':344,'multiline':False]
['text':' narrow','line_number':351,'multiline':False]
['text':' for each floating point op, add versions of the op added by','line_number':357,'multiline':False]
['text':' backend_config','line_number':358,'multiline':False]
['text':' technical debt edge case','line_number':362,'multiline':False]
['text':' pattern format: (c, (b, a))','line_number':368,'multiline':False]
['text':' look from the end, because pattern is in reverse order','line_number':370,'multiline':False]
['text':' case 1: pattern fuses a pattern of ops into an op','line_number':375,'multiline':False]
['text':' example: nn.Conv1d, nn.ReLU fused into nni.ConvReLU1d','line_number':376,'multiline':False]
['text':' case 2: pattern swaps a module into a QAT module','line_number':380,'multiline':False]
['text':' example: nni.ConvReLU1d swapped into nniqat.ConvReLU1d','line_number':381,'multiline':False]
['text':' case 3: reference version of floating point module, such as','line_number':385,'multiline':False]
['text':' nn.Conv2d and nnqr.Conv2d','line_number':386,'multiline':False]
['text':'','line_number':389,'multiline':False]
['text':' Add reference module swaps from default lowering path','line_number':390,'multiline':False]
['text':'','line_number':391,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':399,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':407,'multiline':False]
['text':'','line_number':411,'multiline':False]
['text':' Add function swaps from default lowering path','line_number':412,'multiline':False]
['text':'','line_number':413,'multiline':False]
['text':'','line_number':428,'multiline':False]
['text':' Add other swaps, ideally in the future this could be removed','line_number':429,'multiline':False]
['text':' after the lowering code stops using these.','line_number':430,'multiline':False]
['text':'','line_number':431,'multiline':False]
['text':' add the new connections from backend_config','line_number':439,'multiline':False]
['text':' if we got here, related_op was not found','line_number':478,'multiline':False]
['text':' TODO(future PR): clean this up','line_number':487,'multiline':False]
['text':' TODO(future PR): implement shadowing for binary ops and','line_number':530,'multiline':False]
['text':' uncomment below','line_number':531,'multiline':False]
['text':' toq.add,','line_number':532,'multiline':False]
['text':' toq.mul,','line_number':533,'multiline':False]
['text':' note: nnqd.Linear is an instance of nnq.Linear, so this','line_number':592,'multiline':False]
['text':' check has to happen before the int8 module check','line_number':593,'multiline':False]
