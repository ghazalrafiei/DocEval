['text':' see if any of the module tensors have a parametriztion attached that matches the one passed in','line_number':19,'multiline':False]
['text':' type: ignore[union-attr,operator]','line_number':22,'multiline':False]
['text':' TODO Fix this typing, as Type[Module] has no attribute "from_dense"','line_number':40,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':41,'multiline':False]
['text':' Preserve module's pre forward hooks. They'll be called on quantized input','line_number':43,'multiline':False]
['text':' Preserve module's post forward hooks except _observer_forward_hook','line_number':46,'multiline':False]
['text':' After convert they'll work with quantized output','line_number':47,'multiline':False]
['text':' respect device affinity when swapping modules','line_number':51,'multiline':False]
['text':' string manip to split tensor_fqn into module_fqn and tensor_name','line_number':96,'multiline':False]
['text':' if tensor_fqn is 'weight' then module_fqn and tensor_name are '' and 'weight'','line_number':97,'multiline':False]
['text':' if tensor_fqn is 'linear.weight' then module_fqn and tensor_name are 'linear' and 'weight'','line_number':98,'multiline':False]
['text':' Parametrizations','line_number':112,'multiline':False]
['text':' We don't want to let the parametrizations to save the mask.','line_number':133,'multiline':False]
['text':' That way we make sure that the linear module doesn't store the masks','line_number':134,'multiline':False]
['text':' alongside their parametrizations.','line_number':135,'multiline':False]
