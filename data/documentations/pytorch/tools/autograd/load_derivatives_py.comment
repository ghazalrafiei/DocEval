['text':' Parses derivatives.yaml into autograd functions','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Each autograd function is represented by `DifferentiabilityInfo` containing','line_number':3,'multiline':False]
['text':' a list of `Derivative`. See `torchgen.api.autograd` for the data models.','line_number':4,'multiline':False]
['text':' This function directly adds per-dispatchkey derivative entries for {view}_copy variants of each view op.','line_number':57,'multiline':False]
['text':' Since every {view} and {view}_copy op shares the same derivative formula,','line_number':58,'multiline':False]
['text':' we generate them here instead of duplicating them in the yaml.','line_number':59,'multiline':False]
['text':' See Note [Codegen'd {view}_copy Operators]','line_number':60,'multiline':False]
['text':' Get the map from each view op's name to its corresponding view group','line_number':65,'multiline':False]
['text':' maybe_view_group only needs to be calculated once per info_dispatch_dict','line_number':73,'multiline':False]
['text':' Do some caching as this is a deterministic function','line_number':97,'multiline':False]
['text':' From the parsed native functions, separate out the (generated) view_copy functions,','line_number':105,'multiline':False]
['text':' so we can generate derivatives for them separately.','line_number':106,'multiline':False]
['text':' We need to pull out the view_inplace ops too, since they might have their own derivative entries.','line_number':109,'multiline':False]
['text':' What's the difference between function schema v.s. signature?','line_number':121,'multiline':False]
['text':' function schema is the complete declaration including mutability annotation / default value and etc.','line_number':122,'multiline':False]
['text':' signature is the canonical schema for a group of functions (in-place/out/functional variants)','line_number':123,'multiline':False]
['text':' that are semantically related.','line_number':124,'multiline':False]
['text':' Keep track of how many of which ops we've seen so we can','line_number':134,'multiline':False]
['text':' disambiguate them with a numeric suffix.','line_number':135,'multiline':False]
['text':' infos is a dict that maps FunctionSchema -> a dict of per dispatch key DifferentiabilityInfos','line_number':138,'multiline':False]
['text':' this is useful because in tools/autograd/gen_autograd.py:match_differentiability_info','line_number':139,'multiline':False]
['text':' we ultimately need to categorize the DifferentiabilityInfos by FunctionSchema','line_number':140,'multiline':False]
['text':' Ensure that the old derivatives.yaml schema with no dispatch key can be loaded.','line_number':144,'multiline':False]
['text':' cache both loaded infos as well a a set of all the dispatch_keys/aliases','line_number':164,'multiline':False]
['text':' that appear in derivatives.yaml. used_dispatch_keys is useful for generating','line_number':165,'multiline':False]
['text':' VariableType.cpp where we need a TORCH_LIBRARY_IMPL for every autograd dispatch key used','line_number':166,'multiline':False]
['text':' TODO: Why is this going through CppSignatureGroup, that doesn't make sense...','line_number':172,'multiline':False]
['text':' Check that the referenced derivatives in the formula are in bounds','line_number':211,'multiline':False]
['text':' Handle default return names','line_number':240,'multiline':False]
['text':' The functions taking TensorList handle everything internally','line_number':282,'multiline':False]
['text':' This transformation is based on the observation that for element-wise functions, the Jacobian','line_number':326,'multiline':False]
['text':' matrix is diagonal and thus doing J * v is the same as (v^T J)^T (in practice, we ignore the transpositions)','line_number':327,'multiline':False]
['text':' For the complex case, we use hermitian transpose and get (v.conj() J).conj()','line_number':328,'multiline':False]
['text':' So here we are going to re-use the backward formula and replace two things:','line_number':329,'multiline':False]
['text':' 1) all occurrences of "grad" with "foo_t.conj()", where foo is the name of the unique differentiable input.','line_number':330,'multiline':False]
['text':' 2) all usage of an original input "foo" with its primal value "foo_p".','line_number':331,'multiline':False]
['text':' 3) conjugate the final result','line_number':332,'multiline':False]
['text':' For example, for abs, the backward formula is:','line_number':333,'multiline':False]
['text':'   grad * self.sgn()','line_number':334,'multiline':False]
['text':' And this function generates a forward formula that is:','line_number':335,'multiline':False]
['text':'   (self_t.conj() * self_p.sgn()).conj()','line_number':336,'multiline':False]
['text':' Do replacement 1) of the grad','line_number':341,'multiline':False]
['text':' Do replacement 2) of the input variables','line_number':347,'multiline':False]
['text':' Do the final conjugate 3)','line_number':356,'multiline':False]
['text':' Since there is a single differentiable inputs and we necessarily need its tangent we can','line_number':359,'multiline':False]
['text':' simply require all differentiable input's tangent.','line_number':360,'multiline':False]
['text':' This transformation is based on the observation that linear functions can be written as:','line_number':373,'multiline':False]
['text':'   y = f(x) = A * x','line_number':374,'multiline':False]
['text':' For some matrix A and the Jacobian of the function f is also A.','line_number':375,'multiline':False]
['text':' So doing J * v = A * v = f(v).','line_number':376,'multiline':False]
['text':' Hence to do the jvp, we simply need to evaluate the function at the point v instead of x.','line_number':377,'multiline':False]
['text':' We do this by calling the forward again by replacing any occurrence of the differentiable','line_number':378,'multiline':False]
['text':' input "foo" by it's tangent "foo_t".','line_number':379,'multiline':False]
['text':' Note that multiple inputs are not a problem as long as the function is truly linear wrt to','line_number':380,'multiline':False]
['text':' the vector where all the differentiable inputs are stacked.','line_number':381,'multiline':False]
['text':' Do replacement of input variables','line_number':386,'multiline':False]
['text':' TODO we are trolling','line_number':393,'multiline':False]
['text':' Call into the forward again. We need two cases here to handle both Tensor methods and at:: functions.','line_number':397,'multiline':False]
['text':' All of the input tangents are always used so all of them are required here.','line_number':404,'multiline':False]
['text':' At this point, the formula is final and is not modified anymore.','line_number':408,'multiline':False]
['text':' During forward formula, we use the primal instead of the input Tensors.','line_number':410,'multiline':False]
['text':' This call inspects the formula to find for which input's primal are used.','line_number':411,'multiline':False]
['text':' some functions only have in-place variants','line_number':459,'multiline':False]
['text':' true if any derivative uses "grad"','line_number':474,'multiline':False]
['text':' count of uses of "grads" or "grads[INDEX]"','line_number':475,'multiline':False]
['text':' true if any derivative uses "grad_{name}"','line_number':476,'multiline':False]
['text':' which indices of grads are used','line_number':477,'multiline':False]
['text':' This is a basic sanity check: the number of places we see','line_number':486,'multiline':False]
['text':' "grads" should be no fewer than the number of indices we see','line_number':487,'multiline':False]
['text':' inside "grads". They may not be equal because we may use','line_number':488,'multiline':False]
['text':' "grads" without an index.','line_number':489,'multiline':False]
['text':' Thus if the number is equal, every use of grads is also','line_number':491,'multiline':False]
['text':' indexed.','line_number':492,'multiline':False]
['text':' Set up the derivative information','line_number':528,'multiline':False]
['text':' only used for the assert below','line_number':537,'multiline':False]
['text':' output_differentiability is captured from the enclosed','line_number':538,'multiline':False]
['text':' scope. Don't modify it.','line_number':539,'multiline':False]
['text':'','line_number':540,'multiline':False]
['text':' If it is not present, then no output is explicitly','line_number':541,'multiline':False]
['text':' undifferentiable.','line_number':542,'multiline':False]
['text':'','line_number':543,'multiline':False]
['text':' It may be present and shorter than the length of return','line_number':544,'multiline':False]
['text':' values. If that's the case, any return value that does not','line_number':545,'multiline':False]
['text':' have a corresponding entry is considered not differentiable.','line_number':546,'multiline':False]
['text':' A return is available as a named gradient ...','line_number':548,'multiline':False]
['text':' if it has not been explicitly made undifferentiable','line_number':552,'multiline':False]
['text':' and if it has a name','line_number':554,'multiline':False]
['text':' and if its type is differentiable','line_number':556,'multiline':False]
['text':' Next, let us determine the list of inputs in order.','line_number':589,'multiline':False]
['text':' TODO: do we need eagerly calculate and save it here? Can it be derived','line_number':590,'multiline':False]
['text':' from NativeFunction and `derivatives` on callsites instead?','line_number':591,'multiline':False]
['text':' Postprocess forward derivatives definitions now that we know the differentiable arguments','line_number':596,'multiline':False]
['text':' Test to see if the use of 'grads' makes sense.','line_number':606,'multiline':False]
['text':' NB: Removes 'name' from defn dictionary','line_number':617,'multiline':False]
['text':' NB: Removes 'output_differentiability' from defn dictionary','line_number':620,'multiline':False]
['text':'     `None` means all differentiable.','line_number':621,'multiline':False]
['text':' now map this to the legacy schema; this isn't technically necessary, but we'd need some logic here','line_number':649,'multiline':False]
['text':' to map in-place schemas to the out-of-place variants.','line_number':650,'multiline':False]
['text':' TODO: maybe the logic to handle the legacy schema is no longer necessary?','line_number':651,'multiline':False]
['text':' only assign an op name if we are actually going to calculate a derivative','line_number':703,'multiline':False]
['text':' replace self.sym_sizes() with self_sym_sizes','line_number':761,'multiline':False]
['text':' replace self->sym_sizes() with self_sym_sizes_opt','line_number':769,'multiline':False]
['text':' replace self.sym_blocksize() with self_sym_blocksize_opt','line_number':780,'multiline':False]
['text':' replace self.options() with self_options','line_number':791,'multiline':False]
['text':' replace zeros_like(self) with self_info','line_number':799,'multiline':False]
['text':' at save-time','line_number':805,'multiline':False]
['text':' at eval-time','line_number':806,'multiline':False]
['text':' replace self.sym_size(2) with self_sym_size_2','line_number':809,'multiline':False]
['text':' replace self.numel() with self_numel','line_number':817,'multiline':False]
['text':' replace self.sym_numel() with self_sym_numel','line_number':825,'multiline':False]
['text':' replace to_args_sizes(self) with self_args_sizes','line_number':833,'multiline':False]
['text':' replace to_args_sizes_symint(self) with self_args_sizes','line_number':843,'multiline':False]
['text':' replace to_args_scalartypes(self) with self_args_scalartypes','line_number':853,'multiline':False]
['text':' replace TensorGeometry(self) with self_geometry','line_number':863,'multiline':False]
['text':' replace self.dim() with self_dim','line_number':878,'multiline':False]
['text':' replace self.sym_strides() with self_sym_strides','line_number':886,'multiline':False]
['text':' replace self.layout() with self_layout','line_number':895,'multiline':False]
['text':' replace self.is_conj() with self_conjugate','line_number':903,'multiline':False]
['text':' find which arguments need to be saved','line_number':913,'multiline':False]
['text':' First search the formula for expressions which can be evaluated','line_number':937,'multiline':False]
['text':' when the autograd Function is created to avoid saving variables','line_number':938,'multiline':False]
['text':' c10::optional<std::string> types stored in Backward nodes must be','line_number':959,'multiline':False]
['text':' converted to c10::optional<c10::string_view> before being passed into','line_number':960,'multiline':False]
['text':' the backward function','line_number':961,'multiline':False]
['text':' Find any variables which remain in the formula and save them','line_number':969,'multiline':False]
