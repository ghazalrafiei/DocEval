['text':' Tensors are omitted (as they are stored in TensorIterator), everything else is','line_number':37,'multiline':False]
['text':' passed along  (technically, we can pass tensors along too, it just wastes','line_number':38,'multiline':False]
['text':' argument registers)','line_number':39,'multiline':False]
['text':'','line_number':40,'multiline':False]
['text':' NB: used for CPU only','line_number':41,'multiline':False]
['text':' Dispatch stubs are always plain ints','line_number':43,'multiline':False]
['text':' NB: Tensors in constructor are stored in opmath_t, not scalar_t','line_number':62,'multiline':False]
['text':' because Tensor in constructor = its a scalar tensor partially applied =','line_number':63,'multiline':False]
['text':' it can be higher precision and we want to compute in that higher precision','line_number':64,'multiline':False]
['text':'','line_number':65,'multiline':False]
['text':' NB: CUDA only','line_number':66,'multiline':False]
['text':' Only Tensors ever get passed directly to operator()','line_number':80,'multiline':False]
['text':'','line_number':81,'multiline':False]
['text':' NB: CUDA only','line_number':82,'multiline':False]
['text':' (Actually, this works for CPU too)','line_number':83,'multiline':False]
['text':' The actual ufunc template function the user writes.  Everything here','line_number':93,'multiline':False]
['text':' is done in the computation type.  compute_t is opmath_t in CUDA and scalar_t','line_number':94,'multiline':False]
['text':' in CPU','line_number':95,'multiline':False]
['text':' ufunctors are a CUDA-only concept representing functors that take some of','line_number':142,'multiline':False]
['text':' their arguments on a host-side constructor, and the rest in the device-side','line_number':143,'multiline':False]
['text':' apply.  E.g.,','line_number':144,'multiline':False]
['text':'','line_number':145,'multiline':False]
['text':' template <typename scalar_t>','line_number':146,'multiline':False]
['text':' struct CUDAFunctorOnSelf_add {','line_number':147,'multiline':False]
['text':'   using opmath_t = at::opmath_type<scalar_t>;','line_number':148,'multiline':False]
['text':'   opmath_t other_;','line_number':149,'multiline':False]
['text':'   opmath_t alpha_;','line_number':150,'multiline':False]
['text':'   CUDAFunctorOnSelf_add(opmath_t other, opmath_t alpha) : other_(other), alpha_(alpha) {}','line_number':151,'multiline':False]
['text':'   __device__ scalar_t operator()(scalar_t self) {','line_number':152,'multiline':False]
['text':'     return ufunc::add(static_cast<opmath_t>(self), other_, alpha_);','line_number':153,'multiline':False]
['text':'   }','line_number':154,'multiline':False]
['text':' };','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':' The ctor refers to the constructor CUDAFunctorOnSelf_add, while apply refers','line_number':157,'multiline':False]
['text':' to the operator() definition','line_number':158,'multiline':False]
['text':' put it in the ctor anyway','line_number':167,'multiline':False]
['text':' ufuncs are the inner loop template functions that you wrote in ufunc/add.h','line_number':180,'multiline':False]
['text':' which do the actual computation in question.  E.g.,','line_number':181,'multiline':False]
['text':'','line_number':182,'multiline':False]
['text':' template <typename T>','line_number':183,'multiline':False]
['text':' C10_HOST_DEVICE T add(T self, T other, T alpha) __ubsan_ignore_undefined__ {','line_number':184,'multiline':False]
['text':'   return self + alpha * other;','line_number':185,'multiline':False]
['text':' }','line_number':186,'multiline':False]
['text':'','line_number':187,'multiline':False]
['text':' In this file, we refer to T as compute_t which is bound by caller','line_number':188,'multiline':False]
['text':' Stubs are the DispatchStub trampolines that CPU kernels use to get to their','line_number':196,'multiline':False]
['text':' vectorized versions.  E.g.,','line_number':197,'multiline':False]
['text':'','line_number':198,'multiline':False]
['text':' using structured_binary_fn_alpha = void(*)(TensorIteratorBase&, const Scalar& alpha);','line_number':199,'multiline':False]
['text':' DECLARE_DISPATCH(structured_binary_fn_alpha, add_stub);','line_number':200,'multiline':False]
['text':' stubs drop all tensor arguments (they are implicit in the TensorIterator','line_number':202,'multiline':False]
['text':' argument and keep everything else)','line_number':203,'multiline':False]
