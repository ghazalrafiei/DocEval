['text':' Represents a saved attribute involved in backward calculation.','line_number':22,'multiline':False]
['text':' Note that it can be a derived property of an input argument, e.g.:','line_number':23,'multiline':False]
['text':' we could save `other.scalar_type()` instead of the entire `other` tensor.','line_number':24,'multiline':False]
['text':' The NamedCType holds the updated name and cpp type of the attribute','line_number':27,'multiline':False]
['text':' for the name, Suffix is appended if it's derived property, e.g.: `other_scalar_type`','line_number':28,'multiline':False]
['text':' The expression to read the derived property at save time, e.g.:','line_number':31,'multiline':False]
['text':' `other.scalar_type()`.','line_number':32,'multiline':False]
['text':' Represents a backward formula that calculates derivatives for one','line_number':36,'multiline':False]
['text':' or more tensors.','line_number':37,'multiline':False]
['text':' The formula string (legit C++ expression).','line_number':40,'multiline':False]
['text':' Note that expressions against input arguments have been replaced with the','line_number':41,'multiline':False]
['text':' corresponding saved attributes.','line_number':42,'multiline':False]
['text':' E.g.:','line_number':43,'multiline':False]
['text':'  raw formula: `mul_tensor_backward(grad, self, other.scalar_type())`','line_number':44,'multiline':False]
['text':'         here: `mul_tensor_backward(grad, self, other_scalar_type)`','line_number':45,'multiline':False]
['text':' The formula string before input argument replacement','line_number':48,'multiline':False]
['text':' Names of the arguments for which this formula calculates derivatives.','line_number':51,'multiline':False]
['text':' Saved inputs that are referenced by the formula.','line_number':54,'multiline':False]
['text':' Saved outputs that are referenced by the formula.','line_number':57,'multiline':False]
['text':' Gradients that are referenced by name in the formula.','line_number':60,'multiline':False]
['text':' Represents a forward formula that calculates forward derivatives','line_number':64,'multiline':False]
['text':' for one tensor.','line_number':65,'multiline':False]
['text':' The formula string (legit C++ expression).','line_number':68,'multiline':False]
['text':' Note that special keywords such as "linear" or "element_wise" have been','line_number':69,'multiline':False]
['text':' replaced by the automatically generated formula.','line_number':70,'multiline':False]
['text':' Name of the output arguments for which this formula calculates forward','line_number':73,'multiline':False]
['text':' derivatives','line_number':74,'multiline':False]
['text':' Type of the output arguments for which this formula calculates forward','line_number':77,'multiline':False]
['text':' derivatives','line_number':78,'multiline':False]
['text':' Inputs for which the forward derivatives are required for this formula','line_number':81,'multiline':False]
['text':' Inputs for which the primal is required for this formula','line_number':84,'multiline':False]
['text':' Flag to specify if this formula requires the original value of self','line_number':87,'multiline':False]
['text':' This is only used by inplace operations','line_number':88,'multiline':False]
['text':' If this formula is specified in derivatives.yaml or if we are re-using the','line_number':91,'multiline':False]
['text':' out of place formula for inplace','line_number':92,'multiline':False]
['text':' Represents differentiability info for a NativeFunction.','line_number':96,'multiline':False]
['text':' The base name read from derivatives.yaml.','line_number':99,'multiline':False]
['text':' The matching native function.','line_number':102,'multiline':False]
['text':'','line_number':103,'multiline':False]
['text':' There can be multiple NativeFunction having the same base name:','line_number':104,'multiline':False]
['text':'  - different overloads with different types of input arguments;','line_number':105,'multiline':False]
['text':'  - in-place/out/functional variants of the same function;','line_number':106,'multiline':False]
['text':'','line_number':107,'multiline':False]
['text':' We first use the schema string (under the 'name' key) in derivatives.yaml','line_number':108,'multiline':False]
['text':' to find the NativeFunction having the same schema string.','line_number':109,'multiline':False]
['text':' Then we find the in-place/out/functional variants of the matching function.','line_number':110,'multiline':False]
['text':' Among these variants, we choose the one having the same name as the','line_number':111,'multiline':False]
['text':' derivatives.yaml entry. If there is no exact match, then we choose the','line_number':112,'multiline':False]
['text':' in-place variant.','line_number':113,'multiline':False]
['text':' TODO: maybe the logic to search for all variants is no longer necessary?','line_number':114,'multiline':False]
['text':' The name of the generated autograd function.','line_number':117,'multiline':False]
['text':' It's set only if we will calculate a derivative, i.e.','line_number':118,'multiline':False]
['text':' 'args_with_derivatives' is not empty.','line_number':119,'multiline':False]
['text':' The derivatives formulae for this function.','line_number':122,'multiline':False]
['text':' Note that the length of this sequence is the number of differentiable inputs','line_number':123,'multiline':False]
['text':' The forward derivatives formulae for this function.','line_number':126,'multiline':False]
['text':' Note that the length of this sequence is the number of differentiable outputs','line_number':127,'multiline':False]
['text':' The union of 'saved_inputs' of all 'derivatives'.','line_number':130,'multiline':False]
['text':' The union of 'saved_outputs' of all 'derivatives'.','line_number':133,'multiline':False]
['text':' All named gradients that are available for use, in the same','line_number':136,'multiline':False]
['text':' order as in the grads vector.','line_number':137,'multiline':False]
['text':' The named gradients that are used in any of the derivatives.','line_number':140,'multiline':False]
['text':' Invariant: all(name in available_named_gradients for name in used_named_gradients)','line_number':141,'multiline':False]
['text':' The function's input arguments for which it calculates derivatives.','line_number':144,'multiline':False]
['text':' It's the union of 'var_names' of all 'derivatives', sorted by the','line_number':145,'multiline':False]
['text':' argument order in the function schema.','line_number':146,'multiline':False]
['text':' Names of arguments whose derivative formula is 'non_differentiable'.','line_number':149,'multiline':False]
['text':' Raw data read from derivatives.yaml.','line_number':152,'multiline':False]
['text':' output_differentiability in derivatives.yaml can be a list of','line_number':155,'multiline':False]
['text':' conditions that express if the output is differentiable. In this case,','line_number':156,'multiline':False]
['text':' the number of conditions must match the number of outputs','line_number':157,'multiline':False]
['text':' (NB: we only support one condition right now).','line_number':158,'multiline':False]
['text':' output_differentiability gets populated with True for each condition,','line_number':159,'multiline':False]
['text':' while output_differentiability_conditions gets populated with the conditions','line_number':160,'multiline':False]
['text':' Generates a new DifferentiabilityInfo using the exact same set of derivative information,','line_number':167,'multiline':False]
['text':' but with a new operator name.','line_number':168,'multiline':False]
['text':' This is used when generating "copy" variants of view ops,','line_number':169,'multiline':False]
['text':' which are able to use the exact same derivative formula as the original view op','line_number':170,'multiline':False]
['text':' See Note [Codegen'd {view}_copy Operators]','line_number':171,'multiline':False]
['text':' Append a "_copy" to the base name of the operator (but keep the overload name the same)','line_number':180,'multiline':False]
['text':' Use the "_copy" version of name/func/op','line_number':187,'multiline':False]
['text':' But keep all derivative info the same','line_number':191,'multiline':False]
['text':' Represents a differentiable `Argument`.','line_number':223,'multiline':False]
['text':' How is it different from the `Argument` type?','line_number':224,'multiline':False]
['text':' - It's processed Arguments which are differentiable and only used in the','line_number':225,'multiline':False]
['text':'   context of the autograd codegen;','line_number':226,'multiline':False]
['text':' - It can represent SelfArgument or regular Argument but not TensorOptionsArgument;','line_number':227,'multiline':False]
['text':' TODO: only to keep it byte-for-byte compatible with the old codegen, should remove.','line_number':233,'multiline':False]
['text':' Represents a differentiable `Return`.','line_number':237,'multiline':False]
['text':' How it it different from the `Return` type?','line_number':238,'multiline':False]
['text':' - The name in `Return` is optional. Here it is always populated using the same','line_number':239,'multiline':False]
['text':'   `cpp.return_names()` method.','line_number':240,'multiline':False]
['text':'   TODO: some cpp naming logic (e.g. resolving name conflict) might be irrelevant?','line_number':241,'multiline':False]
['text':' - It's processed Returns which are differentiable, in compliance with the','line_number':242,'multiline':False]
['text':'   `output_differentiability` field defined in derivatives.yaml (if specified),','line_number':243,'multiline':False]
['text':'   and are only used in the context of the autograd codegen;','line_number':244,'multiline':False]
['text':' TODO: only to keep it byte-for-byte compatible with the old codegen, should remove.','line_number':250,'multiline':False]
['text':' TODO: Update comment below since it is out of date.','line_number':261,'multiline':False]
['text':' fn is derived as long as any of its per-key differentiability infos','line_number':280,'multiline':False]
['text':' has_derivatives. dispatch_strategy() is used to guard generation of fns in VariableType','line_number':281,'multiline':False]
['text':' and ADInplaceOrViewType. We want to generate these functions as long as a','line_number':282,'multiline':False]
['text':' derivative is defined for ANY dispatch key.','line_number':283,'multiline':False]
['text':' If the function is abstract (not implemented on at::Type), we must','line_number':287,'multiline':False]
['text':' call the implementation on the derived type with unpacked tensors.','line_number':288,'multiline':False]
['text':' If the function has a derivative specified and is concrete, we could','line_number':290,'multiline':False]
['text':' call either implementation. We prefer the calling the derived','line_number':291,'multiline':False]
['text':' type's implementation with unpacked tensors because it is more','line_number':292,'multiline':False]
['text':' performant in some cases: any internal calls to other ATen functions','line_number':293,'multiline':False]
['text':' won't have the history tracked.','line_number':294,'multiline':False]
['text':' If the function has a type dispatched argument (i.e. is a factory),','line_number':296,'multiline':False]
['text':' we prefer calling the derived type's implementation both because it is','line_number':297,'multiline':False]
['text':' more performant and to ensure factory functions return tensors with _version','line_number':298,'multiline':False]
['text':' of 0 (probably not strictly necessary, but nice to have to keeps versions simple','line_number':299,'multiline':False]
['text':' to understand.','line_number':300,'multiline':False]
['text':' If the function is concrete (we don't have to override it) and we','line_number':304,'multiline':False]
['text':' didn't declare it in derivatives.yaml, we'll assume that it is','line_number':305,'multiline':False]
['text':' actually implemented out of differentiable functions. (This','line_number':306,'multiline':False]
['text':' assumption might not hold, but then you'll see gradcheck fail.)','line_number':307,'multiline':False]
['text':' note(crcrpar): Most foreach functions can reference an out-place `torch` function whose schema kind','line_number':315,'multiline':False]
['text':' is functional for their backward derivatives (and forward derivatives in the future), i.e.,','line_number':316,'multiline':False]
['text':' they would find such one in `functional_info_by_signature`. There however are some exceptions:','line_number':317,'multiline':False]
['text':' Checks if `function_schema` is a native, non-foreach function which `f`, a foreach function','line_number':326,'multiline':False]
['text':' reference to generate derivatives.','line_number':327,'multiline':False]
['text':' TODO(crcrpar): Avoid hard coding "Default" ideally.','line_number':348,'multiline':False]
['text':' note(crcrpar): It seems like `zero`'s info isn't available in functional_info_by_signature','line_number':371,'multiline':False]
['text':' while the info of `zero_` is in non_functional_info_by_signature','line_number':372,'multiline':False]
['text':' non out-place uses the existing Derivative.','line_number':387,'multiline':False]
['text':' note(crcrpar): This context seems necessary to call `cpp.argument_type`','line_number':408,'multiline':False]
['text':' type: ignore[no-redef]','line_number':476,'multiline':False]
['text':' Foreach's result is TensorList','line_number':486,'multiline':False]
['text':' Modify reference forward formula','line_number':494,'multiline':False]
['text':' Assuming ScalarList','line_number':499,'multiline':False]
['text':' Assuming TensorList / Tensor','line_number':504,'multiline':False]
['text':' assert isinstance(foreach_arg.type, ListType), f"{foreach_function.func.name}, {foreach_arg.type}"','line_number':505,'multiline':False]
['text':' Assuming Scalar','line_number':516,'multiline':False]
['text':' note(crcrpar): there should exist a cooler way...','line_number':522,'multiline':False]
['text':' Don't bother matching info to generated out= variants','line_number':588,'multiline':False]
['text':' (1) Check for an exact match','line_number':592,'multiline':False]
['text':' (2) If no exact match, check if the out-of-place variant','line_number':596,'multiline':False]
['text':' of this operator has a match.','line_number':597,'multiline':False]
['text':' i.e mul() for mul_() or mul_out()','line_number':598,'multiline':False]
['text':' note(crcrpar): Check foreach or not because in-place foreach functions use backward defined for the existing','line_number':599,'multiline':False]
['text':' native functions instead of the out-place counterparts.','line_number':600,'multiline':False]
['text':' (3) Some operators have a derivative explicitly defined for the mutable','line_number':605,'multiline':False]
['text':' variant, but get a code-generated out-of-place variant which does *not*','line_number':606,'multiline':False]
['text':' come with a derivative formula.','line_number':607,'multiline':False]
['text':' For the generated out-of-place variant, use the mutable variant's formula','line_number':608,'multiline':False]
['text':' if it exists.','line_number':609,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/76320/files#r874816389','line_number':612,'multiline':False]
['text':' (4) Generate derivative information of foreach functions if none is defined in `derivatives.yaml`','line_number':622,'multiline':False]
['text':' TODO(crcrpar): Avoid hard coding "Default" ideally.','line_number':632,'multiline':False]
['text':' Currently, the '.strides()' to 'strides_or_error' replacement does not support','line_number':645,'multiline':False]
['text':' 'self' derivatives of an inplace function, so we must check for this case.','line_number':646,'multiline':False]
['text':' For functions that have a single def for out-of-place and inplace (like abs())','line_number':673,'multiline':False]
['text':' For inplace functions there is a little bit of work to do:','line_number':675,'multiline':False]
['text':'  1) Validate the formula and make sure the input that is modified in not used:','line_number':676,'multiline':False]
['text':'    - If there is a formula for the inplace variant of the function (is_exact_match == True) then','line_number':677,'multiline':False]
['text':'      we make sure that the original value of the input that is being modified inplace (self_p) is','line_number':678,'multiline':False]
['text':'      not used in the formula. Note that the formula can use "original_self_p" here and that would','line_number':679,'multiline':False]
['text':'      trigger a clone of the original input.','line_number':680,'multiline':False]
['text':'    - If we are re-using the out of place formula (is_exact_match == False) then we replace every','line_number':681,'multiline':False]
['text':'      occurrence of self_p and self_t by original_self_p and original_self_t. These will be','line_number':682,'multiline':False]
['text':'      populated by cloned version of the original input (either the clone done by the backward AD','line_number':683,'multiline':False]
['text':'      logic if self is also used in a backward formula or a special clone that we add).','line_number':684,'multiline':False]
['text':'  2) At this point, there cannot be a self_p in the formula.','line_number':685,'multiline':False]
['text':'  3) Change "result" into "self_p" as by design, in the inplace function codegen, the result is','line_number':686,'multiline':False]
['text':'     simply called self (as it is modified inplace).','line_number':687,'multiline':False]
['text':'  4) Update the required primals data in case it used to contain "result" but should now contain','line_number':688,'multiline':False]
['text':'     "self"','line_number':689,'multiline':False]
['text':'  5) If it is not an exact match, the user formula is not modifying the existing forward grad','line_number':690,'multiline':False]
['text':'     inplace as it should. So add some code that makes sure that we do so if the forward grad','line_number':691,'multiline':False]
['text':'     already exists.','line_number':692,'multiline':False]
['text':' Only single output inplace should exist','line_number':696,'multiline':False]
['text':' For manually defined formulas, don't allow the original value to be used','line_number':708,'multiline':False]
['text':' When the original formula is out of place, we save a clone of the primal','line_number':715,'multiline':False]
['text':' value to be able to access this value if needed','line_number':716,'multiline':False]
['text':' replace "self_p"/"self_t" from the formula by "original_self_p"/"original_self_t"','line_number':717,'multiline':False]
['text':' replace "result" from the formula by "self_p"','line_number':721,'multiline':False]
['text':' NOTE [In-place forward AD formula Optimization]','line_number':734,'multiline':False]
['text':'','line_number':735,'multiline':False]
['text':' This optimization transforms the formula to directly do inplace, i.e.','line_number':736,'multiline':False]
['text':' instead of self_t.copy_(self_t.op()) we do self_t.op_() when the following are met:','line_number':737,'multiline':False]
['text':'','line_number':738,'multiline':False]
['text':' 1) the formula satisfies the pattern: "self_t.op(*args)"','line_number':739,'multiline':False]
['text':' 2) "op" in (1) needs to be the same as the op the derivative is for','line_number':740,'multiline':False]
['text':'','line_number':741,'multiline':False]
['text':' (2) may seem too strict, but currently the only ops that satisfy (1) also satisfy (2)','line_number':742,'multiline':False]
['text':' If there is a need, we can relax (2) to allow any op that has an in-place variant','line_number':743,'multiline':False]
['text':' We want to...','line_number':752,'multiline':False]
['text':'   Match: self_t.op1(other_p.op2(arg))','line_number':753,'multiline':False]
['text':'   Avoid: self_t.op1(args) + self_t.op2(args)','line_number':754,'multiline':False]
['text':'   Avoid: self_t.op1(other_p.op2(arg)) + self_t.op2(args)','line_number':755,'multiline':False]
['text':' Make sure that the forward grad is modified inplace when the original formula','line_number':779,'multiline':False]
['text':' is out of place','line_number':780,'multiline':False]
