['text':' TODO: Matching on CType seems wrong; should be matching on Type','line_number':48,'multiline':False]
['text':' TODO: I don't understand when you should put lazy_ in the name','line_number':60,'multiline':False]
['text':' or not','line_number':61,'multiline':False]
['text':' NB: this is here because right now we aren't treating SymInt[] as a','line_number':75,'multiline':False]
['text':' value type; when we do this needs to move above','line_number':76,'multiline':False]
['text':' NB: we cannot test arg.lazy_type as we've already specified it is an','line_number':77,'multiline':False]
['text':' int64_t and so we cannot distinguish between SymInt and int64_t','line_number':78,'multiline':False]
['text':' converts  all tensor-like arguments to meta tensors. Returns:','line_number':147,'multiline':False]
['text':' (1) a string containing all of the logic that does the conversions.','line_number':148,'multiline':False]
['text':' (2) a context, to be used by translate(), with all of the relevant bindings.','line_number':149,'multiline':False]
['text':' there is no lowering functionality generated unless this IR base class is subclassed and','line_number':184,'multiline':False]
['text':' implemented as a backend-specific node','line_number':185,'multiline':False]
['text':' backends can customize the way the node base class constructor is called,','line_number':199,'multiline':False]
['text':' as long as all of its arguments can be generated from information available from the schema','line_number':200,'multiline':False]
['text':' Shape construction.','line_number':215,'multiline':False]
['text':' Conditionally build shape depending on specified shape property','line_number':216,'multiline':False]
['text':' for now, we just want one IR class decl and soon after also the method defs','line_number':242,'multiline':False]
['text':' and we use the functional version not out/inplace.','line_number':243,'multiline':False]
['text':' This code is just special casing the mapping from string_view -> strings','line_number':256,'multiline':False]
['text':' Generates lazy_{name} variables for LazyTensors wrapping input tensors','line_number':416,'multiline':False]
['text':' values are extracted in isValueType','line_number':433,'multiline':False]
['text':' TODO(alanwaketan): Maybe we want to apply GetLtcTensorOrCreateForWrappedNumber here, but hold it','line_number':447,'multiline':False]
['text':' until we encounter a real world example.','line_number':448,'multiline':False]
['text':' call the meta kernel if it exists, to compute output shape/dtype for our IR','line_number':498,'multiline':False]
['text':' Note [Generated LTC Shape Functions]','line_number':499,'multiline':False]
['text':' LTC uses meta tensors from core to do shape inference when possible, and otherwise','line_number':500,'multiline':False]
['text':' we generate a shape function declaration that needs to be manually implemented.','line_number':501,'multiline':False]
['text':' How do we detect which ops are eligible to use meta tensors?','line_number':502,'multiline':False]
['text':' In general we should be able to use meta tensors not just on structured operators,','line_number':503,'multiline':False]
['text':' but also on composite operators that are implemented in terms of structured kernels.','line_number':504,'multiline':False]
['text':' We don't currently have a way of knowing at codegen time which ops are implemented that way.','line_number':505,'multiline':False]
['text':' This is the case for all view and view_copy operators however, so we're going to','line_number':506,'multiline':False]
['text':' use them specifically for all of the view_copy ops (instead of manually writing shape rules for all of them).','line_number':507,'multiline':False]
['text':' Convert tensor args to the meta device and call it.','line_number':521,'multiline':False]
['text':' (We can't pass in the input tensors directly, because they are "functional wrappers".','line_number':522,'multiline':False]
['text':' If any of the meta kernels call a tensor op and redispatch, we don't want to hit the functionalize kernels.)','line_number':523,'multiline':False]
['text':' Even at::meta:: functions might redispatch, e.g. if they call into view ops.','line_number':524,'multiline':False]
['text':' view_copy ops always have a CompositeExplicitAutogradNonFunctional kernel','line_number':534,'multiline':False]
['text':' TODO: this is trolling','line_number':540,'multiline':False]
['text':' Calculating which dimensions are symbolic','line_number':557,'multiline':False]
['text':' xla uses an instance method for tensor creation, for the time being','line_number':579,'multiline':False]
['text':' TODO(whc) remove this if XLA switches to using static method for creation','line_number':581,'multiline':False]
['text':' See Note [Generated LTC Shape Functions]','line_number':679,'multiline':False]
['text':' Set default properties for Non-Native IRs','line_number':697,'multiline':False]
['text':' non-native is assumed to want symint bindings if you wrote symint','line_number':702,'multiline':False]
