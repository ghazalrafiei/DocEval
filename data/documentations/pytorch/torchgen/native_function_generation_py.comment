['text':' See Note: [Out ops with functional variants that don't get grouped properly]','line_number':30,'multiline':False]
['text':' This has a functional variant, but it's currently marked private.','line_number':32,'multiline':False]
['text':' This function should be marked private as well (*_backward ops aren't exposed to python anyway).','line_number':33,'multiline':False]
['text':' There's a functional variant, _slow_conv2d_backward.output_mask, that isn't grouped properly.','line_number':35,'multiline':False]
['text':' Maybe we can kill this operator in favor of convolution_backward?','line_number':36,'multiline':False]
['text':' See Note: [Mutable ops that cannot get an out variant]','line_number':41,'multiline':False]
['text':' should be out=?','line_number':43,'multiline':False]
['text':' should be out=?','line_number':45,'multiline':False]
['text':' All of these operators don't have any tensor like returns','line_number':49,'multiline':False]
['text':' no return','line_number':51,'multiline':False]
['text':' no return','line_number':52,'multiline':False]
['text':' returns an int','line_number':53,'multiline':False]
['text':' returns an int','line_number':54,'multiline':False]
['text':' returns an int','line_number':55,'multiline':False]
['text':' returns a boolean','line_number':56,'multiline':False]
['text':' no return','line_number':57,'multiline':False]
['text':' returns a Scalar','line_number':58,'multiline':False]
['text':' returns a boolean','line_number':59,'multiline':False]
['text':' returns an int','line_number':60,'multiline':False]
['text':' returns a boolean','line_number':61,'multiline':False]
['text':' returns a boolean','line_number':62,'multiline':False]
['text':' no return','line_number':63,'multiline':False]
['text':' returns a boolean','line_number':64,'multiline':False]
['text':' returns an int','line_number':65,'multiline':False]
['text':' returns a boolean','line_number':66,'multiline':False]
['text':' returns an boolean','line_number':67,'multiline':False]
['text':' returns a boolean','line_number':68,'multiline':False]
['text':' returns a boolean','line_number':69,'multiline':False]
['text':' returns a boolean','line_number':70,'multiline':False]
['text':' returns an int','line_number':71,'multiline':False]
['text':' returns a float','line_number':72,'multiline':False]
['text':' returns an int','line_number':73,'multiline':False]
['text':' returns a QScheme','line_number':74,'multiline':False]
['text':' no return','line_number':75,'multiline':False]
['text':' returns an int','line_number':76,'multiline':False]
['text':' no return','line_number':77,'multiline':False]
['text':' no return','line_number':78,'multiline':False]
['text':' returns a vector of ints','line_number':79,'multiline':False]
['text':' returns a bool','line_number':80,'multiline':False]
['text':' returns an int','line_number':81,'multiline':False]
['text':' polygamma and polygamma.out both exist, but have a','line_number':85,'multiline':False]
['text':' pre-self arg (while polygamma_ does not)','line_number':86,'multiline':False]
['text':' We should either fix this schema so it can be grouped properly,','line_number':87,'multiline':False]
['text':' or allow the codegen to generate new functional/out= NativeFunctions for this op','line_number':88,'multiline':False]
['text':' (which would require changing its overload name to prevent overload ambiguity).','line_number':89,'multiline':False]
['text':' Groups "similar" NativeFunctions together','line_number':94,'multiline':False]
['text':' example add.Tensor, add_.Tensor, add.out','line_number':95,'multiline':False]
['text':' "similar" NativeFunctions are all expected to have an identical `signature()`,','line_number':96,'multiline':False]
['text':' But have differing SchemaKinds.','line_number':97,'multiline':False]
['text':' Returns the out variant overload name given a base function overload name','line_number':111,'multiline':False]
['text':' Helper function: given an inplace FunctionSchema, generate its corresponding out= variant','line_number':116,'multiline':False]
['text':' Example before:','line_number':117,'multiline':False]
['text':'   _add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)','line_number':118,'multiline':False]
['text':' Example after:','line_number':119,'multiline':False]
['text':'   _add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out)','line_number':120,'multiline':False]
['text':' Generating an out= schema from an inplace schema.','line_number':122,'multiline':False]
['text':' The new out= schema has:','line_number':125,'multiline':False]
['text':' - a new out argument with the same type as "func" (but with a mutable annotation)','line_number':126,'multiline':False]
['text':' - The returns (if any) now alias the out= argument instead of "func"','line_number':127,'multiline':False]
['text':' - an "out" overload name','line_number':128,'multiline':False]
['text':' Helper function: given a functional FunctionSchema, generate its corresponding out= variant','line_number':147,'multiline':False]
['text':' Example before:','line_number':148,'multiline':False]
['text':'   _to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None,','line_number':149,'multiline':False]
['text':'       bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor','line_number':150,'multiline':False]
['text':' Example after:','line_number':151,'multiline':False]
['text':'   _to_copy._out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None,','line_number':152,'multiline':False]
['text':'       Tensor(a!) out) -> Tensor(a!)','line_number':153,'multiline':False]
['text':' Generating an out= schema from a functional schema.','line_number':155,'multiline':False]
['text':' The new out= schema has:','line_number':159,'multiline':False]
['text':' - one or more new out argument(s) with the same type as returns (but with a mutable annotation)','line_number':160,'multiline':False]
['text':' - The returns now alias the out= arguments','line_number':161,'multiline':False]
['text':' - an "_out" overload name','line_number':162,'multiline':False]
['text':' Helper function: given a function schema, generate corresponding out arguments, also the updated return annotations.','line_number':174,'multiline':False]
['text':' More of a sanity check - our existing restrictions on schemas should enforce that','line_number':178,'multiline':False]
['text':' mutable schema kinds never return their mutable arguments.','line_number':179,'multiline':False]
['text':' The end result of new_returns is that:','line_number':198,'multiline':False]
['text':' - If every return is a plain tensor, then the new returns == the old returns, but with the out= alias annotations added.','line_number':199,'multiline':False]
['text':' - Otherwise, none of the out arguments show up in the returns (and we're only left with non-tensor-like returns, if any).','line_number':200,'multiline':False]
['text':' The convention for out= schemas is that they only return their out arguments','line_number':212,'multiline':False]
['text':' if the return is a plain Tensor (or if it's a tuple of plain Tensors)','line_number':213,'multiline':False]
['text':' Helper function: given a mutable FunctionSchema, generate its corresponding out= variant','line_number':223,'multiline':False]
['text':' Example before:','line_number':224,'multiline':False]
['text':'   _fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)  # noqa: B950','line_number':225,'multiline':False]
['text':' Example after:','line_number':226,'multiline':False]
['text':'   _fused_moving_avg_obs_fq_helper._out(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False, *, Tensor(e!) out0, Tensor(f!) out1) -> (Tensor(e!), Tensor(f!))  # noqa: B950','line_number':227,'multiline':False]
['text':' Generating an out= schema from a mutable schema.','line_number':229,'multiline':False]
['text':' The new out= schema has:','line_number':231,'multiline':False]
['text':' - Any non-aliased tensor-like returns are converted to mutable, aliased out= arguments','line_number':232,'multiline':False]
['text':'   (if the argument is a tensor then we also return it for method chaining,','line_number':233,'multiline':False]
['text':'   otherwise we return nothing)','line_number':234,'multiline':False]
['text':' - an "out" overload name','line_number':235,'multiline':False]
['text':'','line_number':236,'multiline':False]
['text':' Note that:','line_number':237,'multiline':False]
['text':' (1) This also means that we can *only* generate an out= variant from a mutable schema','line_number':238,'multiline':False]
['text':'     if the mutable schema has at least one tensor-like non-aliasing return.','line_number':239,'multiline':False]
['text':' (2) The generated out= variant still has mutable positional arguments,','line_number':240,'multiline':False]
['text':'     but if necessary we could probably add another out= variant that also','line_number':241,'multiline':False]
['text':'     functionalizes the mutable arguments (a functional_out variant)','line_number':242,'multiline':False]
['text':' This function, given function of one SchemaKind, as well as a target SchemaKind,','line_number':255,'multiline':False]
['text':' generates a new NativeFunction with the same properties, but using the target SchemaKind.','line_number':256,'multiline':False]
['text':' We only actually generate functions for either functional or out= SchemaKinds.','line_number':257,'multiline':False]
['text':' This function returns a tuple, with:','line_number':258,'multiline':False]
['text':' - The generated NativeFunction','line_number':259,'multiline':False]
['text':' - a dictionary of `BackendIndex` objects, describing which dispatch keys','line_number':260,'multiline':False]
['text':'   we will generate kernels for, for the new NativeFunction.','line_number':261,'multiline':False]
['text':'   Details are in the function, but we only generate composite kernels (in some cases) today.','line_number':262,'multiline':False]
['text':' The new "functional" NativeFunction has:','line_number':270,'multiline':False]
['text':' - any mutable arguments have been converted into (immutable) returns.','line_number':271,'multiline':False]
['text':'   (if a mutable argument was not also a return, it gets converted to one)','line_number':272,'multiline':False]
['text':' - "_functional" appended to the base name, ONLY IF this op has a mutable variant.','line_number':273,'multiline':False]
['text':'   See Note [Overload Ambiguity With Functional Variants]','line_number':274,'multiline':False]
['text':' The default grouping logic in signature() actually already does this,','line_number':275,'multiline':False]
['text':' so we can piggy-back off it (but we still want return names)','line_number':276,'multiline':False]
['text':' See Note [Overload Ambiguity With Functional Variants]','line_number':283,'multiline':False]
['text':' We generate out= ops mostly just so that we can pair up NativeFunctions into groups easily,','line_number':290,'multiline':False]
['text':' but at least today, there is no good reason to actually use them.','line_number':291,'multiline':False]
['text':' we'll generate a dispatcher entry for them, but won't actually register any kernels for them.','line_number':292,'multiline':False]
['text':' Generated kernel naming convention for out: <op_name>_<overload_name>. The reason for this is to','line_number':308,'multiline':False]
['text':' disambiguate operator with the same name but different overload name, e.g., `randn.names_out` and','line_number':309,'multiline':False]
['text':' `randn.generator_with_names_out`.','line_number':310,'multiline':False]
['text':' These generated fn's aren't meant to be user friendly- don't generate methods.','line_number':335,'multiline':False]
['text':' Every generated NativeFunction gets a "generated" tag, so it's easy to tell','line_number':356,'multiline':False]
['text':' which NativeFunction objects did not come directly from native_functions.yaml.','line_number':357,'multiline':False]
['text':' This function is responsible for adding generated NativeFunctions which don't appear','line_number':365,'multiline':False]
['text':' explicitly in the codegen.','line_number':366,'multiline':False]
['text':' You can inspect the full list of NativeFunctions yourself with the torchgen package, by running','line_number':367,'multiline':False]
['text':' torchgen.parse_native_yaml("aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/tags.yaml")','line_number':368,'multiline':False]
['text':' (Maybe we should make a friendly API for this)','line_number':369,'multiline':False]
['text':'','line_number':370,'multiline':False]
['text':' Note: this function *mutates* its two inputs,','line_number':371,'multiline':False]
['text':' adding the new NativeFunctions / BackendMetadata to them','line_number':372,'multiline':False]
['text':' The main code for generating new NativeFunctions','line_number':377,'multiline':False]
['text':' First we group of NativeFunctions by schema kind,','line_number':378,'multiline':False]
['text':' then we detect which ones are missing and generate them.','line_number':379,'multiline':False]
['text':' We automatically generate a few native functions that don't exist in the yaml, for a few reasons:','line_number':387,'multiline':False]
['text':' (1) If an operator has an inplace/out= variant but no functional variant, we can generate','line_number':388,'multiline':False]
['text':'     a simple functional variant that the functionalization pass can consume.','line_number':389,'multiline':False]
['text':' (2) If an operator has an inplace or functional but no out= variant, we generate an out=','line_number':390,'multiline':False]
['text':'     variant, mostly so we can easily pair up functions into NativeFunctionsGroup,','line_number':391,'multiline':False]
['text':'     while maintaining the constraint that the out= variant is "required".','line_number':392,'multiline':False]
['text':' Don't bother generating functions trio's for native functions that bypass the dispatcher.','line_number':394,'multiline':False]
['text':' Don't bother generating functional + out= variants for view operators','line_number':396,'multiline':False]
['text':' Don't generate the other variants for CompositeImplicitAutograd operators.','line_number':398,'multiline':False]
['text':' We could probably do this, but the main benefit of generating the function triplets','line_number':399,'multiline':False]
['text':' is for transforms that need them, and transforms don't need to act directly','line_number':400,'multiline':False]
['text':' on CompositeImplicitAutograd operators (since we let them decompose).','line_number':401,'multiline':False]
['text':' Note: [Out ops with functional variants that don't get grouped properly]','line_number':408,'multiline':False]
['text':' In theory we could validly have an out= operator in native_functions.yaml','line_number':409,'multiline':False]
['text':' that has no other variants.','line_number':410,'multiline':False]
['text':' But today, all of the operators where that's the case actually do have','line_number':411,'multiline':False]
['text':' functional variants, that we are just unable to pair up properly.','line_number':412,'multiline':False]
['text':' I think banning this all together is probably safer','line_number':413,'multiline':False]
['text':' (you can always add a functional variant yourself if you want to add a new out= operator).','line_number':414,'multiline':False]
['text':'','line_number':415,'multiline':False]
['text':' We should probably fix the existing cases; this check is to prevent us from adding more over time.','line_number':416,'multiline':False]
['text':' Some inplace ops that have problematic schemas (that we should fix), which prevent us','line_number':426,'multiline':False]
['text':' from generating out= and functional variants','line_number':427,'multiline':False]
['text':' Note: [Mutable ops that cannot get an out variant]','line_number':445,'multiline':False]
['text':' We can only generate an out= variant if either:','line_number':446,'multiline':False]
['text':' - the original function has tensor-like returns (since we can convert them to out kwargs)','line_number':447,'multiline':False]
['text':' - or it's inplace (since we can convert `self` to an out kwarg)','line_number':448,'multiline':False]
['text':' There are only two functions that don't fit this criteria today though,','line_number':449,'multiline':False]
['text':' and they both look like they should be fixed to be out= variants,','line_number':450,'multiline':False]
['text':' so if feels safer to ban this schema all-together','line_number':451,'multiline':False]
['text':' Note: [Loosen the assertion that all functional should have out variant]','line_number':455,'multiline':False]
['text':' By design all functional operators should have our variants. The needs_out check','line_number':456,'multiline':False]
['text':' is loosening this requirement, changing it to only generate out variant if there's','line_number':457,'multiline':False]
['text':' an `autogen` block in the native function, in the long run it should be removed.','line_number':458,'multiline':False]
['text':' FIXME: Remove this after figuring out CI job failures related to min, max, mean','line_number':459,'multiline':False]
['text':' Generate an out= variant','line_number':475,'multiline':False]
['text':' Generate a functional variant, but only do it if the operator got an out= variant','line_number':482,'multiline':False]
['text':' (Functional variants are only useful if we can group up the variants,','line_number':483,'multiline':False]
['text':' which we can only do if they have an out= variant)','line_number':484,'multiline':False]
['text':' Given a function, and the name of a variable corresponding to the output of that function,','line_number':502,'multiline':False]
['text':' gather up all of the individual returns that are not aliased','line_number':503,'multiline':False]
['text':' Generates functional kernels in terms of their inplace.mutable counterparts.','line_number':516,'multiline':False]
['text':' We only do this for "generated" NativeFunctions','line_number':517,'multiline':False]
['text':' We should only be generating these for code-generated NativeFunctions','line_number':520,'multiline':False]
['text':' And we always write the kernel for a generated op in terms of a non-generated op.','line_number':523,'multiline':False]
['text':' We should be guaranteed to have a valid inplace/mutable variant to call into.','line_number':529,'multiline':False]
['text':' See Note: [Mutable Ops Not Using Functionalization]','line_number':530,'multiline':False]
['text':' We can't just directly pass all of the arguments from the functional op into the mutating op.','line_number':539,'multiline':False]
['text':' We need to check for which inputs to the mutating operator are mutable,','line_number':540,'multiline':False]
['text':' and clone those inputs first.','line_number':541,'multiline':False]
['text':' Invariant: mutable arguments on the inner mutable op are always returns on the functional op.','line_number':556,'multiline':False]
['text':' Generates out= kernels in terms of their functional counterparts.','line_number':579,'multiline':False]
['text':' We only do this for "generated" NativeFunctions','line_number':580,'multiline':False]
['text':' We should only be generating these for code-generated NativeFunctions','line_number':583,'multiline':False]
['text':' And we always write the kernel for the out= op in terms of the functional.','line_number':586,'multiline':False]
['text':' Note that the functional op might have also been generated, but we don't have to','line_number':587,'multiline':False]
['text':' worry about cycles, because the generated functional kernels are always implemented','line_number':588,'multiline':False]
['text':' in terms of non-generated kernels (see gen_composite_functional_kernel).','line_number':589,'multiline':False]
['text':' For each return arg in the calling (out=) operator,','line_number':613,'multiline':False]
['text':' If it corresponds to an aliased input, return the input.','line_number':614,'multiline':False]
['text':' Otherwise, return the corresponding output from calling the functional operator.','line_number':615,'multiline':False]
['text':' Kernel name needs to follow the naming convention defined in `generate_function()`','line_number':629,'multiline':False]
