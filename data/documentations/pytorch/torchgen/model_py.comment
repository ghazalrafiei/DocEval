['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':'                           DATA MODEL','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':' Some general principles for our data model.','line_number':17,'multiline':False]
['text':'','line_number':18,'multiline':False]
['text':' - Stop using C++ data types as the internal data representation','line_number':19,'multiline':False]
['text':'   format.  Instead, the internal data structures are centered','line_number':20,'multiline':False]
['text':'   around JIT schema representation.  This avoid a big problem','line_number':21,'multiline':False]
['text':'   with the old codegen where we read in all the types from','line_number':22,'multiline':False]
['text':'   native_functions.yaml and then immediately had to retranslate','line_number':23,'multiline':False]
['text':'   them into C++ types.','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':' - More semantic data representation.  Instead of representing','line_number':26,'multiline':False]
['text':'   everything as dicts and strings, we define dataclasses for','line_number':27,'multiline':False]
['text':'   every interesting entity the code generation has to deal with.','line_number':28,'multiline':False]
['text':'   These dataclasses have strong semantic invariants: for example,','line_number':29,'multiline':False]
['text':'   we generally require them to roundtrip losslessly into the','line_number':30,'multiline':False]
['text':'   form they were parsed from.  These structures are immutable','line_number':31,'multiline':False]
['text':'   and you're expected to populate information once during','line_number':32,'multiline':False]
['text':'   construction.','line_number':33,'multiline':False]
['text':' Represent a source location; used for better error reporting','line_number':36,'multiline':False]
['text':' Valid values of the 'variants' field in native_functions.yaml','line_number':46,'multiline':False]
['text':' Default kernel namespace','line_number':52,'multiline':False]
['text':' NOTE: Keep the list in sync with `DispatchKey` in c10/core/DispatchKey.h','line_number':55,'multiline':False]
['text':' This list guards dispatches that can be used in derivatives.yaml','line_number':59,'multiline':False]
['text':' For now we omit AutogradFunctionality and AutogradOther','line_number':60,'multiline':False]
['text':' This doesn't have to be in sync with the header, it only needs to contain','line_number':68,'multiline':False]
['text':' entries that we actually use in the codegen or want pyi entries for','line_number':69,'multiline':False]
['text':' BEGIN autogenerated','line_number':121,'multiline':False]
['text':' END autogenerated','line_number':197,'multiline':False]
['text':' Set of supported dispatch keys','line_number':240,'multiline':False]
['text':' Meta is a magic key: it is automatically generated for structured','line_number':258,'multiline':False]
['text':' kernels','line_number':259,'multiline':False]
['text':' Dispatch keys that "support all backends".  These codegen slightly differently','line_number':268,'multiline':False]
['text':' then backend specific keys.','line_number':269,'multiline':False]
['text':' CUDA specific dispatch keys','line_number':279,'multiline':False]
['text':' Structured kernel generation is only supported for certain key types;','line_number':291,'multiline':False]
['text':' otherwise use old-style','line_number':292,'multiline':False]
['text':' For now, ufunc dispatch keys coincide with structured keys','line_number':298,'multiline':False]
['text':' This is oddly named ScalarType and not DType for symmetry with C++','line_number':302,'multiline':False]
['text':' NB: Integral doesn't include boolean','line_number':350,'multiline':False]
['text':' NB: Floating doesn't include low precision types','line_number':360,'multiline':False]
['text':' Represents the valid entries for ufunc_inner_loop in native_functions.yaml.','line_number':372,'multiline':False]
['text':' NB: if you add a new UfuncKey, you will teach torchgen.dest.ufunc how','line_number':373,'multiline':False]
['text':' to process it.  Most logic will ignore keys they don't understand, so your','line_number':374,'multiline':False]
['text':' new key will get silently ignored until you hook in logic to deal with it.','line_number':375,'multiline':False]
['text':' These are low level keys that represent exactly one particular','line_number':377,'multiline':False]
['text':' instantiation of the kernel produced by codegen','line_number':378,'multiline':False]
['text':' These are the ones users will usually specify, and','line_number':386,'multiline':False]
['text':' implicitly "fill in" the low level keys','line_number':387,'multiline':False]
['text':' CUDA*, CPUScalar','line_number':388,'multiline':False]
['text':' CUDA*, CPU*','line_number':389,'multiline':False]
['text':' The basic input to the code generation is native_functions.yaml.','line_number':413,'multiline':False]
['text':' The name "native", BTW, comes from the distinction between native','line_number':414,'multiline':False]
['text':' functions and legacy TH functions.  The legacy TH functions are gone,','line_number':415,'multiline':False]
['text':' but the "native" descriptor has stuck.','line_number':416,'multiline':False]
['text':'','line_number':417,'multiline':False]
['text':' NativeFunction models a single entry in native_functions.yaml.  Its','line_number':418,'multiline':False]
['text':' fields roughly correspond to what you would see in the YAML itself,','line_number':419,'multiline':False]
['text':' but after canonicalization and parsing has occurred.','line_number':420,'multiline':False]
['text':'','line_number':421,'multiline':False]
['text':' You can see some of the overall design patterns for how we setup','line_number':422,'multiline':False]
['text':' dataclasses in this class, but we will defer a complete discussion','line_number':423,'multiline':False]
['text':' of this at FunctionSchema.','line_number':424,'multiline':False]
['text':' The namespace for this operator. For example, if we have "at::add"','line_number':427,'multiline':False]
['text':' then the namespace would be "at". This enables ops to be registered','line_number':428,'multiline':False]
['text':' through the same DSL with a custom namespace. If not specified, the','line_number':429,'multiline':False]
['text':' default namespace would be "at".','line_number':430,'multiline':False]
['text':' The function schema of the operator in question.  This schema','line_number':433,'multiline':False]
['text':' has been parsed; see FunctionSchema for more about its structure.','line_number':434,'multiline':False]
['text':' (This type is quoted as we are forward referencing a type','line_number':435,'multiline':False]
['text':' defined later in the file.  I opted for this ordering of the','line_number':436,'multiline':False]
['text':' classes for expository clarity.)','line_number':437,'multiline':False]
['text':' Whether or not to generate mutable tensor arguments like regular','line_number':440,'multiline':False]
['text':' ones','line_number':441,'multiline':False]
['text':' Whether or not to omit automatic generation of a DeviceGuard','line_number':444,'multiline':False]
['text':' How to emit automatic generation of device check','line_number':447,'multiline':False]
['text':' What python module to put the function in','line_number':450,'multiline':False]
['text':' TODO: figure out what this does','line_number':453,'multiline':False]
['text':' If no variants are specified in native_functions.yaml, this is','line_number':456,'multiline':False]
['text':' assumed to be {'function'}.','line_number':457,'multiline':False]
['text':' Whether or not we should skip generating registrations for','line_number':460,'multiline':False]
['text':' this kernel.  This is a bit of a double-edged sword, as manual','line_number':461,'multiline':False]
['text':' registrations don't participate in codegen-based selective build!','line_number':462,'multiline':False]
['text':' Whether or not to skip generating TensorMethod/Functions bindings','line_number':465,'multiline':False]
['text':' for this kernel.  Technically, this doesn't actually skip generating','line_number':466,'multiline':False]
['text':' the binding; instead, the binding gets generated to __dispatch_{funcname}','line_number':467,'multiline':False]
['text':' so you can make use of the normal binding if you need it.','line_number':468,'multiline':False]
['text':' The location in the YAML file were this native function entry was','line_number':471,'multiline':False]
['text':' defined.  This is for conveniently reporting error messages!','line_number':472,'multiline':False]
['text':' A list of operators that are expected to be auto-generated for this NativeFunction.','line_number':475,'multiline':False]
['text':' Note: This list isn't actually directly used by the codegen to generate anything.','line_number':476,'multiline':False]
['text':' Instead, the codegen figures out what operators to generate purely based off of','line_number':477,'multiline':False]
['text':' function schema, and uses the autogen declarations to error check.','line_number':478,'multiline':False]
['text':' We expect every NativeFunction that gets auto-generated be explicitly called out','line_number':479,'multiline':False]
['text':' in native_functions.yaml','line_number':480,'multiline':False]
['text':' If non-empty, this kernel is subject to ufunc codegen.','line_number':483,'multiline':False]
['text':' Sorted by ufunc_key','line_number':484,'multiline':False]
['text':' Whether or not this out functions is a "structured kernel".  Structured','line_number':487,'multiline':False]
['text':' kernels are defined a little differently from normal kernels; in','line_number':488,'multiline':False]
['text':' particular, their shape checking logic is defined separately from','line_number':489,'multiline':False]
['text':' the kernel.  Only out functions can be structured; other functions','line_number':490,'multiline':False]
['text':' delegate to the out function using the structured_delegate keyword.','line_number':491,'multiline':False]
['text':' Every structured kernel must have at least an out and a functional','line_number':492,'multiline':False]
['text':' variant.','line_number':493,'multiline':False]
['text':' Whether or not this non-out function is a structured kernel, defined','line_number':496,'multiline':False]
['text':' in terms of the out kernel referenced by the string here.','line_number':497,'multiline':False]
['text':' Only valid for structured kernels.  Specifies alternative of what','line_number':500,'multiline':False]
['text':' to inherit from when defining the meta class for the structured','line_number':501,'multiline':False]
['text':' operator.  This will usually be TensorIteratorBase.  This also','line_number':502,'multiline':False]
['text':' changes the semantics of set_output to call the parent class.','line_number':503,'multiline':False]
['text':' Structured kernels can declare elements as "precomputed". These elements','line_number':506,'multiline':False]
['text':' are returned by the meta function in one struct and passed to the impl','line_number':507,'multiline':False]
['text':' function in lieu of certain kernel arguments that these precomputed','line_number':508,'multiline':False]
['text':' elements supersede. Information about the names and types of these','line_number':509,'multiline':False]
['text':' precomputed elements and how they correspond to kernel arguments is stored','line_number':510,'multiline':False]
['text':' in this member, if applicable.','line_number':511,'multiline':False]
['text':' Argument names whose default  should be excluded from the C++ interface.','line_number':514,'multiline':False]
['text':' Intended for resolving overload ambiguities between signatures.','line_number':515,'multiline':False]
['text':' Note [Abstract ATen methods]','line_number':518,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':519,'multiline':False]
['text':' An abstract ATen method is one whose dispatch differs between','line_number':520,'multiline':False]
['text':' types.  These are implemented in derived types (with a','line_number':521,'multiline':False]
['text':' standard (throwing) definition in Type).  A concrete ATen','line_number':522,'multiline':False]
['text':' method is one which has the same dispatch for all types;','line_number':523,'multiline':False]
['text':' we just implement it in the base Type.  This is exposed','line_number':524,'multiline':False]
['text':' in Declarations.yaml via a field named 'abstract'.','line_number':525,'multiline':False]
['text':' Whether or not the NativeFunction contains a backend-agnostic kernel','line_number':528,'multiline':False]
['text':' Tags are used to describe semantic information about (groups of) operators,','line_number':534,'multiline':False]
['text':' That aren't easily inferrable directly from the operator's schema.','line_number':535,'multiline':False]
['text':' NB: The benefit of defining a dataclass is that we automatically get','line_number':538,'multiline':False]
['text':' a constructor defined for all the fields we specify.  No need','line_number':539,'multiline':False]
['text':' to explicitly write it out.','line_number':540,'multiline':False]
['text':' We parse both the NativeFunction + backend-specific information about it, which it stored in a corresponding BackendIndex.','line_number':542,'multiline':False]
['text':' only support one level of namespace. E.g., aten::add','line_number':560,'multiline':False]
['text':' All aten ops generated by torchgen receive the pt2_compliant tag.','line_number':654,'multiline':False]
['text':' TODO: verify that the tag is valid and has an entry in tags.yaml','line_number':661,'multiline':False]
['text':' not worth tracking line numbers for dispatch entries','line_number':681,'multiline':False]
['text':' We only allow at most 3 levels of namespace for kernels.','line_number':693,'multiline':False]
['text':' We will append "native" to a custom kernel namespace.','line_number':694,'multiline':False]
['text':' Why is 'structured' included? External backends (e.g.','line_number':699,'multiline':False]
['text':' XLA) opt into which ops are structured independently','line_number':700,'multiline':False]
['text':' of which in-tree ops are structured','line_number':701,'multiline':False]
['text':' We count the number of dispatch keys which have not been ignored to prevent a dispatch table','line_number':714,'multiline':False]
['text':' in which all backend keys are ignored but necessarily kept, remaining compositeimplicit,','line_number':715,'multiline':False]
['text':' from being treated as redundant.','line_number':716,'multiline':False]
['text':' if a function is a structured delegate, deleting the dispatch','line_number':723,'multiline':False]
['text':' table is NOT semantics preserving','line_number':724,'multiline':False]
['text':' TODO: maybe it's better to test the return','line_number':740,'multiline':False]
['text':' Program the BackendIndex for the implicit dispatch entry from ufunc','line_number':807,'multiline':False]
['text':' Delay import ufunc here to avoid circular import issue','line_number':811,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/81294','line_number':812,'multiline':False]
['text':' Structured functions MUST have a dispatch table','line_number':826,'multiline':False]
['text':' We aren't going to store dispatch metadata inline in NativeFunctions;','line_number':853,'multiline':False]
['text':' instead it is separately indexed by backend (so other backends can','line_number':854,'multiline':False]
['text':' add more dispatch entries after the fact).  Reindex the individual','line_number':855,'multiline':False]
['text':' metadata by OperatorName!','line_number':856,'multiline':False]
['text':' don't care if it exists or not; make it easier to use this function','line_number':859,'multiline':False]
['text':' with other yaml parsers that aren't setting __line__ in the dict','line_number':860,'multiline':False]
['text':' Asserts that we can't do in post_init, because they rely on backend-specific info','line_number':864,'multiline':False]
['text':' TODO: probably better to accumulate these errors and report them all','line_number':903,'multiline':False]
['text':' at once','line_number':904,'multiline':False]
['text':' __post_init__ functions in dataclasses can be used to do extra','line_number':914,'multiline':False]
['text':' validation after construction.','line_number':915,'multiline':False]
['text':'','line_number':916,'multiline':False]
['text':' Notice that we don't do any type validation here.  In fact, we','line_number':917,'multiline':False]
['text':' rely exclusively on mypy to check if you've done types correctly!','line_number':918,'multiline':False]
['text':' Validation is for nontrivial invariants that cannot be (conveniently)','line_number':919,'multiline':False]
['text':' encoded in the type system.','line_number':920,'multiline':False]
['text':' Technically, with the asserts above, this assert is impossible to','line_number':945,'multiline':False]
['text':' happen','line_number':946,'multiline':False]
['text':' NB: if your function accidentally has rand/dropout/... in its name','line_number':965,'multiline':False]
['text':' but is not actually random, feel free to amend this to special case','line_number':966,'multiline':False]
['text':' Backwards of dropout is typically deterministic','line_number':976,'multiline':False]
['text':' See Note [resize_ in Functionalization] for more dtails','line_number':1001,'multiline':False]
['text':' A structured kernel is guaranteed to have a functional and out variant, and','line_number':1040,'multiline':False]
['text':' optionally an inplace variant.','line_number':1041,'multiline':False]
['text':'','line_number':1042,'multiline':False]
['text':' NB: we create NativeFunctionsGroup *even if* the function is not','line_number':1043,'multiline':False]
['text':' actually annotated structured.  Test the structured boolean to see if it','line_number':1044,'multiline':False]
['text':' actually is structured or not.','line_number':1045,'multiline':False]
['text':' Whether or not the operator has a meta() function. This information is backend-agnostic.','line_number':1055,'multiline':False]
['text':' See Note [Overload Ambiguity With Functional Variants]','line_number':1082,'multiline':False]
['text':' For now, structured composite kernels are not supported (need some','line_number':1086,'multiline':False]
['text':' design work to figure out how to make the composite case work)','line_number':1087,'multiline':False]
['text':' non-destructive updates please','line_number':1145,'multiline':False]
['text':' There are a few operators which only have functional/inplace variants;','line_number':1152,'multiline':False]
['text':' these don't count as structured for our purposes here','line_number':1153,'multiline':False]
['text':' assuming all variants have the same namespace','line_number':1156,'multiline':False]
['text':' The name of the backend kernel, for a given operator','line_number':1167,'multiline':False]
['text':' for in-tree backends. These names come directly from the 'dispatch" field','line_number':1168,'multiline':False]
['text':' in native_functions.yaml. The dispatch entry is optional; in that','line_number':1169,'multiline':False]
['text':' case, that is equivalent to having written:','line_number':1170,'multiline':False]
['text':'','line_number':1171,'multiline':False]
['text':'   dispatch:','line_number':1172,'multiline':False]
['text':'       CompositeImplicitAutograd: $operator_name','line_number':1173,'multiline':False]
['text':' Whether or not the operator has a structured kernel implemented, for this particular backend.','line_number':1175,'multiline':False]
['text':' For in-tree backends, they all have the same value for structured- this is listed','line_number':1176,'multiline':False]
['text':' in native_functions.yaml.','line_number':1177,'multiline':False]
['text':' However, external backends like XLA can indendently toggle which ops are structured.','line_number':1178,'multiline':False]
['text':' The namespace for kernels, default value: DEFAULT_KERNEL_NAMESPACE','line_number':1181,'multiline':False]
['text':' key is stored here because it affects the semantics of name,','line_number':1192,'multiline':False]
['text':' so its helpful to have them together for further processing','line_number':1193,'multiline':False]
['text':' BackendIndex represents a backend.','line_number':1209,'multiline':False]
['text':' The BackendIndex encodes per-operator information that is potentially different','line_number':1210,'multiline':False]
['text':' for each backend. The most obvious example is the name of the kernel','line_number':1211,'multiline':False]
['text':' (the 'dispatch' entry in native_functions.yaml).','line_number':1212,'multiline':False]
['text':' However, there can be other examples of different backends having different information.','line_number':1213,'multiline':False]
['text':' External backends can choose to opt their kernels to be structured independently from in-tree backends,','line_number':1214,'multiline':False]
['text':' which means that this information isn't inherently tied to a NativeFunction- it's different per backend.','line_number':1215,'multiline':False]
['text':' Mainly important for structured kernels, this determines which variant in the operator group is used to implement the others.','line_number':1219,'multiline':False]
['text':' All in-tree ops use out kernels, while XLA uses functional kernels.','line_number':1220,'multiline':False]
['text':' Whether the backend requires a device guard, and device checks.','line_number':1222,'multiline':False]
['text':' For in-tree backends, this is currently just CUDA/HIP','line_number':1223,'multiline':False]
['text':' For out-of-tree backends, this is currently just Intel XPU','line_number':1224,'multiline':False]
['text':' Whether the backend is in-tree (CPU/CUDA) or out-of-tree (XLA)','line_number':1226,'multiline':False]
['text':' Other backend-specific information that is on a per-operator basis','line_number':1228,'multiline':False]
['text':' TODO: This discrepancy isn't required; we could also generated','line_number':1270,'multiline':False]
['text':' a class for in-tree kernels. It'll just require carefully','line_number':1271,'multiline':False]
['text':' updating every kernel definition + callsite of every in-tree aten kernel.','line_number':1272,'multiline':False]
['text':' The function schema is undoubtedly the most important data structure','line_number':1276,'multiline':False]
['text':' in all of the codegen, as it defines the type signature for operators,','line_number':1277,'multiline':False]
['text':' and most of the code generation we do is type directed (e.g., look at','line_number':1278,'multiline':False]
['text':' the types, decide what to do.  Think about how we code generate','line_number':1279,'multiline':False]
['text':' C++ function stubs!)','line_number':1280,'multiline':False]
['text':'','line_number':1281,'multiline':False]
['text':' We will also see in this class the general structure for how we model','line_number':1282,'multiline':False]
['text':' data in this code generation.  A few notable properties to point out','line_number':1283,'multiline':False]
['text':' ahead of time:','line_number':1284,'multiline':False]
['text':'','line_number':1285,'multiline':False]
['text':'   - These dataclasses are a *lossless* representation of the strings','line_number':1286,'multiline':False]
['text':'     they are parsed from.  In fact, we assert that given the','line_number':1287,'multiline':False]
['text':'     information stored in the dataclass, we can exactly reconstruct','line_number':1288,'multiline':False]
['text':'     the string we parsed from (and assert this inside the parse','line_number':1289,'multiline':False]
['text':'     definition).  There are a few reasons for this:','line_number':1290,'multiline':False]
['text':'','line_number':1291,'multiline':False]
['text':'       - If you find that it is difficult to reconstruct the string','line_number':1292,'multiline':False]
['text':'         given a dataclass, that is a clue that you are data','line_number':1293,'multiline':False]
['text':'         representation is wrong.','line_number':1294,'multiline':False]
['text':'','line_number':1295,'multiline':False]
['text':'       - It helps ensure that all relevant information is present','line_number':1296,'multiline':False]
['text':'         in the dataclass, so that downstream users aren't tempted','line_number':1297,'multiline':False]
['text':'         to reparse the original string to get some information','line_number':1298,'multiline':False]
['text':'         that was omitted.','line_number':1299,'multiline':False]
['text':'','line_number':1300,'multiline':False]
['text':'       - It forces you to represent the data in-memory in the same way','line_number':1301,'multiline':False]
['text':'         it is recorded textually, which makes the dataclasses easier','line_number':1302,'multiline':False]
['text':'         to understand for someone who is familiar with the','line_number':1303,'multiline':False]
['text':'         textual format.  (As a tradeoff, it means you have to model','line_number':1304,'multiline':False]
['text':'         the syntax, even when it is inconvenient.  But maybe that means','line_number':1305,'multiline':False]
['text':'         the syntax is bad!)  If you don't understand the internal','line_number':1306,'multiline':False]
['text':'         representation, go look at the printing code to see how','line_number':1307,'multiline':False]
['text':'         it maps onto the surface syntax!','line_number':1308,'multiline':False]
['text':'','line_number':1309,'multiline':False]
['text':'       - It makes it easy to test the parsing code, as parsing code','line_number':1310,'multiline':False]
['text':'         that is inconsistent with the string code will fail early','line_number':1311,'multiline':False]
['text':'         and loudly.  (As a tradeoff, it makes the parsing code a bit','line_number':1312,'multiline':False]
['text':'         brittle (in particular, with trivial whitespace changes you','line_number':1313,'multiline':False]
['text':'         are likely to trigger an assert error).','line_number':1314,'multiline':False]
['text':'','line_number':1315,'multiline':False]
['text':'     In general, try to make the __str__ code as simple as possible','line_number':1316,'multiline':False]
['text':'     (even at the cost of more complex parsing logic.)  Additionally,','line_number':1317,'multiline':False]
['text':'     try to minimize redundancy in data representation.  (Precomputed','line_number':1318,'multiline':False]
['text':'     fields are OK though: they are defined as a simple function on','line_number':1319,'multiline':False]
['text':'     the canonical representation in question.)','line_number':1320,'multiline':False]
['text':'','line_number':1321,'multiline':False]
['text':'   - These dataclasses are all frozen; once constructed their','line_number':1322,'multiline':False]
['text':'     values never change.  This makes it easy to tell where any','line_number':1323,'multiline':False]
['text':'     given data came from: just look to the constructor.  As a','line_number':1324,'multiline':False]
['text':'     tradeoff, you can't easily "decorate" a schema with extra','line_number':1325,'multiline':False]
['text':'     information from a post-facto analysis.  We impose this','line_number':1326,'multiline':False]
['text':'     restriction to make these structures more understandable.','line_number':1327,'multiline':False]
['text':'','line_number':1328,'multiline':False]
['text':' The name of the operator this function schema describes.','line_number':1331,'multiline':False]
['text':' TODO: Need to handle collisions with argument names at some point','line_number':1336,'multiline':False]
['text':' We should probably get a proper parser here','line_number':1350,'multiline':False]
['text':' We assert earlier that schemas can't have a mix of aliased and non-aliased returns','line_number':1362,'multiline':False]
['text':' We also enforce that if you have any mutable, positional args, then they are not returned.','line_number':1375,'multiline':False]
['text':' This makes it easier to group these functions properly with their functional/out= counterparts.','line_number':1376,'multiline':False]
['text':' Invariant: we expect out arguments to appear as keyword arguments in the schema.','line_number':1381,'multiline':False]
['text':' This means that all mutable returns should be aliased to a keyword argument','line_number':1382,'multiline':False]
['text':' (except for "self", which we explicitly don't treat as an out argument because of its use in methods)','line_number':1383,'multiline':False]
['text':' See Note [is_out_fn]','line_number':1384,'multiline':False]
['text':' Some assertions: We don't want any functions with a return type of "-> (Tensor(a!), Tensor)",','line_number':1398,'multiline':False]
['text':' because:','line_number':1399,'multiline':False]
['text':' (1) It's more annoying to handle properly','line_number':1400,'multiline':False]
['text':' (2) It's unnecessary - you can't method-chain on the first (mutated) output because it's part of a tuple.','line_number':1401,'multiline':False]
['text':' Instead, we expect the (a!) argument to not be returned.','line_number':1402,'multiline':False]
['text':' out= ops that return their mutable inputs are only really useful for method chaining.','line_number':1412,'multiline':False]
['text':' And method chaining is only really useful if the thing you're returning is a plain Tensor.','line_number':1413,'multiline':False]
['text':' So ideally, we'd enforce that out= ops with a single plain mutable tensor should return the tensor,','line_number':1414,'multiline':False]
['text':' and all other types of out= op schemas should return void.','line_number':1415,'multiline':False]
['text':' There are a bunch of existing out= ops that return tuples of tensors though, so we're stuck with allowing that.','line_number':1416,'multiline':False]
['text':' mutable keyword arguments whose name has _scratch_ prefix are','line_number':1423,'multiline':False]
['text':' scratch tensors for memory planning and should not be returned','line_number':1424,'multiline':False]
['text':' All inplace ops with an ordinary `Tensor self` argument should return self,','line_number':1443,'multiline':False]
['text':' to allow for method chaining.','line_number':1444,'multiline':False]
['text':' You can't method chain on non-tensor self arguments though (like a List[Tensor])','line_number':1450,'multiline':False]
['text':' so in all other cases we expect the return type to be none.','line_number':1451,'multiline':False]
['text':' Note [is_out_fn]','line_number':1471,'multiline':False]
['text':'','line_number':1472,'multiline':False]
['text':' out functions are the variants which take an explicit out= argument','line_number':1473,'multiline':False]
['text':' to populate into.  We need to know if a schema corresponds to an','line_number':1474,'multiline':False]
['text':' out function for several reasons:','line_number':1475,'multiline':False]
['text':'','line_number':1476,'multiline':False]
['text':'   - They codegen differently in C++ API','line_number':1477,'multiline':False]
['text':'       - codegen to at::add_out rather than at::add','line_number':1478,'multiline':False]
['text':'       - out argument is moved to front of C++ argument list','line_number':1479,'multiline':False]
['text':'','line_number':1480,'multiline':False]
['text':' out functions are DEFINED to be any function with a keyword-only','line_number':1481,'multiline':False]
['text':' argument that is mutable.  In principle, this could lead to a','line_number':1482,'multiline':False]
['text':' false positive if you define a function that mutates a','line_number':1483,'multiline':False]
['text':' kwarg only argument, but this isn't the "true" output of this','line_number':1484,'multiline':False]
['text':' function.  A more robust definition that would work in this','line_number':1485,'multiline':False]
['text':' case would also look at:','line_number':1486,'multiline':False]
['text':'','line_number':1487,'multiline':False]
['text':'   - The output types.  Out functions take in the arguments','line_number':1488,'multiline':False]
['text':'     they mutate and then return them again; this is sort','line_number':1489,'multiline':False]
['text':'     of "definitionally" what makes something an out function.','line_number':1490,'multiline':False]
['text':'     Historically, we DO check this for consistency.','line_number':1491,'multiline':False]
['text':'   - Correspondence with pure variant.  An out function','line_number':1492,'multiline':False]
['text':'     should have a signature equivalent to its pure variant,','line_number':1493,'multiline':False]
['text':'     but just with extra kwargs for the output elements.  This','line_number':1494,'multiline':False]
['text':'     is difficult to actually check for and historically','line_number':1495,'multiline':False]
['text':'     we only do this check in tools/','line_number':1496,'multiline':False]
['text':' out= and inplace schemas can also have post_self_positional mutable args,','line_number':1516,'multiline':False]
['text':' but we give precedence to out= and inplace when deciding the schema kind.','line_number':1517,'multiline':False]
['text':' Tradeoff: we probably don't want to have to teach codegen that looks at inplace ops','line_number':1518,'multiline':False]
['text':' to also worry about mutable post_self_positional arguments,','line_number':1519,'multiline':False]
['text':' but it seems like a much bigger lift to classify them has having a new schema kind.','line_number':1520,'multiline':False]
['text':' The number of ops that fit in this strange category is small enough that','line_number':1521,'multiline':False]
['text':' we can probably manually write code for them instead of forcing the codegen to handle them.','line_number':1522,'multiline':False]
['text':' For every return:','line_number':1540,'multiline':False]
['text':' - If the return aliases an input, we return the input name','line_number':1541,'multiline':False]
['text':' - Otherwise, we return None.','line_number':1542,'multiline':False]
['text':' If return names were enforced to be consistent with aliasing information, then we wouldn't need this.','line_number':1543,'multiline':False]
['text':' find mutable inputs that are not originally returned, and convert them to returns','line_number':1618,'multiline':False]
['text':' When we're grouping functions we strip the return names,','line_number':1620,'multiline':False]
['text':' but when we're generating the actual functional variants then we follow','line_number':1621,'multiline':False]
['text':' a convention for what to name the returns','line_number':1622,'multiline':False]
['text':' Order is important here (otherwise e.g. inplace with mutable args','line_number':1629,'multiline':False]
['text':' and out= with mutable args won't have the same signature)','line_number':1630,'multiline':False]
['text':' Ordering is important here. We expect the "mutable input" returns to come last.','line_number':1642,'multiline':False]
['text':' See Note [bernoulli.p schema]','line_number':1646,'multiline':False]
['text':' stripped','line_number':1657,'multiline':False]
['text':' omit parentheses','line_number':1683,'multiline':False]
['text':' Here is the rest of the data model, described more briefly.','line_number':1689,'multiline':False]
['text':' Simplified version for what actually shows up in built-ins.','line_number':1692,'multiline':False]
['text':' Look at alias_info.h for expanded syntax.  If you need the structure,','line_number':1693,'multiline':False]
['text':' you also need to make this structure recursive so it can be lined','line_number':1694,'multiline':False]
['text':' up with the type components too.  For primitives this isn't really','line_number':1695,'multiline':False]
['text':' necessary','line_number':1696,'multiline':False]
['text':' Typically only has one element.  Not actually a set so','line_number':1699,'multiline':False]
['text':' we can conveniently assume it is canonically ordered','line_number':1700,'multiline':False]
['text':' TODO: implement a proper parser if this gets more ugly','line_number':1707,'multiline':False]
['text':' Regex Explanation:','line_number':1708,'multiline':False]
['text':' Example: "a! -> a|b"','line_number':1709,'multiline':False]
['text':' Group #1: alias before optional '|', required. Matches the first','line_number':1710,'multiline':False]
['text':'   character 'a' in the example','line_number':1711,'multiline':False]
['text':' Group #2: optional alias set after optional '|', matches empty string','line_number':1712,'multiline':False]
['text':'   in the example','line_number':1713,'multiline':False]
['text':' Group #3: optional "is write" flag, matches '!' in the example.','line_number':1714,'multiline':False]
['text':' Group #4: optional section containing arrow, matches " -> a|b" in the','line_number':1715,'multiline':False]
['text':'   example.','line_number':1716,'multiline':False]
['text':' Group #5: optional alias after set, supports wildcard, matches "a|b"','line_number':1717,'multiline':False]
['text':'   in the example.','line_number':1718,'multiline':False]
['text':' Group #6: optional sub-section of alias after set, matches "|b" in the','line_number':1719,'multiline':False]
['text':'   example.','line_number':1720,'multiline':False]
['text':' The base class for the type system.  This is also loosely modeled','line_number':1750,'multiline':False]
['text':' off of jit_type.h, but we've simplified the hierarchy to focus','line_number':1751,'multiline':False]
['text':' in on the aspects of the type system that matter for code generation','line_number':1752,'multiline':False]
['text':' (for example, there's no SingleElementType subclass anymore).','line_number':1753,'multiline':False]
['text':' You never actually construct a Type; usually it's going to be one','line_number':1754,'multiline':False]
['text':' of the subclasses.  If Python had ADTs this would be one!','line_number':1755,'multiline':False]
['text':' '__torch__.torch.classes.' is the prefix for custom class','line_number':1774,'multiline':False]
['text':' WARNING: These concepts are not very well-defined.  For example,','line_number':1786,'multiline':False]
['text':' is "int?" nullable? How about "int?[]".  They are defined','line_number':1787,'multiline':False]
['text':' so we can conveniently generate legacy Declarations.yaml but','line_number':1788,'multiline':False]
['text':' really we should probably just remove these at some point','line_number':1789,'multiline':False]
['text':' Base types are simple, atomic types with no further structure','line_number':1810,'multiline':False]
['text':' TODO: rename','line_number':1830,'multiline':False]
['text':' Optional types may be specified, or may also be validly given None','line_number':1853,'multiline':False]
['text':' A type representing a PyTorch custom class','line_number':1874,'multiline':False]
['text':' List types specify that we may have multiples of an element.  We','line_number':1901,'multiline':False]
['text':' also support explicit sizes on list types, but these have','line_number':1902,'multiline':False]
['text':' some nontrivial semantics!  (However, for C++ API purposes, explicit','line_number':1903,'multiline':False]
['text':' sizes are mostly erased from the type system.)','line_number':1904,'multiline':False]
['text':'','line_number':1905,'multiline':False]
['text':' DANGER WILL ROBINSON: C++ elaboration depends on elem type; e.g.,','line_number':1906,'multiline':False]
['text':' int[] elaborates differently than bool[3]!','line_number':1907,'multiline':False]
['text':' NB: I didn't put kwarg_only as a boolean field here, unlike','line_number':1932,'multiline':False]
['text':' c10::Argument, so that printing works correctly','line_number':1933,'multiline':False]
['text':' The semantics of the annotation field are a little strange.','line_number':1939,'multiline':False]
['text':'','line_number':1940,'multiline':False]
['text':' Alias annotations parametrize Tensors (since Tensors are the only things','line_number':1941,'multiline':False]
['text':' that can alias.)  This motivates why I write Tensor(a!)?  (and not, for','line_number':1942,'multiline':False]
['text':' example, Tensor?(a!)), because the (a!) describes aliasing on the tensor,','line_number':1943,'multiline':False]
['text':' which may be optional (i.e., the alias annotation should bind first to','line_number':1944,'multiline':False]
['text':' Tensor, before the optional postfix annotation).','line_number':1945,'multiline':False]
['text':'','line_number':1946,'multiline':False]
['text':' However, despite being a property of Tensor, we (and c10::Argument)','line_number':1947,'multiline':False]
['text':' store the annotation at the top level of the Argument, rather than','line_number':1948,'multiline':False]
['text':' inside the embedded Tensor type.  In the C++ version of this','line_number':1949,'multiline':False]
['text':' class, we then go through great lengths to mimic the type','line_number':1950,'multiline':False]
['text':' structure in the annotation structure so we can correlate','line_number':1951,'multiline':False]
['text':' annotations with types.','line_number':1952,'multiline':False]
['text':'','line_number':1953,'multiline':False]
['text':' Now, it turns out, in all applications in code generation, the','line_number':1954,'multiline':False]
['text':' structure of annotated types is very simple.  So we just hard','line_number':1955,'multiline':False]
['text':' code it here.  But if we ever do get anything more complex, this','line_number':1956,'multiline':False]
['text':' model will have to change!','line_number':1957,'multiline':False]
['text':' TODO: deduplicate annotation matching with Return','line_number':1970,'multiline':False]
['text':' If you update this, make sure the __str__ still works too','line_number':1974,'multiline':False]
['text':' If you update this, make sure the __str__ still works too','line_number':2030,'multiline':False]
['text':' Represents the self argument for functions that may be methods','line_number':2065,'multiline':False]
['text':' Bundle of arguments that represent a TensorOptions.  This is mostly','line_number':2071,'multiline':False]
['text':' relevant for the public C++ API but we bake it into the core data','line_number':2072,'multiline':False]
['text':' model because other APIs often have to interact with it','line_number':2073,'multiline':False]
['text':' pre_self_positional is usually empty, but is notably non-empty','line_number':2087,'multiline':False]
['text':' for where.self, where the condition argument comes before the','line_number':2088,'multiline':False]
['text':' self argument','line_number':2089,'multiline':False]
['text':' post_tensor_options is typically memory format, which should be','line_number':2096,'multiline':False]
['text':' part of tensor options but isn't right now, and is usually','line_number':2097,'multiline':False]
['text':' placed after the tensor options arguments','line_number':2098,'multiline':False]
['text':' Unlike in the previous codegen, we have factored out 'out' arguments','line_number':2101,'multiline':False]
['text':' in the canonical representation, removing them from kwarg','line_number':2102,'multiline':False]
['text':' arguments.  This choice is justified by numerous downstream','line_number':2103,'multiline':False]
['text':' transformations which treat out arguments specially; additionally,','line_number':2104,'multiline':False]
['text':' you can see that canonicity is not violated!','line_number':2105,'multiline':False]
['text':' these are also kwarg-only','line_number':2106,'multiline':False]
['text':' NB: doesn't contain out arguments','line_number':2128,'multiline':False]
['text':' dataclasses.replace could be used here, but it is less','line_number':2198,'multiline':False]
['text':' type safe so for now I've opted to type everything out','line_number':2199,'multiline':False]
['text':' Since TensorOptions are dropped, the post_tensor_options_kwargs are','line_number':2218,'multiline':False]
['text':' converted to pre_tensor_options_kwargs','line_number':2219,'multiline':False]
['text':' TensorOptions are dropped in signature,','line_number':2224,'multiline':False]
['text':' so we can pair factory functions with their out= variants.','line_number':2225,'multiline':False]
['text':' out arguments are dropped in signature','line_number':2228,'multiline':False]
['text':' TODO: Use a real parser here; this will get bamboozled','line_number':2255,'multiline':False]
['text':' by signatures that contain things like std::array<bool, 2> (note the space)','line_number':2256,'multiline':False]
['text':' Currently, we rely directly on the invariant that there are NO','line_number':2267,'multiline':False]
['text':' kwarg-only mutating arguments.  If you want to relax this,','line_number':2268,'multiline':False]
['text':' we will need a more semantic way of matching that takes','line_number':2269,'multiline':False]
['text':' into account return arguments.  In that case, you will have','line_number':2270,'multiline':False]
['text':' to manage out computation a level up, in FunctionSchema.  See Note','line_number':2271,'multiline':False]
['text':' [is_out_fn]','line_number':2272,'multiline':False]
['text':' do nothing','line_number':2275,'multiline':False]
['text':' We do this in two phases.  First we parse into three','line_number':2290,'multiline':False]
['text':' main categories: positional, kwarg_only, out.','line_number':2291,'multiline':False]
['text':' Then, we reparse positional and kwarg_only to separate','line_number':2292,'multiline':False]
['text':' out the self argument and tensor options arguments.','line_number':2293,'multiline':False]
['text':' Split self argument','line_number':2297,'multiline':False]
['text':' Group tensor options arguments','line_number':2315,'multiline':False]
['text':' order matters','line_number':2324,'multiline':False]
['text':' If there is enough space...','line_number':2333,'multiline':False]
['text':' And the next len(predicates) arguments look like TensorOptions arguments','line_number':2335,'multiline':False]
['text':' Group them together as one argument','line_number':2341,'multiline':False]
['text':' TODO: These invariants are weirdly asymmetric?','line_number':2374,'multiline':False]
['text':' TODO: Fancier types?','line_number':2375,'multiline':False]
['text':' We don't allow any of the following to have argument annotations,','line_number':2381,'multiline':False]
['text':' to keep things simple.','line_number':2382,'multiline':False]
['text':' Names that validly are __iXXX__ indicating inplace operations.','line_number':2393,'multiline':False]
['text':' Taken from https://www.python.org/dev/peps/pep-0203/#new-methods','line_number':2394,'multiline':False]
['text':' NB: PyTorch hasn't actually implemented all of these','line_number':2395,'multiline':False]
['text':' A BaseOperatorName is what we think of the operator name, without','line_number':2411,'multiline':False]
['text':' the overload name.  Unusually, we don't represent this as just a','line_number':2412,'multiline':False]
['text':' string; instead, we directly represent a few important semantic','line_number':2413,'multiline':False]
['text':' bits of information we derive from the string: namely whether','line_number':2414,'multiline':False]
['text':' or not it's inplace (add_) and whether or not it's a double-underscore','line_number':2415,'multiline':False]
['text':' method (__add__)','line_number':2416,'multiline':False]
['text':' Note [Overload Ambiguity With Functional Variants]','line_number':2422,'multiline':False]
['text':' A handful of operators have both a "mutable" and a "functional" variant.','line_number':2423,'multiline':False]
['text':' (native_batch_norm is a good example, although this isn't the case today).','line_number':2424,'multiline':False]
['text':' For those operators, the mutable and functional variant take in the same set of','line_number':2425,'multiline':False]
['text':' arguments, but have different alias annotations.','line_number':2426,'multiline':False]
['text':' this makes it ambiguous when you try to resolve an OverloadPacket into an overload,','line_number':2427,'multiline':False]
['text':' given a set of input arguments.','line_number':2428,'multiline':False]
['text':'','line_number':2429,'multiline':False]
['text':' So instead of making the "functional" variant in this case a real overload, e.g:','line_number':2430,'multiline':False]
['text':'   native_batch_norm (mutable variant)','line_number':2431,'multiline':False]
['text':'   native_batch_norm.functional (functional variant)','line_number':2432,'multiline':False]
['text':' we make it a new base operator,','line_number':2433,'multiline':False]
['text':'   native_batch_norm_functional (functional variant)','line_number':2434,'multiline':False]
['text':'','line_number':2435,'multiline':False]
['text':' In an ideal world, we would probably invert this so the operators were:','line_number':2436,'multiline':False]
['text':'   native_batch_norm.mutable (mutable variant)','line_number':2437,'multiline':False]
['text':'   native_batch_norm (functional variant)','line_number':2438,'multiline':False]
['text':'','line_number':2439,'multiline':False]
['text':' Doing that is BC-breaking though, so we're stuck with the above modeling.','line_number':2440,'multiline':False]
['text':' temporary, this is not intrinsically true but','line_number':2459,'multiline':False]
['text':' has been historically true for dunder methods','line_number':2460,'multiline':False]
['text':' we support  (but, if we ever got, say, __int__, this would','line_number':2461,'multiline':False]
['text':' be wrong!)','line_number':2462,'multiline':False]
['text':' See Note [Overload Ambiguity With Functional Variants]','line_number':2473,'multiline':False]
['text':' This seems complicated and unnecessary, so banning dunder methods','line_number':2478,'multiline':False]
['text':' for now on ops that have a functional + mutable variant (like native_batch_norm).','line_number':2479,'multiline':False]
['text':' Operator name is the base operator name along with the (typically not','line_number':2508,'multiline':False]
['text':' user visible) overload string.','line_number':2509,'multiline':False]
['text':' NB: This must be synchronized with the naming scheme in','line_number':2532,'multiline':False]
['text':' aten/src/ATen/templates/Operators.h','line_number':2533,'multiline':False]
['text':' Given a function schema "aten::op.overload(...)",','line_number':2534,'multiline':False]
['text':' If there is no overload name, this returns f"{op}"','line_number':2535,'multiline':False]
['text':' If there is an overload name, this returns f"{op}_{overload}"','line_number':2536,'multiline':False]
['text':' NativeFunction objects that are views (f.is_view_op returns True)','line_number':2574,'multiline':False]
['text':' are added into a `NativeFunctionsViewGroup`, which we can use to','line_number':2575,'multiline':False]
['text':' easily access the generated (optional) view_copy NativeFunction.','line_number':2576,'multiline':False]
['text':' It's convenient to group them together, so we pair them up in NativeFunctionsViewGroup.','line_number':2577,'multiline':False]
['text':' See Note [Codegen'd {view}_copy Operators]','line_number':2578,'multiline':False]
['text':'','line_number':2579,'multiline':False]
['text':' One property of this representation is that in order for a view-like op to be part of','line_number':2580,'multiline':False]
['text':' a NativeFunctionsViewGroup, the "aliasing" version of that view op must exist.','line_number':2581,'multiline':False]
['text':' There's one case where that doesn't happen: we have a non-aliasing `narrow_copy.out` op,','line_number':2582,'multiline':False]
['text':' but don't have corresponding aliasing `narrow.out` op.','line_number':2583,'multiline':False]
['text':' This means that `narrow_copy.out` won't appear as a NativeFunctionsViewGroup.','line_number':2584,'multiline':False]
['text':' Note: the {view}_copy operator is optional because we currently don't generate copy variants','line_number':2588,'multiline':False]
['text':' for all view ops. Notably, we don't generate them for CompositeImplicitAutograd views','line_number':2589,'multiline':False]
['text':' (we already get them "for free" through decomposition)','line_number':2590,'multiline':False]
['text':' view_inplace ops are also optional, but every view_inplace op should have out-of-place variant.','line_number':2592,'multiline':False]
['text':' We currently assert that the "group" is consistent.','line_number':2645,'multiline':False]
['text':' If the view op is composite, then its view_inplace op is too.','line_number':2646,'multiline':False]
['text':' Only aliasing (view) operators get a copy variant.','line_number':2651,'multiline':False]
['text':' We don't need to bother generating copy variants for CompositeImplicitAutograd ops,','line_number':2654,'multiline':False]
['text':' because we can let them decompose into base view ops.','line_number':2655,'multiline':False]
['text':' We also don't need to generate copy variants for inplace views.','line_number':2658,'multiline':False]
['text':' Given a NativeFunction that corresponds to a view op,','line_number':2664,'multiline':False]
['text':' returns the OperatorName of the corresponding "copy" variant of the op.','line_number':2665,'multiline':False]
['text':' Right now, when asking for a view op's corresponding "view_copy" name','line_number':2667,'multiline':False]
['text':' we assert for sanity that the op is allowed to have a generated view_copy variant.','line_number':2668,'multiline':False]
['text':' (We can do this because "gets_generated_view_copy()" tell us which ops get a generated view_copy op).','line_number':2669,'multiline':False]
['text':' However, narrow_copy() already exists as an op directly in native_functions.yaml.','line_number':2670,'multiline':False]
['text':' I'm hardcoding narrow_copy here for now to maintain the assert,','line_number':2671,'multiline':False]
['text':' But we could also just get rid of the assert.','line_number':2672,'multiline':False]
['text':' Helper functions for parsing argument lists (both inputs and returns)','line_number':2687,'multiline':False]
['text':' A Precompute instance consists of a map from kernel argument name','line_number':2702,'multiline':False]
['text':' to the list of Argument instances that should replace that','line_number':2703,'multiline':False]
['text':' kernel argument in the impl function.','line_number':2704,'multiline':False]
['text':' A map from kernel argument name -> a list of precomputed','line_number':2707,'multiline':False]
['text':' elements that replaces/supersedes it.','line_number':2708,'multiline':False]
['text':' List of precomputed args added without replacement','line_number':2710,'multiline':False]
['text':' src is a list of strings of the format:','line_number':2717,'multiline':False]
['text':'   {kernel param name} -> {replacement decl}[, {replacement decl}, ...]','line_number':2718,'multiline':False]
['text':'   [{add decl}[, {add decl}, ...]]','line_number':2719,'multiline':False]
['text':' The last line is optional and contains the precomputed parameters that are','line_number':2720,'multiline':False]
['text':' added without replacement.','line_number':2721,'multiline':False]
['text':' The other lines are parsed to get the names of which precomputed elements','line_number':2722,'multiline':False]
['text':' should replace which kernel arguments.','line_number':2723,'multiline':False]
['text':' the template parameters are upper so if these are the','line_number':2748,'multiline':False]
['text':' same then it is ambiguous','line_number':2749,'multiline':False]
