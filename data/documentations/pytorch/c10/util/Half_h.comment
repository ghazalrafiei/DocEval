['text':'/ Defines the Half type (half-precision floating-point) including conversions','line_number':3,'multiline':False]
['text':'/ to standard C types and basic arithmetic operations. Note that arithmetic','line_number':4,'multiline':False]
['text':'/ operations are implemented by converting to floating point and','line_number':5,'multiline':False]
['text':'/ performing the operation in float32, instead of using CUDA half intrinsics.','line_number':6,'multiline':False]
['text':'/ Most uses of this type within ATen are memory bound, including the','line_number':7,'multiline':False]
['text':'/ element-wise kernels, and the half intrinsics aren't efficient on all GPUs.','line_number':8,'multiline':False]
['text':'/ If you are writing a compute bound kernel, you can use the CUDA half','line_number':9,'multiline':False]
['text':'/ intrinsics directly on the Half type from device code.','line_number':10,'multiline':False]
['text':' for SYCL 1.2.1','line_number':50,'multiline':False]
['text':' for SYCL 2020','line_number':52,'multiline':False]
['text':' operator typeid','line_number':55,'multiline':False]
['text':'
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 ','line_number':61,'multiline':True]
['text':'
   * Extend the half-precision floating-point number to 32 bits and shift to the
   * upper part of the 32-bit word:
   *      +---+-----+------------+-------------------+
   *      | S |EEEEE|MM MMMM MMMM|0000 0000 0000 0000|
   *      +---+-----+------------+-------------------+
   * Bits  31  26-30    16-25            0-15
   *
   * S - sign bit, E - bits of the biased exponent, M - bits of the mantissa, 0
   * - zero bits.
   ','line_number':69,'multiline':True]
['text':'
   * Extract the sign of the input number into the high bit of the 32-bit word:
   *
   *      +---+----------------------------------+
   *      | S |0000000 00000000 00000000 00000000|
   *      +---+----------------------------------+
   * Bits  31                 0-31
   ','line_number':81,'multiline':True]
['text':'
   * Extract mantissa and biased exponent of the input number into the bits 0-30
   * of the 32-bit word:
   *
   *      +---+-----+------------+-------------------+
   *      | 0 |EEEEE|MM MMMM MMMM|0000 0000 0000 0000|
   *      +---+-----+------------+-------------------+
   * Bits  30  27-31     17-26            0-16
   ','line_number':90,'multiline':True]
['text':'
   * Renorm shift is the number of bits to shift mantissa left to make the
   * half-precision number normalized. If the initial number is normalized, some
   * of its high 6 bits (sign == 0 and 5-bit exponent) equals one. In this case
   * renorm_shift == 0. If the number is denormalize, renorm_shift > 0. Note
   * that if we shift denormalized nonsign by renorm_shift, the unit bit of
   * mantissa will shift into exponent, turning the biased exponent into 1, and
   * making mantissa normalized (i.e. without leading 1).
   ','line_number':100,'multiline':True]
['text':'
   * Iff half-precision number has exponent of 15, the addition overflows
   * it into bit 31, and the subsequent shift turns the high 9 bits
   * into 1. Thus inf_nan_mask == 0x7F800000 if the half-precision number
   * had exponent of 15 (i.e. was NaN or infinity) 0x00000000 otherwise
   ','line_number':117,'multiline':True]
['text':'
   * Iff nonsign is 0, it overflows into 0xFFFFFFFF, turning bit 31
   * into 1. Otherwise, bit 31 remains 0. The signed shift right by 31
   * broadcasts bit 31 into all bits of the zero_mask. Thus zero_mask ==
   * 0xFFFFFFFF if the half-precision number was zero (+0.0h or -0.0h)
   * 0x00000000 otherwise
   ','line_number':125,'multiline':True]
['text':'
   * 1. Shift nonsign left by renorm_shift to normalize it (if the input
   * was denormal)
   * 2. Shift nonsign right by 3 so the exponent (5 bits originally)
   * becomes an 8-bit field and 10-bit mantissa shifts into the 10 high
   * bits of the 23-bit mantissa of IEEE single-precision number.
   * 3. Add 0x70 to the exponent (starting at bit 23) to compensate the
   * different in exponent bias (0x7F for single-precision number less 0xF
   * for half-precision number).
   * 4. Subtract renorm_shift from the exponent (starting at bit 23) to
   * account for renormalization. As renorm_shift is less than 0x70, this
   * can be combined with step 3.
   * 5. Binary OR with inf_nan_mask to turn the exponent into 0xFF if the
   * input was NaN or infinity.
   * 6. Binary ANDNOT with zero_mask to turn the mantissa and exponent
   * into zero if the input was zero.
   * 7. Combine with the sign of the input number.
   ','line_number':133,'multiline':True]
['text':'
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 ','line_number':157,'multiline':True]
['text':'
   * Extend the half-precision floating-point number to 32 bits and shift to the
   * upper part of the 32-bit word:
   *      +---+-----+------------+-------------------+
   *      | S |EEEEE|MM MMMM MMMM|0000 0000 0000 0000|
   *      +---+-----+------------+-------------------+
   * Bits  31  26-30    16-25            0-15
   *
   * S - sign bit, E - bits of the biased exponent, M - bits of the mantissa, 0
   * - zero bits.
   ','line_number':167,'multiline':True]
['text':'
   * Extract the sign of the input number into the high bit of the 32-bit word:
   *
   *      +---+----------------------------------+
   *      | S |0000000 00000000 00000000 00000000|
   *      +---+----------------------------------+
   * Bits  31                 0-31
   ','line_number':179,'multiline':True]
['text':'
   * Extract mantissa and biased exponent of the input number into the high bits
   * of the 32-bit word:
   *
   *      +-----+------------+---------------------+
   *      |EEEEE|MM MMMM MMMM|0 0000 0000 0000 0000|
   *      +-----+------------+---------------------+
   * Bits  27-31    17-26            0-16
   ','line_number':188,'multiline':True]
['text':'
   * Shift mantissa and exponent into bits 23-28 and bits 13-22 so they become
   * mantissa and exponent of a single-precision floating-point number:
   *
   *       S|Exponent |          Mantissa
   *      +-+---+-----+------------+----------------+
   *      |0|000|EEEEE|MM MMMM MMMM|0 0000 0000 0000|
   *      +-+---+-----+------------+----------------+
   * Bits   | 23-31   |           0-22
   *
   * Next, there are some adjustments to the exponent:
   * - The exponent needs to be corrected by the difference in exponent bias
   * between single-precision and half-precision formats (0x7F - 0xF = 0x70)
   * - Inf and NaN values in the inputs should become Inf and NaN values after
   * conversion to the single-precision number. Therefore, if the biased
   * exponent of the half-precision input was 0x1F (max possible value), the
   * biased exponent of the single-precision output must be 0xFF (max possible
   * value). We do this correction in two steps:
   *   - First, we adjust the exponent by (0xFF - 0x1F) = 0xE0 (see exp_offset
   * below) rather than by 0x70 suggested by the difference in the exponent bias
   * (see above).
   *   - Then we multiply the single-precision result of exponent adjustment by
   * 2**(-112) to reverse the effect of exponent adjustment by 0xE0 less the
   * necessary exponent adjustment by 0x70 due to difference in exponent bias.
   *     The floating-point multiplication hardware would ensure than Inf and
   * NaN would retain their value on at least partially IEEE754-compliant
   * implementations.
   *
   * Note that the above operations do not handle denormal inputs (where biased
   * exponent == 0). However, they also do not operate on denormal inputs, and
   * do not produce denormal results.
   ','line_number':199,'multiline':True]
['text':' const float exp_scale = 0x1.0p-112f;','line_number':232,'multiline':False]
['text':'
   * Convert denormalized half-precision inputs into single-precision results
   * (always normalized). Zero inputs are also handled here.
   *
   * In a denormalized number the biased exponent is zero, and mantissa has
   * on-zero bits. First, we shift mantissa into bits 0-9 of the 32-bit word.
   *
   *                  zeros           |  mantissa
   *      +---------------------------+------------+
   *      |0000 0000 0000 0000 0000 00|MM MMMM MMMM|
   *      +---------------------------+------------+
   * Bits             10-31                0-9
   *
   * Now, remember that denormalized half-precision numbers are represented as:
   *    FP16 = mantissa * 2**(-24).
   * The trick is to construct a normalized single-precision number with the
   * same mantissa and thehalf-precision input and with an exponent which would
   * scale the corresponding mantissa bits to 2**(-24). A normalized
   * single-precision floating-point number is represented as: FP32 = (1 +
   * mantissa * 2**(-23)) * 2**(exponent - 127) Therefore, when the biased
   * exponent is 126, a unit change in the mantissa of the input denormalized
   * half-precision number causes a change of the constructed single-precision
   * number by 2**(-24), i.e. the same amount.
   *
   * The last step is to adjust the bias of the constructed single-precision
   * number. When the input half-precision number is zero, the constructed
   * single-precision number has the value of FP32 = 1 * 2**(126 - 127) =
   * 2**(-1) = 0.5 Therefore, we need to subtract 0.5 from the constructed
   * single-precision number to get the numerical equivalent of the input
   * half-precision number.
   ','line_number':240,'multiline':True]
['text':'
   * - Choose either results of conversion of input as a normalized number, or
   * as a denormalized number, depending on the input exponent. The variable
   * two_w contains input exponent in bits 27-31, therefore if its smaller than
   * 2**27, the input is either a denormal number, or zero.
   * - Combine the result of conversion of exponent and mantissa with the sign
   * of the input number.
   ','line_number':276,'multiline':True]
['text':'
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 16-bit floating-point number in IEEE half-precision format, in bit
 * representation.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 ','line_number':291,'multiline':True]
['text':' const float scale_to_inf = 0x1.0p+112f;','line_number':301,'multiline':False]
['text':' const float scale_to_zero = 0x1.0p-110f;','line_number':302,'multiline':False]
['text':' namespace detail','line_number':336,'multiline':False]
['text':' HIP wants __host__ __device__ tag, CUDA does not','line_number':346,'multiline':False]
['text':' TODO : move to complex.h','line_number':367,'multiline':False]
['text':' Constructors','line_number':373,'multiline':False]
['text':' Half constructor is not constexpr so the following constructor can't','line_number':375,'multiline':False]
['text':' be constexpr','line_number':376,'multiline':False]
['text':' Conversion operator','line_number':382,'multiline':False]
['text':' In some versions of MSVC, there will be a compiler error when building.','line_number':417,'multiline':False]
['text':' C4146: unary minus operator applied to unsigned type, result still unsigned','line_number':418,'multiline':False]
['text':' C4804: unsafe use of type 'bool' in operation','line_number':419,'multiline':False]
['text':' It can be addressed by disabling the following warning.','line_number':420,'multiline':False]
['text':' The overflow checks may involve float to int conversion which may','line_number':428,'multiline':False]
['text':' trigger precision loss warning. Re-enable the warning once the code','line_number':429,'multiline':False]
['text':' is fixed. See T58053069.','line_number':430,'multiline':False]
['text':' bool can be converted to any type.','line_number':436,'multiline':False]
['text':' Without specializing on bool, in pytorch_linux_trusty_py2_7_9_build:','line_number':437,'multiline':False]
['text':' `error: comparison of constant '255' with boolean expression is always false`','line_number':438,'multiline':False]
['text':' for `f > limit::max()` below','line_number':439,'multiline':False]
['text':'f','line_number':441,'multiline':True]
['text':' skip isnan and isinf check for integral types','line_number':445,'multiline':False]
['text':' allow for negative numbers to wrap using two's complement arithmetic.','line_number':451,'multiline':False]
['text':' For example, with uint8, this allows for `a - b` to be treated as','line_number':452,'multiline':False]
['text':' `a + 255 * b`.','line_number':453,'multiline':False]
['text':' casts from complex to real are considered to overflow if the','line_number':481,'multiline':False]
['text':' imaginary component is non-zero','line_number':482,'multiline':False]
['text':' Check for overflow componentwise','line_number':486,'multiline':False]
['text':' (Technically, the imag overflow check is guaranteed to be false','line_number':487,'multiline':False]
['text':' when !is_complex<To>, but any optimizer worth its salt will be','line_number':488,'multiline':False]
['text':' able to figure it out.)','line_number':489,'multiline':False]
['text':' namespace c10','line_number':500,'multiline':False]
['text':' IWYU pragma: keep','line_number':502,'multiline':False]
