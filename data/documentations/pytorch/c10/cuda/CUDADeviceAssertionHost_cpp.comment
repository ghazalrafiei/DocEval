['text':' Line number data type not well-defined between \
                      compilers, so we perform an explicit cast ','line_number':23,'multiline':True]
['text':'/ Get current device id','line_number':35,'multiline':False]
['text':'/ We need our own implementation of this function to prevent','line_number':36,'multiline':False]
['text':'/ an infinite initialization loop for CUDAKernelLaunchRegistry','line_number':37,'multiline':False]
['text':'/ Get a device's compute capability - note that this dangerously assumes','line_number':44,'multiline':False]
['text':'/ that if one CUDA GPU supports device-side assertions they all do. This is','line_number':45,'multiline':False]
['text':'/ probably fine since the latest CUDA GPU that doesn't support UVM is the','line_number':46,'multiline':False]
['text':'/ K80 released 2014-11-17. Mixing that GPU with a newer one is likely to be','line_number':47,'multiline':False]
['text':'/ rare enough that the defensive','line_number':48,'multiline':False]
['text':'/ We need our own implementation of this function to prevent','line_number':49,'multiline':False]
['text':'/ an infinite initialization loop for CUDAKernelLaunchRegistry','line_number':50,'multiline':False]
['text':'/ Get the number of CUDA devices','line_number':59,'multiline':False]
['text':'/ We need our own implementation of this function to prevent','line_number':60,'multiline':False]
['text':'/ an infinite initialization loop for CUDAKernelLaunchRegistry','line_number':61,'multiline':False]
['text':' It looks as though this'll work best on CUDA GPUs with Pascal','line_number':69,'multiline':False]
['text':' architectures or newer, per','line_number':70,'multiline':False]
['text':' https://developer.nvidia.com/blog/unified-memory-cuda-beginners/','line_number':71,'multiline':False]
['text':'/ Deleter for UVM/managed memory pointers','line_number':89,'multiline':False]
['text':' Ignore error in destructor','line_number':91,'multiline':False]
['text':' namespace','line_number':97,'multiline':False]
['text':'/ Check that kernels ran correctly by checking the message buffer. BLOCKING.','line_number':99,'multiline':False]
['text':' Hack that saves a lot of challenging sync logic.','line_number':109,'multiline':False]
['text':' The GPU increments the number of errors it's observed and the CPU can see','line_number':110,'multiline':False]
['text':' that happening immediately which means we can make it here before the GPU','line_number':111,'multiline':False]
['text':' is done writing information about those errors to memory.','line_number':112,'multiline':False]
['text':' A short pause gives it time to finish. Since something's gone wrong, this','line_number':113,'multiline':False]
['text':' pause shouldn't affect perf.','line_number':114,'multiline':False]
['text':' The snapshot causes a brief block. That's okay because this function only','line_number':117,'multiline':False]
['text':' executes if something's gone wrong such that speed is no longer a priority.','line_number':118,'multiline':False]
['text':' Loop over each device that could be managed by the process','line_number':127,'multiline':False]
['text':' Did anything fail?','line_number':131,'multiline':False]
['text':' Something failed, let's talk about that','line_number':139,'multiline':False]
['text':' TODO: It would probably be good to get a stack trace here so that','line_number':232,'multiline':False]
['text':' we can better indicate which launch caused the failure.','line_number':233,'multiline':False]
['text':' This is likely to be the longest-lasting hold on the mutex, but','line_number':251,'multiline':False]
['text':' we only expect it to be called in cases where we're already failing','line_number':252,'multiline':False]
['text':' and speed is no longer important','line_number':253,'multiline':False]
['text':' If we've already set up this GPU with managed memory, return a pointer to','line_number':277,'multiline':False]
['text':' the managed memory. This is a lock-free quick-return path.','line_number':278,'multiline':False]
['text':' Need a lock here so there's not race-condition on creating the new device','line_number':283,'multiline':False]
['text':' assertions buffer','line_number':284,'multiline':False]
['text':' If we've already set up this GPU with managed memory, return a pointer to','line_number':287,'multiline':False]
['text':' the managed memory. This locked path ensures that the device memory is','line_number':288,'multiline':False]
['text':' allocated only once','line_number':289,'multiline':False]
['text':' Otherwise, set up the GPU to be able to use the device-side assertion','line_number':294,'multiline':False]
['text':' system','line_number':295,'multiline':False]
['text':' GPU will establish direct mapping of data in CPU memory, no page faults','line_number':307,'multiline':False]
['text':' will be generated','line_number':308,'multiline':False]
['text':' Initialize the memory from the CPU; otherwise, pages may have to be created','line_number':315,'multiline':False]
['text':' on demand. We think that UVM documentation indicates that first access may','line_number':316,'multiline':False]
['text':' not honor preferred location, which would be bad, if true, because we want','line_number':317,'multiline':False]
['text':' this memory on the host so we can access it post-assertion. Initializing','line_number':318,'multiline':False]
['text':' this on the CPU helps ensure that that's where the memory will live.','line_number':319,'multiline':False]
['text':' Ownership and lifetime management of `uvm_assertions_ptr` now passes to the','line_number':322,'multiline':False]
['text':' uvm_assertions unique_ptr vector','line_number':323,'multiline':False]
['text':' namespace cuda','line_number':346,'multiline':False]
['text':' namespace c10','line_number':347,'multiline':False]
