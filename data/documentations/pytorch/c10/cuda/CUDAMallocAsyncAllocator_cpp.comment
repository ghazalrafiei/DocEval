['text':' CUDA device allocator that uses cudaMallocAsync to implement','line_number':18,'multiline':False]
['text':' the same interface as CUDACachingAllocator.cpp.','line_number':19,'multiline':False]
['text':' Designed to be safe for CUDA graph capture.','line_number':21,'multiline':False]
['text':' Interactions with CUDA graph capture are mediated by','line_number':22,'multiline':False]
['text':' notifyCaptureBegin','line_number':23,'multiline':False]
['text':' notifyCaptureAboutToEnd','line_number':24,'multiline':False]
['text':' notifyCaptureEnded','line_number':25,'multiline':False]
['text':' notifyCaptureDestroy','line_number':26,'multiline':False]
['text':' Implementation details, not declared in CUDACachingAllocator.h','line_number':28,'multiline':False]
['text':' General helpers','line_number':31,'multiline':False]
['text':' recorded_streams holds side usage streams added by record_stream calls.','line_number':58,'multiline':False]
['text':' In other words, it does NOT include the original creation stream.','line_number':59,'multiline':False]
['text':' these don't need to be c10::once_flags as in CUDAGeneratorImpl.cpp','line_number':68,'multiline':False]
['text':' because they'll only be flipped by functions that have locked the mutex.','line_number':69,'multiline':False]
['text':' Possible micro-optimization:','line_number':73,'multiline':False]
['text':' Some accesses to ptr_info are read-only.','line_number':74,'multiline':False]
['text':' We could let those be concurrent with a shared_mutex and','line_number':75,'multiline':False]
['text':' have concurrent calls take a shared_lock.','line_number':76,'multiline':False]
['text':' Keeping it simple with an ordinary mutex for now.','line_number':77,'multiline':False]
['text':'*
 * Note [Avoid freeing uncaptured ptrs during CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * During CUDA graph capture, it's illegal to call cudaFreeAsync
 * on a pointer that came from a non-captured cudaMallocAsync.
 * Unfortunately, Python being what it is, it's impossible to be
 * sure no uncaptured tensor will ever have its destructor called
 * in a capturing region.
 * We avoid errors by
 *  1. remembering if allocated pointers were captured or uncaptured
 *  2. during capture, if we detect an attempt to free an uncaptured
 *     allocation on a capturing stream, don't free it immediately,
 *     just remember it and defer its cudaFreeAsync call to after
 *     the end of capture (specifically, to notifyCaptureEnded).
 ','line_number':80,'multiline':True]
['text':' These two help setMemoryFraction limit the amount of memory','line_number':100,'multiline':False]
['text':' used by PyTorch in particular (as opposed to other libraries','line_number':101,'multiline':False]
['text':' in the same process that might be sharing the same cudaMemPool_t).','line_number':102,'multiline':False]
['text':' Graph-specific helpers','line_number':106,'multiline':False]
['text':'*
 * Note [Avoid dangling free streams during CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * During capture, all stream dependencies must branch out from
 * the stream on which capture began and rejoin this initial stream
 * before capture ends.
 * The user rigs desired forking and joining with event waits.
 * But it's hard to be sure when tensor destructors get called relative
 * to the final joins.
 * For example, suppose a user
 *   forks work stream B from initial capture stream A
 *   creates a tensor T in B
 *   joins by syncing A with B
 *   ends capture.
 * All well and good, right? Maybe not: maybe T went out of scope
 * and its destructor got called AFTER the rejoin, leaving the graph with
 * "unjoined work": a dangling cudaFreeAsync node in stream B.
 * Ensuring that all tensor destructors for all side stream tensors
 * are called before side streams rejoin the main stream is
 * difficult. The user might have to add a bunch of explicit
 * "del"s at the right spots in code that was fine for ordinary
 * eager execution.
 * Fortunately, we can spare the user this burden:
 * during capture, we remember _all_ free streams,
 * and manually rejoin them with the capture stream during
 * notifyCaptureAboutToEnd.
 * This approach is heavy-handed, but hopefully capture only needs to
 * happen once, so we don't mind being heavy-handed.
 *
 * TODO: If, someday, we augment the graph bindings to support recapture
 * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#whole-graph-update
 * (eg, as a way to accommodate dynamic params) we should think more
 * carefully about the CPU overhead of remembering and rejoining
 * all free streams during capture. Maybe it's not a big deal.
 ','line_number':108,'multiline':True]
['text':' Implementation functions','line_number':146,'multiline':False]
['text':' Assumes the caller holds general_mutex','line_number':148,'multiline':False]
['text':' See "Retaining memory in the pool" here:','line_number':153,'multiline':False]
['text':' https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-1/','line_number':154,'multiline':False]
['text':' I think all these are on by default, but I want to enable them','line_number':161,'multiline':False]
['text':' explicitly to ensure awareness.','line_number':162,'multiline':False]
['text':' Grabs a stream from the current device to use as the "unifier" free','line_number':171,'multiline':False]
['text':' stream for allocations that end up used on multiple streams.','line_number':172,'multiline':False]
['text':' CUDACachingAllocator.cpp uses raw cuda events, as do we.','line_number':185,'multiline':False]
['text':' Assumes the caller holds general_mutex','line_number':193,'multiline':False]
['text':' Possible micro-optimization: If we did a value-copy here, we could move','line_number':195,'multiline':False]
['text':' ptr_info.erase(it) up here and drop the lock immediately.','line_number':196,'multiline':False]
['text':' If the usage stream is a null (default) stream,','line_number':200,'multiline':False]
['text':' cudaFreeAsync infers the device from the ambient context,','line_number':201,'multiline':False]
['text':' so we need to set the right ambient context.','line_number':202,'multiline':False]
['text':' ptr was only used on one stream, which must have been','line_number':206,'multiline':False]
['text':' the original allocation stream.','line_number':207,'multiline':False]
['text':' Frees ptr in the original allocation stream.','line_number':208,'multiline':False]
['text':' See Note [Avoid dangling free streams during CUDA graph capture]','line_number':213,'multiline':False]
['text':' ptr was used on many streams. We don't know which was the most recent.','line_number':217,'multiline':False]
['text':' There could even have been multiple most recent usage streams acting','line_number':218,'multiline':False]
['text':' on different regions of the memory.','line_number':219,'multiline':False]
['text':' But cudaFreeAsync only accepts a single most recent usage stream.','line_number':220,'multiline':False]
['text':' We can still safely free ptr with a trick:','line_number':221,'multiline':False]
['text':' Use a dummy "unifying stream", sync the unifying stream with all of','line_number':222,'multiline':False]
['text':' ptr's usage streams, and pass the dummy stream to cudaFreeAsync.','line_number':223,'multiline':False]
['text':' Retrieves the dummy "unifier" stream from the device','line_number':225,'multiline':False]
['text':' on which the pointer was originally allocated.','line_number':226,'multiline':False]
['text':' we're already on creation_stream.device, no need to re-guard','line_number':232,'multiline':False]
['text':' The number of usage streams is typically small (low single digits)','line_number':235,'multiline':False]
['text':' Logic here accommodates the chance some of the usage streams were on','line_number':237,'multiline':False]
['text':' other devices, which is possible if some usage kernels accessed the','line_number':238,'multiline':False]
['text':' memory via p2p.','line_number':239,'multiline':False]
['text':' cudaEventRecord requires that the input event and stream are on the','line_number':241,'multiline':False]
['text':' same device.','line_number':242,'multiline':False]
['text':' Frees ptr in the dummy "unifier" stream.','line_number':248,'multiline':False]
['text':' At this point, unless dummy_unifying_free_stream happens to alias some','line_number':250,'multiline':False]
['text':' future user stream, the allocation is only available for "opportunistic"','line_number':251,'multiline':False]
['text':' reuse, ie, if the CPU sees dummy_unifying_free_stream has reached the','line_number':252,'multiline':False]
['text':' point that all events recorded on all usage streams have resolved from','line_number':253,'multiline':False]
['text':' the CPU's perspective. In theory, we could remove the need for the driver','line_number':254,'multiline':False]
['text':' to do this tracking by e.g. replacing','line_number':255,'multiline':False]
['text':' cudaStreamWaitEvent(dummy_unifying_free_stream.stream, event);','line_number':256,'multiline':False]
['text':' with','line_number':257,'multiline':False]
['text':' cudaStreamWaitEvent(creation_stream.stream, event);','line_number':258,'multiline':False]
['text':' then cudaFreeAsyncing straight back into creation_stream.stream,','line_number':259,'multiline':False]
['text':' but this forces a potentially false dependency of creation_stream.stream','line_number':260,'multiline':False]
['text':' on all the recorded_streams.','line_number':261,'multiline':False]
['text':' See Note [Avoid dangling free streams during CUDA graph capture]','line_number':264,'multiline':False]
['text':' See Note [Avoid freeing uncaptured ptrs during CUDA graph capture]','line_number':296,'multiline':False]
['text':' Remembers the raw pointer, not the iterator.','line_number':297,'multiline':False]
['text':' This forces notifyCaptureEnded to do another lookup,','line_number':298,'multiline':False]
['text':' but avoids the risk the iterator might be invalidated','line_number':299,'multiline':False]
['text':' between now and then.','line_number':300,'multiline':False]
['text':' Symmetric with NativeCachingAllocator::malloc for now,','line_number':317,'multiline':False]
['text':' although I don't think we absolutely need the symmetry.','line_number':318,'multiline':False]
['text':' If stream is a null (default) stream,','line_number':326,'multiline':False]
['text':' cudaMallocAsync infers the device from the ambient context,','line_number':327,'multiline':False]
['text':' so we need to set the right ambient context.','line_number':328,'multiline':False]
['text':' See Note [Avoid freeing uncaptured ptrs during CUDA graph capture]','line_number':335,'multiline':False]
['text':' Defensively checks for preexisting CUDA error state.','line_number':347,'multiline':False]
['text':' TODO: Could we avoid calling cudaMallocAsync while holding general_mutex,','line_number':351,'multiline':False]
['text':' perhaps by letting lazy_init_device use separate once_flags or an internal','line_number':352,'multiline':False]
['text':' static initializer?','line_number':353,'multiline':False]
['text':' Clears CUDA's internal error state so the user, if desired, can catch the','line_number':361,'multiline':False]
['text':' OOM exception, free some stuff on the script side, and retry the','line_number':362,'multiline':False]
['text':' allocation. This aligns with the behavior of alloc_block in','line_number':363,'multiline':False]
['text':' CUDACachingAllocator.cpp.','line_number':364,'multiline':False]
['text':' clear CUDA error','line_number':365,'multiline':False]
['text':' anonymous namespace','line_number':401,'multiline':False]
['text':' Same pattern as CUDACachingAllocator.cpp.','line_number':405,'multiline':False]
['text':' This function should not issue any context-creating calls,','line_number':425,'multiline':False]
['text':' just set up for later calls to init per-device pools based','line_number':426,'multiline':False]
['text':' on the current device each later call sees.','line_number':427,'multiline':False]
['text':' Are there external guarantees init will be called before','line_number':431,'multiline':False]
['text':' any of the allocator's other functions?','line_number':432,'multiline':False]
['text':' std::lock_guard<std::mutex> lk(general_mutex);','line_number':433,'multiline':False]
['text':' Should setMemoryFraction be allowed to trigger a full device context and','line_number':463,'multiline':False]
['text':' pool-creating lazy_init_device, or should we simply assert this device is','line_number':464,'multiline':False]
['text':' already initialized, ie','line_number':465,'multiline':False]
['text':' TORCH_CHECK(devs_initialized_flags[device], ...)?','line_number':466,'multiline':False]
['text':' Alternative: Instead of a manual hard limit, we could use','line_number':475,'multiline':False]
['text':' cudaMemPoolSetAttribute(mempool, cudaMemPoolAttrReleaseThreshold,','line_number':476,'multiline':False]
['text':' &threshold); This is a soft hint: The driver allows the pool's reserved','line_number':477,'multiline':False]
['text':' memory to spike above threshold in regions of high cudaMallocAsync','line_number':478,'multiline':False]
['text':' demand, but opportunistically trims reserved memory back to threshold','line_number':479,'multiline':False]
['text':' when the memory in use is < threshold. I don't like this because it','line_number':480,'multiline':False]
['text':' introduces performance nondeterminism.','line_number':481,'multiline':False]
['text':' The only consumer of cacheInfo is getMaxWorkspaceSize in Conv_v7.cpp.','line_number':500,'multiline':False]
['text':' Afaict, the role of cacheInfo is to give getMaxWorkspaceSize a reasonable','line_number':501,'multiline':False]
['text':' maximum workspace size to use for an upcoming cudnnFind call.','line_number':502,'multiline':False]
['text':'','line_number':503,'multiline':False]
['text':' The native allocator's cacheInfo chooses to return the size of its','line_number':504,'multiline':False]
['text':' largest unused block (which is the largest allocation the native','line_number':505,'multiline':False]
['text':' allocator can service immediately and asynchronously without a','line_number':506,'multiline':False]
['text':' cudaMalloc.','line_number':507,'multiline':False]
['text':'','line_number':508,'multiline':False]
['text':' Here, we use a different heuristic: figure out the max usable workspace','line_number':509,'multiline':False]
['text':' size with a bit of educated trial and error. It's ok to be','line_number':510,'multiline':False]
['text':' perf-inefficient because cacheInfo is a prelude to cudnnFind.','line_number':511,'multiline':False]
['text':'','line_number':512,'multiline':False]
['text':' The algo cache then stores the best-performing algo with workspace <=','line_number':513,'multiline':False]
['text':' maxWorkspaceGuess. Later calls with the same param set hit in cache and','line_number':514,'multiline':False]
['text':' try to allocate the same workspace. If, in one of those future calls,','line_number':515,'multiline':False]
['text':' workspace allocation fails (ie because less ambient memory is available),','line_number':516,'multiline':False]
['text':' the bindings rerun cudnnFind, including calling cacheInfo again','line_number':517,'multiline':False]
['text':' beforehand to estimate a new (smaller) largest-available workspace. Over','line_number':518,'multiline':False]
['text':' a few such calls, the cache should settle to the algo with a workspace','line_number':519,'multiline':False]
['text':' size that's small enough to succeed every time (for that param set).','line_number':520,'multiline':False]
['text':'','line_number':521,'multiline':False]
['text':' So the strategy here is to return a rough, largeish guess and let the','line_number':522,'multiline':False]
['text':' bindings retry to trim as needed over time.','line_number':523,'multiline':False]
['text':'','line_number':524,'multiline':False]
['text':' The only caveat is, even if a workspace is allocated without OOM errors','line_number':525,'multiline':False]
['text':' now and in future calls, it's hard to be sure those later error-free','line_number':526,'multiline':False]
['text':' cudaMallocAsyncs are fast and come straight from the pool (ie,','line_number':527,'multiline':False]
['text':' cudaMallocAsync didn't need to reserve more memory from the system).','line_number':528,'multiline':False]
['text':' Hopefully, after repeated workspace requests, the pool's reserved memory','line_number':529,'multiline':False]
['text':' also stabilizes to a point where they all come straight from the pool.','line_number':530,'multiline':False]
['text':' Defensively checks for preexisting CUDA error state.','line_number':547,'multiline':False]
['text':' Duplicates some logic from mallocAsync to work with the error state','line_number':552,'multiline':False]
['text':' directly instead of repeatedly catching an exception thrown by','line_number':553,'multiline':False]
['text':' mallocAsync.','line_number':554,'multiline':False]
['text':' clear CUDA error','line_number':566,'multiline':False]
['text':' quick and dirty: try half the size next iteration','line_number':567,'multiline':False]
['text':' Empty tensor's storage().data() might be a null ptr. As there is no','line_number':590,'multiline':False]
['text':' blocks associated with those tensors, it is fine to do nothing here.','line_number':591,'multiline':False]
['text':' The pointer should exist in the map already.','line_number':596,'multiline':False]
['text':' Collects stats for device.','line_number':659,'multiline':False]
['text':' If device hasn't been used yet, returns 0s without creating a context.','line_number':660,'multiline':False]
['text':' Memory currently reserved by the mempool','line_number':664,'multiline':False]
['text':' High-water mark of memory reserved by the mempool since last reset','line_number':666,'multiline':False]
['text':' Memory currently in use by the mempool','line_number':668,'multiline':False]
['text':' High-water mark of memory','line_number':670,'multiline':False]
['text':' Many stat types are specific to the native allocator. We leave these','line_number':693,'multiline':False]
['text':' untouched. Their "struct Stat"s will contain zeroed values.','line_number':694,'multiline':False]
['text':' In the native allocator:','line_number':697,'multiline':False]
['text':' allocated_bytes is the total bytes of blocks that have been malloc()ed','line_number':698,'multiline':False]
['text':' and not yet free()d.','line_number':699,'multiline':False]
['text':' active_bytes is the total bytes of blocks that have been malloc()ed but','line_number':700,'multiline':False]
['text':' not yet released back into a free pool. In other words, it includes all','line_number':701,'multiline':False]
['text':' allocated_bytes, as well as the bytes of "limbo state" blocks had have','line_number':702,'multiline':False]
['text':' already been free()ed but not yet free_block()ed back into a pool due to','line_number':703,'multiline':False]
['text':' outstanding stream_uses.','line_number':704,'multiline':False]
['text':'','line_number':705,'multiline':False]
['text':' Here, in the cudaMallocAsync allocator:','line_number':706,'multiline':False]
['text':' We simply ask the driver's opinion about active memory.','line_number':707,'multiline':False]
['text':' We don't bother distinguishing between allocated_bytes and active_bytes.','line_number':708,'multiline':False]
['text':' Using zero as the reset value is the method recommended by Cuda driver','line_number':737,'multiline':False]
['text':' team. Vivek Kini says:','line_number':738,'multiline':False]
['text':'   "Resetting to zero (which is the only valid value when setting','line_number':739,'multiline':False]
['text':'    ReservedMemHigh) resets it to ReservedMemCurrent inside the driver','line_number':740,'multiline':False]
['text':'   (same goes for UsedMemHigh/UsedMemCurrent)"','line_number':741,'multiline':False]
['text':' Alternative: TORCH_WARN','line_number':756,'multiline':False]
['text':' CUDAGraph interactions','line_number':760,'multiline':False]
['text':' See Note [Avoid dangling free streams during CUDA graph capture]','line_number':786,'multiline':False]
['text':' cudaEventRecord requires that the input event and stream are on the','line_number':788,'multiline':False]
['text':' same device.','line_number':789,'multiline':False]
['text':' CUDACachingAllocator.cpp uses raw cuda events, as do we.','line_number':792,'multiline':False]
['text':' Q: Do we need to do anything special here, like clear long-lived','line_number':809,'multiline':False]
['text':'    pointers created during the original capture (for example,','line_number':810,'multiline':False]
['text':'    tensors intended as the graph's I/O surface) that might still','line_number':811,'multiline':False]
['text':'    be resident in ptr_info?','line_number':812,'multiline':False]
['text':' A: I don't think so.','line_number':813,'multiline':False]
['text':'    Those allocations survived capture because the user held','line_number':814,'multiline':False]
['text':'    explicit tensor references to them,','line_number':815,'multiline':False]
['text':'    Those tensors' destructors will call freeAsync() on each pointer','line_number':816,'multiline':False]
['text':'    when the user is done with them.','line_number':817,'multiline':False]
['text':'    The freeAsync()s will probably incur','line_number':818,'multiline':False]
['text':'    TORCH_WARN("Attempting uncaptured free of a captured allocation..."','line_number':819,'multiline':False]
['text':'    but stale ptrs will not permanently leak into ptr_info.','line_number':820,'multiline':False]
['text':' Double-checks allocator backend hasn't changed, which would definitely be','line_number':848,'multiline':False]
['text':' an error. cudaMallocAsync pools are unaffected by','line_number':849,'multiline':False]
['text':' cudaDeviceEnablePeerAccess. We need pool-specific enablement. See','line_number':850,'multiline':False]
['text':' https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-2/','line_number':851,'multiline':False]
['text':' numDescs ','line_number':859,'multiline':True]
['text':' namespace CudaMallocAsync','line_number':897,'multiline':False]
['text':' namespace CUDACachingAllocator','line_number':898,'multiline':False]
['text':' namespace cuda','line_number':899,'multiline':False]
['text':' namespace c10','line_number':900,'multiline':False]
