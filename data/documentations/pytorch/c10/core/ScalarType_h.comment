['text':' For the macros below:','line_number':25,'multiline':False]
['text':' NB: If you want to macro some code for all non-QInt scalar types (i.e. types','line_number':26,'multiline':False]
['text':' with complete information, you probably want one of the','line_number':27,'multiline':False]
['text':' AT_FORALL_SCALAR_TYPES / AT_FORALL_SCALAR_TYPES_AND','line_number':28,'multiline':False]
['text':' macros below, which are designed to behave similarly to the Dispatch macros','line_number':29,'multiline':False]
['text':' with the same name.','line_number':30,'multiline':False]
['text':' NB: Order matters for this macro; it is relied upon in','line_number':32,'multiline':False]
['text':' _promoteTypesLookup and the serialization format.','line_number':33,'multiline':False]
['text':' 0 ','line_number':35,'multiline':True]
['text':' 1 ','line_number':36,'multiline':True]
['text':' 2 ','line_number':37,'multiline':True]
['text':' 3 ','line_number':38,'multiline':True]
['text':' 4 ','line_number':39,'multiline':True]
['text':' 5 ','line_number':40,'multiline':True]
['text':' 6 ','line_number':41,'multiline':True]
['text':' 7 ','line_number':42,'multiline':True]
['text':' 8 ','line_number':43,'multiline':True]
['text':' 9 ','line_number':44,'multiline':True]
['text':' 10 ','line_number':45,'multiline':True]
['text':' 11 ','line_number':46,'multiline':True]
['text':' 12 ','line_number':47,'multiline':True]
['text':' 13 ','line_number':48,'multiline':True]
['text':' 14 ','line_number':49,'multiline':True]
['text':' 15 ','line_number':50,'multiline':True]
['text':' 16 ','line_number':51,'multiline':True]
['text':' 17 ','line_number':52,'multiline':True]
['text':' 18 ','line_number':53,'multiline':True]
['text':' 19 ','line_number':54,'multiline':True]
['text':' 20 ','line_number':55,'multiline':True]
['text':' 21 ','line_number':56,'multiline':True]
['text':' 22 ','line_number':57,'multiline':True]
['text':' 23 ','line_number':58,'multiline':True]
['text':' 24 ','line_number':59,'multiline':True]
['text':' 25 ','line_number':60,'multiline':True]
['text':' 26 ','line_number':61,'multiline':True]
['text':' If you want to support ComplexHalf for real, add ComplexHalf','line_number':63,'multiline':False]
['text':' into this macro (and change the name).  But beware: convert()','line_number':64,'multiline':False]
['text':' doesn't work for all the conversions you need...','line_number':65,'multiline':False]
['text':' These are used to map ScalarTypes to C++ types.','line_number':114,'multiline':False]
['text':' This is a workaround for the CUDA bug which prevents ','line_number':124,'multiline':True]
['text':' ::detail::ScalarTypeToCType<T>::type being used directly due to ','line_number':125,'multiline':True]
['text':' ambiguous reference which can't to be resolved. For some reason it ','line_number':126,'multiline':True]
['text':' can't pick between at::detail and at::cuda::detail. ','line_number':127,'multiline':True]
['text':' For repro example, please see: ','line_number':128,'multiline':True]
['text':' https://gist.github.com/izdeby/952ae7cf256ddb740a73776d39a7e7ba ','line_number':129,'multiline':True]
['text':' TODO: remove once the bug is fixed. ','line_number':130,'multiline':True]
['text':' namespace impl','line_number':141,'multiline':False]
['text':'includeBool=','line_number':391,'multiline':True]
['text':' Don't forget to extend this when adding new QInt types','line_number':415,'multiline':False]
['text':' BFloat16 has range equivalent to Float,','line_number':502,'multiline':False]
['text':' so we map it to ComplexFloat.','line_number':503,'multiline':False]
['text':' see tensor_attributes.rst for detailed explanation and examples','line_number':522,'multiline':False]
['text':' of casting rules.','line_number':523,'multiline':False]
['text':' We disallow complex -> non complex, e.g., float_tensor *= complex is','line_number':525,'multiline':False]
['text':' disallowed.','line_number':526,'multiline':False]
['text':' We disallow float -> integral, e.g., int_tensor *= float is disallowed.','line_number':530,'multiline':False]
['text':' Treat bool as a distinct "category," to be consistent with type promotion','line_number':535,'multiline':False]
['text':' rules (e.g. `bool_tensor + 5 -> int64_tensor`). If `5` was in the same','line_number':536,'multiline':False]
['text':' category as `bool_tensor`, we would not promote. Differing categories','line_number':537,'multiline':False]
['text':' implies `bool_tensor += 5` is disallowed.','line_number':538,'multiline':False]
['text':'','line_number':539,'multiline':False]
['text':' NB: numpy distinguishes "unsigned" as a category to get the desired','line_number':540,'multiline':False]
['text':' `bool_tensor + 5 -> int64_tensor` behavior. We don't, because:','line_number':541,'multiline':False]
['text':' * We don't want the performance hit of checking the runtime sign of','line_number':542,'multiline':False]
['text':' Scalars.','line_number':543,'multiline':False]
['text':' * `uint8_tensor + 5 -> int64_tensor` would be undesirable.','line_number':544,'multiline':False]
['text':' This is generated according to NumPy's promote_types','line_number':552,'multiline':False]
['text':' If the two types are equal, return that type','line_number':571,'multiline':False]
['text':' Handle identically equal types','line_number':576,'multiline':False]
['text':' Bits, Quantized and Float8 are 14 dtypes already handled and not included','line_number':599,'multiline':False]
['text':' in the promotion table below.','line_number':600,'multiline':False]
['text':' Bfloat16 is at position 15 in the ScalerType enum, There are three types','line_number':620,'multiline':False]
['text':' below bf16 not included in the table, Qint8, QUInt8, QInt32. Every other','line_number':621,'multiline':False]
['text':' type above bf16, i.e. {Bits, Quantized, Float8} are not included in the','line_number':622,'multiline':False]
['text':' table.','line_number':623,'multiline':False]
['text':' If either of the types is bf16, we need to shift the type down by the one','line_number':625,'multiline':False]
['text':' missing section in the table that is less then bf16 i.e {QInt8, QUInt8,','line_number':626,'multiline':False]
['text':' QInt32}','line_number':627,'multiline':False]
['text':' We decrease the promotion table by the number of missing types -> 14','line_number':633,'multiline':False]
['text':' and then subtract 1 more from the table since we don't store ud to ud','line_number':634,'multiline':False]
['text':' mapping.','line_number':635,'multiline':False]
['text':' this matrix has to be consistent with','line_number':639,'multiline':False]
['text':' AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS undefined is used where we','line_number':640,'multiline':False]
['text':' are not sure about the correct value for type promotion.','line_number':641,'multiline':False]
['text':' clang-format off','line_number':642,'multiline':False]
['text':'        u1  i1  i2  i4  i8  f2  f4  f8  c2  c4  c8  b1  bf','line_number':646,'multiline':True]
['text':' u1 ','line_number':647,'multiline':True]
['text':' i1 ','line_number':648,'multiline':True]
['text':' i2 ','line_number':649,'multiline':True]
['text':' i4 ','line_number':650,'multiline':True]
['text':' i8 ','line_number':651,'multiline':True]
['text':' f2 ','line_number':652,'multiline':True]
['text':' f4 ','line_number':653,'multiline':True]
['text':' f8 ','line_number':654,'multiline':True]
['text':' c2 ','line_number':655,'multiline':True]
['text':' c4 ','line_number':656,'multiline':True]
['text':' c8 ','line_number':657,'multiline':True]
['text':' b1 ','line_number':658,'multiline':True]
['text':' bf ','line_number':659,'multiline':True]
['text':' clang-format on','line_number':661,'multiline':False]
['text':' namespace c10','line_number':671,'multiline':False]
