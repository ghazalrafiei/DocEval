['text':'# @package recurrent','line_number':1,'multiline':False]
['text':' Module caffe2.python.recurrent','line_number':2,'multiline':False]
['text':' We have to manually scope due to our internal/external blob','line_number':55,'multiline':False]
['text':' relationships.','line_number':56,'multiline':False]
['text':' determine inputs that are considered to be references','line_number':60,'multiline':False]
['text':' it is those that are not referred to in inputs or initial_cell_inputs','line_number':61,'multiline':False]
['text':' These gradients are expected to be available during the backward pass','line_number':71,'multiline':False]
['text':' compute the backward pass of the cell net','line_number':74,'multiline':False]
['text':' Insert operators to re-compute the specified blobs.','line_number':84,'multiline':False]
['text':' They are added in the same order as for the forward pass, thus','line_number':85,'multiline':False]
['text':' the order is correct.','line_number':86,'multiline':False]
['text':' This fires if other outputs than the declared','line_number':93,'multiline':False]
['text':' are computed by the ops that are recomputed','line_number':94,'multiline':False]
['text':' compute blobs used but not defined in the backward pass','line_number':98,'multiline':False]
['text':' also add to the output list the intermediate outputs of fwd_step that','line_number':103,'multiline':False]
['text':' are used by backward.','line_number':104,'multiline':False]
['text':' Internal arguments used by RecurrentNetwork operator','line_number':124,'multiline':False]
['text':' Links are in the format blob_name, recurrent_states, offset.','line_number':126,'multiline':False]
['text':' In the moment t we know that corresponding data block is at','line_number':127,'multiline':False]
['text':' t + offset position in the recurrent_states tensor','line_number':128,'multiline':False]
['text':' Aliases are used to expose outputs to external world','line_number':132,'multiline':False]
['text':' Format (internal_blob, external_blob, offset)','line_number':133,'multiline':False]
['text':' Negative offset stands for going from the end,','line_number':134,'multiline':False]
['text':' positive - from the beginning','line_number':135,'multiline':False]
['text':' States held inputs to the cell net','line_number':138,'multiline':False]
['text':' Recurrent_states is going to be (T + 1) x ...','line_number':143,'multiline':False]
['text':' It stores all inputs and outputs of the cell net over time.','line_number':144,'multiline':False]
['text':' Or their gradients in the case of the backward pass.','line_number':145,'multiline':False]
['text':' If nobody writes to this recurrent input gradient, we need','line_number':165,'multiline':False]
['text':' to make sure it gets to the states grad blob after all.','line_number':166,'multiline':False]
['text':' We do this by using backward_links which triggers an alias','line_number':167,'multiline':False]
['text':' This logic is being used for example in a SumOp case','line_number':168,'multiline':False]
['text':' Splitting to separate lists so we can pass them to c++','line_number':194,'multiline':False]
['text':' where we ensemle them back','line_number':195,'multiline':False]
['text':' Make sure that recurrent gradients accumulate with internal gradients','line_number':201,'multiline':False]
['text':' (if a blob in the backward_cell_net receives gradient from both an','line_number':202,'multiline':False]
['text':' external connection as well as from within the backward_cell_net,','line_number':203,'multiline':False]
['text':' those gradients need to be added together, rather than one overwriting','line_number':204,'multiline':False]
['text':' the other)','line_number':205,'multiline':False]
['text':' In place operation won't cause issues because it takes','line_number':217,'multiline':False]
['text':' existing value of a blob into account','line_number':218,'multiline':False]
['text':' Restore net type since 'rnn' is not recognized outside RNNs','line_number':280,'multiline':False]
['text':' The last output is a list of step workspaces,','line_number':283,'multiline':False]
['text':' which is only needed internally for gradient propagation','line_number':284,'multiline':False]
