['text':'# @package seq2seq_util','line_number':1,'multiline':False]
['text':' Module caffe2.python.examples.seq2seq_util','line_number':2,'multiline':False]
['text':' Adding padding tokens to the vocabulary to maintain consistency with IDs','line_number':29,'multiline':False]
['text':' Concatenate forward and backward results','line_number':170,'multiline':False]
['text':' sequence (all) output locations are at twice their state index','line_number':384,'multiline':False]
['text':' [batch_size, encoder_length, 1]','line_number':392,'multiline':False]
['text':' we do softmax over the whole sequence','line_number':613,'multiline':False]
['text':' (max_length in the batch * batch_size) x decoder embedding size','line_number':614,'multiline':False]
['text':' -1 because we don't know max_length yet','line_number':615,'multiline':False]
