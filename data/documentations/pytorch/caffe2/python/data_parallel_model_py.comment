['text':'# @package data_parallel_model','line_number':1,'multiline':False]
['text':' Module caffe2.python.data_parallel_model','line_number':2,'multiline':False]
['text':' We only import nccl operators when the machine has GPUs','line_number':22,'multiline':False]
['text':' Otherwise the binary can be compiled with CPU-only mode, and','line_number':23,'multiline':False]
['text':' will not be able to find those modules','line_number':24,'multiline':False]
['text':' best-guess','line_number':179,'multiline':False]
['text':' Store some information in the model -- a bit ugly','line_number':186,'multiline':False]
['text':' Keep track of params that were in the model before: they are not','line_number':196,'multiline':False]
['text':' data parallel, so we need to handle them separately','line_number':197,'multiline':False]
['text':' Add input and model','line_number':200,'multiline':False]
['text':' Check that a model that is used for validation/testing has','line_number':214,'multiline':False]
['text':' init_params False, otherwise running the param init net will overwrite','line_number':215,'multiline':False]
['text':' synchronized values by the training net','line_number':216,'multiline':False]
['text':' TODO: make into assert','line_number':227,'multiline':False]
['text':' Losses are not needed for test net','line_number':237,'multiline':False]
['text':' Create parameter map','line_number':248,'multiline':False]
['text':' computed params','line_number':253,'multiline':False]
['text':' Group gradients by device and register to blob lookup','line_number':294,'multiline':False]
['text':' Gradients in reverse order','line_number':317,'multiline':False]
['text':' Configure dagnet to run with only one worker on the first iteration,','line_number':364,'multiline':False]
['text':' to prevent concurrency problems with allocs and nccl.','line_number':365,'multiline':False]
['text':' Add initial parameter syncs','line_number':370,'multiline':False]
['text':' Handle any operations that need to be done after parameter sync','line_number':382,'multiline':False]
['text':' i.e. making sure multi-precision copies of parameters are up-to-date','line_number':383,'multiline':False]
['text':' question: rendezvous structure','line_number':489,'multiline':False]
['text':' num_devices is #devices across all machines','line_number':491,'multiline':False]
['text':' num_workers is #threads to execute the DAG per shard','line_number':493,'multiline':False]
['text':' A net for initializing global model parameters. Its called once in the','line_number':510,'multiline':False]
['text':' same step as net parameters initialization.','line_number':511,'multiline':False]
['text':' A net for computing final parameter updates. Its will run once after','line_number':517,'multiline':False]
['text':' running net (local models updates) for `num_local_iterations` times.','line_number':518,'multiline':False]
['text':' Keep track of params that were in the model before: they are not','line_number':533,'multiline':False]
['text':' data parallel, so we need to handle them separately','line_number':534,'multiline':False]
['text':' A net for broadcasting gpu-0 (master shard) parameters after','line_number':580,'multiline':False]
['text':' running net for `warmup_iterartions`.','line_number':581,'multiline':False]
['text':' (Step-0) Initialize momentum parameters on master device.','line_number':601,'multiline':False]
['text':' (Step-1) Update models for num_local_iterations.','line_number':614,'multiline':False]
['text':' (Step-2) Compute post-local-updates average of the params.','line_number':616,'multiline':False]
['text':' Sum model params across GPUs and store resutls in param_avg blob.','line_number':617,'multiline':False]
['text':' (Step-3) Update momentum params :','line_number':628,'multiline':False]
['text':' param_v = block_momentum * param_v','line_number':629,'multiline':False]
['text':' + block_learning_Rate * (param_avg - param)','line_number':630,'multiline':False]
['text':' if nesterov momentum:','line_number':631,'multiline':False]
['text':' param = param + param_v','line_number':632,'multiline':False]
['text':' - block_momentum * (param_v - param_v_prev)','line_number':633,'multiline':False]
['text':' param_v_prev = param_v','line_number':634,'multiline':False]
['text':' else:','line_number':635,'multiline':False]
['text':' param = param + param_v','line_number':636,'multiline':False]
['text':' TODO(ataei) : Stop building the graph here to get model average ?','line_number':640,'multiline':False]
['text':' Add additional syncs','line_number':687,'multiline':False]
['text':' Reset momentum-SGD parameters','line_number':694,'multiline':False]
['text':' Synchronize DPM at the start of each epoch. This allows shards that','line_number':750,'multiline':False]
['text':' starts an epoch sooner to wait for slower shards.  Without this,','line_number':751,'multiline':False]
['text':' shards that are faster than others will begin training the next epoch','line_number':752,'multiline':False]
['text':' while stragglers are blocked on IO, and may timeout after 30 seconds','line_number':753,'multiline':False]
['text':' (_DEFAULT_TIMEOUT_SEC).','line_number':754,'multiline':False]
['text':' We pass in model.param_init_net so that the barrier net can be run as','line_number':755,'multiline':False]
['text':' part of the param_init_net.','line_number':756,'multiline':False]
['text':' DEPRECATED: See warnings below.','line_number':786,'multiline':False]
['text':' Single host case','line_number':795,'multiline':False]
['text':' Explicitly need to create gradients on each GPU','line_number':872,'multiline':False]
['text':' Add iteration blobs that do not have namescope separately, since','line_number':919,'multiline':False]
['text':' it is important to checkpoint iteration counter','line_number':920,'multiline':False]
['text':' Synchronize to the blob lookup map, as the provided','line_number':942,'multiline':False]
['text':' blobs might have non-parameters, such as momentum blobs.','line_number':943,'multiline':False]
['text':' Run the sync','line_number':982,'multiline':False]
['text':' Copy params from gpu_0 to other','line_number':1010,'multiline':False]
['text':' Note that the root is the root _rank_ and not the root','line_number':1017,'multiline':False]
['text':' _device_. Thus we always use root=0, regardless of the','line_number':1018,'multiline':False]
['text':' devices used.','line_number':1019,'multiline':False]
['text':' TODO: for _shared_model, do only NCCLReduce','line_number':1043,'multiline':False]
['text':' Skip the first device','line_number':1068,'multiline':False]
['text':' Copy from peer to d0','line_number':1072,'multiline':False]
['text':' Special tree reduction for 16 gpus, TODO generalize like in muji.py','line_number':1080,'multiline':False]
['text':' TODO: for _shared_model, no need to broadcast','line_number':1100,'multiline':False]
['text':' Copy between GPU and CPU','line_number':1217,'multiline':False]
['text':' Broadcast locally','line_number':1228,'multiline':False]
['text':' Remove non-master parameters so that they will not receive parameter','line_number':1262,'multiline':False]
['text':' update operators.','line_number':1263,'multiline':False]
['text':' Remove all but master params','line_number':1282,'multiline':False]
['text':' Delete ops that output non-master version of parameter','line_number':1287,'multiline':False]
['text':' Remap inputs to point to the master param','line_number':1295,'multiline':False]
['text':' Remark: NCCLReduce does not support in-place modifications','line_number':1378,'multiline':False]
['text':' so we need a temporary blob','line_number':1379,'multiline':False]
['text':' With Gloo cross GPU and cross machine allreduce','line_number':1396,'multiline':False]
['text':' can be executed in a single operation.','line_number':1397,'multiline':False]
['text':' Try to use GPUDirect if transport == ibverbs.','line_number':1398,'multiline':False]
['text':' Step 1: sum blobs from local GPUs to master GPU','line_number':1404,'multiline':False]
['text':' Temp fix since NCCLReduce does not work','line_number':1408,'multiline':False]
['text':' Step 2: allreduce between all hosts, between master GPUs','line_number':1417,'multiline':False]
['text':' Step 3: broadcast locally','line_number':1423,'multiline':False]
['text':' Now we need to Allreduce blobs on all the GPUs.','line_number':1433,'multiline':False]
['text':' Pick GPU #0 as a master GPU.','line_number':1434,'multiline':False]
['text':' Group by blob_name for reduce.','line_number':1440,'multiline':False]
['text':' Non-reducible','line_number':1443,'multiline':False]
['text':' last_out is used to serialize the execution of nccls','line_number':1455,'multiline':False]
['text':' Sparse gradients: all-gather for indices and values','line_number':1459,'multiline':False]
['text':' Poor man's allreduce','line_number':1509,'multiline':False]
['text':' Copy from master to others -- averaging would be perhaps better,','line_number':1540,'multiline':False]
['text':' but currently NCCLAllReduce is too prone to deadlock','line_number':1541,'multiline':False]
['text':' A helper function to extract a parameter's name','line_number':1553,'multiline':False]
['text':' Format is "a/b/c/d" -> "b/c/d"','line_number':1555,'multiline':False]
['text':' This avoids failing on operators that are only for CPU','line_number':1578,'multiline':False]
['text':' Hack for Iters which have blob in CPU context','line_number':1606,'multiline':False]
['text':' Only consider params that were created to be  "data parallel"','line_number':1649,'multiline':False]
['text':' We don't sync params if the model is shared','line_number':1698,'multiline':False]
['text':' Sanity check','line_number':1713,'multiline':False]
['text':' Remove duplicates and sort','line_number':1718,'multiline':False]
['text':' Sort first based on device id, and then by whole string','line_number':1722,'multiline':False]
['text':' GLOO operators expect the tensor addresses to remain same over','line_number':1758,'multiline':False]
['text':' iterations so we need to remove param grads from the dynamic memory','line_number':1759,'multiline':False]
['text':' management.','line_number':1760,'multiline':False]
['text':' Check if there is an existing CreateCommonWorld','line_number':1824,'multiline':False]
['text':' with the same timeout we're looking for. If so,','line_number':1825,'multiline':False]
['text':' we can clone it instead of creating a new one.','line_number':1826,'multiline':False]
['text':' Find common world timeout','line_number':1832,'multiline':False]
['text':' This common world was created with the same timeout we're','line_number':1841,'multiline':False]
['text':' looking for, so we can clone it','line_number':1842,'multiline':False]
['text':' Sanity','line_number':1959,'multiline':False]
