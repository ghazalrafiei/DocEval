['text':'# @package layers','line_number':1,'multiline':False]
['text':' Module caffe2.python.layers.layers','line_number':2,'multiline':False]
['text':' Some types to simplify descriptions of things traveling between ops','line_number':17,'multiline':False]
['text':' make sure not to set categorical_limit for a non-integer field','line_number':107,'multiline':False]
['text':' The layers support this context will accumulate predictions, labels,','line_number':127,'multiline':False]
['text':' weights. The accumulated data can later be used to compute','line_number':128,'multiline':False]
['text':' calibration or for other','line_number':129,'multiline':False]
['text':' purpose.','line_number':130,'multiline':False]
['text':' need to put the following line (shape) before initialier','line_number':172,'multiline':False]
['text':' shape will be updated once initializer is (re)set','line_number':173,'multiline':False]
['text':' ResetWorkspace to save memory','line_number':218,'multiline':False]
['text':' Contains features accessed in a model layer of a given type','line_number':246,'multiline':False]
['text':' `type`: A string representing the kind of feature, consistent with FeatureSpec','line_number':247,'multiline':False]
['text':' `ids`: A set of feature IDs that are accessed in the model layer','line_number':248,'multiline':False]
['text':' TODO(amalevich): Either return back to lambdas, that add','line_number':374,'multiline':False]
['text':' all params (looks a bit safer and breaking less','line_number':375,'multiline':False]
['text':' abstractions) or extend Net interface to this type of','line_number':376,'multiline':False]
['text':' operations better','line_number':377,'multiline':False]
['text':' TODO(xlwang) init_net._net.op has type google.protobuf.\','line_number':378,'multiline':False]
['text':' internal.containers.RepeatedCompositeFieldContainer, but','line_number':379,'multiline':False]
['text':' the version of protobuf in fbcode does not support append','line_number':380,'multiline':False]
['text':' so extend is used','line_number':381,'multiline':False]
['text':' do not add duplicated init ops','line_number':392,'multiline':False]
['text':' make sure we don't share parameters in the same layer','line_number':414,'multiline':False]
['text':' Namescope below should warranty that all intermediate blobs will be','line_number':429,'multiline':False]
['text':' assiciated with the layer that produces them','line_number':430,'multiline':False]
['text':' Predict layer implementation.','line_number':456,'multiline':False]
['text':' Default eval layer implementation is completely matching','line_number':460,'multiline':False]
['text':' predict layer implementation.','line_number':461,'multiline':False]
['text':' Default train layer implementation is completely matching','line_number':465,'multiline':False]
['text':' eval layer implementation.','line_number':466,'multiline':False]
['text':' This adds operators to accumulate predictions/labels/weights. The','line_number':470,'multiline':False]
['text':' accumulated data can later be used to compute calibration or for other','line_number':471,'multiline':False]
['text':' purpose. Default layer implementation is completely matching eval','line_number':472,'multiline':False]
['text':' layer implementation.','line_number':473,'multiline':False]
['text':' Export output of the layer directly','line_number':484,'multiline':False]
['text':' Export copies of parameters','line_number':491,'multiline':False]
