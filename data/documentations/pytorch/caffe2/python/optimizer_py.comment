['text':' @package optimizer','line_number':1,'multiline':False]
['text':' Module caffe2.python.optimizer','line_number':2,'multiline':False]
['text':' return a dict that contains attributes related to init args only','line_number':86,'multiline':False]
['text':' There is one interesting thing here: since we are minimizing, we are','line_number':141,'multiline':False]
['text':' doing "descent" so the learning rate is set to be negative.','line_number':142,'multiline':False]
['text':' We need to create a dummy learning rate operator to enforce that','line_number':199,'multiline':False]
['text':' iteration counter blob being placed in the trainer nodes. Otherwise,','line_number':200,'multiline':False]
['text':' the Automatic Device Placement (ADP) algorithm for Hierachical','line_number':201,'multiline':False]
['text':' Training (HT) will encounter issues to distribute blobs across group','line_number':202,'multiline':False]
['text':' parameter servers. Note that this learning rate operator will not be','line_number':203,'multiline':False]
['text':' used for any other purpose.','line_number':204,'multiline':False]
['text':' TODO(xlwang): In transfer learning, parameter initialized from pretrained','line_number':272,'multiline':False]
['text':' model might require a different learning rate than otherwise initialized.','line_number':273,'multiline':False]
['text':' To this end, here we implement a python solution where','line_number':274,'multiline':False]
['text':' `base_learning_rate` is scaled by `scale`, by calling','line_number':275,'multiline':False]
['text':' `scale_learning_rate`; Alternatively, we can achieve same effect by','line_number':276,'multiline':False]
['text':' rewriting the LearningRate operator in C++','line_number':277,'multiline':False]
['text':' Note that it is the responsibility of specific optimizer to decide what','line_number':278,'multiline':False]
['text':' logic should be used for `scale_learning_rate`','line_number':279,'multiline':False]
['text':' TODO(zqq): support LARS for sparse parameters','line_number':325,'multiline':False]
['text':' We need negative sign for LR when used directly with WeightedSum','line_number':348,'multiline':False]
['text':' below.','line_number':349,'multiline':False]
['text':' Each GPU/CPU must have its own ONE blob, thus modify the name','line_number':363,'multiline':False]
['text':' to include device information.','line_number':364,'multiline':False]
['text':' If we have a straight fp32 parameter, run the base class','line_number':438,'multiline':False]
['text':' Copy gradient to fp32','line_number':466,'multiline':False]
['text':' update (fused) in fp32','line_number':469,'multiline':False]
['text':' Copy updated param back to fp16','line_number':477,'multiline':False]
['text':' should only be triggered in FP16 training by SpatialBN, which','line_number':507,'multiline':False]
['text':' requires FP32 params in CuDNN.','line_number':508,'multiline':False]
['text':' doing a 32bit update','line_number':513,'multiline':False]
['text':' Have to assume param_info.blob is FP32 as there is no way','line_number':514,'multiline':False]
['text':' (that i currently know of) to query a blob's type in python','line_number':515,'multiline':False]
['text':' doing a 32bit update','line_number':521,'multiline':False]
['text':' Have to assume param_info.blob is FP32 as there is no way','line_number':522,'multiline':False]
['text':' (that i currently know of) to query a blob's type in python','line_number':523,'multiline':False]
['text':' flag set to 1, therefore doing FP32 update','line_number':579,'multiline':False]
['text':' Type/shape inference is not available for this param, fallback','line_number':810,'multiline':False]
['text':' on Shape/Slice logic','line_number':811,'multiline':False]
['text':' mask is provided through a db file','line_number':884,'multiline':False]
['text':' if mask_blob_name is not given use the param name to derive mask name','line_number':885,'multiline':False]
['text':' Skip weight decay for 1d parameters','line_number':993,'multiline':False]
['text':' See https://fburl.com/2jdiwrhy for context.','line_number':1598,'multiline':False]
['text':' Initialize "minibatch in which this parameter was last seen" for smart decay.','line_number':1655,'multiline':False]
['text':' Currently, only SparseAdam support RAdam, other Adam Ops will support later','line_number':1692,'multiline':False]
['text':' hack for position weighted.','line_number':1809,'multiline':False]
['text':' Note: This is number of persistent scalars in YellowFin optimizer.','line_number':1901,'multiline':False]
['text':'       It should always be the number of scalars being used. The same','line_number':1902,'multiline':False]
['text':'       number should be used in class for the operation.','line_number':1903,'multiline':False]
['text':' Infer blob devices by going through the net and param_init_net','line_number':2040,'multiline':False]
['text':' ops and observing the device used to create or use the blob.','line_number':2041,'multiline':False]
['text':' We first check if parameter's device has been inferred. If not,','line_number':2050,'multiline':False]
['text':' we check the gradient. This can happen if parameter is not output','line_number':2051,'multiline':False]
['text':' by any blob but created by a FetchBlob.','line_number':2052,'multiline':False]
['text':' Validate there are no duplicate params','line_number':2138,'multiline':False]
