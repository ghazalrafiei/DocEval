['text':' NOTE: During GPU stress tests, the number of workers exceeds the number','line_number':18,'multiline':False]
['text':'       of GPUs which results in flakiness from GPU contention. As a','line_number':19,'multiline':False]
['text':'       result, deadlines are not enforced on CUDA runs.','line_number':20,'multiline':False]
['text':' original dimension','line_number':54,'multiline':False]
['text':' data tensor','line_number':55,'multiline':False]
['text':' We only do gradient check with float32 types.','line_number':96,'multiline':False]
['text':' gradient not implemented yet','line_number':125,'multiline':False]
['text':' self.assertGradientChecks(gc, op, [X1, X2], 0, [0])','line_number':126,'multiline':False]
['text':' Make X1 and X2 far from each other, since X1=X2 is not differentiable','line_number':176,'multiline':False]
['text':' and the step size of gradient checker is 0.05','line_number':177,'multiline':False]
['text':' TODO(jiayq): enable gradient test when implemented.','line_number':202,'multiline':False]
['text':' Assume we are always operating on CUDA or CPU, since RNG is','line_number':285,'multiline':False]
['text':' inconsistent between CPU and GPU.','line_number':286,'multiline':False]
['text':' Reset each time because 'Y' may already exist in the workspace','line_number':294,'multiline':False]
['text':'   on a different device','line_number':295,'multiline':False]
['text':' TODO: "gru"','line_number':356,'multiline':False]
['text':'there's a bug in miopen for N=1 which would be resolved in the next release.','line_number':364,'multiline':False]
['text':' Random seed, this one happens to pass','line_number':367,'multiline':False]
['text':' set device option','line_number':370,'multiline':False]
['text':' unused in GRU','line_number':418,'multiline':False]
['text':' ignore C','line_number':421,'multiline':False]
['text':' Sets a unique dim and create the input.','line_number':440,'multiline':False]
['text':' Reference','line_number':449,'multiline':False]
['text':' Sets a unique dim and create the input.','line_number':470,'multiline':False]
['text':' Reference','line_number':479,'multiline':False]
['text':' Reference','line_number':547,'multiline':False]
['text':' Reference','line_number':589,'multiline':False]
['text':' python port of Sigrid's implementation','line_number':596,'multiline':False]
['text':' Reference','line_number':635,'multiline':False]
['text':' generate fake subset manually because hypothesis is too complicated :)','line_number':715,'multiline':False]
['text':' Reference','line_number':731,'multiline':False]
['text':' Reference','line_number':741,'multiline':False]
['text':' generate fake subset manually because hypothesis is too complicated :)','line_number':789,'multiline':False]
['text':' Reference','line_number':806,'multiline':False]
['text':' Validator','line_number':833,'multiline':False]
['text':' segment id starts with 0','line_number':1117,'multiline':False]
['text':' noqa','line_number':1290,'multiline':False]
['text':' Create all blobs before racing on multiple threads','line_number':1340,'multiline':False]
['text':' (blob creation is not threadsafe)','line_number':1341,'multiline':False]
['text':' Verify that the rows of the returned blob are a','line_number':1369,'multiline':False]
['text':' permutation. The order may be different due to','line_number':1370,'multiline':False]
['text':' different threads racing.','line_number':1371,'multiline':False]
['text':' Create BlobsQueue for each input queue','line_number':1448,'multiline':False]
['text':' Create multiple producer nets and one producer exist net','line_number':1457,'multiline':False]
['text':' Create one consumer dequeue net and one consumer exist net','line_number':1497,'multiline':False]
['text':' This golden array is dependent on the specified inp_sizes, out_sizes,','line_number':1610,'multiline':False]
['text':' tt_ranks, and seed. Changing these will cause the test to fail.','line_number':1611,'multiline':False]
['text':' index = np.array([0, 1, 2, 1, 4], dtype=int)','line_number':1634,'multiline':False]
['text':' lengths = np.array([3, 2], dtype=int)','line_number':1635,'multiline':False]
['text':' Build a binary tree of FC layers, summing at each node.','line_number':1857,'multiline':False]
['text':' init content and stop signal','line_number':2001,'multiline':False]
['text':' decide if disabled or not','line_number':2011,'multiline':False]
['text':' the body net is just to turn a 0 blob to 1','line_number':2021,'multiline':False]
['text':' always end the loop','line_number':2030,'multiline':False]
['text':' Casting from a float type outside the range of the integral','line_number':2134,'multiline':False]
['text':' type is UB.','line_number':2135,'multiline':False]
['text':' forward testing carried out in the full range of input','line_number':2162,'multiline':False]
['text':' to ensure original test coverage.','line_number':2163,'multiline':False]
['text':' gradient test carried out with reduced input range','line_number':2164,'multiline':False]
['text':' because the sharp increase of the logit curve at 0 and 1','line_number':2165,'multiline':False]
['text':' error increases dramtically when input is close to 0 or 1','line_number':2166,'multiline':False]
['text':' and it will fail the test.','line_number':2167,'multiline':False]
['text':' So we only run gradient test in the range of (0.01, 0.99)','line_number':2168,'multiline':False]
['text':' very occasionally, test may fail due to random accumulated error','line_number':2169,'multiline':False]
['text':' reduce test range to (0.02, 0.98) will improve test stability','line_number':2170,'multiline':False]
['text':' in opt mode, bool is converted into np.bool_','line_number':2204,'multiline':False]
['text':' TODO: name scope external inputs and outputs','line_number':2276,'multiline':False]
['text':' Initialize params for step net in the parent net','line_number':2287,'multiline':False]
['text':' TODO(jiayq): when there are backward and GPU implementations, enable','line_number':2574,'multiline':False]
['text':' these two.','line_number':2575,'multiline':False]
['text':' self.assertDeviceChecks(dc, op, [X, scale, bias], [0])','line_number':2576,'multiline':False]
['text':' self.assertGradientChecks(gc, op, [X, scale, bias], 0, [0])','line_number':2577,'multiline':False]
['text':' Cuda only support 32 bit float','line_number':2595,'multiline':False]
['text':' Cuda version only support int32','line_number':2599,'multiline':False]
['text':' values don't matter','line_number':2606,'multiline':False]
['text':' in place','line_number':2700,'multiline':False]
['text':' or not','line_number':2705,'multiline':False]
