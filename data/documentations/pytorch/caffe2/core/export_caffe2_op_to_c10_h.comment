['text':' This function is inline in the hope that compilers optimizing for speed will','line_number':39,'multiline':False]
['text':' inline it into call_caffe2_op_from_c10, allowing call_op to be inlined and','line_number':40,'multiline':False]
['text':' avoiding the function pointer indirection, while compilers optimizing for','line_number':41,'multiline':False]
['text':' binary size will keep it a separate function instead of inlining it into','line_number':42,'multiline':False]
['text':' a template and will reuse the binary code of this function between ops.','line_number':43,'multiline':False]
['text':' We measured and confirmed that binary size off the instagram ios app is','line_number':44,'multiline':False]
['text':' reduced when having _call_caffe2_op_from_c10 separate from the templated','line_number':45,'multiline':False]
['text':' call_caffe2_op_from_c10.','line_number':46,'multiline':False]
['text':' precondition: on the stack, there's one IValue for each argument of the','line_number':51,'multiline':False]
['text':' c10 schema. The last argument is an optional tensor list that','line_number':52,'multiline':False]
['text':' (if not ivalue::None) contains a preallocated output tensor for each','line_number':53,'multiline':False]
['text':' operator output.','line_number':54,'multiline':False]
['text':' As an invariant, we don't want any autograd gradients to be tracked in','line_number':56,'multiline':False]
['text':' Caffe2 operators.','line_number':57,'multiline':False]
['text':' -1 because the last argument is the list of preallocated tensors','line_number':67,'multiline':False]
['text':' either the schema doesn't support preallocated outputs or it does but','line_number':71,'multiline':False]
['text':' they haven't been passed in. Pass a list of uninitialized tensors to','line_number':72,'multiline':False]
['text':' the caffe2 operator as preallocated outputs.','line_number':73,'multiline':False]
['text':' TODO Avoid vector allocation. One idea would be to keep the std::vector','line_number':80,'multiline':False]
['text':' instances in the cache.','line_number':81,'multiline':False]
['text':' Convert outputs to caffe2::Tensor','line_number':84,'multiline':False]
['text':' postcondition: All inputs are cleared from the stack, there's now one','line_number':114,'multiline':False]
['text':'                IValue for each output which holds the result. This','line_number':115,'multiline':False]
['text':'                might reuse one of the preallocated tensors but doesn't have','line_number':116,'multiline':False]
['text':'                to.','line_number':117,'multiline':False]
['text':'opHandle','line_number':122,'multiline':True]
['text':' namespace detail','line_number':157,'multiline':False]
['text':' namespace caffe2','line_number':158,'multiline':False]
['text':'*
 * To register a caffe2 operator caffe2::MyOperator with the c10 dispatcher,
 * call:
 *
 * In caffe2/operators/MyOperator.h:
 *
 * > C10_DECLARE_EXPORT_CAFFE2_OP_TO_C10(C10MyOperator) // C10MyOperator is the
 * name
 *                                              // used by c10 for this operator
 *
 * In caffe2/operators/MyOperator.cc
 *
 * > C10_EXPORT_CAFFE2_OP_TO_C10_CPU (
 * >    C10MyOperator,
 * >    "_caffe2::C10MyOperator(Tensor input1, int argument2, float argument3)
 * -> (Tensor output1, Tensor output2)" > caffe2::MyOperator<caffe2::CPUContext>
 * // This is the caffe2 operator >                                           //
 * class template > )
 *
 * In caffe2/operators/MyOperator.cu
 *
 * > C10_EXPORT_CAFFE2_OP_TO_C10_CUDA(C10MyOperator ,
 *   caffe2::MyOperator<caffe2::CUDAContext>)
 *
 * Notes:
 * - all macros must be defined in the top level namespace, not in namespace
 *   caffe2.
 * - all operators must call C10_DECLARE_EXPORT_CAFFE2_OP_TO_C10 and
 *   C10_EXPORT_CAFFE2_OP_TO_C10_CPU .
 * - calling C10_EXPORT_CAFFE2_OP_TO_C10_CUDA is optional and can be omitted if
 *   you don't want to expose the operator for CUDA operations.
 * - caffe2 arguments must come after caffe2 inputs, in other words, any tensor
 *   inputs must precede any non-tensor inputs.
 *
 * More complex use cases:
 * - If your operator has a variable number of input tensors, make the first (!)
 *   input an input of type TensorList. There must be no other tensor inputs.
 ','line_number':160,'multiline':True]
['text':' Register the op schema with the c10 dispatcher ','line_number':207,'multiline':True]
['text':' Register call_caffe2_op_from_c10 as a kernel with the c10 dispatcher ','line_number':225,'multiline':True]
['text':' Register call_caffe2_op_from_c10 as a kernel with the c10 dispatcher ','line_number':248,'multiline':True]
['text':' You should never manually call the C10_EXPORT_CAFFE2_OP_TO_C10_HIP macro .','line_number':258,'multiline':False]
['text':' The C10_EXPORT_CAFFE2_OP_TO_C10_CUDA macro from above will be automatically','line_number':259,'multiline':False]
['text':' rewritten to C10_EXPORT_CAFFE2_OP_TO_C10_HIP by hipify .','line_number':260,'multiline':False]
['text':' Register call_caffe2_op_from_c10 as a kernel with the c10 dispatcher ','line_number':262,'multiline':True]
['text':' Don't use c10 dispatcher on mobile because of binary size','line_number':273,'multiline':False]
