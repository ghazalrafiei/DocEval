['text':'
   * Notes: All outputs ivalues must be tensors. Input ivalue list must start
   * with all tensors ("inputs" in caffe2 terminology),
   * followed by non-tensors ("arguments" in caffe2 terminology).
   * Alternatively, inputs can be one tensor list ivalue followed by non-tensors
   * to represent operators with a variable number of inputs.
   ','line_number':65,'multiline':True]
['text':'* @brief Return true if the operator was instantiated with OperatorDef
   * New operators should be instantiated with FunctionSchema
   ','line_number':82,'multiline':True]
['text':'* @brief Checks if the operator has an argument of the given name.
   ','line_number':104,'multiline':True]
['text':' Functions that deal with arguments. Basically, this allows us to map an','line_number':114,'multiline':False]
['text':' argument name to a specific type of argument that we are trying to access.','line_number':115,'multiline':False]
['text':' Get the inputs and outputs as specific types.','line_number':153,'multiline':False]
['text':' TODO(jerryzh): Remove template','line_number':171,'multiline':False]
['text':' and the type argument?','line_number':172,'multiline':False]
['text':' This is to keep the API changes minimal and make refactoring','line_number':173,'multiline':False]
['text':' a bit easier','line_number':174,'multiline':False]
['text':' TODO(jerryzh): We'll need to check device type in Get<T>() later','line_number':183,'multiline':False]
['text':' Get<T>() -> Get<T>(type)','line_number':184,'multiline':False]
['text':' if the first input is a tensor list, we get input tensors by indexing','line_number':199,'multiline':False]
['text':' into that list. currently, this means that only tensors from that list','line_number':200,'multiline':False]
['text':' are accessible as inputs. any hypothetical input tensors that come','line_number':201,'multiline':False]
['text':' after the list are not accessible.','line_number':202,'multiline':False]
['text':' if the first input is not a tensor list, we get input tensors by','line_number':207,'multiline':False]
['text':' indexing into the inputs.','line_number':208,'multiline':False]
['text':' TODO(jerryzh): Remove this template','line_number':241,'multiline':False]
['text':' When you get a Tensor here it is not fully initialized','line_number':248,'multiline':False]
['text':' Fix tensor type','line_number':255,'multiline':False]
['text':' update the tensor in the workspace','line_number':285,'multiline':False]
['text':' Get output Tensor of the operator and CopyFrom the given Tensor','line_number':318,'multiline':False]
['text':' Ouptut Tensor will always have the same data type as `src`','line_number':327,'multiline':False]
['text':' Check whether output j is an alias of input i by comparing Blob pointers,','line_number':369,'multiline':False]
['text':' note this does not check if the two Blobs points to the same Tensor, or if','line_number':370,'multiline':False]
['text':' the Tensor pointers point to the same TensorImpl, or if the Storages alias','line_number':371,'multiline':False]
['text':'stream_id ','line_number':446,'multiline':True]
['text':'stream_id','line_number':458,'multiline':True]
['text':' unused ','line_number':470,'multiline':True]
['text':'stream_id','line_number':470,'multiline':True]
['text':' RunAsync, if implemented by the specific operators, will schedule the','line_number':486,'multiline':False]
['text':' computation on the corresponding context and record the event in its','line_number':487,'multiline':False]
['text':' event_ member object. If the specific operator does not support RunAsync,','line_number':488,'multiline':False]
['text':' it will simply be synchronous as a fallback.','line_number':489,'multiline':False]
['text':' Internal API invoked by observers. Normal callers shouldn't invoke it.','line_number':558,'multiline':False]
['text':' Checks whether stream is ready to execute new computation,','line_number':563,'multiline':False]
['text':' used in stream allocation optimization to skip stream that is currently','line_number':564,'multiline':False]
['text':' busy. Depends on context and operator's device, returns true by default','line_number':565,'multiline':False]
['text':' unused ','line_number':566,'multiline':True]
['text':' Preferably use c10::optional, but nvcc doesn't work','line_number':608,'multiline':False]
['text':' HACK','line_number':614,'multiline':False]
['text':' We preserve the fact that Output() returns Tensor*','line_number':615,'multiline':False]
['text':' by storing Tensor in a vector owned by the','line_number':616,'multiline':False]
['text':' operator.','line_number':617,'multiline':False]
['text':'err_msg','line_number':628,'multiline':True]
['text':' An event used by asynchronous execution.','line_number':654,'multiline':False]
['text':' We need this specialisation because IValue based lists don't support','line_number':711,'multiline':False]
['text':' int16_t. We need to load it as List<int64_t> and transform to int16_t.','line_number':712,'multiline':False]
['text':' OP_SINGLE_ARG provides a shorter initialization choice for initialization of','line_number':726,'multiline':False]
['text':' member variables for the class constructors.','line_number':727,'multiline':False]
['text':' INPUT_TAGS and OUTPUT_TAGS are optional features to name the indices of the','line_number':731,'multiline':False]
['text':' operator's inputs and outputs, in order to avoid confusion. For example, for','line_number':732,'multiline':False]
['text':' a fully convolution layer that has input, weight and bias, you can define its','line_number':733,'multiline':False]
['text':' input tags as:','line_number':734,'multiline':False]
['text':'     INPUT_TAGS(INPUT, WEIGHT, BIAS);','line_number':735,'multiline':False]
['text':' And in the code, instead of doing','line_number':736,'multiline':False]
['text':'     auto& weight = Input(1);','line_number':737,'multiline':False]
['text':' you can now do','line_number':738,'multiline':False]
['text':'     auto& weight = Input(WEIGHT);','line_number':739,'multiline':False]
['text':' to make it more clear.','line_number':740,'multiline':False]
['text':' We need this specialisation because IValue based lists don't support','line_number':766,'multiline':False]
['text':' int16_t. We need to load it as List<int64_t> and transform to int16_t.','line_number':767,'multiline':False]
['text':' Operator is the class that you usually want to derive, if your operator will','line_number':794,'multiline':False]
['text':' run on different devices. You should then implement the RunOnDevice()','line_number':795,'multiline':False]
['text':' function.','line_number':796,'multiline':False]
['text':' In the constructor, we switch to the device so that the child class','line_number':802,'multiline':False]
['text':' constructors will run on that device.','line_number':803,'multiline':False]
['text':' In the constructor, we switch to the device so that the child class','line_number':814,'multiline':False]
['text':' constructors will run on that device.','line_number':815,'multiline':False]
['text':'/ Retrieve a non-owning reference to the input at position 'idx' for this','line_number':821,'multiline':False]
['text':'/ operator.  The returned reference is valid for the duration of the','line_number':822,'multiline':False]
['text':'/ RunOnDevice call.  The optional 'type' parameter can be used to assert a','line_number':823,'multiline':False]
['text':'/ required device type for the input (by default, we assert that the tensor','line_number':824,'multiline':False]
['text':'/ is consistent with the device type implied by the Context parameter of an','line_number':825,'multiline':False]
['text':'/ Operator.)','line_number':826,'multiline':False]
['text':'/ XOutput is a modernized version of Output which returns a Tensor','line_number':833,'multiline':False]
['text':'/ rather than a Tensor* (the raw pointer in the latter case is','line_number':834,'multiline':False]
['text':'/ useless, as Tensor is a pointer type.)','line_number':835,'multiline':False]
['text':' We'll default device to the device of the current Operator Context','line_number':837,'multiline':False]
['text':'/ Retrieve a non-owning pointer to the output at position 'idx',','line_number':845,'multiline':False]
['text':'/ initializing it to have size 'dims' and properties 'options' if','line_number':846,'multiline':False]
['text':'/ there is no pre-existing output or the pre-existing output does','line_number':847,'multiline':False]
['text':'/ not have the correct options.  The returned pointer is valid for','line_number':848,'multiline':False]
['text':'/ the duration of the RunOnDevice call.  If device is not explicitly','line_number':849,'multiline':False]
['text':'/ specified in options, we default to allocating output on the','line_number':850,'multiline':False]
['text':'/ current device of the device type implied by the Context parameter','line_number':851,'multiline':False]
['text':'/ of this Operator.','line_number':852,'multiline':False]
['text':'/','line_number':853,'multiline':False]
['text':'/ Note [Operator::Output what?]','line_number':854,'multiline':False]
['text':'/ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':855,'multiline':False]
['text':'/ The contract of Operator::Output is somewhat complex; it is perhaps better','line_number':856,'multiline':False]
['text':'/ understood in terms of what was historically an idiomatic Caffe2 operator','line_number':857,'multiline':False]
['text':'/ implementation:','line_number':858,'multiline':False]
['text':'/','line_number':859,'multiline':False]
['text':'/     void RunOnDevice() override {','line_number':860,'multiline':False]
['text':'/         auto* output = Output(0, output_size, dtype<float>());','line_number':861,'multiline':False]
['text':'/         float* output_ptr = output->data<float>();','line_number':862,'multiline':False]
['text':'/         // write into output_ptr','line_number':863,'multiline':False]
['text':'/     }','line_number':864,'multiline':False]
['text':'/','line_number':865,'multiline':False]
['text':'/ In the simple case, this code does the following things:','line_number':866,'multiline':False]
['text':'/','line_number':867,'multiline':False]
['text':'/   1. Allocates a new tensor with size 'output_size' and dtype 'float'','line_number':868,'multiline':False]
['text':'/      (and device type whatever the Operator's device type is)','line_number':869,'multiline':False]
['text':'/   2. "Registers" this tensor as the 0th output tensor of this operator','line_number':870,'multiline':False]
['text':'/      (Caffe2 operators don't "return" outputs; instead, outputs','line_number':871,'multiline':False]
['text':'/      are shoved into an output vector which the executor reads out.)','line_number':872,'multiline':False]
['text':'/   3. Returns the tensor, so the operator implementation can write','line_number':873,'multiline':False]
['text':'/      the actual output data into the tensor.','line_number':874,'multiline':False]
['text':'/','line_number':875,'multiline':False]
['text':'/ So what's this business with "pre-existing" outputs?  Caffe2','line_number':876,'multiline':False]
['text':'/ commonly applies an optimization whereby it reuses tensors on','line_number':877,'multiline':False]
['text':'/ subsequent runs of operators in a graph.  It doesn't know ahead','line_number':878,'multiline':False]
['text':'/ of time what intermediate tensors it will need, so the first','line_number':879,'multiline':False]
['text':'/ time it runs a graph it has all of the operators create the outputs','line_number':880,'multiline':False]
['text':'/ necessary (as described above).  However, the second time around,','line_number':881,'multiline':False]
['text':'/ it will reuse all of the tensors created from the first time.','line_number':882,'multiline':False]
['text':'/ If they are lucky, this time the Output() call is a no-op and','line_number':883,'multiline':False]
['text':'/ just returns the old tensor.','line_number':884,'multiline':False]
['text':'/','line_number':885,'multiline':False]
['text':'/ However, we cannot /guarantee/ that the output size will be the','line_number':886,'multiline':False]
['text':'/ same the next time the Operator is called; for example, output','line_number':887,'multiline':False]
['text':'/ size may be data dependent and vary between runs.  In this case,','line_number':888,'multiline':False]
['text':'/ we have to resize it to the correct size.  Resizing is still','line_number':889,'multiline':False]
['text':'/ helpful, as we may be able to fit the output in the same','line_number':890,'multiline':False]
['text':'/ space that was previously used.','line_number':891,'multiline':False]
['text':'/','line_number':892,'multiline':False]
['text':' We'll default device to the device of the current Operator Context','line_number':894,'multiline':False]
['text':'/ Legacy: please consider using the version of Output() which also takes','line_number':902,'multiline':False]
['text':'/ dtype and size as arguments.','line_number':903,'multiline':False]
['text':'/ Get the output Tensor of an operator (allocating it if it is not','line_number':908,'multiline':False]
['text':'/ already initialized), and copy the contents of src into it.','line_number':909,'multiline':False]
['text':'/ You probably don't actually want to use this function (the fact','line_number':910,'multiline':False]
['text':'/ that you have a Tensor to copy from is probably a mistake:','line_number':911,'multiline':False]
['text':'/ you should have written the output into the output tensor,','line_number':912,'multiline':False]
['text':'/ from Output, directly in the first place), but this method','line_number':913,'multiline':False]
['text':'/ is situationally useful.','line_number':914,'multiline':False]
['text':' The run function of Operator switches to the device, and then carries out','line_number':944,'multiline':False]
['text':' the actual computation with RunOnDevice(). You should implement RunOnDevice','line_number':945,'multiline':False]
['text':' instead of Run().','line_number':946,'multiline':False]
['text':' Note: Run does not update operator's event and can be used only with','line_number':947,'multiline':False]
['text':' non-async executors that do not rely on events','line_number':948,'multiline':False]
['text':' Clear floating point exception flags before RunOnDevice. We will test','line_number':955,'multiline':False]
['text':' exception flags afterwards, and raise an error if an exception has','line_number':956,'multiline':False]
['text':' happened.','line_number':957,'multiline':False]
['text':' If glibc is available, use feenableexcept that will raise exception','line_number':964,'multiline':False]
['text':' right away.','line_number':965,'multiline':False]
['text':' throws on error','line_number':1008,'multiline':False]
['text':' Manually set CPU operator's event status to finished,','line_number':1039,'multiline':False]
['text':' unless this is an async CPU operator','line_number':1040,'multiline':False]
['text':' Returns whether operator has async on device part.','line_number':1080,'multiline':False]
['text':' CUDA operators by default have async parts, CPU operators by default','line_number':1081,'multiline':False]
['text':' don't have async parts and are finished after RunOnDevice call.','line_number':1082,'multiline':False]
['text':' Events of operators that don't have async parts are automatically set','line_number':1083,'multiline':False]
['text':' to finished state by RunAsync.','line_number':1084,'multiline':False]
['text':' Defaulting to the value from context (true for CUDA, false for CPU).','line_number':1085,'multiline':False]
['text':' Override in case of async CPU operators','line_number':1086,'multiline':False]
['text':' Async CPU operators are expected to catch all exceptions in async parts','line_number':1087,'multiline':False]
['text':' and set Event to finished/failed state with Event::SetFinished or','line_number':1088,'multiline':False]
['text':' SetFinishedWithException call.','line_number':1089,'multiline':False]
['text':' Returns whether operator's RunOnDevice schedules async on device part and','line_number':1094,'multiline':False]
['text':' can be run without waiting for parent operator's async part to be finished','line_number':1095,'multiline':False]
['text':' on the same device.','line_number':1096,'multiline':False]
['text':' Note: when true, RunOnDevice must not access the content of the input blobs','line_number':1097,'multiline':False]
['text':' as they might not be computed yet','line_number':1098,'multiline':False]
['text':' Note: when true, operator's device needs to support async scheduling:','line_number':1099,'multiline':False]
['text':'  - supports concept of streams: async ops scheduled on the same stream are','line_number':1100,'multiline':False]
['text':'    guaranteed to be executed in the same order they were scheduled','line_number':1101,'multiline':False]
['text':'  - provides non-blocking cross device/cross stream synchronization','line_number':1102,'multiline':False]
['text':'    primitives','line_number':1103,'multiline':False]
['text':'','line_number':1104,'multiline':False]
['text':' By default, assuming an op with an async part can be scheduled','line_number':1105,'multiline':False]
['text':' asynchronously if device supports async scheduling','line_number':1106,'multiline':False]
['text':' using override ','line_number':1133,'multiline':True]
['text':' using override ','line_number':1134,'multiline':True]
['text':' using override ','line_number':1135,'multiline':True]
['text':' using override ','line_number':1136,'multiline':True]
['text':' using override ','line_number':1137,'multiline':True]
['text':' using override ','line_number':1138,'multiline':True]
['text':' using override ','line_number':1139,'multiline':True]
['text':' using override ','line_number':1140,'multiline':True]
['text':' using override ','line_number':1141,'multiline':True]
['text':' using override ','line_number':1142,'multiline':True]
['text':' using override ','line_number':1143,'multiline':True]
['text':' using override ','line_number':1147,'multiline':True]
['text':' using override ','line_number':1148,'multiline':True]
['text':' using override ','line_number':1149,'multiline':True]
['text':' using override ','line_number':1150,'multiline':True]
['text':' using override ','line_number':1151,'multiline':True]
['text':' using override ','line_number':1152,'multiline':True]
['text':' Helpers to implement runtime op polymorphism. Often it's convenient to make','line_number':1162,'multiline':False]
['text':' an op work on different input types (e.g. i32 vs i64 indices) or special-case','line_number':1163,'multiline':False]
['text':' it for particular input size (e.g. ScatterWeightedSum for block size of 1','line_number':1164,'multiline':False]
['text':' doesn't need to call Eigen).','line_number':1165,'multiline':False]
['text':'','line_number':1166,'multiline':False]
['text':' DispatchHelper provides compile-time generation of nested "if" statements,','line_number':1167,'multiline':False]
['text':' e.g. `DispatchHelper<FixedValues<1, 4>>::call(this, block_size);`','line_number':1168,'multiline':False]
['text':' unrolls into:','line_number':1169,'multiline':False]
['text':'   if (block_size == 1) {','line_number':1170,'multiline':False]
['text':'     return DoRunWithValue<1>();','line_number':1171,'multiline':False]
['text':'   } else if (block_size = 4) {','line_number':1172,'multiline':False]
['text':'     return DoRunWithValue<4>();','line_number':1173,'multiline':False]
['text':'   } else {','line_number':1174,'multiline':False]
['text':'     return DoRunWithValue<-1>();','line_number':1175,'multiline':False]
['text':'   }`','line_number':1176,'multiline':False]
['text':'','line_number':1177,'multiline':False]
['text':' DoRunWithValue implementation can use template arguments to do "if"','line_number':1178,'multiline':False]
['text':' statements','line_number':1179,'multiline':False]
['text':' or proxy to functions in math.h which often provide fixed size','line_number':1180,'multiline':False]
['text':' implementation.','line_number':1181,'multiline':False]
['text':'','line_number':1182,'multiline':False]
['text':' Similarly `TensorTypes<int32_t, int64_t>(this, Input(0))` provides branching','line_number':1183,'multiline':False]
['text':' based on type of the first input and calls DoRunWithType.','line_number':1184,'multiline':False]
['text':'','line_number':1185,'multiline':False]
['text':' Note, that the same instance of Op class is used as the method, not class is','line_number':1186,'multiline':False]
['text':' templated. We might consider adding static class-level polymorphism later.','line_number':1187,'multiline':False]
['text':'','line_number':1188,'multiline':False]
['text':' Convenient macro USE_DISPATCH_HELPER is provided for declaring friendship in','line_number':1189,'multiline':False]
['text':' case DoRunWithValue or DoRunWithType are declared non-public.','line_number':1190,'multiline':False]
['text':' Special tag that can be listed in TensorTypes to denote that a special','line_number':1202,'multiline':False]
['text':' implementation in 'RunWithOtherType' needs to be called instead of failing','line_number':1203,'multiline':False]
['text':' Obviously this needs to be the last item in lists, e.g.','line_number':1204,'multiline':False]
['text':' TensorTypes<float, double, GenericTensorImplementation>','line_number':1205,'multiline':False]
['text':' Same as TensorTypes but call DoRunWithType2','line_number':1208,'multiline':False]
['text':'size','line_number':1230,'multiline':True]
['text':' unused ','line_number':1263,'multiline':True]
['text':' The device type registry. This works in two phases:','line_number':1303,'multiline':False]
['text':' (1) gDeviceTypeRegistry() maps the device types values to the actual operator','line_number':1304,'multiline':False]
['text':'     registry function.','line_number':1305,'multiline':False]
['text':' (2) Then, one can call the operator registry function to further create the','line_number':1306,'multiline':False]
['text':'     operators.','line_number':1307,'multiline':False]
['text':' The operator registry. Since we are not expecting a great number of devices,','line_number':1337,'multiline':False]
['text':' we will simply have an if-then type command and allocate the actual','line_number':1338,'multiline':False]
['text':' generation to device-specific registerers.','line_number':1339,'multiline':False]
['text':' Note that although we have CUDA and CUDNN here, the registerers themselves do','line_number':1340,'multiline':False]
['text':' not depend on specific cuda or cudnn libraries. This means that we will be','line_number':1341,'multiline':False]
['text':' able to compile it even when there is no cuda available - we simply do not','line_number':1342,'multiline':False]
['text':' link any cuda or cudnn operators.','line_number':1343,'multiline':False]
['text':' Use these macros to register gradient operators.  They can be automatically','line_number':1363,'multiline':False]
['text':' excluded from builds that don't need them (e.g., mobile).','line_number':1364,'multiline':False]
['text':' No gradients. ','line_number':1366,'multiline':True]
['text':' No gradients. ','line_number':1373,'multiline':True]
['text':' Macros for cudnn since we use it often','line_number':1398,'multiline':False]
['text':' Macros for HIP operators','line_number':1402,'multiline':False]
['text':' Make CUDNN an alias of MIOPEN for HIP ops','line_number':1425,'multiline':False]
['text':' StaticLinkingProtector is a helper class that ensures that the Caffe2','line_number':1427,'multiline':False]
['text':' library is linked correctly with whole archives (in the case of static','line_number':1428,'multiline':False]
['text':' linking). What happens is that when CreateOperator is called for the first','line_number':1429,'multiline':False]
['text':' time, it instantiates an OperatorLinkingProtector object to check if the','line_number':1430,'multiline':False]
['text':' operator registry is empty. If it is empty, this means that we are not','line_number':1431,'multiline':False]
['text':' properly linking the library.','line_number':1432,'multiline':False]
['text':'','line_number':1433,'multiline':False]
['text':' You should not need to use this class.','line_number':1434,'multiline':False]
['text':' Note: this is a check failure instead of an exception, because if','line_number':1438,'multiline':False]
['text':' the linking is wrong, Caffe2 won't be able to run properly anyway,','line_number':1439,'multiline':False]
['text':' so it's better to fail loud.','line_number':1440,'multiline':False]
['text':' If Caffe2 is properly linked with whole archive, there should be more','line_number':1441,'multiline':False]
['text':' than zero registered ops.','line_number':1442,'multiline':False]
['text':' An exception that can be thrown by an operator constructor that notifies','line_number':1453,'multiline':False]
['text':' that it does not support the given setting. This can be usually used for','line_number':1454,'multiline':False]
['text':' specific engines that only implement a subset of the features required by','line_number':1455,'multiline':False]
['text':' the original operator schema.','line_number':1456,'multiline':False]
['text':' TODO(jiayq): make more feature-complete exception message.','line_number':1457,'multiline':False]
['text':' A helper macro that should ONLY be used in the operator constructor to check','line_number':1469,'multiline':False]
['text':' if needed features are met. If not, throws the UnsupportedOperatorFeature','line_number':1470,'multiline':False]
['text':' exception with the given message.','line_number':1471,'multiline':False]
['text':' Creates an operator with the given operator definition.','line_number':1477,'multiline':False]
['text':' Throws on error and never returns nullptr','line_number':1478,'multiline':False]
['text':' User can set the preferred engines as a list of engine names, in','line_number':1488,'multiline':False]
['text':' descending order of preference.','line_number':1489,'multiline':False]
['text':' {device_type -> {operator_name -> EnginePrefType}}','line_number':1491,'multiline':False]
['text':' {device_type -> EnginePrefType}','line_number':1494,'multiline':False]
['text':' Get a set of registered operator names','line_number':1535,'multiline':False]
['text':' Operator logging capabilities','line_number':1538,'multiline':False]
['text':' This is for transferring tensor data between C2 and backends.','line_number':1544,'multiline':False]
['text':' C10_MOBILE','line_number':1594,'multiline':False]
['text':' namespace caffe2','line_number':1596,'multiline':False]
['text':' CAFFE2_CORE_OPERATOR_H_','line_number':1600,'multiline':False]
