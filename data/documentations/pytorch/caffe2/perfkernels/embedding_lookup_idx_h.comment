['text':' clang-format off','line_number':7,'multiline':False]
['text':'*
 * Embedding lookup with reduction.
 *
 * `input` of size data_size * block_size
 * `indices` of size index_size
 * `offsets` of size output_size
 * `weights` nullptr or array of size index_size
 * `out` of size output_size * block_size
 *
 * Behavior is roughly equivalent to pseudocode:
 *
 * pos = 0
 * for (i = 0..output_size-1)
 *   for (k = 0..block_size-1)
 *     out[i*block_size + k] = 0
 *   start_offset = offsets[i]
 *   end_offset = offsets[i+1]
 *   length = end_offset - start_offset
 *   for (j = start_offset..end_offset-1)
 *     for (k = 0..block_size-1)
 *       out[i*block_size + k] += input[indices[pos]*block_size + k] *
 *           (weights ? weights[IS_WEIGHT_POSITIONAL ? j - start_offset : pos] : 1.0)
 *     pos += 1
 *   if (normalize_weights && length > 0)
 *     for (k = 0..block_size-1)
 *       out[i*block_size + k] /= length
 *
 * TODO: make this API also take "offsets" rather than "lengths" to match the
 *       API for PyTorch's EmbeddingBag
 ','line_number':8,'multiline':True]
['text':' clang-format on','line_number':38,'multiline':False]
['text':' optional, can be null for non-weighted sum','line_number':52,'multiline':False]
['text':' optional scale & bias params for uint8 input','line_number':53,'multiline':False]
['text':' namespace caffe2','line_number':57,'multiline':False]
