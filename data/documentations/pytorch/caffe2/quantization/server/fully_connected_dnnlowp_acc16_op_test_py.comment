['text':' correctness test with no quantization error in inputs','line_number':22,'multiline':False]
['text':' fbgemm currently only supports N a multiple of 64','line_number':23,'multiline':False]
['text':' X and W have scale 1, so exactly represented after quantization','line_number':42,'multiline':False]
['text':' This was made sure by having at least one 0 and one 255 for unsigned','line_number':43,'multiline':False]
['text':' 8-bit tensors, and at least one -128 and one 127 for signed 8-bit','line_number':44,'multiline':False]
['text':' tensors.','line_number':45,'multiline':False]
['text':' Since fbgemm_acc16 accumulates to 16-bit, To avoid overflow, we use','line_number':46,'multiline':False]
['text':' small numbers except for those 0, 255, -128, and 127, for this test','line_number':47,'multiline':False]
['text':' We also make sure 255, -128, or 127 are not multiplied together by','line_number':48,'multiline':False]
['text':' putting them in different input channels and the corresponding input','line_number':49,'multiline':False]
['text':' channel in other matrix is 0.','line_number':50,'multiline':False]
['text':' For example, we put 255 in input channel 1 in X, so we make the','line_number':51,'multiline':False]
['text':' corresponding input channel in W all zeros.','line_number':52,'multiline':False]
['text':' No input quantization error in bias','line_number':71,'multiline':False]
['text':' X and W have scale 1, so exactly represented after quantization','line_number':139,'multiline':False]
['text':' This was made sure by having at least one 0 and one 255 for unsigned','line_number':140,'multiline':False]
['text':' 8-bit tensors, and at least one -128 and one 127 for signed 8-bit','line_number':141,'multiline':False]
['text':' tensors.','line_number':142,'multiline':False]
['text':' Since fbgemm_acc16 accumulates to 16-bit, To avoid overflow, we use','line_number':143,'multiline':False]
['text':' small numbers except for those 0, 255, -128, and 127, for this test','line_number':144,'multiline':False]
['text':' We also make sure 255, -128, or 127 are not multiplied together by','line_number':145,'multiline':False]
['text':' putting them in different input channels and the corresponding input','line_number':146,'multiline':False]
['text':' channel in other matrix is 0.','line_number':147,'multiline':False]
['text':' For example, we put 255 in input channel 1 in X, so we make the','line_number':148,'multiline':False]
['text':' corresponding input channel in W all zeros.','line_number':149,'multiline':False]
['text':' No input quantization error in bias','line_number':167,'multiline':False]
