['text':' correctness test with no quantization error in inputs','line_number':23,'multiline':False]
['text':' X and W have scale 1, so exactly represented after quantization','line_number':58,'multiline':False]
['text':' input channels 0 and 1 are all X_min to avoid overflow from vpmaddubsw','line_number':65,'multiline':False]
['text':' when multiplied with W_min and W_max','line_number':66,'multiline':False]
['text':' Make sure we won't have overflows from vpmaddubsw instruction used in','line_number':84,'multiline':False]
['text':' fbgemm','line_number':85,'multiline':False]
['text':' type, engine, do_fuse, skip_requantization','line_number':108,'multiline':False]
['text':' Bias','line_number':154,'multiline':False]
['text':' "quant_param",','line_number':184,'multiline':False]
['text':' When quantized weight is provided, we can't rescale the','line_number':200,'multiline':False]
['text':' output dynamically by looking at the range of output of each','line_number':201,'multiline':False]
['text':' batch, so here we provide the range of output observed from','line_number':202,'multiline':False]
['text':' fp32 reference implementation','line_number':203,'multiline':False]
