['text':' for fused static int8 not valid if less than 0','line_number':37,'multiline':False]
['text':' if (VLOG_IS_ON(3)) ','line_number':108,'multiline':True]
['text':' float in float out, two possibilities','line_number':117,'multiline':False]
['text':' if there are only 3 input (no qparams): dyanmic','line_number':118,'multiline':False]
['text':' if there are 5 input (+ input qparams): fused int8 static','line_number':119,'multiline':False]
['text':' output qparams need to be added anyway even it's dummy when dequantize_output=1','line_number':120,'multiline':False]
['text':' input_params overwrite input arguments','line_number':123,'multiline':False]
['text':' Get quantization parameters','line_number':128,'multiline':False]
['text':' if (VLOG_IS_ON(3)) ','line_number':134,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':164,'multiline':True]
['text':' fast path to use fbgemm','line_number':175,'multiline':False]
['text':' Only when input and output are float, we don't need input to be','line_number':179,'multiline':False]
['text':' quantized.','line_number':180,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':183,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':189,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':201,'multiline':True]
['text':' buffer for packed matrix','line_number':222,'multiline':False]
['text':' group','line_number':223,'multiline':False]
['text':' thread_id','line_number':247,'multiline':False]
['text':' num_threads','line_number':248,'multiline':False]
['text':' thread_id','line_number':269,'multiline':False]
['text':' num_threads','line_number':270,'multiline':False]
['text':' buffer for packed matrix','line_number':281,'multiline':False]
['text':' group','line_number':282,'multiline':False]
['text':' thread_id','line_number':303,'multiline':False]
['text':' num_threads','line_number':304,'multiline':False]
['text':' dequantize_output','line_number':307,'multiline':False]
['text':' Both input and output are float','line_number':311,'multiline':False]
['text':' the path for dyanmic and fused staic','line_number':312,'multiline':False]
['text':' buffer for packed matrix','line_number':323,'multiline':False]
['text':' groups','line_number':326,'multiline':False]
['text':' bias','line_number':341,'multiline':False]
['text':' thread_id','line_number':351,'multiline':False]
['text':' num_threads','line_number':352,'multiline':False]
['text':' bias','line_number':362,'multiline':False]
['text':' thread_id','line_number':372,'multiline':False]
['text':' num_threads','line_number':373,'multiline':False]
['text':' Input quantized and output float','line_number':376,'multiline':False]
['text':' buffer for packed matrix','line_number':385,'multiline':False]
['text':' group','line_number':386,'multiline':False]
['text':' bias','line_number':401,'multiline':False]
['text':' thread_id','line_number':411,'multiline':False]
['text':' num_threads','line_number':412,'multiline':False]
['text':' bias','line_number':422,'multiline':False]
['text':' thread_id','line_number':432,'multiline':False]
['text':' num_threads','line_number':433,'multiline':False]
['text':' dequantize_output','line_number':436,'multiline':False]
['text':' Quantize X','line_number':438,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':441,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':448,'multiline':True]
['text':' #define DNNLOWP_DETAILED_LOG_IN_SLOW_PATH','line_number':458,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':472,'multiline':False]
['text':' for each output element','line_number':497,'multiline':False]
['text':' for each row','line_number':498,'multiline':False]
['text':' Expose the quantized X, W and Y for debugging if debug outputs are','line_number':500,'multiline':False]
['text':' attached to the operator and caffe2_dnnlowp_force_slow_path flag is set','line_number':501,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':518,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':522,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':526,'multiline':False]
['text':' Dump input activation','line_number':541,'multiline':False]
['text':' Dump weight','line_number':544,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':549,'multiline':True]
['text':' Adjust with bias and zero_point and then requantize','line_number':559,'multiline':False]
['text':' See batch_matmul_dnnlowp_op.cc to why we compute column_offsets,','line_number':560,'multiline':False]
['text':' row_offset, and const_offset in this way.','line_number':561,'multiline':False]
['text':' empty column offset means it's folded into bias','line_number':600,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':625,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':651,'multiline':True]
['text':' Choose quantization for X','line_number':655,'multiline':False]
['text':' non-fused static or Dynamic','line_number':656,'multiline':False]
['text':' fused int8 static','line_number':659,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':665,'multiline':True]
['text':' Quantize W','line_number':675,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':709,'multiline':False]
['text':' fast path using fbgemm','line_number':716,'multiline':False]
['text':' ld','line_number':728,'multiline':False]
['text':' is_weight_constant_','line_number':749,'multiline':False]
['text':' !is_weight_constant_','line_number':751,'multiline':False]
['text':'weight','line_number':754,'multiline':True]
['text':' if (VLOG_IS_ON(1)) ','line_number':766,'multiline':True]
['text':' Pre-compute column_offset','line_number':775,'multiline':False]
['text':' If input tensor doesn't use dynamic quantization, we fold column_offsets_','line_number':776,'multiline':False]
['text':' into bias.','line_number':777,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':794,'multiline':True]
['text':' Quantize bias','line_number':804,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':817,'multiline':False]
['text':' If column_offsets_ is empty even when we need column_offsets (asymmetric','line_number':860,'multiline':False]
['text':' quantization in input), it means we need to fuse column_offsets to bias.','line_number':861,'multiline':False]
['text':' When b_quantized_data_ is from pre-packed bias or Int8TensorCPU,','line_number':865,'multiline':False]
['text':' we can't inplace modify so copy to internal b_quantized_ vector.','line_number':866,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':870,'multiline':False]
['text':' From here, W_quantized_ is not used anymore when we have Wq_packed_','line_number':895,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':900,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':922,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':932,'multiline':False]
['text':' to measure quantization error, run ref impl.','line_number':944,'multiline':False]
['text':' if (VLOG_IS_ON(1)) ','line_number':951,'multiline':True]
['text':' NOLINTNEXTLINE(modernize-avoid-bind)','line_number':1007,'multiline':False]
['text':' NOLINTNEXTLINE(modernize-avoid-bind)','line_number':1009,'multiline':False]
['text':' namespace caffe2','line_number':1012,'multiline':False]
