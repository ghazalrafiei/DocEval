['text':' correctness test with no quantization error in inputs','line_number':23,'multiline':False]
['text':' X has scale 1, so exactly represented after quantization','line_number':44,'multiline':False]
['text':' input channels 0 and 1 are all X_min to avoid overflow from vpmaddubsw','line_number':51,'multiline':False]
['text':' when multiplied with W_min and W_max','line_number':52,'multiline':False]
['text':' Each row of W has scale 1 but with different offset, so row-wise','line_number':57,'multiline':False]
['text':' quantization shouldn't have any input quantization error.','line_number':58,'multiline':False]
['text':' Make sure we won't have overflows from vpmaddubsw instruction used in','line_number':68,'multiline':False]
['text':' fbgemm','line_number':69,'multiline':False]
['text':' When pre-packed quantized weight is provided, we can't rescale','line_number':141,'multiline':False]
['text':' the output dynamically by looking at the range of output of','line_number':142,'multiline':False]
['text':' each batch, so here we provide the range of output observed','line_number':143,'multiline':False]
['text':' from fp32 reference implementation','line_number':144,'multiline':False]
