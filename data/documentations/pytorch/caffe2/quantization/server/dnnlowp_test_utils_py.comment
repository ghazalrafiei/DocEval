['text':' This function asserts quantized results (output[1:]) are close enough to','line_number':11,'multiline':False]
['text':' floating point results (output[0]).','line_number':12,'multiline':False]
['text':' The error bound is derived based on assumption that there's no input','line_number':13,'multiline':False]
['text':' quantization error.','line_number':14,'multiline':False]
['text':' should be divided by 2 in an exact math, but divide by 1.9 here','line_number':26,'multiline':False]
['text':' considering finite precision in floating-point numbers','line_number':27,'multiline':False]
['text':' Make sure we won't have overflows from vpmaddubsw instruction used in fbgemm)','line_number':42,'multiline':False]
['text':' Go through the same loop again to double check we don't have any overflow','line_number':59,'multiline':False]
['text':' Make sure we won't have overflows from vpmaddubsw instruction used in','line_number':69,'multiline':False]
['text':' fbgemm (FIXME: this assumes fbgemm is used only for NHWC and im2col','line_number':70,'multiline':False]
['text':' is done in a way that input_channels is the fastest moving','line_number':71,'multiline':False]
['text':' dimension).','line_number':72,'multiline':False]
['text':'','line_number':73,'multiline':False]
['text':' strides, pads, kernels, dilations, and sizes should be tuples with the same dimension','line_number':74,'multiline':False]
['text':' (2 for 2D conv, 3 for 3D conv, and so on)','line_number':75,'multiline':False]
['text':' padding','line_number':125,'multiline':False]
['text':' padding','line_number':131,'multiline':False]
['text':' Go through the same loop again to double check we don't have any overflow','line_number':141,'multiline':False]
['text':' padding','line_number':170,'multiline':False]
['text':' padding','line_number':176,'multiline':False]
['text':' strides, pads, kernels, dilations, and sizes should be tuples with the same dimension','line_number':182,'multiline':False]
['text':' (2 for 2D conv, 3 for 3D conv, and so on)','line_number':183,'multiline':False]
['text':' X and W have scale 1, so exactly represented after quantization','line_number':211,'multiline':False]
['text':' For depthwise convolution, it's not enough to set input channel 0','line_number':216,'multiline':False]
['text':' to all X_min to avoid overflow from vpmaddubsw','line_number':217,'multiline':False]
['text':' Put X_max in a position not to be paired with any padded value.','line_number':230,'multiline':False]
['text':' Put X_min to all positions that can be paired with the X_max value.','line_number':231,'multiline':False]
['text':'','line_number':232,'multiline':False]
['text':' This is an example of a pattern for 3x3x3','line_number':233,'multiline':False]
['text':'  .   .   .   .   .','line_number':234,'multiline':False]
['text':'  .   .   .   .   .','line_number':235,'multiline':False]
['text':'  .   .   .   .   .','line_number':236,'multiline':False]
['text':'  .   .   .   .   .','line_number':237,'multiline':False]
['text':'  .   .   .   .  min','line_number':238,'multiline':False]
['text':'','line_number':239,'multiline':False]
['text':'  .   .   .   .   .','line_number':240,'multiline':False]
['text':'  .   .   .   .  min','line_number':241,'multiline':False]
['text':'  .  min max min  .','line_number':242,'multiline':False]
['text':' min  .   .   .   .','line_number':243,'multiline':False]
['text':'  .   .   .   .   .','line_number':244,'multiline':False]
['text':'','line_number':245,'multiline':False]
['text':' min  .   .   .   .','line_number':246,'multiline':False]
['text':'  .   .   .   .   .','line_number':247,'multiline':False]
['text':'  .   .   .   .   .','line_number':248,'multiline':False]
['text':'  .   .   .   .   .','line_number':249,'multiline':False]
['text':'  .   .   .   .   .','line_number':250,'multiline':False]
['text':' Make sure we have enough dimension','line_number':252,'multiline':False]
['text':' Take subtensor we want to manipulate','line_number':256,'multiline':False]
['text':' Put X_max in the middle of the subtensor','line_number':259,'multiline':False]
['text':' Put X_min to the positions that can be paired with X_max across','line_number':262,'multiline':False]
['text':' the slowest moving dimension','line_number':263,'multiline':False]
['text':' Put X_min to other positions that can be paired with X_max','line_number':266,'multiline':False]
['text':' input channel 0 is all X_min to avoid overflow from vpmaddubsw when','line_number':275,'multiline':False]
['text':' multiplied with W_min and W_max','line_number':276,'multiline':False]
['text':' Make sure each group has different ranges to really see the effect','line_number':304,'multiline':False]
['text':' of group-wise quantization.','line_number':305,'multiline':False]
['text':' Conv','line_number':392,'multiline':False]
['text':' FC','line_number':395,'multiline':False]
['text':' We run DNNLOWP ops multiple times to test their first runs that','line_number':398,'multiline':False]
['text':' do caching so exercises different code paths from the subsequent','line_number':399,'multiline':False]
['text':' runs','line_number':400,'multiline':False]
['text':' self.ws.run re-creates operator every time so this test covers','line_number':402,'multiline':False]
['text':' cases when we have multiple nets sharing the same workspace','line_number':403,'multiline':False]
['text':' workspace.CreateNet + workspace.RunNet reuses the same operator','line_number':428,'multiline':False]
