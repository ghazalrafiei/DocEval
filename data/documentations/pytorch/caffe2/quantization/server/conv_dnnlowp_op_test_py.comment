['text':' correctness test with no quantization error in inputs','line_number':23,'multiline':False]
['text':' If output scale/zp aren't set, it gets computed from ref fp32 op','line_number':97,'multiline':False]
['text':' in DNNLOWP, which isn't possible when we quantize input weights.','line_number':98,'multiline':False]
['text':' Make sure atleast one output is collected to compute output','line_number':99,'multiline':False]
['text':' scale/zp.','line_number':100,'multiline':False]
['text':' Bias','line_number':128,'multiline':False]
['text':' When quantized weight is provided, we can't rescale the','line_number':176,'multiline':False]
['text':' output dynamically by looking at the range of output of each','line_number':177,'multiline':False]
['text':' batch, so here we provide the range of output observed from','line_number':178,'multiline':False]
['text':' fp32 reference implementation','line_number':179,'multiline':False]
['text':' correctness test with no quantization error in inputs','line_number':197,'multiline':False]
['text':' If output scale/zp aren't set, it gets computed from ref fp32 op','line_number':356,'multiline':False]
['text':' in DNNLOWP, which isn't possible when we quantize input weights.','line_number':357,'multiline':False]
['text':' Make sure atleast one output is collected to compute output','line_number':358,'multiline':False]
['text':' scale/zp.','line_number':359,'multiline':False]
['text':' Bias','line_number':378,'multiline':False]
['text':' When quantized weight is provided, we can't rescale the','line_number':423,'multiline':False]
['text':' output dynamically by looking at the range of output of each','line_number':424,'multiline':False]
['text':' batch, so here we provide the range of output observed from','line_number':425,'multiline':False]
['text':' fp32 reference implementation','line_number':426,'multiline':False]
