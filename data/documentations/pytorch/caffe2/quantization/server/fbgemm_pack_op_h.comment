['text':' only for DNNLOWP_ACC16','line_number':33,'multiline':False]
['text':'*
 * Pack a weight matrix that can be used by DNNLOWP Int8Conv operators.
 * DNNLOWP operators can pack matrix on demand during their first invocations
 * but calling this operator to pre-pack can have benefits like saving memory
 * space when multiple operators are sharing the same weight.
 * This operator should be a part of init net to be called once to populate
 * packed blob to be used by Int8Conv DNNLOWP operators in the predictor net
 *
 * This operator optionally can also pre-quantize bias.
 * Then, we should also provide the scale of input activation tensor as in_scale
 * argument.
 ','line_number':41,'multiline':True]
['text':' Save quantized weights right after quantization before layout packing for','line_number':70,'multiline':False]
['text':' performance purpose','line_number':71,'multiline':False]
['text':' only for DNNLOWP_ACC16','line_number':74,'multiline':False]
['text':' Helper functions for packing weights that can be used by','line_number':79,'multiline':False]
['text':' ConvDNNLowPAcc16PackWeightOp, ConvDNNLowPOp, and ConvDNNLowPAcc16Op','line_number':80,'multiline':False]
['text':'*
 * @param W_quantized input quantized weight that is not packed yet
 ','line_number':106,'multiline':True]
['text':'
 * Set up used onnxifi data type constexpr
 * Should always be synced with onnxifi.h
 ','line_number':115,'multiline':True]
['text':' namespace caffe2','line_number':181,'multiline':False]
