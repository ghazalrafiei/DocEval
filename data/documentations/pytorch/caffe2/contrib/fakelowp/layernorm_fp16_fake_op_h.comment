['text':'USE_ACC_FP16','line_number':102,'multiline':True]
['text':' Mean and Standard Deviation computation for the input data','line_number':105,'multiline':False]
['text':' Layer Normalized Output computation','line_number':111,'multiline':False]
['text':' handle scale and bias via fp16_fma','line_number':116,'multiline':False]
['text':'USE_ACC_FP16','line_number':124,'multiline':True]
['text':'USE_ACC_FP16','line_number':130,'multiline':True]
['text':' fma_fp16(A, B, Out) -> Out = A * B + Out','line_number':133,'multiline':False]
['text':' Quantize','line_number':141,'multiline':False]
['text':' We should be using the same quantization fucntion from int8quantize,','line_number':142,'multiline':False]
['text':' but we need to adjust for int8 vs uint8 bias. A simple shift of the output is not enough','line_number':143,'multiline':False]
['text':' because this causes problems when rounding inside the fma.','line_number':144,'multiline':False]
['text':' TODO: figure out how to commonize this with int8 quantize','line_number':145,'multiline':False]
['text':' no clamping ','line_number':156,'multiline':True]
['text':' LayerNorm FP16 FakeLowP Op applies the scales and biases (or gamma and beta)','line_number':198,'multiline':False]
['text':' whenever those inputs are provided else it will omit them.','line_number':199,'multiline':False]
['text':' We are keeping elementwise_affine to keep it consistent with LayerNorm FP32 Op.','line_number':200,'multiline':False]
['text':' namespace caffe2','line_number':207,'multiline':False]
