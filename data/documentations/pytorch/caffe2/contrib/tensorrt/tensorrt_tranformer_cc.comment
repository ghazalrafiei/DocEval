['text':' TODO(yinghai): Remove the awkward conversion between unordered_map and map','line_number':22,'multiline':False]
['text':' Populate shapes from workplace','line_number':27,'multiline':False]
['text':' Set name','line_number':85,'multiline':False]
['text':' Set dims','line_number':90,'multiline':False]
['text':' Set values','line_number':96,'multiline':False]
['text':' namespace','line_number':145,'multiline':False]
['text':' Create a CUDA context and reuse it for potential tensor copies across','line_number':153,'multiline':False]
['text':' devices','line_number':154,'multiline':False]
['text':' Add the names of the initializer blobs that we want to fetch from the','line_number':211,'multiline':False]
['text':' workspace later','line_number':212,'multiline':False]
['text':' Add the input/output','line_number':220,'multiline':False]
['text':' Additional arguments for TRT builder','line_number':230,'multiline':False]
['text':' Set up inputs/outputs in the order of they appearnce in getNbBindings','line_number':255,'multiline':False]
['text':' Convert c2 ops to onnx ops, add const weights if there are any','line_number':286,'multiline':False]
['text':' Convert outputs and compute output shape hints','line_number':324,'multiline':False]
['text':' Convert inputs and figure out weights','line_number':346,'multiline':False]
['text':' Extra intermediate weights created during conversion','line_number':358,'multiline':False]
['text':' Boundary inputs, should not be weights','line_number':364,'multiline':False]
['text':' We add weights as inputs too','line_number':373,'multiline':False]
['text':' Debug stuff','line_number':388,'multiline':False]
['text':' Convert weights to initializing tensors if we are building serializable trt','line_number':393,'multiline':False]
['text':' op or defer it to construction time of trt op','line_number':394,'multiline':False]
['text':' Onnx model is ready. Call onnx-trt to convert to one trt c2 op','line_number':400,'multiline':False]
['text':' for weights that are not referenced anywhere, we remove it from the','line_number':461,'multiline':False]
['text':' original workspace','line_number':462,'multiline':False]
['text':' Cutting off the runnable part and replace with tensor ops. Asssume the nets','line_number':471,'multiline':False]
['text':' were topologically sorted','line_number':472,'multiline':False]
['text':' function to tell whether TensorRT supports a given C2 op or not','line_number':491,'multiline':False]
['text':' function to convert runnable subgraph into a trt op. Note that to keep the','line_number':510,'multiline':False]
['text':' interface clean, we do the double conversion from C2 op to Onnx ops here','line_number':511,'multiline':False]
['text':' but it should be OK as the cost is really small. We also need to keep the','line_number':512,'multiline':False]
['text':' same exporter throughout the process to avoid duplicated dummy name','line_number':513,'multiline':False]
['text':' generation','line_number':514,'multiline':False]
['text':' Need to figure out a proper place to handle device option','line_number':524,'multiline':False]
['text':' namespace caffe2','line_number':533,'multiline':False]
