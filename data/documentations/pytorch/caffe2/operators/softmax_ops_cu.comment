['text':' Sanity check','line_number':84,'multiline':False]
['text':' Subtract max on each cell for numerical reasons','line_number':121,'multiline':False]
['text':' Exponentiate','line_number':128,'multiline':False]
['text':' Normalize','line_number':137,'multiline':False]
['text':' Ignore-label, so set all gradients for this positions','line_number':209,'multiline':False]
['text':' tp zero','line_number':210,'multiline':False]
['text':' Put the intermediate result X - max(X) into Y','line_number':259,'multiline':False]
['text':' Subtract the scale','line_number':261,'multiline':False]
['text':' Exponentiation','line_number':274,'multiline':False]
['text':' Sum exponentiated values','line_number':276,'multiline':False]
['text':' Normalize','line_number':279,'multiline':False]
['text':' namespace','line_number':297,'multiline':False]
['text':' Logits','line_number':301,'multiline':False]
['text':' Labels / targets','line_number':302,'multiline':False]
['text':' batch size','line_number':307,'multiline':False]
['text':' Probabilities from softmax','line_number':311,'multiline':False]
['text':' Average loss','line_number':329,'multiline':False]
['text':' logarithmic output','line_number':360,'multiline':False]
['text':' Compute label xent loss per example','line_number':362,'multiline':False]
['text':' Since we had logarithmic output, we need to exponentiate','line_number':377,'multiline':False]
['text':' them again.','line_number':378,'multiline':False]
['text':' Sum weights','line_number':398,'multiline':False]
['text':' Sum of all losses','line_number':413,'multiline':False]
['text':' Average of input batch size','line_number':421,'multiline':False]
['text':' Logits','line_number':434,'multiline':False]
['text':' Labels / targets','line_number':435,'multiline':False]
['text':' Probabilities from softmax','line_number':443,'multiline':False]
['text':' Softmax for each x,y location','line_number':467,'multiline':False]
['text':' Cross entropy','line_number':475,'multiline':False]
['text':' Average loss','line_number':477,'multiline':False]
['text':' Somewhat awkward scalar passing from device to host','line_number':501,'multiline':False]
['text':' Final scaling','line_number':523,'multiline':False]
['text':' Logits','line_number':534,'multiline':False]
['text':' Labels / targets','line_number':535,'multiline':False]
['text':' Input(2) is weights, if given','line_number':536,'multiline':False]
['text':' Probabilities from softmax','line_number':537,'multiline':False]
['text':' Gradient w.r.t. avg loss','line_number':538,'multiline':False]
['text':' Memory saving trick to share the buffer with the softmax output.','line_number':543,'multiline':False]
['text':' Softmax output is thus overwritten.','line_number':544,'multiline':False]
['text':' batch size','line_number':553,'multiline':False]
['text':' Subtract 1 from labeled positions','line_number':571,'multiline':False]
['text':' Copy softmax probabilities into dX','line_number':574,'multiline':False]
['text':' Weighted version gets the Pdata values internally','line_number':591,'multiline':False]
['text':' Sum weights','line_number':621,'multiline':False]
['text':' Scale by d_avg_loss / N','line_number':636,'multiline':False]
['text':' Logits','line_number':657,'multiline':False]
['text':' Labels / targets','line_number':658,'multiline':False]
['text':' Input(2) is weights, if given','line_number':659,'multiline':False]
['text':' Probabilities from softmax','line_number':660,'multiline':False]
['text':' Gradient w.r.t. avg loss','line_number':661,'multiline':False]
['text':' Memory saving trick to share the buffer with the softmax output.','line_number':666,'multiline':False]
['text':' Softmax output is thus overwritten.','line_number':667,'multiline':False]
['text':' Spatial mode, compute softmax for each x, y location','line_number':680,'multiline':False]
['text':' Copy softmax probabilities into dX. All but the neuron','line_number':698,'multiline':False]
['text':' corresponding to the correct label has gradient equaling e(x_j)','line_number':699,'multiline':False]
['text':' which is the probability under softmax.','line_number':700,'multiline':False]
['text':' Somewhat awkward scalar passing from device to host','line_number':721,'multiline':False]
['text':' Final scaling','line_number':730,'multiline':False]
['text':' Implementation for the CUDA context.','line_number':749,'multiline':False]
['text':' The softmax gradient kernel. This kernel has to be called with the number of','line_number':797,'multiline':False]
['text':' threads per block being no more than SOFTMAX_NUM_THREADS.','line_number':798,'multiline':False]
['text':' A two-level reduction to compute the inner products.','line_number':812,'multiline':False]
['text':' Compute gradient.','line_number':826,'multiline':False]
['text':' namespace','line_number':832,'multiline':False]
['text':' namespace caffe2','line_number':870,'multiline':False]
