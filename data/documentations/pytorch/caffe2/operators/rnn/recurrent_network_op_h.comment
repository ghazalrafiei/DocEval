['text':'context','line_number':71,'multiline':True]
['text':'*
 * Copy external input to the step net into the first item of
 * (T + 1) X batch_size X input_size tensor
 ','line_number':111,'multiline':True]
['text':' Sometimes we want to provide more than one initial step.','line_number':133,'multiline':False]
['text':' For example, if we do a convolution op in step net','line_number':134,'multiline':False]
['text':' and need a sufficient left padding around the input.','line_number':135,'multiline':False]
['text':' This could be used together with links where window != 1.','line_number':136,'multiline':False]
['text':' States at [0, ..., (T + initialStateLength - 1)] (inclusive)','line_number':141,'multiline':False]
['text':' Usually, the initial state is the same for all inputs in the batch.','line_number':151,'multiline':False]
['text':' So the op conveniently accepts 1-D input and copies it batchSize times.','line_number':152,'multiline':False]
['text':' namespace detail','line_number':180,'multiline':False]
['text':' States need to be "global" (since they are shared between','line_number':231,'multiline':False]
['text':' forward and backward).','line_number':232,'multiline':False]
['text':'*
    * Some blobs can be marked as to be recomputed on backward pass.
    * For those blobs, we do not want to allocate on each step workspace,
    * but we instead store that blob in the shared workspace so all
    * steps can use the same buffer on forward pass.
    ','line_number':265,'multiline':True]
['text':' Note: if the blob already was created, this is a no-op.','line_number':276,'multiline':False]
['text':' If we don't have a backward step net, this operator is forward_only','line_number':302,'multiline':False]
['text':' and we can avoid creating multiple workspaces.','line_number':303,'multiline':False]
['text':' With backward pass: we need to create workspace for each timestep','line_number':310,'multiline':False]
['text':' Caller can decide that some of the forward activations','line_number':320,'multiline':False]
['text':' are recomputed on backward pass. Then those activations do not','line_number':321,'multiline':False]
['text':' have to be stored in step workspaces but can be shared.','line_number':322,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-sign-compare)','line_number':325,'multiline':False]
['text':' In forward-only mode, we cycle over workspaces. This limits the amount','line_number':330,'multiline':False]
['text':' of parallelism over timesteps that the RNNExecutor provides. So with','line_number':331,'multiline':False]
['text':' RNN executor we use more workspaces to get better perf.','line_number':332,'multiline':False]
['text':' Use alternating stepWorkspaces when forward_only=True.','line_number':338,'multiline':False]
['text':' Note that the step workspaces can be shared by other ops, thus','line_number':339,'multiline':False]
['text':' we cannot shrink it to 2 if there are more than 2 step workspaces.','line_number':340,'multiline':False]
['text':' Need to limit timestep parallelism because we cycle over workspaces','line_number':354,'multiline':False]
['text':' Use plain Caffe2 nets','line_number':360,'multiline':False]
['text':' Since we have a SimpleNet, there are no races here.','line_number':367,'multiline':False]
['text':' backward ','line_number':413,'multiline':True]
['text':' Add operators to the backward step net to handle accumulation of
       gradients over timesteps
    ','line_number':444,'multiline':True]
['text':' Renaming maps (generated by memonger.py)','line_number':459,'multiline':False]
['text':' Forward inputs come after [outputs_with_grads] gradient inputs','line_number':500,'multiline':False]
['text':' See GetRecurrentNetworkGradient to understand offseting here','line_number':502,'multiline':False]
['text':' backward ','line_number':581,'multiline':True]
['text':'*
      * Add ops to the step net to accumulate input gradients.
      ','line_number':587,'multiline':True]
['text':' Add also the linked blobs to outputs, to ensure correct','line_number':605,'multiline':False]
['text':' chaining.','line_number':606,'multiline':False]
['text':' If a user passes in param_grads mapping, we can copy directly','line_number':629,'multiline':False]
['text':' form a blob where backward cell net written data to.','line_number':630,'multiline':False]
['text':' This becomes handy in a case where gradient from the cell net','line_number':631,'multiline':False]
['text':' is an internal blob of the backward cell. This happens, for example,','line_number':632,'multiline':False]
['text':' when SumOp is the first op of the cell','line_number':633,'multiline':False]
['text':'*
      * Create all output blobs created by ops of the backward step net, they
      * can be shared.
      ','line_number':649,'multiline':True]
['text':' Fill the last timestep with zeros for the gradient','line_number':702,'multiline':False]
['text':' This code assumes that there are several inputs','line_number':710,'multiline':False]
['text':' sequences. Actually it is not supported by the rest of the code,','line_number':711,'multiline':False]
['text':' and numSequences_ is a constant, equal to 1.','line_number':712,'multiline':False]
['text':' Offseting as the first gradInputs_.size() inputs of the op','line_number':714,'multiline':False]
['text':' are from GO. Then all I(0..N).','line_number':715,'multiline':False]
['text':' Create shared blobs for blobs that can be shared between','line_number':763,'multiline':False]
['text':' all timesteps.','line_number':764,'multiline':False]
['text':' See GetRecurrentNetworkGradient to understand offseting here','line_number':788,'multiline':False]
['text':' Outputs of the gradient are inputs of the forward pass.','line_number':789,'multiline':False]
['text':' So we need to offset on all inputs that go before recurrent','line_number':790,'multiline':False]
['text':' initial ones','line_number':791,'multiline':False]
['text':' because first gradInputs_.size() inputs are from GO','line_number':793,'multiline':False]
['text':' Gradient states blob should live. And if it gets changed by the','line_number':804,'multiline':False]
['text':' backward pass, then output should be changed as well. Thus it should','line_number':805,'multiline':False]
['text':' be okay to share data here','line_number':806,'multiline':False]
['text':' We need to do a bunch of Adds any way. So lets not worry about','line_number':810,'multiline':False]
['text':' copy / share data here. One way to speed this up could be a kernel','line_number':811,'multiline':False]
['text':' which sums up several tensors together instead of going 1 by 1','line_number':812,'multiline':False]
['text':' For now we support only one input sequence','line_number':847,'multiline':False]
['text':' Both internal and external appear as both input and output to enforce','line_number':914,'multiline':False]
['text':' correct dependency computation.','line_number':915,'multiline':False]
['text':' namespace caffe2','line_number':945,'multiline':False]
['text':' CAFFE2_OPERATORS_RECURRENT_NETWORK_OP_H_','line_number':947,'multiline':False]
