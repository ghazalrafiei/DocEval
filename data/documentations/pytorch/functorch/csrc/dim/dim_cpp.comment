['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' Many APIs have changed/don't exist anymore','line_number':10,'multiline':False]
['text':' Re-enable this some day','line_number':15,'multiline':False]
['text':'#include <torch/csrc/autograd/python_variable.h>','line_number':30,'multiline':False]
['text':' C++ API functions for objects to','line_number':46,'multiline':False]
['text':' * construct the object, returning a ref-counted handle','line_number':47,'multiline':False]
['text':' * The actual API, with methods that take/return C-typed values','line_number':48,'multiline':False]
['text':' extend minpybind.h to include','line_number':50,'multiline':False]
['text':' * typed handles so that -> can get to their raw API','line_number':51,'multiline':False]
['text':' * object/handle distinction for the typed handles','line_number':52,'multiline':False]
['text':' class Dim: ---------------','line_number':54,'multiline':False]
['text':' globals that depend on the python dim library,','line_number':75,'multiline':False]
['text':' which we can't lookup until we finish initializing the _C module','line_number':76,'multiline':False]
['text':' for stable comparisons in prototype','line_number':135,'multiline':False]
['text':' union of either a negative number indicating which dimension this is from the rhs,','line_number':193,'multiline':False]
['text':' or a pointer to a first-class dimension.','line_number':194,'multiline':False]
['text':' pointers do not have their highest bit set, so checking the number is negative tells us','line_number':195,'multiline':False]
['text':' that it is not a dim.','line_number':196,'multiline':False]
['text':' Dim wrapper methods','line_number':240,'multiline':False]
['text':' Sentinel ','line_number':330,'multiline':True]
['text':' tp_name ','line_number':335,'multiline':True]
['text':' tp_basicsize ','line_number':336,'multiline':True]
['text':' tp_itemsize ','line_number':337,'multiline':True]
['text':' tp_dealloc ','line_number':338,'multiline':True]
['text':' tp_vectorcall_offset ','line_number':339,'multiline':True]
['text':' tp_getattr ','line_number':340,'multiline':True]
['text':' tp_setattr ','line_number':341,'multiline':True]
['text':' tp_as_async ','line_number':342,'multiline':True]
['text':' tp_repr ','line_number':343,'multiline':True]
['text':' tp_as_number ','line_number':344,'multiline':True]
['text':' tp_as_sequence ','line_number':345,'multiline':True]
['text':' tp_as_mapping ','line_number':346,'multiline':True]
['text':' tp_hash ','line_number':347,'multiline':True]
['text':' tp_call ','line_number':348,'multiline':True]
['text':' tp_str ','line_number':349,'multiline':True]
['text':' tp_getattro ','line_number':350,'multiline':True]
['text':' tp_setattro ','line_number':351,'multiline':True]
['text':' tp_as_buffer ','line_number':352,'multiline':True]
['text':' tp_flags ','line_number':353,'multiline':True]
['text':' tp_doc ','line_number':354,'multiline':True]
['text':' tp_traverse ','line_number':355,'multiline':True]
['text':' tp_clear ','line_number':356,'multiline':True]
['text':' tp_richcompare ','line_number':357,'multiline':True]
['text':' tp_weaklistoffset ','line_number':358,'multiline':True]
['text':' tp_iter ','line_number':359,'multiline':True]
['text':' tp_iternext ','line_number':360,'multiline':True]
['text':' tp_methods ','line_number':361,'multiline':True]
['text':' tp_members ','line_number':362,'multiline':True]
['text':' tp_getset ','line_number':363,'multiline':True]
['text':' tp_base ','line_number':364,'multiline':True]
['text':' tp_dict ','line_number':365,'multiline':True]
['text':' tp_descr_get ','line_number':366,'multiline':True]
['text':' tp_descr_set ','line_number':367,'multiline':True]
['text':' tp_dictoffset ','line_number':368,'multiline':True]
['text':' tp_init ','line_number':369,'multiline':True]
['text':' tp_alloc ','line_number':370,'multiline':True]
['text':' tp_new ','line_number':371,'multiline':True]
['text':' class DimList ------------','line_number':374,'multiline':False]
['text':' Sentinel ','line_number':480,'multiline':True]
['text':'lenfunc sq_length;','line_number':504,'multiline':False]
['text':'binaryfunc sq_concat;','line_number':505,'multiline':False]
['text':'ssizeargfunc sq_repeat;','line_number':506,'multiline':False]
['text':'ssizeargfunc sq_item;','line_number':507,'multiline':False]
['text':'void *was_sq_slice;','line_number':508,'multiline':False]
['text':'ssizeobjargproc sq_ass_item;','line_number':509,'multiline':False]
['text':'void *was_sq_ass_slice;','line_number':510,'multiline':False]
['text':'objobjproc sq_contains;','line_number':511,'multiline':False]
['text':'binaryfunc sq_inplace_concat;','line_number':513,'multiline':False]
['text':'ssizeargfunc sq_inplace_repeat;','line_number':514,'multiline':False]
['text':' Sentinel ','line_number':523,'multiline':True]
['text':'lenfunc mp_length;','line_number':549,'multiline':False]
['text':'binaryfunc mp_subscript;','line_number':550,'multiline':False]
['text':'objobjargproc mp_ass_subscript;','line_number':551,'multiline':False]
['text':' tp_name ','line_number':558,'multiline':True]
['text':' tp_basicsize ','line_number':559,'multiline':True]
['text':' tp_itemsize ','line_number':560,'multiline':True]
['text':' tp_dealloc ','line_number':561,'multiline':True]
['text':' tp_vectorcall_offset ','line_number':562,'multiline':True]
['text':' tp_getattr ','line_number':563,'multiline':True]
['text':' tp_setattr ','line_number':564,'multiline':True]
['text':' tp_as_async ','line_number':565,'multiline':True]
['text':' tp_repr ','line_number':566,'multiline':True]
['text':' tp_as_number ','line_number':567,'multiline':True]
['text':' tp_as_sequence ','line_number':568,'multiline':True]
['text':' tp_as_mapping ','line_number':569,'multiline':True]
['text':' tp_hash ','line_number':570,'multiline':True]
['text':' tp_call ','line_number':571,'multiline':True]
['text':' tp_str ','line_number':572,'multiline':True]
['text':' tp_getattro ','line_number':573,'multiline':True]
['text':' tp_setattro ','line_number':574,'multiline':True]
['text':' tp_as_buffer ','line_number':575,'multiline':True]
['text':' tp_flags ','line_number':576,'multiline':True]
['text':' tp_doc ','line_number':577,'multiline':True]
['text':' tp_traverse ','line_number':578,'multiline':True]
['text':' tp_clear ','line_number':579,'multiline':True]
['text':' tp_richcompare ','line_number':580,'multiline':True]
['text':' tp_weaklistoffset ','line_number':581,'multiline':True]
['text':' tp_iter ','line_number':582,'multiline':True]
['text':' tp_iternext ','line_number':583,'multiline':True]
['text':' tp_methods ','line_number':584,'multiline':True]
['text':' tp_members ','line_number':585,'multiline':True]
['text':' tp_getset ','line_number':586,'multiline':True]
['text':' tp_base ','line_number':587,'multiline':True]
['text':' tp_dict ','line_number':588,'multiline':True]
['text':' tp_descr_get ','line_number':589,'multiline':True]
['text':' tp_descr_set ','line_number':590,'multiline':True]
['text':' tp_dictoffset ','line_number':591,'multiline':True]
['text':' tp_init ','line_number':592,'multiline':True]
['text':' tp_alloc ','line_number':593,'multiline':True]
['text':' tp_new ','line_number':594,'multiline':True]
['text':' Tensor -----------------------------','line_number':633,'multiline':False]
['text':' the python wrapper type.','line_number':635,'multiline':False]
['text':' this will outlive the call so','line_number':675,'multiline':False]
['text':' take ownership of temporaries','line_number':676,'multiline':False]
['text':' in vector args','line_number':677,'multiline':False]
['text':' don't force creation of batch tensor if it wasn't alreay provided.','line_number':723,'multiline':False]
['text':' grab ownership of the dims inside levels','line_number':758,'multiline':False]
['text':' version in header does a unnecessary refcount +/-','line_number':772,'multiline':False]
['text':'auto sz = tensor.sizes();','line_number':867,'multiline':False]
['text':'AT_ASSERT(sz[i] == l.dim()->size());','line_number':875,'multiline':False]
['text':' includes named tuples','line_number':972,'multiline':False]
['text':' fast checks that this thing isn't something that is nested.','line_number':1020,'multiline':False]
['text':' because we do not have a autorelase pool yet...','line_number':1056,'multiline':False]
['text':' prereq: isinstance(h, _Tensor)','line_number':1109,'multiline':False]
['text':' Dim or DelayedMulTensor','line_number':1120,'multiline':False]
['text':' fast case: tensor is live in python','line_number':1125,'multiline':False]
['text':'ignore_hermetic_tls=','line_number':1127,'multiline':True]
['text':' grab ownership of the tensors','line_number':1183,'multiline':False]
['text':' XXX - requires a patch to functorch to att set_level','line_number':1191,'multiline':False]
['text':' drop_levels -> if a dim appears in from_levels but not to_levels, it is assumed it has stride 0.','line_number':1214,'multiline':False]
['text':' std::cout << "__torch_function__ " << ((is_pointwise) ? "pointwise" : "functorch") << " " << orig << "\n";','line_number':1238,'multiline':False]
['text':' fast wrap for normal case where operator just returns a tensor.','line_number':1280,'multiline':False]
['text':' std::cout << orig << " calling functorch...\n";','line_number':1292,'multiline':False]
['text':' std::cout << "rl: " << result_levels << "\n";','line_number':1293,'multiline':False]
['text':' something like a mask * rhs, which matrix multiplies don't correctly promote','line_number':1334,'multiline':False]
['text':' std::cout << "__torch_function__ " << "delay" << " " << orig << "\n";','line_number':1345,'multiline':False]
['text':' Sentinel ','line_number':1418,'multiline':True]
['text':' Sentinel ','line_number':1422,'multiline':True]
['text':' tp_name ','line_number':1429,'multiline':True]
['text':' tp_basicsize ','line_number':1430,'multiline':True]
['text':' tp_itemsize ','line_number':1431,'multiline':True]
['text':' tp_dealloc ','line_number':1432,'multiline':True]
['text':' tp_vectorcall_offset ','line_number':1433,'multiline':True]
['text':' tp_getattr ','line_number':1434,'multiline':True]
['text':' tp_setattr ','line_number':1435,'multiline':True]
['text':' tp_as_async ','line_number':1436,'multiline':True]
['text':' tp_repr ','line_number':1437,'multiline':True]
['text':' tp_as_number ','line_number':1438,'multiline':True]
['text':' tp_as_sequence ','line_number':1439,'multiline':True]
['text':' tp_as_mapping ','line_number':1440,'multiline':True]
['text':' tp_hash ','line_number':1441,'multiline':True]
['text':' tp_call ','line_number':1442,'multiline':True]
['text':' tp_str ','line_number':1443,'multiline':True]
['text':' tp_getattro ','line_number':1444,'multiline':True]
['text':' tp_setattro ','line_number':1445,'multiline':True]
['text':' tp_as_buffer ','line_number':1446,'multiline':True]
['text':' tp_flags ','line_number':1447,'multiline':True]
['text':' tp_doc ','line_number':1448,'multiline':True]
['text':' tp_traverse ','line_number':1449,'multiline':True]
['text':' tp_clear ','line_number':1450,'multiline':True]
['text':' tp_richcompare ','line_number':1451,'multiline':True]
['text':' tp_weaklistoffset ','line_number':1452,'multiline':True]
['text':' tp_iter ','line_number':1453,'multiline':True]
['text':' tp_iternext ','line_number':1454,'multiline':True]
['text':' tp_methods ','line_number':1455,'multiline':True]
['text':' tp_members ','line_number':1456,'multiline':True]
['text':' tp_getset ','line_number':1457,'multiline':True]
['text':' tp_base ','line_number':1458,'multiline':True]
['text':' tp_dict ','line_number':1459,'multiline':True]
['text':' tp_descr_get ','line_number':1460,'multiline':True]
['text':' tp_descr_set ','line_number':1461,'multiline':True]
['text':' tp_dictoffset ','line_number':1462,'multiline':True]
['text':' tp_init ','line_number':1463,'multiline':True]
['text':' tp_alloc ','line_number':1464,'multiline':True]
['text':' tp_new ','line_number':1465,'multiline':True]
['text':' dim() --------------------','line_number':1469,'multiline':False]
['text':' Python wrappers that make new reflection primitives available for older runtimes','line_number':1509,'multiline':False]
['text':' On Windows, _PyOpcode_Caches and _PyOpcode_Deopt are private symbols','line_number':1518,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/93854','line_number':1519,'multiline':False]
['text':' When py3.11 adapts bytecode lasti points to the precall','line_number':1591,'multiline':False]
['text':' rather than the call instruction after it','line_number':1592,'multiline':False]
['text':' avoid taking the wrong names for dimensions','line_number':1613,'multiline':False]
['text':' once we fail at finding a name, we can find any more','line_number':1623,'multiline':False]
['text':' lr','line_number':1722,'multiline':False]
['text':' lro','line_number':1726,'multiline':False]
['text':' lo','line_number':1729,'multiline':False]
['text':' std::cout << lhs.levels << " " << rhs.levels << " " << sum << "\n";','line_number':1767,'multiline':False]
['text':' std::cout << lro_dims.dims << " " << lo_dims.dims << " " << ro_dims.dims << " " << lr_dims.dims << "\n";','line_number':1768,'multiline':False]
['text':' no batch, just call mm','line_number':1770,'multiline':False]
['text':' before the new positional dims','line_number':1937,'multiline':False]
['text':' after the new positional dims','line_number':1956,'multiline':False]
['text':' we shorted the number of dimension, so remove them from new levels','line_number':1960,'multiline':False]
['text':' we will renumber them later','line_number':1961,'multiline':False]
['text':' renumber the positional dimension','line_number':1967,'multiline':False]
['text':' if true, then it is safe to just call getitem or setitem, these objects do not need special handling','line_number':2072,'multiline':False]
['text':' requires actual lookup','line_number':2073,'multiline':False]
['text':' can we avoid rechecking?','line_number':2095,'multiline':False]
['text':' can we avoid rechecking?','line_number':2103,'multiline':False]
['text':' std::cout << "calling original getindex " << self_hdl << " " << tup << "\n";','line_number':2124,'multiline':False]
['text':' std::cout << "skipping original getindex\n";','line_number':2128,'multiline':False]
['text':' std::cout << "returning (from_positional)\n";','line_number':2131,'multiline':False]
['text':' we allow for matching single dims to multiple dims,','line_number':2139,'multiline':False]
['text':' so we first have to normalize everything into the case where there is a list on lhs and the rhs','line_number':2140,'multiline':False]
['text':' dims being indexed can be grouped together into a single index space, and we have to','line_number':2159,'multiline':False]
['text':' flatten them int a single dimension before we can index them...','line_number':2160,'multiline':False]
['text':'check_first=','line_number':2183,'multiline':True]
['text':' plausible semantics work for this to have 0 elements (e.g. the index will always be 0)','line_number':2185,'multiline':False]
['text':' value is just dropped','line_number':2186,'multiline':False]
['text':' note: we are using the first level in a flattened group to represent the group for the rest of the op','line_number':2230,'multiline':False]
['text':' we need to be careful not to rely the dimensions size because it doesnt match the size of the whole group','line_number':2231,'multiline':False]
['text':' true -- the indices were flattend out of a tuple, list or sequence...','line_number':2244,'multiline':False]
['text':' a copy of treatSequenceAsTuple modified to add Dim and our wrapped tensors..','line_number':2272,'multiline':False]
['text':' nothing about first class dims here, fallback to getitem','line_number':2304,'multiline':False]
['text':' calculate how many dimensioned have been indexed in order to compute the size of ...','line_number':2321,'multiline':False]
['text':' or expand a potentially unbound dimension list.','line_number':2322,'multiline':False]
['text':' at this point if we haven't seen any Dim objects, we also can fallback to the original getitem.','line_number':2353,'multiline':False]
['text':' std::cout << "__getitem__ " << self << " " << index << "\n";','line_number':2358,'multiline':False]
['text':' expand any unbound dimension list, or expand ... into individual : slices.','line_number':2365,'multiline':False]
['text':' ...','line_number':2371,'multiline':False]
['text':' flatten out any dimensions stored in dimlist elements directly into the inputs','line_number':2381,'multiline':False]
['text':' std::cout << dimlists << " <- dim lists!\n";','line_number':2382,'multiline':False]
['text':' we added more elements to input because of ...','line_number':2385,'multiline':False]
['text':' so we need to also adjust the index to get back to where the','line_number':2386,'multiline':False]
['text':' dimlist existed','line_number':2387,'multiline':False]
['text':' XXX would be better if we used an OwnedSlice in DimList','line_number':2392,'multiline':False]
['text':' At this point:','line_number':2401,'multiline':False]
['text':' ..., DimList have been eliminated','line_number':2402,'multiline':False]
['text':' Dim, Tensor, Tuple[Dim,...], int, slice still remain','line_number':2403,'multiline':False]
['text':' we have to count how many times we see a dimension.','line_number':2406,'multiline':False]
['text':' A[i,j] is a simple binding operation, but A[i, i+j] or A[i, i] requires advanced indexing.','line_number':2407,'multiline':False]
['text':' flat inputs will start with an empty mpy::handle if the','line_number':2423,'multiline':False]
['text':' actual value is in the tensor-like object in the tensor info','line_number':2424,'multiline':False]
['text':' std::cout << "self levels: " << self_info.levels << "\n";','line_number':2451,'multiline':False]
['text':' dim pack','line_number':2487,'multiline':False]
['text':' pair up the indexing expressions with dimension of self it indexes','line_number':2503,'multiline':False]
['text':' self may have first-class dims, which do not participate the indexing.','line_number':2504,'multiline':False]
['text':' grab and index from the positional list','line_number':2511,'multiline':False]
['text':' we might have fewer indices than tensor dimensions,','line_number':2514,'multiline':False]
['text':' which implicitly indexes the remaining dimensions with :','line_number':2515,'multiline':False]
['text':' any training Nones may have no existing dimension associated with them in self.','line_number':2529,'multiline':False]
['text':' we have to restride the tensor to collapse dimension packs and introduce our none dimensions.','line_number':2532,'multiline':False]
['text':' figure out what the shape of the indexing tensors will be','line_number':2538,'multiline':False]
['text':' and what the shape of the resulting tensor will be','line_number':2539,'multiline':False]
['text':' std::cout << "Consider to add " << l << "\n";','line_number':2557,'multiline':False]
['text':' dimesions used once are just binding operations','line_number':2564,'multiline':False]
['text':' note: actual positional indexes are accurately computed later','line_number':2582,'multiline':False]
['text':' indexing dimensions appear in the tensor at the _first use of a tensor_ in the indexing. So insert','line_number':2588,'multiline':False]
['text':' the indexing leveles into the result klevels at this spot','line_number':2589,'multiline':False]
['text':' std::cout << "flat inputs: " << flat_inputs << "\n";','line_number':2594,'multiline':False]
['text':' std::cout << "result_levels: " << result_levels << "\n";','line_number':2595,'multiline':False]
['text':' std::cout << "index_levels: " << index_levels << "\n";','line_number':2596,'multiline':False]
['text':' get all the tensors to be the right shape for indexing','line_number':2598,'multiline':False]
['text':' std::cout << "tensor " << i << " " << tensor_inputs[i].levels << "\n";','line_number':2603,'multiline':False]
['text':' previously we didn't know how many positional dimensions there would be so we couldn't number them right','line_number':2613,'multiline':False]
['text':' so fill it in now.','line_number':2614,'multiline':False]
['text':' otherwise rhs can be a scalar...','line_number':2648,'multiline':False]
['text':' call original split (if self has dimensions this will use torch function to do the split)','line_number':2804,'multiline':False]
['text':' tp_name ','line_number':2922,'multiline':True]
['text':' tp_basicsize ','line_number':2923,'multiline':True]
['text':' tp_itemsize ','line_number':2924,'multiline':True]
['text':' tp_dealloc ','line_number':2925,'multiline':True]
['text':' tp_vectorcall_offset ','line_number':2926,'multiline':True]
['text':' tp_getattr ','line_number':2927,'multiline':True]
['text':' tp_setattr ','line_number':2928,'multiline':True]
['text':' tp_as_async ','line_number':2929,'multiline':True]
['text':' tp_repr ','line_number':2930,'multiline':True]
['text':' tp_as_number ','line_number':2931,'multiline':True]
['text':' tp_as_sequence ','line_number':2932,'multiline':True]
['text':' tp_as_mapping ','line_number':2933,'multiline':True]
['text':' tp_hash ','line_number':2934,'multiline':True]
['text':' tp_call ','line_number':2935,'multiline':True]
['text':' tp_str ','line_number':2936,'multiline':True]
['text':' tp_getattro ','line_number':2937,'multiline':True]
['text':' tp_setattro ','line_number':2938,'multiline':True]
['text':' tp_as_buffer ','line_number':2939,'multiline':True]
['text':' tp_flags ','line_number':2940,'multiline':True]
['text':' tp_doc ','line_number':2941,'multiline':True]
['text':' tp_traverse ','line_number':2942,'multiline':True]
['text':' tp_clear ','line_number':2943,'multiline':True]
['text':' tp_richcompare ','line_number':2944,'multiline':True]
['text':' tp_weaklistoffset ','line_number':2945,'multiline':True]
['text':' tp_iter ','line_number':2946,'multiline':True]
['text':' tp_iternext ','line_number':2947,'multiline':True]
['text':' tp_methods ','line_number':2948,'multiline':True]
['text':' tp_members ','line_number':2949,'multiline':True]
['text':' tp_getset ','line_number':2950,'multiline':True]
['text':' tp_base ','line_number':2951,'multiline':True]
['text':' tp_dict ','line_number':2952,'multiline':True]
['text':' tp_descr_get ','line_number':2953,'multiline':True]
['text':' tp_descr_set ','line_number':2954,'multiline':True]
['text':' tp_dictoffset ','line_number':2955,'multiline':True]
['text':' tp_init ','line_number':2956,'multiline':True]
['text':' tp_alloc ','line_number':2957,'multiline':True]
['text':' tp_new ','line_number':2958,'multiline':True]
['text':' do not include self','line_number':2973,'multiline':False]
['text':' do not include self','line_number':2980,'multiline':False]
['text':' XXX - ignore python function wrapped, we will call torch function directly','line_number':3110,'multiline':False]
['text':' std::cout << "SKIPPING fusion because dtype or keepdim=True specified\n";','line_number':3140,'multiline':False]
['text':' Sentinel ','line_number':3240,'multiline':True]
['text':' name of module ','line_number':3245,'multiline':True]
['text':' module documentation, may be NULL ','line_number':3246,'multiline':True]
['text':' size of per-interpreter state of the module,
                 or -1 if the module keeps state in global variables. ','line_number':3247,'multiline':True]
