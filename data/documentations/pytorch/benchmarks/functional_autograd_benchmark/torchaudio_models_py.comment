['text':' Taken from https://github.com/pytorch/audio/blob/master/torchaudio/models/wav2letter.py','line_number':1,'multiline':False]
['text':' So that we don't need torchaudio to be installed','line_number':2,'multiline':False]
['text':' Taken from  https://github.com/SeanNaren/deepspeech.pytorch with modifications','line_number':116,'multiline':False]
['text':' (TxNxH*2) -> (TxNxH) by sum','line_number':217,'multiline':False]
['text':' Wang et al 2016 - Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks','line_number':222,'multiline':False]
['text':' input shape - sequence, batch, feature - TxNxH','line_number':223,'multiline':False]
['text':' output shape - same as input','line_number':224,'multiline':False]
['text':' Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1','line_number':294,'multiline':False]
['text':' consider adding batch norm?','line_number':320,'multiline':False]
['text':' Collapse feature dimension','line_number':345,'multiline':False]
['text':' TxNxH','line_number':346,'multiline':False]
['text':' no need for lookahead layer in bidirectional','line_number':351,'multiline':False]
['text':' identity in training mode, softmax in eval mode','line_number':356,'multiline':False]
['text':' Taken from https://github.com/pytorch/examples/blob/master/word_language_model/model.py#L108-L152','line_number':380,'multiline':False]
['text':' Not sure how this works in the original code','line_number':452,'multiline':False]
['text':' nn.init.zeros_(self.decoder)','line_number':453,'multiline':False]
['text':' This will be created once during warmup','line_number':459,'multiline':False]
['text':' From https://github.com/pytorch/text/blob/master/torchtext/modules','line_number':475,'multiline':False]
['text':' Scale query','line_number':638,'multiline':False]
['text':' Dot product of q, k','line_number':657,'multiline':False]
