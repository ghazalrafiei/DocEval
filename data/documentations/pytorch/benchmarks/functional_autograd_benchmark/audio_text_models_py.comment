['text':' Commented are the original sizes in the code','line_number':45,'multiline':False]
['text':' 1343','line_number':46,'multiline':False]
['text':' 50','line_number':47,'multiline':False]
['text':' Sequence length for each input','line_number':50,'multiline':False]
['text':' For ctc loss','line_number':78,'multiline':False]
['text':' For most SOTA research, you would like to have embed to 720, nhead to 12, bsz to 64, tgt_len/src_len to 128.','line_number':87,'multiline':False]
['text':' disable dropout for consistency checking','line_number':97,'multiline':False]
['text':' From https://github.com/pytorch/text/blob/master/test/data/test_modules.py#L10','line_number':120,'multiline':False]
['text':' Build torchtext MultiheadAttention module','line_number':122,'multiline':False]
['text':' Don't test any specific loss, just backprop ones for both outputs','line_number':153,'multiline':False]
