['text':'!/usr/bin/env python3','line_number':1,'multiline':False]
['text':'','line_number':2,'multiline':False]
['text':' Measure distributed training iteration time.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' This program performs a sweep over a) a number of model architectures, and','line_number':5,'multiline':False]
['text':' b) an increasing number of processes. This produces a 1-GPU baseline,','line_number':6,'multiline':False]
['text':' an 8-GPU baseline (if applicable), as well as measurements for however','line_number':7,'multiline':False]
['text':' many processes can participate in training.','line_number':8,'multiline':False]
['text':'','line_number':9,'multiline':False]
['text':' Throw away measurements for warmup iterations','line_number':78,'multiline':False]
['text':' Aggregate measurements for better estimation of percentiles','line_number':92,'multiline':False]
['text':' Synthesize the set of benchmarks to run.','line_number':97,'multiline':False]
['text':' This list contain tuples for ("string prefix", [rank...]).','line_number':98,'multiline':False]
['text':' noqa: E999','line_number':107,'multiline':False]
['text':' Every process runs once by themselves to warm up (CUDA init, etc).','line_number':124,'multiline':False]
['text':' Single machine baselines','line_number':127,'multiline':False]
['text':' Multi-machine benchmarks','line_number':133,'multiline':False]
['text':' Run benchmarks in order of increasing number of GPUs','line_number':137,'multiline':False]
['text':' Turn range into materialized list.','line_number':141,'multiline':False]
['text':' The global process group used only for communicating benchmark','line_number':210,'multiline':False]
['text':' metadata, like measurements. Not for benchmarking itself.','line_number':211,'multiline':False]
['text':' Write file with benchmark results if applicable','line_number':277,'multiline':False]
