['text':' mode is used to show the direction of the benchmark:','line_number':10,'multiline':False]
['text':' if 'Q', benchmark quantization, else dequantization','line_number':11,'multiline':False]
['text':' this is reused for per-channel: avoid single channel test','line_number':23,'multiline':False]
['text':' === Per Channel quantization ===','line_number':65,'multiline':False]
['text':' === Fake Quantization ===','line_number':123,'multiline':False]
['text':' Generated benchmarks names start with 'learnable_kernel' or 'original_kernel',','line_number':124,'multiline':False]
['text':'    for ex. 'original_kernel_nbits8_cpu_N1_C1_H256_W256_zero_point_dtypetorch.int32_bwdall'','line_number':125,'multiline':False]
['text':' op_type is used to describe the type of operator used in benchmarking:','line_number':173,'multiline':False]
['text':' learnable_kernel represents the c++ kernel that can backpropagate on','line_number':174,'multiline':False]
['text':' scale and zero point.','line_number':175,'multiline':False]
['text':' original_kernel represents the original fake quantize c++ kernel.','line_number':176,'multiline':False]
['text':' TODO(future PR) Combine config for floating point zero_point with other configs, once it is','line_number':213,'multiline':False]
['text':' fully supported in all fakeQuant operators and devices for','line_number':214,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/61866.','line_number':215,'multiline':False]
['text':' Axis is chosen with respect to the number of channels: C.','line_number':318,'multiline':False]
