['text':'
 When adding a test for an operator implemented in static runtime, there are
 several things that you need to pay attention to:

 1) if the op is an out variant, in the test script of the op,
 instead of:
    def forward(self, input):
      return myop(input)

  do:
    def forward(self, input):
      return myop(input).clone()

 This makes sure that the output of myop is managed by the memory planner and
 exercise the code path in the op impl that otherwise doesn't get exercised. The
 output of the model is not managed by the memory planner, because it needs to
 be returned to the client.

 2) The memory planner rounds up the size of each Tensor's storage to multiples
 of 64 bytes (alignment requirement on AVX512). Make sure the sizes of the input
 tensors in args2 are big enough to trigger resizing.

 3) for view ops such as aten::reshape or aten::to, if you want it to be
 replaced by the copy version with the ReplaceWithCopy pass in passes.h, you
 also want to make sure its output is not returned as the model output. The
 reason is that ReplaceWithCopy only replaces the op whose output is not an
 alias of the model output.
','line_number':21,'multiline':True]
['text':' sum','line_number':83,'multiline':False]
['text':'args2=','line_number':191,'multiline':True]
['text':'use_allclose=','line_number':191,'multiline':True]
['text':'use_allclose=','line_number':192,'multiline':True]
['text':'args2=','line_number':195,'multiline':True]
['text':'use_allclose=','line_number':195,'multiline':True]
['text':'use_allclose=','line_number':196,'multiline':True]
['text':'
  Clone called two times to trigger memory planner for output of first clone.
  The output of last op(second clone) is not managed by memory planner since it
  needs to be returned to the client and cannot be reused by planner.
  ','line_number':201,'multiline':True]
['text':' Case: clone with different set of memory_formats','line_number':212,'multiline':False]
['text':'
  Case: input stride set to 0 (due to expand op)
  calls native clone instead of out variant
  ','line_number':219,'multiline':True]
['text':'
  Case: testing the case of sliced tensor for
  testing non-contiguous tensor storage
  ','line_number':229,'multiline':True]
['text':' no nnc','line_number':439,'multiline':False]
['text':' with nnc','line_number':446,'multiline':False]
['text':' no nnc','line_number':453,'multiline':False]
['text':' logit','line_number':466,'multiline':False]
['text':' 1-D, 1-D','line_number':826,'multiline':False]
['text':' 2-D, 2-D','line_number':829,'multiline':False]
['text':' 1-D, 2-D','line_number':832,'multiline':False]
['text':' 2-D, 1-D','line_number':835,'multiline':False]
['text':' > 2-D , > 2-D','line_number':838,'multiline':False]
['text':' Ensure that the input values are valid.','line_number':962,'multiline':False]
['text':'args2','line_number':1045,'multiline':True]
['text':'use_allclose','line_number':1046,'multiline':True]
['text':'use_equalnan','line_number':1047,'multiline':True]
['text':'use_allclose','line_number':1052,'multiline':True]
['text':'use_equalnan','line_number':1053,'multiline':True]
['text':'args2','line_number':1121,'multiline':True]
['text':'use_allclose','line_number':1121,'multiline':True]
['text':'use_allclose','line_number':1122,'multiline':True]
['text':' exercise reshape_copy and flatten_copy','line_number':1193,'multiline':False]
['text':' exercise reshape_copy','line_number':1203,'multiline':False]
['text':' b is in_contiguous','line_number':1227,'multiline':False]
['text':' exercise flatten_copy','line_number':1280,'multiline':False]
['text':' use_allclose ','line_number':1310,'multiline':True]
['text':' use_equalnan ','line_number':1311,'multiline':True]
['text':' if input is float tensor, b could be alias of a','line_number':1385,'multiline':False]
['text':' default for use_allclose ','line_number':1433,'multiline':True]
['text':' default for use_equalnan ','line_number':1434,'multiline':True]
['text':' check_resize ','line_number':1435,'multiline':True]
['text':' Second set of args tests case where the `other` tensor's dtype','line_number':1441,'multiline':False]
['text':' changes between iterations.','line_number':1442,'multiline':False]
['text':' default for use_allclose ','line_number':1447,'multiline':True]
['text':' default for use_equalnan ','line_number':1448,'multiline':True]
['text':' check_resize ','line_number':1449,'multiline':True]
['text':' dynamic shapes','line_number':1456,'multiline':False]
['text':' float->float, NCHW->NHWC','line_number':1468,'multiline':False]
['text':' float->half','line_number':1474,'multiline':False]
['text':' float->float','line_number':1480,'multiline':False]
['text':' TODO: check if fbgemm is enabled properly in this case','line_number':1491,'multiline':False]
['text':' half->float, NCHW->NHWC','line_number':1492,'multiline':False]
['text':'use_allclose=','line_number':1551,'multiline':True]
['text':'use_equalnan=','line_number':1552,'multiline':True]
['text':'check_resize=','line_number':1553,'multiline':True]
['text':'use_allclose=','line_number':1609,'multiline':True]
['text':'use_equalnan=','line_number':1610,'multiline':True]
['text':'check_resize=','line_number':1611,'multiline':True]
['text':'use_allclose=','line_number':1688,'multiline':True]
['text':'use_equalnan=','line_number':1689,'multiline':True]
['text':'check_resize=','line_number':1690,'multiline':True]
['text':'use_allclose=','line_number':1724,'multiline':True]
['text':'use_equalnan=','line_number':1725,'multiline':True]
['text':'check_resize=','line_number':1726,'multiline':True]
['text':' 2D tensors - cat dim = 0','line_number':1763,'multiline':False]
['text':' 3D tensors - cat dim = 1','line_number':1767,'multiline':False]
['text':' 3D tensors - cat dim = 2','line_number':1771,'multiline':False]
['text':' Negative dim','line_number':1775,'multiline':False]
['text':' run jit graph executor','line_number':1786,'multiline':False]
['text':' run static runtime','line_number':1790,'multiline':False]
['text':' empty','line_number':1828,'multiline':False]
['text':' inline','line_number':1829,'multiline':False]
['text':' max inline size','line_number':1830,'multiline':False]
['text':' minimum outline size','line_number':1831,'multiline':False]
['text':' large outline size','line_number':1832,'multiline':False]
['text':' Index with boolean mask','line_number':1909,'multiline':False]
['text':' Index with tensor','line_number':1914,'multiline':False]
['text':' Index with None','line_number':1927,'multiline':False]
['text':' When indexing with none, the shape of `f` becomes [2, 1, 2],','line_number':1928,'multiline':False]
['text':' so the mask must be reshaped appropriately.','line_number':1929,'multiline':False]
['text':' Index with multiple tensors','line_number':1946,'multiline':False]
['text':' 2x2 tensor','line_number':1947,'multiline':False]
['text':' 3x3x3 tensor','line_number':1952,'multiline':False]
['text':' use_allclose ','line_number':2035,'multiline':True]
['text':' use_equalnan ','line_number':2036,'multiline':True]
['text':' check_resize ','line_number':2037,'multiline':True]
['text':' No need to test these multiple times, args are not tensors','line_number':2099,'multiline':False]
['text':' 2D tensors - stack dim = 0','line_number':2361,'multiline':False]
['text':' 3D tensors - stack dim = 1','line_number':2365,'multiline':False]
['text':' 3D tensors - stack dim = 2','line_number':2369,'multiline':False]
['text':' Negative dim','line_number':2373,'multiline':False]
['text':' Non-serial path','line_number':2377,'multiline':False]
['text':' Fast path','line_number':2381,'multiline':False]
['text':' fmod tensor version','line_number':2394,'multiline':False]
['text':' check for dynamic shapes','line_number':2400,'multiline':False]
['text':' fmod scalar version','line_number':2415,'multiline':False]
['text':' check for dynamic shapes','line_number':2419,'multiline':False]
['text':' test int32 version','line_number':2424,'multiline':False]
['text':' Update the result of aten::list to ensure that a deep copy','line_number':2620,'multiline':False]
['text':' took place','line_number':2621,'multiline':False]
['text':' In this script, the optimization is not applied since there is a','line_number':2837,'multiline':False]
['text':' computation between the TupleUnpack nodes.','line_number':2838,'multiline':False]
['text':' Use allclose and equalnan since outputs may be NaN.','line_number':2856,'multiline':False]
['text':'args2','line_number':2860,'multiline':True]
['text':'use_alloclose','line_number':2861,'multiline':True]
['text':'use_equalnan','line_number':2862,'multiline':True]
['text':'use_allclose','line_number':2867,'multiline':True]
['text':'use_equalnan','line_number':2868,'multiline':True]
['text':' Use allclose and equalnan since outputs may be NaN.','line_number':2880,'multiline':False]
['text':'args2','line_number':2884,'multiline':True]
['text':'use_alloclose','line_number':2885,'multiline':True]
['text':'use_equalnan','line_number':2886,'multiline':True]
['text':'use_allclose','line_number':2891,'multiline':True]
['text':'use_equalnan','line_number':2892,'multiline':True]
['text':' Note that clone is not technically necessary here since this is not','line_number':2928,'multiline':False]
['text':' an out variant, but it suppresses warnings about only have one op','line_number':2929,'multiline':False]
['text':' in testStaticRuntime','line_number':2930,'multiline':False]
['text':' Note: this is a native op, not an out variant, but clone anyways','line_number':2969,'multiline':False]
['text':' to silence warnings in testStaticRuntime','line_number':2970,'multiline':False]
['text':' Conservative so this op doesn't get deleted by dead','line_number':3064,'multiline':False]
['text':' code elimination','line_number':3065,'multiline':False]
['text':' namespace','line_number':3072,'multiline':False]
['text':' The run didn't finish, we didn't allocate the memory planner','line_number':3092,'multiline':False]
['text':' We guarantee that the runtime is still usable after the crash.','line_number':3096,'multiline':False]
['text':' Run again to verify this.','line_number':3097,'multiline':False]
['text':' We guarantee that the runtime is still usable after the crash.','line_number':3125,'multiline':False]
['text':' Run again to verify this.','line_number':3126,'multiline':False]
['text':' Jit Interpreter.','line_number':3179,'multiline':False]
['text':' Static Runtime.','line_number':3186,'multiline':False]
['text':' Make a fresh graph to ensure the pass works in isolation','line_number':3193,'multiline':False]
['text':' If a and b are both uninitialized, then a != b. So just check that the type','line_number':3403,'multiline':False]
['text':' is Any','line_number':3404,'multiline':False]
['text':' Not 0D tensor','line_number':3567,'multiline':False]
['text':' Wrong dtype','line_number':3569,'multiline':False]
['text':' Have to use_allclose even though all NaNs will be replaced - testStaticRuntime','line_number':3663,'multiline':False]
['text':' also checks inputs at the end to make sure they're not changed','line_number':3664,'multiline':False]
['text':'use_allclose=','line_number':3665,'multiline':True]
['text':'use_equalnan=','line_number':3665,'multiline':True]
['text':'use_allclose=','line_number':3666,'multiline':True]
['text':'use_equalnan=','line_number':3666,'multiline':True]
['text':'use_allclose=','line_number':3668,'multiline':True]
['text':'use_equalnan=','line_number':3668,'multiline':True]
['text':'use_allclose=','line_number':3669,'multiline':True]
['text':'use_equalnan=','line_number':3669,'multiline':True]
['text':'use_allclose=','line_number':3671,'multiline':True]
['text':'use_equalnan=','line_number':3671,'multiline':True]
['text':'use_allclose=','line_number':3672,'multiline':True]
['text':'use_equalnan=','line_number':3672,'multiline':True]
['text':' Non-NNC path','line_number':3674,'multiline':False]
['text':'use_allclose=','line_number':3675,'multiline':True]
['text':'use_equalnan=','line_number':3675,'multiline':True]
['text':'use_allclose=','line_number':3676,'multiline':True]
['text':'use_equalnan=','line_number':3676,'multiline':True]
