['text':' Always at least 2d strides to support 2d for_each loops','line_number':53,'multiline':False]
['text':'/ Construction','line_number':105,'multiline':False]
['text':' WARNING:','line_number':157,'multiline':False]
['text':'   This will bypass all shape checking in the TensorIterator. Kernels which call this method','line_number':158,'multiline':False]
['text':'   are expected to check shapes before calling `add_owned_input` or `add_owned_output`.','line_number':159,'multiline':False]
['text':' NOTE: [Computing output strides]','line_number':176,'multiline':False]
['text':' We use the following algorithm to compute output strides','line_number':177,'multiline':False]
['text':' If correctly sized output is provided, we respect its strides and don't change them','line_number':178,'multiline':False]
['text':' Otherwise, if provided output is of incorrect size or no output is provided,','line_number':179,'multiline':False]
['text':' we try to recover permutation that was applied to the inputs','line_number':180,'multiline':False]
['text':' by sorting the strides of the inputs. Precedence is given to the inputs in the order they were added,','line_number':181,'multiline':False]
['text':' and to permutations involving non-broadcasted dimensions','line_number':182,'multiline':False]
['text':' 1. we loop over inputs starting from the first','line_number':183,'multiline':False]
['text':' 2. for all inputs strides of broadcasted dimensions are set to 0, and 0 compares equal to anything. If one','line_number':184,'multiline':False]
['text':' of the dimensions being compared has a stride of 0, we move on to the next tensor to determine if','line_number':185,'multiline':False]
['text':' these dimensions need to be swapped.','line_number':186,'multiline':False]
['text':' 3. strides of dimensions equal to 1 participate in sorting','line_number':187,'multiline':False]
['text':' 4. if 2 strides are equal and neither is 0, we try to break the tie by looking at the corresponding dimensions','line_number':188,'multiline':False]
['text':' of the tensor. Dimensions were permuted if, when iterating from the end, dimensions corresponding to the','line_number':189,'multiline':False]
['text':' same strides are increasing. If dimensions are non-increasing, we move on to the next input to break the tie.','line_number':190,'multiline':False]
['text':'','line_number':191,'multiline':False]
['text':' Instead of applying rule 4 for tie breaking, we could move on to the next tensor directly. This would result in possibly','line_number':192,'multiline':False]
['text':' losing the correct permuation of the first tensor if there are permuted trivial dimensions, but could potentially','line_number':193,'multiline':False]
['text':' improve traversal order of the second tensor. We chose the former option to better propagate channels last layout','line_number':194,'multiline':False]
['text':' for example for a tensor with the sizes N1H1','line_number':195,'multiline':False]
['text':' These rules result in the intuitive behavior that in most cases recovers permutation of either the first argument (if all','line_number':196,'multiline':False]
['text':' arguments are of the same size) or the argument that is not broadcasted, regardless of its position.','line_number':197,'multiline':False]
['text':' As a bonus, it also result in reasonably well-behaved traversal order of the inputs and outputs - in the kernels','line_number':198,'multiline':False]
['text':' output is traversed linearly, and since it closely follows input layouts, inputs are traversed linearly as well','line_number':199,'multiline':False]
['text':'','line_number':200,'multiline':False]
['text':' Examples:','line_number':201,'multiline':False]
['text':' full size tensor + broadcasted tensor with 0 or 1 non-trivial dimensions => strides of output are same','line_number':202,'multiline':False]
['text':' as strides of full size input regardless of the order','line_number':203,'multiline':False]
['text':' 2 tensors of same size but different strides => output strides are the same as first argument','line_number':204,'multiline':False]
['text':'','line_number':205,'multiline':False]
['text':' We also have fast path for memory-dense inputs with the same strides (or, trivially, single memory-dense input)','line_number':206,'multiline':False]
['text':' that outputs a tensor with the same strides as inputs. The only difference in result with the algorithm described','line_number':207,'multiline':False]
['text':' above is for strides for trivial (1) dimensions, where in ambiguous cases for performance reasons we default to','line_number':208,'multiline':False]
['text':' contiguous strides.','line_number':209,'multiline':False]
['text':' Example: tensor with sizes NC11 and strides C1CC will produce output with strides C111 (note differences are only','line_number':210,'multiline':False]
['text':' in the strides of trivial dimensions, so physical layout is unaffected but permutation information is lost)','line_number':211,'multiline':False]
['text':' We might change this behavior in future once performance considerations are resolved','line_number':212,'multiline':False]
['text':' Sort the dimensions based on strides in ascending order with reduced dims','line_number':215,'multiline':False]
['text':' at the front. NOTE: that this inverts the order of C-contiguous tensors.','line_number':216,'multiline':False]
['text':' strides[0] is the fastest moving dimension instead of strides[ndim - 1].','line_number':217,'multiline':False]
['text':' See NOTE: [Computing output strides] and inline  comments for more detailed description','line_number':218,'multiline':False]
['text':' initialize perm with n-1, n-2, ..., 1, 0','line_number':226,'multiline':False]
['text':' Reordering dimensions changes iteraton order','line_number':229,'multiline':False]
['text':' returns 1 if the dim0 should come after dim1, -1 if dim0 should come','line_number':235,'multiline':False]
['text':' before dim1, and 0 if the comparison is ambiguous.','line_number':236,'multiline':False]
['text':' ignore undefined or incorrectly sized tensors','line_number':239,'multiline':False]
['text':' move reduced dimensions to the front','line_number':246,'multiline':False]
['text':' strides of reduced dimensions are always set to 0 by review_reduce_result','line_number':247,'multiline':False]
['text':'move on to the next input if one of the dimensions is broadcasted','line_number':252,'multiline':False]
['text':' it is important to return here only with strict comparisons, for equal strides we try to break the tie later','line_number':255,'multiline':False]
['text':' by comparing corresponding dimensions or if that does not work, moving on to the next tensor','line_number':256,'multiline':False]
['text':'equal strides, use dimensions themselves as the tie-breaker.','line_number':261,'multiline':False]
['text':'at this point, with zero strides out of the way, we are guaranteed that operand dimensions are equal to shape_','line_number':262,'multiline':False]
['text':'return only if dimensions should be swapped, otherwise move on to the next tensor','line_number':265,'multiline':False]
['text':' insertion sort with support for ambiguous comparisons','line_number':274,'multiline':False]
['text':' perform re-ordering of shape and strides','line_number':288,'multiline':False]
['text':' Computes a common dtype using type promotion','line_number':292,'multiline':False]
['text':' See the [Common Dtype Computation] note','line_number':293,'multiline':False]
['text':' Implements the behavior of the following flags:','line_number':318,'multiline':False]
['text':'   - check_all_same_dtype_','line_number':319,'multiline':False]
['text':'   - check_all_same_device_','line_number':320,'multiline':False]
['text':'   - enforce_safe_casting_to_output_','line_number':321,'multiline':False]
['text':'   - promote_inputs_to_common_dtype_','line_number':322,'multiline':False]
['text':'   - cast_common_dtype_to_outputs_','line_number':323,'multiline':False]
['text':'','line_number':324,'multiline':False]
['text':' See their descriptions in TensorIterator.h for details.','line_number':325,'multiline':False]
['text':' NOTE: Checks for more specific behaviors (e.g. the first and second','line_number':326,'multiline':False]
['text':'   inputs must share a dtype, but the third must have the long dtype)','line_number':327,'multiline':False]
['text':'   should be implemented directly and outside of TensorIterator.','line_number':328,'multiline':False]
['text':' Reviews operands (1/2)','line_number':330,'multiline':False]
['text':'   - validates that all input tensors are defined','line_number':331,'multiline':False]
['text':'   - computes common device','line_number':332,'multiline':False]
['text':'   - determines if there are undefined outputs','line_number':333,'multiline':False]
['text':'   - determines if there are different dtypes and attempts','line_number':334,'multiline':False]
['text':'       to quickly acquire a common dtype','line_number':335,'multiline':False]
['text':' NB: despite output_dtype's generic sounding name, it only is','line_number':338,'multiline':False]
['text':' used in a nontrivial way if check_all_same_dtype is true','line_number':339,'multiline':False]
['text':' Validates that all inputs have type information, and that','line_number':346,'multiline':False]
['text':'   if an output is missing type information that we can infer','line_number':347,'multiline':False]
['text':'   the device it should be allocated on.','line_number':348,'multiline':False]
['text':' Validates input tensors are defined','line_number':369,'multiline':False]
['text':' Acquires the first non-CPU device (if any) as the common device','line_number':377,'multiline':False]
['text':' Determines if there are varying input dtypes','line_number':383,'multiline':False]
['text':' NOTE: the common dtype is set to the first defined input dtype observed','line_number':384,'multiline':False]
['text':' op.is_output','line_number':392,'multiline':False]
['text':' Determines if there are varying output dtypes','line_number':393,'multiline':False]
['text':' NOTE: the output dtype is set to the first defined output dtype observed','line_number':394,'multiline':False]
['text':' Checks that either the computation type is computable or unneeded','line_number':405,'multiline':False]
['text':' Checks that all inputs and defined outputs are the same dtype, if requested','line_number':410,'multiline':False]
['text':' Throws an informative error message','line_number':414,'multiline':False]
['text':' Short-circuits if no additional work required','line_number':425,'multiline':False]
['text':' Invalidates common_dtype_ if it could not be inferred','line_number':429,'multiline':False]
['text':' Computes a common dtype, if needed','line_number':434,'multiline':False]
['text':' Promotes common dtype to the default float scalar type, if needed','line_number':439,'multiline':False]
['text':'includeBool=','line_number':441,'multiline':True]
['text':' Reviews operands (2/2)','line_number':445,'multiline':False]
['text':'   - sets metadata for undefined outputs','line_number':446,'multiline':False]
['text':'   - checks that all tensors are on the same device, if requested','line_number':447,'multiline':False]
['text':'   - checks that the common dtype can safely cast to each output, if requested','line_number':448,'multiline':False]
['text':'   - creates temporaries for CPU operations, if needed and requested','line_number':449,'multiline':False]
['text':' Skips undefined tensors','line_number':468,'multiline':False]
['text':' Checks all tensors are on the same device, if requested','line_number':473,'multiline':False]
['text':' Handles CPU scalars on CUDA kernels that support them','line_number':475,'multiline':False]
['text':' Checks safe casting, if requested','line_number':489,'multiline':False]
['text':' Creates temporaries for CPU operations, if needed and requested','line_number':496,'multiline':False]
['text':' TODO: reuse temporaries when possible (e.g. for inplace operations)','line_number':497,'multiline':False]
['text':' Casts to outputs by creating temporaries of the correct dtype (if needed)','line_number':499,'multiline':False]
['text':' NB: we skip this on is_meta_, because the temporary allocation here is','line_number':500,'multiline':False]
['text':' unnecessary if we aren't going to actually do the compute','line_number':501,'multiline':False]
['text':' Marker [Output original_tensor is set]','line_number':504,'multiline':False]
['text':' NB: do NOT use set_output here, as the temporary is NOT a true output;','line_number':505,'multiline':False]
['text':' op.tensor is the true output and it was pre-provided for us.','line_number':506,'multiline':False]
['text':' TODO: The logic for cast_outputs will need to be handled by the','line_number':507,'multiline':False]
['text':' structured kernels implementation.  What probably should happen','line_number':508,'multiline':False]
['text':' is that we pass in the inferred dtype into the out kernel, and','line_number':509,'multiline':False]
['text':' then after calling the out kernel, do the conversion (which','line_number':510,'multiline':False]
['text':' is cast_outputs here), but integrating this with existing','line_number':511,'multiline':False]
['text':' TensorIterator will take a little doing','line_number':512,'multiline':False]
['text':' Promotes inputs by creating temporaries of the correct dtype','line_number':524,'multiline':False]
['text':' Invert the permutation caused by reorder_dimensions. This is not valid','line_number':545,'multiline':False]
['text':' after coalesce_dimensions is called.','line_number':546,'multiline':False]
['text':'no initialization needed, every value in res should be written to.','line_number':549,'multiline':False]
['text':' check if permutation is just an inverted order','line_number':563,'multiline':False]
['text':' can just return contiguous output','line_number':573,'multiline':False]
['text':' it is faster because it avoids allocating 0 size tensor and','line_number':574,'multiline':False]
['text':' resizing and restriding it','line_number':575,'multiline':False]
['text':' Even if we don't resize, we still need to tell set_output about','line_number':586,'multiline':False]
['text':' the output, so that we properly set guard and propagate names','line_number':587,'multiline':False]
['text':' Don't include output tensors if we are resizing, since we will','line_number':606,'multiline':False]
['text':' clobber their names in any case.  (If the output tensor was','line_number':607,'multiline':False]
['text':' also an input tensor, we'll pick it up when it shows up again','line_number':608,'multiline':False]
['text':' in operands).','line_number':609,'multiline':False]
['text':' perform name inference','line_number':611,'multiline':False]
['text':' We can coalesce two adjacent dimensions if either dim has size 1 or if:','line_number':625,'multiline':False]
['text':' shape[n] * stride[n] == stride[n + 1].','line_number':626,'multiline':False]
['text':' replace each operands stride at dim0 with its stride at dim1','line_number':642,'multiline':False]
['text':' Update shape and strides','line_number':716,'multiline':False]
['text':' TODO: check for casting once it's supported','line_number':783,'multiline':False]
['text':' TODO: Now that set_output resizes both the original_tensor','line_number':816,'multiline':False]
['text':' and tensor, this condition should no longer ever be true','line_number':817,'multiline':False]
['text':' Helper to construct a binary op that promotes integer inputs to float.','line_number':872,'multiline':False]
['text':' When 'out' isn't defined (e.g. for the functional operator 'a == b'), we','line_number':894,'multiline':False]
['text':' want the output to be bool. Otherwise (e.g. 'torch.eq(a, b, out=c)') we','line_number':895,'multiline':False]
['text':' don't coerce the output.','line_number':896,'multiline':False]
['text':' Note [special-case bool outputs]','line_number':901,'multiline':False]
['text':' We explicitly don't call `cast_common_dtype_to_outputs` when the output tensor','line_number':902,'multiline':False]
['text':' has `bool` dtype. This is a performance optimization: the functional','line_number':903,'multiline':False]
['text':' version of all comparison/logical ops uses a bool output tensor, and we'd like to','line_number':904,'multiline':False]
['text':' avoid creating a temporary copy of the output.','line_number':905,'multiline':False]
['text':' However, note that all kernels using this TensorIterator will need to special-case when','line_number':906,'multiline':False]
['text':' the output tensor has bool dtype, and provide a lambda of type (scalar_t, scalar_t -> bool).','line_number':907,'multiline':False]
['text':' This cannot be a function because TensorIteratorConfig is not','line_number':959,'multiline':False]
['text':' copyable or movable, so it can't be returned from the function.','line_number':960,'multiline':False]
['text':' This cannot be a function because TensorIteratorConfig is not','line_number':984,'multiline':False]
['text':' copyable or movable, so it can't be returned from the function.','line_number':985,'multiline':False]
['text':' This cannot be a function because TensorIteratorConfig is not','line_number':1006,'multiline':False]
['text':' copyable or movable, so it can't be returned from the function.','line_number':1007,'multiline':False]
['text':' Helper to construct a unary op that forcibly promotes output to boolean.','line_number':1033,'multiline':False]
['text':' Only be used when the output tensor must have boolean type.','line_number':1034,'multiline':False]
['text':' FIXME: workaround for bug: https://github.com/pytorch/pytorch/issues/20342 ','line_number':1087,'multiline':True]
['text':' TODO: not supporting casting to outputs is only really necessary for arg{min,max}','line_number':1110,'multiline':False]
['text':' If *any* of the arguments is a meta tensor, the overall','line_number':1140,'multiline':False]
['text':' computation is a meta computation (don't do any work,','line_number':1141,'multiline':False]
['text':' just compute output information).  This aligns with','line_number':1142,'multiline':False]
['text':' our multiple dispatch semantics.','line_number':1143,'multiline':False]
['text':' TODO: merge this into populate_operands','line_number':1153,'multiline':False]
['text':' check if output is also an input','line_number':1159,'multiline':False]
['text':' Outputs cannot be broadcasted. Check that the shape of the outputs matches','line_number':1170,'multiline':False]
['text':' the inferred shape. There's an exception for write-only tensors to support','line_number':1171,'multiline':False]
['text':' our legacy behavior that functions with `out=` arguments resize their','line_number':1172,'multiline':False]
['text':' outputs.','line_number':1173,'multiline':False]
['text':' for reduction, output size does not match shape_, as output is reduced size, and shape_ is size of the input','line_number':1184,'multiline':False]
['text':' For now, don't include output tensors when we're resizing outputs.','line_number':1220,'multiline':False]
['text':' These shapes don't participate in shape computation.','line_number':1221,'multiline':False]
['text':' This preserves the legacy behavior where torch.add(..., out=dst) resizes','line_number':1222,'multiline':False]
['text':' the destination tensor.  If the output tensor is also an input, we'll','line_number':1223,'multiline':False]
['text':' pick it up later in the operands.','line_number':1224,'multiline':False]
['text':' see NOTE: [Computing output strides]','line_number':1260,'multiline':False]
['text':' std::abs is necessary to handle some special cases where we support negative strides','line_number':1314,'multiline':False]
['text':' see the CUDA backend of at::flip','line_number':1315,'multiline':False]
['text':' This function tries to do a fast setup to avoid needless reordering of dimensions and tracking output strides','line_number':1328,'multiline':False]
['text':' Return true if it can do fast setup or false otherwise','line_number':1329,'multiline':False]
['text':' TODO enable fast handling for reductions','line_number':1330,'multiline':False]
['text':' allocate memory for output, memory format depends on setup_type','line_number':1336,'multiline':False]
['text':' find the index of a defined tensor in operands_ start from input tensor','line_number':1362,'multiline':False]
['text':' NOLINT(cppcoreguidelines-init-variables)','line_number':1363,'multiline':False]
['text':'coalescing dimensions consists of collapsing dimensions to 1 (we are limited to contiguous no-broadcast cases here)','line_number':1380,'multiline':False]
['text':' For linear iteration, only contiguous tensors can be coalesced','line_number':1403,'multiline':False]
['text':' Fast setup of any other format requires changing iteration order','line_number':1404,'multiline':False]
['text':' TODO this leads to ambiguous cases (NC11) to be always treated as contiguous','line_number':1427,'multiline':False]
['text':' Fast setup is allowed only when all the defined tensors have the same shape and strides,','line_number':1436,'multiline':False]
['text':' Iterate from back to check input tensors' strides first, then output tensors'.','line_number':1437,'multiline':False]
['text':' [Note: stride check for non contiguous tensors in fast setup]','line_number':1446,'multiline':False]
['text':' We prevent 3 cases doing fast setup here:','line_number':1447,'multiline':False]
['text':' 1. input tensors have different strides.','line_number':1448,'multiline':False]
['text':' 2. output tensors won't be resized and have different strides.','line_number':1449,'multiline':False]
['text':' 3. input tensors have the same strides, but output tensors have different strides with input tensors.','line_number':1450,'multiline':False]
['text':'    We don't allow re-stride output tensors in this case since it is not compatible with','line_number':1451,'multiline':False]
['text':'    numpy. The behavior in numpy is that if the output tensor has same shape as the input','line_number':1452,'multiline':False]
['text':'    tensor but different strides, the strides of output tensor will be preserved, so we do','line_number':1453,'multiline':False]
['text':'    the same in tensor iterator.','line_number':1454,'multiline':False]
['text':' populate some persistent configuration fields','line_number':1467,'multiline':False]
['text':' fill in operands_ based on configuration','line_number':1471,'multiline':False]
['text':' set is_output and is_read_write flags on appropriate tensors','line_number':1473,'multiline':False]
['text':' Check that the outputs have no internal overlap','line_number':1475,'multiline':False]
['text':' and do not share memory with inputs.','line_number':1476,'multiline':False]
['text':' Check that input dimensions are aligned correctly & compute outnames.','line_number':1478,'multiline':False]
['text':' compute the broadcasted shape','line_number':1480,'multiline':False]
['text':' mark outputs for resizing if necessary','line_number':1482,'multiline':False]
['text':' compute the result dtype and device','line_number':1484,'multiline':False]
['text':' try fast setup output tensor, if failed, fallback to normal setup','line_number':1486,'multiline':False]
['text':' compute each tensor's stride after broadcasting','line_number':1488,'multiline':False]
['text':' re-order dimensions to improve coalescing','line_number':1490,'multiline':False]
['text':' allocate the output tensor if it's not provided','line_number':1492,'multiline':False]
['text':' coalesce adjacent dimensions when possible','line_number':1494,'multiline':False]
['text':' XLA and lazy tensors don't have storage, so they don't have an underlying data pointer.','line_number':1508,'multiline':False]
['text':' Nothing beyond this point is important for meta functions, so it's fine to exit early here.','line_number':1509,'multiline':False]
['text':' Extend the condition to ORT tesnors as ORT tensors also don't have storage.','line_number':1510,'multiline':False]
['text':' zero out offsets','line_number':1524,'multiline':False]
['text':' If the tensor is a scalar, we leave room for it','line_number':1525,'multiline':False]
['text':' So index translations in reduction can access','line_number':1526,'multiline':False]
['text':' a valid value for the offset','line_number':1527,'multiline':False]
['text':' This is the structured kernels' implementation of set_output.  It is','line_number':1532,'multiline':False]
['text':' NEVER actually called directly; instead, a subclass of TensorIteratorBase','line_number':1533,'multiline':False]
['text':' will override set_output to actually do the operation, and then call','line_number':1534,'multiline':False]
['text':' set_output on the TensorIteratorBase to setup TI's metadata.','line_number':1535,'multiline':False]
['text':' The precondition for this function is that maybe_get_output() now','line_number':1536,'multiline':False]
['text':' unconditionally returns a real Tensor (prior to output setting,','line_number':1537,'multiline':False]
['text':' this function may return an undefined tensor.)','line_number':1538,'multiline':False]
['text':' OK, so this is pretty weird.  To understand how we can end up in','line_number':1549,'multiline':False]
['text':' this situation, first look at Marker [Output original_tensor is set].','line_number':1550,'multiline':False]
['text':' That is the sole site where original_tensor may be set on an','line_number':1551,'multiline':False]
['text':' output operand.  Essentially, when we are given an explicit output','line_number':1552,'multiline':False]
['text':' tensor whose dtype doesn't match the computed common dtype from','line_number':1553,'multiline':False]
['text':' the input operands, we do a switcheroo: we replace the (incorrectly','line_number':1554,'multiline':False]
['text':' typed) output tensor with a correctly typed, *temporary* tensor,','line_number':1555,'multiline':False]
['text':' and remember the original tensor in original_tensor (which will','line_number':1556,'multiline':False]
['text':' then get written back to when we cast_outputs).','line_number':1557,'multiline':False]
['text':'','line_number':1558,'multiline':False]
['text':' Now, what if the given output tensor also happened to be zero','line_number':1559,'multiline':False]
['text':' size (meaning that we will_resize it)?  Well, at the call site','line_number':1560,'multiline':False]
['text':' above, we don't necessarily(*) know what the correct shape should','line_number':1561,'multiline':False]
['text':' be, so we give the temporary tensor the same shape as the original.','line_number':1562,'multiline':False]
['text':' At the time of set_output is when we DO know what the correct size','line_number':1563,'multiline':False]
['text':' is, and the subclass's implementation of set_output in structured class','line_number':1564,'multiline':False]
['text':' responsible for resizing original_tensor.  But we still have this','line_number':1565,'multiline':False]
['text':' incorrectly sized temporary output which the structured subclass','line_number':1566,'multiline':False]
['text':' knows nothing about, so we are obligated to also resize it here.','line_number':1567,'multiline':False]
['text':'','line_number':1568,'multiline':False]
['text':' This is a slight memory pessimization, because previously','line_number':1569,'multiline':False]
['text':' original_tensor only got resized at the end of the computation, rather','line_number':1570,'multiline':False]
['text':' than at the beginning (as happens here).  However, the peak memory','line_number':1571,'multiline':False]
['text':' usage is the same, since you need to materialize both original tensor','line_number':1572,'multiline':False]
['text':' and temporary tensor to do the copy.','line_number':1573,'multiline':False]
['text':'','line_number':1574,'multiline':False]
['text':' (*) Actually, technically, we probably do know what the shape','line_number':1575,'multiline':False]
['text':' should be, since we do shape computation before dtype computation.','line_number':1576,'multiline':False]
['text':' So hypothetically we could figure out what the correct shape is','line_number':1577,'multiline':False]
['text':' at that point in time and directly allocate the temporary at','line_number':1578,'multiline':False]
['text':' the right size.','line_number':1579,'multiline':False]
['text':'','line_number':1580,'multiline':False]
['text':' But a better solution is to delay allocation of temporaries until','line_number':1581,'multiline':False]
['text':' after TensorIterator builder, waiting until we actually want','line_number':1582,'multiline':False]
['text':' to do the computation.  That would also remove the necessity','line_number':1583,'multiline':False]
['text':' for the is_meta_ test.','line_number':1584,'multiline':False]
['text':' For simplicity, just always update the cached current_type.','line_number':1599,'multiline':False]
['text':' This is the "traditional" implementation of set_output.  On TensorIterator','line_number':1603,'multiline':False]
['text':' instances, it is invoked directly from various call sites in this file.  No','line_number':1604,'multiline':False]
['text':' funny business.','line_number':1605,'multiline':False]
['text':' NB: intentionally no superclass call','line_number':1607,'multiline':False]
['text':' Not actually used by anything (TensorIterator subclass calls','line_number':1632,'multiline':False]
['text':' its own implementation of set_output which knows exactly where','line_number':1633,'multiline':False]
['text':' all the outputs are), but we have to provide all pure virtual methods','line_number':1634,'multiline':False]
['text':' for MetaBase','line_number':1635,'multiline':False]
['text':'/ SplitUntil32Bit. Recursively splits an iterator into sub-iterators that','line_number':1644,'multiline':False]
['text':'/ can use 32-bit indexing.','line_number':1645,'multiline':False]
['text':' ++ first pops the last element','line_number':1649,'multiline':False]
['text':' namespace at','line_number':1736,'multiline':False]
