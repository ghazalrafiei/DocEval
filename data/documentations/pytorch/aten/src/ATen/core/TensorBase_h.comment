['text':' namespace torch::autograd','line_number':34,'multiline':False]
['text':' Convert Tensor to TensorBase without any need to include Tensor.h','line_number':41,'multiline':False]
['text':' Please read the comment in `VariableFallbackKernel.cpp` about the background of this change.','line_number':47,'multiline':False]
['text':' NOTE: [Tensor vs. TensorBase]','line_number':56,'multiline':False]
['text':'','line_number':57,'multiline':False]
['text':' Tensor, being the central data structure in PyTorch, gets used and','line_number':58,'multiline':False]
['text':' it's header included almost everywhere. Unfortunately this means','line_number':59,'multiline':False]
['text':' every time an operator signature is updated or changed in','line_number':60,'multiline':False]
['text':' native_functions.yaml, you (and every other PyTorch developer) need','line_number':61,'multiline':False]
['text':' to recompile all of ATen and it's dependencies.','line_number':62,'multiline':False]
['text':'','line_number':63,'multiline':False]
['text':' TensorBase aims to break up these header dependencies, and improve','line_number':64,'multiline':False]
['text':' incremental build times for all PyTorch developers. TensorBase','line_number':65,'multiline':False]
['text':' represents a reference counted handle to TensorImpl, exactly the','line_number':66,'multiline':False]
['text':' same as Tensor. However, TensorBase doesn't have code generated','line_number':67,'multiline':False]
['text':' methods in it's API and thus no dependence on native_functions.yaml.','line_number':68,'multiline':False]
['text':'','line_number':69,'multiline':False]
['text':' Usage tips','line_number':70,'multiline':False]
['text':' ----------','line_number':71,'multiline':False]
['text':' - You can `#define TORCH_ASSERT_NO_OPERATORS` at the top of a .cpp','line_number':72,'multiline':False]
['text':'   or .cu file to ensure it has no header dependencies on','line_number':73,'multiline':False]
['text':'   native_functions.yaml (direct or indirect).','line_number':74,'multiline':False]
['text':' - Tensor inherits from TensorBase, so functions taking','line_number':75,'multiline':False]
['text':'   `const TensorBase &` are callable with Tensor as well.','line_number':76,'multiline':False]
['text':' - TensorBase can be converted to tensor with `Tensor(tensor_base)`,','line_number':77,'multiline':False]
['text':'   but this requires a reference-count bump. OptionalTensorRef on','line_number':78,'multiline':False]
['text':'   the other hand can materialize a `const Tensor &` without','line_number':79,'multiline':False]
['text':'   touching the reference-count.','line_number':80,'multiline':False]
['text':' Create a Tensor with a +0 reference count. Special care must be','line_number':86,'multiline':False]
['text':' taken to avoid decrementing this reference count at destruction','line_number':87,'multiline':False]
['text':' time. Intended to support MaybeOwnedTraits<Tensor>.','line_number':88,'multiline':False]
['text':' This constructor should not be used by end users and is an implementation','line_number':95,'multiline':False]
['text':' detail invoked by autogenerated code.','line_number':96,'multiline':False]
['text':' Creates a new wrapper from TensorImpl. Intentionally a free method because','line_number':108,'multiline':False]
['text':' it should be used with care. Checks necessary invariants','line_number':109,'multiline':False]
['text':'/ Should be used if *this can reasonably be expected to be contiguous and','line_number':132,'multiline':False]
['text':'/ performance is important.','line_number':133,'multiline':False]
['text':'/ Compared to contiguous, it saves a reference count','line_number':134,'multiline':False]
['text':'/ increment/decrement if *this is already contiguous, at the cost','line_number':135,'multiline':False]
['text':'/ in all cases of an extra pointer of stack usage, an extra branch','line_number':136,'multiline':False]
['text':'/ to access, and an extra branch at destruction time.','line_number':137,'multiline':False]
['text':' Use .contiguous() instead. Trying to borrow from a prvalue','line_number':141,'multiline':False]
['text':' will only lead to trouble and dangling references.','line_number':142,'multiline':False]
['text':' false is passed to maybe_wrap_dim so behavior is identical to array access (but with wrapping)','line_number':170,'multiline':False]
['text':'wrap_scalar=','line_number':171,'multiline':True]
['text':' false is passed to maybe_wrap_dim so behavior is identical to array access (but with wrapping)','line_number':182,'multiline':False]
['text':'wrap_scalar=','line_number':183,'multiline':True]
['text':' Ban assignment to rvalues, since at::Tensor (weirdly) performs a deep copy here','line_number':222,'multiline':False]
['text':' See impl::get_opt_names in ATen/NamedTensor.h for docs.','line_number':250,'multiline':False]
['text':' See impl::get_names in ATen/NamedTensor.h for docs.','line_number':254,'multiline':False]
['text':' Setting channels_last_strides_exact_match to true forces function to','line_number':272,'multiline':False]
['text':' check 0,1 - sized dimension strides.','line_number':273,'multiline':False]
['text':' Total bytes consumed by the "view" of elements of the array.  Does not','line_number':291,'multiline':False]
['text':' include size of metadata.  The number reported here does not necessarily','line_number':292,'multiline':False]
['text':' correspond to the true physical memory consumed by a tensor; instead,','line_number':293,'multiline':False]
['text':' it reports the memory the tensor would take *if* it were contiguous.','line_number':294,'multiline':False]
['text':' Defined to be numel() * itemsize()','line_number':295,'multiline':False]
['text':' Length of one array element in bytes.  This is the traditional','line_number':324,'multiline':False]
['text':' Numpy naming.','line_number':325,'multiline':False]
['text':' Same as itemsize().  This is the PyTorch naming.','line_number':330,'multiline':False]
['text':' Move the storage backend to shm based','line_number':351,'multiline':False]
['text':' to enable memory sharing across processes.','line_number':352,'multiline':False]
['text':'','line_number':353,'multiline':False]
['text':' NB1: the ideal behavior of this API still requires further discussion','line_number':354,'multiline':False]
['text':' but for now we are inclined to keep it consistent with existing THP behavior','line_number':355,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/4dca9bde0552afc67b5b74f4a0696fe6055709c4/torch/storage.py#L196-L212','line_number':356,'multiline':False]
['text':' so we don't assert on anything here and rely on caller knowing','line_number':357,'multiline':False]
['text':' what it's doing.','line_number':358,'multiline':False]
['text':'','line_number':359,'multiline':False]
['text':' NB2: this currently provides Linux fd based shm support only','line_number':360,'multiline':False]
['text':' to simplify the storage lifetime management logic in ATen','line_number':361,'multiline':False]
['text':' and similarly for now we are not adding support for file system based','line_number':362,'multiline':False]
['text':' shm support like in THP due to additional GC manager support needed','line_number':363,'multiline':False]
['text':' to prevent leaks.','line_number':364,'multiline':False]
['text':' As such, calling this from non supported systems (e.g. Windows) would fail.','line_number':365,'multiline':False]
['text':' sets the conjugate bit of a tensor.','line_number':382,'multiline':False]
['text':' NOTE: Conjugate bit is supposed to be a read-only field. Only change this, if you are sure','line_number':383,'multiline':False]
['text':' that's what you want. Changing this might lead to incorrect behavior since conjugation is','line_number':384,'multiline':False]
['text':' a lazy operation and we rely on this bit to determine if a conjugation needs to be materialized.','line_number':385,'multiline':False]
['text':' sets the negative bit of a tensor.','line_number':394,'multiline':False]
['text':' NOTE: Negative bit is supposed to be a read-only field. Only change this, if you are sure','line_number':395,'multiline':False]
['text':' that's what you want. Changing this might lead to incorrect behavior since we rely on this','line_number':396,'multiline':False]
['text':' bit to determine if a negation needs to be materialized.','line_number':397,'multiline':False]
['text':'/ Returns a `Tensor`'s layout.','line_number':402,'multiline':False]
['text':'/ Returns a `Tensor`'s dtype (`TypeMeta`).','line_number':407,'multiline':False]
['text':'/ Returns a `Tensor`'s device.','line_number':412,'multiline':False]
['text':'/ Returns a `Tensor`'s device index.','line_number':417,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':419,'multiline':False]
['text':'/ Returns if a `Tensor` has CPU backend.','line_number':423,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':425,'multiline':False]
['text':'/ Returns if a `Tensor` has CUDA backend.','line_number':429,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':431,'multiline':False]
['text':'/ Returns if a `Tensor` has IPU backend.','line_number':435,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':437,'multiline':False]
['text':'/ Returns if a `Tensor` has XPU backend.','line_number':441,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':443,'multiline':False]
['text':'/ Returns if a `Tensor` has XLA backend.','line_number':447,'multiline':False]
['text':'/ Returns if a `Tensor` has MTIA backend.','line_number':452,'multiline':False]
['text':'/ Returns if a `Tensor` has HPU backend.','line_number':457,'multiline':False]
['text':'/ Returns if a `Tensor` has Lazy backend.','line_number':462,'multiline':False]
['text':'/ Returns if a `Tensor` has HIP backend.','line_number':467,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':469,'multiline':False]
['text':'/ Returns if a `Tensor` has VE backend.','line_number':473,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':475,'multiline':False]
['text':'/ Returns if a `Tensor` has PrivateUse1 backend.','line_number':479,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':481,'multiline':False]
['text':'/ Returns if a `Tensor` has sparse backend.','line_number':485,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':487,'multiline':False]
['text':'/ Returns is a `Tensor` has a sparse CSR backend.','line_number':491,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':493,'multiline':False]
['text':'/ Returns if a `Tensor` is mkldnn tensor.','line_number':497,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':499,'multiline':False]
['text':'/ Returns if a `Tensor` is mps tensor.','line_number':503,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':505,'multiline':False]
['text':'/ Returns if a `Tensor` is ort tensor.','line_number':509,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':511,'multiline':False]
['text':'/ Returns if a `Tensor` is vulkan tensor.','line_number':515,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':517,'multiline':False]
['text':'/ Returns if a `Tensor` is metal tensor.','line_number':521,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':523,'multiline':False]
['text':'/ Returns if a `Tensor` has quantized backend.','line_number':527,'multiline':False]
['text':' NB: this is not a native function to avoid dispatching overhead.','line_number':529,'multiline':False]
['text':'/ Returns if a `Tensor` is a meta tensor.  Meta tensors can','line_number':533,'multiline':False]
['text':'/ also have other designations.','line_number':534,'multiline':False]
['text':'/ Returns if a `Tensor` is an inference tensor.','line_number':539,'multiline':False]
['text':' Returns if a `Tensor` is a NestedTensor.','line_number':544,'multiline':False]
['text':'/ If a tensor is a quantized tensor, returns its quantizer','line_number':549,'multiline':False]
['text':'/ TODO: it's not in native_functions.yaml yet as it's not exposed to python','line_number':550,'multiline':False]
['text':'/ Returns if a `Tensor` has any dimension names','line_number':553,'multiline':False]
['text':' If a user is using unnamed tensors, then we can short-circuit right here.','line_number':555,'multiline':False]
['text':' Otherwise, impl::has_names attempts to retrieve names.','line_number':556,'multiline':False]
['text':'/ Returns a `Tensor`'s dimension names data structure','line_number':563,'multiline':False]
['text':'/ Returns the `TensorOptions` corresponding to this `Tensor`. Defined in','line_number':572,'multiline':False]
['text':'/ TensorOptions.h.','line_number':573,'multiline':False]
['text':' TODO(#97856) Make this return a const pointer. This currently','line_number':588,'multiline':False]
['text':'              returns a non-const pointer because of the large','line_number':589,'multiline':False]
['text':'              number of clients that we still want to audit before','line_number':590,'multiline':False]
['text':'              migrating to mutable_data_ptr().','line_number':591,'multiline':False]
['text':' Legacy interface during the migration to indicate that a callsite','line_number':602,'multiline':False]
['text':' has not been audited for mutability.','line_number':603,'multiline':False]
['text':'','line_number':604,'multiline':False]
['text':' Do not add new uses of this, use const_data_ptr() if possible,','line_number':605,'multiline':False]
['text':' mutable_data_ptr() otherwise.','line_number':606,'multiline':False]
['text':'','line_number':607,'multiline':False]
['text':' TODO(#97856) Make this return a const pointer. This is currently','line_number':608,'multiline':False]
['text':'              const because of the vast number of clients that','line_number':609,'multiline':False]
['text':'              rely on this.','line_number':610,'multiline':False]
['text':' Purposely not defined here to avoid inlining','line_number':614,'multiline':False]
['text':' Return a `TensorAccessor` for CPU `Tensor`s. You have to specify scalar type and','line_number':617,'multiline':False]
['text':' dimension.','line_number':618,'multiline':False]
['text':' Return a `GenericPackedTensorAccessor` for CUDA `Tensor`s. You have to specify scalar type and','line_number':628,'multiline':False]
['text':' dimension. You can optionally specify RestrictPtrTraits as a template parameter to','line_number':629,'multiline':False]
['text':' cast the data pointer to a __restrict__ pointer.','line_number':630,'multiline':False]
['text':' In order to use this, your CUDA kernel has to take a corresponding GenericPackedTensorAccessor','line_number':631,'multiline':False]
['text':' as an argument.','line_number':632,'multiline':False]
['text':' ~~~~~ Autograd API ~~~~~','line_number':660,'multiline':False]
['text':'/ \fn bool is_leaf() const;','line_number':662,'multiline':False]
['text':'/','line_number':663,'multiline':False]
['text':'/ All Tensors that have `requires_grad()` which is ``false`` will be leaf Tensors by convention.','line_number':664,'multiline':False]
['text':'/','line_number':665,'multiline':False]
['text':'/ For Tensors that have `requires_grad()` which is ``true``, they will be leaf Tensors if they were','line_number':666,'multiline':False]
['text':'/ created by the user. This means that they are not the result of an operation and so','line_number':667,'multiline':False]
['text':'/ `grad_fn()` is `nullptr`.','line_number':668,'multiline':False]
['text':'/','line_number':669,'multiline':False]
['text':'/ Only leaf Tensors will have their `grad()` populated during a call to `backward()`.','line_number':670,'multiline':False]
['text':'/ To get `grad()` populated for non-leaf Tensors, you can use `retain_grad()`.','line_number':671,'multiline':False]
['text':'/','line_number':672,'multiline':False]
['text':'/ Example:','line_number':673,'multiline':False]
['text':'/ @code','line_number':674,'multiline':False]
['text':'/ auto a = torch::rand(10, torch::requires_grad());','line_number':675,'multiline':False]
['text':'/ std::cout << a.is_leaf() << std::endl; // prints `true`','line_number':676,'multiline':False]
['text':'/','line_number':677,'multiline':False]
['text':'/ auto b = torch::rand(10, torch::requires_grad()).to(torch::kCUDA);','line_number':678,'multiline':False]
['text':'/ std::cout << b.is_leaf() << std::endl; // prints `false`','line_number':679,'multiline':False]
['text':'/ // b was created by the operation that cast a cpu Tensor into a cuda Tensor','line_number':680,'multiline':False]
['text':'/','line_number':681,'multiline':False]
['text':'/ auto c = torch::rand(10, torch::requires_grad()) + 2;','line_number':682,'multiline':False]
['text':'/ std::cout << c.is_leaf() << std::endl; // prints `false`','line_number':683,'multiline':False]
['text':'/ // c was created by the addition operation','line_number':684,'multiline':False]
['text':'/','line_number':685,'multiline':False]
['text':'/ auto d = torch::rand(10).cuda();','line_number':686,'multiline':False]
['text':'/ std::cout << d.is_leaf() << std::endl; // prints `true`','line_number':687,'multiline':False]
['text':'/ // d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)','line_number':688,'multiline':False]
['text':'/','line_number':689,'multiline':False]
['text':'/ auto e = torch::rand(10).cuda().requires_grad_();','line_number':690,'multiline':False]
['text':'/ std::cout << e.is_leaf() << std::endl; // prints `true`','line_number':691,'multiline':False]
['text':'/ // e requires gradients and has no operations creating it','line_number':692,'multiline':False]
['text':'/','line_number':693,'multiline':False]
['text':'/ auto f = torch::rand(10, torch::device(torch::kCUDA).requires_grad(true));','line_number':694,'multiline':False]
['text':'/ std::cout << f.is_leaf() << std::endl; // prints `true`','line_number':695,'multiline':False]
['text':'/ // f requires grad, has no operation creating it','line_number':696,'multiline':False]
['text':'/ @endcode','line_number':697,'multiline':False]
['text':'/ \fn void backward(const Tensor & gradient={}, c10::optional<bool> retain_graph=c10::nullopt, bool create_graph=false, c10::optional<TensorList> inputs=c10::nullopt) const;','line_number':699,'multiline':False]
['text':'/','line_number':700,'multiline':False]
['text':'/ Computes the gradient of current tensor with respect to graph leaves.','line_number':701,'multiline':False]
['text':'/','line_number':702,'multiline':False]
['text':'/ The graph is differentiated using the chain rule. If the tensor is','line_number':703,'multiline':False]
['text':'/ non-scalar (i.e. its data has more than one element) and requires','line_number':704,'multiline':False]
['text':'/ gradient, the function additionally requires specifying ``gradient``.','line_number':705,'multiline':False]
['text':'/ It should be a tensor of matching type and location, that contains','line_number':706,'multiline':False]
['text':'/ the gradient of the differentiated function w.r.t. this Tensor.','line_number':707,'multiline':False]
['text':'/','line_number':708,'multiline':False]
['text':'/ This function accumulates gradients in the leaves - you might need to','line_number':709,'multiline':False]
['text':'/ zero them before calling it.','line_number':710,'multiline':False]
['text':'/','line_number':711,'multiline':False]
['text':'/ \param gradient Gradient w.r.t. the','line_number':712,'multiline':False]
['text':'/     tensor. If it is a tensor, it will be automatically converted','line_number':713,'multiline':False]
['text':'/     to a Tensor that does not require grad unless ``create_graph`` is True.','line_number':714,'multiline':False]
['text':'/     None values can be specified for scalar Tensors or ones that','line_number':715,'multiline':False]
['text':'/     don't require grad. If a None value would be acceptable then','line_number':716,'multiline':False]
['text':'/     this argument is optional.','line_number':717,'multiline':False]
['text':'/ \param retain_graph If ``false``, the graph used to compute','line_number':718,'multiline':False]
['text':'/     the grads will be freed. Note that in nearly all cases setting','line_number':719,'multiline':False]
['text':'/     this option to True is not needed and often can be worked around','line_number':720,'multiline':False]
['text':'/     in a much more efficient way. Defaults to the value of','line_number':721,'multiline':False]
['text':'/     ``create_graph``.','line_number':722,'multiline':False]
['text':'/ \param create_graph If ``true``, graph of the derivative will','line_number':723,'multiline':False]
['text':'/     be constructed, allowing to compute higher order derivative','line_number':724,'multiline':False]
['text':'/     products. Defaults to ``false``.','line_number':725,'multiline':False]
['text':'/ \param inputs Inputs w.r.t. which the gradient will be accumulated into','line_number':726,'multiline':False]
['text':'/     ``at::Tensor::grad``. All other Tensors will be ignored. If not','line_number':727,'multiline':False]
['text':'/     provided, the gradient is accumulated into all the leaf Tensors','line_number':728,'multiline':False]
['text':'/     that were used to compute the current tensor.','line_number':729,'multiline':False]
['text':'/     When inputs are provided and a given input is not a leaf,','line_number':730,'multiline':False]
['text':'/     the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).','line_number':731,'multiline':False]
['text':'/     It is an implementation detail on which the user should not rely.','line_number':732,'multiline':False]
['text':'/     See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.','line_number':733,'multiline':False]
['text':'/ \fn Tensor detach() const;','line_number':735,'multiline':False]
['text':'/','line_number':736,'multiline':False]
['text':'/ Returns a new Tensor, detached from the current graph.','line_number':737,'multiline':False]
['text':'/ The result will never require gradient.','line_number':738,'multiline':False]
['text':'/ \fn Tensor & detach_() const;','line_number':740,'multiline':False]
['text':'/','line_number':741,'multiline':False]
['text':'/ Detaches the Tensor from the graph that created it, making it a leaf.','line_number':742,'multiline':False]
['text':'/ Views cannot be detached in-place.','line_number':743,'multiline':False]
['text':'/ \fn void retain_grad() const;','line_number':745,'multiline':False]
['text':'/','line_number':746,'multiline':False]
['text':'/ Enables this Tensor to have their :attr:`grad` populated during','line_number':747,'multiline':False]
['text':'/ :func:`backward`. This is a no-op for leaf tensors.','line_number':748,'multiline':False]
['text':'/ \fn bool retains_grad() const;','line_number':750,'multiline':False]
['text':'/','line_number':751,'multiline':False]
['text':'/ Is ``true`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be','line_number':752,'multiline':False]
['text':'/ populated during :func:`backward`, ``false`` otherwise.','line_number':753,'multiline':False]
['text':' The Forward AD API functions below are low level and are not to be used by end','line_number':763,'multiline':False]
['text':' users who should use the API provided in torch/csrc/autograd.h','line_number':764,'multiline':False]
['text':'/ This function returns the forward gradient for this Tensor at the given level.','line_number':766,'multiline':False]
['text':'/ This function can be used to set the value of the forward grad.','line_number':771,'multiline':False]
['text':'/ Note that the given new_grad might not be used directly if it has different','line_number':772,'multiline':False]
['text':'/ metadata (size/stride/storage offset) compared to this Tensor. In that case,','line_number':773,'multiline':False]
['text':'/ new_grad content will be copied into a new Tensor','line_number':774,'multiline':False]
['text':'/ NOTE: This is similar to the legacy `.data()` function on `Variable`, and is intended','line_number':779,'multiline':False]
['text':'/ to be used from functions that need to access the `Variable`'s equivalent `Tensor`','line_number':780,'multiline':False]
['text':'/ (i.e. `Tensor` that shares the same storage and tensor metadata with the `Variable`).','line_number':781,'multiline':False]
['text':'/','line_number':782,'multiline':False]
['text':'/ One notable difference with the legacy `.data()` function is that changes to the','line_number':783,'multiline':False]
['text':'/ returned `Tensor`'s tensor metadata (e.g. sizes / strides / storage / storage_offset)','line_number':784,'multiline':False]
['text':'/ will not update the original `Variable`, due to the fact that this function','line_number':785,'multiline':False]
['text':'/ shallow-copies the `Variable`'s underlying TensorImpl.','line_number':786,'multiline':False]
['text':'/ NOTE: `var.variable_data()` in C++ has the same semantics as `tensor.data`','line_number':789,'multiline':False]
['text':'/ in Python, which create a new `Variable` that shares the same storage and','line_number':790,'multiline':False]
['text':'/ tensor metadata with the original `Variable`, but with a completely new','line_number':791,'multiline':False]
['text':'/ autograd history.','line_number':792,'multiline':False]
['text':'/','line_number':793,'multiline':False]
['text':'/ NOTE: If we change the tensor metadata (e.g. sizes / strides /','line_number':794,'multiline':False]
['text':'/ storage / storage_offset) of a variable created from `var.variable_data()`, those','line_number':795,'multiline':False]
['text':'/ changes will not update the original variable `var`. In `.variable_data()`, we set','line_number':796,'multiline':False]
['text':'/ `allow_tensor_metadata_change_` to false to make such changes explicitly illegal,','line_number':797,'multiline':False]
['text':'/ in order to prevent users from changing metadata of `var.variable_data()`','line_number':798,'multiline':False]
['text':'/ and expecting the original variable `var` to also be updated.','line_number':799,'multiline':False]
['text':' Gradient Node and Edges','line_number':802,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':803,'multiline':False]
['text':'/ Gets the gradient function of the `Variable`. If this is a leaf variable,','line_number':805,'multiline':False]
['text':'/ the pointer returned will be null.','line_number':806,'multiline':False]
['text':'/','line_number':807,'multiline':False]
['text':'/ For View Variables:','line_number':808,'multiline':False]
['text':'/ Gets the up-to-date grad_fn. If the shared data or base was modified, we','line_number':809,'multiline':False]
['text':'/ re-create the grad_fn to express the up-to-date view relationship between','line_number':810,'multiline':False]
['text':'/ this and the base Variable.','line_number':811,'multiline':False]
['text':' Hooks','line_number':814,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':815,'multiline':False]
['text':'/ Registers a backward hook.','line_number':822,'multiline':False]
['text':'/','line_number':823,'multiline':False]
['text':'/ The hook will be called every time a gradient with respect to the Tensor is computed.','line_number':824,'multiline':False]
['text':'/ The hook should have one of the following signature:','line_number':825,'multiline':False]
['text':'/ ```','line_number':826,'multiline':False]
['text':'/ hook(TensorBase grad) -> TensorBase','line_number':827,'multiline':False]
['text':'/ ```','line_number':828,'multiline':False]
['text':'/ ```','line_number':829,'multiline':False]
['text':'/ hook(TensorBase grad) -> void','line_number':830,'multiline':False]
['text':'/ ```','line_number':831,'multiline':False]
['text':'/ The hook should not modify its argument, but it can optionally return a new gradient','line_number':832,'multiline':False]
['text':'/ which will be used in place of `grad`.','line_number':833,'multiline':False]
['text':'/','line_number':834,'multiline':False]
['text':'/ This function returns the index of the hook in the list which can be used to remove hook.','line_number':835,'multiline':False]
['text':'/','line_number':836,'multiline':False]
['text':'/ Example:','line_number':837,'multiline':False]
['text':'/ @code','line_number':838,'multiline':False]
['text':'/ auto v = torch::tensor({0., 0., 0.}, torch::requires_grad());','line_number':839,'multiline':False]
['text':'/ auto h = v.register_hook([](torch::Tensor grad){ return grad * 2; }); // double the gradient','line_number':840,'multiline':False]
['text':'/ v.backward(torch::tensor({1., 2., 3.}));','line_number':841,'multiline':False]
['text':'/ // This prints:','line_number':842,'multiline':False]
['text':'/ // ```','line_number':843,'multiline':False]
['text':'/ //  2','line_number':844,'multiline':False]
['text':'/ //  4','line_number':845,'multiline':False]
['text':'/ //  6','line_number':846,'multiline':False]
['text':'/ // [ CPUFloatType{3} ]','line_number':847,'multiline':False]
['text':'/ // ```','line_number':848,'multiline':False]
['text':'/ std::cout << v.grad() << std::endl;','line_number':849,'multiline':False]
['text':'/ v.remove_hook(h);  // removes the hook','line_number':850,'multiline':False]
['text':'/ @endcode','line_number':851,'multiline':False]
['text':'/ Remove hook at given position','line_number':862,'multiline':False]
['text':' Variable methods','line_number':865,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':866,'multiline':False]
['text':' View Variables','line_number':884,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':885,'multiline':False]
['text':'/ Returns true if this `Variable` is a view of another `Variable`.','line_number':887,'multiline':False]
['text':'/ Returns the `Variable` that this `Variable` is a view of. If this','line_number':890,'multiline':False]
['text':'/ `Variable` is not a view, throw a `std::runtime_error`.','line_number':891,'multiline':False]
['text':' Miscellaneous','line_number':894,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':895,'multiline':False]
['text':' Return the grad argument in case of a hook with void return type to have an','line_number':913,'multiline':False]
['text':' std::function with Tensor return type','line_number':914,'multiline':False]
['text':' Helper creator for Tensor class which doesn't requires the users to pass','line_number':929,'multiline':False]
['text':' in an intrusive_ptr instead it just converts the argument passed to','line_number':930,'multiline':False]
['text':' requested intrusive_ptr type.','line_number':931,'multiline':False]
['text':' namespace detail','line_number':937,'multiline':False]
['text':' namespace at','line_number':943,'multiline':False]
['text':' NOTE: this can be implemented without the special','line_number':952,'multiline':False]
['text':' unsafe_borrow_t Tensor constructor as','line_number':953,'multiline':False]
['text':'','line_number':954,'multiline':False]
['text':' return borrow_type(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::reclaim(from.unsafeGetTensorImpl()));','line_number':955,'multiline':False]
['text':'','line_number':956,'multiline':False]
['text':' but that hurts inlining due to the nullptr check in the','line_number':957,'multiline':False]
['text':' Tensor(c10::intrusive_ptr<...>) constructor. We already know','line_number':958,'multiline':False]
['text':' that from.impl_ isn't null because from is a valid Tensor, so','line_number':959,'multiline':False]
['text':' we needn't do the check again. (using __builtin_assume can','line_number':960,'multiline':False]
['text':' avoid this, but wouldn't be portable to MSVC.)','line_number':961,'multiline':False]
['text':' See above note: this can be implemented with public API','line_number':967,'multiline':False]
['text':' similarly to createBorrow(), but that would hurt inlining.','line_number':968,'multiline':False]
['text':' "leak" it, but it was already +0.','line_number':973,'multiline':False]
['text':'borrow','line_number':984,'multiline':True]
['text':' namespace c10','line_number':991,'multiline':False]
['text':' namespace symint','line_number':1037,'multiline':False]
['text':' namespace at','line_number':1039,'multiline':False]
