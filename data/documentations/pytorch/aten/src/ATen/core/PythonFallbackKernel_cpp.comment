['text':' This TLS is used to track the state of the dispatcher to be able to restore','line_number':8,'multiline':False]
['text':' it when calling back into python.','line_number':9,'multiline':False]
['text':' It has the following invariant:','line_number':10,'multiline':False]
['text':'  - It must be empty while python code is executed.','line_number':11,'multiline':False]
['text':'  - It should only be set once even for multiple dispatcher calls that do not come','line_number':12,'multiline':False]
['text':'    back to python.','line_number':13,'multiline':False]
['text':' To achieve this, we ensure that the tls is empty by default and emptied again both when','line_number':14,'multiline':False]
['text':' we call into user torch_dispatch or returning back to python after this call.','line_number':15,'multiline':False]
['text':' All the keys below the Python key','line_number':25,'multiline':False]
['text':' This guard assumes that tls_on_entry has a value.','line_number':31,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-unchecked-optional-access)','line_number':34,'multiline':False]
['text':' c10::impl::ForceDispatchKeyGuard dispatcher_guard(tls_on_entry.value());','line_number':50,'multiline':False]
['text':' StashTLSOnEntryGuard stash_guard;','line_number':51,'multiline':False]
['text':' If Torch Dispatch Mode is active, use its PyInterpreter for dispatch','line_number':55,'multiline':False]
['text':' Otherwise, find a PyInterpreter on a Tensor','line_number':63,'multiline':False]
['text':' It is safe to dispatch on the very first Tensor with a pyobj_interpreter','line_number':66,'multiline':False]
['text':' without checking the interpreters of any of the arguments, because when','line_number':67,'multiline':False]
['text':' we actually run dispatch(), we will take out PyObjects in the context','line_number':68,'multiline':False]
['text':' of that interpreter, and this will ensure that everyone is on the same','line_number':69,'multiline':False]
['text':' interpreter.','line_number':70,'multiline':False]
['text':' NB: use toListRef as it doesn't induce refcount bumps (toTensorListRef','line_number':79,'multiline':False]
['text':' is not a thing)','line_number':80,'multiline':False]
['text':' It is ok for the tls to be already set here.','line_number':103,'multiline':False]
['text':' It means that there are multiple calls into the dispatcher not originating from python code.','line_number':104,'multiline':False]
['text':' The guard below will properly ignore such calls.','line_number':105,'multiline':False]
['text':' The PreDispatch key gets a no-op fallback that just redispatches.','line_number':111,'multiline':False]
['text':' The main way this key is used is that we can register a mode to it from python (e.g. TorchProxyDispatchMode, for pre_dispatch tracing)','line_number':112,'multiline':False]
['text':' Can't this be a fallthrough kernel, instead of a fallback that just no-ops and redispatches?','line_number':113,'multiline':False]
['text':' Unfortunately, no: we need a real kernel that is not a fallthrough, in order for the PythonDispatcher to interpose on it.','line_number':114,'multiline':False]
['text':' Alternatively, we could have hardcoded this kernel (in C++) to directly call in TorchProxyDispatchMode.','line_number':115,'multiline':False]
['text':' Doing that in C++ is a pain though, so it's done in python using the PythonDispatcher for convenience.','line_number':116,'multiline':False]
['text':' anonymous namespace','line_number':121,'multiline':False]
['text':' namespace impl','line_number':151,'multiline':False]
['text':' namespace at','line_number':152,'multiline':False]
