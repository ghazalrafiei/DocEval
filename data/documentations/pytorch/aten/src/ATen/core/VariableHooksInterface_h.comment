['text':' A little explanation about why this file exists at all.  We have','line_number':6,'multiline':False]
['text':' a few methods on Tensor class which require access to reified access to','line_number':7,'multiline':False]
['text':' AutogradMeta.  In open source, this isn't a big deal: we just access','line_number':8,'multiline':False]
['text':' torch/csrc/autograd/variable.h from aten/src/ATen/core/Tensor.cpp and','line_number':9,'multiline':False]
['text':' we can put the definitions inline.  This is because everything gets balled','line_number':10,'multiline':False]
['text':' into a single dynamic library in the end.','line_number':11,'multiline':False]
['text':'','line_number':12,'multiline':False]
['text':' However, inside our Facebook internal version of our build system, we','line_number':13,'multiline':False]
['text':' have a split between aten and torch/csrc.  So we cannot simply just','line_number':14,'multiline':False]
['text':' cross this boundary.  "Now wait," you might say, "Why don't we just','line_number':15,'multiline':False]
['text':' merge the libraries inside Facebook".  Well, the problem is that there','line_number':16,'multiline':False]
['text':' are some downstream applications which are at binary size limit, and','line_number':17,'multiline':False]
['text':' incorporating all of the extra code from libtorch would push them','line_number':18,'multiline':False]
['text':' over (admarket/adreview/service:adreviewservice, see also','line_number':19,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/29299)  So if you want to do that,','line_number':20,'multiline':False]
['text':' we have to fix all of the services like this.','line_number':21,'multiline':False]
['text':'','line_number':22,'multiline':False]
['text':' I didn't want to block eliminating Tensor-Variable on this work, so I','line_number':23,'multiline':False]
['text':' had to introduce another dynamic dispatch to get to the variable','line_number':24,'multiline':False]
['text':' implementations (which live in torch/csrc/autograd/variable.cpp, FYI).','line_number':25,'multiline':False]
['text':'','line_number':26,'multiline':False]
['text':' I also considered using our existing dynamic dispatch mechanism, c10','line_number':27,'multiline':False]
['text':' dispatcher, to do this.  However, (1) some of the functions on Tensor','line_number':28,'multiline':False]
['text':' have weird signatures that are not supported by autograd, and (2)','line_number':29,'multiline':False]
['text':' see this bug https://github.com/pytorch/pytorch/issues/30102','line_number':30,'multiline':False]
['text':' namespace torch::autograd','line_number':36,'multiline':False]
['text':' namespace at::impl','line_number':75,'multiline':False]
