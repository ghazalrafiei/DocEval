['text':' The legacy mechanism for dispatching operators in ATen is a Type','line_number':3,'multiline':False]
['text':' object, which is essentially a giant virtual dispatch table','line_number':4,'multiline':False]
['text':' for every operation we support dynamically dispatching over.','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':' This has been deprecated in favor of ATenDispatch, and in the future,','line_number':7,'multiline':False]
['text':' c10 dispatcher.','line_number':8,'multiline':False]
['text':' TODO: Clean up what remains here','line_number':9,'multiline':False]
['text':' A RAII, thread local (!) guard that will disable dispatch to variable','line_number':15,'multiline':False]
['text':' handler.','line_number':16,'multiline':False]
['text':'','line_number':17,'multiline':False]
['text':' NOTE [ Treating Variables as non-Variables in type dispatch ]','line_number':18,'multiline':False]
['text':'','line_number':19,'multiline':False]
['text':' What exactly does AutoDispatchBelowAutograd do?  The short answer is, it causes','line_number':20,'multiline':False]
['text':' dispatches on ATen functions to go to the non-variable implementation,','line_number':21,'multiline':False]
['text':' bypassing autograd handling (and also profiling and tracing).','line_number':22,'multiline':False]
['text':'','line_number':23,'multiline':False]
['text':' To understand why this guard exists, it's helpful to understand the history','line_number':24,'multiline':False]
['text':' behind how Variable was implemented.  Previously, Variables were implemented','line_number':25,'multiline':False]
['text':' as a wrapper on Tensors; so the act of processing a Variable involved','line_number':26,'multiline':False]
['text':' unwrapping the underlying Tensor, and then calling the underlying base','line_number':27,'multiline':False]
['text':' operation on /that/ operation','line_number':28,'multiline':False]
['text':'','line_number':29,'multiline':False]
['text':' However, after the Variable/Tensor merge, there is no concept of unwrapping','line_number':30,'multiline':False]
['text':' a tensor anymore.  If you just call the operation on the same variable','line_number':31,'multiline':False]
['text':' again inside your VariableType handler, you'll dispatch back to','line_number':32,'multiline':False]
['text':' VariableType, which is not what we want.','line_number':33,'multiline':False]
['text':'','line_number':34,'multiline':False]
['text':' The solution to the above problem is to add `at::AutoDispatchBelowAutograd`, which','line_number':35,'multiline':False]
['text':' when enabled will cause `legacyTensorType()` and `getType()` to always return','line_number':36,'multiline':False]
['text':' non-Variable type, even if the tensor being called on is a variable.','line_number':37,'multiline':False]
['text':' Note [AutoDispatchBelowAutograd]
 * AutoDispatchBelowAutograd is **INTERNAL ONLY** that it should be used
 * for kernel implementations and customized C++ kernels.
 * If you are looking for a guard to run workload in inference mode, please use
 * c10::InferenceMode RAII which is user facing API.
 * In the past AutoDispatchBelowAutograd(or its old version AutoNonVariableTypeMode)
 * was used in the user code for inference-only workload, this was under risk of
 * producing wrong results silently in some edge cases. For example:
 * ```
 *  torch::Tensor s = torch::ones({1, 2, 3}).set_requires_grad(true);
 *  torch::Tensor out = s * s;
 *  {
 *    at::AutoDispatchBelowAutograd guard;
 *    s.add_(1);  // Skips version bump on `s`.
 *  }
 *  // WRONG GRADIENT! s.grad() are now computed using `s` value after the
 *  // inplace update.
 *  out.backward(torch::ones_like(out));
 * ```
 * Users should use `c10::InferenceMode` here so that it'll properly throw an
 * error saying "one of the variables needed for gradient computation has be modified."
 ','line_number':39,'multiline':True]
['text':' disable all autograd dispatch keys','line_number':66,'multiline':False]
['text':' TODO: AutoNonVariableTypeMode should be removed in release 1.10.','line_number':70,'multiline':False]
['text':' disable all autograd dispatch keys','line_number':83,'multiline':False]
['text':' Note [AutoDispatchBelowADInplaceOrView]
 * AutoDispatchBelowADInplaceOrView is equivalent to AutoNonVariableTypeMode
 * before we split inplace & view ops out of VariableType kernel.
 * Note this guard is used in VariableType kernels for functional ops
 * as well as ADInplaceOrView kernels for inplace/view ops to enforce the
 * Invariant:
 *   Once you are in VariableType/ADInplaceOrView kernel for an op,
 *   you never go back to a kernel on same dispatch key until
 *   you finish the current op.
 ','line_number':94,'multiline':True]
['text':' disable Autograd & ADInplaceOrView dispatch keys','line_number':108,'multiline':False]
['text':' namespace at','line_number':111,'multiline':False]
