['text':' Note: "level" is a concept that we don't know how to compute in core.','line_number':24,'multiline':False]
['text':' For now I'm retroactively setting this in functorch,','line_number':25,'multiline':False]
['text':' but once Open Multiple Dispatch lands we should be able to calculate this in core.','line_number':26,'multiline':False]
['text':' mirror all of the generic tensor metadata onto the wrapper','line_number':28,'multiline':False]
['text':' In general, the sizes/stride metadata on a tensor can change as it is mutated,','line_number':33,'multiline':False]
['text':' and these changes need to be reflected in the metadata of the wrapper.','line_number':34,'multiline':False]
['text':' All of the keys corresponding to functorch transforms should not be copied over.','line_number':37,'multiline':False]
['text':' Functorch transforms all have their own wrapper tensors (e.g. BatchedTensorImpl) which expect','line_number':38,'multiline':False]
['text':' to participate in the functorch transforms.','line_number':39,'multiline':False]
['text':' We override a bunch of _custom(), so make sure they get called','line_number':41,'multiline':False]
['text':' TODO: metadata copying may not actually be necessary then','line_number':42,'multiline':False]
['text':' E.g. when running torch.compile under inference mode, we need to make sure that','line_number':45,'multiline':False]
['text':' for any inputs that were created outside of inference mode (so they are not inference tensors),','line_number':46,'multiline':False]
['text':' then the functional wrappers that we wrap them with should also not be inference tensors.','line_number':47,'multiline':False]
['text':' Note [Functionalization: Alias Removal]','line_number':68,'multiline':False]
['text':' When someone calls a view() op during the functionalization pass, e.g. 'b = a.view(...)',','line_number':69,'multiline':False]
['text':' we link `b` and `a` to a shared Alias object to preserve the aliasing relationship.','line_number':70,'multiline':False]
['text':'','line_number':71,'multiline':False]
['text':' How do we do that?','line_number':72,'multiline':False]
['text':'','line_number':73,'multiline':False]
['text':' Every FunctionalTensorWrapper contains a dummy FunctionalStorageImpl, which subclasses from c10::StorageImpl.','line_number':74,'multiline':False]
['text':' It doesn't contain any data (similar to MetaTensor storage), but it contains an Alias object that knows about the base tensor.','line_number':75,'multiline':False]
['text':' When a tensor is created through a view operation, both the new and old tensor point to the same FunctionalStorageImpl.','line_number':76,'multiline':False]
['text':'','line_number':77,'multiline':False]
['text':' As mutations are applied to any of the views, we also queue each mutation up on the Alias object, so we can replay them.','line_number':78,'multiline':False]
['text':' When the user requests a tensor that's had a view taken, we check if it's up to date.','line_number':79,'multiline':False]
['text':' If it's not up to date, we first replay all of the queued up mutations onto the alias, and then re-apply the current view','line_number':80,'multiline':False]
['text':' on top of the newly updated alias.','line_number':81,'multiline':False]
['text':'','line_number':82,'multiline':False]
['text':' Why do we queue up and lazily run mutations on the alias, instead of updating the alias eagerly?','line_number':83,'multiline':False]
['text':' This behavior was taken from pytorch/xla, which the alias-removal logic was inspired from.','line_number':84,'multiline':False]
['text':' One benefit of the laziness is that we save work in the cases where a user has multiple views and mutates one of them,','line_number':85,'multiline':False]
['text':' but never uses the other views later in the program (in which case we'll never update the alias).','line_number':86,'multiline':False]
['text':' It also has downsides though: repeatedly applying mutations to the same view without syncing','line_number':87,'multiline':False]
['text':' will silently use up more and more memory as more mutations are queued up.','line_number':88,'multiline':False]
['text':'','line_number':89,'multiline':False]
['text':' Corresponding diagram:','line_number':90,'multiline':False]
['text':'','line_number':91,'multiline':False]
['text':' b = a.view(...)','line_number':92,'multiline':False]
['text':'','line_number':93,'multiline':False]
['text':'        a                                                    b','line_number':94,'multiline':False]
['text':'        |                                                    |     If the user asks for b and it’s out of date,','line_number':95,'multiline':False]
['text':'       \/                                                    \/    We regenerate b by replaying it’s views from the alias.','line_number':96,'multiline':False]
['text':' . - - - - - - - - - - - - - .                    . - - - - - - - - - - - - - .','line_number':97,'multiline':False]
['text':' |  FunctionalTensorWrapper  |                    |  FunctionalTensorWrapper  |','line_number':98,'multiline':False]
['text':' . - - - - - - - - - - - - - .                    . - - - - - - - - - - - - - .','line_number':99,'multiline':False]
['text':' |     value   |   storage   |                    |    storage    |   Value   |','line_number':100,'multiline':False]
['text':' . - - - - - - - - - - - - - .                    . - - - - - - - - - - - - - .','line_number':101,'multiline':False]
['text':'          |                   \                  /                      |','line_number':102,'multiline':False]
['text':'          |                     \              /                        |','line_number':103,'multiline':False]
['text':'          |                       . - - - - - - - - - - - - .           |','line_number':104,'multiline':False]
['text':'          |                       |  FunctionalStorageImpl  |           |','line_number':105,'multiline':False]
['text':'          |                       . - - - - - - - - - - - - .           |','line_number':106,'multiline':False]
['text':'          |                       |         Alias           |           |','line_number':107,'multiline':False]
['text':'          |                       . - - - - - - - - - - - - .           |','line_number':108,'multiline':False]
['text':'          |                       /     mutations to a or b             |','line_number':109,'multiline':False]
['text':'          |                     /       are queued onto Alias           |','line_number':110,'multiline':False]
['text':'          |                   /                                         |','line_number':111,'multiline':False]
['text':'         \/                 /                                           \/','line_number':112,'multiline':False]
['text':' . - - - - - - - - - - - - - .                             . - - - - - - - - - - - - - - - .','line_number':113,'multiline':False]
['text':' |        TensorImpl         |                             |             TensorImpl        |','line_number':114,'multiline':False]
['text':' . - - - - - - - - - - - - - .                             . - - - - - - - - - - - - - - - .','line_number':115,'multiline':False]
['text':' |   value   |   storage     |                             |    storage    |     Value     |','line_number':116,'multiline':False]
['text':' . - - - - - - - - - - - - - .                             . - - - - - - - - - - - - - - - .','line_number':117,'multiline':False]
['text':'          |                                                             |','line_number':118,'multiline':False]
['text':'          |                                                             |','line_number':119,'multiline':False]
['text':'          |                                                             |','line_number':120,'multiline':False]
['text':'          |   In this picture the two tensor views their own storages,  |','line_number':121,'multiline':False]
['text':'          |   have their own storages, but backends like functorch      |','line_number':122,'multiline':False]
['text':'         \/   are allowed to re-alias underneath the pass               \/','line_number':123,'multiline':False]
['text':' . - - - - - - - - - - - - - .                             . - - - - - - - - - - - - - - - .','line_number':124,'multiline':False]
['text':' |    underyling_storage     |                             |      underyling_storage       |','line_number':125,'multiline':False]
['text':' . - - - - - - - - - - - - - .                             . - - - - - - - - - - - - - - - .','line_number':126,'multiline':False]
['text':'','line_number':127,'multiline':False]
['text':' This constructor is only used by view ops.','line_number':128,'multiline':False]
['text':' - view_value: The output tensor that we need to wrap.','line_number':129,'multiline':False]
['text':' - base: The "base" of the view that `view_value` was generated from.','line_number':130,'multiline':False]
['text':' See Note [Functionalization: Alias Removal Part 2] for more details on the mutation replay logic.','line_number':131,'multiline':False]
['text':' Copy the original tensor's ViewMeta vector and push the current one.','line_number':145,'multiline':False]
['text':' copy','line_number':147,'multiline':False]
['text':' alias this tensor's storage with the base tensor's','line_number':150,'multiline':False]
['text':' As an optimization, we used to mark the tensor here as "up-to-date",','line_number':161,'multiline':False]
['text':' That way, code like:','line_number':162,'multiline':False]
['text':'   x = torch.ones(1'000'000)','line_number':163,'multiline':False]
['text':'   x[0].add_(1)','line_number':164,'multiline':False]
['text':' doesn't result in an unnecessary materialization of the base.','line_number':165,'multiline':False]
['text':' This optimization results in the slice temporarily haven't incorrect','line_number':166,'multiline':False]
['text':' stride/storage_offset though, and DCE should handle that optimization anyway.','line_number':167,'multiline':False]
['text':' generation_ = storage_impl->generation();','line_number':168,'multiline':False]
['text':' See Note [Functionalization Pass - Inplace View Ops]','line_number':176,'multiline':False]
['text':' Manually track the fact that this tensor recieved a metadata mutation!','line_number':179,'multiline':False]
['text':' Note [Functionalization Pass - Inplace View Ops]','line_number':181,'multiline':False]
['text':' So, these ops are special - they're mutation AND view ops. They get special codegen.','line_number':182,'multiline':False]
['text':' An example is transpose_, e.g. `a.transpose_()`','line_number':183,'multiline':False]
['text':' Calling transpose_() should ensure that a gets an alias, and append the new ViewMeta to a's current list of ViewMetas.','line_number':184,'multiline':False]
['text':' Note [Functionalization: Mutation Removal]','line_number':190,'multiline':False]
['text':' Mutation removal is used to take a program like this:','line_number':191,'multiline':False]
['text':'','line_number':192,'multiline':False]
['text':' a.add_(b)','line_number':193,'multiline':False]
['text':'','line_number':194,'multiline':False]
['text':' and replace it with a slightly different program that has the same semantics:','line_number':195,'multiline':False]
['text':'','line_number':196,'multiline':False]
['text':' tmp = a.add(b)','line_number':197,'multiline':False]
['text':' a.replace_(tmp)','line_number':198,'multiline':False]
['text':'','line_number':199,'multiline':False]
['text':' Where the replace_() call is implemented directly in the functionalization pass, so it is transparent to the backend.','line_number':200,'multiline':False]
['text':' This is useful for backends that aren't able to handle certain types of mutations, like functorch.','line_number':201,'multiline':False]
['text':'','line_number':202,'multiline':False]
['text':' Why do we need to wrap every tensor in a FunctionalTensorWrapper? Consider this program:','line_number':203,'multiline':False]
['text':'','line_number':204,'multiline':False]
['text':' Before:','line_number':205,'multiline':False]
['text':' tensor.add_(batched_tensor)','line_number':206,'multiline':False]
['text':'','line_number':207,'multiline':False]
['text':' After:','line_number':208,'multiline':False]
['text':' tmp = tensor.add(batched_tensor)','line_number':209,'multiline':False]
['text':' tensor.replace_(tmp)','line_number':210,'multiline':False]
['text':'','line_number':211,'multiline':False]
['text':' In the above, tmp is a batched tensor (because adding a normal tensor to a batched tensor does broadcasting and creates a batched tensor).','line_number':212,'multiline':False]
['text':' But we can't just replace the underlying memory backing `tensor` with `tmp` - a batched tensor takes up more space!','line_number':213,'multiline':False]
['text':' Instead, every input, intermediate and output of the program is wrapped in a FunctionalTensorImpl, which wraps the underlying tensor.','line_number':214,'multiline':False]
['text':' TODO: going to need to change this if we want nested functionalize() transforms.','line_number':216,'multiline':False]
['text':' out= ops are allowed to resize the output tensors, mutating both the data and metadata of the tensor.','line_number':220,'multiline':False]
['text':' We need to propagate that metadata mutation to the wrapper (new size).','line_number':221,'multiline':False]
['text':' .to() should not re-entrantly go through functionalization.','line_number':227,'multiline':False]
['text':' and we want _to_copy() to show up in the graph, not the composite .to() operator','line_number':229,'multiline':False]
['text':' (this can happen if autograd has already run by the time we enter this code)','line_number':230,'multiline':False]
['text':' This mutation happened under no_grad or inference_mode','line_number':236,'multiline':False]
['text':' Current tensor's data was mutated if its storage saw any mutations.','line_number':242,'multiline':False]
['text':' self.set_(src) will cause self to have all of the tensor properties of self.','line_number':247,'multiline':False]
['text':' FREEZE the old storage, preventing mutations to it.','line_number':251,'multiline':False]
['text':' this is a huge pain to handle properly in all cases, so we ban it.','line_number':252,'multiline':False]
['text':' Unsafely swap out the storage with other's storage,','line_number':254,'multiline':False]
['text':' disconnecting `self` with its view chain','line_number':255,'multiline':False]
['text':'/ explicitly mark the tensor as having its storage changed from set_()','line_number':257,'multiline':False]
['text':' Otherwise, we don't actually have a 100% accurate way to check this.','line_number':258,'multiline':False]
['text':' (We could check if the updated value has a new storage than the original value,','line_number':259,'multiline':False]
['text':' but this won't also let us uniquely determine if the tensor **also**','line_number':260,'multiline':False]
['text':' experienced a data mutation).','line_number':261,'multiline':False]
['text':' Note [resize_() in functionalization pass]','line_number':271,'multiline':False]
['text':' resize_() is a special operator in functionalization because it can reallocate its underlying storage.','line_number':272,'multiline':False]
['text':' This function is only ever called in the case that resize_() needs to reallocate its storage to a larger size.','line_number':273,'multiline':False]
['text':'','line_number':274,'multiline':False]
['text':' However, functionalization currently bans the following code:','line_number':275,'multiline':False]
['text':'   a = torch.ones(2)','line_number':276,'multiline':False]
['text':'   b = a.view(2)','line_number':277,'multiline':False]
['text':'   b.resize_(4) # b is a view tensor, that we are trying to increase the storage size of','line_number':278,'multiline':False]
['text':'','line_number':279,'multiline':False]
['text':' Why is this code difficult to handle?','line_number':280,'multiline':False]
['text':' The functionalization pass currently keeps aliases in sync by making the following assumptions:','line_number':281,'multiline':False]
['text':' - The “base” tensor always refers to “all of the data”','line_number':282,'multiline':False]
['text':' - Whenever you have b = view_op(a), “b” should always refer to a subset of “a”s memory.','line_number':283,'multiline':False]
['text':'','line_number':284,'multiline':False]
['text':' The code above breaks that assumption b.resize_(4) actually needs to update "a"','line_number':285,'multiline':False]
['text':' to tell it that it is now actually some slice of a pre-existing larger storage.','line_number':286,'multiline':False]
['text':' We're also no longer re-generate "b" fully from "a" anymore, since "a" refers to a slice of "b"'s data.','line_number':287,'multiline':False]
['text':'','line_number':288,'multiline':False]
['text':' This is probably fixable in theory, but:','line_number':289,'multiline':False]
['text':' - the fix would likey complicated the functionalization logic quite a bit.','line_number':290,'multiline':False]
['text':' - the primary use case for resize_() today is resizing zero-sized tensors in out= variants of operators','line_number':291,'multiline':False]
['text':' - resize_() also can give you weird results today if you try to resize_() a weirdly strided tensor.','line_number':292,'multiline':False]
['text':'','line_number':293,'multiline':False]
['text':' Given all of the above, for now we're just banning the above usage.','line_number':294,'multiline':False]
['text':' If this tensor is not a view (and has no outstanding views taken out on it),','line_number':297,'multiline':False]
['text':' Then it's safe to throw out the old storage and replace it with the new, larger one.','line_number':298,'multiline':False]
['text':' And update the metadata on the wrapper to reflect the new sizes and strides','line_number':303,'multiline':False]
['text':' (Technically we should be guaranteed that the tensor was already contiguous,','line_number':306,'multiline':False]
['text':' since it's guaranteed not to have been a view. Doesnt hurt to run though)','line_number':307,'multiline':False]
['text':' Swapping out the storage of a tensor (aka from a resize_() call) will update the sizes and strides of the tensor,','line_number':309,'multiline':False]
['text':' so we need to record the fact that metadata was mutated.','line_number':310,'multiline':False]
['text':' Reset the storage with the current value_ tensor as the base','line_number':315,'multiline':False]
['text':' Reset the generation so that it matches the new storage','line_number':317,'multiline':False]
['text':' Clear any pre-existing view metas so that base and value_ are semantically the same','line_number':319,'multiline':False]
['text':' Reapply views to get the viewed tensor from the base in alias_','line_number':336,'multiline':False]
['text':' Apply all updates on alias_','line_number':346,'multiline':False]
['text':'src_impl=','line_number':371,'multiline':True]
['text':'dest_impl=','line_number':372,'multiline':True]
['text':'version_counter=','line_number':373,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':374,'multiline':True]
['text':' Note [Wrapped Numbers <> Functionalization]','line_number':432,'multiline':False]
['text':' Note [Wrapped Numbers <> Functionalization]','line_number':463,'multiline':False]
['text':' If the current tensor is not functional, then raise an error','line_number':471,'multiline':False]
['text':' if assert_functional is true. Otherwise, return the input.','line_number':472,'multiline':False]
['text':' from_functional_tensor(Tensor) has asserts to make sure you don't accidentally call','line_number':487,'multiline':False]
['text':' it on a non-functional input,','line_number':488,'multiline':False]
['text':' but from_functional_tensor(TensorList) can recieve a list containing both','line_number':489,'multiline':False]
['text':' functional and non-functional tensors.','line_number':490,'multiline':False]
['text':' Example of when that can happen: torch.cat(function_input_tensor, global_state_tensor).','line_number':491,'multiline':False]
['text':' When that happens, we're okay with only unwrapping the functional tensors.','line_number':492,'multiline':False]
['text':'assert_functional=','line_number':493,'multiline':True]
['text':'assert_functional=','line_number':501,'multiline':True]
['text':' Note [Wrapped Numbers <> Functionalization]','line_number':508,'multiline':False]
['text':' Unfortunately, we can't easily guarantee that wrapped numbers (scalar-tensors)','line_number':509,'multiline':False]
['text':' get wrapped up in a FunctionalTensorWrapper object, since they skip the dispatcher.','line_number':510,'multiline':False]
['text':' That shouldn't matter, since I don't think we're allowed to assign to wrapped numbers anyway.','line_number':511,'multiline':False]
['text':' Not every tensor that hits a functionalization kernel is necessarily a functional tensor.','line_number':514,'multiline':False]
['text':' For example, xla_tensor.copy_(cpu_tensor) needs to hit the functionalization kernel','line_number':515,'multiline':False]
['text':' to sync xla_tensor, but not cpu_tensor.','line_number':516,'multiline':False]
['text':' Note [out_idx in ViewMeta]','line_number':653,'multiline':False]
['text':' When a view op outputs multiple tensors, each output needs its own separate ViewMeta.','line_number':654,'multiline':False]
['text':' Each ViewMeta also tracks the index of the particular output tensor, which is needed in the reverse function.','line_number':655,'multiline':False]
['text':' Note [Propagating strides in the functionalization pass]','line_number':677,'multiline':False]
['text':' In order to properly compute stride information, the functionalization pass','line_number':678,'multiline':False]
['text':' calls each {view} reference implementations with meta tensors.','line_number':679,'multiline':False]
['text':' The output meta tensor's stride info serves as a reference for what the correct strides should be.','line_number':680,'multiline':False]
['text':' namespace impl','line_number':701,'multiline':False]
['text':' Given an **out-of-place** op that might internally call view/inplace ops,','line_number':704,'multiline':False]
['text':' This function will "functionalize" it.','line_number':705,'multiline':False]
['text':' That is, it will call the operator, but removing any intermediate views/mutations','line_number':706,'multiline':False]
['text':' that are performed inside of it.','line_number':707,'multiline':False]
['text':' This is useful for LTC/XLA, which would like to re-use some of our composite kernels','line_number':708,'multiline':False]
['text':' from pytorch core but not have to worry about the view ops that they might call.','line_number':709,'multiline':False]
['text':' e.g. at::block_diag','line_number':710,'multiline':False]
['text':' Wrap all tensor-like inputs into FunctionalTensorWrappers.','line_number':717,'multiline':False]
['text':' When we re-invoke the dispatcher, this will automatically enable the functionalization pass.','line_number':718,'multiline':False]
['text':' Today when you call at::empty(device=lazy), the lazy backend decides whether or not to wrap','line_number':745,'multiline':False]
['text':' the output in a functional tensor based on TLS.','line_number':746,'multiline':False]
['text':' In this code, we're re-entrantly entering functionalization in the same call-stack,','line_number':747,'multiline':False]
['text':' so we need to manually fix up TLS as if it hadn't already been called.','line_number':748,'multiline':False]
['text':' So, we should probably provide a way to directly call a kernel registered to','line_number':754,'multiline':False]
['text':' the `CompositeExplicitAutograd` key.','line_number':755,'multiline':False]
['text':' We can't do that today, so this should be a reasonably good proxy','line_number':756,'multiline':False]
['text':' (It won't work in cases where an op has both a CompositeExplicitAutograd kernel','line_number':757,'multiline':False]
['text':' AND a dedicated meta kernel, but that probably shouldn't ever happen).','line_number':758,'multiline':False]
['text':' namespace functionalization','line_number':790,'multiline':False]
['text':' namespace at','line_number':791,'multiline':False]
