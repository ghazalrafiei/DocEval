['text':'*
 * UnknownQuantizer is a placeholder quantizer for functions that implement
 * quantization in a two step process.  First a tensor is allocated but with
 * unknown quantizer, and then the quantization kernel decides what the final
 * quantizer will be.
 ','line_number':22,'multiline':True]
['text':'*
 * UniformQuantizer is the parent class for all uniform quantizers.
 * These quantization scheme will map float value uniformly to
 * the quantized value. For example, affine quantizer is
 * the most commonly used scheme in this category.
 ','line_number':39,'multiline':True]
['text':'*
 * NonUniformQuantizer is the parent class for all non-uniform quantizers.
 * These quantization scheme may map float value non-uniformly to the quantized
 * value. K-means quantization is a representative example in this category.
 ','line_number':49,'multiline':True]
['text':' There is also StochasticQuantizer which is uniform but not affine','line_number':58,'multiline':False]
['text':'*
 * AffineQuantizer uses affine transformation to do quantization.
 *
 * For quantize:
 * Y = clamp(round(X / scale + zero_point), min, max)
 * For dequantize:
 * X = (Y - zero_point) * scale
 ','line_number':60,'multiline':True]
['text':' Note that we will not have Symmetric Quantizer in backend to reduce','line_number':72,'multiline':False]
['text':' complications in quantized kernel implementation.','line_number':73,'multiline':False]
['text':'*
 * PerTensorAffineQuantizer stores a scale and a zero_point, which is used for
 * all the values in the Tensor.
 ','line_number':75,'multiline':True]
['text':' We use int64_t for consistency with Python','line_number':114,'multiline':False]
['text':'*
 * PerChannelAffineQuantizer is the same as PerTensorAffineQuantizer
 * except that we have an independent scale and zero_point parameter
 * for each channel.
 *
 * Also note that per channel quantization is mostly applied to output channels
 * of weights since per-input channel of weight quantization or per-channel
 * quantization for activations can't be efficiently supported in most of
 * processors since it requires each multiplication result within a single
 * dot-product to have a different scale.
 ','line_number':118,'multiline':True]
['text':'*
 * PerChannelAffineFloatQParamsQuantizer is the same as PerChannelAffineQuantizer
 * except that it expects both scale and zero point to be floating point values.
 *
 * This quantizer uses the kPerChannelAffineFloatQParams qscheme which is a variant of
 * kPerChannelAffine.
 *
 * The quantize equation in this case looks like -
 * Xq = (Xf - zero_point) * inv_scale, where inv_scale = 1.0/scale
 *
 * Note: Usage of floating point zero point is useful in cases where 0 doesn't need to
 * be exactly represented in the quantized space. We can get additional precision by
 * using floating point values for zero point.
 ','line_number':178,'multiline':True]
['text':' This is an internal utility function for getting at the QTensorImpl,','line_number':224,'multiline':False]
['text':' You should only use this for writing low level','line_number':225,'multiline':False]
['text':' setters/getters for QTensorImpl fields; otherwise, you should use','line_number':226,'multiline':False]
['text':' the low level setters/getters that were implemented using this.','line_number':227,'multiline':False]
['text':' This may be called repeatedly, so make sure it's pretty cheap.','line_number':228,'multiline':False]
['text':' double and int64_t are because of the native function API, we only have these','line_number':231,'multiline':False]
['text':' argument types right now in native functions','line_number':232,'multiline':False]
['text':' Create a Quantized Tensor given arguments for normal Tensor and a quantizer','line_number':245,'multiline':False]
['text':' namespace at','line_number':279,'multiline':False]
