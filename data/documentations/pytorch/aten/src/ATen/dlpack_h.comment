['text':'!
 *  Copyright (c) 2017 by Contributors
 * \file dlpack.h
 * \brief The common header of DLPack.
 ','line_number':1,'multiline':True]
['text':'*
 * \brief Compatibility with C++
 ','line_number':9,'multiline':True]
['text':'! \brief The current version of dlpack ','line_number':18,'multiline':True]
['text':'! \brief The current ABI version of dlpack ','line_number':21,'multiline':True]
['text':'! \brief DLPACK_DLL prefix for windows ','line_number':24,'multiline':True]
['text':'!
 * \brief The device type in DLDevice.
 ','line_number':41,'multiline':True]
['text':'! \brief CPU device ','line_number':49,'multiline':True]
['text':'! \brief CUDA GPU device ','line_number':51,'multiline':True]
['text':'!
   * \brief Pinned CUDA CPU memory by cudaMallocHost
   ','line_number':53,'multiline':True]
['text':'! \brief OpenCL devices. ','line_number':57,'multiline':True]
['text':'! \brief Vulkan buffer for next generation graphics. ','line_number':59,'multiline':True]
['text':'! \brief Metal for Apple GPU. ','line_number':61,'multiline':True]
['text':'! \brief Verilog simulator buffer ','line_number':63,'multiline':True]
['text':'! \brief ROCm GPUs for AMD GPUs ','line_number':65,'multiline':True]
['text':'!
   * \brief Pinned ROCm CPU memory allocated by hipMallocHost
   ','line_number':67,'multiline':True]
['text':'!
   * \brief Reserved extension device type,
   * used for quickly test extension device
   * The semantics can differ depending on the implementation.
   ','line_number':71,'multiline':True]
['text':'!
   * \brief CUDA managed/unified memory allocated by cudaMallocManaged
   ','line_number':77,'multiline':True]
['text':'!
   * \brief Unified shared memory allocated on a oneAPI non-partititioned
   * device. Call to oneAPI runtime is required to determine the device
   * type, the USM allocation type and the sycl context it is bound to.
   *
   ','line_number':81,'multiline':True]
['text':'! \brief GPU support for next generation WebGPU standard. ','line_number':88,'multiline':True]
['text':'! \brief Qualcomm Hexagon DSP ','line_number':90,'multiline':True]
['text':'!
 * \brief A Device for Tensor and operator.
 ','line_number':94,'multiline':True]
['text':' NB: This is the only difference from','line_number':97,'multiline':False]
['text':' https://github.com/dmlc/dlpack/blob/v0.7/include/dlpack/dlpack.h Required to','line_number':98,'multiline':False]
['text':' allow forward declaration of DLDevice.','line_number':99,'multiline':False]
['text':'! \brief The device type used in the device. ','line_number':101,'multiline':True]
['text':'!
   * \brief The device index.
   * For vanilla CPU memory, pinned memory, or managed memory, this is set to 0.
   ','line_number':103,'multiline':True]
['text':'!
 * \brief The type code options DLDataType.
 ','line_number':110,'multiline':True]
['text':'! \brief signed integer ','line_number':114,'multiline':True]
['text':'! \brief unsigned integer ','line_number':116,'multiline':True]
['text':'! \brief IEEE floating point ','line_number':118,'multiline':True]
['text':'!
   * \brief Opaque handle type, reserved for testing purposes.
   * Frameworks need to agree on the handle data type for the exchange to be well-defined.
   ','line_number':120,'multiline':True]
['text':'! \brief bfloat16 ','line_number':125,'multiline':True]
['text':'!
   * \brief complex number
   * (C/C++/Python layout: compact struct per complex number)
   ','line_number':127,'multiline':True]
['text':'! \brief boolean ','line_number':132,'multiline':True]
['text':'!
 * \brief The data type the tensor can hold. The data type is assumed to follow the
 * native endian-ness. An explicit error message should be raised when attempting to
 * export an array with non-native endianness
 *
 *  Examples
 *   - float: type_code = 2, bits = 32, lanes = 1
 *   - float4(vectorized 4 float): type_code = 2, bits = 32, lanes = 4
 *   - int8: type_code = 0, bits = 8, lanes = 1
 *   - std::complex<float>: type_code = 5, bits = 64, lanes = 1
 *   - bool: type_code = 6, bits = 8, lanes = 1 (as per common array library convention, the underlying storage size of bool is 8 bits)
 ','line_number':136,'multiline':True]
['text':'!
   * \brief Type code of base types.
   * We keep it uint8_t instead of DLDataTypeCode for minimal memory
   * footprint, but the value should be one of DLDataTypeCode enum values.
   * ','line_number':149,'multiline':True]
['text':'!
   * \brief Number of bits, common choices are 8, 16, 32.
   ','line_number':155,'multiline':True]
['text':'! \brief Number of lanes in the type, used for vector types. ','line_number':159,'multiline':True]
['text':'!
 * \brief Plain C Tensor object, does not manage memory.
 ','line_number':163,'multiline':True]
['text':'!
   * \brief The data pointer points to the allocated data. This will be CUDA
   * device pointer or cl_mem handle in OpenCL. It may be opaque on some device
   * types. This pointer is always aligned to 256 bytes as in CUDA. The
   * `byte_offset` field should be used to point to the beginning of the data.
   *
   * Note that as of Nov 2021, multiply libraries (CuPy, PyTorch, TensorFlow,
   * TVM, perhaps others) do not adhere to this 256 byte aligment requirement
   * on CPU/CUDA/ROCm, and always use `byte_offset=0`.  This must be fixed
   * (after which this note will be updated); at the moment it is recommended
   * to not rely on the data pointer being correctly aligned.
   *
   * For given DLTensor, the size of memory required to store the contents of
   * data is calculated as follows:
   *
   * \code{.c}
   * static inline size_t GetDataSize(const DLTensor* t) {
   *   size_t size = 1;
   *   for (tvm_index_t i = 0; i < t->ndim; ++i) {
   *     size *= t->shape[i];
   *   }
   *   size *= (t->dtype.bits * t->dtype.lanes + 7) / 8;
   *   return size;
   * }
   * \endcode
   ','line_number':167,'multiline':True]
['text':'! \brief The device of the tensor ','line_number':194,'multiline':True]
['text':'! \brief Number of dimensions ','line_number':196,'multiline':True]
['text':'! \brief The data type of the pointer','line_number':198,'multiline':True]
['text':'! \brief The shape of the tensor ','line_number':200,'multiline':True]
['text':'!
   * \brief strides of the tensor (in number of elements, not bytes)
   *  can be NULL, indicating tensor is compact and row-majored.
   ','line_number':202,'multiline':True]
['text':'! \brief The offset in bytes to the beginning pointer to data ','line_number':207,'multiline':True]
['text':'!
 * \brief C Tensor object, manage memory of DLTensor. This data structure is
 *  intended to facilitate the borrowing of DLTensor by another framework. It is
 *  not meant to transfer the tensor. When the borrowing framework doesn't need
 *  the tensor, it should call the deleter to notify the host that the resource
 *  is no longer needed.
 ','line_number':211,'multiline':True]
['text':'! \brief DLTensor which is being memory managed ','line_number':219,'multiline':True]
['text':'! \brief the context of the original host framework of DLManagedTensor in
   *   which DLManagedTensor is used in the framework. It can also be NULL.
   ','line_number':221,'multiline':True]
['text':'! \brief Destructor signature void (*)(void*) - this should be called
   *   to destruct manager_ctx which holds the DLManagedTensor. It can be NULL
   *   if there is no way for the caller to provide a reasonable destructor.
   *   The destructors deletes the argument self as well.
   ','line_number':225,'multiline':True]
['text':' DLPACK_EXTERN_C','line_number':233,'multiline':False]
['text':' DLPACK_DLPACK_H_','line_number':235,'multiline':False]
