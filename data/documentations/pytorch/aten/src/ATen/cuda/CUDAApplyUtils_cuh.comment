['text':'','line_number':14,'multiline':False]
['text':' This file contains pointwise operation functions and kernels that','line_number':15,'multiline':False]
['text':' work on both contiguous and non-contiguous tensor arguments of','line_number':16,'multiline':False]
['text':' arbitrary (up to MAX_CUTORCH_DIMS) dimensioned arguments without','line_number':17,'multiline':False]
['text':' copying or temporary storage.','line_number':18,'multiline':False]
['text':'','line_number':19,'multiline':False]
['text':'
  NOTE [ CUDA_tensor_applyN helpers ]

  The following CUDA_tensor_applyN (where N currently can be 1, 2, 3, or 4)
  functions apply a pointwise operator to N tensor(s).

  The calling convention is

  1. The template arguments should be, sequentially,
    - First N typename args specify the scalar types of each of the N tensors.
    - (Optional) `int step` arg specifies the number of elements processed
      together at the same time.
      Default is 1.
    - A usually omitted (i.e., inferred) typename arg specifies the type of the
      function/functor applied on `N * step` values  in each iteration of each
      CUDA thread.
  2. The arguments should be, sequentially,
    - N tensors
    - op: a function/functor that processes `N * step` values at the same time.
      - If `step == 1`, it must have signature
        `void(*)(scalar1_t&, scalar2_t&, ..., scalarN_t&)`, where
        `scalar*_t`s are the first N typename template args, and the inputs
        are the `N` values from the `N` tensors retrieved at a common index.
      - Otherwise, it must must have signature
          void(*)(int n, scalar1_t&, scalar1_t&, ..., scalar1_t&,  // repeat `step` times
                         scalar2_t&, scalar2_t&, ..., scalar2_t&,  // repeat `step` times
                         ...,
                         scalarN_t&, scalarN_t&, ..., scalarN_t&)  // repeat `step` times
        Different from `step == 1` case, it processes `N * step` values taken
        from `step` common indices. Moreover, the first input `n` represents the
        number of valid indices (it will always have `0 < n <= step`). It will
        almost always be `step`, but at the boundary we may not have full `step`
        elements and `n` can be a lesser value.

        E.g., if `step == 4` and `N == 2`, `op` could be

          [](int n, scalar1_t &u1, scalar1_t &u2, scalar1_t &u3, scalar1_t &u4,
                    scalar2_t &v1, scalar2_t &v2, scalar2_t &v3, scalar2_t &v4) {
            // Only process u1, ..., un and v1, ..., vn.
            // So if `n == 3`, `u4` and `v4` need not to be considered.
          }

      In both cases, the references can actually be const, but at least one of
      them should be non-const in order to write the output.
    - (Optional, but recommended) N TensorArgType args that specify for each
      tensor whether `op` reads AND writes ] (i.e., TensorArgType::ReadWrite),
      or only reads (i.e., TensorArgType::ReadOnly).
      Default is TensorArgType::ReadWrite for first Tensor, and
                 TensorArgType::ReadOnly  for the rest.

  E.g.,

  to compute a = b^2 for a and b of same dtype, we can call

  CUDA_tensor_apply2<scalar, scalar>(
    a, b,
    [] __device__ (scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; }
  );

  to work on 2 values at the same time, we can call

  CUDA_tensor_apply2<scalar1, scalar2, 2>(
    a, b,
    [] __device__ (int n, scalar1 &a_val1, scalar1 &a_val2,
                          const scalar2 &b_val1, const scalar2 &b_val2) {
      // call special vectorized op here, or just do elementwise and enjoy unrolling...
      // if n == 1, only process a_val1 and b_val1
    }
  );
','line_number':21,'multiline':True]
['text':' TODO: combine with TensorArg?  So far that's been for debugging, and this is functional...','line_number':94,'multiline':False]
['text':' Rearrange dimensions for pointwise operations so that strides are in','line_number':99,'multiline':False]
['text':' decreasing order as much as possible, so that kernels have better memory','line_number':100,'multiline':False]
['text':' access patterns.','line_number':101,'multiline':False]
['text':'','line_number':102,'multiline':False]
['text':' For example, consider a binary operation on two "transposed" 2-dim tensors:','line_number':103,'multiline':False]
['text':'    sizes:          256 512','line_number':104,'multiline':False]
['text':'    aInfo->strides:   1 256','line_number':105,'multiline':False]
['text':'    bInfo->strides:   1 256','line_number':106,'multiline':False]
['text':'','line_number':107,'multiline':False]
['text':' Given this, each concurrent memory access inside kernelPointwiseApply2() is','line_number':108,'multiline':False]
['text':' exactly 256 elements apart, resulting in poor performance.','line_number':109,'multiline':False]
['text':'','line_number':110,'multiline':False]
['text':' This function exchanges dimensions so that memory access is contiguous:','line_number':111,'multiline':False]
['text':'    sizes:          512 256','line_number':112,'multiline':False]
['text':'    aInfo->strides: 256   1','line_number':113,'multiline':False]
['text':'    bInfo->strides: 256   1','line_number':114,'multiline':False]
['text':'','line_number':115,'multiline':False]
['text':' (Actually, it becomes even better because now collapseDims() can turn each','line_number':116,'multiline':False]
['text':' input into one contiguous array.)','line_number':117,'multiline':False]
['text':'','line_number':118,'multiline':False]
['text':' In general, given M (<=4) TensorInfo's with N dimensions, we can view each','line_number':119,'multiline':False]
['text':' strides[i] (0 <= i < N) as an M-tuple.  Given each pair i < j, we exchange','line_number':120,'multiline':False]
['text':' strides[i] and [j] if','line_number':121,'multiline':False]
['text':'    (1) strides[i][k] < strides[j][k] for some k (0 <= k < M)','line_number':122,'multiline':False]
['text':'        (exchanging them will benefit input #k), and','line_number':123,'multiline':False]
['text':'    (2) strides[i][k] <= strieds[j][k] for all k','line_number':124,'multiline':False]
['text':'        (exchanging them will not make any input worse).','line_number':125,'multiline':False]
['text':' Bail out if sizes do not match: we are using "deprecated pointwise','line_number':158,'multiline':False]
['text':' behavior" among tensors of different shapes but same number of elements.','line_number':159,'multiline':False]
['text':' No need to consider dimensions of size 1.','line_number':167,'multiline':False]
['text':' Compare the relative sizes of strides between dim #i and dim #j.','line_number':173,'multiline':False]
['text':' The `remaining_steps` argument is used to support Op that operates on','line_number':202,'multiline':False]
['text':' multiple elements at the same time. Generally, the strategy of ApplyOpN is to','line_number':203,'multiline':False]
['text':'  1. Initialize `remaining_steps = step`, where `step` is the template arg of','line_number':204,'multiline':False]
['text':'     CUDA_tensor_applyN helpers. The input arg `n` to `apply()` represents the','line_number':205,'multiline':False]
['text':'     number of elements in bound for this call. It will almost always equal to','line_number':206,'multiline':False]
['text':'     `step` except at boundaries.','line_number':207,'multiline':False]
['text':'  2. If `remaining_steps > 0` convert the current linearIndex to offset (if in','line_number':208,'multiline':False]
['text':'     bound), and recursively call `ApplyOpN` with `remaining_steps - 1`.','line_number':209,'multiline':False]
['text':'  3. At `remaining_steps = 0`,','line_number':210,'multiline':False]
['text':'       if `step = 1`, call `op(tensor1_val, tensor2_val, ...)`;','line_number':211,'multiline':False]
['text':'       if `step > 1`, call `op(n, tensor1_val1, tensor1_val2, ..., tesor1_valstep,','line_number':212,'multiline':False]
['text':'                                  tensor2_val1, tensor2_val2, ..., tesor2_valstep,','line_number':213,'multiline':False]
['text':'                                       ...','line_number':214,'multiline':False]
['text':'                                  tensorN_val1, tensorN_val2, ..., tesorN_valstep);`','line_number':215,'multiline':False]
['text':'','line_number':216,'multiline':False]
['text':' See NOTE [ CUDA_tensor_applyN helpers ] above for how Op may look like.','line_number':217,'multiline':False]
['text':' Convert `linearIndex` into an offset of `a`','line_number':229,'multiline':False]
['text':' Specialize `step=1` case (i.e., `remaining_steps=0` and `len(Offsets)=1`).','line_number':239,'multiline':False]
['text':' We don't need to pass in how many elements need to processed in this case.','line_number':240,'multiline':False]
['text':' Convert `linearIndex` into an offset of `a`','line_number':300,'multiline':False]
['text':' Convert `linearIndex` into an offset of `b`','line_number':304,'multiline':False]
['text':' Specialize `step=1` case (i.e., `remaining_steps=0` and `len(Offsets)=1`).','line_number':314,'multiline':False]
['text':' We don't need to pass in how many elements need to processed in this case.','line_number':315,'multiline':False]
['text':'n','line_number':327,'multiline':True]
['text':'linearIndex','line_number':327,'multiline':True]
['text':' anonymous namespace','line_number':375,'multiline':False]
['text':' Empty tensor; do nothing','line_number':400,'multiline':False]
['text':'
  Expands readable/writable tensors whose indices may be "overlapped."
  This ensures that each element of the tensor is operated on once and only
  once.
  ','line_number':412,'multiline':True]
['text':' Must perform in contiguous space','line_number':421,'multiline':False]
['text':' Must perform in contiguous space','line_number':425,'multiline':False]
['text':' It is possible that the tensor dimensions are able to be collapsed,','line_number':429,'multiline':False]
['text':' and thus we can reduce the actual code complexity of the copy by','line_number':430,'multiline':False]
['text':' exploiting this knowledge statically, since the div/mod is the','line_number':431,'multiline':False]
['text':' most expensive part of the operation, more so than memory accesses.','line_number':432,'multiline':False]
['text':' For instance, when copying a non-contiguous to a contiguous tensor','line_number':433,'multiline':False]
['text':' (or vice versa), the contiguous tensor can be collapsed to one','line_number':434,'multiline':False]
['text':' dimension, and the loop to translate the linear index to the array','line_number':435,'multiline':False]
['text':' index can be similarly collapsed. That is what this unrolling is for.','line_number':436,'multiline':False]
['text':'
    Only instantiates the all 1D special case and the fallback all nD case for
    large (64-bit indexed) tensors to reduce compilation time.
    ','line_number':499,'multiline':True]
['text':' Provides default step = 1 to CUDA_tensor_apply2. ','line_number':524,'multiline':True]
['text':' namespace at::cuda','line_number':537,'multiline':False]
