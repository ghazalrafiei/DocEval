['text':'
  Provides the implementations of CUDA BLAS function templates.
 ','line_number':1,'multiline':True]
['text':' cublasLT was introduced in CUDA 10.1 but we enable only for 11.1 that also','line_number':14,'multiline':False]
['text':' added bf16 support','line_number':15,'multiline':False]
['text':' until hipblas has an API to accept flags, we must use rocblas here','line_number':21,'multiline':False]
['text':' needed to work around calling rocblas API instead of hipblas API','line_number':25,'multiline':False]
['text':' hipblas does not have hipblasSetMathMode','line_number':62,'multiline':False]
['text':' until we use hiblas v2','line_number':64,'multiline':False]
['text':' hipify correctly maps things like CUDA_R_16F to HIP_R_16F,','line_number':65,'multiline':False]
['text':' however hipblas v1 is still using its custom type','line_number':66,'multiline':False]
['text':' Note: leading dimensions generally are checked that they are > 0','line_number':122,'multiline':False]
['text':' and at least as big the result requires (even if the value won't','line_number':123,'multiline':False]
['text':' be used).','line_number':124,'multiline':False]
['text':' Q: Why does Level3 check trans but this doesn't?','line_number':126,'multiline':False]
['text':' A: In level 2, the sizes (m, n) specify the size of A','line_number':127,'multiline':False]
['text':' (independent of trans value). In level 3. the sizes (m, n, k)','line_number':128,'multiline':False]
['text':' specify the sizes of op(A), op(B) where op depend on trans','line_number':129,'multiline':False]
['text':' values.','line_number':130,'multiline':False]
['text':' Note: leading dimensions generally are checked that they are > 0','line_number':147,'multiline':False]
['text':' and at least as big the result requires (even if the value won't','line_number':148,'multiline':False]
['text':' be used).','line_number':149,'multiline':False]
['text':' alignment are in bytes','line_number':171,'multiline':False]
['text':' default size in KiB according to #73328 ','line_number':182,'multiline':True]
['text':' anonymous namespace','line_number':202,'multiline':False]
['text':' LEVEL 3 BLAS FUNCTIONS ','line_number':206,'multiline':True]
['text':' See Note [Writing Nondeterministic Operations]','line_number':231,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':244,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':257,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':272,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':287,'multiline':False]
['text':' USE_ROCM','line_number':329,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':334,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':354,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':367,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':380,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':395,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':410,'multiline':False]
['text':' Disallow fp16 reductions that could lead to unexpected overflow issues.','line_number':458,'multiline':False]
['text':' Following the pattern of CuSparseDescriptor','line_number':547,'multiline':False]
['text':' Defined here for now because this is the only place cublas_lt interface is','line_number':548,'multiline':False]
['text':' used but can be moved to a header once cublas_lt interface is used in','line_number':549,'multiline':False]
['text':' multiple places.','line_number':550,'multiline':False]
['text':' namespace','line_number':623,'multiline':False]
['text':' bias is added in epilogue','line_number':642,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/73328 for reasoning behind','line_number':683,'multiline':False]
['text':' setting this to 1M.','line_number':684,'multiline':False]
['text':' CUDA_VERSION >= 11080','line_number':939,'multiline':False]
['text':' cublas team: alpha and beta need to be the same dtype as of scaleType','line_number':976,'multiline':False]
['text':' Heuristics don't seem to work for int8','line_number':993,'multiline':False]
['text':' Non-zero workspace doesn't seem to work.','line_number':994,'multiline':False]
['text':' !defined(USE_ROCM) && !defined(_MSC_VER)','line_number':1026,'multiline':False]
['text':' ROCm 5.6 hipblas matches the const Dtype *A API, but prior hipblas does not.','line_number':1028,'multiline':False]
['text':' LEVEL 2 BLAS FUNCTIONS ','line_number':1155,'multiline':True]
['text':' See Note [Writing Nondeterministic Operations]','line_number':1168,'multiline':False]
['text':' gemv is bw bound, and does not benefit from TF32. But the precision','line_number':1182,'multiline':False]
['text':' loss still happens on TF32. So we disable it here.','line_number':1183,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':1185,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':1199,'multiline':False]
['text':' gemv is bw bound, and does not benefit from TF32. But the precision','line_number':1211,'multiline':False]
['text':' loss still happens on TF32. So we disable it here.','line_number':1212,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':1214,'multiline':False]
['text':' In general, cublas regards matrices as column-major.','line_number':1226,'multiline':False]
['text':' The cublasS/Dgemv usages in cuda::blas::gemv<float>/<double> above','line_number':1227,'multiline':False]
['text':' require that external blas::gemv callers obey the following convention:','line_number':1228,'multiline':False]
['text':'','line_number':1229,'multiline':False]
['text':' If "a" is row-major with shape (output, summed) in blas::gemv's caller,','line_number':1230,'multiline':False]
['text':' caller interprets it as column-major with shape (summed, output), passes','line_number':1231,'multiline':False]
['text':' summed and output respectively to our local vars m, n, and requests that cublas','line_number':1232,'multiline':False]
['text':' internally transpose ("trans") the column-major interpretation of a.','line_number':1233,'multiline':False]
['text':'','line_number':1234,'multiline':False]
['text':' There's no such thing as "cublasHalfgemv", so here we hack gemv with a gemm.','line_number':1235,'multiline':False]
['text':' However, we must allow the same calling convention, because the caller shouldn't','line_number':1236,'multiline':False]
['text':' have to swap args based on whether it's calling blas::gemv<at::Half> or <float>.','line_number':1237,'multiline':False]
['text':' After swap, local vars m, n contain the output and summed sizes respectively,','line_number':1243,'multiline':False]
['text':' regardless of whether "a" was row-major or column-major in gemv<>'s caller.','line_number':1244,'multiline':False]
['text':' To handle the possibility incy > 1, interprets vector y as column-major matrix with one row','line_number':1246,'multiline':False]
['text':' (shape (1, output)) and leading dim incy.','line_number':1247,'multiline':False]
['text':' trans(a)*x would compute a matrix with one column (shape (output, 1)) which wouldn't match y.','line_number':1248,'multiline':False]
['text':' So instead, we interpret x similarly to y, as a column-major matrix with one row','line_number':1249,'multiline':False]
['text':' (shape (1, summed)) and leading dim incx.  The gemm then carries out x*transpose(trans(a)) to','line_number':1250,'multiline':False]
['text':' produce a matrix with one row (shape (1, output)), matching y.','line_number':1251,'multiline':False]
['text':' LEVEL 1 BLAS FUNCTIONS ','line_number':1268,'multiline':True]
['text':' namespace at::cuda::blas','line_number':1539,'multiline':False]
