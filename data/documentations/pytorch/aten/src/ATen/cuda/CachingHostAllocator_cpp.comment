['text':' Note: cudaEventCreate when concurrently invoked from multiple threads can be','line_number':39,'multiline':False]
['text':' very expensive (at least on certain device/driver combinations). Thus, we a)','line_number':40,'multiline':False]
['text':' serialize event creation at a per-device level, and b) pool the events to','line_number':41,'multiline':False]
['text':' avoid constantly calling cudaEventCreate/cudaEventDestroy. This results in','line_number':42,'multiline':False]
['text':' significant improvements in multithreaded workloads with high allocation','line_number':43,'multiline':False]
['text':' rates.','line_number':44,'multiline':False]
['text':' Try to acquire an event from the per-device pool.','line_number':61,'multiline':False]
['text':' otherwise, allocate a new event that will be returned to the pool on','line_number':70,'multiline':False]
['text':' destruction.','line_number':71,'multiline':False]
['text':' Used for heterogenous lookup support in the free list.','line_number':92,'multiline':False]
['text':' Transparent overloads','line_number':102,'multiline':False]
['text':'*
 * Note [CUDAHostAllocator design]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * We have three key data structures - the free list which stores blocks that
 * are not currently used, the block list which stores all blocks that have been
 * allocated, and the event queue which stores CUDA events and their
 * corresponding blocks.
 *
 * Each of these are protected by a separate mutex. The key design principles
 * are to 1) only hold each mutex for the minimal amount of time possible, 2)
 * never do any possible expensive operations (such as CUDA runtime API calls)
 * while holding the lock.
 *
 * There are three public methods: allocate, free, and record_event. In the
 * allocate path, we first check to see if we can service our request from this
 * free list, and otherwise we create a new block with cudaHostAlloc. In the
 * free path, we insert events (if required) into the event queue, and if
 * possible insert our block back into the free list. In allocate, we first
 * eagerly query events until we find one that is not ready, and insert the
 * corresponding block onto the free list if all the events recorded for a
 * block are ready. In the record_event path, we simply insert the given
 * stream into the set of streams tracked by the specified block. This set of
 * streams is then consumed in the free path.
 *
 * Some of the invariants here are less strict than they could be - for example,
 * we do not enforce that free(Block* block) => block->event_count == 0. This is
 * for compatibility reasons, and we can explore enforcing these in subsequent
 * versions.
 ','line_number':117,'multiline':True]
['text':' First, try to allocate from the free list','line_number':155,'multiline':False]
['text':' Then, create a new block.','line_number':166,'multiline':False]
['text':' Pinned memory pointers allocated by any device can be directly used by','line_number':167,'multiline':False]
['text':' any other device, regardless of the current device at the time of','line_number':168,'multiline':False]
['text':' allocation, since we assume unified addressing. So we grab any existing','line_number':169,'multiline':False]
['text':' primary context, if available. See pytorch/pytorch#21081.','line_number':170,'multiline':False]
['text':' Round up the allocation to the nearest power of two to improve reuse.','line_number':179,'multiline':False]
['text':' Use cudaHostAlloc for allocating pinned memory (global lock in driver)','line_number':186,'multiline':False]
['text':' Note: we can assume that free is correctly paired with alloc,','line_number':208,'multiline':False]
['text':' and thus we do not need to look up the ctx in blocks_.','line_number':209,'multiline':False]
['text':' Note: we need to check if the passed-in `ctx` is valid. This is because','line_number':245,'multiline':False]
['text':' `record_event` (via `CachingHostAllocator_recordEvent`) can be invoked on','line_number':246,'multiline':False]
['text':' an arbitrary tensor, and is not guaranteed to correspond to a pinned','line_number':247,'multiline':False]
['text':' memory allocation. Therefore, we need to check that `ctx` is valid before','line_number':248,'multiline':False]
['text':' proceeding.','line_number':249,'multiline':False]
['text':' Now we know this object is safe to access.','line_number':253,'multiline':False]
['text':' Flush any available blocks into the free_list.','line_number':273,'multiline':False]
['text':' Release cached events from the event pool.','line_number':276,'multiline':False]
['text':' Remove all elements from the free list, remove them from the blocks','line_number':279,'multiline':False]
['text':' list, and free the associated pinned memory allocation. This requires','line_number':280,'multiline':False]
['text':' concurrently holding both the free list mutex and the blocks mutex, and','line_number':281,'multiline':False]
['text':' is the only function that concurrently holds multiple mutexes.','line_number':282,'multiline':False]
['text':' Avoid calling cudaEventDestroy while holding a mutex, so move','line_number':307,'multiline':False]
['text':' intermediate events out of the lock into this object.','line_number':308,'multiline':False]
['text':' otherwise, query the event','line_number':323,'multiline':False]
['text':' now, see if we can handle this element','line_number':325,'multiline':False]
['text':' clear CUDA error','line_number':329,'multiline':False]
['text':' push the event onto the back of the queue if it's not','line_number':330,'multiline':False]
['text':' ready. TODO: do we need some debouncing logic to avoid allocating','line_number':331,'multiline':False]
['text':' threads repeatedly spinning on an event?','line_number':332,'multiline':False]
['text':' Process the events.','line_number':343,'multiline':False]
['text':' pre-fault/map the pages by setting the first byte of the page','line_number':382,'multiline':False]
['text':' If host and device pointer don't match, give a warning and exit','line_number':394,'multiline':False]
['text':' Here we do regular allocation, pre-fault/map the pages, and then do','line_number':406,'multiline':False]
['text':' cudaHostRegister with GPU mapping flags to lock the pages, so we','line_number':407,'multiline':False]
['text':' can minimize the cost for the cuda global lock.','line_number':408,'multiline':False]
['text':' Parallelize the mapping/registering of pages to reduce wall time','line_number':411,'multiline':False]
['text':' 4kB pages','line_number':412,'multiline':False]
['text':' parallelize the mapping of pages with a threadpool','line_number':416,'multiline':False]
['text':' thread task-id','line_number':430,'multiline':False]
['text':' set the promise when mapping pages are done','line_number':433,'multiline':False]
['text':' Map pages in the same thread','line_number':442,'multiline':False]
['text':' Register the mapped pages using cudaHostRegister','line_number':446,'multiline':False]
['text':' Note: sharding this mutex seems to be profitable in heavily multi-threaded','line_number':455,'multiline':False]
['text':' scenarios.','line_number':456,'multiline':False]
['text':' Note: an alternative datastructure can yield significant wins here in','line_number':458,'multiline':False]
['text':' microbenchmarks.','line_number':459,'multiline':False]
['text':' namespace','line_number':466,'multiline':False]
['text':' leak and don't worry about shutdown','line_number':469,'multiline':False]
['text':' Releases cached pinned memory allocations via cudaHostFree','line_number':485,'multiline':False]
['text':' namespace at::cuda','line_number':507,'multiline':False]
