['text':' NOTE: [What is a batching rule?]','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':' A *batching rule* implements the logic of how to call an operator on inputs','line_number':17,'multiline':False]
['text':' that have zero or more additional batch dimensions. When one does a vmap, the','line_number':18,'multiline':False]
['text':' dimension(s) being vmap'ed over get recorded as batch dimensions.','line_number':19,'multiline':False]
['text':'','line_number':20,'multiline':False]
['text':' For example, vmap(torch.add)(x, y)','line_number':21,'multiline':False]
['text':' 1. wraps `x` into batched_x = BatchedTensor(x, bdims=[(lvl=1, dim=0)];','line_number':22,'multiline':False]
['text':' 2. wraps `y` into batched_y = BatchedTensor(y, bdims=[(lvl=1, dim=0)];','line_number':23,'multiline':False]
['text':' 3. and then runs `torch.add(batched_x, batched_y)`.','line_number':24,'multiline':False]
['text':' NOTE: [When should I add a batching rule?]','line_number':26,'multiline':False]
['text':' When you are adding a new operator, you'll need to add a batching rule so','line_number':27,'multiline':False]
['text':' that vmap can work efficiently with said operator. If you do not, we'll attempt','line_number':28,'multiline':False]
['text':' to generate a slow fallback for the batching rule.','line_number':29,'multiline':False]
['text':' NOTE: [How to write batching rules?]','line_number':31,'multiline':False]
['text':' The signature of a batching rule should look like exactly like the C++ signature','line_number':32,'multiline':False]
['text':' of its operator.','line_number':33,'multiline':False]
['text':'','line_number':34,'multiline':False]
['text':' First, see NOTE: [Logical vs physical args] in VmapTransforms.h for terminology.','line_number':35,'multiline':False]
['text':'','line_number':36,'multiline':False]
['text':' At a high level, what a batching rule does is the following:','line_number':37,'multiline':False]
['text':' 1. Converts (logical) BatchedTensors to views on physical tensors.','line_number':38,'multiline':False]
['text':' 2. Converts logical arguments (e.g. dimension indexes, shapes) to physical','line_number':39,'multiline':False]
['text':'    arguments that correspond to the physical tensors.','line_number':40,'multiline':False]
['text':' 3. Calls at:: operations on the physical tensors and arguments to produce','line_number':41,'multiline':False]
['text':'    some physical results.','line_number':42,'multiline':False]
['text':' 4. Converts physical results back to BatchedTensors.','line_number':43,'multiline':False]
['text':'','line_number':44,'multiline':False]
['text':' Steps 1, 2, and 4 differ for operators with different batching behaviors. When','line_number':45,'multiline':False]
['text':' writing a new batching rule, please select a VmapTransform that matches the','line_number':46,'multiline':False]
['text':' batching behavior of your operation. The VmapTransform provides helper functions','line_number':47,'multiline':False]
['text':' to do steps (1), (2), and (4).','line_number':48,'multiline':False]
['text':' (see NOTE: [What is an VmapTransform?] in VmapTransforms.h)','line_number':49,'multiline':False]
['text':' Note: [Future plans]','line_number':51,'multiline':False]
['text':' The API for writing a batching rule isn't stable. In the future, we'd like','line_number':52,'multiline':False]
['text':' to think about the problem of translating these batching rules to TorchScript.','line_number':53,'multiline':False]
['text':' Ideally batching rules in eager mode vs TorchScript would look pretty similar,','line_number':54,'multiline':False]
['text':' if not use the same mechanism. In order to accomplish that we might have to','line_number':55,'multiline':False]
['text':' do some refactoring.','line_number':56,'multiline':False]
['text':' PyTorch allows operations to specify dim 0 and dim -1 on a scalar tensor.','line_number':60,'multiline':False]
['text':' PyTorch has a special case where sum(scalar_tensor, dim=0) does not fail','line_number':68,'multiline':False]
['text':' and instead returns a new scalar tensor (this also happens for dim=-1)','line_number':69,'multiline':False]
['text':' If the following happens:','line_number':70,'multiline':False]
['text':' >>> x = torch.randn(B0)  # the per-examples are all scalars','line_number':71,'multiline':False]
['text':' >>> vmap(partial(torch.sum, dim=0), x)','line_number':72,'multiline':False]
['text':' then we replicate the behavior of sum(scalar_tensor, dim=0).','line_number':73,'multiline':False]
['text':'logical','line_number':74,'multiline':True]
['text':' At this point, we know at least one of the operands is a logical Scalar tensor.','line_number':114,'multiline':False]
['text':' Here we must emulate TensorIterator's special behavior on Scalars.','line_number':115,'multiline':False]
['text':'','line_number':116,'multiline':False]
['text':' As a motivating example, consider the following:','line_number':117,'multiline':False]
['text':'   x = torch.randn(3, 10)','line_number':118,'multiline':False]
['text':'   y = torch.randn(3, dtype=torch.double)','line_number':119,'multiline':False]
['text':'   vmap(torch.mul)(torch.randn(3, 10), torch.randn(3, dtype=torch.double))','line_number':120,'multiline':False]
['text':'','line_number':121,'multiline':False]
['text':' At a per-example level, we are adding FloatTensor[10] and DoubleTensor[];','line_number':122,'multiline':False]
['text':' Type Promotion dictates that the result should be FloatTensor[10].','line_number':123,'multiline':False]
['text':' This means we cannot directly pass the physical tensors (x and y) to','line_number':124,'multiline':False]
['text':' TensorIterator (if we did, it would promote them to DoubleTensor).','line_number':125,'multiline':False]
['text':'','line_number':126,'multiline':False]
['text':' FIXME(rzou): I didn't want to go down the slippery slope of emulating','line_number':127,'multiline':False]
['text':' everything TensorIterator does (it would be better to refactor out the','line_number':128,'multiline':False]
['text':' TensorIterator logic). The one thing that this code doesn't handle','line_number':129,'multiline':False]
['text':' is cross-device logical scalar tensors.','line_number':130,'multiline':False]
['text':'   cpu_tensor = torch.randn(3)','line_number':131,'multiline':False]
['text':'   cuda_tensor = torch.randn(3, 10, device='cuda')','line_number':132,'multiline':False]
['text':'   vmap(torch.mul)(cpu_tensor, cuda_tensor)','line_number':133,'multiline':False]
['text':'','line_number':134,'multiline':False]
['text':' At a per-example level, we are adding CPUTensor[] and CUDATensor[10].','line_number':135,'multiline':False]
['text':' TensorIterator allows for this cross-device operation because one of the','line_number':136,'multiline':False]
['text':' tensors is a Scalar CPU tensor. However, the following code will throw an','line_number':137,'multiline':False]
['text':' error in that case. I don't expect to see many use cases for this, so','line_number':138,'multiline':False]
['text':' this is probably fine as-is.','line_number':139,'multiline':False]
['text':'logical','line_number':161,'multiline':True]
['text':'logical dim','line_number':163,'multiline':True]
['text':' Here, we know we are expanding a (logical) tensor to a larger number','line_number':171,'multiline':False]
['text':' of dimensions. We have to be careful because we can't call expand directly','line_number':172,'multiline':False]
['text':' due to the presence of batch dimensions.','line_number':173,'multiline':False]
['text':'','line_number':174,'multiline':False]
['text':' As an example, let B0 be a batch dimension and consider expand(Tensor[B0, 3], [2, 3]).','line_number':175,'multiline':False]
['text':' The result should be a tensor of size [B0, 2, 3].','line_number':176,'multiline':False]
['text':' A physical view of size [B0, 3] can't directly be expanded to size [B0, 2, 3]','line_number':177,'multiline':False]
['text':' so the strategy here is to view it first as a tensor of size [B0, 1, 3] and','line_number':178,'multiline':False]
['text':' then expand.','line_number':179,'multiline':False]
['text':' NB: unsqueeze has some special handling of its `dim` argument so we can't call','line_number':237,'multiline':False]
['text':' self_physical.getPhysicalDim directly. In particular, native::unsqueeze','line_number':238,'multiline':False]
['text':' wraps the dim to (the logical dimension) + 1, so we need to do that here too.','line_number':239,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/b623bdeabb0aa8da44285d303246e7f8ac06c2a9/aten/src/ATen/native/TensorShape.cpp#L1413','line_number':240,'multiline':False]
['text':'logical_dim','line_number':242,'multiline':True]
['text':' Don't squeeze the batch dims!','line_number':277,'multiline':False]
['text':' Batched Diagonal View','line_number':310,'multiline':False]
['text':'offset','line_number':311,'multiline':True]
['text':'dim1','line_number':311,'multiline':True]
['text':'dim2','line_number':311,'multiline':True]
['text':' Batched Diagonal View','line_number':319,'multiline':False]
['text':'offset','line_number':320,'multiline':True]
['text':'dim1','line_number':320,'multiline':True]
['text':'dim2','line_number':320,'multiline':True]
['text':' Append a dimension of size one to the grad output','line_number':321,'multiline':False]
['text':' PyTorch has a special case where scalar_tensor.transpose(dim0, dim1) works','line_number':328,'multiline':False]
['text':' for dim0, dim1 in {0, -1} and returns the scalar tensor. If the following happens:','line_number':329,'multiline':False]
['text':' >>> x = torch.randn(B0)  # the per-examples are all scalars','line_number':330,'multiline':False]
['text':' >>> vmap(lambda x: x.transpose(0, -1), x)','line_number':331,'multiline':False]
['text':' then we replicate this behavior.','line_number':332,'multiline':False]
['text':'logical','line_number':333,'multiline':True]
['text':' guard against the user passing in a batch of scalar tensors with batch','line_number':480,'multiline':False]
['text':' size equal to 2.','line_number':481,'multiline':False]
['text':' Checks that the smallest batch stride is greater than the largest example','line_number':488,'multiline':False]
['text':' stride. This is something we can support but we choose not to because it's','line_number':489,'multiline':False]
['text':' potentially error prone.','line_number':490,'multiline':False]
['text':' No example dimensions','line_number':497,'multiline':False]
['text':' given (sizes, strides, storage_offset) returns the maximum location that','line_number':508,'multiline':False]
['text':' can be indexed (or nullopt if such a location doesn't exist, e.g., tensors','line_number':509,'multiline':False]
['text':' with zero-size dims).','line_number':510,'multiline':False]
['text':' Let x be the "first slice" of physical_tensor.','line_number':520,'multiline':False]
['text':' This checks that the range of possible memory locations accessible by','line_number':521,'multiline':False]
['text':' x.as_strided(sizes, strides, maybe_storage_offset)','line_number':522,'multiline':False]
['text':' are within the bounds of possible memory locations accessible by x.','line_number':523,'multiline':False]
['text':' The _has_same_storage_numel check is skipped if the tangent is a batched','line_number':585,'multiline':False]
['text':' tensor because using as_strided to access storage locations not indexable','line_number':586,'multiline':False]
['text':' by the input tensor is not supported in vmap','line_number':587,'multiline':False]
['text':' What are the semantics of as_strided inside of vmap?','line_number':591,'multiline':False]
['text':' y = vmap(lambda x: x.as_strided(sizes, strides, offset))(xs)','line_number':592,'multiline':False]
['text':' This returns a view on `x`, `y`, such that each y[i] has:','line_number':593,'multiline':False]
['text':' - sizes: `sizes`','line_number':594,'multiline':False]
['text':' - strides: `strides`','line_number':595,'multiline':False]
['text':' - storage_offset: offset + i * x.stride(batch_dim)','line_number':596,'multiline':False]
['text':'','line_number':597,'multiline':False]
['text':' In other words, it is as if we had treated each x[i] as having storage','line_number':598,'multiline':False]
['text':' offset equal to xs.offset() and called as_strided(sizes, sizes, offset).','line_number':599,'multiline':False]
['text':' (that is equivalent to x[i].as_strided(','line_number':600,'multiline':False]
['text':'    sizes, sizes, offset + x[i].storage_offset() - xs.offset()) for all i)','line_number':601,'multiline':False]
['text':'','line_number':602,'multiline':False]
['text':' Note that this *may* be different from actually running as_strided','line_number':603,'multiline':False]
['text':' in a for-loop. This is due to how as_strided takes in `offset` to be','line_number':604,'multiline':False]
['text':' an *absolute* offset. As an example, consider:','line_number':605,'multiline':False]
['text':' >>> x = torch.tensor([0., 1., 2., 3., 4.]).as_strided([4], [1], 1)','line_number':606,'multiline':False]
['text':' >>> z = [x[i].as_strided([1], [1], 1) for i in range(4)]','line_number':607,'multiline':False]
['text':' Each z[i] is actually the same view on x (z[i] == torch.tensor([1.]))!','line_number':608,'multiline':False]
['text':' However, we consider the above for-loop comprehension to be a user error:','line_number':609,'multiline':False]
['text':' a user should have written the following if they wanted to use as_strided','line_number':610,'multiline':False]
['text':' in a per-sample way:','line_number':611,'multiline':False]
['text':' >>> z = [x[i].as_strided([1], [1], 1 + x[i].storage_offset() - 1) for i in range(4)]','line_number':612,'multiline':False]
['text':' We can't rely on the physical as_strided call to do this for us because','line_number':623,'multiline':False]
['text':' we do some sanity checks on the size/strides before calling into as_strided.','line_number':624,'multiline':False]
['text':' Sanity checks:','line_number':629,'multiline':False]
['text':' 1. All batch dims are at the front in memory layout (not necessary for','line_number':630,'multiline':False]
['text':' correctness, but we are worried the user might be doing crazy things)','line_number':631,'multiline':False]
['text':' 2. as_strided(sizes, strides, storage_offset + tensor[i].offset() - tensor.offset())','line_number':632,'multiline':False]
['text':' is valid for a slice of the input tensor.','line_number':633,'multiline':False]
['text':' See Note: [When will the as_strided batching rule fail?] for details.','line_number':634,'multiline':False]
['text':' physical_strides = physical tensor's batch strides + (logical) strides','line_number':639,'multiline':False]
['text':' If zi = xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':648,'multiline':False]
['text':' is valid for all i, then it turns out that','line_number':649,'multiline':False]
['text':' xs.as_strided(physical_sizes, physical_strides, offset) always succeeds','line_number':650,'multiline':False]
['text':' and creates a tensor y such that each y[i] references the same memory','line_number':651,'multiline':False]
['text':' locations as zi. See NOTE: [When will the as_strided batching rule fail?]','line_number':652,'multiline':False]
['text':' NOTE: [When will the as_strided batching rule fail?]','line_number':658,'multiline':False]
['text':' If zi = xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':659,'multiline':False]
['text':' is valid for all i, then it turns out that','line_number':660,'multiline':False]
['text':' xs.as_strided(physical_sizes, physical_strides, offset) always succeeds and','line_number':661,'multiline':False]
['text':' creates a tensor y such that each y[i] refers to the same memory as zi.','line_number':662,'multiline':False]
['text':'','line_number':663,'multiline':False]
['text':' Let's say we have xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset()).','line_number':664,'multiline':False]
['text':' Furthermore, let's say that as a part of being "valid" this as_strided call','line_number':665,'multiline':False]
['text':' does not return a result that can index memory not indexable by xs[i].','line_number':666,'multiline':False]
['text':'','line_number':667,'multiline':False]
['text':' WLOG, assume that there's only one batch dim and it is at the front of the','line_number':668,'multiline':False]
['text':' `xs` tensor. Let B be the batch size and S be the stride of the batch dim.','line_number':669,'multiline':False]
['text':' - If the batch dim isn't at the front of the tensor, then we can just move it','line_number':670,'multiline':False]
['text':' to the front with movedim/permute. This is always valid because it just swaps','line_number':671,'multiline':False]
['text':' some strides around.','line_number':672,'multiline':False]
['text':' - This proof also works for tensors with multiple batch dims. We just have to','line_number':673,'multiline':False]
['text':' do a little accounting:','line_number':674,'multiline':False]
['text':'   - instead of [B], we'd have [B0, B1, ..., Bk].','line_number':675,'multiline':False]
['text':'   - instead of [S], we'd have [S0, S1, ..., Sk].','line_number':676,'multiline':False]
['text':'   - instead of i, we'd have a list of indices [I0, I1, ..., Ik]','line_number':677,'multiline':False]
['text':'   - instead of S * I, we'd have \sum_{i=0}^k S_i * I_i','line_number':678,'multiline':False]
['text':'','line_number':679,'multiline':False]
['text':' [Equation 1]','line_number':680,'multiline':False]
['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset()) has:','line_number':681,'multiline':False]
['text':' - sizes: sizes','line_number':682,'multiline':False]
['text':' - strides: strides','line_number':683,'multiline':False]
['text':' - offset: offset + S * i','line_number':684,'multiline':False]
['text':'','line_number':685,'multiline':False]
['text':' x.as_strided itself checks that:','line_number':686,'multiline':False]
['text':' - (sizes, strides, offset) are in bounds for `x`'s storage.','line_number':687,'multiline':False]
['text':' - strides are positive','line_number':688,'multiline':False]
['text':' - offset is positive','line_number':689,'multiline':False]
['text':'','line_number':690,'multiline':False]
['text':' Claim 1: if xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':691,'multiline':False]
['text':' is valid, then','line_number':692,'multiline':False]
['text':' ([B] + sizes, [S] + strides, offset + xs.offset()) are in bounds for `xs`'s storage.','line_number':693,'multiline':False]
['text':'','line_number':694,'multiline':False]
['text':' If we have the claim, then xs.as_strided([B] + sizes, [S] + strides, offset)','line_number':695,'multiline':False]
['text':' won't error out. So all we need to check is that the memory locations are','line_number':696,'multiline':False]
['text':' what we expected. See [Hand-wavy proof of Claim 1] for proof (it's not very important)','line_number':697,'multiline':False]
['text':'','line_number':698,'multiline':False]
['text':' xs.as_strided(physical_sizes, physical_strides, offset) is equivalent to','line_number':699,'multiline':False]
['text':' xs.as_strided([B] + sizes, [S] + strides, offset)','line_number':700,'multiline':False]
['text':'','line_number':701,'multiline':False]
['text':' xs.as_strided([B] + sizes, [S] + strides, offset) has:','line_number':702,'multiline':False]
['text':' - sizes: [B] + sizes','line_number':703,'multiline':False]
['text':' - strides: [S] + strides','line_number':704,'multiline':False]
['text':' - offset: offset','line_number':705,'multiline':False]
['text':'','line_number':706,'multiline':False]
['text':' xs.as_strided([B] + sizes, [S] + strides, offset)[i] has:','line_number':707,'multiline':False]
['text':' - sizes: sizes','line_number':708,'multiline':False]
['text':' - strides: strides','line_number':709,'multiline':False]
['text':' - offset: offset + S * i','line_number':710,'multiline':False]
['text':' These memory locations are exactly the same as what we got for [Equation 1],','line_number':711,'multiline':False]
['text':' so the xs.as_strided([B] + sizes, [S] + strides, offset) is valid.','line_number':712,'multiline':False]
['text':'','line_number':713,'multiline':False]
['text':' [Hand-wavy proof of Claim 1]','line_number':714,'multiline':False]
['text':' Part of our definition of being valid is that xs[i].as_strided(...)','line_number':715,'multiline':False]
['text':' must return a tensor that only uses memory indexable by xs[i].','line_number':716,'multiline':False]
['text':' This means that (sizes, strides, offset + xs[i].offset() - xs.offset()) satisfies:','line_number':717,'multiline':False]
['text':'    offset + xs[i].offset() - xs.offset() + 1 + \sum_j (sizes[j] - 1) * strides[j]','line_number':718,'multiline':False]
['text':'    <= xs[i].offset() + 1 + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':719,'multiline':False]
['text':' (the largest-index memory location of xs[i].as_strided(...) must be \leq','line_number':720,'multiline':False]
['text':' the largest-index memory location of xs[i])','line_number':721,'multiline':False]
['text':'','line_number':722,'multiline':False]
['text':' Fiddling that inequality gives us:','line_number':723,'multiline':False]
['text':'    offset - xs.offset() + 1 + \sum_j (sizes[j] - 1) * strides[j]','line_number':724,'multiline':False]
['text':'    <= 1 + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':725,'multiline':False]
['text':'','line_number':726,'multiline':False]
['text':'    offset - xs.offset() + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':727,'multiline':False]
['text':'    <= 1 + (B-1)*S + \sum_j (xs[i].size(j) - 1) * xs[i].stride(j)','line_number':728,'multiline':False]
['text':'','line_number':729,'multiline':False]
['text':'    offset - xs.offset() + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':730,'multiline':False]
['text':'    <= 1 + \sum_j (xs.size(j) - 1) * xs.stride(j)','line_number':731,'multiline':False]
['text':'','line_number':732,'multiline':False]
['text':'    offset + 1 + (B-1)*S + \sum_j (sizes[j] - 1) * strides[j]','line_number':733,'multiline':False]
['text':'    <= xs.offset() + 1 + \sum_j (xs.size(j) - 1) * xs.stride(j)','line_number':734,'multiline':False]
['text':' (the largest-index memory location of xs.as_strided(size, stride, offset)','line_number':735,'multiline':False]
['text':' is \leq than the largest-index memory location of xs)','line_number':736,'multiline':False]
['text':' Under the assumptions we've made, the lower bound (lowest indexed memory)','line_number':737,'multiline':False]
['text':' is trivially within the storage.','line_number':738,'multiline':False]
['text':'','line_number':739,'multiline':False]
['text':' Therefore ([B] + sizes, [S] + strides, offset) are in bounds for','line_number':740,'multiline':False]
['text':' `xs`'s storage.','line_number':741,'multiline':False]
['text':' Memory format support is a little tricky because vmap is allowed to move','line_number':767,'multiline':False]
['text':' around batch dimensions and some memory formats are rank-dependent.','line_number':768,'multiline':False]
['text':' Another weird case is:','line_number':769,'multiline':False]
['text':' - a tensor with MemoryFormat::ChannelsLast MUST have 4 dimensions. Do we','line_number':770,'multiline':False]
['text':'   allow the user to clone a Tensor with 3 logical dimensions and 1 batch','line_number':771,'multiline':False]
['text':'   dim into a ChannelsLast Tensor? What about a Tensor with 3 logical dims','line_number':772,'multiline':False]
['text':'   and N>1 batch dims?','line_number':773,'multiline':False]
['text':' There is an ambiguity here when the batch dims are not at the front of','line_number':781,'multiline':False]
['text':' the tensor.','line_number':782,'multiline':False]
['text':' >>> x = torch.randn(3, B0, 5)','line_number':783,'multiline':False]
['text':' >>> y = vmap(lambda x: x.clone(torch.contiguous_format), in_dims=1, out_dims=0)(x)','line_number':784,'multiline':False]
['text':' >>> y[0].is_contiguous()','line_number':785,'multiline':False]
['text':' ???','line_number':786,'multiline':False]
['text':' Should we make the whole tensor contiguous, or should we','line_number':787,'multiline':False]
['text':' make the non-batch dims contiguous? We've chosen the latter because','line_number':788,'multiline':False]
['text':' philosophically vmap hides the batch dims and operates on a per-sample level.','line_number':789,'multiline':False]
['text':' Note [Batching rules for matmul-like operators]','line_number':802,'multiline':False]
['text':' at::matmul doesn't "de-expand" arguments to get better performance (maybe','line_number':803,'multiline':False]
['text':' it should). In the batching rules for matmul-like operators (dot, mv, mm),','line_number':804,'multiline':False]
['text':' we should be careful not to expand any unnecessary dimensions. e.g., if','line_number':805,'multiline':False]
['text':' only one of the two arguments is a BatchedTensor, then we should try','line_number':806,'multiline':False]
['text':' not to expand batch dimensions onto the other arg.','line_number':807,'multiline':False]
['text':' A shape checking API would be nice...','line_number':812,'multiline':False]
['text':' See Note [Batching rules for matmul-like operators] for why we have cases','line_number':818,'multiline':False]
['text':' self_physical: [L, K], other_physical: [..., K]','line_number':825,'multiline':False]
['text':' We view the tensors as [L, K], [..., K, 1], perform matmul to get','line_number':826,'multiline':False]
['text':' a tensor of size [..., L, 1], and unsqueeze the last dim.','line_number':827,'multiline':False]
['text':' self_physical: [..., L, K], other_physical: [..., K]','line_number':833,'multiline':False]
['text':' We view the tensors as [..., L, K], [..., K, 1], perform matmul to get','line_number':834,'multiline':False]
['text':' a tensor of size [..., L, 1], and unsqueeze the last dim.','line_number':835,'multiline':False]
['text':'logical','line_number':860,'multiline':True]
['text':'logical','line_number':860,'multiline':True]
['text':' See Note [Batching rules for matmul-like operators] for why we have cases','line_number':865,'multiline':False]
['text':' self_physical: [..., K], other_physical: [K]','line_number':867,'multiline':False]
['text':' View the tensors as [..., 1, K] and [K], perform matmul, and unsqueeze.','line_number':868,'multiline':False]
['text':' self_physical: [K], other_physical: [..., K]','line_number':874,'multiline':False]
['text':' View the tensors as [K] and [..., K, 1], perform matmul, and unsqueeze.','line_number':875,'multiline':False]
['text':' self_physical: [..., K], other_physical: [..., K]','line_number':881,'multiline':False]
['text':' View the tensors as [..., 1, K] and [..., K, 1], perform matmul, and unsqueeze.','line_number':882,'multiline':False]
['text':'logical','line_number':893,'multiline':True]
['text':'logical','line_number':893,'multiline':True]
['text':'logical','line_number':907,'multiline':True]
['text':'logical','line_number':907,'multiline':True]
['text':' See Note [Batching rules for matmul-like operators] for why we have cases','line_number':912,'multiline':False]
['text':' NB: stack wraps the dimensionality to (logical dim + 1), so we have to','line_number':947,'multiline':False]
['text':' manually handle that here.','line_number':948,'multiline':False]
['text':'logical','line_number':950,'multiline':True]
['text':' I am quite sad that we need to register operators with exploded TensorOptions,','line_number':955,'multiline':False]
['text':' even though the native:: implementations can use TensorOptions&.','line_number':956,'multiline':False]
['text':' This also makes it hard to metaprogram: i.e., we can't use','line_number':957,'multiline':False]
['text':' unwrap_and_call<..., at::to> because at::to takes TensorOptions& (!!)','line_number':958,'multiline':False]
['text':' Let [B0, B1, B2] be the shape of the batch dims. We're going to create','line_number':1020,'multiline':False]
['text':' the batch dimensions at the front of the tensor (in memory layout),','line_number':1021,'multiline':False]
['text':' irrespective of whether or not they are actually at the front (in memory layout)','line_number':1022,'multiline':False]
['text':' in the original `self` tensor. This is because when a user calls','line_number':1023,'multiline':False]
['text':' `new_empty_strided` in general, the `strides` they provide are for a new','line_number':1024,'multiline':False]
['text':' tensor and have no relation to the strides of the original tensor.','line_number':1025,'multiline':False]
['text':'','line_number':1026,'multiline':False]
['text':' So, the physical shape of the result should be ([B0, B1, B2] + size),','line_number':1027,'multiline':False]
['text':' but what about the physical strides?','line_number':1028,'multiline':False]
['text':'','line_number':1029,'multiline':False]
['text':' We're actually free to pick whatever stride we want:','line_number':1030,'multiline':False]
['text':' e.g., for size=[5, 3], stride=[0, 1], we could decide to','line_number':1031,'multiline':False]
['text':' use','line_number':1032,'multiline':False]
['text':' - physical size: [B0, B1, B2, 5, 3]','line_number':1033,'multiline':False]
['text':' - physical stride: [9999*B1*B2, 9999*B2, 9999, 0, 1]','line_number':1034,'multiline':False]
['text':'','line_number':1035,'multiline':False]
['text':' Let's select some reasonable strides such that:','line_number':1036,'multiline':False]
['text':' - The batch dims are "contiguous" with respect to each other','line_number':1037,'multiline':False]
['text':' - if empty_strided(size, stride) would have created a contiguous Tensor,','line_number':1038,'multiline':False]
['text':' then this new physical Tensor (with batch dims) is also contiguous','line_number':1039,'multiline':False]
['text':'','line_number':1040,'multiline':False]
['text':' Let S be the size of the storage if one were to construct a tensor','line_number':1041,'multiline':False]
['text':' with `size` and `stride` via empty_strided(size, stride).','line_number':1042,'multiline':False]
['text':' Then the physical sizes/strides should be:','line_number':1043,'multiline':False]
['text':' - physical size: [B0, B1, B2, 5, 3]','line_number':1044,'multiline':False]
['text':' - physical stride: [B1 * B2 * S, B2 * S, S, 0, 1]','line_number':1045,'multiline':False]
['text':' physical_strides = [B1 * B2 * S, B2 * S, S]','line_number':1049,'multiline':False]
['text':' physical_strides = [B1 * B2 * S, B2 * S, S] + strides','line_number':1060,'multiline':False]
['text':' NB: Ideally we would like some operators, like size.int, to "fallthrough"','line_number':1080,'multiline':False]
['text':' to the underlying implementation. However, because a BatchedTensor is a','line_number':1081,'multiline':False]
['text':' Tensor wrapper, it only has one dispatch key (Batched) on it. The resolution','line_number':1082,'multiline':False]
['text':' here is to just directly call the underlying implementation.','line_number':1083,'multiline':False]
['text':' inplace operations','line_number':1095,'multiline':False]
['text':' view operations','line_number':1100,'multiline':False]
['text':' composite wrt autograd','line_number':1107,'multiline':False]
['text':' composite wrt autograd','line_number':1109,'multiline':False]
['text':' There is another variant of narrow.  However, we don't','line_number':1110,'multiline':False]
['text':' want to support the other variant yet bc it isn't documented...','line_number':1111,'multiline':False]
['text':' composite wrt autograd','line_number':1112,'multiline':False]
['text':' composite wrt autograd','line_number':1113,'multiline':False]
['text':' composite wrt autograd','line_number':1114,'multiline':False]
['text':' composite wrt autograd','line_number':1115,'multiline':False]
['text':' composite wrt autograd','line_number':1116,'multiline':False]
['text':' composite wrt autograd','line_number':1120,'multiline':False]
['text':' composite wrt autograd','line_number':1129,'multiline':False]
['text':' composite wrt autograd','line_number':1136,'multiline':False]
['text':' clamp operations','line_number':1138,'multiline':False]
['text':' unary pointwise, out-of-place, no additional arguments.','line_number':1143,'multiline':False]
['text':' at::pow has three out-of-place overloads','line_number':1218,'multiline':False]
['text':' for at::result_type, call the native::result_type implementation.','line_number':1231,'multiline':False]
['text':' We don't have to do anything special because native::result_type operates','line_number':1232,'multiline':False]
['text':' on the logical shape of the tensors.','line_number':1233,'multiline':False]
['text':' complex number view operators','line_number':1245,'multiline':False]
['text':' matmul-like operators','line_number':1256,'multiline':False]
['text':' cat/stack','line_number':1262,'multiline':False]
['text':' backward operators','line_number':1266,'multiline':False]
['text':' Tensor.new_* operators','line_number':1272,'multiline':False]
['text':' Comparison ops','line_number':1279,'multiline':False]
['text':' namespace at','line_number':1294,'multiline':False]
