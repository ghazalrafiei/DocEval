['text':'mean=','line_number':69,'multiline':True]
['text':'rstd=','line_number':69,'multiline':True]
['text':' optional ','line_number':74,'multiline':True]
['text':' optional ','line_number':74,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':76,'multiline':False]
['text':' dtype ','line_number':96,'multiline':True]
['text':' layout ','line_number':97,'multiline':True]
['text':' device ','line_number':98,'multiline':True]
['text':' pin_memory ','line_number':99,'multiline':True]
['text':' optional ','line_number':115,'multiline':True]
['text':' optional ','line_number':116,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':118,'multiline':False]
['text':' dtype ','line_number':139,'multiline':True]
['text':' layout ','line_number':140,'multiline':True]
['text':' device ','line_number':141,'multiline':True]
['text':' pin_memory ','line_number':142,'multiline':True]
['text':' dtype ','line_number':148,'multiline':True]
['text':' layout ','line_number':149,'multiline':True]
['text':' device ','line_number':150,'multiline':True]
['text':' pin_memory ','line_number':151,'multiline':True]
['text':' dtype ','line_number':155,'multiline':True]
['text':' layout ','line_number':156,'multiline':True]
['text':' device ','line_number':157,'multiline':True]
['text':' pin_memory ','line_number':158,'multiline':True]
['text':' dtype ','line_number':164,'multiline':True]
['text':' layout ','line_number':165,'multiline':True]
['text':' device ','line_number':166,'multiline':True]
['text':' pin_memory ','line_number':167,'multiline':True]
['text':' dtype ','line_number':171,'multiline':True]
['text':' layout ','line_number':172,'multiline':True]
['text':' device ','line_number':173,'multiline':True]
['text':' pin_memory ','line_number':174,'multiline':True]
['text':' optional ','line_number':186,'multiline':True]
['text':' optional ','line_number':186,'multiline':True]
['text':' cudnn_enable, deprecated ','line_number':188,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':189,'multiline':False]
['text':' Ported from pytorch/xla repo','line_number':201,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':206,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':220,'multiline':False]
['text':' Properly handle zero-size inputs: the view(1, M, -1) call below breaks on this.','line_number':223,'multiline':False]
['text':' Unlike Batch Normalization, which applies scalar scale and bias for each','line_number':233,'multiline':False]
['text':' entire channel/plane with the affine option, Layer Normalization applies','line_number':234,'multiline':False]
['text':' per-element scale and bias. E.g. For input {N, C, H, W}, weight for','line_number':235,'multiline':False]
['text':' batchnorm has shape {C} while weight for layernorm has shape {H, W} or {W}.','line_number':236,'multiline':False]
['text':'weight=','line_number':238,'multiline':True]
['text':'bias=','line_number':238,'multiline':True]
['text':'running_mean=','line_number':238,'multiline':True]
['text':'running_var=','line_number':239,'multiline':True]
['text':'training=','line_number':239,'multiline':True]
['text':'momentum=','line_number':239,'multiline':True]
['text':' namespace native','line_number':262,'multiline':False]
['text':' namespace at','line_number':263,'multiline':False]
