['text':' previously, size [0] tensors were the only possible empty tensors; thus, it wasn't possible','line_number':244,'multiline':False]
['text':' to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors','line_number':245,'multiline':False]
['text':' to be "skipped".  We maintain this behavior for backwards compatibility, but only for this specific','line_number':246,'multiline':False]
['text':' size (i.e. other empty sizes are not skipped).','line_number':247,'multiline':False]
['text':' Checking names before the actual dimensions.','line_number':253,'multiline':False]
['text':' Look for the first valid tensor.','line_number':259,'multiline':False]
['text':' Compute what the output dtype should be:','line_number':273,'multiline':False]
['text':' If the output tensor is defined, we need to take it into account','line_number':278,'multiline':False]
['text':' when computing the actual output dtype and the flags.','line_number':279,'multiline':False]
['text':' Check for type promotion, if the output tensor is defined.','line_number':281,'multiline':False]
['text':' Fallback 'set_output' parameters.','line_number':290,'multiline':False]
['text':' (in case we don't find a valid tensor)','line_number':291,'multiline':False]
['text':' If we found a valid tensor, check whether the input tensors','line_number':297,'multiline':False]
['text':' are compatible, i.e. we can execute `cat` on them.','line_number':298,'multiline':False]
['text':' Compute the output tensor size.','line_number':304,'multiline':False]
['text':' It should have the same shape as any other valid tensor,','line_number':305,'multiline':False]
['text':' except in the dimension 'dim'.','line_number':306,'multiline':False]
['text':' Actually set the output.','line_number':323,'multiline':False]
['text':' Checks for overlaps between the inputs and the output tensor.','line_number':332,'multiline':False]
['text':' namespace meta','line_number':348,'multiline':False]
['text':' unify with cuda implementation?  This is not done to avoid a dispatch in resize_impl_cpu_','line_number':376,'multiline':False]
['text':' We can re-use this kernel for the meta device.','line_number':383,'multiline':False]
['text':' We just need to make sure we don't actually try to resize the (null) storage.','line_number':384,'multiline':False]
['text':'resize_storage=','line_number':385,'multiline':True]
['text':' TODO: dedupe this with empty() symbolic logic','line_number':394,'multiline':False]
['text':' TODO: max with 1','line_number':401,'multiline':False]
['text':' Run this before storage setting so we can access numel','line_number':408,'multiline':False]
['text':' Matches maybe_resize_storage_cpu no-numel behavior','line_number':411,'multiline':False]
['text':' maybe_resize_storage_cpu can handle no storage exists at all but','line_number':413,'multiline':False]
['text':' that should never be the case here','line_number':414,'multiline':False]
['text':' All meta data pointers are the same, so we don't have to "re" allocate','line_number':417,'multiline':False]
['text':' it.  TODO: Actually this might not quite be correct if we use special','line_number':418,'multiline':False]
['text':' pointers to track whether or not fake cuda tensors are pinned or not','line_number':419,'multiline':False]
['text':' this needs to be split along CPU/CUDA lines because we don't have a consistent','line_number':440,'multiline':False]
['text':' way of getting the allocator to use for a device (c10::GetAllocator is not','line_number':441,'multiline':False]
['text':' the same as at::cuda::getCUDADeviceAllocator().','line_number':442,'multiline':False]
['text':' We can't re-use the cpu kernel here because we don't want to use the cpu allocator.','line_number':455,'multiline':False]
['text':' to_broadcast conserves is_coalesced property iff only the last','line_number':510,'multiline':False]
['text':' sparse dimensions are expaned. Possible expansion of dense','line_number':511,'multiline':False]
['text':' dimensions can be discarded as it does not affect the is_coalesce','line_number':512,'multiline':False]
['text':' property.','line_number':513,'multiline':False]
['text':' ones(broadcast_sizes).nonzero() is equivalent to','line_number':528,'multiline':False]
['text':' product(map(arange, broadcast_sizes)) but avoids creating','line_number':529,'multiline':False]
['text':' auxilary arange tensors','line_number':530,'multiline':False]
['text':' fast path for single thread when both inputs and result are contiguous and','line_number':585,'multiline':False]
['text':' not empty, and concat dim is 0','line_number':586,'multiline':False]
['text':' TODO: Add fast cat for higher dimensions and support multi-threaded fast cat','line_number':592,'multiline':False]
['text':' fast path for single thread when both inputs and result are contiguous and not empty','line_number':595,'multiline':False]
['text':' Already checked above','line_number':638,'multiline':False]
['text':' torch.concat, alias for torch.cat','line_number':662,'multiline':False]
['text':' torch.concatenate, alias for torch.cat','line_number':679,'multiline':False]
['text':' should already be wrapped ','line_number':696,'multiline':True]
['text':' Check to see if the shape of tensors is compatible','line_number':708,'multiline':False]
['text':' for being concatenated along a given dimension.','line_number':709,'multiline':False]
['text':' used only for debug messages ','line_number':711,'multiline':True]
['text':' We now need to move the indices of each','line_number':743,'multiline':False]
['text':' input tensor up along `dim` by an appropriate amount.','line_number':744,'multiline':False]
['text':' E.g., if t1 has indices [[2,3,4],[5,6,7]],','line_number':745,'multiline':False]
['text':' and sizes [10, 7]','line_number':746,'multiline':False]
['text':' then torch.cat((t1,t1,t1),1) should have indices','line_number':747,'multiline':False]
['text':' [[2,3,4,2,3,4,2,3,4],[5,6,7,12,13,14,19,20,21]],','line_number':748,'multiline':False]
['text':' so we need to increase idxs[1][3:6] by 7','line_number':749,'multiline':False]
['text':' and idxs[1][6:9] by 14.','line_number':750,'multiline':False]
['text':' cumulative_offset is zero for the first piece, so','line_number':756,'multiline':False]
['text':' don't waste time doing this operation unless i > 0.','line_number':757,'multiline':False]
['text':' Catting along a dense dimension requires us to create new values.','line_number':776,'multiline':False]
['text':' For illustration, consider the sparse 3d tensors t1 and t2,','line_number':777,'multiline':False]
['text':' given by t1 = [[[1,2],[3,4]], ... (zeros) ..., [[5,6],[7,8]]]','line_number':778,'multiline':False]
['text':' and t2 = [... (zeros) ..., [[9, 10], [11,12]], ... (zeros) ...],','line_number':779,'multiline':False]
['text':' Their concatenation along dimension 2 is:','line_number':780,'multiline':False]
['text':' [[[1,2,0,0],[3,4,0,0]], ... (zeros) ..., [[0,0,9,10],[0,0,11,12]], ... (zeros) ..., [[5,6,0,0],[7,8,0,0]]]','line_number':781,'multiline':False]
['text':'','line_number':782,'multiline':False]
['text':' Their values tensors are, respectively,','line_number':783,'multiline':False]
['text':' [[[1,2],[3,4]],[[5,6],[7,8]]] and [[[9,10],[11,12]]].','line_number':784,'multiline':False]
['text':'','line_number':785,'multiline':False]
['text':' and so the values tensor of their concatenation along dim 2 will be:','line_number':786,'multiline':False]
['text':' [[[1,2,0,0],[3,4,0,0]],[[5,6,0,0],[7,8,0,0]],[[0,0,9,10],[0,0,11,12]]]','line_number':787,'multiline':False]
['text':'','line_number':788,'multiline':False]
['text':' which we can get by taking the values tensor of each tensor, catting it with zeros of the appropriate size on the left and right,','line_number':789,'multiline':False]
['text':' and then catting all those results together.','line_number':790,'multiline':False]
['text':' The dimension in each tensor's values object that corresponds to the overall dimension along which we're catting.','line_number':792,'multiline':False]
['text':' The final size along the catted dimension.','line_number':794,'multiline':False]
['text':' dimension 0 of values corresponds to the number of values,','line_number':809,'multiline':False]
['text':' rather than to any logical dimension of the sparse tensor.','line_number':810,'multiline':False]
['text':' This can create an uncoalesced tensor','line_number':832,'multiline':False]
['text':' Sum the dimensions of the tensors, check tensor sizes,','line_number':876,'multiline':False]
['text':' and expand all 0-D and 1-D tensors so that everything','line_number':877,'multiline':False]
['text':' is 2-D','line_number':878,'multiline':False]
['text':' Switching dim 0 to dim 1 is intentional','line_number':896,'multiline':False]
['text':' Copy each tensor into the appropriate location in the result matrix','line_number':914,'multiline':False]
['text':' We need to call split_with_sizes in the case where split_size and dimension size are 0, because','line_number':936,'multiline':False]
['text':' a call to split would discard the number of chunks (because we can have an arbitrary number of','line_number':937,'multiline':False]
['text':' 0-sized chunks adding up to 0).  So, call split_with_sizes with the correct number of chunks,','line_number':938,'multiline':False]
['text':' eventually we will do this for all cases.','line_number':939,'multiline':False]
['text':' NB: intentional, sections specifies number of output tensors, which','line_number':952,'multiline':False]
['text':' cannot be polymorphic','line_number':953,'multiline':False]
['text':' indices tensor could be non-contiguous','line_number':1014,'multiline':False]
['text':' See the comment above in chunk(...)','line_number':1030,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1052,'multiline':False]
['text':' compute storage offset and size for the diagonal','line_number':1055,'multiline':False]
['text':' for positive values of offset (above the main diagonal)','line_number':1056,'multiline':False]
['text':' "leftmost columns" (along dim2) are dropped','line_number':1057,'multiline':False]
['text':' for negative values of offset (below the main diagonal)','line_number':1058,'multiline':False]
['text':' "topmost rows" (along dim1) are dropped.','line_number':1059,'multiline':False]
['text':' Note that we invert +/- in the second to absorb the negative','line_number':1060,'multiline':False]
['text':' sign in the offset.','line_number':1061,'multiline':False]
['text':' NumPy allows you to specify offsets "off the end"; let's just be careful not to','line_number':1068,'multiline':False]
['text':' set a ridiculous storage_offset in that case (technically it shouldn't matter','line_number':1069,'multiline':False]
['text':' because there are no elements in the tensor, but let's be kosher).','line_number':1070,'multiline':False]
['text':' skip','line_number':1072,'multiline':False]
['text':' construct new size and stride: we drop dim1 and dim2 (maximum first for not changing the index of the minimum)','line_number':1079,'multiline':False]
['text':' the new ("joint") dimension is appended to the end of the shape / stride to match numpy semantics','line_number':1080,'multiline':False]
['text':' return view with new parameters','line_number':1090,'multiline':False]
['text':' This is slower than it needs to be because there is no way to modify','line_number':1104,'multiline':False]
['text':' the names of a tensor in-place right now. In the future we should consider','line_number':1105,'multiline':False]
['text':' offering that functionality.','line_number':1106,'multiline':False]
['text':'unused','line_number':1128,'multiline':True]
['text':' We currently do not support per-channel quant for unfold, diagonal, expand, permute.','line_number':1156,'multiline':False]
['text':' TODO: Make this an aten function and replace as_strided_qtensorimpl once that is done.','line_number':1157,'multiline':False]
['text':' NB: The reason this is unchecked is to ensure we don't generate','line_number':1196,'multiline':False]
['text':' guards on the base storage itself when performing as_strided calls.','line_number':1197,'multiline':False]
['text':' Although technically these guards are necessary, in practice they','line_number':1198,'multiline':False]
['text':' cause a lot of guards that falsely refer to base symbols.  We will instead','line_number':1199,'multiline':False]
['text':' rely on AOTAutograd to sort out if we actually have dependence on view','line_number':1200,'multiline':False]
['text':' bases / storage size.','line_number':1201,'multiline':False]
['text':' This is an overloaded function similar to','line_number':1218,'multiline':False]
['text':' Tensor as_strided_qtensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_)','line_number':1219,'multiline':False]
['text':' and is currently not available through the dispatcher. The additional','line_number':1220,'multiline':False]
['text':' input, quantizer, is called by the select & slice methods.','line_number':1221,'multiline':False]
['text':' TODO: Make this function compatible with the dispatcher','line_number':1222,'multiline':False]
['text':' Should just use narrow_copy_out, but this API is used internally at Meta:','line_number':1246,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/87045#issuecomment-1309353561','line_number':1247,'multiline':False]
['text':' narrow_copy_dense_cpu_out always resize output's size, so there only create','line_number':1249,'multiline':False]
['text':' a zero size tensor.','line_number':1250,'multiline':False]
['text':' This means we are narrowing on a dense dim, which is in effect just a
        regular narrow on _values() ','line_number':1279,'multiline':True]
['text':' Should just use narrow_copy_out, but this API is used internally at Meta:','line_number':1289,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/87045#issuecomment-1309353561','line_number':1290,'multiline':False]
['text':' wrap dim if negative and do bound check','line_number':1301,'multiline':False]
['text':' wrap start and do bound check','line_number':1308,'multiline':False]
['text':' resize output','line_number':1327,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':1332,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)','line_number':1337,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)','line_number':1339,'multiline':False]
['text':' This overload exists purely for XLA, because they wanted to pass in "symbolic"','line_number':1407,'multiline':False]
['text':' start via Tensor.','line_number':1408,'multiline':False]
['text':'includeBool=','line_number':1410,'multiline':True]
['text':' creates new indices. It is possible to avoid that if COO','line_number':1493,'multiline':False]
['text':' is allowed to store a permutation vector.','line_number':1494,'multiline':False]
['text':' TODO: apply `is_coalesced ||= new_values.size(0) < 2`.','line_number':1508,'multiline':False]
['text':' Add new leading dimensions to the tensor if the','line_number':1517,'multiline':False]
['text':' number of target dimensions is larger than the','line_number':1518,'multiline':False]
['text':' number of source dimensions.','line_number':1519,'multiline':False]
['text':' return an empty tensor if one of the repeat dimensions is zero','line_number':1541,'multiline':False]
['text':' can't unfold with step 0, so make sure step is at least 1','line_number':1548,'multiline':False]
['text':' (it doesn't matter what it is in that case, because the size is 0).','line_number':1549,'multiline':False]
['text':' If self.size() > len(reps), reps is promoted to self.size() by pre-pending','line_number':1560,'multiline':False]
['text':' 1’s to it to keep the same behaviour as `numpy.tile`.','line_number':1561,'multiline':False]
['text':' Thus for a tensor of shape (2, 3, 4, 5), a dims of (2, 2) is treated','line_number':1562,'multiline':False]
['text':' as (1, 1, 2, 2).','line_number':1563,'multiline':False]
['text':' `torch.tile` is equivalent to the already implemented `torch.Tensor.repeat`','line_number':1572,'multiline':False]
['text':'','line_number':1576,'multiline':False]
['text':' templated for ArrayRef<int64_t> and SmallVector<int64_t> use cases','line_number':1577,'multiline':False]
['text':'','line_number':1578,'multiline':False]
['text':'caller should make sure that sizes and strides are valid for self','line_number':1584,'multiline':False]
['text':'(storage is sufficient, strides are non-negative, strides and sizes array size is the same)','line_number':1585,'multiline':False]
['text':' specialization for symbolic shapes and strides.','line_number':1604,'multiline':False]
['text':' SymIntArrayRef/ArrayRef<c10::SymInt> and SmallVector<c10::SymInt>/SymDimVector','line_number':1605,'multiline':False]
['text':'caller should make sure that sizes and strides are valid for self','line_number':1611,'multiline':False]
['text':'(storage is sufficient, strides are non-negative, strides and sizes array size is the same)','line_number':1612,'multiline':False]
['text':' `computeStride` returns the proper strides to use if this','line_number':1642,'multiline':False]
['text':' `reshape` can be just a view.','line_number':1643,'multiline':False]
['text':' NB: Even though we have viewable geometry and the target strides here,','line_number':1646,'multiline':False]
['text':'     we do not just call `as_strided` on `self` because the backward','line_number':1647,'multiline':False]
['text':'     for `as_strided` is not as efficient as that of `view` (since the','line_number':1648,'multiline':False]
['text':'     former is meant to handle general cases).','line_number':1649,'multiline':False]
['text':'','line_number':1650,'multiline':False]
['text':'     Similarly we don't call `view` because it duplicates some of the work','line_number':1651,'multiline':False]
['text':'     we've already done, and instead call our internal/private operator','line_number':1652,'multiline':False]
['text':'     `_reshape_alias` that essentially does the same thing as `view` and','line_number':1653,'multiline':False]
['text':'     `as_strided` without any of the extra overhead.','line_number':1654,'multiline':False]
['text':' Temporary check to revert to the old behavior/view in cases where the','line_number':1656,'multiline':False]
['text':' device is not supported (e.g. for XLA the operation is not supported','line_number':1657,'multiline':False]
['text':' so we use `view` instead).','line_number':1658,'multiline':False]
['text':'','line_number':1659,'multiline':False]
['text':' We need to do the checks here instead of in `native_functions.yaml`','line_number':1660,'multiline':False]
['text':' to preserve backwards compatibility.','line_number':1661,'multiline':False]
['text':' Duplicate of above code for non-symbolic ints. Kept for BC purposes and to','line_number':1688,'multiline':False]
['text':' minimize breakages.','line_number':1689,'multiline':False]
['text':' `computeStride` returns the proper strides to use if this','line_number':1700,'multiline':False]
['text':' `reshape` can be just a view.','line_number':1701,'multiline':False]
['text':' NB: Even though we have viewable geometry and the target strides here,','line_number':1704,'multiline':False]
['text':'     we do not just call `as_strided` on `self` because the backward','line_number':1705,'multiline':False]
['text':'     for `as_strided` is not as efficient as that of `view` (since the','line_number':1706,'multiline':False]
['text':'     former is meant to handle general cases).','line_number':1707,'multiline':False]
['text':'','line_number':1708,'multiline':False]
['text':'     Similarly we don't call `view` because it duplicates some of the work','line_number':1709,'multiline':False]
['text':'     we've already done, and instead call our internal/private operator','line_number':1710,'multiline':False]
['text':'     `_reshape_alias` that essentially does the same thing as `view` and','line_number':1711,'multiline':False]
['text':'     `as_strided` without any of the extra overhead.','line_number':1712,'multiline':False]
['text':' Temporary check to revert to the old behavior/view in cases where the','line_number':1714,'multiline':False]
['text':' device is not supported (e.g. for XLA the operation is not supported','line_number':1715,'multiline':False]
['text':' so we use `view` instead).','line_number':1716,'multiline':False]
['text':'','line_number':1717,'multiline':False]
['text':' We need to do the checks here instead of in `native_functions.yaml`','line_number':1718,'multiline':False]
['text':' to preserve backwards compatibility.','line_number':1719,'multiline':False]
['text':' This is only used by `reshape` in cases where it would otherwise have dispatched','line_number':1730,'multiline':False]
['text':' to `view`. This removes the overhead of calling `view` which duplicates some of','line_number':1731,'multiline':False]
['text':' the work that's already been done (`infer_size_dv` and `computeStride`).','line_number':1732,'multiline':False]
['text':' return dense part:','line_number':1755,'multiline':False]
['text':' sum promotes integral type to int64 when dtype is not specified.','line_number':1759,'multiline':False]
['text':' dtype ','line_number':1766,'multiline':True]
['text':' layout ','line_number':1767,'multiline':True]
['text':' pin_memory ','line_number':1769,'multiline':True]
['text':' this is an auxiliary function, called by the select&slice methods, that','line_number':1783,'multiline':False]
['text':' creates a new quantizer from the given input','line_number':1784,'multiline':False]
['text':' is_select is true if calling function is select()','line_number':1785,'multiline':False]
['text':' Compute scales&zps for sub-tensor','line_number':1798,'multiline':False]
['text':' *.select(0, start) could alternatively be replaced with *.slice(0, start, end, step), but','line_number':1799,'multiline':False]
['text':' select has less overhead','line_number':1800,'multiline':False]
['text':' Axis only needs to be adjusted if the calling function is select(), since select() reduces','line_number':1805,'multiline':False]
['text':' the number of dimensions of the tensor by 1, and remains unchanged if calling function is slice()','line_number':1806,'multiline':False]
['text':'
    Algorithm:
    index - a 1-D tensor of indicies with shape (n,)
    self - sparse tensor, its shape is sizes = sparse_shape + dense_shape
      indices - 2-D tensor of indices, shape is (sparse_dims, nnz)
      values - (1+len(dense_shape))-D tensor of values, shape is (nnz,) + dense_shape
    index_select(dim, index) returns a sparse tensor with the following data
      new_sizes = sizes[:dim] + (n,) + sizes[dim+1:]
      new_indices - shape is (sparse_dims, new_nnz)
      new_values - shape is (new_nnz,) + dense_shape

      if dim < len(sparse_shape):
          # Find new_indices[dim] of the output sparse tensor and
          # indices at which to select values/indices.
          # The CPP code uses (binary/in a count table) search to find matches and may
          # swap the loop order for better algorithmic complexity.
          new_dim_indices = []
          selected_dim_indices = []
          # This is a brute-force algorithms to convey the main idea.
          # The CPP code below is more efficient but more complicated.
          for i, i_idx in enumerate(indices[dim]):
              for j, j_idx in enumerate(index):
                  if i_idx == j_idx:
                      new_dim_indices.append(j)
                      selected_dim_indices.append(i)
          new_indices = indices.index_select(1, selected_dim_indices)
          new_values = values.index_select(0, selected_dim_indices)
          new_indices[dim] = new_dim_indices
      else:
          new_indices = indices
          new_values = values.index_select(dim - sparse_dim + 1, index);
    ','line_number':1878,'multiline':True]
['text':' Equivalent to t.index_select(dim, idx), but vanilla index_select is not parallel,','line_number':1926,'multiline':False]
['text':' so we use gather instead.','line_number':1927,'multiline':False]
['text':' We use this method to select relevant indices/values','line_number':1928,'multiline':False]
['text':' from the intersection between indices[dim] and the index.','line_number':1929,'multiline':False]
['text':' If indexing into sparse dimensions','line_number':1939,'multiline':False]
['text':' short-circuit if index is empty','line_number':1941,'multiline':False]
['text':' nneg_index = (index < 0) * (index + size) + (index >= 0) * index','line_number':1954,'multiline':False]
['text':' Mark self and dim as used if code is compiled with STRIP_ERROR_MESSAGES','line_number':1963,'multiline':False]
['text':' If nnz is smaller than size, then either indices[dim] or index gets sorted,','line_number':1983,'multiline':False]
['text':' then this is followed by a binary search to find interesections.','line_number':1984,'multiline':False]
['text':' if either dim_indices or index requires sorting, we compare','line_number':1994,'multiline':False]
['text':' the cost of sort + binary search, which is comparing','line_number':1995,'multiline':False]
['text':' (len(dim_indices) + len(index)) * log(len(index)) to','line_number':1996,'multiline':False]
['text':' (len(dim_indices) + len(index)) * log(len(dim_indices)).','line_number':1997,'multiline':False]
['text':' That simplifies to comparing len(dim_indices) to len(index).','line_number':1998,'multiline':False]
['text':' Additionally, we take into consideration potential parallel','line_number':1999,'multiline':False]
['text':' speedup.','line_number':2000,'multiline':False]
['text':' if self is coalesced and dim is 0, then we compare','line_number':2002,'multiline':False]
['text':' index_len * log(len(dim_indices)), which is binary search into dim_indices,','line_number':2003,'multiline':False]
['text':' to (len(index_len) + len(dim_indices)) * log(index_len).','line_number':2004,'multiline':False]
['text':' Additionally, we take into consideration potential parallel','line_number':2005,'multiline':False]
['text':' speedup.','line_number':2006,'multiline':False]
['text':' src is a source of indices to binary search in sorted','line_number':2012,'multiline':False]
['text':' sort dim_indices to binary search into it','line_number':2018,'multiline':False]
['text':' dim_indices is already sorted if self is coalesced and dim == 0','line_number':2020,'multiline':False]
['text':' sort nneg_index to binary search into it','line_number':2030,'multiline':False]
['text':' 1 <= n_threads_src <= std::min(ceil(src.numel() / src_grain_size), max_threads)','line_number':2041,'multiline':False]
['text':' src_int_idx and sorted_int_idx store "i" and "j" indices indicating','line_number':2050,'multiline':False]
['text':' intersections such that src_int_idx[i] == sorted_int_idx[j].','line_number':2051,'multiline':False]
['text':' These intersections are found with binary search and in parallel.','line_number':2052,'multiline':False]
['text':' For each element "i" from src, int_counts define how many','line_number':2055,'multiline':False]
['text':' elements there are in sorted, i.e. "j" indices, corresponding','line_number':2056,'multiline':False]
['text':' to "i", i.e.:','line_number':2057,'multiline':False]
['text':' |{j : src_int_idx[i] == sorted_int_idx[j]}| for each i in src_int_idx.','line_number':2058,'multiline':False]
['text':' fill in src_int_idx, sorted_int_idx, int_counts','line_number':2061,'multiline':False]
['text':' We cannot just use *src_val_lb != src_val because when','line_number':2079,'multiline':False]
['text':' src_val_lb == ptr_sorted_end, dereferencing past-the-end value','line_number':2080,'multiline':False]
['text':' is not well-defined.','line_number':2081,'multiline':False]
['text':' Short-circuit if empty intersection','line_number':2103,'multiline':False]
['text':' Now that we know "i", "j" and the counts, we "unflatten"','line_number':2109,'multiline':False]
['text':' them into two arrays of intersection indices such that','line_number':2110,'multiline':False]
['text':' selected_src = repeat_interleave(src_int_idx, int_counts),','line_number':2111,'multiline':False]
['text':' and selected_sorted is obtained as follows:','line_number':2112,'multiline':False]
['text':' offsets = int_counts.cumsum(0).sub_(int_counts)','line_number':2113,'multiline':False]
['text':' for ii, (j, c) in enumerate(zip(sorted_int_idx, int_counts)):','line_number':2114,'multiline':False]
['text':'     out_slice = slice(offsets[ii], offsets[ii] + c)','line_number':2115,'multiline':False]
['text':'     src_slice = slice(j, j + c)','line_number':2116,'multiline':False]
['text':'     selected_sorted[out_slice] = sorted_int_idx[src_slice]','line_number':2117,'multiline':False]
['text':' fill in selected_sorted, selected_src','line_number':2121,'multiline':False]
['text':' Converts a 1d sorted idx to a compressed 1d compressed idx,','line_number':2158,'multiline':False]
['text':' aka crow in the CSR format. Useful to get a count table in','line_number':2159,'multiline':False]
['text':' a parallelized and no-sync manner.','line_number':2160,'multiline':False]
['text':' TODO: this function is equivalent to _convert_indices_from_coo_to_csr.','line_number':2161,'multiline':False]
['text':' The mentioned function is not public yet.','line_number':2162,'multiline':False]
['text':' If nnz is (much) larger than size, then both indices[dim] and index get sorted','line_number':2190,'multiline':False]
['text':' with a count sort (faster, and no huge nnz-sized chunk memory allocations).','line_number':2191,'multiline':False]
['text':' The element-wise product between the count tables gives us all the intersections.','line_number':2192,'multiline':False]
['text':' Writes into counts (must be preallocated and zero)','line_number':2195,'multiline':False]
['text':' and allows to use external buffers.','line_number':2196,'multiline':False]
['text':' 1 <= n_threads <= min(ceil(len / grain_size), max_threads)','line_number':2221,'multiline':False]
['text':'bins=','line_number':2234,'multiline':True]
['text':'is_sorted=','line_number':2235,'multiline':True]
['text':'run_in_parallel=','line_number':2235,'multiline':True]
['text':'is_sorted=','line_number':2243,'multiline':True]
['text':'grain_size = at::internal::GRAIN_SIZE','line_number':2244,'multiline':True]
['text':'is_sorted=','line_number':2250,'multiline':True]
['text':'grain_size = at::internal::GRAIN_SIZE','line_number':2251,'multiline':True]
['text':' Short-circuit if empty intersection','line_number':2259,'multiline':False]
['text':' skip idx value if not in the intersection','line_number':2340,'multiline':False]
['text':' we do not need idx_counts_per_thread anymore,','line_number':2364,'multiline':False]
['text':' so it is safe to do in-place intersection.','line_number':2365,'multiline':False]
['text':' skip if idx_val is not in the intersection','line_number':2397,'multiline':False]
['text':' Brute-force solution for small values of nnz and index_len','line_number':2427,'multiline':False]
['text':' NOTE: if very critical, replace std::vector with','line_number':2443,'multiline':False]
['text':' a data structure that operates on stack up to some limit.','line_number':2444,'multiline':False]
['text':' 16384','line_number':2470,'multiline':False]
['text':' NOTE: such a condition to avoid overflows in (nnz * index_len)','line_number':2471,'multiline':False]
['text':' A more precise decision could be of the form:','line_number':2480,'multiline':False]
['text':' `nnz < C(nnz, size) * size`, but it requires heavy benchmarking.','line_number':2481,'multiline':False]
['text':' We choose `nnz < size`, which measures theoretical complexity','line_number':2482,'multiline':False]
['text':' and does not rely on runtime performance.','line_number':2483,'multiline':False]
['text':' TODO: perform this analysis and find better C(nnz, size).','line_number':2484,'multiline':False]
['text':' If indexing into dense dimensions','line_number':2495,'multiline':False]
['text':' It is sufficient to just perform `index_select` on values','line_number':2497,'multiline':False]
['text':' if `dim` refers to dense dimensions.','line_number':2498,'multiline':False]
['text':' handle optional parameters','line_number':2519,'multiline':False]
['text':' TODO: support negative strides','line_number':2523,'multiline':False]
['text':' round-up','line_number':2544,'multiline':False]
['text':' NB: it is extremely important to perform a redispatch here for','line_number':2552,'multiline':False]
['text':' the MPS backend; if you call directly to as_strided_tensorimpl,','line_number':2553,'multiline':False]
['text':' the necessary metadata for MPS will not get setup and you will','line_number':2554,'multiline':False]
['text':' get silently wrong results','line_number':2555,'multiline':False]
['text':' TODO(Ailing): do we need to set version_counter here?','line_number':2587,'multiline':False]
['text':'version=','line_number':2589,'multiline':True]
['text':' TODO(Ailing): do we need to set version_counter here?','line_number':2642,'multiline':False]
['text':'version=','line_number':2644,'multiline':True]
['text':' Precondition: tensors is non-empty','line_number':2665,'multiline':False]
['text':'skip_overlap_check','line_number':2681,'multiline':True]
['text':' compute the size of the result','line_number':2682,'multiline':False]
['text':' skip resizing if size of result is same as expected','line_number':2686,'multiline':False]
['text':' raise a warning while resizing if output has one or more elements','line_number':2687,'multiline':False]
['text':' at::native::resize_output(result, result_sizes);','line_number':2688,'multiline':False]
['text':' TODO: restore the above, see https://github.com/pytorch/pytorch/issues/64709','line_number':2689,'multiline':False]
['text':' TODO(msubkhankulov): refactor to use _stack','line_number':2722,'multiline':False]
['text':' one can always split a dimension with view','line_number':2732,'multiline':False]
['text':'dim = tensors[0].ndimension() cannot be efficiently handled by view','line_number':2733,'multiline':False]
['text':' CPU specific implementation','line_number':2738,'multiline':False]
['text':' default backend','line_number':2747,'multiline':False]
['text':' TODO(msubkhankulov): refactor to use _stack_out','line_number':2752,'multiline':False]
['text':'can take fast cat path','line_number':2766,'multiline':False]
['text':' swap row0 and row1','line_number':2839,'multiline':False]
['text':' torch.row_stack, alias for torch.vstack','line_number':2855,'multiline':False]
['text':' reshape 0D or 1D tensor t into (t.numel(), 1)','line_number':2867,'multiline':False]
['text':' Sparse COO is an exceptional sparse format as it allows transpose','line_number':2930,'multiline':False]
['text':' to be a view operation which is a convinient property for','line_number':2931,'multiline':False]
['text':' in-place operations. For other sparse formats, the in-place','line_number':2932,'multiline':False]
['text':' transpose would not be possible without shuffling the specified','line_number':2933,'multiline':False]
['text':' values. So we don't support this as it would defeat the purpose','line_number':2934,'multiline':False]
['text':' of in-place opeations of being memory-efficient.','line_number':2935,'multiline':False]
['text':' Transpose implementation for sparse compressed layouts','line_number':2953,'multiline':False]
['text':' NB: We assume that dim1,dim0 have already been wrapped','line_number':2954,'multiline':False]
['text':' In theory it works, but missing to_dense coverage to test','line_number':2974,'multiline':False]
['text':' Classify transpose "type"','line_number':2979,'multiline':False]
['text':' We have validated everything, early exit for equal dims (no effect)','line_number':3023,'multiline':False]
['text':' NB: This code should work, but is untestable due to lack of support for','line_number':3039,'multiline':False]
['text':' dense dimensions in to_dense. The Debug assert is present to emphasize','line_number':3040,'multiline':False]
['text':' the fact that the block should not be possible to hit this code block','line_number':3041,'multiline':False]
['text':' un-blocked: 2 sparse dims map to single nnz dim, so dense dim0/1 are','line_number':3047,'multiline':False]
['text':' one position left','line_number':3048,'multiline':False]
['text':' blocked: 2 sparse dims map to 3 (nnz, ) + blocksize dims, so dense','line_number':3050,'multiline':False]
['text':' dim0/1 are one position right','line_number':3051,'multiline':False]
['text':'if (transpose_type == TransposeDim::Sparse) ','line_number':3053,'multiline':True]
['text':' Flip the layout','line_number':3054,'multiline':False]
['text':' un-blocked: no change to values, layout is flipped.','line_number':3059,'multiline':False]
['text':' blocked: the blocks are nested under the sparse dims so they must be','line_number':3061,'multiline':False]
['text':' transposed as well.','line_number':3062,'multiline':False]
['text':' namespace','line_number':3076,'multiline':False]
['text':' Transpose of a tensor is a view operation.','line_number':3099,'multiline':False]
['text':' Named type instead of a pair/tuple so that we can be sure to','line_number':3182,'multiline':False]
['text':' construct the vectors in place and get NRVO.','line_number':3183,'multiline':False]
['text':' dim is present if squeezing a single dimension and absent if squeezing all dimensions','line_number':3201,'multiline':False]
['text':' TODO: quantized Tensor support for SymInt needs to be added but basic building blocs','line_number':3229,'multiline':False]
['text':' are missing for now.','line_number':3230,'multiline':False]
['text':' NOTE [ Unsafe View ]','line_number':3305,'multiline':False]
['text':' _unsafe_view() differs from view() in that the returned tensor isn't treated','line_number':3306,'multiline':False]
['text':' as a view for the purposes of automatic differentiation. (It's not listed in','line_number':3307,'multiline':False]
['text':' VIEW_FUNCTIONS in gen_inplace_or_view_type.py).  It's only safe to use if the `self` tensor','line_number':3308,'multiline':False]
['text':' is temporary. For example, the viewed tensor here (a + b) is discarded immediately','line_number':3309,'multiline':False]
['text':' after viewing:','line_number':3310,'multiline':False]
['text':'','line_number':3311,'multiline':False]
['text':'  res = at::_unsafe_view(a + b, size);','line_number':3312,'multiline':False]
['text':'','line_number':3313,'multiline':False]
['text':' This is a hack because in-place operations on tensors treated like views','line_number':3314,'multiline':False]
['text':' can be much more expensive than the same operations on non-view tensors.','line_number':3315,'multiline':False]
['text':' We don't want to infer_size on the entire shape, because that can give us an extra degree','line_number':3403,'multiline':False]
['text':' of freedom we don't want; for example, consider shape [0, 1, 3, 0], with start_dim=1, end_dim=2.','line_number':3404,'multiline':False]
['text':' It's clear we want result shape [0, 3, 0] but passing [0, -1, 0] to infer_size means the -1','line_number':3405,'multiline':False]
['text':' can take on any value and satisfy the constraints.','line_number':3406,'multiline':False]
['text':' at::infer_size would throw std::runtime_error for invalid size,','line_number':3492,'multiline':False]
['text':' catch the runtime_error and display the error message in a more user-friendly way','line_number':3493,'multiline':False]
['text':' for both tensors and named tensors','line_number':3494,'multiline':False]
['text':'indexing=','line_number':3551,'multiline':True]
['text':' Input tensors is of type TensorList, which is an alias to a','line_number':3564,'multiline':False]
['text':' constant array slice, which doesn't allow for mutations. We may','line_number':3565,'multiline':False]
['text':' need to swap our first two elements if indexing is "ij", so we','line_number':3566,'multiline':False]
['text':' unconditionally create a vector that we can reorder to keep the','line_number':3567,'multiline':False]
['text':' implementation simple.','line_number':3568,'multiline':False]
['text':'','line_number':3569,'multiline':False]
['text':' We are not concerned with the performance of this relative to','line_number':3570,'multiline':False]
['text':' constructor a grid for each input.','line_number':3571,'multiline':False]
['text':' Whether or not to swap the first two tensors.','line_number':3575,'multiline':False]
['text':'','line_number':3576,'multiline':False]
['text':' We only swap if there are at least two* input tensors (obviously)','line_number':3577,'multiline':False]
['text':' and if indexing is "xy".','line_number':3578,'multiline':False]
['text':'','line_number':3579,'multiline':False]
['text':' A reminder about "xy" semantics: "xy" semantics implies that the','line_number':3580,'multiline':False]
['text':' output grids are in the cartesian coordinate system. Thus the','line_number':3581,'multiline':False]
['text':' first dimension is the "x" axis (corresponding to column) and the','line_number':3582,'multiline':False]
['text':' second dimension is the "y" axis (corresponding to row). Tensors,','line_number':3583,'multiline':False]
['text':' however, generally consider the first axis to be the row and the','line_number':3584,'multiline':False]
['text':' second axis to be the columns. Thus we flip the two dimensions in','line_number':3585,'multiline':False]
['text':' contrast to "ij" indexing.','line_number':3586,'multiline':False]
['text':'','line_number':3587,'multiline':False]
['text':' It turns out that it's easiest to implement this by just swapping','line_number':3588,'multiline':False]
['text':' the first two inputs. However, the order of the outputs still','line_number':3589,'multiline':False]
['text':' must correspond to the order of the inputs. Thus we also must','line_number':3590,'multiline':False]
['text':' swap the outputs if we swapped the inputs.','line_number':3591,'multiline':False]
['text':'','line_number':3592,'multiline':False]
['text':' * Why do we even support this function for exactly one input?','line_number':3593,'multiline':False]
['text':' We can only swap if there are multiple tensors.','line_number':3597,'multiline':False]
['text':' Only "xy" and "ij" are supported, and we already checked for','line_number':3603,'multiline':False]
['text':' "xy" above. Only "ij" remains as a valid mode.','line_number':3604,'multiline':False]
['text':' treat 0D tensors as if they were a 1D tensor','line_number':3614,'multiline':False]
['text':' select this dimension to infer','line_number':3620,'multiline':False]
['text':' restore to previous value','line_number':3622,'multiline':False]
['text':' Remember we need to also swap the outputs if we swapped the inputs.','line_number':3625,'multiline':False]
['text':' Numpy-style `a.T`: returns the tensor','line_number':3632,'multiline':False]
['text':' with dims reversed','line_number':3633,'multiline':False]
['text':' Added in PyTorch 2.0','line_number':3644,'multiline':False]
['text':' Added in PyTorch 2.0','line_number':3657,'multiline':False]
['text':' anonymous namespace','line_number':3681,'multiline':False]
['text':' Added in PyTorch 2.0','line_number':3685,'multiline':False]
['text':'transpose=','line_number':3688,'multiline':True]
['text':' Added in PyTorch 2.0','line_number':3693,'multiline':False]
['text':'transpose=','line_number':3696,'multiline':True]
['text':'transpose=','line_number':3703,'multiline':True]
['text':' NB: detach() is not the same thing as alias()! The main difference is that','line_number':3716,'multiline':False]
['text':' detach does not allow metadata change while alias does.','line_number':3717,'multiline':False]
['text':' NB: The ADInplaceOrView logic will overwrite these with the','line_number':3719,'multiline':False]
['text':' appropriate values if it runs; otherwise these are the values.','line_number':3720,'multiline':False]
['text':'version_counter=','line_number':3721,'multiline':True]
['text':'allow_tensor_metadata_change=','line_number':3722,'multiline':True]
['text':' some special handling to deal with allow d == 0 when self.dim() == 0','line_number':3726,'multiline':False]
['text':'wrap_scalar=','line_number':3728,'multiline':True]
['text':' The if handles the self.dim() == 0 case','line_number':3738,'multiline':False]
['text':' We return a copy of the diagonal','line_number':3752,'multiline':False]
['text':' handle the case of scalar tensor as a no-op','line_number':3804,'multiline':False]
['text':' TODO: The algorithm below can probably be optimized.','line_number':3808,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/pull/41480#discussion_r456100505','line_number':3809,'multiline':False]
['text':' Algorithm Walkthrough','line_number':3811,'multiline':False]
['text':' Example Input','line_number':3812,'multiline':False]
['text':' Variable State:','line_number':3813,'multiline':False]
['text':'     normalized_src = 0, 1','line_number':3814,'multiline':False]
['text':'     normalized_dst = 2, 4','line_number':3815,'multiline':False]
['text':'     self_dim = 5','line_number':3816,'multiline':False]
['text':' We initialize two vectors to track update to the dims','line_number':3821,'multiline':False]
['text':' `order` contains the final order of the dim positions.','line_number':3822,'multiline':False]
['text':' Variable State:','line_number':3823,'multiline':False]
['text':'     order = NA, NA, NA, NA, NA','line_number':3824,'multiline':False]
['text':'     source_dims = 0, 1, 2, 3, 4','line_number':3825,'multiline':False]
['text':'     destination_dims = 0, 1, 2, 3, 4','line_number':3826,'multiline':False]
['text':' We mark and update position for the dim provided by user','line_number':3830,'multiline':False]
['text':' i.e. `normalized_src` and `normalized_dims`','line_number':3831,'multiline':False]
['text':' Variable State:','line_number':3832,'multiline':False]
['text':'     order = NA, NA, 0, NA, 1','line_number':3833,'multiline':False]
['text':'     source_dims = -1, -1, 2, 3, 4','line_number':3834,'multiline':False]
['text':'     destination_dims = 0, 1, -1, 3, -1','line_number':3835,'multiline':False]
['text':' Remove the dims whose position we already know,','line_number':3842,'multiline':False]
['text':' the ones marked with -1 in previous step','line_number':3843,'multiline':False]
['text':' Variable State:','line_number':3844,'multiline':False]
['text':'     source_dims = 2, 3, 4','line_number':3845,'multiline':False]
['text':'     destination_dims = 0, 1, 3','line_number':3846,'multiline':False]
['text':' Update the position of the remaining dimensions.','line_number':3854,'multiline':False]
['text':' `source_dims` now contains the original position','line_number':3855,'multiline':False]
['text':' `destination_dims` contains the new position it will shifted to','line_number':3856,'multiline':False]
['text':' after considering the user inputs.','line_number':3857,'multiline':False]
['text':' Variable State:','line_number':3858,'multiline':False]
['text':'     order = 2, 3, 0, 4, 1','line_number':3859,'multiline':False]
['text':' If unflatten an empty tensor, create a new empty tensor using','line_number':3908,'multiline':False]
['text':' flat tensor Options.','line_number':3909,'multiline':False]
['text':' This can avoid the unflattened empty tensor to share the same storage','line_number':3910,'multiline':False]
['text':' with other unflatten tensors.','line_number':3911,'multiline':False]
['text':' Clones a tensor by cloning the underlying storage that it came from,','line_number':3923,'multiline':False]
['text':' which allows us to replicate the exact strides/storage_offset in the cloned tensor.','line_number':3924,'multiline':False]
['text':' Note [*_scatter ops preserve strides]','line_number':3925,'multiline':False]
['text':' In order for functionalization to preserve stride correctness, the *_scatter','line_number':3926,'multiline':False]
['text':' operators that it calls must preserve the striding behavior of their inputs.','line_number':3927,'multiline':False]
['text':' Specifically, the output of *_scatter(base, mutated_view, ...)','line_number':3928,'multiline':False]
['text':' should have identical size/stride/storage_offset to "base".','line_number':3929,'multiline':False]
['text':' In cases where the input tensor has internal memory overlap, we cannot actually','line_number':3932,'multiline':False]
['text':' preserve the strides/storage_offset of the input tensor, because','line_number':3933,'multiline':False]
['text':' *_scatter ops will try to copy_() into the cloned tensor.','line_number':3934,'multiline':False]
['text':' However, this should **never** show up in functionalized user code;','line_number':3935,'multiline':False]
['text':' most aten ops that try to mutate a tensor with internal memory overlap would error anyway.','line_number':3936,'multiline':False]
['text':'','line_number':3937,'multiline':False]
['text':' The one place that this does come up is in autograd - if there's a select_scatter','line_number':3938,'multiline':False]
['text':' in the forward, then autograd will generate one for the backward.','line_number':3939,'multiline':False]
['text':' If the input to the select_scatter is grad_output, then this could be an expanded tensor','line_number':3940,'multiline':False]
['text':' with internal overlap.','line_number':3941,'multiline':False]
['text':' See Note [*_scatter ops preserve strides]','line_number':3957,'multiline':False]
['text':' See Note [*_scatter ops preserve strides]','line_number':3972,'multiline':False]
['text':' See Note [as_strided_scatter backward support]','line_number':3980,'multiline':False]
['text':' See Note [*_scatter ops preserve strides]','line_number':3982,'multiline':False]
['text':' The default implementation of lift is a no-op.','line_number':3990,'multiline':False]
['text':' If TLS is set appropriately (for wrapper-tensor keys like Functionalize or functorch transforms),','line_number':3991,'multiline':False]
['text':' then we'll dispatch to one of their implementations, which will properly lift the tensor into a wrapper.','line_number':3992,'multiline':False]
['text':' See notes in native_functions.yaml','line_number':3997,'multiline':False]
['text':' Autogen kernels for tensor list ops dont work on XLA. TODO(jakeszwe)','line_number':4002,'multiline':False]
['text':' namespace native','line_number':4038,'multiline':False]
['text':' namespace at','line_number':4039,'multiline':False]
