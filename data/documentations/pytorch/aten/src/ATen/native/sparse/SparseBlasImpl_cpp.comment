['text':' Required for checking whether Triton kernels are available','line_number':8,'multiline':False]
['text':' Triton works only with blocksizes which are powers of 2.','line_number':32,'multiline':False]
['text':' Dtype and blocksize checks for potential Triton usage.','line_number':39,'multiline':False]
['text':' lhs is retiled to (b0, b1) while rhs is to (b1, b0),','line_number':46,'multiline':False]
['text':' so the result is tiled to (b0, b0) and we need to make','line_number':47,'multiline':False]
['text':' sure that strided.size(-1) is divisible by b0.','line_number':48,'multiline':False]
['text':' Device restrictions','line_number':59,'multiline':False]
['text':' Layout restrictions.','line_number':64,'multiline':False]
['text':' Dtype restrictions.','line_number':70,'multiline':False]
['text':' Dim restrictions.','line_number':74,'multiline':False]
['text':' Matrix product size compatibility.','line_number':83,'multiline':False]
['text':' We assume that result is properly resized.','line_number':89,'multiline':False]
['text':' We refer to these as (b0, b1) in the comments below.','line_number':100,'multiline':False]
['text':' No stable support for ROCM in Triton yet.','line_number':106,'multiline':False]
['text':' else the schema is not defined and/or the key is not
         overwritten, so skip and execute the code below. ','line_number':117,'multiline':True]
['text':' (..., r, c) -> (..., r / b0, c / b1, b0, b1)','line_number':122,'multiline':False]
['text':' NOTE: this function ALWAYS creates a view upon successful execution.','line_number':123,'multiline':False]
['text':' Note that sparse values are (..., b0, b1). This means that','line_number':141,'multiline':False]
['text':' the strided input has to be "tilable" to (..., b1, x) with','line_number':142,'multiline':False]
['text':' any x >= 1 such that all the shapes are (block) matrix product','line_number':143,'multiline':False]
['text':' compatible. The matrix product will then have shape (..., b0, x).','line_number':144,'multiline':False]
['text':' This in turn means the result has to be "tilable" to','line_number':145,'multiline':False]
['text':' (..., b0, x).','line_number':146,'multiline':False]
['text':'','line_number':147,'multiline':False]
['text':' These observations imply the following restrictions:','line_number':148,'multiline':False]
['text':' 1. strided.size(-2) has to be divisible by b1.','line_number':149,'multiline':False]
['text':' 2. result.size(-2) has to be divisible by b0.','line_number':150,'multiline':False]
['text':' 3. both strided.size(-1) and result.size(-1)','line_number':151,'multiline':False]
['text':'    have to be divisible by x.','line_number':152,'multiline':False]
['text':'','line_number':153,'multiline':False]
['text':' Restrictions 1 and 2 are trivially satisfied.','line_number':154,'multiline':False]
['text':' Regarding restriction 3:','line_number':155,'multiline':False]
['text':' it would make sense to take the largest possible x for better','line_number':156,'multiline':False]
['text':' performance since it is very likely that the last dimension','line_number':157,'multiline':False]
['text':' is contiguous. As such, this value is exactly','line_number':158,'multiline':False]
['text':' x = strided.size(-1), since strided.size(-1) == result.size(-1)','line_number':159,'multiline':False]
['text':' See the comments above. This is our x.','line_number':161,'multiline':False]
['text':' Left argument is (..., b0, b1) and right is (..., b1, x).','line_number':167,'multiline':False]
['text':' This naturally implies the result should be "tilable" as','line_number':168,'multiline':False]
['text':' (..., b0, x).','line_number':169,'multiline':False]
['text':' Select block rows of the strided input that intersect with the block colums of the sparse input.','line_number':180,'multiline':False]
['text':' Promote to float if output is half or bfloat16 for better precision','line_number':183,'multiline':False]
['text':' Now that we know which block rows intersect with which block columns,','line_number':186,'multiline':False]
['text':' we can perform matrix products between pairs of blocks.','line_number':187,'multiline':False]
['text':' NOTE: .to is a no-op when result.scalar_type() == mm_dtype.','line_number':188,'multiline':False]
['text':' Having pairwise block matrix products stored in pairwise_block_mm,','line_number':192,'multiline':False]
['text':' it is sufficient to sum all the block products that share the same row','line_number':193,'multiline':False]
['text':' encoded in the sparse index. Since the reduction step is done via','line_number':194,'multiline':False]
['text':' advanced indexing methods, the compressed index ought to get converted','line_number':195,'multiline':False]
['text':' to the COO format.','line_number':196,'multiline':False]
['text':' Reduction step.','line_number':202,'multiline':False]
['text':' If result is neither half nor bfloat16, do everyting in-place.','line_number':203,'multiline':False]
['text':' Zero out and sum over the blocks that share the same row indices.','line_number':205,'multiline':False]
['text':'dim=','line_number':208,'multiline':True]
['text':'index=','line_number':209,'multiline':True]
['text':'source=','line_number':210,'multiline':True]
['text':' Otherwise accumulate into a buffer and then copy.','line_number':212,'multiline':False]
['text':' No need to zero out, sum over the blocks goes into a buffer','line_number':214,'multiline':False]
['text':' followed by a copy into result.','line_number':215,'multiline':False]
['text':'dim=','line_number':220,'multiline':True]
['text':'index=','line_number':221,'multiline':True]
['text':'source=','line_number':222,'multiline':True]
['text':' No stable support for ROCM in Triton yet.','line_number':237,'multiline':False]
['text':' else triton_kernel returned NotImplemented, continue
             with the generic method below ','line_number':252,'multiline':True]
['text':' else the schema is not defined and/or the key is not
           overwritten, so skip and execute the code below. ','line_number':255,'multiline':True]
['text':' If result is not the same as self, it could always be used as out argument to mm.','line_number':262,'multiline':False]
['text':' Process beta','line_number':268,'multiline':False]
['text':' Otherwise we need to allocate external memory for mm if beta != 0.','line_number':277,'multiline':False]
['text':' Process beta','line_number':279,'multiline':False]
['text':' anonymous namespace','line_number':393,'multiline':False]
['text':' !AT_USE_MKL_SPARSE()','line_number':394,'multiline':False]
['text':'
  Computes a sparse matrix-dense vector product defined as
  y <- alpha*op(A)*x + beta*y

  Args:
  * `mat` - Tensor storing sparse m x n matrix A.
  * `vec` - Tensor storing dense vector x of size n.
  * `result` - [in] Tensor storing dense vector y of size m.
               [out] result of the operation.
','line_number':396,'multiline':True]
['text':'
  Computes a sum of two sparse matrices defined as
  result <- mat1 + alpha*mat2

  Args:
  * `mat1` - CSR Tensor storing sparse m x n matrix.
  * `mat2` - CSR Tensor storing sparse m x n matrix.
  * `result` - [in] CSR Tensor storing sparse m x n matrix.
               [out] result of the operation.
','line_number':427,'multiline':True]
['text':' namespace cpu','line_number':470,'multiline':False]
['text':' namespace at::native::sparse::impl','line_number':471,'multiline':False]
