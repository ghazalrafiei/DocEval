['text':' #define TORCH_ASSERT_ONLY_METHOD_OPERATORS','line_number':1,'multiline':False]
['text':'is_coalesced=','line_number':91,'multiline':True]
['text':' namespace (anonymous)','line_number':131,'multiline':False]
['text':' Generic formulation for unary operators which map 0 -> 0 so','line_number':133,'multiline':False]
['text':' we can just transform self.values() and preserve the sparsity pattern.','line_number':134,'multiline':False]
['text':'','line_number':135,'multiline':False]
['text':' Any non-linear function requires the tensor to be coalesced before','line_number':136,'multiline':False]
['text':' we can calculate the result. This also means inplace calculations','line_number':137,'multiline':False]
['text':' are only possible on coalesced tensors.','line_number':138,'multiline':False]
['text':' relu function has no declaration, it may be unused in Pytorch.','line_number':191,'multiline':False]
['text':' But we keep it and ignore the warning here until verified in the future.','line_number':192,'multiline':False]
['text':' Threshold_backward is not unary but it is the backward used for relu which is','line_number':209,'multiline':False]
['text':' unary','line_number':210,'multiline':False]
['text':' namespace at::native','line_number':283,'multiline':False]
