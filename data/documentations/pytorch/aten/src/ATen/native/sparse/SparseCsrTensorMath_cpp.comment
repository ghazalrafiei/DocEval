['text':' namespace meta','line_number':159,'multiline':False]
['text':' For the case of (0x0) result tensor, manually resize `result` tensor','line_number':171,'multiline':False]
['text':' to the size of `self` tensor','line_number':172,'multiline':False]
['text':' copy_sparse_compressed_ internally checks the sizes of result and self tensors','line_number':176,'multiline':False]
['text':' Hence no external size check required','line_number':177,'multiline':False]
['text':' end anonymous namespace','line_number':225,'multiline':False]
['text':' certain utiliy functions are usable from sparse COO.','line_number':230,'multiline':False]
['text':' // TODO: Use a specialized CSR kernel for performance if needed','line_number':234,'multiline':False]
['text':' NOTE: intersection_binary_op_with_wrapped_scalar assumes scalar.numel() == 1.','line_number':253,'multiline':False]
['text':' NOTE: intersection_binary_op_with_wrapped_scalar_ assumes scalar.numel() == 1.','line_number':270,'multiline':False]
['text':' Safe to use squeeze here, we already know that scalar safely broadcasts.','line_number':277,'multiline':False]
['text':' Check if either of the arguments is a wrapped Scalar','line_number':283,'multiline':False]
['text':' CSR is 2d!','line_number':304,'multiline':False]
['text':' redispatch!','line_number':306,'multiline':False]
['text':' redispatch!','line_number':315,'multiline':False]
['text':' To handle type promotion for inputs to unary ops,','line_number':325,'multiline':False]
['text':' we first get the result from the underlined op, and use the result','line_number':326,'multiline':False]
['text':' to create a sparse compressed tensor, which is used as the input to the out=','line_number':327,'multiline':False]
['text':' variant','line_number':328,'multiline':False]
['text':' namespace','line_number':351,'multiline':False]
['text':' Only accept squares sparse matrices or dense input as a vector','line_number':353,'multiline':False]
['text':' TODO: Check what happens with MKL, the output error reported with non square','line_number':354,'multiline':False]
['text':' matrices tends to be high See:','line_number':355,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/58770','line_number':356,'multiline':False]
['text':' TODO: keeping this for BC but the method used here may lead to','line_number':415,'multiline':False]
['text':' incorrect indices.','line_number':416,'multiline':False]
['text':' TODO: keeping this for BC but the method used here cannot','line_number':419,'multiline':False]
['text':' support batch dimensions because sparse COO tensors are batch','line_number':420,'multiline':False]
['text':' dimension ignorant.','line_number':421,'multiline':False]
['text':'
    csr.zero_() resets nnz to 0.

    If the original sparsity pattern needs to be preserved, use
    `csr.values().zero_()` instead.

    The above behavior also implies that torch.zeros_like(csr) returns
    a new tensor with nnz == 0. If one needs a zeros_like semantics
    where the result has the same sparsity pattern as input, then use
    `result = csr.clone(); result.values.zero_();`
  ','line_number':447,'multiline':True]
['text':' Implementation of Unary Ufuncs, those supported for Sparse CSR Layout
 * Only simple funcs, with 0->0 correspondence are currently supported. ','line_number':463,'multiline':True]
['text':' Exhaustive list of the unary ufuncs supported by sparse compressed','line_number':490,'multiline':False]
['text':' With addition of `round.decimals` overload, using CREATE_UNARY_UFUNC leads','line_number':517,'multiline':False]
['text':' to unresolved overload.','line_number':518,'multiline':False]
['text':' angle, isneginf, isposinf and signbit currently don't have an inplace variant','line_number':557,'multiline':False]
['text':' isnan and isinf don't have an out variant','line_number':563,'multiline':False]
['text':' Functions for matrix multiplication.','line_number':621,'multiline':False]
['text':' result = beta * self + alpha (mat1 @ mat2)','line_number':622,'multiline':False]
['text':' TODO: remove this, there are no codegenerated checks for devices yet','line_number':630,'multiline':False]
['text':' All the checks are from addmm_out_cuda_impl (ATen/native/cuda/Blas.cpp) and','line_number':636,'multiline':False]
['text':' TORCH_META_FUNC(addmm) (ATen/native/LinearAlgebra.cpp)','line_number':637,'multiline':False]
['text':' TODO: remove code duplication and unify code','line_number':638,'multiline':False]
['text':' Don't expand self if this is an in-place operation','line_number':647,'multiline':False]
['text':' If result gets resized and is sparse compressed,','line_number':679,'multiline':False]
['text':' it's compressed_indices tensor will contain junk values','line_number':680,'multiline':False]
['text':' so the whole tensor is not a valid compressed tensor.','line_number':681,'multiline':False]
['text':' To combat that, result needs to get zeroed out.','line_number':682,'multiline':False]
['text':' According to docs, when beta==0 values in self should be ignored.','line_number':690,'multiline':False]
['text':' nans and infs should not propagate','line_number':691,'multiline':False]
['text':' The custom impl addmm_out_sparse_csr_native_cpu only supports CSR @','line_number':701,'multiline':False]
['text':' strided -> strided','line_number':702,'multiline':False]
['text':' Return sparse','line_number':791,'multiline':False]
['text':' TODO: Expensive conversion to CSR. Should add native support for CSC.','line_number':801,'multiline':False]
['text':' Covers CSC @ CSR','line_number':802,'multiline':False]
['text':' Covers CSR @ CSC','line_number':803,'multiline':False]
['text':' Covers CSC @ CSC','line_number':804,'multiline':False]
['text':' TODO: This is a costly conversion. We should have','line_number':808,'multiline':False]
['text':' native support for CSC.','line_number':809,'multiline':False]
['text':' Default to taking options from mat1','line_number':812,'multiline':False]
['text':' if either  arg is strided we return strided, so update the options if','line_number':815,'multiline':False]
['text':' mat2 is strided.','line_number':816,'multiline':False]
['text':' Functions for element-wise addition.','line_number':827,'multiline':False]
['text':' redispatch!','line_number':835,'multiline':False]
['text':' redispatch!','line_number':842,'multiline':False]
['text':'
    Reductions on sparse CSR tensors using masked semantics.

    - A CSR tensor is a 2D tensor that is specified by a 3-tuple
      (crow_indices, col_indices, values).

    - To support a reduction operator on a CSR tensor, define:

template <typename scalar_t>
struct Reduction...Op {
  inline scalar_t operator()(const scalar_t& a, const scalar_t& b) const {
    return a ... b;
  }
  inline scalar_t identity() const { return ...; }
};

Tensor _sparse_csr_..._cpu(const Tensor& input, IntArrayRef dims_to_sum, bool keepdim, c10::optional<ScalarType> dtype) {
  ...
      result = reduce_sparse_csr_cpu_template<scalar_t>(input_, dims_to_sum, keepdim, Reduction...Op<scalar_t>());
  ...
  return result;
}

      and add the following

        - func: _sparse_csr_op.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
          dispatch:
            SparseCsrCUDA: _sparse_csr_..._cpu

      to native_functions.yaml

      Use ReductionAddOp and _sparse_csr_sum implementation as an example.

    - Since a CSR tensor dimensionality is always 2, only reductions
      with keepdim=True can be supported.

','line_number':979,'multiline':True]
['text':'
    Consider the following sparse tensor:

    1 * * * *
    * * * 2 *
    * * 3 * *
    * * * * *
    4 * 5 * *

    that has CSR representation

      crow_indices = [0, 1, 2, 3, 3, 5]
      col_indices = [0, 3, 2, 0, 2]
      values = [1, 2, 3, 4, 5]

    Reduction with dim=0 results:

    rop(1,4) * rop(3,5) 2 *

    that has CSR representation

      new_crow_indices = [0, 3]
      new_col_indices = [0, 2, 3]
      new_values = [rop(1, 4], rop(3, 5), 2]

    In general, the CSR representation data can be computed as follows:

      new_col_indices, col_map = col_indices.unique(sorted=True, return_inverse=True)
      nnz = new_col_indices.numel()
      new_crow_indices = [0, nnz]
      new_values.resize(nnz); new_values.fill_(identity)
      for i in range(col_indices.numel()):
          new_values[col_map[i]] = rop(new_values[col_map[i], values[i])
   ','line_number':1021,'multiline':True]
['text':'
    Calling at::_unique constitutes the main bottleneck of this
    function. However, it is still about 5x faster than using the
    invariant:
      csr.sum(dim=0) == csr.transpose(0, 1).sum(dim=1)
  ','line_number':1062,'multiline':True]
['text':' Set `is_cuda` = `true` in acc_type in CPU backend. Because the accumulate type','line_number':1075,'multiline':False]
['text':' of float should be float in current scenario. In CUDA, float is the accumulate type','line_number':1076,'multiline':False]
['text':' of float, while in CPU, double is the accumulate type of float.','line_number':1077,'multiline':False]
['text':' There is no point in parallelizing the following for-loop','line_number':1090,'multiline':False]
['text':' because about 99.3% of the computation time is spent in the','line_number':1091,'multiline':False]
['text':' at::_unique call above.','line_number':1092,'multiline':False]
['text':'
    Consider the following sparse tensor:

    1 * * * *
    * * * 2 *
    * * 3 * *
    * * * * *
    4 * 5 * *

    that has CSR representation

      crow_indices = [0, 1, 2, 3, 3, 5]
      col_indices = [0, 3, 2, 0, 2]
      values = [1, 2, 3, 4, 5]

    Reduction with dim=1 results:

    1
    2
    3
    *
    rop(4, 5)

    that has CSR representation

      new_crow_indices = [0, 1, 2, 3, 3, 4]
      new_col_indices = [0, 0, 0, 0]
      new_values = [1, 2, 3, rop(4, 5)]

    In general, the result CSR data can be computed as follows:

      new_crow_indices = [0]
      for i in range(1, nrows+1):
          new_crow_indices[i] = new_crow_indices[i-1] + (crow_indices[i] == crow_indices[i-1])
      nnz = new_crow_indices[-1]
      new_col_indices = zeros(nnz)
      new_values.resize(nnz)
      j = -1
      for i in range(1, nrows+1):
          if crow_indices[i] == crow_indices[i-1]:
              continue
          j += 1
          new_values[j] = rop(values[crow_indices[i] : crow_indices[i-1]])
  ','line_number':1109,'multiline':True]
['text':' Set `is_cuda` = `true` in acc_type in CPU backend. Because the accumulate type','line_number':1163,'multiline':False]
['text':' of float should be float in current scenario. In CUDA, float is the accumulate type','line_number':1164,'multiline':False]
['text':' of float, while in CPU, double is the accumulate type of float.','line_number':1165,'multiline':False]
['text':' TODO: we can likely do about 3x better than parallel_reduce:

In [2]: t=torch.randn(5000, 5000).to_sparse_csr()

In [3]: %timeit torch._sparse_csr_sum(t, dim=(0, 1), keepdim=True)
3.39 ms ± 898 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [4]: %timeit torch.sum(t.values())
1.07 ms ± 291 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
  ','line_number':1231,'multiline':True]
['text':' Set `is_cuda` = `true` in acc_type in CPU backend. Because the accumulate type','line_number':1242,'multiline':False]
['text':' of float should be float in current scenario. In CUDA, float is the accumulate type','line_number':1243,'multiline':False]
['text':' of float, while in CPU, double is the accumulate type of float.','line_number':1244,'multiline':False]
['text':'includeBool=','line_number':1265,'multiline':True]
['text':' effective after gh-29137 has been resolved','line_number':1293,'multiline':False]
['text':' after gh-29137 is resolved, delete this if-block','line_number':1308,'multiline':False]
['text':' namespace','line_number':1331,'multiline':False]
['text':' Set `is_cuda` = `true` in acc_type in CPU backend. Because the accumulate type','line_number':1339,'multiline':False]
['text':' of float should be float in current scenario. In CUDA, float is the accumulate type','line_number':1340,'multiline':False]
['text':' of float, while in CPU, double is the accumulate type of float.','line_number':1341,'multiline':False]
['text':'train','line_number':1374,'multiline':True]
['text':' init output to be all zeros, for `rows` that has no nonzero elements,','line_number':1384,'multiline':False]
['text':' the corresponding rows in the output will be zero.','line_number':1385,'multiline':False]
['text':' only need to calculate the out args','line_number':1394,'multiline':False]
['text':' for reduce type "amax" and "amin" for training','line_number':1395,'multiline':False]
['text':' allocate memory and init with invalid index','line_number':1403,'multiline':False]
['text':'train','line_number':1424,'multiline':True]
['text':' `row`: row indices of COO format','line_number':1433,'multiline':False]
['text':' `ccol`: ccol indices of CSC format (with permute)','line_number':1434,'multiline':False]
['text':' `permute`: permute pattern from CSR to CSC','line_number':1435,'multiline':False]
['text':'','line_number':1436,'multiline':False]
['text':' TODO: optimize the following section,','line_number':1437,'multiline':False]
['text':' currently `argsort` is sequential.','line_number':1438,'multiline':False]
['text':'transpose','line_number':1446,'multiline':True]
['text':' calculte the global index for CSC','line_number':1449,'multiline':False]
['text':' and get the conversion permute pattern','line_number':1450,'multiline':False]
['text':'column indices','line_number':1455,'multiline':True]
['text':'column count','line_number':1456,'multiline':True]
['text':' grad_input has the same indices and nnz with input','line_number':1462,'multiline':False]
['text':' namespace native','line_number':1490,'multiline':False]
['text':' namespace at','line_number':1491,'multiline':False]
