['text':'  Copyright Â© 2022 Apple Inc.','line_number':1,'multiline':False]
['text':' Modify the shape','line_number':30,'multiline':False]
['text':' ChannelsLast','line_number':34,'multiline':False]
['text':' Mean shape should remain unchanged in backward','line_number':42,'multiline':False]
['text':' Set axes of reduction','line_number':54,'multiline':False]
['text':' namespace mps','line_number':64,'multiline':False]
['text':' Inverse standard deviation now becomes variance (without epsilon)','line_number':66,'multiline':False]
['text':' Number of elements in one channel, needed for bessel correction term','line_number':122,'multiline':False]
['text':' Input shape changes based on memory format','line_number':126,'multiline':False]
['text':' Shape which can be broadcasted with input','line_number':128,'multiline':False]
['text':' Reduction axes','line_number':130,'multiline':False]
['text':' Dim where channels are located','line_number':147,'multiline':False]
['text':' Should have shape of mean','line_number':163,'multiline':False]
['text':' Mean and inv std tensors to be saved and returned','line_number':177,'multiline':False]
['text':' Running stats inplace update','line_number':181,'multiline':False]
['text':'
      If train:
        If has_running_mean:
          Update the running stats to be stored into save_mean and save_var,
          AND to be used in current batchnorm computation
        Else:
          Just calculate the var using batch variance
      If not train:
        Check if running mean exists (maybe do this check before making graph)
        Copy the running mean into the mean to be saved
        Calculate the save_var directly from the running variance

      Compute the batch norm output and stats to be saved
      ','line_number':189,'multiline':True]
['text':' Compute mean and variance of the current batch','line_number':206,'multiline':False]
['text':' TODO: This is not the formula used in PyTorch, is this OK? Seems more robust','line_number':211,'multiline':False]
['text':' float besselCorrectionTerm = float(N) / std::max(N - 1.0f, 1.0f);','line_number':212,'multiline':False]
['text':' Compute updated running mean','line_number':226,'multiline':False]
['text':' Compute updated running var','line_number':236,'multiline':False]
['text':' Update saved mean and inverse std tensor','line_number':247,'multiline':False]
['text':' Update saved mean and inverse std tensor','line_number':258,'multiline':False]
['text':' Test','line_number':261,'multiline':False]
['text':' Compute output of batch norm','line_number':268,'multiline':False]
['text':' Reshape saved mean and var to fit output','line_number':277,'multiline':False]
['text':' Running stats inplace update','line_number':282,'multiline':False]
['text':' If train and has_running_mean, add updated running mean to the output','line_number':346,'multiline':False]
['text':' TODO: Accumulate type?','line_number':380,'multiline':False]
['text':' at::toAccumulateType(self.scalar_type(), /*is_cuda=*/false),','line_number':381,'multiline':False]
['text':' TODO: Accumulate type?','line_number':388,'multiline':False]
['text':' at::toAccumulateType(self.scalar_type(), /*is_cuda=*/false),','line_number':389,'multiline':False]
['text':' Batch norm backward','line_number':473,'multiline':False]
['text':' Assuming that if grad_input_mask of weight is 1, then the weight is available','line_number':493,'multiline':False]
['text':' Derive from MPSCachedGraph','line_number':513,'multiline':False]
['text':' Broadcast with input','line_number':559,'multiline':False]
['text':' Reduction axes','line_number':561,'multiline':False]
['text':' NCHW - Channels dim is 1','line_number':573,'multiline':False]
['text':' Shape is the ORIGINAL NCHW shape','line_number':577,'multiline':False]
['text':' Mean and inv std tensors to be saved and returned','line_number':590,'multiline':False]
['text':' Reshape/transpose the input as needed','line_number':606,'multiline':False]
['text':' Use save_mean and save_var','line_number':620,'multiline':False]
['text':' Use running mean and running var','line_number':658,'multiline':False]
['text':' Reshape/transpose the input as needed','line_number':724,'multiline':False]
['text':' Layer norm forward for MPS','line_number':806,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':825,'multiline':False]
['text':' Unlike Batch Normalization, which applies scalar scale and bias for each','line_number':828,'multiline':False]
['text':' entire channel/plane with the affine option, Layer Normalization applies','line_number':829,'multiline':False]
['text':' per-element scale and bias. E.g. For input {N, C, H, W}, weight for','line_number':830,'multiline':False]
['text':' batchnorm has shape {C} while weight for layernorm has shape {H, W} or {W}.','line_number':831,'multiline':False]
['text':'weight=','line_number':833,'multiline':True]
['text':'bias=','line_number':834,'multiline':True]
['text':'running_mean=','line_number':835,'multiline':True]
['text':'running_var=','line_number':836,'multiline':True]
['text':'training=','line_number':837,'multiline':True]
['text':'momentum=','line_number':838,'multiline':True]
['text':' Suppress unused variable','line_number':857,'multiline':False]
['text':' optional ','line_number':870,'multiline':True]
['text':' optional ','line_number':871,'multiline':True]
['text':' dtype ','line_number':891,'multiline':True]
['text':' layout ','line_number':892,'multiline':True]
['text':' device ','line_number':893,'multiline':True]
['text':' pin_memory ','line_number':894,'multiline':True]
['text':' dtype ','line_number':899,'multiline':True]
['text':' layout ','line_number':900,'multiline':True]
['text':' device ','line_number':901,'multiline':True]
['text':' pin_memory ','line_number':902,'multiline':True]
['text':' dtype ','line_number':905,'multiline':True]
['text':' layout ','line_number':906,'multiline':True]
['text':' device ','line_number':907,'multiline':True]
['text':' pin_memory ','line_number':908,'multiline':True]
['text':' dtype ','line_number':913,'multiline':True]
['text':' layout ','line_number':914,'multiline':True]
['text':' device ','line_number':915,'multiline':True]
['text':' pin_memory ','line_number':916,'multiline':True]
['text':' dtype ','line_number':919,'multiline':True]
['text':' layout ','line_number':920,'multiline':True]
['text':' device ','line_number':921,'multiline':True]
['text':' pin_memory ','line_number':922,'multiline':True]
['text':' Derive from MPSCachedGraph','line_number':928,'multiline':False]
['text':' const auto memory_format = input.suggest_memory_format();','line_number':949,'multiline':False]
['text':' Axes along which to reduce to get "batch norm" gradient','line_number':963,'multiline':False]
['text':' This will be applied on shape [1, M, -1]','line_number':964,'multiline':False]
['text':' Shape of input to do "batch norm" backward','line_number':969,'multiline':False]
['text':' This is [1, M, -1]','line_number':970,'multiline':False]
['text':' Shape of mean to do "batch norm" backward','line_number':977,'multiline':False]
['text':' This is [1, M, [1,1,1..1]]','line_number':978,'multiline':False]
['text':' Shape of gamma to multiply with "batch norm" backward','line_number':986,'multiline':False]
['text':' This is [1, 1, -1]','line_number':987,'multiline':False]
['text':' Mean and inv std tensors to be saved and returned','line_number':1004,'multiline':False]
['text':' Reshape input to [1, M, -1]','line_number':1028,'multiline':False]
['text':' Reshape mean and rstd to [1, M, -1]','line_number':1029,'multiline':False]
['text':' Reshape gamma to [1, 1, -1] (-1 has N dims)','line_number':1030,'multiline':False]
['text':' Do this at the end','line_number':1034,'multiline':False]
['text':' TODO: Reduce redundant computation','line_number':1054,'multiline':False]
['text':' reverseVariance is square of rstd','line_number':1079,'multiline':False]
['text':' namespace at::native','line_number':1166,'multiline':False]
