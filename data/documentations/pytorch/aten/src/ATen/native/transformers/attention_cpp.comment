['text':' namespace','line_number':91,'multiline':False]
['text':' shape: [B, T, 3 x D]','line_number':185,'multiline':False]
['text':' self-attention','line_number':190,'multiline':False]
['text':' encoder-decoder attention','line_number':193,'multiline':False]
['text':' TODO: is there a more efficient way to set this up?','line_number':194,'multiline':False]
['text':' TODO: can we stay nested insted of using cat? Probably just make a','line_number':195,'multiline':False]
['text':' NestedTensor out of the matmul results or something?','line_number':196,'multiline':False]
['text':' TODO: can we stay nested instead of using cat?','line_number':213,'multiline':False]
['text':' compute q = (q + q_bias) / sqrt(dim_per_head), k = k + k_bias, v = v + v_bias','line_number':223,'multiline':False]
['text':' query shape: [B, T, D]','line_number':269,'multiline':False]
['text':' qkv_weight shape: [3 * D, D]','line_number':270,'multiline':False]
['text':' shape: [B, T, 3 x D]','line_number':330,'multiline':False]
['text':' shape: 3 x [B, num_head, T, dim_per_head]','line_number':354,'multiline':False]
['text':' Not used any more, allow free','line_number':356,'multiline':False]
['text':' shape: [B, num_head, T, T]','line_number':371,'multiline':False]
['text':' q & k are dead but cannot be freed because they were packed with v','line_number':373,'multiline':False]
['text':' shape: [B, num_head, T, T]','line_number':381,'multiline':False]
['text':' TODO: long-term, have a kernel that works with','line_number':382,'multiline':False]
['text':' NestedTensor directly if there is no mask passed','line_number':383,'multiline':False]
['text':' shape: [B, num_head, T, dim_per_head]','line_number':389,'multiline':False]
['text':' reuse storage for q; we're done with it','line_number':390,'multiline':False]
['text':' qkv is not dead; we just reused storage for q!','line_number':392,'multiline':False]
['text':' shape: [B, T, D]','line_number':403,'multiline':False]
['text':' Fuse transform_0213 inside','line_number':404,'multiline':False]
['text':' weights are not needed for full transformer, so don't worry too','line_number':411,'multiline':False]
['text':' much about performance -- we implement this just to make use','line_number':412,'multiline':False]
['text':' cases that don't disable need_weights still get some speedup.','line_number':413,'multiline':False]
['text':' This function is used to produce an attn_mask','line_number':505,'multiline':False]
['text':' in a standard format that can be consumed by both','line_number':506,'multiline':False]
['text':' the math and memory efficient attn_mask implementation','line_number':507,'multiline':False]
['text':'  Args:','line_number':508,'multiline':False]
['text':'    attn_mask: attn_mask of shape (B, L, S) or (L, S) or (B, N_heads, L, S)','line_number':509,'multiline':False]
['text':' Pass through','line_number':511,'multiline':False]
['text':' Convert boolean mask to additive mask; need to invert mask to indicate what','line_number':515,'multiline':False]
['text':' to mask *out*.','line_number':516,'multiline':False]
['text':' TODO Use the max type of the input and output','line_number':519,'multiline':False]
['text':' Otherwise, attn_mask represents an additive attention tensor','line_number':524,'multiline':False]
['text':' Memory Efficient Attention requires a padded attn mask bias','line_number':527,'multiline':False]
['text':' This function pads the attn_mask bias to be a multiple of 16','line_number':528,'multiline':False]
['text':' Then slices the padded bias to the original size','line_number':529,'multiline':False]
['text':' We apply this function to the top level SDPA so that','line_number':530,'multiline':False]
['text':' if padding is done it will be tracked for backward automatically','line_number':531,'multiline':False]
['text':' FlashAttentionV2 requires that head dimension be a multiple of 8','line_number':567,'multiline':False]
['text':' This was previously done within the kernel, however','line_number':568,'multiline':False]
['text':' This causes the kernel to maybe alias query, key, value','line_number':569,'multiline':False]
['text':' So instead we pad the head_dimensions to be a multiple of 8 in the composite','line_number':570,'multiline':False]
['text':' region','line_number':571,'multiline':False]
['text':' namespace','line_number':595,'multiline':False]
['text':' Computes scaled dot product attention on query, key and value tensors, using','line_number':597,'multiline':False]
['text':' an optional attention mask if passed, and applying dropout if a probability','line_number':598,'multiline':False]
['text':' greater than 0.0 is specified.','line_number':599,'multiline':False]
['text':'','line_number':600,'multiline':False]
['text':' Args:','line_number':601,'multiline':False]
['text':'     query (Tensor): Query tensor; shape (N, ..., L, E)','line_number':602,'multiline':False]
['text':'     key (Tensor): Key tensor; shape (N, ..., S, E)','line_number':603,'multiline':False]
['text':'     value (Tensor): Value tensor; shape (N, ..., S, E)','line_number':604,'multiline':False]
['text':'     attn_mask (optional Tensor): Attention mask; shape (N, ..., L, S) or (L, S). Currently, only a boolean mask','line_number':605,'multiline':False]
['text':'         is supported, where a value of True indicates that the element *should* take part in attention.','line_number':606,'multiline':False]
['text':'     dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied','line_number':607,'multiline':False]
['text':'     need_attn_weights (bool): If true, the second return value will contain the attention weights used;','line_number':608,'multiline':False]
['text':'         otherwise, the second return value is unspecified','line_number':609,'multiline':False]
['text':'     is_causal (bool): If true, assumes causal attention masking; for this case, attn_mask should not be set.','line_number':610,'multiline':False]
['text':'         TODO: Consider removing this flag before promoting this function to the public API. It's possible','line_number':611,'multiline':False]
['text':'         to get specialized support for causal masks (and other types of masking e.g. local attention / block','line_number':612,'multiline':False]
['text':'         sparse masks) via tensor subclassing, allowing for a leaner API.','line_number':613,'multiline':False]
['text':'','line_number':614,'multiline':False]
['text':' Returns a tuple containing:','line_number':615,'multiline':False]
['text':'     output (Tensor): Attention output; shape (N, ..., L, E)','line_number':616,'multiline':False]
['text':'     attn_weights (Tensor): Attention weighting; shape (N, ..., L, S)','line_number':617,'multiline':False]
['text':'','line_number':618,'multiline':False]
['text':' Shape legend:','line_number':619,'multiline':False]
['text':'     N: Batch size','line_number':620,'multiline':False]
['text':'     ...: Any number of other batch dimensions (optional)','line_number':621,'multiline':False]
['text':'     S: Source sequence length','line_number':622,'multiline':False]
['text':'     L: Target sequence length','line_number':623,'multiline':False]
['text':'     E: Embedding dimension','line_number':624,'multiline':False]
['text':' We need to calculate the scale based off the OG head dim size','line_number':650,'multiline':False]
['text':'return_debug_mask','line_number':653,'multiline':True]
['text':' For the CPU case we do not need to pad the last dim','line_number':656,'multiline':False]
['text':'return_debug_mask','line_number':658,'multiline':True]
['text':'dropout_mask','line_number':680,'multiline':True]
['text':' Naive, composite implementation defined here.','line_number':702,'multiline':False]
['text':' Scale q, k before matmul for stability see https://tinyurl.com/sudb9s96 for math','line_number':704,'multiline':False]
['text':' Replace attn_mask with causal mask; lower triangular elements take part in attention.','line_number':715,'multiline':False]
['text':' In order to validate the correctness of the fused kernels, we need to','line_number':731,'multiline':False]
['text':' use the same dropout mask in order to compare the results.','line_number':732,'multiline':False]
['text':' query shape: [B, T, D]','line_number':857,'multiline':False]
['text':' qkv_weight shape: [3 * D, D]','line_number':858,'multiline':False]
['text':' shape: [B, T, 3 x D]','line_number':906,'multiline':False]
['text':' shape: 3 x [B, num_head, T, dim_per_head]','line_number':909,'multiline':False]
['text':' Not used any more, allow free','line_number':911,'multiline':False]
['text':' shape: [B, T, D]','line_number':935,'multiline':False]
['text':' Fuse transform_0213 inside','line_number':936,'multiline':False]
['text':' namespace native','line_number':944,'multiline':False]
['text':' namespace at','line_number':945,'multiline':False]
