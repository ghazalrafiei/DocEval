['text':'*****************************************************************************
 * Copyright (c) 2023, Tri Dao.
 *****************************************************************************','line_number':1,'multiline':True]
['text':' If Is_local is true, Is_causal should be false','line_number':18,'multiline':False]
['text':' printf("smem_size = %d\n", smem_size);','line_number':43,'multiline':False]
['text':' Work-around for gcc 7. It doesn't like nested BOOL_SWITCH.','line_number':45,'multiline':False]
['text':' https://github.com/kokkos/kokkos-kernels/issues/349','line_number':46,'multiline':False]
['text':' https://github.com/HazyResearch/flash-attention/issues/21','line_number':47,'multiline':False]
['text':' Will only return softmax if dropout, to reduce compilation time.','line_number':58,'multiline':False]
['text':' If not IsEvenKConst, we also set IsEvenMNConst to false to reduce number of templates.','line_number':59,'multiline':False]
['text':' If return_softmax, set IsEvenMNConst to false to reduce number of templates','line_number':60,'multiline':False]
['text':' If head dim > 128, set IsEvenMNConst to false to reduce number of templates','line_number':61,'multiline':False]
['text':' If Is_local, set Is_causal to false','line_number':62,'multiline':False]
['text':' printf("IsEvenMNConst = %d, IsEvenKConst = %d, Is_local = %d, Is_causal = %d, ReturnSoftmaxConst = %d, Is_dropout = %d\n", int(IsEvenMNConst), int(IsEvenKConst), int(Is_local), int(Is_causal), int(ReturnSoftmaxConst), int(Is_dropout));','line_number':64,'multiline':False]
['text':' auto kernel = &flash_fwd_kernel<Kernel_traits, false, Is_causal, false, true, true, false>;','line_number':65,'multiline':False]
['text':' int ctas_per_sm;','line_number':70,'multiline':False]
['text':' cudaError status_ = cudaOccupancyMaxActiveBlocksPerMultiprocessor(','line_number':71,'multiline':False]
['text':'     &ctas_per_sm, kernel, Kernel_traits::kNThreads, smem_size);','line_number':72,'multiline':False]
['text':' printf("smem_size = %d, CTAs per SM = %d\n", int(smem_size), ctas_per_sm);','line_number':73,'multiline':False]
['text':' If Append_KV, then we must have seqlen_offsets, which means cu_seqlens_k != nullptr.','line_number':97,'multiline':False]
['text':' If not IsEvenKConst, we also set IsEvenMNConst to false to reduce number of templates.','line_number':98,'multiline':False]
['text':' If Is_local, set Is_causal to false','line_number':99,'multiline':False]
['text':' auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, true, Split, Append_KV>;','line_number':101,'multiline':False]
['text':' auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, IsEvenKConst>;','line_number':102,'multiline':False]
['text':' We want kBlockM to be as small as possible for more parallelism.','line_number':116,'multiline':False]
['text':' With 128 threads we can load 512 elements at a time, so if headdim is divisible by 128, kBlockM = 4.','line_number':117,'multiline':False]
['text':' If headdim is divisible by 64, then we set kBlockM = 8, etc.','line_number':118,'multiline':False]
['text':' Fixed for all head dimensions','line_number':144,'multiline':False]
['text':' TD [2023-08-28]: nvcc segfaults for headdim 96 with block size 64 x 256,','line_number':145,'multiline':False]
['text':' and for headdim 192 with block size 64 x 128.','line_number':146,'multiline':False]
['text':' Also for headdim 160 with block size 64 x 128 after the rotary addition.','line_number':147,'multiline':False]
['text':' Using 8 warps is 18% slower for seqlen=2k, 2 warps is 5% slower','line_number':168,'multiline':False]
['text':' Using block size (64 x 256) is 27% slower for seqlen=2k','line_number':169,'multiline':False]
['text':' Using block size (256 x 64) is 85% slower for seqlen=2k, because of register spilling','line_number':170,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':172,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':173,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':176,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':177,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 128, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':178,'multiline':False]
['text':' For sm86 or sm89, 64 x 64 is the fastest for causal (because it's square),','line_number':191,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':201,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':202,'multiline':False]
['text':' These two are always slower','line_number':203,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<96, 128, 128, 4, true, T>>(params, stream);','line_number':204,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<96, 64, 128, 4, true, T>>(params, stream);','line_number':205,'multiline':False]
['text':' For sm86 or sm89, 64 x 64 is the fastest for causal (because it's square),','line_number':218,'multiline':False]
['text':' and 128 x 32 (48 KB smem) is the fastest for non-causal since we get 2 CTAs per SM.','line_number':219,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':229,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, true, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':230,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 128, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':231,'multiline':False]
['text':' Using 8 warps (128 x 128 and 256 x 64) is 28% slower for seqlen=2k','line_number':232,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 128, 8, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':233,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 8, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':234,'multiline':False]
['text':' 1st ones are good for H100, A100','line_number':235,'multiline':False]
['text':' 2nd one is good for A6000 bc we get slightly better occupancy','line_number':236,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 64, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':239,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 4, true, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':240,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 4, true, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':241,'multiline':False]
['text':' For A100, H100, 128 x 32 is the fastest.','line_number':254,'multiline':False]
['text':' For sm86 or sm89, 64 x 64 is the fastest for causal (because it's square),','line_number':255,'multiline':False]
['text':' and 128 x 64 with 8 warps is the fastest for non-causal.','line_number':256,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 4, false, true, T>, Is_dropout, Is_causal>(params, stream);','line_number':266,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':267,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, false, T>>(params, stream);','line_number':268,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 128, 4, false, T>>(params, stream);','line_number':269,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 64, 4, false, T>>(params, stream);','line_number':270,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 8, false, T>>(params, stream);','line_number':271,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 128, 8, false, T>>(params, stream);','line_number':272,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 32, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':287,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 8, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':288,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 64, 4, false, T>>(params, stream);','line_number':289,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 128, 4, false, T>>(params, stream);','line_number':290,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 128, 8, false, T>>(params, stream);','line_number':291,'multiline':False]
['text':' printf("max_smem_per_block = %d\n", max_smem_per_block);','line_number':307,'multiline':False]
['text':' 112 KB','line_number':310,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':315,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 32, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':316,'multiline':False]
['text':' We can't do 128 x 32 with 8 warps because with headdim 224, kBlockKSmem = 32.','line_number':317,'multiline':False]
['text':' If we have N = 32, there are only 1024 elements to load at once, where each load','line_number':318,'multiline':False]
['text':' is 8 elements. This means we can only use 128 threads and not 256 threads.','line_number':319,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 8, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':320,'multiline':False]
['text':' printf("max_smem_per_sm = %d, max_smem_per_block = %d\n", max_smem_per_sm, max_smem_per_block);','line_number':338,'multiline':False]
['text':' For A100, we want to run with 128 x 64 (128KB smem).','line_number':341,'multiline':False]
['text':' For H100 we want to run with 64 x 64 (96KB smem) since then we can get 2 CTAs per SM.','line_number':342,'multiline':False]
['text':' 64 KB','line_number':348,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 64, 32, 4, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':349,'multiline':False]
['text':' 96 KB','line_number':350,'multiline':False]
['text':' run_flash_fwd<Flash_fwd_kernel_traits<Headdim, 128, 32, 8, false, false, T>, Is_dropout, Is_causal>(params, stream);','line_number':351,'multiline':False]
['text':' namespace pytorch_fmha','line_number':356,'multiline':False]
