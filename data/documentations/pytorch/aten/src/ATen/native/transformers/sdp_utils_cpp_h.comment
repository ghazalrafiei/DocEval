['text':' Note that if this changed make sure to update','line_number':32,'multiline':False]
['text':' the templated enum in mem_eff/kernel_forward.h and mem_eff/kernel_backward.h','line_number':33,'multiline':False]
['text':' This helper function creates a constexpr std::array','line_number':61,'multiline':False]
['text':' From a compile time list of values','line_number':62,'multiline':False]
['text':' num_head_dims is ragged','line_number':157,'multiline':False]
['text':' This is being called inside sdp with shape [batch, heads, {seq_len}, dim]','line_number':171,'multiline':False]
['text':' When this function is called we are assured that the nt is dim==4','line_number':187,'multiline':False]
['text':' short circuit if any is unsafe','line_number':192,'multiline':False]
['text':' We now know none of the inputs have ragged num_heads, so we can safely','line_number':213,'multiline':False]
['text':' access .size(1)','line_number':214,'multiline':False]
['text':' Return false if have nested tensor','line_number':237,'multiline':False]
['text':' This is expected to be called after check_tensor_shapes ensuring that the','line_number':314,'multiline':False]
['text':' size() calls won't error since the inputs are all 4 dimensional','line_number':315,'multiline':False]
['text':' This is expected to be called after check_tensor_shapes ensuring that the','line_number':348,'multiline':False]
['text':' size() calls won't error since the inputs are all 4 dimensional','line_number':349,'multiline':False]
['text':' num_heads logic for nested input is checked in','line_number':357,'multiline':False]
['text':' check_for_seq_len_0_nested_tensor as there is handling there to make sure','line_number':358,'multiline':False]
['text':' num_heads is not ragged','line_number':359,'multiline':False]
['text':' try to broadcast batchsize','line_number':369,'multiline':False]
['text':' if only one of k or v require broadcasting of batch size, the other','line_number':373,'multiline':False]
['text':' must have a consistent seq_len dim','line_number':374,'multiline':False]
['text':' In some cases people will pass in 0 sized tensors, this will','line_number':390,'multiline':False]
['text':' cause the fused path to error with unaligned mask','line_number':391,'multiline':False]
['text':' The stride checking for NestedTensors is done within the kernel','line_number':405,'multiline':False]
['text':' And .contiguous will be called if needed','line_number':406,'multiline':False]
['text':' This function checks that the last dimension of the inputs to','line_number':408,'multiline':False]
['text':' fused_attention have stride 1','line_number':409,'multiline':False]
['text':' We check the global context to see if user has explicitly turned of flash','line_number':440,'multiline':False]
['text':' sdp kernels','line_number':441,'multiline':False]
['text':' We check the global context to see if user has explicitly turned of','line_number':452,'multiline':False]
['text':' mem_efficient sdp kernels','line_number':453,'multiline':False]
['text':' namespace sdp','line_number':464,'multiline':False]
