['text':' See Note [ATen preprocessor philosophy]','line_number':34,'multiline':False]
['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':72,'multiline':False]
['text':' namespace at::native','line_number':78,'multiline':False]
['text':' AT_CUDNN_ENABLED()','line_number':80,'multiline':False]
['text':' DropoutDescriptor','line_number':87,'multiline':False]
['text':' RNNDescriptor','line_number':111,'multiline':False]
['text':' In some cases, a use of RNNDescriptor does not rely on the','line_number':175,'multiline':False]
['text':' DropoutDescriptor.  In this case, we fake up a no-dropout','line_number':176,'multiline':False]
['text':' descriptor to make the RNN descriptor initialization go through.','line_number':177,'multiline':False]
['text':' This is used by _cudnn_rnn_flatten_weight, which needs an','line_number':178,'multiline':False]
['text':' RNNDescriptor for get_parameters(), but does not actually need','line_number':179,'multiline':False]
['text':' a fully initialized dropout descriptor.  This lets us avoid','line_number':180,'multiline':False]
['text':' having to pass the dropout state to flatten, which has no business','line_number':181,'multiline':False]
['text':' knowing what the dropout state is.','line_number':182,'multiline':False]
['text':' TensorDescriptor list','line_number':190,'multiline':False]
['text':' To be mutated in the loop','line_number':195,'multiline':False]
['text':' NB: cuDNN RNN API does not support 2d descriptors, so we','line_number':199,'multiline':False]
['text':' must pad it out to 3d.','line_number':200,'multiline':False]
['text':' The best way to understand the meaning of the values stored in','line_number':215,'multiline':False]
['text':' this struct is to consider each of the possible ways our','line_number':216,'multiline':False]
['text':' input can be structured.','line_number':217,'multiline':False]
['text':'','line_number':218,'multiline':False]
['text':' Suppose you want to run RNN on the following variable','line_number':219,'multiline':False]
['text':' length inputs:','line_number':220,'multiline':False]
['text':'','line_number':221,'multiline':False]
['text':'    Sequence 1: ABCD','line_number':222,'multiline':False]
['text':'    Sequence 2: EF','line_number':223,'multiline':False]
['text':'    Sequence 3: G','line_number':224,'multiline':False]
['text':'','line_number':225,'multiline':False]
['text':' (Let _ be padding when we have non-packed representations.)','line_number':226,'multiline':False]
['text':'','line_number':227,'multiline':False]
['text':' # Packed input (batch_sizes is non-empty)','line_number':228,'multiline':False]
['text':'','line_number':229,'multiline':False]
['text':'  input_size','line_number':230,'multiline':False]
['text':' +------+                    +','line_number':231,'multiline':False]
['text':' | A    |                    |','line_number':232,'multiline':False]
['text':' | E    | mini_batch =       |','line_number':233,'multiline':False]
['text':' | G    | batch_sizes[0] = 3 |','line_number':234,'multiline':False]
['text':' +------+                    |','line_number':235,'multiline':False]
['text':' | B    |                    | batch_sizes_sum = 7','line_number':236,'multiline':False]
['text':' | F    | batch_sizes[1] = 2 |','line_number':237,'multiline':False]
['text':' +------+                    |','line_number':238,'multiline':False]
['text':' | C    | batch_sizes[2] = 1 |','line_number':239,'multiline':False]
['text':' +------+                    |','line_number':240,'multiline':False]
['text':' | D    | batch_sizes[3] = 1 |','line_number':241,'multiline':False]
['text':' +------+                    +','line_number':242,'multiline':False]
['text':'','line_number':243,'multiline':False]
['text':'              (seq_length = 4)','line_number':244,'multiline':False]
['text':'','line_number':245,'multiline':False]
['text':'    input.size() = batch_sizes_sum x input_size','line_number':246,'multiline':False]
['text':'','line_number':247,'multiline':False]
['text':' # Unpacked input (batch_first = false)','line_number':248,'multiline':False]
['text':'','line_number':249,'multiline':False]
['text':'  mini_batch = 3','line_number':250,'multiline':False]
['text':' +-------+','line_number':251,'multiline':False]
['text':' | A E G |','line_number':252,'multiline':False]
['text':' | B F _ | seq_length = 4','line_number':253,'multiline':False]
['text':' | C _ _ |','line_number':254,'multiline':False]
['text':' | D _ _ |','line_number':255,'multiline':False]
['text':' +-------+','line_number':256,'multiline':False]
['text':'    ...    input_size','line_number':257,'multiline':False]
['text':' +-------+','line_number':258,'multiline':False]
['text':'','line_number':259,'multiline':False]
['text':'    input.size() = seq_length x mini_batch x input_size','line_number':260,'multiline':False]
['text':'','line_number':261,'multiline':False]
['text':' # Unpacked input (batch_first = true)','line_number':262,'multiline':False]
['text':'','line_number':263,'multiline':False]
['text':'  seq_length = 4','line_number':264,'multiline':False]
['text':' +---------+','line_number':265,'multiline':False]
['text':' | A B C D |','line_number':266,'multiline':False]
['text':' | E F _ _ | mini_batch = 3','line_number':267,'multiline':False]
['text':' | G _ _ _ |','line_number':268,'multiline':False]
['text':' +---------+','line_number':269,'multiline':False]
['text':'     ...     input_size','line_number':270,'multiline':False]
['text':' +---------+','line_number':271,'multiline':False]
['text':'','line_number':272,'multiline':False]
['text':'    input.size() = mini_batch x seq_length x input_size','line_number':273,'multiline':False]
['text':'','line_number':274,'multiline':False]
['text':' NB: this is not input.size(), which is an IntArrayRef; instead, this','line_number':279,'multiline':False]
['text':' size of the inner-most dimension.  In NL applications, this is usually','line_number':280,'multiline':False]
['text':' the size of the embedding.  You can also think of this as the size','line_number':281,'multiline':False]
['text':' of the "channel" dimension (at risk of confusing vision researchers :)','line_number':282,'multiline':False]
['text':' Only valid when !is_input_packed','line_number':284,'multiline':False]
['text':' == sum(batch_sizes)','line_number':285,'multiline':False]
['text':' NB: When input is packed, the mini_batch size is NOT the size','line_number':296,'multiline':False]
['text':' of the outer dimension','line_number':297,'multiline':False]
['text':' TODO: Actually, would this make ASAN's job harder catching','line_number':309,'multiline':False]
['text':' an uninitialized access?','line_number':310,'multiline':False]
['text':' something bogus in case we access it','line_number':311,'multiline':False]
['text':' TODO: check x for consistency with input_size?','line_number':315,'multiline':False]
['text':' Everything together','line_number':326,'multiline':False]
['text':' NB: Doesn't include the weight descriptor','line_number':334,'multiline':False]
['text':' NB: this won't actually lay out the tensor descriptor pointers','line_number':337,'multiline':False]
['text':' in the right way, so you'll have to preprocess them','line_number':338,'multiline':False]
['text':' TODO: This is annoying, having to put the cudnnTensorDescriptor_t','line_number':358,'multiline':False]
['text':' in a contiguous array...','line_number':359,'multiline':False]
['text':' assuming it's LSTM which has 8 "linear layers" (i.e. 4 weights and 4 biases)','line_number':412,'multiline':False]
['text':'handle=','line_number':416,'multiline':True]
['text':'rnnDesc=','line_number':417,'multiline':True]
['text':'layer=','line_number':418,'multiline':True]
['text':'xDesc=','line_number':419,'multiline':True]
['text':'wDesc=','line_number':420,'multiline':True]
['text':'w=','line_number':421,'multiline':True]
['text':'linLayerID=','line_number':422,'multiline':True]
['text':'linLayerMatDesc=','line_number':423,'multiline':True]
['text':'linLayerMat=','line_number':424,'multiline':True]
['text':' Generate a new parameter tensor which is a view into the weight_buf.','line_number':447,'multiline':False]
['text':'
    Returns weight and bias tensors for each layer of the RNN. These tensors
    are views on the underlying weight buffer allocated by CuDNN.

    Note: for LSTM and GRU, which have multiple parameters of each type (4 and 3, respectively),
          these parameters are concatenated along the first dimension.
          These parameters are returned in a consistent order by CuDNN:
              (reset, forget, cell, output) for LSTM
              (reset, input, new) for GRU
    Args:
        fn: The RNN function object holding the RNN state
        handle: a CuDNN handle
        weight_buf: a 1D tensor containing the CuDNN-allocated weight (or grad_weight) buffer
    Returns:
        parameters: [(weight_ih, weight_hh, bias_ih, bias_hh)*], with length equal to the num_layers.
            This is represented as a pair of vector, and outer-dimension stride
            (NB: Can't return MatrixRef because we need to allocate the underlying tensor)
  ','line_number':454,'multiline':True]
['text':' stride0','line_number':472,'multiline':False]
['text':' for all the RNN types provided by CUDNN, all the ih weights','line_number':525,'multiline':False]
['text':' are the same size and are allocated in a contiguous chunk','line_number':526,'multiline':False]
['text':' (same for the hh weights, and the ih and hh biases).','line_number':527,'multiline':False]
['text':' Since we're storing all the weights in a single tensor anyway,','line_number':528,'multiline':False]
['text':' might as well merge the CUDNN ones into a single tensor as well','line_number':529,'multiline':False]
['text':' We could also exclude bias params by restricting cudnn_methods to just { cudnnGetRNNLinLayerMatrixParams }','line_number':532,'multiline':False]
['text':' at the very top.  However, to do so would throw off the cur_offset account, which is currently a strict','line_number':533,'multiline':False]
['text':' and informative check that all params are laid out the way we think they are.  If include_bias is false,','line_number':534,'multiline':False]
['text':' I'd rather keep full cur_offset checks rather than save some CPU overhead by skipping the cudnn_method =','line_number':535,'multiline':False]
['text':' cudnnGetRNNLinLayerBiasParams iteration.','line_number':536,'multiline':False]
['text':' Generate a new parameter tensor which is a view into the weight_buf.','line_number':538,'multiline':False]
['text':' for cudnn_method','line_number':550,'multiline':False]
['text':' for layer','line_number':563,'multiline':False]
['text':' This is a lightweight version of the method above used to quickly get the expected','line_number':567,'multiline':False]
['text':' parameter offsets.','line_number':568,'multiline':False]
['text':' This API returns a separate pointer for weight of every gate,','line_number':586,'multiline':False]
['text':' but we represent them as a single tensor, so we're only interested','line_number':587,'multiline':False]
['text':' in a very limited subset of possible values.','line_number':588,'multiline':False]
['text':' assuming it's LSTM which has 8 "linear layers" (i.e. 4 weights and 4 biases)','line_number':608,'multiline':False]
['text':' if copying, allow_type_change may be true or false.','line_number':631,'multiline':False]
['text':' if viewing, allow_type_change must be false.','line_number':632,'multiline':False]
['text':' NOTE: these lists have all weights before all biases, so if the layer','line_number':650,'multiline':False]
['text':' doesn't use biases, iteration will terminate once layer_params_from ends','line_number':651,'multiline':False]
['text':' and ignore them.','line_number':652,'multiline':False]
['text':' NOTE: there is an exception from the above statement. If LSTMs with projections','line_number':654,'multiline':False]
['text':' are used, weights layout will be w_ih, w_hh, b_ih, b_hh, w_hr. So need to handle no-bias','line_number':655,'multiline':False]
['text':' case specially, because will need to copy 0->0, 1->1, 2->4. This case can be uniquely','line_number':656,'multiline':False]
['text':' identified by checking if number of defined parameters for each layer is 3.','line_number':657,'multiline':False]
['text':' Excludes Turing from using persistent rnn.','line_number':734,'multiline':False]
['text':' technically, batch size should be multiple of 8, but there are quite a few multiple-of-8 batchsizes that give bad perf,','line_number':737,'multiline':False]
['text':' weed them out','line_number':738,'multiline':False]
['text':' SM count check excludes A30 (similar issue to A40)','line_number':745,'multiline':False]
['text':' Excludes sm_86 GPU devices from using persistent rnn.','line_number':747,'multiline':False]
['text':' This is because there are some edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.','line_number':748,'multiline':False]
['text':' Based on tests by Vasily Volkov and xwang233.  Vasily only tried bsize <= 128,','line_number':751,'multiline':False]
['text':' so conservatively enable persistence for bsize <= 128 only.','line_number':752,'multiline':False]
['text':' TODO:  Run more tests for bsize > 128.','line_number':753,'multiline':False]
['text':' Persistent GRU performance is flakier than other RNN types.  Exclude them for now.','line_number':755,'multiline':False]
['text':' TODO:  Write a more refined GRU heuristic.','line_number':756,'multiline':False]
['text':' Persistent LSTMs are comparable to or better than non-persistent for bsize <= 128.','line_number':759,'multiline':False]
['text':' Persistent RNN_RELU and TANH show poor performance when bsize >= 96 AND hidden size >= 896.','line_number':762,'multiline':False]
['text':' 8.2.1','line_number':773,'multiline':False]
['text':' backward ','line_number':784,'multiline':True]
['text':' LSTM with projections only works with standard algorithm','line_number':800,'multiline':False]
['text':' Persistent algos typically don't work for packed inputs with sequence lengths that vary','line_number':805,'multiline':False]
['text':' across batch elements, and will return CUDNN_STATUS_NOT_SUPPORTED if attempted. See','line_number':806,'multiline':False]
['text':' https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#features-of-rnn-functions','line_number':807,'multiline':False]
['text':' 8.2.1','line_number':810,'multiline':False]
['text':' anonymous namespace','line_number':835,'multiline':False]
['text':' Utilities exposed in RNNUtils.h','line_number':837,'multiline':False]
['text':'=false','line_number':854,'multiline':True]
['text':'=true','line_number':855,'multiline':True]
['text':' flat_buf_datatype is accepted as a separate argument (rather than extracted','line_number':856,'multiline':False]
['text':' from flat_buf_options) because to extract flat_buf_datatype from','line_number':857,'multiline':False]
['text':' flat_buf_options, we'd need to say auto flat_buf_datatype =','line_number':858,'multiline':False]
['text':' getCudnnDataTypeFromScalarType(typeMetaToScalarType(options.dtype()));','line_number':859,'multiline':False]
['text':' typeMetaToScalarType is a surprisingly nontrivial function.  We should','line_number':860,'multiline':False]
['text':' avoid it if we can.','line_number':861,'multiline':False]
['text':' Why do we pad to 5 dims here (and elsewhere)?','line_number':881,'multiline':False]
['text':' https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnRNNForwardTraining','line_number':882,'multiline':False]
['text':' expects descriptors padded to 3 dimensions.','line_number':883,'multiline':False]
['text':' Slice off views into weight_buf','line_number':893,'multiline':False]
['text':' Copy weights','line_number':901,'multiline':False]
['text':'copy=','line_number':902,'multiline':True]
['text':' Update the storage','line_number':904,'multiline':False]
['text':' There is a special case for LSTM with projections and no bias,','line_number':906,'multiline':False]
['text':' where weight copy is done in 0->0, 1->1, 2->4 layout','line_number':907,'multiline':False]
['text':' namespace cudnn_rnn','line_number':928,'multiline':False]
['text':' NB: does inplace update into TensorList','line_number':932,'multiline':False]
['text':' It would be a relatively simple matter to refactor this into multiple','line_number':933,'multiline':False]
['text':' functions, only one of which does an inplace update, but we leave this','line_number':934,'multiline':False]
['text':' for future work','line_number':935,'multiline':False]
['text':' returns flat weight_buf','line_number':943,'multiline':False]
['text':'flat_buf_datatype=','line_number':954,'multiline':True]
['text':'flat_buf_options=','line_number':955,'multiline':True]
['text':'set_orig_weights_to_flat_buf=','line_number':956,'multiline':True]
['text':' NB: when fn_batch_sizes is empty, that means no batch sizes was specified','line_number':964,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':972,'multiline':False]
['text':'check_dtype=','line_number':978,'multiline':True]
['text':' TODO: Set device to input','line_number':995,'multiline':False]
['text':' TODO: can batch_first be a wrapper around this function?','line_number':1002,'multiline':False]
['text':' NB: Not allowed to return undefined tensors','line_number':1024,'multiline':False]
['text':' NB: Previously, the test was for fn.requires_grad, but we don't have','line_number':1062,'multiline':False]
['text':' this information.  Use 'train' as a proxy.','line_number':1063,'multiline':False]
['text':' inference','line_number':1088,'multiline':False]
['text':' TODO: Set device to input','line_number':1134,'multiline':False]
['text':' TODO: more compact way of saying this','line_number':1163,'multiline':False]
['text':' TODO: put this in the correct device???','line_number':1207,'multiline':False]
['text':' NB: This MUST BE CALLED AFTER _cudnn_rnn_backward_input.','line_number':1234,'multiline':False]
['text':' We'll give a user friendly combined function...','line_number':1235,'multiline':False]
['text':' TODO: I think tensor geometry sufficient for weight_buf/weight','line_number':1237,'multiline':False]
['text':' TODO: the above were the only checks in rnn.py, but it doesn't seem','line_number':1281,'multiline':False]
['text':' like these checks are enough','line_number':1282,'multiline':False]
['text':' We need this dispatcher because _cudnn_rnn_backward_weight has a stringent','line_number':1343,'multiline':False]
['text':' ordering requirement with _cudnn_rnn_backward_input','line_number':1344,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':1353,'multiline':False]
['text':' NB: unconditionally compute this gradient, because it mutates reserve','line_number':1370,'multiline':False]
['text':' TODO: I am not sure if we actually need the 'dropout' and 'train' parameters','line_number':1379,'multiline':False]
['text':' to initialize just the state tensor','line_number':1380,'multiline':False]
['text':'','line_number':1381,'multiline':False]
['text':' NB: You can have any color you like, as long as it's a CUDA byte','line_number':1382,'multiline':False]
['text':' tensor.  Why does this function take a TensorOptions at all in that case?','line_number':1383,'multiline':False]
['text':' This is a factory function: it produces tensors but takes no tensors','line_number':1384,'multiline':False]
['text':' as input.  The codegen currently assumes that ALL factory functions','line_number':1385,'multiline':False]
['text':' take TensorOptions, so it's just a lot easier for this function to','line_number':1386,'multiline':False]
['text':' be bound if it also does it.','line_number':1387,'multiline':False]
['text':' See [Note: hacky wrapper removal for TensorOptions]','line_number':1393,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1403,'multiline':False]
['text':' CUDA dispatch for the generic RNN ops (at::lstm, at::gru, ...)','line_number':1404,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':1405,'multiline':False]
['text':' Helpers for working with different hidden types.','line_number':1409,'multiline':False]
['text':'*
 * Note [DropoutState and CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * (1) Telling a capturing stream to wait on an event recorded in a non-capturing stream is an error.
 * (2) Telling a non-capturing stream to wait on an event recorded during capture is also an error.
 *
 * So DropoutState's usage syncs could error if an RNN with dropout is called in an uncaptured region
 * then called in a captured region (triggering 1), or called in a captured region then called
 # in an uncaptured region (triggering 2).
 *
 * To prevent 1 and 2, lock() only syncs on the last usage event if it was recorded in the same
 * capture state as the current state (which also means the same graph, if capture is in progress).
 *
 * The solution should be safe as long as capture obeys the following restrictions:
 *  - Only one capture may be underway at a time in a given process.
 *  - While a capture is underway, no calls to eager ops on noncapturing streams (on any thread)
 *    may interleave with the captured ops.
 *
 * TODO: As people experiment with capture, keep an eye out for use cases that might need to
 * relax those restrictions.
 *
 * See https://github.com/pytorch/pytorch/pull/56433 for more discussion.
 ','line_number':1435,'multiline':True]
['text':' Both buffer and event are lazily instantiated when a dropout state is needed','line_number':1460,'multiline':False]
['text':' for the first time. Note that in this case needed != used, as we don't need','line_number':1461,'multiline':False]
['text':' a buffer to e.g. run RNNs in test mode.','line_number':1462,'multiline':False]
['text':' cudaStreamGetCaptureInfo will never give back a capture id of 0, so 0 can serve','line_number':1467,'multiline':False]
['text':' as a sentinel value that capture was not underway.','line_number':1468,'multiline':False]
['text':' Every time we use a dropout state, we need to synchronize with its event,','line_number':1473,'multiline':False]
['text':' to make sure all previous uses finish running before this one starts. Once','line_number':1474,'multiline':False]
['text':' we're done, we record the event to allow others to synchronize with this kernel.','line_number':1475,'multiline':False]
['text':' Those events are really needed only for inter-stream sync on a single GPU.','line_number':1476,'multiline':False]
['text':' I doubt anyone will want to run cuDNN RNNs in parallel on a single GPU, so','line_number':1477,'multiline':False]
['text':' they should end up being complete no-ops.','line_number':1478,'multiline':False]
['text':' NB: We can't ignore the lock even when event is undefined, because someone','line_number':1480,'multiline':False]
['text':' could then define it before we get to unlock().','line_number':1481,'multiline':False]
['text':' See Note [DropoutState and CUDA graph capture]','line_number':1485,'multiline':False]
['text':' See Note [DropoutState and CUDA graph capture]','line_number':1506,'multiline':False]
['text':' Each state is slightly over 2MB and initialized lazily, so it's fine to cache them.','line_number':1522,'multiline':False]
['text':' NB: CUDA binds the event to a device at creation time, so we can initialize it','line_number':1540,'multiline':False]
['text':' only now, when we know we're on the correct device.','line_number':1541,'multiline':False]
['text':' Prepare all relevant descriptors','line_number':1554,'multiline':False]
['text':' Something very naughty is happening here.  try_get_weight_buf','line_number':1559,'multiline':False]
['text':' is called from _cudnn_impl, which is a *composite*.  In other words,','line_number':1560,'multiline':False]
['text':' inside the composite function we need to query cudnn to figure out how big','line_number':1561,'multiline':False]
['text':' the weight buf actually is going to be.  This clearly cannot be done','line_number':1562,'multiline':False]
['text':' symbolically.  For now, we insert guards here; but once we have the black','line_number':1563,'multiline':False]
['text':' box handling for dynamic shapes, we could also hypothetically infer out','line_number':1564,'multiline':False]
['text':' the relationships','line_number':1565,'multiline':False]
['text':' datatype for x_desc comes from any_param, not input.','line_number':1572,'multiline':False]
['text':' try_get_weight_buf's job is to check "is the weight buffer correctly laid out','line_number':1573,'multiline':False]
['text':' for us to run it with input of the same datatype?"','line_number':1574,'multiline':False]
['text':' Try to get parameter storage','line_number':1579,'multiline':False]
['text':' Get and check data pointers','line_number':1588,'multiline':False]
['text':' For LSTM models with projections hidden size could be different','line_number':1636,'multiline':False]
['text':' TODO:  try_get_weight_buf returns a Tensor, but _cudnn_rnn below takes a c10::optional<Tensor>','line_number':1642,'multiline':False]
['text':' in weight_buf's slot.  Do we want try_get_weight_buf to return a c10::optional<Tensor>','line_number':1643,'multiline':False]
['text':' instead of a defined or undefined Tensor?','line_number':1644,'multiline':False]
['text':' cudnn_output = std::tuple<output, hy, cy, reserve, new_weight_buf>','line_number':1659,'multiline':False]
['text':'batch_first=','line_number':1662,'multiline':True]
['text':' For LSTM models with projections hidden size could be different','line_number':1678,'multiline':False]
['text':' cudnn_output = std::tuple<output, hy, cy, reserve, new_weight_buf>','line_number':1692,'multiline':False]
['text':'batch_sizes=','line_number':1696,'multiline':True]
['text':' anonymous namepsace','line_number':1751,'multiline':False]
['text':' namespace at::native','line_number':1753,'multiline':False]
['text':' AT_CUDNN_ENABLED()','line_number':1755,'multiline':False]
