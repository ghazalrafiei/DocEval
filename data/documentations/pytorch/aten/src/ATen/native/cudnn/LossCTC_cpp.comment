['text':' See Note [ATen preprocessor philosophy]','line_number':25,'multiline':False]
['text':' AT_CUDNN_ENABLED','line_number':62,'multiline':False]
['text':' we don't know that input_lengths and target_lengths have the same size','line_number':90,'multiline':False]
['text':' (they should, but we didn't check yet)','line_number':91,'multiline':False]
['text':' target length < 256 is documented, but we see illegal memory accesses','line_number':97,'multiline':False]
['text':' when target lengths > input lengths for CuDNN','line_number':98,'multiline':False]
['text':' only used for backward','line_number':121,'multiline':False]
['text':' ?','line_number':129,'multiline':False]
['text':' checked in dispatch:','line_number':140,'multiline':False]
['text':' assert other conditions for cudnnCTCLoss: all label lengths <= 256','line_number':141,'multiline':False]
['text':' all input lengths = logprob.size(0)','line_number':142,'multiline':False]
['text':' so the CuDNN gradient semantics have changed between 7.1 and 7.6,','line_number':150,'multiline':False]
['text':' this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.','line_number':151,'multiline':False]
['text':' namespace at::native','line_number':206,'multiline':False]
