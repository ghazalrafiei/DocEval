['text':' special case copy where tensor is contiguous and src is a transposed matrix','line_number':66,'multiline':False]
['text':' This can be generalized to most copies, but it's trickier','line_number':67,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':69,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)','line_number':72,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)','line_number':75,'multiline':False]
['text':' The code below is implemented with the assumption that sizes are equal','line_number':80,'multiline':False]
['text':' 1. copy columns from src to buf','line_number':98,'multiline':False]
['text':' 2. transpose buf in place','line_number':103,'multiline':False]
['text':' 3. copy rows from buf to dst','line_number':115,'multiline':False]
['text':' Devices directly supported by this copy implementation. Other device types','line_number':124,'multiline':False]
['text':' (e.g. XLA) may be supported by overriding copy_ and _copy_from.','line_number':125,'multiline':False]
['text':' namespace','line_number':131,'multiline':False]
['text':' TODO: this should be handled during dispatch, but that's missing...','line_number':137,'multiline':False]
['text':' FBGeMM kernel support exists only for the following case,','line_number':141,'multiline':False]
['text':' 1. Memory Format for source and destination tensors is contiguous.','line_number':142,'multiline':False]
['text':' 2. Device for both the source and destination tensor is CPU.','line_number':143,'multiline':False]
['text':' 3. dtype conversion between FP32->FP16 and FP16->FP32.','line_number':144,'multiline':False]
['text':' This checks that self.sizes() == src.sizes() because this code path doesn't','line_number':145,'multiline':False]
['text':' support broadcasting. This also guards against out of bounds memory access','line_number':146,'multiline':False]
['text':' when copying, see fbgemm::Float16ToFloat_ref.','line_number':147,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/88543','line_number':148,'multiline':False]
['text':' Copies into meta self are OK and just ignored (similar to inplace)','line_number':198,'multiline':False]
['text':' Re-dispatch copies when either src or self device not implemented here (e.g. XLA).','line_number':214,'multiline':False]
['text':' _copy_from has a proper device dispatch setup.','line_number':215,'multiline':False]
['text':' This includes:','line_number':216,'multiline':False]
['text':'   cpu_tensor.copy_(xla_tensor) => xla_tensor._copy_from(cpu_tensor)','line_number':217,'multiline':False]
['text':'   xla_tensor.copy_(cpu_tensor) => cpu_tensor._copy_from(xla_tensor)','line_number':218,'multiline':False]
['text':' Both the _copy_from calls above will be dispatched to XLA's _copy_from kernels.','line_number':219,'multiline':False]
['text':' Exit early if self and src are views of the same data','line_number':252,'multiline':False]
['text':' TODO: if we need to, we can also enable this path for quantized tensor','line_number':288,'multiline':False]
['text':' copy() is the "functional" form of copy_(). It exists so we can properly functionalize copy_(), but:','line_number':308,'multiline':False]
['text':' (1) It isn't exposed to the frontend (no python bindings)','line_number':309,'multiline':False]
['text':' (2) It isn't exposed to the backend (it's a composite, that decomposes into to() and expand_as() calls.','line_number':310,'multiline':False]
['text':' Called when we are copying into an overlapping index `dst`, but we don't','line_number':333,'multiline':False]
['text':' care which writer wins. Hacky but it works. This is only used by','line_number':334,'multiline':False]
['text':' CUDA_tensor_apply2 in case that there are write overlaps.','line_number':335,'multiline':False]
['text':' FIXME: really, overlapping writes should be illegal/an error in Torch','line_number':336,'multiline':False]
['text':'non_blocking=','line_number':345,'multiline':True]
['text':' namespace native','line_number':354,'multiline':False]
['text':' namespace at','line_number':355,'multiline':False]
