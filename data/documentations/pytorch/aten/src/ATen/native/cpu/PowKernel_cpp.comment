['text':' The source-code of kernels for float, double and complex types is similar,','line_number':43,'multiline':False]
['text':' barring a small distinction - even if the output dtype is float, a double','line_number':44,'multiline':False]
['text':' exponent can be used. But Complex types' computation doesn't allow standard','line_number':45,'multiline':False]
['text':' & double-precision to be mixed, since std::pow takes either complex64 inputs,','line_number':46,'multiline':False]
['text':' or complex128 inputs, but not both. So, in order to provide a common path for','line_number':47,'multiline':False]
['text':' float, double & complex types, template parameter cast_scalar_t is being used','line_number':48,'multiline':False]
['text':' to resolve the aforementioned distinction. This approach also allows BFloat16','line_number':49,'multiline':False]
['text':' to use this common-path. Half cannot currently use it, as AVX2 support for','line_number':50,'multiline':False]
['text':' sqrt & rsqrt doesn't currently exist for it.','line_number':51,'multiline':False]
['text':' .5 (sqrt), -.5 (rsqrt) and -1 (reciprocal) specializations are handled','line_number':55,'multiline':False]
['text':' in pow_tensor_scalar_kernel','line_number':56,'multiline':False]
['text':' prevent multiple calls to iter.common_dtype()','line_number':92,'multiline':False]
['text':' Dispatch to fast specialization for sqrt, rsqrt and reciprocal','line_number':97,'multiline':False]
['text':' anonymous namespace','line_number':145,'multiline':False]
['text':' namespace at::native','line_number':150,'multiline':False]
