['text':' Given a linear index, returns the offset of the tensor.','line_number':32,'multiline':False]
['text':' Implements the same algorithm as its (legacy) GPU version cuda::detail::IndexToOffset','line_number':33,'multiline':False]
['text':' OffsetCalculator implements yet again the same algorithm but in a column-major order','line_number':34,'multiline':False]
['text':' This kernel follows the same strategy as `cpu_index_kernel`','line_number':59,'multiline':False]
['text':' Even though the indexed_tensor is const, we modify it through the data_ptr','line_number':60,'multiline':False]
['text':' This is a bit dirty, but otherwise it would be necessary to innecessarily add tensor','line_number':61,'multiline':False]
['text':' with zero strides to `iter` which would not be much better','line_number':62,'multiline':False]
['text':' When launch the parallel version, set a relative small grain size less than the INTERNAL::GRAIN_SIZE','line_number':64,'multiline':False]
['text':' to make the whole available thread numbers get more balanced work load and a better cache location.','line_number':65,'multiline':False]
['text':' The grain size here is chosen by the op benchmark to overcome the thread launch overhead','line_number':66,'multiline':False]
['text':' Perhaps tweak this number for `put_`? This number was tweaked for `index_put`','line_number':67,'multiline':False]
['text':' iter could be const, but for_each does not have a const version','line_number':108,'multiline':False]
['text':' nb. This deterministic issue the same as that of `index_put_kernel`','line_number':110,'multiline':False]
['text':' See Note [Enabling Deterministic Operations]','line_number':111,'multiline':False]
['text':' Parallel cpu_put_kernel with accumulation is nondeterministic, so we','line_number':112,'multiline':False]
['text':' must enable serial execution if deterministic algorithms are enabled.','line_number':113,'multiline':False]
['text':' TODO: investigate parallelization of the accumulate kernel.','line_number':123,'multiline':False]
['text':' Unlike the non-accumulate case, this needs to be thread-safe.','line_number':124,'multiline':False]
['text':'serial_execution=','line_number':129,'multiline':True]
['text':' NOTE: duplicate indices are only supported if accumulate is true.','line_number':153,'multiline':False]
['text':' See Note [Enabling Deterministic Operations]','line_number':156,'multiline':False]
['text':' Parallel cpu_index_kernel with accumulation is nondeterministic, so we','line_number':157,'multiline':False]
['text':' must enable serial execution if deterministic algorithms are enabled.','line_number':158,'multiline':False]
['text':' TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,','line_number':168,'multiline':False]
['text':' this needs to be thread-safe.','line_number':169,'multiline':False]
['text':'serial_execution=','line_number':172,'multiline':True]
['text':'serial_execution=','line_number':177,'multiline':True]
['text':' Here ntensors is defined for output and 1 input. But tensor iterator has defined output, input','line_number':451,'multiline':False]
['text':' and restrided_input (see aten/src/ATen/native/TensorTransformations.cpp#L64-L66) but we use only','line_number':452,'multiline':False]
['text':' output and input.','line_number':453,'multiline':False]
['text':' vectorized loop with negative stride for output','line_number':467,'multiline':False]
['text':' data[0] unaligned pre-pass','line_number':478,'multiline':False]
['text':' Empirically found that it is faster to process 3 data items together vs 2 or 4','line_number':485,'multiline':False]
['text':' flip the vector: 1234 -> 4321','line_number':490,'multiline':False]
['text':' advance:','line_number':505,'multiline':False]
['text':' This is a vertical flip specialization using memcpy to speed-up the runtime','line_number':518,'multiline':False]
['text':' Here ntensors is defined for output and 1 input. But tensor iterator has defined output, input','line_number':522,'multiline':False]
['text':' and restrided_input (see aten/src/ATen/native/TensorTransformations.cpp#L64-L66) but we use only','line_number':523,'multiline':False]
['text':' output and input.','line_number':524,'multiline':False]
['text':' advance:','line_number':546,'multiline':False]
['text':' Example for num channels=3 and dtype=uint8','line_number':582,'multiline':False]
['text':' -> data_stride = 3','line_number':583,'multiline':False]
['text':' -> usable_vec_stride = 30','line_number':584,'multiline':False]
['text':' -> usable_vec_half_stride = 15','line_number':585,'multiline':False]
['text':' Data: (1 2 3) (4 5 6) (7 8 9) (10 11 12) (13 14 15) (16 17 18) (19 20 21) (22 23 24) (25 26 27) (28 29 30) (31 32 33)','line_number':586,'multiline':False]
['text':' load by 2 parts','line_number':587,'multiline':False]
['text':' R = [ (1 2 3) (4 5 6) (7 8 9) (10 11 12) (13 14 15) (16 | (16 17 18) (19 20 21) (22 23 24) (25 26 27) (28 29 30) (31 ]','line_number':588,'multiline':False]
['text':' flip(R) ->','line_number':589,'multiline':False]
['text':' R = [ 31 (28 29 30) (25 26 27) (22 23 24) (19 20 21) (16 17 18) | 16 (13 14 15) (10 11 12) (7 8 9) (4 5 6) (1 2 3) ]','line_number':590,'multiline':False]
['text':'','line_number':591,'multiline':False]
['text':' Write in 2 parts','line_number':592,'multiline':False]
['text':' Output pointer: output_ptr = data[0]                                                                                  v','line_number':593,'multiline':False]
['text':' - Init:','line_number':594,'multiline':False]
['text':'                (X X X)  (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X) (X X X) (X X X)','line_number':595,'multiline':False]
['text':' 0) Move to initial position: output_ptr = data[0] + data_stride - vec_size / 2;','line_number':596,'multiline':False]
['text':'                                                                          v','line_number':597,'multiline':False]
['text':'                (X X X)  (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X)    (X X X) (X X X) (X X X)','line_number':598,'multiline':False]
['text':' - In the loop:','line_number':599,'multiline':False]
['text':' 1) Write 1st block from output_ptr','line_number':600,'multiline':False]
['text':'                                                                            v','line_number':601,'multiline':False]
['text':'                                                                            |----> vec_size / 2 ---------------------------|','line_number':602,'multiline':False]
['text':' Output part 1: (X X X)  (X X X)    (X X X)    (X X X)    (X X X)     (X X 16)  (13 14 15) (10 11 12) (7 8 9) (4 5 6) (1 2 3)','line_number':603,'multiline':False]
['text':' 2) Write 2nd block from output_ptr - usable_vec_half_stride:','line_number':604,'multiline':False]
['text':'                                                                            v','line_number':605,'multiline':False]
['text':'                     |-----> vec_size / 2 ----------------------------------|','line_number':606,'multiline':False]
['text':' Output part 2: (X X 31) (28 29 30) (25 26 27) (22 23 24) (19 20 21) (16 17 18) (13 14 15) (10 11 12) (7 8 9) (4 5 6) (1 2 3)','line_number':607,'multiline':False]
['text':'','line_number':608,'multiline':False]
['text':' 3) Move to the next position: output_ptr -= usable_vec_stride','line_number':609,'multiline':False]
['text':'','line_number':610,'multiline':False]
['text':' - After the loop:','line_number':611,'multiline':False]
['text':' 4) Move to write position','line_number':612,'multiline':False]
['text':'                 v','line_number':613,'multiline':False]
['text':'                (X X 31) (28 29 30) (25 26 27) (22 23 24) (19 20 21) (16 17 18) (13 14 15) (10 11 12) (7 8 9) (4 5 6) (1 2 3)','line_number':614,'multiline':False]
['text':' load 256-bits by two 128-bits parts','line_number':626,'multiline':False]
['text':' write output in two parts','line_number':634,'multiline':False]
['text':' Generate avx mask once','line_number':653,'multiline':False]
['text':' Here ntensors is defined for output and 1 input. But tensor iterator has defined output, input','line_number':658,'multiline':False]
['text':' and restrided_input (see aten/src/ATen/native/TensorTransformations.cpp#L64-L66) but we use only','line_number':659,'multiline':False]
['text':' output and input.','line_number':660,'multiline':False]
['text':' advance:','line_number':684,'multiline':False]
['text':'dummy input','line_number':701,'multiline':True]
['text':' Special case: horizontal flip with vectorization and input is contiguous','line_number':709,'multiline':False]
['text':' Context: horizontal flip leads to strides[0] < 0 and','line_number':710,'multiline':False]
['text':' thus is_contiguous condition is not satisfied and non-vectorized code path is taken.','line_number':711,'multiline':False]
['text':' Ignoring half and bfloat16 as cpu_hflip_vec is slower than cpu_kernel_vec','line_number':713,'multiline':False]
['text':' Replace AT_DISPATCH_ALL_TYPES_AND by manual if/else due to internal test failures:','line_number':715,'multiline':False]
['text':' - "dtype 'Float' not selected for kernel tag hflip_cpu"','line_number':716,'multiline':False]
['text':' - "dtype 'Long' not selected for kernel tag hflip_cpu"','line_number':717,'multiline':False]
['text':'','line_number':718,'multiline':False]
['text':' AT_DISPATCH_ALL_TYPES_AND(kBool,','line_number':719,'multiline':False]
['text':'     iter_dtype, "hflip_cpu", [&iter] {','line_number':720,'multiline':False]
['text':'       cpu_hflip_vec<scalar_t>(iter);','line_number':721,'multiline':False]
['text':' });','line_number':722,'multiline':False]
['text':' other dtypes (float16, bfloat16, complex) are handled by cpu_kernel_vec (see below)','line_number':742,'multiline':False]
['text':' Special cases:','line_number':744,'multiline':False]
['text':' a) channels last hflip on (N, C, H, W) and outer_stride(=dtype_size * C) in [2, 16]','line_number':745,'multiline':False]
['text':' b) flip dim=-2 on (N, ..., M, C) and outer_stride(=dtype_size * C) in [2, 16]','line_number':746,'multiline':False]
['text':' checks if dim=1 is contiguous as well','line_number':752,'multiline':False]
['text':' Special case: vertical flip using memcpy (faster than generic cpu_kernel_vec)','line_number':756,'multiline':False]
['text':'dummy input','line_number':762,'multiline':True]
['text':'dummy input','line_number':765,'multiline':True]
['text':' anonymous namespace','line_number':772,'multiline':False]
['text':' namespace at::native','line_number':786,'multiline':False]
