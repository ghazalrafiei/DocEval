['text':' First the required LAPACK implementations are registered here.','line_number':120,'multiline':False]
['text':' A comment above the registered LAPACK routine suggest which batched','line_number':121,'multiline':False]
['text':' linear algebra function uses that routine','line_number':122,'multiline':False]
['text':' getrf','line_number':125,'multiline':False]
['text':' potrs','line_number':131,'multiline':False]
['text':' potrf','line_number':137,'multiline':False]
['text':' potri','line_number':143,'multiline':False]
['text':' sytrf','line_number':149,'multiline':False]
['text':' hetrf','line_number':187,'multiline':False]
['text':' sytrs','line_number':207,'multiline':False]
['text':' hetrs','line_number':249,'multiline':False]
['text':' geqrf','line_number':271,'multiline':False]
['text':' orgqr','line_number':277,'multiline':False]
['text':' ormqr','line_number':283,'multiline':False]
['text':' syevd','line_number':289,'multiline':False]
['text':' geev','line_number':295,'multiline':False]
['text':' gesdd','line_number':315,'multiline':False]
['text':' getrs','line_number':325,'multiline':False]
['text':' gels','line_number':331,'multiline':False]
['text':' gelsd','line_number':345,'multiline':False]
['text':' gelsy','line_number':363,'multiline':False]
['text':' gelss','line_number':383,'multiline':False]
['text':' trsm','line_number':405,'multiline':False]
['text':' prefer column major strides','line_number':423,'multiline':False]
['text':'f-contig=','line_number':424,'multiline':True]
['text':' LD','line_number':425,'multiline':False]
['text':' pivots','line_number':428,'multiline':False]
['text':' info','line_number':431,'multiline':False]
['text':' pivots is allowed to be any integer type','line_number':453,'multiline':False]
['text':' LAPACK we use is 32-bit interface while cuSOLVER uses 64-bit interface for integers','line_number':454,'multiline':False]
['text':'includeBool=','line_number':456,'multiline':True]
['text':' prefer column major strides','line_number':470,'multiline':False]
['text':'column_major=','line_number':471,'multiline':True]
['text':' make column major strides for BLAS','line_number':487,'multiline':False]
['text':'f-contig=','line_number':488,'multiline':True]
['text':' make column major strides for BLAS','line_number':491,'multiline':False]
['text':'f_contig=','line_number':492,'multiline':True]
['text':' no broadcasting for non-strided layout','line_number':495,'multiline':False]
['text':' make row major strides for Sparse BLAS','line_number':496,'multiline':False]
['text':' return 0-sized tensor','line_number':497,'multiline':False]
['text':' dtype','line_number':507,'multiline':False]
['text':' NumPy compat: Two types of 'B' tensors are supported:','line_number':513,'multiline':False]
['text':' - 1D tensor or batch of 1D tensors (vector case)','line_number':514,'multiline':False]
['text':' - 2D tensor or batch of 2D tensors (matrix case)','line_number':515,'multiline':False]
['text':' matrix shapes','line_number':519,'multiline':False]
['text':'left=','line_number':520,'multiline':True]
['text':' Check that B can be broadcasted to the shape of A','line_number':522,'multiline':False]
['text':' We disallow the broadcasting of B as a vector when left=False as, in that case, A.shape = (*, 1, 1)','line_number':524,'multiline':False]
['text':'column_major=','line_number':528,'multiline':True]
['text':' LU','line_number':535,'multiline':False]
['text':'f-contig*=','line_number':536,'multiline':True]
['text':' pivots','line_number':539,'multiline':False]
['text':' info','line_number':542,'multiline':False]
['text':'allow_low_precision_dtypes','line_number':548,'multiline':True]
['text':'f-contig*=','line_number':552,'multiline':True]
['text':' info','line_number':555,'multiline':False]
['text':' make column major strides for BLAS','line_number':565,'multiline':False]
['text':'f-contig*=','line_number':566,'multiline':True]
['text':' Set sizes to the size of pivots','line_number':569,'multiline':False]
['text':' Set sizes to the size of info','line_number':574,'multiline':False]
['text':' dtype','line_number':584,'multiline':False]
['text':' matrix shapes','line_number':592,'multiline':False]
['text':'','line_number':595,'multiline':False]
['text':' batches','line_number':599,'multiline':False]
['text':' This one checks that B can be broadcasted to the shape of A','line_number':605,'multiline':False]
['text':'column_major=','line_number':607,'multiline':True]
['text':' L','line_number':621,'multiline':False]
['text':'f-contig*=','line_number':622,'multiline':True]
['text':' info','line_number':625,'multiline':False]
['text':'f-contig*=','line_number':644,'multiline':True]
['text':' For readability','line_number':650,'multiline':False]
['text':'f-contig*=','line_number':653,'multiline':True]
['text':' Prepare sizes for U','line_number':670,'multiline':False]
['text':'f-contig*=','line_number':673,'multiline':True]
['text':' Prepare sizes for Vh','line_number':676,'multiline':False]
['text':' We need to distinguish the cuSOLVER case, as the cuSOLVER algorithms we use','line_number':680,'multiline':False]
['text':' expect F-contig matrices, but they compute V rather than Vh','line_number':681,'multiline':False]
['text':'f-contig*=','line_number':683,'multiline':True]
['text':' Prepare sizes for S. S is always real, even when A is complex.','line_number':690,'multiline':False]
['text':' P.shape[-2:] == (m, m) (or size zero if pivot == False)','line_number':709,'multiline':False]
['text':' L.shape[-2:] == (m, k)','line_number':718,'multiline':False]
['text':' U.shape[-2:] == (k, n)','line_number':722,'multiline':False]
['text':' eigenvectors','line_number':740,'multiline':False]
['text':'f-contig*=','line_number':741,'multiline':True]
['text':' eigenvalues','line_number':747,'multiline':False]
['text':' P.shape[-2:] == (m, m) (or size zero if pivot == False)','line_number':760,'multiline':False]
['text':' L.shape[-2:] == (m, k)','line_number':768,'multiline':False]
['text':' U.shape[-2:] == (k, n)','line_number':772,'multiline':False]
['text':' namespace meta','line_number':778,'multiline':False]
['text':' Define the per-batch functions to be used in the main implementation of the batched','line_number':783,'multiline':False]
['text':' linear algebra operations','line_number':784,'multiline':False]
['text':' unused','line_number':913,'multiline':False]
['text':' unused','line_number':914,'multiline':False]
['text':' unused','line_number':919,'multiline':False]
['text':' unused','line_number':920,'multiline':False]
['text':' lapack [sd]geev wants to separate output arrays: wr and wi for the real','line_number':925,'multiline':False]
['text':' and imaginary parts','line_number':926,'multiline':False]
['text':' unused','line_number':929,'multiline':False]
['text':' lapack [sd]geev wants to separate output arrays: wr and wi for the real','line_number':934,'multiline':False]
['text':' and imaginary parts','line_number':935,'multiline':False]
['text':' unused','line_number':938,'multiline':False]
['text':' If it's all zeros, we return early.','line_number':1507,'multiline':False]
['text':' We optimise for the most likely case.','line_number':1508,'multiline':False]
['text':' batch_str needn't be set for matrices','line_number':1517,'multiline':False]
['text':' Find the first non-zero info','line_number':1519,'multiline':False]
['text':' Reference LAPACK 3.10+ changed `info` behavior for inputs with non-finite values','line_number':1529,'multiline':False]
['text':' Previously, it would return `info` > 0, but now it returns `info` = -4','line_number':1530,'multiline':False]
['text':' OpenBLAS 0.3.15+ uses the Reference LAPACK 3.10+.','line_number':1531,'multiline':False]
['text':' MKL 2022.0+ uses the Reference LAPACK 3.10+.','line_number':1532,'multiline':False]
['text':' Older version of MKL and OpenBLAS follow the old behavior (return `info` > 0).','line_number':1533,'multiline':False]
['text':' Here we check for the case where `info` is -4 and raise an error','line_number':1534,'multiline':False]
['text':' inv, inverse, cholesky_inverse, etc.','line_number':1543,'multiline':False]
['text':' solve, linalg_solve, cholesky_solve, etc.','line_number':1547,'multiline':False]
['text':' We should never reach this point as info was non-zero','line_number':1571,'multiline':False]
['text':' If an input requires fw or bw grad then we need to go down a different','line_number':1575,'multiline':False]
['text':' (slower) path to ensure that the gradients are computable.','line_number':1576,'multiline':False]
['text':' That is what `_may_require_fw_or_bw_grad` is helpful for.','line_number':1577,'multiline':False]
['text':'','line_number':1578,'multiline':False]
['text':' Why is there a isTensorSubclassLike check here?','line_number':1579,'multiline':False]
['text':' Without it, this function can lead to composite compliance problems, which','line_number':1580,'multiline':False]
['text':' may lead to bugs in functorch, where a Tensor Subclass that doesn't','line_number':1581,'multiline':False]
['text':' require grad may wrap a Tensor subclass that requires grad.','line_number':1582,'multiline':False]
['text':'level ','line_number':1585,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg.inv ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1589,'multiline':False]
['text':' Fill result with the identity','line_number':1591,'multiline':False]
['text':'left','line_number':1594,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ cholesky_solve ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1622,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':1641,'multiline':False]
['text':' Supports arbitrary batch dimensions for self and A','line_number':1667,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ cholesky ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1687,'multiline':False]
['text':' self.shape[:-2]','line_number':1711,'multiline':False]
['text':' fill the raw_cholesky_output with the result','line_number':1714,'multiline':False]
['text':' Nothing to do there','line_number':1752,'multiline':False]
['text':' We can perform this optimisation just on CPU as it fails for MAGMA','line_number':1759,'multiline':False]
['text':' due to some bug','line_number':1760,'multiline':False]
['text':'check_errors=','line_number':1788,'multiline':True]
['text':'check_errors=','line_number':1795,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ cholesky_inverse ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1800,'multiline':False]
['text':' if result has no elements we can modify it','line_number':1815,'multiline':False]
['text':' result tensor must be in batched column major order (Fortran contiguous)','line_number':1821,'multiline':False]
['text':' cholesky_inverse_stub (apply_cholesky_inverse) performs calculations in-place and result must be a copy of input','line_number':1825,'multiline':False]
['text':' infos must be contiguous','line_number':1828,'multiline':False]
['text':' MAGMA requires 'infos' to reside in CPU memory, therefore we create 'infos' only on CPU for now.','line_number':1841,'multiline':False]
['text':' if result is not empty and not in batched column major format','line_number':1851,'multiline':False]
['text':' or result does not have the same dtype as input','line_number':1853,'multiline':False]
['text':' or result does not have the expected shape','line_number':1854,'multiline':False]
['text':' we have to allocate a temporary tensor','line_number':1855,'multiline':False]
['text':' use result's memory directly','line_number':1862,'multiline':False]
['text':' Now check LAPACK/MAGMA error codes','line_number':1866,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg.solve ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1877,'multiline':False]
['text':' Auxiliary function that returns the LU decomposition to use it in the backward','line_number':1879,'multiline':False]
['text':' Possible optimization: Compute the LU factorization of A^T if A is contiguous','line_number':1888,'multiline':False]
['text':' Then we solve A^T X = B with adjoint=True','line_number':1889,'multiline':False]
['text':' This saves a copy as A doesn't need to be copied into an F-contig matrix in lu_factor','line_number':1890,'multiline':False]
['text':' This optimization makes functorch's batching rule difficult. See NOTE [ solve_ex Batch Rule Contiguity ]','line_number':1891,'multiline':False]
['text':' [numpy-compat] Handle vectors on the rhs','line_number':1901,'multiline':False]
['text':'adjoint','line_number':1905,'multiline':True]
['text':' We implement linalg_solve_ex as a composite function of _linalg_solve','line_number':1920,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ lu_factor ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1949,'multiline':False]
['text':' zero out the infos as it will have one element if the input is a matrix of size (0, 0)','line_number':1960,'multiline':False]
['text':' We pass check_errors as we want to use lu_factor rather than lu_factor_ex in the errors','line_number':1977,'multiline':False]
['text':'check_errors=','line_number':1978,'multiline':True]
['text':'check_errors=','line_number':1985,'multiline':True]
['text':' TODO Deprecate this function in favour of linalg_lu_factor_ex','line_number':1990,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg_lu ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2006,'multiline':False]
['text':' A.shape[-2:] == (m, n)','line_number':2018,'multiline':False]
['text':' P.shape[-2:] == (m, m)','line_number':2019,'multiline':False]
['text':' L.shape[-2:] == (m, k)','line_number':2020,'multiline':False]
['text':' U.shape[-2:] == (k, n)','line_number':2021,'multiline':False]
['text':' with k = min(m, n)','line_number':2022,'multiline':False]
['text':' Use L as it has the correct size','line_number':2024,'multiline':False]
['text':'check_errors=','line_number':2033,'multiline':True]
['text':'unpack_lu=','line_number':2039,'multiline':True]
['text':'unpack_pivots=','line_number':2040,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ lu_unpack ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2043,'multiline':False]
['text':' A.shape[-2:] == (m, n)','line_number':2055,'multiline':False]
['text':' P.shape[-2:] == (m, m)','line_number':2056,'multiline':False]
['text':' L.shape[-2:] == (m, k)','line_number':2057,'multiline':False]
['text':' U.shape[-2:] == (k, n)','line_number':2058,'multiline':False]
['text':' with k = min(m, n)','line_number':2059,'multiline':False]
['text':' The order of triu and tril is important as we may have LU.is_same(L)','line_number':2063,'multiline':False]
['text':' The order of triu and tril is important as we may have LU.is_same(U)','line_number':2068,'multiline':False]
['text':' lu_factor_ex returns an int32 1-based indexing, which is what we have in `pivots`','line_number':2075,'multiline':False]
['text':' We transform that to a proper permutation of the indices {0, ..., m-1}','line_number':2076,'multiline':False]
['text':' Fill `perm` with the identity permutation (perhaps batched)','line_number':2079,'multiline':False]
['text':' Note that perm is of type kLong and pivots is a 1-indexed kInt.','line_number':2084,'multiline':False]
['text':' This is taken into account in the unpack_pivots kernel','line_number':2085,'multiline':False]
['text':'squash_dim=','line_number':2090,'multiline':True]
['text':' Transform the permutation into a permutation matrix','line_number':2097,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg_lu_solve ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2103,'multiline':False]
['text':' Trivial case','line_number':2112,'multiline':False]
['text':' Solve A^H X = B^H. Then we return X^H','line_number':2117,'multiline':False]
['text':' Copy B (or B^H) into result','line_number':2123,'multiline':False]
['text':' Make LU / pivots F-contiguous','line_number':2128,'multiline':False]
['text':'row_major=','line_number':2131,'multiline':True]
['text':' Conj-transpose back in-place','line_number':2139,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ triangular_solve ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2172,'multiline':False]
['text':'
Solves the matrix equation 'input' @ 'result' = 'other' for the 'result'.
The result of the computation is saved in-place in 'result' tensor,
'clone_input' will be a copy of 'input',
'infos' is used to store information for possible checks for error,
'upper' controls the portion of input matrix to consider in computations,
'transpose' if true then 'input.mT()' @ 'result' = 'other' is solved,
'unitriangular' if true then the diagonal elements of 'input' are assumed to be 1
and the actual diagonal values are not used.
','line_number':2176,'multiline':True]
['text':' These internal asserts make explicit the assumptions in the implementation','line_number':2199,'multiline':False]
['text':' Error check with the actual error messages are done on the higher level of','line_number':2200,'multiline':False]
['text':' the hierarchy of calls','line_number':2201,'multiline':False]
['text':' if 'result' has no elements we can modify it','line_number':2213,'multiline':False]
['text':' make 'result' to have Fortran contiguous memory layout','line_number':2216,'multiline':False]
['text':' if 'clone_input' has no elements we can modify it','line_number':2219,'multiline':False]
['text':' make 'clone_input' to have Fortran contiguous memory layout','line_number':2222,'multiline':False]
['text':' 'result' and 'clone_input' must be in batched column major order (Fortran contiguous)','line_number':2225,'multiline':False]
['text':' triangular_solve_stub performs calculations in-place','line_number':2229,'multiline':False]
['text':' 'result' must be a copy of 'other'','line_number':2230,'multiline':False]
['text':' 'clone_input' must be a copy of 'input'','line_number':2231,'multiline':False]
['text':'left=','line_number':2237,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ qr ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2260,'multiline':False]
['text':' if 'QR' has no elements we can modify it','line_number':2273,'multiline':False]
['text':' make Fortran-contiguous','line_number':2276,'multiline':False]
['text':' input.shape[:-2]','line_number':2279,'multiline':False]
['text':' QR tensor must be in batched column major order (Fortran contiguous)','line_number':2285,'multiline':False]
['text':' tau tensor must be contiguous','line_number':2289,'multiline':False]
['text':' geqrf_stub (apply_geqrf) performs calculations in-place and 'QR' must be a copy of input','line_number':2293,'multiline':False]
['text':' 'a' is used in documentation and native_functions.yml','line_number':2301,'multiline':False]
['text':' input.shape[:-2]','line_number':2310,'multiline':False]
['text':' if 'QR' is not empty and not in batched column major format','line_number':2319,'multiline':False]
['text':' or 'QR' does not have the expected shape','line_number':2321,'multiline':False]
['text':' or 'QR' does not have the same dtype as input','line_number':2322,'multiline':False]
['text':' we have to allocate a temporary tensor','line_number':2323,'multiline':False]
['text':' or 'tau' does not have the expected shape','line_number':2326,'multiline':False]
['text':' or 'tau' does not have the same dtype as input','line_number':2327,'multiline':False]
['text':' use "out" tensors' storage directly','line_number':2340,'multiline':False]
['text':'
  Computes the QR decomposition using GEQRF and ORGQR operations.
  This is an in-place function and Q, R tensors must have correct shape and be Fortran contiguous.

  Args:
  * `input` - [in] Input tensor for QR decomposition
  * `Q` - [out] Tensor containing the Q matrices of QR decomposition
  * `R` - [out] Tensor containing the R matrices of QR decomposition
  * `compute_q` - controls whether the Q tensor is computed
  * `reduced_mode` - controls the size of Q and R tensors

  For further details, please see the LAPACK documentation for GEQRF and ORGQR.
','line_number':2354,'multiline':True]
['text':' We need an auxiliary tensor to call geqrf','line_number':2378,'multiline':False]
['text':' geqrf requires m x n workspace input that is modified in-place','line_number':2384,'multiline':False]
['text':' We try to use Q. If it doesn't fit, we try to use R','line_number':2385,'multiline':False]
['text':' If m > n and compute_q==false, it won't fit into Q or R, so we neet to create an auxiliary tensor','line_number':2386,'multiline':False]
['text':' Split QR into Q (unless compute_q == false) and R','line_number':2400,'multiline':False]
['text':' Copy QR into Q','line_number':2402,'multiline':False]
['text':' If the result didn't fit in Q and compute_q == true is because Q is not of size m x n (i.e. it's of size m x m)','line_number':2404,'multiline':False]
['text':' Copy QR into R from Q or the aux tensor','line_number':2414,'multiline':False]
['text':' Next perform ORGQR for Q using the result from GEQRF','line_number':2419,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ orgqr ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2449,'multiline':False]
['text':'
  The householder_product (orgqr) function allows reconstruction of an orthogonal (or unitary) matrix Q,
  from a sequence of elementary reflectors, such as is produced by the geqrf function.

  Args:
  * `input` - Tensor with the directions of the elementary reflectors below the diagonal.
  * `tau` - Tensor containing the magnitudes of the elementary reflectors.
  * `result` - result Tensor, which will contain the orthogonal (or unitary) matrix Q.

  For further details, please see the LAPACK/MAGMA documentation.
','line_number':2453,'multiline':True]
['text':' if result has no elements we can modify it','line_number':2475,'multiline':False]
['text':' result tensor must be in batched column major order (Fortran contiguous)','line_number':2481,'multiline':False]
['text':' tau tensor must be contiguous','line_number':2485,'multiline':False]
['text':' orgqr_stub (apply_orgqr) performs calculations in-place and result must be a copy of input','line_number':2492,'multiline':False]
['text':' input.shape[:-2]','line_number':2515,'multiline':False]
['text':' tau.shape[:-1]','line_number':2516,'multiline':False]
['text':' TODO: uncomment the following when passing incorrectly sized 'result' is not allowed','line_number':2533,'multiline':False]
['text':' if (result.numel() != 0) {','line_number':2534,'multiline':False]
['text':'   // Resize messes up the strides, so let's not use at::native::resize_output','line_number':2535,'multiline':False]
['text':'   TORCH_CHECK(result.sizes().equals(input.sizes()),','line_number':2536,'multiline':False]
['text':'   "result shape ", result.sizes(), " does not match input shape ", input.sizes());','line_number':2537,'multiline':False]
['text':' }','line_number':2538,'multiline':False]
['text':' if result is not empty and not in batched column major format','line_number':2547,'multiline':False]
['text':' or result does not have the same dtype as input','line_number':2549,'multiline':False]
['text':' or result does not have the expected shape','line_number':2550,'multiline':False]
['text':' we have to allocate a temporary tensor','line_number':2551,'multiline':False]
['text':' use result's storage directly','line_number':2558,'multiline':False]
['text':' torch.orgqr is an alias of torch.linalg.householder_product','line_number':2571,'multiline':False]
['text':' torch.linalg.householder_product is the preferred new function','line_number':2572,'multiline':False]
['text':' if 'result' has no elements we can modify it','line_number':2599,'multiline':False]
['text':' 'result' tensor must be in batched column major order (Fortran contiguous)','line_number':2605,'multiline':False]
['text':' 'tau' tensor must be contiguous','line_number':2609,'multiline':False]
['text':' 'input' tensor must be Fortran contiguous','line_number':2616,'multiline':False]
['text':' ormqr_stub (apply_ormqr) performs calculations in-place and 'result' must be a copy of 'other'','line_number':2624,'multiline':False]
['text':' input.shape[:-2]','line_number':2667,'multiline':False]
['text':' tau.shape[:-1]','line_number':2668,'multiline':False]
['text':' other.shape[:-2]','line_number':2674,'multiline':False]
['text':' if result is not empty and not in batched column major format','line_number':2704,'multiline':False]
['text':' or result does not have the expected shape','line_number':2706,'multiline':False]
['text':' we have to allocate a temporary tensor','line_number':2707,'multiline':False]
['text':' use result's storage directly','line_number':2714,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg_eigh ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2727,'multiline':False]
['text':'
  Computes eigenvalues and eigenvectors of the tensor 'input'.

  Args:
  * 'input' - input Tensor for eigendecomposition
  * 'values' - Tensor to store computed eigenvalues
  * 'vectors' - Tensor to store computed eigenvectors
  * 'infos' - Tensor to store LAPACK/MAGMA/cuSOLVER error codes
  * 'compute_eigenvectors' - controls whether eigenvectors should be computed
  * 'uplo' - controls the portion of input matrix to consider in computations, allowed values are "u", "U", "l", "L"
    "u", "U" - upper triangular portion of the input matrix is used in computations; "l", "L" - lower.
','line_number':2731,'multiline':True]
['text':' We need a tensor to hold A','line_number':2760,'multiline':False]
['text':'is_matrix','line_number':2767,'multiline':True]
['text':' TODO (Good intro task) Implement linalg_eigh_ex_out','line_number':2771,'multiline':False]
['text':'compute_v','line_number':2772,'multiline':True]
['text':'compute_v=','line_number':2776,'multiline':True]
['text':'compute_v=','line_number':2782,'multiline':True]
['text':'comptue_v=','line_number':2787,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg_eig ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':2791,'multiline':False]
['text':' This function returns complex-valued eigenvectors that is obtained from LAPACK GEEV's real-valued output','line_number':2793,'multiline':False]
['text':' This function is also used for the MAGMA path because intermediate MAGMA's results live on CPU','line_number':2794,'multiline':False]
['text':' From GEEV documentation:','line_number':2797,'multiline':False]
['text':' Complex conjugate pairs of eigenvalues appear consecutively with the eigenvalue having the positive imaginary part first','line_number':2798,'multiline':False]
['text':' If the j-th eigenvalue is real, then v(j) = VR(:,j), the j-th column of VR.','line_number':2799,'multiline':False]
['text':' If the j-th and (j+1)-st eigenvalues form a complex conjugate pair, then v(j) = VR(:,j) + i*VR(:,j+1) and v(j+1) = VR(:,j) - i*VR(:,j+1).','line_number':2800,'multiline':False]
['text':' eigenvalue is real, then v(j) = VR(:,j)','line_number':2815,'multiline':False]
['text':' v(j)   = VR(:,j) + i*VR(:,j+1)','line_number':2821,'multiline':False]
['text':' v(j+1) = VR(:,j) - i*VR(:,j+1)','line_number':2822,'multiline':False]
['text':' These asserts make explicit the requirements on tensors for 'linalg_eig_make_complex_eigenvectors_impl'','line_number':2831,'multiline':False]
['text':' MAGMA doesn't have GPU interface for GEEV routine, it requires inputs to be on CPU','line_number':2853,'multiline':False]
['text':' therefore we create all intermediate tensors on CPU','line_number':2854,'multiline':False]
['text':' These internal asserts make explicit the assumptions in the implementation','line_number':2857,'multiline':False]
['text':' Error check with the actual error messages are done on the higher level of the hierarchy of calls','line_number':2858,'multiline':False]
['text':' for real-valued 'input', eigenvalues can be real-valued or complex-valued','line_number':2862,'multiline':False]
['text':' for real-valued 'input', eigenvectors can be real-valued or complex-valued','line_number':2866,'multiline':False]
['text':' if 'vectors' has no elements we can modify it','line_number':2877,'multiline':False]
['text':' make 'vectors' to have Fortran contiguous memory layout','line_number':2880,'multiline':False]
['text':' if 'values' has no elements we can modify it','line_number':2883,'multiline':False]
['text':' input.shape[:-1]','line_number':2884,'multiline':False]
['text':' 'vectors' must be in batched column major order (Fortran contiguous)','line_number':2889,'multiline':False]
['text':' 'values' must be contiguous','line_number':2895,'multiline':False]
['text':' if 'input' is complex then use 'values' directly else create a temporary to hold the real and imaginary parts','line_number':2899,'multiline':False]
['text':' and then use at::complex_out','line_number':2900,'multiline':False]
['text':' if 'input' is complex then use 'vectors' directly else maybe create a temporary to hold real vectors','line_number':2903,'multiline':False]
['text':' and then use linalg_eig_make_complex_eigenvectors','line_number':2904,'multiline':False]
['text':' first n elements to hold the real portion of the output and the last n elements to hold the imaginary portion','line_number':2907,'multiline':False]
['text':' input.shape[:-2]','line_number':2908,'multiline':False]
['text':' linalg_eig_stub expects real-valued tensor to store eigenvectors','line_number':2912,'multiline':False]
['text':' output of linalg_eig_stub need to be post-processed later to produce complex-valued eigenvectors','line_number':2913,'multiline':False]
['text':' we do this post-processing only if 'vectors' is complex-valued','line_number':2914,'multiline':False]
['text':' otherwise storage of 'vectors' is used directly','line_number':2915,'multiline':False]
['text':' make 'maybe_complex_vectors' to have Fortran contiguous memory layout','line_number':2918,'multiline':False]
['text':' MAGMA uses a hybrid CPU-GPU algorithm that performs well only for large matrices','line_number':2922,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/pull/52491#issuecomment-795685687','line_number':2923,'multiline':False]
['text':' Here we call CPU path for matrices smaller than 2048x2048','line_number':2924,'multiline':False]
['text':' that should be in general significantly faster than calling MAGMA','line_number':2925,'multiline':False]
['text':' if input is not complex we need to do some post-processing','line_number':2932,'multiline':False]
['text':' extract real and imaginary parts of the output','line_number':2934,'multiline':False]
['text':'dim=','line_number':2935,'multiline':True]
['text':'start=','line_number':2935,'multiline':True]
['text':'end','line_number':2935,'multiline':True]
['text':'dim=','line_number':2936,'multiline':True]
['text':'start=','line_number':2936,'multiline':True]
['text':' if the imaginary part is zero we don't need to do anything','line_number':2938,'multiline':False]
['text':' does nothing for !vectors.is_complex() because vectors.is_same(maybe_complex_vectors) == true','line_number':2943,'multiline':False]
['text':' unlike NumPy for real-valued inputs the output is always complex-valued','line_number':2969,'multiline':False]
['text':' MAGMA doesn't have GPU interface for GEEV routine, it requires inputs to be on CPU','line_number':2975,'multiline':False]
['text':' if result is not empty and not in batched column major format we have to allocate a temporary tensor','line_number':2979,'multiline':False]
['text':' input.shape[:-1]','line_number':2988,'multiline':False]
['text':' if result is not empty and not in batched column major format','line_number':2992,'multiline':False]
['text':' or result does not have the expected shape','line_number':2995,'multiline':False]
['text':' or result does not have the expected dtype','line_number':2998,'multiline':False]
['text':' we will allocate a temporary tensor and do the copy','line_number':3001,'multiline':False]
['text':' because MAGMA's GEEV takes CPU inputs and returns CPU outputs','line_number':3003,'multiline':False]
['text':' "out" tensors that are on GPU device can't be used directly','line_number':3004,'multiline':False]
['text':' determine the appropriate scalar_type for the temporary tensors','line_number':3008,'multiline':False]
['text':' for real-valued input we can have either real- or complex-valued output','line_number':3012,'multiline':False]
['text':' use 'values' storage directly','line_number':3027,'multiline':False]
['text':' use 'vectors' storage directly','line_number':3033,'multiline':False]
['text':' use 'values' and 'vectors' storage directly','line_number':3039,'multiline':False]
['text':' Now check LAPACK/MAGMA error codes','line_number':3043,'multiline':False]
['text':' unlike NumPy for real-valued inputs the output is always complex-valued','line_number':3061,'multiline':False]
['text':' MAGMA doesn't have GPU interface for GEEV routine, it requires inputs to be on CPU','line_number':3065,'multiline':False]
['text':' input.shape[:-1]','line_number':3071,'multiline':False]
['text':' if result is not empty and not in batched column major format','line_number':3074,'multiline':False]
['text':' or result does not have the expected shape','line_number':3076,'multiline':False]
['text':' or result does not have the expected dtype','line_number':3078,'multiline':False]
['text':' we will allocate a temporary tensor and do the copy','line_number':3080,'multiline':False]
['text':' because MAGMA's GEEV takes CPU inputs and returns CPU outputs','line_number':3082,'multiline':False]
['text':' 'values' tensor that is on GPU device can't be used directly','line_number':3083,'multiline':False]
['text':' determine the appropriate scalar_type for the temporary tensors','line_number':3086,'multiline':False]
['text':' for real-valued input we can have either real- or complex-valued output','line_number':3089,'multiline':False]
['text':'compute_eigenvectors=','line_number':3097,'multiline':True]
['text':' use 'values' storage directly','line_number':3100,'multiline':False]
['text':'compute_eigenvectors=','line_number':3101,'multiline':True]
['text':' Now check LAPACK/MAGMA error codes','line_number':3104,'multiline':False]
['text':' if input requires grad we must compute the eigenvectors to make this function differentiable','line_number':3110,'multiline':False]
['text':' the eigenvectors are not exposed to the user','line_number':3111,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg_svd ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':3124,'multiline':False]
['text':' torch.svd, implemented in terms of torch.linalg.svd. There are two main
   differences:

    1. the 2nd parameter is bool some=True, which if effectively the opposite
       of full_matrices=True

    2. svd returns V, while linalg.svd returns Vh = V^H
','line_number':3126,'multiline':True]
['text':' Half optimisation half precondition for some parts of the LAPACK / cuSOLVER','line_number':3144,'multiline':False]
['text':' In particular, the call to lapackSvd to compute lwork fails otherwise','line_number':3145,'multiline':False]
['text':' Needed in the case that we have e.g. A.shape == (3, 0) and full_matrices=True','line_number':3147,'multiline':False]
['text':' We fill U or Vh with the identity matrix as it's a valid SVD for the empty matrix','line_number':3148,'multiline':False]
['text':' We need to distinguish the cuSOLVER case, as cuSOLVER expects F-contig matrices, but','line_number':3162,'multiline':False]
['text':' it computes V rather than Vh','line_number':3163,'multiline':False]
['text':' A always needs to be copied as its contents will be destroyed during the computaton of the SVD','line_number':3168,'multiline':False]
['text':' Now, MAGMA needs the copy to be on CPU, while cuSOLVER needs it to be on CUDA, so we'll defer','line_number':3169,'multiline':False]
['text':' the copy as a column major matrix to the backends.','line_number':3170,'multiline':False]
['text':' TODO This should be removed, and the code checking for convergence should be lifted','line_number':3180,'multiline':False]
['text':' from svd_cusolver to this function. We should then make sure that this function','line_number':3181,'multiline':False]
['text':' never errors out.','line_number':3182,'multiline':False]
['text':'is_matrix','line_number':3183,'multiline':True]
['text':' This function does not have an _ex variant as we always check errors inside','line_number':3193,'multiline':False]
['text':' to assure the convergence of the algorithm anyway. See','line_number':3194,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/28293','line_number':3195,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/64237','line_number':3196,'multiline':False]
['text':'','line_number':3197,'multiline':False]
['text':' We must delegate both linalg_svd and linalg_svdvals to','line_number':3198,'multiline':False]
['text':' _linalg_svd (rather than delegating linalg_svdvals to linalg_svd) because','line_number':3199,'multiline':False]
['text':'   1. We don't want to expose the `compute_uv` parameter in svd','line_number':3200,'multiline':False]
['text':'   2. We would like to make use of the `compute_uv=False` optimisation within svdvals','line_number':3201,'multiline':False]
['text':' The only way to achieve these two things and still abide by the compositionality rules','line_number':3202,'multiline':False]
['text':' is by dispatching to another function.','line_number':3203,'multiline':False]
['text':'compute_uv=','line_number':3204,'multiline':True]
['text':'compute_uv=','line_number':3209,'multiline':True]
['text':' See note in linalg_svd for why this function does not have an _ex variant','line_number':3212,'multiline':False]
['text':' Dummies','line_number':3214,'multiline':False]
['text':'full_matrices=','line_number':3217,'multiline':True]
['text':'comptue_uv=','line_number':3217,'multiline':True]
['text':'driver=','line_number':3217,'multiline':True]
['text':'full_matrices=','line_number':3222,'multiline':True]
['text':'compute_uv=','line_number':3223,'multiline':True]
['text':'driver=','line_number':3224,'multiline':True]
['text':'full_matrices=','line_number':3234,'multiline':True]
['text':' We cannot use `_set_conj` as it does not play well with backwards','line_number':3237,'multiline':False]
['text':' some == false returns U, Vh of size (m, m), (n, n) full of zeros','line_number':3248,'multiline':False]
['text':' TODO: uncomment the following when svd is deprecated not only in docs','line_number':3267,'multiline':False]
['text':' torch/xla is blocking the transition from at::svd to at::linalg_svd in at::linalg_pinv code','line_number':3268,'multiline':False]
['text':' see https://github.com/pytorch/xla/issues/2755','line_number':3269,'multiline':False]
['text':' TORCH_WARN_ONCE(','line_number':3270,'multiline':False]
['text':'     "torch.svd is deprecated in favor of torch.linalg.svd and will be ",','line_number':3271,'multiline':False]
['text':'     "removed in a future PyTorch release.\n",','line_number':3272,'multiline':False]
['text':'     "U, S, V = torch.svd(A, some=some, compute_uv=True) (default)\n",','line_number':3273,'multiline':False]
['text':'     "should be replaced with\n",','line_number':3274,'multiline':False]
['text':'     "U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\n",','line_number':3275,'multiline':False]
['text':'     "V = Vh.mH\n",','line_number':3276,'multiline':False]
['text':'     "and\n",','line_number':3277,'multiline':False]
['text':'     "_, S, _ = torch.svd(A, some=some, compute_uv=False)\n",','line_number':3278,'multiline':False]
['text':'     "should be replaced with\n",','line_number':3279,'multiline':False]
['text':'     "S = torch.linalg.svdvals(A)");','line_number':3280,'multiline':False]
['text':'full_matrices=','line_number':3284,'multiline':True]
['text':' some == false returns U, Vh of size (m, m), (n, n) full of zeros','line_number':3287,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ lstsq ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':3301,'multiline':False]
['text':'
  Solves a least squares problem. That is minimizing the squared Frobenius norm of |B - A X|.

  Input args:
  * 'input' - Tensor containing batches of m-by-n matrix A.
  * 'other' - Tensor containing batches of max(m, n)-by-nrhs matrix B.
  * 'cond' - relative tolerance for determining rank of A.
  * 'driver' - the name of the LAPACK driver that is used to compute the solution.
  Output args (modified in-place):
  * 'solution' - Tensor to store the solution matrix X.
  * 'residuals' - Tensor to store values of the residual sum of squares for each column of the solution.
  * 'rank' - Tensor to store the rank of A.
  * 'singular_values' - Tensor to store the singular values of A.
  * 'infos' - Tensor to store error codes of linear algebra math library.

  For further details, please see the LAPACK documentation for GELS/GELSY/GELSS/GELSD routines.
','line_number':3305,'multiline':True]
['text':' These internal asserts make explicit the assumptions in the implementation','line_number':3332,'multiline':False]
['text':' Error check with the actual error messages are done on the higher level of','line_number':3333,'multiline':False]
['text':' the hierarchy of calls','line_number':3334,'multiline':False]
['text':' we need to unsqueeze 'other' because 2-dimensional tensors are expected in the implementation','line_number':3362,'multiline':False]
['text':' the actual shape of the solution returned is (*, n,) or (*, n, nrhs)','line_number':3368,'multiline':False]
['text':' but LAPACK requires extra dimensions to store raw residuals','line_number':3369,'multiline':False]
['text':' so the expected shape is (*, max(m, n),) or (*, max(m, n), nrhs)','line_number':3370,'multiline':False]
['text':' if 'solution' has no elements we can modify it','line_number':3379,'multiline':False]
['text':' if 'solution' is non-empty it must have the expected shape','line_number':3391,'multiline':False]
['text':' 'solution' must be in batched column major order (Fortran contiguous) for 2D inputs','line_number':3394,'multiline':False]
['text':' or C contiguous for 1D input','line_number':3395,'multiline':False]
['text':' for 1-dimensional 'other', we need to unsqueeze the 'solution' before passing to "apply_solve"','line_number':3402,'multiline':False]
['text':' _linalg_lstsq_helper_ performs calculations in-place and 'solution' must be a copy of other_2d','line_number':3407,'multiline':False]
['text':' if 'rank' is empty we might resize it','line_number':3410,'multiline':False]
['text':' gels driver doesn't set 'rank'','line_number':3412,'multiline':False]
['text':' if 'rank' is non-empty it must have the expected shape and be contiguous','line_number':3416,'multiline':False]
['text':' if 'singular_values' is empty we might resize it','line_number':3422,'multiline':False]
['text':' if 'singular_values' is non-empty it must have the expected shape and be contiguous','line_number':3429,'multiline':False]
['text':' 'input' is modified in-place so we need a column-major copy','line_number':3435,'multiline':False]
['text':' now the actual call that computes the result in-place (apply_lstsq)','line_number':3438,'multiline':False]
['text':' residuals are available only if m > n and drivers other than gelsy used','line_number':3441,'multiline':False]
['text':' if the driver is gelss or gelsd then the residuals are available only if rank == n','line_number':3443,'multiline':False]
['text':' it is not clear what to do if some matrices have rank < n in case of batched input','line_number':3449,'multiline':False]
['text':' For now let's compute the residuals only if all matrices have rank equal to n','line_number':3450,'multiline':False]
['text':' This behaviour may be changed in the future','line_number':3451,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/56483','line_number':3452,'multiline':False]
['text':' LAPACK stores residuals data for postprocessing in rows n:(m-n)','line_number':3457,'multiline':False]
['text':'dim=','line_number':3458,'multiline':True]
['text':'start=','line_number':3458,'multiline':True]
['text':'length','line_number':3458,'multiline':True]
['text':'dim=','line_number':3465,'multiline':True]
['text':'keepdim=','line_number':3465,'multiline':True]
['text':'dtype','line_number':3465,'multiline':True]
['text':'dim=','line_number':3468,'multiline':True]
['text':'start=','line_number':3468,'multiline':True]
['text':'length','line_number':3468,'multiline':True]
['text':' manually restride original','line_number':3469,'multiline':False]
['text':' for 1-dimensional 'other', we need to squeeze the solution after "apply_lstsq"','line_number':3475,'multiline':False]
['text':' if `driver` is empty, we set driver_str to "gels" if working with CUDA tensors,','line_number':3482,'multiline':False]
['text':' otherwise to "gelsy" driver.','line_number':3483,'multiline':False]
['text':' check whether the user provided name is a valid driver name','line_number':3485,'multiline':False]
['text':' convert `driver_str` to lower case inplace.','line_number':3488,'multiline':False]
['text':' else if (input.is_cuda())','line_number':3500,'multiline':False]
['text':' if driver name is not provided, set to default 'gelsy' if on CPU,','line_number':3507,'multiline':False]
['text':' or to `gels` if on CUDA.','line_number':3508,'multiline':False]
['text':' 'solution' is expected to have same dtype as input','line_number':3548,'multiline':False]
['text':' 'residuals' is expected to have real float dtype','line_number':3551,'multiline':False]
['text':' 'rank' is expected to have integer dtype','line_number':3555,'multiline':False]
['text':' actual LAPACK calls use int32_t type for rank, but we promote it to int64_t','line_number':3556,'multiline':False]
['text':' to be consistent with torch.linalg.matrix_rank output dtype','line_number':3557,'multiline':False]
['text':' 'singular_values' is expected to have real float dtype','line_number':3561,'multiline':False]
['text':' set default rcond value','line_number':3566,'multiline':False]
['text':' now check whether the provided output tensors can be used directly','line_number':3573,'multiline':False]
['text':' Two types of 'other' tensors are supported:','line_number':3575,'multiline':False]
['text':' - 1-dimensional (1D) tensor or batch of 1D tensors (vector case)','line_number':3576,'multiline':False]
['text':' - 2-dimensional (2D) tensor or batch of 2D tensors (matrix case)','line_number':3577,'multiline':False]
['text':' original torch.lstsq supported only the matrix case, while NumPy works for both cases','line_number':3578,'multiline':False]
['text':' for the batched input we need to be able to distinguish them','line_number':3579,'multiline':False]
['text':' auto expected_batched_rhs_shape = IntArrayRef(input.sizes().data(), input.dim() - 1); // input.shape[:-1]','line_number':3580,'multiline':False]
['text':' bool vector_case = other.dim() == 1 || (input.dim() - 1 == other.dim() && other.sizes().equals(expected_batched_rhs_shape));','line_number':3581,'multiline':False]
['text':' provided output tensor can be used directly if:','line_number':3584,'multiline':False]
['text':' 1. the shape matches the expected shape','line_number':3585,'multiline':False]
['text':' 2. the dtype matches the expected dtype','line_number':3586,'multiline':False]
['text':' 3. the tensor is contiguous','line_number':3587,'multiline':False]
['text':' Checks for the 'solution' tensor','line_number':3589,'multiline':False]
['text':' the actual shape of the shape of the solution returned in (*, n,) or (*, n, nrhs)','line_number':3591,'multiline':False]
['text':' but LAPACK requires extra dimensions so the expected shape is (*, max(m, n),) or (*, max(m, n), nrhs)','line_number':3592,'multiline':False]
['text':' 'residuals' is not checked here because at::sum_out(residuals, ...) does that','line_number':3608,'multiline':False]
['text':' Checks for the 'rank' tensor','line_number':3612,'multiline':False]
['text':' rank is a scalar value for each matrix in the batch so','line_number':3613,'multiline':False]
['text':' rank's expected shape is equal to input.shape[0:input.ndim-2]','line_number':3614,'multiline':False]
['text':' gels driver doesn't set 'rank'','line_number':3618,'multiline':False]
['text':' Checks for the 'singular_values' tensor','line_number':3624,'multiline':False]
['text':' singular values are computed only with "gelsd" and "gelss" drivers currently','line_number':3625,'multiline':False]
['text':' if solution is not empty and not in batched column major format','line_number':3637,'multiline':False]
['text':' or solution does not have the same dtype as input','line_number':3639,'multiline':False]
['text':' or solution does not have the expected shape','line_number':3640,'multiline':False]
['text':' we have to allocate temporary tensors','line_number':3650,'multiline':False]
['text':' else use the provided output storage directly','line_number':3670,'multiline':False]
['text':' LAPACK workspace query segfalts if the input has 0 in batch dimensions.','line_number':3700,'multiline':False]
['text':' We decided not to include upper flag in the API.','line_number':3706,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/69828#issuecomment-1015143819','line_number':3707,'multiline':False]
['text':' We can revisit this decision later and remove upper completely','line_number':3708,'multiline':False]
['text':' also from low level functions or add it to the public API.','line_number':3709,'multiline':False]
['text':' call ldl_factor_stub that fills the result tensors','line_number':3717,'multiline':False]
['text':' We pass check_errors as we want to use lu_factor rather than lu_factor_ex','line_number':3733,'multiline':False]
['text':' in the errors','line_number':3734,'multiline':False]
['text':'check_errors=','line_number':3736,'multiline':True]
['text':'check_errors=','line_number':3746,'multiline':True]
['text':'row_major=','line_number':3766,'multiline':True]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ solve_triangular ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':3774,'multiline':False]
['text':' out checks','line_number':3781,'multiline':False]
['text':' Computes x^H y','line_number':3787,'multiline':False]
['text':'dim=','line_number':3792,'multiline':True]
['text':' Computes x^H y','line_number':3801,'multiline':False]
['text':'dim=','line_number':3805,'multiline':True]
['text':'
Solves the matrix equation AX = B for A triangular.
'left' If true solves AX = B, if false solves XA = B
'upper' controls the portion of input matrix to consider in computations,
'unitriangular' if true then we assume diag(A) to be ones
'out' The tensor with the result. If A == out, A will be modified in place
','line_number':3809,'multiline':True]
['text':'don't check errors','line_number':3825,'multiline':True]
['text':' We'll write F-contig / F-transpose for FORTRAN contiguous / FORTRAN transpose etc','line_number':3827,'multiline':False]
['text':' We say that a matrix is F-ready if it's F-contig OR F-transpose','line_number':3828,'multiline':False]
['text':' At this point, A, B have been broadcasted but may or may not be F-ready','line_number':3829,'multiline':False]
['text':' The following algorithm minimises copies and allocations. In pseudocode:','line_number':3831,'multiline':False]
['text':' if out is wrong size:','line_number':3832,'multiline':False]
['text':'   resize_output(out)','line_number':3833,'multiline':False]
['text':' # Invariant: out is the right size','line_number':3834,'multiline':False]
['text':' Tensor out_f; # Tensor that we will pass to FORTRAN','line_number':3835,'multiline':False]
['text':' if out is F-ready:','line_number':3836,'multiline':False]
['text':'   out_f = out;','line_number':3837,'multiline':False]
['text':' else:','line_number':3838,'multiline':False]
['text':'   Allocate out_f F-ready','line_number':3839,'multiline':False]
['text':' if B != out_f:','line_number':3840,'multiline':False]
['text':'   copy B into out_f','line_number':3841,'multiline':False]
['text':' # Invariant: out_f F-ready and has B copied into it','line_number':3842,'multiline':False]
['text':' if out_f is F-transposed:','line_number':3843,'multiline':False]
['text':'   transpose equation','line_number':3844,'multiline':False]
['text':' if out_f is conj:','line_number':3845,'multiline':False]
['text':'   conjugate equation','line_number':3846,'multiline':False]
['text':' # Invariant: out_f is not conjugated and F-contig','line_number':3847,'multiline':False]
['text':' Tensor A_f; # Tensor that will be sent to FORTRAN','line_number':3848,'multiline':False]
['text':' if A is F-ready:','line_number':3849,'multiline':False]
['text':'   if A is conj and A is not transposed:','line_number':3850,'multiline':False]
['text':'     # We need to clone A in this case. See [Cloning A]','line_number':3851,'multiline':False]
['text':'     clone A F-contig into A_f','line_number':3852,'multiline':False]
['text':'   else:','line_number':3853,'multiline':False]
['text':'     A_f = A;','line_number':3854,'multiline':False]
['text':' else:','line_number':3855,'multiline':False]
['text':'   clone A F-contig into A_f','line_number':3856,'multiline':False]
['text':' # Invariant: out_f is F-contig and A_f is F-ready','line_number':3857,'multiline':False]
['text':' # We pass FORTRAN the flags indicating if A_f is transposed and or conjugated','line_number':3858,'multiline':False]
['text':'','line_number':3859,'multiline':False]
['text':' # Here we undo the conjugations / transposes on out_f if needed','line_number':3860,'multiline':False]
['text':'','line_number':3861,'multiline':False]
['text':' if out_f not same out:','line_number':3862,'multiline':False]
['text':'   copy out_f into out','line_number':3863,'multiline':False]
['text':' return out','line_number':3864,'multiline':False]
['text':'','line_number':3865,'multiline':False]
['text':' Note: The logic for the negative bit is the same as that for the conjugate bit','line_number':3866,'multiline':False]
['text':'','line_number':3867,'multiline':False]
['text':' Note: [Cloning A] If we are careful when allocating B when it needs to be allocated at the','line_number':3868,'multiline':False]
['text':' beginning of the algorithm, it is possible to always elide the copy of A here.','line_number':3869,'multiline':False]
['text':' Via this trick, the algorithm will copy at most one of A or B (never both) whenever A','line_number':3870,'multiline':False]
['text':' and B are F-ready and not A.is_neg() (which happens almost always in practice).','line_number':3871,'multiline':False]
['text':' When called as f(A, B, out=B) in most practical cases it'll perform no copies.','line_number':3872,'multiline':False]
['text':' See Note: [Cloning A]','line_number':3876,'multiline':False]
['text':' poorman's reimplementation of resize_output with result F-contig','line_number':3880,'multiline':False]
['text':' make 'out' have Fortran contiguous memory layout','line_number':3883,'multiline':False]
['text':' Invariant: out has the right size, so we'll be able to copy into it later on','line_number':3886,'multiline':False]
['text':' the out that will go into fortran','line_number':3888,'multiline':False]
['text':' We use C10_LIKELY mostly for documentation as it helps following what's the most likely path','line_number':3889,'multiline':False]
['text':' See Note: [Cloning A]','line_number':3897,'multiline':False]
['text':' Invariant: out_f F-ready and has B copied into it','line_number':3904,'multiline':False]
['text':' out_f is F-transposed','line_number':3906,'multiline':False]
['text':' No need to conjugate anything if out_f is conj as AX = conj(B) <=> conj(A)conj(X) = B','line_number':3916,'multiline':False]
['text':' and X = B after the algortihm. We just anotate that A is conjugated later on','line_number':3917,'multiline':False]
['text':' The solution will be written into out_f, so it'll be conjugated already','line_number':3918,'multiline':False]
['text':' The A that will go into fortran','line_number':3920,'multiline':False]
['text':' We first anotate with flags on A_f all the conj / transpose / neg coming from out','line_number':3926,'multiline':False]
['text':' and then we clone the resulting tensor to resolve all of them in memory','line_number':3927,'multiline':False]
['text':' This choice is to be consistent with how we flip `upper` later on','line_number':3938,'multiline':False]
['text':' Note that this is the same reasoning we apply for neg and conj below','line_number':3939,'multiline':False]
['text':' If B has neg or out or transpose, then we need to resolve it in memory','line_number':3940,'multiline':False]
['text':' Cases A_is_neg (remember that B.is_neg() iff out_f.is_same(B))','line_number':3946,'multiline':False]
['text':' -AX = -B => A(-X) = B. Swap neg of A_f. Nothing to do on X as X.is_same(B).','line_number':3947,'multiline':False]
['text':' -AX = B. We resolve the neg in memory','line_number':3948,'multiline':False]
['text':' AX = -B => -A -X = B. We resolve the neg in memory for A,','line_number':3949,'multiline':False]
['text':'                       Since X.is_same(B), we already have that X.is_neg() == true','line_number':3950,'multiline':False]
['text':' We do the neg with a view, as this will be resolved in the clone below','line_number':3952,'multiline':False]
['text':' We resolve the transpose if necessary and then leave A_f F-transposed,','line_number':3958,'multiline':False]
['text':' as BLAS can handle the case F-transposed and conjugated','line_number':3959,'multiline':False]
['text':' As we've already resolved the conj of A in the clone','line_number':3965,'multiline':False]
['text':' We follow the same logic as above, only that in this case we need to perform the','line_number':3968,'multiline':False]
['text':' negation in memory','line_number':3969,'multiline':False]
['text':' As we've already resolved the conj of A in the negationa bove','line_number':3976,'multiline':False]
['text':' Invariant: out_f is F-contig and A_f is F-ready','line_number':3979,'multiline':False]
['text':' neg has been resolved','line_number':3980,'multiline':False]
['text':' If we pass the matrix physically F-transposed, we need to change the parity of upper','line_number':3982,'multiline':False]
['text':'left=','line_number':3989,'multiline':True]
['text':'upper=','line_number':3990,'multiline':True]
['text':'transpose','line_number':3991,'multiline':True]
['text':'unitriangular=','line_number':3992,'multiline':True]
['text':' Append cumprod of the oher 0...n-1 powers','line_number':4031,'multiline':False]
['text':' The row of ones','line_number':4034,'multiline':False]
['text':'dim=','line_number':4037,'multiline':True]
['text':' namespace at::native','line_number':4039,'multiline':False]
