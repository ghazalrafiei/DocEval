['text':' namespace meta','line_number':76,'multiline':False]
['text':' For some ambiguous cases, it is possible a channels last contiguous Tensor has','line_number':121,'multiline':False]
['text':'   `suggest_memory_format` of Contiguous.','line_number':122,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/63224 for details.','line_number':123,'multiline':False]
['text':' optional ','line_number':133,'multiline':True]
['text':' optional ','line_number':133,'multiline':True]
['text':' optional ','line_number':134,'multiline':True]
['text':' optional ','line_number':134,'multiline':True]
['text':' inference contiguous path','line_number':144,'multiline':False]
['text':' Helper to convert 1d tensors to an nd tensor that broadcasts with input','line_number':154,'multiline':False]
['text':' All elements go into the channel dimension','line_number':155,'multiline':False]
['text':' non-contiguous path','line_number':242,'multiline':False]
['text':'squash_dims=','line_number':248,'multiline':True]
['text':' compute variance per input','line_number':256,'multiline':False]
['text':' update running averages','line_number':265,'multiline':False]
['text':'dim=','line_number':292,'multiline':True]
['text':'keepdim=','line_number':292,'multiline':True]
['text':' since we are directly manipulating pointers in contiguous path,','line_number':321,'multiline':False]
['text':' need to make sure input and grad_out have the same memory format.','line_number':322,'multiline':False]
['text':' Reduce all dimensions except dim=1','line_number':351,'multiline':False]
['text':'dims=','line_number':358,'multiline':True]
['text':'squash_dims=','line_number':365,'multiline':True]
['text':'squash_dims=','line_number':376,'multiline':True]
['text':'squash_dims=','line_number':385,'multiline':True]
['text':' dot product of the Q(X) and gradOuput','line_number':413,'multiline':False]
['text':' when in training mode','line_number':426,'multiline':False]
['text':' Q(X) = X - E[x] ; i.e. input centered to zero mean','line_number':427,'multiline':False]
['text':' Y = Q(X) / sigma    ; i.e. BN output before weight and bias','line_number':428,'multiline':False]
['text':' dL/dX = (Q(dL/dY) - dot(Y, dL/dY) * Y) / sigma * w','line_number':429,'multiline':False]
['text':' projection of gradOutput on to output scaled by std','line_number':431,'multiline':False]
['text':' when in evaluation mode','line_number':455,'multiline':False]
['text':' Q(X) = X - running_mean  ; i.e. input centered to zero mean','line_number':456,'multiline':False]
['text':' Y = Q(X) / running_std    ; i.e. BN output before weight and bias','line_number':457,'multiline':False]
['text':' dL/dX = w / running_std','line_number':458,'multiline':False]
['text':' _batch_norm_impl_index(_backward) are used in the JIT be able to keep the run-time selection','line_number':482,'multiline':False]
['text':' of backends, while enabling it to keep the information about the used backend, so that it can','line_number':483,'multiline':False]
['text':' use its corresponding backward implementation.','line_number':484,'multiline':False]
['text':' XXX: The indices of backends need to be kept synchronized between this function and its _backward.','line_number':485,'multiline':False]
['text':' optional ','line_number':487,'multiline':True]
['text':' optional ','line_number':487,'multiline':True]
['text':' optional ','line_number':487,'multiline':True]
['text':' optional ','line_number':487,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':489,'multiline':False]
['text':'is_cuda=','line_number':501,'multiline':True]
['text':' don't return view of input, don't return empty tensor because it will break gradient chain','line_number':505,'multiline':False]
['text':' spatial, training','line_number':539,'multiline':False]
['text':'spatial, eval','line_number':540,'multiline':False]
['text':' some cuDNN kernels have 32-bit indexing limitations','line_number':544,'multiline':False]
['text':' optional ','line_number':597,'multiline':True]
['text':' optional ','line_number':597,'multiline':True]
['text':' optional ','line_number':597,'multiline':True]
['text':' optional ','line_number':597,'multiline':True]
['text':' optional ','line_number':597,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':599,'multiline':False]
['text':' don't return empty tensor because it will break gradient chain','line_number':612,'multiline':False]
['text':' backward in inference mode is not supported in cudnn, fallback to native','line_number':628,'multiline':False]
['text':' TODO: _batch_norm_impl_index_backward is only used in JIT. cudnn NHWC','line_number':632,'multiline':False]
['text':' format conversion is done inside cudnn_batch_norm_backward instead','line_number':633,'multiline':False]
['text':' optional ','line_number':654,'multiline':True]
['text':' optional ','line_number':654,'multiline':True]
['text':' optional ','line_number':654,'multiline':True]
['text':' optional ','line_number':654,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':656,'multiline':False]
['text':' we alias running_mean and running_var because they are const but we want to modify their data','line_number':680,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':693,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':712,'multiline':False]
['text':' Resize out','line_number':720,'multiline':False]
['text':' Resize save_mean and save_var','line_number':731,'multiline':False]
['text':' Resize save_mean and save_var','line_number':741,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':755,'multiline':False]
['text':' Prepare output tensor','line_number':764,'multiline':False]
['text':' Prepare save_mean and save_var','line_number':772,'multiline':False]
['text':'dim=','line_number':787,'multiline':True]
['text':'keepdim=','line_number':787,'multiline':True]
['text':'dim=','line_number':795,'multiline':True]
['text':'keepdim=','line_number':795,'multiline':True]
['text':'train=','line_number':817,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':833,'multiline':False]
['text':' For cuda half, calculate norm in float precision then cast','line_number':862,'multiline':False]
['text':' normalization factor to half','line_number':863,'multiline':False]
['text':'is_cuda=','line_number':865,'multiline':True]
['text':'keepdim=','line_number':869,'multiline':True]
['text':'dtype=','line_number':869,'multiline':True]
['text':'keepdim=','line_number':872,'multiline':True]
['text':' at::native','line_number':888,'multiline':False]
