['text':' returns floor(log2(n))','line_number':30,'multiline':False]
['text':' returns reduced fraction numerator & denominator','line_number':40,'multiline':False]
['text':' get GCD of num and denom using Euclid's algorithm.','line_number':42,'multiline':False]
['text':' Can replace this with std::gcd if we ever support c++17.','line_number':43,'multiline':False]
['text':' swap(a,b)','line_number':48,'multiline':False]
['text':' a is now the GCD','line_number':54,'multiline':False]
['text':'template for changing MAX_NUM_THREADS based on op dtype','line_number':59,'multiline':False]
['text':'idx','line_number':263,'multiline':True]
['text':' wrap a normal reduction that ignores the index','line_number':270,'multiline':False]
['text':'ReduceJitOp is almost like ReduceOp, but it doesn't have ops functor that specifies reduction operations','line_number':283,'multiline':False]
['text':'Maybe we can find a way to unify ReduceOp and ReduceJitOp','line_number':284,'multiline':False]
['text':'TODO for now arg_t is always opmath_t of the input, later we'll need to change it','line_number':287,'multiline':False]
['text':'TODO - ReduceJitOp will probably need to be changed for reductions that need full functor,','line_number':291,'multiline':False]
['text':'not just wrapper','line_number':292,'multiline':False]
['text':'it accepts at most two destinations','line_number':298,'multiline':False]
['text':' acc_buf used for accumulation among sub Tensor Iterator when accumulation on','line_number':299,'multiline':False]
['text':' output is not permissible','line_number':300,'multiline':False]
['text':' cta_buf used for accumulation between blocks during global reduction','line_number':302,'multiline':False]
['text':'it accepts at most two destinations','line_number':360,'multiline':False]
['text':' acc_buf used for accumulation among sub Tensor Iterator when accumulation on','line_number':361,'multiline':False]
['text':' output is not permissible','line_number':362,'multiline':False]
['text':' cta_buf used for accumulation between blocks during global reduction','line_number':364,'multiline':False]
['text':' reduce at the header of input_slice where memory is not aligned,','line_number':486,'multiline':False]
['text':' so that thread_reduce will have an aligned memory to work on.','line_number':487,'multiline':False]
['text':' Handle the head of input slice where data is not aligned','line_number':505,'multiline':False]
['text':' Do the vectorized reduction','line_number':521,'multiline':False]
['text':' Multiple accumulators to remove dependency between unrolled loops.','line_number':527,'multiline':False]
['text':' tail','line_number':545,'multiline':False]
['text':' combine accumulators','line_number':555,'multiline':False]
['text':' Multiple accumulators to remove dependency between unrolled loops.','line_number':572,'multiline':False]
['text':' tail','line_number':601,'multiline':False]
['text':' combine accumulators','line_number':625,'multiline':False]
['text':' This function should never be called --','line_number':726,'multiline':False]
['text':' it's the version of `accumulate_in_output`','line_number':727,'multiline':False]
['text':' when accumulation in the output is not possible.','line_number':728,'multiline':False]
['text':' This function should never be called --','line_number':739,'multiline':False]
['text':' it's the version of `get_accumulated_output`','line_number':740,'multiline':False]
['text':' when accumulation in the output is not possible.','line_number':741,'multiline':False]
['text':'Currently implemented for max of two outputs','line_number':758,'multiline':False]
['text':' base offset is computed assuming element size being sizeof(T1), so we need to make a','line_number':766,'multiline':False]
['text':' correction to obtain the correct base offset','line_number':767,'multiline':False]
['text':' make sure writes are globally visible','line_number':805,'multiline':False]
['text':' if multiple warps in this block wrote to staging, make sure they're all done','line_number':806,'multiline':False]
['text':' reusing output buffer for accumulation.','line_number':943,'multiline':False]
['text':' Start by assuming that each thread handles a single output and all','line_number':999,'multiline':False]
['text':' the inputs for that output.','line_number':1000,'multiline':False]
['text':' Adjust block size to map block width to fastest changing dimension of input','line_number':1013,'multiline':False]
['text':' tensor. This grants the best possible memory accessing pattern, given that','line_number':1014,'multiline':False]
['text':' for non-contiguous tensor with space in between, we cannot have perfect','line_number':1015,'multiline':False]
['text':' memory coalescing.','line_number':1016,'multiline':False]
['text':'arg=','line_number':1019,'multiline':True]
['text':'arg=','line_number':1020,'multiline':True]
['text':' Notice that dim0 & dim1 does NOT guarantee any launch configuration here!','line_number':1021,'multiline':False]
['text':' dim0 & dim1 are more like the upper bound of the block dimension. The','line_number':1022,'multiline':False]
['text':' actual launch config and reduction scheme is determined by setting values','line_number':1023,'multiline':False]
['text':' to `config.input_mult` and `config.output_mult`.','line_number':1024,'multiline':False]
['text':' We try to max out dim1 so that we have enough threads per CTA to deliver','line_number':1025,'multiline':False]
['text':' performance for larger problem size.','line_number':1026,'multiline':False]
['text':' Map block.x to the fastest reducing dimension. It implies:','line_number':1028,'multiline':False]
['text':'   1. block_x_reduce is required.','line_number':1029,'multiline':False]
['text':'   2. block.y now max out to num_outputs.','line_number':1030,'multiline':False]
['text':'arg=','line_number':1033,'multiline':True]
['text':' Map block.x to the fastest non reducing dimension. It implies:','line_number':1035,'multiline':False]
['text':'   1. block_x_reduce is turned off.','line_number':1036,'multiline':False]
['text':'   2. block.y now max out to inputs_per_output.','line_number':1037,'multiline':False]
['text':'arg=','line_number':1040,'multiline':True]
['text':' We do vectorization to gain better memory access, there are two cases which we call','line_number':1049,'multiline':False]
['text':' "vectorize along input" and "vectorize along output". Note that the "input/output"','line_number':1050,'multiline':False]
['text':' here does not mean we are vectorizing load/store instructions. We always only vectorize','line_number':1051,'multiline':False]
['text':' load instructions.','line_number':1052,'multiline':False]
['text':'','line_number':1053,'multiline':False]
['text':' Case 1: "vectorize along input"','line_number':1054,'multiline':False]
['text':' This case happens when we are reducing along fastest moving dimesion. In such case, threads','line_number':1055,'multiline':False]
['text':' with the same threadIdx.y works on the same reduction cooperatively and will produce results','line_number':1056,'multiline':False]
['text':' for the same ouput. In such case, values in each loaded vector always correspond to the same ouput.','line_number':1057,'multiline':False]
['text':'','line_number':1058,'multiline':False]
['text':' Case 2: "vectorize along output"','line_number':1059,'multiline':False]
['text':' This case happens when the fastest moving dimesion is not the dimension of reduction. In such case,','line_number':1060,'multiline':False]
['text':' threads with different threadIdx.x are independent and will produce results for different outputs.','line_number':1061,'multiline':False]
['text':' In such case, values in each loaded vector always correspond to different outputs.','line_number':1062,'multiline':False]
['text':' Case 1: "vectorize along input"','line_number':1065,'multiline':False]
['text':' Note that if vt0 < ReduceConfig::vec_size, then this means the register pressure could be high, in such case,','line_number':1066,'multiline':False]
['text':' we should avoid vectorization.','line_number':1067,'multiline':False]
['text':' Case 2: "vectorize along output"','line_number':1071,'multiline':False]
['text':' Adjust block_width and block_height','line_number':1077,'multiline':False]
['text':' Split the input across lanes if the input is contiguous in the reduced','line_number':1084,'multiline':False]
['text':' dimension. This will require reduction between threads using warp','line_number':1085,'multiline':False]
['text':' shuffle instructions and shared memory (if block_width > warpSize).','line_number':1086,'multiline':False]
['text':' Otherwise split the output across lanes in a warp.','line_number':1089,'multiline':False]
['text':' Divide the input across warps in a thread-block, if that leaves at least','line_number':1097,'multiline':False]
['text':' 16 elements to be summed by each thread. This will require inter-warp','line_number':1098,'multiline':False]
['text':' reduction using shared memory.','line_number':1099,'multiline':False]
['text':' Otherwise, each warp handles a separate output.','line_number':1102,'multiline':False]
['text':' Divide the input across thread-blocks if the amount of work per-thread','line_number':1111,'multiline':False]
['text':' is large enough and the size of the output is small enough. This will','line_number':1112,'multiline':False]
['text':' require a reduction using global memory.','line_number':1113,'multiline':False]
['text':' If we decide to split input across blocks, as long as we can get enough','line_number':1114,'multiline':False]
['text':' number of blocks (`target_grid_size`) to balance SM, we should still','line_number':1115,'multiline':False]
['text':' make the number of values per thread large for best performance.','line_number':1116,'multiline':False]
['text':' We want the minimum of ctas_per_output1 and ctas_per_output2, so that each thread can have','line_number':1120,'multiline':False]
['text':' a large number of values to deal with. But we don't want values_per_thread to be larger than','line_number':1121,'multiline':False]
['text':' max_values_per_thread','line_number':1122,'multiline':False]
['text':' at::Half/at::ComplexHalf overflows easily as it's range is very small.','line_number':1138,'multiline':False]
['text':' So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we','line_number':1139,'multiline':False]
['text':' set can_accumulate_in_output to False.','line_number':1140,'multiline':False]
['text':' at::BFloat16 has lower precision and can lead to rounding errors.','line_number':1146,'multiline':False]
['text':' So when scalar_t and out_scalar_t are at::BFloat16, we','line_number':1147,'multiline':False]
['text':' set can_accumulate_in_output to False.','line_number':1148,'multiline':False]
['text':' The acc_buf_ptr is a shared pointer. It is create at the first entrance and','line_number':1158,'multiline':False]
['text':' reused by all recursive function calls.','line_number':1159,'multiline':False]
['text':' acc_buf_ptr holds buffer used for accumulation among multiple sub_iter','line_number':1161,'multiline':False]
['text':' when accumulation in output is not possible.','line_number':1162,'multiline':False]
['text':'iter.strides is in bytes','line_number':1168,'multiline':False]
['text':'TODO this is 100 lines of almost-copy-paste, because we have to have different template args for this function','line_number':1235,'multiline':False]
['text':'try unifying with gpu_reduce_kernel','line_number':1236,'multiline':False]
['text':'TODO - this will be different for more complicated reductions, but for now reductions using','line_number':1242,'multiline':False]
['text':'func_wrapper all have arg_t = opmath','line_number':1243,'multiline':False]
['text':' at::Half/at::ComplexHalf overflows easily as it's range is very small.','line_number':1245,'multiline':False]
['text':' So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we','line_number':1246,'multiline':False]
['text':' set can_accumulate_in_output to False.','line_number':1247,'multiline':False]
['text':' at::BFloat16 has lower precision and can lead to rounding errors.','line_number':1253,'multiline':False]
['text':' So when scalar_t and out_scalar_t are at::BFloat16, we','line_number':1254,'multiline':False]
['text':' set can_accumulate_in_output to False.','line_number':1255,'multiline':False]
['text':' The acc_buf_ptr is a shared pointer. It is create at the first entrance and','line_number':1266,'multiline':False]
['text':' reused by all recursive function calls.','line_number':1267,'multiline':False]
['text':' acc_buf_ptr holds buffer used for accumulation among multiple sub_iter','line_number':1269,'multiline':False]
['text':' when accumulation in output is not possible.','line_number':1270,'multiline':False]
['text':'iter.strides is in bytes','line_number':1276,'multiline':False]
['text':'TODO','line_number':1277,'multiline':False]
['text':'TODO','line_number':1280,'multiline':False]
['text':'TODO - for now we support a single input, we may be able to relax this constraint','line_number':1297,'multiline':False]
['text':' namespace at::native','line_number':1354,'multiline':False]
