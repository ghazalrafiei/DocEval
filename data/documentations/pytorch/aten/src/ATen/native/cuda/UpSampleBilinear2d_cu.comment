['text':' Adapted from interp.cpp from Caffe util by Pauline Luc','line_number':1,'multiline':False]
['text':' Originally developed by George Papandreou','line_number':2,'multiline':False]
['text':' 0:width2-1','line_number':51,'multiline':False]
['text':' 0:height2-1','line_number':52,'multiline':False]
['text':'cubic=','line_number':55,'multiline':True]
['text':'','line_number':60,'multiline':False]
['text':'cubic=','line_number':62,'multiline':True]
['text':'','line_number':67,'multiline':False]
['text':'cubic=','line_number':106,'multiline':True]
['text':'cubic=','line_number':113,'multiline':True]
['text':' Backward (adjoint) operation 1 <- 2 (accumulates)','line_number':130,'multiline':False]
['text':' 0:width2-1','line_number':149,'multiline':False]
['text':' 0:height2-1','line_number':151,'multiline':False]
['text':'','line_number':153,'multiline':False]
['text':'cubic=','line_number':155,'multiline':True]
['text':'','line_number':160,'multiline':False]
['text':'cubic=','line_number':162,'multiline':True]
['text':'','line_number':167,'multiline':False]
['text':'cubic=','line_number':221,'multiline':True]
['text':'cubic=','line_number':228,'multiline':True]
['text':' heuristic: only use channels_last path when it's faster than the contiguous path','line_number':289,'multiline':False]
['text':' const int num_kernels = output_height * output_width;','line_number':305,'multiline':False]
['text':' non-channels_last case, not necessarily contiguous','line_number':332,'multiline':False]
['text':' initialization to zero is required here. As we launch one thread per output','line_number':385,'multiline':False]
['text':' element, and atomicAdd to input gradient. Given a sparse sampling case, our','line_number':386,'multiline':False]
['text':' threads are not covering the whole input tensor.','line_number':387,'multiline':False]
['text':' This is needed for non-contiguous tensors.','line_number':435,'multiline':False]
['text':' Code for upsampling with antialias','line_number':471,'multiline':False]
['text':' 256 performs better then 1024','line_number':473,'multiline':False]
['text':' Setup weights and a buffer using shared memory','line_number':504,'multiline':False]
['text':' Compute weights and kernel spans','line_number':512,'multiline':False]
['text':' All threadIdx.y have the same wx weights','line_number':522,'multiline':False]
['text':' All threadIdx.x have the same wy weights','line_number':534,'multiline':False]
['text':' interpolate on y-axis for ymin to ymin + ysize','line_number':550,'multiline':False]
['text':' Code for upsampling with antialias','line_number':564,'multiline':False]
['text':' 256 performs better then 1024','line_number':566,'multiline':False]
['text':' special case: output just copy','line_number':588,'multiline':False]
['text':' Setup weights using shared memory','line_number':609,'multiline':False]
['text':' Compute weights and kernel spans','line_number':614,'multiline':False]
['text':' All threadIdx.y have the same wx weights','line_number':624,'multiline':False]
['text':' All threadIdx.x have the same wy weights','line_number':636,'multiline':False]
['text':' In the code below interp_filter_t distinguishes between bilinear and bicubic interpolations','line_number':668,'multiline':False]
['text':' InterpFilter as BilinearFilterFunctor <--> bilinear','line_number':669,'multiline':False]
['text':' InterpFilter as BicubicFilterFunctor <--> bicubic','line_number':670,'multiline':False]
['text':' TODO: remove this when the cuda kernel is updated to support the channels_last memory format.','line_number':682,'multiline':False]
['text':' This is a temporary hack to prevent a silence correctness issue when calling this kernel','line_number':683,'multiline':False]
['text':' with tensors in channels_last format.','line_number':684,'multiline':False]
['text':' We are using shared memory to store weights wx, wy and a buffer of size wy unique per thread','line_number':715,'multiline':False]
['text':' Let's compute block_y size depending on given height_scale and width_scale','line_number':716,'multiline':False]
['text':' We have the following relationship:','line_number':717,'multiline':False]
['text':' shmem_size / sizeofdtype =','line_number':718,'multiline':False]
['text':'  interp_width * block_x +   <-- wx allocation','line_number':719,'multiline':False]
['text':'  interp_height * block_y * (block_x + 1)   <-- wy and buffer allocations','line_number':720,'multiline':False]
['text':' Compute actual size of required shared memory and verify if we can allocate it','line_number':737,'multiline':False]
['text':' - wx and wy size:','line_number':738,'multiline':False]
['text':' - buffer size:','line_number':740,'multiline':False]
['text':' In the code below interp_filter_t distinguishes between bilinear and bicubic interpolations','line_number':762,'multiline':False]
['text':' InterpFilter as BilinearFilterFunctor <--> bilinear','line_number':763,'multiline':False]
['text':' InterpFilter as BicubicFilterFunctor <--> bicubic','line_number':764,'multiline':False]
['text':' Inspired from UpSampleBicubic2d.cu::upsample_bicubic2d_backward_out_cuda_template','line_number':775,'multiline':False]
['text':' namespace','line_number':841,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':861,'multiline':False]
['text':' Nondeterministic because of atomicAdd usage','line_number':862,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':888,'multiline':False]
['text':' Nondeterministic because of atomicAdd usage','line_number':889,'multiline':False]
['text':' We define bicubic anti-alias function implementations in this file instead of','line_number':895,'multiline':False]
['text':' UpSampleBicubic2d.cu as we are using a single generic implementation','line_number':896,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':916,'multiline':False]
['text':' Nondeterministic because of atomicAdd usage','line_number':917,'multiline':False]
['text':' namespace at::native','line_number':923,'multiline':False]
