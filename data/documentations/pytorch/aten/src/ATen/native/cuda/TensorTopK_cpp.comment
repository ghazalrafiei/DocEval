['text':' TODO: remove this when CUDA <11.6 is no longer supported','line_number':22,'multiline':False]
['text':' stable= ','line_number':30,'multiline':True]
['text':' TODO: remove this when CUDA <11.6 is no longer supported','line_number':35,'multiline':False]
['text':' This heuristics is based on the experiment in https://github.com/pytorch/pytorch/pull/68632','line_number':39,'multiline':False]
['text':' Bool is not support by topk','line_number':41,'multiline':False]
['text':' If k is 0 the result is an empty tensor, so we don't need to launch a kernel.','line_number':63,'multiline':False]
['text':' Sort the results if the user wants them sorted, since our','line_number':70,'multiline':False]
['text':' selection routine does not ensure sorting','line_number':71,'multiline':False]
['text':' This avoids any memory allocations and performs all sorting','line_number':74,'multiline':False]
['text':' work inplace along the slice','line_number':75,'multiline':False]
['text':' Depend upon the backup sort that returns indices, which we','line_number':79,'multiline':False]
['text':' can use in conjunction with gather to produce the original','line_number':80,'multiline':False]
['text':' indices.','line_number':81,'multiline':False]
['text':' This is not the most efficient implementation, especially since','line_number':82,'multiline':False]
['text':' there are memory allocations performed here. If the user desires','line_number':83,'multiline':False]
['text':' greater performance, they should torch.gather() the results','line_number':84,'multiline':False]
['text':' themselves using the reported indices, providing previously','line_number':85,'multiline':False]
['text':' allocated tensors to receive the results.','line_number':86,'multiline':False]
['text':' stable= ','line_number':90,'multiline':True]
['text':' namespace at::native','line_number':97,'multiline':False]
