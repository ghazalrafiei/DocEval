['text':' Block size for weight_norm_*_first_dim_kernel.','line_number':25,'multiline':False]
['text':' Currently, kernels are non-persistent.','line_number':26,'multiline':False]
['text':' Dialing up the block size to, say 1024, can improve performance by','line_number':27,'multiline':False]
['text':' increase the amount of cache available per block, which can improve cache hit rate.','line_number':28,'multiline':False]
['text':' However, this is less efficient for short rows.  256 is pretty versatile.','line_number':29,'multiline':False]
['text':' May be worth implementing heuristics later.','line_number':30,'multiline':False]
['text':' Block size for weight_norm_*_last_dim_kernel.','line_number':33,'multiline':False]
['text':' This is tricker than the first_dim case because we must make blocks','line_number':34,'multiline':False]
['text':' at least 16 fast elements wide to ensure fully-coalesced half-precision accesses.','line_number':35,'multiline':False]
['text':' Since output-element parallelism is along the fast dimension, this reduces the number of','line_number':36,'multiline':False]
['text':' blocks we can launch by 16X.','line_number':37,'multiline':False]
['text':' Somewhat versatile strategy: max out intra-block parallelism by extending','line_number':39,'multiline':False]
['text':' blocks across the slow dimension up to the hardware-max block size of 1024.','line_number':40,'multiline':False]
['text':' lanes is intended to be <= 32.','line_number':54,'multiline':False]
['text':' blockSize is intended to be a multiple of 32.','line_number':58,'multiline':False]
['text':' __SYNCWARP();','line_number':83,'multiline':False]
['text':' EpilogueOp','line_number':92,'multiline':False]
['text':' Make sure the smem result is visible to all warps.','line_number':95,'multiline':False]
['text':' We are norming each slowest-dim row of the tensor separately.','line_number':109,'multiline':False]
['text':' For now, assign one block to each row.','line_number':110,'multiline':False]
['text':' Logical index offset for this flattened row','line_number':115,'multiline':False]
['text':' Hack to get around nvcc complaining when an smem array is declared with the same name','line_number':118,'multiline':False]
['text':' but different types in different kernels (in this case different instantiations)','line_number':119,'multiline':False]
['text':' extern __shared__ accscalar_t s[]; // error: declaration is incompatible with previous "s"','line_number':120,'multiline':False]
['text':' AccumOp, could do Kahan here','line_number':128,'multiline':False]
['text':' Broadcast load, could use shared memory instead.','line_number':139,'multiline':False]
['text':' for consistency with backward kernel','line_number':142,'multiline':False]
['text':' Write data to output','line_number':144,'multiline':False]
['text':' AccumOp, could do Kahan here','line_number':180,'multiline':False]
['text':' Better to pass an EpilogueOp to reduce_block_into_lanes?','line_number':187,'multiline':False]
['text':' For now, assign one block to each row.','line_number':225,'multiline':False]
['text':' Logical index offset for this flattened row','line_number':230,'multiline':False]
['text':' Hack to get around nvcc complaining when an smem array is declared with the same name','line_number':233,'multiline':False]
['text':' but different types in different kernels (in this case different instantiations)','line_number':234,'multiline':False]
['text':' extern __shared__ accscalar_t s[]; // error: declaration is incompatible with previous "s"','line_number':235,'multiline':False]
['text':' AccumOp, could do Kahan here','line_number':244,'multiline':False]
['text':' Could choose to save reciprocal of norm instead I suppose, but norms is probably','line_number':250,'multiline':False]
['text':' more handy to keep around.','line_number':251,'multiline':False]
['text':' Broadcast load; could use shared memory instead.','line_number':252,'multiline':False]
['text':' Write g gradients.','line_number':256,'multiline':False]
['text':' Broadcast load, could use shared memory instead.','line_number':260,'multiline':False]
['text':' Write v gradients.  We are reusing values that were loaded earlier, so there','line_number':263,'multiline':False]
['text':' is an optimization opportunity here (store values persistently).','line_number':264,'multiline':False]
['text':' AccumOp, could do Kahan here','line_number':301,'multiline':False]
['text':' Broadcast load; could use shared memory instead.','line_number':309,'multiline':False]
['text':' Write g gradients.','line_number':313,'multiline':False]
['text':' Entire block pulls these values, could use shared memory instead.','line_number':317,'multiline':False]
['text':' Write v gradients.','line_number':320,'multiline':False]
['text':' anonymous namespace','line_number':335,'multiline':False]
['text':' weight_norm_fused does have a derivative defined in derivatives.yaml, therefore, VariableType.cpp','line_number':344,'multiline':False]
['text':' sends the unpacked g.data() as the argument.  In other words, we expect "g" is a bare Tensor here.','line_number':345,'multiline':False]
['text':' norms is only needed to stash for backward.','line_number':347,'multiline':False]
['text':' g.scalar_type() may be at::ScalarType::Double, Float, or Half or BFloat16','line_number':348,'multiline':False]
['text':' If Half or BFloat16, stash norms as float.','line_number':349,'multiline':False]
['text':' Will this create norms on the same device as g, regardless of what the thread's default','line_number':352,'multiline':False]
['text':' current device is?  I believe so, because Type::* functions are DeviceGuard()ed.','line_number':353,'multiline':False]
['text':' Find logical size of each flattened slowest-dim row','line_number':360,'multiline':False]
['text':' Precompute slower_dims_size and fast_dim_size','line_number':389,'multiline':False]
['text':' The kernel execution is asynchronous, so this will only catch errors on the kernel launch,','line_number':420,'multiline':False]
['text':' not the kernel's execution.  Errors in kernel execution aren't guaranteed to be caught','line_number':421,'multiline':False]
['text':' until a later error check on a synchronizing CUDA call.  Unfortunately, without manually','line_number':422,'multiline':False]
['text':' synchronizing here, the foregoing is the best we can do.','line_number':423,'multiline':False]
['text':' These checks should always succeed, because weight_norm_fused_backward should only','line_number':435,'multiline':False]
['text':' ever be recorded in the autograd graph via weight_norm, which passes contiguous v and g.','line_number':436,'multiline':False]
['text':' Find logical size of each flattened slowest-dim row','line_number':449,'multiline':False]
['text':' Precompute slower_dims_size and fast_dim_size because they involve dynamically indexing an array.','line_number':480,'multiline':False]
['text':' The kernel execution is asynchronous, so this will only catch errors on the kernel launch,','line_number':513,'multiline':False]
['text':' not the kernel's execution.  Errors in kernel execution aren't guaranteed to be caught','line_number':514,'multiline':False]
['text':' until a later error check on a synchronizing CUDA call.  Unfortunately, without manually','line_number':515,'multiline':False]
['text':' synchronizing here, the foregoing is the best we can do.','line_number':516,'multiline':False]
['text':' namespace at::native','line_number':525,'multiline':False]
