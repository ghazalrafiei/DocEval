['text':' TODO: https://github.com/pytorch/pytorch/pull/59380#pullrequestreview-725310492','line_number':39,'multiline':False]
['text':' common case','line_number':49,'multiline':False]
['text':' common case','line_number':68,'multiline':False]
['text':' namespace','line_number':115,'multiline':False]
['text':' gemm call requires leading dimension and stride parameters to be non-zero','line_number':135,'multiline':False]
['text':' Make sure to keep addmm_cuda below in sync with this code; it','line_number':181,'multiline':False]
['text':' preflights a check to try to avoid actually needing to call','line_number':182,'multiline':False]
['text':' expand().','line_number':183,'multiline':False]
['text':' Strangely, if mat2 has only 1 row or column, we get','line_number':202,'multiline':False]
['text':' CUBLAS_STATUS_INVALID_VALUE error from cublasLtMatmulAlgoGetHeuristic.','line_number':203,'multiline':False]
['text':' self.dim() == 1 && result.dim() == 2 && self.sizes()[0] == mat2_sizes[1]','line_number':204,'multiline':False]
['text':' is to use lt interface only when self is bias.','line_number':205,'multiline':False]
['text':' for cuda 11.4, cublasLtMatmul is activated','line_number':206,'multiline':False]
['text':' the last two conditions is to skip 16b transA and non-trans-B having','line_number':207,'multiline':False]
['text':' leading dim >> rows when they are sliced from a large tensor','line_number':208,'multiline':False]
['text':' see fbcode/caffe2/test/test_linalg.py:test_corner_cases_of_cublasltmatmul','line_number':209,'multiline':False]
['text':' avoid leaing dim >> rows bugs','line_number':221,'multiline':False]
['text':' By definition, when beta==0, values in self should be ignored. nans and infs','line_number':260,'multiline':False]
['text':' should not propagate','line_number':261,'multiline':False]
['text':' TODO: We could squeeze some perf by calling at::cuda::mul_out here instead, to bypass the dispatcher.','line_number':265,'multiline':False]
['text':' That requires some fixing some internal build dependencies though.','line_number':266,'multiline':False]
['text':' layout ','line_number':273,'multiline':True]
['text':' pin_memory ','line_number':275,'multiline':True]
['text':' GELU is not supported (and does not compile!) prior','line_number':305,'multiline':False]
['text':' to CUDA 11.4. Have observed accuracy issues with','line_number':306,'multiline':False]
['text':' GELU epilogue in 11.4; disabling the GELU epilogue','line_number':307,'multiline':False]
['text':' path for CUDA version < 11.8.','line_number':308,'multiline':False]
['text':' Preprocessor gate here needs to match the inverse of the check','line_number':356,'multiline':False]
['text':' gating activation_to_gemm_and_blas_arg above; here we are manually','line_number':357,'multiline':False]
['text':' performing a post-GELU because we weren't able to use the GELU','line_number':358,'multiline':False]
['text':' epilogue above.','line_number':359,'multiline':False]
['text':' handle pathological cases that blas may not like','line_number':373,'multiline':False]
['text':' If batch is 1 call gemm rather than bgemm','line_number':425,'multiline':False]
['text':' anonymous namespace','line_number':454,'multiline':False]
['text':' anonymous namespace','line_number':518,'multiline':False]
['text':' shortcut for an empty matrix','line_number':622,'multiline':False]
['text':' By definition, when beta==0, values in self should be ignored. nans and infs','line_number':623,'multiline':False]
['text':' should not propagate','line_number':624,'multiline':False]
['text':' layout ','line_number':632,'multiline':True]
['text':' pin_memory ','line_number':632,'multiline':True]
['text':'if beta is 0, result contents will be zeroed later','line_number':635,'multiline':False]
['text':' Check for contiguity of `vec` and update `vec_stride` accordingly','line_number':642,'multiline':False]
['text':' A vector can be contiguous and have a stride of zero if it has it is of length 1','line_number':644,'multiline':False]
['text':' NOTE: cuBLAS is currently broken for some combination of transposed inputs.','line_number':672,'multiline':False]
['text':' Computes matrix multiply + bias while applying scaling to input and output matrices and computes amax','line_number':723,'multiline':False]
['text':' Scales are only applicable when matrices are of Float8 type and assumbed to be equal to 1.0 by default.','line_number':724,'multiline':False]
['text':' If output matrix type is 16 or 32-bit type, neither scale_result is applied nor amax is computed.','line_number':725,'multiline':False]
['text':' Known limitations:','line_number':726,'multiline':False]
['text':'  - Only works if mat1 is row-major and mat2 is column-major','line_number':727,'multiline':False]
['text':'  - Only works if matrices sizes are divisible by 32','line_number':728,'multiline':False]
['text':' Check sizes','line_number':738,'multiline':False]
['text':' Check types','line_number':764,'multiline':False]
['text':' Type restrictions imposed by CuBLASLt as of CUDA-12.1','line_number':769,'multiline':False]
['text':' namespace at::native','line_number':845,'multiline':False]
