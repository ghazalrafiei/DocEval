['text':' Load values.','line_number':40,'multiline':False]
['text':' Update param, grad, 1st and 2nd order momentum.','line_number':56,'multiline':False]
['text':' todo(crcrpar): use lerp','line_number':67,'multiline':False]
['text':' ref: https://developer.nvidia.com/blog/lerp-faster-cuda/','line_number':68,'multiline':False]
['text':' Store results.','line_number':84,'multiline':False]
['text':' [note: Conditional Gradient Store when `optimizer.step` is called by GradScaler]','line_number':97,'multiline':False]
['text':' When a user is training their model(s) with an FP16 AMP recipe,','line_number':98,'multiline':False]
['text':' parameter updates are done via `grad_scaler.step(optimizer)` instead of `optimizer.step()`.','line_number':99,'multiline':False]
['text':' For most optimizers, GradScaler unscales gradients on behalf of those optimizers.','line_number':100,'multiline':False]
['text':' Also, before `.step`, it makes sure that all the gradients involved are finite, which incurs a device sync.','line_number':101,'multiline':False]
['text':' On the other hand, fused optimizers set their member variable of `_step_supports_amp_scaling` to `True`','line_number':102,'multiline':False]
['text':' in order to remove the device sync above. This means that fused optimizers have to have','line_number':103,'multiline':False]
['text':' their CUDA kernels (a) unscale gradients and (b) skip parameter updates accordingly.','line_number':104,'multiline':False]
['text':' To be functionally on par with `torch.optim` optimizers and `_multi_tensor` ones,','line_number':105,'multiline':False]
['text':' the kernel below writes out gradients only when `grad_scale_ptr != nullptr.','line_number':106,'multiline':False]
['text':' namespace','line_number':171,'multiline':False]
['text':' namespace at::native','line_number':173,'multiline':False]
