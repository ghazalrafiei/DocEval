['text':'
  Memory types used for the 3 histogram implementations.
  See `CUDA_tensor_histogram` below.
 ','line_number':28,'multiline':True]
['text':'is_cuda=','line_number':37,'multiline':True]
['text':'is_cuda=','line_number':38,'multiline':True]
['text':' (only applicable for histc)','line_number':41,'multiline':False]
['text':' while each bin is inclusive at the lower end and exclusive at the higher,','line_number':42,'multiline':False]
['text':' i.e. [start, end) the last bin is inclusive at both, i.e. [start, end], in','line_number':43,'multiline':False]
['text':' order to include maxvalue if exists therefore when bin == nbins, adjust bin','line_number':44,'multiline':False]
['text':' to the last bin','line_number':45,'multiline':False]
['text':'
  Kernel for computing the histogram of the input.
 ','line_number':52,'multiline':True]
['text':' output ','line_number':66,'multiline':True]
['text':' partial output ','line_number':67,'multiline':True]
['text':' input ','line_number':68,'multiline':True]
['text':'is_cuda=','line_number':70,'multiline':True]
['text':'is_cuda=','line_number':71,'multiline':True]
['text':'//////////////////////// Shared memory //////////////////////////','line_number':78,'multiline':False]
['text':' atomically add to block specific shared memory','line_number':79,'multiline':False]
['text':' then atomically add to the global output tensor','line_number':80,'multiline':False]
['text':' Convert `linearIndex` into an offset of `b`','line_number':87,'multiline':False]
['text':' Use value at `b` as an offset of `smem`','line_number':92,'multiline':False]
['text':' NOTE: atomically update output bin count.','line_number':99,'multiline':False]
['text':'   Atomic update is imp since __syncthread() will only synchronize threads','line_number':100,'multiline':False]
['text':'   in a given block, not across blocks.','line_number':101,'multiline':False]
['text':'//////////////////////// Global memory //////////////////////////','line_number':109,'multiline':False]
['text':' atomically add to the output tensor','line_number':110,'multiline':False]
['text':' compute histogram for the block','line_number':111,'multiline':False]
['text':' Convert `linearIndex` into an offset of `b`','line_number':113,'multiline':False]
['text':' Use value at `b` as an offset of `a`','line_number':118,'multiline':False]
['text':'
  Calculate the frequency of the input values.

  `a` contains the final output or the histogram.
  Input `b` is assumed to be 1-D non-negative int array.
  `c` optionally contains the weight vector.
  See `help torch.bincount` for details on the math.

  3 implementations based of input size and memory usage:
    case: enough shared mem
        SHARED: Each block atomically adds to it's own **shared** hist copy,
        then atomically updates the global tensor.
    case: no enough shared mem
        GLOBAL: all threads atomically update to a single **global** hist copy.
 ','line_number':157,'multiline':True]
['text':' output ','line_number':174,'multiline':True]
['text':' input ','line_number':175,'multiline':True]
['text':' weights(optional) ','line_number':176,'multiline':True]
['text':'is_cuda=','line_number':178,'multiline':True]
['text':'is_cuda=','line_number':179,'multiline':True]
['text':' 8 guard bytes','line_number':202,'multiline':False]
['text':' determine memory type to use in the kernel','line_number':203,'multiline':False]
['text':' Solve equations:','line_number':205,'multiline':False]
['text':' (1) #(smem atomicAdd per SM) = totalElements / min(grid.x, #SM)','line_number':206,'multiline':False]
['text':' (2) #(gmem atomicAdd) = grid.x * nbins','line_number':207,'multiline':False]
['text':' (3) RATIO_OF_GMEM_ATOMIC_ADD_TO_SMEM_ATOMIC_ADD = #(gmem atomicAdd) / #(smem atomicAdd per SM)','line_number':208,'multiline':False]
['text':' namespace cuda','line_number':244,'multiline':False]
['text':'/////////////// bincount /////////////////','line_number':247,'multiline':False]
['text':' layout ','line_number':260,'multiline':True]
['text':' pin_memory ','line_number':262,'multiline':True]
['text':' we are using acc_type for the bounds, in particular int64_t for integers','line_number':278,'multiline':False]
['text':' in order to avoid overflows (e.g. using 256 bins for dtype uint8)','line_number':279,'multiline':False]
['text':'is_cuda=','line_number':280,'multiline':True]
['text':' alloc output counter on GPU','line_number':283,'multiline':False]
['text':' layout ','line_number':298,'multiline':True]
['text':' pin_memory ','line_number':300,'multiline':True]
['text':'/////////////// histc /////////////////','line_number':307,'multiline':False]
['text':'is_cuda=','line_number':312,'multiline':True]
['text':'is_cuda=','line_number':313,'multiline':True]
['text':' layout ','line_number':320,'multiline':True]
['text':' pin_memory ','line_number':322,'multiline':True]
['text':' namespace','line_number':359,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':365,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':370,'multiline':False]
['text':' Nondeterministic if weights are given, because of floating point','line_number':371,'multiline':False]
['text':' atomicAdd usage','line_number':372,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':392,'multiline':False]
['text':' Nondeterministic because of atomicAdd usage','line_number':393,'multiline':False]
['text':'is_cuda=','line_number':396,'multiline':True]
['text':' namespace native','line_number':408,'multiline':False]
['text':' namespace at','line_number':409,'multiline':False]
