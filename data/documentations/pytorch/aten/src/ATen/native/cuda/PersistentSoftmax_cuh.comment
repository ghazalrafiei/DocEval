['text':' The softmax_warp_* methods perform softmax forward and backward propagation on samples spanning the fast dimension.','line_number':46,'multiline':False]
['text':' Each sample contains element_count scalar elements. element_count can be any integer value <= 1024.','line_number':47,'multiline':False]
['text':' The template arguments have the following meaning:','line_number':48,'multiline':False]
['text':' One "WARP" works on one "BATCH". One "BATCH" contains "WARP_BATCH" samples.','line_number':49,'multiline':False]
['text':' WARP_BATCH is equal to 1 when element_count is large, and > 1 when element_count is small.','line_number':50,'multiline':False]
['text':' A "WARP" contains "C10_WARPS_SIZE" threads, these treads are guaranteed to belong to the same warp.','line_number':51,'multiline':False]
['text':' This is important because it means only __shfl_ instructions are required for reductions.','line_number':52,'multiline':False]
['text':' Note that this means WARP_SIZE must be a power of two and <= architecture warp size.','line_number':53,'multiline':False]
['text':' CUDA warp size is 32 for all existing GPU architectures, but there is no guarantee this will not change for future arch.','line_number':54,'multiline':False]
['text':' ROCm warp size is 64 for all currently ROCm-supported GPU architectures, but this may change for future archs.','line_number':55,'multiline':False]
['text':' is_log_softmax is a flag indicating whether SoftMax or LogSoftMax should be computed.','line_number':56,'multiline':False]
['text':' is_masked is a flag indicating whether SoftMax or MaskedSoftMax should be computed.','line_number':57,'multiline':False]
['text':' The template can be instantiated with any floating point type for the type arguments input_t, output_t and acc_t.','line_number':58,'multiline':False]
['text':' This allows SoftMax to be fused with a cast immediately following the SoftMax.','line_number':59,'multiline':False]
['text':' The mask should have the same shape as input, with a boolean indicate if the value is masked.','line_number':60,'multiline':False]
['text':' The head_chunk_size is only used for transformer mask softmax, equals to H * D * D.','line_number':61,'multiline':False]
['text':' For instance:','line_number':62,'multiline':False]
['text':' input_t=half,  acc_t=float, output_t=half  => read half tensor, float accumulators, write half tensor.','line_number':63,'multiline':False]
['text':' input_t=half,  acc_t=float, output_t=float => read half tensor, float accumulators, write float tensor.','line_number':64,'multiline':False]
['text':' input_t_float, acc_t=float, output_t=half  => read float tensor, float accumulators, write half tensor.','line_number':65,'multiline':False]
['text':' WARP_SIZE and WARP_BATCH must match the return values batches_per_warp and warp_size of method warp_softmax_forward_kernel.','line_number':70,'multiline':False]
['text':' batch_size might not be a multiple of WARP_BATCH. Check how','line_number':78,'multiline':False]
['text':' many batches have to computed within this WARP.','line_number':79,'multiline':False]
['text':' there might be multiple batches per warp. compute the index within the batch','line_number':84,'multiline':False]
['text':' The nested loops over WARP_BATCH and then WARP_ITERATIONS can be simplified to one loop,','line_number':96,'multiline':False]
['text':' but I think doing so would obfuscate the logic of the algorithm, thus I chose to keep','line_number':97,'multiline':False]
['text':' the nested loops.','line_number':98,'multiline':False]
['text':' This should have no impact on performance because the loops are unrolled anyway.','line_number':99,'multiline':False]
['text':' load data from global memory','line_number':101,'multiline':False]
['text':' compute max_value','line_number':115,'multiline':False]
['text':' Masked values are treated as -infinity, and std::exp(-infinity) is 0.','line_number':176,'multiline':False]
['text':' store result','line_number':190,'multiline':False]
['text':' WARP_SIZE and WARP_BATCH must match the return values batches_per_warp and warp_size of method warp_softmax_backward_kernel.','line_number':217,'multiline':False]
['text':' batch_size might not be a multiple of WARP_BATCH. Check how','line_number':225,'multiline':False]
['text':' many batches have to computed within this WARP.','line_number':226,'multiline':False]
['text':' there might be multiple batches per warp. compute the index within the batch','line_number':231,'multiline':False]
['text':' the first element to process by the current thread','line_number':234,'multiline':False]
['text':' The nested loops over WARP_BATCH and then WARP_ITERATIONS can be simplified to one loop,','line_number':243,'multiline':False]
['text':' but I think doing so would obfuscate the logic of the algorithm, thus I chose to keep','line_number':244,'multiline':False]
['text':' the nested loops.','line_number':245,'multiline':False]
['text':' This should have no impact on performance because the loops are unrolled anyway.','line_number':246,'multiline':False]
['text':' load data from global memory','line_number':248,'multiline':False]
['text':' store result','line_number':277,'multiline':False]
['text':' compute gradients','line_number':289,'multiline':False]
['text':' end of anonymous namespace','line_number':300,'multiline':False]
['text':' This value must match the WARP_SIZE constexpr value computed inside softmax_warp_forward.','line_number':312,'multiline':False]
['text':' This value must match the WARP_BATCH constexpr value computed inside softmax_warp_forward.','line_number':316,'multiline':False]
['text':' use 128 threads per block to maximimize gpu utilization','line_number':319,'multiline':False]
['text':' Launch code would be more elegant if C++ supported FOR CONSTEXPR','line_number':326,'multiline':False]
['text':' 1','line_number':335,'multiline':False]
['text':' 2','line_number':336,'multiline':False]
['text':' 4','line_number':337,'multiline':False]
['text':' 8','line_number':338,'multiline':False]
['text':' 16','line_number':339,'multiline':False]
['text':' 32','line_number':340,'multiline':False]
['text':' 64','line_number':341,'multiline':False]
['text':' 128','line_number':342,'multiline':False]
['text':' 256','line_number':343,'multiline':False]
['text':' 512','line_number':344,'multiline':False]
['text':' 1024','line_number':345,'multiline':False]
['text':' This value must match the WARP_SIZE constexpr value computed inside softmax_warp_backward.','line_number':362,'multiline':False]
['text':' This value must match the WARP_BATCH constexpr value computed inside softmax_warp_backward.','line_number':366,'multiline':False]
['text':' use 128 threads per block to maximimize gpu utilization','line_number':369,'multiline':False]
['text':' Launch code would be more elegant if C++ supported FOR CONSTEXPR','line_number':376,'multiline':False]
['text':' 1','line_number':386,'multiline':False]
['text':' 2','line_number':387,'multiline':False]
['text':' 4','line_number':388,'multiline':False]
['text':' 8','line_number':389,'multiline':False]
['text':' 16','line_number':390,'multiline':False]
['text':' 32','line_number':391,'multiline':False]
['text':' 64','line_number':392,'multiline':False]
['text':' 128','line_number':393,'multiline':False]
['text':' 256','line_number':394,'multiline':False]
['text':' 512','line_number':395,'multiline':False]
['text':' 1024','line_number':396,'multiline':False]
