['text':' Copyright (c) 2018 MathInf GmbH, Thomas Viehmann','line_number':1,'multiline':False]
['text':' Licensed under the BSD-3-Clause license','line_number':2,'multiline':False]
['text':' This is the GPU implementation of the Connectionist Temporal Loss.','line_number':3,'multiline':False]
['text':' We mostly follow Graves.','line_number':4,'multiline':False]
['text':' 1. Graves et al: http://www.cs.toronto.edu/~graves/icml_2006.pdf','line_number':5,'multiline':False]
['text':' We use the equations from above link, but note that [1] has 1-based indexing and we (of course) use 0-based.','line_number':6,'multiline':False]
['text':' Graves et al call the probabilities y, we use log_probs (also calling them inputs)','line_number':7,'multiline':False]
['text':' A few optimizations (similar to those here, but also some I didn't take) are described in','line_number':8,'multiline':False]
['text':' 2. Minmin Sun: http://on-demand.gputechconf.com/gtc/2016/presentation/s6383-minmin-sun-speech-recognition.pdf','line_number':9,'multiline':False]
['text':' this ad-hoc converts from targets (l in [1]) to augmented targets (l' in [1])','line_number':43,'multiline':False]
['text':' so if l is l_0 l_1 ... l_(tl-1) then this looks up idx in','line_number':44,'multiline':False]
['text':' l' = BLANK l_0 BLANK l_1 BLANK ... BLANK l_(tl-1) BLANK','line_number':45,'multiline':False]
['text':' - note that no bound-checking is done','line_number':46,'multiline':False]
['text':' - it is important to only call it witth idx == 0 if the target length is 0','line_number':47,'multiline':False]
['text':' - __restrict__ impact to be measured, see','line_number':48,'multiline':False]
['text':'   https://devblogs.nvidia.com/cuda-pro-tip-optimize-pointer-aliasing/','line_number':49,'multiline':False]
['text':' this kernel is a relatively straightforward implementation of the alpha calculation in the forward backward algorithm (section 4.1).','line_number':64,'multiline':False]
['text':' A (minor) twist is that we are using log-calculations to enhance numerical stability (log_probs and log_alpha).','line_number':65,'multiline':False]
['text':' In total it would be more efficient to compute the beta in the same kernel (e.g. cudnn does this). While the beta are not','line_number':66,'multiline':False]
['text':' needed for the loss itself (just the grad), we can return log_alpha+log_beta (so same space as currently) and the overhead','line_number':67,'multiline':False]
['text':' is small and the use-case for loss without grad is relatively limited.','line_number':68,'multiline':False]
['text':' We parallelize by batch and target sequence. Empirically, it is faster to loop over the input (log probs) sequence  and do','line_number':69,'multiline':False]
['text':' target in parallel, even if it means more frequent __syncthreads.','line_number':70,'multiline':False]
['text':' In contrast to the cuDNN implementation, we allow large target lengths. For this we need that all previous `s` have been','line_number':71,'multiline':False]
['text':' computed when we start a new block_s. This is why we have our own for loop here.','line_number':72,'multiline':False]
['text':' bookkeeping','line_number':89,'multiline':False]
['text':' first row (t=0), the three equations for alpha_1 above eq (6)','line_number':100,'multiline':False]
['text':' la_input_stride * 0 ','line_number':124,'multiline':True]
['text':' These two only depend on s, so we can cache them.','line_number':130,'multiline':False]
['text':' l_s in eq (6)','line_number':131,'multiline':False]
['text':' flag which of the two cases in eq (6) we have','line_number':132,'multiline':False]
['text':' on cuda 9 we might use partial synchronization of only the threads within the same batch','line_number':153,'multiline':False]
['text':' only for valid t, s. This is equation (6) and (7), la1, la2, la3 are the three summands,','line_number':155,'multiline':False]
['text':' lamax is the maximum for the logsumexp trick.','line_number':156,'multiline':False]
['text':' when all are neginf. (then the whole thing is neginf, but we can pretend)','line_number':174,'multiline':False]
['text':' otherwise we just set to neginf','line_number':180,'multiline':False]
['text':' on cuda 9 we might use partial synchronization of only the threads within the same batch','line_number':186,'multiline':False]
['text':' compute the loss (eq (8))','line_number':188,'multiline':False]
['text':' The forward computation. Lot's of admin and a call to the alpha kernel.','line_number':203,'multiline':False]
['text':' Note: we do not check that the labels are in the valid range. As we use','line_number':204,'multiline':False]
['text':' them for indexing in the kernels, you'll see memory errors when you','line_number':205,'multiline':False]
['text':' pass corrupt labels.','line_number':206,'multiline':False]
['text':' We support both a 2-dimensional tensor as targets (one set of targets in each row) and','line_number':207,'multiline':False]
['text':' a 1-dimensional tensor where all targets are concatenated (and we use target_lengths','line_number':208,'multiline':False]
['text':' to figure out where they begin).','line_number':209,'multiline':False]
['text':' We return log_alpha (currently, might change to (log_alpha+log_beta) to be passed to the','line_number':210,'multiline':False]
['text':' backward. The dispatch function will only return the loss.','line_number':211,'multiline':False]
['text':' log_probs: input_len x batch_size x num_labels','line_number':214,'multiline':False]
['text':' targets [int64]: batch_size x target_length OR sum(target_lengths)','line_number':215,'multiline':False]
['text':' concatenated targets','line_number':237,'multiline':False]
['text':' batch x max_target_length','line_number':248,'multiline':False]
['text':' dim is 2','line_number':249,'multiline':False]
['text':' Very likely, we could be more clever here, e.g. learning (or genralizing and reusing) from SoftMax.cu...','line_number':276,'multiline':False]
['text':' we need 72 or so 32 bit registers for double','line_number':277,'multiline':False]
['text':' The second (backward) half of the forward backward algorithm, (10) and (11). This is parallel to the','line_number':300,'multiline':False]
['text':' alpha kernel above. (As mentioned above, it might make sense do the calculation in the alpha kernel.)','line_number':301,'multiline':False]
['text':' "first" row, the beta initiaization before eq (10) (t=target_length - differes per batch)','line_number':325,'multiline':False]
['text':' false for target_length == 0','line_number':331,'multiline':False]
['text':' go backward in s','line_number':347,'multiline':False]
['text':' now go backward in t. Note that we need to skip the last timestep that we did above.','line_number':371,'multiline':False]
['text':' on cuda 9 we might use partial synchronization of only the threads within the same batch item','line_number':373,'multiline':False]
['text':' This implements the subtrahend of equation (16) for all *nonblank* characters.','line_number':412,'multiline':False]
['text':' It assumes you have probs in gradient_data when called','line_number':413,'multiline':False]
['text':' and it modifies gradient_data to be, the gradient.','line_number':414,'multiline':False]
['text':' In order to facilitate this inplace update, We don't actually do this in logspace.','line_number':415,'multiline':False]
['text':' (The other variant implemented uses log_space and the differences seem to be','line_number':416,'multiline':False]
['text':'  not so problematic at least with unit normal distributed test activations.)','line_number':417,'multiline':False]
['text':' Internally this uses atomicAdd because different threads may write to the same','line_number':418,'multiline':False]
['text':' gradient position.','line_number':419,'multiline':False]
['text':' This is parallelised over b and s again.','line_number':420,'multiline':False]
['text':' Note that for us, the Z of eqn (16) is actually constant for all t and it is the','line_number':421,'multiline':False]
['text':' likelihood - this is why we use the negative log likelihood below.','line_number':422,'multiline':False]
['text':' We also multiply by the input gradient to keep with standard autograd style.','line_number':423,'multiline':False]
['text':' I took this trick from [2], for moderate alphabet sizes a log-space','line_number':424,'multiline':False]
['text':' calculation (with an atomic log add) is similarly in performance, but for large','line_number':425,'multiline':False]
['text':' alphabets the inplace nature is a considerable advantage.','line_number':426,'multiline':False]
['text':' note, this directly indexes into targets, not targets prime!','line_number':445,'multiline':False]
['text':' This is the naive implementation of equation (16). It is parallelised in batch and input timestep.','line_number':477,'multiline':False]
['text':' It appears to be faster than the above method for small batch sizes.','line_number':478,'multiline':False]
['text':' collected[b, t, target'[s]] "log+=" log_alpha[t, s]+log_beta[t, s]','line_number':512,'multiline':False]
['text':' if target_length == 0, s == 0','line_number':514,'multiline':False]
['text':' This is to zero gradients which corresponding to the out-of-sequence position','line_number':548,'multiline':False]
['text':' Those gradients should not be used in any model update since the input','line_number':549,'multiline':False]
['text':' elements are padded','line_number':550,'multiline':False]
['text':' (T, B, D) layout ','line_number':557,'multiline':True]
['text':' (B, ) layout ','line_number':558,'multiline':True]
['text':' T ','line_number':562,'multiline':True]
['text':' B ','line_number':563,'multiline':True]
['text':' D ','line_number':564,'multiline':True]
['text':' The backward. It essentially computes eq 16 by using the above kernels.','line_number':582,'multiline':False]
['text':' We don't do a lot of checking as we envision this to be called only when backpropagating through a (well-checked) forward.','line_number':583,'multiline':False]
['text':' concatenated targets','line_number':596,'multiline':False]
['text':' batch x max_target_length','line_number':607,'multiline':False]
['text':' dim is 2','line_number':608,'multiline':False]
['text':' targets.size(1) might be larger','line_number':614,'multiline':False]
['text':' initialization for log(sum (alpha beta))','line_number':623,'multiline':False]
['text':' As above, there may be better configurations to use.','line_number':625,'multiline':False]
['text':' we need 72 or so 32 bit registers for double','line_number':626,'multiline':False]
['text':' Very crude heuristic for what is a small problem., based on linearly regressing problem dimensions on','line_number':649,'multiline':False]
['text':' the (capped) difference of timings.','line_number':650,'multiline':False]
['text':' Note that for OK problems target length <= input length, so we','line_number':651,'multiline':False]
['text':' only consider input length.','line_number':652,'multiline':False]
['text':' large alphabet, large batch','line_number':654,'multiline':False]
['text':' this computes the probs, minuend in (16)','line_number':655,'multiline':False]
['text':' now we compute the subtrahend for the blanks. It is a straightforward reduction because we know that','line_number':657,'multiline':False]
['text':' blanks are in every other position.','line_number':658,'multiline':False]
['text':' maybe we should kernelize this, too.','line_number':659,'multiline':False]
['text':' scale by output gradient (blanks and first summand of non-blanks)','line_number':671,'multiline':False]
['text':' For the non-blank characters, we use a kernel to compute the subtrahend.','line_number':677,'multiline':False]
['text':' Again we might configure block and grid in a better way.','line_number':678,'multiline':False]
['text':' small problem, use naive algorithm','line_number':704,'multiline':False]
['text':' Still no block/grid configuration guru...','line_number':705,'multiline':False]
['text':' catch launch errors','line_number':726,'multiline':False]
['text':' zero those invalid graident elements due to padding','line_number':729,'multiline':False]
['text':' namespace','line_number':756,'multiline':False]
['text':' only used for backward','line_number':759,'multiline':False]
['text':' See Note [Writing Nondeterministic Operations]','line_number':771,'multiline':False]
['text':' Nondeterministic because of atomicAdd usage','line_number':772,'multiline':False]
['text':' at::native','line_number':783,'multiline':False]
