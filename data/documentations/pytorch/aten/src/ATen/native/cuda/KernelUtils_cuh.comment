['text':' for channels-last','line_number':20,'multiline':False]
['text':' fastSpecializedAtomicAdd (and fastAtomicAdd) are an optimization','line_number':29,'multiline':False]
['text':' that speed up half-precision atomics.  The situation with half','line_number':30,'multiline':False]
['text':' precision atomics is that we have a slow __half atomic, and','line_number':31,'multiline':False]
['text':' a fast vectored __half2 atomic (this can be worth up to a 6x','line_number':32,'multiline':False]
['text':' speedup, see https://github.com/pytorch/pytorch/pull/21879).','line_number':33,'multiline':False]
['text':' We can convert a __half atomic into a __half2 atomic by simply','line_number':34,'multiline':False]
['text':' pairing the __half with a zero entry on the left/right depending','line_number':35,'multiline':False]
['text':' on alignment... but only if this wouldn't cause an out of bounds','line_number':36,'multiline':False]
['text':' access!  Thus, you must specify tensor and numel so we can check','line_number':37,'multiline':False]
['text':' if you would be out-of-bounds and use a plain __half atomic if','line_number':38,'multiline':False]
['text':' you would be.','line_number':39,'multiline':False]
['text':' Accounts for the chance tensor falls on an odd 16 bit alignment (ie, not 32 bit aligned)','line_number':57,'multiline':False]
['text':' Accounts for the chance tensor falls on an odd 16 bit alignment (ie, not 32 bit aligned)','line_number':97,'multiline':False]
['text':' namespace native','line_number':148,'multiline':False]
['text':' namespace at','line_number':149,'multiline':False]
