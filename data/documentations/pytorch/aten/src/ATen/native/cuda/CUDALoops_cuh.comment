['text':' This file provides two functions to help write GPU elementwise kernels:','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':'   gpu_kernel(TensorIterator iter, <lambda>)','line_number':5,'multiline':False]
['text':'   gpu_kernel_with_scalars(TensorIterator iter, <lambda>)','line_number':6,'multiline':False]
['text':'','line_number':7,'multiline':False]
['text':' The gpu_kernel_with_scalars generates specializations that support a','line_number':8,'multiline':False]
['text':' single scalar CPU argument, such as from `cuda_tensor + 5`. The CPU scalar','line_number':9,'multiline':False]
['text':' is lifted to a kernel parameter instead of copying to device memory.','line_number':10,'multiline':False]
['text':' This should be  used in conjunction with TensorIterator::allow_cpu_scalars_,','line_number':11,'multiline':False]
['text':' which is the default for TensorIterator::binary_op. Otherwise, all inputs','line_number':12,'multiline':False]
['text':' and the output must be on the GPU.','line_number':13,'multiline':False]
['text':'','line_number':14,'multiline':False]
['text':' For example, to write a reciprocal kernel for GPU float Tensors:','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':'   gpu_kernel(iter, []GPU_LAMBDA(float a) {','line_number':17,'multiline':False]
['text':'    return 1.0f / a;','line_number':18,'multiline':False]
['text':'   });','line_number':19,'multiline':False]
['text':'','line_number':20,'multiline':False]
['text':' To write a multiplication kernel for GPU float Tensors where one argument','line_number':21,'multiline':False]
['text':' may be a CPU scalar:','line_number':22,'multiline':False]
['text':'','line_number':23,'multiline':False]
['text':'   gpu_kernel_with_scalars(iter, []GPU_LAMBDA(float a, float b) {','line_number':24,'multiline':False]
['text':'     return a * b;','line_number':25,'multiline':False]
['text':'   });','line_number':26,'multiline':False]
['text':'','line_number':27,'multiline':False]
['text':' See BinaryOpsKernel.cu for the complete implementation','line_number':28,'multiline':False]
['text':'','line_number':29,'multiline':False]
['text':' if this block handles the reminder,','line_number':63,'multiline':False]
['text':' just do a naive unrolled loop','line_number':64,'multiline':False]
['text':' if this block has a full `block_work_size` data to handle, use','line_number':77,'multiline':False]
['text':' vectorized memory access','line_number':78,'multiline':False]
['text':' this function assume trivial 1d and no dynamic casting','line_number':107,'multiline':False]
['text':' namespace native','line_number':334,'multiline':False]
['text':' namespace at','line_number':335,'multiline':False]
