['text':' Maximum size per grid dimension that we assume (compute capability >= 2.0)','line_number':11,'multiline':False]
['text':' Resize output value, index Tensors to appropriate sizes (i.e. the same as','line_number':27,'multiline':False]
['text':' the input Tensor, except at dim=dimension, the size is 1)','line_number':28,'multiline':False]
['text':' If sliceSize is 1, copy input to values and set indices','line_number':44,'multiline':False]
['text':' Beginning our optimized implementation. First thing we want to do is to','line_number':55,'multiline':False]
['text':' transpose the input Tensor along the sort dimension, and then make it','line_number':56,'multiline':False]
['text':' contiguous.','line_number':57,'multiline':False]
['text':' We also need to view the values and indices Tensors as transposed in order','line_number':61,'multiline':False]
['text':' to properly determine the offset into the underlying storage in which to','line_number':62,'multiline':False]
['text':' place the mode and index for a particular set of dimension values.','line_number':63,'multiline':False]
['text':' Requirements for fused kernel implementation:','line_number':67,'multiline':False]
['text':'','line_number':68,'multiline':False]
['text':' 1. sliceSize <= 2 * max threads per block','line_number':69,'multiline':False]
['text':' 2. uses one block per slice, so number of slices must be less than the','line_number':70,'multiline':False]
['text':' maximum number of blocks for a kernel launch','line_number':71,'multiline':False]
['text':' 3. Can use 32-bit index math for indexing (mainly just for implementation','line_number':72,'multiline':False]
['text':' conciseness, could be changed)','line_number':73,'multiline':False]
['text':'','line_number':74,'multiline':False]
['text':' MAX_BLOCK_SIZE and MAX_GRID_SIZE come from:','line_number':75,'multiline':False]
['text':'     ATen/native/cuda/SortingCommon.cuh','line_number':76,'multiline':False]
['text':' [Note: CUDA torch.mode clones self]','line_number':83,'multiline':False]
['text':'','line_number':84,'multiline':False]
['text':' If transposed is already contiguous, it will return a tensor with the','line_number':85,'multiline':False]
['text':' same storage. So, since we do not want to modify self, we clone it.','line_number':86,'multiline':False]
['text':' namespace at::native','line_number':102,'multiline':False]
