['text':'//////////////////////////////////////////////////////////////////////////////','line_number':11,'multiline':False]
['text':'/ Specialization for row-major output (OperatorClass TensorOp), bf16 activation & bf16 weight','line_number':13,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':15,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':17,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':19,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':21,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':23,'multiline':False]
['text':'/ Tag indicating architecture to tune for','line_number':25,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':27,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':29,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':31,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':33,'multiline':False]
['text':'/ Use zfill or predicate for out-of-bound cp.async','line_number':35,'multiline':False]
['text':'/ Gather operand A by using an index array','line_number':37,'multiline':False]
['text':'/ Gather operand B by using an index array','line_number':39,'multiline':False]
['text':' Conversions only needed pre-ampere. This will trigger mma pipeline, so we convert before STS.','line_number':62,'multiline':False]
['text':' Define the MmaCore components','line_number':68,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':91,'multiline':False]
['text':' Define the threadblock-scoped pipelined matrix multiply','line_number':101,'multiline':False]
['text':' bf16 x bf16 specialization on Ampere to use mma multistage for 2 stage. Helps avoid reg spills on','line_number':112,'multiline':False]
['text':' large tile when not enough shared mem is present to do 3+ stage','line_number':113,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':115,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':117,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':119,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':121,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':123,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':125,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':127,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':129,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':131,'multiline':False]
['text':'/ Use zfill or predicate for out-of-bound cp.async','line_number':133,'multiline':False]
['text':'/ Gather operand A by using an index array','line_number':135,'multiline':False]
['text':'/ Gather operand B by using an index array','line_number':137,'multiline':False]
['text':' Define the MmaCore components','line_number':159,'multiline':False]
['text':' 3 is used on purpose here to trigger components for mma multistage','line_number':160,'multiline':False]
['text':' Define iterators over tiles from the A operand','line_number':174,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':186,'multiline':False]
['text':' Define the threadblock-scoped multistage matrix multiply','line_number':198,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':212,'multiline':False]
['text':'/ Specialization for row-major output (OperatorClass TensorOp), bf16 activation & int8 weight','line_number':214,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':216,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':218,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':220,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':222,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':224,'multiline':False]
['text':'/ Tag indicating architecture to tune for','line_number':226,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':228,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':230,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':232,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':234,'multiline':False]
['text':' Define the MmaCore components','line_number':275,'multiline':False]
['text':' Define iterators over tiles from the A operand','line_number':278,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':281,'multiline':False]
['text':' Define the threadblock-scoped pipelined matrix multiply','line_number':284,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':288,'multiline':False]
['text':'/ Specialization for row-major output (OperatorClass TensorOp), bf16 activation & int4 weight','line_number':289,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':291,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':293,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':295,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':297,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':299,'multiline':False]
['text':'/ Tag indicating architecture to tune for','line_number':301,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':303,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':305,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':307,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':309,'multiline':False]
['text':' Define the MmaCore components','line_number':350,'multiline':False]
['text':' Define iterators over tiles from the A operand','line_number':353,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':356,'multiline':False]
['text':' Define the threadblock-scoped pipelined matrix multiply','line_number':359,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':364,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':366,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':368,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':370,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':372,'multiline':False]
['text':'/ Tag indicating architecture to tune for','line_number':374,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':376,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':378,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':380,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':382,'multiline':False]
['text':'/','line_number':384,'multiline':False]
['text':'/ Shared memory clear option','line_number':386,'multiline':False]
['text':' Define the MmaCore components','line_number':430,'multiline':False]
['text':' Define iterators over tiles from the A operand','line_number':433,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':436,'multiline':False]
['text':' Define the threadblock-scoped pipelined matrix multiply','line_number':439,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':443,'multiline':False]
['text':'/ Specialization for row-major output (OperatorClass TensorOp), fp16 activation & int4 weight','line_number':444,'multiline':False]
['text':'/ Layout type for A matrix operand','line_number':446,'multiline':False]
['text':'/ Access granularity of A matrix in units of elements','line_number':448,'multiline':False]
['text':'/ Layout type for B matrix operand','line_number':450,'multiline':False]
['text':'/ Access granularity of B matrix in units of elements','line_number':452,'multiline':False]
['text':'/ Element type for internal accumulation','line_number':454,'multiline':False]
['text':'/ Tag indicating architecture to tune for','line_number':456,'multiline':False]
['text':'/ Threadblock-level tile size (concept: GemmShape)','line_number':458,'multiline':False]
['text':'/ Warp-level tile size (concept: GemmShape)','line_number':460,'multiline':False]
['text':'/ Instruction-level tile size (concept: GemmShape)','line_number':462,'multiline':False]
['text':'/ Operation performed by GEMM','line_number':464,'multiline':False]
['text':'/','line_number':466,'multiline':False]
['text':'/ Shared memory clear option','line_number':468,'multiline':False]
['text':' Define the MmaCore components','line_number':512,'multiline':False]
['text':' Define iterators over tiles from the A operand','line_number':515,'multiline':False]
['text':' Define iterators over tiles from the B operand','line_number':518,'multiline':False]
['text':' Define the threadblock-scoped pipelined matrix multiply','line_number':521,'multiline':False]
['text':' namespace threadblock','line_number':525,'multiline':False]
['text':' namespace gemm','line_number':526,'multiline':False]
['text':' namespace cutlass','line_number':527,'multiline':False]
