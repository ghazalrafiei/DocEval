['text':' Zero norm','line_number':35,'multiline':False]
['text':'p','line_number':37,'multiline':True]
['text':'p','line_number':38,'multiline':True]
['text':' One norm','line_number':42,'multiline':False]
['text':'p','line_number':44,'multiline':True]
['text':'p','line_number':45,'multiline':True]
['text':'dist','line_number':47,'multiline':True]
['text':'p','line_number':47,'multiline':True]
['text':' Special case backward when p is less than two','line_number':50,'multiline':False]
['text':' Two norm','line_number':57,'multiline':False]
['text':'p','line_number':59,'multiline':True]
['text':'p','line_number':60,'multiline':True]
['text':'p','line_number':62,'multiline':True]
['text':' General p norm','line_number':65,'multiline':False]
['text':' Inf norm','line_number':73,'multiline':False]
['text':'p','line_number':75,'multiline':True]
['text':'p','line_number':76,'multiline':True]
['text':'p','line_number':78,'multiline':True]
['text':' The -1 accounts for floating point truncation issues','line_number':101,'multiline':False]
['text':' The -1 accounts for floating point truncation issues','line_number':165,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/15511 demonstrated we need to do','line_number':244,'multiline':False]
['text':' some math in fp64 -- this is just minimizing the amount of fp64 math we do on the device.','line_number':245,'multiline':False]
['text':' NB: be careful with changing block_y; as it's currently written, grid_y is limited to be 2^16.','line_number':274,'multiline':False]
['text':' block_y of 64 gives us max pdist dim1 of 2**24','line_number':275,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/15511 demonstrated we need to do','line_number':281,'multiline':False]
['text':' some math in fp64 -- this is just minimizing the amount of fp64 math we do on the device.','line_number':282,'multiline':False]
['text':' Just like we do in the CPU code, assume that result is always batched','line_number':314,'multiline':False]
['text':'current implementation supports only gradient that can be collapsed to 1D. However, to avoid checking this assumption,','line_number':332,'multiline':False]
['text':'we call grad.contiguous() before backward, so stride is guaranteed to be 1','line_number':333,'multiline':False]
['text':' anonymous namespace','line_number':358,'multiline':False]
['text':' at::native','line_number':365,'multiline':False]
