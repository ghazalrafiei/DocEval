['text':'**************************************************************************************************
 * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *************************************************************************************************','line_number':1,'multiline':True]
['text':'! \file
    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
','line_number':31,'multiline':True]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':46,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':52,'multiline':False]
['text':' SFINAE trick so I can keep the same loop code for Volta and dispatch to the','line_number':53,'multiline':False]
['text':' correct warp level mma. On volta, all data is stored to shared memory as FP16.','line_number':54,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':76,'multiline':False]
['text':'/ Structure to compute the matrix product targeting CUDA cores and SIMT math','line_number':78,'multiline':False]
['text':'/ instructions.','line_number':79,'multiline':False]
['text':'/ Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':81,'multiline':False]
['text':'/ Policy describing tuning details (concept: MmaPolicy)','line_number':83,'multiline':False]
['text':'/ The type of the scales','line_number':85,'multiline':False]
['text':'/ Number of stages,','line_number':87,'multiline':False]
['text':'/ Used for partial specialization','line_number':89,'multiline':False]
['text':'/< Size of the Gemm problem - concept: gemm::GemmShape<>','line_number':93,'multiline':False]
['text':'/< Policy describing tuning details','line_number':96,'multiline':False]
['text':'/< Type of the scale to be loaded','line_number':99,'multiline':False]
['text':'','line_number':102,'multiline':False]
['text':' Dependent types','line_number':103,'multiline':False]
['text':'','line_number':104,'multiline':False]
['text':'/ Warp-level Mma','line_number':106,'multiline':False]
['text':'/ Shape describing the overall GEMM computed from shared memory','line_number':109,'multiline':False]
['text':'/ by each warp.','line_number':110,'multiline':False]
['text':'/ Shape describing the number of warps filling the CTA','line_number':113,'multiline':False]
['text':'/ Number of warp-level GEMM oeprations','line_number':116,'multiline':False]
['text':'/ Number of stages','line_number':125,'multiline':False]
['text':'/ Tensor reference to the A operand','line_number':128,'multiline':False]
['text':'/ Tensor reference to the B operand','line_number':131,'multiline':False]
['text':'','line_number':134,'multiline':False]
['text':' Nested structs','line_number':135,'multiline':False]
['text':'','line_number':136,'multiline':False]
['text':'/ Shared storage object needed by threadblock-scoped GEMM','line_number':138,'multiline':False]
['text':'','line_number':141,'multiline':False]
['text':' Type definitions','line_number':142,'multiline':False]
['text':'','line_number':143,'multiline':False]
['text':'/ Shape of the A matrix operand in shared memory','line_number':145,'multiline':False]
['text':'/ Shape of the B matrix operand in shared memory','line_number':149,'multiline':False]
['text':'','line_number':154,'multiline':False]
['text':' Data members','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':'/ Buffer for A operand','line_number':158,'multiline':False]
['text':'/ Buffer for B operand','line_number':161,'multiline':False]
['text':'/ Buffer to hold scales for threadblock','line_number':164,'multiline':False]
['text':'','line_number':168,'multiline':False]
['text':' Methods','line_number':169,'multiline':False]
['text':'','line_number':170,'multiline':False]
['text':'/ Returns a layout object for the A matrix','line_number':172,'multiline':False]
['text':'/ Returns a layout object for the B matrix','line_number':179,'multiline':False]
['text':'/ Returns a TensorRef to the A operand','line_number':186,'multiline':False]
['text':'/ Returns a TensorRef to the B operand','line_number':193,'multiline':False]
['text':'','line_number':202,'multiline':False]
['text':' Data members','line_number':203,'multiline':False]
['text':'','line_number':204,'multiline':False]
['text':'/ Iterator to load a warp-scoped tile of A operand from shared memory','line_number':206,'multiline':False]
['text':'/ Iterator to load a warp-scoped tile of B operand from shared memory','line_number':209,'multiline':False]
['text':'/ Construct from tensor references','line_number':213,'multiline':False]
['text':'/< Shared storage needed for internal use by threadblock-scoped GEMM','line_number':216,'multiline':False]
['text':'/< ID within the threadblock','line_number':218,'multiline':False]
['text':'/< ID of warp','line_number':220,'multiline':False]
['text':'/< ID of each thread within a warp','line_number':222,'multiline':False]
['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':230,'multiline':False]
['text':' namespace threadblock','line_number':232,'multiline':False]
['text':' namespace gemm','line_number':233,'multiline':False]
['text':' namespace cutlass','line_number':234,'multiline':False]
['text':'///////////////////////////////////////////////////////////////////////////////////////////////','line_number':236,'multiline':False]
