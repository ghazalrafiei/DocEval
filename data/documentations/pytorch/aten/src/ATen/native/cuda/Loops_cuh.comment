['text':' array size can not be 0, this happens when N == 0','line_number':19,'multiline':False]
['text':' load','line_number':54,'multiline':False]
['text':' compute','line_number':57,'multiline':False]
['text':' store','line_number':65,'multiline':False]
['text':' namespace at::native','line_number':69,'multiline':False]
['text':' Note:','line_number':71,'multiline':False]
['text':' CUDA and ROCm get diverged in this PR:','line_number':72,'multiline':False]
['text':'   https://github.com/pytorch/pytorch/pull/32383','line_number':73,'multiline':False]
['text':' Because for some reason trying to enable vectorized','line_number':74,'multiline':False]
['text':' memory access introduce regression on ROCm.','line_number':75,'multiline':False]
['text':' NB: scalar is stored in higher precision!','line_number':138,'multiline':False]
['text':' NB: scalar is stored in higher precision!','line_number':152,'multiline':False]
['text':' Though seemingly noop, this inserts casts from arg1_t to func_t's type','line_number':159,'multiline':False]
['text':' (which may be higher precision), as well as casts to return_t','line_number':160,'multiline':False]
['text':' Unlike gpu_kernel_with_scalars, this allows you to pass a func_t which','line_number':171,'multiline':False]
['text':' accepts inputs at higher precision (typically opmath_t), but then','line_number':172,'multiline':False]
['text':' ensure that we load from memory at the correct precision (scalar_t)','line_number':173,'multiline':False]
['text':' to avoid expensive loads.  For the whole sordid story see','line_number':174,'multiline':False]
['text':' https://dev-discuss.pytorch.org/t/cuda-loops-case-study-code-generation-vs-templates/302','line_number':175,'multiline':False]
['text':' TODO: When all kernels that use gpu_kernel_with_scalars are','line_number':190,'multiline':False]
['text':' ported to structured, this device guard can be deleted.  This','line_number':191,'multiline':False]
['text':' works around incorrect device guard generation for pre-structured','line_number':192,'multiline':False]
['text':' kernels device guards, but structured kernels do it right and','line_number':193,'multiline':False]
['text':' we can assume the device is already set correctly','line_number':194,'multiline':False]
['text':' Use symmetric property of the functor to reduce number of kernels,','line_number':208,'multiline':False]
['text':' requires f(a, b) == f(b, a)','line_number':209,'multiline':False]
['text':' TODO: When all kernels that use gpu_kernel_with_scalars are','line_number':227,'multiline':False]
['text':' ported to structured, this device guard can be deleted.  This','line_number':228,'multiline':False]
['text':' works around incorrect device guard generation for pre-structured','line_number':229,'multiline':False]
['text':' kernels device guards, but structured kernels do it right and','line_number':230,'multiline':False]
['text':' we can assume the device is already set correctly','line_number':231,'multiline':False]
['text':' Legacy variant that assumes that func_t has the correct types','line_number':246,'multiline':False]
['text':' that we expect to load from memory','line_number':247,'multiline':False]
['text':' functions for `gpu_kernel_multiple_outputs`.','line_number':260,'multiline':False]
['text':' check the return type is `thrust::tuple`, not `std::tuple`.','line_number':262,'multiline':False]
['text':' namespace','line_number':312,'multiline':False]
['text':'namespace at::native','line_number':336,'multiline':False]
