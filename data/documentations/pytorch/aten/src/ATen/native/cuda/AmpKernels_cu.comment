['text':' Thin wrapper around https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE_1g57a3c8313f570282a1a7bcc78743b08e,','line_number':16,'multiline':False]
['text':' to ensure the Cuda math library's isfinite is actually what gets called in','line_number':17,'multiline':False]
['text':' _amp_non_finite_check_and_unscale_cuda_'s gpu_kernel lambda.','line_number':18,'multiline':False]
['text':'','line_number':19,'multiline':False]
['text':' isfinite_ensure_cuda_math is defined outside at::native because:','line_number':20,'multiline':False]
['text':' - A bare call to "isfinite(val)" inside at::native causes nvcc to prefer the unrelated','line_number':21,'multiline':False]
['text':'   Tensor at::native::isfinite(const Tensor&), resulting in an error:','line_number':22,'multiline':False]
['text':'   "no suitable constructor exists to convert from "float" to "at::Tensor""','line_number':23,'multiline':False]
['text':' - Unfortunately, the Cuda math library documentation doesn't say how (or if) you can provide a full namespace path','line_number':24,'multiline':False]
['text':'   to ensure that its version of a particular function is invoked.  It only shows bare (not-namespaced)','line_number':25,'multiline':False]
['text':'   calls to its routines inside kernel or device functions.','line_number':26,'multiline':False]
['text':' - "std::isfinite(val)" in the gpu_kernel lambda causes an "unspecified launch failure" at runtime with cuda 9 on Windows.','line_number':27,'multiline':False]
['text':'','line_number':28,'multiline':False]
['text':' isfinite_ensure_cuda_math, declared at file scope outside the at::native region, uses isfinite as math library docs','line_number':29,'multiline':False]
['text':' suggest and allows disambiguated usage in the lambda within the at::native region.','line_number':30,'multiline':False]
['text':' GPU_LAMBDA is defined as __host__ __device__ (see Loops.cuh), so I need the __host__ keyword or else nvcc complains that','line_number':31,'multiline':False]
['text':' "calling a __device__ function("isfinite_ensure_cuda_math") from a __host__ __device__ function("operator()") is not allowed."','line_number':32,'multiline':False]
['text':' Single-tensor fallback for _amp_foreach_non_finite_check_and_unscale_cuda_.','line_number':41,'multiline':False]
['text':' Handles individual tensors that are acceptable to unscale but not MTA-safe.','line_number':42,'multiline':False]
['text':' The only way we reach this function is through _amp_foreach_non_finite_check_and_unscale_cuda_, so no input checks.','line_number':47,'multiline':False]
['text':' It's not obvious gpu_kernel always guards onto its argument.  Guarding here just in case.','line_number':49,'multiline':False]
['text':' Acts on scaled_grad in place.','line_number':52,'multiline':False]
['text':' Every thread accesses inv_scale, but it will hit in cache.','line_number':70,'multiline':False]
['text':' anonymous namespace','line_number':76,'multiline':False]
['text':' Multiplies each tensor in scaled_grads by inv_scale in-place.','line_number':79,'multiline':False]
['text':' If any element of any tensor in scaled_grads is inf or NaN, sets found_inf to 1.0.','line_number':80,'multiline':False]
['text':' Uses multi tensor apply (MTA) to process all MTA-safe tensors.','line_number':81,'multiline':False]
['text':'','line_number':82,'multiline':False]
['text':' Args:','line_number':83,'multiline':False]
['text':' scaled_grads:  A TensorList of scaled gradient tensors.  May contain infs or NaNs.','line_number':84,'multiline':False]
['text':' found_inf:  A single-element float tensor to which 1.0 will be written if any gradient contain infs/nans.','line_number':85,'multiline':False]
['text':'             Pre-zeroing found_inf, if appropriate, is the responsibility of the caller.','line_number':86,'multiline':False]
['text':' inv_scale:  The inverse of the scale factor by which scaled_grads are currently multiplied.','line_number':87,'multiline':False]
['text':' Ensures client code (GradScaler) filtered scaled_grads by dtype.','line_number':103,'multiline':False]
['text':' is_non_overlapping_and_dense() is not available in Python.','line_number':108,'multiline':False]
['text':' GradScaler can't filter for it. We need to filter here.','line_number':109,'multiline':False]
['text':' Hopefully common case.','line_number':111,'multiline':False]
['text':' can_use_fast_route is true, which confirms:','line_number':112,'multiline':False]
['text':'  - all scaled_grads are strided','line_number':113,'multiline':False]
['text':'  - all scaled_grads are non overlapping and dense','line_number':114,'multiline':False]
['text':'  - all scaled_grads are on the same device','line_number':115,'multiline':False]
['text':'  - all scaled_grads are of the same dtype','line_number':116,'multiline':False]
['text':' Sets up MTA launch to use scaled_grads as-is.','line_number':118,'multiline':False]
['text':' Hopefully uncommon case.','line_number':121,'multiline':False]
['text':' can_use_fast_route is an all-or-nothing check.  In this path it was false,','line_number':122,'multiline':False]
['text':' so any of the above confirmations could have gone wrong.','line_number':123,'multiline':False]
['text':' We filter MTA-safe tensors into an MTA-able list.','line_number':124,'multiline':False]
['text':' If a tensor is acceptable but not MTA-safe, we fall back to the TensorIterator kernel.','line_number':125,'multiline':False]
['text':' If a tensor is unacceptable, we throw an error to blame GradScaler.','line_number':126,'multiline':False]
['text':' Ensures GradScaler filtered scaled_grads by device.','line_number':132,'multiline':False]
['text':' t is acceptable but not MTA-safe.  Falls back to single-tensor TensorIterator kernel.','line_number':137,'multiline':False]
['text':' multi_tensor_apply guards onto tensor_lists[0][0], no need to guard explicitly.','line_number':159,'multiline':False]
['text':' depth ','line_number':162,'multiline':True]
['text':' r_args_depth ','line_number':163,'multiline':True]
['text':' res_arg_index ','line_number':164,'multiline':True]
['text':' There is a slight asymmetry here with the TensorIterator kernel above.','line_number':166,'multiline':False]
['text':' MTA Functors ensure val comes in as opmath_t rather than scalar_t.','line_number':167,'multiline':False]
['text':' Every thread accesses inv_scale, but it will hit in cache.','line_number':171,'multiline':False]
['text':' amp_update_scale_cuda_kernel is launched with a single thread to compute the new scale.','line_number':179,'multiline':False]
['text':' The scale factor is maintained and updated on the GPU to avoid synchronization.','line_number':180,'multiline':False]
['text':' Entering this branch means we just carried out a successful step,','line_number':192,'multiline':False]
['text':' so growth_tracker is incremented before comparing to growth_interval.','line_number':193,'multiline':False]
['text':' Do not grow the scale past fp32 bounds to inf.','line_number':197,'multiline':False]
['text':' _amp_update_scale_cuda asynchronously updates the scale tensor in place.','line_number':209,'multiline':False]
['text':'','line_number':210,'multiline':False]
['text':' Args:','line_number':211,'multiline':False]
['text':' current_scale:  A one-element cuda float tensor containing the scale value.','line_number':212,'multiline':False]
['text':' growth_tracker:  A one-element torch.cuda.IntTensor containing the number of recent consecutive unskipped steps.','line_number':213,'multiline':False]
['text':' found_inf:  A one-element cuda float tensor. If > 0, indicates that infs/nans were found by the relevant','line_number':214,'multiline':False]
['text':'             prior _amp_non_finite_check_and_unscale_cuda call, and 0 if no infs/nans were found.','line_number':215,'multiline':False]
['text':' growth_factor:  Multiplier if no infs/NaNs were found (typically slightly > 1).','line_number':216,'multiline':False]
['text':' backoff_factor:  Multiplier if infs/NaNs were found (typically 0.5).','line_number':217,'multiline':False]
['text':' growth_interval:  Number of consecutive unskipped steps that must occur for current_scale to be multiplied by','line_number':218,'multiline':False]
['text':'                   growth_factor.','line_number':219,'multiline':False]
['text':'','line_number':220,'multiline':False]
['text':' Returns:','line_number':221,'multiline':False]
['text':' current_scale','line_number':222,'multiline':False]
['text':' namespace at::native','line_number':252,'multiline':False]
