['text':' hyper-volume to column, CPU','line_number':27,'multiline':False]
['text':' column to hyper-volume, CPU','line_number':85,'multiline':False]
['text':'
   check tensor data locations
','line_number':143,'multiline':True]
['text':' checking data locations of user-provided tensor arguments','line_number':151,'multiline':False]
['text':' we are not checking the data locations of other tensor','line_number':159,'multiline':False]
['text':' arguments such as output, grad_input, etc because of these are','line_number':160,'multiline':False]
['text':' allocated based on input options and hence these tensors always','line_number':161,'multiline':False]
['text':' have the same data location as of input tensor.','line_number':162,'multiline':False]
['text':'
  slow_conv_dilated_all_cpu_template

  Main worker. Computes tensors output, grad_input, grad_weight,
  and/or grad_bias if defined, respectively.
 ','line_number':165,'multiline':True]
['text':' The rear part of input tensor sizes:','line_number':189,'multiline':False]
['text':' The rear part of output tensor sizes:','line_number':191,'multiline':False]
['text':' Temporary buffer:','line_number':197,'multiline':False]
['text':' Initialize','line_number':208,'multiline':False]
['text':' Helpers','line_number':218,'multiline':False]
['text':' For each elt in batch, do:','line_number':225,'multiline':False]
['text':' Matrix multiply per output:','line_number':227,'multiline':False]
['text':' Output','line_number':230,'multiline':False]
['text':'
            Compute:

              output_n = bias * ones^T

            where

              bias is viewed as bias.view(nOutputPlane, 1)

              ones is viewed as ones.view(outputHeight * outputWidth, 1)

              output_n is viewed as output_n.view(nOutputPlane, outputHeight
          * outputWidth)

          gemm assumes column-major matrices:

            output_n^T = ones * bias^T
            C = alpha * op(A) * op(B)
            op(A) = 't', op(B) = 'n', alpha=1, beta=0
          ','line_number':234,'multiline':True]
['text':' The following for-loop is equivalent to the above','line_number':254,'multiline':False]
['text':' gemm setup but avoids allocation of ones tensor:','line_number':255,'multiline':False]
['text':' Extract columns:','line_number':260,'multiline':False]
['text':'
          Compute:

            output_n = weight * columns + output_n

          where

            weight is viewed as weight.view(nOutputPlane, nInputPlane * kD *
          kH * kW)

            columns size is (nInputPlane * kH * kW) x (outputHeight *
          outputWidth)

            output_n is viewed as output_n.view(nOutputPlane, outputHeight *
          outputWidth)

          gemm assumes column-major matrices:

          channels last:
            output_n^T = weight *columns^T + output_n^T
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 't', op(B) = 'n', alpha=1, beta=1

          channels first:
            output_n^T = columns^T * weight^T + output_n^T
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 'n', op(B) = 'n', alpha=1, beta=1
        ','line_number':272,'multiline':True]
['text':'transa=','line_number':302,'multiline':True]
['text':'transb=','line_number':303,'multiline':True]
['text':'     m=','line_number':304,'multiline':True]
['text':'     n=','line_number':305,'multiline':True]
['text':'     k=','line_number':306,'multiline':True]
['text':' alpha=','line_number':307,'multiline':True]
['text':'     A=','line_number':308,'multiline':True]
['text':'   lda=','line_number':309,'multiline':True]
['text':'     B=','line_number':310,'multiline':True]
['text':'   lda=','line_number':311,'multiline':True]
['text':'  beta=','line_number':312,'multiline':True]
['text':'     C=','line_number':313,'multiline':True]
['text':'   ldc=','line_number':314,'multiline':True]
['text':'transa=','line_number':317,'multiline':True]
['text':'transb=','line_number':318,'multiline':True]
['text':'     m=','line_number':319,'multiline':True]
['text':'     n=','line_number':320,'multiline':True]
['text':'     k=','line_number':321,'multiline':True]
['text':' alpha=','line_number':322,'multiline':True]
['text':'     A=','line_number':323,'multiline':True]
['text':'   lda=','line_number':324,'multiline':True]
['text':'     B=','line_number':325,'multiline':True]
['text':'   ldb=','line_number':326,'multiline':True]
['text':'  beta=','line_number':327,'multiline':True]
['text':'     C=','line_number':328,'multiline':True]
['text':'   ldc=','line_number':329,'multiline':True]
['text':' All gradients','line_number':332,'multiline':False]
['text':' Gradient of input:','line_number':336,'multiline':False]
['text':'
          Compute:

            columns = weight^T * grad_output_n

          where

            weight is viewed as weight.view(nOutputPlane, nInputPlane * kH *
          kW)

            grad_output_n is viewed as grad_output_n.view(nOutputPlane,
          outputHeight * outputWidth)

            columns size is (nInputPlane * kH * kW) x (outputHeight *
          outputWidth)

          gemm assumes column-major matrices:

          channels last:
            columns^T = weight^T * grad_output_n^T
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 'n', op(B) = 'n', alpha=1, beta=0

          channels first:
            columns^T = grad_output_n^T * weight
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 'n', op(B) = 't', alpha=1, beta=0
         ','line_number':338,'multiline':True]
['text':'transa=','line_number':368,'multiline':True]
['text':'transb=','line_number':369,'multiline':True]
['text':'     m=','line_number':370,'multiline':True]
['text':'     n=','line_number':371,'multiline':True]
['text':'     k=','line_number':372,'multiline':True]
['text':' alpha=','line_number':373,'multiline':True]
['text':'     A=','line_number':374,'multiline':True]
['text':'   lda=','line_number':375,'multiline':True]
['text':'     B=','line_number':376,'multiline':True]
['text':'   ldb=','line_number':377,'multiline':True]
['text':'  beta=','line_number':378,'multiline':True]
['text':'     C=','line_number':379,'multiline':True]
['text':'   ldc=','line_number':380,'multiline':True]
['text':'transa=','line_number':383,'multiline':True]
['text':'transb=','line_number':384,'multiline':True]
['text':'     m=','line_number':385,'multiline':True]
['text':'     n=','line_number':386,'multiline':True]
['text':'     k=','line_number':387,'multiline':True]
['text':' alpha=','line_number':388,'multiline':True]
['text':'     A=','line_number':389,'multiline':True]
['text':'   lda=','line_number':390,'multiline':True]
['text':'     B=','line_number':391,'multiline':True]
['text':'   ldb=','line_number':392,'multiline':True]
['text':'  beta=','line_number':393,'multiline':True]
['text':'     C=','line_number':394,'multiline':True]
['text':'   ldc=','line_number':395,'multiline':True]
['text':' Unpack columns back into input:','line_number':397,'multiline':False]
['text':' Gradient of weight:','line_number':413,'multiline':False]
['text':' Extract columns:','line_number':415,'multiline':False]
['text':' TODO: expose as argument?','line_number':427,'multiline':False]
['text':'
          Compute:

            grad_weight = scale * grad_output_n * columns^T + grad_weight

          where

            grad_output_n is viewed as grad_output_n.view(nOutputPlane,
          outputHeight * outputWidth)

            columns size is (nInputPlane * kD * kH * kW) x (outputHeight *
          outputWidth)

            grad_weight is viewed as grad_weight.view(nOutputPlane,
          nInputPlane * kH * kW)

          gemm assumes column-major matrices:

          channels last:
            grad_weight^T = scale * columns^T * grad_output_n + grad_weight^T
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 'n', op(B) = 't', alpha=scale, beta=1

          channels first:
            grad_weight^T = scale * columns * grad_output_n^T + grad_weight^T
            C = alpha * op(A) * op(B) + beta * C
            op(A) = 't', op(B) = 'n', alpha=scale, beta=1
        ','line_number':428,'multiline':True]
['text':'transa=','line_number':458,'multiline':True]
['text':'transb=','line_number':459,'multiline':True]
['text':'     m=','line_number':460,'multiline':True]
['text':'     n=','line_number':461,'multiline':True]
['text':'     k=','line_number':462,'multiline':True]
['text':' alpha=','line_number':463,'multiline':True]
['text':'     A=','line_number':464,'multiline':True]
['text':'   lda=','line_number':465,'multiline':True]
['text':'     B=','line_number':466,'multiline':True]
['text':'   ldb=','line_number':467,'multiline':True]
['text':'  beta=','line_number':468,'multiline':True]
['text':'     C=','line_number':469,'multiline':True]
['text':'   ldc=','line_number':470,'multiline':True]
['text':'transa=','line_number':473,'multiline':True]
['text':'transb=','line_number':474,'multiline':True]
['text':'     m=','line_number':475,'multiline':True]
['text':'     n=','line_number':476,'multiline':True]
['text':'     k=','line_number':477,'multiline':True]
['text':' alpha=','line_number':478,'multiline':True]
['text':'     A=','line_number':479,'multiline':True]
['text':'   lda=','line_number':480,'multiline':True]
['text':'     B=','line_number':481,'multiline':True]
['text':'   ldb=','line_number':482,'multiline':True]
['text':'  beta=','line_number':483,'multiline':True]
['text':'     C=','line_number':484,'multiline':True]
['text':'   ldc=','line_number':485,'multiline':True]
['text':' Gradient of bias:','line_number':489,'multiline':False]
['text':'
          Compute:
            grad_bias = scale * grad_output_n * ones + grad_bias

          where

            grad_bias is viewed as grad_bias.view(nOutputPlane, 1)

            ones is viewed as ones.view(outputHeight * outputWidth, 1)

            grad_output_n is viewed as grad_output_n.view(nOutputPlane,
          outputHeight * outputWidth)

          gemm assumes column-major matrices:

            grad_bias^T = scale * grad_output_n * ones + grad_bias^T
            y = alpha * op(A) * x + beta * y
            op(A) = 't', alpha=scale, beta=1
         ','line_number':491,'multiline':True]
['text':' The following expression is equivalent to the above','line_number':510,'multiline':False]
['text':' gemm setup but avoids allocation of ones tensor:','line_number':511,'multiline':False]
['text':'
          TODO: when scale != 1 is introduced then use:
            grad_bias += scale * grad_output_n.sum(dims);
         ','line_number':513,'multiline':True]
['text':' slow_conv_dilated_all_cpu_template','line_number':521,'multiline':False]
['text':' namespace','line_number':523,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':532,'multiline':False]
['text':' calculate output tensor size','line_number':551,'multiline':False]
['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':554,'multiline':False]
['text':' insert batch dimension without affecting the original tensor.','line_number':555,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':587,'multiline':False]
['text':' calculate output tensor size','line_number':603,'multiline':False]
['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':606,'multiline':False]
['text':' insert batch dimension without affecting the original tensor.','line_number':607,'multiline':False]
['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':655,'multiline':False]
['text':' insert batch dimension without affecting the original tensor.','line_number':656,'multiline':False]
['text':' compute only gradients for which the corresponding output_mask is true:','line_number':663,'multiline':False]
['text':' template function assumes batched tensors.  unsqueeze(0) will','line_number':711,'multiline':False]
['text':' insert batch dimension without affecting the original tensor.','line_number':712,'multiline':False]
['text':' compute only gradients for which the corresponding output_mask is true:','line_number':719,'multiline':False]
['text':' namespace native','line_number':748,'multiline':False]
['text':' namespace at','line_number':749,'multiline':False]
