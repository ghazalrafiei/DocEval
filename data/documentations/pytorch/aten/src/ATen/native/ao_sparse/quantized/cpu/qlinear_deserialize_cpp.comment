['text':'*
 * Adapted from Fbgemm BCSRMatrix::unpack, but with non-zero zero points and
 * without tiling
 * https://github.com/pytorch/FBGEMM/blob/9d7c48a65419d0350f9e9e72f31e05bfe37e85a4/src/FbgemmSparseDense.cc#L154
 ','line_number':36,'multiline':True]
['text':' zero out destination','line_number':51,'multiline':False]
['text':' For the current tile, rowBPtr starts from currentTileIdx','line_number':64,'multiline':False]
['text':' Are we within bounds of destination matrix?','line_number':69,'multiline':False]
['text':' USE_FBGEMM','line_number':79,'multiline':False]
['text':' namespace','line_number':80,'multiline':False]
['text':' Unpack the untiled bcsr, then pack it in tiled form','line_number':96,'multiline':False]
['text':' The output channel axis is 0','line_number':111,'multiline':False]
['text':' Subtract 128 because we serialize as +128, which s best for','line_number':120,'multiline':False]
['text':' minimizing memory footprint for QNNPack','line_number':121,'multiline':False]
['text':' Unpack as non backend specific untiled BCSR then pack as Fbgemm tiled BCSR','line_number':135,'multiline':False]
['text':' because untiled Fbgemm BCSR currently doesn't exist','line_number':136,'multiline':False]
['text':' USE_FBGEMM','line_number':164,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)','line_number':195,'multiline':False]
['text':' Pad amount (8) comes from make_zero_points_and_scales_tensor','line_number':240,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/f8c1acea1e78573c04cd18893c4abff9eea64b03/aten/src/ATen/native/quantized/cpu/qnnpack_utils.h#L468','line_number':241,'multiline':False]
['text':' Pad with 1','line_number':250,'multiline':False]
['text':' Pad with 0;','line_number':253,'multiline':False]
['text':' USE_PYTORCH_QNNPACK','line_number':318,'multiline':False]
['text':' namespace sparse','line_number':320,'multiline':False]
['text':' namespace ao','line_number':321,'multiline':False]
