['text':' NOLINTNEXTLINE(bugprone-branch-clone)','line_number':181,'multiline':False]
['text':' These are normal binary ops that preserve dtype','line_number':320,'multiline':False]
['text':' namespace meta','line_number':374,'multiline':False]
['text':' redispatch!','line_number':873,'multiline':False]
['text':' redispatch!','line_number':878,'multiline':False]
['text':' redispatch!','line_number':883,'multiline':False]
['text':' WARNING: There doesn't appear to be any testing for this function','line_number':887,'multiline':False]
['text':' with sparse self input.','line_number':888,'multiline':False]
['text':' redispatch!','line_number':890,'multiline':False]
['text':' WARNING: This function, with a sparse self, is currently only','line_number':893,'multiline':False]
['text':' exercised by DistributedDataParallelTest.test_sparse_gradients','line_number':894,'multiline':False]
['text':' (you need to exercise it from C++, because this overload is never','line_number':895,'multiline':False]
['text':' used for Python)','line_number':896,'multiline':False]
['text':' redispatch!','line_number':898,'multiline':False]
['text':' redispatch!','line_number':902,'multiline':False]
['text':' redispatch!','line_number':906,'multiline':False]
['text':' divide, alias for div','line_number':909,'multiline':False]
['text':' true_divide, an alias for div','line_number':950,'multiline':False]
['text':' TODO: Make this structured to undo the perf regression from native:: removal','line_number':991,'multiline':False]
['text':' in call here','line_number':992,'multiline':False]
['text':' redispatch!','line_number':994,'multiline':False]
['text':' redispatch!','line_number':998,'multiline':False]
['text':' hack to use the TensorIterator to get the correct broadcasting and type promotion logic','line_number':1016,'multiline':False]
['text':' hack to use the TensorIterator to get the correct broadcasting and type promotion logic','line_number':1025,'multiline':False]
['text':' 0/0, return full NAN','line_number':1032,'multiline':False]
['text':' 0/x, return zero tensor','line_number':1036,'multiline':False]
['text':' x/0, return full INF','line_number':1042,'multiline':False]
['text':' x/y -- unreachable, see TORCH_INTERNAL_ASSERT above','line_number':1046,'multiline':False]
['text':' hack to use the TensorIterator to get the correct broadcasting and type promotion logic','line_number':1054,'multiline':False]
['text':' hack to use the TensorIterator to get the correct broadcasting and type','line_number':1090,'multiline':False]
['text':' promotion logic (see add_zerotensor)','line_number':1091,'multiline':False]
['text':' multiply, alias for mul','line_number':1104,'multiline':False]
['text':' redispatch!','line_number':1126,'multiline':False]
['text':' redispatch!','line_number':1130,'multiline':False]
['text':' subtract, alias for sub','line_number':1133,'multiline':False]
['text':' redispatch!','line_number':1155,'multiline':False]
['text':' TODO: Make this structured to undo the perf regression from native:: removal','line_number':1158,'multiline':False]
['text':' in call here','line_number':1159,'multiline':False]
['text':' redispatch','line_number':1170,'multiline':False]
['text':' redispatch','line_number':1175,'multiline':False]
['text':' redispatch','line_number':1180,'multiline':False]
['text':' Legacy and interfaces. They are aliased to bitwise_and* functions','line_number':1208,'multiline':False]
['text':' Legacy or interfaces. They are aliased to bitwise_or* functions','line_number':1241,'multiline':False]
['text':' Legacy xor interfaces. They are aliased to bitwise_xor* functions','line_number':1274,'multiline':False]
['text':' We need explicit cast to OutFunc because each *_out func is overloaded twice. Without An explicit cast, merely','line_number':1420,'multiline':False]
['text':' referring to *_out function is ambiguious.','line_number':1421,'multiline':False]
['text':' less, alias for torch.lt','line_number':1424,'multiline':False]
['text':' less_equal, alias for torch.le','line_number':1432,'multiline':False]
['text':' greater, alias for torch.gt','line_number':1440,'multiline':False]
['text':' greater_equal, alias for torch.ge','line_number':1448,'multiline':False]
['text':' not_equal, alias for torch.ne','line_number':1474,'multiline':False]
['text':' binary max, alias for maximum','line_number':1503,'multiline':False]
['text':' binary min, alias for minimum','line_number':1512,'multiline':False]
['text':' redispatch','line_number':1530,'multiline':False]
['text':' redispatch','line_number':1535,'multiline':False]
['text':' redispatch','line_number':1540,'multiline':False]
['text':' Note: this function is only for testing.','line_number':1544,'multiline':False]
['text':' It is undocumented and should not be used outside of tests.','line_number':1545,'multiline':False]
['text':' namespace native','line_number':1612,'multiline':False]
['text':' namespace at','line_number':1613,'multiline':False]
