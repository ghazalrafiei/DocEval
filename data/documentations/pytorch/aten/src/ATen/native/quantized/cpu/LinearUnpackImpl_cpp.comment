['text':' The output channel axis is 0','line_number':45,'multiline':False]
['text':' packB->printPackedMatrix("packedB inside fbgemm_unpack','line_number':52,'multiline':False]
['text':' (QLinearUnpackWeightInt8): ");','line_number':53,'multiline':False]
['text':' USE_FBGEMM','line_number':59,'multiline':False]
['text':' Unpacking requires reverting *make_zero_points_and_scales_tensor*','line_number':68,'multiline':False]
['text':' function in QnnpackUtils.h Please refer for a detail mechanism.','line_number':69,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/QnnpackUtils.h#L469','line_number':70,'multiline':False]
['text':' w_scales and w_zero_points are different from original scales & zero','line_number':71,'multiline':False]
['text':' points with padding & casting etc','line_number':72,'multiline':False]
['text':' The output channel axis is 0','line_number':97,'multiline':False]
['text':' See for the subtraction 128','line_number':106,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp#L319','line_number':107,'multiline':False]
['text':' USE_PYTORCH_QNNPACK','line_number':117,'multiline':False]
['text':' USE_FBGEMM','line_number':135,'multiline':False]
['text':' #if AT_MKLDNN_ENABLED()','line_number':142,'multiline':False]
