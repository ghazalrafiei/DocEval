['text':' for _empty_affine_q...','line_number':20,'multiline':False]
['text':' for empty_affine_qu...','line_number':21,'multiline':False]
['text':' for empty','line_number':22,'multiline':False]
['text':' for quantize_per_ch...','line_number':23,'multiline':False]
['text':' for quantize_per_te...','line_number':24,'multiline':False]
['text':' uint8 * int8 -> uint8 (no quantization/dequantization)','line_number':42,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have','line_number':44,'multiline':False]
['text':' the same numerics across different machines. Therefore, we do not provide','line_number':45,'multiline':False]
['text':' a fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':46,'multiline':False]
['text':' TODO: contiguous is called for further jit optimizations.','line_number':55,'multiline':False]
['text':' C(output) = A(input) x B(weight), where C, A, B are M x N, M x K, K x N','line_number':63,'multiline':False]
['text':' matrices, respectively.','line_number':64,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':65,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':77,'multiline':False]
['text':' Process the per tensor quantization.','line_number':87,'multiline':False]
['text':' Process the per channel quantization.','line_number':92,'multiline':False]
['text':' The resulting matrix here is 2-D, let's view it with the original','line_number':114,'multiline':False]
['text':' left hand dimensions of the input. Here are two examples:','line_number':115,'multiline':False]
['text':' 1. If the input tensor is {M, K}, the output tensor is {M, N}.','line_number':116,'multiline':False]
['text':' 2. If the input tensor is {b, M, K}, the output tensor is {b, M, N}.','line_number':117,'multiline':False]
['text':' Resize output Tensor','line_number':120,'multiline':False]
['text':' Allocate a buffer for fbgemmPacked to use','line_number':123,'multiline':False]
['text':' This operation does the following:','line_number':129,'multiline':False]
['text':' 1) Creates a "row buffer" vector with offset values that must be','line_number':130,'multiline':False]
['text':'    added to the integer matrix multiplication operation to ensure','line_number':131,'multiline':False]
['text':'    correctness. This "row buffer" is also called the row offset, and','line_number':132,'multiline':False]
['text':'    it is needed when we use affine quantization for weights.','line_number':133,'multiline':False]
['text':' 2) Packs the resulting quantized matrix into vector-register and','line_number':134,'multiline':False]
['text':'    cache friendly tiles.','line_number':135,'multiline':False]
['text':'','line_number':136,'multiline':False]
['text':'  Note this is not executed eagerly, but rather within the','line_number':137,'multiline':False]
['text':'  fbgemmPacked call below.','line_number':138,'multiline':False]
['text':'trans=','line_number':140,'multiline':True]
['text':'nRow=','line_number':141,'multiline':True]
['text':'nCol=','line_number':142,'multiline':True]
['text':'smat=','line_number':143,'multiline':True]
['text':'ld=','line_number':144,'multiline':True]
['text':'pmat=','line_number':145,'multiline':True]
['text':' Currently, packA manages ownership of `pmat`.','line_number':145,'multiline':False]
['text':' TODO: Consider a way to pre-allocate and reuse','line_number':146,'multiline':False]
['text':' pmat buffer.','line_number':147,'multiline':False]
['text':' ReQuantizeOutput requires pointers to the zero point values,','line_number':149,'multiline':False]
['text':' since in the case of rowwise quantization these will be arrays rather','line_number':150,'multiline':False]
['text':' than scalars. But in this case, we're doing whole-tensor quantization','line_number':151,'multiline':False]
['text':' so we just pass a pointer to the scale values (and internally','line_number':152,'multiline':False]
['text':' ReQuantizeOutput won't index past 0.','line_number':153,'multiline':False]
['text':' This is the end of the pipeline, pass the resulting matrix through.','line_number':155,'multiline':False]
['text':' Process the per tensor quantization.','line_number':159,'multiline':False]
['text':'','line_number':160,'multiline':False]
['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':161,'multiline':False]
['text':' operation does:','line_number':162,'multiline':False]
['text':'  1) Add in row and column offsets to the rows and columns,','line_number':163,'multiline':False]
['text':'  respectively.','line_number':164,'multiline':False]
['text':'  2) Add in the bias term.','line_number':165,'multiline':False]
['text':' nCol ','line_number':179,'multiline':True]
['text':' groups ','line_number':180,'multiline':True]
['text':' Do the GEMM','line_number':183,'multiline':False]
['text':'packA=','line_number':185,'multiline':True]
['text':'packB=','line_number':186,'multiline':True]
['text':'C=','line_number':187,'multiline':True]
['text':'C_buffer=','line_number':188,'multiline':True]
['text':'ldc=','line_number':189,'multiline':True]
['text':'outProcess=','line_number':190,'multiline':True]
['text':'thread_id=','line_number':191,'multiline':True]
['text':'num_threads=','line_number':192,'multiline':True]
['text':' Process the per channel quantization.','line_number':194,'multiline':False]
['text':'','line_number':195,'multiline':False]
['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':196,'multiline':False]
['text':' operation does:','line_number':197,'multiline':False]
['text':'  1) Add in row and column offsets to the rows and columns,','line_number':198,'multiline':False]
['text':'  respectively.','line_number':199,'multiline':False]
['text':'  2) Add in the bias term.','line_number':200,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-argument-comment)','line_number':214,'multiline':False]
['text':'nCol=','line_number':215,'multiline':True]
['text':' groups','line_number':216,'multiline':True]
['text':' Do the GEMM','line_number':219,'multiline':False]
['text':'packA=','line_number':221,'multiline':True]
['text':'packB=','line_number':222,'multiline':True]
['text':'C=','line_number':223,'multiline':True]
['text':'C_buffer=','line_number':224,'multiline':True]
['text':'ldc=','line_number':225,'multiline':True]
['text':'outProcess=','line_number':226,'multiline':True]
['text':'thread_id=','line_number':227,'multiline':True]
['text':'num_threads=','line_number':228,'multiline':True]
['text':' Allocate output Tensor','line_number':240,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':333,'multiline':False]
['text':' Allocate output Tensor and a buffer for fbgemmPacked to use','line_number':354,'multiline':False]
['text':'trans=','line_number':364,'multiline':True]
['text':'nRow=','line_number':365,'multiline':True]
['text':'nCol=','line_number':366,'multiline':True]
['text':'smat=','line_number':367,'multiline':True]
['text':'ld=','line_number':368,'multiline':True]
['text':'pmat=','line_number':369,'multiline':True]
['text':'scale=','line_number':370,'multiline':True]
['text':'zero_pt=','line_number':371,'multiline':True]
['text':' Process the per tensor quantization.','line_number':376,'multiline':False]
['text':'','line_number':377,'multiline':False]
['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':378,'multiline':False]
['text':' operation does:','line_number':379,'multiline':False]
['text':'  1) Add in row and column offsets to the rows and columns,','line_number':380,'multiline':False]
['text':'  respectively.','line_number':381,'multiline':False]
['text':'  2) Add in the bias term.','line_number':382,'multiline':False]
['text':' nCol ','line_number':393,'multiline':True]
['text':' Do the GEMM','line_number':395,'multiline':False]
['text':'packA=','line_number':397,'multiline':True]
['text':'packB=','line_number':398,'multiline':True]
['text':'C=','line_number':399,'multiline':True]
['text':'C_buffer=','line_number':400,'multiline':True]
['text':'ldc=','line_number':401,'multiline':True]
['text':'outProcess=','line_number':402,'multiline':True]
['text':'thread_id=','line_number':403,'multiline':True]
['text':'num_threads=','line_number':404,'multiline':True]
['text':' Process the per channel quantization.','line_number':406,'multiline':False]
['text':'','line_number':407,'multiline':False]
['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':408,'multiline':False]
['text':' operation does:','line_number':409,'multiline':False]
['text':'  1) Add in row and column offsets to the rows and columns,','line_number':410,'multiline':False]
['text':'  respectively.','line_number':411,'multiline':False]
['text':'  2) Add in the bias term.','line_number':412,'multiline':False]
['text':' nCol ','line_number':425,'multiline':True]
['text':' Do the GEMM','line_number':427,'multiline':False]
['text':'packA=','line_number':429,'multiline':True]
['text':'packB=','line_number':430,'multiline':True]
['text':'C=','line_number':431,'multiline':True]
['text':'C_buffer=','line_number':432,'multiline':True]
['text':'ldc=','line_number':433,'multiline':True]
['text':'outProcess=','line_number':434,'multiline':True]
['text':'thread_id=','line_number':435,'multiline':True]
['text':'num_threads=','line_number':436,'multiline':True]
['text':' USE_FBGEMM','line_number':443,'multiline':False]
['text':' TODO: add per_channel support in the future when xnnp supports it','line_number':448,'multiline':False]
['text':' Create an operator iff not already created','line_number':475,'multiline':False]
['text':' Update the input scale so we may cache the op','line_number':479,'multiline':False]
['text':' prepare weights','line_number':486,'multiline':False]
['text':' copy from the original weight and take care of dtype change if necessary','line_number':497,'multiline':False]
['text':' Original bias was float, so we requantize it here.','line_number':501,'multiline':False]
['text':' output limits','line_number':504,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':506,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':510,'multiline':False]
['text':' Create an operator','line_number':514,'multiline':False]
['text':' input_channels ','line_number':516,'multiline':True]
['text':' output_channels ','line_number':517,'multiline':True]
['text':' input_stride ','line_number':518,'multiline':True]
['text':' output_stride ','line_number':519,'multiline':True]
['text':' flags ','line_number':531,'multiline':True]
['text':'
   * Allocate output Tensor and a buffer for XNNPACK to use
   * The resulting matrix here is 2-D, let's view it with the original
   * left hand dimensions of the input. Here are two examples:
   * 1. If the input tensor is {M, K}, the output tensor is {M, N}.
   * 2. If the input tensor is {b, M, K}, the output tensor is {b, M, N}.
   ','line_number':543,'multiline':True]
['text':' layout ','line_number':555,'multiline':True]
['text':' pin_memory ','line_number':557,'multiline':True]
['text':' calculate batch_size','line_number':562,'multiline':False]
['text':' Setup the operator','line_number':568,'multiline':False]
['text':' batch_size ','line_number':571,'multiline':True]
['text':' Run the opeator','line_number':584,'multiline':False]
['text':' Linear op','line_number':586,'multiline':False]
['text':' threadpool','line_number':587,'multiline':False]
['text':' USE_XNNPACK','line_number':598,'multiline':False]
['text':' Weight packing is not thread safe','line_number':616,'multiline':False]
['text':' Get the original weight and adjust it to uint8 from int8','line_number':625,'multiline':False]
['text':' We calculate requant scale here as the vector holding the requant scale','line_number':631,'multiline':False]
['text':' is owned by this module. The pointer is then passed to qnnpack backend.','line_number':632,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':634,'multiline':False]
['text':' Original bias was float, so we requantize it here.','line_number':647,'multiline':False]
['text':' Update the input scale to not pack again.','line_number':651,'multiline':False]
['text':' input_channels ','line_number':655,'multiline':True]
['text':' output_channels ','line_number':656,'multiline':True]
['text':' On mobile, we release the original weight by resetting the intrusive_ptr.','line_number':663,'multiline':False]
['text':' Calling unpack after this will throw an assertion.','line_number':664,'multiline':False]
['text':' Allocate output Tensor and a buffer for QNNPACK to use','line_number':683,'multiline':False]
['text':' The resulting matrix here is 2-D, let's view it with the original','line_number':684,'multiline':False]
['text':' left hand dimensions of the input. Here are two examples:','line_number':685,'multiline':False]
['text':' 1. If the input tensor is {M, K}, the output tensor is {M, N}.','line_number':686,'multiline':False]
['text':' 2. If the input tensor is {b, M, K}, the output tensor is {b, M, N}.','line_number':687,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':697,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':702,'multiline':False]
['text':' batch_size ','line_number':708,'multiline':True]
['text':' input_channels ','line_number':709,'multiline':True]
['text':' output_channels ','line_number':710,'multiline':True]
['text':' input_stride ','line_number':718,'multiline':True]
['text':' output_stride ','line_number':721,'multiline':True]
['text':' TODO (Ashkan): Disabling temporarily.','line_number':722,'multiline':False]
['text':' Throws a floating point exception with OSS pthreadpool.','line_number':723,'multiline':False]
['text':' threadpool ','line_number':724,'multiline':True]
['text':' xnnp does not currently support
                                        per-channel fully connected op ','line_number':740,'multiline':True]
['text':' don't want this to fall through to QNNPACK ','line_number':743,'multiline':True]
['text':' USE_XNNPACK','line_number':750,'multiline':False]
['text':' fall through for unsupported types, configs, or shapes ','line_number':760,'multiline':True]
['text':' USE_XNNPACK','line_number':761,'multiline':False]
['text':' fall through for unsupported types, configs, or shapes ','line_number':773,'multiline':True]
['text':' USE_XNNPACK','line_number':774,'multiline':False]
['text':' USE_PYTORCH_QNNPACK','line_number':778,'multiline':False]
['text':'scale=','line_number':804,'multiline':True]
['text':'alpha=','line_number':804,'multiline':True]
['text':' Scales of ONEDNN and PyTorch are reciprocal','line_number':814,'multiline':False]
['text':' Compute: Use ideep::matmul_forward to support asymmetric quantization','line_number':818,'multiline':False]
['text':' Allocate output Tensor','line_number':819,'multiline':False]
['text':' Bias might be modified outside (e.g. by quantization bias correction).','line_number':833,'multiline':False]
['text':' If so, update the prepacked bias as well.','line_number':834,'multiline':False]
['text':' Primitive cache is initialized when called for the first time','line_number':840,'multiline':False]
['text':' and won't be updated afterwards.','line_number':841,'multiline':False]
['text':'accum scale','line_number':844,'multiline':True]
['text':'accum zero point','line_number':844,'multiline':True]
['text':'is_dynamic=','line_number':847,'multiline':True]
['text':' int8 CPU Tensor, not QTensor','line_number':905,'multiline':False]
['text':' int8 tensor from MkldnnCPU','line_number':908,'multiline':False]
['text':' plain tensor','line_number':911,'multiline':False]
['text':' e.g. "none", "relu"','line_number':915,'multiline':False]
['text':' If the input has more than two dimensions, we will reshape it to a 2-dimensional form','line_number':934,'multiline':False]
['text':' for calculation and subsequently reshape the output back.','line_number':935,'multiline':False]
['text':' Create onednn primitive','line_number':970,'multiline':False]
['text':' Reorder weight if needed','line_number':999,'multiline':False]
['text':' Prepare args and execute primitive','line_number':1002,'multiline':False]
['text':' #if AT_MKLDNN_ENABLED()','line_number':1033,'multiline':False]
['text':' int8 CPU tensor, not QTensor','line_number':1122,'multiline':False]
['text':' int8 tensor from MkldnnCPU','line_number':1125,'multiline':False]
['text':' namespace','line_number':1171,'multiline':False]
['text':' namespace native','line_number':1172,'multiline':False]
['text':' namespace at','line_number':1173,'multiline':False]
