['text':' Note: out is assumed to be the same size as self and other.','line_number':46,'multiline':False]
['text':' Note: Multiplication is only supported when self, other, out are of the same','line_number':47,'multiline':False]
['text':'       dtype.','line_number':48,'multiline':False]
['text':' using qa memory format for qb to allow xnnpack kernel to flatten all the','line_number':73,'multiline':False]
['text':' dims','line_number':74,'multiline':False]
['text':' layout ','line_number':82,'multiline':True]
['text':' pin_memory ','line_number':84,'multiline':True]
['text':'
     * FIXME: use acticationLimits<T>()
     * With <T>, MSVC runs into "error C3862: indetifier activationLimits not
     * found".
     ','line_number':102,'multiline':True]
['text':' create xnnpack multiply operator ...','line_number':117,'multiline':False]
['text':' set up operator','line_number':142,'multiline':False]
['text':' Run the operator','line_number':161,'multiline':False]
['text':' xnn_operator_t op ','line_number':163,'multiline':True]
['text':' pthreadpool_t threadpool ','line_number':164,'multiline':True]
['text':' use XNNPACK','line_number':175,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':183,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':185,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':189,'multiline':False]
['text':' Strided "memset"','line_number':208,'multiline':False]
['text':' Set all values to 0','line_number':209,'multiline':False]
['text':' other_val < 0.0 ','line_number':219,'multiline':True]
['text':' xq' = q_max + q_min - x_q','line_number':223,'multiline':False]
['text':' USE_XNNPACK','line_number':252,'multiline':False]
['text':' `torch.jit.trace` will trace Scalar as Tensor','line_number':308,'multiline':False]
['text':' This can be removed after broadcast is supported and','line_number':309,'multiline':False]
['text':' all variations of `quantized::mul` is merged into `quantized::mul`','line_number':310,'multiline':False]
['text':' `torch.jit.trace` will trace Scalar as Tensor','line_number':323,'multiline':False]
['text':' This can be removed after broadcast is supported and','line_number':324,'multiline':False]
['text':' all variations of `quantized::mul` is merged into `quantized::mul`','line_number':325,'multiline':False]
['text':'ReLUFused=','line_number':336,'multiline':True]
['text':'ReLUFused=','line_number':337,'multiline':True]
['text':'ReLUFused=','line_number':338,'multiline':True]
['text':'ReLUFused=','line_number':339,'multiline':True]
['text':'ReLUFused=','line_number':340,'multiline':True]
['text':'ReLUFused=','line_number':341,'multiline':True]
['text':'ReLUFused=','line_number':342,'multiline':True]
['text':'ReLUFused=','line_number':343,'multiline':True]
['text':'ReLUFused=','line_number':344,'multiline':True]
['text':'ReLUFused=','line_number':345,'multiline':True]
['text':' deprecated functions, kept for backward compatibility','line_number':346,'multiline':False]
['text':'ReLUFused=','line_number':347,'multiline':True]
['text':'ReLUFused=','line_number':348,'multiline':True]
['text':'ReLUFused=','line_number':349,'multiline':True]
['text':'ReLUFused=','line_number':350,'multiline':True]
['text':'ReLUFused=','line_number':351,'multiline':True]
['text':'ReLUFused=','line_number':352,'multiline':True]
['text':' TODO: remove after broadcasting is supported','line_number':353,'multiline':False]
['text':'ReLUFused=','line_number':354,'multiline':True]
['text':'ReLUFused=','line_number':355,'multiline':True]
['text':'ReLUFused=','line_number':356,'multiline':True]
['text':'ReLUFused=','line_number':357,'multiline':True]
['text':' namespace','line_number':360,'multiline':False]
['text':' namespace at::native','line_number':361,'multiline':False]
