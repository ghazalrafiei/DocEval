['text':' for dequantize','line_number':19,'multiline':False]
['text':'m=','line_number':34,'multiline':True]
['text':'min=','line_number':35,'multiline':True]
['text':'max=','line_number':36,'multiline':True]
['text':'len=','line_number':37,'multiline':True]
['text':' Input tensor is quantized as 8-bit unsigned values','line_number':39,'multiline':False]
['text':' Calculate scale and zero point for quantization of input tensor','line_number':43,'multiline':False]
['text':'min=','line_number':45,'multiline':True]
['text':'max=','line_number':46,'multiline':True]
['text':'qmin=','line_number':47,'multiline':True]
['text':'qmax=','line_number':48,'multiline':True]
['text':'preserve_sparsity=','line_number':50,'multiline':True]
['text':'force_scale_power_of_two=','line_number':51,'multiline':True]
['text':'reduce_range=','line_number':52,'multiline':True]
['text':' Quantize input','line_number':54,'multiline':False]
['text':' TODO: optimized kernel that outputs fp32 so','line_number':61,'multiline':False]
['text':' this step isn't necessary','line_number':62,'multiline':False]
['text':' USE_FBGEMM','line_number':73,'multiline':False]
['text':' On empty input, no output data will be generated,','line_number':85,'multiline':False]
['text':' so use arbitrary qparams.','line_number':86,'multiline':False]
['text':' Otherwise...','line_number':89,'multiline':False]
['text':' Input tensor is quantized as 8-bit unsigned values','line_number':95,'multiline':False]
['text':' Calculate scale and zero point for quantization of input tensor','line_number':99,'multiline':False]
['text':'min=','line_number':101,'multiline':True]
['text':'max=','line_number':102,'multiline':True]
['text':'qmin=','line_number':103,'multiline':True]
['text':'qmax=','line_number':104,'multiline':True]
['text':'preserve_sparsity=','line_number':106,'multiline':True]
['text':'force_scale_power_of_two=','line_number':107,'multiline':True]
['text':'reduce_range=','line_number':108,'multiline':True]
['text':' note: this is set to false rather than','line_number':108,'multiline':False]
['text':' reduce_range for qnnpack','line_number':109,'multiline':False]
['text':' Quantize input','line_number':111,'multiline':False]
['text':' TODO: optimized kernel that outputs fp32 so','line_number':118,'multiline':False]
['text':' this step isn't necessary','line_number':119,'multiline':False]
['text':' USE_PYTORCH_QNNPACK','line_number':130,'multiline':False]
['text':' Find min/max of input','line_number':139,'multiline':False]
['text':' Input tensor is quantized as 8-bit unsigned values','line_number':146,'multiline':False]
['text':' Calculate scale and zero point for quantization of input tensor','line_number':150,'multiline':False]
['text':'min=','line_number':152,'multiline':True]
['text':'max=','line_number':153,'multiline':True]
['text':'qmin=','line_number':154,'multiline':True]
['text':'qmax=','line_number':155,'multiline':True]
['text':'preserve_sparsity=','line_number':157,'multiline':True]
['text':'force_scale_power_of_two=','line_number':158,'multiline':True]
['text':'reduce_range=','line_number':159,'multiline':True]
['text':' Quantize input','line_number':161,'multiline':False]
['text':'accum','line_number':166,'multiline':True]
['text':' TODO: Modify ideep to allow fp32 input & output','line_number':168,'multiline':False]
['text':' to avoid explicit `quantize - dequantize`','line_number':169,'multiline':False]
['text':' AT_MKLDNN_ENABLED()','line_number':181,'multiline':False]
['text':' note: this works for both Conv and ConvT due to transpose()','line_number':187,'multiline':False]
['text':' note: this works for both Conv and ConvT due to transpose()','line_number':200,'multiline':False]
['text':' N, C, L -> N, C, 1, L','line_number':208,'multiline':False]
['text':' N, C, 1, L -> N, C, L','line_number':211,'multiline':False]
['text':' transpose','line_number':227,'multiline':False]
['text':' namespace','line_number':239,'multiline':False]
['text':' namespace native','line_number':240,'multiline':False]
['text':' namespace at','line_number':241,'multiline':False]
