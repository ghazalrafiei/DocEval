['text':' input_scale','line_number':13,'multiline':False]
['text':' input_zero_point','line_number':14,'multiline':False]
['text':' input_shape','line_number':15,'multiline':False]
['text':' output_scale','line_number':16,'multiline':False]
['text':' output_zero_point','line_number':17,'multiline':False]
['text':' OMP_number_of_threads','line_number':18,'multiline':False]
['text':' accum_scale','line_number':19,'multiline':False]
['text':' accum_zero_point','line_number':20,'multiline':False]
['text':' Base class of primitive cache','line_number':31,'multiline':False]
['text':' For dynamic qlinear, scale and zero point','line_number':60,'multiline':False]
['text':' are set at execution time. So we only need to compare','line_number':61,'multiline':False]
['text':' the rest part of key.','line_number':62,'multiline':False]
['text':'alpha=','line_number':325,'multiline':True]
['text':' Try to reorder tensor to expected desc at runtime','line_number':332,'multiline':False]
['text':' Do it in a `try...catch...` manner to avoid oneDNN's errors','line_number':333,'multiline':False]
['text':' TODO: Move it to third_party/ideep','line_number':334,'multiline':False]
['text':' ONEDNN requires symmetric quantization of weight','line_number':350,'multiline':False]
['text':' Use this util function to check.','line_number':351,'multiline':False]
['text':' This case is currently not supported in PyTorch','line_number':361,'multiline':False]
['text':' but we do not want to raise an error in this util function.','line_number':362,'multiline':False]
['text':' This case is currently not supported in PyTorch','line_number':372,'multiline':False]
['text':' but we do not want to raise an error in this util function.','line_number':373,'multiline':False]
['text':' When qengine is x86, use this util func to check if onednn kernel','line_number':379,'multiline':False]
['text':' is preferred than fbgemm's to get better performance.','line_number':380,'multiline':False]
['text':' Performance of onednn is only validated on Linux right now.','line_number':386,'multiline':False]
['text':' Also, the heuristics for dispatching are based on perf data on Linux.','line_number':387,'multiline':False]
['text':' So, for x86 qengine, we always use fbgemm kernels if OS is not Linux.','line_number':388,'multiline':False]
['text':' TODO Support more OSs.','line_number':389,'multiline':False]
['text':' onednn_utils','line_number':402,'multiline':False]
['text':' from CPU backend instead of QuantizedCPU','line_number':405,'multiline':False]
['text':' Weight zero points must be 0 for onednn','line_number':406,'multiline':False]
['text':' contains quantized values but not QTensor','line_number':416,'multiline':False]
['text':' MKLDNN tensor with quantized values','line_number':419,'multiline':False]
['text':' Bias is packed if not None','line_number':422,'multiline':False]
['text':' accum to fused with conv add','line_number':430,'multiline':False]
['text':' #if AT_MKLDNN_ENABLED()','line_number':440,'multiline':False]
