['text':' Note: out is assumed to be the same size as self and other.','line_number':49,'multiline':False]
['text':' Note: Addition is only supported when self, other, out are of the same dtype.','line_number':50,'multiline':False]
['text':' To implement tensor-scalar addition in quantized space, we simply','line_number':66,'multiline':False]
['text':' adjust the quantization parameters based on the following rules:','line_number':67,'multiline':False]
['text':'','line_number':68,'multiline':False]
['text':' Let s = scale, z = zero point, c = other.toFloat(), c_q = round(c/s)','line_number':69,'multiline':False]
['text':' q_min = lowest representable value of scalar type','line_number':70,'multiline':False]
['text':' q_max = highest representable value of scalar type','line_number':71,'multiline':False]
['text':'','line_number':72,'multiline':False]
['text':' Let s' = the calculated scale or the output','line_number':73,'multiline':False]
['text':' z' = the calculated zero-point for the output','line_number':74,'multiline':False]
['text':'','line_number':75,'multiline':False]
['text':' If q_min > z - c_q','line_number':76,'multiline':False]
['text':'   s' = [(q_max - (z - c_q)]/[q_max - q_min] * s','line_number':77,'multiline':False]
['text':'   z' = q_min','line_number':78,'multiline':False]
['text':'   Xq' = at::requantize_from_int(Xq - z + c_q, s/s', z')','line_number':79,'multiline':False]
['text':' If q_max < z - c_q','line_number':80,'multiline':False]
['text':'   s' = [z - c_q -q_min]/[q_max - q_min] * s','line_number':81,'multiline':False]
['text':'   z' = q_max','line_number':82,'multiline':False]
['text':'   Xq' = at::requantize_from_int(Xq - z + c_q, s/s', z')','line_number':83,'multiline':False]
['text':' Else','line_number':84,'multiline':False]
['text':'   s' = s','line_number':85,'multiline':False]
['text':'   z' = z - c_q','line_number':86,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-signed-char-misuse)','line_number':92,'multiline':False]
['text':' Reason for use qa's memory format for qb is that for the underlying','line_number':148,'multiline':False]
['text':' kernel can flatten all the dims and iterate over both the tensors.','line_number':149,'multiline':False]
['text':' In most cases, both qa and qb are in same memory format.','line_number':150,'multiline':False]
['text':' When they are not there is a copy overhead to make it contiguous','line_number':151,'multiline':False]
['text':' in qa's memory format.','line_number':152,'multiline':False]
['text':' layout ','line_number':163,'multiline':True]
['text':' pin_memory ','line_number':165,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':180,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':185,'multiline':False]
['text':' input size ','line_number':190,'multiline':True]
['text':' a zero_point ','line_number':191,'multiline':True]
['text':' a scale ','line_number':192,'multiline':True]
['text':' b zero_point ','line_number':193,'multiline':True]
['text':' b scale ','line_number':194,'multiline':True]
['text':' sum zero_point ','line_number':195,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)','line_number':196,'multiline':False]
['text':' sum scale ','line_number':197,'multiline':True]
['text':' output min ','line_number':198,'multiline':True]
['text':' output max ','line_number':199,'multiline':True]
['text':' flags ','line_number':200,'multiline':True]
['text':' add op ','line_number':211,'multiline':True]
['text':' batch size ','line_number':212,'multiline':True]
['text':' a data ','line_number':213,'multiline':True]
['text':' A stride ','line_number':214,'multiline':True]
['text':' b data ','line_number':215,'multiline':True]
['text':' B stride ','line_number':216,'multiline':True]
['text':' output data ','line_number':217,'multiline':True]
['text':' sum stride ','line_number':218,'multiline':True]
['text':' USE_PYTORCH_QNNPACK','line_number':233,'multiline':False]
['text':' int8_t input1_zero_point   ','line_number':249,'multiline':True]
['text':' float input1_scale         ','line_number':250,'multiline':True]
['text':' int8_t input2_zero_point   ','line_number':251,'multiline':True]
['text':' float input2_scale         ','line_number':252,'multiline':True]
['text':' int8_t output_zero_point   ','line_number':253,'multiline':True]
['text':' float output_scale         ','line_number':254,'multiline':True]
['text':' int8_t output_min          ','line_number':255,'multiline':True]
['text':' int8_t output_max          ','line_number':256,'multiline':True]
['text':' uint32_t flags             ','line_number':257,'multiline':True]
['text':' xnn_operator_t* add_op_out ','line_number':258,'multiline':True]
['text':' xnn_operator_t add_op      ','line_number':271,'multiline':True]
['text':' size_t num_input1_dims     ','line_number':272,'multiline':True]
['text':' const size_t* input1_shape ','line_number':273,'multiline':True]
['text':' size_t num_input2_dims     ','line_number':274,'multiline':True]
['text':' const size_t* input2_shape ','line_number':275,'multiline':True]
['text':' const int8_t* input1       ','line_number':276,'multiline':True]
['text':' const int8_t* input2       ','line_number':277,'multiline':True]
['text':' int8_t* output             ','line_number':278,'multiline':True]
['text':' pthreadpool_t threadpool   ','line_number':279,'multiline':True]
['text':' using qa memory format for qb to allow xnnpack kernel to flatten all the','line_number':289,'multiline':False]
['text':' dims','line_number':290,'multiline':False]
['text':' layout ','line_number':303,'multiline':True]
['text':' pin_memory ','line_number':305,'multiline':True]
['text':'
     * FIXME: use acticationLimits<T>()
     * With <T>, MSVC runs into "error C3862: indetifier activationLimits not found".
     ','line_number':320,'multiline':True]
['text':' Create an operator','line_number':331,'multiline':False]
['text':' Setup the operator','line_number':351,'multiline':False]
['text':' Run the operator','line_number':364,'multiline':False]
['text':' xnn_operator_t op ','line_number':366,'multiline':True]
['text':' pthreadpool_t threadpool ','line_number':367,'multiline':True]
['text':' USE_XNNPACK','line_number':373,'multiline':False]
['text':' USE_XNNPACK','line_number':388,'multiline':False]
['text':' qnnpack does not support boradcasting ','line_number':391,'multiline':True]
['text':' USE_PYTORCH_QNNPACK','line_number':395,'multiline':False]
['text':' `torch.jit.trace` will trace Scalar as Tensor','line_number':440,'multiline':False]
['text':' This can be removed after broadcast is supported and','line_number':441,'multiline':False]
['text':' all variations of `quantized::add` is merged into `quantized::add`','line_number':442,'multiline':False]
['text':' `torch.jit.trace` will trace Scalar as Tensor','line_number':448,'multiline':False]
['text':' This can be removed after broadcast is supported and','line_number':449,'multiline':False]
['text':' all variations of `quantized::add` is merged into `quantized::add`','line_number':450,'multiline':False]
['text':'ReLUFused=','line_number':457,'multiline':True]
['text':'ReLUFused=','line_number':458,'multiline':True]
['text':'ReLUFused=','line_number':459,'multiline':True]
['text':'ReLUFused=','line_number':460,'multiline':True]
['text':'ReLUFused=','line_number':461,'multiline':True]
['text':'ReLUFused=','line_number':462,'multiline':True]
['text':'ReLUFused=','line_number':463,'multiline':True]
['text':'ReLUFused=','line_number':464,'multiline':True]
['text':'ReLUFused=','line_number':465,'multiline':True]
['text':'ReLUFused=','line_number':466,'multiline':True]
['text':' deprecated functions, kept for backward compatibility','line_number':467,'multiline':False]
['text':'ReLUFused=','line_number':468,'multiline':True]
['text':'ReLUFused=','line_number':469,'multiline':True]
['text':'ReLUFused=','line_number':470,'multiline':True]
['text':'ReLUFused=','line_number':471,'multiline':True]
['text':'ReLUFused=','line_number':472,'multiline':True]
['text':'ReLUFused=','line_number':473,'multiline':True]
['text':'ReLUFused=','line_number':474,'multiline':True]
['text':'ReLUFused=','line_number':475,'multiline':True]
['text':'ReLUFused=','line_number':476,'multiline':True]
['text':'ReLUFused=','line_number':477,'multiline':True]
['text':'ReLUFused=','line_number':481,'multiline':True]
['text':' namespace','line_number':484,'multiline':False]
['text':' namespace at::native','line_number':490,'multiline':False]
