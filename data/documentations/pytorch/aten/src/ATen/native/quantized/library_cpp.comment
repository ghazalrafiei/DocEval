['text':' deprecated functions, kept for backward compatibility','line_number':28,'multiline':False]
['text':' TODO: remove after broadcasting is supported','line_number':35,'multiline':False]
['text':' This is needed for graph mode quantization, when we fuse','line_number':40,'multiline':False]
['text':' dequant - aten::batch_norm - quant into quantized::batch_norm','line_number':41,'multiline':False]
['text':' and dimension is unknown given only the aten op call','line_number':42,'multiline':False]
['text':' quantized::batch_norm supports both 2d and 3d batch norm right now','line_number':43,'multiline':False]
['text':' it should also support 1d batch_norm after quantized::batch_norm1d is','line_number':44,'multiline':False]
['text':' implemented','line_number':45,'multiline':False]
['text':' conv_prepack is deprecated, please use conv2d_prepack for 2D conv.','line_number':76,'multiline':False]
['text':' conv_unpack is deprecated, please use conv2d_unpack for 2D conv.','line_number':81,'multiline':False]
['text':' conv_tranpsose','line_number':99,'multiline':False]
['text':' Corresponding pattern (the ops with `*` are part of the pattern that','line_number':155,'multiline':False]
['text':' represents the computation of quantized::linear_with_input_q_dq_qweight_dq_output_fp32):','line_number':156,'multiline':False]
['text':' input -> q* -> dq* -> linear* ->','line_number':157,'multiline':False]
['text':'         qweight -> dq* /','line_number':158,'multiline':False]
['text':'','line_number':159,'multiline':False]
['text':' After fusion:','line_number':160,'multiline':False]
['text':' input -> quantized::linear_with_input_q_dq_qweight_dq_output_fp32* ->','line_number':161,'multiline':False]
['text':'         qweight /','line_number':162,'multiline':False]
['text':'','line_number':163,'multiline':False]
['text':' Additional Note: the weight is packed as well','line_number':164,'multiline':False]
['text':' Params:','line_number':165,'multiline':False]
['text':'    X: float32 Tensor, will be quantized to quint8 in the op','line_number':166,'multiline':False]
['text':'    W_prepack: packed qint8 quantized weight and bias','line_number':167,'multiline':False]
['text':' Returns:','line_number':168,'multiline':False]
['text':'    Y: float32 Tensor','line_number':169,'multiline':False]
['text':' Corresponding pattern (the ops with `*` are part of the pattern that','line_number':171,'multiline':False]
['text':' represents the computation of quantized::linear_with_input_q_dq_qweight_dq_relu_output_fp32):','line_number':172,'multiline':False]
['text':' input -> q* -> dq* -> linear* -> relu* ->','line_number':173,'multiline':False]
['text':'         qweight -> dq* /','line_number':174,'multiline':False]
['text':'','line_number':175,'multiline':False]
['text':' After fusion:','line_number':176,'multiline':False]
['text':' input -> quantized::linear_with_input_q_dq_qweight_dq_relu_output_fp32* ->','line_number':177,'multiline':False]
['text':'         qweight /','line_number':178,'multiline':False]
['text':'','line_number':179,'multiline':False]
['text':' Additional Note: the weight is packed as well','line_number':180,'multiline':False]
['text':' Params:','line_number':181,'multiline':False]
['text':'    X: float32 Tensor, will be quantized to quint8 in the op','line_number':182,'multiline':False]
['text':'    W_prepack: packed qint8 quantized weight and bias','line_number':183,'multiline':False]
['text':' Returns:','line_number':184,'multiline':False]
['text':'    Y: float32 Tensor','line_number':185,'multiline':False]
['text':' deprecated functions, kept for backward compatibility','line_number':206,'multiline':False]
['text':' TODO: remove after broadcasting is supported','line_number':213,'multiline':False]
['text':' According to #33294: The "_" prefix registration will be','line_number':227,'multiline':False]
['text':' removed when the operators are all migrated to mobile.','line_number':228,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/36510','line_number':229,'multiline':False]
['text':' New OP definition for Quantization in PyTorch 2.0 Export','line_number':252,'multiline':False]
['text':' Weight Prepack','line_number':253,'multiline':False]
['text':' Conv1D/2D/3D with unary postop','line_number':256,'multiline':False]
['text':' Conv2D with binary postop','line_number':261,'multiline':False]
['text':' Linear prepack','line_number':264,'multiline':False]
['text':' Linear with unary postop','line_number':267,'multiline':False]
