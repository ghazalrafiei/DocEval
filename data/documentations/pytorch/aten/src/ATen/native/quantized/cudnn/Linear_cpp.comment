['text':' for the definition of AT_CUDNN_ENABLED','line_number':2,'multiline':False]
['text':' TODO: there is a table from input dtype and weight dtype to operator dtype,','line_number':28,'multiline':False]
['text':' we can derive the operator dtype based on input dtype','line_number':29,'multiline':False]
['text':' FIXME: make this thread-safe by reusing the benchmark cache in Conv_v7.cpp','line_number':36,'multiline':False]
['text':' we currently set the maximum number of input dimensions to 5','line_number':38,'multiline':False]
['text':' this can be increased, if necessary','line_number':39,'multiline':False]
['text':' default to -1 when no bias','line_number':56,'multiline':False]
['text':' operator datatype needs to be int32 for int8 matmul, but we can','line_number':63,'multiline':False]
['text':' set the datatype for output tensor to int32 or fp32','line_number':64,'multiline':False]
['text':' TODO: we can use cudnn_frontend::ExecutionPlanCache when it supports caching','line_number':81,'multiline':False]
['text':' multiple operators','line_number':82,'multiline':False]
['text':' reference: https://github.com/NVIDIA/cudnn-frontend/blob/main/samples/conv_sample.cpp#L293','line_number':83,'multiline':False]
['text':'static cudnn_frontend::ExecutionPlanCache plan_cache("sample_cache");','line_number':84,'multiline':False]
['text':' currently we only support int8 symmetric (zero_point = 0 for inputs and output) quantized linear op','line_number':86,'multiline':False]
['text':' We implement relu(act_int8 * transpose(w_int8) + [bias_fp32/(act_scale * w_scale] ) * ( act_scale * w_scale / out_scale )','line_number':87,'multiline':False]
['text':' which requires 5 cudnn ops (1 matmul, 2 multiplication, 1 add, and 1 relu ops)','line_number':88,'multiline':False]
['text':' matmul op: linear_op','line_number':89,'multiline':False]
['text':' Multiplication ops: rhs_mult_op, requant_op','line_number':90,'multiline':False]
['text':' Addition op: add_op','line_number':91,'multiline':False]
['text':' Relu op: relu_op','line_number':92,'multiline':False]
['text':' the input bias is a 1-D tensor whose size is the same as the size of the last dimension of quantized_output','line_number':105,'multiline':False]
['text':' we need to add trailing dimensions in order to properly broadcast bias, otherwise broadcast_to will fail.','line_number':106,'multiline':False]
['text':' the number of trailling dimensions is quantized_output.dim() - 2. We also prepend a leading dimension for clarity','line_number':107,'multiline':False]
['text':' memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are','line_number':119,'multiline':False]
['text':' used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two','line_number':120,'multiline':False]
['text':' CacheKey objects have the same user defined parameters, but','line_number':121,'multiline':False]
['text':' different padded values, resulting in different hash outputs.','line_number':122,'multiline':False]
['text':' the matmul operation is input * transpose(weight), so we will work with the transposed weight','line_number':137,'multiline':False]
['text':' cudnn expects tensors to be at least 3D. weight_transposed is currently 2D. we will create a 3D view','line_number':139,'multiline':False]
['text':' by prepending a leading dummy dimension (cudnn expects leading dimensions to be the dummy dimensions)','line_number':140,'multiline':False]
['text':' linear_op computes act_int8 * tranpose(w_int8) (matrix multiplication)','line_number':175,'multiline':False]
['text':' where act_int8 and w_int8 are the input and weight variables, resp.','line_number':176,'multiline':False]
['text':' output is a fp32 tensor','line_number':177,'multiline':False]
['text':' for virtual tensors, the alignment is not used, so we can just put an arbitrary value here, e.g., key.output_alignment','line_number':181,'multiline':False]
['text':' std::cout << "operator:" << linear_op.describe() << std::endl;','line_number':185,'multiline':False]
['text':' we can't directly assign bias_mult_op becauase operator= is deleted for cudnn_frontend::Operation;','line_number':190,'multiline':False]
['text':' alternatively, I think we can use std::unique_ptr and dynamically allocate these builder ops','line_number':191,'multiline':False]
['text':' but here, we chose to do it statically. c10::optional<T>::emplace() enables this approach','line_number':192,'multiline':False]
['text':' bias_mult_op computes bias_fp32 / (act_scale * w_scale) or bias_fp32 * (1 / (act_scale * w_scale))','line_number':194,'multiline':False]
['text':' where bias_multiplier = (1 / (act_scale * w_scale))','line_number':195,'multiline':False]
['text':' output is a fp32 tensor','line_number':196,'multiline':False]
['text':' we use inplace operation here where the output is assigned to the input','line_number':197,'multiline':False]
['text':' TODO: I think we should be able to make this a virtual tensor, but we would need cudnn to support','line_number':201,'multiline':False]
['text':' setbdesc(ManagedOpaqueDescriptor const &raw_tensor) first','line_number':202,'multiline':False]
['text':' computes (act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)])','line_number':207,'multiline':False]
['text':' where the 1st and 2nd summands is output of linear op and broadcasted_bias, resp.','line_number':208,'multiline':False]
['text':' output is a fp32 tensor','line_number':209,'multiline':False]
['text':' we use inplace operation here where the output is assigned to the input','line_number':210,'multiline':False]
['text':' TODO: An additional entry for broadcasted_bias in the uid-data_ptr pairing','line_number':213,'multiline':False]
['text':' appears to be needed in the current version of cudnn (8.4.0). Without it, some','line_number':214,'multiline':False]
['text':' test cases are failing. NVIDIA is currently investigating this issue.','line_number':215,'multiline':False]
['text':' When this issue is fixed, we can change 'n' back to 'd' and remove the additional entry in uid and data_ptrs in variant pack above','line_number':216,'multiline':False]
['text':' relu_op computes relu(act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]','line_number':223,'multiline':False]
['text':' or relu(act_int8 * w_int8) if bias is not present.','line_number':224,'multiline':False]
['text':' output is a fp32 tensor','line_number':225,'multiline':False]
['text':' we use inplace operation here where the output is assigned to the input','line_number':229,'multiline':False]
['text':' for virtual tensors, the alignment is not used, so we can just put an arbitrary value here, e.g., key.output_alignment','line_number':232,'multiline':False]
['text':' requant_op computes relu(act_int8 * w_int8 + [bias_fp32/(act_scale * w_scale)]) / (out_scale / (act_scale * w_scale))','line_number':238,'multiline':False]
['text':' or relu(act_int8 * w_int8) / (out_scale / (act_scale * w_scale))) if bias is not present.','line_number':239,'multiline':False]
['text':' output is a fp32 tensor','line_number':240,'multiline':False]
['text':' // std::cout << "operator:" << requant_op.describe() << std::endl;','line_number':247,'multiline':False]
['text':' std::cout << "opGraph: " << opGraph.describe() << std::endl;','line_number':263,'multiline':False]
['text':' output Tensor will be a clampped int8 Tensor','line_number':296,'multiline':False]
['text':' both act and weight will be int8 Tensor','line_number':297,'multiline':False]
['text':' Numerics are the same as conv (see aten/src/ATen/native/quantized/Conv.cpp):','line_number':298,'multiline':False]
['text':' 2D','line_number':304,'multiline':False]
['text':' output channels','line_number':305,'multiline':False]
['text':' cudnn expects tensors to be at least 3D. we will prepend a dummy dimension for quantized_output','line_number':306,'multiline':False]
['text':' cudnn expects tensors to be at least 3D. act is currently 2D. we will create a 3D view','line_number':315,'multiline':False]
['text':' cudnn expects leading dimensions to be the dummy dimensions','line_number':317,'multiline':False]
['text':' TODO: check all zero_points are zero/all tensors are symmetrically quantized','line_number':351,'multiline':False]
['text':' namespace','line_number':365,'multiline':False]
['text':' namespace native','line_number':366,'multiline':False]
['text':' namespace at','line_number':367,'multiline':False]
['text':' HAS_CUDNN_V8','line_number':370,'multiline':False]
['text':' AT_CUDNN_ENABLED','line_number':371,'multiline':False]
['text':' USE_CUDA','line_number':372,'multiline':False]
