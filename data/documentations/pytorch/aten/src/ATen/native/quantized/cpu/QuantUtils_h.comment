['text':' Convert raw 16 bits half precision floating point number','line_number':22,'multiline':False]
['text':' to single precision floating point number.','line_number':23,'multiline':False]
['text':' 0.0009765625f = 0x1p-10 = 2^-10;','line_number':30,'multiline':False]
['text':' A structure to hold quantization parameters 'scale' and 'zero_point'.','line_number':50,'multiline':False]
['text':' The meaning of these values is as the constants in the quantization equation','line_number':51,'multiline':False]
['text':'','line_number':52,'multiline':False]
['text':'   real_value = scale * (quantized_value - zero_point)','line_number':53,'multiline':False]
['text':'','line_number':54,'multiline':False]
['text':' In other words, 'zero_point' is the quantized value that corresponds','line_number':55,'multiline':False]
['text':' to the real value 0, and 'scale' is the difference of real values','line_number':56,'multiline':False]
['text':' corresponding to consecutive quantized values.','line_number':57,'multiline':False]
['text':' Use fp16_min as the small scale cutoff because we don't want to use scales in','line_number':64,'multiline':False]
['text':' fp16 subnormal range. This is to be consistent with Glow and FakeLowP','line_number':65,'multiline':False]
['text':' implementation for NNPI.','line_number':66,'multiline':False]
['text':' Following implementation should be identical to fbgemm::ChooseQuantizationParams','line_number':69,'multiline':False]
['text':' We extend the [min, max] interval to ensure that it contains 0.','line_number':95,'multiline':False]
['text':' Otherwise, we would not meet the requirement that 0 be an exactly','line_number':96,'multiline':False]
['text':' representable value.','line_number':97,'multiline':False]
['text':' Use double precision for intermediate computation but use single precision','line_number':105,'multiline':False]
['text':' in final number to reflect the actual number used during quantization.','line_number':106,'multiline':False]
['text':' If scale is 0 or too small so its reciprocal is infinity, we arbitrary','line_number':108,'multiline':False]
['text':' adjust the scale to 0.1 . We want to avoid scale's reciprocal being','line_number':109,'multiline':False]
['text':' infinity because some of fbgemm code pre-computes scale's reciprocal to do','line_number':110,'multiline':False]
['text':' multiplication instead of division in the time critical part of code.','line_number':111,'multiline':False]
['text':' Cut off small scale','line_number':125,'multiline':False]
['text':' Adjust the min and max based on the new scale','line_number':129,'multiline':False]
['text':' Zero-point computation.','line_number':141,'multiline':False]
['text':' First the initial floating-point computation. The zero-point can be','line_number':142,'multiline':False]
['text':' determined from solving an affine equation for any known pair','line_number':143,'multiline':False]
['text':' (real value, corresponding quantized value).','line_number':144,'multiline':False]
['text':' We know two such pairs: (rmin, qmin) and (rmax, qmax).','line_number':145,'multiline':False]
['text':' The arithmetic error on the zero point computed from either pair','line_number':146,'multiline':False]
['text':' will be roughly machine_epsilon * (sum of absolute values of terms)','line_number':147,'multiline':False]
['text':' so we want to use the variant that adds the smaller terms.','line_number':148,'multiline':False]
['text':' for symmetric quantization (preserve_sparsity == true), we force zero_point','line_number':160,'multiline':False]
['text':' to be a middle value between qmin and qmax.','line_number':161,'multiline':False]
['text':' If either min or max is 0, then we just use 0 as zero_point.','line_number':162,'multiline':False]
['text':' Now we need to nudge the zero point to be an integer','line_number':167,'multiline':False]
['text':' (our zero points are integer, and this is motivated by the requirement','line_number':168,'multiline':False]
['text':' to be able to represent the real value "0" exactly as a quantized value,','line_number':169,'multiline':False]
['text':' which is required in multiple places, for example in Im2col with zero','line_number':170,'multiline':False]
['text':' padding).','line_number':171,'multiline':False]
['text':' This function helps to convert the Conv1D dimensions usable by the Conv2d op.','line_number':187,'multiline':False]
['text':' The range for using FP16 quantization of weights requires that the elements','line_number':202,'multiline':False]
['text':' should be in the range of [5.96e-8, 65504]. If it is out of range, then the','line_number':203,'multiline':False]
['text':' number will be saturated to max or min representable values by FP16.','line_number':204,'multiline':False]
['text':' Util function for quantizing bias.','line_number':219,'multiline':False]
['text':' namespace quant_utils','line_number':239,'multiline':False]
