['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':29,'multiline':False]
['text':' The last 2 values are used to store the FP32 scale and zero_point','line_number':33,'multiline':False]
['text':' values per row.','line_number':34,'multiline':False]
['text':' Calculate the output shape, accounting for the last n bytes to be used','line_number':41,'multiline':False]
['text':' for scale/bias rest of the entries are packed depending on the bit_width.','line_number':42,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':54,'multiline':False]
['text':' Allocate output weight tensor based on the bit_width','line_number':57,'multiline':False]
['text':' The output channel axis is 0','line_number':63,'multiline':False]
['text':' We create empty qtensor with the full output shape, and dtype set to','line_number':67,'multiline':False]
['text':' quint4x2 This will internally allocate appropriate storage bytes to','line_number':68,'multiline':False]
['text':' account for the packed nature of this dtype.','line_number':69,'multiline':False]
['text':' The output channel axis is 0','line_number':74,'multiline':False]
['text':' Copy over the data from the packed weight to the output.','line_number':79,'multiline':False]
['text':' For sub-byte tensors this will copy the packed bytes over since the','line_number':80,'multiline':False]
['text':' sub_byte qtensors are expected to store data in packed format.','line_number':81,'multiline':False]
['text':' output_columns','line_number':88,'multiline':False]
['text':' The "last" dimension of an N-Dimensioned batch of embedding bags is','line_number':107,'multiline':False]
['text':' quantization channel. E.g. for a 2D embedding bag, this has','line_number':108,'multiline':False]
['text':' [ row, col ] dimensions, for batched of embedding bags, dimensions might be','line_number':109,'multiline':False]
['text':' [ batch, row, col ].','line_number':110,'multiline':False]
['text':'','line_number':111,'multiline':False]
['text':' Python Batched Embedding Example:','line_number':112,'multiline':False]
['text':' weights = torch.from_numpy((np.random.random_sample((','line_number':113,'multiline':False]
['text':'          2, 10, 3)).squeeze() + 1).astype(np.float32))','line_number':114,'multiline':False]
['text':' assert(weights.size() == torch.Size([2, 10, 3]))','line_number':115,'multiline':False]
['text':' # NOTE: 8 bytes (columns) are added due to fp32 zero_point and scales','line_number':116,'multiline':False]
['text':' packed_weights = torch.ops.quantized.embedding_bag_byte_prepack(weights)','line_number':117,'multiline':False]
['text':' assert(packed_weights.size() == torch.Size([2, 10, 11]))','line_number':118,'multiline':False]
['text':' unpacked_weights = torch.ops.quantized.embedding_bag_byte_unpack(packed_weights)','line_number':119,'multiline':False]
['text':' assert(unpacked_weights.size() == torch.Size([2, 10, 3]))','line_number':120,'multiline':False]
['text':' The last 2 values are used to store the FP32 scale and zero_point values','line_number':125,'multiline':False]
['text':' per row.','line_number':126,'multiline':False]
['text':' output_columns','line_number':154,'multiline':False]
['text':' input_rows','line_number':155,'multiline':False]
['text':' USE_FBGEMM','line_number':156,'multiline':False]
['text':' The last 2 values are used to store the FP32 scale and zero_point values','line_number':174,'multiline':False]
['text':' per row.','line_number':175,'multiline':False]
['text':' The last 4 bytes per row are two fp16 scale and zero_point.','line_number':193,'multiline':False]
['text':' The rest of input_columns is the number of values in the original row.','line_number':194,'multiline':False]
['text':' output_columns','line_number':230,'multiline':False]
['text':' input_rows','line_number':231,'multiline':False]
['text':' USE_FBGEMM','line_number':232,'multiline':False]
['text':' De-quantizes the result of the qembeddingbag_4bit_prepack operator.','line_number':237,'multiline':False]
['text':' The input is expected to first have quantized values,','line_number':238,'multiline':False]
['text':' then 2-byte fp16 scale and 2-byte zero_offset.','line_number':239,'multiline':False]
['text':' The output is a matrix containing only the values, but de-quantized.','line_number':240,'multiline':False]
['text':' De-quantization is performed by multiplying each value by its','line_number':241,'multiline':False]
['text':' row's scale and zero_point parameters. The de-quantized values','line_number':242,'multiline':False]
['text':' will thus not be exactly equal to the original, un-quantized','line_number':243,'multiline':False]
['text':' floating point values.','line_number':244,'multiline':False]
['text':'BIT_RATE','line_number':246,'multiline':True]
['text':' De-quantizes the result of the qembeddingbag_2bit_prepack operator.','line_number':249,'multiline':False]
['text':' The input is expected to first have quantized values,','line_number':250,'multiline':False]
['text':' then 2-byte fp16 scale and 2-byte zero_offset.','line_number':251,'multiline':False]
['text':' The output is a matrix containing only the values, but de-quantized.','line_number':252,'multiline':False]
['text':' De-quantization is performed by multiplying each value by its','line_number':253,'multiline':False]
['text':' row's scale and zero_point parameters. The de-quantized values','line_number':254,'multiline':False]
['text':' will thus not be exactly equal to the original, un-quantized','line_number':255,'multiline':False]
['text':' floating point values.','line_number':256,'multiline':False]
['text':'BIT_RATE','line_number':258,'multiline':True]
['text':' Unpack the packed embedding_bag weights using TorchBind custom class.','line_number':282,'multiline':False]
['text':' TODO extend to support 4-bit qtensor.','line_number':283,'multiline':False]
['text':' namespace','line_number':295,'multiline':False]
['text':' namespace native','line_number':296,'multiline':False]
['text':' namespace at','line_number':297,'multiline':False]
