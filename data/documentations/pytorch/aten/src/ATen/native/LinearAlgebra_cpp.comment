['text':' Casting a large integer to a double will just introduce an error for','line_number':203,'multiline':False]
['text':' values larger than 10^53 (same for negative numbers), so that's fine.','line_number':204,'multiline':False]
['text':' For more context, see issue 52783','line_number':207,'multiline':False]
['text':' If the tensor is empty and norm < 0 || norm == infty','line_number':208,'multiline':False]
['text':'   - We cannot reduce the whole tensor','line_number':209,'multiline':False]
['text':'   - We cannot reduce over an empty dimension','line_number':210,'multiline':False]
['text':' dim=None or dim=() reduces the whole tensor','line_number':212,'multiline':False]
['text':' det','line_number':240,'multiline':False]
['text':' LU','line_number':243,'multiline':False]
['text':'f-contig*=','line_number':244,'multiline':True]
['text':' pivots','line_number':247,'multiline':False]
['text':'low_precision','line_number':253,'multiline':True]
['text':' sign','line_number':260,'multiline':False]
['text':' logabsdet','line_number':263,'multiline':False]
['text':' LU','line_number':266,'multiline':False]
['text':'f-contig*=','line_number':267,'multiline':True]
['text':' pivots','line_number':270,'multiline':False]
['text':' 'set_output' does not resize for in-place calls','line_number':293,'multiline':False]
['text':' Error is raised if called from in-place overload with incorrect shape','line_number':296,'multiline':False]
['text':' namespace meta','line_number':331,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg.det ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':337,'multiline':False]
['text':' As P is a permutation matrix','line_number':339,'multiline':False]
['text':' det(P) = 1 if it's an even permutation and det(P) = -1 if it's an odd permutation','line_number':340,'multiline':False]
['text':'keepdim=','line_number':343,'multiline':True]
['text':'dtype=','line_number':343,'multiline':True]
['text':' take 0 to 1 and 1 to -1','line_number':345,'multiline':False]
['text':' Auxiliary function that returns the LU decomposition to use it in the backward','line_number':350,'multiline':False]
['text':' info is an aux tensor','line_number':352,'multiline':False]
['text':' Optimisation: lu_factor_ex requires the input to be F-contig, otherwise it copies','line_number':354,'multiline':False]
['text':' Use the transpose of if A is contiguous since det(A^T) = det(A)','line_number':355,'multiline':False]
['text':' We limit this to real matrices, but it could also be implemented for complex matrices','line_number':356,'multiline':False]
['text':' det = det_P * prod(diag(LU))','line_number':359,'multiline':False]
['text':'dim=','line_number':360,'multiline':True]
['text':' torch.det, alias for torch.linalg.det','line_number':374,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linalg.slogdet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':379,'multiline':False]
['text':' Auxiliary function that returns the LU decomposition to use it in the backward','line_number':381,'multiline':False]
['text':' info is an aux tensor','line_number':383,'multiline':False]
['text':' Optimisation: lu_factor_ex requires the input to be F-contig, otherwise it copies','line_number':385,'multiline':False]
['text':' Use the transpose of if A is contiguous since det(A^T) = det(A)','line_number':386,'multiline':False]
['text':' We limit this to real matrices, but it could also be implemented for complex matrices','line_number':387,'multiline':False]
['text':' sign','line_number':391,'multiline':False]
['text':' logabsdet','line_number':394,'multiline':False]
['text':' Alias','line_number':410,'multiline':False]
['text':'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ logdet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':419,'multiline':False]
['text':'low_precision','line_number':423,'multiline':True]
['text':' This function extracts the optional Tensors for atol and rtol','line_number':436,'multiline':False]
['text':' Default value for atol is zero','line_number':437,'multiline':False]
['text':' Default value for rtol is eps*max(rows, cols)','line_number':438,'multiline':False]
['text':' If atol is specified and rtol is not specified then default value for rtol is zero','line_number':439,'multiline':False]
['text':' It is used for matrix_rank and pinv','line_number':440,'multiline':False]
['text':' anonymous namespace','line_number':484,'multiline':False]
['text':' FIXME: Whenever we have a nice lstsq, we should dispatch this function to simply be','line_number':491,'multiline':False]
['text':' `torch.lstsq(A, torch.eye(A.shape[-1]), atol=atol, rtol=rtol)`','line_number':492,'multiline':False]
['text':' with a driver that supports singular inputs','line_number':493,'multiline':False]
['text':' The implementation below uses operations that do not work for zero numel tensors','line_number':505,'multiline':False]
['text':' therefore we need this early return for 'input.numel() == 0' case','line_number':506,'multiline':False]
['text':' TODO: replace input.svd with linalg_svd when torch/xla can work with at::linalg_svd','line_number':508,'multiline':False]
['text':' If not Hermitian use singular value decomposition, else use eigenvalue decomposition','line_number':513,'multiline':False]
['text':' TODO: replace input.svd with linalg_svd','line_number':516,'multiline':False]
['text':' using linalg_svd breaks pytorch/xla, see https://github.com/pytorch/xla/issues/2755','line_number':517,'multiline':False]
['text':'dim=','line_number':519,'multiline':True]
['text':'start=','line_number':519,'multiline':True]
['text':'length=','line_number':519,'multiline':True]
['text':' singular values are sorted in descending order','line_number':519,'multiline':False]
['text':' computes V @ diag(S_pseudoinv) @ U.conj().T','line_number':522,'multiline':False]
['text':' For Hermitian matrices, singular values equal to abs(eigenvalues)','line_number':527,'multiline':False]
['text':' eigenvalues are sorted in ascending order starting with negative values, we need a maximum value of abs(eigenvalues)','line_number':529,'multiline':False]
['text':'dim=','line_number':530,'multiline':True]
['text':'keepdim=','line_number':530,'multiline':True]
['text':' computes U @ diag(S_pseudoinv) @ U.conj().T','line_number':533,'multiline':False]
['text':' For NumPy compatibility the rcond argument is used as relative tolerance','line_number':545,'multiline':False]
['text':' For NumPy compatibility the rcond argument is used as relative tolerance','line_number':552,'multiline':False]
['text':' TODO: implement _out variant avoiding copy and using already allocated storage directly','line_number':556,'multiline':False]
['text':'hermitian=','line_number':601,'multiline':True]
['text':' matrix_power implementation','line_number':604,'multiline':False]
['text':'*
 * @brief Raises the input matrix to the given power n
 *
 * If the exponent n is negative, the inverse of the input
 * matrix will be raised to power abs(n).
 *
 * @param self (batched) square matrix to raise to power n
 * @param n exponent to raise matrix (or matrices in batch) to
 * @param _out optional tensor to write the output to
 * @return Tensor input matrix raised to power n
 ','line_number':607,'multiline':True]
['text':' For n=0 we return the identity matrix of the same shape as input.','line_number':632,'multiline':False]
['text':' Clone input to include result in the autograd graph','line_number':635,'multiline':False]
['text':' For negative n we inverte the input matrix before raising to power abs(n)','line_number':649,'multiline':False]
['text':' Fast paths for small powers','line_number':653,'multiline':False]
['text':' This is a binary decomposition of n.','line_number':662,'multiline':False]
['text':' Moving from the least significant bit to the most significant bit','line_number':663,'multiline':False]
['text':' This is done to reduce the number of matrix multiplications','line_number':664,'multiline':False]
['text':' by raising the input matrix in powers of 2','line_number':665,'multiline':False]
['text':' The total number of matrix multiplications are','line_number':666,'multiline':False]
['text':' number of bits + number of bits that equal 1 ~ O(log n)','line_number':667,'multiline':False]
['text':' instead of O(n)','line_number':668,'multiline':False]
['text':' Last multiplication can use the out version','line_number':676,'multiline':False]
['text':' namespace','line_number':686,'multiline':False]
['text':' Computes the rank of 'input' and saves the result in-place in 'result'.','line_number':707,'multiline':False]
['text':' 'hermitian' controls whether SVD or eigendecomposition is used for computing the singular values','line_number':708,'multiline':False]
['text':' 'atol' and 'rtol' are the absolute and relative tolerances, respectively.','line_number':709,'multiline':False]
['text':' NumPy doesn't take into account possible input with no elements and it errors on max not defined for this case','line_number':728,'multiline':False]
['text':' Let's output 0 for this case, since that kind of matrices have zero number of non-zero rows, hence rank is 0.','line_number':729,'multiline':False]
['text':' We compute matrix rank as the number of singular or absolute eigen values','line_number':735,'multiline':False]
['text':' that are above max(atol, rtol * max(S)) threshold','line_number':736,'multiline':False]
['text':' singular values are sorted in descending order','line_number':740,'multiline':False]
['text':'dim=','line_number':741,'multiline':True]
['text':'start=','line_number':741,'multiline':True]
['text':'length=','line_number':741,'multiline':True]
['text':' eigenvalues are sorted in ascending order starting with negative values, we need a maximum value of abs(eigenvalues)','line_number':745,'multiline':False]
['text':'dim=','line_number':746,'multiline':True]
['text':'keepdim=','line_number':746,'multiline':True]
['text':'dim=','line_number':752,'multiline':True]
['text':'dim=','line_number':756,'multiline':True]
['text':' Matrices or batch of matrices are allowed','line_number':761,'multiline':False]
['text':' For Composite Compliance, allocate `result` of correct shape to','line_number':763,'multiline':False]
['text':' avoid resizing in `out` variant.','line_number':764,'multiline':False]
['text':' See also `NOTE [matrix rank output shape]`','line_number':765,'multiline':False]
['text':' anonymous namespace','line_number':774,'multiline':False]
['text':' Matrices or batch of matrices are allowed','line_number':782,'multiline':False]
['text':' For NumPy compatibility tol is not scaled with max(singular_value) if the value for tol is provided','line_number':812,'multiline':False]
['text':' It is assumed that the provided value is the absolute tolerance','line_number':813,'multiline':False]
['text':' For NumPy compatibility tol is not scaled with max(singular_value) if the value for tol is provided','line_number':820,'multiline':False]
['text':' It is assumed that the provided value is the absolute tolerance','line_number':821,'multiline':False]
['text':' multi_dot helper functions','line_number':840,'multiline':False]
['text':'*
 * @brief Computes the optimal matrix chain multiplication order
 *
 * Follows the dynamic programming algorithm from Cormen et al,
 * "Introduction to Algorithms, Third Edition", Chapter 15.2,
 * p. 370-378. Note that the book uses 1-based indexing.
 *
 * The cost of multiplying two matrices with sizes p x q and q x r
 * is defined here as p * q * r. The optimal multiplication order
 * is the one that minimizes the total cost.
 *
 * @param tensors list of 2D tensors
 * @return a 2D vector s used by #matrix_chain_multiplication to construct
 *         the optimal matrix multiplication order. The optimal multiplication
 *         order for multiplying tensors i...j is to multiply tensors i...s[i, j]
 *         and tensors (s[i, j] + 1)...j first and then the result of that.
 ','line_number':843,'multiline':True]
['text':' Tensor i has dimensions p[i] x p[i + 1]','line_number':863,'multiline':False]
['text':' m[i, j] = k where k is the minimum cost for multiplying tensors i...j','line_number':870,'multiline':False]
['text':' s[i, j] = k where k is the index at which to split the list such that','line_number':873,'multiline':False]
['text':' optimally multiplying matrices i...k and k...j first and then the resulting','line_number':874,'multiline':False]
['text':' matrices is the optimal order for multiplying matrices i...j.','line_number':875,'multiline':False]
['text':' Compute the optimal multiplication order','line_number':878,'multiline':False]
['text':'*
 * @brief Recursively multiplies the tensors i...j using the given order
 *
 * @param tensors matrices to multiply together
 * @param order optimal chain multiplication order from #matrix_chain_order
 * @param i index of first tensor to be multiplied
 * @param j index of last tensor to be multiplied
 * @return Tensor result of multiplying tensors[i...j] together.
 ','line_number':896,'multiline':True]
['text':' Implements torch.linalg.multi_dot','line_number':918,'multiline':False]
['text':' If the first tensor is 1D of size n view it as a row vector (1, n)','line_number':926,'multiline':False]
['text':' If the last tensor is 1D of size n view it as a column vector (n, 1)','line_number':940,'multiline':False]
['text':' Ensure middle tensors are 2D','line_number':954,'multiline':False]
['text':' Ensure all tensors have the same device and dtype and check','line_number':966,'multiline':False]
['text':' that the shapes can be multiplied','line_number':967,'multiline':False]
['text':' If the last and last tensors have shapes (a, b) and (b, c) the','line_number':1017,'multiline':False]
['text':' output has shape (a, c). If either the first or last tensor is 1D','line_number':1018,'multiline':False]
['text':' a and/or c dimensions will be implicitely size 1 and will be ommited','line_number':1019,'multiline':False]
['text':' from the output. e.g. for inputs (a, b) x (b) the output has shape (a,).','line_number':1020,'multiline':False]
['text':' View output as 2D for simplicity of computation.','line_number':1023,'multiline':False]
['text':' The resize_ and view calls below are to ensure the','line_number':1027,'multiline':False]
['text':' output shape respects the original dimensionality of','line_number':1028,'multiline':False]
['text':' the first and last tensors which we are now viewed as 2D','line_number':1029,'multiline':False]
['text':' Why the separate implementation for 3 matrices?','line_number':1036,'multiline':False]
['text':' The logic for three matrices is much faster when done directly','line_number':1037,'multiline':False]
['text':' Requires 1 comparison to 4 comparisons and fewer arithmetic operations','line_number':1038,'multiline':False]
['text':' The matrices are of size (a x b), (b x c), (c x d)','line_number':1045,'multiline':False]
['text':' cost_1 is the cost of parenthesizing (a x b) and (b x c) and then','line_number':1046,'multiline':False]
['text':' combining (c x d) cost_2 is the cost of parenthesizing (b x c) and (c x','line_number':1047,'multiline':False]
['text':' d) and then combining (a x b)','line_number':1048,'multiline':False]
['text':' Algorithm for multiplying 4 or more matrices','line_number':1063,'multiline':False]
['text':' We manually implement the first recursive layer here so we can use mm_out','line_number':1069,'multiline':False]
['text':' for the final multiplication','line_number':1070,'multiline':False]
['text':' namespace','line_number':1079,'multiline':False]
['text':' The math_addr and math_addr_out functions support backends','line_number':1211,'multiline':False]
['text':' other than CPU and CUDA, such as XLA.','line_number':1212,'multiline':False]
['text':' They are implemented using the composition of existing ops','line_number':1213,'multiline':False]
['text':' when beta==0, values in self should be ignored,','line_number':1217,'multiline':False]
['text':' nans and infs in self should not propagate.','line_number':1218,'multiline':False]
['text':' Validates safe casting','line_number':1246,'multiline':False]
['text':' torch.ger, alias for torch.outer','line_number':1257,'multiline':False]
['text':' If either self or other is a scalar just multiply them','line_number':1271,'multiline':False]
['text':' Last dimension should match (tensordot does not enforce this)','line_number':1277,'multiline':False]
['text':' If either self or other is a scalar just multiply them','line_number':1292,'multiline':False]
['text':' Last dimension should match (tensordot does not enforce this)','line_number':1297,'multiline':False]
['text':' torch.outer is implemented as a composite op using reshape and mul','line_number':1312,'multiline':False]
['text':' Minimum dimension requirement for MKLDNN; derived based on experiments.','line_number':1340,'multiline':False]
['text':' By default, it's only enabled on Neoverse V1.','line_number':1341,'multiline':False]
['text':' Minimum size requirement for MKLDNN; derived based on experiments.','line_number':1359,'multiline':False]
['text':' By default, it's only enabled on Neoverse V1.','line_number':1360,'multiline':False]
['text':' Array access is faster than .size(n) and .stride(n)','line_number':1390,'multiline':False]
['text':' Some paths in the code below do not handle multiplications of the form [a, 0] x [0, b]','line_number':1411,'multiline':False]
['text':' Cast result as matrix a','line_number':1431,'multiline':False]
['text':' make c FORTRAN contiguous','line_number':1445,'multiline':False]
['text':' Cast m1 as matrix a','line_number':1453,'multiline':False]
['text':' Need lda >= max(1, (transpose_a ? k : m)) ','line_number':1456,'multiline':True]
['text':' Cast m2 as matrix b','line_number':1470,'multiline':False]
['text':' Need ldm2_ >= max(1, (transpose_m2 == 'n' ? k : n)) ','line_number':1473,'multiline':True]
['text':' Always ensure the conjugation for c is resolved since there's no way to specify c's conjugation in the gemm call','line_number':1491,'multiline':False]
['text':' On AArch64 if LHS matrix in BLAS routine is transposed but RHS is not then','line_number':1496,'multiline':False]
['text':' it is faster to call oneDNN matrix multiplication primitive with RHS*LHS','line_number':1497,'multiline':False]
['text':' that will call then into Arm® Compute Library (ACL) GEMM kernel and also','line_number':1498,'multiline':False]
['text':' additionally have support for running kernel with BF16 instructions','line_number':1499,'multiline':False]
['text':' We have dispatched to ACL GEMM for single precision float','line_number':1504,'multiline':False]
['text':' so do not need to dispatch to BLAS GEMM below','line_number':1505,'multiline':False]
['text':' Apply BLAS routine','line_number':1512,'multiline':False]
['text':' accumulate output once','line_number':1569,'multiline':False]
['text':'is_bmm ? opmath_t(0) : opmath_t(r2[j]);','line_number':1647,'multiline':False]
['text':' For beta == 0, the r's value will be ignored, especially for nan value.','line_number':1655,'multiline':False]
['text':' gemm expects fortran order matrices, so we swap argument order to transpose everything','line_number':1682,'multiline':False]
['text':' This tries to apply some optimizations to bmm/baddbmm:','line_number':1710,'multiline':False]
['text':' - When the operand size is small, computation are parallelized over the batch','line_number':1711,'multiline':False]
['text':'   dimension using OMP and naive matrix multiplication is applied.','line_number':1712,'multiline':False]
['text':' - When the operand size is larger than the threshold, if compiled with MKL, MKL's batch gemm is used.','line_number':1713,'multiline':False]
['text':' - Otherwise, we use a series of matrix multiplications.','line_number':1714,'multiline':False]
['text':' The threshold of 400 for the first has not been thoroughly benchmarked yet and may have room for further','line_number':1715,'multiline':False]
['text':' optimization, it likely depends on the characteristics of the CPU, MKL will be different from non-MKL etc.,','line_number':1716,'multiline':False]
['text':' but this seems to be a first starting point.','line_number':1717,'multiline':False]
['text':' is_bmm_out: true for bmm_out, false for baddbmm_','line_number':1720,'multiline':False]
['text':' self_or_result is "self" for baddbmm_ and "result" for bmm_out','line_number':1721,'multiline':False]
['text':' handle pathological cases that blas may not like','line_number':1732,'multiline':False]
['text':' we do not care dimension's stride if its size equals to 1','line_number':1748,'multiline':False]
['text':' split along batch dimension','line_number':1778,'multiline':False]
['text':'
     * We only do multithreading when Inference mode is enabled because various
     * thread local state is not appropriately propagated through
     * at::parallel_for. e.g. RecordFunction related state, dispatchKeySet Big
     * concern with this is that if we use at::parallel_for where state is not
     * propagated then dispatch machinery may work differently on main thread
     * vs. other threads, leading to undefined behavior.
     * Thus it is recommended to not use at::parallel_for where lambdas do
     * ops that go through dispatcher.
     * For now we circument this by InferenceMode guard in order to unlock
     * performance.
     * Longer term we probably want a separate API that explicitly calls out
     * the TLS that it propagates.
     * Also note that this is enabled for mobile only because blas
     * implementation for non-mobile build is already multithreaded.
     ','line_number':1780,'multiline':True]
['text':' Benchmarking was done as follows:','line_number':1796,'multiline':False]
['text':' bmm_test: operator benchmark under','line_number':1797,'multiline':False]
['text':' benchmarks/operator_benchmarks/pt/bmm_test.py Ran this benchmark for','line_number':1798,'multiline':False]
['text':' various matrix sizes on Samsung S8U','line_number':1799,'multiline':False]
['text':' check if the input & output tensors are on the same device.','line_number':1872,'multiline':False]
['text':' check if the input & output tensors are on the same device.','line_number':1888,'multiline':False]
['text':' We check that we can fold the larger tensor into a matrix and dispatch to mm or mv rather than','line_number':1901,'multiline':False]
['text':' to bmm. We want to make sure we can do so without incurring in any extra copy','line_number':1902,'multiline':False]
['text':' We order the tensors. t1 will be the larger tensor','line_number':1905,'multiline':False]
['text':' We can always transpose tensor2 as the dimensions are always >= 1 (precondition from matmul)','line_number':1906,'multiline':False]
['text':' and tensor1_larger iff tensor2.dim() > tensor1.dim(9','line_number':1907,'multiline':False]
['text':' Just fold for dim_t1 >= 3 and (dim_t2 == 1 || dim_t2 == 2)','line_number':1914,'multiline':False]
['text':' In this case we *do* incur in an extra copy to avoid creating an unnecessary large tensor in the backward','line_number':1919,'multiline':False]
['text':' Suppose we don't fold here. Let t1.shape = [b, m, n] t2.shape = [n, k] like in a transformer','line_number':1920,'multiline':False]
['text':' t2 will be expanded to a tensor of shape [b, n, k] and then we do t1.bmm(t2_expanded)','line_number':1921,'multiline':False]
['text':' The issue appears in the backward.','line_number':1922,'multiline':False]
['text':' The output gradient g of this operation would have shape [b, m, k]','line_number':1923,'multiline':False]
['text':' The backward wrt. t2 of bmm would be given by t1.mH @ g, which has shape [b, n, k]','line_number':1924,'multiline':False]
['text':' Then, the backward of expand is simply `sum(0)`. As such, we are instantiating a tensor','line_number':1925,'multiline':False]
['text':' of shape [b, n, k] unnacessarily, which may cause a large memory footprint, and in the','line_number':1926,'multiline':False]
['text':' worst case, an OOM','line_number':1927,'multiline':False]
['text':' Don't fold in this case, as we would have to call mm on the transposed tensor, the result','line_number':1933,'multiline':False]
['text':' would be contiguous, and then we would need to transpose it and call contiguous on it, thus','line_number':1934,'multiline':False]
['text':' having to copy the tensor','line_number':1935,'multiline':False]
['text':' Can always fold if the tensor is empty','line_number':1940,'multiline':False]
['text':' This serves as a precondition for the code below','line_number':1941,'multiline':False]
['text':' t1->view(-1, t1->size(-1)) does not copy only when the first n-1 dimensions are contiguous','line_number':1946,'multiline':False]
['text':' in the sense that t1_stride[i] = t1_stride[i+1]*t1_shape[i+1]','line_number':1947,'multiline':False]
['text':'
Matrix product of two Tensors.
The behavior depends on the dimensionality of the Tensors as follows:
- If both Tensors are 1-dimensional, (1d) the dot product (scalar) is returned.
- If the arguments are 2D - 1D or 1D - 2D, the matrix-vector product is returned.
- If both arguments are 2D, the matrix-matrix product is returned.
- If one of the arguments is ND with N >= 3 and the other is 1D or 2D, and some
  conditions on the strides apply (see should_fold) we fold the first N-1 dimensions
  of the ND argument to form a matrix, call mm or mv, reshape it back to ND and return it
- Otherwise, we return bmm, after broadcasting and folding the batched dimensions if
  there's more than one
','line_number':1958,'multiline':True]
['text':' This is checked up here to simplify the logic below','line_number':1978,'multiline':False]
['text':' Note that the strings are just evaluated on failure, so almost always we just evaluate','line_number':1979,'multiline':False]
['text':' the condition and move on','line_number':1980,'multiline':False]
['text':' dim_tensor1 >=3 && (dim_tensor2 == 1 || dim_tensor2 == 2) ||','line_number':1998,'multiline':False]
['text':' dim_tensor2 >=3 && (dim_tensor1 == 1 || dim_tensor1 == 2)','line_number':1999,'multiline':False]
['text':' and at least one of the following two conditions hold','line_number':2000,'multiline':False]
['text':' - the small tensor requires grad (see should_fold for the why)','line_number':2001,'multiline':False]
['text':' - we can fold the larger tensor t1 into a matrix as t1.view(-1, t1.size(-1)) without copying','line_number':2002,'multiline':False]
['text':' optimization: use mm instead of bmm by folding the batch of the larger tensor','line_number':2004,'multiline':False]
['text':' into its leading matrix dimension','line_number':2005,'multiline':False]
['text':' Invariant: t1->dim() >= 3 && (t2->dim() == 1 || t2->dim() == 2)','line_number':2013,'multiline':False]
['text':'            and *t1 and *t2 are matmul-compatible','line_number':2014,'multiline':False]
['text':' Why not t1->view(-1, sizes_1.back())?','line_number':2016,'multiline':False]
['text':' If the last dim is 0, then view(-1, 0) won't work because the -1 becomes ambiguous.','line_number':2017,'multiline':False]
['text':' This can happen in e.g. [3, 5, 0] @ [0, 0].','line_number':2018,'multiline':False]
['text':' Readjust output_shape if we are multiplying by a matrix','line_number':2023,'multiline':False]
['text':' This will almost always be a view.','line_number':2028,'multiline':False]
['text':' It may not be a view if t2->requires_grad(). See should_fold for an explanation','line_number':2029,'multiline':False]
['text':' This copies if we perform a 2D @ 3D and the first tensor requires_grad','line_number':2034,'multiline':False]
['text':' See should_fold for why.','line_number':2035,'multiline':False]
['text':' If mm_out were differentiable, we could use it here, and pass a result with the','line_number':2036,'multiline':False]
['text':' correct strides to avoid this unnecessary copy.','line_number':2037,'multiline':False]
['text':' See the !has_out branch for an explanation','line_number':2043,'multiline':False]
['text':' Resize output into the correct shape','line_number':2046,'multiline':False]
['text':' We then reshape the output to the expected shape and call mm/mv','line_number':2049,'multiline':False]
['text':' and transpose back if necessary','line_number':2050,'multiline':False]
['text':' dim_tensor1 >= 3 || dim_tensor2 >= 3','line_number':2064,'multiline':False]
['text':' We track m1 vs m2 separately even though they must match for nicer error messages','line_number':2065,'multiline':False]
['text':' Same optimization for the gradients as that in should_fold','line_number':2074,'multiline':False]
['text':' If we're going to broadcast we force it to go through the should_fold branch','line_number':2075,'multiline':False]
['text':' flatten expanded batches','line_number':2088,'multiline':False]
['text':' We need to treat the dim_tensor2 == 1 case separately as broadcasting would not convert','line_number':2094,'multiline':False]
['text':' a vector of shape (n,) into a batch of matrices of shape (*, n, 1)','line_number':2095,'multiline':False]
['text':' torch.linalg.matmul, alias for torch.matmul','line_number':2156,'multiline':False]
['text':' torch.linalg.diagonal, alias for torch.diagonal with dim1=-2, dim2=-1 as defaults','line_number':2165,'multiline':False]
['text':' helper methods for matrix_exp','line_number':2170,'multiline':False]
['text':' we consider 6 Taylor expansions of degree','line_number':2176,'multiline':False]
['text':' 1, 2, 4, 8, 12, 18','line_number':2177,'multiline':False]
['text':' Allocates a buffers of uninitialized or zero values','line_number':2184,'multiline':False]
['text':' of shape [n_copies, a.size()]','line_number':2185,'multiline':False]
['text':' Makes `buffer` to store `num_matrices` number of matrices needed for','line_number':2199,'multiline':False]
['text':' compute the matrix exponentials of different orders, i.e.','line_number':2200,'multiline':False]
['text':' first `num_matrices` matrices from the list l := {I, A, A^2, A^3, A^6}','line_number':2201,'multiline':False]
['text':' in a contiguous block of memory such that','line_number':2202,'multiline':False]
['text':' buffer[0, ...] = l[0], // I','line_number':2203,'multiline':False]
['text':' buffer[1, ...] = l[1], // A','line_number':2204,'multiline':False]
['text':' ...','line_number':2205,'multiline':False]
['text':' buffer[num_matrices - 1, ...] = l[num_matries - 1]','line_number':2206,'multiline':False]
['text':' fill I','line_number':2210,'multiline':False]
['text':' fill a','line_number':2218,'multiline':False]
['text':' fill a^2','line_number':2221,'multiline':False]
['text':' out for a^2','line_number':2223,'multiline':False]
['text':' fill a^3','line_number':2232,'multiline':False]
['text':' out for a^3','line_number':2234,'multiline':False]
['text':' fill a^6','line_number':2243,'multiline':False]
['text':' out for a^6','line_number':2245,'multiline':False]
['text':' convert a 1D blob to a 2D Tensor of size [1, blob.size()]','line_number':2264,'multiline':False]
['text':' such that blob.device() == in.device())','line_number':2265,'multiline':False]
['text':' designed to be used with _compute_linear_combination','line_number':2266,'multiline':False]
['text':' we convert to void* expecitly because begin() returns','line_number':2272,'multiline':False]
['text':' a pointer to a constant.','line_number':2273,'multiline':False]
['text':' Blob is assumed to be a 1D array, that is why','line_number':2274,'multiline':False]
['text':' we also insert a fake dimension so that the result could directly','line_number':2275,'multiline':False]
['text':' be used in _compute_linear_combination','line_number':2276,'multiline':False]
['text':' _blob_to_Tensor converts blob to a 2D tensor for _compute_linear_combination.','line_number':2286,'multiline':False]
['text':' If this tensor is of shape (1, *), the result of _compute_linear_combination','line_number':2287,'multiline':False]
['text':' is going to be of shape (1, *t.shape) so we squeeze(0) so that','line_number':2288,'multiline':False]
['text':' for any t with t.dim() >= 1: t.dim() == _compute_linear_combination(t, ...).dim().','line_number':2289,'multiline':False]
['text':' I + A','line_number':2295,'multiline':False]
['text':' 2 for {I, A}','line_number':2297,'multiline':False]
['text':' I + A + A^2 / 2','line_number':2303,'multiline':False]
['text':' 3 for {I, A, A^2}','line_number':2306,'multiline':False]
['text':' I + A + A^2 * (I / 2 + A / 6 + A^2 / 24)','line_number':2312,'multiline':False]
['text':' 3 for {I, A, A^2}','line_number':2316,'multiline':False]
['text':' output for A^2 * (I / 2 + A / 6 + A^2 / 24)','line_number':2319,'multiline':False]
['text':' contains A^2','line_number':2323,'multiline':False]
['text':' computes (I / 2 + A / 6 + A^2 / 24)','line_number':2325,'multiline':False]
['text':' I + A + A^2 * (I / 2 + A / 6 + A^2 / 24)','line_number':2332,'multiline':False]
['text':' 3 for {I, A, A^2}','line_number':2351,'multiline':False]
['text':' output for A4','line_number':2354,'multiline':False]
['text':' A4 =  A2 * (x1 * A + x2 * A2)','line_number':2356,'multiline':False]
['text':' As.select(0, 2) = A^2','line_number':2359,'multiline':False]
['text':' extract {A, A^2} from As','line_number':2362,'multiline':False]
['text':' output for A8','line_number':2368,'multiline':False]
['text':' A8 = (x3 * A2 + A4) * (x4 * I + x5 * A + x6 * A2 + x7 * A4)','line_number':2370,'multiline':False]
['text':' x3 * A2 + A4','line_number':2373,'multiline':False]
['text':' return I + A + y2 * A2 + A8;','line_number':2384,'multiline':False]
['text':' gather coefficients `b` from above into a tensor,','line_number':2420,'multiline':False]
['text':' and move them to device `device_of(A)`','line_number':2421,'multiline':False]
['text':' output for A6','line_number':2435,'multiline':False]
['text':' compute A6','line_number':2437,'multiline':False]
['text':' gather coefficients `b` from above into a tensor,','line_number':2492,'multiline':False]
['text':' and move them to device `device_of(A)`','line_number':2493,'multiline':False]
['text':' tmp buffer for this matrix product','line_number':2507,'multiline':False]
['text':' compute A9','line_number':2509,'multiline':False]
['text':' Scale','line_number':2529,'multiline':False]
['text':' We eventually need to do the matrix multiplication to calculate the result.','line_number':2530,'multiline':False]
['text':' For example, if we have `norm` equal to [27, 6, 6, 0.05], we will end up to','line_number':2531,'multiline':False]
['text':' get `s` as [4, 1, 1, 0], so we can use it to get the result by calculating','line_number':2532,'multiline':False]
['text':' matrix[0]^(2^4), matrix[1]^(2^1) and matrix[2]^(2^1) one by one to get the','line_number':2533,'multiline':False]
['text':' result, such "one by one calculation" will be quite slow.','line_number':2534,'multiline':False]
['text':'min=','line_number':2535,'multiline':True]
['text':' Sort:','line_number':2540,'multiline':False]
['text':' Consider inputs are square matrix, so if we first power `matrix 0,1,2`, then','line_number':2541,'multiline':False]
['text':' the remain thing will only be multiply `matrix 0` by (2^4 - 1) times, which','line_number':2542,'multiline':False]
['text':' gives us an opportunity to calculate the matrix multiplication in a batch.','line_number':2543,'multiline':False]
['text':' The first thing we need to do is sort tensor `s`, which will be helpful to','line_number':2544,'multiline':False]
['text':' do the matrix multiplication by range.','line_number':2545,'multiline':False]
['text':' With above example, `sorted_s` is [0, 1, 1, 4], we also will need the index','line_number':2547,'multiline':False]
['text':' info, so we can use it to compose the result back.','line_number':2548,'multiline':False]
['text':'dim=','line_number':2549,'multiline':True]
['text':' Then we call `unique_consecutive` and we will use it to split `sorted_s`,','line_number':2551,'multiline':False]
['text':' with above example, `split_counts` is [1, 2, 1].','line_number':2552,'multiline':False]
['text':'return_counts=','line_number':2553,'multiline':True]
['text':' We also need to know the index of the last element of each split, so we can','line_number':2554,'multiline':False]
['text':' know how many times we need to do the multiplication for each split matrix.','line_number':2555,'multiline':False]
['text':' Notice that, we will not need to calculate the actual pows, because we will','line_number':2556,'multiline':False]
['text':' use the cumulative matrix multiplication.','line_number':2557,'multiline':False]
['text':' With about example, `mul_times` will be [0, 1, 3].','line_number':2558,'multiline':False]
['text':'dim=','line_number':2559,'multiline':True]
['text':'min=','line_number':2560,'multiline':True]
['text':'prepend=','line_number':2561,'multiline':True]
['text':' Square','line_number':2563,'multiline':False]
['text':' We now will do the matrix muplication in a batch, with above example:','line_number':2571,'multiline':False]
['text':' 1. Multiply all matrices by 0 (`mul_times[0]`) times, then do `slice`','line_number':2572,'multiline':False]
['text':' to get the remain matrices by acc[1:] (`split_counts[0]`),','line_number':2573,'multiline':False]
['text':' 2. Multiply remain matrices by 1 times and slice to acc[2:]','line_number':2574,'multiline':False]
['text':' 3. Multiply remain matrices by 3 times and slice to acc[1:]','line_number':2575,'multiline':False]
['text':' All processed matrices will be stored in `output_pieces`.','line_number':2576,'multiline':False]
['text':' To avoid AMP autocasting caused by at::matmul','line_number':2581,'multiline':False]
['text':' Compose the result back','line_number':2589,'multiline':False]
['text':' To prevent undefined behavior which outputs "normal" result from a matrix','line_number':2607,'multiline':False]
['text':' contains NaN values, we put NaN values in `res`, so if input has NaN values,','line_number':2608,'multiline':False]
['text':' its computation will be skipped to return the NaN contained `res` directly.','line_number':2609,'multiline':False]
['text':' `norm_cpu` is used to decide which Tensors require which approximation','line_number':2612,'multiline':False]
['text':' based on their norm. This decision takes place on CPU.','line_number':2613,'multiline':False]
['text':' It requires moving data back and forth between devices when `a` is on CUDA,','line_number':2614,'multiline':False]
['text':' but at the cost of only one sigle CPU-CUDA synchronization (instead of 6),','line_number':2615,'multiline':False]
['text':' and better performance overall (benchmarked).','line_number':2616,'multiline':False]
['text':' nonzero returns a 2D tensor, hence squeeze(-1) to make it 1D','line_number':2631,'multiline':False]
['text':' nonzero returns a 2D tensor, hence squeeze(-1) to make it 1D','line_number':2645,'multiline':False]
['text':' matrix exponential','line_number':2671,'multiline':False]
['text':' squash batch dimensions to one dimension for simplicity','line_number':2673,'multiline':False]
['text':' deg 1','line_number':2679,'multiline':False]
['text':' deg 2','line_number':2680,'multiline':False]
['text':' deg 4','line_number':2681,'multiline':False]
['text':' deg 8','line_number':2682,'multiline':False]
['text':' deg 12','line_number':2683,'multiline':False]
['text':' deg 18','line_number':2684,'multiline':False]
['text':' if Double or ComplexDouble','line_number':2690,'multiline':False]
['text':' deg 1','line_number':2692,'multiline':False]
['text':' deg 2','line_number':2693,'multiline':False]
['text':' deg 4','line_number':2694,'multiline':False]
['text':' deg 8','line_number':2695,'multiline':False]
['text':' deg 12','line_number':2696,'multiline':False]
['text':' deg 18','line_number':2697,'multiline':False]
['text':' TODO This should be deprecated in favor of linalg_matrix_exp_differential','line_number':2705,'multiline':False]
['text':'      in FunctionsManual.cpp','line_number':2706,'multiline':False]
['text':' end anon namespace','line_number':2727,'multiline':False]
['text':' Computes the matrix exponential for a given batch of squared matrices.','line_number':2729,'multiline':False]
['text':' The implementaion is based on:','line_number':2730,'multiline':False]
['text':'','line_number':2731,'multiline':False]
['text':' Bader, P.; Blanes, S.; Casas, F.','line_number':2732,'multiline':False]
['text':' Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.','line_number':2733,'multiline':False]
['text':' Mathematics 2019, 7, 1174.','line_number':2734,'multiline':False]
['text':'','line_number':2735,'multiline':False]
['text':' Trivial cases','line_number':2742,'multiline':False]
['text':' Alias','line_number':2753,'multiline':False]
['text':' TODO This should be deprecated in favor of linalg_matrix_exp_differential','line_number':2758,'multiline':False]
['text':'      in FunctionsManual.cpp','line_number':2759,'multiline':False]
['text':' Casting a large integer to a double will just introduce an error for','line_number':2771,'multiline':False]
['text':' values larger than 10^53 (same for negative numbers), so that's fine.','line_number':2772,'multiline':False]
['text':' No need to handle opt_dtype explicitly as it is already encoded in the dtype of result','line_number':2775,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/52648','line_number':2777,'multiline':False]
['text':' Reductions always use `std::abs` to compute the absolute value. In the backward of this','line_number':2778,'multiline':False]
['text':' function, we need to locate the index that was selected as the largest value. To do so','line_number':2779,'multiline':False]
['text':' we do self.abs() == result to locate the index of the largest element.','line_number':2780,'multiline':False]
['text':' Now, self.abs() may dispatch to a vectorized implementation which gives sliiightly different','line_number':2781,'multiline':False]
['text':' results to the std::abs(std::complex<T>) implementation.','line_number':2782,'multiline':False]
['text':' As such, to be able to compute the correct index in the backward, we need to use self.abs()','line_number':2783,'multiline':False]
['text':' both in the forward and in the backward','line_number':2784,'multiline':False]
['text':' A','line_number':2801,'multiline':False]
['text':'low_precision','line_number':2803,'multiline':True]
['text':' dim','line_number':2805,'multiline':False]
['text':' wrap first to identify weird scenarios like A.ndim = 2, dim = (1, -1)','line_number':2807,'multiline':False]
['text':' dim is modified in place while wrapping it','line_number':2808,'multiline':False]
['text':' dtype','line_number':2812,'multiline':False]
['text':' Check ord first as it will be used in the dtype check of A','line_number':2822,'multiline':False]
['text':' Check A, dim, and dtype','line_number':2828,'multiline':False]
['text':'low_precision','line_number':2829,'multiline':True]
['text':' Move dims to the end','line_number':2833,'multiline':False]
['text':' 1, -1, inf, -inf','line_number':2843,'multiline':False]
['text':' The infty norm is like the 1 norm on the transposed matrix','line_number':2844,'multiline':False]
['text':' If the first reduction removes one dim from the front (dim_[0] < dim_[1]), after this','line_number':2849,'multiline':False]
['text':' reduction dim_[1] will be off by one','line_number':2850,'multiline':False]
['text':' fro / nuc','line_number':2875,'multiline':False]
['text':' Check ord first as it will be used in the dtype check of A','line_number':2882,'multiline':False]
['text':' Check A, dim, and dtype','line_number':2886,'multiline':False]
['text':'low_precision','line_number':2887,'multiline':True]
['text':' nuc','line_number':2891,'multiline':False]
['text':' Move dims to the end','line_number':2894,'multiline':False]
['text':' Numerical or None norms','line_number':2922,'multiline':False]
['text':' If ord=None, we'll always use the 2-norm or frob norm (which are the same) so we go through','line_number':2934,'multiline':False]
['text':' vector_norm','line_number':2935,'multiline':False]
['text':' Frobenius and nuclear norms','line_number':2959,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':2984,'multiline':False]
['text':'                              Frobenius Norm                                //','line_number':2985,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':2986,'multiline':False]
['text':' This frobenius norm is just wrong, but well','line_number':2997,'multiline':False]
['text':' Dispatch to at::norm as it is implemented for Sparse and MPS backends','line_number':3000,'multiline':False]
['text':' TODO Make the backends implement vector_norm and matrix_norm','line_number':3001,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':3022,'multiline':False]
['text':'                                Nuclear Norm                                //','line_number':3023,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':3024,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':3066,'multiline':False]
['text':'                              linalg.cond                                   //','line_number':3067,'multiline':False]
['text':'//////////////////////////////////////////////////////////////////////////////','line_number':3068,'multiline':False]
['text':' This function helps to dispatch norm computations depending on 'ord' of variant type','line_number':3071,'multiline':False]
['text':' fix multiplication of zero and infinity for NumPy compatibility','line_number':3082,'multiline':False]
['text':' Return zero for each matrix in the batch','line_number':3088,'multiline':False]
['text':' Numerical or None norms','line_number':3111,'multiline':False]
['text':' The default case is using 2-norm','line_number':3115,'multiline':False]
['text':' NumPy doesn't define the condition number for 0x0 matrices, we return 0.0 for such input','line_number':3121,'multiline':False]
['text':' If ord == None or ord == ±2','line_number':3127,'multiline':False]
['text':' singular values are sorted in descending order','line_number':3130,'multiline':False]
['text':'dim=','line_number':3131,'multiline':True]
['text':'start=','line_number':3131,'multiline':True]
['text':'length=','line_number':3131,'multiline':True]
['text':'dim=','line_number':3132,'multiline':True]
['text':'start=','line_number':3132,'multiline':True]
['text':'length=','line_number':3132,'multiline':True]
['text':' squeeze the result for NumPy compatibility','line_number':3139,'multiline':False]
['text':' ord == ±1 ord == ±inf','line_number':3143,'multiline':False]
['text':' ord == ±1','line_number':3144,'multiline':False]
['text':' ord == ±inf','line_number':3146,'multiline':False]
['text':' Frobenius or nuclear norms','line_number':3163,'multiline':False]
['text':' NumPy doesn't define the condition number for 0x0 matrices, we return 0.0 for such input','line_number':3169,'multiline':False]
['text':' calling matrix_norm with "nuc" on inputs with infinities raises an error','line_number':3175,'multiline':False]
['text':' therefore we use the mathematical definition of nuclear norm directly','line_number':3176,'multiline':False]
['text':' instead of going through the matrix_norm','line_number':3177,'multiline':False]
['text':' TODO: implement _out variant avoiding copy and using already allocated storage directly','line_number':3185,'multiline':False]
['text':'
  The idea is to reduce the problem to 2D square matrix inversion.
  Step 1. Calculate the shape of the result and the shape of the intermediate 2D matrix.
  Step 2. Reshape `self` to 2D matrix.
  Step 3. Invert the 2D matrix self.to_2D()
          There is no quick way to find out whether the matrix is invertible,
          so at this stage an error from at::inverse can be thrown.
          Note that for CUDA this causes cross-device memory synchronization that can be slow.
  Step 4. reshape the result.
  ','line_number':3198,'multiline':True]
['text':' self[ind:]','line_number':3210,'multiline':False]
['text':' self[:ind]','line_number':3212,'multiline':False]
['text':' Check whether the self tensor can be reshaped to the 2D square matrix','line_number':3218,'multiline':False]
['text':' Concatenate shape_ind_end and shape_start_ind to form the shape of the result','line_number':3223,'multiline':False]
['text':' self[ind:] + self[:ind]','line_number':3224,'multiline':False]
['text':' If the reshaped self is not invertible catch this error','line_number':3227,'multiline':False]
['text':'check_errors=','line_number':3228,'multiline':True]
['text':'is_matrix','line_number':3229,'multiline':True]
['text':' TODO: implement _out variant avoiding copy and using already allocated storage directly','line_number':3234,'multiline':False]
['text':'
  The idea is to reduce the problem to 2D matrix solve.
  Step 1. (optional) `self` is permuted with `dims` such that dimensions from `dims` are moved to the right.
  For example, if we have 4D input with the shape (1, 2, 3, 4) and dims=(0, 2),
  then the result of permutation would have the shape (2, 4, 1, 3).
  Step 2. reshape `self` to 2D matrix.
  Step 3. solve the matrix equation self.to_2D() @ result = other.to_1D()
  Step 4. reshape the result.
  ','line_number':3246,'multiline':True]
['text':' move dimensions of `self_` from `dims` to the end','line_number':3258,'multiline':False]
['text':' result_shape is self_.sizes[-(an-other.dim):]','line_number':3265,'multiline':False]
['text':' Check whether the self tensor can be reshaped to the 2D square matrix','line_number':3271,'multiline':False]
['text':' normally `other` would be flattened by at::linalg_solve expects 2D input','line_number':3278,'multiline':False]
['text':'
Calculates the Kronecker product between two Tensors.
','line_number':3342,'multiline':True]
['text':' namespace native','line_number':3353,'multiline':False]
['text':' namespace at','line_number':3354,'multiline':False]
