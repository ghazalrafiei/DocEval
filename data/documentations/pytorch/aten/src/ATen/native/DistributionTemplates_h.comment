['text':' ==================================================== Random ========================================================','line_number':27,'multiline':False]
['text':' The purpose of `update_from` and `update_to` is to find the closest valid int64_t number that can be used as actual `from`.','line_number':29,'multiline':False]
['text':' The current implementation of `random_` uses uint64_t arithmetics and casts the result to the target dtype(scalar_t).','line_number':30,'multiline':False]
['text':' This casting can result in generating numbers that happen to be greater or equal to `to` value. For instance:','line_number':31,'multiline':False]
['text':'','line_number':32,'multiline':False]
['text':'    auto actual = torch::empty({3, 3}, torch::half);','line_number':33,'multiline':False]
['text':'    actual.random_(0, 65504);','line_number':34,'multiline':False]
['text':'','line_number':35,'multiline':False]
['text':' If random's uint64_t arithmetics produces 65503 as a random value after casting to torch::half it becomes 65504','line_number':36,'multiline':False]
['text':' and violates the requirement that random value must be less than `to`. To resolve this issue `update_from` and `update_to`','line_number':37,'multiline':False]
['text':' moves `from` to the right and `to` to the left to the next closest value that won't go outside [from, to) after casting to','line_number':38,'multiline':False]
['text':' the target dtype. For `to` = 65504 it moves left for (1 << (log2(to) - 11 + 1)) = 32 and becomes 65472, which is previous','line_number':39,'multiline':False]
['text':' available number for torch::half dtype.','line_number':40,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-core.UndefinedBinaryOperatorResult)','line_number':52,'multiline':False]
['text':' NOLINTNEXTLINE(clang-analyzer-core.UndefinedBinaryOperatorResult)','line_number':69,'multiline':False]
['text':' Return earlier for not invoking kernel.','line_number':75,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/103418 for more details','line_number':76,'multiline':False]
['text':'includeBool=','line_number':113,'multiline':True]
['text':' [from, to)','line_number':130,'multiline':False]
['text':' [from, std::numeric_limits<int64_t>::max()]','line_number':145,'multiline':False]
['text':'includeBool=','line_number':154,'multiline':True]
['text':' [std::numeric_limits<int64_t>::lowest(), std::numeric_limits<int64_t>::max()]','line_number':170,'multiline':False]
['text':' range = 2^64','line_number':171,'multiline':False]
['text':' ==================================================== Normal ========================================================','line_number':178,'multiline':False]
['text':' variance for normal distribution of the real and imaginary values','line_number':200,'multiline':False]
['text':' is half of the input variance','line_number':201,'multiline':False]
['text':' CUDA NB: addcmul_out copies the tensor to be added into the output.','line_number':227,'multiline':False]
['text':' The previous function here was addcmul_out(output, mean_tensor, output, std, 1);','line_number':228,'multiline':False]
['text':' The third argument is not a constant reference and hence the samples in output are overwritten.','line_number':229,'multiline':False]
['text':' Consequently, the computation performed is mean_tensor + mean_tensor * std instead of mean_tensor + output * std','line_number':230,'multiline':False]
['text':' CUDA NB: addcmul_out copies the tensor to be added into the output.','line_number':241,'multiline':False]
['text':' The previous function here was addcmul_out(output, mean, output, std, 1);','line_number':242,'multiline':False]
['text':' The third argument is not a constant reference and hence the samples in output are overwritten.','line_number':243,'multiline':False]
['text':' Consequently, the computation performed is mean + mean * std instead of mean + output * std','line_number':244,'multiline':False]
['text':' ==================================================== Uniform =======================================================','line_number':274,'multiline':False]
['text':' ================================================== LogNormal =======================================================','line_number':304,'multiline':False]
['text':' =================================================== Geometric ======================================================','line_number':315,'multiline':False]
['text':' ================================================== Exponential =====================================================','line_number':326,'multiline':False]
['text':' ==================================================== Cauchy ========================================================','line_number':337,'multiline':False]
['text':' TODO: instead of variable name 'sigma', use 'gamma' or 'scale'','line_number':341,'multiline':False]
['text':' the variance, squared sigma, is undefined for cauchy distribution','line_number':342,'multiline':False]
['text':' ==================================================== Bernoulli =====================================================','line_number':351,'multiline':False]
['text':' result.resize_as_(self) requires self to have same dtype as result, so we','line_number':373,'multiline':False]
['text':' use resize_ instead.','line_number':374,'multiline':False]
['text':' TODO: Fix resize_as_. See pytorch/pytorch#11665.','line_number':375,'multiline':False]
['text':' namespace at::native::templates','line_number':385,'multiline':False]
