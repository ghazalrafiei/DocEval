['text':' Since either x1 or x2 could be broadcasted','line_number':49,'multiline':False]
['text':' This is to guarantee that the contiguous memory is passed to the backward pass','line_number':57,'multiline':False]
['text':'* This function does the fist part of the euclidean distance calculation
   * We divide it in two steps to simplify dealing with subgradients in the
   * backward step ','line_number':67,'multiline':True]
['text':' TODO: This is bad; this test should apply universally','line_number':88,'multiline':False]
['text':' 0 - default value. If p = 2 and r1 > 25 or r2 > 25 (these values are based on performance metrics),','line_number':92,'multiline':False]
['text':' it will try to compute distance using matrix multiplication approach','line_number':93,'multiline':False]
['text':' 1 - force to use matrix multiplication for p = 2','line_number':94,'multiline':False]
['text':' 2 - do not use matrix multiplication for p = 2','line_number':95,'multiline':False]
['text':' See Note [cdist relies on cdist_impl redispatching]','line_number':102,'multiline':False]
['text':' Keep this condition in sync with the condition at the Note','line_number':103,'multiline':False]
['text':'For batch calculation we expand all dimensions(except the last two) to one, with size that equals to product of them.','line_number':112,'multiline':False]
['text':'The last two dimensions will stay the same','line_number':113,'multiline':False]
['text':' See Note [cdist relies on cdist_impl redispatching]','line_number':138,'multiline':False]
['text':' Keep the condition above in sync with the condition at the Note','line_number':139,'multiline':False]
['text':' Special case for empty input: always call the version with explicit autograd to ensure the graph is properly connected','line_number':159,'multiline':False]
['text':' Note [cdist relies on cdist_impl redispatching]','line_number':164,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':165,'multiline':False]
['text':' This is for pytorch to figure the backward pass itself','line_number':166,'multiline':False]
['text':' when p=2.  Keep this condition in sync with the See Note reference','line_number':167,'multiline':False]
['text':' Broadcasting might generate non-contiguous Tensors, so handle it before doing checks','line_number':192,'multiline':False]
['text':' Compute the linearized batch size','line_number':207,'multiline':False]
['text':' Gracefully handle empty Tensors','line_number':210,'multiline':False]
['text':' Use x1.size() here and not the original size of _x1.size() as this gradient is not taking broadcasting into account','line_number':239,'multiline':False]
['text':' Broadcasting will be handled automatically by the autograd engine','line_number':240,'multiline':False]
['text':'
   * cosine_similarity(x1, x2) = <x1, x2> / (||x1|| * ||x2||)
   *
   * The current implementation is an improvement over the previous version.
   *
   * Previous implementation:
   * 1. Compute num = <x1, x2>,
   * 2. Compute denom = ||x1|| * ||x2||,
   * 3. Compute denom = max(denom, eps) to avoid division by zero,
   * 4. Return num / denom.
   *
   * Previous implementation has the following issues:
   * 1. Chance of losing precision in <x1, x2> when ||x1|| and ||x2|| are large.
   * 2. Chance of losing precision in ||x1|| * ||x2|| when ||x1|| and ||x2|| are large.
   * 3. Losing precision may cause |cosing_similarity(x1, x2)| > 1.0.
   *
   * Current implementation:
   * 1. Compute x1_normalized = x1 / max(||x1||, eps),
   *            x2_normalized = x2 / max(||x2||, eps),
   * 2. Return <x1_normalized, x2_normalized>.
   *
   * The current implementation improves over the previous one by:
   * 1. Making sure that <x1, x2> and ||x1|| * ||x2|| are not computed explicitly,
   *    hence avoiding floating point overflows.
   * 2. Both methods might have issues with computing ||x1|| and ||x2||, but for
   *    the current method this is the only source of the floating point imprecision.
   * 3. Makes sure |cosing_similarity(x1, x2)| <= 1.0.
   *
   ','line_number':275,'multiline':True]
['text':' We accept integral types (and bools lol) but vector_norm does not','line_number':308,'multiline':False]
['text':'încludeBool=','line_number':309,'multiline':True]
['text':'încludeBool=','line_number':310,'multiline':True]
['text':' We want to divide each tensor by its norm first, as it's more numerically stable.','line_number':317,'multiline':False]
['text':' This keeps the result between -1.0 and 1.0','line_number':318,'multiline':False]
['text':' We clone them, as we're going to modify them in-place','line_number':319,'multiline':False]
['text':' This allows the gradients to propagate propertly all the way to x1 and x2','line_number':320,'multiline':False]
['text':'dim=','line_number':321,'multiline':True]
['text':'keepdim=','line_number':321,'multiline':True]
['text':'dim=','line_number':322,'multiline':True]
['text':'keepdim=','line_number':322,'multiline':True]
['text':' namespace at::native','line_number':333,'multiline':False]
