['text':' #define TORCH_ASSERT_ONLY_METHOD_OPERATORS','line_number':1,'multiline':False]
['text':' dense_to_sparse_{csr,bsr,csc,bsc} common helpers','line_number':69,'multiline':False]
['text':' Preparation fo the N-D dense -> sparse compressed conversion.','line_number':71,'multiline':False]
['text':' The N-D input is converted to 3-D (single batch dim) where we check that the','line_number':72,'multiline':False]
['text':' product of batch dims is nonzero and for each batch the sparse matrix','line_number':73,'multiline':False]
['text':' contained within has the same number of non-zero elements.','line_number':74,'multiline':False]
['text':' The batches are joined along the compressed axis. The generation of indices','line_number':75,'multiline':False]
['text':' for this matrix can be performed in a single step followed by a single step','line_number':76,'multiline':False]
['text':' conversion to restore the batch dimension.','line_number':77,'multiline':False]
['text':' For inputs with more than 1 batch dim we flatten them out.','line_number':84,'multiline':False]
['text':' Input shape (b0, b1 ..., bn, r, c) -> (b0 * b1 * ... * bn, r ,c)','line_number':85,'multiline':False]
['text':' For informative messaging form the name of the function','line_number':90,'multiline':False]
['text':' to_sparse_{csr,csc,bsr,bsc}.','line_number':91,'multiline':False]
['text':' We want the message to match the function name so generate the','line_number':95,'multiline':False]
['text':' lowercase acronym for the layout','line_number':96,'multiline':False]
['text':' Compute the number of non-zero elements in the first batch, expand to full','line_number':100,'multiline':False]
['text':' size','line_number':101,'multiline':False]
['text':' We need to join batches into a matrix increasing the length of the','line_number':107,'multiline':False]
['text':' compressed axis. This allows us to create indices for a compressed matrix','line_number':108,'multiline':False]
['text':' and de-batch them later (two kernels). Otherwise we would have to create','line_number':109,'multiline':False]
['text':' indices for each batch individually requiring n_batch kernels. For csr/bsr,','line_number':110,'multiline':False]
['text':' we already have the batch dim adjacent to the compressed axis and can','line_number':111,'multiline':False]
['text':' flatten them together. For csc/bsc, we need to transpose first.','line_number':112,'multiline':False]
['text':' For BSR/CSR (b, r, c) -> (b*r, c)','line_number':113,'multiline':False]
['text':' For BSC/CSC (b, c, r) -> (r, b*c)','line_number':114,'multiline':False]
['text':' This function unfolds the compressed indices of a compressed sparse matrix','line_number':128,'multiline':False]
['text':' into a batched compressed sparse tensor.','line_number':129,'multiline':False]
['text':' This is analogous to an unflatten-like operation:','line_number':130,'multiline':False]
['text':' unflatten(0, {b, r}) for csr/bsr with input shape (r*b, c)','line_number':131,'multiline':False]
['text':'          (output shape (b, r, c))','line_number':132,'multiline':False]
['text':' unflatten(1, {b, c}).transpose(0,1) for csc/bsc with input shape (r, c*b)','line_number':133,'multiline':False]
['text':'          (output shape (r, b, c) unflatten, (b, r, c) unflatten + transpose)','line_number':134,'multiline':False]
['text':' This only operates on the compressed indices as the plain indices and values','line_number':135,'multiline':False]
['text':' can be manipulated as described above without special handling.','line_number':136,'multiline':False]
['text':' It is a prerequisite for the conversion that the sparsity pattern is sane for','line_number':137,'multiline':False]
['text':' the batched shape. That is each batch has the same number of nonzero','line_number':138,'multiline':False]
['text':' elements.','line_number':139,'multiline':False]
['text':' If the compressed dimension has length zero there is 1 element in each','line_number':150,'multiline':False]
['text':' batch and it is zero we already have this result formed','line_number':151,'multiline':False]
['text':' Slice the compressed indices ignoring the leading 0 element and reshape','line_number':153,'multiline':False]
['text':' to n-batch rows','line_number':154,'multiline':False]
['text':' Slice the compressed indices again selecting the elements corresponding','line_number':157,'multiline':False]
['text':' to the batch boundary. The values here will be increasing multiples of','line_number':158,'multiline':False]
['text':' nnz per batch. Reshape to n-batch rows (1 col) for broadcasting.','line_number':159,'multiline':False]
['text':' This is equivalent to arange(n_batch) * nnz_per_batch with the same','line_number':160,'multiline':False]
['text':' reshape','line_number':161,'multiline':False]
['text':' Subtracting the offsets from each row of the reshaped compressed indices','line_number':164,'multiline':False]
['text':' gives us the compressed indices within the batch. The leading element of','line_number':165,'multiline':False]
['text':' each row is not computed as it is always zero.  We copy into the view on','line_number':166,'multiline':False]
['text':' the output buffer.','line_number':167,'multiline':False]
['text':' After generating member tensors for sparse_compressed matrix, if the target','line_number':174,'multiline':False]
['text':' shape is N-D we must reform the batch dimensions.','line_number':175,'multiline':False]
['text':' Single kernel is used to restore one batch dimension in the compressed','line_number':176,'multiline':False]
['text':' indices. From there full batch shape is restored by reshape. No special','line_number':177,'multiline':False]
['text':' handling is needed for restoring batch dimensions of the values or','line_number':178,'multiline':False]
['text':' plain_indices it can be done with reshape/unflatten.','line_number':179,'multiline':False]
['text':' NOTE: using this conversion requires the nnz per batch is the same for all','line_number':189,'multiline':False]
['text':' batches that will be formed. We ensured this was the case on the way in so','line_number':190,'multiline':False]
['text':' it is safe to use this conversion.','line_number':191,'multiline':False]
['text':'out_int32','line_number':193,'multiline':True]
['text':' We can infer the last dim of the reshape targets, it will be nnz or','line_number':195,'multiline':False]
['text':' nrow/ncol+1 depending on the layout and member tensor targeted.','line_number':196,'multiline':False]
['text':' -1 will be nnz per batch','line_number':200,'multiline':False]
['text':' -1 will be ncols (bsc,csc) or nrows (bsr,csr) + 1','line_number':202,'multiline':False]
['text':' -1 will be nnz (per batch).','line_number':204,'multiline':False]
['text':' Note: Unflatten rather than reshape as it will work','line_number':205,'multiline':False]
['text':' for both blocked and unblocked layouts. reshape works for unblocked layouts','line_number':206,'multiline':False]
['text':' only','line_number':207,'multiline':False]
['text':' namespace','line_number':210,'multiline':False]
['text':' Take a Device that may not have device_index set (i.e., having it as -1','line_number':212,'multiline':False]
['text':' representing the current device) and return the corresponding Device','line_number':213,'multiline':False]
['text':' according to the actual device at the time of this function call.  No-op','line_number':214,'multiline':False]
['text':' if the device_index is set.','line_number':215,'multiline':False]
['text':' memory_format is handled separately due to MemoryFormat::Preserve logic','line_number':252,'multiline':False]
['text':' TODO: Use the dispatcher for this.','line_number':256,'multiline':False]
['text':' Currently there are unenumerated extensibility issues preventing this.','line_number':257,'multiline':False]
['text':' force copy since we are in _to_copy','line_number':275,'multiline':False]
['text':' force copy since we are in _to_copy','line_number':285,'multiline':False]
['text':' force copy since we are in _to_copy','line_number':295,'multiline':False]
['text':' See Note [Explicit nullopt MemoryFormat argument]','line_number':344,'multiline':False]
['text':' TODO: empty_quantized does not work here. It raises an exception in CheckMemoryFormat.h prior to','line_number':345,'multiline':False]
['text':' empty_affine_quantizd/_empty_per_channel_affine_quantized calls','line_number':346,'multiline':False]
['text':' at::empty also does not work here because there is no proper at::empty support for quantized tensors','line_number':347,'multiline':False]
['text':' as it would return a quantized tensor with an UnknownQuantizer','line_number':348,'multiline':False]
['text':' NOTE: static runtime's to_maybe_copy_out relies on details of this','line_number':364,'multiline':False]
['text':' check; if you change how it works, please update static runtime as','line_number':365,'multiline':False]
['text':' well.','line_number':366,'multiline':False]
['text':' fast path','line_number':394,'multiline':False]
['text':' If input tensor is fp32, cast it to fp16, otherwise leave it alone.','line_number':402,'multiline':False]
['text':' (this is intended to be used internally by the JIT autocast implementation)','line_number':403,'multiline':False]
['text':' If input tensor is fp16, cast it to fp32, otherwise leave it alone.','line_number':425,'multiline':False]
['text':' (this is intended to be used internally by the JIT autocast implementation)','line_number':426,'multiline':False]
['text':' This op is important primarily for lazy / graph-based backends.','line_number':497,'multiline':False]
['text':' While this vanilla implementation loops through each tensor and independently converts it to cpu,','line_number':498,'multiline':False]
['text':' a lazy backend like XLA might need to tell sync updates across tensors.','line_number':499,'multiline':False]
['text':'
    For historical reasons, to_dense backward implements masked
    semantics for sparse tensors, that is, gradients with respect to
    unspecified elements are ignored.  The masked_grad kw argument of
    to_dense is introduced to allow to_dense to be used in the
    non-masked semantics context. However, for BC reasons, the default
    value to masked_grad kw argument is set True as a first instance.
    Eventually, we should eliminate the masked_grad kw argument and
    let to_dense backward to behave according to non-masked
    semantics. Masked semantics of tensors is implemented in the
    framework of masked tensors.
  ','line_number':509,'multiline':True]
['text':' TODO: return grad as it is','line_number':525,'multiline':False]
['text':' Autograd operates on the coalesced assumption, i.e. no duplicate values.','line_number':528,'multiline':False]
['text':' TODO: return grad as it is','line_number':532,'multiline':False]
['text':' TODO: add efficient CSR/CSC support for sparse_mask','line_number':537,'multiline':False]
['text':' TODO: return grad as it is','line_number':541,'multiline':False]
['text':'blocksize=','line_number':542,'multiline':True]
['text':'dense_dim=','line_number':542,'multiline':True]
['text':' TODO: add efficient BSR/BSC support for sparse_mask','line_number':546,'multiline':False]
['text':' TODO: return grad as it is','line_number':551,'multiline':False]
['text':' Pad shape so we can treat non-batched like batched, we will','line_number':624,'multiline':False]
['text':' squeeze out the phantom batch dim at the end.','line_number':625,'multiline':False]
['text':' Flatten batch dims','line_number':632,'multiline':False]
['text':' At this point there is only one batch dim, existed already or was','line_number':639,'multiline':False]
['text':' flattened from multiple batch dims.  Now, reshape the resulting','line_number':640,'multiline':False]
['text':' dense matrix so that this single batch dim is joined with sparse','line_number':641,'multiline':False]
['text':' dims into a single dim, so that the remaining dims are only block','line_number':642,'multiline':False]
['text':' dims eventually, and then dense dims.','line_number':643,'multiline':False]
['text':' Calculate batch, row and column indices for non-zeros in the','line_number':661,'multiline':False]
['text':' sparse matrix, and use these to calculate correspoding indices','line_number':662,'multiline':False]
['text':' into the dense matrix reshaped as above.  Then, update dense','line_number':663,'multiline':False]
['text':' matrix by adding sparse matrix values into elements with indices','line_number':664,'multiline':False]
['text':' calculated this way.','line_number':665,'multiline':False]
['text':' Un-tile the result.  The final reshape uses the original','line_number':689,'multiline':False]
['text':' self.sizes() which will squeeze out the extra batch dim if we put','line_number':690,'multiline':False]
['text':' one in.','line_number':691,'multiline':False]
['text':' Computes the strides for view_dtype output when the view dtype is','line_number':702,'multiline':False]
['text':' smaller than the original dtype','line_number':703,'multiline':False]
['text':' Computes the strides for view_dtype output when the view dtype is','line_number':720,'multiline':False]
['text':' larger than the original dtype','line_number':721,'multiline':False]
['text':' Downsizing element size','line_number':770,'multiline':False]
['text':' Upsizing element size','line_number':786,'multiline':False]
['text':' This code turns a matrix into a sequence of blocks','line_number':819,'multiline':False]
['text':'','line_number':820,'multiline':False]
['text':' Given matrix','line_number':821,'multiline':False]
['text':'','line_number':822,'multiline':False]
['text':'  1  2  3  4','line_number':823,'multiline':False]
['text':'  5  6  7  8','line_number':824,'multiline':False]
['text':'  9 10 11 12','line_number':825,'multiline':False]
['text':' 14 15 16 17','line_number':826,'multiline':False]
['text':'','line_number':827,'multiline':False]
['text':' _tile_tensor(matrix, {2, 2}) will yield the following 2 by 2 blocks','line_number':828,'multiline':False]
['text':'','line_number':829,'multiline':False]
['text':'  1  2 |  3  4 |  9 10 | 11 12','line_number':830,'multiline':False]
['text':'  5  6 |  7  8 | 14 15 | 16 17','line_number':831,'multiline':False]
['text':'','line_number':832,'multiline':False]
['text':'  via a 4D Tensor of shape (2, 2, 2, 2)','line_number':833,'multiline':False]
['text':'','line_number':834,'multiline':False]
['text':' Same as _tile_tensor, just per matrix entry of self, if self is 3D.','line_number':852,'multiline':False]
['text':' This function returns a vector of the indices at which given','line_number':867,'multiline':False]
['text':' boolean mask is True. at::nonzero can achieve the same, but','line_number':868,'multiline':False]
['text':' we yet have to compare the performance difference.','line_number':869,'multiline':False]
['text':' Sparse layout conversions Start','line_number':895,'multiline':False]
['text':' Reshape values so that the block dims are explicitly added, and','line_number':1017,'multiline':False]
['text':' calculate a mask tensor that has only batch and sparse dims, and','line_number':1018,'multiline':False]
['text':' value true whenever sparse matrix has a non-zero element over','line_number':1019,'multiline':False]
['text':' corresponding block and dense dims, and false otherwise.','line_number':1020,'multiline':False]
['text':' Prepare for the conversion, in particular join the batch dims','line_number':1032,'multiline':False]
['text':' and the compressed dim into the single dim.','line_number':1033,'multiline':False]
['text':' Calculate sparse matrix row and col indices and then, depending','line_number':1038,'multiline':False]
['text':' on the target layout, corresponding compressed and sparse','line_number':1039,'multiline':False]
['text':' indices.  Use the mask tensor calculate above to generate sparse','line_number':1040,'multiline':False]
['text':' matrix values tensor.','line_number':1041,'multiline':False]
['text':'out_int32','line_number':1049,'multiline':True]
['text':'out_int32','line_number':1058,'multiline':True]
['text':' Restore the batch dims and compressed dim.','line_number':1067,'multiline':False]
['text':' Create compressed sparse matrix with the target layout.','line_number':1072,'multiline':False]
['text':' many sparse CUDA kernels require','line_number':1187,'multiline':False]
['text':' contiguity, see issue #12633','line_number':1188,'multiline':False]
['text':' In this cases, indices is a clone of nz, which is a tensor of shape (0,','line_number':1197,'multiline':False]
['text':' 1). Given sparse tensor invariants, values should be shape (1,)','line_number':1198,'multiline':False]
['text':' NOTE: errors on non-compressed sparse layouts.','line_number':1211,'multiline':False]
['text':' Suppose compressed_indices represent rows of an input in either','line_number':1214,'multiline':False]
['text':' CSR or BSR sparse compressed format.','line_number':1215,'multiline':False]
['text':' In order to convert a batched CSR/BSR index into a batched CSC/BSC index','line_number':1216,'multiline':False]
['text':' we perform the following steps:','line_number':1217,'multiline':False]
['text':' 1. Convert a sparse compressed index representing batches of matrices of','line_number':1218,'multiline':False]
['text':'    shape (b, r, c) to a sparse compressed index that represents a single','line_number':1219,'multiline':False]
['text':'    matrix of shape (b * r, c).','line_number':1220,'multiline':False]
['text':' 2. Turn the compressed indices of the matrix of shape (b * r, c) into','line_number':1221,'multiline':False]
['text':'    COO indices.','line_number':1222,'multiline':False]
['text':' 3. Map these COO indices into the COO indices of a matrix of shape (r, b * c)','line_number':1223,'multiline':False]
['text':'    such that if A is a matrix of shape (b * r, c) and B is a matrix of shape','line_number':1224,'multiline':False]
['text':'    (r, b * c) such that','line_number':1225,'multiline':False]
['text':'    A[(k * r):(k * r + r), :] = B[:, (k * c):(k * c + c)] for all k in arange(b),','line_number':1226,'multiline':False]
['text':'    then A[i, j] = B[i', j'].','line_number':1227,'multiline':False]
['text':'    This is equivalent to finding indices that match values of matrices','line_number':1228,'multiline':False]
['text':'    tiled vertically to values of the same matrices tiled horizontally.','line_number':1229,'multiline':False]
['text':' 4. Convert the COO indices to the CSC/BSC indices and form the output.','line_number':1230,'multiline':False]
['text':'','line_number':1231,'multiline':False]
['text':' NOTE: the reason behind vertical/horizontal tiling is to be able to transform','line_number':1232,'multiline':False]
['text':'       indices over all matrices in the batch in a single kernel call, since','line_number':1233,'multiline':False]
['text':'       all the existing coo <-> compressed indices conversion methods assume','line_number':1234,'multiline':False]
['text':'       a single matrix.','line_number':1235,'multiline':False]
['text':'','line_number':1236,'multiline':False]
['text':' CSC/BSC inputs are handled in a similar fashion with a "transposed" argument.','line_number':1237,'multiline':False]
['text':' See the comments below for detailed explanations on how exactly each step','line_number':1238,'multiline':False]
['text':' is performed.','line_number':1239,'multiline':False]
['text':' Insert fake batch dim for simplicity','line_number':1248,'multiline':False]
['text':' NOTE: these sparse_dim are true sparse dims only for CSR/CSC','line_number':1256,'multiline':False]
['text':' inputs.  And for BSR/BSC these are <true sparse dims> /','line_number':1257,'multiline':False]
['text':' <blocksize>.  In other words, sparse_dim stores ranges of valid','line_number':1258,'multiline':False]
['text':' indices in the row/col dims.','line_number':1259,'multiline':False]
['text':' batch_sizes_nonempty stores at least one, potentially fake, batch dimension.','line_number':1270,'multiline':False]
['text':' rebatch_sizes_nonempty is equivalent to batch_sizes_nonempty.push_back(-1),','line_number':1271,'multiline':False]
['text':' and is used to unflatten batch dimensions from a dimension of size','line_number':1272,'multiline':False]
['text':' (batch_numel * dim_size,) for some dim_size.','line_number':1273,'multiline':False]
['text':' Equivalent to (arange(batch_numel_nonzero).mul_(nnz)).reshape(batch_sizes_nonempty).','line_number':1283,'multiline':False]
['text':' We just compute it differently to use `add` kernel in place of `mul` for better','line_number':1284,'multiline':False]
['text':' performance.','line_number':1285,'multiline':False]
['text':' Step 1 for CSR/BSR inputs:','line_number':1295,'multiline':False]
['text':' Convert a sparse compressed index representing batches of matrices of','line_number':1296,'multiline':False]
['text':' shape (b, r, c) to a sparse compressed index that represents a single','line_number':1297,'multiline':False]
['text':' matrix of shape (b * r, c).','line_number':1298,'multiline':False]
['text':' The algorithm is identical for CSC/BSC inputs, with the batch dimensions','line_number':1299,'multiline':False]
['text':' flattened in the "transposed" dimension.','line_number':1300,'multiline':False]
['text':' Extract offsets only relevant for the first :-1 elements in a row/col.','line_number':1302,'multiline':False]
['text':' batch_offsets offsets each individual matrix row/col offsets by the total','line_number':1304,'multiline':False]
['text':' sum of nnz's of all the matrices with the smaller batch index.','line_number':1305,'multiline':False]
['text':' compressed_offsets + batch_offsets creates an offset vector for a 2d matrix','line_number':1308,'multiline':False]
['text':' that is stored in a compressed sparse format.','line_number':1309,'multiline':False]
['text':' By appending nnz * batch_numel_nonzero to (compressed_offsets + batch_offsets)','line_number':1314,'multiline':False]
['text':' a compressed index of a 2d matrix is formed.','line_number':1315,'multiline':False]
['text':' More involved for compressed indices, but pretty easy for plain_indices and values:','line_number':1319,'multiline':False]
['text':' just squash batch dimensions.','line_number':1320,'multiline':False]
['text':' NOTE: values are not 2d! They just represent values of a sparse compressed 2d matrix.','line_number':1322,'multiline':False]
['text':' Step 2 & 3:','line_number':1327,'multiline':False]
['text':'','line_number':1328,'multiline':False]
['text':' Turn the compressed indices of the matrix of shape (b * r, c) into COO indices.','line_number':1329,'multiline':False]
['text':'','line_number':1330,'multiline':False]
['text':' Map these COO indices into the COO indices of a matrix of shape (r, b * c)','line_number':1331,'multiline':False]
['text':' such that if A is a matrix of shape (b * r, c) and B is a matrix of shape','line_number':1332,'multiline':False]
['text':' (r, b * c) such that','line_number':1333,'multiline':False]
['text':' A[(k * r):(k * r + r), :] = B[:, (k * c):(k * c + c)] for all k in arange(b),','line_number':1334,'multiline':False]
['text':' then A[i, j] = B[i', j'].','line_number':1335,'multiline':False]
['text':' This is equivalent to finding indices that match values of matrices','line_number':1336,'multiline':False]
['text':' tiled vertically to values of the same matrices tiled horizontally.','line_number':1337,'multiline':False]
['text':' coo <-> sparse index conversions assume CSR/BSR inputs.','line_number':1339,'multiline':False]
['text':' To CSC/BSC inputs these indices will appear "transposed".','line_number':1340,'multiline':False]
['text':'transpose=','line_number':1347,'multiline':True]
['text':' Flip rows/cols for convenience.','line_number':1347,'multiline':False]
['text':' Convert COO indices of (b * r, c) to (r, b * c).','line_number':1348,'multiline':False]
['text':' It is a map (i, j) -> {','line_number':1349,'multiline':False]
['text':'    b = i // r','line_number':1350,'multiline':False]
['text':'    i' = i % r','line_number':1351,'multiline':False]
['text':'    j' = j + b * c','line_number':1352,'multiline':False]
['text':'    return (i', j')','line_number':1353,'multiline':False]
['text':' }','line_number':1354,'multiline':False]
['text':' NOTE: we used transposed=true above!','line_number':1355,'multiline':False]
['text':' Modify i, j in-place.','line_number':1359,'multiline':False]
['text':' Step 4:','line_number':1365,'multiline':False]
['text':' Convert the COO indices to the CSC/BSC indices and form the output.','line_number':1366,'multiline':False]
['text':' We need to sort COO indices along the "tranposed" dim to satisfy the','line_number':1367,'multiline':False]
['text':' invariant of sorted plain indices.','line_number':1368,'multiline':False]
['text':' Hash coo indices by converting 2d indices to linear offsets with','line_number':1369,'multiline':False]
['text':' more "weight" (aka stride) placed on the "transposed" dimension.','line_number':1370,'multiline':False]
['text':' Kill fake batch dim if it was inserted.','line_number':1394,'multiline':False]
['text':' namespace','line_number':1545,'multiline':False]
['text':'
 * Based on
 * https://github.com/scipy/scipy/blob/8a64c938ddf1ae4c02a08d2c5e38daeb8d061d38/scipy/sparse/sparsetools/csr.h
 * Modified to ensure sorted BSR column indices.
 ','line_number':1588,'multiline':True]
['text':' Tensor size along compressed dimension','line_number':1595,'multiline':False]
['text':' Tensor size along plain dimension','line_number':1596,'multiline':False]
['text':' Block size along compressed dimensions','line_number':1597,'multiline':False]
['text':' Block size along plain dimension','line_number':1598,'multiline':False]
['text':' Number of elements in dense dimensions','line_number':1599,'multiline':False]
['text':' All blocks are possible, that is, may be allocated if a single','line_number':1606,'multiline':False]
['text':' non-zero value lives within them. Otherwise they're not.','line_number':1607,'multiline':False]
['text':' Allocate pointers for all possible plain blocks plus 1','line_number':1609,'multiline':False]
['text':' Number of blocks along compressed dim','line_number':1615,'multiline':False]
['text':' Number of blocks along plain_dim','line_number':1617,'multiline':False]
['text':' Number of elements per block','line_number':1620,'multiline':False]
['text':' Number of blocks overall','line_number':1622,'multiline':False]
['text':' Iterate over blocks along compressed dim','line_number':1627,'multiline':False]
['text':' Iterate over blocks along plain dim to locate non-zero blocks,','line_number':1629,'multiline':False]
['text':' this guarantees sorted plain dim indices','line_number':1630,'multiline':False]
['text':' plain dim element index','line_number':1633,'multiline':False]
['text':' Iterate over compressed dim within block','line_number':1643,'multiline':False]
['text':' compressed dim index','line_number':1645,'multiline':False]
['text':' plain dim index','line_number':1647,'multiline':False]
['text':' Block corresponding to plain dim index','line_number':1649,'multiline':False]
['text':' Plain dim index within block','line_number':1651,'multiline':False]
['text':' Specific blocks entries should not be visited more than','line_number':1654,'multiline':False]
['text':' once.  Scipy code does an addition here. Why?','line_number':1655,'multiline':False]
['text':' A possible answer: Scipy code supports "uncoalesced CSR"','line_number':1656,'multiline':False]
['text':' format that allows repeated plain dim indices, and','line_number':1657,'multiline':False]
['text':' compressed and plain indices may be unsorted.','line_number':1658,'multiline':False]
['text':' Scipy code has','line_number':1664,'multiline':False]
['text':'
      for (I i = input_compressed_indices[C * block_c];
           i < input_compressed_indices[C * (block_c + 1)];
           i++) {
             blocks[input_plain_indices[i] / P] = 0;
           }
    ','line_number':1665,'multiline':True]
['text':' but we don't need it because the modified code (see the block_p','line_number':1672,'multiline':False]
['text':' loop above) does not need to evaluate `blocks[block_p] == 0`','line_number':1673,'multiline':False]
['text':' that the original code did.','line_number':1674,'multiline':False]
['text':'
 * Based on
 * https://github.com/scipy/scipy/blob/8a64c938ddf1ae4c02a08d2c5e38daeb8d061d38/scipy/sparse/sparsetools/csr.h
 ','line_number':1679,'multiline':True]
['text':' Tensor size along compressed dimension','line_number':1685,'multiline':False]
['text':' Tensor size along plain dimension','line_number':1686,'multiline':False]
['text':' Block size along compressed dimensions','line_number':1687,'multiline':False]
['text':' Block size along plain dimension','line_number':1688,'multiline':False]
['text':' Compressed indices','line_number':1689,'multiline':False]
['text':' Plain indices','line_number':1690,'multiline':False]
['text':' First we determine the number of blocks needed. For each given','line_number':1719,'multiline':False]
['text':' block, if it contains a non-zero element we will allocate values','line_number':1720,'multiline':False]
['text':' and indices for it.','line_number':1721,'multiline':False]
['text':' Next we copy over non-zero elements into the allocated blocks.','line_number':1748,'multiline':False]
['text':' Only CSR is trivially coalesced','line_number':1841,'multiline':False]
['text':' Scale indices that identify blocks to element-wise coordinates that correspond','line_number':1859,'multiline':False]
['text':' to the top-left corner of each block.','line_number':1860,'multiline':False]
['text':' Now that we know top-left block coordinates, we offset them with element-wise','line_number':1862,'multiline':False]
['text':' coordinates in the block to get the result.','line_number':1863,'multiline':False]
['text':' NOTE: indices is mapped from (dim, nnz) to (dim, nnz, 1),','line_number':1864,'multiline':False]
['text':' and block_coo_indices is mapped from (dim, block_numel) to','line_number':1865,'multiline':False]
['text':' (dim, 1, block_numel), so the result has shape','line_number':1866,'multiline':False]
['text':' (dim, nnz, block_numel).','line_number':1867,'multiline':False]
['text':' Squash the nnz and the block_numel dimension','line_number':1869,'multiline':False]
['text':' to produce valid nnz dimension of a COO tensor.','line_number':1870,'multiline':False]
['text':' BSRs not spanning across several rows produces coalesced results.','line_number':1875,'multiline':False]
['text':'dtype=','line_number':1889,'multiline':True]
['text':'masked_grad=','line_number':1889,'multiline':True]
['text':' Sparse layout conversions End','line_number':1986,'multiline':False]
['text':'dtype=','line_number':1990,'multiline':True]
['text':'layout=','line_number':1990,'multiline':True]
['text':'device=','line_number':1991,'multiline':True]
['text':'pin_memory=','line_number':1991,'multiline':True]
['text':' needs to handle wrapped numbers, so dtype promotion works properly.','line_number':1992,'multiline':False]
['text':' namespace native','line_number':2013,'multiline':False]
['text':' namespace at','line_number':2014,'multiline':False]
