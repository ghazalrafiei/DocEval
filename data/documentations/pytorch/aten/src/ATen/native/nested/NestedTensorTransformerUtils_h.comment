['text':'*
 * This function will take nested query, key, and value
 * and will preprocess it in order to run with either
 * the flash-attention or efficient-attention kernels.
 * @return A tuple containing all the necessary data for running the fused
 * kernels
 ','line_number':8,'multiline':True]
['text':'*
 * This function will take nested query, key, and value, grad_out, and out
 * and will preprocess it in order to run with either
 * the flash-attention or efficient-attention kernels backwards.
 * We use both functions to avoid having to do the same preprocessing
 * for cumulative_sequence_length_q and cumulative_sequence_length_kv
 * @return A tuple containing all the necessary data for running the fused
 * kernels
 ','line_number':21,'multiline':True]
['text':' namespace preprocessing','line_number':42,'multiline':False]
['text':' namespace native','line_number':43,'multiline':False]
['text':' namespace at','line_number':44,'multiline':False]
