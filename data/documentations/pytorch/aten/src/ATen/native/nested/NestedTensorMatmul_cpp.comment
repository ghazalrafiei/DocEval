['text':' dispatcher should have guaranteed that at least one is nested','line_number':29,'multiline':False]
['text':' create a contiguous output','line_number':47,'multiline':False]
['text':' call tensor mm','line_number':68,'multiline':False]
['text':' TODO: `padding nested tensor -> bmm -> remove padding` may be more efficient','line_number':69,'multiline':False]
['text':'       until we have specialized nested tensor bmm kernel','line_number':70,'multiline':False]
['text':'       useful resource: `aten/src/ATen/native/cpu/LinearAlgebra.cpp/bmm_out_or_baddbmm_`','line_number':71,'multiline':False]
['text':'                        `aten/src/ATen/native/cuda/Blas.cpp/baddbmm_out_cuda_impl`','line_number':72,'multiline':False]
['text':' Tensor self = self_.contiguous();','line_number':83,'multiline':False]
['text':' Tensor mat2 = mat2_.contiguous();','line_number':84,'multiline':False]
['text':' self [N, n_heads, *, head_dim]','line_number':85,'multiline':False]
['text':' mat2 [N, n_heads, head_dim, *]','line_number':86,'multiline':False]
['text':' metadata for self','line_number':89,'multiline':False]
['text':' metadata for mat2','line_number':95,'multiline':False]
['text':' viewed metadata for self','line_number':104,'multiline':False]
['text':' viewed metadata for mat2','line_number':113,'multiline':False]
['text':' view self as [N * n_heads, *, head_dim] (collapse first 2 dims)','line_number':150,'multiline':False]
['text':' view mat2 as [N * n_heads, head_dim, *] (collapse first 2_dims)','line_number':154,'multiline':False]
['text':' output [N * n_heads, *, *]','line_number':158,'multiline':False]
['text':' generate metadata for viewing output as [N, n_heads, *, *]','line_number':161,'multiline':False]
['text':' output of bmm should be contiguous so stride calculations should hold','line_number':162,'multiline':False]
['text':' nt: NT of shape (B, *, C, D)','line_number':193,'multiline':False]
['text':' other: dense tensor of shape (D, E)','line_number':194,'multiline':False]
['text':' output: NT of shape (B, *, C, E)','line_number':195,'multiline':False]
['text':' View nt buffer as 3D jagged for matmul','line_number':197,'multiline':False]
['text':' Wrap result into nested tensor','line_number':202,'multiline':False]
['text':' Note [nested tensor matmul]','line_number':214,'multiline':False]
['text':' This is really a generalized batched matmul dedicated to nested tensors,','line_number':215,'multiline':False]
['text':' where `self` and `mat2` have same number (>= 3) of dimensions.','line_number':216,'multiline':False]
['text':' The last 2 dimensions will be considered as matrix dimensions,','line_number':217,'multiline':False]
['text':' so they should be matrix-multiplicable.','line_number':218,'multiline':False]
['text':' The leading dimensions are considered as batch dimensions,','line_number':219,'multiline':False]
['text':' and since nested tensor does not support broadcasting for now,','line_number':220,'multiline':False]
['text':' for each batch dimension `self` and `mat2` must have same size.','line_number':221,'multiline':False]
['text':' TODO: Should make full matmul semantics support some day','line_number':222,'multiline':False]
['text':' special case of NT (B, *, C, D) with broadcasted dense (D, E)','line_number':224,'multiline':False]
['text':' to_padded_tensor only supports contiguous inputs','line_number':242,'multiline':False]
['text':' dispatcher should have guaranteed that at least one is nested','line_number':245,'multiline':False]
['text':' Ensure batch dimensions have the same sizes (no broadcasting).','line_number':264,'multiline':False]
['text':' Ensure last dim of self and second last dim of mat2 have the same size','line_number':275,'multiline':False]
['text':' use bmm inference-only fast path for [N, n_heads, *, head_dim] [N, n_heads, head_dim, *]','line_number':284,'multiline':False]
['text':' Construct output size from input sizes','line_number':298,'multiline':False]
['text':' The last entry in every row of output_sizes should be last column of mat2_sizes','line_number':300,'multiline':False]
['text':' TODO: this is a very quick and dirty implementation','line_number':311,'multiline':False]
['text':'       should improve it to avoid the intermediate memory usage','line_number':312,'multiline':False]
['text':' TODO: this is to reproduce function_result_ptr->opt_sizes_','line_number':315,'multiline':False]
['text':'       if an accessor is provided in the future, can replace this','line_number':316,'multiline':False]
['text':' namespace native','line_number':332,'multiline':False]
['text':' namespace at','line_number':333,'multiline':False]
