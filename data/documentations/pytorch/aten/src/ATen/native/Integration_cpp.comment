['text':' The estimated integral of a function y of x,','line_number':25,'multiline':False]
['text':' sampled at points (y_1, ..., y_n) that are separated by distance (dx_1, ..., dx_{n-1}),','line_number':26,'multiline':False]
['text':' is given by the trapezoid rule:','line_number':27,'multiline':False]
['text':'','line_number':28,'multiline':False]
['text':' \sum_{i=1}^{n-1}  dx_i * (y_i + y_{i+1}) / 2','line_number':29,'multiline':False]
['text':'','line_number':30,'multiline':False]
['text':' TODO: if we extend TensorIterator to accept 3 inputs,','line_number':31,'multiline':False]
['text':' we can probably make this a bit more performant.','line_number':32,'multiline':False]
['text':' If the dimensions of 'dx' and '(left + right)' do not match','line_number':36,'multiline':False]
['text':' broadcasting is attempted here.','line_number':37,'multiline':False]
['text':' When dx is constant, the above formula simplifies','line_number':41,'multiline':False]
['text':' to dx * [(\sum_{i=1}^n y_i) - (y_1 + y_n)/2]','line_number':42,'multiline':False]
['text':' Given the current shape of a Tensor and a target number of dimensions,','line_number':67,'multiline':False]
['text':' returns a new shape with the same values as the original shape,','line_number':68,'multiline':False]
['text':' but with '1's padded in the beginning to match the target number of dimensions.','line_number':69,'multiline':False]
['text':' For example, curr_shape = (5,5,5) and target_n_dim = 6 ==> (1,1,1,5,5,5)','line_number':70,'multiline':False]
['text':' Note that no padding will be added if the current shape has the greater than or equal','line_number':71,'multiline':False]
['text':' number of dimensions than the target numbers of dimensions.','line_number':72,'multiline':False]
['text':' asking for the integral with zero samples is a bit nonsensical,','line_number':88,'multiline':False]
['text':' but we'll return "0" to match numpy behavior.','line_number':89,'multiline':False]
['text':' Note that we explicitly choose not to broadcast 'x' to match the shape of 'y' here because','line_number':95,'multiline':False]
['text':' we want to follow NumPy's behavior of broadcasting 'dx' and 'dy' together after the differences are taken.','line_number':96,'multiline':False]
['text':' This step takes 'x' with dimension (n,), and returns 'x_view' with','line_number':98,'multiline':False]
['text':' dimension (1,1,...,n,...,1,1) based on dim and y.dim() so that, later on, 'dx'','line_number':99,'multiline':False]
['text':' can be broadcast to match 'dy' at the correct dimensions.','line_number':100,'multiline':False]
['text':' shape = [1] * y.','line_number':102,'multiline':False]
['text':' shape[axis] = d.shape[0]','line_number':103,'multiline':False]
['text':' When 'y' has more dimension than 'x', this step takes 'x' with dimension (n_1, n_2, ...),','line_number':106,'multiline':False]
['text':' and add '1's as dimensions in front to become (1, 1, ..., n_1, n_2), matching the dimension of 'y'.','line_number':107,'multiline':False]
['text':' This allows the subsequent slicing operations to proceed with any 'dim' without going out of bound.','line_number':108,'multiline':False]
['text':' Note the .slice operation reduces the dimension along 'dim' by 1,','line_number':114,'multiline':False]
['text':' while the sizes of other dimensions are untouched.','line_number':115,'multiline':False]
['text':' see above','line_number':124,'multiline':False]
['text':' See trapezoid for implementation notes','line_number':146,'multiline':False]
['text':' shape = [1] * y.','line_number':148,'multiline':False]
['text':' shape[axis] = d.shape[0]','line_number':149,'multiline':False]
['text':' See trapezoid for implementation notes','line_number':152,'multiline':False]
['text':' namespace at::native','line_number':172,'multiline':False]
