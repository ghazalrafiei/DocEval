['text':' Parse environment variable "TORCH_LINEAR_FLATTEN_3D"','line_number':39,'multiline':False]
['text':' Uninitialized value','line_number':41,'multiline':False]
['text':' `_flatten_nd_linear` flattens all but the last dimension of the input tensor','line_number':54,'multiline':False]
['text':' before passing it to linear operation','line_number':55,'multiline':False]
['text':' can't use -1 in reshape because it errors when a dimension is 0','line_number':58,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':73,'multiline':False]
['text':' Fused op is marginally faster.','line_number':87,'multiline':False]
['text':' Also hit the fused path for contiguous 3D input, if not using xla','line_number':91,'multiline':False]
['text':' backend. Reshaping/flattening has some performance implications on xla.','line_number':92,'multiline':False]
['text':' If user forces flattening via env var','line_number':98,'multiline':False]
['text':' for composite compliance use out-of-place version of `add`','line_number':105,'multiline':False]
['text':'level','line_number':107,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':118,'multiline':False]
['text':' Fused op is marginally faster.','line_number':124,'multiline':False]
['text':' sumproduct_pair computes `(left*right).sum(sumdims)` by means of permutation and','line_number':134,'multiline':False]
['text':' batch matrix multiplication','line_number':135,'multiline':False]
['text':' its main purpose is to provide a pairwise reduction for einsum','line_number':136,'multiline':False]
['text':' assumes that tensors have been pre-unsqueezed (so that all dimensions match - after broadcasting)','line_number':138,'multiline':False]
['text':' but makes no other assumptions on the order of dimensions','line_number':139,'multiline':False]
['text':' dimensions that will be part of the output (i.e. not summed over) in three vectors:','line_number':145,'multiline':False]
['text':' dims in lro appear in left, right and output, similarly, lo: left and output, ro: right and output','line_number':146,'multiline':False]
['text':' also the sizes are kept track of for reshaping','line_number':147,'multiline':False]
['text':' first dimensions that will be summed over after multiplication','line_number':155,'multiline':False]
['text':' dimensions nontrivially in both left and right must be of the same size','line_number':156,'multiline':False]
['text':' if it is only in one of left and right, we can sum right away','line_number':159,'multiline':False]
['text':' now deal with dimensions that will be in the output','line_number':164,'multiline':False]
['text':' dimensions nontrivially in both left and right must be of the same size','line_number':165,'multiline':False]
['text':' keep track of dimensions appearing only once','line_number':169,'multiline':False]
['text':' we now work with the following permutations / shapes.','line_number':177,'multiline':False]
['text':' the pipeline is permute inputs -> reshape inputs -> batch matrix mul -> reshape(view) output -> permute output','line_number':178,'multiline':False]
['text':' output: "lro, lo, 1-for-summed-dims, ro" with original shape dimensions','line_number':179,'multiline':False]
['text':' left:   "lro, lo, summed" permuted with lpermutation and the three flattened','line_number':180,'multiline':False]
['text':' right:  "lro, summed, ro" permuted with rpermutation and the three flattened','line_number':181,'multiline':False]
['text':' then the permuted output is a view of bmm(left, right)','line_number':182,'multiline':False]
['text':' finally, opermutation reverts the permutation to the original order of dimensions','line_number':183,'multiline':False]
['text':' avoid warning about not using d','line_number':189,'multiline':False]
['text':' now we can execute the operations above','line_number':220,'multiline':False]
['text':' finally squeeze summed dimensions if desired','line_number':226,'multiline':False]
['text':' There are roughly three parts to computing einsum:','line_number':239,'multiline':False]
['text':' 1. Parse equation to extract the labels for each input operand and output','line_number':240,'multiline':False]
['text':' 2. Unsqueeze missing dimensions from input operands and permute to align them','line_number':241,'multiline':False]
['text':' 3. Compute result by multiplying input operands and summing contraction','line_number':242,'multiline':False]
['text':'    dimensions. We do the last part by reducing to bmm.','line_number':243,'multiline':False]
['text':' If a path is specified, we reduce in the order specified by the path, else we','line_number':244,'multiline':False]
['text':' default to going left => right. The path is a list of indices processed the same','line_number':245,'multiline':False]
['text':' way as opt-einsum: https://optimized-einsum.readthedocs.io/en/stable/path_finding.html#format-of-the-path','line_number':246,'multiline':False]
['text':' Labels must be in range [A-Za-z]','line_number':261,'multiline':False]
['text':' Code used to identify ELLIPSIS ("...")','line_number':265,'multiline':False]
['text':' Convert label in [A-Za-z] to subscript in [0, TOTAL_LABELS)','line_number':268,'multiline':False]
['text':' Convert subscript in [0, TOTAL_LABELS) to label in [A-Za-z]','line_number':273,'multiline':False]
['text':' Find arrow (->) to split equation into lhs and rhs','line_number':278,'multiline':False]
['text':' Convert labels for input operands into an index in [0, 52) and store','line_number':282,'multiline':False]
['text':' them in op_labels for each operand along with ELLIPSIS if present.','line_number':283,'multiline':False]
['text':' Ignore spaces','line_number':291,'multiline':False]
['text':' Only one ellipsis per operand can be given','line_number':296,'multiline':False]
['text':' Ensure it's a valid ellipsis','line_number':302,'multiline':False]
['text':' Move onto next operand','line_number':312,'multiline':False]
['text':' Parse label','line_number':321,'multiline':False]
['text':' The maximum number of dimensions covered by any ellipsis, needed when','line_number':337,'multiline':False]
['text':' unsqueezing missing dimensions from operands to permute and broadcast','line_number':338,'multiline':False]
['text':' Compute label frequency and number of dimensions covered by ellipsis','line_number':341,'multiline':False]
['text':' We do this after parsing labels to make it more readable and simpler','line_number':342,'multiline':False]
['text':' to compute the number of dimensions covered by ellipsis.','line_number':343,'multiline':False]
['text':' We want to align the dimensions of every input tensor to have','line_number':373,'multiline':False]
['text':' shape out_dims + sum_dims. For this, we create a mapping of label','line_number':374,'multiline':False]
['text':' to index into the permuted shape.','line_number':375,'multiline':False]
['text':' Current index in the permuted shape','line_number':378,'multiline':False]
['text':' Start index of ellipsis dimensions in the permuted shape','line_number':381,'multiline':False]
['text':' Implicit output is ellipsis (...) + labels seen only once','line_number':386,'multiline':False]
['text':' ell_in_output is used to stop us from reducing ellipses dims later','line_number':388,'multiline':False]
['text':' Parse explicit output','line_number':396,'multiline':False]
['text':' Ignore spaces','line_number':402,'multiline':False]
['text':' There can only be one ellipsis in the output','line_number':407,'multiline':False]
['text':' Ensure ellipsis is correct','line_number':411,'multiline':False]
['text':' Ensure label appeared at least once for some input operand and at','line_number':427,'multiline':False]
['text':' most once for the output','line_number':428,'multiline':False]
['text':' Save number of dimensions in output before adding contraction dims (dims to sum out)','line_number':440,'multiline':False]
['text':' If ellipsis is not part of the output, add to contraction dimensions','line_number':443,'multiline':False]
['text':' Add contraction labels (labels not present in output)','line_number':449,'multiline':False]
['text':' Next: we check the sizes, take diagonals for repeated labels, unsqueeze','line_number':456,'multiline':False]
['text':' missing dimensions so all operands have the same dimensions and permute','line_number':457,'multiline':False]
['text':' the operands to align the dimensions following the indices computed above.','line_number':458,'multiline':False]
['text':' We also count how many operands have dimension with size != 1 for each','line_number':459,'multiline':False]
['text':' label used to identify which dimensions can be contracted.','line_number':460,'multiline':False]
['text':' Iterate over each dimension covered by ellipsis','line_number':471,'multiline':False]
['text':' Update ellipsis size','line_number':475,'multiline':False]
['text':' Update subscript','line_number':494,'multiline':False]
['text':' Repeated label, take diagonal','line_number':510,'multiline':False]
['text':' Add dimensions for missing labels','line_number':526,'multiline':False]
['text':' Contract','line_number':539,'multiline':False]
['text':' Collect dimensions that can be summed now','line_number':566,'multiline':False]
['text':' Sum multiple dims at a time to minimize the number of kernel calls to sum','line_number':587,'multiline':False]
['text':' Sum out contraction dims','line_number':602,'multiline':False]
['text':' if there were ops to contract, we would have already done so','line_number':604,'multiline':False]
['text':' in the previous loop and all the dims to sum are now 1','line_number':605,'multiline':False]
['text':' NB: use view instead of squeeze (or sum) for faster (mps) performance','line_number':606,'multiline':False]
['text':' _trilinear computes a trilinear einstein sum with an unrolled dimension','line_number':623,'multiline':False]
['text':' the result is `(i1.unsqueeze(expand1)*i2.unsqueeze(expand2)*i2.unsqueeze(expand3)).sum(sumdim)`','line_number':624,'multiline':False]
['text':' the computation is unrolled in the unroll_dim dimension','line_number':625,'multiline':False]
['text':' its main purpose is to unify the computations in bilinear and bilinear_backward','line_number':626,'multiline':False]
['text':' asserts...','line_number':642,'multiline':False]
['text':' Three conditionals are necessary since this function is meant to work for both','line_number':674,'multiline':False]
['text':' forward and backward, which changes the dimensions of the inputs.','line_number':675,'multiline':False]
['text':' Note that if output has zero elems is because (at least) one of i1, i2, i3 has zero elems.','line_number':676,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':703,'multiline':False]
['text':' implements tensordot, a matrix-multiplication-like contraction, but the dimensions given','line_number':735,'multiline':False]
['text':' in the two dimension lists','line_number':736,'multiline':False]
['text':' total size of the contracted dimensions','line_number':740,'multiline':False]
['text':' broadcasted dimensions can be summed right away','line_number':746,'multiline':False]
['text':' p1, p2: input permutations','line_number':759,'multiline':False]
['text':' rsizes: sizes of the result','line_number':760,'multiline':False]
['text':' number of non-contracted elements in input1','line_number':764,'multiline':False]
['text':' number of non-contracted elements in input2','line_number':765,'multiline':False]
['text':' fill the permutations and compute sizes','line_number':767,'multiline':False]
['text':' permut and reshape for matrix multiplication','line_number':788,'multiline':False]
['text':' multiply and reshape to target size','line_number':791,'multiline':False]
['text':' check if the input & output tensors are on the same device.','line_number':802,'multiline':False]
['text':' check if the computed result has the same dtype as the out tensor','line_number':808,'multiline':False]
['text':' (because tensordot does not support type promotion)','line_number':809,'multiline':False]
['text':' namespace at::native','line_number':819,'multiline':False]
