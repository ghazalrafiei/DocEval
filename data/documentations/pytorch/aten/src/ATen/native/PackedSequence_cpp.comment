['text':' This method returns `(data, batch_sizes)`, which are then passed into a','line_number':29,'multiline':False]
['text':' `PackedSequence` constructor.','line_number':30,'multiline':False]
['text':' `data` can be on arbitrary device and of arbitrary dtype, but `batch_sizes`','line_number':31,'multiline':False]
['text':' must be a CPU int64 tensor.','line_number':32,'multiline':False]
['text':' See NOTE [ device and dtype of a PackedSequence ]','line_number':33,'multiline':False]
['text':' NB: enforce_sorted is implemented at a Python level, but the sortedness','line_number':51,'multiline':False]
['text':' check lives here. If enforce_sorted=False then this error should never','line_number':52,'multiline':False]
['text':' get called.','line_number':53,'multiline':False]
['text':' == [-1, *input.shape[2:]]','line_number':66,'multiline':False]
['text':' To understand what's going on in this loop imagine that the input is a padded 2D','line_number':75,'multiline':False]
['text':' array that looks like this (x = valid entry, . = padding)','line_number':76,'multiline':False]
['text':'','line_number':77,'multiline':False]
['text':'  1 1 1 1 1','line_number':78,'multiline':False]
['text':'  2 2 2 . .','line_number':79,'multiline':False]
['text':'  2 2 2 . .','line_number':80,'multiline':False]
['text':'  4 . . . .','line_number':81,'multiline':False]
['text':'  4 . . . .','line_number':82,'multiline':False]
['text':'','line_number':83,'multiline':False]
['text':' Where the vertical dimension corresponds to time, and horizontal dim to batch.','line_number':84,'multiline':False]
['text':' In this example, the lengths array will be equal to [5, 3, 3, 1, 1], and we will','line_number':85,'multiline':False]
['text':' iterate over them in reverse order (from the rightmost column to the left).','line_number':86,'multiline':False]
['text':' We want to avoid eager slicing of the input at every time step, and wait for','line_number':87,'multiline':False]
['text':' the moments where the length increases. In this example, that will happen at the','line_number':88,'multiline':False]
['text':' first, second and fourth steps. Then, we slice out the whole block of the input','line_number':89,'multiline':False]
['text':' that corresponds to this length, and hasn't been sliced yet (the steps at which each','line_number':90,'multiline':False]
['text':' element is sliced are annotated in the array above).  You can think of this as if we','line_number':91,'multiline':False]
['text':' were scanning the sequences from the shortest one, and every time we realize there's','line_number':92,'multiline':False]
['text':' more elements below in our column, we lower the counter (prev_l), and append the new','line_number':93,'multiline':False]
['text':' block to the output.','line_number':94,'multiline':False]
['text':' `grad` could be on arbitrary device and of arbitrary dtype, but `_batch_sizes`','line_number':112,'multiline':False]
['text':' is guaranteed to be a CPU int64 tensor.','line_number':113,'multiline':False]
['text':' See NOTE [ device and dtype of a PackedSequence ]','line_number':114,'multiline':False]
['text':' NOTE: this op advertises as CompositeImplicitAutograd, but uses data_ptr().','line_number':126,'multiline':False]
['text':' we should fix this.','line_number':127,'multiline':False]
['text':' == [max_seq_length, max_batch_size, *var_data.size()[1:]]','line_number':158,'multiline':False]
['text':' This will be modified at every iteration, but we reserve memory for it now.','line_number':168,'multiline':False]
['text':' == [-1, -1, *var_data.size()[1:]]','line_number':169,'multiline':False]
['text':' The lines below are equivalent to this:','line_number':180,'multiline':False]
['text':' output[prev_i:i, :prev_batch_size] = tmp.view(i - prev_i, prev_batch_size, *input.shape[2:])','line_number':181,'multiline':False]
['text':' use index notation to prevent duplicate references to the tensor','line_number':230,'multiline':False]
['text':' namespace at::native','line_number':240,'multiline':False]
