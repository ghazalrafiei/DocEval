['text':' Copyright (c) 2018 MathInf GmbH, Thomas Viehmann','line_number':1,'multiline':False]
['text':' Licensed under the BSD-3-Clause license','line_number':2,'multiline':False]
['text':' This is the CPU implementation of the Connectionist Temporal Loss.','line_number':3,'multiline':False]
['text':' We mostly follow Graves.','line_number':4,'multiline':False]
['text':' 1. Graves et al: http://www.cs.toronto.edu/~graves/icml_2006.pdf','line_number':5,'multiline':False]
['text':' We use the equations from above link, but note that [1] has 1-based indexing and we (of course) use 0-based.','line_number':6,'multiline':False]
['text':' Graves et al call the probabilities y, we use log_probs (also calling them inputs)','line_number':7,'multiline':False]
['text':' this ad-hoc converts from targets (l in [1]) to augmented targets (l' in [1]) note that no bound-checking is done','line_number':46,'multiline':False]
['text':' log_probs: input_len x batch_size x num_labels','line_number':58,'multiline':False]
['text':' targets [int64]: batch_size x target_length OR sum(target_lengths)','line_number':59,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':74,'multiline':False]
['text':' concatenated targets','line_number':78,'multiline':False]
['text':' batch x max_target_length','line_number':89,'multiline':False]
['text':' dim is 2','line_number':90,'multiline':False]
['text':' This kernel is a relatively straightforward implementation of the alpha calculation in the forward backward algorithm (section 4.1).','line_number':116,'multiline':False]
['text':' A (minor) twist is that we are using log-calculations to enhance numerical stability (log_probs and log_alpha).','line_number':117,'multiline':False]
['text':' The function returns the loss and the alphas, the alphas are kept for the backward step. The wrapper (ctc_loss below) hides','line_number':118,'multiline':False]
['text':' the alphas from the user by only returning the loss.','line_number':119,'multiline':False]
['text':' log_probs: input_len x batch_size x num_labels','line_number':122,'multiline':False]
['text':' targets [int64]: batch_size x target_length OR sum(target_lengths)','line_number':123,'multiline':False]
['text':' alpha calculation for the first row, the three equations for alpha_1 above eq (6)','line_number':148,'multiline':False]
['text':' first the default','line_number':149,'multiline':False]
['text':' the first two items of alpha_t above eq (6)','line_number':159,'multiline':False]
['text':' now the loop over the inputs','line_number':164,'multiline':False]
['text':' this loop over s could be parallel/vectorized, too, but the required items are one index apart','line_number':168,'multiline':False]
['text':' alternatively, one might consider moving s to the outer loop to cache current_target_prime more (but then it needs to be descending)','line_number':169,'multiline':False]
['text':' for the cuda implementation, that gave a speed boost.','line_number':170,'multiline':False]
['text':' This is eq (6) and (7), la1,2,3 are the three summands. We keep track of the maximum for the logsumexp calculation.','line_number':171,'multiline':False]
['text':' cannot do neginf-neginf','line_number':191,'multiline':False]
['text':' this is the assignment of eq (6)','line_number':193,'multiline':False]
['text':' the likelihood is the sum of the last two alphas, eq (8), the loss is the negative log likelihood','line_number':197,'multiline':False]
['text':' if the target is empty then there is no preceding BLANK state and hence there is no path to merge','line_number':199,'multiline':False]
['text':' This is the backward. It consists of two phases:','line_number':215,'multiline':False]
['text':' a) computing the beta analogous to the alphas in the forward (backward half of the forward-backward algorithm) (eq (10) and (11))','line_number':216,'multiline':False]
['text':' b) collecting the per-activation characters for all s and wrapping the gradient (eq (16), the collection is the sum)','line_number':217,'multiline':False]
['text':' at this point, this is log of empty sum','line_number':226,'multiline':False]
['text':' The admin bits. We don't do much checking and assume that the forward did.','line_number':228,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':229,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':231,'multiline':False]
['text':' concatenated targets','line_number':235,'multiline':False]
['text':' batch x max_target_length','line_number':246,'multiline':False]
['text':' dim is 2','line_number':247,'multiline':False]
['text':' could be optimized to use only 2 rows','line_number':256,'multiline':False]
['text':' Fill is idempotent, so overlap is okay','line_number':267,'multiline':False]
['text':'squash_dims=','line_number':274,'multiline':True]
['text':'squash_dims=','line_number':275,'multiline':True]
['text':'squash_dims=','line_number':276,'multiline':True]
['text':' grad_batch.zero_();','line_number':287,'multiline':False]
['text':' the initialization of beta before eq (10)','line_number':300,'multiline':False]
['text':' here we do the fill for each batch item separately, as the input lengths will differ, so the t in which','line_number':301,'multiline':False]
['text':' we start varies','line_number':302,'multiline':False]
['text':' log_beta.select(0, b).select(1, input_length-1).fill_(neginf);','line_number':304,'multiline':False]
['text':' the first two are a blank and a non-blank, so we know they are different and we don't need to do log+','line_number':316,'multiline':False]
['text':' now loop applying eq (10) / (11)','line_number':321,'multiline':False]
['text':' this loop over s could be parallel/vectorized and doesn't really need to be descending...','line_number':323,'multiline':False]
['text':' alternatively, one might consider moving s to the outer loop to cache current_target_prime more (but then it needs to be descending)','line_number':324,'multiline':False]
['text':' for the cuda implementation, that gave a speed boost.','line_number':325,'multiline':False]
['text':' one might check whether one can vectorize this better when done after the t-loop...','line_number':350,'multiline':False]
['text':' now that we have beta, we fill in the sum of alpha*beta in eq (16)','line_number':351,'multiline':False]
['text':' in contrast to the cuda implementation, we only parallelize over the batch, so we don't have a concurrency','line_number':352,'multiline':False]
['text':' issue (several s can map to the same target character)','line_number':353,'multiline':False]
['text':' collected[b, t, target'[s]] "log+=" log_alpha[t, s]+log_beta[t, s]','line_number':354,'multiline':False]
['text':' now grad has the sum of eq (16)','line_number':366,'multiline':False]
['text':' now we wrap up the calculation by adding in the remaining items of eq (16)','line_number':367,'multiline':False]
['text':' this could be a great target for further vectorization.','line_number':368,'multiline':False]
['text':' grad is the output gradient, nll is the loss. Note that the likelihood -nll is the Z of eq (16)','line_number':369,'multiline':False]
['text':' or go for the full thing?','line_number':371,'multiline':False]
['text':' zero the remainder','line_number':379,'multiline':False]
['text':' grad_batch.select(0, l).zero_();','line_number':381,'multiline':False]
['text':' namespace','line_number':390,'multiline':False]
['text':' only used for backwards','line_number':393,'multiline':False]
['text':' only used for backwards','line_number':409,'multiline':False]
['text':'includeBool=','line_number':421,'multiline':True]
['text':'includeBool=','line_number':422,'multiline':True]
['text':'includeBool=','line_number':454,'multiline':True]
['text':'includeBool=','line_number':456,'multiline':True]
['text':' this wrapper function dispatches to the native and cudnn implementations and hides the alpha/grad from the user (by just returning the loss)','line_number':479,'multiline':False]
['text':' the gradient is implemented for _cudnn_ctc_loss (just in derivatives.yaml) and _ctc_loss and this function has automatic gradients','line_number':480,'multiline':False]
['text':' it also handles the reduction if desired','line_number':481,'multiline':False]
['text':' non-deterministic ctc loss on cudnn disabled due to inconsistent results','line_number':493,'multiline':False]
['text':' see: https://github.com/pytorch/pytorch/issues/21680','line_number':494,'multiline':False]
['text':'deterministic=','line_number':495,'multiline':True]
['text':' if the targets are on CPU (which you need for CuDNN, let's move them to','line_number':497,'multiline':False]
['text':' GPU as a service for the user)','line_number':498,'multiline':False]
['text':' namespace','line_number':519,'multiline':False]
['text':' Convenience function accepting Tensors','line_number':525,'multiline':False]
['text':' Composite Compliant path for TensorSubclasses','line_number':529,'multiline':False]
['text':' Fast path (which accesses data_ptr) and less operator dispatches for','line_number':533,'multiline':False]
['text':' regular tensors','line_number':534,'multiline':False]
['text':'includeBool=','line_number':535,'multiline':True]
['text':'includeBool=','line_number':536,'multiline':True]
['text':' at::native','line_number':545,'multiline':False]
