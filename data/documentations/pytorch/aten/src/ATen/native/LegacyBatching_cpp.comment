['text':' Adds a batch dimension to the tensor `self` out-of-place','line_number':13,'multiline':False]
['text':' Returns a Tensor with batch dim with level `level` turned into a regular dimension,','line_number':30,'multiline':False]
['text':' as well as a logical dim index of where said dimension is in the returned tensor.','line_number':31,'multiline':False]
['text':' A call to this function is always followed by a call to `movedim`.','line_number':32,'multiline':False]
['text':'','line_number':33,'multiline':False]
['text':' Preconditions: A BatchDim with level `level` must exist inside `batched`.','line_number':34,'multiline':False]
['text':'','line_number':35,'multiline':False]
['text':' The reason why we want to return the index of where said dimension is in the returned','line_number':36,'multiline':False]
['text':' tensor is because we want to keep track of which dimension used to be the batch','line_number':37,'multiline':False]
['text':' dimension so that we can move it to the correct logical dimension specified by','line_number':38,'multiline':False]
['text':' `out_dims` in vmap. For example, if we had','line_number':39,'multiline':False]
['text':' >>> x = torch.randn(2, 3, 5)','line_number':40,'multiline':False]
['text':' >>> vmap(lambda x: x, in_dims=0, out_dims=1)(x)','line_number':41,'multiline':False]
['text':' then right when we are about to exit the vmap block, x is a BatchedTensor with a','line_number':42,'multiline':False]
['text':' batch dimension at (physical) index 0. Note that the batch dimension doesn't','line_number':43,'multiline':False]
['text':' always have to exist at (physical) index 0. When we undo the batch dimension,','line_number':44,'multiline':False]
['text':' we want to move it to dimension 1 (as specified by out_dims). So we return the','line_number':45,'multiline':False]
['text':' index at which the batch dim appears so that we can move it to the correct place.','line_number':46,'multiline':False]
['text':' later down the line via a call to `movedim`.','line_number':47,'multiline':False]
['text':' Because a BatchDim with level `level` must exist inside `batched,','line_number':65,'multiline':False]
['text':' we should have found a `newly_exposed_logical_dim`.','line_number':66,'multiline':False]
['text':' at::movedim but may return the original tensor if dst is the same as src.','line_number':79,'multiline':False]
['text':' Removes the batch dim with level `level` from `self`. If this causes the','line_number':90,'multiline':False]
['text':' last batch dim to be removed from a BatchedTensor, then this returns a','line_number':91,'multiline':False]
['text':' regular Tensor.','line_number':92,'multiline':False]
['text':'','line_number':93,'multiline':False]
['text':' If the `level` of the batch dim to remove does not exist in `self`, then we','line_number':94,'multiline':False]
['text':' add the batch dim in. This can happen if `self` didn't interact with a tensor','line_number':95,'multiline':False]
['text':' inside the vmap level, for example,','line_number':96,'multiline':False]
['text':'     self = torch.randn(3)','line_number':97,'multiline':False]
['text':'     y = torch.randn(5)','line_number':98,'multiline':False]
['text':'     out = vmap(lambda x: vmap(lambda y: x)(y))(self)','line_number':99,'multiline':False]
['text':'     assert out.shape == (3, 5)','line_number':100,'multiline':False]
['text':' Inside the inner vmap, `x` is a BatchedTensor with a single batch dimension','line_number':101,'multiline':False]
['text':' corresponding to the *outer* vmap level and it doesn't have any dimensions that','line_number':102,'multiline':False]
['text':' correspond to the inner vmap level so we need to create one for the user.','line_number':103,'multiline':False]
['text':'','line_number':104,'multiline':False]
['text':' `out_dim` controls where we should put the batch dimension in the output tensor.','line_number':105,'multiline':False]
['text':' Must be batched if has_level(self, /*any_level*/)','line_number':114,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':119,'multiline':False]
['text':' namespace native','line_number':125,'multiline':False]
['text':' namespace at','line_number':126,'multiline':False]
