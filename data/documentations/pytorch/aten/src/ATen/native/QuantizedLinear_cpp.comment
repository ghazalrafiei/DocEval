['text':' USE_FBGEMM','line_number':32,'multiline':False]
['text':' namespace caffe2','line_number':36,'multiline':False]
['text':' Required for cpp_custom_type_hack to work','line_number':40,'multiline':False]
['text':' namespace caffe2','line_number':43,'multiline':False]
['text':' USE_FBGEMM','line_number':44,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':59,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':60,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':61,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':71,'multiline':False]
['text':' Calculate statistics for quantization of the input Tensor','line_number':82,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':83,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':85,'multiline':False]
['text':'m=','line_number':88,'multiline':True]
['text':'min=','line_number':89,'multiline':True]
['text':'max=','line_number':90,'multiline':True]
['text':'len=','line_number':91,'multiline':True]
['text':' Input tensor is quantized as 8-bit unsigned values','line_number':93,'multiline':False]
['text':' Calculate scale and zero point for quantization of input tensor','line_number':98,'multiline':False]
['text':'min=','line_number':100,'multiline':True]
['text':'max=','line_number':101,'multiline':True]
['text':'qmin=','line_number':102,'multiline':True]
['text':'qmax=','line_number':103,'multiline':True]
['text':'preserve_sparsity=','line_number':104,'multiline':True]
['text':' ReQuantizeForFloat requires pointers to the scale and zero point values,','line_number':107,'multiline':False]
['text':' since in the case of rowwise quantization these will be arrays rather than','line_number':108,'multiline':False]
['text':' scalars. But in this case, we're doing whole-tensor quantization so we just','line_number':109,'multiline':False]
['text':' pass a pointer to the scale values (and internally ReQuantizeFor Float','line_number':110,'multiline':False]
['text':' won't index past 0','line_number':111,'multiline':False]
['text':' Allocate output Tensor and a buffer for fbgemmPacked to use','line_number':119,'multiline':False]
['text':' Pull out the PackBMatrix instance from the owning tensor','line_number':125,'multiline':False]
['text':' This operation does the following:','line_number':131,'multiline':False]
['text':' 1) Quantizes the input matrix given the statistics we've calculated','line_number':132,'multiline':False]
['text':'    above.','line_number':133,'multiline':False]
['text':' 2) Creates a "row buffer" vector with offset values that must be added','line_number':134,'multiline':False]
['text':'    to the integer matrix multiplication operation to ensure correctness.','line_number':135,'multiline':False]
['text':' 3) Packs the resulting quantized matrix into vector-register and cache','line_number':136,'multiline':False]
['text':'    friendly tiles.','line_number':137,'multiline':False]
['text':'','line_number':138,'multiline':False]
['text':'  Note this is not executed eagerly, but rather within the fbgemmPacked','line_number':139,'multiline':False]
['text':'  call below.','line_number':140,'multiline':False]
['text':'trans=','line_number':142,'multiline':True]
['text':'nRow=','line_number':143,'multiline':True]
['text':'nCol=','line_number':144,'multiline':True]
['text':'smat=','line_number':145,'multiline':True]
['text':'ld=','line_number':146,'multiline':True]
['text':'pmat=','line_number':147,'multiline':True]
['text':' pack_a manages ownership of `pmat`','line_number':147,'multiline':False]
['text':'scale=','line_number':148,'multiline':True]
['text':'zero_pt=','line_number':149,'multiline':True]
['text':' This is the end of the pipeline, pass the resulting matrix through','line_number':151,'multiline':False]
['text':' After the uint8 * int8 matrix multiplication is performed, this','line_number':154,'multiline':False]
['text':' operation does:','line_number':155,'multiline':False]
['text':'  1) Add in row and column offsets to the rows and columns, respectively','line_number':156,'multiline':False]
['text':'  2) Dequantize the results into floating point','line_number':157,'multiline':False]
['text':'  3) Add in the bias term','line_number':158,'multiline':False]
['text':' FUSE_RELU ','line_number':159,'multiline':True]
['text':'nextop=','line_number':160,'multiline':True]
['text':'Aq_scale=','line_number':161,'multiline':True]
['text':'Bq_scale=','line_number':162,'multiline':True]
['text':'Aq_zero_point=','line_number':163,'multiline':True]
['text':'Bq_zero_point=','line_number':164,'multiline':True]
['text':'row_offsets=','line_number':165,'multiline':True]
['text':'col_offsets=','line_number':166,'multiline':True]
['text':'bias=','line_number':167,'multiline':True]
['text':'nCol=','line_number':168,'multiline':True]
['text':' Do the GEMM','line_number':169,'multiline':False]
['text':'packA=','line_number':171,'multiline':True]
['text':'packB=','line_number':172,'multiline':True]
['text':'C=','line_number':173,'multiline':True]
['text':'C_buffer=','line_number':174,'multiline':True]
['text':'ldc=','line_number':175,'multiline':True]
['text':'outProcess=','line_number':176,'multiline':True]
['text':'thread_id=','line_number':177,'multiline':True]
['text':'num_threads=','line_number':178,'multiline':True]
['text':' Calculate the column offsets','line_number':205,'multiline':False]
['text':' Note this includes the sum of the columns as well as the scalar term','line_number':206,'multiline':False]
['text':' B_zero_point * K, whereas the row_offsets created by','line_number':207,'multiline':False]
['text':' PackAWithQuantRowOffset is only the sum of the A rows.','line_number':208,'multiline':False]
['text':' namespace','line_number':224,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':231,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':232,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':233,'multiline':False]
['text':' Calculate weight statistics','line_number':237,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':238,'multiline':False]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':240,'multiline':False]
['text':'m=','line_number':243,'multiline':True]
['text':'min=','line_number':244,'multiline':True]
['text':'max=','line_number':245,'multiline':True]
['text':'len=','line_number':246,'multiline':True]
['text':' Choose parameters for quantizing the weight as 8-bit signed integer','line_number':248,'multiline':False]
['text':'min=','line_number':253,'multiline':True]
['text':'max=','line_number':254,'multiline':True]
['text':'qmin=','line_number':255,'multiline':True]
['text':'qmax=','line_number':256,'multiline':True]
['text':'preserve_sparsity=','line_number':257,'multiline':True]
['text':' Tensor quantized = at::native::empty_cpu(','line_number':267,'multiline':False]
['text':'     weight_contig.sizes(), weight_contig.options().dtype(at::kChar));','line_number':268,'multiline':False]
['text':'LEGACY','line_number':269,'multiline':True]
['text':'src=','line_number':270,'multiline':True]
['text':'dst=','line_number':271,'multiline':True]
['text':'len=','line_number':272,'multiline':True]
['text':'qparams=','line_number':273,'multiline':True]
['text':' Calculate column offsets of the weight and store them away in a tensor.','line_number':275,'multiline':False]
['text':' Similarly to quantization, this can be done once and cached.','line_number':276,'multiline':False]
['text':'K=','line_number':285,'multiline':True]
['text':'N=','line_number':286,'multiline':True]
['text':'Bint8=','line_number':287,'multiline':True]
['text':'B_zero_point=','line_number':288,'multiline':True]
['text':'col_offsets=','line_number':289,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':299,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':300,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':301,'multiline':False]
['text':'trans=','line_number':308,'multiline':True]
['text':'nRow=','line_number':309,'multiline':True]
['text':'nCol=','line_number':310,'multiline':True]
['text':'smat=','line_number':311,'multiline':True]
['text':'ld=','line_number':312,'multiline':True]
['text':'pmat=','line_number':313,'multiline':True]
['text':' PackBMatrix manages ownership of pmat','line_number':313,'multiline':False]
['text':'groups=','line_number':314,'multiline':True]
['text':' Replace after https://github.com/pytorch/pytorch/issues/24354 is fixed','line_number':322,'multiline':False]
['text':' TORCH_WARN(','line_number':323,'multiline':False]
['text':'     "fbgemm_pack_quantized_matrix(weight, K, N) will be deprecated soon."','line_number':324,'multiline':False]
['text':'     "Please use fbgemm_pack_quantized_matrix(weight) instead.");','line_number':325,'multiline':False]
['text':' Convert raw 16 bits half precision floating point number','line_number':332,'multiline':False]
['text':' to single precision floating point number.','line_number':333,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':340,'multiline':False]
['text':' 0.0009765625f = 0x1p-10 = 2^-10','line_number':341,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':342,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':345,'multiline':False]
['text':' The range for using FP16 quantization of weights requires that the elements','line_number':362,'multiline':False]
['text':' should be in the range of [5.96e-8, 65504]. If it is out of range, then the','line_number':363,'multiline':False]
['text':' number will be saturated to max or min representable values by FP16.','line_number':364,'multiline':False]
['text':' namespace','line_number':378,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':384,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':385,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':386,'multiline':False]
['text':' TODO(mingzhe09088):','line_number':395,'multiline':False]
['text':' Consider using a functor here in PackedGemmMatrixFP16','line_number':396,'multiline':False]
['text':' Comments from (XQ): Not entirely sure this make_unique is safe. make_unique','line_number':397,'multiline':False]
['text':' is created with regular "new", and freed through TypeMetaData::deleteFn in','line_number':398,'multiline':False]
['text':' this function. This is perfectly fine if the tensors are created and freed','line_number':399,'multiline':False]
['text':' within this translation unit. It might be very problematic if that tensor','line_number':400,'multiline':False]
['text':' flows across dll boundaries.','line_number':401,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':419,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':420,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':421,'multiline':False]
['text':' Pull out the PackedGemmMatrixFP16 instance from the owning tensor','line_number':427,'multiline':False]
['text':' NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)','line_number':438,'multiline':False]
['text':' Call the fp16 gemm interface','line_number':445,'multiline':False]
['text':' Add bias term','line_number':454,'multiline':False]
['text':' USE_FBGEMM','line_number':468,'multiline':False]
['text':'input','line_number':471,'multiline':True]
['text':'weight','line_number':472,'multiline':True]
['text':'packed','line_number':473,'multiline':True]
['text':'col_offsets','line_number':474,'multiline':True]
['text':'weight_scale','line_number':475,'multiline':True]
['text':'weight_zero_point','line_number':476,'multiline':True]
['text':'bias','line_number':477,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':481,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':482,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':483,'multiline':False]
['text':'input','line_number':489,'multiline':True]
['text':'weight','line_number':490,'multiline':True]
['text':'packed','line_number':491,'multiline':True]
['text':'col_offsets','line_number':492,'multiline':True]
['text':'weight_scale','line_number':493,'multiline':True]
['text':'weight_zero_point','line_number':494,'multiline':True]
['text':'bias','line_number':495,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':499,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':500,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':501,'multiline':False]
['text':'weight','line_number':507,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':511,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':512,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':513,'multiline':False]
['text':'input','line_number':518,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':522,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':523,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':524,'multiline':False]
['text':'input','line_number':530,'multiline':True]
['text':'K','line_number':531,'multiline':True]
['text':'N','line_number':532,'multiline':True]
['text':' We make a strong guarantee that models using these operators will have the','line_number':536,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':537,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':538,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':547,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':548,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':549,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':561,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':562,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':563,'multiline':False]
['text':' We make a strong guarantee that models using these operators will have the','line_number':575,'multiline':False]
['text':' same numerics across different machines. Therefore, we do not provide a','line_number':576,'multiline':False]
['text':' fallback path and rather fail loudly if we cannot run FBGEMM.','line_number':577,'multiline':False]
['text':' USE_FBGEMM','line_number':582,'multiline':False]
['text':' namespace native','line_number':584,'multiline':False]
['text':' namespace at','line_number':585,'multiline':False]
