['text':' TODO: Reduce this extra TensorIterator construction for Reduction::Mean & Sum.','line_number':79,'multiline':False]
['text':' We do another TensorIterator construction in the IMPL for the two cases.','line_number':80,'multiline':False]
['text':' namespace meta','line_number':101,'multiline':False]
['text':' For Composite Compliance,','line_number':185,'multiline':False]
['text':' In Forward AD, if `margin_diff` is a CCT but its tangent isn't,','line_number':186,'multiline':False]
['text':' using inplace clamp_min doesn't work because we end up writing','line_number':187,'multiline':False]
['text':' the CCT in-place to the tangent','line_number':188,'multiline':False]
['text':'level','line_number':189,'multiline':True]
['text':' The distance swap is described in the paper "Learning shallow','line_number':212,'multiline':False]
['text':' convolutional feature descriptors with triplet losses" by V. Balntas, E.','line_number':213,'multiline':False]
['text':' Riba et al.  If True, and if the positive example is closer to the','line_number':214,'multiline':False]
['text':' negative example than the anchor is, swaps the positive example and the','line_number':215,'multiline':False]
['text':' anchor in the loss computation.','line_number':216,'multiline':False]
['text':' For Composite Compliance,','line_number':227,'multiline':False]
['text':' In Forward AD, if `margin_diff` is a CCT but its tangent isn't,','line_number':228,'multiline':False]
['text':' using inplace clamp_min doesn't work because we end up writing','line_number':229,'multiline':False]
['text':' the CCT in-place to the tangent','line_number':230,'multiline':False]
['text':'level','line_number':231,'multiline':True]
['text':'include_bool','line_number':241,'multiline':True]
['text':'include_bool','line_number':242,'multiline':True]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':254,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':264,'multiline':False]
['text':' Binary cross entropy tensor is defined by the equation:','line_number':289,'multiline':False]
['text':' L = -w (y ln(x) + (1-y) ln(1-x))','line_number':290,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':308,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':318,'multiline':False]
['text':' The gradient is the partial derivative of BCELoss','line_number':335,'multiline':False]
['text':' with respect to x','line_number':336,'multiline':False]
['text':' d(L)/d(x) = -w (y - x) / (x - x^2)','line_number':337,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':356,'multiline':False]
['text':' pos_weight need to be broadcasted, thus mul(target) is not inplace.','line_number':365,'multiline':False]
['text':' inplace version of: grad_input = -norm * target * z / (1. + z) * grad_output;','line_number':399,'multiline':False]
['text':' compute inplace variant of: output = at::log1p(at::exp(-input * target));','line_number':416,'multiline':False]
['text':' namespace at::native','line_number':513,'multiline':False]
