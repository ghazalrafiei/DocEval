['text':' namespace native','line_number':64,'multiline':False]
['text':' namespace at','line_number':65,'multiline':False]
['text':' AT_MKLDNN_ENABLED','line_number':67,'multiline':False]
['text':' Use mkldnn post ops to perform the add.','line_number':104,'multiline':False]
['text':' NOTE: View as c-contiguous to avoid extra reordering in mkldnn','line_number':109,'multiline':False]
['text':' Use identity: C = AB <=> C^T = B^T A^T','line_number':110,'multiline':False]
['text':'sizes=','line_number':125,'multiline':True]
['text':'strides=','line_number':127,'multiline':True]
['text':'sizes=','line_number':130,'multiline':True]
['text':'strides=','line_number':132,'multiline':True]
['text':'sizes=','line_number':135,'multiline':True]
['text':'strides=','line_number':137,'multiline':True]
['text':' ideep will query onednn expect format of output','line_number':145,'multiline':False]
['text':' if given output format is not expected, ideep will re-init an output buffer','line_number':146,'multiline':False]
['text':' under this case, we need copy the re-inited buffer back to given buffer','line_number':147,'multiline':False]
['text':'sizes=','line_number':149,'multiline':True]
['text':'strides=','line_number':151,'multiline':True]
['text':' aten::addmm','line_number':188,'multiline':False]
['text':' aten::bmm, aten::baddbmm','line_number':189,'multiline':False]
['text':' aten::mv','line_number':190,'multiline':False]
['text':' aten::dot','line_number':191,'multiline':False]
['text':' oneDNN fast-maths mode (enabled by setting the environment variable ONEDNN_DEFAULT_FPMATH_MODE=BF16) will dispatch','line_number':195,'multiline':False]
['text':' fp32 inputs to bf16 kernels where HW permits. So, both fp32 and bf16 inputs are permitted.','line_number':196,'multiline':False]
['text':' device needs to support bf16 if the inputs are of bf16 type','line_number':200,'multiline':False]
['text':' "addmm", "addbmm" "baddbmm" in pytorch allow bias to be 2-D or 3-D tensor','line_number':229,'multiline':False]
['text':' but mkldnn matmul primitive only support bias be 1-D tensors','line_number':230,'multiline':False]
['text':' to address their differences, we use mkldnn post ops to perform a fused "add" after matrix multiplication is over','line_number':231,'multiline':False]
['text':' If alpha = 0, dose not need actually do gemm computation','line_number':233,'multiline':False]
['text':' dim = 3','line_number':244,'multiline':False]
['text':' Mkldnn only optimized for contiguous or transposed (transpose last 2 dim if 3-D tensor) format now','line_number':249,'multiline':False]
['text':' Will remove this "contiguous" after mkldnn have fully supported','line_number':250,'multiline':False]
['text':' Make sure mat1 and mat2 have default contiguous strides if they are contiguous tensors for better performance.','line_number':253,'multiline':False]
['text':' mkldnn_matmul only proceed CPU tensor','line_number':257,'multiline':False]
['text':' ideep will query onednn expect format of output','line_number':264,'multiline':False]
['text':' if given output format is not expected, ideep will re-init an output buffer','line_number':265,'multiline':False]
['text':' under this case, we need copy the re-inited buffer back to given buffer','line_number':266,'multiline':False]
['text':' aten::dot','line_number':272,'multiline':False]
['text':' if dim = 2, mat1's size = (m * n), mat2's size = (n * k)','line_number':279,'multiline':False]
['text':' else if dim = 3, mat1's size = (b * m * n), mat2's size = (b * n * k)','line_number':280,'multiline':False]
['text':' else called from aten::mv, mat1.size = (m * n), mat2.size = (n)','line_number':281,'multiline':False]
['text':' only m * n * b * k(if exist) are large enough we can get benefit from mkldnn optimized gemm kernel','line_number':282,'multiline':False]
['text':' aten::dot','line_number':285,'multiline':False]
['text':' aten::mv','line_number':288,'multiline':False]
['text':' aten::addmm','line_number':291,'multiline':False]
['text':' aten::bmm, aten::baddbmm','line_number':294,'multiline':False]
['text':'onednn fastmath mode can leverage bf16 HW even for the fp32 input, e.g. Arm Neoverse V1','line_number':305,'multiline':False]
['text':'so, don't restrict the mkldnn_matmul only for bf16 inputs, allow it for float as well','line_number':306,'multiline':False]
['text':' namespace native','line_number':350,'multiline':False]
['text':' namespace at','line_number':351,'multiline':False]
['text':' AT_MKLDNN_ENABLED','line_number':353,'multiline':False]
