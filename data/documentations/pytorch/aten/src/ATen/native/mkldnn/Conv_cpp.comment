['text':' AT_MKLDNN_ENABLED','line_number':82,'multiline':False]
['text':' follow check rules from native/Convolution.cpp without transpose supported','line_number':91,'multiline':False]
['text':' log new kernel size considering dilation','line_number':140,'multiline':False]
['text':' If kernel size is incorrect','line_number':150,'multiline':False]
['text':' Note [MKLDNN Convolution Memory Formats]','line_number':172,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':173,'multiline':False]
['text':' MKLDNN has 3 types of memory formats in convolution:','line_number':174,'multiline':False]
['text':'','line_number':175,'multiline':False]
['text':' In case memory format passed from PyTorch (aka. user layout)','line_number':176,'multiline':False]
['text':' differs from the internal layout which MKLDNN used, a `reorder` is needed;','line_number':177,'multiline':False]
['text':' otherwise when user layout is identical to internal layout,','line_number':178,'multiline':False]
['text':' MKLDNN uses a memory `view` upon an existing CPU tensor.','line_number':179,'multiline':False]
['text':'','line_number':180,'multiline':False]
['text':' 1. NCHW (CPU tensor, contiguous)','line_number':181,'multiline':False]
['text':'  input reorder:  NCHW(user) -> Blocked(internal)','line_number':182,'multiline':False]
['text':'  weight reorder: OIHW(user) -> Blocked(internal)','line_number':183,'multiline':False]
['text':'  output reorder: Blocked(internal) -> NCHW(user)','line_number':184,'multiline':False]
['text':'','line_number':185,'multiline':False]
['text':' 2. NHWC: (CPU tensor, channels last)','line_number':186,'multiline':False]
['text':'  input view:     NHWC(user) -> NHWC(internal)','line_number':187,'multiline':False]
['text':'  weight reorder: OHWI(user) -> Blocked(internal)','line_number':188,'multiline':False]
['text':'  output view:    NHWC(internal) -> NHWC(user)','line_number':189,'multiline':False]
['text':'','line_number':190,'multiline':False]
['text':' 3. Blocked (MKLDNN tensor):','line_number':191,'multiline':False]
['text':'  By explicitly converting a tensor to mkldnn, e.g. `x.to_mkldnn()`,','line_number':192,'multiline':False]
['text':'  blocked format will propagate between layers. Input, output will be in blocked format.','line_number':193,'multiline':False]
['text':'','line_number':194,'multiline':False]
['text':'  For inference case, weight can be prepacked into blocked format by','line_number':195,'multiline':False]
['text':'  (so as to save weight reoder overhead):','line_number':196,'multiline':False]
['text':'      model = torch.utils.mkldnn.to_mkldnn(model)','line_number':197,'multiline':False]
['text':'','line_number':198,'multiline':False]
['text':'  For training case, grad_output can be CPU tensor or MKLDNN tensor,','line_number':199,'multiline':False]
['text':'  but weight/bias and grad_weight/grad_bias are always CPU tensor.','line_number':200,'multiline':False]
['text':'','line_number':201,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':279,'multiline':False]
['text':' Fuse convolution+binary_op+unary_op for good performance, which doing such','line_number':373,'multiline':False]
['text':' operation: output=unary_op(binary_op(conv(input_t, ...), other_t, alpha)).','line_number':374,'multiline':False]
['text':' The binary_attr means which binary_op is, it can be "add", or','line_number':375,'multiline':False]
['text':' other binary operation. the unary_attr means which unary_op is,','line_number':376,'multiline':False]
['text':' it can be "relu" or other unary operation, if it is none, meaning that','line_number':377,'multiline':False]
['text':' there doesn't have a unary post op. unary_scalars and unary_algorithm','line_number':378,'multiline':False]
['text':' are the parameters of the unary op, such as "hardtanh" has scalar parameters,','line_number':379,'multiline':False]
['text':' "gelu" has algorithm parameters.','line_number':380,'multiline':False]
['text':' Make sure inputs have same type(device, layout, dtype), device is cpu and','line_number':406,'multiline':False]
['text':' dtype is float, bfloat16 or half.','line_number':407,'multiline':False]
['text':' TODO: support broadcast binary fusion.','line_number':419,'multiline':False]
['text':' Only calling fusion path for channels_last path.','line_number':423,'multiline':False]
['text':' TODO: OneDNN doesn't optimize well for groups > 1 case, it will be enabled','line_number':424,'multiline':False]
['text':' at next OneDNN release.','line_number':425,'multiline':False]
['text':' Now, we only support conv+binary+relu.','line_number':434,'multiline':False]
['text':' is_channels_last ','line_number':488,'multiline':True]
['text':' is_channels_last ','line_number':502,'multiline':True]
['text':' Fallback case, if inputs are not channels last or have different dtype,','line_number':507,'multiline':False]
['text':' OneDNN fusion may have performance regression.','line_number':508,'multiline':False]
['text':' Fuse convolution+binary_op+unary_op for good performance, which doing','line_number':537,'multiline':False]
['text':' such operation: other_t=unary_op(binary_op(conv(input_t, ...), other_t,','line_number':538,'multiline':False]
['text':' alpha)). The binary_attr means which binary_op is, it can be "add", or other','line_number':539,'multiline':False]
['text':' binary operation. the unary_attr means which unary_op is, it can be "relu" or','line_number':540,'multiline':False]
['text':' other unary operation, if it is none, meaning that there doesn't have a unary','line_number':541,'multiline':False]
['text':' post op. unary_scalars and unary_algorithm are the parameters of the unary','line_number':542,'multiline':False]
['text':' op, such as "hardtanh" has scalar parameters "gelu" has algorithm parameters.','line_number':543,'multiline':False]
['text':' other_t += convolution(...), other_t = unary(other_t)','line_number':559,'multiline':False]
['text':' Make sure inputs have same type(device, layout, dtype), device is cpu and','line_number':577,'multiline':False]
['text':' dtype is float, bfloat16 or half.','line_number':578,'multiline':False]
['text':' Only calling fusion path for channels_last path and the output is contiguous tensor(channels_last).','line_number':592,'multiline':False]
['text':' Fallback case, if inputs are not channels last or have different dtype,','line_number':619,'multiline':False]
['text':' OneDNN fusion may have performance regression.','line_number':620,'multiline':False]
['text':' The size of weight_t is the prepacked size.','line_number':642,'multiline':False]
['text':'  Groups > 1: [g*o, i/g, ...]','line_number':643,'multiline':False]
['text':'  Groups == 1: [o, i, ...]','line_number':644,'multiline':False]
['text':' Returns original weight size in [i, o, ...]','line_number':645,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':685,'multiline':False]
['text':' mkldnn transposed convolution has weight in logical order of OIHW or OIDHW,','line_number':711,'multiline':False]
['text':' while PyTorch has IOHW or IODHW, `._tranpose()` switches strides (no memory copy).','line_number':712,'multiline':False]
['text':' namespace at::native','line_number':1129,'multiline':False]
