['text':'wrap_scalar=','line_number':57,'multiline':True]
['text':' Build the output size, which is the dim being selected set to','line_number':64,'multiline':False]
['text':' size k','line_number':65,'multiline':False]
['text':' See issue: https://github.com/pytorch/pytorch/issues/65863','line_number':81,'multiline':False]
['text':' Strides should be dense, so as not to allocate too much memory.','line_number':82,'multiline':False]
['text':' We either use 'self' strides, or infer dense strides from them.','line_number':83,'multiline':False]
['text':' namespace meta','line_number':92,'multiline':False]
['text':' Note from TH:
   I cut and pasted (slightly adapted) the quicksort code from
   Sedgewick's 1978 "Implementing Quicksort Programs" article
   http://www.csie.ntu.edu.tw/~b93076/p847-sedgewick.pdf

   It is the state of the art existing implementation. The macros
   are here to make as close a match as possible to the pseudocode of
   Program 2 p.851

   Note that other partition schemes exist, and are typically presented
   in textbook, but those are less efficient. See e.g.
   http://cs.stackexchange.com/questions/11458/quicksort-partitioning-hoare-vs-lomuto

   Julien, November 12th 2013
','line_number':114,'multiline':True]
['text':' NOLINTNEXTLINE(cppcoreguidelines-init-variables)','line_number':135,'multiline':False]
['text':' One element only','line_number':142,'multiline':False]
['text':' Two elements only','line_number':145,'multiline':False]
['text':' Use median of three for pivot choice','line_number':152,'multiline':False]
['text':' Re-set active partition','line_number':181,'multiline':False]
['text':' Compute output shape: q_size + reduced_size','line_number':231,'multiline':False]
['text':' Checks that all q values are between 0 and 1, inclusive','line_number':259,'multiline':False]
['text':' NOTE: this check is only performed when running on the CPU to avoid','line_number':260,'multiline':False]
['text':' synchronizing an accelerator with the CPU','line_number':261,'multiline':False]
['text':' Flatten input if no dim provided else move dim to reduce as last dimension.','line_number':268,'multiline':False]
['text':' Sort to efficiently query kth values.','line_number':269,'multiline':False]
['text':' Treat q as a 1D tensor for the following computations','line_number':279,'multiline':False]
['text':' View input as reduced_size + size of dim to reduce','line_number':284,'multiline':False]
['text':' Ensure converting from int64_t to double won't overflow','line_number':290,'multiline':False]
['text':' Convert q in [0, 1] to ranks in [0, reduction_size)','line_number':295,'multiline':False]
['text':' For nanquantile, compute ranks based on number of non-nan values.','line_number':298,'multiline':False]
['text':' If all values are nan, set rank to 0 so the quantile computed is nan.','line_number':299,'multiline':False]
['text':' For Composite Compliance,','line_number':301,'multiline':False]
['text':' if `ranks` is `CCT` but it's tangent is a regular Tensor,','line_number':302,'multiline':False]
['text':' then while computing jvp, we end calling `masked_fill_`','line_number':303,'multiline':False]
['text':' on a regular Tensor with CCT args, so we call','line_number':304,'multiline':False]
['text':' `masked_fill` instead.','line_number':305,'multiline':False]
['text':'level=','line_number':306,'multiline':True]
['text':' For quantile, compute ranks based on reduction size. If there is nan','line_number':312,'multiline':False]
['text':' set rank to last index so the quantile computed will be nan.','line_number':313,'multiline':False]
['text':' adjust ranks based on the interpolation mode','line_number':320,'multiline':False]
['text':' Actual interpolation is only needed for the liner and midpoint modes','line_number':332,'multiline':False]
['text':' calculate weights for linear and midpoint','line_number':335,'multiline':False]
['text':' Interpolate to compute quantiles and store in values_below','line_number':340,'multiline':False]
['text':' For Composite Compliance,','line_number':343,'multiline':False]
['text':' if either `values_below`, `values_above` or `weights` are a CCT','line_number':344,'multiline':False]
['text':' or tangents of `value_above` and `weights` are a CCT,','line_number':345,'multiline':False]
['text':' but if the tangent of `value_below` is a regular Tensor,','line_number':346,'multiline':False]
['text':' then while computing jvp, we will end-up copying a `CCT`,','line_number':347,'multiline':False]
['text':' into regular Tensor. So we use out-of-place variant of `lerp`','line_number':348,'multiline':False]
['text':'level=','line_number':352,'multiline':True]
['text':'level=','line_number':352,'multiline':True]
['text':'level=','line_number':354,'multiline':True]
['text':'level=','line_number':355,'multiline':True]
['text':' If q is scalar, remove last dim to match out shape','line_number':363,'multiline':False]
['text':' Move quantiles to first dim to match out shape','line_number':366,'multiline':False]
['text':' namespace','line_number':373,'multiline':False]
['text':'wrap_scalar=','line_number':425,'multiline':True]
['text':'squash_dims=','line_number':453,'multiline':True]
['text':' we want NaN to be sorted as top for numpy compatibility','line_number':476,'multiline':False]
['text':'grain_size=','line_number':494,'multiline':True]
['text':' Computes both the median and its index along dimension dim of the input','line_number':504,'multiline':False]
['text':' Ensure #dim is the same for all tensors required for dim_apply','line_number':533,'multiline':False]
['text':' Make dim to reduce contiguous (stride=1)','line_number':538,'multiline':False]
['text':'squash_dims=','line_number':550,'multiline':True]
['text':' For torch.median, search for NaN and return it if found','line_number':563,'multiline':False]
['text':' Vector of indices for indirectly partitioning input around median','line_number':573,'multiline':False]
['text':' We partition the input around the median indirectly using the indices','line_number':579,'multiline':False]
['text':' vector so that nth points to the index of the median in the unmodified','line_number':580,'multiline':False]
['text':' input tensor.','line_number':581,'multiline':False]
['text':' If we got here, there are no nan values','line_number':584,'multiline':False]
['text':' For torch.nanmedian, compute median of non-nan values only','line_number':590,'multiline':False]
['text':'grain_size=','line_number':604,'multiline':True]
['text':' Computes the median of all values in the input','line_number':610,'multiline':False]
['text':' Return nan for empty tensors','line_number':615,'multiline':False]
['text':' Clone the input tensor so we can partition it around the median value','line_number':620,'multiline':False]
['text':' For torch.median, if there are nan values return nan','line_number':629,'multiline':False]
['text':' If we got here, there are no nan values','line_number':637,'multiline':False]
['text':' For torch.nanmedian, compute median of non-nan values only','line_number':641,'multiline':False]
['text':' namespace','line_number':655,'multiline':False]
['text':'ignore_nan=','line_number':671,'multiline':True]
['text':'ignore_nan=','line_number':705,'multiline':True]
['text':'ignore_nan=','line_number':734,'multiline':True]
['text':'ignore_nan=','line_number':768,'multiline':True]
['text':'wrap_scalar=','line_number':837,'multiline':True]
['text':'ignore_nan=','line_number':859,'multiline':True]
['text':'ignore_nan=','line_number':894,'multiline':True]
['text':'ignore_nan=','line_number':906,'multiline':True]
['text':'ignore_nan=','line_number':941,'multiline':True]
['text':' check if self is scalar','line_number':952,'multiline':False]
['text':' namespace native','line_number':996,'multiline':False]
['text':' namespace at','line_number':997,'multiline':False]
