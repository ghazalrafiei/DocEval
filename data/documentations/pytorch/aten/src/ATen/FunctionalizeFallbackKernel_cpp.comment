['text':' we should wrap the output if any inputs were wrapped,','line_number':81,'multiline':False]
['text':' OR if we're hitting a factory function (with no tensor inputs)','line_number':82,'multiline':False]
['text':' resize_() is special because:','line_number':112,'multiline':False]
['text':' - when we resize to a larger size, it acts as a mutation','line_number':113,'multiline':False]
['text':' - when we resize to a smaller size, it acts as a view','line_number':114,'multiline':False]
['text':' See Note [resize_ in Functionalization] for more dtails','line_number':115,'multiline':False]
['text':' First unwrap the tensor arguments','line_number':117,'multiline':False]
['text':' Case 1: arguments are not functional tensors, so we no-op and redispatch.','line_number':125,'multiline':False]
['text':' Case 2: actually functionalize resize_()','line_number':132,'multiline':False]
['text':' If resize_() actually increases the size of the storage, then we need to tell FunctionalTensorWrapper about it.','line_number':145,'multiline':False]
['text':' See Note[resize_() in functionalization pass]','line_number':146,'multiline':False]
['text':' See the note - we're guaranteed at this point that "self" is *not* a view (and has no outstanding views)','line_number':149,'multiline':False]
['text':' So we don't need to treat the output of resize as view tensor.','line_number':150,'multiline':False]
['text':' Otherwise, we know that we're resizing to a smaller size.','line_number':154,'multiline':False]
['text':' resize_() is effectively a view operator.','line_number':155,'multiline':False]
['text':' The output of resizing is equivalent to taking a slice of a larger tensor.','line_number':156,'multiline':False]
['text':' We have to emulate this "slicing" with an as_strided call.','line_number':157,'multiline':False]
['text':' See Note [Exporting and compiling a graph with lift_fresh_copy]','line_number':184,'multiline':False]
['text':' Note [Exporting and compiling a graph with lift_fresh_copy]','line_number':195,'multiline':False]
['text':' If out is already a functional tensor, don't wrap it twice.','line_number':196,'multiline':False]
['text':' In theory this could be useful if we want to nest functionalization with itself,','line_number':197,'multiline':False]
['text':' but that isn't really a use case today.','line_number':198,'multiline':False]
['text':' Needed for https://github.com/pytorch/pytorch/issues/105327','line_number':199,'multiline':False]
['text':' If the target device is empty, then the output tensor should be on the same device as the input','line_number':210,'multiline':False]
['text':' note I only need this because the to.dtype/to.dtype_layout overload calls this, so we skip the op above.','line_number':215,'multiline':False]
['text':' We should probably get rid of this though.','line_number':216,'multiline':False]
['text':' sync any pending updates','line_number':227,'multiline':False]
['text':' pass the unwrapped tensor to the backend','line_number':229,'multiline':False]
['text':' Special case: if the Functionalize key is not in TLS, we assume that we're running','line_number':238,'multiline':False]
['text':' on a lazy backend (LTC).','line_number':239,'multiline':False]
['text':' In that case, if we're copying to a non-functionalize-enabled device,','line_number':240,'multiline':False]
['text':' then the functionalization pass should "end". We need to sync any updates on the input','line_number':241,'multiline':False]
['text':' tensor, but we shouldn't wrap the output.','line_number':242,'multiline':False]
['text':' Why is _unsafe_view special-cased here?','line_number':252,'multiline':False]
['text':' Basically just to satisfy autograd's debug asserts.','line_number':253,'multiline':False]
['text':' The situation:','line_number':254,'multiline':False]
['text':' - _unsafe_view's autograd kernel has debug asserts to confirm','line_number':255,'multiline':False]
['text':'   that the input and output alias storage.','line_number':256,'multiline':False]
['text':' - _unsafe_view's schema in native_functions.yaml','line_number':257,'multiline':False]
['text':'   does not contain alias annotations, so it advertises as non-aliasing.','line_number':258,'multiline':False]
['text':' - functionalization will then treat _unsafe_view like a non-aliasing op.','line_number':259,'multiline':False]
['text':'   Specifically, autograd will redispatch to functionalization's','line_number':260,'multiline':False]
['text':'   boxed fallback kernel, which creates a new FunctionalTensorWrapper output','line_number':261,'multiline':False]
['text':'   that does **not** alias storage with the input, tripping the assert.','line_number':262,'multiline':False]
['text':' The kernel written here just manually re-ifies the aliasing relationship.','line_number':263,'multiline':False]
['text':'','line_number':264,'multiline':False]
['text':' Another way to handle this would be to fix unsafe_view's alias annotations','line_number':265,'multiline':False]
['text':' in native_functions.yaml, but I think this would be a pessimization.','line_number':266,'multiline':False]
['text':' The idea with _unsafe_view is that you're guaranteed that the input','line_number':267,'multiline':False]
['text':' is a temporary, and don't actually have to worry about propagating','line_number':268,'multiline':False]
['text':' mutations between the input and output.','line_number':269,'multiline':False]
['text':' See  Note [Propagating strides in the functionalization pass]','line_number':293,'multiline':False]
['text':' (for _unsafe_view, I'm just manually doing the shape inference rule here instead of calling the meta function for unsafe_view)','line_number':294,'multiline':False]
['text':' error case','line_number':303,'multiline':False]
['text':' nop case','line_number':310,'multiline':False]
['text':' The overloads of set_() that take in a storage should never','line_number':335,'multiline':False]
['text':' appear with torch.compile, because dynamo graph breaks','line_number':336,'multiline':False]
