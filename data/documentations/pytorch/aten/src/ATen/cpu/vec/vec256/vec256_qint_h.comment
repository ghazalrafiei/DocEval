['text':' DO NOT DEFINE STATIC DATA IN THIS HEADER!','line_number':3,'multiline':False]
['text':' See Note [Do not compile initializers with AVX]','line_number':4,'multiline':False]
['text':' This file defines Vectorized<> for the quantized types.','line_number':18,'multiline':False]
['text':'','line_number':19,'multiline':False]
['text':'','line_number':20,'multiline':False]
['text':' Currently, we simply use these classes as efficient converters between','line_number':21,'multiline':False]
['text':' the quantized types and Vectorized<float>, usually in bandwidth-bound cases','line_number':22,'multiline':False]
['text':' where doing the arithmetic in full-precision is acceptable (e.g.','line_number':23,'multiline':False]
['text':' elementwise operators).','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':'','line_number':26,'multiline':False]
['text':' Conversions are as follows:','line_number':27,'multiline':False]
['text':'  Vectorized<qint8> -> 4x Vectorized<float>','line_number':28,'multiline':False]
['text':'  Vectorized<quint8> -> 4x Vectorized<float>','line_number':29,'multiline':False]
['text':'  Vectorized<qint32> -> 1x Vectorized<float>','line_number':30,'multiline':False]
['text':'','line_number':31,'multiline':False]
['text':' The size of the returned float vector is specified by the special','line_number':32,'multiline':False]
['text':' constexpr function float_num_vecs. The type of the value returned','line_number':33,'multiline':False]
['text':' from dequantize (and expected as an argument to quantize) is','line_number':34,'multiline':False]
['text':' specified by float_vec_return_type.','line_number':35,'multiline':False]
['text':'','line_number':36,'multiline':False]
['text':' When writing kernels with these vectors, it is expected that floating-','line_number':37,'multiline':False]
['text':' point operations will be carried out in a loop over Vectorized<T>::float_num_vecs','line_number':38,'multiline':False]
['text':' iterations.','line_number':39,'multiline':False]
['text':'first','line_number':67,'multiline':True]
['text':'second','line_number':68,'multiline':True]
['text':'min_val','line_number':69,'multiline':True]
['text':'max_val','line_number':70,'multiline':True]
['text':' This function is for linkage only, will not be used','line_number':71,'multiline':False]
['text':' Note: this function only convert inputs number of elements equal to at::vec::Vectorized<float>.size()','line_number':100,'multiline':False]
['text':' Only handle first 64 bits','line_number':101,'multiline':False]
['text':' Convert from 8*uint8 to 8*int32','line_number':103,'multiline':False]
['text':' Convert from 8*int32 to 8*float','line_number':105,'multiline':False]
['text':' Convert from float32 to int32 with truncation','line_number':110,'multiline':False]
['text':' Convert from int32 to int16 using signed saturation','line_number':113,'multiline':False]
['text':' Convert from int16 to uint8 using unsigned saturation','line_number':119,'multiline':False]
['text':' This is the largest int32 value < int32_max exactly representable in float','line_number':139,'multiline':False]
['text':' clang-format off','line_number':144,'multiline':False]
['text':' clang-format on','line_number':154,'multiline':False]
['text':' x','line_number':161,'multiline':False]
['text':' If the floating point value is greater than int32_max,','line_number':164,'multiline':False]
['text':' _mm256_cvtps_epi32 converts them to -ve. Clip at int32_float_max_val to','line_number':165,'multiline':False]
['text':' Clip at int32_float_max_val to avoid this.','line_number':166,'multiline':False]
['text':' y','line_number':169,'multiline':False]
['text':' z','line_number':174,'multiline':False]
['text':' w','line_number':179,'multiline':False]
['text':' add zero point','line_number':190,'multiline':False]
['text':' Additional 8-lane AVX2 version to take advantage when len is smaller','line_number':206,'multiline':False]
['text':' based on fbgemm::QuantizeAvx2 (https://github.com/pytorch/FBGEMM)','line_number':207,'multiline':False]
['text':' Not exactly the same behavior as the vectorized code.','line_number':228,'multiline':False]
['text':' The vectorized code above always rounds to even in halfway cases','line_number':229,'multiline':False]
['text':' (https://software.intel.com/en-us/node/523819), but std::nearbyint','line_number':230,'multiline':False]
['text':' does the same only when the current rounding mode is FE_TONEAREST.','line_number':231,'multiline':False]
['text':' However, in practice, this should not be a problem because most cases','line_number':232,'multiline':False]
['text':' use the default rounding mode FE_TONEAREST.','line_number':233,'multiline':False]
['text':' Note that we cannot implement the same behavior as the vectorized code','line_number':234,'multiline':False]
['text':' using std::round because it does rounding away from zero in halfway','line_number':235,'multiline':False]
['text':' cases.','line_number':236,'multiline':False]
['text':' Broadcast constructor','line_number':269,'multiline':False]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':289,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':290,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':291,'multiline':False]
['text':'zero_point','line_number':302,'multiline':True]
['text':'inverse_scale','line_number':319,'multiline':True]
['text':'precision=','line_number':322,'multiline':True]
['text':' Load from memory constructor','line_number':363,'multiline':False]
['text':'
 * Convert values from int32 back to int8/uint8
 ','line_number':388,'multiline':True]
['text':' Add zero point ','line_number':413,'multiline':True]
['text':' Pack to int16_t and saturate ','line_number':419,'multiline':True]
['text':'
   * xyzw_clamped_v has results in the following layout so we need to
   * permute: x0-3 y0-3 z0-3 w0-3 x4-7 y4-7 z4-7 w4-7
   ','line_number':426,'multiline':True]
['text':' Broadcast constructor','line_number':458,'multiline':False]
['text':' This is needed because the compiler emits awful code for the default','line_number':464,'multiline':False]
['text':' constructor for moving the enum','line_number':465,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-deprecated-copy)','line_number':466,'multiline':False]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':488,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':489,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':490,'multiline':False]
['text':'zero_point','line_number':507,'multiline':True]
['text':'scale','line_number':552,'multiline':True]
['text':' Load from memory constructor','line_number':623,'multiline':False]
['text':' Broadcast constructor','line_number':658,'multiline':False]
['text':' NOLINTNEXTLINE(clang-diagnostic-deprecated-copy)','line_number':664,'multiline':False]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':686,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':687,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':688,'multiline':False]
['text':'zero_point','line_number':705,'multiline':True]
['text':'scale','line_number':750,'multiline':True]
['text':' Load from memory constructor','line_number':821,'multiline':False]
['text':' NOTE: These are low-performance implementations that we fall back on','line_number':834,'multiline':False]
['text':' if we are not building with AVX2. This may not be an issue, because','line_number':835,'multiline':False]
['text':' currently for quantization we assume the user has at least AVX512','line_number':836,'multiline':False]
['text':' installed, so these can simply act as a reference implementation.','line_number':837,'multiline':False]
['text':'','line_number':838,'multiline':False]
['text':' If in the future we relax this requirement (AVX2+), we should probably','line_number':839,'multiline':False]
['text':' revisit these implementations','line_number':840,'multiline':False]
['text':'scale_zp_premul','line_number':883,'multiline':True]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':945,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':946,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':947,'multiline':False]
['text':'inverse_scale','line_number':960,'multiline':True]
['text':'precision=','line_number':968,'multiline':True]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':1090,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':1091,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':1092,'multiline':False]
['text':'inverse_scale','line_number':1105,'multiline':True]
['text':' Ensure uninitialized memory does not change the output value See https://github.com/pytorch/pytorch/issues/32502','line_number':1223,'multiline':False]
['text':' for more details. We do not initialize arrays to zero using "={0}" because gcc would compile it to two','line_number':1224,'multiline':False]
['text':' instructions while a loop would be compiled to one instruction.','line_number':1225,'multiline':False]
['text':'inverse_scale','line_number':1238,'multiline':True]
['text':' if defined(CPU_CAPABILITY_AVX2) && !defined(_MSC_VER)','line_number':1326,'multiline':False]
['text':' namespace at::vec::CPU_CAPABILITY','line_number':1327,'multiline':False]
