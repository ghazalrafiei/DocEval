['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' NOTE: [functorch's TensorWrapper]','line_number':15,'multiline':False]
['text':'','line_number':16,'multiline':False]
['text':' Taking better suggestions for a name. TensorWrapper is the wrapper Tensor','line_number':17,'multiline':False]
['text':' Subclass for functorch's grad-based transforms (grad, vjp, jvp). It is','line_number':18,'multiline':False]
['text':' analogous to how vmap uses BatchedTensor as the wrapper Tensor subclass.','line_number':19,'multiline':False]
['text':'','line_number':20,'multiline':False]
['text':' If you're familiar with the Tensor-Variable merge, TensorWrapper is effectively','line_number':21,'multiline':False]
['text':' another Variable.','line_number':22,'multiline':False]
['text':'','line_number':23,'multiline':False]
['text':' Consider grad(grad(torch.sin))(x). This wraps `x` as TensorWrapper(TensorWrapper(x)).','line_number':24,'multiline':False]
['text':' The reason why is so that each TensorWrapper can hold its own AutogradMeta and','line_number':25,'multiline':False]
['text':' participate in a **separate** autograd graph.','line_number':26,'multiline':False]
['text':'','line_number':27,'multiline':False]
['text':' There are alternative designs we could have chosen (e.g. each grad transform','line_number':28,'multiline':False]
['text':' stores a weak map of Tensor -> AutogradMeta); the benefit of the TensorWrapper','line_number':29,'multiline':False]
['text':' design is that we can re-use existing VariableType kernels (i.e. Autograd kernels)','line_number':30,'multiline':False]
['text':' without much modification. Since a TensorWrapper looks like a regular Tensor,','line_number':31,'multiline':False]
['text':' the VariableType kernel can pull out the AutogradMeta struct from where it','line_number':32,'multiline':False]
['text':' expects and extend the autograd graph','line_number':33,'multiline':False]
['text':' if true, this came from an operation that aliases an immutable tensor','line_number':41,'multiline':False]
['text':' Overrides necessary for autograd','line_number':60,'multiline':False]
['text':' TensorWrapper receives a boolean flag on whether or not the Grad Interpreter','line_number':75,'multiline':False]
['text':' that created it is still alive or not.','line_number':76,'multiline':False]
['text':' If the Grad Interpreter is no longer alive then it attempts to behave like','line_number':77,'multiline':False]
['text':' a regular Tensor.','line_number':78,'multiline':False]
['text':'','line_number':79,'multiline':False]
['text':' When we exit the level, this wrapper may be marked as "not alive".','line_number':80,'multiline':False]
['text':' Wrappers that are not alive:','line_number':81,'multiline':False]
['text':' 1) May still have autograd metadata on them','line_number':82,'multiline':False]
['text':' 2) Forward dispatches to the underlying value()','line_number':83,'multiline':False]
['text':' There are two variants of makeTensorWrapper: one that accepts a level','line_number':87,'multiline':False]
['text':' and one that accepts an Interpreter.','line_number':88,'multiline':False]
['text':'','line_number':89,'multiline':False]
['text':' The one that accepts a level tries to automatically get the life handle from the','line_number':90,'multiline':False]
['text':' interpreter on the DynamicLayerStack.','line_number':91,'multiline':False]
['text':' It needs to be used with caution: if the interpreter is not on the','line_number':92,'multiline':False]
['text':' DynamicLayerStack, then we won't be able to find the life handle.','line_number':93,'multiline':False]
['text':'','line_number':94,'multiline':False]
['text':' In practice this isn't a problem: when we're constructing TensorWrapper in','line_number':95,'multiline':False]
['text':' Python, the corresponding interpreter is on the stack.','line_number':96,'multiline':False]
['text':' namespace at::functorch','line_number':103,'multiline':False]
