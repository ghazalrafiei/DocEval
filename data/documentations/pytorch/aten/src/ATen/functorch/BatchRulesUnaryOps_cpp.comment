['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' Memory format support is a little tricky because vmap is allowed to move','line_number':18,'multiline':False]
['text':' around batch dimensions and some memory formats are rank-dependent.','line_number':19,'multiline':False]
['text':' Another weird case is:','line_number':20,'multiline':False]
['text':' - a tensor with MemoryFormat::ChannelsLast MUST have 4 dimensions. Do we','line_number':21,'multiline':False]
['text':'   allow the user to clone a Tensor with 3 logical dimensions and 1 batch','line_number':22,'multiline':False]
['text':'   dim into a ChannelsLast Tensor? What about a Tensor with 3 logical dims','line_number':23,'multiline':False]
['text':'   and N>1 batch dims?','line_number':24,'multiline':False]
['text':' There is an ambiguity here when the batch dims are not at the front of','line_number':32,'multiline':False]
['text':' the tensor.','line_number':33,'multiline':False]
['text':' >>> x = torch.randn(3, B0, 5)','line_number':34,'multiline':False]
['text':' >>> y = vmap(lambda x: x.clone(torch.contiguous_format), in_dims=1, out_dims=0)(x)','line_number':35,'multiline':False]
['text':' >>> y[0].is_contiguous()','line_number':36,'multiline':False]
['text':' ???','line_number':37,'multiline':False]
['text':' Should we make the whole tensor contiguous, or should we','line_number':38,'multiline':False]
['text':' make the non-batch dims contiguous? We've chosen the latter because','line_number':39,'multiline':False]
['text':' philosophically vmap hides the batch dims and operates on a per-sample level.','line_number':40,'multiline':False]
['text':' guard against the user passing in a batch of scalar tensors with batch','line_number':53,'multiline':False]
['text':' size equal to 2.','line_number':54,'multiline':False]
['text':' special-related','line_number':138,'multiline':False]
['text':' torch.special.* functions','line_number':144,'multiline':False]
['text':' Activation functions (from https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)','line_number':163,'multiline':False]
