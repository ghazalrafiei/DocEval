['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' Given a linear index, return the actual index.','line_number':43,'multiline':False]
['text':' Example: Given linear_idx = 3, sizes = [5, 2], we would return [1, 0]','line_number':44,'multiline':False]
['text':' The general flow of the algorithm is as follows.','line_number':88,'multiline':False]
['text':' - First, we figure out which arguments are BatchedTensors and save them','line_number':89,'multiline':False]
['text':'   to a vector. We also store a vector of which index of the arguments list','line_number':90,'multiline':False]
['text':'   each BatchedTensor appears in. This will be useful for bookkeeping later.','line_number':91,'multiline':False]
['text':' - Next, we apply the MultiBatchVmapTransform to all of the BatchedTensors.','line_number':92,'multiline':False]
['text':'   This returns a vector of VmapPhysicalView that hold tensors that contain','line_number':93,'multiline':False]
['text':'   all of the collective batch dimensions at the front of the tensors.','line_number':94,'multiline':False]
['text':' - Then, we attempt to call `op` once per slice of the inputs. To do this,','line_number':95,'multiline':False]
['text':'   we repeatedly we slice the input arguments (if they are BatchedTensors),','line_number':96,'multiline':False]
['text':'   put the sliced (or a not-sliced) version of the input onto the stack, invoke','line_number':97,'multiline':False]
['text':'   the operator, and then pop the results off the stack.','line_number':98,'multiline':False]
['text':'in_place','line_number':101,'multiline':True]
['text':' `self` is the Tensor being modified in-place','line_number':107,'multiline':False]
['text':' Figure out which arguments are BatchedTensor. Save them to a vector.','line_number':115,'multiline':False]
['text':' For each BatchedTensor, also record what position of `arguments` they came from.','line_number':116,'multiline':False]
['text':' NOTE: [vmap-incompatible in-place operations]','line_number':133,'multiline':False]
['text':' In-place operations on `self` are not possible if there exists some vmap','line_number':134,'multiline':False]
['text':' level `l` such that `self` is not being vmapped on that level but another','line_number':135,'multiline':False]
['text':' argument is. For example, let B0 be a batch dim inside vmap and consider','line_number':136,'multiline':False]
['text':' vmap(Tensor.add_, in_dims=(None, 0))(torch.ones(3), torch.ones(B0, 3))','line_number':137,'multiline':False]
['text':' - self is torch.ones(3) and does not participate in this vmap','line_number':138,'multiline':False]
['text':' - other is BatchedTensor(torch.ones(B0, 3))','line_number':139,'multiline':False]
['text':' There's no way to do self.add_(other) because `other` has more elements','line_number':140,'multiline':False]
['text':' elements than `self` due to being vmapped over.','line_number':141,'multiline':False]
['text':'','line_number':142,'multiline':False]
['text':' In the vmap fallback, we should error out when we detect this.','line_number':143,'multiline':False]
['text':' Find one vmap level to complain about','line_number':146,'multiline':False]
['text':' The following prints out "vmap: aten::add_(tensor, ...) is not possible",','line_number':149,'multiline':False]
['text':' but it would be better to print out "tensor.add_(...) is not possible".','line_number':150,'multiline':False]
['text':' Afaict there's no official way to get the add_ and there is no way to','line_number':151,'multiline':False]
['text':' tell if an operator has method or function variants.','line_number':152,'multiline':False]
['text':' MultiBatchVmapTransform the BatchedTensor arguments. This returns','line_number':167,'multiline':False]
['text':' VmapPhysicalViews that contain all of the batch dimensions.','line_number':168,'multiline':False]
['text':' Compute the total number of batches','line_number':172,'multiline':False]
['text':' Without a shape-checking API, we're unable to compute the correct shape of','line_number':178,'multiline':False]
['text':' the output so we just error out.','line_number':179,'multiline':False]
['text':' Strategy: For each batch, we are going to push slices (where applicable)','line_number':184,'multiline':False]
['text':' of the arguments onto `stack`, and call `op`.','line_number':185,'multiline':False]
['text':' We assume that torch::jit::Stack is backed by vector<IValue> for','line_number':191,'multiline':False]
['text':' simplicity. When that is not the case, this code should be updated.','line_number':192,'multiline':False]
['text':' argument isn't a BatchedTensor','line_number':196,'multiline':False]
['text':' argument is a BatchedTensor','line_number':200,'multiline':False]
['text':' Return the tensor that was written to in-place','line_number':213,'multiline':False]
['text':' NOTE [vmap through backward and undefined grad]','line_number':223,'multiline':False]
['text':' While vmapping through backward functions (to compute batched grad), it','line_number':224,'multiline':False]
['text':' is possible for the backward function to return an undefined grad for some','line_number':225,'multiline':False]
['text':' grad_input for each example. In that case, we return an undefined grad.','line_number':226,'multiline':False]
['text':'','line_number':227,'multiline':False]
['text':' It is theoretically posssible for *some* of the examples to produce an','line_number':228,'multiline':False]
['text':' undefined grad (a kernel could peek at the gradient values and return an','line_number':229,'multiline':False]
['text':' undefined tensor if it determines the gradient is full of zeros). We','line_number':230,'multiline':False]
['text':' could handle this by treating the undefined grad as a zero-filled tensor','line_number':231,'multiline':False]
['text':' of the correct shape while stacking the tensors together. However I expect','line_number':232,'multiline':False]
['text':' this to happen very rarely (I have not been able to find an example in our','line_number':233,'multiline':False]
['text':' codebase) so we just error out in this case.','line_number':234,'multiline':False]
['text':' TODO: Consider rewriting the following to look like:','line_number':244,'multiline':False]
['text':' https://gist.github.com/zou3519/7b7c6a4a258d580f62d1d969851be6b1<Paste>','line_number':245,'multiline':False]
['text':' The general flow of the algorithm is as follows.','line_number':247,'multiline':False]
['text':' - First, we figure out which arguments are BatchedTensors and save them','line_number':248,'multiline':False]
['text':'   to a vector. We also store a vector of which index of the arguments list','line_number':249,'multiline':False]
['text':'   each BatchedTensor appears in. This will be useful for bookkeeping later.','line_number':250,'multiline':False]
['text':' - Next, we apply the MultiBatchVmapTransform to all of the BatchedTensors.','line_number':251,'multiline':False]
['text':'   This returns a vector of VmapPhysicalView that hold tensors that contain','line_number':252,'multiline':False]
['text':'   all of the collective batch dimensions at the front of the tensors.','line_number':253,'multiline':False]
['text':' - Then, we attempt to call `op` once per slice of the inputs. To do this,','line_number':254,'multiline':False]
['text':'   we repeatedly we slice the input arguments (if they are BatchedTensors),','line_number':255,'multiline':False]
['text':'   put the sliced (or a not-sliced) version of the input onto the stack, invoke','line_number':256,'multiline':False]
['text':'   the operator, and then pop the results off the stack.','line_number':257,'multiline':False]
['text':' - Each result obtained from the previous step is a slice of the total result,','line_number':258,'multiline':False]
['text':'   so we stack those tensors together to form the final result.','line_number':259,'multiline':False]
['text':'in_place','line_number':286,'multiline':True]
['text':' Figure out which arguments are BatchedTensor. Save them to a vector.','line_number':290,'multiline':False]
['text':' For each BatchedTensor, also record what position of `arguments` they came from.','line_number':291,'multiline':False]
['text':' MultiBatchVmapTransform the BatchedTensor arguments. This returns','line_number':312,'multiline':False]
['text':' VmapPhysicalViews that contain all of the batch dimensions.','line_number':313,'multiline':False]
['text':' Compute the total number of batches','line_number':317,'multiline':False]
['text':' Without a shape-checking API, we're unable to compute the correct shape of','line_number':322,'multiline':False]
['text':' the output so we just error out.','line_number':323,'multiline':False]
['text':' Strategy: For each batch, we are going to push slices (where applicable)','line_number':328,'multiline':False]
['text':' of the arguments onto `stack`, call `op`, and store the result in','line_number':329,'multiline':False]
['text':' `output_shards`.','line_number':330,'multiline':False]
['text':'','line_number':331,'multiline':False]
['text':' NOTE: [Output shards layout]','line_number':332,'multiline':False]
['text':' Assume that the operator has three outputs: a, b, c.','line_number':333,'multiline':False]
['text':' The layout of output_shards is as follows:','line_number':334,'multiline':False]
['text':' [ a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, c3]','line_number':335,'multiline':False]
['text':' This is so that we can call at::stack([a0...a3]), at::stack([b0...b3])','line_number':336,'multiline':False]
['text':' more easily in the next step.','line_number':337,'multiline':False]
['text':' We assume that torch::jit::Stack is backed by vector<IValue> for','line_number':345,'multiline':False]
['text':' simplicity. When that is not the case, this code should be updated.','line_number':346,'multiline':False]
['text':' argument isn't a BatchedTensor','line_number':350,'multiline':False]
['text':' argument is a BatchedTensor','line_number':354,'multiline':False]
['text':' std::cout << "[Fallback]: ";','line_number':363,'multiline':False]
['text':' at::dump_tensor((*stack)[stack->size() - 1].toTensor());','line_number':364,'multiline':False]
['text':' Store the result into `output_shards`. See NOTE: [Output shards layout]','line_number':368,'multiline':False]
['text':' to learn about the details of how we store the shards.','line_number':369,'multiline':False]
['text':' For each output Tensor, stack the shards of the tensor together to form a return','line_number':377,'multiline':False]
['text':' See NOTE [vmap through backward and undefined grad]','line_number':384,'multiline':False]
['text':'in_place','line_number':427,'multiline':True]
['text':'is_nested','line_number':427,'multiline':True]
['text':' Figure out which arguments are BatchedTensor. Save them to a vector.','line_number':431,'multiline':False]
['text':' For each BatchedTensor, also record what position of `arguments` they came from.','line_number':432,'multiline':False]
['text':' We assume that torch::jit::Stack is backed by vector<IValue> for','line_number':473,'multiline':False]
['text':' simplicity. When that is not the case, this code should be updated.','line_number':474,'multiline':False]
['text':' argument isn't a BatchedTensor','line_number':478,'multiline':False]
['text':' argument is a BatchedTensor','line_number':482,'multiline':False]
['text':' Store the result into `output_shards`. See NOTE: [Output shards layout]','line_number':491,'multiline':False]
['text':' to learn about the details of how we store the shards.','line_number':492,'multiline':False]
['text':' For each output Tensor, stack the shards of the tensor together to form a nested return','line_number':500,'multiline':False]
['text':' TODO: Determine when the output needs to be nested and when it can be non-nested?','line_number':501,'multiline':False]
['text':' NB: NTs only support batching over dim 0','line_number':508,'multiline':False]
['text':' namespace at','line_number':518,'multiline':False]
