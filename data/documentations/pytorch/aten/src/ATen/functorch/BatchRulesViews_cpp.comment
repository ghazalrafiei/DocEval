['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' Note [Adding vmap support for an operator]','line_number':22,'multiline':False]
['text':' Hey there! So you have an operator and you want to get it to work with vmap.','line_number':23,'multiline':False]
['text':' For example, let's say you just invented the `sum.int` operator and want to make','line_number':24,'multiline':False]
['text':' it so that the following works.','line_number':25,'multiline':False]
['text':' >>> tensor = torch.randn(B, 3)','line_number':26,'multiline':False]
['text':' >>> vmap(torch.sum, (0, None))(tensor, 0)` works','line_number':27,'multiline':False]
['text':' There are three main ways to do so.','line_number':28,'multiline':False]
['text':'','line_number':29,'multiline':False]
['text':' Note [Writing batch rule for out-of-place operators]','line_number':30,'multiline':False]
['text':' If your operator is out-of-place, you can write a batch rule for it.','line_number':31,'multiline':False]
['text':' The batch rule defines how to perform the operator on inputs where each','line_number':32,'multiline':False]
['text':' Tensor input may have an additional dimension that is being vmapped over.','line_number':33,'multiline':False]
['text':' We refer to this dimension as the *batch dimension* or bdim for short.','line_number':34,'multiline':False]
['text':'','line_number':35,'multiline':False]
['text':' For example, let's consider writing a batch rule for','line_number':36,'multiline':False]
['text':' `Tensor sum(const Tensor& self, int64_t dim)`. The signature of the','line_number':37,'multiline':False]
['text':' batch rule has an additional optional<int64_t> argument after each','line_number':38,'multiline':False]
['text':' Tensor argument and return. So, in this case, the batch rule has signature','line_number':39,'multiline':False]
['text':'   tuple<Tensor,optional<int64_t>> sum_batch_rule(','line_number':40,'multiline':False]
['text':'       const Tensor& self, optional<int64_t> self_bdim, int64_t dim);','line_number':41,'multiline':False]
['text':'','line_number':42,'multiline':False]
['text':' The vmap call above invokes the batch rule with `self = tensor`,','line_number':43,'multiline':False]
['text':' `self_bdim = 0`, and `dim = 0`. Note that there are **no BatchedTensors**','line_number':44,'multiline':False]
['text':' involved in this case; there exists some plumbing that automatically unwraps','line_number':45,'multiline':False]
['text':' BatchedTensors before calling the batch rule.','line_number':46,'multiline':False]
['text':'','line_number':47,'multiline':False]
['text':' To write the logic of the batch rule: think about the semantics of the','line_number':48,'multiline':False]
['text':' `sum` operation if `self` had an additional dimension (indicated by self_bdim):','line_number':49,'multiline':False]
['text':' - If `self_bdim` is null, then we just do `result = self.sum(dim)` as usual','line_number':50,'multiline':False]
['text':' - If `self_bdim` is not-null, then we need to modify `dim`. `dim` is equal','line_number':51,'multiline':False]
['text':'   to whatever the user passed in (0 in this case), but we should actually','line_number':52,'multiline':False]
['text':'   perform the reduction over dimension 1 and do `result = self.sum(1)`','line_number':53,'multiline':False]
['text':'   because dim 0 is being vmapped over.','line_number':54,'multiline':False]
['text':' Finally, we return the result as well as a new bdim','line_number':55,'multiline':False]
['text':' - If `self_bdim` is null, then there's no batch dim in the result.','line_number':56,'multiline':False]
['text':' - If `self_bdim` is not-null, then we return where the bdim is.','line_number':57,'multiline':False]
['text':'   Since we invoked `result = self.sum(1)`, the bdim is still at dim 0.','line_number':58,'multiline':False]
['text':'','line_number':59,'multiline':False]
['text':' Now that we have written `sum_batch_rule`, we have to register it inside a','line_number':60,'multiline':False]
['text':' TORCH_LIBRARY_IMPL block:','line_number':61,'multiline':False]
['text':'   TORCH_LIBRARY_IMPL(aten, FuncTorchBatched, m) {','line_number':62,'multiline':False]
['text':'     ...','line_number':63,'multiline':False]
['text':'     VMAP_SUPPORT2(sum, int, sum_batch_rule);','line_number':64,'multiline':False]
['text':'     ...','line_number':65,'multiline':False]
['text':'   }','line_number':66,'multiline':False]
['text':'','line_number':67,'multiline':False]
['text':' Note [Reusing batch rules to add vmap support for a complicated operator]','line_number':68,'multiline':False]
['text':' Can't figure out how to write a batch rule for a big operation? If the','line_number':69,'multiline':False]
['text':' operation can be expressed as a composition of other operations that do have','line_number':70,'multiline':False]
['text':' batch rules, then that is another way to add vmap support. For example,','line_number':71,'multiline':False]
['text':' consider the following schema','line_number':72,'multiline':False]
['text':'   func: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)','line_number':73,'multiline':False]
['text':' and assume we already have batching rules for basic arithmetic operators.','line_number':74,'multiline':False]
['text':'','line_number':75,'multiline':False]
['text':' To add vmap support, define a decomposition using the same signature:','line_number':76,'multiline':False]
['text':'   Tensor addcmul_decomp(const Tensor& self, const Tensor& tensor1,','line_number':77,'multiline':False]
['text':'                         const Tensor& tensor2, const Scalar& value) {','line_number':78,'multiline':False]
['text':'     auto product = torch.mul(tensor1, tensor2);','line_number':79,'multiline':False]
['text':'     return torch.add(self, product, value);','line_number':80,'multiline':False]
['text':'   }','line_number':81,'multiline':False]
['text':' And register it inside a TORCH_LIBRARY_IMPL block:','line_number':82,'multiline':False]
['text':'   TORCH_LIBRARY_IMPL(aten, FuncTorchBatched, m) {','line_number':83,'multiline':False]
['text':'     ...','line_number':84,'multiline':False]
['text':'     m.impl("addcmul", addcmul_decomp);','line_number':85,'multiline':False]
['text':'     ...','line_number':86,'multiline':False]
['text':'   }','line_number':87,'multiline':False]
['text':'','line_number':88,'multiline':False]
['text':' Note [Writing batch rule for in-place operators]','line_number':89,'multiline':False]
['text':' TODO: This is kinda complicated. Saving this for a future date.','line_number':90,'multiline':False]
['text':' NB: repeat is not actually a view, but it is in this file','line_number':104,'multiline':False]
['text':' See if the view is valid. If it's not, then we copy.','line_number':128,'multiline':False]
['text':' It's OK to copy, because _unsafe_view(x) guarantees that x isn't used','line_number':129,'multiline':False]
['text':' anymore.','line_number':130,'multiline':False]
['text':' TODO: The following algorithm only works for batch dim == 0.','line_number':171,'multiline':False]
['text':' To get it to work for something else we need the ability to modify','line_number':172,'multiline':False]
['text':' the BatchDims attribute of BatchedTensorImpl','line_number':173,'multiline':False]
['text':' Resize the wrapped tensor','line_number':176,'multiline':False]
['text':' Update the sizes and strides of the wrapper','line_number':183,'multiline':False]
['text':' Special case for scalar arrays to replicate PyTorch behavior.','line_number':193,'multiline':False]
['text':' Manually calculate the output shape by eliding all dimensions of','line_number':198,'multiline':False]
['text':' size 1 keeping track of where the batch index started and where it','line_number':199,'multiline':False]
['text':' ended up moving to. We also ensure we do not drop the batch index.','line_number':200,'multiline':False]
['text':' Keep only dimensions != 1 and the batch dimension (irrespective of size).','line_number':208,'multiline':False]
['text':' Only increment for the dimensions that will be kept in the output.','line_number':214,'multiline':False]
['text':' Special case for scalar arrays to replicate PyTorch behavior.','line_number':229,'multiline':False]
['text':' Adjust any dimensions higher than the batch dimension','line_number':238,'multiline':False]
['text':' A column before batch dimension will be dropped so adjust accordingly.','line_number':246,'multiline':False]
['text':' Since dimension to be squeezed is after the batch dimension adjust by one to account','line_number':250,'multiline':False]
['text':' for the original batch dimension. In this case batch dimension won't move.','line_number':251,'multiline':False]
['text':' We will do something like: t.reshape(a, -1).roll(1, dims=[1, ]).reshape(old_shape)','line_number':296,'multiline':False]
['text':' NOTE: For scalar tensor, we don't need to unsqueeze as reshape','line_number':305,'multiline':False]
['text':' with `old_shape` takes care of it.','line_number':306,'multiline':False]
['text':' PyTorch has a special case where scalar_tensor.transpose(dim0, dim1) works','line_number':361,'multiline':False]
['text':' for dim0, dim1 in {0, -1} and returns the scalar tensor. If the following happens:','line_number':362,'multiline':False]
['text':' >>> x = torch.randn(B0)  # the per-examples are all scalars','line_number':363,'multiline':False]
['text':' >>> vmap(lambda x: x.transpose(0, -1), x)','line_number':364,'multiline':False]
['text':' then we replicate this behavior.','line_number':365,'multiline':False]
['text':'physical','line_number':366,'multiline':True]
['text':' copy batch size','line_number':427,'multiline':False]
['text':' Here, we know we are expanding a (logical) tensor to a larger number','line_number':463,'multiline':False]
['text':' of dimensions. We have to be careful because we can't call expand directly','line_number':464,'multiline':False]
['text':' due to the presence of batch dimensions.','line_number':465,'multiline':False]
['text':'','line_number':466,'multiline':False]
['text':' As an example, let B0 be a batch dimension and consider expand(Tensor[B0, 3], [2, 3]).','line_number':467,'multiline':False]
['text':' The result should be a tensor of size [B0, 2, 3].','line_number':468,'multiline':False]
['text':' A physical view of size [B0, 3] can't directly be expanded to size [B0, 2, 3]','line_number':469,'multiline':False]
['text':' so the strategy here is to view it first as a tensor of size [B0, 1, 3] and','line_number':470,'multiline':False]
['text':' then expand.','line_number':471,'multiline':False]
['text':'init_value','line_number':473,'multiline':True]
['text':' CompositeExplicitAutograd, should not go in BatchRulesDecompositions.cpp','line_number':585,'multiline':False]
['text':' CompositeExplicitAutograd, should not go in BatchRulesDecompositions.cpp','line_number':586,'multiline':False]
