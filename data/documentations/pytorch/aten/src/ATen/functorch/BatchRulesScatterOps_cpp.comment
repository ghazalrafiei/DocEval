['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' There are 3 main cases:','line_number':65,'multiline':False]
['text':' 1. self is batched, indices/values are not batched','line_number':66,'multiline':False]
['text':' In this case, we just need to augment indices with a None at the front to','line_number':67,'multiline':False]
['text':' basically broadcast the indexing across the batch dimension of self.','line_number':68,'multiline':False]
['text':'','line_number':69,'multiline':False]
['text':' 2. self is not batched, some indices are batched.','line_number':70,'multiline':False]
['text':' In this case, we don't need to do anything - indices will automatically','line_number':71,'multiline':False]
['text':' broadcast to work with the unbatched self.','line_number':72,'multiline':False]
['text':'','line_number':73,'multiline':False]
['text':' 3. self is batched, some indices are batched.','line_number':74,'multiline':False]
['text':' In this case, we simply need to add an arange that indexes along the first','line_number':75,'multiline':False]
['text':' dimension (i.e. the batch dimension). We also need to make sure this','line_number':76,'multiline':False]
['text':' broadcasts with the rest of the indices.','line_number':77,'multiline':False]
['text':'','line_number':78,'multiline':False]
['text':' In all three cases, depending on if advanced indices are adjacent we will','line_number':79,'multiline':False]
['text':' have to permute the output.','line_number':80,'multiline':False]
['text':' See NOTE: [advanced indexing (index.Tensor) batch rule] for more details','line_number':81,'multiline':False]
['text':'','line_number':82,'multiline':False]
['text':' There is one more case worth mentioning - boolean tensor indices. If we','line_number':83,'multiline':False]
['text':' have "batched" boolean tensor indices, that is unrepresentable, as each','line_number':84,'multiline':False]
['text':' batch would result in a tensor with different values.','line_number':85,'multiline':False]
['text':' do nothing','line_number':112,'multiline':False]
['text':' TODO: this is O(N)','line_number':118,'multiline':False]
['text':' Define an "advanced index" to be a selection object that is','line_number':124,'multiline':False]
['text':' a non-trivial Tensor (i.e. it does not represent :).','line_number':125,'multiline':False]
['text':' See NOTE: [advanced indices adjacent] for definition','line_number':136,'multiline':False]
['text':' Given a Tensor[B, <first_region>, <second_region>, ...]','line_number':154,'multiline':False]
['text':' Swaps the regions to produce Tensor[B, <second_region>, <first_region>, ...]','line_number':155,'multiline':False]
['text':'','line_number':156,'multiline':False]
['text':' Concretely speaking, given','line_number':157,'multiline':False]
['text':' - tensor: Tensor[B, 2, 3, 4, 5, 6, 7, 8]','line_number':158,'multiline':False]
['text':' - first_region_size: 2','line_number':159,'multiline':False]
['text':' - second_region_size: 3','line_number':160,'multiline':False]
['text':' Produces:','line_number':161,'multiline':False]
['text':' - result: Tensor[B, 4, 5, 6, 2, 3, 7, 8]','line_number':162,'multiline':False]
['text':'                     -------  ----','line_number':163,'multiline':False]
['text':'                     region2  region1','line_number':164,'multiline':False]
['text':' NOTE: [advanced indexing (index.Tensor) batch rule]','line_number':181,'multiline':False]
['text':'','line_number':182,'multiline':False]
['text':' This is a three step procedure:','line_number':183,'multiline':False]
['text':' 1. batch `indices`. Depends on self_bdim and indices_bdim.','line_number':184,'multiline':False]
['text':' 2. call at::index','line_number':185,'multiline':False]
['text':' 3. (maybe) reorder the dimensions in the result.','line_number':186,'multiline':False]
['text':' Why is step 3 necessary? Let's take a detour first.','line_number':187,'multiline':False]
['text':'','line_number':188,'multiline':False]
['text':' NOTE: [advanced indices adjacent]','line_number':189,'multiline':False]
['text':' Definition: In a list of optional<Tensor> indices,','line_number':190,'multiline':False]
['text':' we say that "advanced indices are adjacent" if ALL advanced indices are','line_number':191,'multiline':False]
['text':' not separated by a None (slice).','line_number':192,'multiline':False]
['text':'','line_number':193,'multiline':False]
['text':' So, for example,','line_number':194,'multiline':False]
['text':' [:, :, (0, 1), (0, 1), :] -> True','line_number':195,'multiline':False]
['text':' [:, (0, 1), :, (0, 1), :] -> False, the advanced indices are separated by a slice','line_number':196,'multiline':False]
['text':'','line_number':197,'multiline':False]
['text':' See https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing','line_number':198,'multiline':False]
['text':' for more details.','line_number':199,'multiline':False]
['text':'','line_number':200,'multiline':False]
['text':' NOTE: [Why is step 3 necessary?]','line_number':201,'multiline':False]
['text':'','line_number':202,'multiline':False]
['text':' In the original self[*indices] expression,','line_number':203,'multiline':False]
['text':' depending on whether or not the "advanced indices inside `indices` are','line_number':204,'multiline':False]
['text':' adjacent", something different happens.','line_number':205,'multiline':False]
['text':'','line_number':206,'multiline':False]
['text':' For example:','line_number':207,'multiline':False]
['text':' - self: Tensor[4, 5, 6, 7]','line_number':208,'multiline':False]
['text':' - indices: [:, (0, 1), (0, 1), :] (advanced indices are adjacent)','line_number':209,'multiline':False]
['text':' - self[*indices]: Tensor[4, 2, 7]','line_number':210,'multiline':False]
['text':' If advanced indices are adjacent, you get the output you would expect.','line_number':211,'multiline':False]
['text':' (0, 1), (0, 1) says "please index these two dimensions at (0, 0) and (1, 1)','line_number':212,'multiline':False]
['text':' to produce two elements".','line_number':213,'multiline':False]
['text':'','line_number':214,'multiline':False]
['text':' If advanced indices are not adjacent, it is ambiguous to where the new','line_number':215,'multiline':False]
['text':' dimension of size 2 should go. The numpy spec says it should go at the very','line_number':216,'multiline':False]
['text':' front of the Tensor.','line_number':217,'multiline':False]
['text':'','line_number':218,'multiline':False]
['text':' - self: Tensor[4, 5, 6, 7]','line_number':219,'multiline':False]
['text':' - indices: [:, (0, 1), :, (0, 1)] (advanced indices not adjacent)','line_number':220,'multiline':False]
['text':' - self[*indices]: Tensor[2, 4, 6]','line_number':221,'multiline':False]
['text':'','line_number':222,'multiline':False]
['text':' Now, this leads to some weird interactions with vmap.','line_number':223,'multiline':False]
['text':' The indices might originally have adjacent advanced indices, but after','line_number':224,'multiline':False]
['text':' batching them with "batchIndices", they may no longer be adjacent!','line_number':225,'multiline':False]
['text':' - indices: [:, (0, 1), (0, 1)]','line_number':226,'multiline':False]
['text':' - batched_indices (for example): [(0, 1), :, (0, 1), (0, 1)]','line_number':227,'multiline':False]
['text':' This leads to the dimension of size 2 appearing somewhere else.','line_number':228,'multiline':False]
['text':'','line_number':229,'multiline':False]
['text':' There are a couple of different cases that we walk through in the code below.','line_number':230,'multiline':False]
['text':'','line_number':231,'multiline':False]
['text':' Background reading for why we care about if the advanced indices are adjacent:','line_number':232,'multiline':False]
['text':' https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing','line_number':233,'multiline':False]
['text':' Step 1','line_number':238,'multiline':False]
['text':' Step 2','line_number':243,'multiline':False]
['text':' Step 3: There are three cases (these match the cases outlined in batchIndices)','line_number':246,'multiline':False]
['text':' Case 1','line_number':252,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':255,'multiline':False]
['text':' indices: [:, Tensor[2, 2], Tensor[2, 2], :]','line_number':256,'multiline':False]
['text':' batched_indices: [:, :, Tensor[2, 2], Tensor[2, 2], :]','line_number':257,'multiline':False]
['text':' res: Tensor[B, 5, 2, 2, 8]','line_number':258,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7]','line_number':261,'multiline':False]
['text':' indices: [Tensor[2, 2], :, Tensor[2, 2]]','line_number':262,'multiline':False]
['text':' batched_indices: [:, Tensor[2, 2], :, Tensor[2, 2]]','line_number':263,'multiline':False]
['text':' res: Tensor[2, 2, B, 6]','line_number':264,'multiline':False]
['text':' Case 2','line_number':269,'multiline':False]
['text':' self: Tensor[5, 6, 7, 8]','line_number':272,'multiline':False]
['text':' indices: [:, :, Tensor[B, 2, 2], Tensor[2, 2]]','line_number':273,'multiline':False]
['text':' batched_indices: indices (no change)','line_number':274,'multiline':False]
['text':' res: Tensor[5, 6, B, 2, 2]','line_number':275,'multiline':False]
['text':' self: Tensor[5, 6, 7, 8, 9]','line_number':278,'multiline':False]
['text':' indices: [:, :, Tensor[B, 2, 2], :, Tensor[2, 2]]','line_number':279,'multiline':False]
['text':' batched_indices: indices (no change)','line_number':280,'multiline':False]
['text':' res: Tensor[B, 2, 2, 5, 6, 8]','line_number':281,'multiline':False]
['text':' Case 3: self_batched and indices_batched','line_number':286,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':289,'multiline':False]
['text':' indices: [:, Tensor[B, 2, 2], :, Tensor[2, 2]]','line_number':290,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 2), :, Tensor[B, 2, 2], :, Tensor[2, 2]]','line_number':291,'multiline':False]
['text':' res: Tensor[B, 2, 2, 5, 7]','line_number':292,'multiline':False]
['text':' In other words, in batched_indices, advanced indices are adjacent','line_number':295,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':297,'multiline':False]
['text':' indices: [Tensor[B, 2, 2], Tensor[2, 2], :, :]','line_number':298,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 2), Tensor[B, 2, 2], Tensor[2, 2], :, :]','line_number':299,'multiline':False]
['text':' res: Tensor[B, 2, 2, 7, 8]','line_number':300,'multiline':False]
['text':' This is the tricky case. In indices, advanced indices are adjacent.','line_number':303,'multiline':False]
['text':' In batched_indices, advanced indices are no longer adjacent','line_number':304,'multiline':False]
['text':'','line_number':305,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8, 9]','line_number':306,'multiline':False]
['text':' indices: [:, :, Tensor[B, 2, 3], Tensor[2, 3], :]','line_number':307,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 3), :, :, Tensor[B, 2, 3], Tensor[2, 3], :]','line_number':308,'multiline':False]
['text':' res: Tensor[B, 2, 3, 5, 6, 9]','line_number':309,'multiline':False]
['text':' expected: Tensor[B, 5, 6, 2, 3, 9]','line_number':310,'multiline':False]
['text':'','line_number':311,'multiline':False]
['text':' The resolution is to move dims around until we get the right shape.','line_number':312,'multiline':False]
['text':' The result is set up as [B, <maxIndexDim>, <leading_nones>, ...]','line_number':313,'multiline':False]
['text':' we just have to move the <leading_nones> to before the <maxIndexDim> to produce','line_number':314,'multiline':False]
['text':' [B, <leading_nones>, <maxIndexDim>, ...]','line_number':315,'multiline':False]
['text':' plumbing done since we don't support List<optional<Tensor>> in codegen','line_number':319,'multiline':False]
['text':' Code is mostly duplicated from','line_number':349,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/fb0e27d38a8fdab4e1c14d6378c9e41cb30fd6a3','line_number':350,'multiline':False]
['text':' /aten/src/ATen/native/TensorAdvancedIndexing.cpp#L294-L312','line_number':351,'multiline':False]
['text':' Replace indexed dimensions in src with stride 0 and the size of the result tensor.','line_number':367,'multiline':False]
['text':' The offset in these dimensions is computed by the kernel using the index tensor's','line_number':368,'multiline':False]
['text':' values and the stride of src. The new shape is not meaningful. It's used to make','line_number':369,'multiline':False]
['text':' the shape compatible with the result tensor.','line_number':370,'multiline':False]
['text':' Code is mostly duplicated from','line_number':378,'multiline':False]
['text':' https://github.com/pytorch/pytorch/blob/fb0e27d38a8fdab4e1c14d6378c9e41cb30fd6a3','line_number':379,'multiline':False]
['text':' /aten/src/ATen/native/TensorAdvancedIndexing.cpp#L379-L405','line_number':380,'multiline':False]
['text':' first expand BoolTensor (masks) or ByteTensor (masks) into 1 or more LongTensors','line_number':384,'multiline':False]
['text':' next broadcast all index tensors together','line_number':386,'multiline':False]
['text':' add missing null Tensors so that it matches self.dim()','line_number':393,'multiline':False]
['text':' if the non-null indices are not all adjacent, transpose self and indices','line_number':397,'multiline':False]
['text':' together so that they're adjacent at the front','line_number':398,'multiline':False]
['text':' for inplace variants `index_put_` and `_index_put_impl_` we find the batch_size','line_number':416,'multiline':False]
['text':' here while for `index_put` does it outside of this function.','line_number':417,'multiline':False]
['text':' we've already made sure that self has bdim at 0.','line_number':423,'multiline':False]
['text':'self_bdim=','line_number':424,'multiline':True]
['text':' handle broadcasting support for values','line_number':428,'multiline':False]
['text':' Eg. Given `indexed_shape.size()` is 5 and','line_number':429,'multiline':False]
['text':' shape of `values` is (N, 2, 3), then following block','line_number':430,'multiline':False]
['text':' will reshape `values` to (N, 1, 1, 2, 3).','line_number':431,'multiline':False]
['text':' number of unit dims (for broadcasting value to indexed_shape)','line_number':435,'multiline':False]
['text':' add the batch-dim','line_number':439,'multiline':False]
['text':' insert the unit dims for broadcasting.','line_number':442,'multiline':False]
['text':' since batch-dim is already be filled.','line_number':444,'multiline':False]
['text':' since batch and unit dims are already be filled.','line_number':448,'multiline':False]
['text':' namespace','line_number':484,'multiline':False]
['text':' plumbing done since we don't support List<optional<Tensor>> in codegen','line_number':504,'multiline':False]
['text':' plumbing done since we don't support List<optional<Tensor>> in codegen','line_number':543,'multiline':False]
['text':' NB: values has its B dimension at the front','line_number':572,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':575,'multiline':False]
['text':' indices: [:, Tensor[2, 2], Tensor[2, 2], :]','line_number':576,'multiline':False]
['text':' batched_indices: [:, :, Tensor[2, 2], Tensor[2, 2], :]','line_number':577,'multiline':False]
['text':' required values: Tensor[B, 5, 2, 2, 8]','line_number':578,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7]','line_number':581,'multiline':False]
['text':' indices: [Tensor[2, 2], :, Tensor[2, 2]]','line_number':582,'multiline':False]
['text':' batched_indices: [:, Tensor[2, 2], :, Tensor[2, 2]]','line_number':583,'multiline':False]
['text':' required values: Tensor[2, 2, B, 6]','line_number':584,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':588,'multiline':False]
['text':' indices: [:, Tensor[B, 2, 2], :, Tensor[2, 2]]','line_number':589,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 2), :, Tensor[B, 2, 2], :, Tensor[2, 2]]','line_number':590,'multiline':False]
['text':' required values: Tensor[B, 2, 2, 5, 7]','line_number':591,'multiline':False]
['text':' In other words, in batched_indices, advanced indices are adjacent','line_number':594,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8]','line_number':596,'multiline':False]
['text':' indices: [Tensor[B, 2, 2], Tensor[2, 2], :, :]','line_number':597,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 2), Tensor[B, 2, 2], Tensor[2, 2], :, :]','line_number':598,'multiline':False]
['text':' required values: Tensor[B, 2, 2, 7, 8]','line_number':599,'multiline':False]
['text':' This is the tricky case. In indices, advanced indices are adjacent.','line_number':602,'multiline':False]
['text':' In batched_indices, advanced indices are no longer adjacent','line_number':603,'multiline':False]
['text':'','line_number':604,'multiline':False]
['text':' self: Tensor[B, 5, 6, 7, 8, 9]','line_number':605,'multiline':False]
['text':' indices: [:, :, Tensor[B, 2, 3], Tensor[2, 3], :]','line_number':606,'multiline':False]
['text':' batched_indices: [arange(B).expand(B, 2, 3), :, :, Tensor[B, 2, 3], Tensor[2, 3], :]','line_number':607,'multiline':False]
['text':' required values: Tensor[B, 2, 3, 5, 6, 9]','line_number':608,'multiline':False]
['text':' actual values: Tensor[B, 5, 6, 2, 3, 9]','line_number':609,'multiline':False]
['text':'','line_number':610,'multiline':False]
['text':' The resolution is to move dims around until we get the right shape.','line_number':611,'multiline':False]
['text':' The values is set up as [B, <leading_nones>, <maxIndexDim>, ...]','line_number':612,'multiline':False]
['text':' we just have to move the <maxIndexDim> to before the <leading_nones> to produce','line_number':613,'multiline':False]
['text':' [B, <maxIndexDim>, <leading_nones>, ...]','line_number':614,'multiline':False]
['text':' find the batch_size','line_number':628,'multiline':False]
['text':' one or more of the indices is batched.','line_number':633,'multiline':False]
['text':' Why do we need to permute values?','line_number':647,'multiline':False]
['text':' See NOTE [Advanced indexing (index.Tensor) batch rule] for details,','line_number':648,'multiline':False]
['text':' but the gist is that index_put effectively does the following:','line_number':649,'multiline':False]
['text':' - result = self_.clone()','line_number':650,'multiline':False]
['text':' - result[indices_] = values','line_number':651,'multiline':False]
['text':' - return result','line_number':652,'multiline':False]
['text':' Now, the problem is, result[indices_] might return a Tensor whose shape is','line_number':653,'multiline':False]
['text':' the shape of values, but permuted. This is because the shape of result[indices_]','line_number':654,'multiline':False]
['text':' depends on if the original indices "have adjacent advanced indices"','line_number':655,'multiline':False]
['text':' and the batched `indices_` might change the "have adjacent advanced indices" property','line_number':656,'multiline':False]
['text':' plumbing done since we don't support List<optional<Tensor>> in codegen','line_number':663,'multiline':False]
['text':'has_batch_dim','line_number':707,'multiline':True]
['text':' result should have same shape as self','line_number':710,'multiline':False]
['text':'has_batch_dim','line_number':745,'multiline':True]
['text':' result should have same shape as self','line_number':748,'multiline':False]
['text':' namespace','line_number':755,'multiline':False]
['text':'has_batch_dim','line_number':824,'multiline':True]
['text':' result should have same rank as index','line_number':827,'multiline':False]
['text':' setup new_index_shape as [BS, 1, ..., idx_size, ..., 1]','line_number':840,'multiline':False]
['text':' to reshape index_','line_number':841,'multiline':False]
['text':' get non-batch size of index tensor','line_number':842,'multiline':False]
['text':' Now apply expand to index_','line_number':849,'multiline':False]
['text':' output of gather has same dimension as `index` while','line_number':867,'multiline':False]
['text':' output of index_select has same dimension as self','line_number':868,'multiline':False]
['text':' Eg. t = torch.tensor(1)','line_number':869,'multiline':False]
['text':'     idx = torch.tensor([0])','line_number':870,'multiline':False]
['text':'     torch.index_select(t, 0, idx) # 0-D','line_number':871,'multiline':False]
['text':'     torch.gather(t, 0, idx) # 1-D','line_number':872,'multiline':False]
['text':' Note [Fix vmap slice_scatter]','line_number':892,'multiline':False]
['text':' registers a decomposition for `slice_scatter` that calls into `slice.src`','line_number':893,'multiline':False]
['text':' *_scatter operators have some special semantics though, that we can't easily','line_number':894,'multiline':False]
['text':' through a decomposition: slice_scatter's output needs to have the same','line_number':895,'multiline':False]
['text':' size, size, strides and storage_offset as the input.','line_number':896,'multiline':False]
['text':' supports negative index','line_number':910,'multiline':False]
['text':' Handle scalar tensors... self, other can be scalar tensors','line_number':950,'multiline':False]
['text':' Index is batched. For-loop and stack is the best thing I can come up with','line_number':982,'multiline':False]
['text':' right now. We really want generalized index_add kernel in PyTorch','line_number':983,'multiline':False]
['text':' compute max logical rank','line_number':1035,'multiline':False]
['text':' If the dimensions aren't aligned, we need to line them up.','line_number':1043,'multiline':False]
['text':' Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]','line_number':1044,'multiline':False]
['text':' Note that only tensors that have a batch dim need to be modified.','line_number':1045,'multiline':False]
['text':' Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed','line_number':1046,'multiline':False]
['text':' If self_logical_rank == 0, the batch dim is certainly 0, and we must apply batched indices to each row.','line_number':1089,'multiline':False]
['text':' Do for-loop for in-place because we cannot reshape','line_number':1132,'multiline':False]
['text':' `self_` having an incompatible stride without copying.','line_number':1133,'multiline':False]
['text':' Do for-loop for in-place because we cannot reshape','line_number':1184,'multiline':False]
['text':' `self_` having an incompatible stride without copying.','line_number':1185,'multiline':False]
['text':' If value has a batch dim, we do for-loop as well because','line_number':1186,'multiline':False]
['text':' index_fill_ supports 1-element tensor only.','line_number':1187,'multiline':False]
['text':' calling .item() on value is safe here because value is guaranteed to not be a batched tensor.','line_number':1202,'multiline':False]
['text':' as_strided_scatter does not work with the for-loop fallback today,','line_number':1265,'multiline':False]
['text':' because as_strided_scatter will return an output that matches','line_number':1266,'multiline':False]
['text':' the strides/storage_offset of its input.','line_number':1267,'multiline':False]
['text':' With the for loop fallback, each input tensor is a slice into','line_number':1268,'multiline':False]
['text':' the larger batched tensor.','line_number':1269,'multiline':False]
