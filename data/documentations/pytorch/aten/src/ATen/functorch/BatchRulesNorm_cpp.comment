['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' There's a weird case where mean, rstd can both have shape (0,).','line_number':22,'multiline':False]
['text':' It's possible that this is a bug on the PyTorch side.','line_number':23,'multiline':False]
['text':' When that happens we don't want to return a BatchedTensor.','line_number':24,'multiline':False]
['text':' NB: Batch dim, if it exists, is assumed to be the first dim','line_number':32,'multiline':False]
['text':' cudnn and miopen require a weight','line_number':71,'multiline':False]
['text':' without this, get "strides() called on undefined Tensor" on cuda','line_number':72,'multiline':False]
['text':' [C, B, *]','line_number':74,'multiline':False]
['text':'channels dim','line_number':81,'multiline':True]
['text':' cudnn and miopen require a weight','line_number':96,'multiline':False]
['text':' without this, get "strides() called on undefined Tensor" on cuda','line_number':97,'multiline':False]
['text':' [(B0, C), B, *]','line_number':99,'multiline':False]
['text':' [B0, C, B, *]','line_number':100,'multiline':False]
['text':' [B0, C]','line_number':102,'multiline':False]
['text':' [B0, C]','line_number':104,'multiline':False]
['text':' [B0, B, C, *], because some arg must have been batched, the output must be batched','line_number':122,'multiline':False]
['text':' for either of these to have bdims, the input, running_mean, or running_var must have had a bdim','line_number':141,'multiline':False]
['text':' ensure all inputs have bdim.','line_number':155,'multiline':False]
['text':'channels dim','line_number':175,'multiline':True]
['text':' [B0, B, C, *] -> [B, (B0, C), *]','line_number':180,'multiline':False]
['text':' contiguous called if there is a tensor given','line_number':187,'multiline':False]
['text':' contiguous called if there is a tensor given','line_number':188,'multiline':False]
['text':' [B, B0, C, *]','line_number':193,'multiline':False]
['text':' [B0, B, C, *]','line_number':194,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':210,'multiline':False]
['text':' NB: not sure why these are optional...these are required from the forward','line_number':217,'multiline':False]
['text':' plumbing','line_number':223,'multiline':False]
['text':' results','line_number':257,'multiline':False]
['text':' batch_norm can't operate on 1D tensors so the output will be at least 2D','line_number':262,'multiline':False]
['text':' NB: output isn't saved...','line_number':267,'multiline':False]
['text':' [B0, C, B, *]','line_number':276,'multiline':False]
['text':' [B0, B, C, *]','line_number':280,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':300,'multiline':False]
['text':' [B0 * N, C, *]','line_number':369,'multiline':False]
['text':' [B0 * N, C, *]','line_number':370,'multiline':False]
['text':' [B0 * N, G]','line_number':371,'multiline':False]
['text':' [B0 * N, G]','line_number':372,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':390,'multiline':False]
['text':' plumbing','line_number':394,'multiline':False]
['text':' results','line_number':419,'multiline':False]
['text':' group_norm can't operate on 1D tensors so the output will be at least 2D','line_number':424,'multiline':False]
['text':' (0, 1, 2), 1 -> (0, 2, 3)','line_number':469,'multiline':False]
['text':' Ugh, hard to deduplicate','line_number':491,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':523,'multiline':False]
['text':'has_bdim','line_number':539,'multiline':True]
['text':'has_bdim','line_number':547,'multiline':True]
['text':' ensure grad_out / input have bdim.','line_number':572,'multiline':False]
['text':' See [Note: hacky wrapper removal for optional tensor]','line_number':599,'multiline':False]
['text':' plumbing','line_number':605,'multiline':False]
['text':' results','line_number':637,'multiline':False]
['text':' NB: output isn't saved...','line_number':651,'multiline':False]
['text':' in experiments, reserve was never set to anything other than empty by cuda','line_number':706,'multiline':False]
['text':' this should be ensured by batch_norm_impl','line_number':874,'multiline':False]
['text':' NB: This is NOT good. In the ideal world, we do NOT want to convert the new legit op back into native_batch_norm','line_number':878,'multiline':False]
['text':' as native_batch_norm has a problematic schema--it promises it is functional when it is not. However, vmap doesn't','line_number':879,'multiline':False]
['text':' work with dynamo anyway so we gain some buffer room to do wrong things here. The (reasonable) hope is that we will','line_number':880,'multiline':False]
['text':' make native_batch_norm composite implicit within a few weeks and we can fix this before vmap works with dynamo.','line_number':881,'multiline':False]
