['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' functorch stores some TLS. Inside the TLS is the stack of transforms.','line_number':77,'multiline':False]
['text':' Unfortunately, since functorch isn't a part of libtorch, we have','line_number':78,'multiline':False]
['text':' a level of indirection. FuncTorchTLSBase is the interface that lives in libtorch,','line_number':79,'multiline':False]
['text':' while FuncTorchTLS implements all the methods and stores data.','line_number':80,'multiline':False]
['text':'','line_number':81,'multiline':False]
['text':' TODO: after functorch C++ code is moved into PyTorch, we can get rid of','line_number':82,'multiline':False]
['text':' this layer of indirection.','line_number':83,'multiline':False]
['text':' Raw pointer usage OK, `state` keeps the pointer alive','line_number':137,'multiline':False]
['text':' NB: this function should be called while holding the GIL to avoid races','line_number':262,'multiline':False]
['text':' NB: this function should be called while holding the GIL to avoid races','line_number':279,'multiline':False]
['text':' Tensor?[] translates to a c10::List<IValue> so we need to peek inside List','line_number':319,'multiline':False]
['text':' TODO: might be more efficient if we scan first then not copy? Depends.','line_number':322,'multiline':False]
['text':' sanity checks','line_number':350,'multiline':False]
['text':' Check that the first argument is being written to','line_number':374,'multiline':False]
['text':' Check that none of the other args are being aliased','line_number':379,'multiline':False]
['text':' Check that the first tensor is being returned (i.e., output has a (a!))','line_number':386,'multiline':False]
['text':' for everything currently in native_functions, each input aliases at most one output (tensor list counts as one output)','line_number':394,'multiline':False]
['text':' NOTE: [functorch front and back key fallbacks]','line_number':419,'multiline':False]
['text':'','line_number':420,'multiline':False]
['text':' Please read NOTE: [functorch interpreter stack] first for some context.','line_number':421,'multiline':False]
['text':' The following doc also provides some visuals:','line_number':422,'multiline':False]
['text':' https://docs.google.com/document/d/14qyaa3xIjmVxYiMLlIlQErunYgR_uR1WupsKMZlnGY4/edit','line_number':423,'multiline':False]
['text':'','line_number':424,'multiline':False]
['text':' functorch's "stack of transforms" is implemented as the following:','line_number':425,'multiline':False]
['text':' - each transform is associated with one or more dispatch keys in the PyTorch','line_number':426,'multiline':False]
['text':'   dispatcher. For example, vmap -> {FuncTorchBatched, FuncTorchVmapMode},','line_number':427,'multiline':False]
['text':'   Autograd -> {Autograd{Backend}, ADInplaceOrView}','line_number':428,'multiline':False]
['text':' - Whenever a functorch transform is active, the FuncTorchDynamicLayer{Front, Back}Mode','line_number':429,'multiline':False]
['text':'   keys are added to the dispatcher's local dispatch key set.','line_number':430,'multiline':False]
['text':'','line_number':431,'multiline':False]
['text':' DynamicLayerFrontMode is responsible for:','line_number':432,'multiline':False]
['text':' 1. selecting the transform that is at the top of the stack and grabbing its','line_number':433,'multiline':False]
['text':'    interpreter','line_number':434,'multiline':False]
['text':' 2. Calling interpreter.process(), which does the following:','line_number':435,'multiline':False]
['text':' 2a. enables/disables a bunch of dispatch keys, so that the only dispatch','line_number':436,'multiline':False]
['text':'     keys that are enabled are the ones that belong to the transform.','line_number':437,'multiline':False]
['text':' 2b. redispatching','line_number':438,'multiline':False]
['text':'','line_number':439,'multiline':False]
['text':' Eventually, DynamicLayerBackMode captures the redispatch from the transforms.','line_number':440,'multiline':False]
['text':' DynamicLayerBackMode is responsible for:','line_number':441,'multiline':False]
['text':' - redirecting back to DynamicLayerFrontMode','line_number':442,'multiline':False]
['text':' Save the current LocalDispatchKeySet (to the current DynamicLayer).','line_number':455,'multiline':False]
['text':' Upon exiting the current scope, that LocalDispatchKeySet gets restored.','line_number':456,'multiline':False]
['text':' When the current DynamicLayer dispatches to the next (inner) DynamicLayer,','line_number':457,'multiline':False]
['text':' it will also temporarily restore the saved LocalDispatchKeySet.','line_number':458,'multiline':False]
['text':' Unwrap escaped GradWrappers','line_number':461,'multiline':False]
['text':' right now grad_special_case as a bool is sufficient because this is the only special case for grad. If we need to add','line_number':474,'multiline':False]
['text':' more special cases, it's more scalable to add an enum to know which op we're looking at without looking at the schema','line_number':475,'multiline':False]
['text':' WithoutTop stores the popped DynamicLayer object.','line_number':481,'multiline':False]
['text':' used for functions that have aliasing operations but should be treated like they're out of place (i.e. lift_fresh)','line_number':485,'multiline':False]
['text':' lift_fresh: it's must be freshly allocated and should be wrapped. User shouldn't have access to input version','line_number':507,'multiline':False]
['text':' alias: this is needed for the CompositeImplicit instance norm (running_mean/var get set to be a wrapped value)','line_number':508,'multiline':False]
['text':'        It's not a user facing function, but is more prone to possible errors','line_number':509,'multiline':False]
['text':' namespace at','line_number':515,'multiline':False]
