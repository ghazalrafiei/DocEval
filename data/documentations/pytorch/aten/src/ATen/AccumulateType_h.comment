['text':' Defines the accumulation type for a scalar type.','line_number':10,'multiline':False]
['text':' Example:','line_number':11,'multiline':False]
['text':'   using accscalar_t = acc_type<scalar_t, /*is_cuda*/true>;','line_number':12,'multiline':False]
['text':'','line_number':13,'multiline':False]
['text':' Accumulation types are an important concept in numeric computing','line_number':14,'multiline':False]
['text':' because you frequently want to perform intermediate computations','line_number':15,'multiline':False]
['text':' at a higher precision than the input and output precision, to avoid','line_number':16,'multiline':False]
['text':' compounding internal rounding errors.  Accumulation is the most','line_number':17,'multiline':False]
['text':' well-known intermediate computation (it is of great importance for','line_number':18,'multiline':False]
['text':' sum reduction and matrix multiply, for example), but in PyTorch','line_number':19,'multiline':False]
['text':' acc_type ends up getting used for all sorts of other intermediate','line_number':20,'multiline':False]
['text':' computations, so it perhaps would be more accurately (ahem) called an','line_number':21,'multiline':False]
['text':' "accurate" type.  acc_type is especially important for reduced','line_number':22,'multiline':False]
['text':' precision operations like float16 and bfloat16, where relatively','line_number':23,'multiline':False]
['text':' benign looking inputs can easily end up overflowing/underflowing.','line_number':24,'multiline':False]
['text':'','line_number':25,'multiline':False]
['text':' acc_type is parametrized by whether or not you are running on CUDA','line_number':26,'multiline':False]
['text':' or not, because on CUDA double precision operations are expensive','line_number':27,'multiline':False]
['text':' and so by default, we don't actually want to use double as an','line_number':28,'multiline':False]
['text':' acc_type on CUDA.  A lot of things are typed out below, but','line_number':29,'multiline':False]
['text':' basically, the table is generated by a few rules:','line_number':30,'multiline':False]
['text':'','line_number':31,'multiline':False]
['text':'  If bool:','line_number':32,'multiline':False]
['text':'      Use 'bool' as acc_type.','line_number':33,'multiline':False]
['text':'  If floating point:','line_number':34,'multiline':False]
['text':'      If CUDA, use 'float' as acc_type (unless scalar_t is double),','line_number':35,'multiline':False]
['text':'      otherwise (CPU) use 'double'','line_number':36,'multiline':False]
['text':'  If integral:','line_number':37,'multiline':False]
['text':'      Use 'int64_t' as acc_type','line_number':38,'multiline':False]
['text':'','line_number':39,'multiline':False]
['text':' You're not forced to use this template; if you happen to know','line_number':40,'multiline':False]
['text':' something specific about your use case, you can specify your own','line_number':41,'multiline':False]
['text':' desired behavior.  This template, however, will give you a reasonable','line_number':42,'multiline':False]
['text':' default that will work for all dtypes supported in PyTorch.','line_number':43,'multiline':False]
['text':' namespace at','line_number':147,'multiline':False]
