['text':' namespace','line_number':72,'multiline':False]
['text':'*******************************************************************
Logic to extract the promote type from any Tensor or TensorList args.
*******************************************************************','line_number':120,'multiline':True]
['text':' Overload to catch Tensor args.','line_number':124,'multiline':False]
['text':' If nextArg is floating-point, compare its scalar_type with our','line_number':125,'multiline':False]
['text':' current best guess for the promote type, and update if necessary.','line_number':126,'multiline':False]
['text':' ignores double tensors','line_number':140,'multiline':False]
['text':' prioritizes float over lower_precision_fp','line_number':142,'multiline':False]
['text':' Overload to catch TensorList args (for e.g. cat, stack).','line_number':154,'multiline':False]
['text':' Reuses the overload above to process each Tensor in the list.','line_number':155,'multiline':False]
['text':' Template to catch non-Tensor args (no-op that returns current best guess)','line_number':176,'multiline':False]
['text':' Overload for the tail case.','line_number':185,'multiline':False]
['text':' Unpack args and determine if incoming lower_precision_fp tensors need to be','line_number':192,'multiline':False]
['text':' promoted to float32. Non-Tensor arguments are ignored.','line_number':193,'multiline':False]
['text':'***************************************************
Logic to apply cached casting to any Tensor argument.
***************************************************','line_number':204,'multiline':True]
['text':' Overload to catch Tensor args','line_number':215,'multiline':False]
['text':' Overload to process optional<Tensor>','line_number':221,'multiline':False]
['text':' Overload to process TensorLists','line_number':233,'multiline':False]
['text':' Template to catch non-Tensor args.','line_number':258,'multiline':False]
['text':'******************************************************
Logic to flip an output dtype flag.
Keep it simple for now by assuming only one such flag is
present in the argument list.  If I ever need a function
with more than flag I'll figure out something else.
The policy is:
If the user has explicity specified a dtype, respect it.
Otherwise, set it to the autocast type.
*******************************************************','line_number':267,'multiline':True]
['text':' Overload to catch dtype flags','line_number':277,'multiline':False]
['text':' Template to catch other args','line_number':284,'multiline':False]
['text':' Policies correspond to op categories that need code-divergent handling.','line_number':307,'multiline':False]
['text':' Wrapper templates below are specialized based on a policy template parameter.','line_number':308,'multiline':False]
['text':' Cast all inputs to lower_precision_fp before','line_number':310,'multiline':False]
['text':' running the op. Currently, lower_precision_fp is','line_number':311,'multiline':False]
['text':' fp16 for AutocastCUDA, and is defined by user','line_number':312,'multiline':False]
['text':' (default bf16) for AutocastCPU or other device.','line_number':313,'multiline':False]
['text':' Cast all inputs to at::kFloat before running the op.','line_number':314,'multiline':False]
['text':' Treats functions (like softmax) that','line_number':315,'multiline':False]
['text':'  1. we'd like to run in fp32 and','line_number':316,'multiline':False]
['text':'  2. have a c10::optional<ScalarType> arg that controls','line_number':317,'multiline':False]
['text':'  the output type.','line_number':318,'multiline':False]
['text':' fp32_set_opt_dtype wrappers' policy is: if the output','line_number':319,'multiline':False]
['text':' type is already set, don't touch it, otherwise, set','line_number':320,'multiline':False]
['text':' it to at::kFloat.','line_number':321,'multiline':False]
['text':' Treats functions (like norm) that','line_number':322,'multiline':False]
['text':'  1. we'd like to run in fp32 and','line_number':323,'multiline':False]
['text':'  2. have some overloads that accept an output type and','line_number':324,'multiline':False]
['text':'  other overloads that don't.','line_number':325,'multiline':False]
['text':' fp32_append_dtype wrappers wrap the overloads that don't','line_number':326,'multiline':False]
['text':' have an output dtype.','line_number':327,'multiline':False]
['text':' The wrapper policy is:  append at::kFloat to the args,','line_number':328,'multiline':False]
['text':' and redispatch to the type-aware overload.','line_number':329,'multiline':False]
['text':' Run in the widest dtype among several args.','line_number':330,'multiline':False]
['text':'*******************************************************************************************************
Templates to provide wrapper functions

I'm copying the pattern used in core/boxing/impl/WrapFunctionIntoFunctor.h to
extract args and return type. (see also
https://stackoverflow.com/questions/46533698/how-to-deduce-argument-list-from-function-pointer)

This strategy uses an exterior "WrapFunction" that extracts arguments on behalf
of (in my case several specializations of) an interior "WrapFunction_".
Interior WrapFunction_ specializations are defined for each CastPolicy.
*******************************************************************************************************','line_number':333,'multiline':True]
['text':' Base template for WrapFunction_, which is specialized to contain a "call"','line_number':345,'multiline':False]
['text':' method each CastPolicy','line_number':346,'multiline':False]
['text':' CastPolicy::lower_precision_fp General_DeviceType','line_number':356,'multiline':False]
['text':' CastPolicy::fp32 General_DeviceType','line_number':380,'multiline':False]
['text':' CastPolicy::fp32_set_opt_dtype General_DeviceType','line_number':401,'multiline':False]
['text':' If ineligible, calls F with unaltered args.  Does not set opt dtype,','line_number':421,'multiline':False]
['text':' because setting opt dtype explicitly may interfere with internal','line_number':422,'multiline':False]
['text':' implicit promotion decisions.','line_number':423,'multiline':False]
['text':' CastPolicy::fp32_append_dtype General_DeviceType','line_number':429,'multiline':False]
['text':' CastPolicy::promote General_DeviceType','line_number':452,'multiline':False]
['text':' Wrapper to infer return_type and parameter_types for WrapFunction_ (imitating','line_number':477,'multiline':False]
['text':' core/boxing/impl/WrapFunctionIntoFunctor.h)','line_number':478,'multiline':False]
['text':' The signature for which we're registering.  The','line_number':482,'multiline':False]
['text':' dispatcher's calling code invokes our registered','line_number':483,'multiline':False]
['text':' functions with arguments matching Registered, so we','line_number':484,'multiline':False]
['text':' register WrapFunction_::call methods with a matching','line_number':485,'multiline':False]
['text':' signature to properly field those arguments.','line_number':486,'multiline':False]
['text':' guts::function_traits below extracts return_type and','line_number':487,'multiline':False]
['text':' parameter_types from Registered, which WrapFunction_','line_number':488,'multiline':False]
['text':' templates above use to declare their call methods.','line_number':489,'multiline':False]
['text':' The signature for the function we're redispatching to.','line_number':490,'multiline':False]
['text':' In most cases this is the same as Registered, but for','line_number':491,'multiline':False]
['text':' some ops (for example, ops where we append a dtype)','line_number':492,'multiline':False]
['text':' it's useful to redispatch to a function with a','line_number':493,'multiline':False]
['text':' different signature.','line_number':494,'multiline':False]
['text':' The actual function we're redispatching to.','line_number':495,'multiline':False]
['text':'****************************************************************************************************************
This section performs load-time registration for autocast wrappers.

It's debatable at what level operations should be patched.  We'd like casts to
be autograd-exposed and precede autograd history recording, so that for
lower_precision_fp ops, input tensors are saved for backward in
lower_precision_fp rather than fp32.  Saving inputs in lower_precision_fp
can significantly reduce a model's memory footprint.

Option 1 (strawman):  Patch only at the level of explicit calls into
cudnn/cublas (cudnn_convolution, etc), because those are the code paths that are
guaranteed to use Tensor Cores, therefore they're the ones that will benefit
most from lower_precision_fp.   Potential pitfall:  convolutions (and other ops)
are wrapped in several layers of at::* calls.  If one of those happens to record
autograd history, then we've lost the opportunity to save inputs in
lower_precision_fp.

Option 2:  Patch the Python-exposed surface of calls, to make 100% sure autograd
history recording can't sneak in ahead of autocast.  This mirrors Apex most
closely.

I think Option 2 is the right answer for all ops, not just convolutions. Option
2 is what I implement here.
****************************************************************************************************************','line_number':506,'multiline':True]
['text':'*******************************************************************************************************************
Explicit registration for out-of-place ops

The stuff below could be codegenned.  Ed said
> you are going to have to write the function definition at some point, I
wouldn't try to get clever about it Therefore, for the moment, this is all
copy pasted in from VariableTypeEverything.cpp with appropriate substitutions.
*******************************************************************************************************************','line_number':531,'multiline':True]
['text':' namespace at::autocast','line_number':540,'multiline':False]
['text':' Common cases where registration signature matches redispatch signature','line_number':544,'multiline':False]
['text':' (that's why SIGNATURE is repeated in the WrapFunction instantiation)','line_number':545,'multiline':False]
['text':' Less-common but still useful case: redispatching to a function','line_number':566,'multiline':False]
['text':' with a new signature (e.g. appending a dtype)','line_number':567,'multiline':False]
['text':' KERNEL_CPU/KERNEL_CPU2/KERNEL_DIFFERENT_REDISPATCH_SIGNATURE_CPU','line_number':584,'multiline':False]
['text':' registration for AutocastCPU','line_number':585,'multiline':False]
['text':' KERNEL_CUDA/KERNEL_CUDA2/KERNEL_DIFFERENT_REDISPATCH_SIGNATURE_CUDA','line_number':605,'multiline':False]
['text':' registration for AutocastCUDA','line_number':606,'multiline':False]
['text':' KERNEL_PRIVATEUSEONE/KERNEL_PRIVATEUSEONE2/','line_number':626,'multiline':False]
['text':' KERNEL_DIFFERENT_REDISPATCH_SIGNATURE_PRIVATEUSEONE','line_number':627,'multiline':False]
['text':' registration for AutocastPrivateUse1','line_number':628,'multiline':False]
