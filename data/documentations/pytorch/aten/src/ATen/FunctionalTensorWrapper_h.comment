['text':' Note [Functionalization Pass In Core]','line_number':16,'multiline':False]
['text':' The Functionalization pass is used to remove aliasing from a pytorch program.','line_number':17,'multiline':False]
['text':'','line_number':18,'multiline':False]
['text':' This is useful for backends that don't support aliasing, like XLA and Vulkan.','line_number':19,'multiline':False]
['text':' It's also necessary in order to remove mutation from a program, which is','line_number':20,'multiline':False]
['text':' needed in Functorch.','line_number':21,'multiline':False]
['text':'','line_number':22,'multiline':False]
['text':' Consider this program:','line_number':23,'multiline':False]
['text':' a = torch.ones(...)','line_number':24,'multiline':False]
['text':' b = a.view(...)','line_number':25,'multiline':False]
['text':' b.add_(1)','line_number':26,'multiline':False]
['text':'','line_number':27,'multiline':False]
['text':' In this program, b is meant to alias with a due to the use of view(). At the','line_number':28,'multiline':False]
['text':' end of the program, both a and b are full of 2's. However, backends that','line_number':29,'multiline':False]
['text':' don't support aliasing aren't able to correctly implement the view()','line_number':30,'multiline':False]
['text':' operator. Instead, they can opt into the Functionalization pass, which will','line_number':31,'multiline':False]
['text':' sit between the user and the backend, and provide the necessary aliasing','line_number':32,'multiline':False]
['text':' logic.','line_number':33,'multiline':False]
['text':'','line_number':34,'multiline':False]
['text':' The functionalization pass will turn the above program into a slightly','line_number':35,'multiline':False]
['text':' different program that has the same semantics, transparently to the user,','line_number':36,'multiline':False]
['text':' that backends like XLA/Vulkan are able to implement a = torch.ones(...) b =','line_number':37,'multiline':False]
['text':' a.view_copy(...)  # view() replaced with view_copy(). Backends like','line_number':38,'multiline':False]
['text':' XLA/Vulkan can implement this! b.add_(1) a.add_(1)  # Our functionalization','line_number':39,'multiline':False]
['text':' pass machinery knows that a and b are aliased - it applies b's mutation to a','line_number':40,'multiline':False]
['text':' too.','line_number':41,'multiline':False]
['text':'','line_number':42,'multiline':False]
['text':' So, how does the functionalization pass keep track of which tensors are','line_number':43,'multiline':False]
['text':' aliased? The pass works by wrapping EVERY tensor in the program inside of a','line_number':44,'multiline':False]
['text':' FunctionalTensorWrapper, which knows about its alias'd tensors.','line_number':45,'multiline':False]
['text':'','line_number':46,'multiline':False]
['text':' See Note [Functionalization: Alias Removal] for details on the aliasing','line_number':47,'multiline':False]
['text':' machinery. See Note [Functionalization: Mutation Removal] for details on','line_number':48,'multiline':False]
['text':' mutation removal.','line_number':49,'multiline':False]
['text':' Additional constructor to create a FunctionalTensorWrapper directly from an','line_number':52,'multiline':False]
['text':' underlying tensor that was created from a view. For example, the code b =','line_number':53,'multiline':False]
['text':' a.view1() will generate a constructor call to FunctionalTensorWrapper(b, a,','line_number':54,'multiline':False]
['text':' view1_meta)','line_number':55,'multiline':False]
['text':' Get the underlying, actual tensor, that doesn't know anything about','line_number':61,'multiline':False]
['text':' functionalization.','line_number':62,'multiline':False]
['text':' The concept of "level" is only ever important to functorch; it's exposed','line_number':66,'multiline':False]
['text':' here as more of a hook for functorch to use.','line_number':67,'multiline':False]
['text':' Denotes a mutation that's hidden from autograd,','line_number':78,'multiline':False]
['text':' e.g. for the purposes of passing a tensor to a triton kernel','line_number':79,'multiline':False]
['text':' Are all the mutations happening to the tensor hidden from autograd','line_number':86,'multiline':False]
['text':' Did all mutations happen under no_grad or inference_mode','line_number':90,'multiline':False]
['text':' (We also need to ignore mutations fully hidden from autograd here)','line_number':91,'multiline':False]
['text':' Sync's the underlying tensor with its alias, if it's out of date. This','line_number':98,'multiline':False]
['text':' involves two steps: 1) Apply any pending updates/mutations to the alias 2)','line_number':99,'multiline':False]
['text':' Replay the views (if any) to regenerate the current tensor off of the','line_number':100,'multiline':False]
['text':' updated alias.','line_number':101,'multiline':False]
['text':' Performs step (1) of the sync. This is its own public API because it's','line_number':103,'multiline':False]
['text':' needed by view_inplace ops like transpose_. See Note [Functionalization','line_number':104,'multiline':False]
['text':' Pass - Inplace View Ops]','line_number':105,'multiline':False]
['text':' Performs step (2) of the sync. This is its own public API because it's','line_number':107,'multiline':False]
['text':' needed by functorch. functorch wants to make sure that all input tensors to','line_number':108,'multiline':False]
['text':' a functionalized program have been properly synced so it can properly','line_number':109,'multiline':False]
['text':' propagate mutations to inputs. It can't just call sync_(), because the','line_number':110,'multiline':False]
['text':' FunctionalTensorWrapper will look like it has no aliases and sync_ will be','line_number':111,'multiline':False]
['text':' a noop. We use the reference count on storage_ to determine if the wrapper','line_number':112,'multiline':False]
['text':' is aliased, and by the time functorch is ready to propagate updates to','line_number':113,'multiline':False]
['text':' inputs, any intermediate views of the input created by the program will','line_number':114,'multiline':False]
['text':' have been deallocated. This function also returns whether or not the base','line_number':115,'multiline':False]
['text':' actually had any updates to apply.','line_number':116,'multiline':False]
['text':' Takes the current state of value_ and snapshots it, sending it as a pending','line_number':118,'multiline':False]
['text':' update to the alias.','line_number':119,'multiline':False]
['text':' When any tensor is mutated, the tensor increments its alias's "generation".','line_number':121,'multiline':False]
['text':' Separately, each tensor maintains its own "generation" counter, which is','line_number':122,'multiline':False]
['text':' used to determine if it's up-to-date with its alias. The act of syncing a','line_number':123,'multiline':False]
['text':' tensor will set a tensor's generation equal to its alias's generation.','line_number':124,'multiline':False]
['text':' Freezes the storage of this tensor, preventing subsequent mutations','line_number':126,'multiline':False]
['text':' Every FunctionalTensorWrapper contains a vector<ViewMeta> objects','line_number':128,'multiline':False]
['text':' describing the series of view ops that ran to generate the current tensor','line_number':129,'multiline':False]
['text':' from the base tensor. This method is used by inplace-view ops like','line_number':130,'multiline':False]
['text':' transpose_. It appends a ViewMeta to the existing stack, and refreshes the','line_number':131,'multiline':False]
['text':' tensor by replaying the views off of the alias.','line_number':132,'multiline':False]
['text':' Custom implementation of self.set_(src)','line_number':135,'multiline':False]
['text':' Returns whether the current tensor's data was ever mutated','line_number':138,'multiline':False]
['text':'','line_number':140,'multiline':False]
['text':' Returns whether the current FunctionalTensorWrapper','line_number':141,'multiline':False]
['text':' experienced a set_() call.','line_number':142,'multiline':False]
['text':' The functionalization pass can be used to remove mutations.','line_number':147,'multiline':False]
['text':' It does so by replacing any mutation op with it's corresponding','line_number':148,'multiline':False]
['text':' out-of-place op, followed by a call to replace_(). e.g:','line_number':149,'multiline':False]
['text':'','line_number':150,'multiline':False]
['text':' a.add_(1)','line_number':151,'multiline':False]
['text':'','line_number':152,'multiline':False]
['text':' will turn into:','line_number':153,'multiline':False]
['text':'','line_number':154,'multiline':False]
['text':' tmp = a.add(1)','line_number':155,'multiline':False]
['text':' a.replace_(tmp)','line_number':156,'multiline':False]
['text':'','line_number':157,'multiline':False]
['text':' replace_() swaps out the wrapped tensor, value_, with tmp.','line_number':158,'multiline':False]
['text':' See Note[resize_() in functionalization pass]','line_number':165,'multiline':False]
['text':' Replaces the storage with a new functional storage,','line_number':168,'multiline':False]
['text':' and clears the view_metas_ stack.','line_number':169,'multiline':False]
['text':' WARNING: Calling this function will sever the aliasing relationship between','line_number':170,'multiline':False]
['text':' the current FunctionalTensorWrapper and any of its outstanding aliases.','line_number':171,'multiline':False]
['text':' Please only call if you know what you're doing.','line_number':172,'multiline':False]
['text':' FunctionalTensorWrapper overrides all custom size/stride function,','line_number':185,'multiline':False]
['text':' so that if the inner tensor has a custom implementation','line_number':186,'multiline':False]
['text':' we make sure to call that implementation.','line_number':187,'multiline':False]
['text':' This is used to re-implement shallow_copy_and_detach for','line_number':204,'multiline':False]
['text':' FunctionalTensorWrapper. The implementation is identical, but we just need','line_number':205,'multiline':False]
['text':' to return a subclass instead of a plain TensorImpl.','line_number':206,'multiline':False]
['text':' TODO: maybe it's possible to arrange for that to happen automatically','line_number':207,'multiline':False]
['text':' without an override here?','line_number':208,'multiline':False]
['text':' Note that value is not taken by reference: internally, the wrapper will','line_number':214,'multiline':False]
['text':' change the value tensor that it points to over time.','line_number':215,'multiline':False]
['text':' These two counters are used for identifying','line_number':218,'multiline':False]
['text':' whether all the mutations on a given tensor are hidden from autograd or','line_number':219,'multiline':False]
['text':' not. If we have an input mutation that is hidden from autograd, then once','line_number':220,'multiline':False]
['text':' we convert the input mutation to a copy_() we know it will be safe to hide','line_number':221,'multiline':False]
['text':' the copy_() from autograd as well.','line_number':222,'multiline':False]
['text':' Did the tensor experience a set_() call.','line_number':228,'multiline':False]
['text':' Utility functions for the functionalization pass.','line_number':235,'multiline':False]
['text':' These two methods are XLA-specific logic and are no-ops','line_number':296,'multiline':False]
['text':' for the normal functionalization flow.','line_number':297,'multiline':False]
['text':'  ~~~~~ TLS used in functionalization ~~~~~','line_number':322,'multiline':False]
['text':' namespace impl','line_number':351,'multiline':False]
['text':' Helper function to call an out-of-place composite aten kernel that may use','line_number':353,'multiline':False]
['text':' mutations / views internally, and functionalize them.','line_number':354,'multiline':False]
['text':' BoxedKernelWrapper knows to ignore this keyset argument,','line_number':376,'multiline':False]
['text':' because functionalize_op_helper doesn't take in a DispatchKeySet','line_number':377,'multiline':False]
['text':' namespace functionalization','line_number':391,'multiline':False]
['text':' namespace at','line_number':392,'multiline':False]
