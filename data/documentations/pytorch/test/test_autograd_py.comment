['text':' Owner(s): ["module: autograd"]','line_number':1,'multiline':False]
['text':' Ensure that the error Node works','line_number':86,'multiline':False]
['text':' Ensure that if we don't ask for y, it doesn't crash','line_number':95,'multiline':False]
['text':' Ensure that we don't run extra view Node','line_number':105,'multiline':False]
['text':' This should never be called!','line_number':110,'multiline':False]
['text':' Decorating class is deprecated and should not be used','line_number':123,'multiline':False]
['text':' Not applied to methods','line_number':131,'multiline':False]
['text':' Show that we can actually construct the class','line_number':134,'multiline':False]
['text':' Decorating functions or methods is fine though','line_number':138,'multiline':False]
['text':' Accessing .grad on leaf','line_number':164,'multiline':False]
['text':' Accessing .grad on non-leaf','line_number':169,'multiline':False]
['text':' Accessing .grad on non-leaf that retains gradients','line_number':174,'multiline':False]
['text':' NOTE: self is the test case here','line_number':205,'multiline':False]
['text':' Test that undefined tensors returned from custom backward function','line_number':272,'multiline':False]
['text':' are propagated as undefined and not tensor full of zeroes','line_number':273,'multiline':False]
['text':' Trigger exception','line_number':336,'multiline':False]
['text':' Check exception occurs','line_number':344,'multiline':False]
['text':' Save scale','line_number':381,'multiline':False]
['text':' Verify grads','line_number':389,'multiline':False]
['text':' Validate running backward.','line_number':418,'multiline':False]
['text':' Test gradcheck','line_number':424,'multiline':False]
['text':' test to ensure grad(grad)check runs successfully even if there is an','line_number':475,'multiline':False]
['text':' unrelated (but differentiable) inputs','line_number':476,'multiline':False]
['text':' if grad for nextafter ends up being implemented, this should be changed','line_number':489,'multiline':False]
['text':' if forward AD ends up being implemented for torch.igamma, choose a different op','line_number':507,'multiline':False]
['text':' .backward(inputs=) is OK','line_number':549,'multiline':False]
['text':' .backward() is OK','line_number':554,'multiline':False]
['text':' .grad is NOT OK when leaf is passed (this is the current state, subject to change)','line_number':559,'multiline':False]
['text':' .grad is OK when non-leaf is passed','line_number':563,'multiline':False]
['text':' Check a non-leaf','line_number':568,'multiline':False]
['text':' Verify other errors are raised','line_number':576,'multiline':False]
['text':' Multiple outputs with some non-Tensor outputs.','line_number':624,'multiline':False]
['text':' Accumulate in-place when create_graph is False','line_number':699,'multiline':False]
['text':' Accumulate out-of-place when create_graph is False','line_number':703,'multiline':False]
['text':' Accumulate dense gradient to sparse gradient will change the `params.grad` reference','line_number':716,'multiline':False]
['text':' never accumulates in-place','line_number':720,'multiline':False]
['text':' Accumulate dense gradient to dense gradient will preserve the `params.grad` reference,','line_number':723,'multiline':False]
['text':' but only if create_graph=False.','line_number':724,'multiline':False]
['text':' Accumulate sparse gradient to sparse gradient will preserve the `params.grad` reference,','line_number':731,'multiline':False]
['text':' but only if create_graph=False.','line_number':732,'multiline':False]
['text':' Test that grad_outputs and outputs have the same shape','line_number':791,'multiline':False]
['text':' We need to ensure that our main types of Node work (regular cpp Nodes,','line_number':811,'multiline':False]
['text':' AccumulateGrad Nodes and custom Function)','line_number':812,'multiline':False]
['text':' Works','line_number':840,'multiline':False]
['text':' Works','line_number':873,'multiline':False]
['text':' All should work in this case','line_number':889,'multiline':False]
['text':' This checks an edge case for function callbacks','line_number':924,'multiline':False]
['text':' We want to capture two grads of a function, but can only','line_number':925,'multiline':False]
['text':' register a single callback.','line_number':926,'multiline':False]
['text':' This checks an edge case for register_hook.','line_number':945,'multiline':False]
['text':' We want to capture grad of a nonleaf tensor,','line_number':946,'multiline':False]
['text':' but avoid segfault during backward of other nonleaf tensors','line_number':947,'multiline':False]
['text':' `allow_unused` set to True implicitly','line_number':978,'multiline':False]
['text':' both hooks should be called, in order','line_number':1009,'multiline':False]
['text':' both tensors should have been modified','line_number':1023,'multiline':False]
['text':' should error!','line_number':1032,'multiline':False]
['text':' make a copy for reference','line_number':1055,'multiline':False]
['text':' freeze a copy of the params to compare later','line_number':1070,'multiline':False]
['text':' After removing the handle, the model should no longer update.','line_number':1075,'multiline':False]
['text':' thing_to_put_in_hook should be kept alive by tensor','line_number':1110,'multiline':False]
['text':' thing_to_put_in_hook should be cleaned','line_number':1114,'multiline':False]
['text':' the hooks should run in the order of:','line_number':1138,'multiline':False]
['text':'   1. tensor prehook','line_number':1139,'multiline':False]
['text':'   2. acc_grad prehook','line_number':1140,'multiline':False]
['text':'   3. tensor post acc_grad hook','line_number':1141,'multiline':False]
['text':'   4. acc_grad posthook','line_number':1142,'multiline':False]
['text':' so that would be ((1 - 2) / 5 + 0.5) * 10 = 3','line_number':1143,'multiline':False]
['text':' Create a hook that do not have a __name__ attribute','line_number':1147,'multiline':False]
['text':' Should run fine','line_number':1155,'multiline':False]
['text':' Hooks registered to tensor are ordered before those','line_number':1158,'multiline':False]
['text':' that are registered to grad_fn','line_number':1159,'multiline':False]
['text':' grad also runs hooks on accumulate grad nodes, even though','line_number':1185,'multiline':False]
['text':' the accumulate grad nodes are not actually executed','line_number':1186,'multiline':False]
['text':' retains_grad hooks would not observe modifications by all pre hooks','line_number':1194,'multiline':False]
['text':' because they are executed after','line_number':1195,'multiline':False]
['text':' Post hooks on accumulate should be able to observe changes to','line_number':1220,'multiline':False]
['text':' grad made by tensor prehooks','line_number':1221,'multiline':False]
['text':' grad executes the tensor hooks of the next node but not','line_number':1244,'multiline':False]
['text':' grad_fn pre hooks or the post hooks','line_number':1245,'multiline':False]
['text':' define a helper for dividing intermediates into groups','line_number':1305,'multiline':False]
['text':' Compute the d loss / d intermediates in chunks of shard_size','line_number':1309,'multiline':False]
['text':' Compute rest of backward pass','line_number':1313,'multiline':False]
['text':' this should succeed now','line_number':1334,'multiline':False]
['text':' too many','line_number':1346,'multiline':False]
['text':' too few','line_number':1348,'multiline':False]
['text':' this should succeed','line_number':1350,'multiline':False]
['text':' Make sure x and y have grad accumulators allocated','line_number':1355,'multiline':False]
['text':' This is slightly different than the case above, because z doesn't even','line_number':1363,'multiline':False]
['text':' have a grad accumulator allocated.','line_number':1364,'multiline':False]
['text':' allow_unused=False, but grads contains None inside, should throw','line_number':1370,'multiline':False]
['text':' Test that certain nodes are not erroneously executed when an input','line_number':1376,'multiline':False]
['text':' is unreachable. See #39784','line_number':1377,'multiline':False]
['text':' allow_unused is implicitly True!','line_number':1401,'multiline':False]
['text':' Size([2, 2])','line_number':1407,'multiline':False]
['text':' Size([3, 2, 2])','line_number':1408,'multiline':False]
['text':' Detect shape mismatch','line_number':1412,'multiline':False]
['text':' Scalar outputs','line_number':1417,'multiline':False]
['text':' Size([])','line_number':1418,'multiline':False]
['text':' Size([3])','line_number':1419,'multiline':False]
['text':' We consider scalar and sized-1 to be a mismatch. This is consistent with current non-batched behavior.','line_number':1423,'multiline':False]
['text':' register posthook x 2','line_number':1505,'multiline':False]
['text':' register prehook x 3','line_number':1508,'multiline':False]
['text':' Return None','line_number':1521,'multiline':False]
['text':' Compute gradients without hooks','line_number':1535,'multiline':False]
['text':' Compute gradients with hooks','line_number':1540,'multiline':False]
['text':' Compare','line_number':1554,'multiline':False]
['text':' Test with custom Function','line_number':1557,'multiline':False]
['text':' Simply remove hooks','line_number':1591,'multiline':False]
['text':' Remove hooks during backward','line_number':1608,'multiline':False]
['text':' Remove hook that is already removed is OK','line_number':1615,'multiline':False]
['text':' Hooks that registered first run first','line_number':1623,'multiline':False]
['text':' Tests hooks for autograd function implemented in C++','line_number':1633,'multiline':False]
['text':' WARNING: this is a test for autograd internals.','line_number':1653,'multiline':False]
['text':' You should never have to use such things in your code.','line_number':1654,'multiline':False]
['text':' It should be possible to call retain_grad() multiple times','line_number':1685,'multiline':False]
['text':' Gradient should be accumulated','line_number':1689,'multiline':False]
['text':' It should be a no-op for leaves','line_number':1697,'multiline':False]
['text':' NB: See test/cpp/api/autograd.cpp for more tests on the interaction between','line_number':1703,'multiline':False]
['text':'     retains_grad and hooks in cpp','line_number':1704,'multiline':False]
['text':' Inplace multiple times is OK','line_number':1714,'multiline':False]
['text':' node has two retains_grad hooks','line_number':1737,'multiline':False]
['text':' the retain_grad hook multi-output node refers should now be a nullptr','line_number':1739,'multiline':False]
['text':' The old grad_fn, slice, wouldn't be part of the graph during backward','line_number':1761,'multiline':False]
['text':' so if the retains grad were not properly updated to the new grad_fn,','line_number':1762,'multiline':False]
['text':' the grad would still be None','line_number':1763,'multiline':False]
['text':' Check that the second hook gets registered to the new version of tensor','line_number':1768,'multiline':False]
['text':' x2 from mul, x2 from fn2','line_number':1774,'multiline':False]
['text':' Inplace multiple times is OK','line_number':1803,'multiline':False]
['text':' node refers to two hook dicts','line_number':1841,'multiline':False]
['text':' out1 no longer no longer points to its old hook dict','line_number':1842,'multiline':False]
['text':' fn2 is registered to out1's new hook dict','line_number':1844,'multiline':False]
['text':' There might be a better UX here, but this is the way it is now','line_number':1850,'multiline':False]
['text':' We need to explicitly trigger an update to view to update its grad_fn','line_number':1869,'multiline':False]
['text':' The hooks originally registered to view are not fired, one must explicitly','line_number':1873,'multiline':False]
['text':' trigger an update to the view's grad_fn, and then register a new hook','line_number':1874,'multiline':False]
['text':' No gradcheck support for BSR/BSC, so the grads are checked explicitly','line_number':1947,'multiline':False]
['text':' We should only be testing cases with sparse inputs, and at least one','line_number':1959,'multiline':False]
['text':' input needs to require grad so we can call a backward pass','line_number':1960,'multiline':False]
['text':' detaching as `a` needs to be a leaf','line_number':1965,'multiline':False]
['text':' detaching as `b` needs to be a leaf','line_number':1969,'multiline':False]
['text':' Redo with only dense tensors','line_number':1980,'multiline':False]
['text':' NB: we currently raise an exception if any arguments to backwards','line_number':2019,'multiline':False]
['text':' have requires_grad=False and don't have a grad_fn. We may want to','line_number':2020,'multiline':False]
['text':' relax that check to a warning.','line_number':2021,'multiline':False]
['text':' backward doesn't have an allow_unused flag, so the behavior of backward','line_number':2082,'multiline':False]
['text':' when variable is not part of the graph is as if allow_used were true','line_number':2083,'multiline':False]
['text':' x.grad will simply be None.','line_number':2084,'multiline':False]
['text':' build a "chain" computation graph','line_number':2141,'multiline':False]
['text':' graph deletion occurs when the above locals go out of scope.','line_number':2145,'multiline':False]
['text':' In this case `del y` will trigger it but it's easier to leave','line_number':2146,'multiline':False]
['text':' it to Python to delete the locals.','line_number':2147,'multiline':False]
['text':' Should not stack overflow','line_number':2149,'multiline':False]
['text':' Hold the two previous values','line_number':2159,'multiline':False]
['text':' Build a "chain with skip connections" graph','line_number':2162,'multiline':False]
['text':' Definitely pick one tensor to add','line_number':2169,'multiline':False]
['text':' Possibly add other tensors','line_number':2172,'multiline':False]
['text':' graph deletion occurs when the above locals go out of scope.','line_number':2177,'multiline':False]
['text':' Should not stack overflow','line_number':2179,'multiline':False]
['text':' build deeply nested computation graph','line_number':2197,'multiline':False]
['text':' graph deletion occurs when the above locals go out of scope.','line_number':2201,'multiline':False]
['text':' Should not stack overflow','line_number':2203,'multiline':False]
['text':' If we kept x in the derivative Function of x * 2 we would','line_number':2207,'multiline':False]
['text':' get an error in the backward that would complain that we've','line_number':2208,'multiline':False]
['text':' modified x, which was needed for gradient computation.','line_number':2209,'multiline':False]
['text':' Since we should elide unnecessary saves, this test should pass.','line_number':2210,'multiline':False]
['text':' test nested decorator and with-statement on no_grad','line_number':2244,'multiline':False]
['text':' enable_grad_decorator_recursive and no_grad_decorator_recursive call each other','line_number':2283,'multiline':False]
['text':' recursively, to ensure that the decorators preserve the caller's setting','line_number':2284,'multiline':False]
['text':' enable_grad_context_manager_recursive and no_grad_context_manager_recursive call','line_number':2299,'multiline':False]
['text':' each other recursively, to ensure that the decorators preserve the caller's setting','line_number':2300,'multiline':False]
['text':' advanced indexing, with less dim, or ellipsis','line_number':2568,'multiline':False]
['text':' advanced indexing, with less dim, or ellipsis','line_number':2587,'multiline':False]
['text':' advanced indexing, with a tensor wrapped in a variable','line_number':2595,'multiline':False]
['text':' with advanced indexing','line_number':2619,'multiline':False]
['text':' Example from https://github.com/pytorch/pytorch/issues/24853.','line_number':2652,'multiline':False]
['text':' if `index(tensor, indices)` saves `tensor` for backwards, then it will','line_number':2653,'multiline':False]
['text':' trigger a version check on `tensor` during the backward pass, which','line_number':2654,'multiline':False]
['text':' will cause the following code to error because `tensor` gets modified','line_number':2655,'multiline':False]
['text':' by the indexing line.','line_number':2656,'multiline':False]
['text':' Make sure backward isn't called on these','line_number':2703,'multiline':False]
['text':' non-leaf','line_number':2741,'multiline':False]
['text':' basic case, should be able to modify inplace while requires_grad is False','line_number':2748,'multiline':False]
['text':' same but with a view','line_number':2755,'multiline':False]
['text':' should fail if requires_grad = True when we modify inplace','line_number':2763,'multiline':False]
['text':' Should not warn for grad','line_number':2841,'multiline':False]
['text':' Add doesn't need it's inputs to do backward, so it shouldn't raise','line_number':2873,'multiline':False]
['text':' Mul saves both inputs in forward, so it should raise','line_number':2875,'multiline':False]
['text':' w is a the last expression, so this should succeed','line_number':2882,'multiline':False]
['text':' r doesn't use the modified value in backward, so it should succeed','line_number':2884,'multiline':False]
['text':' q uses dirty z, so it should raise','line_number':2886,'multiline':False]
['text':' x should be still usable','line_number':2908,'multiline':False]
['text':' This used to segfault because MyFunction would send back null','line_number':2957,'multiline':False]
['text':' gradients to MulBackward, which is implemented in C++. C++','line_number':2958,'multiline':False]
['text':' implemented functions expect incoming grad_outputs to be non-null.','line_number':2959,'multiline':False]
['text':' Can't modify leaf variables in-place','line_number':3016,'multiline':False]
['text':' Functions which modify views in-place must return only one output','line_number':3018,'multiline':False]
['text':' case when x broadcasts to as y[1]','line_number':3046,'multiline':False]
['text':' both select and sum return Scalars in ATen; ensure they work together.','line_number':3086,'multiline':False]
['text':' TODO: opinfo this or move to the sparse test suite','line_number':3137,'multiline':False]
['text':' After raising warning, should still return an instance','line_number':3203,'multiline':False]
['text':' THPFunction is the base class of both grad_fn and autograd functions,','line_number':3211,'multiline':False]
['text':' which means that a lot of accessors on them may segfault. Test that we','line_number':3212,'multiline':False]
['text':' properly error in this case.','line_number':3213,'multiline':False]
['text':' this currently fails, but shouldn't','line_number':3244,'multiline':False]
['text':' At this point in time, it complains that the graph has been freed','line_number':3264,'multiline':False]
['text':' (which indeed true, although a somewhat indirect way of stating the','line_number':3265,'multiline':False]
['text':' problem).','line_number':3266,'multiline':False]
['text':' This test failed the equality check in PR #22983; it's an interesting','line_number':3270,'multiline':False]
['text':' and different test case worth enshrining.  mult1 is not testing','line_number':3271,'multiline':False]
['text':' anything that interesting, but mult2 is the interesting case.','line_number':3272,'multiline':False]
['text':' This test failed complaining that buffers had already been freed','line_number':3303,'multiline':False]
['text':' prior to #22983.  Also pretty interesting test case.','line_number':3304,'multiline':False]
['text':' this is equivalent, but uses the output of .forward() in .backward()','line_number':3317,'multiline':False]
['text':' should not error!','line_number':3340,'multiline':False]
['text':' This is an incorrect gradient, but we assume that's what the user','line_number':3357,'multiline':False]
['text':' wanted. detach() is an advanced option.','line_number':3358,'multiline':False]
['text':' in-place detach','line_number':3361,'multiline':False]
['text':' this won't backprop to x','line_number':3368,'multiline':False]
['text':' in-place detach on a view raises an exception','line_number':3372,'multiline':False]
['text':' saves x','line_number':3390,'multiline':False]
['text':' TODO: t.dtype should work','line_number':3435,'multiline':False]
['text':' one of these has to be the non-default device','line_number':3455,'multiline':False]
['text':' Make sure hooks only receive grad from usage of q, not x.','line_number':3505,'multiline':False]
['text':' This tests checks backward engine for a very subtle bug that appreared','line_number':3604,'multiline':False]
['text':' in one of the initial versions of autograd. Gradients tensors were','line_number':3605,'multiline':False]
['text':' simply stored in lists while the function waited for all its gradients','line_number':3606,'multiline':False]
['text':' to be computed. However, sometimes an output was used multiple times,','line_number':3607,'multiline':False]
['text':' so the gradients needed to be summed. Engine used to keep a need_copy','line_number':3608,'multiline':False]
['text':' set of tensors that will need a clone upon next addition and removed','line_number':3609,'multiline':False]
['text':' them from the set as soon as the clone was performed. However, this','line_number':3610,'multiline':False]
['text':' could lead to incorrect results if the same gradient tensor was','line_number':3611,'multiline':False]
['text':' buffered in three places in the graph:','line_number':3612,'multiline':False]
['text':' 1. When accumulating gradients in one of these places it was cloned','line_number':3613,'multiline':False]
['text':'    and removed from need_copy set.','line_number':3614,'multiline':False]
['text':' 2. When accumulating in second place, it wasn't in the need_copy set,','line_number':3615,'multiline':False]
['text':'    so the gradients were simply accumulated in-place (which already','line_number':3616,'multiline':False]
['text':'    modified the grad in 3rd place)','line_number':3617,'multiline':False]
['text':' 3. When accumulating in the third place, it wasn't in the need_copy set','line_number':3618,'multiline':False]
['text':'    as well, so the incoming gradient was summed in-place, yielding','line_number':3619,'multiline':False]
['text':'    incorrect results in all functions, except the first one.','line_number':3620,'multiline':False]
['text':' Simulate that we're in the middle of the graph','line_number':3623,'multiline':False]
['text':' This op will just return grad_output two times in backward','line_number':3627,'multiline':False]
['text':' Simulate a long branch, so grad_output will get buffered.','line_number':3630,'multiline':False]
['text':' expected gradients are:','line_number':3637,'multiline':False]
['text':' for x: 34 (16 from final a, 16 from final c, 2 from add2)','line_number':3638,'multiline':False]
['text':' for y: 17 (16 from final b, 1 from add2)','line_number':3639,'multiline':False]
['text':' separate F1 from F2 by another op','line_number':3718,'multiline':False]
['text':' This should not mutate the global grad mode!','line_number':3744,'multiline':False]
['text':' This is non-idiomatic usage!','line_number':3752,'multiline':False]
['text':' More idiomatic usage: torch.set_grad_enabled(False)(inner_func)','line_number':3753,'multiline':False]
['text':' this will consume the set_grad_enabled global mutation!','line_number':3757,'multiline':False]
['text':' Reentrant starts on CPU thread, finishs on GPU thread','line_number':3786,'multiline':False]
['text':' Parent graph.','line_number':3793,'multiline':False]
['text':' Reentrant child graph.','line_number':3797,'multiline':False]
['text':' Reentrant backward in child will throw an error.','line_number':3811,'multiline':False]
['text':' LOBPCG uses a random initial eigenspace approximation','line_number':3849,'multiline':False]
['text':' if parameter `X` is not provided.','line_number':3850,'multiline':False]
['text':' This may cause a non-deterministic behavior','line_number':3851,'multiline':False]
['text':' when it comes to the sign of an eigenvector','line_number':3852,'multiline':False]
['text':' (note if v is an eigenvector, so is -v),','line_number':3853,'multiline':False]
['text':' hence we eliminate this non-determinism','line_number':3854,'multiline':False]
['text':' by making sure that each column of U','line_number':3855,'multiline':False]
['text':' gets multiplied by the sign of its max (in absolute value) element.','line_number':3856,'multiline':False]
['text':' Also, gradcheck changes the content of the input by +/- eps (default to 1e-06)','line_number':3857,'multiline':False]
['text':' to compute the numerical gradient which can also cause the signs to flip.','line_number':3858,'multiline':False]
['text':' TODO: review if this can be ported to OpInfos or moved to test_linalg.py','line_number':3864,'multiline':False]
['text':' Custom gradient vectors for better stability due to some','line_number':3872,'multiline':False]
['text':' non-determinism in the lobpcg's forward.','line_number':3873,'multiline':False]
['text':' Note it is not required if symeig is in forward instead (tested).','line_number':3874,'multiline':False]
['text':' check whether A.grad is symmetric','line_number':3879,'multiline':False]
['text':' Create a reference cycle that contains an','line_number':3902,'multiline':False]
['text':' intermediary Variable in the graph','line_number':3903,'multiline':False]
['text':' This will segfault if things have been erroneously released','line_number':3912,'multiline':False]
['text':' TODO: review porting these to OpInfo tests','line_number':3915,'multiline':False]
['text':' or grad accumulator','line_number':3954,'multiline':False]
['text':' register hooks that log the order in which they are called','line_number':3966,'multiline':False]
['text':' Basic example: single path','line_number':3975,'multiline':False]
['text':' We don't exactly follow sequence_nr order','line_number':3984,'multiline':False]
['text':' Accumulate grad node has more than one input','line_number':3997,'multiline':False]
['text':' Multiple roots are also OK','line_number':4009,'multiline':False]
['text':' TODO: Uncomment after update to hooks behavior','line_number':4022,'multiline':False]
['text':' self.assertEqual(predicted[0], grad_fns(*actual))','line_number':4023,'multiline':False]
['text':' Case where next node is nullptr','line_number':4026,'multiline':False]
['text':' Case where two `inputs` on the same path','line_number':4037,'multiline':False]
['text':' TODO: Uncomment after update to hooks behavior','line_number':4048,'multiline':False]
['text':' self.assertEqual(predicted[0], grad_fns(*actual))','line_number':4049,'multiline':False]
['text':' Case where `inputs` specifies a subgraph','line_number':4052,'multiline':False]
['text':' TODO: Uncomment after update to hooks behavior','line_number':4064,'multiline':False]
['text':' self.assertEqual(predicted[0], grad_fns(*actual))','line_number':4065,'multiline':False]
['text':' Errors when not called in a backward','line_number':4068,'multiline':False]
['text':' Errors when context manager not enabled','line_number':4072,'multiline':False]
['text':' mutate the view, triggering autograd view-replay logic','line_number':4081,'multiline':False]
['text':' Test as a context manager','line_number':4087,'multiline':False]
['text':' Test as a function','line_number':4109,'multiline':False]
['text':' version counter doesn't change inside of the context manager','line_number':4128,'multiline':False]
['text':' Don't use node.name() here as it is not consistent on windows','line_number':4143,'multiline':False]
['text':' expecting aten::add, aten::sum to have the sequence numbers,','line_number':4196,'multiline':False]
['text':' expecting the corresponding backward nodes to have the same numbers','line_number':4197,'multiline':False]
['text':' as the forward ops','line_number':4198,'multiline':False]
['text':' check that nested ops (e.g. empty) don't have','line_number':4213,'multiline':False]
['text':' sequence number','line_number':4214,'multiline':False]
['text':' we expect that profiler will be able','line_number':4248,'multiline':False]
['text':' propagate across fork','line_number':4249,'multiline':False]
['text':' note: continuation (and rf's end) can','line_number':4252,'multiline':False]
['text':' be executed in a different thread','line_number':4253,'multiline':False]
['text':' Test the new _record_function ops work','line_number':4291,'multiline':False]
['text':' Note: Remove once record_function uses these directly','line_number':4292,'multiline':False]
['text':' [[thread_id, [(start, end, id), ....]], ...]','line_number':4313,'multiline':False]
['text':' Using list instead of a dict so order is guaranteed for any Python','line_number':4314,'multiline':False]
['text':' version','line_number':4315,'multiline':False]
['text':' Note that [1, 3] pushes out [0, 2] first. Then we record [1, 2]','line_number':4336,'multiline':False]
['text':' as a child of [1, 3]','line_number':4337,'multiline':False]
['text':' aggregate stats','line_number':4368,'multiline':False]
['text':' average stats','line_number':4374,'multiline':False]
['text':' make it us which is profiler default','line_number':4423,'multiline':False]
['text':' We can also use record_function to decorate arbitrary function','line_number':4470,'multiline':False]
['text':' test that exiting the record function after starting another one','line_number':4484,'multiline':False]
['text':' doesn't throw.','line_number':4485,'multiline':False]
['text':' test that exiting the record function after ending another one','line_number':4490,'multiline':False]
['text':' doesn't throw.','line_number':4491,'multiline':False]
['text':' real and imag are only implemented for complex tensors.','line_number':4500,'multiline':False]
['text':' Test an in-place operation on a view in which the in-place op saves','line_number':4512,'multiline':False]
['text':' its output. Previously, this created a reference cycle.','line_number':4513,'multiline':False]
['text':' Issue #21875: Fail faster (when we try to modify the view vs. in backward())','line_number':4531,'multiline':False]
['text':' Issue #10532: Make sure that this does not raise RuntimeError.','line_number':4541,'multiline':False]
['text':' https://discuss.pytorch.org/t/freeing-buffer-strange-behavior/31955/8','line_number':4552,'multiline':False]
['text':' Issue 23502: Test that b's grad_fn is preserved.','line_number':4570,'multiline':False]
['text':' Issue #21875: Fail faster (when we try to modify the view vs. in backward())','line_number':4584,'multiline':False]
['text':' out=... functions don't support automatic differentiation currently','line_number':4594,'multiline':False]
['text':' the inputs can require grad if we're in no_grad() mode','line_number':4597,'multiline':False]
['text':' we should throw an exception if the output requires grad','line_number':4605,'multiline':False]
['text':' Generate a nan','line_number':4621,'multiline':False]
['text':' Should not fail','line_number':4629,'multiline':False]
['text':' generate a nan','line_number':4697,'multiline':False]
['text':' should not fail','line_number':4708,'multiline':False]
['text':' PyTorch won't throw warnings if there is an error','line_number':4732,'multiline':False]
['text':' but we'd want to at least see them in stderr','line_number':4733,'multiline':False]
['text':' if the warnings don't throw, they will be handled as regular warnings','line_number':4747,'multiline':False]
['text':' if the warning throws, it will be printed to sys.stderr','line_number':4763,'multiline':False]
['text':' Test that python objects created are properly cleaned up when assign_parent is called','line_number':4782,'multiline':False]
['text':' we use torch.exp here but any function that will construct a new node in its','line_number':4785,'multiline':False]
['text':' backward call in grad mode will work','line_number':4786,'multiline':False]
['text':' ExpBackward calls mul, creating the MulBackward node when create_graph=True.','line_number':4790,'multiline':False]
['text':' In anomaly mode, a PyObject referencing MulBackward's "parent" ExpBackward is added to','line_number':4791,'multiline':False]
['text':' MulBackward's anomaly metadata dict, creating the following reference chain:','line_number':4792,'multiline':False]
['text':'','line_number':4793,'multiline':False]
['text':' grad -> MulBackward -> PyObject -> ExpBackward','line_number':4794,'multiline':False]
['text':'','line_number':4795,'multiline':False]
['text':' We add a weak reference to a new Foo object, which we insert into ExpBackward's metadata dict','line_number':4799,'multiline':False]
['text':'','line_number':4800,'multiline':False]
['text':' (PyObject) -> ExpBackward -> dict -> *Foo*','line_number':4801,'multiline':False]
['text':'            t ----^        WeakRef ---^','line_number':4802,'multiline':False]
['text':'','line_number':4803,'multiline':False]
['text':' We want to test that when grad goes out of scope at the end of this function that PyObject is destroyed','line_number':4804,'multiline':False]
['text':' We can test this by seeing whether Foo is not kept alive once t is destroyed','line_number':4805,'multiline':False]
['text':' Test if metadata dict PyObject is properly destroyed','line_number':4820,'multiline':False]
['text':' This is similar to the construction in test_anomaly_assign_parent_cleanup:','line_number':4822,'multiline':False]
['text':'','line_number':4823,'multiline':False]
['text':' MyFuncBackward2 -> PyObject -> MyFuncBackward -> dict -> Foo','line_number':4824,'multiline':False]
['text':'                               out ---^         WeakRef ---^','line_number':4825,'multiline':False]
['text':'','line_number':4826,'multiline':False]
['text':' We want to check that Foo is still properly destroyed even when MyFunc2Backward's','line_number':4827,'multiline':False]
['text':' AnomalyMetadata calls printstack, which does some python object manipulation.','line_number':4828,'multiline':False]
['text':'','line_number':4829,'multiline':False]
['text':' You might be wondering why we still have to test_anomaly_assign_parent_cleanup,','line_number':4830,'multiline':False]
['text':' since if PyObject is not destroyed here, wouldn't this test would detect that also?','line_number':4831,'multiline':False]
['text':' The answer is that custom function's PyObject (THPFunction) actually only hold','line_number':4832,'multiline':False]
['text':' a weak reference to the c++ node!','line_number':4833,'multiline':False]
['text':' create autograd function that saves grad pointer as class static','line_number':4903,'multiline':False]
['text':' non-contiguous grad should be copied','line_number':4928,'multiline':False]
['text':' test case that should trigger no copy for one of a,b','line_number':4932,'multiline':False]
['text':' check a,b uses different grad buffer','line_number':4938,'multiline':False]
['text':' check one of them is using the computed buffer','line_number':4940,'multiline':False]
['text':' create autograd function that saves grad pointer as class static','line_number':4944,'multiline':False]
['text':' Create a sparse tensor with non-contigous indices and values','line_number':4966,'multiline':False]
['text':' and return as grad.','line_number':4967,'multiline':False]
['text':' test case that should trigger no copy for one of a,b','line_number':4982,'multiline':False]
['text':' check a,b uses different grad buffer','line_number':4989,'multiline':False]
['text':' check one of them is using the computed buffer','line_number':4991,'multiline':False]
['text':' Run backwards multiple times to ensure accumulation works.','line_number':4994,'multiline':False]
['text':' non-contiguous indices and value, we should trigger a copy.','line_number':4998,'multiline':False]
['text':' check a,b uses different grad buffer','line_number':5006,'multiline':False]
['text':' Verify we cloned both grads.','line_number':5008,'multiline':False]
['text':' Run backwards multiple times to ensure accumulation works.','line_number':5012,'multiline':False]
['text':' when none of the inputs require grad (always raises even if raise_exception=False)','line_number':5082,'multiline':False]
['text':' (warning) when inputs are not double precision','line_number':5087,'multiline':False]
['text':' when layout is not mkldnn(aka has strides) and input has a dimension with stride 0. (always raises','line_number':5092,'multiline':False]
['text':' even if raise_exception=False)','line_number':5093,'multiline':False]
['text':' when mkldnn inputs, forward mode testing is not allowed','line_number':5104,'multiline':False]
['text':' Update tolerances below to make sure the gradient match even in single precision floats','line_number':5105,'multiline':False]
['text':' Use the warning assert to hide the float32 warning','line_number':5106,'multiline':False]
['text':' when sparse outputs (always raise even if raise_exception=False)','line_number':5121,'multiline':False]
['text':' when mkldnn outputs (always raise even if raise_exception=False)','line_number':5127,'multiline':False]
['text':' When none of the outputs are differentiable, but numerical gradient is not zero','line_number':5136,'multiline':False]
['text':' succeed when no outputs at all','line_number':5142,'multiline':False]
['text':' runtime error while compute batched grad (print big error)','line_number':5150,'multiline':False]
['text':' when grad_input is sparse and has incorrect sparse_dim/dense_dim','line_number':5159,'multiline':False]
['text':' when backward not multiplied by grad_output (non-sparse case)','line_number':5175,'multiline':False]
['text':' when backward not multiplied by grad_output (sparse case)','line_number':5185,'multiline':False]
['text':' when layout of grad_input is not the same as input','line_number':5196,'multiline':False]
['text':' when encounter runtime error while running backward','line_number':5214,'multiline':False]
['text':' R -> R, C -> C','line_number':5232,'multiline':False]
['text':' R -> C','line_number':5246,'multiline':False]
['text':' C -> R','line_number':5255,'multiline':False]
['text':' get_numerical_jacobian is deprecated and no longer used internally by gradcheck','line_number':5326,'multiline':False]
['text':' get_numerical_jacobian requires fn to take inputs as a tuple','line_number':5330,'multiline':False]
['text':' and returns the jacobian wrt the first output','line_number':5331,'multiline':False]
['text':' Checks that when raise_exception=False, non-GradcheckErrors are not caught by gradcheck','line_number':5401,'multiline':False]
['text':' Hacky way to check if we're currently inside a forward ad level','line_number':5413,'multiline':False]
['text':' Test for all inputs and outputs being real','line_number':5425,'multiline':False]
['text':' Test for one input and one output being complex','line_number':5437,'multiline':False]
['text':' Test for all inputs and outputs being complex','line_number':5444,'multiline':False]
['text':' Currently requires_grad is used as a easy way for gradcheck to know','line_number':5452,'multiline':False]
['text':' which inputs of the function are meant to be differentiable','line_number':5453,'multiline':False]
['text':' This test checks that when the inputs are passed to the function they should not have','line_number':5454,'multiline':False]
['text':' requires_grad=True even though they may have requires_grad=True when passed','line_number':5455,'multiline':False]
['text':' to gradcheck','line_number':5456,'multiline':False]
['text':' Currently requires_grad is used as a easy way for gradcheck to know','line_number':5487,'multiline':False]
['text':' which inputs of the function are meant to be differentiable','line_number':5488,'multiline':False]
['text':' NB: In slow gradcheck we need to loop through numel times so use numel = 1 to ensure','line_number':5501,'multiline':False]
['text':'     that fast and slow have the same counts','line_number':5502,'multiline':False]
['text':' (2) once per input','line_number':5507,'multiline':False]
['text':' (+4): (once with normal ZT (+1), once with efficient ZT (+1)) for each input (x2)','line_number':5512,'multiline':False]
['text':' (+6): (compute batch of 2 with vmap (+1), with a loop (+2)) for each input (x2)','line_number':5517,'multiline':False]
['text':' Repeat the previous test except we mark one input with requires_grad=False','line_number':5520,'multiline':False]
['text':' NB: _test_undefined_forward_mode is only (+1), when function has single differentiable input, not (+2)!','line_number':5521,'multiline':False]
['text':'     Otherwise, other counts are halved.','line_number':5522,'multiline':False]
['text':' 1 + 1 + 3','line_number':5527,'multiline':False]
['text':' If both fail, backward AD failure "hides" forward AD failure','line_number':5580,'multiline':False]
['text':' multiple inputs and outputs with non-tensors inputs','line_number':5591,'multiline':False]
['text':' unrelated inputs: tangent for c is None','line_number':5597,'multiline':False]
['text':' In-place op bumps version','line_number':5624,'multiline':False]
['text':' Differentiable view shares version counter','line_number':5629,'multiline':False]
['text':' `x.data = y` preserves version counter of `x`','line_number':5635,'multiline':False]
['text':' Make sure `x` is still using the same version counter it shares with `xz`','line_number':5641,'multiline':False]
['text':' In-place op on `xz` also updates version of `x`,','line_number':5644,'multiline':False]
['text':' because they share the version counter','line_number':5645,'multiline':False]
['text':' Dense tensor has impl of type `TensorImpl`, while sparse tensor has impl','line_number':5650,'multiline':False]
['text':' of type `SparseTensorImpl`.','line_number':5651,'multiline':False]
['text':' The autograd engine creates worker threads only when GPU devices are present.','line_number':5694,'multiline':False]
['text':' So make sure that we do shutdown threads when we're testing cuda and make sure','line_number':5695,'multiline':False]
['text':' that there is no thread to shutdown when we're not using cuda.','line_number':5696,'multiline':False]
['text':' Test stack overflow escape mechanism','line_number':5721,'multiline':False]
['text':' This will cause stack overflow if reentrant calls are handled','line_number':5723,'multiline':False]
['text':' in the same thread recursively','line_number':5724,'multiline':False]
['text':' Test stack overflow escape mechanism multiple times','line_number':5727,'multiline':False]
['text':' to ensure reusing workers in the pool works fine','line_number':5728,'multiline':False]
['text':' The tasks for the Reentrant and MyFunction backward() will be added','line_number':5766,'multiline':False]
['text':' to the queue in the autograd engine at the same time. The backward','line_number':5767,'multiline':False]
['text':' for Reentrant will be executed first, which will then add other','line_number':5768,'multiline':False]
['text':' backward tasks to the queue. We want to ensure all the reentrant tasks','line_number':5769,'multiline':False]
['text':' are prioritized over the MyFunction backward task regardless of their','line_number':5770,'multiline':False]
['text':' sequence numbers','line_number':5771,'multiline':False]
['text':' small proxy network for some complex reasoning we want to do per input','line_number':5784,'multiline':False]
['text':' compute mean as a proxy for some joint reasoning','line_number':5799,'multiline':False]
['text':' torch.mm is on autocast's list of ops that should run in','line_number':5806,'multiline':False]
['text':' the autocast precision','line_number':5807,'multiline':False]
['text':' Without saving + recasting the autocast type, would raise error in autograd','line_number':5829,'multiline':False]
['text':' about mismatched dtypes.','line_number':5830,'multiline':False]
['text':' triggers recomputation to check it runs in bfloat','line_number':5831,'multiline':False]
['text':' pre-allocate the grad so that increased memory usage is mainly','line_number':5866,'multiline':False]
['text':' due to activations.','line_number':5867,'multiline':False]
['text':' Accessing the saved Tensors a second time will raise because','line_number':5920,'multiline':False]
['text':' recomputed tensors get cleared as soon as they are unpacked.','line_number':5921,'multiline':False]
['text':' A recomputation is only triggered if your backward has a new','line_number':5922,'multiline':False]
['text':' graph-task id.','line_number':5923,'multiline':False]
['text':' Passing explicitly should not warn','line_number':5978,'multiline':False]
['text':' Not passing explicitly warns','line_number':5983,'multiline':False]
['text':' Passing explicitly should not warn','line_number':6000,'multiline':False]
['text':' Not passing explicitly warns','line_number':6005,'multiline':False]
['text':' Save fewer tensors during recompute','line_number':6039,'multiline':False]
['text':' Save more tensors during recompute','line_number':6045,'multiline':False]
['text':' If early stopping is enabled, we would not raise (the results would be correct anyway)','line_number':6053,'multiline':False]
['text':' Save the same number of tensors but the shape is different','line_number':6057,'multiline':False]
['text':' Get the debug message if debug=True','line_number':6063,'multiline':False]
['text':' Recomputed variables only persist within a particular backward call.','line_number':6099,'multiline':False]
['text':' If _saved_result is accessed outside of a backward, it will trigger','line_number':6100,'multiline':False]
['text':' a recompute. And afterwards, those recomputed results are immediately','line_number':6101,'multiline':False]
['text':' cleared.','line_number':6102,'multiline':False]
['text':' Second access will trigger another recompute','line_number':6105,'multiline':False]
['text':' Backward clears the saved variable','line_number':6108,'multiline':False]
['text':' Now it raises an error','line_number':6111,'multiline':False]
['text':' small proxy network for some complex reasoning we want to do per input','line_number':6129,'multiline':False]
['text':' Module holder for testing activation checkpointing with no_reentrant','line_number':6136,'multiline':False]
['text':' supports kwargs.','line_number':6137,'multiline':False]
['text':' Run model with and without checkpointing and verify gradients are','line_number':6148,'multiline':False]
['text':' equivalent, regardless of if inputs require grads or not.','line_number':6149,'multiline':False]
['text':' compute mean as a proxy for some joint reasoning','line_number':6165,'multiline':False]
['text':' out does not require grad','line_number':6210,'multiline':False]
['text':' Make loss require grad, otherwise we would run into','line_number':6212,'multiline':False]
['text':' "element 0 of tensors does not require grad and does not have a grad_fn"','line_number':6213,'multiline':False]
['text':' Using w outside the checkpoint','line_number':6287,'multiline':False]
['text':' Using w inside the checkpoint','line_number':6288,'multiline':False]
['text':' should only call hook once','line_number':6291,'multiline':False]
['text':' Add a callback to execute.','line_number':6397,'multiline':False]
['text':' Add a callback to execute.','line_number':6411,'multiline':False]
['text':' Reentrant backward call.','line_number':6413,'multiline':False]
['text':' Verify callback is called only once.','line_number':6428,'multiline':False]
['text':' Verify callback is called only once.','line_number':6434,'multiline':False]
['text':' Verify callback is called twice.','line_number':6440,'multiline':False]
['text':' Add some sort of gradient penalty by directly updating the gradients','line_number':6453,'multiline':False]
['text':' Forward pass','line_number':6464,'multiline':False]
['text':' Compute the gradients','line_number':6467,'multiline':False]
['text':' Add some sort of gradient penalty by directly updating the gradients','line_number':6476,'multiline':False]
['text':' Forward pass','line_number':6484,'multiline':False]
['text':' Compute the gradients','line_number':6488,'multiline':False]
['text':' Check that the getter of each type returns what we want','line_number':6493,'multiline':False]
['text':' See `gen_autograd_functions.py` for how the getters are generated','line_number':6494,'multiline':False]
['text':'','line_number':6495,'multiline':False]
['text':' This test is only meant to check if the codegen'd bindings work','line_number':6496,'multiline':False]
['text':' Please help update this test if you update the names of any the fields we check!','line_number':6497,'multiline':False]
['text':'','line_number':6498,'multiline':False]
['text':' TODO: I don't think we have a backward saving a list of tensors','line_number':6503,'multiline':False]
['text':'       at the moment. It used to be stack, but for no reason...','line_number':6504,'multiline':False]
['text':'       see discussion in #84993','line_number':6505,'multiline':False]
['text':' self.assertEqual(out.grad_fn._saved_tensors, (a, b))              # TewnsorList -> Tuple[Tensor]','line_number':6506,'multiline':False]
['text':' int64_t -> int','line_number':6510,'multiline':False]
['text':' TODO: interestingly, this only happens if indexing into a list grad_fn._raw_saved_tensors[0],','line_number':6518,'multiline':False]
['text':'       not when using a saved tensor, see discussion in #84993','line_number':6519,'multiline':False]
['text':' with self.assertRaisesRegex(RuntimeError, "after they have already been freed"):','line_number':6520,'multiline':False]
['text':'     out2.grad_fn._raw_saved_self','line_number':6521,'multiline':False]
['text':' c10::List<c10::optional<Tensor>> -> Tuple[Tensor?]','line_number':6527,'multiline':False]
['text':' SymIntArrayRef -> Tuple[SymInt]','line_number':6530,'multiline':False]
['text':' IntArrayRef -> Tuple[int]','line_number':6538,'multiline':False]
['text':' c10::optional<IntArrayRef> -> int[]?','line_number':6549,'multiline':False]
['text':' bool -> bool','line_number':6551,'multiline':False]
['text':' c10::optional<ArrayRef<double>> -> float[]?','line_number':6554,'multiline':False]
['text':' c10::optional<ArrayRef<double>> -> float[]?','line_number':6556,'multiline':False]
['text':' c10::optional<SymIntArrayRef> -> SymInt[]?','line_number':6560,'multiline':False]
['text':' TODO: This is BAD! we converted a c10::nullopt into a (0,)','line_number':6562,'multiline':False]
['text':' int64_t','line_number':6567,'multiline':False]
['text':' int64_t','line_number':6568,'multiline':False]
['text':' SymInt','line_number':6572,'multiline':False]
['text':' SymInt','line_number':6573,'multiline':False]
['text':' double -> float','line_number':6581,'multiline':False]
['text':' c10:optional<double> -> float?','line_number':6586,'multiline':False]
['text':' std::string -> str','line_number':6594,'multiline':False]
['text':' c10::optional<std::string> -> str?','line_number':6598,'multiline':False]
['text':' c10::optional<std::string> -> str?','line_number':6600,'multiline':False]
['text':' Scalar(complex double) -> complex','line_number':6604,'multiline':False]
['text':' Scalar(complex float) -> complex','line_number':6607,'multiline':False]
['text':' Scalar(floating point) -> float','line_number':6609,'multiline':False]
['text':' Scalar(integral) -> int','line_number':6611,'multiline':False]
['text':' Scalar(bool) -> bool','line_number':6613,'multiline':False]
['text':' c10:optional<int64_t> -> int?','line_number':6617,'multiline':False]
['text':' saved variable when output','line_number':6624,'multiline':False]
['text':' c10:optional<Tensor> -> Tensor?','line_number':6633,'multiline':False]
['text':' ArrayRef<Scalar> -> Tuple[Scalar, ...]','line_number':6641,'multiline':False]
['text':' We can't tell the underlying tensor is None without unpacking it','line_number':6673,'multiline':False]
['text':' We catch that error when the user calls register_hooks on it','line_number':6676,'multiline':False]
['text':' Using a reference to the SavedTensor object after the','line_number':6689,'multiline':False]
['text':' saved variables have been released can lead to undefined behavior','line_number':6690,'multiline':False]
['text':' Node is a "virtual" base class of codegen'd nodes. This means that','line_number':6702,'multiline':False]
['text':' isinstance and issubclass are overridden, but mro is unchanged','line_number':6703,'multiline':False]
['text':' Some nodes have codegened registrations to the torch._C._function module','line_number':6709,'multiline':False]
['text':' Other nodes have manual registrations to the torch._C._function module','line_number':6714,'multiline':False]
['text':' Special cases','line_number':6720,'multiline':False]
['text':' Custom function case','line_number':6726,'multiline':False]
['text':' This is not necessarily the absolute correct behavior, but this is the current','line_number':6747,'multiline':False]
['text':' one. This test is here to make sure that any change to this behavior is detected','line_number':6748,'multiline':False]
['text':' and not silent. The TODOs below mark the places with unexpected behavior.','line_number':6749,'multiline':False]
['text':' Note that any change in these test will be BC-breaking and should be done carefully.','line_number':6750,'multiline':False]
['text':' This test checks the behavior of two codegen functions (view_as and unbind)','line_number':6752,'multiline':False]
['text':' with respect to view tracking and inplace operation on the output.','line_number':6753,'multiline':False]
['text':' Are they differentiable views?','line_number':6767,'multiline':False]
['text':' Are inplace allowed?','line_number':6769,'multiline':False]
['text':' Are they differentiable views?','line_number':6775,'multiline':False]
['text':' Are inplace allowed?','line_number':6778,'multiline':False]
['text':' should_raise contains None if it should not raise','line_number':6782,'multiline':False]
['text':' should_raise contains a string of the error if it should raise','line_number':6783,'multiline':False]
['text':' The 3 elements are for view_as, first output of unbind and second output of unbind','line_number':6784,'multiline':False]
['text':' Original Tensor does not require grad','line_number':6806,'multiline':False]
['text':' Tensor being written does require grad','line_number':6809,'multiline':False]
['text':' Take an invalid view on 'a' that should raise an error (warns during deprecation)','line_number':6812,'multiline':False]
['text':' Extra test for copy_ that is a manual implementation and could be easily','line_number':6818,'multiline':False]
['text':' forgotten when the codegen is updated (warns during deprecation)','line_number':6819,'multiline':False]
['text':' Functions that should throw must properly throw','line_number':6827,'multiline':False]
['text':' Sanity check that views that should work still work','line_number':6835,'multiline':False]
['text':' This is not necessarily the absolute correct behavior, but this is the current','line_number':6841,'multiline':False]
['text':' one. This test is here to make sure that any change to this behavior is detected','line_number':6842,'multiline':False]
['text':' and not silent. The TODOs below mark the places with unexpected behavior.','line_number':6843,'multiline':False]
['text':' Note that any change in these test will be BC-breaking and should be done carefully.','line_number':6844,'multiline':False]
['text':' This checks the autograd.Function behavior when we return one or multiple outputs','line_number':6846,'multiline':False]
['text':' while one of these is an input, a view of an input or of a temporary tensor.','line_number':6847,'multiline':False]
['text':' This indicator is used to track how many times the backward function was called','line_number':6849,'multiline':False]
['text':' This indicator is used to check if the argument `ga` contains non-zero values','line_number':6851,'multiline':False]
['text':' Used for special casing the tests below','line_number':6917,'multiline':False]
['text':' never modify a, b inplace for gracheck','line_number':6921,'multiline':False]
['text':' Was the custom backward called properly','line_number':6953,'multiline':False]
['text':' For the case where the backward is called','line_number':6955,'multiline':False]
['text':' Tests creation_meta properly handled for inplace views','line_number':6977,'multiline':False]
['text':' This test checks the behavior of inplace-view functions when','line_number':6990,'multiline':False]
['text':' the views are created in grad mode or not','line_number':6991,'multiline':False]
['text':' 1. Create a view with `grad_mode=grad_mode_view`','line_number':6993,'multiline':False]
['text':' 2. Perform inplace view with `grad_mode=grad_mode_iview`','line_number':7002,'multiline':False]
['text':' If error is None, check that runs without error','line_number':7009,'multiline':False]
['text':' 3. Do inplace on the (new) view','line_number':7011,'multiline':False]
['text':' If error is None, check that runs without error','line_number':7016,'multiline':False]
['text':' expected error when we do inplace_view on original view','line_number':7028,'multiline':False]
['text':' expected error when we do inplace on the resulting view','line_number':7029,'multiline':False]
['text':' This list was created by logging gen_inplace_or_view_type.py','line_number':7051,'multiline':False]
['text':'   detach_ is excluded for this test because it cannot be applied to','line_number':7052,'multiline':False]
['text':'   views and thus does not return a view','line_number':7053,'multiline':False]
['text':' Special handling for printing view created in no-grad and modified','line_number':7072,'multiline':False]
['text':' in-placed in no-grad.','line_number':7073,'multiline':False]
['text':' TODO This is not the correct behavior -','line_number':7105,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/49825#issuecomment-794466627','line_number':7106,'multiline':False]
['text':' This test is here to make sure that any change to this behavior is detected','line_number':7108,'multiline':False]
['text':' and not silent. The TODOs below mark the places with unexpected behavior.','line_number':7109,'multiline':False]
['text':' TODO: this is a bug!','line_number':7126,'multiline':False]
['text':' once this is fixed, it should have the transpose removed:','line_number':7127,'multiline':False]
['text':' self.assertEqual(non_inplace_grad, inplace_grad)','line_number':7128,'multiline':False]
['text':' This is not necessarily the absolute correct behavior, but this is the current','line_number':7132,'multiline':False]
['text':' one. This test is here to make sure that any change to this behavior is detected','line_number':7133,'multiline':False]
['text':' and not silent. The TODOs below mark the places with unexpected behavior.','line_number':7134,'multiline':False]
['text':' Note that any change in these test will be BC-breaking and should be done carefully.','line_number':7135,'multiline':False]
['text':' This checks that multiples views in the forward are properly traced and how they','line_number':7137,'multiline':False]
['text':' behave with respect to inplace operations.','line_number':7138,'multiline':False]
['text':' This indicator is used to track how many times the backward function was called','line_number':7140,'multiline':False]
['text':' This is not necessarily the absolute correct behavior, but this is the current','line_number':7174,'multiline':False]
['text':' one. This test is here to make sure that any change to this behavior is detected','line_number':7175,'multiline':False]
['text':' and not silent. The TODOs below mark the places with unexpected behavior.','line_number':7176,'multiline':False]
['text':' Note that any change in these test will be BC-breaking and should be done carefully.','line_number':7177,'multiline':False]
['text':' This test checks custom autograd.Function that perform inplace operations','line_number':7179,'multiline':False]
['text':' I) Single output','line_number':7183,'multiline':False]
['text':' No extra inplace','line_number':7200,'multiline':False]
['text':' With extra inplace on the output','line_number':7205,'multiline':False]
['text':' The input is a view','line_number':7212,'multiline':False]
['text':' Should not give non-inputs to mark_dirty','line_number':7218,'multiline':False]
['text':' II) Multiple outputs','line_number':7240,'multiline':False]
['text':' No extra inplace','line_number':7253,'multiline':False]
['text':' With extra inplace on the output','line_number':7259,'multiline':False]
['text':' The input is a view','line_number':7266,'multiline':False]
['text':' III) Inplace + other op','line_number':7271,'multiline':False]
['text':' We don't reuse the input','line_number':7284,'multiline':False]
['text':' Make sure that tensor is always returned as-is if marked dirty','line_number':7317,'multiline':False]
['text':' jvp must properly modify the input grad if mark_dirty is set','line_number':7324,'multiline':False]
['text':' Calling the custom function should operate as if we called an equivalent op','line_number':7368,'multiline':False]
['text':' Check that in-place modification on view throws','line_number':7371,'multiline':False]
['text':' Case where original==False','line_number':7425,'multiline':False]
['text':' Case where original==True','line_number':7427,'multiline':False]
['text':' Break the view in the gradients!','line_number':7512,'multiline':False]
['text':' Result should be a view, just of the wrong thing','line_number':7515,'multiline':False]
['text':' Don't do the change inplace','line_number':7549,'multiline':False]
['text':' Wrong gradient formula','line_number':7580,'multiline':False]
['text':' If we return an input as-is in forward, that is treated','line_number':7618,'multiline':False]
['text':' as if self.view_as(self) is performed. If jvp returns x.view_as(x),','line_number':7619,'multiline':False]
['text':' this is OK.','line_number':7620,'multiline':False]
['text':' Expect this to raise an error','line_number':7622,'multiline':False]
['text':' Expect this to raise the same error','line_number':7624,'multiline':False]
['text':' When saved for backward, but not saved for forward','line_number':7707,'multiline':False]
['text':' returns differentiable type, marked non-differentiable','line_number':7733,'multiline':False]
['text':' returns non-differentiable type, NOT marked non-differentiable','line_number':7756,'multiline':False]
['text':' Here we test the internal functions to make sure all of them are','line_number':7868,'multiline':False]
['text':' covered on top of the public API','line_number':7869,'multiline':False]
['text':' This looks public but is actually manually deleted from the','line_number':7873,'multiline':False]
['text':' torch namespace in torch/functional.py','line_number':7874,'multiline':False]
['text':' We don't test `unique_dim_consecutive` here.','line_number':7879,'multiline':False]
['text':' It looks public but the python binding is actually manually disabled in','line_number':7880,'multiline':False]
['text':' tools/autograd/gen_python_functions.py','line_number':7881,'multiline':False]
['text':' The backward clears the saved_variables but not the __dict__','line_number':7923,'multiline':False]
['text':' If BackwardHook saves grad_output, it can create a cycle when we perform backward','line_number':7931,'multiline':False]
['text':' with create_graph=True','line_number':7932,'multiline':False]
['text':'','line_number':7933,'multiline':False]
['text':'   grad_output -> grad_output.grad_fn -> graph -> hook -> grad_output','line_number':7934,'multiline':False]
['text':'','line_number':7935,'multiline':False]
['text':' Dummy class for the purpose of creating a weakref','line_number':7937,'multiline':False]
['text':' Save dummy object to graph and get a weak ref to it','line_number':7954,'multiline':False]
['text':' Remove the backward + create_graph=True cycle','line_number':7965,'multiline':False]
['text':' This creates a cycle between the hook and grad_fn_b','line_number':7983,'multiline':False]
['text':' hook -> closure -> grad_fn_b (python) -> grad_fn (cpp) -> hook (cpp)','line_number':7984,'multiline':False]
['text':' -> dict -> hook','line_number':7985,'multiline':False]
['text':'','line_number':7986,'multiline':False]
['text':' This test is testing that the grad_fn_b (python) only traverses the','line_number':7987,'multiline':False]
['text':' dict if it is the only one holding a reference to the grad_fn_b (cpp)','line_number':7988,'multiline':False]
['text':' shared_ptr','line_number':7989,'multiline':False]
['text':'','line_number':7990,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/102174','line_number':7991,'multiline':False]
['text':' Make sure this hook's closure holds onto grad_fn_b','line_number':8016,'multiline':False]
['text':' This forms a cycle between the hook and grad_fn_b','line_number':8017,'multiline':False]
['text':' We also hold onto a sentinel object 'obj' to track','line_number':8018,'multiline':False]
['text':' whether this cycle is still alive. See 'ref' below.','line_number':8019,'multiline':False]
['text':' Make sure gc does not clear the cycle noted above.','line_number':8037,'multiline':False]
['text':' e.g. the hook is alive and gets fired even after gc runs','line_number':8038,'multiline':False]
['text':' ref is still alive because the use_count of the cpp grad_fn','line_number':8042,'multiline':False]
['text':' shared_ptr > 1 since (1) the python grad_fn is alive, and (2) the','line_number':8043,'multiline':False]
['text':' rest of the graph holds onto the shared_ptr','line_number':8044,'multiline':False]
['text':' Then delete the rest of the graph and check that ref is dead','line_number':8047,'multiline':False]
['text':' We should not error, and counter should not be incremented','line_number':8069,'multiline':False]
['text':' An op that returns sparse gradients','line_number':8075,'multiline':False]
['text':' An op that returns the gradients as-is','line_number':8079,'multiline':False]
['text':' Given gradients should not be modified inplace','line_number':8088,'multiline':False]
['text':' a is leaf','line_number':8098,'multiline':False]
['text':' b is not an output','line_number':8103,'multiline':False]
['text':' d is an output','line_number':8106,'multiline':False]
['text':' a is left untouched','line_number':8115,'multiline':False]
['text':' Tests that packing/unpacking a SavedVariable works correctly with user-defined hooks','line_number':8134,'multiline':False]
['text':' The saved_original / did_not_save_original distinction corresponds to the `save_original`','line_number':8135,'multiline':False]
['text':' attribute of `SavedVariable`.','line_number':8136,'multiline':False]
['text':' double backward','line_number':8163,'multiline':False]
['text':' leaf','line_number':8203,'multiline':False]
['text':' not leaf, not output','line_number':8206,'multiline':False]
['text':' Detaching a tensor that is saved input raises','line_number':8210,'multiline':False]
['text':' Detaching a tensor that is saved as output is OK','line_number':8217,'multiline':False]
['text':' Tests that packing/unpacking a SavedVariable works correctly with user-defined hooks','line_number':8224,'multiline':False]
['text':' The saved_original / did_not_save_original distinction corresponds to the `save_original`','line_number':8225,'multiline':False]
['text':' attribute of `SavedVariable`.','line_number':8226,'multiline':False]
['text':' Tests that default hooks are properly registered, used and reset','line_number':8237,'multiline':False]
['text':' The saved_original / did_not_save_original distinction corresponds to the `save_original`','line_number':8238,'multiline':False]
['text':' attribute of `SavedVariable`.','line_number':8239,'multiline':False]
['text':' See also:','line_number':8240,'multiline':False]
['text':'  - test_saved_variable_packing_unpacking_saved_original_with_hooks','line_number':8241,'multiline':False]
['text':' should raise two warnings from a being saved twice','line_number':8253,'multiline':False]
['text':' Exited hooks correctly','line_number':8280,'multiline':False]
['text':' See also test_saved_variable_packing_unpacking_did_not_save_original_with_hooks','line_number':8289,'multiline':False]
['text':' factor 2 because only a is saved once','line_number':8466,'multiline':False]
['text':' factor 4 because pow_backward is grad * (exp * self.pow(exp - 1))','line_number':8476,'multiline':False]
['text':' so grad is saved and self (i.e. a) is saved','line_number':8477,'multiline':False]
['text':' combining the two above blocks: 2 * 4 = 8','line_number':8487,'multiline':False]
['text':' note that in that sense, a is saved twice','line_number':8488,'multiline':False]
['text':' 3 is saved as a saved tensor because it is a wrapped number, but','line_number':8498,'multiline':False]
['text':' wrapped numbers should be special cased to not trigger saved variable hooks','line_number':8499,'multiline':False]
['text':' FloatTensor','line_number':8527,'multiline':False]
['text':' DoubleTensor','line_number':8529,'multiline':False]
['text':' Sparse tensor','line_number':8531,'multiline':False]
['text':' with grad','line_number':8541,'multiline':False]
['text':' without grad','line_number':8549,'multiline':False]
['text':' with hooks','line_number':8560,'multiline':False]
['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':8663,'multiline':False]
['text':' fail, so just set CWD to this script's directory','line_number':8664,'multiline':False]
['text':' It is ok to have an extra long timeout here as a timeout means the test failed','line_number':8666,'multiline':False]
['text':' During gradcheck with fast_mode=True, we create a random vector on the CPU device using a CPU generator.','line_number':8748,'multiline':False]
['text':' This test ensures that this still works when the default device is set to something else by the user.','line_number':8749,'multiline':False]
['text':' Optimization to share the same tensor!','line_number':8803,'multiline':False]
['text':' See: test_metadata_check_checks_storage_numel for the reverse of this test','line_number':8824,'multiline':False]
['text':' No copy is made','line_number':8834,'multiline':False]
['text':' as_strided raises','line_number':8837,'multiline':False]
['text':' Ensure that a failing test won't make others fail','line_number':8847,'multiline':False]
['text':' Create a new Tensor and weak reference','line_number':8855,'multiline':False]
['text':' Sanity check that the helper function works as expected','line_number':8859,'multiline':False]
['text':' Main test code','line_number':8866,'multiline':False]
['text':' Make sure that the tangent we provided has been re-used as is','line_number':8876,'multiline':False]
['text':' Make sure that dual is keeping the tangent alive','line_number':8879,'multiline':False]
['text':' Make sure that the dual level does not keep the c++','line_number':8883,'multiline':False]
['text':' version of the tangent alive','line_number':8884,'multiline':False]
['text':' # Verify that mutating unpacked tangent does not affect the original tangent','line_number':8907,'multiline':False]
['text':' as_strided runs without error','line_number':8912,'multiline':False]
['text':' Make sure the _has_same_storage_numel is a fallthrough, so that','line_number':8928,'multiline':False]
['text':' conj bit does not materialize. If it materializes it would','line_number':8929,'multiline':False]
['text':' cause the layout check to fail for views that do not index the','line_number':8930,'multiline':False]
['text':' the entire storage.','line_number':8931,'multiline':False]
['text':' Make sure the _has_same_storage_numel is a fallthrough, so that','line_number':8943,'multiline':False]
['text':' conj bit does not materialize. If it materializes it would','line_number':8944,'multiline':False]
['text':' cause the layout check to fail for views that do not index the','line_number':8945,'multiline':False]
['text':' the entire storage.','line_number':8946,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/80507','line_number':8982,'multiline':False]
['text':' Check that no copy is made','line_number':8988,'multiline':False]
['text':' The following test functions want to ensure all the following behaviors:','line_number':8998,'multiline':False]
['text':'   - Ensure that default level system in the python binding works','line_number':8999,'multiline':False]
['text':'   - Ensure that only level 0 exists and nesting is properly disabled','line_number':9000,'multiline':False]
['text':'   - Ensure that printing works fine','line_number':9001,'multiline':False]
['text':'   - Ensure that basic packing/unpacking works','line_number':9002,'multiline':False]
['text':'   - Ensure that advanced packing/unpacking works','line_number':9003,'multiline':False]
['text':'     - For memory / version counter share','line_number':9004,'multiline':False]
['text':'     - For backward AD (regular ops)','line_number':9005,'multiline':False]
['text':'   - Ensure that view + inplace for both modes work fine','line_number':9006,'multiline':False]
['text':'   - Ensure we do proper cleanup on exit of a level','line_number':9007,'multiline':False]
['text':' We don't actually need to enforce that these two are the exact same python','line_number':9018,'multiline':False]
['text':' object, feel free to relax in the future','line_number':9019,'multiline':False]
['text':' Tests some private helper functions to enable/disable fwd grad mode','line_number':9027,'multiline':False]
['text':' Tests a private helper function','line_number':9042,'multiline':False]
['text':' For now only level 0 exists','line_number':9059,'multiline':False]
['text':' This test checks that codegen silently ignores undefined outputs','line_number':9077,'multiline':False]
['text':' Below, grad_input is specified as False in grad_output_mask, so','line_number':9078,'multiline':False]
['text':' convolution backward will return a undefined tensor in that position.','line_number':9079,'multiline':False]
['text':' Note that for this test to work we need to make sure either grad_output','line_number':9080,'multiline':False]
['text':' or weight to be a dual tensor, so grad_input requires forward grad','line_number':9081,'multiline':False]
['text':' Make sure we can re-enable autograd here','line_number':9121,'multiline':False]
['text':' Only the primal has "alias" called on it','line_number':9133,'multiline':False]
['text':' Float Primal and Long Tangent','line_number':9147,'multiline':False]
['text':' Long Primal and Long Tangent','line_number':9151,'multiline':False]
['text':' Long Primal and Float Tangent','line_number':9155,'multiline':False]
['text':' Check unpacked dual is returned as a named tuple','line_number':9182,'multiline':False]
['text':' NB: Every invocation of unpack_dual returns a new tensor view','line_number':9183,'multiline':False]
['text':' Check that packing/unpacking did not change the input','line_number':9188,'multiline':False]
['text':' Memory and version counter check','line_number':9197,'multiline':False]
['text':' Ensure that they are sharing memory and version counter','line_number':9201,'multiline':False]
['text':' Ensure we properly share the version counter','line_number':9204,'multiline':False]
['text':' Unpacking should only create aliases as well','line_number':9209,'multiline':False]
['text':' And the tangent is actually re-used as-is so it is still the same Tensor','line_number':9213,'multiline':False]
['text':' Ensure we properly share the version counter','line_number':9216,'multiline':False]
['text':' backward mode check','line_number':9224,'multiline':False]
['text':' Check that backward gradients properly propagates through packing/unpacking','line_number':9229,'multiline':False]
['text':' Check that forward gradients are impacted by detach()','line_number':9241,'multiline':False]
['text':' Check that forward gradients are not impacted by no_grad','line_number':9249,'multiline':False]
['text':' Check that forward gradients are not impacted by inplace detach','line_number':9258,'multiline':False]
['text':' Do clones to be able to compare the values updated inplace','line_number':9271,'multiline':False]
['text':' with the original content of these Tensors','line_number':9272,'multiline':False]
['text':' Note that in this test, we use "update" to mean computing the right tangent for the dual','line_number':9277,'multiline':False]
['text':' All the inplace operations here are expected to update the primal value of the Tensors but','line_number':9278,'multiline':False]
['text':' not always their tangents.','line_number':9279,'multiline':False]
['text':' Also all mentions of "non differentiable view" here means non forward differentiable view','line_number':9280,'multiline':False]
['text':' unless specified otherwise.','line_number':9281,'multiline':False]
['text':' See note [Forward Grad View/inplace] for more details on how these views work.','line_number':9282,'multiline':False]
['text':' Check that inplace ops do not update non-differentiable views','line_number':9284,'multiline':False]
['text':' Non differentiable view','line_number':9285,'multiline':False]
['text':' Check that non differentiable view's tangent was not updated','line_number':9288,'multiline':False]
['text':' Check that the computed result is correct','line_number':9290,'multiline':False]
['text':' Other non differentiable view','line_number':9295,'multiline':False]
['text':' Ensure dual's tangent did not change','line_number':9300,'multiline':False]
['text':' Ensure dual's primal did not change','line_number':9304,'multiline':False]
['text':' Do clones to be able to compare the values updated inplace','line_number':9313,'multiline':False]
['text':' with the original content of these Tensors','line_number':9314,'multiline':False]
['text':' Check that inplace ops do update differentiable view but stop at non differentiable ones','line_number':9319,'multiline':False]
['text':' A non differentiable view','line_number':9320,'multiline':False]
['text':' A differentiable view','line_number':9322,'multiline':False]
['text':' Check that non differentiable view was not updated','line_number':9325,'multiline':False]
['text':' Check that differentiable view was updated','line_number':9327,'multiline':False]
['text':' Check that we track differentiable view even for Tensors that are not dual','line_number':9331,'multiline':False]
['text':' Updates on view should as well','line_number':9335,'multiline':False]
['text':' Unused values get a gradient of 0','line_number':9339,'multiline':False]
['text':' Check that forward non-differentiable views do prevent gradient update','line_number':9342,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/67800','line_number':9349,'multiline':False]
['text':' The codepath may depend on the op. At the time writing, when self is not a dual tensor','line_number':9350,'multiline':False]
['text':' the resulting forward grad for self for...','line_number':9351,'multiline':False]
['text':' - add_ has the same layout as self','line_number':9352,'multiline':False]
['text':' - mul_ has the same layout as other','line_number':9353,'multiline':False]
['text':' This is kind of fragile because the above depends on how the forward grad expression','line_number':9354,'multiline':False]
['text':' is written. For add and mul at least, the output inherits the layout of LHS.','line_number':9355,'multiline':False]
['text':' We want to handle at least these two cases.','line_number':9356,'multiline':False]
['text':' Add more to this list?','line_number':9357,'multiline':False]
['text':' Verify that a view relationship is created for both the primal and tangent','line_number':9374,'multiline':False]
['text':' Default detach is both forward and backward non-differentiable','line_number':9407,'multiline':False]
['text':' No differentiable outputs, shouldn't error','line_number':9433,'multiline':False]
['text':' Inplace','line_number':9436,'multiline':False]
['text':' Check size/strides match for feature dims only','line_number':9449,'multiline':False]
['text':' Check that we generate strides reasonably','line_number':9454,'multiline':False]
['text':' TensorOptions is same','line_number':9463,'multiline':False]
['text':' non-contiguous case','line_number':9473,'multiline':False]
['text':' tensor is not a view, but still does not index entirety of storage','line_number':9482,'multiline':False]
['text':' Zero-numel tensors','line_number':9487,'multiline':False]
['text':' Scalar tensor','line_number':9492,'multiline':False]
['text':' Create an object with a c++ cycle as:','line_number':9503,'multiline':False]
['text':' db -> AutogradMeta -> ForwardGrad -> db's grad','line_number':9504,'multiline':False]
['text':' db's grad -> AutogradMeta -> MulBackward','line_number':9505,'multiline':False]
['text':' MulBackward -> SavedVariable -> db','line_number':9506,'multiline':False]
['text':' This test make sure that we don't deadlock on exit of this','line_number':9511,'multiline':False]
['text':' context manager. If you do, there is something wrong with the','line_number':9512,'multiline':False]
['text':' locking of the forward ad level most likely','line_number':9513,'multiline':False]
['text':' Generic device type autograd tests.','line_number':9515,'multiline':False]
['text':' tests that gradients are evenly distributed when there are multiple max/min values','line_number':9529,'multiline':False]
['text':' tested here instead of adding a SampleInput as the backward for this case is non-differentiable for gradgrad','line_number':9530,'multiline':False]
['text':' as is the case for test_min_max_median_backprops_to_all_values above','line_number':9531,'multiline':False]
['text':' test that double backward raises an error for the case where 2 zeros in src','line_number':9544,'multiline':False]
['text':' are scattered to the same position in self','line_number':9545,'multiline':False]
['text':' check that this case passes on gradcheck','line_number':9551,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':9556,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':9568,'multiline':False]
['text':' See NOTE [ Sparse: autograd and API ] on the expected behavior of this test','line_number':9571,'multiline':False]
['text':' FIXME: make gradgradcheck work.','line_number':9591,'multiline':False]
['text':' gradgradcheck(fn, (inp,), check_batched_grad=False)','line_number':9592,'multiline':False]
['text':' assert that _values is non-differentiable','line_number':9594,'multiline':False]
['text':' sparse first','line_number':9635,'multiline':False]
['text':' dense first','line_number':9639,'multiline':False]
['text':' sparse only','line_number':9643,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':9658,'multiline':False]
['text':' integral -> integral','line_number':9661,'multiline':False]
['text':' floating point -> floating point','line_number':9667,'multiline':False]
['text':' integral -> floating point','line_number':9679,'multiline':False]
['text':' check we can convert something that loses precision','line_number':9680,'multiline':False]
['text':' floating point -> integral','line_number':9686,'multiline':False]
['text':' bool, nonzero','line_number':9696,'multiline':False]
['text':' should always work','line_number':9729,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/22843','line_number':9741,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/36956','line_number':9749,'multiline':False]
['text':' Parent graph cpu graph.','line_number':9760,'multiline':False]
['text':' Child gpu graph (much longer than parent graph).','line_number':9764,'multiline':False]
['text':' Reentrant backward in child will take much longer.','line_number':9777,'multiline':False]
['text':' Parent gpu graph.','line_number':9781,'multiline':False]
['text':' Parent graph will error out first, while child graph will continue executing.','line_number':9785,'multiline':False]
['text':' No grads should be accumulated since child graph will stop execution','line_number':9789,'multiline':False]
['text':' after parent receives error.','line_number':9790,'multiline':False]
['text':' we don't need CUDA synchronize because the statistics are not tracked at','line_number':9798,'multiline':False]
['text':' actual freeing, but at when marking the block as free.','line_number':9799,'multiline':False]
['text':' Run as separate function so that gc can clean up everything when we','line_number':9806,'multiline':False]
['text':' check for memory usage.','line_number':9807,'multiline':False]
['text':' Wait for autograd thread to cleanup failed tasks.','line_number':9810,'multiline':False]
['text':' the test doesn't work on MPS','line_number':9819,'multiline':False]
['text':' TODO: see if these tests can be ported to OpInfos or moved to where's test suite','line_number':9820,'multiline':False]
['text':' the test doesn't work on MPS','line_number':9837,'multiline':False]
['text':' In the end the memory usage should remain equal, because neither of','line_number':9865,'multiline':False]
['text':' (x + 2) and ((x + 2) * m) should be kept alive for backward, while the','line_number':9866,'multiline':False]
['text':' previous allocation of z had the same size as the current one.','line_number':9867,'multiline':False]
['text':' This test is not intended to ensure correctness of nvtx ranges.','line_number':9881,'multiline':False]
['text':' That would require something a great deal more complex (you'd have to create a','line_number':9882,'multiline':False]
['text':' profile in a subprocess, open it, and parse the sql somehow).','line_number':9883,'multiline':False]
['text':' This test is merely intended to catch if emit_nvtx breaks on construction.','line_number':9884,'multiline':False]
['text':' this checks whether it is possible to not require','line_number':9892,'multiline':False]
['text':' weight parameters, but require inputs, see #7722','line_number':9893,'multiline':False]
['text':' This test is not intended to ensure correctness of itt ranges.','line_number':9904,'multiline':False]
['text':' That would require something a great deal more complex (you'd have to create a','line_number':9905,'multiline':False]
['text':' profile in a subprocess, open it, and parse the sql somehow).','line_number':9906,'multiline':False]
['text':' This test is merely intended to catch if emit_itt breaks on construction.','line_number':9907,'multiline':False]
['text':' the test doesn't work as randn is not supported with type long','line_number':9912,'multiline':False]
['text':' Tests that the wrong type raises','line_number':9917,'multiline':False]
['text':' Tests that the wrong shape raises','line_number':9921,'multiline':False]
['text':' Tests that the wrong dtype raises','line_number':9925,'multiline':False]
['text':' Tests that self-assignment raises','line_number':9929,'multiline':False]
['text':' Tests device -> cpu grad assignment raises','line_number':9933,'multiline':False]
['text':' Tests half type on CUDA','line_number':9939,'multiline':False]
['text':' Tests cross-device assignment raises','line_number':9944,'multiline':False]
['text':' check that current device matches the variable's device','line_number':9975,'multiline':False]
['text':' At the time of writing this test, copy_ is not generated from native_functions.yaml','line_number':10000,'multiline':False]
['text':' there was a bug that bfloat16 was not recognized as floating.','line_number':10001,'multiline':False]
['text':' copy_ allows the src to have a different shape from self as long as src is','line_number':10012,'multiline':False]
['text':' broadcastable to self. Make sure forward AD handles this case.','line_number':10013,'multiline':False]
['text':' Reentrant starts on GPU thread, finishs on GPU thread','line_number':10052,'multiline':False]
['text':' Reentrant starts on CPU thread, finishs on GPU thread','line_number':10057,'multiline':False]
['text':' set ReentrantFunc node to GPU to emit tasks to GPU queue','line_number':10059,'multiline':False]
['text':' Reentrant starts on GPU thread, finishs on CPU thread','line_number':10064,'multiline':False]
['text':' set ReentrantFunc node to CPU to emit tasks to CPU queue','line_number':10066,'multiline':False]
['text':' Output on gpu so that this task will be associated with the gpu thread','line_number':10073,'multiline':False]
['text':' Artificially increase the priority of the next op to make sure it runs','line_number':10075,'multiline':False]
['text':' as soon as we reach it before the ops of branch1.','line_number':10076,'multiline':False]
['text':' Slow branch of ops on gpu so that the work queue for the gpu thread','line_number':10081,'multiline':False]
['text':' won't empty too quickly. They also have smaller priorities than the','line_number':10082,'multiline':False]
['text':' ones created by fn_on_gpu','line_number':10083,'multiline':False]
['text':' Perform checkpoint on cpu tensors. So the last op performed in the reentrant','line_number':10088,'multiline':False]
['text':' autograd is an AccumulateGrad that runs on the cpu thread for the gpu thread.','line_number':10089,'multiline':False]
['text':' So the cpu thread will notify the gpu thread with an empty NodeTask.','line_number':10090,'multiline':False]
['text':' This will segfault if the empty NodeTask is not handled properly in the','line_number':10097,'multiline':False]
['text':' gpu thread ReadyQueue','line_number':10098,'multiline':False]
['text':' modify view and back-prop through base','line_number':10102,'multiline':False]
['text':' modify view and backprop through view-of-view','line_number':10111,'multiline':False]
['text':' modify view-of-view and backprop through base','line_number':10121,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10131,'multiline':False]
['text':' Perform an in-place operation on a view of a non-leaf variable.','line_number':10133,'multiline':False]
['text':' Force a graph update with grad disabled.','line_number':10139,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10145,'multiline':False]
['text':' gradcheck modifications to views','line_number':10147,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10168,'multiline':False]
['text':' MPS backend doesn't support double types','line_number':10176,'multiline':False]
['text':' MPS backend doesn't support double types','line_number':10184,'multiline':False]
['text':' in-place modification to view makes base require grad','line_number':10186,'multiline':False]
['text':' modify view and backprop through view','line_number':10202,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10210,'multiline':False]
['text':' Test that an in-place operation on a base that forced it to require','line_number':10212,'multiline':False]
['text':' grad also forces any previous views to require grad and backprop','line_number':10213,'multiline':False]
['text':' correctly','line_number':10214,'multiline':False]
['text':' v is now dependent on r due to the in-place op on x','line_number':10222,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10229,'multiline':False]
['text':' in-place modifications of Python-autograd created view','line_number':10231,'multiline':False]
['text':' the test doesn't work on MPS as double types are not supported','line_number':10307,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/38315','line_number':10309,'multiline':False]
['text':' Expand inside the function to make sure the input to','line_number':10314,'multiline':False]
['text':' gradcheck does not have overlapping memory','line_number':10315,'multiline':False]
['text':' (1) If leaf is non-overlapping and dense, grad's layout should match its leaf.','line_number':10331,'multiline':False]
['text':' checks (1) for broadcasted gradients','line_number':10338,'multiline':False]
['text':' checks (1) for non-broadcasted gradients','line_number':10343,'multiline':False]
['text':' (2) If leaf isn't dense, checks that grads are rowmajor contiguous.','line_number':10350,'multiline':False]
['text':' checks (2) for broadcasted gradients','line_number':10354,'multiline':False]
['text':' checks (2) for non-broadcasted gradients','line_number':10357,'multiline':False]
['text':' Just make sure the op doesn't raise an error','line_number':10390,'multiline':False]
['text':' and resulting tensor has requires_grad=False.','line_number':10391,'multiline':False]
['text':' Test warning during backward are always propagated as python warnings (gh-50209)','line_number':10401,'multiline':False]
['text':' NOTE: For device=cuda, warning gets propagated from a worker thread','line_number':10402,'multiline':False]
['text':' OpInfo doesn't naturally support input of mixed types, hence this test here.','line_number':10421,'multiline':False]
['text':' In the following cases, `resize` is no-op,','line_number':10443,'multiline':False]
['text':' so no version bumps.','line_number':10444,'multiline':False]
['text':' How does this work?','line_number':10484,'multiline':False]
['text':' modify the view','line_number':10505,'multiline':False]
['text':' self.assertClonedLenEqual(ctx, 1)','line_number':10507,'multiline':False]
['text':' Make sure we don't clone if the tensor was once saved, but','line_number':10538,'multiline':False]
['text':' by the time we do in-place, it is no longer saved','line_number':10539,'multiline':False]
['text':' We should only clone once','line_number':10555,'multiline':False]
['text':' in this case, it is no longer a view it seems','line_number':10589,'multiline':False]
['text':' Out of context','line_number':10606,'multiline':False]
['text':' Different context','line_number':10615,'multiline':False]
['text':' new tensors created through constructors are inference tensors','line_number':10675,'multiline':False]
['text':' requires_grad doesn't change inference tensor behavior in InferenceMode','line_number':10680,'multiline':False]
['text':' `a` gets saved outside of inference mode','line_number':10693,'multiline':False]
['text':' tensors created outside of inference mode aren't','line_number':10699,'multiline':False]
['text':' inference tensors, so they will still have their','line_number':10700,'multiline':False]
['text':' version counters tracked','line_number':10701,'multiline':False]
['text':' performing a non-view operation produces a inference tensor','line_number':10715,'multiline':False]
['text':' that does not require grad','line_number':10716,'multiline':False]
['text':' after performing inplace operation, tensor is still','line_number':10727,'multiline':False]
['text':' an inference tensor','line_number':10728,'multiline':False]
['text':' inplace ops with manual kernel for ADInplaceOrView key in VariableTypeManual.cpp','line_number':10735,'multiline':False]
['text':' perform view operation produces inference tensor','line_number':10745,'multiline':False]
['text':' that does not require grad','line_number':10746,'multiline':False]
['text':' leaf variable that requires grad is being used in an inplace','line_number':10771,'multiline':False]
['text':' operation when requires_grad=True','line_number':10772,'multiline':False]
['text':' inplace -> inplace','line_number':10803,'multiline':False]
['text':' inplace -> inplace -> view','line_number':10808,'multiline':False]
['text':' inplace -> inplace','line_number':10830,'multiline':False]
['text':' inplace -> inplace -> view','line_number':10835,'multiline':False]
['text':' view -> view','line_number':10853,'multiline':False]
['text':' view -> view -> inplace','line_number':10860,'multiline':False]
['text':' Accessing is_leaf in python tries to update grad_fn and raises:','line_number':10865,'multiline':False]
['text':' A view was created in inference mode and its base or','line_number':10866,'multiline':False]
['text':' another view of its base has been modified inplace in normal mode','line_number':10867,'multiline':False]
['text':' tmp.is_leaf','line_number':10868,'multiline':False]
['text':' add is safe since it doesn't save any variable for backward','line_number':10909,'multiline':False]
['text':' leaf inference tensor with requires_grad=True can still have gradient','line_number':10914,'multiline':False]
['text':' TODO: Test this with an autograd.Function when it works','line_number':10923,'multiline':False]
['text':'       stack stopped capturing a TensorList input','line_number':10924,'multiline':False]
['text':' # inference tensor in TensorList input','line_number':10925,'multiline':False]
['text':' inputs = [s, c]','line_number':10926,'multiline':False]
['text':' with self.assertRaisesRegex(RuntimeError, err_msg):','line_number':10927,'multiline':False]
['text':'     torch.stack(inputs)','line_number':10928,'multiline':False]
['text':' inference tensor in TensorList input','line_number':10945,'multiline':False]
['text':' view_as is a composite op which calls view with only one','line_number':10963,'multiline':False]
['text':' tensor argument. So there isn't a mixed inference and normal','line_number':10964,'multiline':False]
['text':' tensor inputs for view ops','line_number':10965,'multiline':False]
['text':' this is fine since its equivalent as s.view(c.sizes()) which','line_number':10970,'multiline':False]
['text':' isn't a mixed input scenario','line_number':10971,'multiline':False]
['text':' Test whether exception in child thread','line_number':11049,'multiline':False]
['text':' are propagated to main thread.','line_number':11050,'multiline':False]
['text':' simple multithreaded backward that create threads in the beginning of training','line_number':11058,'multiline':False]
['text':' and everything else is training separately, i.e. inputs, operations, etc.','line_number':11059,'multiline':False]
['text':' simple multithreaded backward with only shared inputs (i.e. This is common','line_number':11069,'multiline':False]
['text':' for things like Hogwild multithreaded training with multiple CPU threads)','line_number':11070,'multiline':False]
['text':' Since we are calling backward from multiple threads','line_number':11077,'multiline':False]
['text':' and all threads share the same input, when we do backward','line_number':11078,'multiline':False]
['text':' concurrently, different backwards will all accumulate to','line_number':11079,'multiline':False]
['text':' the same .grad for each input, and the gradients should','line_number':11080,'multiline':False]
['text':' be equal to num_threads * gradient','line_number':11081,'multiline':False]
['text':' since we use functional grad() api, gradients will not','line_number':11090,'multiline':False]
['text':' be accumulate to the same place and should be the same','line_number':11091,'multiline':False]
['text':' Multihooks should behave independently per execution of backward','line_number':11095,'multiline':False]
['text':' Test that the hook fired the number of times we ran backward','line_number':11096,'multiline':False]
['text':' even if those executions occur concurrently on different threads','line_number':11097,'multiline':False]
['text':' Leave one hook partially applied','line_number':11127,'multiline':False]
['text':' DataParallel is calling the forward in different threads','line_number':11173,'multiline':False]
['text':' without progating TLS, so hooks should not be called here','line_number':11174,'multiline':False]
['text':' DataParallel only uses one thread','line_number':11177,'multiline':False]
['text':' so hooks should be called here','line_number':11178,'multiline':False]
['text':' hooks should be called here','line_number':11188,'multiline':False]
['text':' User might write a network that starts on one CPU thread, then runs its second half','line_number':11192,'multiline':False]
['text':' concurrently with other threads (either via python threading or fork/join calls),','line_number':11193,'multiline':False]
['text':' then calls backward()/grad() on BOTH threads, like a Y pattern from input at the','line_number':11194,'multiline':False]
['text':' bottom to output at the top. This way part of the GraphTask is being shared across','line_number':11195,'multiline':False]
['text':' different threads and we need to ensure user specify retain_graph=True, otherwise','line_number':11196,'multiline':False]
['text':' error out with the correct error message','line_number':11197,'multiline':False]
['text':' Case 1: multiple backward with python threads, retain_graph=False','line_number':11199,'multiline':False]
['text':' should throw error in some threads with no retain_graph.','line_number':11200,'multiline':False]
['text':' at least one thread will be success in this case, all other threads should raise','line_number':11215,'multiline':False]
['text':' with the error that throw to user to recommend them specify retain_graph=True','line_number':11216,'multiline':False]
['text':' multiple backward with python threads, no error with retain_graph=True','line_number':11219,'multiline':False]
['text':' result should equal to num_thread * gradients','line_number':11227,'multiline':False]
['text':' multiple backward with jit threads (fork/join primitive)','line_number':11231,'multiline':False]
['text':' similar to test_python_thread_in_middle, we test with retain_graph=False/True','line_number':11232,'multiline':False]
['text':' Case 1: multiple grad() calls with jit threads, retain_graph=False','line_number':11234,'multiline':False]
['text':' should throw error in some threads with no retain_graph.','line_number':11235,'multiline':False]
['text':' Case 2: no error with retain_graph=True','line_number':11255,'multiline':False]
['text':' TODO(@anjali411): add an OpInfo based test for torch.cat','line_number':11294,'multiline':False]
['text':' Issue: https://github.com/pytorch/pytorch/issues/51627','line_number':11295,'multiline':False]
['text':'        https://github.com/pytorch/pytorch/issues/75852','line_number':11296,'multiline':False]
['text':' Test as a context manager','line_number':11314,'multiline':False]
['text':' function <> tuple of function arbitrarily wrapped in checkpoint in various ways','line_number':11378,'multiline':False]
['text':' The hook is registered on the original graph','line_number':11540,'multiline':False]
['text':' And backward is performed on the original graph','line_number':11542,'multiline':False]
['text':' do backward again, but skip over the part of the graph where','line_number':11555,'multiline':False]
['text':' the hook was registered','line_number':11556,'multiline':False]
['text':' Since clone does not save anything, it is not recomputed iff','line_number':11573,'multiline':False]
['text':' early stop is enabled.','line_number':11574,'multiline':False]
['text':' Early stopping is enabled by default','line_number':11577,'multiline':False]
['text':' Try using the context manager to set early stopping to False.','line_number':11583,'multiline':False]
['text':' Expect early stopping to be disabled for all checkpoints ran under','line_number':11584,'multiline':False]
['text':' the context manager, even though context manager is no longer active','line_number':11585,'multiline':False]
['text':' when backward/recomputation is performed.','line_number':11586,'multiline':False]
['text':' Case 1: We have one tensor saved and its the input','line_number':11596,'multiline':False]
['text':' We have two different counters here because in this case we actually','line_number':11598,'multiline':False]
['text':' do call into x.sin() at the python level during recomputation whether','line_number':11599,'multiline':False]
['text':' or not early stop is enabled. This is because the early stopping','line_number':11600,'multiline':False]
['text':' only happens at the autograd level (preventing us from reaching the','line_number':11601,'multiline':False]
['text':' backend).','line_number':11602,'multiline':False]
['text':' With early stopping (enabled by default)','line_number':11620,'multiline':False]
['text':' Without early stopping','line_number':11628,'multiline':False]
['text':' Case 2: Forward saves no tensors','line_number':11638,'multiline':False]
['text':' Since unpack isn't even called, counter is 1 whether or not early stop','line_number':11640,'multiline':False]
['text':' is enabled!','line_number':11641,'multiline':False]
['text':' With early stopping (enabled by default)','line_number':11648,'multiline':False]
['text':' Without early stopping','line_number':11654,'multiline':False]
['text':' using _test_autograd_multiple_dispatch.fullcoverage which has','line_number':11666,'multiline':False]
['text':' registrations in derivatives.yaml for Default, AutogradCUDA and NestedTensorAutograd','line_number':11667,'multiline':False]
['text':' bogus default gradient registered for Autograd is grad + 1','line_number':11673,'multiline':False]
['text':' bogus gradient registered for AutogradCUDA is grad * 2','line_number':11676,'multiline':False]
['text':' test registered AutogradNestedTensor formula','line_number':11679,'multiline':False]
['text':' bogus gradient for AutogradNestedTensor is grad * grad','line_number':11690,'multiline':False]
['text':' using _test_autograd_multiple_dispatch.ntonly','line_number':11696,'multiline':False]
['text':' which has registrations in derivatives.yaml for NestedTensorAutograd and otherwise is CompositeImplicit','line_number':11697,'multiline':False]
['text':' t.grad is just out.grad by composite op since _test_autograd_multiple_dispatch is just a clone','line_number':11702,'multiline':False]
['text':' test registered AutogradNestedTensor formula','line_number':11705,'multiline':False]
['text':' bogus gradient for AutogradNestedTensor is grad * grad + grad','line_number':11716,'multiline':False]
['text':' check that forward mode AD is only registered for the Default','line_number':11721,'multiline':False]
['text':' dispatch for _test_autograd_multiple_dispatch.fullcoverage and not AutogradCUDA','line_number':11722,'multiline':False]
['text':' tests that view_copy derivative formulas are also generated per dispatch key','line_number':11740,'multiline':False]
['text':' from their respective view ops in derivatives.yaml','line_number':11741,'multiline':False]
['text':' _test_autograd_multiple_dispatch_view does a .view(-1) on the input','line_number':11744,'multiline':False]
['text':' forward and backward give the same shape + result','line_number':11752,'multiline':False]
['text':' backward results are per-dispatch-key in derivatives.yaml','line_number':11755,'multiline':False]
['text':' gradient registered to AutogradCUDA is grad.reshape_as(self) + 1','line_number':11757,'multiline':False]
['text':' Default gradient registered is grad.reshape_as(self)','line_number':11760,'multiline':False]
['text':' Tests that sum.dim_IntList's input is not saved for regular tensors but is saved for nested tensors','line_number':11765,'multiline':False]
['text':' Don't modify the input inplace','line_number':11767,'multiline':False]
['text':' sum's input is not saved for regular Tensors','line_number':11774,'multiline':False]
['text':' sum's input is saved for Nested Tensors','line_number':11777,'multiline':False]
['text':' Expect to see that even though c has the smallest sequence number, it is still the first node to get run in autograd.','line_number':11867,'multiline':False]
['text':' Also check that although a comes first during the forward, after giving it priority with sequence_nr,','line_number':11868,'multiline':False]
['text':' its autograd node is run before that of b.','line_number':11869,'multiline':False]
['text':' Import test cases from below autograd/ here. These are found','line_number':11877,'multiline':False]
['text':' implicitly by the loader, so Flake8 thinks they are unused, hence','line_number':11878,'multiline':False]
['text':' the suppressions.','line_number':11879,'multiline':False]
['text':' noqa: F401','line_number':11881,'multiline':False]
['text':' noqa: F401','line_number':11882,'multiline':False]
['text':' e.g., TestAutogradDeviceTypeCPU and TestAutogradDeviceTypeCUDA','line_number':11884,'multiline':False]
