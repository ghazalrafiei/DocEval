['text':' Owner(s): ["module: optimizer"]','line_number':1,'multiline':False]
['text':' load_tests from common_utils is used to automatically filter tests for','line_number':38,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':39,'multiline':False]
['text':' For rosenbrock tests, it is mandated that the param is a tensor with 2 numbers','line_number':68,'multiline':False]
['text':' NB: We torture test the optimizer by returning an','line_number':90,'multiline':False]
['text':' uncoalesced sparse tensor','line_number':91,'multiline':False]
['text':' Depending on w, provide only the x or y gradient','line_number':93,'multiline':False]
['text':' Do cyclic coordinate descent','line_number':127,'multiline':False]
['text':' Tolerance is increased due to floating point error from different','line_number':137,'multiline':False]
['text':' code path for dense case: x v.s. x - x / 4.0 + x / 4.0','line_number':138,'multiline':False]
['text':' to check if the optimizer can be printed as a string','line_number':196,'multiline':False]
['text':' Note: disable dynamo on this function','line_number':222,'multiline':False]
['text':' This allows us to continue running actual logic of the optimizer','line_number':223,'multiline':False]
['text':' tests in dynamo without tracing this test code which has a lot of unsupported','line_number':224,'multiline':False]
['text':' behavior','line_number':225,'multiline':False]
['text':' Note: Disable dynamo on this function','line_number':233,'multiline':False]
['text':' This avoids a bug where input_cuda is not detected in the environment','line_number':234,'multiline':False]
['text':' because it currently is not defined in the local environmet. Unable to repro','line_number':235,'multiline':False]
['text':' anywhere else however and this is test code that we don't need to spend','line_number':236,'multiline':False]
['text':' time getting dynamo to trace unless the issue repros in real models.','line_number':237,'multiline':False]
['text':' Prime the optimizer','line_number':249,'multiline':False]
['text':' Clone the weights and construct new optimizer for them','line_number':252,'multiline':False]
['text':' Load state dict','line_number':258,'multiline':False]
['text':' Run both optimizers in parallel','line_number':262,'multiline':False]
['text':' Make sure state dict is deterministic with equal but not identical parameters','line_number':268,'multiline':False]
['text':' Make sure repeated parameters have identical representation in state dict','line_number':270,'multiline':False]
['text':' Make sure that optimizers that support maximize can load older models','line_number':277,'multiline':False]
['text':' Make sure we can still step','line_number':284,'multiline':False]
['text':' Undo these changes before proceeding!','line_number':286,'multiline':False]
['text':' Make sure that optimizers that support foreach can load older models','line_number':288,'multiline':False]
['text':' Make sure we can still step','line_number':294,'multiline':False]
['text':' Undo these changes before proceeding!','line_number':296,'multiline':False]
['text':' Make sure that loading optimizers with step not wrapped in tensor can work','line_number':299,'multiline':False]
['text':' Check that state dict can be loaded even when we cast parameters','line_number':309,'multiline':False]
['text':' to a different type and move to a different device.','line_number':310,'multiline':False]
['text':' Make sure state_dict_c isn't modified by merely calling load_state_dict','line_number':329,'multiline':False]
['text':' Make sure that device of state['step'] is still CPU','line_number':332,'multiline':False]
['text':' validate deepcopy() copies all public attributes','line_number':346,'multiline':False]
['text':' non-contiguous parameters','line_number':397,'multiline':False]
['text':' CUDA','line_number':407,'multiline':False]
['text':' Multi-GPU','line_number':419,'multiline':False]
['text':' Specifically test that inputting params of different dtypes and devices','line_number':674,'multiline':False]
['text':' is handled equivalently on the foreach and fused implementations as the','line_number':675,'multiline':False]
['text':' single tensor implementations. We need multiple GPUs (vs just a CPU and','line_number':676,'multiline':False]
['text':' GPU) because fused adam only works on GPUs. (Thus we only run the tests','line_number':677,'multiline':False]
['text':' that call into this helper when TEST_MULTIGPU.)','line_number':678,'multiline':False]
['text':' single tensor ASGD does not support capturable','line_number':701,'multiline':False]
['text':' Increasing the tolerance as we are collating lots of ops together for optimizers and','line_number':723,'multiline':False]
['text':' the designated tolerances are for single op only.','line_number':724,'multiline':False]
['text':' check that optimizer states are the same','line_number':731,'multiline':False]
['text':' why 7? iteration 7 is where we start to see differences for RAdam','line_number':744,'multiline':False]
['text':' params interacting with the small eps value, because that's right','line_number':745,'multiline':False]
['text':' after rho_t becomes greater than 5 in step 6.','line_number':746,'multiline':False]
['text':' single tensor ASGD does not support capturable','line_number':766,'multiline':False]
['text':' foreach/fused optimizers should be tested with a param_groups['params'] with','line_number':770,'multiline':False]
['text':' zero_size tensor as its last param.','line_number':771,'multiline':False]
['text':' ref: https://github.com/pytorch/pytorch/issues/100701','line_number':772,'multiline':False]
['text':' Test that step behaves as expected (a no-op) when grads are set to None','line_number':785,'multiline':False]
['text':' check that optimizer states are the same','line_number':804,'multiline':False]
['text':' single tensor ASGD does not support capturable','line_number':822,'multiline':False]
['text':' The 128 is critical here! Our CUDACachingAllocator allocates in blocks of 512,','line_number':828,'multiline':False]
['text':' meaning any tensor that occupies <512 bytes of memory will allocate a whole','line_number':829,'multiline':False]
['text':' 512 bytes anyway. We use 128 (since datasize would be 4 bytes) so that param','line_number':830,'multiline':False]
['text':' is size 512 exactly, making our later calculations for intermediate_size easy.','line_number':831,'multiline':False]
['text':' we expect a budget of 1 intermediate most of the time','line_number':852,'multiline':False]
['text':' with capturable in Adam(W), we have 2 extra intermediates for the bias_corrections','line_number':855,'multiline':False]
['text':' with Adadelta, we have 2 extra for (acc_delta + eps) and (square_avg + eps)','line_number':856,'multiline':False]
['text':' ASGD allocates axs, 2x mus, 2x etas, and grads at the same time','line_number':857,'multiline':False]
['text':' with capturable in NAdam, we have 3 extra intermediates for the','line_number':860,'multiline':False]
['text':' bias_correction, mus, and mu_nexts','line_number':861,'multiline':False]
['text':' NAdam uses two intermediates at the same time (grads & exp_avg_sq_sqrt)','line_number':865,'multiline':False]
['text':' Adagrad uses std and grads at the same time','line_number':866,'multiline':False]
['text':' RMSprop uses avg and grads','line_number':867,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/110940','line_number':965,'multiline':False]
['text':' We coerce step to always be float32','line_number':966,'multiline':False]
['text':' note(crcrpar): H100 wasn't sufficient for Adamax, surprisingly','line_number':988,'multiline':False]
['text':' foreach for lr tensors tested in multi configs','line_number':1183,'multiline':False]
['text':' foreach for lr tensors tested in multi configs','line_number':1249,'multiline':False]
['text':' ROCm precision is too low to pass this test','line_number':1286,'multiline':False]
['text':' Handles https://github.com/pytorch/pytorch/issues/69698','line_number':1288,'multiline':False]
['text':' Handles https://github.com/pytorch/pytorch/issues/110606','line_number':1328,'multiline':False]
['text':' NAdamW tests','line_number':1371,'multiline':False]
['text':' RAdamW tests','line_number':1566,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/issues/84560','line_number':1728,'multiline':False]
['text':' self._test_complex_2d(optimizer)','line_number':1729,'multiline':False]
['text':' Intentionally construct a multidimensional empty v for the sparse grad','line_number':1874,'multiline':False]
['text':' Single dim v passes the test while multidim correctly repros the issue','line_number':1875,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/82486','line_number':1876,'multiline':False]
['text':' NOTE: This SIMULATES a fused/capturable optimizer with state moved to CPU, issue 103256','line_number':1933,'multiline':False]
['text':' How do we get there? Users typically create CUDA models on fused optimizers and then','line_number':1934,'multiline':False]
['text':' store checkpoints on CPU as CUDA memory is limited with torch.load(...map_location="cpu").','line_number':1935,'multiline':False]
['text':' Since this is a unit test, it is more expedient to simulate what the state_dict','line_number':1936,'multiline':False]
['text':' would look like, which is basically CPU tensors with fused/capturable flag = True.','line_number':1937,'multiline':False]
['text':' load','line_number':1947,'multiline':False]
['text':' check if pre hooks were registered','line_number':1970,'multiline':False]
['text':' remove handles, take step and verify that hook is no longer registered','line_number':1973,'multiline':False]
['text':' check if pre hooks were registered','line_number':1992,'multiline':False]
['text':' remove handles, take step and verify that hook is no longer registered','line_number':1995,'multiline':False]
['text':' register global hooks to both optimizers','line_number':2024,'multiline':False]
['text':' register local hooks','line_number':2028,'multiline':False]
['text':' remove all hooks','line_number':2041,'multiline':False]
['text':' The typical use case for returning a state dict is to drastically modify the state dict.','line_number':2081,'multiline':False]
['text':' I will simulate by simply making a deep copy and ensuring that my_state_dict still gets used','line_number':2082,'multiline':False]
['text':' usually one would have a new opt instance here, but it's all the same here','line_number':2124,'multiline':False]
['text':' If prepend were False would be 0.003 but since prepend is True, the other hook overrides','line_number':2131,'multiline':False]
['text':' Ignored is the list of values in `opt_differentiable_state`, we do this','line_number':2157,'multiline':False]
['text':' for `gradcheck` to correctly track the state tensors as function inputs','line_number':2158,'multiline':False]
['text':' because otherwise it can't unpack the values in the `opt_differentiable_state`','line_number':2159,'multiline':False]
['text':' dict','line_number':2160,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2202,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2203,'multiline':False]
['text':' This can cause issues with large values and nan due to sqrt ops','line_number':2233,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2261,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2262,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2283,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2284,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2304,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2305,'multiline':False]
['text':' `step` `eta` & `mu` are not continuous variables (even though we define them as floats)','line_number':2328,'multiline':False]
['text':' and so they shouldn't require gradients.','line_number':2329,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2351,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2352,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2373,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2374,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2398,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2399,'multiline':False]
['text':' `step` is not a continuous variable (even though we define it as a float)','line_number':2433,'multiline':False]
['text':' and so it shouldn't require gradients.','line_number':2434,'multiline':False]
