['text':' Owner(s): ["module: optimizer", "module: LrScheduler" ]','line_number':1,'multiline':False]
['text':' load_tests from common_utils is used to automatically filter tests for','line_number':40,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':41,'multiline':False]
['text':' To ensure that there are no reference cycles in scheduler,','line_number':150,'multiline':False]
['text':' we need to turn off the garbage collector. Since gc will','line_number':151,'multiline':False]
['text':' automatically collect unreachable objects.','line_number':152,'multiline':False]
['text':' restore','line_number':157,'multiline':False]
['text':' allow any warning to be raised','line_number':162,'multiline':False]
['text':' allow any warning to be raised','line_number':176,'multiline':False]
['text':' allow any warning to be raised','line_number':193,'multiline':False]
['text':' allow any warning to be raised','line_number':210,'multiline':False]
['text':' allow any warning to be raised','line_number':227,'multiline':False]
['text':' emulate use-case with optimizer.step overridden','line_number':231,'multiline':False]
['text':' allow any warning to be raised','line_number':252,'multiline':False]
['text':' allow any warning to be raised','line_number':257,'multiline':False]
['text':' allow any warning to be raised','line_number':266,'multiline':False]
['text':' allow any warning to be raised','line_number':271,'multiline':False]
['text':' allow any warning to be raised','line_number':280,'multiline':False]
['text':' emulate use-case with optimizer.step overridden','line_number':284,'multiline':False]
['text':' lr = 0.05     if epoch < 3','line_number':337,'multiline':False]
['text':' lr = 0.005    if 30 <= epoch < 6','line_number':338,'multiline':False]
['text':' lr = 0.0005   if epoch >= 9','line_number':339,'multiline':False]
['text':' lr = 0.05     if epoch < 2','line_number':358,'multiline':False]
['text':' lr = 0.005    if 2 <= epoch < 5','line_number':359,'multiline':False]
['text':' lr = 0.0005   if 5 <= epoch < 9','line_number':360,'multiline':False]
['text':' lr = 0.00005   if 9 <= epoch','line_number':361,'multiline':False]
['text':' lr = 0.05     if epoch < 2','line_number':369,'multiline':False]
['text':' lr = 0.005    if 2 <= epoch < 5','line_number':370,'multiline':False]
['text':' lr = 0.0005   if epoch < 9','line_number':371,'multiline':False]
['text':' lr = 0.00005   if epoch >= 9','line_number':372,'multiline':False]
['text':' lr = 0.05     if epoch < 2','line_number':380,'multiline':False]
['text':' lr = 0.005    if 2 <= epoch < 5','line_number':381,'multiline':False]
['text':' lr = 0.0005   if epoch < 9','line_number':382,'multiline':False]
['text':' lr = 0.00005   if epoch >= 9','line_number':383,'multiline':False]
['text':' lr = 0.025     if epoch < 5','line_number':391,'multiline':False]
['text':' lr = 0.005    if 5 <= epoch','line_number':392,'multiline':False]
['text':' lr = 0.025     if epoch == 0','line_number':400,'multiline':False]
['text':' lr = 0.03125   if epoch == 1','line_number':401,'multiline':False]
['text':' lr = 0.0375    if epoch == 2','line_number':402,'multiline':False]
['text':' lr = 0.04375   if epoch == 3','line_number':403,'multiline':False]
['text':' lr = 0.005     if 4 <= epoch','line_number':404,'multiline':False]
['text':' lr = 0.025     if epoch < 5','line_number':425,'multiline':False]
['text':' lr = 0.005    if 5 <= epoch','line_number':426,'multiline':False]
['text':' lr = 0.025     if epoch == 0','line_number':434,'multiline':False]
['text':' lr = 0.03125   if epoch == 1','line_number':435,'multiline':False]
['text':' lr = 0.0375    if epoch == 2','line_number':436,'multiline':False]
['text':' lr = 0.04375   if epoch == 3','line_number':437,'multiline':False]
['text':' lr = 0.005     if 4 <= epoch','line_number':438,'multiline':False]
['text':' lr = 0.025     if epoch < 5','line_number':463,'multiline':False]
['text':' lr = 0.005    if 5 <= epoch','line_number':464,'multiline':False]
['text':' lr = 0.025     if epoch == 0','line_number':472,'multiline':False]
['text':' lr = 0.03125   if epoch == 1','line_number':473,'multiline':False]
['text':' lr = 0.0375    if epoch == 2','line_number':474,'multiline':False]
['text':' lr = 0.04375   if epoch == 3','line_number':475,'multiline':False]
['text':' lr = 0.005     if 4 <= epoch','line_number':476,'multiline':False]
['text':' 1.025 > 1.1**0.25','line_number':623,'multiline':False]
['text':' Ensure that multiple schedulers does not affect the initial learning rate','line_number':744,'multiline':False]
['text':' test runs step before checking lr','line_number':987,'multiline':False]
['text':' test runs step before checking lr','line_number':1009,'multiline':False]
['text':' test runs step before checking lr','line_number':1031,'multiline':False]
['text':' test runs step before checking lr','line_number':1051,'multiline':False]
['text':' 1.025 > 1.1**0.25','line_number':1052,'multiline':False]
['text':' test runs step before checking lr','line_number':1072,'multiline':False]
['text':' Note [Temporarily set optimizer to Adam]','line_number':1479,'multiline':False]
['text':' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~','line_number':1480,'multiline':False]
['text':' The TestLRScheduler object carries around an SGD optimizer to avoid having to','line_number':1481,'multiline':False]
['text':' instantiate one for every test. This gets in the way for our very specific case','line_number':1482,'multiline':False]
['text':' in which we need to use Adam (or really any optimizer that doesn't use momentum)','line_number':1483,'multiline':False]
['text':' in order to test that the momentum bug in CyclicLR is fixed (the bug is described','line_number':1484,'multiline':False]
['text':' in more detail in https://github.com/pytorch/pytorch/issues/19003 ).','line_number':1485,'multiline':False]
['text':' set optimizer back to SGD','line_number':1509,'multiline':False]
['text':' Case 1: Built-in mode','line_number':1534,'multiline':False]
['text':' Case 2: Custom `scale_fn`, a function object','line_number':1542,'multiline':False]
['text':' Case 3: Custom `scale_fn`, a callable class','line_number':1552,'multiline':False]
['text':' Case 1: Built-in mode','line_number':1571,'multiline':False]
['text':' Case 2: Custom `scale_fn`','line_number':1580,'multiline':False]
['text':' set optimizer back to SGD','line_number':1704,'multiline':False]
['text':' same swa_lr for different param_groups','line_number':1830,'multiline':False]
['text':' separate swa_lr for different param_groups','line_number':1858,'multiline':False]
['text':' Test that SWALR raises errors for incorrect hyper-parameters','line_number':1910,'multiline':False]
['text':' step before assert: skip initial lr','line_number':2035,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/21965','line_number':2219,'multiline':False]
['text':' No warning is raised when verbose is the default value.','line_number':2270,'multiline':False]
