['text':' Owner(s): ["NNC"]','line_number':1,'multiline':False]
['text':' these needs to be set before `common_utils`','line_number':14,'multiline':False]
['text':' infers `GRAPH_EXECUTOR`.','line_number':15,'multiline':False]
['text':' this file **requires** these settings','line_number':16,'multiline':False]
['text':' and setting them after `GRAPH_EXECUTOR` is','line_number':17,'multiline':False]
['text':' inferred erroneously runs or skips','line_number':18,'multiline':False]
['text':' some tests','line_number':19,'multiline':False]
['text':' noqa: F401','line_number':42,'multiline':False]
['text':' note: `self.dynamic_shapes` instatiated in specialization of class','line_number':91,'multiline':False]
['text':' defined below','line_number':92,'multiline':False]
['text':' TODO - upstream','line_number':120,'multiline':False]
['text':' double check we fused','line_number':177,'multiline':False]
['text':' we use a bigger tensor now (size 2)','line_number':180,'multiline':False]
['text':' if we won't trigger a recompilation','line_number':181,'multiline':False]
['text':' we will still create a tensor up to (size 1)','line_number':182,'multiline':False]
['text':' if the type check fails','line_number':183,'multiline':False]
['text':' shape changed if we don't trigger recompilation','line_number':185,'multiline':False]
['text':' we would compute the wrong result silently','line_number':186,'multiline':False]
['text':' A smoke test to make sure we won't use the same kernel for contiguous','line_number':266,'multiline':False]
['text':' and non-contiguous arguments.','line_number':267,'multiline':False]
['text':' TODO: add optionally enabled debug counters to the fuser to verify','line_number':268,'multiline':False]
['text':'       that we really can tell the difference between configurations','line_number':269,'multiline':False]
['text':' Note: Non fused inputs must be float to prevent loss of precision','line_number':305,'multiline':False]
['text':' Verifies outputs','line_number':312,'multiline':False]
['text':' Verifies gradients','line_number':319,'multiline':False]
['text':' single fusion node causes error','line_number':329,'multiline':False]
['text':' We shouldn't treat cat nodes as broadcasting. All their inputs','line_number':332,'multiline':False]
['text':' need to be checked for having the same map size, before we can','line_number':333,'multiline':False]
['text':' run the kernel.','line_number':334,'multiline':False]
['text':' NOTE: y is broadcastable to x, but output of f(x, y) should have','line_number':338,'multiline':False]
['text':' shape 3x4, and not 4x4.','line_number':339,'multiline':False]
['text':' splitSize = 1','line_number':380,'multiline':False]
['text':' contiguous case','line_number':383,'multiline':False]
['text':' non-contiguous case','line_number':386,'multiline':False]
['text':' XXX: The old fuser does broadcast_tensors but the new fuser doesn't.','line_number':412,'multiline':False]
['text':' FileCheck().check("broadcast_tensors").check('with ' + FUSION_GROUP + '_') \','line_number':413,'multiline':False]
['text':'     .check_count('ConstantChunk', 2, exactly=True).run(str(graph))','line_number':414,'multiline':False]
['text':' The arguments are intentionally used out of order as a test to see','line_number':446,'multiline':False]
['text':' if the fusion compiler adds extra args in the correct order','line_number':447,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':592,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':593,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':594,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':622,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':623,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':624,'multiline':False]
['text':' TODO: We leak CUDA memory here because the traced graph holds onto a','line_number':700,'multiline':False]
['text':' constant-ified tensor. Since the Python-global CompilationUnit is alive','line_number':701,'multiline':False]
['text':' until the end of the process, the memory is effectively leaked.','line_number':702,'multiline':False]
['text':' Removed `_cuda` suffix from this test which disables leak-checking.','line_number':703,'multiline':False]
['text':' If this is a real problem, we'll need to revisit Torchscript Function','line_number':704,'multiline':False]
['text':' lifetimes in Python.','line_number':705,'multiline':False]
['text':' scalar weight overload','line_number':712,'multiline':False]
['text':' tensor weight overload','line_number':716,'multiline':False]
['text':' TODO: uncomment when TE enables support for scalar tensors','line_number':724,'multiline':False]
['text':' ge_weight_tensor = self.checkTrace(foo_weight_tensor, (start, end))','line_number':725,'multiline':False]
['text':' graph = ge_weight_tensor.graph_for(start, end)','line_number':726,'multiline':False]
['text':' self.assertAllFused(graph)','line_number':727,'multiline':False]
['text':' disabling concat causes error with single concat node','line_number':730,'multiline':False]
['text':' XXX: TE fuser can handle concats in a fusion group.','line_number':742,'multiline':False]
['text':' FileCheck().check("FusedConcat").check_next("return").run(str(graph))','line_number':743,'multiline':False]
['text':' the if node and the fusion group inside it should only have one output','line_number':763,'multiline':False]
['text':' Invariant: the output of prim::FusedConcat may','line_number':768,'multiline':False]
['text':' not be an input to any node inside the FusionGroup.','line_number':769,'multiline':False]
['text':' XXX: TE fuser can handle concats inside a fusion group.','line_number':782,'multiline':False]
['text':' FileCheck().check("FusedConcat").check_next("return").run(str(graph))','line_number':783,'multiline':False]
['text':' use another function otherwise we will bailout','line_number':818,'multiline':False]
['text':' and won't be able to do fused checks','line_number':819,'multiline':False]
['text':' Should not crash; these should compile different kernels.','line_number':843,'multiline':False]
['text':' TODO: we're currently not checking 'device' in the type info when pulling','line_number':849,'multiline':False]
['text':' nodes into a fusion group. We should fix that and re-enable this test.','line_number':850,'multiline':False]
['text':' fusion: lambda x. x * x * x * x * x','line_number':858,'multiline':False]
['text':' There are 3 FusionGroups. Because they have the same graph, they','line_number':871,'multiline':False]
['text':' should reuse the same KernelSpec in the KernelSpec cache.','line_number':872,'multiline':False]
['text':' XXX: This assumes that the same kernel isn't already used by another test','line_number':877,'multiline':False]
['text':' FIXME: Use the TE fuser's way of querying the cache.','line_number':878,'multiline':False]
['text':' self.assertEqual(new_cache_size - prev_cache_size, 1)','line_number':879,'multiline':False]
['text':' single fusion node causes error','line_number':900,'multiline':False]
['text':' TODO... Chunk','line_number':907,'multiline':False]
['text':' XXX: TE fuser can handle concats inside a fusion group.','line_number':911,'multiline':False]
['text':' FileCheck().check("FusedConcat").check_next("return").run(str(graph))','line_number':912,'multiline':False]
['text':' lstm has gates = x.mm(w_ih.t()) + hx.mm(w_hh.t()) + b_ih + b_hh.','line_number':916,'multiline':False]
['text':' Test that any permutation of this will still result in one FusionGroup.','line_number':917,'multiline':False]
['text':' TODO: Fuser doesn't work at all when inputs require grad. Fix that','line_number':936,'multiline':False]
['text':' TODO: chunk','line_number':943,'multiline':False]
['text':' TODO: chunk','line_number':959,'multiline':False]
['text':' only enabled on gpu','line_number':1007,'multiline':False]
['text':' If using profiling, a different function is needed to test different','line_number':1029,'multiline':False]
['text':' shapes, or we'll use a cached script.','line_number':1030,'multiline':False]
['text':' test that broadcasting random produces correct results','line_number':1046,'multiline':False]
['text':' Currently we don't pull constants into fusion groups, because in some','line_number':1112,'multiline':False]
['text':' cases it could remove the constant from the original graph and now our','line_number':1113,'multiline':False]
['text':' fusion group needs to return that constant for its other users.','line_number':1114,'multiline':False]
['text':' Instead of never pulling constants into the fusion group, we should just','line_number':1115,'multiline':False]
['text':' be more careful at how we rewrite its users.','line_number':1116,'multiline':False]
['text':' TODO: fix that and reenable the test.','line_number':1117,'multiline':False]
['text':' Check that the fused graph computes correct results when the scalar','line_number':1141,'multiline':False]
['text':' input changes.','line_number':1142,'multiline':False]
['text':' The TE fuser supports fusion of non-constant scalars','line_number':1148,'multiline':False]
['text':' test no op','line_number':1192,'multiline':False]
['text':' test not fusing non-const inputs','line_number':1201,'multiline':False]
['text':' test not fusing to_pinned inputs','line_number':1210,'multiline':False]
['text':' test across-device not supported','line_number':1220,'multiline':False]
['text':' reuses cast impl, smaller dtype set for faster test','line_number':1231,'multiline':False]
['text':' TODO: Add back when https://github.com/pytorch/pytorch/issues/55905 is closed','line_number':1250,'multiline':False]
['text':' use freezing to make non-Tensor args to `to` constant','line_number':1259,'multiline':False]
['text':' TODO: Add back when https://github.com/pytorch/pytorch/issues/55905 is closed','line_number':1272,'multiline':False]
['text':' torch.float16,','line_number':1273,'multiline':False]
['text':' TODO: Add back when https://github.com/pytorch/pytorch/issues/55905 is closed','line_number':1314,'multiline':False]
['text':' TODO: Add back when https://github.com/pytorch/pytorch/issues/55905 is closed','line_number':1337,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1346,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1347,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1348,'multiline':False]
['text':' TODO broken on int8 since','line_number':1398,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/85144','line_number':1399,'multiline':False]
['text':' RuntimeError: Invalid integral op_type: 23','line_number':1400,'multiline':False]
['text':' torch.ceil,','line_number':1401,'multiline':False]
['text':' torch.floor,','line_number':1402,'multiline':False]
['text':' torch.round,','line_number':1403,'multiline':False]
['text':' torch.trunc,','line_number':1404,'multiline':False]
['text':' TODO: broken on ROCm?','line_number':1406,'multiline':False]
['text':' F.hardshrink,','line_number':1407,'multiline':False]
['text':' TODO: broken since type promotion was added','line_number':1410,'multiline':False]
['text':' lambda x: torch.clamp(x, -10, 10),','line_number':1411,'multiline':False]
['text':' TODO: Add back when https://github.com/pytorch/pytorch/issues/55905 is closed','line_number':1416,'multiline':False]
['text':' todo - re-enable. fails with .500','line_number':1419,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1429,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1430,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1431,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1481,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1482,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1483,'multiline':False]
['text':' Interpret the graph','line_number':1526,'multiline':False]
['text':' If we can't interpret this IR, don't bother checking NNC.','line_number':1532,'multiline':False]
['text':' Compile the graph','line_number':1535,'multiline':False]
['text':' Run the graph','line_number':1541,'multiline':False]
['text':' No cuda support for ext calls yet','line_number':1557,'multiline':False]
['text':' Only 2D x 2D matrix multiply is supported. For non-supported sizes we','line_number':1570,'multiline':False]
['text':' still want to run results verification to test that we didn't','line_number':1571,'multiline':False]
['text':' accidentally fuse it, but we skip the 'is-fused' check.','line_number':1572,'multiline':False]
['text':' TODO: add support for other shape combinations and make this set empty:','line_number':1573,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1591,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1592,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1593,'multiline':False]
['text':' FIXME: Fails in IR Eval: torch.int64 and_ cpu','line_number':1611,'multiline':False]
['text':' Maybe we should split this into separate tests to speed it up by','line_number':1626,'multiline':False]
['text':' only using  scalar values relevant to particular ops','line_number':1627,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1637,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1638,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1639,'multiline':False]
['text':' Maybe we should split this into separate tests to speed it up by','line_number':1660,'multiline':False]
['text':' only using  scalar values relevant to particular ops','line_number':1661,'multiline':False]
['text':' skip 0','line_number':1662,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1671,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1672,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1673,'multiline':False]
['text':' FIXME: 'pow' fails with dtype=torch.float16/device=cuda/scalar=0','line_number':1688,'multiline':False]
['text':' torch.float16,','line_number':1689,'multiline':False]
['text':' torch.bool intentionally not included','line_number':1692,'multiline':False]
['text':' Maybe we should split this into separate tests to speed it up by','line_number':1697,'multiline':False]
['text':' only using  scalar values relevant to particular ops','line_number':1698,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1708,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1709,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1710,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1740,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1741,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1742,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1771,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1772,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1773,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1804,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1805,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1806,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1837,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1838,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1839,'multiline':False]
['text':' If eager mode doesn't support a dtype/op/device combo,','line_number':1869,'multiline':False]
['text':' neither does the fuser.  Catch everything to avoid needing to','line_number':1870,'multiline':False]
['text':' guess what errors might be thrown by eager.','line_number':1871,'multiline':False]
['text':' Test extracted from Super-SloMo: https://github.com/avinashpaliwal/Super-SloMo','line_number':1882,'multiline':False]
['text':' A few interesting things happen here: strided inputs of mixed size,','line_number':1883,'multiline':False]
['text':' plus outputs of mixed shapes.  The latter characteristic happened to','line_number':1884,'multiline':False]
['text':' expose a memory corruption bug due to not properly guarding the','line_number':1885,'multiline':False]
['text':' outputs.','line_number':1886,'multiline':False]
['text':' Putting a use of k in a never-executed conditional prevents','line_number':1918,'multiline':False]
['text':' profiling its type, which leaves it as "Tensor".  If we','line_number':1919,'multiline':False]
['text':' propagate Tensor back to the definition of k, we have to be','line_number':1920,'multiline':False]
['text':' careful not to create a fusion group containing it.','line_number':1921,'multiline':False]
['text':' TODO: re-enable fusion, which doesn't work right now. just test correctness for now','line_number':1964,'multiline':False]
['text':' self.assertAllFused(script.graph_for(a, b))','line_number':1965,'multiline':False]
['text':' self.assertAllFused(script.graph_for(a, s))','line_number':1967,'multiline':False]
['text':' self.assertAllFused(script.graph_for(s, b))','line_number':1969,'multiline':False]
['text':' CPU fuser doesn't support float16.','line_number':2005,'multiline':False]
['text':' TODO: not sure why not updated graph isn't reflected in last_optimized_graph','line_number':2127,'multiline':False]
['text':' Note: concat of ys and zs will have the same size for each','line_number':2141,'multiline':False]
['text':' pair, even though the individual ys and zs do not.','line_number':2142,'multiline':False]
['text':' TODO: once the adaptive_avg_pool2d is available in OpInfo DB, this','line_number':2164,'multiline':False]
['text':' test should be moved there','line_number':2165,'multiline':False]
['text':' Warm up with size=1 tensor; since the loop iterates once the','line_number':2189,'multiline':False]
['text':' profile data will be "burned in" assuming size=1, and then','line_number':2190,'multiline':False]
['text':' unrolled.','line_number':2191,'multiline':False]
['text':' Now when an input hits the unrolled path, it will produce an','line_number':2198,'multiline':False]
['text':' incorrectly-sized tensor, since size=1 has been burned in.','line_number':2199,'multiline':False]
['text':' TODO: Are `NaN`'s actually ok here or did this pass silently before, because `equal_nan=True` was the','line_number':2208,'multiline':False]
['text':'  default?','line_number':2209,'multiline':False]
['text':' We should see only one optimized kernel','line_number':2311,'multiline':False]
['text':' We had an issue where buildShapeExpressions would fail as show below:','line_number':2436,'multiline':False]
['text':'','line_number':2437,'multiline':False]
['text':' %1 : Tensor = Constant[..]  # not supported, we don't build this shape','line_number':2438,'multiline':False]
['text':' %2 : Tensor = Constant[..]  # not supported','line_number':2439,'multiline':False]
['text':' %3 : Tensor = aten::add(%1, %2)  # inputs not supported, we don't build shape','line_number':2440,'multiline':False]
['text':' ... = prim::ConstantChunk[..](%3)  # it forgets to check whether input shapes exist, and fails','line_number':2441,'multiline':False]
['text':' make sure that we are actually testing the right scenario','line_number':2457,'multiline':False]
['text':' make sure this doesn't error out','line_number':2465,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/75476','line_number':2472,'multiline':False]
['text':' If your OpInfo test causes this test to fail, add it here','line_number':2614,'multiline':False]
['text':' Purpose of this class is to allow super() calls.','line_number':2625,'multiline':False]
['text':' super() [with no arguments] fails, presumably because of how instantiate_device_type_tests works.','line_number':2626,'multiline':False]
['text':' super(TestNNCOpInfo, self) fails because TestNNCOpInfo gets deleted from global scope.','line_number':2627,'multiline':False]
['text':' super(JitCommonTestCase, self).fn() would skip JitCommonTestCase.fn() implementation','line_number':2628,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/35600','line_number':2737,'multiline':False]
['text':' each torch.jit.trace adds state to the _python_cu compilation unit','line_number':2738,'multiline':False]
['text':' since this test traces a lot of functions, out-of-memory can occur','line_number':2739,'multiline':False]
['text':' if the CU is not cleared.','line_number':2740,'multiline':False]
['text':' CPU fuser not currently used in fbcode','line_number':2743,'multiline':False]
['text':' Purpose of this class is to allow super() calls. (See TestNNCOpInfoParent)','line_number':2747,'multiline':False]
['text':' TODO: force LLVM. need to add it to asan, mac, windows builds + sandcastle','line_number':2760,'multiline':False]
['text':' torch._C._jit_set_te_must_use_llvm_cpu(True)','line_number':2761,'multiline':False]
['text':' Set the seed to 1. This tests the codepath through random','line_number':2776,'multiline':False]
['text':' transformation.','line_number':2777,'multiline':False]
['text':' Set it back to 0.','line_number':2792,'multiline':False]
