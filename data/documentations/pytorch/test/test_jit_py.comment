['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]
['text':' This is how we include tests located in test/jit/...','line_number':5,'multiline':False]
['text':' They are included here so that they are invoked when you call `test_jit.py`,','line_number':6,'multiline':False]
['text':' do not run these test files directly.','line_number':7,'multiline':False]
['text':' noqa: F401','line_number':8,'multiline':False]
['text':' noqa: F401','line_number':9,'multiline':False]
['text':' noqa: F401','line_number':10,'multiline':False]
['text':' noqa: F401','line_number':11,'multiline':False]
['text':' noqa: F401','line_number':12,'multiline':False]
['text':' noqa: F401','line_number':13,'multiline':False]
['text':' noqa: F401','line_number':14,'multiline':False]
['text':' noqa: F401','line_number':15,'multiline':False]
['text':' noqa: F401','line_number':16,'multiline':False]
['text':' noqa: F401','line_number':17,'multiline':False]
['text':' noqa: F401','line_number':18,'multiline':False]
['text':' noqa: F401','line_number':19,'multiline':False]
['text':' noqa: F401','line_number':20,'multiline':False]
['text':' noqa: F401','line_number':21,'multiline':False]
['text':' noqa: F401','line_number':22,'multiline':False]
['text':' noqa: F401','line_number':23,'multiline':False]
['text':' noqa: F401','line_number':24,'multiline':False]
['text':' noqa: F401','line_number':25,'multiline':False]
['text':' noqa: F401','line_number':26,'multiline':False]
['text':' noqa: F401','line_number':27,'multiline':False]
['text':' noqa: F401','line_number':28,'multiline':False]
['text':' noqa: F401','line_number':29,'multiline':False]
['text':' noqa: F401','line_number':30,'multiline':False]
['text':' noqa: F401','line_number':31,'multiline':False]
['text':' noqa: F401','line_number':32,'multiline':False]
['text':' noqa: F401','line_number':33,'multiline':False]
['text':' noqa: F401','line_number':34,'multiline':False]
['text':' noqa: F401','line_number':35,'multiline':False]
['text':' noqa: F401','line_number':36,'multiline':False]
['text':' noqa: F401','line_number':37,'multiline':False]
['text':' noqa: F401','line_number':38,'multiline':False]
['text':' noqa: F401','line_number':39,'multiline':False]
['text':' noqa: F401','line_number':40,'multiline':False]
['text':' noqa: F401  # noqa: F401','line_number':41,'multiline':False]
['text':' noqa: F401','line_number':42,'multiline':False]
['text':' noqa: F401','line_number':43,'multiline':False]
['text':' noqa: F401','line_number':44,'multiline':False]
['text':' noqa: F401','line_number':45,'multiline':False]
['text':' noqa: F401','line_number':46,'multiline':False]
['text':' noqa: F401','line_number':47,'multiline':False]
['text':' noqa: F401','line_number':48,'multiline':False]
['text':' noqa: F401','line_number':49,'multiline':False]
['text':' noqa: F401','line_number':50,'multiline':False]
['text':' noqa: F401','line_number':51,'multiline':False]
['text':' noqa: F401','line_number':52,'multiline':False]
['text':' noqa: F401','line_number':53,'multiline':False]
['text':' noqa: F401','line_number':54,'multiline':False]
['text':' noqa: F401','line_number':55,'multiline':False]
['text':' noqa: F401','line_number':56,'multiline':False]
['text':' noqa: F401','line_number':57,'multiline':False]
['text':' noqa: F401','line_number':58,'multiline':False]
['text':' noqa: F401','line_number':59,'multiline':False]
['text':' noqa: F401','line_number':60,'multiline':False]
['text':' noqa: F401','line_number':61,'multiline':False]
['text':' noqa: F401','line_number':62,'multiline':False]
['text':' noqa: F401','line_number':63,'multiline':False]
['text':' noqa: F401','line_number':64,'multiline':False]
['text':' noqa: F401','line_number':65,'multiline':False]
['text':' noqa: F401','line_number':66,'multiline':False]
['text':' noqa: F401','line_number':67,'multiline':False]
['text':' noqa: F401','line_number':68,'multiline':False]
['text':' noqa: F401','line_number':69,'multiline':False]
['text':' noqa: F401','line_number':70,'multiline':False]
['text':' noqa: F401','line_number':71,'multiline':False]
['text':' noqa: F401','line_number':72,'multiline':False]
['text':' noqa: F401','line_number':73,'multiline':False]
['text':' noqa: F401','line_number':74,'multiline':False]
['text':' noqa: F401','line_number':75,'multiline':False]
['text':' noqa: F401','line_number':76,'multiline':False]
['text':' noqa: F401','line_number':77,'multiline':False]
['text':' noqa: F401','line_number':78,'multiline':False]
['text':' Torch','line_number':80,'multiline':False]
['text':' noqa: F401','line_number':84,'multiline':False]
['text':' Testing utils','line_number':95,'multiline':False]
['text':' For testing truediv in python 2','line_number':115,'multiline':False]
['text':' Standard library','line_number':119,'multiline':False]
['text':' TODO: setting false on test itself is not working','line_number':156,'multiline':False]
['text':' these tests are disabled because BailOut nodes','line_number':167,'multiline':False]
['text':' inserted by ProfilingExecutor interfere with','line_number':168,'multiline':False]
['text':' subgraph slicing of Differentiable Graphs','line_number':169,'multiline':False]
['text':' functional','line_number':171,'multiline':False]
['text':' AutogradJitGenerated','line_number':184,'multiline':False]
['text':' TODO: enable TE in PE when all tests are fixed','line_number':205,'multiline':False]
['text':' Code reference: https://github.com/pytorch/translate/blob/master/pytorch_translate/rnn_cell.py#L27:44','line_number':241,'multiline':False]
['text':' Section 2.1 in https://arxiv.org/pdf/1606.06630.pdf','line_number':245,'multiline':False]
['text':' Same as LSTMCell after this point','line_number':247,'multiline':False]
['text':' Just to allocate weights with correct sizes','line_number':264,'multiline':False]
['text':' Note: for Python 2 the order seems to be unstable','line_number':314,'multiline':False]
['text':' Running JIT passes requires that we own the graph (with a shared_ptr).','line_number':327,'multiline':False]
['text':' The debug state struct does not own its graph so we make a copy of it.','line_number':328,'multiline':False]
['text':' helper function to get sum of List[Tensor]','line_number':332,'multiline':False]
['text':' has to be at top level or Pickle complains','line_number':340,'multiline':False]
['text':' Resetting','line_number':366,'multiline':False]
['text':' expecting to see other_fn TS function call','line_number':390,'multiline':False]
['text':' with cpu time >= mul cpu time and','line_number':391,'multiline':False]
['text':' a forked other_fn','line_number':392,'multiline':False]
['text':' a small tensor in the first 4GB','line_number':418,'multiline':False]
['text':' a large tensor in the first 4GB that ends outside of it','line_number':420,'multiline':False]
['text':' a small tensor in >4GB space','line_number':422,'multiline':False]
['text':' s large tensor in the > 4GB space','line_number':424,'multiline':False]
['text':' This test asserts that the serialization archive includes a `constants.pkl`','line_number':439,'multiline':False]
['text':' file. This file is used by `torch.load` to determine whether a zip file','line_number':440,'multiline':False]
['text':' is a normal eager-mode serialization zip or a jit serialization zip. If','line_number':441,'multiline':False]
['text':' you are deleting `constants.pkl`, make sure to update `torch.serialization.load`','line_number':442,'multiline':False]
['text':' so it is still able to figure out which is which.','line_number':443,'multiline':False]
['text':' main purpose is checking map_location works','line_number':473,'multiline':False]
['text':' restore to the saved devices','line_number':500,'multiline':False]
['text':' restore all to cpu using string','line_number':507,'multiline':False]
['text':' restore all to first gpu using device','line_number':513,'multiline':False]
['text':' compute and compare the results','line_number':519,'multiline':False]
['text':' add, relu_','line_number':583,'multiline':False]
['text':' add_, relu_','line_number':612,'multiline':False]
['text':' Because in place add_ will overwrite a','line_number':617,'multiline':False]
['text':' Since _add_relu_ does inplace mutation ensure','line_number':632,'multiline':False]
['text':' a_copy is modified','line_number':633,'multiline':False]
['text':' add_out, relu_','line_number':650,'multiline':False]
['text':' Because in place add_ will overwrite a','line_number':655,'multiline':False]
['text':' Since _add_relu_ with out=a does inplace mutation ensure','line_number':670,'multiline':False]
['text':' a_copy is modified','line_number':671,'multiline':False]
['text':' if result == 2 we will trigger a bailout and','line_number':690,'multiline':False]
['text':' the unprofiled graph should return the correct result','line_number':691,'multiline':False]
['text':' x.dtype, TODO: dtype long -> instance conversion','line_number':785,'multiline':False]
['text':' x.layout TODO: layout long -> instance conversion','line_number':796,'multiline':False]
['text':' Conv','line_number':886,'multiline':False]
['text':' ConvTransposed','line_number':890,'multiline':False]
['text':' shouldn't throw','line_number':913,'multiline':False]
['text':' after flushing there shouldn't be','line_number':916,'multiline':False]
['text':' no opt plan','line_number':917,'multiline':False]
['text':' after flushing there shouldn't be','line_number':929,'multiline':False]
['text':' no opt plan','line_number':930,'multiline':False]
['text':' ensure jit was able to compile','line_number':966,'multiline':False]
['text':' ensure jit was able to compile','line_number':983,'multiline':False]
['text':' XXX: Unfortunately ScriptModule won't simply become Module now,','line_number':999,'multiline':False]
['text':' because that requires disabling the JIT at startup time, which','line_number':1000,'multiline':False]
['text':' we can't do in here.','line_number':1001,'multiline':False]
['text':' We need to or those two conditions to make it work with all versions of Python','line_number':1002,'multiline':False]
['text':' test batchnorm and dropout train/eval','line_number':1030,'multiline':False]
['text':' 1D','line_number':1088,'multiline':False]
['text':' 2D','line_number':1089,'multiline':False]
['text':' 3D','line_number':1090,'multiline':False]
['text':' type: (Tensor, Tensor) -> List[Optional[Tensor]]','line_number':1123,'multiline':False]
['text':' type: (Tensor, Tensor) -> List[Optional[Tensor]]','line_number':1128,'multiline':False]
['text':' type: (Tensor, Tensor) -> List[Optional[Tensor]]','line_number':1134,'multiline':False]
['text':' type: (Tensor, Tensor) -> None','line_number':1139,'multiline':False]
['text':' type: (Tensor) -> None','line_number':1164,'multiline':False]
['text':' type: (Tensor) -> None','line_number':1171,'multiline':False]
['text':' type: (Tensor) -> None','line_number':1177,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tensor','line_number':1210,'multiline':False]
['text':' TODO: adapt this test to check that GraphExecutor treats them differently','line_number':1240,'multiline':False]
['text':' type: (int) -> Tuple[int, int]','line_number':1326,'multiline':False]
['text':' non-aliasing types can be CSEd','line_number':1329,'multiline':False]
['text':' mul(mul(mul(mul(x,y),z),x),y) --> mul(mul(mulmul(x,y,z), x), y) -->','line_number':1357,'multiline':False]
['text':' --> mulmul(mulmul(x,y,z), x, y)','line_number':1358,'multiline':False]
['text':' Check that overlapping matches are handled correctly','line_number':1380,'multiline':False]
['text':' mul(mul(mul(x,y),z),x) --> mul(mulmul(x,y,z), x)','line_number':1381,'multiline':False]
['text':' Check add(mul(x,y),z) --> muladd(x,y,z) replacement','line_number':1402,'multiline':False]
['text':' Check add(mul(x,y),z) --> sub(add(x,y),z) replacement','line_number':1424,'multiline':False]
['text':' Check mul(x,y) --> x replacement','line_number':1448,'multiline':False]
['text':' Check match::module behavior','line_number':1469,'multiline':False]
['text':' Check source range preservation for two node transforms add -> my_add','line_number':1532,'multiline':False]
['text':' Check source range preservation for add-add -> double_add transform','line_number':1561,'multiline':False]
['text':' fuse nodes','line_number':1562,'multiline':False]
['text':' Check source range preservation for mul -> add + add transform','line_number':1588,'multiline':False]
['text':' split node','line_number':1589,'multiline':False]
['text':' Check lack of source range preservation for mul-mul-> double_mul transform','line_number':1619,'multiline':False]
['text':' TODO: update verify to work with GraphExecutors','line_number':1695,'multiline':False]
['text':' TODO: adapt to a GraphExecutor test','line_number':1709,'multiline':False]
['text':' To reduce confusion about expected behaviors:','line_number':1903,'multiline':False]
['text':'   requires_grad controls whether dropout is symbolically differentiated.','line_number':1904,'multiline':False]
['text':'   training controls whether bernoulli_ is called inside symbolic differentiation of dropout.','line_number':1905,'multiline':False]
['text':' * When requires_grad == training, the expected behaviors are obvious.','line_number':1906,'multiline':False]
['text':' * When requires_grad=True and training=False, bernoulli_ might still show up in the graph.','line_number':1907,'multiline':False]
['text':'   But it's in a branch that's not called. That's why we have separate checks for autograd','line_number':1908,'multiline':False]
['text':'   profiler to make sure it's not run.','line_number':1909,'multiline':False]
['text':' * When requires_grad=False and training=True, bernoulli_ must be run since it's the expected','line_number':1910,'multiline':False]
['text':'   behavior for the dropout layer in training mode. It's independent of whether graph requires','line_number':1911,'multiline':False]
['text':'   gradient. In fact bernoulli_ comes from autograd instead of autodiff in this case.','line_number':1912,'multiline':False]
['text':' See comments in test_dropout_module_requires_grad.','line_number':1941,'multiline':False]
['text':' Dropout AD is dispatched to _fused_dropout in CUDA case,','line_number':1952,'multiline':False]
['text':' which is not included in TestJitGeneratedFunctional','line_number':1953,'multiline':False]
['text':' TODO(#40882): previously we assert exact matches between eager and JIT result:','line_number':1972,'multiline':False]
['text':'  self.assertEqual(out, out_ref)','line_number':1973,'multiline':False]
['text':'  self.assertEqual(grad, grad_ref)','line_number':1974,'multiline':False]
['text':' This test was disabled during legacy -> profiling executor transition.','line_number':1975,'multiline':False]
['text':' Currently JIT fused results doesn't match eager result exactly due to some changes merged in between.','line_number':1976,'multiline':False]
['text':' We temporarily only check statstical difference but it should be reverted once the issue is fixed.','line_number':1977,'multiline':False]
['text':' h/t Chillee for this ambiguous case','line_number':1993,'multiline':False]
['text':' Test if pure python object can be hold as IValue and conversion','line_number':2071,'multiline':False]
['text':' between IValue and PyObject are correct','line_number':2072,'multiline':False]
['text':' test for numpy object','line_number':2073,'multiline':False]
['text':' test for function object','line_number':2078,'multiline':False]
['text':' test for memory management','line_number':2082,'multiline':False]
['text':' we need to ensure IValue correctly call incref/decref to avoid','line_number':2083,'multiline':False]
['text':' dangling behavior and potential memory leaks during conversions','line_number':2084,'multiline':False]
['text':' create a scope and do the conversion -> ivalue -> pyobject','line_number':2086,'multiline':False]
['text':' this func return a new pyobject that refcount + 1','line_number':2087,'multiline':False]
['text':' after the test_func_scope_helper_call, the refcount of','line_number':2098,'multiline':False]
['text':' test_input should be equal to the original refcount','line_number':2099,'multiline':False]
['text':' otherwise we get either dangling pointer or memory leak!','line_number':2100,'multiline':False]
['text':' type: (Tensor) -> bool','line_number':2144,'multiline':False]
['text':' type: (Tensor) -> bool','line_number':2186,'multiline':False]
['text':' type: (int) -> int','line_number':2210,'multiline':False]
['text':' type: () -> Optional[int]','line_number':2268,'multiline':False]
['text':' testing that 1 // 0 error is not thrownn','line_number':2295,'multiline':False]
['text':' checking y = a[4] does not error in constant propagation','line_number':2299,'multiline':False]
['text':' type: (bool)','line_number':2301,'multiline':False]
['text':' type: (bool)','line_number':2328,'multiline':False]
['text':' -> c0, c1','line_number':2347,'multiline':False]
['text':' -> c0','line_number':2348,'multiline':False]
['text':' -> c0','line_number':2349,'multiline':False]
['text':' -> c0, c1','line_number':2354,'multiline':False]
['text':' inlined','line_number':2357,'multiline':False]
['text':' dynamic','line_number':2358,'multiline':False]
['text':' set to 5','line_number':2359,'multiline':False]
['text':' type: (bool, int) -> int','line_number':2376,'multiline':False]
['text':' constant gets pooled','line_number':2397,'multiline':False]
['text':' type: (int) -> None','line_number':2403,'multiline':False]
['text':' TODO(gmagogsfm): Refactor this test to reduce complexity.','line_number':2419,'multiline':False]
['text':' constants: primitives: int, double, bool, str, lists of primitives,','line_number':2426,'multiline':False]
['text':' and tuples','line_number':2427,'multiline':False]
['text':' check_constant constructs the second dict with another Tensor','line_number':2462,'multiline':False]
['text':' which fails the comparison','line_number':2463,'multiline':False]
['text':' testing node hashing','line_number':2472,'multiline':False]
['text':' prim::None return adds one constant','line_number':2485,'multiline':False]
['text':' node hashing correctly working, no CSE occurs','line_number':2488,'multiline':False]
['text':' generate dicts with built-in types (excluding torch.Tensor)','line_number':2499,'multiline':False]
['text':' test that equal tuples and dicts correctly work with node hashing','line_number':2502,'multiline':False]
['text':' NB: We make sure to pass in a batch with a different max sequence','line_number':2574,'multiline':False]
['text':' length to ensure that the argument stashing for pad_packed works','line_number':2575,'multiline':False]
['text':' properly.','line_number':2576,'multiline':False]
['text':' FIXME: use 0 instead of a.','line_number':2632,'multiline':False]
['text':' c = 0','line_number':2633,'multiline':False]
['text':' type: (Optional[int]) -> Optional[int]','line_number':2783,'multiline':False]
['text':' type: (Tensor, float, int) -> Tensor','line_number':2791,'multiline':False]
['text':' noqa: T484','line_number':2799,'multiline':False]
['text':' type: (Tensor, float, int) -> Tensor','line_number':2800,'multiline':False]
['text':' type: (Dict[str, int]) -> Dict[str, int]','line_number':2805,'multiline':False]
['text':' type: (Tuple[int, List[Tensor]])','line_number':2824,'multiline':False]
['text':' noqa: B006','line_number':2828,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tensor','line_number':2946,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':2950,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':2955,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':2965,'multiline':False]
['text':' Issue #19351: python2 and python3 go through different paths.','line_number':2983,'multiline':False]
['text':' python2 returns '<module 'torch.ops' (built-in)>'','line_number':2984,'multiline':False]
['text':' python3 uses __file__ and return','line_number':2985,'multiline':False]
['text':' '<module 'torch.ops' from '/scratch/ailzhang/pytorch/torch/_ops.py'>'','line_number':2986,'multiline':False]
['text':' noqa: PIE804','line_number':3066,'multiline':False]
['text':' noqa: PIE804','line_number':3068,'multiline':False]
['text':' Tests that calling torch.jit.script repeated on function is allowed.','line_number':3074,'multiline':False]
['text':' sub','line_number':3110,'multiline':False]
['text':' add','line_number':3111,'multiline':False]
['text':' noqa: E704','line_number':3183,'multiline':False]
['text':' profile','line_number':3198,'multiline':False]
['text':' optimize','line_number':3200,'multiline':False]
['text':' functional dominated guard','line_number':3213,'multiline':False]
['text':' dominated guard of non-functional value','line_number':3231,'multiline':False]
['text':' there should still be a Bailout after disable_grad call','line_number':3272,'multiline':False]
['text':' noqa: B902','line_number':3309,'multiline':False]
['text':' noqa: B902','line_number':3310,'multiline':False]
['text':' type: () -> int','line_number':3311,'multiline':False]
['text':' type: (int) -> Tensor','line_number':3324,'multiline':False]
['text':' type: (int, int) -> Tuple[Tensor, List[int]]','line_number':3340,'multiline':False]
['text':' type: (int, int) -> Tuple[Tensor, List[int]]','line_number':3362,'multiline':False]
['text':' m has a constant attribute, but we can't','line_number':3436,'multiline':False]
['text':' assign to it','line_number':3437,'multiline':False]
['text':' we should be able to use self.foo as a float here','line_number':3473,'multiline':False]
['text':' see [local resolution in python]','line_number':3515,'multiline':False]
['text':' type: () -> MyTuple','line_number':3520,'multiline':False]
['text':' Only check compilation','line_number':3523,'multiline':False]
['text':' type: () -> MyTuple','line_number':3526,'multiline':False]
['text':' type: () -> MyTuple','line_number':3534,'multiline':False]
['text':' shouldn't throw a type error','line_number':3543,'multiline':False]
['text':' type: (Tensor) -> int','line_number':3551,'multiline':False]
['text':' invalid, but should be ignored','line_number':3552,'multiline':False]
['text':' Non-working example','line_number':3568,'multiline':False]
['text':' Working example','line_number':3577,'multiline':False]
['text':' test directly compiling function','line_number':3595,'multiline':False]
['text':' test compiling it within another function','line_number':3599,'multiline':False]
['text':' signed 64 bit max','line_number':3608,'multiline':False]
['text':' Python interprets this as inf','line_number':3664,'multiline':False]
['text':' checkScript doesn't work since assertEqual doesn't consider','line_number':3671,'multiline':False]
['text':' `inf` == `inf`','line_number':3672,'multiline':False]
['text':' type: (Device) -> Tuple[str, Optional[int]]','line_number':3681,'multiline':False]
['text':' cannot use because bar is not defined','line_number':3749,'multiline':False]
['text':'type: (Int) -> Int # noqa: E265','line_number':3849,'multiline':False]
['text':'type   : (Int) -> Int # noqa: E265','line_number':3856,'multiline':False]
['text':'     type: (Int) -> Int','line_number':3863,'multiline':False]
['text':' type: (Int, Int) -> Int','line_number':3894,'multiline':False]
['text':' type: (Int, Int) -> Int','line_number':3900,'multiline':False]
['text':' type: (Int, Int) -> Int','line_number':3905,'multiline':False]
['text':' type: (Int, Int) -> Int','line_number':3911,'multiline':False]
['text':' type: print("Hello") -> Tensor # noqa: F723','line_number':3918,'multiline':False]
['text':' This test generates random tree-like programs to fuzz test','line_number':3929,'multiline':False]
['text':' that the interpreter does not have a bug in its stack manipulation','line_number':3930,'multiline':False]
['text':' code. An assert in that code ensures individual operators are','line_number':3931,'multiline':False]
['text':' not reordered.','line_number':3932,'multiline':False]
['text':' module type creation is not deterministic, so we have to sort','line_number':4018,'multiline':False]
['text':' the result','line_number':4019,'multiline':False]
['text':' type: (List[int]) -> int','line_number':4074,'multiline':False]
['text':' type: (List[int]) -> int','line_number':4079,'multiline':False]
['text':' type: (int) -> int','line_number':4098,'multiline':False]
['text':' check double use of training','line_number':4103,'multiline':False]
['text':' type: (Tensor, List[int]) -> Tensor','line_number':4148,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':4158,'multiline':False]
['text':' use precise assert, we are checking floating point details','line_number':4198,'multiline':False]
['text':' test annotate none types','line_number':4221,'multiline':False]
['text':' misleading name to make sure we resolve by function','line_number':4229,'multiline':False]
['text':' type: (int)','line_number':4244,'multiline':False]
['text':' type: (SomeClass) -> bool','line_number':4249,'multiline':False]
['text':' type: (int)','line_number':4258,'multiline':False]
['text':' type: (SomeClass) -> bool','line_number':4263,'multiline':False]
['text':' type: (SomeClass) -> bool','line_number':4303,'multiline':False]
['text':' with self.assertRaisesRegex(RuntimeError, "")','line_number':4306,'multiline':False]
['text':' type: (SomeNonAddableClass) -> bool','line_number':4326,'multiline':False]
['text':' type: (int)','line_number':4342,'multiline':False]
['text':' type: (SomeClass) -> bool','line_number':4347,'multiline':False]
['text':' type: (int)','line_number':4356,'multiline':False]
['text':' type: (SomeClass) -> bool','line_number':4361,'multiline':False]
['text':' NB: not using getExportImportCopy because that takes a different','line_number':4427,'multiline':False]
['text':' code path that calls CompilationUnit._import rather than','line_number':4428,'multiline':False]
['text':' going through the full save/load pathway','line_number':4429,'multiline':False]
['text':' make long_arg_name[1] does not get reordered after the argmax','line_number':4637,'multiline':False]
['text':' because the compilation unit ingests python strings','line_number':4796,'multiline':False]
['text':' to use an escape sequence escape the backslash (\\n = \n)','line_number':4797,'multiline':False]
['text':' python wrapper around the script function is a different pointer,','line_number':4812,'multiline':False]
['text':' but the underlying script function graph is the same','line_number':4813,'multiline':False]
['text':' caching doesn't get tripped up by same qualname','line_number':4821,'multiline':False]
['text':' caching doesnt increase refcounts to function (holds weak reference)','line_number':4826,'multiline':False]
['text':' type: (int, float) -> float','line_number':4950,'multiline':False]
['text':' type: () -> float','line_number':4954,'multiline':False]
['text':' type: (int, int) -> float','line_number':4961,'multiline':False]
['text':' see that scalar exponent works with cuda base (#19253)','line_number':4984,'multiline':False]
['text':' type: (Tensor, float) -> Tensor','line_number':4989,'multiline':False]
['text':' type: (float, Tensor) -> Tensor','line_number':4997,'multiline':False]
['text':' Torchscript assumes type Tensor by default, so we need this explicit','line_number':5067,'multiline':False]
['text':' declaration.','line_number':5068,'multiline':False]
['text':' basic slices','line_number':5087,'multiline':False]
['text':' multi-dim: indexes','line_number':5100,'multiline':False]
['text':' multi-dim: mixed slicing and indexing','line_number':5106,'multiline':False]
['text':' zero-sized slices','line_number':5116,'multiline':False]
['text':' trivial expression usage','line_number':5120,'multiline':False]
['text':' None for new dimensions','line_number':5124,'multiline':False]
['text':' dynamic expression usage','line_number':5135,'multiline':False]
['text':' positive striding','line_number':5139,'multiline':False]
['text':' negative striding','line_number':5155,'multiline':False]
['text':' only step is specified','line_number':5165,'multiline':False]
['text':' striding strings','line_number':5181,'multiline':False]
['text':' type: (List[str]) -> List[int]','line_number':5233,'multiline':False]
['text':' Direct list iteration not supported','line_number':5235,'multiline':False]
['text':' type: (List[List[str]]) -> List[List[int]]','line_number':5243,'multiline':False]
['text':' Direct list iteration not supported','line_number':5245,'multiline':False]
['text':' type: (Optional[List[int]]) -> int','line_number':5256,'multiline':False]
['text':' type: (Optional[int], Optional[bool], Optional[Tensor]) -> Tuple[Optional[int], Optional[bool], Optional[Tensor]]','line_number':5276,'multiline':False]
['text':' type: (bool) -> None','line_number':5281,'multiline':False]
['text':' even though the c & d escape scope, we are still able','line_number':5302,'multiline':False]
['text':' pool them into one constant because they are the same object','line_number':5303,'multiline':False]
['text':' dont pool constants bc it would introduce observable alias relationship changing','line_number':5316,'multiline':False]
['text':' type: (float, float) -> float','line_number':5333,'multiline':False]
['text':' test that shape analysis is written correctly for sum with OptionalIntArrayRef[1] dim argument','line_number':5373,'multiline':False]
['text':' type: (Tensor, int) -> List[Tensor]','line_number':5456,'multiline':False]
['text':' type: (List[Tensor]) -> List[Tensor]','line_number':5478,'multiline':False]
['text':' NOTE: cannot optimize yet because broadcasts are not inserted before the fuser runs','line_number':5517,'multiline':False]
['text':' type: (Tensor, bool) -> float','line_number':5524,'multiline':False]
['text':' check prim::profile are inserted','line_number':5534,'multiline':False]
['text':' this call is optimized for','line_number':5537,'multiline':False]
['text':' the given shape of (2, 3)','line_number':5538,'multiline':False]
['text':' change shape to (3)','line_number':5540,'multiline':False]
['text':' so we go down a bailout path','line_number':5541,'multiline':False]
['text':' check prim::BailOuts are inserted','line_number':5543,'multiline':False]
['text':' this triggers all 3 bailouts','line_number':5546,'multiline':False]
['text':' this triggers 2 bailouts','line_number':5548,'multiline':False]
['text':' resize_ and resize_as resize the input tensor. because our shape analysis','line_number':5591,'multiline':False]
['text':' is flow invariant, we set any Tensor that can alias a resized Tensor','line_number':5592,'multiline':False]
['text':' to the base Tensor Type, without size information.','line_number':5593,'multiline':False]
['text':' testing that value which is an input of a graph gets handled','line_number':5595,'multiline':False]
['text':' for i in range(10):','line_number':5615,'multiline':False]
['text':' first input and output of b.resize_ is b','line_number':5624,'multiline':False]
['text':' correctly propagates to b alias set','line_number':5628,'multiline':False]
['text':' x doesn't alias a resized op so it shouldn't be set to base Tensor type','line_number':5648,'multiline':False]
['text':' return is resized','line_number':5650,'multiline':False]
['text':' type: (Tensor, Tensor, int) -> Tensor','line_number':5674,'multiline':False]
['text':' x requires grad, y does not','line_number':5679,'multiline':False]
['text':' testing that requires grad analysis correctly exits, with its input','line_number':5680,'multiline':False]
['text':' to the loop (x) requiring grad and its output to the loop not requiring grad','line_number':5681,'multiline':False]
['text':' and the output of the node conservatively setting grad to true','line_number':5682,'multiline':False]
['text':' TODO: simplify this test as it's very sensitive','line_number':5694,'multiline':False]
['text':' the optimized graph will have 3 loops','line_number':5695,'multiline':False]
['text':' the original loop is peeled','line_number':5696,'multiline':False]
['text':' peeled loop also gets unrolled','line_number':5697,'multiline':False]
['text':' 'min', 'max' were previously tested but are now replaced with ternary expressions','line_number':5838,'multiline':False]
['text':' instead of fmin() and fmax()','line_number':5839,'multiline':False]
['text':' type: (int) -> str','line_number':5893,'multiline':False]
['text':' type: (str) -> int','line_number':5901,'multiline':False]
['text':' type: (int) -> int','line_number':5999,'multiline':False]
['text':' type: (int, int) -> int','line_number':6012,'multiline':False]
['text':' type: (int, int) -> int','line_number':6027,'multiline':False]
['text':' LHS, RHS both alwaysNone, dispatch always_none_branch','line_number':6054,'multiline':False]
['text':' only emit one prim::Constant','line_number':6055,'multiline':False]
['text':' type: (Optional[Tensor]) -> int','line_number':6067,'multiline':False]
['text':' LHS maybeNone: emit normal if stmt that contains 3 constants','line_number':6068,'multiline':False]
['text':' type: (Optional[Tensor]) -> int','line_number':6080,'multiline':False]
['text':' RHS maybeNone, emit normal if stmt that contains 3 constants','line_number':6081,'multiline':False]
['text':' LHS neverNone, RHS alwaysNone dispatch never_none_branch','line_number':6093,'multiline':False]
['text':' only emit one prim::Constant','line_number':6094,'multiline':False]
['text':' LHS alwaysNone, RHS neverNone dispatch never_none_branch','line_number':6106,'multiline':False]
['text':' only emit one prim::Constant','line_number':6107,'multiline':False]
['text':' noqa: W605','line_number':6169,'multiline':False]
['text':' type: (int) -> int','line_number':6175,'multiline':False]
['text':' type: (float) -> int','line_number':6185,'multiline':False]
['text':' noqa: W605','line_number':6194,'multiline':False]
['text':' noqa: F634','line_number':6198,'multiline':False]
['text':' type: (Optional[int]) -> int','line_number':6225,'multiline':False]
['text':' type: (Optional[int]) -> None','line_number':6231,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6237,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6243,'multiline':False]
['text':' type: (Optional[int]) -> None','line_number':6251,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> int','line_number':6257,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6266,'multiline':False]
['text':' backwards compatibility','line_number':6272,'multiline':False]
['text':' type: (Optional[int]) -> int','line_number':6275,'multiline':False]
['text':' noqa: T484','line_number':6280,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6285,'multiline':False]
['text':' noqa: T484','line_number':6287,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6292,'multiline':False]
['text':' noqa: T484','line_number':6296,'multiline':False]
['text':' type: (Optional[int]) -> None','line_number':6301,'multiline':False]
['text':' noqa: T484','line_number':6304,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> None','line_number':6309,'multiline':False]
['text':' noqa: T484','line_number':6312,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> int','line_number':6317,'multiline':False]
['text':' type: (Optional[Tensor], int) -> int','line_number':6329,'multiline':False]
['text':' check if input is disconnected','line_number':6339,'multiline':False]
['text':' type: (Optional[Tensor], Tensor, bool) -> Tensor','line_number':6350,'multiline':False]
['text':' type: (Optional[List[int]], int) -> int','line_number':6370,'multiline':False]
['text':' check if input is disconnected','line_number':6383,'multiline':False]
['text':' type: (Optional[List[int]], List[int], bool) -> List[int]','line_number':6394,'multiline':False]
['text':' type: (bool)','line_number':6413,'multiline':False]
['text':' type: (bool, Tuple[Optional[List[Tensor]]])','line_number':6422,'multiline':False]
['text':' type: (int, int) -> int','line_number':6440,'multiline':False]
['text':' type: (int, int) -> Tuple[int, int]','line_number':6456,'multiline':False]
['text':' type: (float, float) -> Tuple[float, float]','line_number':6460,'multiline':False]
['text':' type: (int, float) -> Tuple[float, float]','line_number':6464,'multiline':False]
['text':' type: (float, int) -> Tuple[float, float]','line_number':6468,'multiline':False]
['text':' We can't use assertEqual because of a couple of differences:','line_number':6553,'multiline':False]
['text':' 1. nan == nan should return true','line_number':6554,'multiline':False]
['text':' 2. When python functions throw an exception, we usually want to silently ignore them.','line_number':6555,'multiline':False]
['text':' (ie: We want to return `nan` for math.sqrt(-5))','line_number':6556,'multiline':False]
['text':' math.pow() behavior has changed in 3.11, see https://docs.python.org/3/library/math.html#math.pow','line_number':6567,'multiline':False]
['text':' type: (int, int) -> int','line_number':6600,'multiline':False]
['text':' type: (Optional[Tensor]) -> Optional[Tensor]','line_number':6618,'multiline':False]
['text':' test undefined tensor None as default param','line_number':6624,'multiline':False]
['text':' type: (Optional[Tensor]) -> Tensor','line_number':6626,'multiline':False]
['text':' test typical None as default param','line_number':6639,'multiline':False]
['text':' type: (Optional[float]) -> float','line_number':6641,'multiline':False]
['text':' Test for True branch when condition variable','line_number':6709,'multiline':False]
['text':' is annotated as Final','line_number':6710,'multiline':False]
['text':' Test for True branch when condition variable','line_number':6721,'multiline':False]
['text':' is annotated as Final','line_number':6722,'multiline':False]
['text':' XXX: this should stay quiet in stay propagation and only fail in the interpreter','line_number':6870,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tensor','line_number':6921,'multiline':False]
['text':' type: (IFace) -> IFace','line_number':6928,'multiline':False]
['text':' type: (int) -> int','line_number':6942,'multiline':False]
['text':' TODO we need to test exponent operator '**' and bitwise not','line_number':6946,'multiline':False]
['text':' operator '~' once they are properly supported.','line_number':6947,'multiline':False]
['text':' type: (bool, bool) -> Tuple[bool, bool, bool]','line_number':6967,'multiline':False]
['text':' type: (Tensor, int) -> Tuple[Tensor, Tensor]','line_number':6977,'multiline':False]
['text':' type: (List[bool]) -> bool','line_number':7002,'multiline':False]
['text':' type: (List[int]) -> bool','line_number':7013,'multiline':False]
['text':' type: (List[float]) -> bool','line_number':7020,'multiline':False]
['text':' test Scalar overloads','line_number':7055,'multiline':False]
['text':' type: (float) -> float','line_number':7076,'multiline':False]
['text':' type: (int) -> int','line_number':7080,'multiline':False]
['text':' Testing bitwise shorthand aug assignment','line_number':7101,'multiline':False]
['text':' int -> int','line_number':7197,'multiline':False]
['text':' float -> float','line_number':7201,'multiline':False]
['text':' FIXME: things like 2 / long_tensor are not implemented correctly','line_number':7263,'multiline':False]
['text':' Look in torch/_tensor.py to see how pytorch implements it.','line_number':7264,'multiline':False]
['text':' % operator does not take: const % tensor','line_number':7268,'multiline':False]
['text':' torchscript returns int tensor, python returns float tensor','line_number':7317,'multiline':False]
['text':' both should have completed shapes','line_number':7350,'multiline':False]
['text':' first should have type set second should not','line_number':7355,'multiline':False]
['text':' type: (bool) -> Tuple[Tensor, Tensor, Tensor]','line_number':7376,'multiline':False]
['text':' tensor from empty list is type float in python and annotated type in torchscript','line_number':7429,'multiline':False]
['text':' Skip unsigned tensor initializaton for signed values on 3.10','line_number':7432,'multiline':False]
['text':' equality NYI for half tensor','line_number':7441,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tensor, Tensor, Tensor]','line_number':7449,'multiline':False]
['text':' need to clear function cache so we re run shape analysis','line_number':7460,'multiline':False]
['text':' requires_grad property shouldn't be accidentally set by shape analysis','line_number':7483,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':7488,'multiline':False]
['text':' This comment is at the wrong indent','line_number':7513,'multiline':False]
['text':' adapted from test in test_torch','line_number':7518,'multiline':False]
['text':' Test AD: aten::to(Tensor self, int dtype, bool non_blocking, bool copy) -> Tensor','line_number':7585,'multiline':False]
['text':' Test AD: aten::to(Tensor self, Device? device, int? dtype, bool non_blocking, bool copy) -> Tensor','line_number':7595,'multiline':False]
['text':' Test AD: aten::to(Tensor self, Tensor other, bool non_blocking, bool copy) -> Tensor','line_number':7604,'multiline':False]
['text':' test not operator in python','line_number':7622,'multiline':False]
['text':' TODO: add more tests when bool conversions ready','line_number':7623,'multiline':False]
['text':' test is and is not operator in python','line_number':7630,'multiline':False]
['text':' do literals product to try any types combinations','line_number':7651,'multiline':False]
['text':' type: (Optional[int]) -> int','line_number':7658,'multiline':False]
['text':' type: (Optional[int], Optional[int]) -> int','line_number':7668,'multiline':False]
['text':' type: (Any, Any) -> int','line_number':7679,'multiline':False]
['text':' type: (Any) -> Tensor','line_number':7689,'multiline':False]
['text':' any binding will infer the type, if it infers','line_number':7699,'multiline':False]
['text':' a specialized tensor type `x` Dict type will fail isinstance check','line_number':7700,'multiline':False]
['text':' test isinstance operator for static type checking','line_number':7711,'multiline':False]
['text':' do zipping to try different types','line_number':7735,'multiline':False]
['text':' test optional isinstance check','line_number':7739,'multiline':False]
['text':' type: (Optional[int]) -> bool','line_number':7742,'multiline':False]
['text':' testing that the "did exit" transform values are not loop block','line_number':7819,'multiline':False]
['text':' outputs (and thus not affecting one loop from another)','line_number':7820,'multiline':False]
['text':' type: (int)','line_number':7827,'multiline':False]
['text':' type: (int)','line_number':7884,'multiline':False]
['text':' type: (int)','line_number':7893,'multiline':False]
['text':' type: (int)','line_number':7929,'multiline':False]
['text':' type: (int)','line_number':7942,'multiline':False]
['text':' type: (int)','line_number':7962,'multiline':False]
['text':' type: (int, int)','line_number':7978,'multiline':False]
['text':' type: (int)','line_number':8004,'multiline':False]
['text':' type: (int)','line_number':8017,'multiline':False]
['text':' type: (int)','line_number':8034,'multiline':False]
['text':' use of k tests the pathway where we have to insert unitialized','line_number':8047,'multiline':False]
['text':' test submodule','line_number':8227,'multiline':False]
['text':' test parameters','line_number':8230,'multiline':False]
['text':' test defining a method from a string','line_number':8233,'multiline':False]
['text':' test script methods','line_number':8238,'multiline':False]
['text':' test use of parameter','line_number':8242,'multiline':False]
['text':' not checking data, just dtype, size etc','line_number':8293,'multiline':False]
['text':' sorts last use to the end','line_number':8328,'multiline':False]
['text':' find the last output, then all subsequent uses','line_number':8385,'multiline':False]
['text':' skip past node body','line_number':8387,'multiline':False]
['text':' the canonical order is the same order as the first use','line_number':8394,'multiline':False]
['text':' appears in text','line_number':8395,'multiline':False]
['text':' type: (bool) -> Tuple[int, int]','line_number':8402,'multiline':False]
['text':' type: (bool) -> Tuple[int, int]','line_number':8419,'multiline':False]
['text':' type: (bool, int) -> (None)','line_number':8437,'multiline':False]
['text':' type: (int) -> (None)','line_number':8453,'multiline':False]
['text':' c is used, then unused should be ordered by alphabetical','line_number':8464,'multiline':False]
['text':' checks with distinct range matchings','line_number':8550,'multiline':False]
['text':' special case representing wrapped number','line_number':8578,'multiline':False]
['text':' a couple of ops aren't implemented for non-floating types','line_number':8597,'multiline':False]
['text':' uncomment for debugging a failed test:','line_number':8602,'multiline':False]
['text':' print("testing {}".format(return_line))','line_number':8603,'multiline':False]
['text':' subtract not supported for bool','line_number':8663,'multiline':False]
['text':' div is not implemented correctly for mixed-type or int params','line_number':8666,'multiline':False]
['text':' uncomment for debugging a failed test:','line_number':8672,'multiline':False]
['text':' print("testing {}".format(return_line))','line_number':8673,'multiline':False]
['text':' use dim=-1 to represent a python/jit scalar.','line_number':8682,'multiline':False]
['text':' jit only supports int/float scalars.','line_number':8685,'multiline':False]
['text':' check that we can change python attributes','line_number':8794,'multiline':False]
['text':' and that those changes are picked up in script methods','line_number':8795,'multiline':False]
['text':' verify that we capture human understandable class name','line_number':8949,'multiline':False]
['text':' TODO: add param mutation test case after JIT support it','line_number':8954,'multiline':False]
['text':' This is a flag so we can test that the pack method was called','line_number':9087,'multiline':False]
['text':' This is a flag so we can test that the unpack method was called','line_number':9089,'multiline':False]
['text':' Test save path','line_number':9111,'multiline':False]
['text':' ensure pack was called before serialization','line_number':9115,'multiline':False]
['text':' ensure unpack was called after serialization so as to leave the module in an initialized state','line_number':9117,'multiline':False]
['text':' Test load paths','line_number':9122,'multiline':False]
['text':' type: (Tensor, int) -> Tensor','line_number':9130,'multiline':False]
['text':' type: (Tensor, int) -> Tensor','line_number':9137,'multiline':False]
['text':' inlining optimizations should have cleaned up linear if statement','line_number':9261,'multiline':False]
['text':' since forward() makes three aliases to the input `rep` before passing','line_number':9684,'multiline':False]
['text':' it to StarTestSumAndReturnThree(), in-place behavior will be different','line_number':9685,'multiline':False]
['text':' than the above out of place.','line_number':9686,'multiline':False]
['text':' set padding value so we can test equivalence','line_number':9735,'multiline':False]
['text':' type: (Tensor)','line_number':9752,'multiline':False]
['text':' lengths is flipped, so is output','line_number':9759,'multiline':False]
['text':' type: (List[Tensor], bool, float) -> Tensor','line_number':9771,'multiline':False]
['text':' type: (List[Tensor], bool) -> Tensor','line_number':9775,'multiline':False]
['text':' useless comment that is not indented correctly  # noqa: E115','line_number':9827,'multiline':False]
['text':' should compile without an error','line_number':9833,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':9885,'multiline':False]
['text':' test copy','line_number':9890,'multiline':False]
['text':' We want to support the use case of attaching a different `forward` method','line_number':9895,'multiline':False]
['text':' Generic forward dispatch','line_number':9898,'multiline':False]
['text':' Replace the forward method','line_number':9904,'multiline':False]
['text':' Should not use this forward method','line_number':9911,'multiline':False]
['text':' type: (Tensor, int, Tuple[Tensor, int]) -> Tuple[int, Tensor]','line_number':9923,'multiline':False]
['text':' type: () -> Tensor','line_number':9939,'multiline':False]
['text':' noqa: T484','line_number':9940,'multiline':False]
['text':' testing that different length lists don't throw error on cat in shape prop','line_number':9996,'multiline':False]
['text':' testing dynamic is appropriately set for y and z','line_number':10011,'multiline':False]
['text':' allowing a unififed int?[] would cause a runtime error b/c','line_number':10030,'multiline':False]
['text':' the index operation expects int?[] to be a generic list,','line_number':10031,'multiline':False]
['text':' but in the true branch the IValue will be a int list','line_number':10032,'multiline':False]
['text':' type: (bool) -> Optional[int]','line_number':10036,'multiline':False]
['text':' noqa: T484','line_number':10040,'multiline':False]
['text':' type: (bool) -> Tuple[Tensor, List[Tensor]]','line_number':10045,'multiline':False]
['text':' testing that tensor type of lists is unified','line_number':10056,'multiline':False]
['text':' noqa: E731','line_number':10070,'multiline':False]
['text':' specialized tensor in graph','line_number':10078,'multiline':False]
['text':' when tensor is loaded as constant it isnt specialized','line_number':10083,'multiline':False]
['text':' type: (float, BroadcastingList3[float]) -> List[float]','line_number':10091,'multiline':False]
['text':' type: (BroadcastingList3[int]) -> List[int]','line_number':10103,'multiline':False]
['text':' noqa: T484','line_number':10114,'multiline':False]
['text':' type: (BroadcastingListx[int]) -> List[int]  # noqa: T484','line_number':10116,'multiline':False]
['text':' using CU so that flake8 error on int[2] is not raised (noqa not working)','line_number':10119,'multiline':False]
['text':' Testing that the builtin call to embedding_renorm_ correctly throws','line_number':10148,'multiline':False]
['text':' Error when .backward() is called on its input','line_number':10149,'multiline':False]
['text':' type: (Tensor, Tensor, float) -> None','line_number':10156,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor]','line_number':10176,'multiline':False]
['text':' type: (str, Tuple[str, str]) -> Tuple[str, int, str, str]','line_number':10198,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':10272,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tuple[Tensor, Tensor]','line_number':10277,'multiline':False]
['text':' type: (int, Tensor) -> Tensor','line_number':10366,'multiline':False]
['text':' test submodule','line_number':10418,'multiline':False]
['text':' check to make sure the storage wasn't resized','line_number':10503,'multiline':False]
['text':' check to make sure the storage wasn't resized','line_number':10522,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10592,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10600,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10608,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10615,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10622,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10630,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10638,'multiline':False]
['text':' type: (Tensor) -> List[int]','line_number':10645,'multiline':False]
['text':' on the first iteration of the loop it appears that','line_number':10676,'multiline':False]
['text':' c should have a expand to the size of b','line_number':10677,'multiline':False]
['text':' but on the second+ iterations, there is no broadcast and the','line_number':10678,'multiline':False]
['text':' sizes are different.','line_number':10679,'multiline':False]
['text':' previously this would cause the compiler to (1) enter an infinite','line_number':10680,'multiline':False]
['text':' loop trying to compute the shape, and (2) insert invalid','line_number':10681,'multiline':False]
['text':' broadcasts.','line_number':10682,'multiline':False]
['text':' this test ensure we don't regress on these issues','line_number':10683,'multiline':False]
['text':' default arg dim','line_number':10755,'multiline':False]
['text':' keywords out of order','line_number':10761,'multiline':False]
['text':' mix const/non-const attributes','line_number':10767,'multiline':False]
['text':' Initialize param and input values','line_number':10880,'multiline':False]
['text':' Clone trainable params','line_number':10885,'multiline':False]
['text':' Test symbolic differentiation','line_number':10891,'multiline':False]
['text':' clone params for autograd reference','line_number':10895,'multiline':False]
['text':' Initialize param and input values','line_number':10923,'multiline':False]
['text':' Test symbolic differentiation','line_number':10944,'multiline':False]
['text':' Run Forward and Backward thrice to trigger autodiff graph','line_number':10945,'multiline':False]
['text':' run jitted module','line_number':10956,'multiline':False]
['text':' reference computation','line_number':10959,'multiline':False]
['text':' Testing shape analysis correctly setting type','line_number':11006,'multiline':False]
['text':' script module','line_number':11053,'multiline':False]
['text':' profiling/optimization runs','line_number':11063,'multiline':False]
['text':' Testing shape analysis correctly setting type','line_number':11102,'multiline':False]
['text':' fn = self.checkScript(test_rand, ())','line_number':11112,'multiline':False]
['text':' out = fn()','line_number':11113,'multiline':False]
['text':' self.assertEqual(out.dtype, torch.float)','line_number':11114,'multiline':False]
['text':' TupleConstruct output type is not correct here.','line_number':11141,'multiline':False]
['text':' After the pass, the output type should've been updated.','line_number':11150,'multiline':False]
['text':' TODO(henrytu): Add test for RefineTypes for NamedTuple when it's supported by IR parser.','line_number':11153,'multiline':False]
['text':' Need to inline otherwise we see instances of Function.','line_number':11179,'multiline':False]
['text':' We would have to use torch.linear/dropout to get around it otherwise.','line_number':11180,'multiline':False]
['text':' entirely unrolled','line_number':11256,'multiline':False]
['text':' inner loop with 8 subs followed by loop epilogue','line_number':11273,'multiline':False]
['text':' noqa: B902','line_number':11394,'multiline':False]
['text':' noqa: B902','line_number':11395,'multiline':False]
['text':' type: (int, int, int) -> int','line_number':11463,'multiline':False]
['text':' zipping over two','line_number':11558,'multiline':False]
['text':' variable length, modulelist','line_number':11622,'multiline':False]
['text':' modulelist, variable length','line_number':11632,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11644,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11654,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11664,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11674,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11686,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11696,'multiline':False]
['text':' defaults to List of Tensor for empty modulelist','line_number':11735,'multiline':False]
['text':' noqa: C416','line_number':11739,'multiline':False]
['text':' i in comprehension doesn't write to function scope','line_number':11747,'multiline':False]
['text':' noqa: C416','line_number':11750,'multiline':False]
['text':' type: (List[int], List[int]) -> int','line_number':11757,'multiline':False]
['text':' type: (List[int], List[int], List[int]) -> int','line_number':11767,'multiline':False]
['text':' type: (List[int], List[int], List[int]) -> int','line_number':11777,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11789,'multiline':False]
['text':' type: (List[int], List[int], List[int]) -> int','line_number':11799,'multiline':False]
['text':' type: (List[int], List[int]) -> int','line_number':11808,'multiline':False]
['text':' type: (List[int], List[int]) -> int','line_number':11818,'multiline':False]
['text':' type: (float) -> int','line_number':11852,'multiline':False]
['text':' to avoid defining sum_list in multiple tests','line_number':11870,'multiline':False]
['text':' type: (List[int]) -> int','line_number':11873,'multiline':False]
['text':' type: () -> int','line_number':11894,'multiline':False]
['text':' type: (int) -> int','line_number':11908,'multiline':False]
['text':' noqa: T484','line_number':11910,'multiline':False]
['text':' type: (str) -> str','line_number':11929,'multiline':False]
['text':' type: (List[str]) -> str','line_number':11939,'multiline':False]
['text':' type: (Dict[str, int]) -> int','line_number':11950,'multiline':False]
['text':' type: (Dict[str, int]) -> Tuple[str, int]','line_number':11959,'multiline':False]
['text':' type: (List[int], List[int]) -> int','line_number':11980,'multiline':False]
['text':' type: (Tuple[int, float]) -> float','line_number':11990,'multiline':False]
['text':' type: (Tuple[Tuple[int, int], Tuple[int, int]]) -> int','line_number':11999,'multiline':False]
['text':' type: (Tuple[Tensor, Tensor], Tensor) -> Tensor','line_number':12026,'multiline':False]
['text':' type: (int, Tuple[int, Tuple[int, int]], Tuple[int, int]) -> int','line_number':12034,'multiline':False]
['text':' type: (List[int], Tensor, int) -> Tuple[int, Tensor, int]','line_number':12041,'multiline':False]
['text':' type: () -> Tuple[int, int, Tuple[int, int], Tuple[int, int]]','line_number':12048,'multiline':False]
['text':' type: (Tuple[int, int]) -> Tuple[int, int]','line_number':12055,'multiline':False]
['text':' side effect','line_number':12087,'multiline':False]
['text':' ordering','line_number':12091,'multiline':False]
['text':' this has to be a module otherwise attr lookup would not be','line_number':12182,'multiline':False]
['text':' allowed in the first place','line_number':12183,'multiline':False]
['text':' type: () -> Tuple[Tuple[Tensor, Tensor]]','line_number':12231,'multiline':False]
['text':' noqa: T484','line_number':12232,'multiline':False]
['text':' Tests for calling between different front-end modes','line_number':12239,'multiline':False]
['text':' The neg op in the python function should be properly inlined to the','line_number':12248,'multiline':False]
['text':' graph','line_number':12249,'multiline':False]
['text':' Note: the parameter self.param from the Python module is inlined','line_number':12267,'multiline':False]
['text':' into the graph','line_number':12268,'multiline':False]
['text':' Note: parameter self.param from the traced module should appear as','line_number':12349,'multiline':False]
['text':' an input to the graph and the neg op from the Python function should','line_number':12350,'multiline':False]
['text':' be properly inlined','line_number':12351,'multiline':False]
['text':' type: (int, int) -> int','line_number':12404,'multiline':False]
['text':' Note: the call to python_fn appears as `^python_fn()` and is called','line_number':12440,'multiline':False]
['text':' as a PythonOp in the interpreter','line_number':12441,'multiline':False]
['text':' Note: call to pm(x) appears as ^<python_value>() in the trace.','line_number':12461,'multiline':False]
['text':' Parameters are NOT inlined.','line_number':12462,'multiline':False]
['text':' Note: the call into PythonMod appears as ^forward(). Parameters','line_number':12529,'multiline':False]
['text':' are NOT inlined','line_number':12530,'multiline':False]
['text':' Note: the parameters from both modules should appear in the flattened','line_number':12574,'multiline':False]
['text':' input list to the graph. The mm op from ScriptMod1 should be properly','line_number':12575,'multiline':False]
['text':' inlined','line_number':12576,'multiline':False]
['text':' 3 % values in graph input lists, two mms in body','line_number':12577,'multiline':False]
['text':' type: (bool) -> int','line_number':12599,'multiline':False]
['text':' type: (bool) -> None','line_number':12613,'multiline':False]
['text':' fails if a tuple can't be lowered','line_number':12717,'multiline':False]
['text':' type: (Optional[int]) -> int','line_number':12722,'multiline':False]
['text':' noqa: T484','line_number':12724,'multiline':False]
['text':' type: () -> int','line_number':12743,'multiline':False]
['text':' type: (Tensor, Tuple[Tensor, Tensor, Tensor], Tuple[Tensor, Tuple[Tensor, Tensor]]) -> Tensor','line_number':12763,'multiline':False]
['text':' type: (Tuple[Tensor, Tensor], Tensor) -> Tuple[Tensor, Tensor, Tensor]','line_number':12772,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tuple[Tensor, Tensor], Tensor]','line_number':12783,'multiline':False]
['text':' noqa: T484','line_number':12784,'multiline':False]
['text':' type: (Tuple[Tensor, Tensor]) -> Tensor','line_number':12790,'multiline':False]
['text':' noqa: T484','line_number':12791,'multiline':False]
['text':' type: (Tensor, float) -> float','line_number':12796,'multiline':False]
['text':' type: (Tuple[Tensor, Tensor], Tensor) -> Tensor','line_number':12808,'multiline':False]
['text':' Load from filename','line_number':12846,'multiline':False]
['text':' Load from stream','line_number':12853,'multiline':False]
['text':' Check if the peak sizes at most differ by an empirically obtained factor','line_number':12862,'multiline':False]
['text':' for each type, the input type annotation and corresponding return type annotation','line_number':12865,'multiline':False]
['text':' replacing code input & return type pair','line_number':12879,'multiline':False]
['text':' ***** Type annotation tests ****','line_number':12883,'multiline':False]
['text':' Test combinations of:','line_number':12884,'multiline':False]
['text':' {String frontend, Python AST Frontend}','line_number':12885,'multiline':False]
['text':' {Python 3-style type annotations, MyPy-style type comments}','line_number':12886,'multiline':False]
['text':' {Script method, Script function}','line_number':12887,'multiline':False]
['text':'  String frontend , Python 3-style type annotations , Script function','line_number':12889,'multiline':False]
['text':'  String frontend , Python 3-style type annotations , Script method','line_number':12901,'multiline':False]
['text':' clear the class registry as we will be defining foo multiple times','line_number':12913,'multiline':False]
['text':'  String frontend , MyPy-style type comments , Script function','line_number':12920,'multiline':False]
['text':'  String frontend , MyPy-style type comments , Script method','line_number':12933,'multiline':False]
['text':' clear the class registry as we will be defining foo multiple times','line_number':12947,'multiline':False]
['text':'  Python AST Frontend , Python 3-style type annotations , Script function','line_number':12954,'multiline':False]
['text':' type: Tensor','line_number':12997,'multiline':False]
['text':' type: Tensor','line_number':12998,'multiline':False]
['text':' type: Tensor','line_number':12999,'multiline':False]
['text':' type: (int, int, int) -> Tensor','line_number':13001,'multiline':False]
['text':' type: bad type line  # noqa: F723','line_number':13002,'multiline':False]
['text':' type: Tensor','line_number':13008,'multiline':False]
['text':' type: Tensor','line_number':13010,'multiline':False]
['text':' type: (int, int, int) -> Tensor','line_number':13012,'multiline':False]
['text':' TODO: this should be supported but is difficult to parse','line_number':13015,'multiline':False]
['text':' type: Tensor','line_number':13018,'multiline':False]
['text':' type: Tensor','line_number':13020,'multiline':False]
['text':' type: (...) -> Tensor','line_number':13022,'multiline':False]
['text':'  Python AST Frontend , Python 3-style type annotations , Script method','line_number':13025,'multiline':False]
['text':'  Python AST Frontend , MyPy-style type comments , Script function','line_number':13046,'multiline':False]
['text':'  Python AST Frontend , MyPy-style type comments , Script method','line_number':13062,'multiline':False]
['text':' Tests that "# type: ignore[*]" is supported in type lines and is','line_number':13080,'multiline':False]
['text':' properly ignored.','line_number':13081,'multiline':False]
['text':' type: ignore','line_number':13084,'multiline':False]
['text':' type: ignore[no-redef]','line_number':13088,'multiline':False]
['text':' Initialize a ScriptModule that uses the weak module above multiple times','line_number':13152,'multiline':False]
['text':' Run same calculation as module','line_number':13169,'multiline':False]
['text':' strong_mod.weak.submodule has been recursively scripted','line_number':13217,'multiline':False]
['text':' Re-assignment is not tracked','line_number':13223,'multiline':False]
['text':' Only test that this compiles','line_number':13228,'multiline':False]
['text':' jank way to test if there is no error','line_number':13287,'multiline':False]
['text':' type: (bool) -> int','line_number':13294,'multiline':False]
['text':' TODO: Python print broadcasting list','line_number':13371,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':13373,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tensor, Tensor]','line_number':13378,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':13383,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tensor, Tensor]','line_number':13388,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':13393,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':13398,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tensor, Tensor]','line_number':13403,'multiline':False]
['text':' type: (Tensor, Tensor) -> List[int]','line_number':13411,'multiline':False]
['text':' type: (int) -> int','line_number':13427,'multiline':False]
['text':' type: (float) -> int','line_number':13432,'multiline':False]
['text':' type: (str) -> int','line_number':13437,'multiline':False]
['text':' b should be cleaned up but not a','line_number':13476,'multiline':False]
['text':' a should be cleaned up but not b','line_number':13491,'multiline':False]
['text':' shouldn't clean up `a` even though it's not used in the output','line_number':13501,'multiline':False]
['text':' c does not get cleaned up because there is a wildcard + mutation','line_number':13515,'multiline':False]
['text':' type: (str)','line_number':13581,'multiline':False]
['text':' type: (str) -> int','line_number':13588,'multiline':False]
['text':' type: (str) -> Tensor','line_number':13595,'multiline':False]
['text':' noqa: T484','line_number':13596,'multiline':False]
['text':' type: (int) -> str','line_number':13603,'multiline':False]
['text':' type: (float) -> float','line_number':13611,'multiline':False]
['text':' type: (int) -> float','line_number':13615,'multiline':False]
['text':' type: (int) -> str','line_number':13623,'multiline':False]
['text':' type: (int) -> str','line_number':13627,'multiline':False]
['text':' type: (int) -> str','line_number':13631,'multiline':False]
['text':' Check original module','line_number':13687,'multiline':False]
['text':' Check top level module','line_number':13691,'multiline':False]
['text':' Check submodule','line_number':13695,'multiline':False]
['text':' Check simpler module','line_number':13699,'multiline':False]
['text':' type: (str) -> str','line_number':13730,'multiline':False]
['text':' type: (str) -> str','line_number':13734,'multiline':False]
['text':' type: (str) -> str','line_number':13738,'multiline':False]
['text':' type: (str) -> str','line_number':13742,'multiline':False]
['text':' Loop then two if's added in exit transform','line_number':13792,'multiline':False]
['text':' type: (Optional[int]) -> int','line_number':13833,'multiline':False]
['text':' type: (int) -> int','line_number':13847,'multiline':False]
['text':' FUNCTION LOOKS LIKE:','line_number':13860,'multiline':False]
['text':' graph(%x.1 : int):','line_number':13861,'multiline':False]
['text':'   %7 : str = prim::Constant[value="Exception"]()','line_number':13862,'multiline':False]
['text':'   %2 : int = prim::Constant[value=1]()','line_number':13863,'multiline':False]
['text':'   %5 : int = prim::Constant[value=2]()','line_number':13864,'multiline':False]
['text':'   %19 : int = prim::Uninitialized()','line_number':13865,'multiline':False]
['text':'   %3 : bool = aten::eq(%x.1, %2)','line_number':13866,'multiline':False]
['text':'   %20 : int = prim::If(%3)','line_number':13867,'multiline':False]
['text':'     block0():','line_number':13868,'multiline':False]
['text':'       -> (%2)','line_number':13869,'multiline':False]
['text':'     block1():','line_number':13870,'multiline':False]
['text':'       %6 : bool = aten::eq(%x.1, %5)','line_number':13871,'multiline':False]
['text':'        = prim::If(%6)','line_number':13872,'multiline':False]
['text':'         block0():','line_number':13873,'multiline':False]
['text':'            = prim::RaiseException(%7)','line_number':13874,'multiline':False]
['text':'           -> ()','line_number':13875,'multiline':False]
['text':'         block1():','line_number':13876,'multiline':False]
['text':'            = prim::RaiseException(%7)','line_number':13877,'multiline':False]
['text':'           -> ()','line_number':13878,'multiline':False]
['text':'       -> (%19)','line_number':13879,'multiline':False]
['text':'   return (%20)','line_number':13880,'multiline':False]
['text':' type: (int) -> int','line_number':13883,'multiline':False]
['text':' type: (int)','line_number':13893,'multiline':False]
['text':' one if added to guard a + 2','line_number':13906,'multiline':False]
['text':' type: (int)','line_number':13909,'multiline':False]
['text':' if guard gets optimized away','line_number':13921,'multiline':False]
['text':' type: (int)','line_number':13925,'multiline':False]
['text':' no ifs added to guard print','line_number':13937,'multiline':False]
['text':' final a + 1 gets inlined into the first branch and optimized away','line_number':13966,'multiline':False]
['text':' type: (int) -> int','line_number':14030,'multiline':False]
['text':' type: (int) -> int','line_number':14042,'multiline':False]
['text':' type: () -> int','line_number':14067,'multiline':False]
['text':' type: (Tensor) -> int','line_number':14073,'multiline':False]
['text':' type: () -> Optional[int]','line_number':14080,'multiline':False]
['text':' type: (int) -> int','line_number':14092,'multiline':False]
['text':' type: (int, int) -> (int)','line_number':14109,'multiline':False]
['text':' type: (int) -> int','line_number':14123,'multiline':False]
['text':' type: (int) -> int','line_number':14132,'multiline':False]
['text':' Build test code','line_number':14155,'multiline':False]
['text':' Compare functions','line_number':14165,'multiline':False]
['text':' Build test code','line_number':14179,'multiline':False]
['text':' Compare functions','line_number':14187,'multiline':False]
['text':' Change the seed of the default generator to make','line_number':14196,'multiline':False]
['text':' sure that we're using the provided generator','line_number':14197,'multiline':False]
['text':' will not be compiled','line_number':14252,'multiline':False]
['text':' will not be compiled','line_number':14285,'multiline':False]
['text':' type: (int) -> int','line_number':14294,'multiline':False]
['text':' type: (_MyNamedTuple) -> Tensor','line_number':14308,'multiline':False]
['text':' noqa: UP014','line_number':14321,'multiline':False]
['text':' TODO: We can't use `checkScript` with the NamedTuple factory','line_number':14437,'multiline':False]
['text':' constructor. Using the factory constructor with TorchScript','line_number':14438,'multiline':False]
['text':' TorchScript creates an anonymous `NamedTuple` class instead of','line_number':14439,'multiline':False]
['text':' preserving the actual name. For example, the actual generated','line_number':14440,'multiline':False]
['text':' signature in this case is:','line_number':14441,'multiline':False]
['text':'   graph(%x.1 : NamedTuple(x : Tensor, y : Tensor))','line_number':14442,'multiline':False]
['text':' It looks like similar test cases have had this issue as well','line_number':14443,'multiline':False]
['text':' (see: `test_namedtuple_python`).','line_number':14444,'multiline':False]
['text':' type: (Optional[List[int]]) -> int','line_number':14452,'multiline':False]
['text':' TODO: pyflakes currently does not compose @overload annotation with other','line_number':14465,'multiline':False]
['text':' decorators. This is fixed on master but not on version 2.1.1.','line_number':14466,'multiline':False]
['text':' Next version update remove noqa and add @typing.overload annotation','line_number':14467,'multiline':False]
['text':' noqa: F811','line_number':14469,'multiline':False]
['text':' noqa: F811','line_number':14470,'multiline':False]
['text':' type: (int) -> int','line_number':14471,'multiline':False]
['text':' noqa: F811','line_number':14474,'multiline':False]
['text':' noqa: F811','line_number':14475,'multiline':False]
['text':' type: (float) -> float','line_number':14476,'multiline':False]
['text':' noqa: F811','line_number':14479,'multiline':False]
['text':' testing that the functions are cached','line_number':14487,'multiline':False]
['text':' testing that new functions added work with caching','line_number':14495,'multiline':False]
['text':' noqa: F811','line_number':14496,'multiline':False]
['text':' noqa: F811','line_number':14497,'multiline':False]
['text':' type: (str) -> str','line_number':14498,'multiline':False]
['text':' testing new function same qualified name','line_number':14505,'multiline':False]
['text':' noqa: F811','line_number':14506,'multiline':False]
['text':' noqa: F811','line_number':14507,'multiline':False]
['text':' type: (int, int) -> int','line_number':14508,'multiline':False]
['text':' currently we take the default values have to be specified in the','line_number':14520,'multiline':False]
['text':' overload as well - TODO take them from implementation and apply','line_number':14521,'multiline':False]
['text':' where the type is valid.','line_number':14522,'multiline':False]
['text':' noqa: F811','line_number':14523,'multiline':False]
['text':' noqa: F811','line_number':14524,'multiline':False]
['text':' type: (str) -> str','line_number':14525,'multiline':False]
['text':' noqa: F811','line_number':14528,'multiline':False]
['text':' noqa: F811','line_number':14529,'multiline':False]
['text':' type: (float) -> float','line_number':14530,'multiline':False]
['text':' noqa: F811','line_number':14533,'multiline':False]
['text':' noqa: F811','line_number':14555,'multiline':False]
['text':' noqa: F811','line_number':14556,'multiline':False]
['text':' type: (str, str) -> (str)','line_number':14557,'multiline':False]
['text':' noqa: F811','line_number':14560,'multiline':False]
['text':' noqa: F811','line_number':14561,'multiline':False]
['text':' type: (int, int) -> (int)','line_number':14562,'multiline':False]
['text':' noqa: F811','line_number':14565,'multiline':False]
['text':' noqa: F811','line_number':14575,'multiline':False]
['text':' noqa: F811','line_number':14576,'multiline':False]
['text':' type: (int) -> (int)','line_number':14577,'multiline':False]
['text':' noqa: F811','line_number':14580,'multiline':False]
['text':' noqa: F811','line_number':14591,'multiline':False]
['text':' noqa: F811','line_number':14592,'multiline':False]
['text':' type: (int, int) -> (int)','line_number':14593,'multiline':False]
['text':' noqa: F811','line_number':14596,'multiline':False]
['text':' type: (int, int) -> (int)','line_number':14597,'multiline':False]
['text':' noqa: F811','line_number':14604,'multiline':False]
['text':' noqa: F811','line_number':14605,'multiline':False]
['text':' type: (int) -> int','line_number':14606,'multiline':False]
['text':' noqa: F811','line_number':14609,'multiline':False]
['text':' noqa: F811','line_number':14610,'multiline':False]
['text':' type: (str) -> str','line_number':14611,'multiline':False]
['text':' noqa: F811','line_number':14614,'multiline':False]
['text':' noqa: F811','line_number':14622,'multiline':False]
['text':' noqa: F811','line_number':14623,'multiline':False]
['text':' type: (float) -> float','line_number':14624,'multiline':False]
['text':' noqa: F811','line_number':14627,'multiline':False]
['text':' noqa: F811','line_number':14628,'multiline':False]
['text':' type: (int, int) -> int','line_number':14629,'multiline':False]
['text':' noqa: F811','line_number':14632,'multiline':False]
['text':' type: (Union[float, int], int, int)','line_number':14633,'multiline':False]
['text':' noqa: F811','line_number':14641,'multiline':False]
['text':' type: () -> int','line_number':14643,'multiline':False]
['text':' noqa: F811','line_number':14646,'multiline':False]
['text':' noqa: E704','line_number':14668,'multiline':False]
['text':' noqa: F811','line_number':14670,'multiline':False]
['text':' noqa: F811','line_number':14671,'multiline':False]
['text':' noqa: F811','line_number':14685,'multiline':False]
['text':' noqa: F811','line_number':14686,'multiline':False]
['text':' noqa: F811','line_number':14713,'multiline':False]
['text':' noqa: F811','line_number':14714,'multiline':False]
['text':' type: (float, str) -> (float)','line_number':14715,'multiline':False]
['text':' noqa: F811','line_number':14718,'multiline':False]
['text':' noqa: F811','line_number':14719,'multiline':False]
['text':' type: (float, float) -> (float)','line_number':14720,'multiline':False]
['text':' noqa: F811','line_number':14723,'multiline':False]
['text':' noqa: F811','line_number':14739,'multiline':False]
['text':' noqa: F811','line_number':14740,'multiline':False]
['text':' type: (Tuple[Tensor, Tensor]) -> Tensor','line_number':14741,'multiline':False]
['text':' noqa: F811','line_number':14744,'multiline':False]
['text':' noqa: F811','line_number':14745,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':14746,'multiline':False]
['text':' noqa: F811','line_number':14749,'multiline':False]
['text':' noqa: F811','line_number':14773,'multiline':False]
['text':' noqa: F811','line_number':14774,'multiline':False]
['text':' noqa: F811','line_number':14777,'multiline':False]
['text':' noqa: F811','line_number':14778,'multiline':False]
['text':' type: (int) -> (int)','line_number':14779,'multiline':False]
['text':' noqa: F811','line_number':14782,'multiline':False]
['text':' noqa: F811','line_number':14793,'multiline':False]
['text':' noqa: F811','line_number':14794,'multiline':False]
['text':' type: (str) -> (int)','line_number':14795,'multiline':False]
['text':' noqa: F811','line_number':14798,'multiline':False]
['text':' noqa: F811','line_number':14799,'multiline':False]
['text':' type: (int) -> (int)','line_number':14800,'multiline':False]
['text':' noqa: F811','line_number':14803,'multiline':False]
['text':' testing overload declared first, then non-overload','line_number':14813,'multiline':False]
['text':' noqa: F811','line_number':14816,'multiline':False]
['text':' noqa: F811','line_number':14817,'multiline':False]
['text':' type: (int) -> int','line_number':14818,'multiline':False]
['text':' noqa: F811','line_number':14821,'multiline':False]
['text':' noqa: F811','line_number':14822,'multiline':False]
['text':' type: (Tensor) -> Tensor','line_number':14823,'multiline':False]
['text':' noqa: F811','line_number':14826,'multiline':False]
['text':' noqa: F811','line_number':14833,'multiline':False]
['text':' testing non-overload declared first, then overload','line_number':14839,'multiline':False]
['text':' noqa: F811','line_number':14851,'multiline':False]
['text':' noqa: F811','line_number':14852,'multiline':False]
['text':' noqa: F811','line_number':14855,'multiline':False]
['text':' noqa: F811','line_number':14856,'multiline':False]
['text':' type: (int) -> (int)','line_number':14857,'multiline':False]
['text':' noqa: F811','line_number':14860,'multiline':False]
['text':' type: Tensor','line_number':14965,'multiline':False]
['text':' type: Tensor','line_number':14966,'multiline':False]
['text':' type: Tensor','line_number':14967,'multiline':False]
['text':' type: int','line_number':14968,'multiline':False]
['text':' type: int','line_number':14969,'multiline':False]
['text':' type: Tensor','line_number':14970,'multiline':False]
['text':' type: Tensor','line_number':14971,'multiline':False]
['text':' type: Optional[Tensor]','line_number':14972,'multiline':False]
['text':' type: Optional[Tensor]','line_number':14973,'multiline':False]
['text':' type: bool','line_number':14974,'multiline':False]
['text':' type: float','line_number':14975,'multiline':False]
['text':' type: Tensor','line_number':14976,'multiline':False]
['text':' type: Tensor','line_number':14977,'multiline':False]
['text':' type: bool','line_number':14978,'multiline':False]
['text':' type: Optional[Tensor]','line_number':14979,'multiline':False]
['text':' type: bool','line_number':14980,'multiline':False]
['text':' type: Optional[Tensor]','line_number':14981,'multiline':False]
['text':' type: (...) -> Tuple[Tensor, Optional[Tensor]]','line_number':14983,'multiline':False]
['text':' print("rel. error: ")','line_number':15025,'multiline':False]
['text':' print(jit_out / py_out - 1)','line_number':15026,'multiline':False]
['text':' print(jit_out/py_out-1)','line_number':15117,'multiline':False]
['text':' print(torch.allclose(jit_out, py_out, atol=5e-4, rtol=1e-4))','line_number':15118,'multiline':False]
['text':' type: (List[Tensor]) -> Tensor','line_number':15123,'multiline':False]
['text':' type: (List[Tensor]) -> Tensor','line_number':15127,'multiline':False]
['text':' Assert ignored code is run','line_number':15168,'multiline':False]
['text':' type: (Tensor) -> Tuple[Tensor, Tensor]','line_number':15182,'multiline':False]
['text':' type: (Tensor, Tensor) -> Tensor','line_number':15187,'multiline':False]
['text':' type: (Tensor, bool) -> Tuple[Tensor, Tensor]','line_number':15191,'multiline':False]
['text':' type: (str) -> Tensor','line_number':15238,'multiline':False]
['text':' TODO: re-enable module hook when Python printing of attributes is','line_number':15242,'multiline':False]
['text':' supported','line_number':15243,'multiline':False]
['text':' type: (str) -> int','line_number':15312,'multiline':False]
['text':' type: (int) -> (int)','line_number':15454,'multiline':False]
['text':' type: (str) -> List[str]','line_number':15479,'multiline':False]
['text':' the text of the string should only appear once in the pickling','line_number':15485,'multiline':False]
['text':' noqa: E704','line_number':15641,'multiline':False]
['text':' noqa: F811','line_number':15643,'multiline':False]
['text':' assert that loaded_y changed as well','line_number':15714,'multiline':False]
['text':' type: (str) -> None','line_number':15730,'multiline':False]
['text':' type: (str) -> List[str]','line_number':15738,'multiline':False]
['text':' 🤷🤷🤷🤷','line_number':15746,'multiline':False]
['text':' type: (Dict[str, int]) -> List[int]','line_number':15775,'multiline':False]
['text':' type: int','line_number':15873,'multiline':False]
['text':' type: int','line_number':15874,'multiline':False]
['text':' type: (...) -> int','line_number':15876,'multiline':False]
['text':' type: int','line_number':15877,'multiline':False]
['text':' type: int','line_number':15882,'multiline':False]
['text':' type: int','line_number':15883,'multiline':False]
['text':' type: (...) -> None','line_number':15885,'multiline':False]
['text':' type: int','line_number':15887,'multiline':False]
['text':' type: int','line_number':15888,'multiline':False]
['text':' Tests the case where a torch.Tensor subclass (like Parameter) is used as','line_number':15910,'multiline':False]
['text':' input.','line_number':15911,'multiline':False]
['text':' this attribute doesn't exist on `Inner`','line_number':15931,'multiline':False]
['text':' This should properly complain that `self.inner` doesn't have the attribute `b`','line_number':15937,'multiline':False]
['text':' access inner elements','line_number':15969,'multiline':False]
['text':' known to be failing in tracer','line_number':16027,'multiline':False]
['text':' The following fail due to #12024.','line_number':16029,'multiline':False]
['text':' A prim::ListConstruct is involved and the indices get traced as TensorType,','line_number':16030,'multiline':False]
['text':' which always require_grad. This causes a crash in autodiff.','line_number':16031,'multiline':False]
['text':' jit doesn't support sparse tensors.','line_number':16041,'multiline':False]
['text':' slogdet tests use itemgetter to select its only differentiable output,','line_number':16047,'multiline':False]
['text':' but this happens outside of the graph we handle, so there are fewer','line_number':16048,'multiline':False]
['text':' reference outputs than graph outputs.','line_number':16049,'multiline':False]
['text':' chunk returns a list in scripting and we don't unpack the list,','line_number':16064,'multiline':False]
['text':' Thus it won't be replaced by ConstantChunk and run AD.','line_number':16065,'multiline':False]
['text':' It's explicitly checked in test_chunk_constant_script_ad','line_number':16066,'multiline':False]
['text':' Similary for split, it's replaced by split_with_sizes in tracing,','line_number':16067,'multiline':False]
['text':' but we don't have AD formula for aten::split(Tensor, int[], int),','line_number':16068,'multiline':False]
['text':' an op registered in JIT so AD is not triggered in scripting.','line_number':16069,'multiline':False]
['text':' no support for BroadcastingList in python printer','line_number':16086,'multiline':False]
['text':' aliases, which may appear in method_tests but are tested elsewhere','line_number':16097,'multiline':False]
['text':' Disable tests for lu from common_methods_invocations.py','line_number':16100,'multiline':False]
['text':' TODO(@nikitaved) Enable jit tests once autograd.Function does support scripting','line_number':16101,'multiline':False]
['text':' UBSAN per-function exclusions don't seem to work with OpenMP pragmas,','line_number':16115,'multiline':False]
['text':' and we have to disable the failing tests here instead.','line_number':16116,'multiline':False]
['text':' eval() is not supported, so skip these tests','line_number':16161,'multiline':False]
['text':' Create module to use the script method','line_number':16206,'multiline':False]
['text':' check __repr__','line_number':16216,'multiline':False]
['text':' Construct a normal nn module to stay consistent with create_script_module','line_number':16227,'multiline':False]
['text':' and make use of a single global rng_state in module initialization','line_number':16228,'multiline':False]
['text':' Set up inputs from tuple of sizes or constructor fn','line_number':16233,'multiline':False]
['text':' Extra parameters to forward()','line_number':16260,'multiline':False]
['text':' TODO(issue#52052) Neither this nor no_grad should be required','line_number':16267,'multiline':False]
['text':' if check_against_reference() is updated to check gradients','line_number':16268,'multiline':False]
['text':' w.r.t. weights and then only check w.r.t. inputs if any','line_number':16269,'multiline':False]
['text':' inputs require it.','line_number':16270,'multiline':False]
['text':' Check against Python module as reference','line_number':16273,'multiline':False]
['text':' normalized check_ad is 3-element tuple: (bool, List[str], List[str])','line_number':16295,'multiline':False]
['text':' issue gh-32561','line_number':16315,'multiline':False]
