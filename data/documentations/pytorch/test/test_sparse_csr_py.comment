['text':' Owner(s): ["module: sparse"]','line_number':1,'multiline':False]
['text':' load_tests from torch.testing._internal.common_utils is used to automatically filter tests for','line_number':31,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':32,'multiline':False]
['text':' cusparseSpSM was added in 11.3.1 but we don't have access to patch version','line_number':39,'multiline':False]
['text':' cusparseSpGEMM was added in 11.0','line_number':44,'multiline':False]
['text':' cusparseSDDMM was added in 11.2.1 but we don't have access to patch version','line_number':49,'multiline':False]
['text':' This should be just an import from test_linalg instead of code duplication','line_number':71,'multiline':False]
['text':' but https://github.com/pytorch/pytorch/pull/63511#discussion_r733989701','line_number':72,'multiline':False]
['text':' Here we test the correctness of the crow_indices algorithm','line_number':136,'multiline':False]
['text':' and testing it on CPU and with int32 dtype will be','line_number':137,'multiline':False]
['text':' sufficient.','line_number':138,'multiline':False]
['text':' list inputs to factory/constructor function without','line_number':214,'multiline':False]
['text':' specifying device will result a sparse compressed tensor','line_number':215,'multiline':False]
['text':' on CPU. So, skip testing against cuda device as unused.','line_number':216,'multiline':False]
['text':' skip zero-sized tensors for list inputs:','line_number':244,'multiline':False]
['text':' skip shape inference for zero-sized tensor','line_number':249,'multiline':False]
['text':' inputs because (i) the shape determined from','line_number':250,'multiline':False]
['text':' an empty list is ambiguous, and (ii) the','line_number':251,'multiline':False]
['text':' size of the plain dimension defined as','line_number':252,'multiline':False]
['text':' max(plain_indices) is undefined if','line_number':253,'multiline':False]
['text':' plain_indices has no values','line_number':254,'multiline':False]
['text':' using local patterns for test_print stability','line_number':348,'multiline':False]
['text':' 2 x 3 batch of 3 x 2 tensors, trivial blocksize, non-hybrid/hybrid:','line_number':350,'multiline':False]
['text':' tensor with non-trivial blocksize, non-hybrid/hybrid:','line_number':363,'multiline':False]
['text':' non-hybrid cases are covered by the enable_hybrid==False loop','line_number':385,'multiline':False]
['text':' (number of dimensions, the corresponding block size)','line_number':430,'multiline':False]
['text':' FIXME: remove in followup once integer support is landed for segment_reduce','line_number':492,'multiline':False]
['text':' Some strided samples (with inf, nan elements) appear to share','line_number':518,'multiline':False]
['text':' storage, so we must clone:','line_number':519,'multiline':False]
['text':' that is, the validation returns the sparse sample','line_number':523,'multiline':False]
['text':' wrapped within ErrorInput instance','line_number':524,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':528,'multiline':False]
['text':' Re-define atol and rtol for operations that result values','line_number':532,'multiline':False]
['text':' are random (and hence, non-comparable) be we still want to','line_number':533,'multiline':False]
['text':' check the shape, dtype, etc attributes of the results:','line_number':534,'multiline':False]
['text':' check empty batch','line_number':586,'multiline':False]
['text':' CUDA kernel asserts are not recoverable, so we skip these for now','line_number':716,'multiline':False]
['text':' Skip invalid size input as a valid size is estimated for other inputs','line_number':832,'multiline':False]
['text':' nnz cannot be stored in int32 crow_indices','line_number':869,'multiline':False]
['text':' but the `crow_indices[..., -1] == nnz`` check happens after the overflow validation','line_number':870,'multiline':False]
['text':' So we can use `nnz - 1` here to avoid `value cannot be converted to type int32 without overflow`','line_number':871,'multiline':False]
['text':' during construction of crow_indices','line_number':872,'multiline':False]
['text':' to_dense does not support hybrid inputs','line_number':899,'multiline':False]
['text':' a shameless copy of TestViewOps.is_view_of','line_number':924,'multiline':False]
['text':' unreachable','line_number':945,'multiline':False]
['text':' Tests the limits of the number of specified elements in CSR tensors, see gh-102520.','line_number':996,'multiline':False]
['text':' select from batch dimensions','line_number':1042,'multiline':False]
['text':' selecting rows/col with batch dims not allowed','line_number':1052,'multiline':False]
['text':' select from sparse dimensions','line_number':1054,'multiline':False]
['text':' assigning to sparse through indexing is disabled','line_number':1061,'multiline':False]
['text':' select from sparse dimensions without removing batch dims','line_number':1065,'multiline':False]
['text':' resize to larger shape doesn't add specified elements','line_number':1094,'multiline':False]
['text':' resize to smaller shape trims specified elements','line_number':1102,'multiline':False]
['text':' trim batched dimensions','line_number':1106,'multiline':False]
['text':' shape is inherited from a','line_number':1121,'multiline':False]
['text':' other metadata is not affected','line_number':1123,'multiline':False]
['text':' We don't check the content of br_plain_indices and br_compressed_indices','line_number':1139,'multiline':False]
['text':' because it is not well-defined (the content depends on the original','line_number':1140,'multiline':False]
['text':' shape of `b` that `resize_as` ought to discard) nor needed (the','line_number':1141,'multiline':False]
['text':' subsequent operation likely updates the indices and values of `b` anyway).','line_number':1142,'multiline':False]
['text':' the device/dtype of indices should always be unaffected','line_number':1143,'multiline':False]
['text':' values are generated empty, shape is updated','line_number':1148,'multiline':False]
['text':' the device/dtype of indices should always be unaffected','line_number':1150,'multiline':False]
['text':' nnz will be picked up from a via new shape of values','line_number':1154,'multiline':False]
['text':' post resize the invariants of the layout are respected','line_number':1157,'multiline':False]
['text':' same size, resize should not trigger','line_number':1174,'multiline':False]
['text':' This test will not always trigger a resize, if the layouts are the same nothing should happen to b.','line_number':1183,'multiline':False]
['text':' The invariants of the function as checked should still hold','line_number':1184,'multiline':False]
['text':' same ndim, but bigger, more nnz, different dtype, different blocksize if blocked','line_number':1187,'multiline':False]
['text':' different device, only check on cuda pass as we know we are testing in an environment','line_number':1197,'multiline':False]
['text':' that has multiple devices','line_number':1198,'multiline':False]
['text':' TODO: .cpu() does not seem to work correctly for sparse. Causes a call to `copy_` which','line_number':1200,'multiline':False]
['text':' complains about incompatible nnz between src and self?','line_number':1201,'multiline':False]
['text':' error on a strided','line_number':1212,'multiline':False]
['text':' error on b strided','line_number':1218,'multiline':False]
['text':' error if layout does not match, transpose induces layout flip','line_number':1224,'multiline':False]
['text':' resizing of columns to smaller size is not implemented','line_number':1244,'multiline':False]
['text':' TODO: Support auto generation of device check for sparse tensors','line_number':1395,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/59058','line_number':1396,'multiline':False]
['text':' hmm, the test passes ok on CUDA when Rocm is not available:','line_number':1437,'multiline':False]
['text':' TODO: disable the invariant checks within torch.baddbmm that','line_number':1442,'multiline':False]
['text':' constructs unconventional csr tensors leading to','line_number':1443,'multiline':False]
['text':' RuntimeError: tensor dimensionality must be sum of batch,','line_number':1444,'multiline':False]
['text':'     base, and dense dimensionalities (=0 + 2 + 0) but got 3','line_number':1445,'multiline':False]
['text':' when invariant checking is enabled. When done, undecorate run_test.','line_number':1446,'multiline':False]
['text':' a_batched is a regular CSR tensor but with a batch dimension in the shape','line_number':1469,'multiline':False]
['text':' a_batched is a regular CSR tensor but with a batch','line_number':1502,'multiline':False]
['text':' dimension in the shape. It is unorthodox in PyTorch','line_number':1503,'multiline':False]
['text':' to represent a batch sparse tensor in this way,','line_number':1504,'multiline':False]
['text':' hence checking the tensor invariants is locally','line_number':1505,'multiline':False]
['text':' turned off.','line_number':1506,'multiline':False]
['text':' TODO: block_size 1 is broken','line_number':1538,'multiline':False]
['text':' assume numpy/scipy spmatrix','line_number':1560,'multiline':False]
['text':' the ref takes no out kwarg','line_number':1566,'multiline':False]
['text':' transpose inplace to propagate out to checking context','line_number':1568,'multiline':False]
['text':' TODO: Explicitly disable block size 1 support','line_number':1658,'multiline':False]
['text':' if (TEST_WITH_ROCM or not TEST_CUSPARSE_GENERIC) and block_size == 1:','line_number':1659,'multiline':False]
['text':'     return','line_number':1660,'multiline':False]
['text':' Test column-major blocks','line_number':1673,'multiline':False]
['text':' TODO: When unitriangular=True results are not correct on CPU','line_number':1702,'multiline':False]
['text':' TODO: When upper=False some generated inputs might crash on CPU','line_number':1706,'multiline':False]
['text':' TODO: replace with torch method when implemented to_dense() on block sparse tensor','line_number':1717,'multiline':False]
['text':' TODO: zeros on the diagonal are not handled for CPU path','line_number':1734,'multiline':False]
['text':' there's no way to query this info from MKL','line_number':1735,'multiline':False]
['text':' Test column-major blocks','line_number':1759,'multiline':False]
['text':' TODO: addmm doesn't support strided result for sparse inputs.','line_number':1777,'multiline':False]
['text':' res = beta * t  + alpha * (x @ y)','line_number':1778,'multiline':False]
['text':' Test mm({CSR, CSC}, {CSR, CSC})','line_number':1828,'multiline':False]
['text':' Test 0-strided','line_number':1936,'multiline':False]
['text':' Test beta=0, M=nan','line_number':1942,'multiline':False]
['text':' Test transpose','line_number':1948,'multiline':False]
['text':' Test 0-strided','line_number':1970,'multiline':False]
['text':' Test beta=0, M=nan','line_number':1976,'multiline':False]
['text':' Test transpose','line_number':1982,'multiline':False]
['text':' test that the errors are the same for dense and sparse versions','line_number':2022,'multiline':False]
['text':' shapes must be compatible for matrix multiplication','line_number':2026,'multiline':False]
['text':' mat2 must be a matrix','line_number':2035,'multiline':False]
['text':' the first input needs to be 1D or 2D','line_number':2044,'multiline':False]
['text':' test that the errors are the same for dense and sparse versions','line_number':2062,'multiline':False]
['text':' shapes must be compatible for matrix multiplication','line_number':2066,'multiline':False]
['text':' mat2 must be a matrix','line_number':2075,'multiline':False]
['text':' sparse.to_dense() uses torch.add internally so if torch.add is wrong,','line_number':2093,'multiline':False]
['text':' the dense tensor will be wrong but this test would still pass','line_number':2094,'multiline':False]
['text':' there's a separate test that checks for the correctness of the .to_dense() call','line_number':2095,'multiline':False]
['text':' Non contiguous dense tensor','line_number':2104,'multiline':False]
['text':' TODO: This whole test should be migrated to OpInfos','line_number':2126,'multiline':False]
['text':' Forward comparison','line_number':2131,'multiline':False]
['text':' TODO: While result of mul(dense, csr) is csr, it is not fully compressed.','line_number':2137,'multiline':False]
['text':' That means it may contain materialized zeros, since the dense argument','line_number':2138,'multiline':False]
['text':' is converted according to the sparsity pattern of csr. In the future','line_number':2139,'multiline':False]
['text':' we might require the result to be fully compressed.','line_number':2140,'multiline':False]
['text':' Grad comparison','line_number':2144,'multiline':False]
['text':' csr * csr -> csr with csr, csr gradients','line_number':2149,'multiline':False]
['text':' TODO: Currently strided Tensors cannot have csr gradients','line_number':2166,'multiline':False]
['text':' dense * csr -> csr with csr, dense gradients','line_number':2167,'multiline':False]
['text':' csr * dense -> csr with dense, csr gradients','line_number':2174,'multiline':False]
['text':' TODO: enable hybrid once to_dense supports it','line_number':2186,'multiline':False]
['text':' ComplexHalf is experimental','line_number':2194,'multiline':False]
['text':' BUG: dispatcher ignores mul.Scalar(Tensor, Scalar)','line_number':2204,'multiline':False]
['text':' This issues is circumvented in the mul(Tensor, Tensor) kernel.','line_number':2205,'multiline':False]
['text':' ROCm fails when nnz = 0','line_number':2225,'multiline':False]
['text':' Make sure diagonal elements are not materialized.','line_number':2283,'multiline':False]
['text':' This is to exercise `unitriangular=True` not relying on','line_number':2284,'multiline':False]
['text':' explicit presence of these indices.','line_number':2285,'multiline':False]
['text':' test out with C contiguous strides','line_number':2314,'multiline':False]
['text':' test out with F contiguous strides','line_number':2322,'multiline':False]
['text':' test out with discontiguous strides','line_number':2331,'multiline':False]
['text':' Add a test case for size 0 a and b tensors','line_number':2388,'multiline':False]
['text':' Compute sparse result','line_number':2421,'multiline':False]
['text':' Compute dense result and compare with sparse result','line_number':2426,'multiline':False]
['text':' test that the errors are the same for dense and sparse sampled versions','line_number':2464,'multiline':False]
['text':' import re','line_number':2465,'multiline':False]
['text':' shapes must be compatible for matrix multiplication','line_number':2467,'multiline':False]
['text':' mat1 must be a matrix','line_number':2473,'multiline':False]
['text':' mat2 must be a matrix','line_number':2477,'multiline':False]
['text':' scatter_reduce expect index to be int64','line_number':2561,'multiline':False]
['text':' by setting nnz < M, create empty rows','line_number':2589,'multiline':False]
['text':' we are doing blocking with 4x vector length in the kernel,','line_number':2593,'multiline':False]
['text':' so need to test when K > 4x vector length','line_number':2594,'multiline':False]
['text':' Currently, there is no rule in PyTorch for filling zeros in the outputs','line_number':2619,'multiline':False]
['text':'   from operations on Sparse CSR tensors. Hence only those operators are supported','line_number':2620,'multiline':False]
['text':'   which have 0->0 correspondence, example: sin(0) = 0, tan(0) = 0 but','line_number':2621,'multiline':False]
['text':'   cos(0) = 1 (and hence it's not supported).','line_number':2622,'multiline':False]
['text':' Note: here, we do this test only for unary operators','line_number':2623,'multiline':False]
['text':' Sparse CSR only supports 2D tensors as inputs','line_number':2655,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2656,'multiline':False]
['text':' Sparse CSR only supports 2D tensors as inputs','line_number':2679,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2680,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2710,'multiline':False]
['text':' We must skip samples of low dimensionality, we can't covert them to sparsed compressed layouts','line_number':2716,'multiline':False]
['text':' Compute sparse result','line_number':2727,'multiline':False]
['text':' Compute dense result and compare with sparse result','line_number':2734,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2752,'multiline':False]
['text':' This path tests the autograd path wrt dense inputs','line_number':2760,'multiline':False]
['text':' noncontiguous','line_number':2771,'multiline':False]
['text':' Now test the autograd path wrt sparse inputs','line_number':2776,'multiline':False]
['text':' gradcheck doesn't work for sparse CSR yet, compare against dense path','line_number':2789,'multiline':False]
['text':' Compute sparse result','line_number':2790,'multiline':False]
['text':' Compute dense result and compare with sparse result','line_number':2801,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2821,'multiline':False]
['text':' TODO: Remove detach once we have autograd support for CSR input','line_number':2827,'multiline':False]
['text':' noncontiguous','line_number':2838,'multiline':False]
['text':' mv currently work only on CUDA','line_number':2849,'multiline':False]
['text':' Fail early to prevent silent success with this test','line_number':2854,'multiline':False]
['text':' Here we assume that the signature is op(sparse_input, dense_input) -> dense_output','line_number':2859,'multiline':False]
['text':' TODO: Remove detach once we have autograd support for CSR input','line_number':2861,'multiline':False]
['text':' noncontiguous','line_number':2872,'multiline':False]
['text':' correct layout','line_number':2922,'multiline':False]
['text':' transpose must be return a view','line_number':2924,'multiline':False]
['text':' result uses unsafe construction, so we check invariants','line_number':2926,'multiline':False]
['text':' transpose must be return a view','line_number':2932,'multiline':False]
['text':' result uses unsafe construction, so we check invariants','line_number':2934,'multiline':False]
['text':' correct layout','line_number':2940,'multiline':False]
['text':' transpose must be return a view','line_number':2942,'multiline':False]
['text':' result uses unsafe construction, so we check invariants','line_number':2944,'multiline':False]
['text':' expect all transpose to throw','line_number':2990,'multiline':False]
['text':' batch/sparse, sparse/dense only and full hybrid cases','line_number':3009,'multiline':False]
['text':' sparse only cases','line_number':3011,'multiline':False]
['text':' for blocked all combinations of 2,1 should be valid blocksizes','line_number':3017,'multiline':False]
['text':' repeat the realistic sparseity case with varried block sizes','line_number':3021,'multiline':False]
['text':' TODO: This is a stopgap for a rigorous extension of our autograd tests','line_number':3030,'multiline':False]
['text':' to test the functionality of detach','line_number':3031,'multiline':False]
['text':' No native scipy BSC support?','line_number':3053,'multiline':False]
['text':' BSR -> CSR is not yet supported','line_number':3088,'multiline':False]
['text':' BSR -> CSC is not yet supported','line_number':3091,'multiline':False]
['text':' BSC -> CSR is not yet supported','line_number':3094,'multiline':False]
['text':' BSC -> CSC is not yet supported','line_number':3097,'multiline':False]
['text':' CSR -> BSR only works for non-batched inputs','line_number':3100,'multiline':False]
['text':' CSR -> BSC only works for non-batched inputs','line_number':3104,'multiline':False]
['text':' CSC -> BSR only works for non-batched inputs','line_number':3108,'multiline':False]
['text':' CSC -> BSC only works for non-batched inputs','line_number':3112,'multiline':False]
['text':' change of blocksize upon conversion is not yet supported.','line_number':3127,'multiline':False]
['text':' helpers','line_number':3157,'multiline':False]
['text':' scipy has no bsc layout, so we check against the bsr layout of the tranposed dense','line_number':3160,'multiline':False]
['text':' we must tranpose the blocks before comparing','line_number':3177,'multiline':False]
['text':' Calculate COO indices for sparse matrix.','line_number':3183,'multiline':False]
['text':' If sparse matrix layout blocked, rearrange dense matrix','line_number':3195,'multiline':False]
['text':' so that the shape past first two dimensions match the','line_number':3196,'multiline':False]
['text':' shape of sparse matrix values.','line_number':3197,'multiline':False]
['text':' Verify that non-zero values of the sparse matrix are','line_number':3207,'multiline':False]
['text':' equal to corresponding values of the dense matrix.','line_number':3208,'multiline':False]
['text':' Verify that the remaining elements of the dense matrix','line_number':3211,'multiline':False]
['text':' are 0, i.e. that dense are sparse matrix are fully','line_number':3212,'multiline':False]
['text':' equal.','line_number':3213,'multiline':False]
['text':' sanity check, selecting batch of to_<layout> and dense[batch].to_<layout> should give the same result','line_number':3229,'multiline':False]
['text':' generate a dense tensor','line_number':3237,'multiline':False]
['text':' introduce some sparsty, mask is sparse shape, element applies to entire dense sub-tensor (hybrid) and is','line_number':3240,'multiline':False]
['text':' applied to each batch','line_number':3241,'multiline':False]
['text':' manually expand to match hybrid shape','line_number':3243,'multiline':False]
['text':' mask will broadcast over the batch dims if present','line_number':3248,'multiline':False]
['text':' note: order is important here, the hybrid-ness decides the inner content check which is used to build the','line_number':3252,'multiline':False]
['text':' batched checker (if needed)','line_number':3253,'multiline':False]
['text':' general cases, always run','line_number':3265,'multiline':False]
['text':' special cases for batched tensors','line_number':3274,'multiline':False]
['text':' batched sparse tensors need only have the same number of non-zeros in each batch not nessesarily the','line_number':3276,'multiline':False]
['text':' same sparsity pattern in each batch','line_number':3277,'multiline':False]
['text':' number of elements/blocks in each batch (total not nnz)','line_number':3284,'multiline':False]
['text':' if we are blocked the mask is genereated for the block valued elemetns','line_number':3287,'multiline':False]
['text':' random bool vector w/ length equal to max possible nnz for the sparse_shape','line_number':3290,'multiline':False]
['text':' stack random permutations of the source for each batch','line_number':3294,'multiline':False]
['text':' for blocked we need to do a bit of extra work to expand the mask from blocked-space to element-space','line_number':3298,'multiline':False]
['text':' if batches have different nnz we expect the conversion to throw','line_number':3314,'multiline':False]
['text':' Should throw if there is a zero in the batch size','line_number':3327,'multiline':False]
['text':' TODO: Remove this once support has been enabled','line_number':3350,'multiline':False]
['text':' TODO: Remove this once support has been enabled','line_number':3353,'multiline':False]
['text':' no-op if triton is present','line_number':3393,'multiline':False]
['text':' NOTE: batch dims with zero sizes are not supported in `to_sparse_bsr`.','line_number':3438,'multiline':False]
['text':' General correctness','line_number':3443,'multiline':False]
['text':' Test long rows which exceed Triton's max numel limit set to 2 ** 17','line_number':3456,'multiline':False]
['text':' kernel != kernel_impl means dispatch was already registered.','line_number':3483,'multiline':False]
['text':' This is exactly what we need!','line_number':3484,'multiline':False]
['text':' Note that each value in a non-zero block is in range block_size * [low^2, high^2).','line_number':3487,'multiline':False]
['text':' NOTE: batch dims with zero sizes are not supported in `to_sparse_bsr`.','line_number':3490,'multiline':False]
['text':' Whether to make inputs orthogonal so that the product is zero','line_number':3494,'multiline':False]
['text':' NOTE: do not get confused, it will be transposed','line_number':3499,'multiline':False]
['text':' Test against linear to check dispatch','line_number':3509,'multiline':False]
['text':' which takes place for torch.half and torch.bfloat16.','line_number':3510,'multiline':False]
['text':' Check dispatch worked with non-trivial outputs','line_number':3515,'multiline':False]
['text':' Otherwise check correctness against bmm','line_number':3520,'multiline':False]
['text':' since nn.linear does not support bsr.dim() > 2.','line_number':3521,'multiline':False]
['text':' check whether bsr_dense_mm handles different grid sizes','line_number':3530,'multiline':False]
['text':' None means max possible grid size which is CUDA-dependent.','line_number':3531,'multiline':False]
['text':' Blocksizes check','line_number':3571,'multiline':False]
['text':' out check','line_number':3578,'multiline':False]
['text':' Note that each value in a non-zero block is in range block_size * [low^2, high^2).','line_number':3600,'multiline':False]
['text':' NOTE: batch dims with zero sizes are not supported in `to_sparse_bsr`.','line_number':3607,'multiline':False]
['text':' We make attn_mask block lower/upper triangular so that BSR and Strided','line_number':3616,'multiline':False]
['text':' function variants are directly comparable.','line_number':3617,'multiline':False]
['text':' NOTE: only boolean mask is directly compatible with the Strided version','line_number':3622,'multiline':False]
['text':' without any pre-/post-processing. Hence we test against a boolean mask.','line_number':3623,'multiline':False]
['text':' Note that each value in a non-zero block is in range block_size * [low^2, high^2).','line_number':3646,'multiline':False]
['text':' NOTE: batch dims with zero sizes are not supported in `to_sparse_bsr`.','line_number':3649,'multiline':False]
['text':' Test not powers of 2 ks as well.','line_number':3655,'multiline':False]
['text':' Non-trivial sparsity pattern.','line_number':3657,'multiline':False]
['text':' Plus with tril inputs the result is also tril,','line_number':3658,'multiline':False]
['text':' so we can compare BSR and CSR implementations.','line_number':3659,'multiline':False]
['text':' For testing `out=` let's make values to have "weird" strides','line_number':3678,'multiline':False]
['text':' so that if the kernel modifies values to it's needs, the result','line_number':3679,'multiline':False]
['text':' is being compied into out.values.','line_number':3680,'multiline':False]
['text':' Check different grid sizes to make sure that input slicing works','line_number':3698,'multiline':False]
['text':' if this input is larger than the grid.','line_number':3699,'multiline':False]
['text':' Note that each value in a non-zero block is in range blocksize * [low^2, high^2).','line_number':3765,'multiline':False]
['text':' NOTE: batch dims with zero sizes are not supported in `to_sparse_bsr`.','line_number':3768,'multiline':False]
['text':' ensure that there was at least one succesful test:','line_number':3797,'multiline':False]
['text':' deleting object leads to dead key','line_number':3817,'multiline':False]
['text':' key with different storage offset and shape:','line_number':3822,'multiline':False]
['text':' key with different strides:','line_number':3825,'multiline':False]
['text':' when object dies, make sure that key represents a dead','line_number':3828,'multiline':False]
['text':' object as well:','line_number':3829,'multiline':False]
['text':' Storing a tensor as a dict key:','line_number':3833,'multiline':False]
['text':' t3 and t3_ reference the same data, so, the key becomes dead','line_number':3846,'multiline':False]
['text':' (that is, its .obj property returns None) until all','line_number':3847,'multiline':False]
['text':' references are deleted:','line_number':3848,'multiline':False]
['text':' Storing a tensor as a dict key and value:','line_number':3856,'multiline':False]
['text':' when object is deleted, the key represents an alive object','line_number':3862,'multiline':False]
['text':' because the object is referenced by the dict item value:','line_number':3863,'multiline':False]
['text':' This also means that the life time of the tensor is same as','line_number':3866,'multiline':False]
['text':' the life time of the corresponding dict item:','line_number':3867,'multiline':False]
['text':' Storing a tensor as a dict key and value wrapped with TensorAsKey:','line_number':3871,'multiline':False]
['text':' when object is deleted, it will be dead as the wrapped value','line_number':3878,'multiline':False]
['text':' hold the tensor instance as a weakref:','line_number':3879,'multiline':False]
['text':' but key is still valid:','line_number':3882,'multiline':False]
['text':' todo: eliminate this skip','line_number':3934,'multiline':False]
['text':' Find optimal kernel parameters, the speed-up is','line_number':3953,'multiline':False]
['text':' about 10x for running this test.','line_number':3954,'multiline':False]
['text':'','line_number':3955,'multiline':False]
['text':' Enable this if-block when the test method is','line_number':3956,'multiline':False]
['text':' updated, run the test, and finally, disable the','line_number':3957,'multiline':False]
['text':' if-block.','line_number':3958,'multiline':False]
['text':' this will update torch/sparse/_triton_ops_meta.py','line_number':3965,'multiline':False]
['text':' Test non-contiguous input tensors:','line_number':3976,'multiline':False]
['text':' todo: add bsr_dense_linear to the set below (currently,','line_number':3986,'multiline':False]
['text':' nn.linear has unnecessarily restrictive arguments','line_number':3987,'multiline':False]
['text':' checks).','line_number':3988,'multiline':False]
['text':' e.g., TestSparseCSRCPU and TestSparseCSRCUDA','line_number':4044,'multiline':False]
