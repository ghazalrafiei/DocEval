['text':' Owner(s): ["module: unknown"]','line_number':1,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':33,'multiline':False]
['text':' load_tests from torch.testing._internal.common_utils is used to automatically filter tests for','line_number':35,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':36,'multiline':False]
['text':' This runs checkpoint_sequential on each of the nets in','line_number':56,'multiline':False]
['text':' module_lists_to_compare, and compares them against the uncheckpointed model.','line_number':57,'multiline':False]
['text':' To compare, it checks outputs as well as input gradients and parameter gradients','line_number':58,'multiline':False]
['text':' not checkpointed','line_number':67,'multiline':False]
['text':' checkpointed model by passing list of modules','line_number':78,'multiline':False]
['text':' pass list of modules to checkpoint','line_number':82,'multiline':False]
['text':' compare outputs as well as the gradients of input and parameters','line_number':94,'multiline':False]
['text':' Test whether checkpoint is being triggered or not. For this, we check','line_number':100,'multiline':False]
['text':' the number of times forward pass happens','line_number':101,'multiline':False]
['text':' For reentrant, need to have autograd actually','line_number':112,'multiline':False]
['text':' pack a tensor to trigger recomp','line_number':113,'multiline':False]
['text':' checkpointed','line_number':117,'multiline':False]
['text':' checkpointed','line_number':145,'multiline':False]
['text':' works with use_reentrant=False, and grads are the same','line_number':153,'multiline':False]
['text':' check outputs are the same','line_number':159,'multiline':False]
['text':' Compare uncheckpointed model with its checkpointed counterparts','line_number':178,'multiline':False]
['text':' In addition to running checkpoint_sequential on the nn.Sequential','line_number':179,'multiline':False]
['text':' instance, we also run the function on the list of functions within','line_number':180,'multiline':False]
['text':' the module.','line_number':181,'multiline':False]
['text':' Compare uncheckpointed model with its checkpointed counterparts.','line_number':213,'multiline':False]
['text':' type: ignore[call-arg]','line_number':234,'multiline':False]
['text':' type: ignore[call-arg]','line_number':245,'multiline':False]
['text':' This should run without error','line_number':312,'multiline':False]
['text':' Validate running backward.','line_number':347,'multiline':False]
['text':' Reset grads, run without checkpoint and validate we receive same grads.','line_number':356,'multiline':False]
['text':' tensor 2 is used for other application logic','line_number':387,'multiline':False]
['text':' I don't know how to check if the temporary saved variable buffer','line_number':407,'multiline':False]
['text':' get de-allocated directly. So using cuda memory usage as a proxy','line_number':408,'multiline':False]
['text':' Track that at each step of the backward, some Tensor were','line_number':414,'multiline':False]
['text':' de-allocated (which correspond to the checkpoint storage being','line_number':415,'multiline':False]
['text':' emptied at each step)','line_number':416,'multiline':False]
['text':' The main property of this function is that it contains multiple','line_number':430,'multiline':False]
['text':' operations that save gradients in a chain.','line_number':431,'multiline':False]
['text':' In a regular backward, buffers get eagerly freed','line_number':448,'multiline':False]
['text':' In a retain_grad backward, buffers get preserved','line_number':451,'multiline':False]
['text':' In a regular backward with checkpoint, buffers get eagerly freed','line_number':454,'multiline':False]
['text':' In a retain_grad backward with checkpoint, buffers get eagerly freed','line_number':457,'multiline':False]
['text':' self.dataset is a Tensor here; technically not a valid input because','line_number':487,'multiline':False]
['text':' not a Dataset subclass, but needs to stay working so add ignore's','line_number':488,'multiline':False]
['text':' for type checking with mypy','line_number':489,'multiline':False]
['text':' type: ignore[arg-type]','line_number':490,'multiline':False]
['text':' type: ignore[arg-type]','line_number':498,'multiline':False]
['text':' type: ignore[arg-type]','line_number':507,'multiline':False]
['text':' type: ignore[arg-type]','line_number':516,'multiline':False]
['text':' noqa: P204','line_number':534,'multiline':False]
['text':' Check that this fails due to missing args','line_number':556,'multiline':False]
['text':' This should succeed','line_number':560,'multiline':False]
['text':' Up to five lines away from the heading, there should be the version number','line_number':571,'multiline':False]
['text':' This assumes that after the cProfile output section we have','line_number':579,'multiline':False]
['text':' the autograd profiler output','line_number':580,'multiline':False]
['text':' This assumes that after the autograd profiler output is the end of the','line_number':589,'multiline':False]
['text':' output.','line_number':590,'multiline':False]
['text':' Case 1, check the case when len(dims1) < len(dims2) and numel(dims2) > 1','line_number':655,'multiline':False]
['text':' Case 2, check the case when len(dims1) < len(dims2) and numel(dims2) == 1','line_number':660,'multiline':False]
['text':' Case 3, check the case when len(dims1) > len(dims2) and numel(dims2) == 1','line_number':665,'multiline':False]
['text':' Case 4, check the case when len(dims1) > len(dims2) and dims1[x:] == dims2','line_number':670,'multiline':False]
['text':' Case 5, check the case when len(dims1) > len(dims2), but dims1[x:] != dims2','line_number':675,'multiline':False]
['text':' Case 6, check the equal case, no broadcast','line_number':680,'multiline':False]
['text':' Case 7, check the case when len(dims1) == len(dims2), but dims1 != dims2','line_number':685,'multiline':False]
['text':' Case 8, check the case when len(dims1) == len(dims2) and numel(s2) == 1','line_number':690,'multiline':False]
['text':' noqa: F401','line_number':698,'multiline':False]
['text':' verify assertions work as expected','line_number':703,'multiline':False]
['text':' bool argument','line_number':704,'multiline':False]
['text':' tensor argument','line_number':708,'multiline':False]
['text':' scriptable','line_number':720,'multiline':False]
['text':' data can be passed without errors','line_number':722,'multiline':False]
['text':' Windows prints "\r\n" for newlines.','line_number':768,'multiline':False]
['text':' Clean up from test_external_module_register','line_number':810,'multiline':False]
['text':' Built-in module','line_number':817,'multiline':False]
['text':' Wrong device type','line_number':821,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':826,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':830,'multiline':False]
['text':' No supporting for override','line_number':832,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':845,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':852,'multiline':False]
['text':' Only test samples which don't have Tensor inputs.  However,','line_number':913,'multiline':False]
['text':' we don't test the factory property on OpInfo as it is very,','line_number':914,'multiline':False]
['text':' very incomplete','line_number':915,'multiline':False]
['text':' Many OpInfos will explicitly pass in a device.  DeviceContext','line_number':921,'multiline':False]
['text':' will respect device if it is explicitly specified.  To test','line_number':922,'multiline':False]
['text':' DeviceContext, we have to remove the device kwarg in this case.','line_number':923,'multiline':False]
['text':' NB: Can't pass None to sample_inputs, the function can't','line_number':924,'multiline':False]
['text':' handle it.','line_number':925,'multiline':False]
['text':' cached','line_number':983,'multiline':False]
