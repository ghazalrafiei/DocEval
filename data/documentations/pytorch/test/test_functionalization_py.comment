['text':' Owner(s): ["module: codegen"]','line_number':1,'multiline':False]
['text':' We can unify testing and use functionalize() here instead','line_number':25,'multiline':False]
['text':' if/when functorch moves into core.','line_number':26,'multiline':False]
['text':' This is basically a crappy version of `functionalize()`.','line_number':27,'multiline':False]
['text':' Existing deficiency in functionalize():','line_number':52,'multiline':False]
['text':' we don't correctly mutate input metadata (yet?)','line_number':53,'multiline':False]
['text':' Compare outputs (and mutated inputs), with and without functionalization.','line_number':79,'multiline':False]
['text':' The reinplacing pass is only valid to run with reapply_views=True.','line_number':83,'multiline':False]
['text':' NOTE: for now, need to pass in fresh inputs here, because make_fx','line_number':87,'multiline':False]
['text':' will directly mutate the inputs that you trace with.','line_number':88,'multiline':False]
['text':' Once this is fixed we can clean this up.','line_number':89,'multiline':False]
['text':' functionalize() deficiency: input metadata mutations aren't propagated properly,','line_number':92,'multiline':False]
['text':' so we just need to skip checks here for the tests that exercise that.','line_number':93,'multiline':False]
['text':' input mutations should still occur','line_number':99,'multiline':False]
['text':' Handle tests with multi-tensor outputs','line_number':102,'multiline':False]
['text':' y should have been updated.','line_number':121,'multiline':False]
['text':' z should have been updated too.','line_number':123,'multiline':False]
['text':' We should probaby get the crossref test to work,','line_number':155,'multiline':False]
['text':' but fixing it for Storage() objects is annoying.','line_number':156,'multiline':False]
['text':' noqa: B950','line_number':220,'multiline':False]
['text':' simple test: 1 view op, 1 inplace op','line_number':224,'multiline':False]
['text':' the out= tensor will get resized, since it has size=0 to start.','line_number':267,'multiline':False]
['text':' aminmax.out returns a tuple of tensors.','line_number':303,'multiline':False]
['text':' functionalization should properly handle the tuple.','line_number':304,'multiline':False]
['text':' This test requires that *_scatter ops are able to return','line_number':380,'multiline':False]
['text':' non-contiguous tensors.','line_number':381,'multiline':False]
['text':' simple test: 1 view op, 1 inplace op','line_number':392,'multiline':False]
['text':' test for the case where we functionalize an inplace op on the other tensor - not a view.','line_number':402,'multiline':False]
['text':' This is worth checking because the tensor will have an empty ViewMeta stack, which needs to be special cased.','line_number':403,'multiline':False]
['text':' Some ops that are mutable are neither inplace nor out= ops.','line_number':437,'multiline':False]
['text':' They also need special handling.','line_number':438,'multiline':False]
['text':' noqa: B950','line_number':458,'multiline':False]
['text':' Test an op with TensorList input','line_number':482,'multiline':False]
['text':' test: view ops that take a subset of the original tensor (select/diagonal)','line_number':527,'multiline':False]
['text':' simple test: there are pending updates afterwards, which the test syncs manually','line_number':567,'multiline':False]
['text':' There should be no clone in the graph','line_number':599,'multiline':False]
['text':' test: view ops that return multiple tensors (split)','line_number':606,'multiline':False]
['text':' noqa: B950','line_number':638,'multiline':False]
['text':' test: view + inplace op (transpose_)','line_number':642,'multiline':False]
['text':' noqa: B950','line_number':666,'multiline':False]
['text':' test: an operator that takes in a List[Optional[Tensor]] argument','line_number':670,'multiline':False]
['text':' (index_put)','line_number':671,'multiline':False]
['text':' noqa: B950','line_number':692,'multiline':False]
['text':' test: the pass can handle scalar inputs properly','line_number':696,'multiline':False]
['text':' ops like ge_() are allowed to change the dtype of the input.','line_number':724,'multiline':False]
['text':' functionalization should pick up on that.','line_number':725,'multiline':False]
['text':' noqa: B950','line_number':752,'multiline':False]
['text':' This tests that we don't have any unnecessary views in the trace.','line_number':774,'multiline':False]
['text':' If the input wasn't mutated, we don't need to regenerate it,','line_number':775,'multiline':False]
['text':' so there should be a total of 1 op in the output trace.','line_number':776,'multiline':False]
['text':' test: everything','line_number':790,'multiline':False]
['text':' noqa: B950','line_number':855,'multiline':False]
['text':' y and z are aliases inside of the function, and that aliasing relationship should be maintained.','line_number':943,'multiline':False]
['text':' copy_() gets its own test, because it used to be special cased in functionalization.','line_number':948,'multiline':False]
['text':' However, now it works pretty similar to other functional ops','line_number':949,'multiline':False]
['text':' Test 1: copy_() with same dtype and shape','line_number':958,'multiline':False]
['text':' to() is a composite op that noops when the dtype/shape match, so nothing gets logged.','line_number':959,'multiline':False]
['text':' self.assert_functionalization(f, torch.ones(2))','line_number':960,'multiline':False]
['text':' Test 2: copy_() with same dtype, different shape','line_number':993,'multiline':False]
['text':' Test 3: copy_() with different dtype, same shape','line_number':1027,'multiline':False]
['text':' noqa: B950','line_number':1044,'multiline':False]
['text':' noqa: B950','line_number':1059,'multiline':False]
['text':' Test 4: copy_() with different dtype, different shape','line_number':1061,'multiline':False]
['text':' noqa: B950','line_number':1078,'multiline':False]
['text':' noqa: B950','line_number':1093,'multiline':False]
['text':' Once some existing SymInt bugs are ironed out, we should update','line_number':1096,'multiline':False]
['text':' this test to plumb FakeSymbolicTensors through it','line_number':1097,'multiline':False]
['text':' Resizing to a smaller size doesn't affect storage','line_number':1149,'multiline':False]
['text':' noqa: B950','line_number':1183,'multiline':False]
['text':' resizing a tensor to a larger size is only currently allowed','line_number':1221,'multiline':False]
['text':' if the tensor-to-resize is not a view / has no outstanding views.','line_number':1222,'multiline':False]
['text':' See Note [resize_() in functionalization pass]','line_number':1223,'multiline':False]
['text':' Do a mutation to ensure that aliases of the output of resize_()','line_number':1226,'multiline':False]
['text':' propagate mutations correctly.','line_number':1227,'multiline':False]
['text':' I'm using fill_ specifically because I want to guarantee that','line_number':1228,'multiline':False]
['text':' none of the output has uninitialized memory at the end','line_number':1229,'multiline':False]
['text':' (since these tests compare the data output against a reference impl)','line_number':1230,'multiline':False]
['text':' resizing a tensor to a larger size is only currently allowed','line_number':1272,'multiline':False]
['text':' if the tensor-to-resize is not a view / has no outstanding views.','line_number':1273,'multiline':False]
['text':' See Note [resize_() in functionalization pass]','line_number':1274,'multiline':False]
['text':' This should fail','line_number':1275,'multiline':False]
['text':' Create a view of x','line_number':1289,'multiline':False]
['text':' The view, y, gets deallocated at the end of this function','line_number':1292,'multiline':False]
['text':' Calling g(x) should mutate x','line_number':1295,'multiline':False]
['text':' We expect x to be synced here, even though the alias created in g() has been deallocated!','line_number':1297,'multiline':False]
['text':' Make sure that functionalization ran the "+" kernel','line_number':1315,'multiline':False]
['text':' with a functional + non-functional tensor, and wrapped the output appropriately.','line_number':1316,'multiline':False]
['text':' When dealing with mixed functional + non functional tensors,','line_number':1325,'multiline':False]
['text':' normal_tensor.add_(functional_tensor) is not valid','line_number':1326,'multiline':False]
['text':' because normal_tensor would need to be "promoted" to a functional tensor.','line_number':1327,'multiline':False]
['text':' noqa: B950','line_number':1349,'multiline':False]
['text':' On Windows, for instance_norm, the alias_copy's are reordered to come right before they need to be used','line_number':1373,'multiline':False]
['text':' whereas on other platforms, the alias_copy's are before the view_copy's.','line_number':1374,'multiline':False]
['text':' e.g., the alias_copy after the getitem_4 assignment would be moved to be right before the copy assignment.','line_number':1375,'multiline':False]
['text':' noqa: B950','line_number':1411,'multiline':False]
['text':' noqa: B950','line_number':1450,'multiline':False]
['text':' x: (1, 5)','line_number':1455,'multiline':False]
['text':' noqa: B950','line_number':1488,'multiline':False]
['text':' noqa: B950','line_number':1508,'multiline':False]
['text':' This tests our python shims around C++ Functionalization: FunctionalTensor and FunctionalTensorMode','line_number':1510,'multiline':False]
['text':' Note [Disabling Functionalize TLS Above Python Functionalization]','line_number':1520,'multiline':False]
['text':' This UX is pretty annoying (although python functionalization's main customer is AOTAutograd,','line_number':1521,'multiline':False]
['text':' and is not really advertised as a user API).','line_number':1522,'multiline':False]
['text':' We need to explicitly disable functionalization when using python FunctionalTensor and FunctionalTensorMode.','line_number':1523,'multiline':False]
['text':' Why? FunctionalTensor is a wrapper tensor that holds an inner FunctionalTensorWrapper.','line_number':1524,'multiline':False]
['text':' Since the inner tensor has `DispatchKey.Functionalize` in its keyset, then by default,','line_number':1525,'multiline':False]
['text':' our FunctionalTensor will inherit the same keyset.','line_number':1526,'multiline':False]
['text':' We don't have an easy way of directly mutating a tensor's keyset from python,','line_number':1527,'multiline':False]
['text':' so globally disabling functionalization here is easier.','line_number':1528,'multiline':False]
['text':' Make a non-leaf','line_number':1536,'multiline':False]
