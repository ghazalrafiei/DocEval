['text':' Owner(s): ["module: tests"]','line_number':1,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':33,'multiline':False]
['text':' Protects against includes accidentally setting the default dtype','line_number':65,'multiline':False]
['text':' load_tests from torch.testing._internal.common_utils is used to automatically filter tests for','line_number':68,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':69,'multiline':False]
['text':' Tests Vital Signs for Torch','line_number':88,'multiline':False]
['text':' FIXME: document or deprecate whatever this is','line_number':89,'multiline':False]
['text':' This tests the code path of setting a vital','line_number':100,'multiline':False]
['text':' FIXME: document or deprecate whatever this is','line_number':113,'multiline':False]
['text':' TODO: move all tensor creation to common ops','line_number':126,'multiline':False]
['text':' Validates that mathematical constants are defined properly, as required by','line_number':133,'multiline':False]
['text':' the Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html)','line_number':134,'multiline':False]
['text':' Skip quantized dtypes for CUDA, since they're not supported','line_number':201,'multiline':False]
['text':' This is OK, it changes the meta storage size without allocating','line_number':376,'multiline':False]
['text':' Test fix for issue #80733','line_number':381,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/80733','line_number':382,'multiline':False]
['text':' Check that deepcopy preserves sharing','line_number':401,'multiline':False]
['text':' Check that deepcopy preserves attributes','line_number':410,'multiline':False]
['text':' output is identical to input:','line_number':445,'multiline':False]
['text':' output and input are independent:','line_number':447,'multiline':False]
['text':' output partially overlaps with input:','line_number':449,'multiline':False]
['text':' output is transpose of input:','line_number':457,'multiline':False]
['text':' select full dimensionality','line_number':492,'multiline':False]
['text':' select actual dimensions for ops:','line_number':500,'multiline':False]
['text':' larger: full ndims, individual sizes may be reduced','line_number':501,'multiline':False]
['text':' smaller: possibly reduced ndims, sizes may be reduced','line_number':502,'multiline':False]
['text':' no reduced singleton dimension','line_number':508,'multiline':False]
['text':' larger may have reduced singleton dimension','line_number':511,'multiline':False]
['text':' smaller may have reduced singleton dimension','line_number':514,'multiline':False]
['text':' collected tests of ops that used scalar_check in Declarations.cwrap for','line_number':522,'multiline':False]
['text':' correctness','line_number':523,'multiline':False]
['text':' remainder','line_number':528,'multiline':False]
['text':' fmod','line_number':534,'multiline':False]
['text':' exp, cos, cosh, tan, atan, tanh, erf, erfc, reciprocal','line_number':540,'multiline':False]
['text':' clamp','line_number':566,'multiline':False]
['text':' cumsum, cumprod, cummax, cummin','line_number':574,'multiline':False]
['text':' sort, topk','line_number':581,'multiline':False]
['text':' max, min','line_number':587,'multiline':False]
['text':' lshift, rshift','line_number':598,'multiline':False]
['text':' or','line_number':611,'multiline':False]
['text':' and','line_number':618,'multiline':False]
['text':' clone','line_number':625,'multiline':False]
['text':' masked_select','line_number':631,'multiline':False]
['text':' mode','line_number':639,'multiline':False]
['text':' max','line_number':645,'multiline':False]
['text':' amax','line_number':651,'multiline':False]
['text':' min','line_number':657,'multiline':False]
['text':' amin','line_number':663,'multiline':False]
['text':' set_','line_number':669,'multiline':False]
['text':' take','line_number':682,'multiline':False]
['text':' gather','line_number':686,'multiline':False]
['text':' normal','line_number':692,'multiline':False]
['text':' std must be >= 0','line_number':693,'multiline':False]
['text':' documentation says out shape matches shape of mean','line_number':695,'multiline':False]
['text':' TODO: this behavior differs on CPU and GPU, see https://github.com/pytorch/pytorch/issues/30480.','line_number':701,'multiline':False]
['text':' self.assertEqual((), torch.normal(zero_d, one_d).shape)','line_number':702,'multiline':False]
['text':' self.assertEqual((), torch.normal(1, one_d).shape)','line_number':703,'multiline':False]
['text':' convolutions.  Yes, we are testing nn.functional here; seems justified','line_number':705,'multiline':False]
['text':' given its similar to the other tests','line_number':706,'multiline':False]
['text':' nll_loss -- verify input can't be 0-dimensional.','line_number':711,'multiline':False]
['text':' verify output is 0-dimensional when reduction != 'none'','line_number':714,'multiline':False]
['text':' Test that `torch._check_tensor_all` raises errors in the correct cases','line_number':720,'multiline':False]
['text':' cond must be a tensor','line_number':726,'multiline':False]
['text':' cond tensor must be boolean','line_number':730,'multiline':False]
['text':' Should not raise error','line_number':751,'multiline':False]
['text':' Choose a random element to set to false','line_number':759,'multiline':False]
['text':' Test a simple failure message','line_number':766,'multiline':False]
['text':' Test message with tensor','line_number':771,'multiline':False]
['text':' Test format string message','line_number':778,'multiline':False]
['text':' Test that `TORCH_CHECK_TENSOR_ALL` raises errors that propagate from C++ to Python','line_number':785,'multiline':False]
['text':' Should not raise error','line_number':804,'multiline':False]
['text':' Choose a random element to set to false','line_number':812,'multiline':False]
['text':' Uses mismatched arange out size to trigger a warning','line_number':819,'multiline':False]
['text':' Creates long string in advance to avoid a too-long Python line','line_number':823,'multiline':False]
['text':' nvfuser deprecation warning filter','line_number':825,'multiline':False]
['text':' Checks eager-mode cpp warning','line_number':833,'multiline':False]
['text':' Checks for cpp context in the warning message','line_number':839,'multiline':False]
['text':' Checks the Python features of the warning','line_number':843,'multiline':False]
['text':' Note: the eager mode warning refers to the line in the function','line_number':844,'multiline':False]
['text':' that throws the warning.','line_number':845,'multiline':False]
['text':' Checks jitted cpp warning','line_number':849,'multiline':False]
['text':' Checks for cpp context in the warning message','line_number':855,'multiline':False]
['text':' Checks the Python features of the warning','line_number':859,'multiline':False]
['text':' Note: the jitted warning's lineno refers to the call to the jitted','line_number':860,'multiline':False]
['text':' function, which in our test suite has a layer of indirection','line_number':861,'multiline':False]
['text':' that makes checking the Python lineno fragile','line_number':862,'multiline':False]
['text':' Checks jitted Python warning','line_number':865,'multiline':False]
['text':' The jit mimics an eager-mode Python warning in this case','line_number':869,'multiline':False]
['text':' Checks the Python features of the warning','line_number':878,'multiline':False]
['text':' FIXME: move to test_testing','line_number':882,'multiline':False]
['text':' Check that we can catch a TORCH_WARN_ONCE warning twice','line_number':885,'multiline':False]
['text':' since assertWarnsOnceRegex uses set_warn_always(True) which changes','line_number':886,'multiline':False]
['text':' TORCH_WARN_ONCE to TORCH_WARN','line_number':887,'multiline':False]
['text':' OK, got it once, now try again','line_number':893,'multiline':False]
['text':' Make sure emitting two warnings will pass the assertWarnsOnceRegex','line_number':897,'multiline':False]
['text':' context manager','line_number':898,'multiline':False]
['text':' t + 1 allocates a new tensor for result using empty','line_number':937,'multiline':False]
['text':' TODO: this test should be in test_nn.py','line_number':962,'multiline':False]
['text':' 3d','line_number':979,'multiline':False]
['text':' TODO: this test should be in test_nn.py','line_number':985,'multiline':False]
['text':' ConvTranspose3d works for large input tensors (gh-32866)','line_number':989,'multiline':False]
['text':' test that sizes must match','line_number':1018,'multiline':False]
['text':' test that legacy empty size behavior used to be respected (i.e. all','line_number':1024,'multiline':False]
['text':' empty tensors were logically collapsed to size [0]).','line_number':1025,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/72650','line_number':1031,'multiline':False]
['text':' functions with three tensor arguments','line_number':1043,'multiline':False]
['text':' create another smaller tensor','line_number':1056,'multiline':False]
['text':' map and map2 are not implementd on CUDA tensors','line_number':1062,'multiline':False]
['text':' run through tensor versions of functions','line_number':1066,'multiline':False]
['text':' and verify fully expanded inputs give same results','line_number':1067,'multiline':False]
['text':' test various orders','line_number':1086,'multiline':False]
['text':' ignore last iter when small2 is None','line_number':1090,'multiline':False]
['text':' now for torch. versions of functions','line_number':1097,'multiline':False]
['text':' test various orders','line_number':1118,'multiline':False]
['text':' ignore last iter when small2 is None','line_number':1122,'multiline':False]
['text':' now for in place functions','line_number':1127,'multiline':False]
['text':' in-place tensor is not broadcastable; test only guaranteed','line_number':1128,'multiline':False]
['text':' to work by broadcasting other argument(s)','line_number':1129,'multiline':False]
['text':' need to clone largeExpanded so we can reuse, since functions are in-place','line_number':1133,'multiline':False]
['text':' in-place pointwise operations don't actually work if the in-place','line_number':1154,'multiline':False]
['text':' tensor is 0-strided (numpy has the same issue)','line_number':1155,'multiline':False]
['text':' Functionalization converts the inplace to an out-of-place, which causes us to error.','line_number':1174,'multiline':False]
['text':' We should fix this, but "error probably on bad inputs" isn't a hi-pri PT2 item.','line_number':1175,'multiline':False]
['text':' (function, (tensor sizes))','line_number':1193,'multiline':False]
['text':' (CuBLAS workspace config, is deterministic)','line_number':1199,'multiline':False]
['text':' Create processes to test each combination of test cases and config settings','line_number':1213,'multiline':False]
['text':' On Windows, opening the subprocess with the default CWD makes `import torch`','line_number':1250,'multiline':False]
['text':' fail, so just set CWD to this script's directory','line_number':1251,'multiline':False]
['text':' size, stride, resize_size','line_number':1274,'multiline':False]
['text':' If storage size was increased, check that the new section is','line_number':1302,'multiline':False]
['text':' filled with NaN/MAX_INT. Otherwise, check that the storages are','line_number':1303,'multiline':False]
['text':' equal.','line_number':1304,'multiline':False]
['text':' When deterministic algorithms are enabled, `torch.empty` should fill floating','line_number':1325,'multiline':False]
['text':' point tensors with NaN and integer tensors with MAX_INT','line_number':1326,'multiline':False]
['text':' FIXME: update OpInfos to support "nondeterministic samples" and port these tests','line_number':1353,'multiline':False]
['text':'   to that architecture','line_number':1354,'multiline':False]
['text':' size, padding','line_number':1522,'multiline':False]
['text':' Nondeterministic alert should only be raised if the forward call was','line_number':1658,'multiline':False]
['text':' nondeterministic','line_number':1659,'multiline':False]
['text':' If the forward call was deterministic, nondeterministic alert should','line_number':1670,'multiline':False]
['text':' not be raised','line_number':1671,'multiline':False]
['text':' expected a non-determinitic error, but it was not raised','line_number':1743,'multiline':False]
['text':' warn_only=False correctly raises RuntimeError: put_ does not have a deterministic implementation','line_number':1755,'multiline':False]
['text':' warn_only=True logs warning from the FallbackKernel: torch.ops.aten.put_.default, instead of as UserWarning:','line_number':1756,'multiline':False]
['text':' [W Context.cpp:%(lineno)] Warning: put_ does not have a deterministic implementation','line_number':1757,'multiline':False]
['text':' Error should only be raised when device is CUDA and weights are','line_number':1785,'multiline':False]
['text':' given','line_number':1786,'multiline':False]
['text':' Ensures that kthvalue throws nondeterministic alerts in the correct cases','line_number':1797,'multiline':False]
['text':' input, grid','line_number':1852,'multiline':False]
['text':' 3d','line_number':1853,'multiline':False]
['text':' 2d','line_number':1854,'multiline':False]
['text':' Wrapper for the 2d, 3d, and cuDNN functions listed below.','line_number':1867,'multiline':False]
['text':' Expects 2d input.','line_number':1873,'multiline':False]
['text':' Expects 3d input.','line_number':1879,'multiline':False]
['text':' Expects 2d input.','line_number':1885,'multiline':False]
['text':' Expects 2d input, on CUDA.','line_number':1891,'multiline':False]
['text':' Doesn't work on CPU and ROCm.','line_number':1892,'multiline':False]
['text':' Ensures that median throws nondeterministic alerts in the correct cases','line_number':1911,'multiline':False]
['text':' FIXME: move to test_scatter_gather_ops','line_number':1946,'multiline':False]
['text':' FIXME: move to test_scatter_gather_ops','line_number':1972,'multiline':False]
['text':' FIXME: move to test_scatter_gather_ops','line_number':1977,'multiline':False]
['text':' FIXME: move to test_scatter_gather_ops','line_number':1982,'multiline':False]
['text':' Checking if scatter_add is deterministic','line_number':1995,'multiline':False]
['text':' FIXME: move to test_scatter_gather_ops','line_number':2007,'multiline':False]
['text':' taking boolean value of a tensor synchronizes','line_number':2041,'multiline':False]
['text':' prepare inputs for subsequent ops','line_number':2046,'multiline':False]
['text':' exercise single argument function signature','line_number':2093,'multiline':False]
['text':' RngUniform not implemented for Integral type in XLA test','line_number':2138,'multiline':False]
['text':' Need to draw a lot of samples to cover every random floating point number.','line_number':2171,'multiline':False]
['text':' probability of drawing "1" is 0','line_number':2172,'multiline':False]
['text':' probability of drawing "1" is 1','line_number':2176,'multiline':False]
['text':' Tests extremal behavior','line_number':2187,'multiline':False]
['text':' Tests that negative lambda fails','line_number':2191,'multiline':False]
['text':' naively, 0 in exponential can be generated with probability 2^-24','line_number':2198,'multiline':False]
['text':' so we need more samples to check if it's not generated','line_number':2199,'multiline':False]
['text':' instead of doing one','line_number':2200,'multiline':False]
['text':' don't test CPU, that would be a long test','line_number':2201,'multiline':False]
['text':' torch.float16 will have `inf` because of its smaller range.','line_number':2314,'multiline':False]
['text':' Tests extremal behavior','line_number':2326,'multiline':False]
['text':' Tests non-positive rate fails','line_number':2330,'multiline':False]
['text':' FIXME: find test suite for pdist and cdist','line_number':2348,'multiline':False]
['text':' Maybe merge into OpInfo?','line_number':2532,'multiline':False]
['text':' to avoid extremum','line_number':2540,'multiline':False]
['text':' Do a backward pass to check that it is valid for large','line_number':2545,'multiline':False]
['text':' matrices','line_number':2546,'multiline':False]
['text':' Ensure that cdist backward with p<1 does not produce NaNs','line_number':2552,'multiline':False]
['text':' Test to detect issues in cdist gradient calculation','line_number':2566,'multiline':False]
['text':' When the distances are 0','line_number':2567,'multiline':False]
['text':' Check that the backward passs does not contain invalid','line_number':2577,'multiline':False]
['text':' values such as nan or inf','line_number':2578,'multiline':False]
['text':' Check that cummulative sum over a zero length dimension doesn't crash on backprop.','line_number':2609,'multiline':False]
['text':' Also check that cumsum over other dimensions in a tensor with a zero-length','line_number':2610,'multiline':False]
['text':' dimensiuon also works','line_number':2611,'multiline':False]
['text':' Also include a basic suite of similar tests for other bases cases.','line_number':2612,'multiline':False]
['text':' Check that backward does not crash','line_number':2618,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2620,'multiline':False]
['text':' Check a scalar example','line_number':2623,'multiline':False]
['text':' Check that backward does not crash','line_number':2627,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2629,'multiline':False]
['text':' Check that cummulative prod over a zero length dimension doesn't crash on backprop.','line_number':2661,'multiline':False]
['text':' Also check that cumprod over other dimensions in a tensor with a zero-length','line_number':2662,'multiline':False]
['text':' dimensiuon also works','line_number':2663,'multiline':False]
['text':' Also include a basic suite of similar tests for other bases cases.','line_number':2664,'multiline':False]
['text':' Check that backward does not crash','line_number':2670,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2672,'multiline':False]
['text':' Check a scalar example','line_number':2675,'multiline':False]
['text':' Check that backward does not crash','line_number':2679,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2681,'multiline':False]
['text':' test inf and nan input','line_number':2704,'multiline':False]
['text':' op shouldn't support values, indices with a dtype, device type or layout','line_number':2709,'multiline':False]
['text':' different from that of input tensor','line_number':2710,'multiline':False]
['text':' Check that op over a zero length dimension doesn't crash on backprop.','line_number':2719,'multiline':False]
['text':' Also check that op over other dimensions in a tensor with a zero-length','line_number':2720,'multiline':False]
['text':' dimension also works','line_number':2721,'multiline':False]
['text':' Also include a basic suite of similar tests for other bases cases.','line_number':2722,'multiline':False]
['text':' Check that backward does not crash','line_number':2728,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2730,'multiline':False]
['text':' Check a scalar example','line_number':2733,'multiline':False]
['text':' Check that backward does not crash','line_number':2736,'multiline':False]
['text':' Check that output maintained correct shape','line_number':2738,'multiline':False]
['text':' check -inf and nan handling','line_number':2765,'multiline':False]
['text':' Check that out is actually inplace','line_number':2775,'multiline':False]
['text':' Check input and inplace_output type mismatch','line_number':2784,'multiline':False]
['text':' Helper for test_diff to compare with NumPy reference implementation','line_number':2793,'multiline':False]
['text':' test when no prepend and append','line_number':2805,'multiline':False]
['text':' test when prepend and append's size along dim is 1','line_number':2811,'multiline':False]
['text':' test when prepend and append's size along dim != 1','line_number':2817,'multiline':False]
['text':' All tensors appear contiguous on XLA','line_number':2823,'multiline':False]
['text':' RngNormal not implemented for type f16 for XLA','line_number':2844,'multiline':False]
['text':' if the given input arg is not a list, it returns a list of single element: [arg]','line_number':2877,'multiline':False]
['text':' To ensure inf, -inf, and nan values do not cause divergence between Numpy and PyTorch.','line_number':2881,'multiline':False]
['text':' There are two types of possible divergence:','line_number':2882,'multiline':False]
['text':' 1. When we compute a,b both real numbers and has very small absolute values (i.e. very near to 0.0)','line_number':2883,'multiline':False]
['text':' then, result of a/b be inf, -inf and nan, and this cause divergence.','line_number':2884,'multiline':False]
['text':' 2. When we are dividing complex numbers by zero. For example, when a = torch.tensor(3+5j) we have','line_number':2885,'multiline':False]
['text':' a/0 to be equal to nan + nan*j in PyTorch and inf + inf*j in Numpy.','line_number':2886,'multiline':False]
['text':' nan_to_num is not defined for complex tensors in PyTorch.','line_number':2890,'multiline':False]
['text':' shape, dims format','line_number':2920,'multiline':False]
['text':' filter shape by dims before passing filtered shape to create_* functions','line_number':2938,'multiline':False]
['text':' Test behaviour for inf and nan values','line_number':2956,'multiline':False]
['text':' Test behaviour in very big tensors','line_number':2961,'multiline':False]
['text':' Type promotion fails on Numpy when spacing is given as complex number and input is given as real.','line_number':3013,'multiline':False]
['text':' Result is given just as real number and all the imaginary parts to be equal to zero.','line_number':3014,'multiline':False]
['text':' Avoid self.assertEqual to save memory.','line_number':3050,'multiline':False]
['text':' only small dtype not to get oom','line_number':3056,'multiline':False]
['text':' initialization to avoid overflow and half caveats','line_number':3060,'multiline':False]
['text':' only small dtype not to get oom','line_number':3068,'multiline':False]
['text':' initialization to avoid overflow and half caveats','line_number':3073,'multiline':False]
['text':' TODO: Fix this. It reproduces with aot_eager too, and looks like a functionalization bug.','line_number':3097,'multiline':False]
['text':' (the problematic case seems rare, as we're calling an out= op directly from user code,','line_number':3098,'multiline':False]
['text':' where the passed-in out tensors are non-contiguous).','line_number':3099,'multiline':False]
['text':' FIXME: move to shape ops test suite','line_number':3126,'multiline':False]
['text':' FIXME: move to shape ops test suite','line_number':3137,'multiline':False]
['text':' unfold on a 0-dimensional tensor should always return a 1-d dimensional','line_number':3140,'multiline':False]
['text':' tensor of shape [size] (i.e., the second parameter to unfold)','line_number':3141,'multiline':False]
['text':' FIXME: move to data movement test suite','line_number':3147,'multiline':False]
['text':' copy is a shallow copy, only copies the tensor view,','line_number':3155,'multiline':False]
['text':' not the data','line_number':3156,'multiline':False]
['text':' FIXME: move to data movement test suite','line_number':3186,'multiline':False]
['text':' issue: https://github.com/pytorch/pytorch/issues/106051','line_number':3210,'multiline':False]
['text':' FIXME: move to data movement test suite','line_number':3227,'multiline':False]
['text':' stride zero, size 1 axis, not contiguous','line_number':3256,'multiline':False]
['text':' github issue: https://github.com/pytorch/pytorch/issues/64176','line_number':3262,'multiline':False]
['text':' should retain permutation after densification','line_number':3265,'multiline':False]
['text':' FIXME: move to elementwise ternary test suite','line_number':3268,'multiline':False]
['text':' Returns floating or integral scalar corresponding to dtype','line_number':3272,'multiline':False]
['text':' FIXME: move to shape ops test suite','line_number':3311,'multiline':False]
['text':' see https://github.com/pytorch/pytorch/issues/91690.','line_number':3321,'multiline':False]
['text':' FIXME: move to indexing test suite','line_number':3327,'multiline':False]
['text':' fill rows in idx with reduction inits if include_self=False','line_number':3351,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3377,'multiline':False]
['text':' We just test for num_copy <= num_dest, as otherwise there are repeated indices','line_number':3380,'multiline':False]
['text':' and the behavior is undefined','line_number':3381,'multiline':False]
['text':' More thorough testing as in index_add','line_number':3394,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3409,'multiline':False]
['text':' onlyNativeDeviceTypes due to an XLA error:','line_number':3410,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/53256','line_number':3411,'multiline':False]
['text':' Create the 8 possible combinations of scalar sizes for target / index / source','line_number':3415,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3424,'multiline':False]
['text':' We do not test the GPU as the CUDA_ASSERT would break the CUDA context','line_number':3427,'multiline':False]
['text':' Too large of an index','line_number':3432,'multiline':False]
['text':' Too small (negative indices)','line_number':3439,'multiline':False]
['text':' Too small (very negative indices) - they should be unsupported even','line_number':3444,'multiline':False]
['text':' when support for negative indices is implemented for index_copy_','line_number':3445,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3464,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3484,'multiline':False]
['text':' on CPU it should be deterministic regardless of the deterministic mode','line_number':3490,'multiline':False]
['text':' FIXME: find a test suite for the put operator','line_number':3502,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3522,'multiline':False]
['text':' Make sure that the result stays 0-dim while applied to','line_number':3533,'multiline':False]
['text':' a 0-dim input','line_number':3534,'multiline':False]
['text':' FIXME: move to test indexing','line_number':3539,'multiline':False]
['text':' The test fails for zero-dimensional tensors on XLA','line_number':3540,'multiline':False]
['text':' bfloat16 is just used on GPU, so it's not supported on numpy','line_number':3551,'multiline':False]
['text':' Create the 4 possible combinations of scalar sizes for index / source','line_number':3579,'multiline':False]
['text':' FIXME: find a test suite for the take operator','line_number':3587,'multiline':False]
['text':' Create the 4 possible combinations of scalar sizes for source / index','line_number':3613,'multiline':False]
['text':' FIXME: find a test suite for the put operator','line_number':3620,'multiline':False]
['text':' The bool instance does not work on GPU. See','line_number':3621,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/54317','line_number':3622,'multiline':False]
['text':' If accumulate=True, `put_` should be deterministic regardless of the inputs on CPU','line_number':3642,'multiline':False]
['text':' On CUDA it may not be, but the test has enough tolerance to account for this','line_number':3643,'multiline':False]
['text':' out-place','line_number':3653,'multiline':False]
['text':' in-place','line_number':3657,'multiline':False]
['text':' Create the 8 possible combinations of scalar sizes for target / index / source','line_number':3662,'multiline':False]
['text':' out-place','line_number':3669,'multiline':False]
['text':' in-place','line_number':3671,'multiline':False]
['text':' Empty case','line_number':3680,'multiline':False]
['text':' FIXME: find a test suite for the put operator','line_number':3691,'multiline':False]
['text':' The bool instance does not work on GPU. See','line_number':3692,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/54317','line_number':3693,'multiline':False]
['text':' Test for parallel adds with accumulate == True','line_number':3696,'multiline':False]
['text':' Less numbers to avoid overflow with low_precision','line_number':3698,'multiline':False]
['text':' Grainsize is 3000 for the for_loop to be parallized on CPU','line_number':3699,'multiline':False]
['text':' Bfloat16 has a particularly bad performance here','line_number':3701,'multiline':False]
['text':' This operation is nondeterministic on GPU, so we are generous with the rtol','line_number':3702,'multiline':False]
['text':' Dump everything into the 0-th position','line_number':3706,'multiline':False]
['text':' FIXME: find a test suite for the take operator','line_number':3715,'multiline':False]
['text':' FIXME: find a test suite for the put operator','line_number':3724,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3734,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3791,'multiline':False]
['text':' TODO: remove this after scatter_add_ is deprecated.','line_number':3792,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3839,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3850,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3861,'multiline':False]
['text':' FIXME: port to test_scatter_gather_ops.py','line_number':3870,'multiline':False]
['text':' FIXME: find a test suite for the masked scatter operator','line_number':3879,'multiline':False]
['text':' Bound checking in CUDA is done inside a kernel','line_number':3905,'multiline':False]
['text':' in order to avoid synchronization, but this means','line_number':3906,'multiline':False]
['text':' we can not clear the failures. So there is no way','line_number':3907,'multiline':False]
['text':' to test it then recover.','line_number':3908,'multiline':False]
['text':' make src smaller. this should fail','line_number':3910,'multiline':False]
['text':' empty tensor','line_number':3915,'multiline':False]
['text':' FIXME: find a test suite for the masked scatter operator','line_number':3926,'multiline':False]
['text':' FIXME: find a test suite for the masked scatter operator','line_number':3940,'multiline':False]
['text':'   test_scatter_gather_ops or test_masked_ops?','line_number':3941,'multiline':False]
['text':' FIXME: find a test suite for the masked select operator','line_number':3951,'multiline':False]
['text':' Since half on CPU is not supported, need to skip the remaining test cases','line_number':3979,'multiline':False]
['text':' Ensure that masks are expanded to match tensor properly','line_number':3983,'multiline':False]
['text':' Ensure that tensor is expanded to match mask properly','line_number':3995,'multiline':False]
['text':' FIXME: find a test suite for the masked select operator','line_number':4001,'multiline':False]
['text':' FIXME: find a test suite for the masked fill operator','line_number':4020,'multiline':False]
['text':' test non-contiguous case','line_number':4043,'multiline':False]
['text':' FIXME: find a test suite for the masked fill operator','line_number':4056,'multiline':False]
['text':' flatten','line_number':4069,'multiline':False]
['text':' squeeze, unsqueeze','line_number':4074,'multiline':False]
['text':' transpose, t','line_number':4079,'multiline':False]
['text':' select','line_number':4084,'multiline':False]
['text':' repeat, permute','line_number':4087,'multiline':False]
['text':' diagonal, diagflat','line_number':4091,'multiline':False]
['text':' off the end offsets are valid','line_number':4094,'multiline':False]
['text':' check non-zero sized offsets off the end','line_number':4097,'multiline':False]
['text':' stack, split, chunk','line_number':4106,'multiline':False]
['text':' NOTE: split_with_sizes behaves differently than NumPy in that it','line_number':4114,'multiline':False]
['text':' takes sizes rather than offsets','line_number':4115,'multiline':False]
['text':' This is strange because the split size is larger than the dim size, but consistent with','line_number':4120,'multiline':False]
['text':' how split handles that case generally (when no 0s are involved).','line_number':4121,'multiline':False]
['text':' functions that operate over a dimension but don't reduce.','line_number':4125,'multiline':False]
['text':' size stride','line_number':4130,'multiline':False]
['text':' softmax, logsoftmax','line_number':4139,'multiline':False]
['text':' cumsum, cumprod, cummax, cummin','line_number':4148,'multiline':False]
['text':' flip','line_number':4160,'multiline':False]
['text':' roll','line_number':4164,'multiline':False]
['text':' unbind','line_number':4170,'multiline':False]
['text':' cross','line_number':4175,'multiline':False]
['text':' renorm','line_number':4179,'multiline':False]
['text':' sort','line_number':4183,'multiline':False]
['text':' topk','line_number':4187,'multiline':False]
['text':' gather','line_number':4194,'multiline':False]
['text':' scatter, scatter_add','line_number':4205,'multiline':False]
['text':' index_fill, index_copy, index_add','line_number':4218,'multiline':False]
['text':' index fill/copy/add non-empty','line_number':4242,'multiline':False]
['text':' index_select','line_number':4250,'multiline':False]
['text':' non-empty','line_number':4254,'multiline':False]
['text':' FIXME: find a test suite for the pdist operator','line_number':4281,'multiline':False]
['text':' use dim0>=46342 for forward, see:','line_number':4288,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/30583','line_number':4289,'multiline':False]
['text':' Compare output using GPU with the CPU implementation','line_number':4290,'multiline':False]
['text':' 50k * 4 bytes = 200 KB','line_number':4291,'multiline':False]
['text':' Will require 1249975000 float32s','line_number':4292,'multiline':False]
['text':' ~1250M * 4 bytes = 5 GB on CPU','line_number':4293,'multiline':False]
['text':' 5 GB on GPU + 5GB on CPU','line_number':4294,'multiline':False]
['text':' Workaround for large memory overhead of self.assertTrue (see #84944)','line_number':4295,'multiline':False]
['text':' ~20GB in allclose','line_number':4296,'multiline':False]
['text':' FIXME: move to elementwise ternary test suite','line_number':4298,'multiline':False]
['text':' Returns floating or integral scalar corresponding to dtype','line_number':4303,'multiline':False]
['text':' Integer division with addcdiv is prohibited','line_number':4336,'multiline':False]
['text':' FIXME: move to an elementwise ternary test suite and make this an OpInfo test','line_number':4365,'multiline':False]
['text':' RuntimeError not raised','line_number':4392,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4403,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4404,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4420,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4421,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4437,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4438,'multiline':False]
['text':' Warning not triggered','line_number':4439,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4452,'multiline':False]
['text':' RuntimeError not raised','line_number':4453,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4462,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors)','line_number':4463,'multiline':False]
['text':' RuntimeError not raised','line_number':4464,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4477,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4478,'multiline':False]
['text':' RuntimeError not raised','line_number':4479,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4499,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4500,'multiline':False]
['text':' UserWarning not triggered','line_number':4501,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4521,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4522,'multiline':False]
['text':' UserWarning not triggered','line_number':4523,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4538,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4539,'multiline':False]
['text':' RuntimeError not raised','line_number':4540,'multiline':False]
['text':' FIXME: convert to ErrorInputs','line_number':4550,'multiline':False]
['text':' (but have to extend ErrorInputs to handle inplace-only errors!)','line_number':4551,'multiline':False]
['text':' FIXME: move to test distributions','line_number':4565,'multiline':False]
['text':' FIXME: move to test distributions','line_number':4574,'multiline':False]
['text':' FIXME: convert this to an automated OpInfo test','line_number':4584,'multiline':False]
['text':' verify that all operators with `device_guard: False` behave properly with multiple devices.','line_number':4588,'multiline':False]
['text':' TODO: if we had operator introspection we could figure out this set of operators automatically...','line_number':4589,'multiline':False]
['text':' property ops','line_number':4594,'multiline':False]
['text':' sparse property ops','line_number':4608,'multiline':False]
['text':' in-place ops','line_number':4622,'multiline':False]
['text':' shape modification','line_number':4638,'multiline':False]
['text':' chunk, split, etc.','line_number':4654,'multiline':False]
['text':' to','line_number':4668,'multiline':False]
['text':' Note - reports a leak of 512 bytes on CUDA device 1','line_number':4691,'multiline':False]
['text':' FIXME: move to test_serialization','line_number':4704,'multiline':False]
['text':' Note: Tests works with one but prefers more devices','line_number':4706,'multiline':False]
['text':' FIXME: move memory format tests to their own test class/suite','line_number':4726,'multiline':False]
['text':' test cases when strides matter in ambiguous tensors','line_number':4769,'multiline':False]
['text':' FIXME: make this a elementwise unary and elementwise binary OpInfo test','line_number':4822,'multiline':False]
['text':' FIXME: make this a elementwise unary and elementwise binary OpInfo test','line_number':4987,'multiline':False]
['text':' we produce memory dense outputs, so when input is strided on the last dimension','line_number':4996,'multiline':False]
['text':' we need to divide by that dimension stride to compare input and result strides','line_number':4997,'multiline':False]
['text':' torch.eq by default calls TensorIterator with defined output, torch.add with undefined','line_number':5020,'multiline':False]
['text':' memory dense, sliced and ambiguous sliced (ambiguous dense loses permutation information)','line_number':5023,'multiline':False]
['text':' torch.randint(3, 5, **kwargs), // unsupported','line_number':5052,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5075,'multiline':False]
['text':' num dim = 2','line_number':5090,'multiline':False]
['text':' sanity check','line_number':5098,'multiline':False]
['text':' with replacement','line_number':5102,'multiline':False]
['text':' indices that shouldn't be sampled (<0 means none)','line_number':5106,'multiline':False]
['text':' without replacement','line_number':5123,'multiline':False]
['text':' indices that shouldn't be sampled (<0 means none)','line_number':5127,'multiline':False]
['text':' vector','line_number':5147,'multiline':False]
['text':' index that shouldn't be sampled','line_number':5150,'multiline':False]
['text':' CUDA misalignment issue (#46702)','line_number':5161,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5169,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5191,'multiline':False]
['text':' expect no more than 1 repeating elements generated in 2 attempts','line_number':5201,'multiline':False]
['text':' the probability of at least element being repeated is surprisingly large, 18%','line_number':5202,'multiline':False]
['text':' expect no more than 1 repeating elements generated in 2 attempts','line_number':5207,'multiline':False]
['text':' xc is a channels last tensor','line_number':5215,'multiline':False]
['text':' xc is not memory dense, but looks like channels last','line_number':5217,'multiline':False]
['text':' We don't preserve non-dense striding','line_number':5218,'multiline':False]
['text':' TODO copy _like constructors to stride permutation instead of just layout','line_number':5255,'multiline':False]
['text':' Test 'float' separately to avoid float->float no-op.','line_number':5367,'multiline':False]
['text':' FIXME: move to test_serialization','line_number':5395,'multiline':False]
['text':' This test is not in test_cuda.py because it should pass in 3 cases:','line_number':5397,'multiline':False]
['text':'  1. cuda is not available.','line_number':5398,'multiline':False]
['text':'  2. cuda is available but device is not cuda.','line_number':5399,'multiline':False]
['text':'  3. cuda is available and device is cuda.','line_number':5400,'multiline':False]
['text':' In case 1, a and b disable themselves on construction and shouldn't try to pickle workhorse attributes.','line_number':5401,'multiline':False]
['text':' In case 2, a and b are enabled.  Workhorse attributes participate in pickling, but none are lazy-inited','line_number':5402,'multiline':False]
['text':' to cuda Tensors, because I don't want to do cuda things if device is not cuda.','line_number':5403,'multiline':False]
['text':' In case 3, a and b are enabled and we may also try lazy-initing _scale to a cuda tensor.','line_number':5404,'multiline':False]
['text':' Dummy a.scale() call lazy-inits a._scale Tensor.','line_number':5411,'multiline':False]
['text':' The following three lines should work whether or not cuda is available.','line_number':5414,'multiline':False]
['text':' supplies a dummy key to test the defaultdict's default_factory','line_number':5424,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5430,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5437,'multiline':False]
['text':' FIXME: move to test distributions','line_number':5442,'multiline':False]
['text':' num dim = 2','line_number':5461,'multiline':False]
['text':' sanity check','line_number':5469,'multiline':False]
['text':' FIXME: move to elementwise ternary test suite','line_number':5472,'multiline':False]
['text':' As the test fails with Runtime Error not raised on XLA','line_number':5473,'multiline':False]
['text':' Tests ScalarxScalar, ScalarxTensor and TensorxScalar','line_number':5476,'multiline':False]
['text':' variant of `where` against NumPy version with','line_number':5477,'multiline':False]
['text':' handcrafted values.','line_number':5478,'multiline':False]
['text':' Use different values for `x` and `y`','line_number':5491,'multiline':False]
['text':' as they are the output values which are compared.','line_number':5492,'multiline':False]
['text':' NumPy aggressively promotes to double, hence cast to output to correct dtype','line_number':5505,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/58354','line_number':5524,'multiline':False]
['text':' First call to backward','line_number':5538,'multiline':False]
['text':' Second call to backward','line_number':5542,'multiline':False]
['text':' After removing the hook, make sure the usual gradient is returned','line_number':5545,'multiline':False]
['text':' FIXME: get PyTorch/XLA to run test_testing','line_number':5553,'multiline':False]
['text':' This test should ideally be in test_testing.py,','line_number':5554,'multiline':False]
['text':' but since pytorch/xla runs tests from test_torch.py, we have it here.','line_number':5555,'multiline':False]
['text':' Should not reach here!','line_number':5559,'multiline':False]
['text':' FIXME: get PyTorch/XLA to run test_testing','line_number':5562,'multiline':False]
['text':' This test should ideally be in test_testing.py,','line_number':5563,'multiline':False]
['text':' but since pytorch/xla runs tests from test_torch.py, we have it here.','line_number':5564,'multiline':False]
['text':' FIXME: get PyTorch/XLA to run test_testing','line_number':5570,'multiline':False]
['text':' This test should ideally be in test_testing.py,','line_number':5571,'multiline':False]
['text':' but since pytorch/xla runs tests from test_torch.py, we have it here.','line_number':5572,'multiline':False]
['text':' Verify that self.assertRaisesRegex only checks the Error and ignores','line_number':5574,'multiline':False]
['text':' message for non-native devices.','line_number':5575,'multiline':False]
['text':' XLA raises RuntimeError with a different message.','line_number':5581,'multiline':False]
['text':' torch.can_cast(torch.int16, torch.uint8) returns True','line_number':5588,'multiline':False]
['text':' which isn't actually safe-cast.','line_number':5589,'multiline':False]
['text':' This function returns False in this case.','line_number':5590,'multiline':False]
['text':' Make tensor does not support generating','line_number':5600,'multiline':False]
['text':' complex32 tensor','line_number':5601,'multiline':False]
['text':' source is always converted to contiguous by the op.','line_number':5643,'multiline':False]
['text':' t: contig, mask: contig','line_number':5646,'multiline':False]
['text':' t: non-contig, mask: non-contig','line_number':5649,'multiline':False]
['text':' t: contig, mask: non-contig','line_number':5653,'multiline':False]
['text':' t: non-contig, mask: contig','line_number':5657,'multiline':False]
['text':' Tests that compare a device's computation with the (gold-standard) CPU's.','line_number':5662,'multiline':False]
['text':' FIXME: move to indexing test suite','line_number':5666,'multiline':False]
['text':' FIXME: move to serialization test suite','line_number':5681,'multiline':False]
['text':' FIXME: move to serialization test suite','line_number':5694,'multiline':False]
['text':' FIXME: move to data movement test suite','line_number':5710,'multiline':False]
['text':' FIXME: moved to indexing test suite','line_number':5745,'multiline':False]
['text':' test getitem','line_number':5749,'multiline':False]
['text':' test setitem','line_number':5753,'multiline':False]
['text':' Index device tensor with cpu tensor','line_number':5767,'multiline':False]
['text':' Index device tensor with mixed cpu, device tensors','line_number':5773,'multiline':False]
['text':' test getitem','line_number':5782,'multiline':False]
['text':' Index cpu tensor with device tensor','line_number':5790,'multiline':False]
['text':' Index cpu tensor with mixed cpu, device tensors','line_number':5796,'multiline':False]
['text':' Index device tensor with mixed cpu, device tensors on different devices','line_number':5805,'multiline':False]
['text':' FIXME: move to data movement test suite','line_number':5811,'multiline':False]
['text':' FIXME: move to an elementwise ternary test suite','line_number':5823,'multiline':False]
['text':' shape','line_number':5828,'multiline':False]
['text':' non-contiguous','line_number':5829,'multiline':False]
['text':' Test broadcasting min & max','line_number':5856,'multiline':False]
['text':' Test broadcasting x','line_number':5861,'multiline':False]
['text':' we implemented custom deallocation for subclasses, so it behooves','line_number':5871,'multiline':False]
['text':' us to make sure all of these bits work.  We'll use __del__ to','line_number':5872,'multiline':False]
['text':' track if objects die or not','line_number':5873,'multiline':False]
['text':' 1D Tensor Tests','line_number':5911,'multiline':False]
['text':' 2D Tensor Tests','line_number':5931,'multiline':False]
['text':' This relies on __index__() being correct - but we have separate tests for that','line_number':5969,'multiline':False]
['text':' Test `torch._check*` functions','line_number':6005,'multiline':False]
['text':' check function, expected error','line_number':6008,'multiline':False]
['text':' cond=True should not raise an error','line_number':6017,'multiline':False]
['text':' Test default failure message for cond=False','line_number':6020,'multiline':False]
['text':' Test a simple failure message','line_number':6025,'multiline':False]
['text':' Test message with tensor','line_number':6030,'multiline':False]
['text':' Test format string message','line_number':6037,'multiline':False]
['text':' Test incorrect `cond` arg type','line_number':6044,'multiline':False]
['text':' FIXME: move to indexing test suite','line_number':6051,'multiline':False]
['text':' index_add_ without alpha argument','line_number':6067,'multiline':False]
['text':' index_add_ with alpha argument','line_number':6073,'multiline':False]
['text':' FIXME: resolve comment below and move this to indexing test suite','line_number':6080,'multiline':False]
['text':' add coverage for issue with atomic add that appeared only for','line_number':6081,'multiline':False]
['text':' specific dtypes on cuda:','line_number':6082,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/29153','line_number':6083,'multiline':False]
['text':' index_add calls atomicAdd on cuda.','line_number':6096,'multiline':False]
['text':' Check whether index_add can get correct result when','line_number':6108,'multiline':False]
['text':' alpha is 1, and dtype of index is torch.long,','line_number':6109,'multiline':False]
['text':' i.e., using scatter_add','line_number':6110,'multiline':False]
['text':' scatter_add uses fp32 as accumulate type, while index_add doesn't.','line_number':6128,'multiline':False]
['text':' Check bound','line_number':6137,'multiline':False]
['text':' Ensure the output does not require grad regardless of inputs requiring gard or not.','line_number':6157,'multiline':False]
['text':' The output of factory functions should not be part of any computational graph.','line_number':6158,'multiline':False]
['text':' FIXME: move to shape ops test suite','line_number':6180,'multiline':False]
['text':' test args: tensor, int, sizes','line_number':6182,'multiline':False]
['text':' test invalid args: tensor, str, sizes','line_number':6200,'multiline':False]
['text':' test invalid args: tensor, str, namedshape','line_number':6204,'multiline':False]
['text':' test other invalid arguments','line_number':6208,'multiline':False]
['text':' Test that warnings generated from C++ are translated to the correct type','line_number':6224,'multiline':False]
['text':' function, warning type, message','line_number':6227,'multiline':False]
['text':' test argument names','line_number':6303,'multiline':False]
['text':' 1. case when source is tensor','line_number':6305,'multiline':False]
['text':' 2. case when source is storage','line_number':6308,'multiline':False]
['text':' 3. case when source is storage, and other args also specified','line_number':6311,'multiline':False]
['text':' change dtype','line_number':6325,'multiline':False]
['text':' change device','line_number':6331,'multiline':False]
['text':' cpu -> cuda','line_number':6335,'multiline':False]
['text':' cuda -> cpu','line_number':6341,'multiline':False]
['text':' FIXME: move this test test_testing.py (along with allclose testing)','line_number':6347,'multiline':False]
['text':' NOTE: test_equal will be deprecated in favor of torch.testing.assert_close','line_number':6348,'multiline':False]
['text':'   once torch.testing is out of beta','line_number':6349,'multiline':False]
['text':' Contiguous, 1D','line_number':6356,'multiline':False]
['text':' Non contiguous, 2D','line_number':6371,'multiline':False]
['text':' Different dtypes','line_number':6386,'multiline':False]
['text':' Fast path test: tensor flags, like neg and conj','line_number':6393,'multiline':False]
['text':' FIXME: Disable the following check due to the inductor failure','line_number':6402,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/100340 and','line_number':6403,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/98175','line_number':6404,'multiline':False]
['text':' FIXME: Disable the following check due to the inductor failure','line_number':6416,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/100340 and','line_number':6417,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/98175','line_number':6418,'multiline':False]
['text':' Fast path test: two tensors share the same storage, but different dtype','line_number':6422,'multiline':False]
['text':' Fast path test: two tensors share the same storage, but different strides','line_number':6431,'multiline':False]
['text':' Fast path: tensor containing `nan` is not equal to self','line_number':6440,'multiline':False]
['text':' These tests are portable, not necessarily strict for your system.','line_number':6493,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/37703','line_number':6597,'multiline':False]
['text':' test default generators are equal','line_number':6604,'multiline':False]
['text':' tests Generator API','line_number':6607,'multiline':False]
['text':' manual_seed, seed, initial_seed, get_state, set_state','line_number':6608,'multiline':False]
['text':' theoretical values from Joe Kuo 2010','line_number':6639,'multiline':False]
['text':' theoretical values unknown: convergence properties checked','line_number':6653,'multiline':False]
['text':' alternate fast forwarding with sampling','line_number':6727,'multiline':False]
['text':' resampling still having N=2**n','line_number':6765,'multiline':False]
['text':' accepts integer arguments','line_number':6789,'multiline':False]
['text':' doesn't accept floating point variables','line_number':6792,'multiline':False]
['text':' accepts floating point and integer arguments','line_number':6796,'multiline':False]
['text':' accepts floating point and integer tensors','line_number':6801,'multiline':False]
['text':' doesn't accept variables with requires_grad','line_number':6804,'multiline':False]
['text':'  parse with integer variables','line_number':6809,'multiline':False]
['text':' parse with numpy integers','line_number':6812,'multiline':False]
['text':' fail parse with float variables','line_number':6818,'multiline':False]
['text':' fail parse with numpy floats','line_number':6820,'multiline':False]
['text':' fail parse with > 1 element variables','line_number':6824,'multiline':False]
['text':' fail parse with additional positional args after intlist arg','line_number':6830,'multiline':False]
['text':' Legacy <type>Storage constructor errors','line_number':6894,'multiline':False]
['text':' TypedStorage constructor errors','line_number':6936,'multiline':False]
['text':' Test that internal versions of functions related to TypedStorage do not','line_number':7126,'multiline':False]
['text':' produce a deprecation warning','line_number':7127,'multiline':False]
['text':' Check that each of the TypedStorage internal function calls do not','line_number':7188,'multiline':False]
['text':' produce a deprecation warning','line_number':7189,'multiline':False]
['text':' Test that public functions related to TypedStorage produce a deprecation','line_number':7195,'multiline':False]
['text':' warning','line_number':7196,'multiline':False]
['text':' Check that each of the TypedStorage function calls produce a warning','line_number':7222,'multiline':False]
['text':' if warnings are reset between each','line_number':7223,'multiline':False]
['text':' Test that only the first warning is raised by default','line_number':7236,'multiline':False]
['text':' Check the line of code from the warning's stack','line_number':7247,'multiline':False]
['text':' Check that warnings are not emitted if it happened in the past','line_number':7252,'multiline':False]
['text':' check mapping','line_number':7266,'multiline':False]
['text':' check changes to t1 from t2','line_number':7271,'multiline':False]
['text':' check changes to t2 from t1','line_number':7276,'multiline':False]
['text':' release the tensors','line_number':7281,'multiline':False]
['text':' check mapping','line_number':7297,'multiline':False]
['text':' check changes to t1 from t2','line_number':7302,'multiline':False]
['text':' check changes to t2 from t1','line_number':7307,'multiline':False]
['text':' release the tensors','line_number':7312,'multiline':False]
['text':' HalfTensor does not support fill','line_number':7326,'multiline':False]
['text':' test half tensor','line_number':7334,'multiline':False]
['text':' Fix once fill is enabled for bfloat16','line_number':7340,'multiline':False]
['text':' test complex tensor','line_number':7350,'multiline':False]
['text':' complex tensor print uses two formatters, one for real values','line_number':7351,'multiline':False]
['text':' and the other for imag values. this is consistent with numpy','line_number':7352,'multiline':False]
['text':' test complex half tensor','line_number':7357,'multiline':False]
['text':' test scientific notation for complex tensors','line_number':7362,'multiline':False]
['text':' test big integer','line_number':7367,'multiline':False]
['text':' test scientific notation','line_number':7372,'multiline':False]
['text':' test scientific notation using set_printoptions','line_number':7377,'multiline':False]
['text':' reset to the default value','line_number':7385,'multiline':False]
['text':' test no leading space if all elements positive','line_number':7387,'multiline':False]
['text':' test for leading space if there are negative elements','line_number':7392,'multiline':False]
['text':' test inf and nan','line_number':7397,'multiline':False]
['text':' test dtype','line_number':7409,'multiline':False]
['text':' test changing default dtype','line_number':7418,'multiline':False]
['text':' test summary','line_number':7426,'multiline':False]
['text':' test internal summary function','line_number':7431,'multiline':False]
['text':' test device','line_number':7438,'multiline':False]
['text':' test changing default to cuda','line_number':7444,'multiline':False]
['text':' test printing a tensor on a different gpu than current one.','line_number':7449,'multiline':False]
['text':' test printing cpu tensor when default device is cuda','line_number':7455,'multiline':False]
['text':' test integral floats and requires_grad','line_number':7462,'multiline':False]
['text':' test non-contiguous print','line_number':7467,'multiline':False]
['text':' sliced tensor should have > PRINT_OPTS.threshold elements','line_number':7468,'multiline':False]
['text':' test print 0-dim tensor: there's no 0-dim in Numpy, we match arrayprint style','line_number':7522,'multiline':False]
['text':' test print boolean tensor','line_number':7527,'multiline':False]
['text':' [Numpy] test print float in sci_mode when min < 0.0001.','line_number':7536,'multiline':False]
['text':' [Numpy] test print complex in sci_mode when real_min < 0.0001 and (or) imag_min < 0.0001.','line_number':7541,'multiline':False]
['text':' [Numpy] test print float in sci_mode when max > 1e8.','line_number':7546,'multiline':False]
['text':' TODO: Pytorch uses fixed precision to print, while Numpy uses dragon4_scientific','line_number':7547,'multiline':False]
['text':' to do automatic trimming and padding.','line_number':7548,'multiline':False]
['text':' [Numpy] test print float in sci_mode when max / min > 1000.','line_number':7553,'multiline':False]
['text':' [Numpy] test print int max / min > 1000, no sci_mode','line_number':7558,'multiline':False]
['text':' [Numpy] test print int > 1e8, no sci_mode','line_number':7563,'multiline':False]
['text':' 1e9','line_number':7564,'multiline':False]
['text':' [Numpy] test printing float in int_mode','line_number':7568,'multiline':False]
['text':' [Numpy] test printing float in int_mode in sci format when max / min > 1000.','line_number':7573,'multiline':False]
['text':' TypeError would be better','line_number':7620,'multiline':False]
['text':' test that pin_memory on already pinned tensor has no effect','line_number':7634,'multiline':False]
['text':' message includes both Double and Long','line_number':7641,'multiline':False]
['text':' Calls model with a LongTensor input but DoubleTensor weights','line_number':7644,'multiline':False]
['text':' skip this test for now as it affects all tests','line_number':7707,'multiline':False]
['text':' tiny_float to zero','line_number':7723,'multiline':False]
['text':' tiny_float is not converted to zero in double type','line_number':7725,'multiline':False]
['text':' tiny_double to zero','line_number':7727,'multiline':False]
['text':' We can't usefully test the output; just make sure this doesn't crash','line_number':7731,'multiline':False]
['text':' This method is primarily exposed for torchvision's resize','line_number':7742,'multiline':False]
['text':' We have to ensure that method is torchscriptable as torchvision's resize','line_number':7745,'multiline':False]
['text':' should be torchscriptable','line_number':7746,'multiline':False]
['text':' Just a smoketest to make sure our slowTest decorator works.','line_number':7751,'multiline':False]
['text':' NB: we must not be built with CUDA; if we are built with CUDA but no CUDA','line_number':7792,'multiline':False]
['text':' is available, we get a different error.','line_number':7793,'multiline':False]
['text':' Check for contiguous tensors','line_number':7809,'multiline':False]
['text':' Checks for zero strides','line_number':7813,'multiline':False]
['text':' Check for zero strided, size 1 axis, in non-contiguous storage (gh-33812)','line_number':7818,'multiline':False]
['text':' Metadata changes are allowed on view tensors that are created from detach().','line_number':7826,'multiline':False]
['text':' test that we can call c10 ops and they return a reasonable result','line_number':7830,'multiline':False]
['text':' transpose doesn't really change the underlying physical memory','line_number':7877,'multiline':False]
['text':' so expecting dim_order change to reflect that (like strides)','line_number':7878,'multiline':False]
['text':' raise an error when trying to subclass FloatTensor','line_number':7893,'multiline':False]
['text':' but allow subclassing Tensor:','line_number':7898,'multiline':False]
['text':' non-contiguous tensor','line_number':7945,'multiline':False]
['text':' input nchw in (2,1,1,1), (2,2,2,2)','line_number':7970,'multiline':False]
['text':' output nchw in (2,1,1,1), (2,2,2,2)','line_number':7982,'multiline':False]
['text':' contiguous case','line_number':8002,'multiline':False]
['text':' non-contiguous case','line_number':8005,'multiline':False]
['text':' channels last case','line_number':8008,'multiline':False]
['text':' FIXME: move these meta tests to their own test suite/class or','line_number':8015,'multiline':False]
['text':'   distribute them among the appropriate test suites for their ops','line_number':8016,'multiline':False]
['text':' TODO: this test should be triggered by test_nn.py but right','line_number':8031,'multiline':False]
['text':' now meta is not enabled (and even if it was, we are probably','line_number':8032,'multiline':False]
['text':' missing too many meta functions to get through the test unmolested)','line_number':8033,'multiline':False]
['text':' NB: Can't make the exponent too big, or it will overflow','line_number':8035,'multiline':False]
['text':' signed 64-bit integer','line_number':8036,'multiline':False]
['text':' TODO: the out tests cannot be triggered by test_nn.py because','line_number':8042,'multiline':False]
['text':' we don't actually do out= arguments for nn functions, so there','line_number':8043,'multiline':False]
['text':' is no public API by which to get the out version','line_number':8044,'multiline':False]
['text':' interpolate doesn't seem to support out=','line_number':8046,'multiline':False]
['text':' (not sure why passing None here doesn't work? How strange...)','line_number':8047,'multiline':False]
['text':' TODO: the out tests cannot be triggered by test_nn.py because','line_number':8054,'multiline':False]
['text':' we don't actually do out= arguments for nn functions, so there','line_number':8055,'multiline':False]
['text':' is no public API by which to get the out version','line_number':8056,'multiline':False]
['text':' Make sure we don't clobber strides of out tensor.  NB: this','line_number':8058,'multiline':False]
['text':' test must be done on 2d/3d, because 1d doesn't have any meaningful','line_number':8059,'multiline':False]
['text':' layout support','line_number':8060,'multiline':False]
['text':' But if resize occurs, do clobber','line_number':8071,'multiline':False]
['text':' Complain if out dtype mismatch','line_number':8077,'multiline':False]
['text':' Complain if out device mismatch','line_number':8085,'multiline':False]
['text':' FIXME: compiling should properly error with a device mismatch.','line_number':8088,'multiline':False]
['text':' From https://github.com/pytorch/pytorch/issues/53815','line_number':8096,'multiline':False]
['text':' inputs have same size','line_number':8114,'multiline':False]
['text':' scalar case','line_number':8120,'multiline':False]
['text':' inputs are expandable tensors','line_number':8124,'multiline':False]
['text':' inputs are non-expandable tensors, but they have same number of elements','line_number':8128,'multiline':False]
['text':' inputs are non-expandable tensors and they don't have same number of elements','line_number':8140,'multiline':False]
['text':' output and inputs are size compatible','line_number':8147,'multiline':False]
['text':' output and inputs are not size compatible','line_number':8150,'multiline':False]
['text':' inputs are not expandable, output size is not the same as mean','line_number':8161,'multiline':False]
['text':' Test whether the output's memory layout is correct','line_number':8165,'multiline':False]
['text':' contiguous case fast setup','line_number':8188,'multiline':False]
['text':' channels last case fast setup','line_number':8192,'multiline':False]
['text':' non contiguous case fast setup (dense, non-overlapping, same shape and strides)','line_number':8200,'multiline':False]
['text':' non contiguous case fast setup (dense, non-overlapping)','line_number':8208,'multiline':False]
['text':' input tensors have same shape and strides','line_number':8209,'multiline':False]
['text':' output tensor have same shape as input tensors but different stride','line_number':8210,'multiline':False]
['text':' output tensor should preserve its strides in this case','line_number':8211,'multiline':False]
['text':' non contiguous case non fast setup','line_number':8220,'multiline':False]
['text':' Tests to make sure we still handle .data properly until it is removed','line_number':8228,'multiline':False]
['text':' .data allows to change the Tensors types inplace, check that we still','line_number':8230,'multiline':False]
['text':' raise a nice error.','line_number':8231,'multiline':False]
['text':' message includes both Double and ComplexFloat','line_number':8234,'multiline':False]
['text':' Calls model with a LongTensor input but DoubleTensor weights','line_number':8237,'multiline':False]
['text':' we should be able to "modify" slices of a 0-element','line_number':8245,'multiline':False]
['text':' array without an error being raised due to','line_number':8246,'multiline':False]
['text':' trying to resize its storage','line_number':8247,'multiline':False]
['text':' FIXME: Extend this test and put in a TensorProperties test class','line_number':8258,'multiline':False]
['text':' Verifies that (deep)copies of dtypes are the same objects','line_number':8264,'multiline':False]
['text':' FIXME: Put the following random tests into their own test class or test suite','line_number':8278,'multiline':False]
['text':' Fork the random number stream at this point','line_number':8293,'multiline':False]
['text':' Dramatically alter the internal state of the main generator','line_number':8299,'multiline':False]
['text':' Check all boundary cases of valid seed value inputs','line_number':8346,'multiline':False]
['text':' (seed, expected_initial_seed)','line_number':8348,'multiline':False]
['text':' Positive seeds should be unchanged','line_number':8349,'multiline':False]
['text':' Negative seeds wrap around starting from the largest seed value','line_number':8354,'multiline':False]
['text':' FIXME: Describe this test and port to the generic device framework in a more','line_number':8370,'multiline':False]
['text':'   appropriate test suite for the copy operation','line_number':8371,'multiline':False]
['text':' Validates regression reported in https://github.com/pytorch/pytorch/issues/45269','line_number':8384,'multiline':False]
['text':' FIXME: Port to a more appropriate test suite','line_number':8397,'multiline':False]
['text':' FIXME: Port to a more appropriate test suite','line_number':8402,'multiline':False]
['text':' Fails with inductor (and aot_eager) because functionalization replaces copy_ with copy,','line_number':8403,'multiline':False]
['text':' which doesn't properly error on bad inputs.','line_number':8404,'multiline':False]
['text':' Testing in-place copy where it attempt to write from many memory','line_number':8406,'multiline':False]
['text':' storage to a single storage would cause RuntimeError to be thrown','line_number':8407,'multiline':False]
['text':' Check that fbgemm code no longer reads memory out of bounds, see','line_number':8411,'multiline':False]
['text':' copy_impl and fbgemm::Float16ToFloat_ref.','line_number':8412,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/88543','line_number':8413,'multiline':False]
['text':' Types to test different code paths in copy_impl.','line_number':8415,'multiline':False]
['text':' out_dtype, src_dtype','line_number':8417,'multiline':False]
['text':' fbgemm','line_number':8418,'multiline':False]
['text':' fbgemm','line_number':8419,'multiline':False]
['text':' TensorIterator','line_number':8420,'multiline':False]
['text':' out_shape, src_shape, is_ok','line_number':8424,'multiline':False]
['text':' These cases used to crash with fbgemm, make sure these also raise','line_number':8425,'multiline':False]
['text':' exceptions with TensorIterator.','line_number':8426,'multiline':False]
['text':' same strides, not allowed by TI','line_number':8427,'multiline':False]
['text':' same strides, not allowed by TI','line_number':8428,'multiline':False]
['text':' different strides','line_number':8429,'multiline':False]
['text':' different strides','line_number':8430,'multiline':False]
['text':' different strides','line_number':8431,'multiline':False]
['text':' same numel','line_number':8432,'multiline':False]
['text':' These cases should pass with fbgemm and TensorIterator.','line_number':8434,'multiline':False]
['text':' same strides','line_number':8435,'multiline':False]
['text':' same strides','line_number':8436,'multiline':False]
['text':' different strides, allowed by TI','line_number':8437,'multiline':False]
['text':' different strides, allowed by TI','line_number':8438,'multiline':False]
['text':' FIXME: Port to a more appropriate test suite','line_number':8455,'multiline':False]
['text':' TODO: compressed sparse tensors currently don't support data_ptr.','line_number':8493,'multiline':False]
['text':' Exercising failure will allow us to widen coverage of this test once it does.','line_number':8494,'multiline':False]
['text':' While compressed sparse tensors don't have a concept of data_ptr','line_number':8497,'multiline':False]
['text':' the underlying tensors do. The implementation of to appropriately forwards','line_number':8498,'multiline':False]
['text':' the call to the components, which is what we're test here.','line_number':8499,'multiline':False]
['text':' in cuda10_1 sparse_csr is beta','line_number':8524,'multiline':False]
['text':' FIXME: describe this test','line_number':8527,'multiline':False]
['text':' Check that the correct type is returned.','line_number':8540,'multiline':False]
['text':' Check that the data is equal.','line_number':8545,'multiline':False]
['text':' Check that the data is equal even after modification.','line_number':8554,'multiline':False]
['text':' Check that member variables are passed through.','line_number':8559,'multiline':False]
['text':' Test that autograd is propagated.','line_number':8564,'multiline':False]
['text':' Run a calculation on the tensor.','line_number':8567,'multiline':False]
['text':' Cast exp_t to a subclass.','line_number':8570,'multiline':False]
['text':' Make sure that t.grad was initially None','line_number':8573,'multiline':False]
['text':' Run the autograd calculation.','line_number':8576,'multiline':False]
['text':' Make sure autograd was propagated to the original tensor','line_number':8579,'multiline':False]
['text':' declared with requires_grad.','line_number':8580,'multiline':False]
['text':' Make sure invalid subclasses raise nice errors','line_number':8583,'multiline':False]
['text':' FIXME: Port to a test suite that better fits slicing','line_number':8591,'multiline':False]
['text':' start and stop are clamped to the size of dim','line_number':8597,'multiline':False]
['text':' if start >= stop then the result is empty','line_number':8599,'multiline':False]
['text':' out of bounds is also empty','line_number':8602,'multiline':False]
['text':' additional correctness checks','line_number':8604,'multiline':False]
['text':' FIXME: port to a quantization test suite','line_number':8617,'multiline':False]
['text':' FIXME: port to a distributed test suite -- also... how could this be OOMing on Windows CUDA?','line_number':8637,'multiline':False]
['text':' n_sample = 1 is a special case, test n_sample=2 which is more general','line_number':8654,'multiline':False]
['text':' Should not be reached','line_number':8656,'multiline':False]
['text':' FIXME: port to more appropriate test suite','line_number':8665,'multiline':False]
['text':' Tests that the use_deterministic_flag can be set as expected','line_number':8748,'multiline':False]
['text':' Tests that torch.utils.deterministic.fill_uninitialized_memory can be set as expected','line_number':8789,'multiline':False]
['text':' FIXME: All of the following should be marked as expected failures','line_number':8922,'multiline':False]
['text':' so that it is easier to tell when missing has been added.','line_number':8923,'multiline':False]
['text':' FIXME: fix all the skipped ones below!','line_number':8924,'multiline':False]
['text':' TODO: add torch.* tests when we have proper namespacing on ATen functions','line_number':8949,'multiline':False]
['text':' test_namespace(torch)','line_number':8950,'multiline':False]
['text':' FIXME: deprecate torch.Tensor constructor','line_number':8952,'multiline':False]
['text':' ensure sharing is not broken','line_number':8966,'multiline':False]
['text':' Direct construction not OK','line_number':8971,'multiline':False]
['text':' But construction of subclass is OK','line_number':8974,'multiline':False]
['text':' Direct construction not OK','line_number':8981,'multiline':False]
['text':' But construction of subclass is OK','line_number':8984,'multiline':False]
['text':' OK to call super().__new__, see','line_number':8992,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/57421','line_number':8993,'multiline':False]
['text':' OK to call super().__new__, see','line_number':9004,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/57421','line_number':9005,'multiline':False]
['text':' put something on __dict__','line_number':9016,'multiline':False]
['text':' x is dead in Python','line_number':9019,'multiline':False]
['text':' it's live','line_number':9021,'multiline':False]
['text':' it's dead again','line_number':9022,'multiline':False]
['text':' x is dead in Python','line_number':9032,'multiline':False]
['text':' it's live','line_number':9034,'multiline':False]
['text':' it's dead again','line_number':9035,'multiline':False]
['text':' C++ reference should keep the cycle live!','line_number':9320,'multiline':False]
['text':' This exercise THPVariable_subtype_traverse','line_number':9321,'multiline':False]
['text':' NB: Because z.grad is a reference done entirely in C++, cycles','line_number':9322,'multiline':False]
['text':' involving it directly are NOT broken by Python GC; you've','line_number':9323,'multiline':False]
['text':' set up a good old C++ reference cycle which we cannot safely','line_number':9324,'multiline':False]
['text':' break (because C++ references are allowed to be accessed','line_number':9325,'multiline':False]
['text':' multithreaded-ly) (TODO: except maybe if you can prove that','line_number':9326,'multiline':False]
['text':' only Python has access to the C++ object, in which case you can','line_number':9327,'multiline':False]
['text':' also prove that no multithreaded access occurs)','line_number':9328,'multiline':False]
['text':' C++ reference should keep the cycle live!','line_number':9363,'multiline':False]
['text':' This exercise THPVariable_subtype_traverse','line_number':9364,'multiline':False]
['text':' NB: Because z.grad is a reference done entirely in C++, cycles','line_number':9365,'multiline':False]
['text':' involving it directly are NOT broken by Python GC; you've','line_number':9366,'multiline':False]
['text':' set up a good old C++ reference cycle which we cannot safely','line_number':9367,'multiline':False]
['text':' break (because C++ references are allowed to be accessed','line_number':9368,'multiline':False]
['text':' multithreaded-ly) (TODO: except maybe if you can prove that','line_number':9369,'multiline':False]
['text':' only Python has access to the C++ object, in which case you can','line_number':9370,'multiline':False]
['text':' also prove that no multithreaded access occurs)','line_number':9371,'multiline':False]
['text':' FIXME: move to test_autograd?','line_number':9484,'multiline':False]
['text':' this hits a special setter, it's not just a __dict__ entry','line_number':9496,'multiline':False]
['text':' Ideally, x would keep the tensor live.  But CPython doesn't','line_number':9520,'multiline':False]
['text':' provide enough hooks to do this.  So it will go dead and x','line_number':9521,'multiline':False]
['text':' will transmute into an undefined tensor.  Not great, but the','line_number':9522,'multiline':False]
['text':' best we can do.','line_number':9523,'multiline':False]
['text':' Ideally, x would keep the storage live.  But CPython doesn't','line_number':9536,'multiline':False]
['text':' provide enough hooks to do this.  So it will go dead and x','line_number':9537,'multiline':False]
['text':' will transmute into storage with null StorageImpl. Not great, but the','line_number':9538,'multiline':False]
['text':' best we can do.','line_number':9539,'multiline':False]
['text':' Use this to manually fix weak references after dereferencing them','line_number':9553,'multiline':False]
['text':' Use this to manually fix weak reference after dereferencing them','line_number':9565,'multiline':False]
['text':' FIXME: move to test_linalg','line_number':9604,'multiline':False]
['text':' transposed tensors','line_number':9621,'multiline':False]
['text':' broadcasting tensors','line_number':9628,'multiline':False]
['text':' zero-sized tensors','line_number':9635,'multiline':False]
['text':' Note that this is not in test_cuda.py as this whole file is skipped when cuda','line_number':9667,'multiline':False]
['text':' is not available.','line_number':9668,'multiline':False]
['text':' b is generated through torch.where function with not_zero being a scalar parameter','line_number':9683,'multiline':False]
['text':' c is generated through Tensor.where method with not_zero being a scalar parameter','line_number':9685,'multiline':False]
['text':' Refcount values get modified by Dynamo resume frames','line_number':9718,'multiline':False]
['text':' The following block extends TestTorch with negative dim wrapping tests','line_number':9800,'multiline':False]
['text':' FIXME: replace these with OpInfo sample inputs or systemic OpInfo tests','line_number':9801,'multiline':False]
['text':' Functions to test negative dimension wrapping','line_number':9802,'multiline':False]
['text':' TODO: these empy classes are temporarily instantiated for XLA compatibility','line_number':9906,'multiline':False]
['text':'   once XLA updates their test suite it should be removed','line_number':9907,'multiline':False]
['text':' Generates tests','line_number':9914,'multiline':False]
['text':' Note: test generation must be done at file scope, not within main, or','line_number':9915,'multiline':False]
['text':' pytest will fail.','line_number':9916,'multiline':False]
