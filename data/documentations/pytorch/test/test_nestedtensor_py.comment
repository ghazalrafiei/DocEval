['text':' Owner(s): ["module: nestedtensor"]','line_number':1,'multiline':False]
['text':' Tests are ported from pytorch/nestedtensor.','line_number':45,'multiline':False]
['text':' This makes porting as_nested_tensor easier in the future.','line_number':46,'multiline':False]
['text':' yield as_nested_tensor','line_number':50,'multiline':False]
['text':' Helper function to generate a pair of random nested tensors','line_number':53,'multiline':False]
['text':' one is contiguous, the other is not, but they appear to have same entries','line_number':54,'multiline':False]
['text':' an output nested tensor consists of','line_number':55,'multiline':False]
['text':' * `len(ragged_sizes)` matrices','line_number':56,'multiline':False]
['text':' * matrices[i].shape == (20, ragged_sizes[i])','line_number':57,'multiline':False]
['text':' contiguous nested tensor','line_number':64,'multiline':False]
['text':' noncontiguous nested tensor','line_number':69,'multiline':False]
['text':' Helper functions to pad a noncontiguous nested tensor','line_number':74,'multiline':False]
['text':' can be replaced once to_padded_tensor supports noncontiguous memory','line_number':75,'multiline':False]
['text':' Helper function to generate a random nested tensor','line_number':100,'multiline':False]
['text':' Select a random idx that will be required to be non-empty','line_number':117,'multiline':False]
['text':' Alternate approach to generating a random NT.','line_number':134,'multiline':False]
['text':' dims should be something like [5, None, 10], with None indicating that a','line_number':135,'multiline':False]
['text':' random ragged structure should be used','line_number':136,'multiline':False]
['text':' Creates an NT matching another NT's number of components and','line_number':147,'multiline':False]
['text':' shape / ragged structure for all dims specified to be -1.','line_number':148,'multiline':False]
['text':' Suppress errors is enabled by default in the test suite. We disable','line_number':172,'multiline':False]
['text':' suppress errors in particular for NestedTensors to ensure that dynamo','line_number':173,'multiline':False]
['text':' graph breaks cleanly.','line_number':174,'multiline':False]
['text':' Both of these tests are necessary, because we're using','line_number':308,'multiline':False]
['text':' torch_function.','line_number':309,'multiline':False]
['text':' TODO: Re-enable this once using torch_dispatch','line_number':311,'multiline':False]
['text':' _test_fn(lambda x, dim: torch.unbind(x, dim))','line_number':312,'multiline':False]
['text':' self.assertEqual(default_nested_tensor.nested_dim(), 1)','line_number':339,'multiline':False]
['text':' self.assertEqual(default_nested_tensor.nested_size(), ())','line_number':340,'multiline':False]
['text':' TODO: Re-enable once we have a performance driven','line_number':349,'multiline':False]
['text':' use case and implementation.','line_number':350,'multiline':False]
['text':' self.assertEqual(default_nested_tensor.is_pinned(),','line_number':351,'multiline':False]
['text':'                  default_tensor.is_pinned())','line_number':352,'multiline':False]
['text':' Interesting edge case','line_number':381,'multiline':False]
['text':' Test empty case','line_number':433,'multiline':False]
['text':' Test contiguous case','line_number':440,'multiline':False]
['text':' Test non_contiguous case','line_number':444,'multiline':False]
['text':' Test querying by memory_format','line_number':448,'multiline':False]
['text':' dim=0 success case','line_number':607,'multiline':False]
['text':' No constraints on ragged structures matching.','line_number':608,'multiline':False]
['text':' dim=-1 success case','line_number':616,'multiline':False]
['text':' shape (B, *, D)','line_number':617,'multiline':False]
['text':' shape (B, *, D'); same structure as x but dim=-1 differs','line_number':619,'multiline':False]
['text':' should be shape (B, *, D + D') when supported','line_number':621,'multiline':False]
['text':' dim between 0 and -1 success case','line_number':626,'multiline':False]
['text':' same structure as x but dim=2 differs','line_number':628,'multiline':False]
['text':' error case: mixed NT / dense inputs','line_number':634,'multiline':False]
['text':' error case: NTs with different dims','line_number':641,'multiline':False]
['text':' error case: non-contiguous NT','line_number':648,'multiline':False]
['text':' transpose to put ragged dim next to batch dim','line_number':650,'multiline':False]
['text':' error case: multiple ragged dims in inputs','line_number':656,'multiline':False]
['text':' error case: ragged dim not next to batch dim','line_number':663,'multiline':False]
['text':' error case: NTs with different batch sizes','line_number':670,'multiline':False]
['text':' error case: NTs with different ragged structures','line_number':677,'multiline':False]
['text':' Helper function to generate a pair of random nested tensors','line_number':694,'multiline':False]
['text':' the 2 nested tensors have same shapes','line_number':695,'multiline':False]
['text':' This is an incorrect gradient, but we assume that's what the user','line_number':731,'multiline':False]
['text':' wanted. detach() is an advanced option.','line_number':732,'multiline':False]
['text':' Simple shapes test','line_number':767,'multiline':False]
['text':' More complex nt test with different lengths for each tensor','line_number':778,'multiline':False]
['text':' Test with multidimensional tensors after irregular dim','line_number':791,'multiline':False]
['text':' (run only with smaller dimensions to ensure fast execution)','line_number':792,'multiline':False]
['text':' Test where the normalizing dimensions are not all','line_number':804,'multiline':False]
['text':' nested tensor * nested tensor','line_number':856,'multiline':False]
['text':' TODO: test noncontiguous to_padded_tensor','line_number':957,'multiline':False]
['text':' For now this tests the functionality of noncontiguous_to_padded_tensor','line_number':958,'multiline':False]
['text':' and the error message of to_padded_tensor','line_number':959,'multiline':False]
['text':' since to_padded_tensor does not support noncontiguous buffer yet','line_number':960,'multiline':False]
['text':' test noncontiguous_to_padded_tensor functionality','line_number':965,'multiline':False]
['text':' test to_padded_tensor error message','line_number':969,'multiline':False]
['text':' edge case: empty nested tensor','line_number':985,'multiline':False]
['text':' normal case','line_number':988,'multiline':False]
['text':' single index: only support integer in the batch dimension','line_number':992,'multiline':False]
['text':' tuple of indices: only support integer in the batch dimension','line_number':999,'multiline':False]
['text':'                 + all possible indexing in the original tensor dimensions','line_number':1000,'multiline':False]
['text':' test select on non-batch dimensions','line_number':1006,'multiline':False]
['text':' make sure indexing returns a view','line_number':1013,'multiline':False]
['text':' Test that indexing works when requires_grad_(True)','line_number':1021,'multiline':False]
['text':' previously this was failing because the backward kernel for select.int uses .sizes()','line_number':1022,'multiline':False]
['text':' should work regardless of contiguity','line_number':1064,'multiline':False]
['text':' Transformer use case','line_number':1073,'multiline':False]
['text':' Failure chunking on ragged dimensions','line_number':1095,'multiline':False]
['text':' Failure on non-contiguous nt','line_number':1103,'multiline':False]
['text':' Failure when calling non divisible n_chunks','line_number':1108,'multiline':False]
['text':' Failure when calling backward on a chunk','line_number':1114,'multiline':False]
['text':' Failure calling on ragged dimensions','line_number':1147,'multiline':False]
['text':' Failure calling on non-last dimension','line_number':1152,'multiline':False]
['text':' Failure on non-contiguous nt','line_number':1157,'multiline':False]
['text':' Failure when calling with split_sizes that don't cover the full dim size','line_number':1163,'multiline':False]
['text':' don't add up to 20','line_number':1164,'multiline':False]
['text':' [B, *, D], [B, 1, D] case','line_number':1230,'multiline':False]
['text':' [B, *], [B, 1] case','line_number':1236,'multiline':False]
['text':' nested tensor * nested tensor','line_number':1246,'multiline':False]
['text':' nested tensor * scalar','line_number':1251,'multiline':False]
['text':' error case: numel == 1 but dim > 0','line_number':1263,'multiline':False]
['text':' nested tensor * nested tensor','line_number':1321,'multiline':False]
['text':' nested tensor * scalar','line_number':1326,'multiline':False]
['text':' error case: numel == 1 but dim > 0','line_number':1341,'multiline':False]
['text':' test backward','line_number':1370,'multiline':False]
['text':' generate gradient tensor that has the same size as the output','line_number':1371,'multiline':False]
['text':' Test error inputs','line_number':1386,'multiline':False]
['text':' Since we don't have access to the buffer in python this is harder to show what','line_number':1398,'multiline':False]
['text':' we are testing for. When we call chunk on a consistent dim of a NT','line_number':1399,'multiline':False]
['text':' for chunk_size > 1 the resulting tensors are views of the original NT','line_number':1400,'multiline':False]
['text':' whose numels is now less than the size of the buffer. Clone was','line_number':1401,'multiline':False]
['text':' previously creating a new NT with a buffer that was the same size as the','line_number':1402,'multiline':False]
['text':' original.','line_number':1403,'multiline':False]
['text':' Split up the last dimension which has a consistent size of 20 into 5 chunks','line_number':1406,'multiline':False]
['text':' # Check chunks are contiguous after calling contiguous','line_number':1409,'multiline':False]
['text':' Verify the values match','line_number':1419,'multiline':False]
['text':' Verify modifying nt2 doesn't affect nt1','line_number':1421,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "bernoulli_scalar_cpu_" not implemented for 'Half'','line_number':1433,'multiline':False]
['text':' edge case: empty nested tensor','line_number':1438,'multiline':False]
['text':' TODO: support empty NT in jagged layout','line_number':1439,'multiline':False]
['text':' normal nested tensor','line_number':1444,'multiline':False]
['text':' edge case: invalid dropout','line_number':1450,'multiline':False]
['text':' edge case: no dropout','line_number':1455,'multiline':False]
['text':' edge case: all dropout','line_number':1461,'multiline':False]
['text':' normal case: normal dropout','line_number':1468,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "softmax_kernel_impl" not implemented for 'Half'','line_number':1507,'multiline':False]
['text':' normal nested tensor','line_number':1510,'multiline':False]
['text':' error case: softmax across nested dimension','line_number':1513,'multiline':False]
['text':' error case: dimension out of range','line_number':1524,'multiline':False]
['text':' normal case: should equal to padding -inf','line_number':1527,'multiline':False]
['text':' if an entire slice is padded, then softmax will return 0.0 / 0.0 = nan','line_number':1533,'multiline':False]
['text':' however, physically speaking that should be 0.0','line_number':1534,'multiline':False]
['text':' edge case: empty nested tensor','line_number':1537,'multiline':False]
['text':' edge case: nesting scalars','line_number':1541,'multiline':False]
['text':' error case: one is nested but the other is not','line_number':1555,'multiline':False]
['text':' error case: not 3D tensors','line_number':1568,'multiline':False]
['text':' error case: incompatible batch size','line_number':1612,'multiline':False]
['text':' error case: underlying matrices cannot be multiplied','line_number':1628,'multiline':False]
['text':' normal nested tensor','line_number':1635,'multiline':False]
['text':' test tensorcore path','line_number':1645,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "addmm_impl_cpu_" not implemented for 'Half'','line_number':1661,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "addmm_impl_cpu_" not implemented for 'Half'','line_number':1666,'multiline':False]
['text':' [N, n_head, *, head_dim], [N, n_head, head_dim, *]','line_number':1683,'multiline':False]
['text':' test with noncontiguous','line_number':1699,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "bmm" not implemented for 'Half'','line_number':1710,'multiline':False]
['text':' error case: one is nested but the other is not','line_number':1713,'multiline':False]
['text':' error case: not 3+D tensors','line_number':1726,'multiline':False]
['text':' error case: incompatible batch size','line_number':1770,'multiline':False]
['text':' error case: incompatible (wrong) batch sizes that shouldn't even broadcast?','line_number':1786,'multiline':False]
['text':' error case: incompatible batch sizes that should technically broadcast','line_number':1798,'multiline':False]
['text':' error case: underlying matrices cannot be multiplied','line_number':1810,'multiline':False]
['text':' normal nested tensor: 3D','line_number':1817,'multiline':False]
['text':' normal nested tensor: 4D (with testing for batch_size=1)','line_number':1823,'multiline':False]
['text':' normal nested tensor: 5D','line_number':1833,'multiline':False]
['text':' only supported on CUDA for now','line_number':1844,'multiline':False]
['text':' NT (B, *, C, D) with T (D, E) broadcasting case','line_number':1847,'multiline':False]
['text':' should be equivalent to matmul-ing each component with the dense tensor','line_number':1852,'multiline':False]
['text':' cannot test torch.float16 because: RuntimeError: "bmm" not implemented for 'Half'','line_number':1857,'multiline':False]
['text':' success case','line_number':1875,'multiline':False]
['text':' invalid nested tensor dimension','line_number':1878,'multiline':False]
['text':' invalid weight shape','line_number':1885,'multiline':False]
['text':' inconsistent last dim of nested tensor','line_number':1891,'multiline':False]
['text':' Mismatch of nested tensor last dim and weight dimension','line_number':1898,'multiline':False]
['text':' Nested tensor input and nested weight','line_number':1905,'multiline':False]
['text':' TODO: test noncontiguous linear','line_number':1911,'multiline':False]
['text':' For now this tests the error message of linear','line_number':1912,'multiline':False]
['text':' since linear does not support noncontiguous buffer yet','line_number':1913,'multiline':False]
['text':' error case: transpose nested dimension','line_number':1937,'multiline':False]
['text':' error case: dimension out of range','line_number':1948,'multiline':False]
['text':' normal case','line_number':1951,'multiline':False]
['text':' error case: squeeze no dimension','line_number':1963,'multiline':False]
['text':' error case: squeeze nested dimension','line_number':1969,'multiline':False]
['text':' error case: dimension out of range','line_number':1975,'multiline':False]
['text':' error case: squeeze nested tensor of singleton tensors','line_number':1977,'multiline':False]
['text':' squeezing a dim which does not have size 1 should be a no-op','line_number':1986,'multiline':False]
['text':' test cases that should work','line_number':1990,'multiline':False]
['text':' cannot unsqueeze batch dim','line_number':1995,'multiline':False]
['text':' negative dim will correspond to unsqueeze() applied at dim = dim + nt.dim() + 1','line_number':1998,'multiline':False]
['text':' col_index into nt size tensor is requires subtraction of 1 to ignore batch dim','line_number':2000,'multiline':False]
['text':' Construct in default mode and transpose while in inference mode','line_number':2018,'multiline':False]
['text':' Construct and transpose while in inference mode','line_number':2026,'multiline':False]
['text':' error case: empty shape','line_number':2038,'multiline':False]
['text':' error case: empty nested tensor','line_number':2044,'multiline':False]
['text':' error case: -1 for batch size','line_number':2051,'multiline':False]
['text':' normal case','line_number':2062,'multiline':False]
['text':' error case, trying to reshape batch dim to a legit shape','line_number':2067,'multiline':False]
['text':' inherit only the ragged dimension','line_number':2073,'multiline':False]
['text':' (2, 20) -> (2, 5, 4)','line_number':2074,'multiline':False]
['text':' (3, 20) -> (3, 5, 4)','line_number':2075,'multiline':False]
['text':' (2, 3, 20) -> (2, 3, 5, 4) -> (2, 4, 5, 4)','line_number':2077,'multiline':False]
['text':' more than one -1 (even for "old" dims), should fail','line_number':2081,'multiline':False]
['text':' this attempts to do # (2, (2, 3), 5, 4) -> (2, (2, 3), 5, 2, 2)','line_number':2082,'multiline':False]
['text':' but we ban "inherit old behavior" for >1 dimension','line_number':2083,'multiline':False]
['text':' Construct in default mode and view while in inference mode','line_number':2092,'multiline':False]
['text':' Construct and view while in inference mode','line_number':2100,'multiline':False]
['text':' error case: empty shape','line_number':2112,'multiline':False]
['text':' error case: empty nested tensor','line_number':2118,'multiline':False]
['text':' error case: -1 for batch size','line_number':2125,'multiline':False]
['text':' normal case','line_number':2136,'multiline':False]
['text':' (2, (2, 3), 20)','line_number':2139,'multiline':False]
['text':' error case, trying to reshape batch dim to a legit shape','line_number':2141,'multiline':False]
['text':' inherit only the ragged dimension','line_number':2147,'multiline':False]
['text':' (2, 20) -> (2, 5, 4)','line_number':2148,'multiline':False]
['text':' (3, 20) -> (3, 5, 4)','line_number':2149,'multiline':False]
['text':' (2, 3, 20) -> (2, 3, 5, 4) -> (2, 4, 5, 4)','line_number':2151,'multiline':False]
['text':' more than one -1 (even for "old" dims), should fail','line_number':2155,'multiline':False]
['text':' this attempts to do # (2, (2, 3), 5, 4) -> (2, (2, 3), 5, 2, 2)','line_number':2156,'multiline':False]
['text':' but we ban "inherit old behavior" for >1 dimension','line_number':2157,'multiline':False]
['text':' narrow on dim=0 from start to end','line_number':2168,'multiline':False]
['text':' ensure output is a view','line_number':2173,'multiline':False]
['text':' dim != 0 is not supported','line_number':2178,'multiline':False]
['text':' error case: non-contiguous NT','line_number':2183,'multiline':False]
['text':' Shape: (N, L, E); ragged L','line_number':2196,'multiline':False]
['text':' Shape: (N, S, E); ragged S','line_number':2199,'multiline':False]
['text':' In the 4D case the L and S is ragged','line_number':2203,'multiline':False]
['text':' Shape: (N, N', L, E); ragged N' and L','line_number':2204,'multiline':False]
['text':' Shape: (N, N', S, E); ragged N' and S','line_number':2206,'multiline':False]
['text':' Shape: (N, L, S); ragged L and S matching above','line_number':2215,'multiline':False]
['text':' no dropout for reproducibility','line_number':2218,'multiline':False]
['text':' Success case: no attn_mask set and is_causal=False.','line_number':2220,'multiline':False]
['text':' Error case: explicit attn_mask set.','line_number':2232,'multiline':False]
['text':' Error case: is_causal=True.','line_number':2237,'multiline':False]
['text':' Create empty on same device as original nested tensor','line_number':2247,'multiline':False]
['text':' Check changing dtype of empty_like nested tensor output','line_number':2262,'multiline':False]
['text':' Create tensor for autograd','line_number':2271,'multiline':False]
['text':' Test noncontiguous tensor does not fail to copy','line_number':2275,'multiline':False]
['text':' Test the contiguous memory format option','line_number':2282,'multiline':False]
['text':' Test other memory formats fail','line_number':2291,'multiline':False]
['text':' Note [Gradcheck args check_batched_grad=False] the common_utils testing version of gradcheck','line_number':2298,'multiline':False]
['text':' includes the default parameters used for testing ops with gradcheck. However nested tensor','line_number':2299,'multiline':False]
['text':' does not support the stack op therefore we turn it off for these tests','line_number':2300,'multiline':False]
['text':' tensors with requires_grad=False are leaves','line_number':2318,'multiline':False]
['text':'  Grad check doesn't work with nested yet.','line_number':2370,'multiline':False]
['text':' d/dnt_1 (nt + nt_1) = 1*grad_output','line_number':2371,'multiline':False]
['text':' Test Factory Functions','line_number':2409,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2434,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2446,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2460,'multiline':False]
['text':' This implictily tests to_padded_tensor grads','line_number':2473,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2654,'multiline':False]
['text':' Test linear with no bias added','line_number':2660,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2674,'multiline':False]
['text':' Test linear with no bias added','line_number':2681,'multiline':False]
['text':' This implicitly tests to_padded_tensor grads','line_number':2692,'multiline':False]
['text':' softmax over last dim','line_number':2696,'multiline':False]
['text':' Previously would error when input NT doesn't require grad','line_number':2856,'multiline':False]
['text':' NotImplementedError: Cannot access storage of UndefinedTensorImpl','line_number':2857,'multiline':False]
['text':' TODO: OOM https://github.com/pytorch/pytorch/issues/95562','line_number':2879,'multiline':False]
['text':' TODO: OOM https://github.com/pytorch/pytorch/issues/95562','line_number':2896,'multiline':False]
['text':' Could either mark slow or reduce size','line_number':2898,'multiline':False]
['text':' Found in torch/testing/_comparison.py','line_number':2914,'multiline':False]
['text':' Fill in the nans with the default rtol','line_number':2921,'multiline':False]
['text':' torch.isclose() has weird behavior around see:','line_number':2944,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/102400','line_number':2945,'multiline':False]
['text':' We can probably parametrizing existing tests instead of having a separate','line_number':2950,'multiline':False]
['text':' test class as we begin to support more ops. Also maybe rewrite with OpInfos.','line_number':2951,'multiline':False]
['text':' TODO: consolidate with the below','line_number':2953,'multiline':False]
['text':' Purposefully introduce mixed requires_grad settings for the components','line_number':2971,'multiline':False]
['text':' when include_requires_grad=True.','line_number':2972,'multiline':False]
['text':' (B, *, D) with B=4','line_number':2974,'multiline':False]
['text':' (B, *, D_0, D_1) with B=5','line_number':2981,'multiline':False]
['text':' (B, *, D) with B=3 in list form','line_number':2993,'multiline':False]
['text':' Incorrect usage: shape check will fail if the offsets tensor are not','line_number':3058,'multiline':False]
['text':'                  the same exact tensor object','line_number':3059,'multiline':False]
['text':' Correct usage: chain the calls using the same offsets tensor object','line_number':3068,'multiline':False]
['text':' (B, j0, 3, 4)','line_number':3121,'multiline':False]
['text':' (B, j0, ?, ?) + (?) -> (B, j0, ?, ?)','line_number':3123,'multiline':False]
['text':' (B, j0, ?, ?) + (?, ?) -> (B, j0, ?, ?)','line_number':3124,'multiline':False]
['text':' (B, j0, ?, ?) + (1, ?, ?) -> (B, j0, ?, ?)','line_number':3125,'multiline':False]
['text':' Unsupported: (B, j0, ?, ?) + (1, 1, 1, ?, ?) -> (1, B, j0, ?, ?)','line_number':3126,'multiline':False]
['text':' (1, 1, 1, 1, 4), (unsupported today)','line_number':3133,'multiline':False]
['text':' (B, j0, 3, 4)','line_number':3147,'multiline':False]
['text':' Check shape correctness','line_number':3150,'multiline':False]
['text':' dims, expected shape, expected keepdim shape','line_number':3152,'multiline':False]
['text':' j0 is represented as None','line_number':3153,'multiline':False]
['text':' Check values correctness','line_number':3180,'multiline':False]
['text':' raggedness not reduced','line_number':3181,'multiline':False]
['text':' flatten to avoid having to replicate unsqueeze logic depending on keepdim','line_number':3186,'multiline':False]
['text':' raggedness reduced away','line_number':3189,'multiline':False]
['text':' should be both conceptually equal and metadata equivalent','line_number':3215,'multiline':False]
['text':' should be conceptually equal but not necessarily metadata equivalent','line_number':3218,'multiline':False]
['text':' test that pin_memory on already pinned tensor has no effect','line_number':3232,'multiline':False]
['text':' Validate a bunch of properties after NT construction.','line_number':3237,'multiline':False]
['text':' Make sure grads -don't- flow back into original tensors for nested_tensor()','line_number':3269,'multiline':False]
['text':' NB: as_nested_tensor(tensor_list) doesn't support lists of lists for tensor_list','line_number':3281,'multiline':False]
['text':' nt.requires_grad=True should be set if at least one component requires grad','line_number':3290,'multiline':False]
['text':' Make sure grads flow back into original tensors for as_nested_tensor()','line_number':3293,'multiline':False]
['text':' offsets should still be int64 on the original device','line_number':3331,'multiline':False]
['text':' TODO: Use this approach when unbind is functional','line_number':3380,'multiline':False]
['text':' unbinded_nt = nt.unbind()','line_number':3381,'multiline':False]
['text':' for i in range(starts.shape[0]):','line_number':3382,'multiline':False]
['text':'     self.assertEqual(torch.arange(starts[i], starts[i] + lengths[i], device=device, dtype=torch.int64), unbinded_nt[i])','line_number':3383,'multiline':False]
['text':' Test contiguous case','line_number':3417,'multiline':False]
['text':' Test narrow case','line_number':3420,'multiline':False]
['text':' Test querying by memory_format','line_number':3424,'multiline':False]
['text':' transpose ragged dim','line_number':3434,'multiline':False]
['text':' pointwise ops are not supported on ragged dim transposed jagged layout NTs','line_number':3436,'multiline':False]
['text':' Note 1: CPU Fused kernels do not support nested, Math is missing ops to work with NT jagged','line_number':3440,'multiline':False]
['text':' Note 2: Unless running on newer GPUs, only mem-effn or math are available, and mem-effn','line_number':3441,'multiline':False]
['text':' will fail with gradients and math has ops that aren't implemented. Therefore, in','line_number':3442,'multiline':False]
['text':' order to get some kernel to work with most GPUs, we have to disable gradients in','line_number':3443,'multiline':False]
['text':' this more general test','line_number':3444,'multiline':False]
['text':' Note 3: ROCm only supports the math kernel, which doesn't work with jagged NTs','line_number':3445,'multiline':False]
['text':' Simplest case: 1 sentence, no batching','line_number':3463,'multiline':False]
['text':' See note below for why we detach here.','line_number':3467,'multiline':False]
['text':' High Precision Math Reference','line_number':3482,'multiline':False]
['text':' Low Precision Math Reference','line_number':3492,'multiline':False]
['text':' Compute tolerances','line_number':3496,'multiline':False]
['text':' Simple case: 2 sentences, no extra params','line_number':3509,'multiline':False]
['text':' NB: we make sure the leaf tensor we compute gradients for is the view-ed tensor before','line_number':3513,'multiline':False]
['text':' it is transposed. This is because today we cannot backward through view or unbind a','line_number':3514,'multiline':False]
['text':' transposed tensor.','line_number':3515,'multiline':False]
['text':' Default','line_number':3547,'multiline':False]
['text':' Test dispatcher works by calling only mem-effn and math (as they are safe for all devices)','line_number':3550,'multiline':False]
['text':' Will fail bc unsupported ops','line_number':3554,'multiline':False]
['text':' TODO: Add remaining ops, or implement a different math dispatch for jagged','line_number':3555,'multiline':False]
['text':' This requires NT -> NT views to work in inductor, which is a TODO','line_number':3561,'multiline':False]
['text':' noqa: E301','line_number':3562,'multiline':False]
['text':' Simplest case: 1 sentence, no batching','line_number':3579,'multiline':False]
['text':' High Precision Math Reference','line_number':3595,'multiline':False]
['text':' Low Precision Math Reference','line_number':3600,'multiline':False]
['text':' shape (B, P*, S, D)','line_number':3616,'multiline':False]
['text':' B: batch size','line_number':3617,'multiline':False]
['text':' P*: ragged number of prompts','line_number':3618,'multiline':False]
['text':' S: (constant) sequence length','line_number':3619,'multiline':False]
['text':' D: embedding size','line_number':3620,'multiline':False]
['text':' should be equivalent to just running the buffers through','line_number':3628,'multiline':False]
