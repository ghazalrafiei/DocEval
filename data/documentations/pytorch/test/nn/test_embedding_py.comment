['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]
['text':' Seed RNG so we get the same Embedding each time','line_number':26,'multiline':False]
['text':' test output and that weight matrix was renormalized','line_number':128,'multiline':False]
['text':' A silly test for eager, this test is useful for when we run under PYTORCH_TEST_WITH_DYNAMO=1','line_number':142,'multiline':False]
['text':' as it ensures that setattr correctly works.','line_number':143,'multiline':False]
['text':' Make sure that error is thrown if padding_idx is out of bounds','line_number':178,'multiline':False]
['text':' Test case from https://github.com/pytorch/pytorch/issues/89677','line_number':229,'multiline':False]
['text':' can't use gradcheck since in place renorm makes analytical gradients different from produced ones','line_number':307,'multiline':False]
['text':' can't use gradcheck since in place renorm makes analytical gradients different from produced ones','line_number':325,'multiline':False]
['text':' negative indexing check for padding_idx','line_number':353,'multiline':False]
['text':' padding_idx=-2, num_embeddings=10 ==> index 8 padded','line_number':354,'multiline':False]
['text':' change padding vector','line_number':367,'multiline':False]
['text':' out of bounds check for padding_idx','line_number':376,'multiline':False]
['text':' Need large N to trigger all the methods we have implemented','line_number':382,'multiline':False]
['text':' test double backward','line_number':391,'multiline':False]
['text':' Check correctness of torch.nn.functional.embedding_bag forward and','line_number':400,'multiline':False]
['text':' backward functions with padding_idx, given a 1D input separated into bags','line_number':401,'multiline':False]
['text':' with an offset array. Compare against an equivalent 2D input that uses','line_number':402,'multiline':False]
['text':' padding indices to fill in the gaps indicated by the offset array','line_number':403,'multiline':False]
['text':' Make one bag full and one bag empty, for extra coverage','line_number':420,'multiline':False]
['text':' embedding_bag requires first entry of offsets to be 0','line_number':437,'multiline':False]
['text':' Convert a 1-D indices-offsets representation into 2-D. Fill any empty','line_number':449,'multiline':False]
['text':' indices with padding_idx','line_number':450,'multiline':False]
['text':' Determine the start and end position of the bag within indices_1D','line_number':457,'multiline':False]
['text':' Pull out the bag's indices from indices_1D, and fill any','line_number':462,'multiline':False]
['text':' remaining space with padding indices','line_number':463,'multiline':False]
['text':' Max sparse and bfloat16 are not supported','line_number':477,'multiline':False]
['text':' If 1D input does not use a padding index, we still need one for the 2D input,','line_number':487,'multiline':False]
['text':' so we can add one dummy word to the weights to act as the padded word','line_number':488,'multiline':False]
['text':' Sometimes, half dtype gradients mismatch by a greater amount','line_number':526,'multiline':False]
['text':' than other dtypes','line_number':527,'multiline':False]
['text':' Check correctness of torch.nn.functional.embedding_bag forward and','line_number':536,'multiline':False]
['text':' backward functions with padding_idx, given a 2D indices input. Compare','line_number':537,'multiline':False]
['text':' against torch.nn.functional.embedding followed by a reduction.','line_number':538,'multiline':False]
['text':' Use a Python implementation of embedding_bag with padding_idx support','line_number':543,'multiline':False]
['text':' to check torch.nn.functional.embedding_bag correctness','line_number':544,'multiline':False]
['text':' We must avoid including elements at padding_idx in the','line_number':556,'multiline':False]
['text':' sum/mean, so multiply those elements by 0, and multiply','line_number':557,'multiline':False]
['text':' all other elements by 1','line_number':558,'multiline':False]
['text':' We must avoid allowing elements at padding_idx to be chosen','line_number':567,'multiline':False]
['text':' as the max, so set those elements to negative infinity','line_number':568,'multiline':False]
['text':' If a row is all padding, set its corresponding result row to 0.','line_number':576,'multiline':False]
['text':' This is needed because the above mean and max mode','line_number':577,'multiline':False]
['text':' implementations set these elements to nan and -inf, respectively','line_number':578,'multiline':False]
['text':' Max sparse and bfloat16 are not supported','line_number':591,'multiline':False]
['text':' Fill one row with duplicate index so we can test with a fully','line_number':602,'multiline':False]
['text':' padded row','line_number':603,'multiline':False]
['text':' Check forward with a Python implementation of padding_idx embedding_bag','line_number':615,'multiline':False]
['text':' Sometimes, half dtype gradients mismatch by a greater amount','line_number':637,'multiline':False]
['text':' than other dtypes','line_number':638,'multiline':False]
['text':' nn.Embedding only takes LongTensor as input','line_number':652,'multiline':False]
['text':' negative out-of-bound','line_number':680,'multiline':False]
['text':' positive out-of-bound','line_number':682,'multiline':False]
['text':' Only `sum` supports per_sample_weight','line_number':686,'multiline':False]
['text':' Failure 1: mismatched embeddings / per_sample_weights dtype','line_number':734,'multiline':False]
['text':' Failure 2.1: input/per_sample_weights have different sizes (1d input)','line_number':746,'multiline':False]
['text':' Failure 2.2: input/per_sample_weights have different sizes (2d input)','line_number':753,'multiline':False]
['text':' Failure 3: Unsupported per_sample_weights and mode=('max', 'mean')','line_number':760,'multiline':False]
['text':' Test empty input and per sample weight, and backward pass. There was a CUDA','line_number':832,'multiline':False]
['text':' invalid configuration bug (more context in #46572)','line_number':833,'multiline':False]
['text':' the reference impl doesn't have grad fn for empty input; but the grad should','line_number':853,'multiline':False]
['text':' simply be a zero tensor','line_number':854,'multiline':False]
['text':' To prevent large gradients, weights should sum to 1 for each bag','line_number':974,'multiline':False]
['text':' We have more floating point error here because we are dealing with larger numbers','line_number':1009,'multiline':False]
['text':' Simple case','line_number':1031,'multiline':False]
['text':' B * L > 1000','line_number':1034,'multiline':False]
['text':' Large num_embedding','line_number':1037,'multiline':False]
['text':' Large embedding_dim','line_number':1040,'multiline':False]
['text':' Test CUDA Dense on half precision','line_number':1050,'multiline':False]
['text':' check a known test example','line_number':1069,'multiline':False]
['text':' check same example except as 2D (2 x 3)','line_number':1131,'multiline':False]
['text':' test all empty bags','line_number':1143,'multiline':False]
['text':' now compare EmbeddingBag vs Embedding + Sum/Mean, for constant bag length','line_number':1153,'multiline':False]
['text':' check that giving illegal input combos raises error','line_number':1161,'multiline':False]
['text':' see 'todo' in test_embedding_bag.','line_number':1189,'multiline':False]
['text':' TODO: figure out why precision on sparse embeddings isn't the','line_number':1192,'multiline':False]
['text':' same as for dense.','line_number':1193,'multiline':False]
['text':' This is non-contiguous strided.','line_number':1223,'multiline':False]
['text':' Contig-strided.','line_number':1224,'multiline':False]
['text':' currently fails on XLA','line_number':1243,'multiline':False]
['text':' currently fails on XLA','line_number':1254,'multiline':False]
