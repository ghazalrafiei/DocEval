['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]
['text':' Found in torch/testing/_comparison.py','line_number':66,'multiline':False]
['text':' Fill in the nans with the default rtol','line_number':77,'multiline':False]
['text':' torch.isclose() has weird behavior around see:','line_number':100,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/102400','line_number':101,'multiline':False]
['text':' Add a placeholder, an empty list causes "An empty arg_values was passed to @parametrize"','line_number':129,'multiline':False]
['text':' TODO use NT ops when we have implemented Max for NestedTensor instead of unrolling','line_number':170,'multiline':False]
['text':' [N, T, D]','line_number':190,'multiline':False]
['text':' [T, T]','line_number':191,'multiline':False]
['text':' Generate 3D results','line_number':199,'multiline':False]
['text':' [N, T, D]','line_number':202,'multiline':False]
['text':' [N, T, D]','line_number':205,'multiline':False]
['text':' Expect uint8 type not supported','line_number':239,'multiline':False]
['text':' Expect long type not supported','line_number':250,'multiline':False]
['text':' disable fast path','line_number':287,'multiline':False]
['text':' enable fast path','line_number':289,'multiline':False]
['text':' bs, seqlen, d_model','line_number':305,'multiline':False]
['text':' each input is (input, mask)','line_number':339,'multiline':False]
['text':' softmax.cu switches from fast->slowpath at masked seqlen 1024. test 1024.','line_number':356,'multiline':False]
['text':' softmax.cu switches from fast->slowpath at masked seqlen 1024. test range of masks above 1024.','line_number':368,'multiline':False]
['text':' float input','line_number':381,'multiline':False]
['text':' bool mask','line_number':382,'multiline':False]
['text':' reference','line_number':391,'multiline':False]
['text':' Make sure fastpath_output is same shape as slowpath_output and mask.','line_number':392,'multiline':False]
['text':' When enable_nested_tensor=true, fastpath_output may be smaller than input tensor.','line_number':393,'multiline':False]
['text':' Eg if input bs=1, seqlen=6, and we mask out 2 tokens, fastpath_output will have bs=1, seqlen=4.','line_number':394,'multiline':False]
['text':' Expand back to old size to match.','line_number':395,'multiline':False]
['text':' no garauntees on output corresponding to masked tokens, so they may vary between slow/fast path. set all to 0.','line_number':400,'multiline':False]
['text':' set constant weights of the model','line_number':420,'multiline':False]
['text':' set constant weights of the model','line_number':470,'multiline':False]
['text':' this is a deterministic test for TransformerEncoder','line_number':480,'multiline':False]
['text':' deterministic input','line_number':497,'multiline':False]
['text':' all 0 src_mask','line_number':524,'multiline':False]
['text':' all 0','line_number':530,'multiline':False]
['text':' test case 2, multiple layers no norm','line_number':554,'multiline':False]
['text':' test case 3, multiple layers with norm','line_number':591,'multiline':False]
['text':' d_model = 4','line_number':592,'multiline':False]
['text':' TODO: remove set default dtype to double by making ref_output more precise.','line_number':632,'multiline':False]
['text':' Added because this test was copied from test_nn.py, which has default','line_number':633,'multiline':False]
['text':' dtype double. If default dtype is float, tests will say tensors not close because','line_number':634,'multiline':False]
['text':' ref output precision too low','line_number':635,'multiline':False]
['text':' transformer fast path requires no grad','line_number':640,'multiline':False]
['text':' here I masked 5 columns instead of just one','line_number':770,'multiline':False]
['text':' CPU unit test has_torch_functions in test environment,','line_number':775,'multiline':False]
['text':'   preventing successful completion','line_number':776,'multiline':False]
['text':' here I masked 5 columns instead of just one','line_number':793,'multiline':False]
['text':' here I masked 5 columns instead of just one','line_number':794,'multiline':False]
['text':' here I masked 5 columns instead of just one','line_number':795,'multiline':False]
['text':' here I masked 5 columns instead of just one','line_number':796,'multiline':False]
['text':' brazenly adapted from test_transformerencoderlayer_src_mask to test execution of','line_number':814,'multiline':False]
['text':' torchscripted transformerencoderlayer subclass','line_number':815,'multiline':False]
['text':' bs, seqlen, d_model','line_number':833,'multiline':False]
['text':' bs, seqlen, d_model','line_number':871,'multiline':False]
['text':' torch.nn.Embedding. Must have a padding_idx field','line_number':901,'multiline':False]
['text':' torch encoder that you can map weights from','line_number':904,'multiline':False]
['text':' make embedding behavior same as other encoders','line_number':917,'multiline':False]
['text':' TODO: verify what this is','line_number':922,'multiline':False]
['text':' (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)','line_number':973,'multiline':False]
['text':' (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)','line_number':982,'multiline':False]
['text':' TODO: Support cross-device / dtype testing properly when instantiate_device_type_tests() is used.','line_number':985,'multiline':False]
['text':' This test compares python and C++ implementations of SDP.','line_number':992,'multiline':False]
['text':' Python impl only supports float mask and 3D inputs.','line_number':1013,'multiline':False]
['text':' NB: Don't pass attn_mask here','line_number':1028,'multiline':False]
['text':' Error case: both explicit attn_mask and is_causal are set','line_number':1032,'multiline':False]
['text':' If mock was called, fastpath was taken','line_number':1092,'multiline':False]
['text':' If mock was called with nested tensors, sparsity fastpath was taken','line_number':1095,'multiline':False]
['text':' Left aligned mask results in sparsity fastpath','line_number':1110,'multiline':False]
['text':' Not aligned mask results in fastpath','line_number':1113,'multiline':False]
['text':' If nested tensor disabled, fastpath is always taken','line_number':1119,'multiline':False]
['text':' Fast path is taken if both attention mask and key padding mask are present','line_number':1122,'multiline':False]
['text':' Mask check disabled results in sparisty fastpath, independently of the mask','line_number':1128,'multiline':False]
['text':' Test failing MHA when bias was NoneType','line_number':1132,'multiline':False]
['text':' completes without error','line_number':1138,'multiline':False]
['text':' training with is_causal','line_number':1141,'multiline':False]
['text':' inference with is_causal','line_number':1169,'multiline':False]
['text':' Can't give only is_causal','line_number':1178,'multiline':False]
['text':' # Passing a causal mask sets is_causal to 1','line_number':1183,'multiline':False]
['text':' check expected numerical values with all kernels','line_number':1193,'multiline':False]
['text':' fails with embedding size not multiple of 4','line_number':1220,'multiline':False]
['text':' Missing EFFICIENT_ATTENTION','line_number':1229,'multiline':False]
['text':' See check_requires_grad_and_head_dim_gt64_and_sm_ge86 in pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.h','line_number':1258,'multiline':False]
['text':' Should not fail because inputs don't require grad','line_number':1266,'multiline':False]
['text':' Should fail because inputs require grad','line_number':1271,'multiline':False]
['text':' Dim is not 4','line_number':1299,'multiline':False]
['text':'  Fused Kernels don't support broadcasting for dense inputs','line_number':1317,'multiline':False]
['text':' Passing in a q,k,v with 0 length sequences will error','line_number':1332,'multiline':False]
['text':' Passing in a q,k,v with 0 length sequences will error','line_number':1346,'multiline':False]
['text':' The embed dim per head is not divisible by 8 for flash attention','line_number':1361,'multiline':False]
['text':' Invalid dtype for both Flash Attention and Mem Efficient Attention','line_number':1377,'multiline':False]
['text':' Failures for unsupported SDP args','line_number':1389,'multiline':False]
['text':' Non-None attention mask','line_number':1393,'multiline':False]
['text':' The alignment is depdent on arch so we specifiy SM80OrLater','line_number':1401,'multiline':False]
['text':' Note: do not truncate the list according to platforms. These tests should always raise errors.','line_number':1446,'multiline':False]
['text':' Different datatypes','line_number':1450,'multiline':False]
['text':' Different devices','line_number':1460,'multiline':False]
['text':' 1 dimensional input','line_number':1470,'multiline':False]
['text':' Missing EFFICIENT_ATTENTION','line_number':1478,'multiline':False]
['text':' one of k,v needs to be broadcasted and other has non consistent seq_len dim','line_number':1481,'multiline':False]
['text':' make sure some seq_lens are 0','line_number':1534,'multiline':False]
['text':' create a dense query','line_number':1563,'multiline':False]
['text':' This should match the block sizes in the CUDA kernel','line_number':1594,'multiline':False]
['text':' Mask is only interesting when we are setting dropout','line_number':1595,'multiline':False]
['text':' Only include sm86 and sm89, exclude sm80 (A100)','line_number':1599,'multiline':False]
['text':' Test that cpu and nestedtensor cpu return MATH backend','line_number':1675,'multiline':False]
['text':' (B, nh, T, hs)','line_number':1731,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/105190.','line_number':1765,'multiline':False]
['text':' Reshape S using PyTorch native functions','line_number':1810,'multiline':False]
['text':' Need to zero out things not in attention_mask in case S was initialized with random values','line_number':1822,'multiline':False]
['text':' and some of those values aren't overwritten.','line_number':1823,'multiline':False]
['text':' Test Packed','line_number':1946,'multiline':False]
['text':' Missing nested and EFFICIENT_ATTENTION','line_number':1969,'multiline':False]
['text':' Test Packed','line_number':1990,'multiline':False]
['text':' Cast up and compare','line_number':2079,'multiline':False]
['text':' Small matrices','line_number':2082,'multiline':False]
['text':' Cast up and compare','line_number':2128,'multiline':False]
['text':' Since we are doing the compute on fp16 we have to bump the tolerance','line_number':2129,'multiline':False]
['text':' Bump down the tolearnce for blfoat16','line_number':2130,'multiline':False]
['text':' Missing nested and EFFICIENT_ATTENTION','line_number':2135,'multiline':False]
['text':' Change dtype to float32 so that efficient attention should get chosen','line_number':2155,'multiline':False]
['text':' Need big seq_len to ensure that num_splits > 1','line_number':2203,'multiline':False]
['text':' Run once to establish baseline','line_number':2214,'multiline':False]
['text':' Re-run the op with the same upward grad and check that the backward is','line_number':2220,'multiline':False]
['text':' not deterministic','line_number':2221,'multiline':False]
['text':' Re-run the op with the same upward grad and check that the backward is','line_number':2239,'multiline':False]
['text':' deterministic now that we have enforced it','line_number':2240,'multiline':False]
['text':' verified passing successfully on H100','line_number':2251,'multiline':False]
['text':' Run the math kernel on low precision references','line_number':2283,'multiline':False]
['text':' Create real output','line_number':2289,'multiline':False]
['text':' Set the seed and run the kernel','line_number':2291,'multiline':False]
['text':' High Precision Math Reference','line_number':2297,'multiline':False]
['text':' Low Precision Math Reference','line_number':2300,'multiline':False]
['text':' Create the dropout_mask','line_number':2306,'multiline':False]
['text':' High Precision Math Reference','line_number':2309,'multiline':False]
['text':' Low Precision Math Reference','line_number':2312,'multiline':False]
['text':' [Note] Fused Tolerances','line_number':2323,'multiline':False]
['text':' Establish the numerical error between the "true" high precision math output','line_number':2324,'multiline':False]
['text':' and the low precision math reference. We use this reference for the atol','line_number':2325,'multiline':False]
['text':' And we use the default rtol for the low precision type.','line_number':2326,'multiline':False]
['text':' We then provide a fudge factor for gradients respectively to account','line_number':2327,'multiline':False]
['text':' for the use of the fused kernel rather than the eager implemntation.','line_number':2328,'multiline':False]
['text':' Fudge Factor when dropout is enabled','line_number':2331,'multiline':False]
['text':' TODO: Investigate why grad_k needs larger tolerances','line_number':2337,'multiline':False]
['text':' Run the math kernel on low precision references','line_number':2387,'multiline':False]
['text':' Create real output','line_number':2395,'multiline':False]
['text':' Set the seed and run the kernel','line_number':2397,'multiline':False]
['text':' High Precision Math Reference','line_number':2404,'multiline':False]
['text':' Low Precision Math Reference','line_number':2407,'multiline':False]
['text':' Create the dropout_mask','line_number':2413,'multiline':False]
['text':' High Precision Math Reference','line_number':2417,'multiline':False]
['text':' Low Precision Math Reference','line_number':2421,'multiline':False]
['text':' [Note] Fused Tolerances','line_number':2433,'multiline':False]
['text':' Establish the numerical error between the "true" high precision math output','line_number':2434,'multiline':False]
['text':' and the low precision math reference. We use this reference for the atol','line_number':2435,'multiline':False]
['text':' And we use the default rtol for the low precision type.','line_number':2436,'multiline':False]
['text':' We then provide a fudge factor for gradients respectively to account','line_number':2437,'multiline':False]
['text':' for the use of the fused kernel rather than the eager implemntation.','line_number':2438,'multiline':False]
['text':' Fudge Factor when dropout is enabled','line_number':2441,'multiline':False]
['text':' TODO: Investigate why grad_k needs larger tolerances','line_number':2448,'multiline':False]
['text':' Run the math kernel on low precision references','line_number':2506,'multiline':False]
['text':' Problem: We pad sizes in the composite region of the top level SDPA. But we need the','line_number':2515,'multiline':False]
['text':' Debug mask when have dropout. So I am going to manualy pad up here when testing dropout','line_number':2516,'multiline':False]
['text':' High Precision Math Reference','line_number':2520,'multiline':False]
['text':' Low Precision Math Reference','line_number':2523,'multiline':False]
['text':' scale needs to be calculated on the og head_size','line_number':2530,'multiline':False]
['text':' Build dropout_mask','line_number':2537,'multiline':False]
['text':' High Precision Math Reference','line_number':2548,'multiline':False]
['text':' Low Precision Math Reference','line_number':2551,'multiline':False]
['text':' backward for flash attention on sm86 and sm89 for headdim > 64 currently disabled','line_number':2558,'multiline':False]
['text':' See [Note] Fused Tolerances above','line_number':2566,'multiline':False]
['text':' TODO: Investigate why grad_q needs larger tolerances','line_number':2570,'multiline':False]
['text':' FIXME: "capturing stream has unjoined work"','line_number':2586,'multiline':False]
['text':' Build dropout_mask','line_number':2618,'multiline':False]
['text':' Run the math kernel on low precision references','line_number':2645,'multiline':False]
['text':' warmup','line_number':2651,'multiline':False]
['text':' Set the global seed before capture','line_number':2654,'multiline':False]
['text':' Create real output','line_number':2663,'multiline':False]
['text':' Create real output','line_number':2675,'multiline':False]
['text':' test non-zero intragraph offset','line_number':2677,'multiline':False]
['text':' Create real output','line_number':2678,'multiline':False]
['text':' replays produce different results','line_number':2688,'multiline':False]
['text':' High Precision Math Reference','line_number':2693,'multiline':False]
['text':' Low Precision Math Reference','line_number':2696,'multiline':False]
['text':' Create the dropout_mask','line_number':2700,'multiline':False]
['text':' High Precision Math Reference','line_number':2703,'multiline':False]
['text':' Low Precision Math Reference','line_number':2707,'multiline':False]
['text':' [Note] Fused Tolerances','line_number':2720,'multiline':False]
['text':' Establish the numerical error between the "true" high precision math output','line_number':2721,'multiline':False]
['text':' and the low precision math reference. We use this reference for the atol','line_number':2722,'multiline':False]
['text':' And we use the default rtol for the low precision type.','line_number':2723,'multiline':False]
['text':' We then provide a fudge factor for gradients respectively to account','line_number':2724,'multiline':False]
['text':' for the use of the fused kernel rather than the eager implemntation.','line_number':2725,'multiline':False]
['text':' Fudge Factor when dropout is enabled','line_number':2728,'multiline':False]
['text':' TODO: Investigate why grad_k needs larger tolerances','line_number':2734,'multiline':False]
['text':' Nested Tensor','line_number':2749,'multiline':False]
['text':' make sure some seq_lens are 1','line_number':2757,'multiline':False]
['text':' Nested tensor','line_number':2784,'multiline':False]
['text':' handle case where all batch_sizes are 1','line_number':2821,'multiline':False]
['text':' handle case where all num_heads are 1','line_number':2828,'multiline':False]
['text':' (1, seq_len, 1, head_dim) -> (batch, seq_len, num_heads, head_dim)','line_number':2841,'multiline':False]
['text':' (1, seq_len, num_heads, head_dim) -> (batch, seq_len, num_heads, head_dim)','line_number':2845,'multiline':False]
['text':' (batch, seq_len, 1, head_dim) -> (batch, seq_len, num_heads, head_dim)','line_number':2848,'multiline':False]
['text':' create a dense query','line_number':2882,'multiline':False]
['text':' (1, 1, num_heads, head_dim) -> (batch, 1, num_heads, head_dim)','line_number':2887,'multiline':False]
['text':' (batch, seq_lens, 1, head_dim) -> (batch, seq_lens, num_heads, head_dim)','line_number':2889,'multiline':False]
['text':' Nested tensor','line_number':2908,'multiline':False]
['text':' TODO we should support this','line_number':2922,'multiline':False]
['text':' Set one entry to max length','line_number':2928,'multiline':False]
['text':' Run the math kernel on low precision references','line_number':2941,'multiline':False]
['text':' High Precision Math Reference','line_number':2956,'multiline':False]
['text':' Low Precision Math Reference','line_number':2959,'multiline':False]
['text':' Create real output','line_number':2963,'multiline':False]
['text':' High Precision Math Reference','line_number':2992,'multiline':False]
['text':' Low Precision Math Reference','line_number':2996,'multiline':False]
['text':' See [Note] Fused Tolerances above','line_number':3007,'multiline':False]
['text':' Use default assert_close tolerances for dtypes','line_number':3056,'multiline':False]
['text':' No support for the second variant for now','line_number':3067,'multiline':False]
['text':' No support for the second variant for now','line_number':3096,'multiline':False]
