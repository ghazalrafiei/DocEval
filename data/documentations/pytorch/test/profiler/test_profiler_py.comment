['text':' Owner(s): ["oncall: profiler"]','line_number':1,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/69023','line_number':89,'multiline':False]
['text':' with CUDA events leaking the increase in memory was ~7 MB between','line_number':105,'multiline':False]
['text':' profiler invocations above','line_number':106,'multiline':False]
['text':' Only testing that emit_nvtx runs when','line_number':130,'multiline':False]
['text':' record_shapes option is enabled.','line_number':131,'multiline':False]
['text':' repro taken from #75504','line_number':144,'multiline':False]
['text':' Launch in a separate process to catch hanging/illegal memory errors','line_number':145,'multiline':False]
['text':' and to make sure CUPTI isn't already initialized.','line_number':146,'multiline':False]
['text':' ^ this will throw an exception if the script fails.','line_number':172,'multiline':False]
['text':' Only testing that emit_itt runs when','line_number':193,'multiline':False]
['text':' record_shapes option is enabled.','line_number':194,'multiline':False]
['text':' The object itself is an iterator','line_number':279,'multiline':False]
['text':' This creates the 1st iterator','line_number':281,'multiline':False]
['text':' type: ignore[attr-defined]','line_number':282,'multiline':False]
['text':' Create a temp file to save execution trace data.','line_number':357,'multiline':False]
['text':' cleanup','line_number':383,'multiline':False]
['text':' Create a temp file to save execution trace data.','line_number':399,'multiline':False]
['text':' Expected tensor object tuple size, in th form of:','line_number':417,'multiline':False]
['text':' [tensor_id, storage_id, offset, numel, itemsize, device_str]','line_number':418,'multiline':False]
['text':' Check if tensor tuple representation size is correct.','line_number':427,'multiline':False]
['text':' Create a temp file to save execution trace data.','line_number':435,'multiline':False]
['text':' Create a temp file to save execution trace data.','line_number':476,'multiline':False]
['text':' avoid automatic inlining','line_number':526,'multiline':False]
['text':' TODO: https://github.com/pytorch/kineto/issues/617','line_number':573,'multiline':False]
['text':' Large number of background threads','line_number':599,'multiline':False]
['text':' some of which finish during profiling','line_number':605,'multiline':False]
['text':' And the profiled section is also multithreaded','line_number':609,'multiline':False]
['text':' Main thread','line_number':628,'multiline':False]
['text':' Fixed point that we can use to test capture of functions','line_number':652,'multiline':False]
['text':' which are already running when profiling is enabled.','line_number':653,'multiline':False]
['text':' Threads added while the profiler are running will not be observed','line_number':693,'multiline':False]
['text':' since there is no way to hook into Python's thread start call to','line_number':694,'multiline':False]
['text':' register the observer. These are here purely to verify safety.','line_number':695,'multiline':False]
['text':' It is very important that we clean up everything because the','line_number':708,'multiline':False]
['text':' Python tracer will detect ALL active threads. (Even orphans from','line_number':709,'multiline':False]
['text':' prior failed tests.) If we don't clean up properly we can','line_number':710,'multiline':False]
['text':' contaminate subsequent tests.','line_number':711,'multiline':False]
['text':' Profiler uses uint64_t max as a placeholder until TID can be determined.','line_number':732,'multiline':False]
['text':' rerun to avoid initial start overhead','line_number':764,'multiline':False]
['text':' print(output)','line_number':769,'multiline':False]
['text':' p.export_chrome_trace("/tmp/test_trace.json")','line_number':785,'multiline':False]
['text':' collecting allocs / deallocs','line_number':818,'multiline':False]
['text':' Memory should be an instantaneous event.','line_number':887,'multiline':False]
['text':' check top-level memory events','line_number':933,'multiline':False]
['text':' Memory should be an instantaneous event.','line_number':990,'multiline':False]
['text':' Create multiple instances, expect each func is hooked only one time.','line_number':1112,'multiline':False]
['text':' Nested wrappers(repeated patching) will make following test fail.','line_number':1113,'multiline':False]
['text':' "+1" because the final iteration will enter __next__ but skip the loop body.','line_number':1130,'multiline':False]
['text':' Test on pickle/unpickle. Expect to work in multi-processing.','line_number':1137,'multiline':False]
['text':' Test on customized optimizer.','line_number':1143,'multiline':False]
['text':' test that nested tensor won't cause exception during flop compute','line_number':1169,'multiline':False]
['text':' print(output)','line_number':1202,'multiline':False]
['text':' p.export_chrome_trace("/tmp/test_trace_" + str(called_num[0]) + ".json")','line_number':1203,'multiline':False]
['text':' case without schedule','line_number':1223,'multiline':False]
['text':' print(output)','line_number':1231,'multiline':False]
['text':' Manually call the hook. TODO: Remove this once we add the','line_number':1280,'multiline':False]
['text':' profiler step hooks in the Optimizer class that will get triggered above.','line_number':1281,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/88446','line_number':1282,'multiline':False]
['text':' test case for gzip file format','line_number':1356,'multiline':False]
['text':' read the trace and expect valid json','line_number':1411,'multiline':False]
['text':' if the JSON generated by export_chrome_trace is not valid, this will throw and fail the test.','line_number':1412,'multiline':False]
['text':' test empty trace','line_number':1416,'multiline':False]
['text':' saving an empty trace','line_number':1419,'multiline':False]
['text':' Same test but for cuda.','line_number':1423,'multiline':False]
['text':' Now validate the json','line_number':1435,'multiline':False]
['text':' This test is broken on Windows, the likely reason is that kineto/CUPTI','line_number':1532,'multiline':False]
['text':' is not supported that particular environment. Once the CI stabilizes','line_number':1533,'multiline':False]
['text':' we can narrow the condition so Windows is checked as well (TODO)','line_number':1534,'multiline':False]
['text':' fname = "/tmp/kineto_out.json"','line_number':1554,'multiline':False]
['text':' Autograd profiler','line_number':1577,'multiline':False]
['text':' Kineto profiler','line_number':1581,'multiline':False]
['text':' intentionally vague tests to protect against possible future changes','line_number':1618,'multiline':False]
['text':' of mm to addmm or other impl, or changing internal order of args','line_number':1619,'multiline':False]
['text':' AFAIK event list is part of legacy profiler and/or used when kineto is not available.','line_number':1802,'multiline':False]
['text':' This test has basic sanity checks to test against obvious regressions.','line_number':1803,'multiline':False]
['text':' event_list._build_tree()','line_number':1810,'multiline':False]
['text':' Views of tensors can share the same storage, but have different TensorImpls','line_number':1859,'multiline':False]
['text':' Resize should create a new data_ptr but keep the TensorImpl the same.','line_number':1869,'multiline':False]
['text':' `.set_` points a Tensor at an existing storage.','line_number':1873,'multiline':False]
['text':' Profiler matches ground truth from Python API.','line_number':1888,'multiline':False]
['text':' Views are handled correctly.','line_number':1891,'multiline':False]
['text':' The same Tensor used in multiple calls gives identical results.','line_number':1895,'multiline':False]
['text':' Mutations to the underlying storage are reflected. (But ID is shared.)','line_number':1900,'multiline':False]
['text':' Calling `set_` with an existing Tensor makes them share an ID.','line_number':1906,'multiline':False]
['text':' Determines if new storage is created before or after the old one','line_number':1940,'multiline':False]
['text':' is destroyed.','line_number':1941,'multiline':False]
['text':' This keeps the StorageImpls alive and preserves the chain.','line_number':1974,'multiline':False]
['text':' (Despite the `set_()` call.)','line_number':1975,'multiline':False]
['text':' Free storage in a deterministic fashion.','line_number':1979,'multiline':False]
['text':' Determines if new storage is created before or after the old one','line_number':1984,'multiline':False]
['text':' is destroyed.','line_number':1985,'multiline':False]
['text':' Free storage in a deterministic fashion.','line_number':2044,'multiline':False]
['text':' Mark `x`','line_number':2152,'multiline':False]
['text':' Mark weight momentum','line_number':2155,'multiline':False]
['text':' Mark weight gradient','line_number':2156,'multiline':False]
['text':' Marked Tensors act as ground truth for python tracer IDs.','line_number':2165,'multiline':False]
['text':' Use linear op to identify weight ground truth.','line_number':2173,'multiline':False]
['text':' Module','line_number':2179,'multiline':False]
['text':' Optimizer','line_number':2194,'multiline':False]
['text':' Weight and bias','line_number':2200,'multiline':False]
['text':' Check that we handle first step (lazy initalization) and steady state.','line_number':2208,'multiline':False]
['text':' Introduce other operations and allocations to check robustness','line_number':2214,'multiline':False]
['text':' We need to use `x` post resize for profiler to determine its ID.','line_number':2220,'multiline':False]
['text':' Introduce other operations and allocations to check robustness','line_number':2223,'multiline':False]
['text':' Ensure `x` is the last variable collected to make it easier to','line_number':2226,'multiline':False]
['text':' find the deallocation event.','line_number':2227,'multiline':False]
['text':' Make sure IDs are consistent between allocations and op inputs','line_number':2246,'multiline':False]
['text':' Destruction of the old storage for x.','line_number':2256,'multiline':False]
['text':' Make sure ID is retained through change in storage.','line_number':2260,'multiline':False]
['text':' Deletion when `x` goes out of scope.','line_number':2264,'multiline':False]
['text':' The second argument to the add gets promotoed to a zerodim Tensor','line_number':2432,'multiline':False]
['text':' avoiding OptInfo duplicates from iterations','line_number':2486,'multiline':False]
['text':' Make sure the profiler collected all optimizer state and check','line_number':2509,'multiline':False]
['text':' that the address recorded by the profiler is correct.','line_number':2510,'multiline':False]
['text':' fp32 -> 4 bytes','line_number':2558,'multiline':False]
['text':' total_reserved is only for CUDACachingAllocator','line_number':2565,'multiline':False]
['text':' Python will only close over variables used in the function.','line_number':2590,'multiline':False]
['text':' Use a factory function to ensure the test scope never sees strong','line_number':2605,'multiline':False]
['text':' references. `del` has strange semantics that interact with closures','line_number':2606,'multiline':False]
['text':' at an AST level, so this is simpler.','line_number':2607,'multiline':False]
['text':' `outer` holds the last reference via closure.','line_number':2613,'multiline':False]
['text':' We have to use Mock because time series data is too flaky to test','line_number':2880,'multiline':False]
['text':' For traces with only cpu events, we expect empty queue depth list','line_number':2919,'multiline':False]
['text':' It is platform dependent whether the path will include "profiler/"','line_number':3195,'multiline':False]
