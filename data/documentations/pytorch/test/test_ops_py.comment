['text':' Owner(s): ["module: unknown"]','line_number':1,'multiline':False]
['text':' variant testing is only done with torch.float and torch.cfloat to avoid','line_number':88,'multiline':False]
['text':'   excessive test times and maximize signal to noise ratio','line_number':89,'multiline':False]
['text':' Get names of all the operators which have ref in their entry in OpInfo (testing infra)','line_number':94,'multiline':False]
['text':'   except for elementwise unary operators (separately implemented in test/test_unary_ufuncs.py),','line_number':95,'multiline':False]
['text':'   elementwise binary operators (separately implemented in test_binary_ufuncs.py),','line_number':96,'multiline':False]
['text':'   reduction operations (separately impelemented in test_reductions.py),','line_number':97,'multiline':False]
['text':'   and Spectral Functions (separately implemented for only 1D as of now, in test/test_spectral_ops.py)','line_number':98,'multiline':False]
['text':' Create a list of operators that are a subset of _ref_test_ops but don't have a','line_number':119,'multiline':False]
['text':' numpy ref to compare them too, If both CPU and CUDA are compared to numpy','line_number':120,'multiline':False]
['text':' then they do not need to be compared to each other','line_number':121,'multiline':False]
['text':' Tests that apply to all operators and aren't related to any particular','line_number':126,'multiline':False]
['text':'   system','line_number':127,'multiline':False]
['text':' Verifies, on teardown, that no OpInfo is still using dynamic dtypes in CI','line_number':131,'multiline':False]
['text':' Assure no opinfo entry has dynamic_dtypes','line_number':141,'multiline':False]
['text':' Validates that each OpInfo works correctly on different CUDA devices','line_number':149,'multiline':False]
['text':' NOTE: only tests on first sample','line_number':156,'multiline':False]
['text':' reduction version of these operators','line_number':181,'multiline':False]
['text':' not pointwise','line_number':194,'multiline':False]
['text':' could not find op from kernel dispatch string','line_number':222,'multiline':False]
['text':' no op definition for it, but defined with DEFINE_DISPATCH ?','line_number':232,'multiline':False]
['text':' TODO: tags are not propagated to generated overload,','line_number':245,'multiline':False]
['text':' and there's no way of specifying them','line_number':246,'multiline':False]
['text':' Tests that the function and its (ndarray-accepting) reference produce the same','line_number':255,'multiline':False]
['text':'   values on the tensors from sample_inputs func for the corresponding op.','line_number':256,'multiline':False]
['text':' This test runs in double and complex double precision because','line_number':257,'multiline':False]
['text':' NumPy does computation internally using double precision for many functions','line_number':258,'multiline':False]
['text':' resulting in possible equality check failures.','line_number':259,'multiline':False]
['text':' noqa: E121','line_number':268,'multiline':False]
['text':' Sets the default dtype to NumPy's default dtype of double','line_number':271,'multiline':False]
['text':' Tests that the cpu and gpu results are consistent','line_number':278,'multiline':False]
['text':' output_process_fn_grad has a very unfortunate name','line_number':297,'multiline':False]
['text':' We use this function in linalg extensively to postprocess the inputs of functions','line_number':298,'multiline':False]
['text':' that are not completely well-defined. Think svd and muliplying the singular vectors by -1.','line_number':299,'multiline':False]
['text':' CPU and CUDA implementations of the SVD can return valid SVDs that are different.','line_number':300,'multiline':False]
['text':' We use this function to compare them.','line_number':301,'multiline':False]
['text':' Lower tolerance because we are running this as a `@slowTest`','line_number':305,'multiline':False]
['text':' Don't want the periodic tests to fail frequently','line_number':306,'multiline':False]
['text':' Tests that experimental Python References can propagate shape, dtype,','line_number':309,'multiline':False]
['text':' and device metadata properly.','line_number':310,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/78050 for a discussion of stride propagation.','line_number':311,'multiline':False]
['text':' TODO: iterate over requires_grad true/false','line_number':325,'multiline':False]
['text':' NOTE: this test works by comparing the reference','line_number':360,'multiline':False]
['text':' Computes the dtype the more precise computatino would occur in','line_number':394,'multiline':False]
['text':' Note: bool and integer dtypes do not have more','line_number':397,'multiline':False]
['text':' precise dtypes -- they simply must be close','line_number':398,'multiline':False]
['text':' Checks if the results are close','line_number':405,'multiline':False]
['text':' Raises the error if the precise dtype comparison wouldn't be','line_number':416,'multiline':False]
['text':' different','line_number':417,'multiline':False]
['text':' Goes to next sample if these results are close','line_number':424,'multiline':False]
['text':' If the results are not close, checks that the','line_number':428,'multiline':False]
['text':' reference is more accurate than the torch op','line_number':429,'multiline':False]
['text':' Special-cases boolean comparisons','line_number':441,'multiline':False]
['text':' TODO: consider adding some tolerance to this comparison','line_number':461,'multiline':False]
['text':' Reports numerical accuracy discrepancies','line_number':466,'multiline':False]
['text':' Tests that experimental Python References perform the same computation','line_number':471,'multiline':False]
['text':' as the operators they reference, when operator calls in the torch','line_number':472,'multiline':False]
['text':' namesapce are remapped to the refs namespace (torch.foo becomes refs.foo).','line_number':473,'multiline':False]
['text':' In this test, primTorch refs call into the refs namespace','line_number':478,'multiline':False]
['text':' For example, a ref with torch.foo in it will calls refs.foo instead','line_number':479,'multiline':False]
['text':' Direct calls to refs and prims are not affected','line_number':480,'multiline':False]
['text':' Tests that experimental Python References perform the same computation','line_number':483,'multiline':False]
['text':' as the operators they reference, when operator calls in the torch','line_number':484,'multiline':False]
['text':' namespace are preserved (torch.foo remains torch.foo).','line_number':485,'multiline':False]
['text':' In this test, refs call into the torch namespace (after the initial invocation)','line_number':490,'multiline':False]
['text':' For example, a ref with torch.foo in it will call torch.foo instead of refs.foo','line_number':491,'multiline':False]
['text':' Direct calls to refs and prims are not translated','line_number':492,'multiline':False]
['text':' skip zero-dim tensors for some composites of reduction operations and view','line_number':501,'multiline':False]
['text':' Tests that the function produces the same result when called with','line_number':565,'multiline':False]
['text':'   noncontiguous tensors.','line_number':566,'multiline':False]
['text':' TODO: get working with Windows by addressing failing operators','line_number':567,'multiline':False]
['text':' TODO: get working with ASAN by addressing failing operators','line_number':568,'multiline':False]
['text':' validates forward','line_number':589,'multiline':False]
['text':' Validate backward','line_number':595,'multiline':False]
['text':' Short-circuits if the op doesn't support grad in this device x dtype','line_number':596,'multiline':False]
['text':' Filter output elements that do not require grad','line_number':607,'multiline':False]
['text':' Nothing to do if it returns a scalar or things like that','line_number':619,'multiline':False]
['text':' Concatenate inputs into a tuple','line_number':622,'multiline':False]
['text':' Filter the elemnts that are tensors that require grad','line_number':634,'multiline':False]
['text':' Some functions may not use all the inputs to generate gradients. One of the','line_number':644,'multiline':False]
['text':' few examples of this "odd" behaviour is F.hinge_embedding_loss','line_number':645,'multiline':False]
['text':' Separates one case from the following test_out because many ops don't properly implement the','line_number':657,'multiline':False]
['text':'   incorrectly sized out parameter warning properly yet','line_number':658,'multiline':False]
['text':' Cases test here:','line_number':659,'multiline':False]
['text':'   - out= with the correct dtype and device, but the wrong shape','line_number':660,'multiline':False]
['text':' Prefers running in float32 but has a fallback for the first listed supported dtype','line_number':665,'multiline':False]
['text':' Ops from python_ref_db point to python decomps that are potentially','line_number':675,'multiline':False]
['text':' wrapped with `torch._prims_common.wrappers.out_wrapper`. Unwrap these','line_number':676,'multiline':False]
['text':' ops before testing to avoid clashing with OpInfo.supports_out','line_number':677,'multiline':False]
['text':' calls it normally to get the expected result','line_number':684,'multiline':False]
['text':' Short-circuits if output is not a single tensor or an','line_number':688,'multiline':False]
['text':'   iterable of tensors','line_number':689,'multiline':False]
['text':' Validates the op doesn't support out if it claims not to','line_number':697,'multiline':False]
['text':' A wrapper around map that works with single tensors and always','line_number':703,'multiline':False]
['text':'   instantiates the map. Used below to apply transforms to','line_number':704,'multiline':False]
['text':'   single tensor and iterable tensor outputs.','line_number':705,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':710,'multiline':False]
['text':' Extracts strides from a tensor or iterable of tensors into a tuple','line_number':713,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':718,'multiline':False]
['text':' Extracts data pointers from a tensor or iterable of tensors into a tuple','line_number':721,'multiline':False]
['text':' NOTE: only extracts on the CPU and CUDA device types since some','line_number':722,'multiline':False]
['text':'   device types don't have storage','line_number':723,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':731,'multiline':False]
['text':' Case Zero: out= with the correct dtype and device, but the wrong shape','line_number':753,'multiline':False]
['text':'   Expected behavior: if nonempty, resize with a warning.','line_number':754,'multiline':False]
['text':' Handles scalar tensor case (empty list)','line_number':759,'multiline':False]
['text':' Verifies the out values are correct','line_number':765,'multiline':False]
['text':' Additionally validates that the appropriate warning is thrown if a nonempty','line_number':768,'multiline':False]
['text':'   tensor is resized.','line_number':769,'multiline':False]
['text':' Validates ops implement the correct out= behavior','line_number':784,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch','line_number':785,'multiline':False]
['text':'   for a description of the correct behavior','line_number':786,'multiline':False]
['text':' Validates the following cases:','line_number':787,'multiline':False]
['text':'   - Case 0: out has the correct shape, dtype, and device but is full of extremal values','line_number':788,'multiline':False]
['text':'   - Case 1: out has the correct shape, dtype, and device but is noncontiguous','line_number':789,'multiline':False]
['text':'   - Case 2: out has the correct dtype and device, but is zero elements','line_number':790,'multiline':False]
['text':'   - Case 3: out has the correct shape and dtype, but is on a different device type','line_number':791,'multiline':False]
['text':'   - Case 4: out has the correct shape and device, but a dtype that cannot','line_number':792,'multiline':False]
['text':'       "safely" cast to','line_number':793,'multiline':False]
['text':'','line_number':794,'multiline':False]
['text':' Case 3 and 4 are slightly different when the op is a factory function:','line_number':795,'multiline':False]
['text':'   - if device, dtype are NOT passed, any combination of dtype/device should be OK for out','line_number':796,'multiline':False]
['text':'   - if device, dtype are passed, device and dtype should match','line_number':797,'multiline':False]
['text':' Prefers running in float32 but has a fallback for the first listed supported dtype','line_number':800,'multiline':False]
['text':' Ops from python_ref_db point to python decomps that are potentially','line_number':803,'multiline':False]
['text':' wrapped with `torch._prims_common.wrappers.out_wrapper`. Unwrap these','line_number':804,'multiline':False]
['text':' ops before testing to avoid clashing with OpInfo.supports_out','line_number':805,'multiline':False]
['text':' calls it normally to get the expected result','line_number':811,'multiline':False]
['text':' Short-circuits if output is not a single tensor or an','line_number':815,'multiline':False]
['text':'   iterable of tensors','line_number':816,'multiline':False]
['text':' Validates the op doesn't support out if it claims not to','line_number':824,'multiline':False]
['text':' A wrapper around map that works with single tensors and always','line_number':830,'multiline':False]
['text':'   instantiates the map. Used below to apply transforms to','line_number':831,'multiline':False]
['text':'   single tensor and iterable tensor outputs.','line_number':832,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':837,'multiline':False]
['text':' Extracts strides from a tensor or iterable of tensors into a tuple','line_number':840,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':845,'multiline':False]
['text':' Extracts data pointers from a tensor or iterable of tensors into a tuple','line_number':848,'multiline':False]
['text':' NOTE: only extracts on the CPU and CUDA device types since some','line_number':849,'multiline':False]
['text':'   device types don't have storage','line_number':850,'multiline':False]
['text':' assumes (see above) that out is an iterable of tensors','line_number':858,'multiline':False]
['text':' Case 0: out= with the correct shape, dtype, and device','line_number':878,'multiline':False]
['text':'   but NaN values for floating point and complex tensors, and','line_number':879,'multiline':False]
['text':'   maximum values for integer tensors.','line_number':880,'multiline':False]
['text':'   Expected behavior: out= values have no effect on the computation.','line_number':881,'multiline':False]
['text':' for non-integer types fills with NaN','line_number':887,'multiline':False]
['text':' Case 1: out= with the correct shape, dtype, and device,','line_number':893,'multiline':False]
['text':'   but noncontiguous.','line_number':894,'multiline':False]
['text':'   Expected behavior: strides are respected and `out` storage is not changed.','line_number':895,'multiline':False]
['text':' Case 2: out= with the correct dtype and device, but has no elements.','line_number':903,'multiline':False]
['text':'   Expected behavior: resize without warning.','line_number':904,'multiline':False]
['text':' Also validates that no warning is thrown when this out is resized','line_number':910,'multiline':False]
['text':' Verifies no warning is a resize warning','line_number':916,'multiline':False]
['text':' Case 3: out= with correct shape and dtype, but wrong device.','line_number':923,'multiline':False]
['text':' Case 4: out= with correct shape and device, but a dtype','line_number':951,'multiline':False]
['text':'   that output cannot be "safely" cast to (long).','line_number':952,'multiline':False]
['text':'   Expected behavior: error.','line_number':953,'multiline':False]
['text':' NOTE: this case is filtered by dtype since some ops produce','line_number':954,'multiline':False]
['text':'   bool tensors, for example, which can be safely cast to any','line_number':955,'multiline':False]
['text':'   dtype. It is applied when single tensors are floating point or complex','line_number':956,'multiline':False]
['text':'   dtypes, or if an op returns multiple tensors when at least one such','line_number':957,'multiline':False]
['text':'   tensor is a floating point or complex dtype.','line_number':958,'multiline':False]
['text':' Tests that the forward and backward passes of operations produce the','line_number':1022,'multiline':False]
['text':'   same values for the cross-product of op variants (method, inplace)','line_number':1023,'multiline':False]
['text':'   against eager's gold standard op function variant','line_number':1024,'multiline':False]
['text':' Acquires variants (method variant, inplace variant, operator variant, inplace_operator variant, aliases)','line_number':1027,'multiline':False]
['text':' list of all inplace ops: inplace variant + alias inplace variants if exist','line_number':1035,'multiline':False]
['text':' TODO: Check grad for all Tensors requiring grad if sample.input is TensorList','line_number':1065,'multiline':False]
['text':' Computes function forward and backward values','line_number':1072,'multiline':False]
['text':' Skips inplace variants if the output dtype is not the same as','line_number':1083,'multiline':False]
['text':'   the input dtype','line_number':1084,'multiline':False]
['text':' TODO: backward consistency only supported for single tensor outputs','line_number':1092,'multiline':False]
['text':' TODO: backward consistency only checked on sample.input, not all','line_number':1093,'multiline':False]
['text':'   tensor inputs','line_number':1094,'multiline':False]
['text':' TODO: update to handle checking grads of all tensor inputs as','line_number':1095,'multiline':False]
['text':'   derived from each tensor output','line_number':1096,'multiline':False]
['text':' Test eager consistency','line_number':1106,'multiline':False]
['text':' Skips inplace ops','line_number':1108,'multiline':False]
['text':' Compares variant's forward','line_number':1112,'multiline':False]
['text':' Note: copies the to-be-modified input when testing the inplace variant','line_number':1113,'multiline':False]
['text':' skip samples with kwargs for operator variants','line_number':1136,'multiline':False]
['text':' Compares variant's backward','line_number':1142,'multiline':False]
['text':' Skips inplace variants if the output dtype is not the same as','line_number':1156,'multiline':False]
['text':'   the input dtype','line_number':1157,'multiline':False]
['text':' skip samples with kwargs for operator variants','line_number':1183,'multiline':False]
['text':' TODO Support non-tensor outputs if they exist for inplace ops','line_number':1187,'multiline':False]
['text':' Reference testing for operations in complex32 against complex64.','line_number':1204,'multiline':False]
['text':' NOTE: We test against complex64 as NumPy doesn't have a complex32 equivalent dtype.','line_number':1205,'multiline':False]
['text':' sample.transform applies the lambda to torch.Tensor and torch.dtype.','line_number':1213,'multiline':False]
['text':' However, we only want to apply it to Tensors with dtype `torch.complex32`..','line_number':1214,'multiline':False]
['text':' Since range of chalf is much less compared to cfloat,','line_number':1222,'multiline':False]
['text':' we get `inf`s easily (eg. with `pow`, `exp`),','line_number':1223,'multiline':False]
['text':' so we cast `cfloat` back to `chalf`.','line_number':1224,'multiline':False]
['text':' `exact_dtype` is False because for ops like real, imag','line_number':1228,'multiline':False]
['text':' we get different dtypes for `actual` and `expected`','line_number':1229,'multiline':False]
['text':' `chalf` input -> `half` output','line_number':1230,'multiline':False]
['text':' `cfloat` input -> `float` output','line_number':1231,'multiline':False]
['text':' Test boolean values other than 0x00 and 0x01 (gh-54789)','line_number':1238,'multiline':False]
['text':' Map False -> 0 and True -> Random value in [2, 255]','line_number':1243,'multiline':False]
['text':' Validates that each OpInfo specifies its forward and backward dtypes','line_number':1260,'multiline':False]
['text':'   correctly for CPU and CUDA devices','line_number':1261,'multiline':False]
['text':' Check complex32 support only if the op claims.','line_number':1266,'multiline':False]
['text':' TODO: Once the complex32 support is better, we should add check for complex32 unconditionally.','line_number':1267,'multiline':False]
['text':' dtypes to try to backward in','line_number':1275,'multiline':False]
['text':' lists for (un)supported dtypes','line_number':1280,'multiline':False]
['text':' tries to acquire samples - failure indicates lack of support','line_number':1296,'multiline':False]
['text':' tries to call operator with the sample - failure indicates','line_number':1307,'multiline':False]
['text':'   lack of support','line_number':1308,'multiline':False]
['text':' NOTE: some ops will fail in forward if their inputs','line_number':1313,'multiline':False]
['text':'   require grad but they don't support computing the gradient','line_number':1314,'multiline':False]
['text':'   in that type! This is a bug in the op!','line_number':1315,'multiline':False]
['text':' Checks for backward support in the same dtype, if the input has','line_number':1319,'multiline':False]
['text':' one or more tensors requiring grad','line_number':1320,'multiline':False]
['text':' Note: this grad may not have the same dtype as dtype','line_number':1351,'multiline':False]
['text':' For functions like complex (float -> complex) or abs','line_number':1352,'multiline':False]
['text':'   (complex -> float) the grad tensor will have a','line_number':1353,'multiline':False]
['text':'   different dtype than the input.','line_number':1354,'multiline':False]
['text':'   For simplicity, this is still modeled as these ops','line_number':1355,'multiline':False]
['text':'   supporting grad in the input dtype.','line_number':1356,'multiline':False]
['text':' Checks that dtypes are listed correctly and generates an informative','line_number':1364,'multiline':False]
['text':'   error message','line_number':1365,'multiline':False]
['text':' Partially supporting a dtype is not an error, but we print a warning','line_number':1386,'multiline':False]
['text':' Reference operators often support additional dtypes, and that's OK','line_number':1413,'multiline':False]
['text':' Generates error msg','line_number':1421,'multiline':False]
['text':' Validates that each OpInfo that sets promotes_int_to_float=True does as it says','line_number':1460,'multiline':False]
['text':' Checks if the operator (if it is composite) is written to support most','line_number':1474,'multiline':False]
['text':' backends and Tensor subclasses. See "CompositeImplicitAutograd Compliance"','line_number':1475,'multiline':False]
['text':' in aten/src/ATen/native/README.md for more details','line_number':1476,'multiline':False]
['text':' We pass assertEqual so that decorators like `toleranceOverride`','line_number':1500,'multiline':False]
['text':' actually work (otherwise they silently do nothing!)','line_number':1501,'multiline':False]
['text':' We pass assertEqual so that decorators like `toleranceOverride`','line_number':1523,'multiline':False]
['text':' actually work (otherwise they silently do nothing!)','line_number':1524,'multiline':False]
['text':' Tests that','line_number':1530,'multiline':False]
['text':' 1. The operator's output for physically conjugated/negated tensors and conjugate/negative view tensors','line_number':1531,'multiline':False]
['text':' produces the same value','line_number':1532,'multiline':False]
['text':' 2. The gradients are same in both cases mentioned in (1)','line_number':1533,'multiline':False]
['text':' 3. If the operator's inplace variant is supported, tests that the inplace operation','line_number':1534,'multiline':False]
['text':'    produces the correct value when called on a conjugate/negative view tensor and that the output','line_number':1535,'multiline':False]
['text':'    has its conj/neg bit set to true','line_number':1536,'multiline':False]
['text':' This test only runs for C -> R and C -> C functions','line_number':1537,'multiline':False]
['text':' TODO: add tests for `R->C` functions','line_number':1538,'multiline':False]
['text':' Note: This test runs for functions that take both tensors and tensorlists as input.','line_number':1539,'multiline':False]
['text':' helper function to clone and conjugate/negate the input if its a tensor','line_number':1553,'multiline':False]
['text':' else clone the sequence and conjugate/negate the first element in the sequence','line_number':1554,'multiline':False]
['text':' If a requires_grad argument is provided the tensor being conjugated/negated will','line_number':1555,'multiline':False]
['text':' have its requires_grad set to that value.','line_number':1556,'multiline':False]
['text':' Ensure view represents the original sample input','line_number':1561,'multiline':False]
['text':' Note: .conj() is not called under no_grad mode since it's not allowed to modify a','line_number':1563,'multiline':False]
['text':' view created in no_grad mode. Here it's ok to do so, so as a workaround we call conj','line_number':1564,'multiline':False]
['text':' before resetting the requires_grad field for input','line_number':1565,'multiline':False]
['text':' Computes function forward value with a physically conjugated/negated tensor and','line_number':1583,'multiline':False]
['text':' a conj/neg view tensor and verifies that the output in both case are equal.','line_number':1584,'multiline':False]
['text':' If the op has an inplace variant, and the input doesn't require broadcasting','line_number':1589,'multiline':False]
['text':' and has the same dtype as output, verify that the inplace operation on a conjugated/negated','line_number':1590,'multiline':False]
['text':' input produces correct output, and the output tensor has the conj/neg bit set to True','line_number':1591,'multiline':False]
['text':' TODO: backward consistency only supported for single tensor outputs','line_number':1604,'multiline':False]
['text':' TODO: backward consistency only checked on sample.input, not all','line_number':1605,'multiline':False]
['text':'   tensor inputs','line_number':1606,'multiline':False]
['text':' TODO: update to handle checking grads of all tensor inputs as','line_number':1607,'multiline':False]
['text':'   derived from each tensor output','line_number':1608,'multiline':False]
['text':' a repeat of the above test if output is not complex valued','line_number':1632,'multiline':False]
['text':' Only test one sample','line_number':1703,'multiline':False]
['text':' input strides and size may have been altered due to the result of an inplace op','line_number':1716,'multiline':False]
['text':' TODO: extend this test to test ops with multiple outputs and ops like native_batch_norm(_legit).out','line_number':1720,'multiline':False]
['text':' which mutate not necessarily the first input.','line_number':1721,'multiline':False]
['text':' resize_ should probably have inplace_view tag. Not adding the tag since it','line_number':1725,'multiline':False]
['text':' breaks some codegen logic','line_number':1726,'multiline':False]
['text':' Reference: https://github.com/pytorch/pytorch/issues/78759','line_number':1730,'multiline':False]
['text':' TODO: use self.assertIn when we have separate tests for each tag','line_number':1732,'multiline':False]
['text':' A mode that when enabled runs correctness checks to ensure','line_number':1735,'multiline':False]
['text':' that operators have expected tags based on their input and','line_number':1736,'multiline':False]
['text':' ouput tensor properties','line_number':1737,'multiline':False]
['text':' Test to verify the correctness for tags in `tags.yaml`, also available for access through `torch.Tags`','line_number':1749,'multiline':False]
['text':' TODO: Test tags for ops that return a list of tensors','line_number':1756,'multiline':False]
['text':' TODO: add test for aliases: https://github.com/pytorch/pytorch/issues/78761','line_number':1763,'multiline':False]
['text':' TODO: References that do not have an entry in python_ref_db','line_number':1777,'multiline':False]
['text':' These should be tested with their out-of-place counterparts','line_number':1804,'multiline':False]
['text':' duplicated in _decomp and _refs','line_number':1812,'multiline':False]
['text':' duplicated as refs do not have decent support for advanced indexing','line_number':1817,'multiline':False]
['text':' these are not aten ops?','line_number':1822,'multiline':False]
['text':' CompositeImplicitAutograd','line_number':1845,'multiline':False]
['text':' ref implementation missing kwargs','line_number':1901,'multiline':False]
['text':' missing "layout"','line_number':1902,'multiline':False]
['text':' missing "decimals"','line_number':1903,'multiline':False]
['text':' missing "layout"','line_number':1904,'multiline':False]
['text':' other','line_number':1905,'multiline':False]
['text':' only refs._block_diag_iterable is in decomposition table','line_number':1906,'multiline':False]
['text':' intentional; direct empty is faster and has less guards','line_number':1907,'multiline':False]
['text':' intentional; direct empty is faster and has less guards','line_number':1908,'multiline':False]
['text':' _prims._as_strided_meta: "reduce() of empty sequence with no initial value"','line_number':1910,'multiline':False]
['text':' torch._C._jit_get_operation: No such operator aten::copy_to','line_number':1911,'multiline':False]
['text':' 'bool' object has no attribute 'dtype'','line_number':1912,'multiline':False]
['text':' Calls _prims.conj','line_number':1913,'multiline':False]
['text':' TorchInductor does not support complex at the moment.','line_number':1918,'multiline':False]
['text':' Intentionally don't use assertIn to avoid printing the','line_number':1929,'multiline':False]
['text':' (very large) container','line_number':1930,'multiline':False]
['text':' failing input','line_number':1949,'multiline':False]
['text':' aweights cannot be negtaive','line_number':1950,'multiline':False]
['text':' window overlap add min: 0','line_number':1951,'multiline':False]
['text':' The tensor has a non-zero number of elements, but its data is not allocated yet','line_number':1952,'multiline':False]
['text':' aten::linalg_eigvalsh.out' with arguments from the 'Meta' backend','line_number':1953,'multiline':False]
['text':' Could not run 'aten::eye.m_out' with arguments from the 'Meta' backend','line_number':1954,'multiline':False]
['text':' "linalg.pinv",  # Could not run 'aten::pinv.out' with arguments from the 'Meta' backen','line_number':1955,'multiline':False]
['text':' Could not run 'aten::linalg_eigvalsh.out' with arguments from the 'Meta' backend','line_number':1956,'multiline':False]
['text':' tensor.mH is only supported on matrices or batches of matrices. Got 1-D tensor','line_number':1957,'multiline':False]
['text':' Could not run 'aten::linalg_solve' with arguments from the 'Meta' backend','line_number':1958,'multiline':False]
['text':' Could not run 'aten::linalg_solve' with arguments from the 'Meta'','line_number':1959,'multiline':False]
['text':' MALLOC ERROR: debug','line_number':1960,'multiline':False]
['text':' Could not run 'aten::multinomial' with arguments from the 'Meta' backend','line_number':1961,'multiline':False]
['text':' Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend','line_number':1962,'multiline':False]
['text':' Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend','line_number':1963,'multiline':False]
['text':' Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend','line_number':1964,'multiline':False]
['text':' logical_not() got an unexpected keyword argument 'out'','line_number':1965,'multiline':False]
['text':' quantile() q values must be in the range [0, 1]','line_number':1966,'multiline':False]
['text':' quantile() q values must be in the range [0, 1]','line_number':1967,'multiline':False]
['text':' The tensor has a non-zero number of elements, but its data is not allocated yet','line_number':1968,'multiline':False]
['text':' sometimes errors','line_number':1969,'multiline':False]
['text':' sometimes errors','line_number':1970,'multiline':False]
['text':' The tensor has a non-zero number of elements','line_number':1971,'multiline':False]
['text':' Could not run 'aten::_to_sparse' with arguments from the 'Meta' backend','line_number':1972,'multiline':False]
['text':' The tensor has a non-zero number of elements, but its data is not allocated yet','line_number':1973,'multiline':False]
['text':' cannot repeat_interleave a meta tensor without output_size','line_number':1974,'multiline':False]
['text':' sparsity not supported','line_number':1975,'multiline':False]
['text':' Can not infer total number of classes from meta. no way at present to throw DynamicOutputShapeException','line_number':1976,'multiline':False]
['text':' Fails only for one overload with DataDependentOutputException (hence skip).','line_number':1978,'multiline':False]
['text':' TODO: investigate/fix','line_number':1983,'multiline':False]
['text':' some inputs invoke dynamic output shape operators, some do not','line_number':1999,'multiline':False]
['text':' Mismatch in aten._conj_physical.default','line_number':2029,'multiline':False]
['text':' Mismatch in aten._conj_physical.default','line_number':2030,'multiline':False]
['text':' if you see a shape exception here, you may need to add','line_number':2085,'multiline':False]
['text':' a `dynamic_output_shape` tag to an operator','line_number':2086,'multiline':False]
['text':' prims/decomps must correctly model strides,','line_number':2088,'multiline':False]
['text':' see https://github.com/pytorch/pytorch/issues/78050#issuecomment-1253950325','line_number':2089,'multiline':False]
['text':' skip these to speed up tests','line_number':2179,'multiline':False]
['text':' TODO: enable check_aliasing, batch norm fails','line_number':2187,'multiline':False]
