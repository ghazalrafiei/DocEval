['text':' Owner(s): ["module: nn"]','line_number':1,'multiline':False]
['text':' avoids property checks in assertEquals','line_number':46,'multiline':False]
['text':' avoids property checks in assertEquals','line_number':47,'multiline':False]
['text':' avoids property checks in assertEquals','line_number':49,'multiline':False]
['text':' get per sample grads with ExpandedWeights objects','line_number':152,'multiline':False]
['text':' grad doesn't work with ExpandedWeight because it calls __torch_function__','line_number':161,'multiline':False]
['text':' get per sample grads with for loop','line_number':164,'multiline':False]
['text':' check equality','line_number':169,'multiline':False]
['text':' don't check equality of `input.grad`s since these vanilla tensors won't be scaled','line_number':172,'multiline':False]
['text':' embedding flips its argument order for autograd tests','line_number':182,'multiline':False]
['text':' embedding flips its argument order for autograd tests','line_number':191,'multiline':False]
['text':' embedding flips its argument order for autograd tests','line_number':200,'multiline':False]
['text':' embedding flips its argument order for autograd tests','line_number':212,'multiline':False]
['text':' get per sample grads with ExpandedWeights objects','line_number':218,'multiline':False]
['text':' grad doesn't work with ExpandedWeight because it calls __torch_function__','line_number':224,'multiline':False]
['text':' embedding flips its argument order for autograd tests','line_number':230,'multiline':False]
['text':' group norm has to call native_group_norm. This checks that it hits the same errors','line_number':385,'multiline':False]
['text':' that normal group norm would','line_number':386,'multiline':False]
['text':' 5 is not divisible by 2','line_number':392,'multiline':False]
['text':' get per sample grads with ExpandedWeights context manager','line_number':406,'multiline':False]
['text':' get per sample grads with a for loop','line_number':420,'multiline':False]
['text':' h's batch dim is always the first dim. Must be contiguous for CUDA','line_number':427,'multiline':False]
['text':' get per sample grads with ExpandedWeights context manager, calling .backward() twice','line_number':456,'multiline':False]
['text':' get per sample grads with a for loop, running over the input twice','line_number':469,'multiline':False]
['text':' get per sample grads with ExpandedWeights context manager','line_number':490,'multiline':False]
['text':' compute the per sample grads with a for loop','line_number':503,'multiline':False]
['text':' if the RNN tests use unbatched inputs--batch the inputs','line_number':553,'multiline':False]
['text':' populate grad_sample fields','line_number':587,'multiline':False]
['text':' reset to not have grad_sample fields','line_number':590,'multiline':False]
['text':' would prefer for it to error because input is not pytree-able but that's hard to detect','line_number':638,'multiline':False]
['text':' TODO: functional call bug, sam will fix','line_number':642,'multiline':False]
['text':' TODO: Once all of these use ModuleInfo, replace with ModuleInfo tests','line_number':683,'multiline':False]
['text':' These currently use the legacy nn tests','line_number':684,'multiline':False]
['text':' since this checks derivatives, only use double for precision','line_number':705,'multiline':False]
['text':' ------------- HELPER FUNCTIONS -----------------','line_number':709,'multiline':False]
['text':' input of rank 1 means no batch dim','line_number':742,'multiline':False]
['text':' would cause inter-batch operations','line_number':745,'multiline':False]
['text':' currently can't deal with padding computation on Python level','line_number':747,'multiline':False]
['text':' there's no batch size','line_number':751,'multiline':False]
['text':' 0 is not a valid batch size','line_number':754,'multiline':False]
['text':' get per sample grads by getting derivative for each input in a for loop','line_number':759,'multiline':False]
