['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]
['text':' common input tensors','line_number':19,'multiline':False]
['text':' runtime values for autocast enable argument are not supported','line_number':84,'multiline':False]
['text':' runtime values for autocast enable argument are not supported','line_number':94,'multiline':False]
['text':' multiple uses of the same input value','line_number':112,'multiline':False]
['text':' fp32 policy should not narrow fp64 to fp32!','line_number':140,'multiline':False]
['text':' this works find in regular Python, but it creates a delicate','line_number':215,'multiline':False]
['text':' situation in TorchScript where the types are not consistent across','line_number':216,'multiline':False]
['text':' the then/else branches','line_number':217,'multiline':False]
['text':' another, more complex case of divergent types','line_number':233,'multiline':False]
['text':' conditional autocast expressions are not supported','line_number':257,'multiline':False]
['text':' TODO: fix and enable this test?','line_number':303,'multiline':False]
['text':'   (we could technically fix this, but is it really worth it?)','line_number':304,'multiline':False]
['text':' scripting inside eager autocast','line_number':366,'multiline':False]
['text':' traced inside scripting','line_number':379,'multiline':False]
['text':' traced with autocast inside scripting','line_number':395,'multiline':False]
['text':' scripted called from traced','line_number':413,'multiline':False]
['text':' scripted called from traced with autocast','line_number':428,'multiline':False]
['text':' this is equivalent to running scripted functions inside autocast)','line_number':473,'multiline':False]
['text':' (see also test_eager_and_script)','line_number':474,'multiline':False]
['text':' no dtype provided is not currently supported','line_number':538,'multiline':False]
['text':' no dtype provided is not currently supported','line_number':543,'multiline':False]
['text':' no cast op should be observed when executing outside autocast context','line_number':593,'multiline':False]
['text':' run optimization','line_number':606,'multiline':False]
['text':' make sure this doesn't throw an error','line_number':656,'multiline':False]
['text':' sanity check: this isn't supported currently when global autocasting','line_number':661,'multiline':False]
['text':' isn't enabled','line_number':662,'multiline':False]
['text':' sanity check','line_number':677,'multiline':False]
['text':' make sure that the runtime pass doesn't duplicate autocast nodes','line_number':683,'multiline':False]
['text':' freezing should pre-cast the constant self.x to remove one autocast call','line_number':703,'multiline':False]
['text':' the runtime autocasting pass will re-insert the second autocast call,','line_number':706,'multiline':False]
['text':' but constant propagation will merge it with the constant that it's casting.','line_number':707,'multiline':False]
['text':' NHWC 3D case not support yet','line_number':817,'multiline':False]
['text':' In this testcase, we will check whether cat has done the promotion in AMP with mixed dtype inputs.','line_number':827,'multiline':False]
['text':' To avoid the fusion group from TE, we will disable the fuser here.','line_number':828,'multiline':False]
['text':' torch.is_autocast_enabled should not be able to move inside of the autocast context.','line_number':884,'multiline':False]
['text':' bx = is_autocast_cpu_enabled() result should be False iff (vx = mm(x, y)).dtype is float','line_number':920,'multiline':False]
