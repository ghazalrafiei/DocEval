['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Forward twice.','line_number':129,'multiline':False]
['text':' TODO (rohan-varma): remove model','line_number':158,'multiline':False]
['text':' Regardless of `assert_fn`, the number of parameters should be the same','line_number':164,'multiline':False]
['text':' For non-FSDP roots, the non FSDP portion can still have parameters on rank 0,','line_number':306,'multiline':False]
['text':' so bypass the check for now.','line_number':307,'multiline':False]
['text':' Possibly wrap new model in activation checkpoint wrapper to test save/','line_number':352,'multiline':False]
['text':' load with this wrapper','line_number':353,'multiline':False]
['text':' Would fail if checkpoint_wrapper did not correctly implement state_dict pre/post hooks','line_number':363,'multiline':False]
['text':' not supported','line_number':384,'multiline':False]
['text':' Manually wrap FSDP without AC','line_number':390,'multiline':False]
['text':' Manually wrap FSDP with AC as `FSDP(CheckpointWrapper(module))`','line_number':398,'multiline':False]
['text':' Save, load, and compare the two models','line_number':407,'multiline':False]
['text':' Force model parameters and buffers to be nonzero','line_number':472,'multiline':False]
['text':' Initialize a non-wrapped model on all ranks','line_number':482,'multiline':False]
['text':' Only load the checkpoint on rank 0','line_number':489,'multiline':False]
['text':' Broadcast the module states from rank 0 with `sync_module_states=True`','line_number':497,'multiline':False]
['text':' Check FSDP models are equal across ranks','line_number':504,'multiline':False]
['text':' Check FSDP models correctly loaded the checkpoint','line_number':511,'multiline':False]
['text':' not supported','line_number':543,'multiline':False]
['text':' Run a forward/backward to compute gradients to test the case','line_number':565,'multiline':False]
['text':' where there are gradients populated','line_number':566,'multiline':False]
['text':' Verify fp16 is the type','line_number':591,'multiline':False]
['text':' Run a forward/backward to compute gradients to test the case','line_number':600,'multiline':False]
['text':' where there are gradients populated','line_number':601,'multiline':False]
['text':' zero the model to ensure parameters are different.','line_number':607,'multiline':False]
['text':' Verify parameters are the same in the new model.','line_number':611,'multiline':False]
['text':' not supported','line_number':643,'multiline':False]
['text':' zero the model to ensure parameters are different.','line_number':674,'multiline':False]
['text':' Verify parameters are the same in the new model.','line_number':678,'multiline':False]
['text':' Ensure some training occurred','line_number':722,'multiline':False]
['text':' Save a copy of the state_dict','line_number':724,'multiline':False]
['text':' Ensure checkpointed params have the full param dtype','line_number':741,'multiline':False]
['text':' Load state_dict into zeroed model','line_number':745,'multiline':False]
['text':' keep everything deterministic for input data','line_number':760,'multiline':False]
['text':' TODO: Move this test to common_fsdp.','line_number':795,'multiline':False]
['text':' Keys should match local model.','line_number':850,'multiline':False]
['text':' get FSDP state_dict. Note that by default we return full_state_dict.','line_number':892,'multiline':False]
['text':' Create zeroed local model','line_number':906,'multiline':False]
['text':' Nothing should be FSDP','line_number':914,'multiline':False]
['text':' Load fsdp's full state dict into the local and verify params are as','line_number':924,'multiline':False]
['text':' expected.','line_number':925,'multiline':False]
['text':' Full name of linear_skip param tensors in SkipModel, as would be','line_number':945,'multiline':False]
['text':' stored in checkpoint.','line_number':946,'multiline':False]
['text':' skip SkipModule','line_number':952,'multiline':False]
['text':' Wrap FSDP','line_number':955,'multiline':False]
['text':' reattach','line_number':957,'multiline':False]
['text':' Run a forward pass','line_number':962,'multiline':False]
['text':' TODO: parameters in linear_skip_tensor_names should not be handled','line_number':973,'multiline':False]
['text':' by FSDP.state_dict(). Have a check once this is implemented in','line_number':974,'multiline':False]
['text':' FSDP.state_dict().','line_number':975,'multiline':False]
['text':' Check that it can be loaded into FSDP.','line_number':977,'multiline':False]
['text':' FlatParameter has not supported deepcopy yet.','line_number':984,'multiline':False]
['text':' Test that the checkpoint can be loaded into a local model.','line_number':990,'multiline':False]
['text':' Initialize an FSDP-wrapped model with an ignored module that includes','line_number':1026,'multiline':False]
['text':' both parameters and a buffer','line_number':1027,'multiline':False]
['text':' Note that when model.inner is not ignored this test also ensures','line_number':1045,'multiline':False]
['text':' non-ignored buffers are not cloned.','line_number':1046,'multiline':False]
['text':' expect fp16 model.inner.buffer with mixed_precisions','line_number':1051,'multiline':False]
['text':' expect fp32 sd.inner.buffer after restoring to original precision','line_number':1052,'multiline':False]
['text':' so skip AssertEqual','line_number':1053,'multiline':False]
['text':' Check that the ignored parameters and all buffers are not cloned','line_number':1073,'multiline':False]
['text':' should not apply mixed_precision to ignored buffers','line_number':1085,'multiline':False]
['text':' Check that the state dict can be loaded into a non-wrapped version of','line_number':1090,'multiline':False]
['text':' the model','line_number':1091,'multiline':False]
['text':' Check that if we save a state dict again, the ignored parameters and','line_number':1102,'multiline':False]
['text':' buffer still have the same data pointer','line_number':1103,'multiline':False]
['text':' Offload to CPU to simulate CPU state_dict load','line_number':1208,'multiline':False]
['text':' Mimic resharding from 4 GPUs to 2 GPUs','line_number':1263,'multiline':False]
