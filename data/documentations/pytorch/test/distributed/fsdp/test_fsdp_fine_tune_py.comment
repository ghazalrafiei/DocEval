['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Assume that the linears are leaf modules, meaning that we can pass','line_number':51,'multiline':False]
['text':' `recurse=True` to have this to work for both pre/post FSDP wrapping','line_number':52,'multiline':False]
['text':' Only set for every other linear to test mixing frozen/non-frozen','line_number':54,'multiline':False]
['text':' If the input does not require gradient, then the 0th','line_number':113,'multiline':False]
['text':' frozen linear gets resharded in the catch-all reshard','line_number':114,'multiline':False]
['text':' since we cannot register an autograd hook on it','line_number':115,'multiline':False]
['text':' This follows the normal post-backward hook path','line_number':120,'multiline':False]
['text':' interleave a `no_grad` step to validate post-backward hooks are not registered in that context','line_number':131,'multiline':False]
['text':' and that `requires_grad` is reset appropriately when unfreezing','line_number':132,'multiline':False]
['text':' Unfreeze the parameters on the last step to emulate some','line_number':136,'multiline':False]
['text':' kinds of fine-tuning','line_number':137,'multiline':False]
['text':' Layer `layer_no_grad` and `layer_with_grad` are called','line_number':166,'multiline':False]
['text':' multiple times, IOW, their parameters are used multiple times','line_number':167,'multiline':False]
['text':' during forward pass.','line_number':168,'multiline':False]
['text':' Make sure calling the same layer multiple times works','line_number':172,'multiline':False]
['text':' regardless whether gradient is enabled.','line_number':173,'multiline':False]
