['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' FSDP_CTOR is the supported way forward, but keep WRAP_API in case we miss','line_number':117,'multiline':False]
['text':' any use cases and fix them to work with FSDP_CTOR over time.','line_number':118,'multiline':False]
['text':' following modules were not wrapped by the policy.','line_number':158,'multiline':False]
['text':' TODO: test the various init modes.','line_number':173,'multiline':False]
['text':' if nested=True, the FSDP module will be nested one layer deep','line_number':175,'multiline':False]
['text':' and we should pick that up.','line_number':176,'multiline':False]
['text':' Batchnorms should be wrapped','line_number':238,'multiline':False]
['text':' Wrapping should be FSDP(FSDP(BatchNormNet(FSDP(BN))))','line_number':275,'multiline':False]
['text':' and not FSDP(FSDP(BatchNormNet(BN))) (in the latter the inner','line_number':276,'multiline':False]
['text':' BN is not individually wrapped.)','line_number':277,'multiline':False]
['text':' if we just wrapped BN container, individual batchnorms are not','line_number':287,'multiline':False]
['text':' wrapped.','line_number':288,'multiline':False]
['text':' they don't work together, expected','line_number':319,'multiline':False]
['text':' wrap all modules','line_number':348,'multiline':False]
['text':' Run model a few times for sanity check.','line_number':372,'multiline':False]
['text':' For all the tests here, we use a fake group','line_number':385,'multiline':False]
['text':' Use a size-2 dummy PG to avoid clamping the sharding strategy to','line_number':530,'multiline':False]
['text':' `NO_SHARD` as for a size-1 PG','line_number':531,'multiline':False]
['text':' We currently override batch norm modules to use fp32','line_number':567,'multiline':False]
['text':' Assert children of multihead attention are not wrapped','line_number':661,'multiline':False]
['text':' Model was wrapped in FSDP as no inner modules were wrapped.','line_number':685,'multiline':False]
['text':' CPU offload and CUDA after don't work together as expected.','line_number':698,'multiline':False]
['text':' Random port in case the next test run quickly, same port would cause conflict.','line_number':708,'multiline':False]
['text':' NOTE: We move model to CUDA after init with FSDP to simulate real use','line_number':720,'multiline':False]
['text':' cases where full model cannot be loaded onto GPU, but their shards can.','line_number':721,'multiline':False]
['text':' All non-ignored modules should be wrapped with FSDP','line_number':768,'multiline':False]
['text':' Since the 2nd linear (`sequential[1]`) is ignored, the wrapping','line_number':797,'multiline':False]
['text':' policy does not exceed the parameter threshold before the inner','line_number':798,'multiline':False]
['text':' sequential (`sequential[2]`) anymore; hence, it flattens','line_number':799,'multiline':False]
['text':' `sequential[0]` and `sequential[2][0]` into `model` and leaves','line_number':800,'multiline':False]
['text':' `sequential[1]` and `sequential[2][1]` as-is since they are ignored','line_number':801,'multiline':False]
['text':' Wrap only LoRA modules','line_number':867,'multiline':False]
['text':' Additionally wrap attention','line_number':874,'multiline':False]
['text':' Additionally wrap decoders','line_number':879,'multiline':False]
['text':' Do not wrap the LoRA-A modules (meaning mixed frozen/non-frozen)','line_number':884,'multiline':False]
['text':' Wrapping the attention manages all parameters except those from','line_number':890,'multiline':False]
['text':' the LoRA-B module, which is separately wrapped and all nonfrozen','line_number':891,'multiline':False]
['text':' Now ignore those LoRA-A modules' parameters','line_number':933,'multiline':False]
