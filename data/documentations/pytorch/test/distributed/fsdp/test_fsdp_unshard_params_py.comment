['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Assumes depth-first `.parameters()`','line_number':67,'multiline':False]
['text':' Write a known value to all elements of the *sharded* parameter or','line_number':72,'multiline':False]
['text':' `FlatParameter` to check','line_number':73,'multiline':False]
['text':' Zero the *unsharded* parameters','line_number':77,'multiline':False]
['text':' Check the 0th singleton element of the sharded parameter to see if','line_number':82,'multiline':False]
['text':' the zeroing from inside the context persists','line_number':83,'multiline':False]
['text':' For `use_orig_params=True` and `NO_SHARD`, the parameter','line_number':86,'multiline':False]
['text':' preserves the original 2D shape, so we must access one more time','line_number':87,'multiline':False]
['text':' When FSDP does not use a sharded strategy and is not offloading','line_number':90,'multiline':False]
['text':' parameters to CPU, it directly exposes the tensor storage that','line_number':91,'multiline':False]
['text':' serves as the unsharded source of truth, so the write is always','line_number':92,'multiline':False]
['text':' reflected regardless of `writeback`.','line_number':93,'multiline':False]
['text':' Apply FSDP such that the root module does not have FSDP applied,','line_number':129,'multiline':False]
['text':' while there are multiple FSDP root submodules (as proven later)','line_number':130,'multiline':False]
['text':' Hard code the following names because getting them is non-trivial','line_number':143,'multiline':False]
['text':' even if FSDP uses mixed precision','line_number':171,'multiline':False]
['text':' Check that each `FlatParameter` has the sharded size as a','line_number':175,'multiline':False]
['text':' proxy for it being resharded','line_number':176,'multiline':False]
['text':' Prove the number of FSDP roots after lazy initialization','line_number':188,'multiline':False]
['text':' Write a known value to the *sharded* `FlatParameter`','line_number':241,'multiline':False]
['text':' For nonzero ranks, this write is to padding','line_number':243,'multiline':False]
['text':' NOTE: This checks that writes to padding did not persist, which is','line_number':249,'multiline':False]
['text':' *not* strictly required for correctness.','line_number':250,'multiline':False]
['text':' did not write to padding','line_number':251,'multiline':False]
['text':' wrote to padding','line_number':253,'multiline':False]
['text':' NOTE: This assumes uniform sharding with padding across ranks.','line_number':298,'multiline':False]
['text':' Validate the expected behavior: the root does not reshard after','line_number':306,'multiline':False]
['text':' forward; the non-root reshards after forward; and both reshard after','line_number':307,'multiline':False]
['text':' backward','line_number':308,'multiline':False]
['text':' Check that with parameter unsharding in between forward and backward','line_number':319,'multiline':False]
['text':' as well as after backward, the reshard behavior matches','line_number':320,'multiline':False]
['text':' After forcing full precision, we must invalidate the existing','line_number':330,'multiline':False]
['text':' unsharded low-precision flat parameter since it will not persist','line_number':331,'multiline':False]
['text':' changes from the `summon_full_params()` context, so we cannot','line_number':332,'multiline':False]
['text':' respect the reshard behavior','line_number':333,'multiline':False]
['text':' Hard code the numel values based on the model','line_number':383,'multiline':False]
['text':' Account for unsharded padding: since each `FlatParameter` only','line_number':387,'multiline':False]
['text':' has one original parameter, we only need to pad for divisibility','line_number':388,'multiline':False]
['text':' by world size and not address alignment','line_number':389,'multiline':False]
['text':' Round up the sharded numel to account for padding','line_number':398,'multiline':False]
['text':' Wrap the top-level with FSDP since `named_parameters()` and','line_number':438,'multiline':False]
['text':' `named_buffers` will contain FSDP prefixes if called on a non-FSDP','line_number':439,'multiline':False]
['text':' root module','line_number':440,'multiline':False]
['text':' Ensure that the tensor is not all zeros, which would','line_number':512,'multiline':False]
['text':' mean that the multiplication is vacuous','line_number':513,'multiline':False]
['text':' Modify the DDP gradients in the same way for parity','line_number':530,'multiline':False]
['text':' some configs are not implemented yet','line_number':539,'multiline':False]
['text':' unused','line_number':548,'multiline':False]
['text':' Check calling after backward','line_number':572,'multiline':False]
['text':' Check calling between forward and backward','line_number':582,'multiline':False]
['text':' TODO: `offload_to_cpu=True` with `NO_SHARD` is not supported yet. See','line_number':646,'multiline':False]
['text':' `test_offload_to_cpu_no_shard_raises()`.','line_number':647,'multiline':False]
