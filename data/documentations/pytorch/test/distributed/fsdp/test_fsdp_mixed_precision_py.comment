['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Various mixed precision configs to test under.','line_number':71,'multiline':False]
['text':' Params and buffers are not cast, comm only happens','line_number':78,'multiline':False]
['text':' in reduced precision.','line_number':79,'multiline':False]
['text':' Only parameters are cast (thus comm should happen in the param_dtype precision)','line_number':82,'multiline':False]
['text':' Nothing is cast (thus param, comm, grad, and buffer should be in the full precision)','line_number':87,'multiline':False]
['text':' Buffer original dtype, which can differ from model params.','line_number':101,'multiline':False]
['text':' Use a configurable buffer name to avoid all submodules sharing the','line_number':167,'multiline':False]
['text':' same buffer name, which may hide prefixed vs. unprefixed name bugs','line_number':168,'multiline':False]
['text':' Param and input should be the mixed precision type','line_number':178,'multiline':False]
['text':' Buffer should be in specified precision as well.','line_number':190,'multiline':False]
['text':' In FSDP, self.params should point to the right type.','line_number':193,'multiline':False]
['text':' Single param assumption','line_number':197,'multiline':False]
['text':' FSDP unit is currently active if it is not using the param','line_number':200,'multiline':False]
['text':' local shard. This supports both FULL_SHARD and SHARD_GRAD_OP','line_number':201,'multiline':False]
['text':' cases. In FULL_SHARD, we have the additional property that','line_number':202,'multiline':False]
['text':' param._full_param_padded has not been freed.','line_number':203,'multiline':False]
['text':' This FSDP unit is active, verify param points to mixed','line_number':214,'multiline':False]
['text':' _unshard should have also freed the fp16 shard.','line_number':216,'multiline':False]
['text':' Shard is never allocated if param_dtype mixed precision is not','line_number':217,'multiline':False]
['text':' enabled.','line_number':218,'multiline':False]
['text':' This FSDP unit is not active as full param has been','line_number':224,'multiline':False]
['text':' freed or not yet allocated. Ensure param points to full','line_number':225,'multiline':False]
['text':' precision param.','line_number':226,'multiline':False]
['text':' We should have gotten at least one active FSDP unit for sharded','line_number':228,'multiline':False]
['text':' (world size > 1) cases. For cases where param is not sharded','line_number':229,'multiline':False]
['text':' (ie world_size == 1) it is a bit hard to check if FSDP unit is active','line_number':230,'multiline':False]
['text':' as we'd always point to the local shard, so we rely on the forward','line_number':231,'multiline':False]
['text':' pass self.lin(inp) working well and inp being reduced precision to','line_number':232,'multiline':False]
['text':' implicitly validate that the param is indeed in the reduced precision.','line_number':233,'multiline':False]
['text':' reduce_dtype has higher priority than param_dtype, because mixed_precision','line_number':323,'multiline':False]
['text':' supports overriding param_dtype with reduce_dtype to control the','line_number':324,'multiline':False]
['text':' reduction precision. In the case where reduce_dtype == param_dtype','line_number':325,'multiline':False]
['text':' this tests that gradients are in the expected precision as well.','line_number':326,'multiline':False]
['text':' If reduce_dtype is not specified (is None) we comm. in the param_dtype','line_number':327,'multiline':False]
['text':' if that is specified, otherwise full precision dtype.','line_number':328,'multiline':False]
['text':' Patch reduce_scatter to add validation for mixed precision types.','line_number':420,'multiline':False]
['text':' Forward pass of LinearMixedPrecision check casting of','line_number':436,'multiline':False]
['text':' inputs, params, buffers.','line_number':437,'multiline':False]
['text':' Buffers should be casted.','line_number':441,'multiline':False]
['text':' p._mp_shard should be freed.','line_number':447,'multiline':False]
['text':' We never should have allocated an _mp_shard.','line_number':451,'multiline':False]
['text':' Will run patched reduce scatter that validates mixed_precision','line_number':460,'multiline':False]
['text':' types in backward.','line_number':461,'multiline':False]
['text':' Buffers stay casted even after backwards.','line_number':463,'multiline':False]
['text':' p._mp_shard should be freed.','line_number':469,'multiline':False]
['text':' Ensure params and grads are in full precision,','line_number':475,'multiline':False]
['text':' as after fwd/backward we maintain full precision shards.','line_number':476,'multiline':False]
['text':' Unscale the gradients and step','line_number':484,'multiline':False]
['text':' Update the scale factor','line_number':486,'multiline':False]
['text':' Summon full params should be in full precision','line_number':489,'multiline':False]
['text':' It is not expected for summon_full_params to allocate','line_number':491,'multiline':False]
['text':' a mixed precision shard.','line_number':492,'multiline':False]
['text':' Note that buffers are cast only once and only restored','line_number':501,'multiline':False]
['text':' to the original buffer dtype in state_dict, so','line_number':502,'multiline':False]
['text':' summon_full_params is not expected to restore buffer','line_number':503,'multiline':False]
['text':' types to their original.','line_number':504,'multiline':False]
['text':' state_dict should be in full precision','line_number':512,'multiline':False]
['text':' Parameters and buffers are checkpointed in their','line_number':515,'multiline':False]
['text':' original dtypes, which may be different.','line_number':516,'multiline':False]
['text':' After state_dict, buffer's dtype should have been restored','line_number':526,'multiline':False]
['text':' to the mixed precision one.','line_number':527,'multiline':False]
['text':' Note that we don't exercise all possible different configs so as to','line_number':554,'multiline':False]
['text':' not increase test TTS too much.','line_number':555,'multiline':False]
['text':' Basic test to ensure int inputs are not casted which would break','line_number':587,'multiline':False]
['text':' modules such as embedding tables.','line_number':588,'multiline':False]
['text':' TODO: `test_mp_embedding_reduce()` fails if we do not wrap the','line_number':598,'multiline':False]
['text':' entire `TransformerWithSharedParams` with a single top-level FSDP','line_number':599,'multiline':False]
['text':' This would fail if we casted integer module inputs such as for','line_number':610,'multiline':False]
['text':' embedding tables.','line_number':611,'multiline':False]
['text':' Batchnorm units should be wrapped individually. Validate this by','line_number':678,'multiline':False]
['text':' ensuring there are equal no. of FSDP units that are BN as BN units','line_number':679,'multiline':False]
['text':' in original resnet model.','line_number':680,'multiline':False]
['text':' Would throw type mismatch issue without mixed precision autowrapping.','line_number':688,'multiline':False]
['text':' FSDP detects that mixed precision + batchnorm will cause issues','line_number':729,'multiline':False]
['text':' and thus wrap batchnorm in a distinct FSDP unit that does not','line_number':730,'multiline':False]
['text':' use mixed precision.','line_number':731,'multiline':False]
['text':' policy should not have wrapped any other submodules','line_number':752,'multiline':False]
['text':' Overall mixed precision is still enabled','line_number':756,'multiline':False]
['text':' Without FSDP BN mixed precision fix, this would result in','line_number':760,'multiline':False]
['text':' RuntimeError: Expected counts to have type Half but got Float','line_number':761,'multiline':False]
['text':' for syncBN','line_number':762,'multiline':False]
['text':' Loss should be in fp16','line_number':844,'multiline':False]
['text':' Grads should be in fp32 as we upcast them','line_number':847,'multiline':False]
['text':' Now in eval mode, loss should be fp32 if use_full_prec_in_eval is set.','line_number':852,'multiline':False]
['text':' model.eval() + forward pass should make the buffers in full prec again','line_number':896,'multiline':False]
['text':' Add pre-forward hooks','line_number':897,'multiline':False]
['text':' model.train() + forward again should make buffers in fp16','line_number':931,'multiline':False]
['text':' cast reduction for batchnorm also just in this test, to make','line_number':952,'multiline':False]
['text':' validation easier.','line_number':953,'multiline':False]
['text':' Patch reduce_scatter to add validation for mixed precision types.','line_number':971,'multiline':False]
['text':' Use an input with dtype not equal to the mixed precision','line_number':1023,'multiline':False]
['text':' `param_dtype` so that it gets cast','line_number':1024,'multiline':False]
['text':' Check that `x_float` preserves its dtype, meaning that the gradient','line_number':1033,'multiline':False]
['text':' propagated via `ToCopyBackward0`','line_number':1034,'multiline':False]
['text':' Note that we don't exercise all possible different configs so as to','line_number':1056,'multiline':False]
['text':' not increase test TTS too much.','line_number':1057,'multiline':False]
['text':' float16 on one submodule and float32 on everything else','line_number':1144,'multiline':False]
['text':' float16 on one submodule and float32 on everything else','line_number':1165,'multiline':False]
['text':' float16 on one submodule and float32 on everything else','line_number':1186,'multiline':False]
['text':' For submodules with different precisions, right now current design','line_number':1206,'multiline':False]
['text':' does not support the case when the root FSDP instance wraps a submodule','line_number':1207,'multiline':False]
['text':' that is not the first one executed. Because for that submodule, its inputs','line_number':1208,'multiline':False]
['text':' (or previous submodule's outputs) have no way to be casted, instead,','line_number':1209,'multiline':False]
['text':' the root module's inputs are casted upfront before entering','line_number':1210,'multiline':False]
['text':' root module's forward','line_number':1211,'multiline':False]
['text':' Inputs are casted in root module in default, inputs of submodules are not','line_number':1275,'multiline':False]
['text':' explicitly casted, so the external inputs ``y`` of module ``self.l2`` is','line_number':1276,'multiline':False]
['text':' not casted.','line_number':1277,'multiline':False]
['text':' We mainly want to test `SHARD_GRAD_OP` since it surfaced','line_number':1296,'multiline':False]
['text':' the original bug of not using the right EMA parameters','line_number':1297,'multiline':False]
['text':' for eval, but we also test the others for completeness','line_number':1298,'multiline':False]
['text':' Use main copy for training and EMA copy for eval','line_number':1319,'multiline':False]
['text':' An iteration consists of training forward/backward/optimizer,','line_number':1343,'multiline':False]
['text':' updating the EMA copy with the main copy, and eval forward','line_number':1344,'multiline':False]
['text':' Check that the eval outputs differ from iteration to iteration as a','line_number':1359,'multiline':False]
['text':' proxy for eval using the correct EMA parameters','line_number':1360,'multiline':False]
