['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Mismatched process groups for intra-node','line_number':124,'multiline':False]
['text':' Errors during _lazy_init','line_number':135,'multiline':False]
['text':' Mismatched process groups for inter-node','line_number':142,'multiline':False]
['text':' Create groups like (0, 4), (1, 5), (2, 6) etc and assign appropriately','line_number':175,'multiline':False]
['text':' Initialize optimizer states','line_number':191,'multiline':False]
['text':' Create groups like (0, 4), (1, 5), (2, 6) etc and assign appropriately','line_number':225,'multiline':False]
['text':' TODO - add test for ZeRO-2 style sharding ensure params are not','line_number':265,'multiline':False]
['text':' resharded after forward.','line_number':266,'multiline':False]
['text':' All FSDP modules should have state.process_group as the process group over which to','line_number':300,'multiline':False]
['text':' shard (default process group), and state._inter_node_pg (process group containing only','line_number':301,'multiline':False]
['text':' this rank)','line_number':302,'multiline':False]
['text':' TODO: This needs to be replaced if we deprecate','line_number':306,'multiline':False]
['text':' `FSDP.sharding_strategy` to only use the handle one.','line_number':307,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/90857','line_number':308,'multiline':False]
['text':' process_group should be across the node, which is just the','line_number':317,'multiline':False]
['text':' whole world here.','line_number':318,'multiline':False]
['text':' All fsdp modules should share the same process groups','line_number':329,'multiline':False]
['text':' Use the HSDP strategy for the transformer module','line_number':457,'multiline':False]
['text':' Use `FULL_SHARD` for the embedding and output projection','line_number':459,'multiline':False]
