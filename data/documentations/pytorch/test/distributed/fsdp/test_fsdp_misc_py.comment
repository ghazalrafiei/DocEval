['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Check that FSDP parameters are moved to `device_id` for a CPU module','line_number':114,'multiline':False]
['text':' Check that specifying `device_id` for a GPU module already on that','line_number':122,'multiline':False]
['text':' device does not raise an error','line_number':123,'multiline':False]
['text':' Check that passing in `torch.device("cuda")` for a GPU module warns','line_number':131,'multiline':False]
['text':' Test FSDP validation with SHARD_GRAD_OP and forward_prefetch','line_number':149,'multiline':False]
['text':' When use_second_layer=True, b is involved in forward computation but does','line_number':254,'multiline':False]
['text':' not receive grad in backward. Otherwise, b is not involved in forward','line_number':255,'multiline':False]
['text':' computation.','line_number':256,'multiline':False]
['text':' self.a receives grad, self.b does not','line_number':287,'multiline':False]
['text':' use both params in loss computation. Later,','line_number':354,'multiline':False]
['text':' b will go unused and we check grads are the','line_number':355,'multiline':False]
['text':' same as local training.','line_number':356,'multiline':False]
['text':' Ensure at least some change from previous params, otherwise','line_number':369,'multiline':False]
['text':' above check would be vacuously true.','line_number':370,'multiline':False]
['text':' Verify params initially equal','line_number':436,'multiline':False]
['text':' Overlapped optimizer FSDP module should have sharded_grad as None.','line_number':455,'multiline':False]
['text':' Note: FSDP without optimizer overlap won't set sharded_grad to None until the next','line_number':466,'multiline':False]
['text':' pre-forward since it needs to run FSDP specific logic that picks up that set_to_none=True','line_number':467,'multiline':False]
['text':' has been called (or that the gradients have been otherwise set to None)','line_number':468,'multiline':False]
['text':' Verify parameters are different than prev iteration','line_number':470,'multiline':False]
['text':' Verify overlap and non overlapped are the same','line_number':481,'multiline':False]
['text':' Move me to MT test once warning logging and backward collective issue','line_number':542,'multiline':False]
['text':' is resolved.','line_number':543,'multiline':False]
['text':' Ensure fwd + backward can be performed after moving to CUDA.','line_number':563,'multiline':False]
['text':' CPU input also tests that input is correctly moved to appropriate','line_number':564,'multiline':False]
['text':' CUDA device.','line_number':565,'multiline':False]
['text':' Check that `device_id` with `sync_module_states=True` works','line_number':595,'multiline':False]
['text':' Each rank's buffers should be 0s since rank 0 is the source, and they','line_number':610,'multiline':False]
['text':' should be on GPU since we specified `device_id`','line_number':611,'multiline':False]
['text':' Ensure hooks are registered','line_number':655,'multiline':False]
['text':' TODO: we should check backward() and param is resharded','line_number':659,'multiline':False]
['text':' as well, but this is blocked by','line_number':660,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/83107 and','line_number':661,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/83129','line_number':662,'multiline':False]
['text':' Choose a wrapping policy such that there are (1) nested FSDP','line_number':723,'multiline':False]
['text':' instances and (2) the parent FSDP instance has managed parameters','line_number':724,'multiline':False]
['text':' TODO: override FSDP MT Thread _run to set this instead of here for','line_number':741,'multiline':False]
['text':' every test.','line_number':742,'multiline':False]
['text':' Move wrapped modules to CUDA before wrapping with FSDP','line_number':753,'multiline':False]
['text':' Should raise error since rank 1 is given `device_id=0` when','line_number':755,'multiline':False]
['text':' the model is on cuda:1','line_number':756,'multiline':False]
['text':' without device_id, we hit an error','line_number':778,'multiline':False]
['text':' Test with param_init_fn','line_number':798,'multiline':False]
['text':' TODO: override FSDP MT Thread _run to set this instead of here for','line_number':853,'multiline':False]
['text':' every test.','line_number':854,'multiline':False]
['text':' Test CPU','line_number':856,'multiline':False]
['text':' Test CUDA','line_number':859,'multiline':False]
['text':' Test CPU + device_id','line_number':862,'multiline':False]
['text':' For modules with no params, wrong device_id will raise error about','line_number':865,'multiline':False]
['text':' inconsistency between compute_device and device_id, since compute_device','line_number':866,'multiline':False]
['text':' is computed as torch.cuda.current_device when there are no params.','line_number':867,'multiline':False]
['text':' Seed via rank to make model different across ranks','line_number':891,'multiline':False]
['text':' Passing sync_module_states into FSDP makes model the same during init.','line_number':901,'multiline':False]
['text':' sync_module_states also works with CPU module with device_id passed in','line_number':908,'multiline':False]
['text':' Passing sync_module_states into FSDP makes model the same during init.','line_number':913,'multiline':False]
['text':' Manually construct this list but verify against the global list of','line_number':926,'multiline':False]
['text':' homogeneous attribute names','line_number':927,'multiline':False]
['text':' Run a forward to trigger lazy initialization and the error','line_number':965,'multiline':False]
['text':' If the user already passes `NO_SHARD`, then there should not be a','line_number':985,'multiline':False]
['text':' warning','line_number':986,'multiline':False]
['text':' trigger all warnings','line_number':988,'multiline':False]
['text':' Check that a warning is issued','line_number':996,'multiline':False]
['text':' - Pass `FULL_SHARD` or `None`','line_number':998,'multiline':False]
['text':' - Pass `SHARD_GRAD_OP`','line_number':1006,'multiline':False]
['text':' Incorrectly not moving from CPU -> GPU','line_number':1022,'multiline':False]
['text':' Incorrectly moving from CPU -> GPU','line_number':1033,'multiline':False]
['text':' Construct FSDP module without changing any environment variables and','line_number':1075,'multiline':False]
['text':' run forward, which triggers both unsharded and sharded view setting','line_number':1076,'multiline':False]
['text':' Repeat with unsafe setattr explicitly enabled','line_number':1084,'multiline':False]
['text':' Repeat with unsafe setattr explicitly disabled','line_number':1092,'multiline':False]
