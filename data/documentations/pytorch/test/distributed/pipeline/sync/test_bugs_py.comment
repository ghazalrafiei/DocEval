['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Copyright 2019 Kakao Brain','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.','line_number':5,'multiline':False]
['text':'','line_number':6,'multiline':False]
['text':' This source code is licensed under the BSD license found in the','line_number':7,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':8,'multiline':False]
['text':' A Python autograd function might fail with this error:','line_number':19,'multiline':False]
['text':'','line_number':20,'multiline':False]
['text':'   RuntimeError: Returning Variables sharing storage with other Variables','line_number':21,'multiline':False]
['text':'   that require grad is not supported in Python functions. Please submit a','line_number':22,'multiline':False]
['text':'   feature request if you hit this error.','line_number':23,'multiline':False]
['text':'','line_number':24,'multiline':False]
['text':' It doesn't look like an essential restriction. But it happens on the','line_number':25,'multiline':False]
['text':' current PyTorch version. To avoid it, we should detach the tensor before','line_number':26,'multiline':False]
['text':' returning by identity autograd functions, such as Wait, Fork, and Join.','line_number':27,'multiline':False]
['text':'','line_number':28,'multiline':False]
['text':' In v0.0.2, once a failed partition receives a normal message','line_number':51,'multiline':False]
['text':' (non-closing) for the next micro-batch, a hang occurred. The reason was','line_number':52,'multiline':False]
['text':' that a failed partition didn't call in_queue.task_done() on a normal','line_number':53,'multiline':False]
['text':' message. So the former partition was blocked at out_queue.join() for the','line_number':54,'multiline':False]
['text':' next of next micro-batch.','line_number':55,'multiline':False]
['text':' In v0.0.3, Wait is applied to only the first tensor on a micro-batch.','line_number':76,'multiline':False]
['text':' Under this behavior, if checkpointing was disabled, there's a possibility','line_number':77,'multiline':False]
['text':' that gradient accumulations on other tensors are not synchronized','line_number':78,'multiline':False]
['text':' properly to the copy stream.','line_number':79,'multiline':False]
