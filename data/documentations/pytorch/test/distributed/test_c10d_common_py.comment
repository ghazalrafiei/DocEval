['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' load_tests from common_utils is used to automatically filter tests for','line_number':52,'multiline':False]
['text':' sharding on sandcastle. This line silences flake warnings','line_number':53,'multiline':False]
['text':' catch "Address already in use" error and report it to the main','line_number':98,'multiline':False]
['text':' thread','line_number':99,'multiline':False]
['text':' waiting time should be 1s, use 3s to rule out false alarm','line_number':124,'multiline':False]
['text':' let @retry_on_connect_failures handle the error','line_number':127,'multiline':False]
['text':' we need to create a separate store just for the store barrier test','line_number':140,'multiline':False]
['text':' 1 missing worker will cause it to timeout','line_number':148,'multiline':False]
['text':' we expect the world_size-1 threads to have failed','line_number':176,'multiline':False]
['text':' Could say','line_number':265,'multiline':False]
['text':' x = self.conv0(x).to(device=self.conv1.weight.device, dtype=self.dtypes[1])','line_number':266,'multiline':False]
['text':' etc.  But I don't want to appeal to the weights' devices directly, because part of this test's purpose','line_number':267,'multiline':False]
['text':' is to verify weights are where expected if the model gets replicated.','line_number':268,'multiline':False]
['text':' DistributedDataParallel test doesn't seem to call FileStore destructor','line_number':305,'multiline':False]
['text':' TODO: investigate this test and the test is known to have issues','line_number':306,'multiline':False]
['text':' Use this hack to remove files for that test','line_number':307,'multiline':False]
['text':' to reproduce the same training results','line_number':398,'multiline':False]
['text':' A list of tests for ddp with activation checkpointing','line_number':431,'multiline':False]
['text':' when gradient_as_bucket_view=True, False.','line_number':432,'multiline':False]
['text':' Most of the tests are referred to','line_number':433,'multiline':False]
['text':' https://github.com/facebookresearch/fairscale/blob/main/tests/nn/pipe/test_checkpoint_ddp.py','line_number':434,'multiline':False]
['text':' Share weights','line_number':471,'multiline':False]
['text':' Share weights','line_number':498,'multiline':False]
['text':' find_unused_parameters does not make a difference, since it is','line_number':528,'multiline':False]
['text':' ignored for static graph.','line_number':529,'multiline':False]
['text':' test passes when static_graph is true','line_number':562,'multiline':False]
['text':' Test passes when static_graph=True.','line_number':613,'multiline':False]
['text':' Grads can be none sometimes due to dynamic module not using','line_number':635,'multiline':False]
['text':' all params.','line_number':636,'multiline':False]
['text':' Grads can be none sometimes due to dynamic module not using','line_number':654,'multiline':False]
['text':' all params.','line_number':655,'multiline':False]
['text':' DDP works as expected if there is weight sharing among layers','line_number':659,'multiline':False]
['text':' check two model parameters over 2 iterations','line_number':769,'multiline':False]
['text':' single cpu/gpu training','line_number':771,'multiline':False]
['text':' DDP training, DDP scatters subsets of input_cpu to nodes/GPUs','line_number':774,'multiline':False]
['text':' Update weights and run a second iteration to shake out errors','line_number':785,'multiline':False]
['text':' Shuffle the input so that DDP input is different','line_number':794,'multiline':False]
['text':' Register a DDP communication hook if any.','line_number':809,'multiline':False]
['text':' Register a built-in DDP communication hook if defined','line_number':826,'multiline':False]
['text':' Run forward','line_number':833,'multiline':False]
['text':' Run backward','line_number':836,'multiline':False]
['text':' Add ones to fut's result.','line_number':848,'multiline':False]
['text':' only rank 0 receives empty inputs','line_number':880,'multiline':False]
['text':' input requires grad, this will trigger the collective communication','line_number':887,'multiline':False]
['text':' in the backward pass','line_number':888,'multiline':False]
['text':' input does not requires grad','line_number':892,'multiline':False]
['text':' all ranks receive empty inputs','line_number':896,'multiline':False]
['text':' input requires grad, this will trigger the collective communication','line_number':903,'multiline':False]
['text':' in the backward pass','line_number':904,'multiline':False]
['text':' input does not requires grad','line_number':908,'multiline':False]
['text':' only rank 0 receives empty inputs','line_number':932,'multiline':False]
['text':' all ranks receive empty inputs','line_number':941,'multiline':False]
['text':' use manual_seed to make sure local models start with the same values','line_number':977,'multiline':False]
['text':' We use a separate pg to verify the sequence numbers, otherwise these','line_number':1097,'multiline':False]
['text':' collectives will themselves increment the sequence number.','line_number':1098,'multiline':False]
['text':' verify initial sequence numbers. Use a distinct process group for','line_number':1104,'multiline':False]
['text':' verification to keep counts as expected with respect to process_group.','line_number':1105,'multiline':False]
['text':' Verify sequence numbers are appropriately incremented','line_number':1120,'multiline':False]
['text':' Test when certain ranks don't call collectives','line_number':1132,'multiline':False]
['text':' Now ranks 0 and 2 should be lagging by 1.','line_number':1135,'multiline':False]
['text':' test alltoall_base','line_number':1350,'multiline':False]
['text':' Variant of AbstractCommTest that expects world size of 4','line_number':1357,'multiline':False]
['text':' PTD sorts ranks before creating the PG, so [3, 1] actually gets assigned ranks [1, 0]','line_number':1390,'multiline':False]
['text':' split the world in 2 PGs','line_number':1409,'multiline':False]
['text':' split the world in 2 PGs','line_number':1439,'multiline':False]
['text':' Default should be off','line_number':1482,'multiline':False]
['text':' By default, TCPServer lives on rank 0. So rank 0 needs to make','line_number':1543,'multiline':False]
['text':' sure that it does not exit too early before other ranks finish','line_number':1544,'multiline':False]
['text':' using the store.','line_number':1545,'multiline':False]
['text':' Note that, _store_based_barrier does not solve this problem, as','line_number':1546,'multiline':False]
['text':' all ranks need to run at least one store.add(key, 0) before','line_number':1547,'multiline':False]
['text':' exiting, but there is no guarantee that rank 0 is still alive at','line_number':1548,'multiline':False]
['text':' that point.','line_number':1549,'multiline':False]
['text':' Ensure backend config can be created with the following arguments','line_number':1624,'multiline':False]
['text':' ensures these configs strings are valid and no ValueError is raised','line_number':1641,'multiline':False]
['text':' Ensure backend config will raise ValueError with the following arguments','line_number':1645,'multiline':False]
['text':' trailing comma','line_number':1647,'multiline':False]
['text':' duplicate device','line_number':1648,'multiline':False]
['text':' test all_gather','line_number':1662,'multiline':False]
['text':' test all_gather','line_number':1688,'multiline':False]
['text':' test all_reduce','line_number':1696,'multiline':False]
['text':' test broadcast','line_number':1701,'multiline':False]
['text':' test reduce_scatter','line_number':1706,'multiline':False]
['text':' test send','line_number':1722,'multiline':False]
['text':' test recv','line_number':1730,'multiline':False]
['text':' intentionally not calling into `destroy_process_group` as not all','line_number':1736,'multiline':False]
['text':' user applications would explicitly that.','line_number':1737,'multiline':False]
['text':' creates both gloo and nccl backend','line_number':1761,'multiline':False]
['text':' skip if the backend is not available on the system','line_number':1772,'multiline':False]
['text':' call collective with varying tensors to ensure that the tensors are','line_number':1804,'multiline':False]
['text':' correctly dispatched','line_number':1805,'multiline':False]
['text':' TODO: this will be updated in the future to not be backend specific','line_number':1807,'multiline':False]
['text':' ensure supported devices (cpu, cuda) succeeds during dispatch call','line_number':1809,'multiline':False]
['text':' multi tensor collectives','line_number':1811,'multiline':False]
['text':' gloo does not support reduce_scatter or all_to_all','line_number':1819,'multiline':False]
['text':' TODO: backend will be replaced with a non specified backend','line_number':1828,'multiline':False]
['text':' TODO: this will be updated in the future to not be backend specific','line_number':1859,'multiline':False]
['text':' test alltoall_base','line_number':1875,'multiline':False]
['text':' N.B.: explicitly wrapping with CommTensor instead of updating','line_number':1899,'multiline':False]
['text':' all_reduce Python implementation, as the later will need more','line_number':1900,'multiline':False]
['text':' discussion.','line_number':1901,'multiline':False]
['text':' this wait() will be ignored in tracing mode as','line_number':1904,'multiline':False]
['text':' ProxyTorchDispatchMode only supports torch.Tensor, _ProxyTensor,','line_number':1905,'multiline':False]
['text':' and torch.nn.Parameter objects','line_number':1906,'multiline':False]
['text':' trace fn into a GraphModule','line_number':1917,'multiline':False]
['text':' make sure the mul op indeed waits for comm','line_number':1922,'multiline':False]
['text':' Update input to make sure we are not recording it as constant during','line_number':1945,'multiline':False]
['text':' tracing.','line_number':1946,'multiline':False]
['text':' check correctness','line_number':1953,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/issues/87191','line_number':2049,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/pull/87303#discussion_r1002879700','line_number':2059,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/issues/90072','line_number':2087,'multiline':False]
['text':' this calls `ReduceOp.__eq__(self, other)`','line_number':2095,'multiline':False]
['text':' TODO(crcrpar): This needs to be `assertEqual` for the associativity even though','line_number':2100,'multiline':False]
['text':' the comparison of `RedOpType` and `ReduceOp` sounds less likely to happen compared','line_number':2101,'multiline':False]
['text':' to that of `ReduceOp` and `RedOptype`.','line_number':2102,'multiline':False]
['text':' this calls `RedOpType.__eq__(self, other)`','line_number':2103,'multiline':False]
