['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Use same seed.','line_number':51,'multiline':False]
['text':' Copy the weights from local embedding bag.','line_number':73,'multiline':False]
['text':' Shard the parameter.','line_number':78,'multiline':False]
['text':' Run sharded computation','line_number':81,'multiline':False]
['text':' inputs different on each rank','line_number':82,'multiline':False]
['text':' We need to generate certain length offset for each rank.','line_number':90,'multiline':False]
['text':' The current implementation and dist API does not support','line_number':91,'multiline':False]
['text':' the case when the offset has different lengths.','line_number':92,'multiline':False]
['text':' input_size[0] >> offset_size, so the while loop will not','line_number':93,'multiline':False]
['text':' for too long.','line_number':94,'multiline':False]
['text':' If max_norm is set, we need to ensure that the renorm has been applied across','line_number':104,'multiline':False]
['text':' inputs from all ranks.','line_number':105,'multiline':False]
['text':' Run local computation','line_number':119,'multiline':False]
['text':' Compare local weight and shared one to ensure the renorm','line_number':126,'multiline':False]
['text':' as expected.','line_number':127,'multiline':False]
['text':' Verify','line_number':139,'multiline':False]
['text':' Validate for torch.nn.functional.embedding_bag version.','line_number':142,'multiline':False]
