['text':' Owner(s): ["oncall: distributed"]','line_number':1,'multiline':False]
['text':' Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.','line_number':3,'multiline':False]
['text':'','line_number':4,'multiline':False]
['text':' This source code is licensed under the BSD license found in the','line_number':5,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':6,'multiline':False]
['text':' Use GLOO on GPU when running CUDA + Windows','line_number':50,'multiline':False]
['text':' Windows only has GLOO, but GLOO GPU works. And use GLOO CPU when','line_number':55,'multiline':False]
['text':' no GPUs are available.','line_number':56,'multiline':False]
['text':' TODO: skip_but_pass_in_sandcastle_if does not work here.','line_number':103,'multiline':False]
['text':' rank 0 is the only rank since the world size is 1','line_number':113,'multiline':False]
['text':' Check that the state dict has keys compliant with PyTorch','line_number':133,'multiline':False]
['text':' Check that the state has the expected keys','line_number':137,'multiline':False]
['text':' Check that the state and the `param_groups` attribute are in sync','line_number':144,'multiline':False]
['text':' Check that the state is reloaded with the correct values and device','line_number':152,'multiline':False]
['text':' We should we using `LR1` and not `LR2` after reloading, both within','line_number':160,'multiline':False]
['text':' the optimizer and as exposed by the `param_groups` attribute','line_number':161,'multiline':False]
['text':' Check that the exposed `param_groups`` are on the proper device','line_number':171,'multiline':False]
['text':' Dummy optimizer which adds a new key to the param groups','line_number':227,'multiline':False]
['text':' Test various constructor inputs in the form: (input, expected error)','line_number':284,'multiline':False]
['text':' empty parameter list','line_number':286,'multiline':False]
['text':' non-iterable: `torch.Tensor`','line_number':287,'multiline':False]
['text':' non-iterable: `float`','line_number':288,'multiline':False]
['text':' iterable of dict','line_number':295,'multiline':False]
['text':' iterable containing invalid type','line_number':299,'multiline':False]
['text':' `params` as a generator','line_number':300,'multiline':False]
['text':' `params` as a list','line_number':301,'multiline':False]
['text':' Test constructing with multiple parameter groups more thoroughly','line_number':312,'multiline':False]
['text':' Check that model parameters match','line_number':385,'multiline':False]
['text':' Check that model buffers match','line_number':394,'multiline':False]
['text':' Test with all parameters trainable to begin with','line_number':550,'multiline':False]
['text':' Make sure that the params are trainable so that they are factored','line_number':558,'multiline':False]
['text':' into the size-based parameter partitioning','line_number':559,'multiline':False]
['text':' Verify that new group is added to the correct partition, making','line_number':566,'multiline':False]
['text':' all partitions have the same elements','line_number':567,'multiline':False]
['text':' Test a pathological config with a first big non-trainable param','line_number':575,'multiline':False]
['text':' Make sure that all but the first param are trainable so that they','line_number':581,'multiline':False]
['text':' are factored into the size-based parameter partitioning','line_number':582,'multiline':False]
['text':' Construct `optim1` with both parameter groups upfront','line_number':619,'multiline':False]
['text':' Construct `optim2` by adding the second parameter after','line_number':628,'multiline':False]
['text':' Construct `optim3` as a non-sharded optimizer','line_number':636,'multiline':False]
['text':' Check parity over a few iterations','line_number':644,'multiline':False]
['text':' ensure there exists state to shard','line_number':684,'multiline':False]
['text':' Run a dummy step so that the optimizer state dict exists','line_number':694,'multiline':False]
['text':' Get the optimizer state on the reference rank','line_number':697,'multiline':False]
['text':' Check that the state has the correct size','line_number':700,'multiline':False]
['text':' Load the optimizer state on all ranks without any exceptions','line_number':709,'multiline':False]
['text':' Skip the test if below the minimum world size since then the test is','line_number':721,'multiline':False]
['text':' trivial','line_number':722,'multiline':False]
['text':' Use GPU if enough are available, or fall back to CPU otherwise, which','line_number':733,'multiline':False]
['text':' is fine since Gloo backend supports both','line_number':734,'multiline':False]
['text':' Create a new process group consisting of the even ranks to exercise','line_number':739,'multiline':False]
['text':' the case where the global and local ranks do not necessarily match','line_number':740,'multiline':False]
['text':' Ranks not participating in the new process group are no longer needed','line_number':746,'multiline':False]
['text':' Set different seeds across ranks so that each rank gets different','line_number':750,'multiline':False]
['text':' training data and hence the model sync check is meaningful','line_number':751,'multiline':False]
['text':' Check that the parameters match across ranks after a step','line_number':780,'multiline':False]
['text':' ensure there exists state to shard','line_number':811,'multiline':False]
['text':' Use string to appease the internal test name parser','line_number':820,'multiline':False]
['text':' Define a base model with a different buffer for each rank','line_number':852,'multiline':False]
['text':' Define models/optimizers for DDP with ZeRO and DDP with local','line_number':862,'multiline':False]
['text':' optimizer','line_number':863,'multiline':False]
['text':' Check that the model is properly synchronized between ranks','line_number':889,'multiline':False]
['text':' at construction time','line_number':890,'multiline':False]
['text':' Check that parity is maintained','line_number':931,'multiline':False]
['text':' For the second half of batches, change the parameter','line_number':934,'multiline':False]
['text':' trainability to further test parity','line_number':935,'multiline':False]
['text':' Check that the `state_dict` checkpoints are compatible between','line_number':940,'multiline':False]
['text':' the local optimizer and ZeRO','line_number':941,'multiline':False]
['text':' - Get states','line_number':943,'multiline':False]
['text':' - Cross-load the states','line_number':956,'multiline':False]
['text':' Run one step and check that the models are still the same','line_number':957,'multiline':False]
['text':' - Reload their respective states','line_number':963,'multiline':False]
['text':' Run one step and check that the models are still the same','line_number':964,'multiline':False]
['text':' DDP ensures correct gradients in data parallel training, so DDP with','line_number':991,'multiline':False]
['text':' local optimizers on uneven inputs should be equivalent to ZeRO on','line_number':992,'multiline':False]
['text':' uneven inputs with gradients being manually set','line_number':993,'multiline':False]
['text':' Use uneven inputs: rank i has i extra inputs','line_number':1005,'multiline':False]
['text':' Save the gradients and parameters from DDP as the ground truth; do','line_number':1009,'multiline':False]
['text':' so on the last-joining rank (in this case, the largest rank)','line_number':1010,'multiline':False]
['text':' Broadcast the saved gradients and parameters to all of the other','line_number':1030,'multiline':False]
['text':' ranks (which joined early)','line_number':1031,'multiline':False]
['text':' TODO: Replace this `_broadcast_object` with `broadcast_object_list`','line_number':1041,'multiline':False]
['text':' once the latter supports loading to the destination device instead','line_number':1042,'multiline':False]
['text':' of the source device','line_number':1043,'multiline':False]
['text':' A process must still set the remaining gradients after joining, so we','line_number':1045,'multiline':False]
['text':' define a join hook to do this before the ZeRO join hook','line_number':1046,'multiline':False]
['text':' remaining gradients to set (in order)','line_number':1049,'multiline':False]
['text':' Notify join context that this process has not joined','line_number':1095,'multiline':False]
['text':' Set gradients manually','line_number':1097,'multiline':False]
['text':' Perform optimizer step and check parity','line_number':1103,'multiline':False]
['text':' Use two processes each with two GPUs','line_number':1131,'multiline':False]
['text':' Ensure the parameters are the same across the two models','line_number':1170,'multiline':False]
['text':' Compare parity between DDP with model parallelism using ZeRO and','line_number':1179,'multiline':False]
['text':' a local model using a local optimizer','line_number':1180,'multiline':False]
['text':' Increased tolerances are needed to pass when using TF32','line_number':1208,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/67764','line_number':1209,'multiline':False]
['text':' Enable determinism in cudnn operators','line_number':1279,'multiline':False]
['text':' Set up the DDP model overlapping with ZeRO','line_number':1284,'multiline':False]
['text':' Set up the DDP model with local optimizer','line_number':1310,'multiline':False]
['text':' Check that the parameters match initially','line_number':1325,'multiline':False]
['text':' Save the parameters to ensure they were updated','line_number':1331,'multiline':False]
['text':' Ensure that this test runs independently','line_number':1336,'multiline':False]
['text':' Run the DDP model overlapping with ZeRO','line_number':1339,'multiline':False]
['text':' NOTE: Overlapping currently requires 2 or 3 warmup iterations','line_number':1340,'multiline':False]
['text':' to ensure DDP buckets have been rebuilt (depending on the','line_number':1341,'multiline':False]
['text':' value of `static_graph`)','line_number':1342,'multiline':False]
['text':' Run the DDP model with local optimizer','line_number':1354,'multiline':False]
['text':' Check that the parameters are equal','line_number':1363,'multiline':False]
['text':' Check that the parameters were updated','line_number':1369,'multiline':False]
['text':' Ensure that this test runs independently','line_number':1375,'multiline':False]
['text':' NOTE: The test is skipped if using Windows since functional optimizers','line_number':1378,'multiline':False]
['text':' are not currently supported.','line_number':1379,'multiline':False]
['text':' Add `False` once the Gloo sync issue causing hangs is fixed','line_number':1387,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/issues/62300','line_number':1388,'multiline':False]
['text':' ! unittest should not be used here, else the tests are not properly registered','line_number':1441,'multiline':False]
