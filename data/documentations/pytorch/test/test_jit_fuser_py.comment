['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]
['text':' A smoke test to make sure we won't use the same kernel for contiguous','line_number':101,'multiline':False]
['text':' and non-contiguous arguments.','line_number':102,'multiline':False]
['text':' TODO: add optionally enabled debug counters to the fuser to verify','line_number':103,'multiline':False]
['text':'       that we really can tell the difference between configurations','line_number':104,'multiline':False]
['text':' Note: Non fused inputs must be float to prevent loss of precision','line_number':150,'multiline':False]
['text':' Verifies outputs','line_number':157,'multiline':False]
['text':' Verifies gradients','line_number':164,'multiline':False]
['text':' We shouldn't treat cat nodes as broadcasting. All their inputs','line_number':175,'multiline':False]
['text':' need to be checked for having the same map size, before we can','line_number':176,'multiline':False]
['text':' run the kernel.','line_number':177,'multiline':False]
['text':' NOTE: y is broadcastable to x, but output of f(x, y) should have','line_number':181,'multiline':False]
['text':' shape 3x4, and not 4x4.','line_number':182,'multiline':False]
['text':' splitSize = 1','line_number':230,'multiline':False]
['text':' contiguous case','line_number':233,'multiline':False]
['text':' non-contiguous case','line_number':236,'multiline':False]
['text':' The arguments are intentionally used out of order as a test to see','line_number':291,'multiline':False]
['text':' if the fusion compiler adds extra args in the correct order','line_number':292,'multiline':False]
['text':' skip_check to skip extra bailout nodes in between','line_number':369,'multiline':False]
['text':' TODO: We leak CUDA memory here because the traced graph holds onto a','line_number':436,'multiline':False]
['text':' constant-ified tensor. Since the Python-global CompilationUnit is alive','line_number':437,'multiline':False]
['text':' until the end of the process, the memory is effectively leaked.','line_number':438,'multiline':False]
['text':' Removed `_cuda` suffix from this test which disables leak-checking.','line_number':439,'multiline':False]
['text':' If this is a real problem, we'll need to revisit Torchscript Function','line_number':440,'multiline':False]
['text':' lifetimes in Python.','line_number':441,'multiline':False]
['text':' scalar weight overload','line_number':448,'multiline':False]
['text':' tensor weight overload','line_number':452,'multiline':False]
['text':' Invariant: the output of prim::FusedConcat may','line_number':479,'multiline':False]
['text':' not be an input to any node inside the FusionGroup.','line_number':480,'multiline':False]
['text':' FIXME: We need differentiation for CNNs for this optimization to trigger','line_number':528,'multiline':False]
['text':' Check that normalization op has really been decomposed','line_number':539,'multiline':False]
['text':' test for batchnorm decompose','line_number':553,'multiline':False]
['text':' test for layernorm decompose','line_number':558,'multiline':False]
['text':' use another function otherwise we will bailout','line_number':584,'multiline':False]
['text':' and won't be able to do fused checks','line_number':585,'multiline':False]
['text':' See that fusion kernel outputs are deduplicated when removing  _grad_sum_to_size in the fuser's compilation','line_number':598,'multiline':False]
['text':' see the discussion in PR #14957.','line_number':599,'multiline':False]
['text':' check that a, b share storage, i.e. were generated as a single output in the fuser','line_number':614,'multiline':False]
['text':' This checks if most of Intersection over Union is fused.','line_number':621,'multiline':False]
['text':' In particular, the backward contains many _grad_sum_to_size.','line_number':622,'multiline':False]
['text':' [N,M]','line_number':624,'multiline':False]
['text':' [N,M]','line_number':629,'multiline':False]
['text':' [N,M]','line_number':630,'multiline':False]
['text':' [N,M]','line_number':631,'multiline':False]
['text':' [N,1]','line_number':633,'multiline':False]
['text':' [1,M]','line_number':634,'multiline':False]
['text':' unsqueezing can currently not be fused','line_number':640,'multiline':False]
['text':' [N,1]','line_number':641,'multiline':False]
['text':' [1,N]','line_number':645,'multiline':False]
['text':' Should not crash; these should compile different kernels.','line_number':674,'multiline':False]
['text':' fusion: lambda x. x * x * x * x * x','line_number':688,'multiline':False]
['text':' There are 3 FusionGroups. Because they have the same graph, they','line_number':701,'multiline':False]
['text':' should reuse the same KernelSpec in the KernelSpec cache.','line_number':702,'multiline':False]
['text':' XXX: This assumes that the same kernel isn't already used by another test','line_number':707,'multiline':False]
['text':' Everything is differentiable but TupleConstruct return','line_number':731,'multiline':False]
['text':' By default, on Ampere or later GPUs, LSTM computes float tensors at TF32 precision.','line_number':743,'multiline':False]
['text':' We want float tensors to be computed at full precision in order to use the default precision','line_number':744,'multiline':False]
['text':' lstm has gates = x.mm(w_ih.t()) + hx.mm(w_hh.t()) + b_ih + b_hh.','line_number':754,'multiline':False]
['text':' Test that any permutation of this will still result in one FusionGroup.','line_number':755,'multiline':False]
['text':' TODO: Fuser doesn't work at all when inputs require grad. Fix that','line_number':774,'multiline':False]
['text':' By default, on Ampere or later GPUs, LSTM computes float tensors at TF32 precision.','line_number':776,'multiline':False]
['text':' We want float tensors to be computed at full precision in order to use the default precision','line_number':777,'multiline':False]
['text':' .check_not("aten::add") don't get pulled into FusionGroup because of BailOuts','line_number':783,'multiline':False]
['text':' test that broadcasting random produces correct results','line_number':884,'multiline':False]
['text':' XXX: right now we only support fusing scalars if','line_number':918,'multiline':False]
['text':' they're constant (#9940)','line_number':919,'multiline':False]
['text':' if we have s2, then the s1 are _grad_sum_to_size'd','line_number':967,'multiline':False]
['text':' recompile, so we don't trigger bailouts','line_number':971,'multiline':False]
['text':' this is a workaround for the backward graphs not being','line_number':979,'multiline':False]
['text':' in order for Python 2','line_number':980,'multiline':False]
