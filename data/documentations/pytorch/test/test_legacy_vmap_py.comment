['text':' Owner(s): ["module: vmap"]','line_number':1,'multiline':False]
['text':' This is the same thing as','line_number':91,'multiline':False]
['text':' def returns_tuple_of_tensors(x):','line_number':92,'multiline':False]
['text':'     return x, x','line_number':93,'multiline':False]
['text':' should not throw','line_number':105,'multiline':False]
['text':' jax supports these, but we don't yet','line_number':108,'multiline':False]
['text':' Unsupported view op','line_number':143,'multiline':False]
['text':' The fallback doesn't support TensorList','line_number':159,'multiline':False]
['text':' Don't support non-tensor returns. This is a limitation of vmap;','line_number':163,'multiline':False]
['text':' functions that don't return tensors must be special cased','line_number':164,'multiline':False]
['text':' Basic test','line_number':169,'multiline':False]
['text':' Test that the batch dimension gets permuted to dim 2','line_number':175,'multiline':False]
['text':' negative out_dim','line_number':181,'multiline':False]
['text':' check that out_dims works on ALL outputs','line_number':187,'multiline':False]
['text':' use out_dims with the maximum vmap-able tensor dims (64 dims)','line_number':193,'multiline':False]
['text':' test something that is not the identity function','line_number':201,'multiline':False]
['text':' Inner vmap has non-zero out_dim','line_number':235,'multiline':False]
['text':' all vmaps have non-zero out_dim','line_number':240,'multiline':False]
['text':' throwing in some negative out_dims','line_number':245,'multiline':False]
['text':' testing fn that isn't the identity','line_number':250,'multiline':False]
['text':' Test that we accept out_dims=(1,) for a function with one output.','line_number':261,'multiline':False]
['text':' Too many out_dims','line_number':283,'multiline':False]
['text':' Too few out_dims','line_number':289,'multiline':False]
['text':' TODO(rzou): This error message isn't that great. It comes straight','line_number':296,'multiline':False]
['text':' from maybe_wrap_dim. Consider doing a try-catch-(add some context) to','line_number':297,'multiline':False]
['text':' the error message in the future in C++','line_number':298,'multiline':False]
['text':' Implicit out_dims = 0; vmap will move the batch dim to the front.','line_number':309,'multiline':False]
['text':' None in_dim for a Tensor means we don't map over it','line_number':325,'multiline':False]
['text':' None in_dim for non-tensor arguments','line_number':330,'multiline':False]
['text':' Same in_dim as out_dim, vmap over identity','line_number':343,'multiline':False]
['text':' Different in_dim from out_dim, vmap over identity','line_number':348,'multiline':False]
['text':' Same in_dim as out_dim, vmap over operation','line_number':357,'multiline':False]
['text':' Different in_dim as out_dim, vmap over operation','line_number':361,'multiline':False]
['text':' Basic nested test.','line_number':366,'multiline':False]
['text':' Single layer of nesting','line_number':375,'multiline':False]
['text':' Multiple layers of nesting','line_number':397,'multiline':False]
['text':' The following should not throw','line_number':414,'multiline':False]
['text':' The following should not throw','line_number':430,'multiline':False]
['text':' the following are errors in jax (and will always be errors)','line_number':443,'multiline':False]
['text':' The following should not throw','line_number':451,'multiline':False]
['text':' the following should not throw','line_number':472,'multiline':False]
['text':' NB: One day we will implement a batching rule for torch.atan2.','line_number':477,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':478,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':479,'multiline':False]
['text':' The single warning here is the "vmap is experimental"','line_number':485,'multiline':False]
['text':' warning, not a warning from the vmap fallback path.','line_number':486,'multiline':False]
['text':' NB: One day we will implement a batching rule for torch.atan2.','line_number':490,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':491,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':492,'multiline':False]
['text':' NB: One day we will implement a batching rule for torch.atan2.','line_number':510,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':511,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':512,'multiline':False]
['text':' NB: One day we will implement a batching rule for torch.atan2.','line_number':541,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':542,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':543,'multiline':False]
['text':' fallback on torch.atan2','line_number':551,'multiline':False]
['text':' fallback on torch.atan2, nested vmap','line_number':557,'multiline':False]
['text':' big batch size (total 10000)','line_number':563,'multiline':False]
['text':' NB: One day we will implement a batching rule for masked_fill','line_number':570,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':571,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':572,'multiline':False]
['text':' NB: One day we will implement a batching rule for torch.var_mean','line_number':591,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':592,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':593,'multiline':False]
['text':' fallback correctness on torch.var_mean','line_number':599,'multiline':False]
['text':' nested vmap','line_number':604,'multiline':False]
['text':' big batch size, nested vmap','line_number':610,'multiline':False]
['text':' Test the in-place fallback on an in-place method that takes no','line_number':617,'multiline':False]
['text':' additional Tensor arguments. This is the simplest case of the fallback.','line_number':618,'multiline':False]
['text':' NB: One day we will implement a batching rule for acos_.','line_number':619,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':620,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':621,'multiline':False]
['text':' Single vmap','line_number':628,'multiline':False]
['text':' Single vmap + different out_dim produces a view(!)','line_number':635,'multiline':False]
['text':' Nested vmap','line_number':642,'multiline':False]
['text':' Nested vmap, large batch size','line_number':649,'multiline':False]
['text':' NB: One day we will implement a batching rule for atan2_','line_number':657,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':658,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':659,'multiline':False]
['text':' Single vmap','line_number':667,'multiline':False]
['text':' Nested vmap','line_number':675,'multiline':False]
['text':' big batch size (total 10000)','line_number':683,'multiline':False]
['text':' NB: One day we will implement a batching rule for atan2_','line_number':692,'multiline':False]
['text':' If/when we do, this test should be replaced to test the fallback','line_number':693,'multiline':False]
['text':' path on another operator to avoid bitrot.','line_number':694,'multiline':False]
['text':' op(left, right): All of the levels in right are found in left','line_number':703,'multiline':False]
['text':' op(left, right): Some of the levels in right are not found in left','line_number':716,'multiline':False]
['text':' Runs vmap(get_vjp)(v), which should not error out.','line_number':818,'multiline':False]
['text':' The backward formula for convolution returns an undefined','line_number':819,'multiline':False]
['text':' Tensor for grad_bias because the original bias does not exist.','line_number':820,'multiline':False]
['text':'','line_number':821,'multiline':False]
['text':' In the future we'll probably add a batching rule for convolution','line_number':822,'multiline':False]
['text':' backward. When this happens, we should modify this test to use a','line_number':823,'multiline':False]
['text':' different op (and/or create and use a dummy operator) to avoid bitrot.','line_number':824,'multiline':False]
['text':' Tests vmap(op, in_dims, out_dims)(*inputs) by comparing the output to a','line_number':875,'multiline':False]
['text':' (slow) sequential map+stack fallback.','line_number':876,'multiline':False]
['text':'','line_number':877,'multiline':False]
['text':' check_view: Test if the first returned output is a view of the first input','line_number':878,'multiline':False]
['text':' check_propagates_grad: Test if the operation propagates gradients.','line_number':879,'multiline':False]
['text':' Assuming input[0] is a floating-point tensor. Check if the vmap','line_number':896,'multiline':False]
['text':' operation propagates the requires_grad flag to the zeroth output.','line_number':897,'multiline':False]
['text':' Some vmap operators are implemented in a way that assumes that','line_number':898,'multiline':False]
['text':' they are composite with respect to autograd. If the operator ever is','line_number':899,'multiline':False]
['text':' changed to not be composite with respect to autograd, then the','line_number':900,'multiline':False]
['text':' following check should fail.','line_number':901,'multiline':False]
['text':' All tests of TestVmapBase check that the slow vmap fallback is never invoked.','line_number':915,'multiline':False]
['text':' This is so that we can incrementally add batching rules for operators to','line_number':916,'multiline':False]
['text':' replace the slow vmap fallback path for said operators. To skip this check,','line_number':917,'multiline':False]
['text':' please use the allowVmapFallbackUsage decorator.','line_number':918,'multiline':False]
['text':'','line_number':919,'multiline':False]
['text':' NB: Don't add tests to TestVmapBase directly, unless you want them to run','line_number':920,'multiline':False]
['text':' on every subclass of TestVmapBase. Add them to e.g. TestVmapOperators.','line_number':921,'multiline':False]
['text':'','line_number':922,'multiline':False]
['text':' NB: TestVmapBase is a nested class. This prevents test runners from picking','line_number':923,'multiline':False]
['text':' it up and running it.','line_number':924,'multiline':False]
['text':' One day we'll implement a batching rule for torch.var_mean.','line_number':960,'multiline':False]
['text':' When that happens, please change the example to use an','line_number':961,'multiline':False]
['text':' operator that doesn't have a batching rule implemented.','line_number':962,'multiline':False]
['text':' One day we'll implement a batching rule for torch.var_mean.','line_number':971,'multiline':False]
['text':' When that happens, please change the example to use an','line_number':972,'multiline':False]
['text':' operator that doesn't have a batching rule implemented.','line_number':973,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':997,'multiline':False]
['text':' Doubly nested vmap','line_number':1002,'multiline':False]
['text':' Some basic tests','line_number':1045,'multiline':False]
['text':' Test that the per-examples are contiguous when using torch.contiguous_format','line_number':1052,'multiline':False]
['text':' Basic arithmetic','line_number':1082,'multiline':False]
['text':' Single vmap: op(Tensor, Tensor)','line_number':1100,'multiline':False]
['text':' Nested vmap: op(Tensor, Tensor)','line_number':1109,'multiline':False]
['text':' Python number overload: op(Tensor, Number) (and vice-versa)','line_number':1114,'multiline':False]
['text':' Type promotion: op(Logical Scalar Tensor, Logical Scalar Tensor)','line_number':1120,'multiline':False]
['text':' Type promotion: op(Tensor, Logical Scalar Tensor) (and vice-versa)','line_number':1125,'multiline':False]
['text':' TODO(rzou): fix the following','line_number':1132,'multiline':False]
['text':' # Test cross-device scalars','line_number':1133,'multiline':False]
['text':' number = get_number(getter)','line_number':1134,'multiline':False]
['text':' self._test_unary(lambda t: op(t, number), getter, device='cuda')','line_number':1135,'multiline':False]
['text':' self._test_unary(lambda t: op(number, t), getter, device='cuda')','line_number':1136,'multiline':False]
['text':' self._test_unary(lambda t: op(t, torch.tensor(number)), getter, device='cuda')','line_number':1137,'multiline':False]
['text':' single vmap test','line_number':1146,'multiline':False]
['text':' contiguous','line_number':1149,'multiline':False]
['text':' non-contiguous','line_number':1151,'multiline':False]
['text':' non-zero storage offset','line_number':1153,'multiline':False]
['text':' non-contiguous strides, zero storage offset','line_number':1155,'multiline':False]
['text':' non-contiguous strides, non-zero storage offset','line_number':1157,'multiline':False]
['text':' Broadcast','line_number':1165,'multiline':False]
['text':' transpose','line_number':1167,'multiline':False]
['text':' select','line_number':1169,'multiline':False]
['text':' Nested vmap test','line_number':1172,'multiline':False]
['text':' Check that mal-formatted size/strides doesn't crash','line_number':1181,'multiline':False]
['text':' Sanity check #1: we require the batch dims to be at the front of the','line_number':1186,'multiline':False]
['text':' tensor (in memory layout).','line_number':1187,'multiline':False]
['text':' All the Sanity check #2{a,b,c} cases check that','line_number':1196,'multiline':False]
['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1197,'multiline':False]
['text':' doesn't index memory that is out of bounds of xs[i]. This condition','line_number':1198,'multiline':False]
['text':' is important to the correctness of the as_strided batching rule','line_number':1199,'multiline':False]
['text':' (see NOTE: [When will the as_strided_batching_rule fail?])','line_number':1200,'multiline':False]
['text':' Sanity check #2a: The maximum indexable location of','line_number':1202,'multiline':False]
['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1203,'multiline':False]
['text':' is less than or equal to the maximum indexable location of xs[i].','line_number':1204,'multiline':False]
['text':' Sanity check #2b: The min indexable location of','line_number':1216,'multiline':False]
['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1217,'multiline':False]
['text':' is greater than or equal to the min indexable location of xs[i].','line_number':1218,'multiline':False]
['text':' Sanity check #2c:','line_number':1223,'multiline':False]
['text':' xs[i] is a zero-dim tensor, but','line_number':1224,'multiline':False]
['text':' xs[i].as_strided(sizes, strides, offset + xs[i].offset() - xs.offset())','line_number':1225,'multiline':False]
['text':' is not','line_number':1226,'multiline':False]
['text':' shape mismatch','line_number':1236,'multiline':False]
['text':' left arg is vmapped','line_number':1245,'multiline':False]
['text':' right arg is vmapped','line_number':1250,'multiline':False]
['text':' both args are vmapped','line_number':1255,'multiline':False]
['text':' Quick hack b/c vmap can't accept a list of tensors as an argument','line_number':1265,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':1289,'multiline':False]
['text':' Doubly nested vmap','line_number':1294,'multiline':False]
['text':' correctness tests','line_number':1300,'multiline':False]
['text':' check that torch.conj on a non-complex tensor returns the same tensor','line_number':1304,'multiline':False]
['text':' check that contiguous returns the original tensor if the per-examples','line_number':1314,'multiline':False]
['text':' are already contiguous','line_number':1315,'multiline':False]
['text':' tests for torch.split(self, split_size: int, dim)','line_number':1353,'multiline':False]
['text':' Single vmap: op(Tensor, Tensor)','line_number':1388,'multiline':False]
['text':' Nested vmap: op(Tensor, Tensor)','line_number':1396,'multiline':False]
['text':' test number as inputs','line_number':1401,'multiline':False]
['text':' shape mismatch','line_number':1422,'multiline':False]
['text':' left arg is vmapped','line_number':1431,'multiline':False]
['text':' right arg is vmapped','line_number':1436,'multiline':False]
['text':' both args are vmapped','line_number':1441,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':1469,'multiline':False]
['text':' Doubly nested vmap','line_number':1474,'multiline':False]
['text':' test when value is a batched tensor for fill_ operator','line_number':1480,'multiline':False]
['text':' Runtime Error is thrown when the tensor being written to isn't being vmapped over','line_number':1486,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':1499,'multiline':False]
['text':' Doubly nested vmap','line_number':1505,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':1532,'multiline':False]
['text':' Doubly nested vmap','line_number':1537,'multiline':False]
['text':' Interesting case #1: Batch dim directly before dim of size 2','line_number':1543,'multiline':False]
['text':' Interesting case #2: Batch dim at end of tensor, success cases','line_number':1547,'multiline':False]
['text':' view_as_complex requires that the dim with size 2 have stride 1','line_number':1548,'multiline':False]
['text':' in order for the view to function propertly','line_number':1549,'multiline':False]
['text':' Interesting case #3: Batch dim at end of tensor, failure cases','line_number':1554,'multiline':False]
['text':' Invalid input: no dimension of size 2','line_number':1561,'multiline':False]
['text':' Invalid input: Batch dim has size 2, but the logical last dim does','line_number':1568,'multiline':False]
['text':' not have size 2','line_number':1569,'multiline':False]
['text':' Single batch dim','line_number':1612,'multiline':False]
['text':' Multiple batch dims','line_number':1625,'multiline':False]
['text':' is_contiguous on empty tensor is True','line_number':1638,'multiline':False]
['text':' is_contiguous with other memory formats','line_number':1647,'multiline':False]
['text':' movedim(tensor, int, int) variant','line_number':1664,'multiline':False]
['text':' movedim(tensor, intlist, intlist) variant','line_number':1671,'multiline':False]
['text':' shape mismatch','line_number':1684,'multiline':False]
['text':' left arg is vmapped','line_number':1693,'multiline':False]
['text':' right arg is vmapped','line_number':1698,'multiline':False]
['text':' both args are vmapped','line_number':1703,'multiline':False]
['text':' shape mismatch','line_number':1714,'multiline':False]
['text':' left arg is vmapped','line_number':1723,'multiline':False]
['text':' right arg is vmapped','line_number':1728,'multiline':False]
['text':' both args are vmapped','line_number':1733,'multiline':False]
['text':' Empty is non-deterministic so we just check that the shape of the','line_number':1752,'multiline':False]
['text':' output tensor is what we expect and that the vmap fallback isn't used.','line_number':1753,'multiline':False]
['text':' Empty is non-deterministic so we just check that the size and shape','line_number':1768,'multiline':False]
['text':' of the output are what we expect and that the vmap fallback isn't used','line_number':1769,'multiline':False]
['text':' contiguous case','line_number':1792,'multiline':False]
['text':' expanded','line_number':1796,'multiline':False]
['text':' some of these cases are pretty strange, just verifying that if','line_number':1800,'multiline':False]
['text':' empty_strided allows them then BatchedTensor.new_empty_strided','line_number':1801,'multiline':False]
['text':' can as well','line_number':1802,'multiline':False]
['text':' Quick hack b/c vmap can't accept a list of tensors as an argument','line_number':1830,'multiline':False]
['text':' Single vmap, various in_dims / out_dims','line_number':1871,'multiline':False]
['text':' Doubly nested vmap','line_number':1880,'multiline':False]
['text':' tests for torch.tensor_split(self, indices_or_sections: int, dim)','line_number':1961,'multiline':False]
['text':' tests for torch.tensor_split(self, indices_or_sections: List[int], dim)','line_number':1969,'multiline':False]
['text':' tests for torch.split(self, split_size: int, dim)','line_number':1982,'multiline':False]
['text':' tests for torch.split(self, split_size: List[int], dim)','line_number':1990,'multiline':False]
['text':' Special case: scalar tensor','line_number':2021,'multiline':False]
['text':' also test some casting methods','line_number':2060,'multiline':False]
['text':' We should error out if the view would produce an incorrect result','line_number':2096,'multiline':False]
['text':' We should error out if the view would produce an incorrect result','line_number':2111,'multiline':False]
['text':' out-of-place on BatchedTensor','line_number':2132,'multiline':False]
['text':' out-of-place on captured tensor','line_number':2145,'multiline':False]
['text':' in-place on BatchedTensor','line_number':2158,'multiline':False]
['text':' in-place on captured tensor','line_number':2170,'multiline':False]
['text':' factory functions','line_number':2182,'multiline':False]
['text':' Tests batched gradient computation of outputs = op(*args, **kwargs)','line_number':2220,'multiline':False]
['text':' by comparing it to a sequential map+stack fallback.','line_number':2221,'multiline':False]
['text':'','line_number':2222,'multiline':False]
['text':' output_process_fn: a function that maps the outputs to the part','line_number':2223,'multiline':False]
['text':'       that should be differentiated.','line_number':2224,'multiline':False]
['text':' batch_size: the batch dim size for the batched grad','line_number':2225,'multiline':False]
['text':' Tests batched second grad computation of outputs = op(*args, **kwargs).','line_number':2239,'multiline':False]
['text':' by comparing it to a sequential map+stack fallback.','line_number':2240,'multiline':False]
['text':'','line_number':2241,'multiline':False]
['text':' output_process_fn: a function that maps the outputs to the part','line_number':2242,'multiline':False]
['text':'       that should be differentiated.','line_number':2243,'multiline':False]
['text':' batch_size: the batch dim size for the batched grad','line_number':2244,'multiline':False]
['text':'','line_number':2245,'multiline':False]
['text':' NB: we only test computing batched gradients in the second gradient','line_number':2246,'multiline':False]
['text':' computation. One specific use case that does this is computing the hessian','line_number':2247,'multiline':False]
['text':' matrix of a scalar-valued function; this is useful in Bayesian Logistic','line_number':2248,'multiline':False]
['text':' Regression.','line_number':2249,'multiline':False]
['text':' It might be useful to have a test that computes batched first gradients and','line_number':2250,'multiline':False]
['text':' then uses those to compute batched second gradients in the future.','line_number':2251,'multiline':False]
['text':' Same thing as summing together all of the outputs and calling .backward()','line_number':2258,'multiline':False]
['text':' Make sure the function is non-trivially twice differentiable','line_number':2427,'multiline':False]
['text':' Make sure the function is non-trivially twice differentiable','line_number':2441,'multiline':False]
