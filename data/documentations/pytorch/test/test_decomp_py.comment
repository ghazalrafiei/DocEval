['text':' Owner(s): ["module: decompositions"]','line_number':1,'multiline':False]
['text':' TODO: this isn't going to work with non-aten namespaces','line_number':42,'multiline':False]
['text':' All operators that can have decomp tests','line_number':47,'multiline':False]
['text':' Version of autograd.grad with some differences:','line_number':86,'multiline':False]
['text':'   - pytree inputs is allowed (but leaves of the pytree have to all','line_number':87,'multiline':False]
['text':'     be tensors)','line_number':88,'multiline':False]
['text':'   - if an input is not used as part of derivatives, we will return a','line_number':89,'multiline':False]
['text':'     zero-filled tensor for the result','line_number':90,'multiline':False]
['text':' Returns the "default" rtol and atol for comparing scalars or','line_number':154,'multiline':False]
['text':' tensors of the given dtypes.','line_number':155,'multiline':False]
['text':' see https://github.com/pytorch/pytorch/pull/96264','line_number':195,'multiline':False]
['text':' Before adding an entry to this table, make sure your decomposition is right :)','line_number':219,'multiline':False]
['text':' Due to strange epsilon behaviors, see https://github.com/pytorch/pytorch/issues/73161','line_number':221,'multiline':False]
['text':' This exceeds default tolerances only on CPU, on CUDA it's fine','line_number':228,'multiline':False]
['text':' Exceeds tolerances on CUDA, likely due to fma','line_number':230,'multiline':False]
['text':' The decomposition is TOO correct. It computes everything in int64, so sometimes','line_number':235,'multiline':False]
['text':' there's an off-by-one error. See','line_number':236,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/81996','line_number':237,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/82230','line_number':238,'multiline':False]
['text':' Given f, returns an f' such that:','line_number':267,'multiline':False]
['text':' - f' takes only positional arguments','line_number':268,'multiline':False]
['text':' - All arguments to f' are floating-point Tensors','line_number':269,'multiline':False]
['text':' - All outputs of f' are floating-point Tensors','line_number':270,'multiline':False]
['text':' TODO We should check that the integer outputs also agree','line_number':293,'multiline':False]
['text':' NB: This also upcasts dtype arguments','line_number':305,'multiline':False]
['text':' TODO: handle complex correctly','line_number':306,'multiline':False]
['text':' CUBLAS_STATUS_NOT_SUPPORTED when calling','line_number':329,'multiline':False]
['text':' `cublasGemmStridedBatchedExFix(handle, opa, opb, (int)m, (int)n, (int)k,','line_number':330,'multiline':False]
['text':' (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF,','line_number':331,'multiline':False]
['text':' (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec,','line_number':332,'multiline':False]
['text':' (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`','line_number':333,'multiline':False]
['text':' randomness','line_number':335,'multiline':False]
['text':' aten.special_ndtr was not decomposed','line_number':336,'multiline':False]
['text':' AssertionError: False is not true : aten.item was not decomposed, saw calls for: aten._local_scalar_dense.default.','line_number':341,'multiline':False]
['text':' It's the only in-place op without an out-of-place equivalent in the Python API','line_number':344,'multiline':False]
['text':' Its OpInfo wrongly registers it as `torch.zero_(x.clone())`.','line_number':345,'multiline':False]
['text':' No idea what's going on here','line_number':348,'multiline':False]
['text':' In the recursive test logsumexp.default fails with args = (torch.tensor(-math.inf), [])','line_number':349,'multiline':False]
['text':' in the test, but it seems to pass when tested locally and in the logsumexp test','line_number':350,'multiline':False]
['text':' exp_vml_cpu not implemented for Half','line_number':354,'multiline':False]
['text':' sin_vml_cpu not implemented for Half','line_number':357,'multiline':False]
['text':' CompositeAutogradImplicit','line_number':359,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/81669','line_number':360,'multiline':False]
['text':' This decomp runs before autograd.','line_number':362,'multiline':False]
['text':' Decomposition registered as Autograd','line_number':365,'multiline':False]
['text':' diag was not decomposed (it just registers a decomp for diag_out, torch.diag is CompImplicit)','line_number':368,'multiline':False]
['text':' _softmax_backward_data's CPU kernel for bfloat16 always return the grad_input as float32','line_number':370,'multiline':False]
['text':' native_batch_norm is only implicit when python dispatcher is on (and noncomposite otherwise)','line_number':373,'multiline':False]
['text':' aten.empty_strided was not decomposed','line_number':378,'multiline':False]
['text':' Decomposed backward formula is not as precise','line_number':382,'multiline':False]
['text':' Helpful snippet for testing coverage','line_number':390,'multiline':False]
['text':' Helpful snippet for Horace to create his google sheet :)','line_number':399,'multiline':False]
['text':' These are all things that we haven't coded decompositions','line_number':418,'multiline':False]
['text':' to handle correctly.  Maybe they should.','line_number':419,'multiline':False]
['text':' Decompositions will generally change the behavior of Tensor-like','line_number':425,'multiline':False]
['text':' subclasses, so bypass tests in this case too','line_number':426,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':436,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':438,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':439,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':440,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':441,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':442,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':444,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':445,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':447,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':450,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':455,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':456,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':457,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':458,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':459,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':460,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':462,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':463,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':464,'multiline':False]
['text':' slow: fails with --timeout=360 secs','line_number':465,'multiline':False]
['text':' slow: takes 46 sec on A100','line_number':470,'multiline':False]
['text':' slow: takes 800+ sec on A100','line_number':471,'multiline':False]
['text':' slow: takes 800 sec on A100','line_number':472,'multiline':False]
['text':' slow: takes 800 sec on A100','line_number':473,'multiline':False]
['text':' slow: takes 44 sec on A100','line_number':474,'multiline':False]
['text':' slow: takes 60 sec on A100','line_number':475,'multiline':False]
['text':' slow: takes 170 sec on A100','line_number':476,'multiline':False]
['text':' slow: takes 118 sec on A100','line_number':477,'multiline':False]
['text':' slow: takes 50 sec on A100','line_number':478,'multiline':False]
['text':' slow: takes 70 sec on A100','line_number':479,'multiline':False]
['text':' slow: takes 49 sec on A100','line_number':480,'multiline':False]
['text':' NB: This actually overlaps with test_comprehensive, but it only','line_number':487,'multiline':False]
['text':' runs on things that are definitely decomposed so it's a lot faster','line_number':488,'multiline':False]
['text':' to run','line_number':489,'multiline':False]
['text':' rrelu_with_noise behavior depends on a) whether elements in the input','line_number':537,'multiline':False]
['text':' are <= 0, and b) whether we're in training mode. Cover all cases:','line_number':538,'multiline':False]
['text':' Now with training=True:','line_number':559,'multiline':False]
['text':' only tests RNNs since we have py dispsatcher decomps for them','line_number':578,'multiline':False]
['text':' without this check, incorrect decomps at the python dispatcher level can still pass because','line_number':596,'multiline':False]
['text':' they're checking aten decomps at the torch_dispatch level','line_number':597,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/100970','line_number':601,'multiline':False]
['text':' We check the correctness of each decomposition right after running it.','line_number':619,'multiline':False]
['text':' So, when we encounter a decomposition, we run the function normally, and','line_number':620,'multiline':False]
['text':' then run the decomposition, and ensure they're identical.','line_number':621,'multiline':False]
['text':' Stuff we shouldn't bother testing','line_number':632,'multiline':False]
['text':' (TODO: remove detach from the decomp table?)','line_number':633,'multiline':False]
['text':' N.b. Testing in-place ops would need dedicated logic','line_number':634,'multiline':False]
['text':' non-deterministic ops','line_number':638,'multiline':False]
['text':' We take 2 main strategies for verifying correctness/numerical stability of decompositions','line_number':659,'multiline':False]
['text':' The first one is simply tolerance checking between decomp_out and pytorch_out','line_number':660,'multiline':False]
['text':' However, for fp16/bf16 and reductions, this becomes very','line_number':661,'multiline':False]
['text':' finicky, as there are not many guarantees we can make.','line_number':662,'multiline':False]
['text':' So, for fp16/bf16, we instead compare the difference of','line_number':663,'multiline':False]
['text':' {decomp_out, pytorch_out_64} and {pytorch_out,','line_number':664,'multiline':False]
['text':' pytorch_out_64}. In other words, we compare how far the','line_number':665,'multiline':False]
['text':' decomposition and pytorch are from the "ground truth" (i.e.','line_number':666,'multiline':False]
['text':' fp64). If the decomposition results in more error, we error','line_number':667,'multiline':False]
['text':' We also decompose the decomposition recursively for','line_number':669,'multiline':False]
['text':' further coverage, as some paths not be exercised directly by','line_number':670,'multiline':False]
['text':' OpInfos (sadly) but just by other ops','line_number':671,'multiline':False]
['text':' Execute recursively via DFS, to find the root of a possible error first','line_number':677,'multiline':False]
['text':' At this stage we should not be decomposing an in-place op','line_number':683,'multiline':False]
['text':' We'd like to have decompositions that decompose out-of-place ops into out-of-place ops','line_number':684,'multiline':False]
['text':'  because decompositions are run after functionalisation and we would not like them to','line_number':685,'multiline':False]
['text':'  de-functionalise the graph, as that would break AoTAutograd','line_number':686,'multiline':False]
['text':' We run the real function *after* the decomposition to make sure that the','line_number':687,'multiline':False]
['text':' decomposition does not modify any of the inputs in-place. If it does','line_number':688,'multiline':False]
['text':' real_out should be differen than decom_out so we should catch this','line_number':689,'multiline':False]
['text':' TODO: OpInfo really ought to error out for this case, but it's','line_number':740,'multiline':False]
['text':' not exercised in test_ops_gradients atm.  The problem is not','line_number':741,'multiline':False]
['text':' complex32 per-se (which is supported by data movement only ops)','line_number':742,'multiline':False]
['text':' but that when we do backwards we expect other ops like add to work','line_number':743,'multiline':False]
['text':' Once https://github.com/pytorch/pytorch/pull/75965/ I can','line_number':758,'multiline':False]
['text':' store the called list on the mode object instance and no','line_number':759,'multiline':False]
['text':' explicit clearing is necessary as I will create a fresh mode','line_number':760,'multiline':False]
['text':' for each region','line_number':761,'multiline':False]
['text':' CompositeImplicitAutograd ops are transparent to the tracer, so don't need decompositions','line_number':958,'multiline':False]
['text':' has_key fails for some jit-registered ops, which shouldn't be','line_number':961,'multiline':False]
['text':' relevant here anyway','line_number':962,'multiline':False]
['text':' This is for operators that are only registered in some CI','line_number':985,'multiline':False]
['text':' configurations, so would cause the test to fail','line_number':986,'multiline':False]
['text':' If a decomposition isn't included in the core decompositions,','line_number':996,'multiline':False]
['text':' then it must decompose a core ATen operator.','line_number':997,'multiline':False]
['text':'','line_number':998,'multiline':False]
['text':' See NOTE [Core ATen Ops]','line_number':999,'multiline':False]
['text':'','line_number':1000,'multiline':False]
['text':' If this test fails then either:','line_number':1001,'multiline':False]
['text':' - Add the decomposition to torch._decomp.core_aten_decompositions,','line_number':1002,'multiline':False]
['text':'   if decomposition should be used by inductor (not a core operator).','line_number':1003,'multiline':False]
['text':' - Run this test again with EXPECTTEST_ACCEPT=1 to update the list of','line_number':1004,'multiline':False]
['text':'   core ATen operators (and inductor will not use the decomposition).','line_number':1005,'multiline':False]
['text':' Some decompositions are registered for CompositeImplicitAutograd','line_number':1007,'multiline':False]
['text':' operators, which never appear in AOTAutograd's graph so are never used.','line_number':1008,'multiline':False]
