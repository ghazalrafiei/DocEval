['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' These function receive one array_like arg and return one array_like result','line_number':25,'multiline':False]
['text':' w.bincount,     # XXX: input dtypes','line_number':35,'multiline':False]
['text':' torch: bitwise_not_cpu not implemented for 'Float'','line_number':61,'multiline':False]
['text':' a np.transpose -specific test','line_number':144,'multiline':False]
['text':' shape = (1, 2, 3)','line_number':154,'multiline':False]
['text':' reshape expects `newshape`','line_number':189,'multiline':False]
['text':' for a single argument, broadcast_arrays returns a tuple, while','line_number':283,'multiline':False]
['text':' atleast_?d return an array','line_number':284,'multiline':False]
['text':' https://github.com/Quansight-Labs/numpy_pytorch_interop/pull/121#discussion_r1172824545','line_number':353,'multiline':False]
['text':' w.tril_indices_from,','line_number':354,'multiline':False]
['text':' w.triu_indices_from,','line_number':355,'multiline':False]
['text':' Check that unknown args to decorated functions fail','line_number':417,'multiline':False]
['text':' unknown positional args','line_number':420,'multiline':False]
['text':' unknown kwarg','line_number':424,'multiline':False]
['text':' check a function 5 arguments and 4 defaults: this should work','line_number':433,'multiline':False]
['text':' five arguments, four defaults: this should fail','line_number':436,'multiline':False]
['text':' cannot broadcast => error out','line_number':452,'multiline':False]
['text':' broadcast src against dst','line_number':456,'multiline':False]
['text':' force the type cast','line_number':468,'multiline':False]
['text':' ("out1, out2 not implemented")','line_number':500,'multiline':False]
['text':' check that the out= machinery handles no out at all','line_number':513,'multiline':False]
['text':' smoke test that the "NotImplemented" annotation is picked up','line_number':529,'multiline':False]
['text':' by default, both floats and ints 64 bit','line_number':537,'multiline':False]
['text':' restore the','line_number':558,'multiline':False]
['text':' make sure operations with float16 tensors give same results on CUDA and on CPU','line_number':591,'multiline':False]
