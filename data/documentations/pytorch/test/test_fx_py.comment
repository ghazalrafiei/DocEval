['text':' Owner(s): ["module: fx"]','line_number':1,'multiline':False]
['text':' noqa: F401','line_number':41,'multiline':False]
['text':' noqa: F401','line_number':42,'multiline':False]
['text':' noqa: F401','line_number':43,'multiline':False]
['text':' noqa: F401','line_number':44,'multiline':False]
['text':' noqa: F401','line_number':45,'multiline':False]
['text':' noqa: F401','line_number':46,'multiline':False]
['text':' noqa: F401','line_number':47,'multiline':False]
['text':' noqa: F401','line_number':48,'multiline':False]
['text':' noqa: F401','line_number':49,'multiline':False]
['text':' noqa: F401','line_number':51,'multiline':False]
['text':' noqa: F401','line_number':52,'multiline':False]
['text':' Used for test_autowrap_function. Autowrapped functions need to be global','line_number':80,'multiline':False]
['text':' used in test_pytree. It's all the way out here because pickling a GraphModule','line_number':87,'multiline':False]
['text':' that uses Point errors out if Point is local to the function','line_number':88,'multiline':False]
['text':' Test wrap() passing both a function name as well as a function','line_number':91,'multiline':False]
['text':' directly','line_number':92,'multiline':False]
['text':' Test wrapping twice doesn't break anything','line_number':97,'multiline':False]
['text':' for testing pytrees','line_number':151,'multiline':False]
['text':' noqa: B209','line_number':152,'multiline':False]
['text':' Checking for mutable operations whil tracing is feature flagged','line_number':169,'multiline':False]
['text':' Enable it in testing but not by default','line_number':170,'multiline':False]
['text':' test for issue described at https://github.com/pytorch/pytorch/issues/63883','line_number':235,'multiline':False]
['text':' noqa: B902','line_number':293,'multiline':False]
['text':' Custom delegate to disallow in-place tensor operations','line_number':363,'multiline':False]
['text':' Test method','line_number':373,'multiline':False]
['text':' Test free function','line_number':384,'multiline':False]
['text':' Test symbolic node as an arg','line_number':393,'multiline':False]
['text':' Custom delegate to make it so that there are no leaf modules, everything','line_number':404,'multiline':False]
['text':' should get traced through','line_number':405,'multiline':False]
['text':' test that we can use proxy objects to generate more graph code later for things that do not need to work with modules.','line_number':524,'multiline':False]
['text':' test that we can use proxy objects to generate more graph code later for things that do not need to work with modules.','line_number':589,'multiline':False]
['text':' saving the original list because we will insert new nodes as a part of a test','line_number':606,'multiline':False]
['text':' verify that copying the node does not lose the stack trace','line_number':614,'multiline':False]
['text':' nodes after Transformer should still preserve the original node's stack trace','line_number':631,'multiline':False]
['text':' test custom codegen','line_number':651,'multiline':False]
['text':' This test exercises the case where we use FX to translate from Python','line_number':688,'multiline':False]
['text':' code to some native callable object','line_number':689,'multiline':False]
['text':'','line_number':690,'multiline':False]
['text':' For the purposes of testing, we use ElementwiseInterpreter defined','line_number':691,'multiline':False]
['text':' in test_custom_class.cpp.','line_number':692,'multiline':False]
['text':'','line_number':693,'multiline':False]
['text':' We test that we can','line_number':694,'multiline':False]
['text':' 1) Construct a native callable from FX IR','line_number':695,'multiline':False]
['text':' 2) Construct a drop-in replacement module that delegates to the','line_number':696,'multiline':False]
['text':'    native callable rather than the original code','line_number':697,'multiline':False]
['text':' 3) Run both the original code and native callable wrapper with','line_number':698,'multiline':False]
['text':'    equivalent results','line_number':699,'multiline':False]
['text':' 4) TorchScript compile the native callable wrapper and confirm','line_number':700,'multiline':False]
['text':'    equivalent results with the reference','line_number':701,'multiline':False]
['text':' 5) TorchScript serialize and deserialize the native callable','line_number':702,'multiline':False]
['text':'    and confirm equivalent results with the reference','line_number':703,'multiline':False]
['text':' We use this simple Module as a reference computation','line_number':705,'multiline':False]
['text':' This is what a lowering pass might look like: a function that takes','line_number':712,'multiline':False]
['text':' a valid nn.Module, symbolically traces it, lowers the Module to some','line_number':713,'multiline':False]
['text':' representation, and wraps that representation up into another','line_number':714,'multiline':False]
['text':' nn.Module instance that handles dispatch to the compiled/lowered code.','line_number':715,'multiline':False]
['text':' ===== Stage 1: Symbolic trace the module =====','line_number':717,'multiline':False]
['text':' ===== Stage 2: Lower GraphModule representation to the C++','line_number':720,'multiline':False]
['text':'       interpreter's instruction format ======','line_number':721,'multiline':False]
['text':' For each instruction, create a triple','line_number':733,'multiline':False]
['text':' (instruction_name : str, inputs : List[str], output : str)','line_number':734,'multiline':False]
['text':' to feed into the C++ interpreter','line_number':735,'multiline':False]
['text':' Placeholders specify function argument names. Save these','line_number':741,'multiline':False]
['text':' for later when we generate the wrapper GraphModule','line_number':742,'multiline':False]
['text':' Pull out constants. These constants will later be','line_number':749,'multiline':False]
['text':' fed to the interpreter C++ object via add_constant()','line_number':750,'multiline':False]
['text':' Load constants','line_number':767,'multiline':False]
['text':' Specify names for positional input arguments','line_number':770,'multiline':False]
['text':' Load instructions','line_number':772,'multiline':False]
['text':' Specify name for single output','line_number':774,'multiline':False]
['text':' ===== Stage 3: Create a wrapper GraphModule around the interpreter =====','line_number':778,'multiline':False]
['text':' Create a graph that: 1) Takes function arguments 2) Invokes the interpreter','line_number':786,'multiline':False]
['text':' 3) Returns the speficied return value','line_number':787,'multiline':False]
['text':' FIXME: The following code could be greatly simplified by symbolic_trace'ing','line_number':789,'multiline':False]
['text':' the wrapper with a Tracer that considers the Wrapper instance a root','line_number':790,'multiline':False]
['text':' module, however, I can't get `__call__` exposed on TorchBind classes','line_number':791,'multiline':False]
['text':' without it messing up Python `hasattr` for some reason. More digging','line_number':792,'multiline':False]
['text':' into CPython's implementation of hasattr is probably in order...','line_number':793,'multiline':False]
['text':' Add placeholders for fn inputs','line_number':796,'multiline':False]
['text':' Get the interpreter object','line_number':801,'multiline':False]
['text':' Add a node to call the interpreter instance','line_number':804,'multiline':False]
['text':' Register output','line_number':808,'multiline':False]
['text':' Return final GraphModule!!!','line_number':813,'multiline':False]
['text':' Lower GraphModule to C++ interpreter','line_number':817,'multiline':False]
['text':' Compare correctness with original module','line_number':820,'multiline':False]
['text':' Test TorchScript compilation','line_number':826,'multiline':False]
['text':' Test TorchScript ser/de','line_number':831,'multiline':False]
['text':' Test that Graph pretty-print prints friendly name for targets','line_number':1119,'multiline':False]
['text':' in `operator` and `builtins`','line_number':1120,'multiline':False]
['text':' TorchScript seems to ignore attributes that start with `__`.','line_number':1157,'multiline':False]
['text':' We used to call anonymous Tensor values `__tensor_constant*`, but','line_number':1158,'multiline':False]
['text':' they were getting ignored by script. Now they're called','line_number':1159,'multiline':False]
['text':' `_tensor_constant*`','line_number':1160,'multiline':False]
['text':' Check function(s) are wrapped','line_number':1177,'multiline':False]
['text':' `int` would normally throw a TypeError as argument can't be `Proxy`','line_number':1178,'multiline':False]
['text':' Test scriptability','line_number':1185,'multiline':False]
['text':' Test scriptability','line_number':1215,'multiline':False]
['text':' Test non-proxy len','line_number':1222,'multiline':False]
['text':' verify traceability','line_number':1542,'multiline':False]
['text':' verify assertion on traced model works correctly at runtime','line_number':1544,'multiline':False]
['text':' verify the symbolically traced module is scriptable','line_number':1548,'multiline':False]
['text':' Not normally traceable; good reason to make','line_number':1572,'multiline':False]
['text':' this module a leaf.','line_number':1573,'multiline':False]
['text':' Make sure we're testing all opcodes','line_number':1648,'multiline':False]
['text':' Test shape propagation and make sure results match actual','line_number':1660,'multiline':False]
['text':' contiguous layout','line_number':1673,'multiline':False]
['text':' NB: the implementation of conv may not preserve the memory format,','line_number':1686,'multiline':False]
['text':' unfortunately. The best we can do is just check that the placeholder','line_number':1687,'multiline':False]
['text':' node is channels-last','line_number':1688,'multiline':False]
['text':' NB: the implementation of conv may not preserve the memory format,','line_number':1743,'multiline':False]
['text':' unfortunately. The best we can do is just check that the placeholder','line_number':1744,'multiline':False]
['text':' node is channels-last','line_number':1745,'multiline':False]
['text':' Neg doesn't have in-place','line_number':2165,'multiline':False]
['text':' Test deleting with uses both in another Node and at the output','line_number':2293,'multiline':False]
['text':' zed = z + z + z -> zed = z + z + x','line_number':2378,'multiline':False]
['text':' z = x + y -> z = y + y','line_number':2382,'multiline':False]
['text':' Check Pair NamedTuple works when inlined into the function call.','line_number':2681,'multiline':False]
['text':' fx creates `self._tensor_constant0` here','line_number':2731,'multiline':False]
['text':' need to run this test in a subproc to work around:','line_number':2743,'multiline':False]
['text':'   https://github.com/pytorch/pytorch/issues/50710','line_number':2744,'multiline':False]
['text':' Do not change this to `capture_stderr` or another context','line_number':2878,'multiline':False]
['text':' manager without ensuring that the output is as expected','line_number':2879,'multiline':False]
['text':' Test that we added the "dropout" submodule','line_number':3126,'multiline':False]
['text':' Test `get_submodule` with an added submodule','line_number':3129,'multiline':False]
['text':' Test that the "conv" submodule is still there','line_number':3132,'multiline':False]
['text':' Test `get_submodule` with an original module','line_number':3135,'multiline':False]
['text':' Test that the "conv" node is NOT still there','line_number':3138,'multiline':False]
['text':' Test that the "conv" submodule is now gone','line_number':3144,'multiline':False]
['text':' Test `get_submodule` with a deleted submodule','line_number':3147,'multiline':False]
['text':' Test `get_attr` warnings','line_number':3152,'multiline':False]
['text':' Test `get_parameter`','line_number':3174,'multiline':False]
['text':' Test `get_buffer`','line_number':3183,'multiline':False]
['text':' Test non-nested attributes','line_number':3192,'multiline':False]
['text':' Insert some unused submodules','line_number':3196,'multiline':False]
['text':' Garbage collection','line_number':3202,'multiline':False]
['text':' Test that all the unused submodules are gone','line_number':3205,'multiline':False]
['text':' Test that we didn't delete any unused Parameters or buffers','line_number':3211,'multiline':False]
['text':' Gradient was not calculated for the module stated and buffers','line_number':3274,'multiline':False]
['text':' Recompile calls added "for fun", since they','line_number':3321,'multiline':False]
['text':' chain __call__ wrappers.','line_number':3322,'multiline':False]
['text':'','line_number':3324,'multiline':False]
['text':' Test: B as a regular, non-leaf module','line_number':3325,'multiline':False]
['text':'','line_number':3326,'multiline':False]
['text':' Test graphmodule/submodule a is not inlined.','line_number':3334,'multiline':False]
['text':' Test submodule b is not treated as leaf.','line_number':3339,'multiline':False]
['text':' Test assert custom __call__ on submodule b was honored.','line_number':3342,'multiline':False]
['text':'','line_number':3350,'multiline':False]
['text':' Test: B as a regular, leaf module','line_number':3351,'multiline':False]
['text':' symbolic_trace should only patch torch.nn.Module.__call__,','line_number':3352,'multiline':False]
['text':' which means B.__call__ should still execute','line_number':3353,'multiline':False]
['text':'','line_number':3354,'multiline':False]
['text':' Test graphmodule/submodule a is not inlined.','line_number':3363,'multiline':False]
['text':' Test submodule b is leaf:','line_number':3368,'multiline':False]
['text':' Test b.__call__ was run','line_number':3373,'multiline':False]
['text':'','line_number':3377,'multiline':False]
['text':' Test: B as GraphModule leaf','line_number':3378,'multiline':False]
['text':' __call__ not honored since symbolic_trace directly invokes forward()','line_number':3379,'multiline':False]
['text':'','line_number':3380,'multiline':False]
['text':' Create new GraphModule based on original, either w/ dict or root module.','line_number':3413,'multiline':False]
['text':' Check that both my_buff and my_param are found and the same.','line_number':3421,'multiline':False]
['text':' noqa: F401','line_number':3494,'multiline':False]
['text':' Used to test that you can use your own placeholder class','line_number':3540,'multiline':False]
['text':' (f_return_custom, Foo(PH, PH)), # Don't currently support output pytrees','line_number':3593,'multiline':False]
['text':' Due to unflattening of dict, the batch argument','line_number':3648,'multiline':False]
['text':' will be split into two separate nodes with the names','line_number':3649,'multiline':False]
['text':' "batch_1" and "batch_2", referring to the keys','line_number':3650,'multiline':False]
['text':' "f1" and "f2" respectively in the dict.','line_number':3651,'multiline':False]
['text':' Ensures that tags on nodes are NOT overwritten by PH attributes with same attr name (tag)','line_number':3678,'multiline':False]
['text':' Ensure that tag is still "foo" and not "bar" (from PHWithTag)','line_number':3696,'multiline':False]
['text':' circular reference','line_number':3826,'multiline':False]
['text':' finishes','line_number':3827,'multiline':False]
['text':' Use the raw enum.','line_number':3838,'multiline':False]
['text':' Pass the enum as argument.','line_number':3843,'multiline':False]
['text':' Checking for mutable operations whil tracing is feature flagged','line_number':3881,'multiline':False]
['text':' Enable it in testing but not by default','line_number':3882,'multiline':False]
['text':' Iterate through overloads until we hit a match. If we exit this','line_number':3899,'multiline':False]
['text':' loop via `else`, we haven't found a match','line_number':3900,'multiline':False]
['text':' Checking for mutable operations whil tracing is feature flagged','line_number':3918,'multiline':False]
['text':' Enable it in testing but not by default','line_number':3919,'multiline':False]
['text':' Need to fix up some default value strings.','line_number':3954,'multiline':False]
['text':' First case: modules. Default module `repr` contains the FS path of the module.','line_number':3955,'multiline':False]
['text':' Don't leak that','line_number':3956,'multiline':False]
['text':' Second case: callables. Callables (such as lambdas) encode their address in','line_number':3960,'multiline':False]
['text':' their string repr. Don't do that','line_number':3961,'multiline':False]
['text':' Forward ref','line_number':3988,'multiline':False]
['text':' Handle types with contained types','line_number':4026,'multiline':False]
['text':' Callables contain a bare List for arguments','line_number':4029,'multiline':False]
['text':' Python 3.8 puts type vars into __args__ for unbound types such as Dict','line_number':4032,'multiline':False]
['text':' Unbound types don't have `__origin__` in some Python versions, so fix that up here.','line_number':4042,'multiline':False]
['text':' Annoying hack to detect Optional','line_number':4048,'multiline':False]
['text':' noqa: TRY200','line_number':4096,'multiline':False]
['text':' Only want objects in torch.fx','line_number':4147,'multiline':False]
['text':' Only want objects in public namespaces','line_number':4150,'multiline':False]
['text':' Checking for mutable operations whil tracing is feature flagged','line_number':4194,'multiline':False]
['text':' Enable it in testing but not by default','line_number':4195,'multiline':False]
['text':' List of nn.functionals with Tensor inputs but not with type annotation','line_number':4347,'multiline':False]
['text':' Inconsistent behavior between Python 3.8 and other Python versions:','line_number':4363,'multiline':False]
['text':' - Python 3.8+: Re-raise internal exception like `PROXY_ITERATED`','line_number':4364,'multiline':False]
['text':' - Other Python: Raise `argument of type 'Proxy' is not iterable` due to the same','line_number':4365,'multiline':False]
['text':'                 internal exception above','line_number':4366,'multiline':False]
['text':' Use the following map to override the expected exception for Python 3.8','line_number':4367,'multiline':False]
['text':' Ignore internal functions','line_number':4387,'multiline':False]
['text':' Ignore supporting functions','line_number':4390,'multiline':False]
['text':' Ignore non-callable object like modules','line_number':4394,'multiline':False]
['text':' No signature or Object is not supported','line_number':4406,'multiline':False]
['text':' Checking for mutable operations while tracing is feature flagged','line_number':4460,'multiline':False]
['text':' Enable it in testing but not by default','line_number':4461,'multiline':False]
