['text':' Owner(s): ["module: onnx"]','line_number':1,'multiline':False]
['text':' Einsum is supported since opset 12. It should be unconvertible at opset 9.','line_number':103,'multiline':False]
['text':' Einsum is supported since opset 12. It should be unconvertible at opset 9.','line_number':130,'multiline':False]
['text':' Einsum is supported since opset 12','line_number':151,'multiline':False]
['text':' index exceeds dimension','line_number':316,'multiline':False]
['text':' index relative to the end','line_number':338,'multiline':False]
['text':' Why did I insert a Cast here?  There appears to be intentional','line_number':456,'multiline':False]
['text':' behavior in ONNX constant folding where constant tensors which','line_number':457,'multiline':False]
['text':' are not attached to any known to be foldable onnx','line_number':458,'multiline':False]
['text':' operations don't get extracted into the initializer graph.  So','line_number':459,'multiline':False]
['text':' without these casts, we will actually fail to pull out one of','line_number':460,'multiline':False]
['text':' the constants, thus failing constant folding.  I think the','line_number':461,'multiline':False]
['text':' test is wrong but I don't have time to write a more correct','line_number':462,'multiline':False]
['text':' test (I think the right way to go about the test is to setup','line_number':463,'multiline':False]
['text':' a predicate for what invariant graphs should hold after','line_number':464,'multiline':False]
['text':' constant folding, and then verify this predicate holds.','line_number':465,'multiline':False]
['text':' I think the asserts below are an attempt at this predicate,','line_number':466,'multiline':False]
['text':' but it is not right!)','line_number':467,'multiline':False]
['text':'','line_number':468,'multiline':False]
['text':' More commentary at','line_number':469,'multiline':False]
['text':' https://github.com/pytorch/pytorch/pull/18698/files#r340107552','line_number':470,'multiline':False]
['text':' Unsqueeze op parameter "axes" as an input instead of as an attribute when opset version >= 13','line_number':517,'multiline':False]
['text':' upsample scale is a constant, not a model parameter,','line_number':714,'multiline':False]
['text':' therefore should not be added as initializer after constant folding.','line_number':715,'multiline':False]
['text':' test verbose=False (default)','line_number':742,'multiline':False]
['text':' test verbose=True','line_number':744,'multiline':False]
['text':' NB: remove this test once DataParallel can be correctly handled','line_number':747,'multiline':False]
['text':' Export with scripting to keep output as Sequence type.','line_number':767,'multiline':False]
['text':' Tracing unpacks the list.','line_number':768,'multiline':False]
['text':' Case 1: dynamic axis','line_number':772,'multiline':False]
['text':' Case 2: no dynamic axes.','line_number':790,'multiline':False]
['text':' set mode to in inference mode and export in training mode','line_number':811,'multiline':False]
['text':' verify that the model state is preserved','line_number':821,'multiline':False]
['text':' set mode to training mode and export in inference mode','line_number':824,'multiline':False]
['text':' verify that the model state is preserved','line_number':834,'multiline':False]
['text':' Freezing is only implemented in eval mode. So we need to call eval()','line_number':854,'multiline':False]
['text':' jit.freeze removes the training attribute in the module','line_number':857,'multiline':False]
['text':' Export specified modules. Test against specifying modules that won't','line_number':894,'multiline':False]
['text':' exist in the exported model.','line_number':895,'multiline':False]
['text':' Model export in inference mode will remove dropout node,','line_number':896,'multiline':False]
['text':' thus the dropout module no longer exist in graph.','line_number':897,'multiline':False]
['text':' Check function definition','line_number':913,'multiline':False]
['text':' Check local function nodes','line_number':924,'multiline':False]
['text':' Export specified modules.','line_number':935,'multiline':False]
['text':' Export with empty specified modules. Normal export.','line_number':950,'multiline':False]
['text':' Export all modules. Should contain {M, CELU, LayerNorm}.','line_number':964,'multiline':False]
['text':' Failing after ONNX 1.13.0','line_number':1018,'multiline':False]
['text':' Concatenation of scalars inserts unscoped tensors in IR graph.','line_number':1023,'multiline':False]
['text':' This test cases checks the issue where an object does not have an attribute.','line_number':1097,'multiline':False]
['text':' When enabling `export_modules_as_functions = True`, the exporter could return an','line_number':1098,'multiline':False]
['text':' AttributeError. With this test case, we check that the export passes successfully','line_number':1099,'multiline':False]
['text':' without any AttributeError exceptions.','line_number':1100,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/pull/109759 for an example. The exception that','line_number':1101,'multiline':False]
['text':' this test tries to avoid is `AttributeError: 'Embedding' object has no attribute 'freeze'`.','line_number':1102,'multiline':False]
['text':' Allows the test case to print `Skipping module attribute 'freeze'`','line_number':1133,'multiline':False]
['text':' 'self.constant' is designed to be the same for all layers,','line_number':1205,'multiline':False]
['text':' hence it is common sub expression.','line_number':1206,'multiline':False]
['text':' NOTE: Duplicated constants are populated due to implicit casting in scalar_type_analysis,','line_number':1225,'multiline':False]
['text':'       so we expect 3 constants with different scopes. The 3 constants are for the 3 layers.','line_number':1226,'multiline':False]
['text':'       If CSE in exporter is improved later, this test needs to be updated.','line_number':1227,'multiline':False]
['text':'       It should expect 1 constant, with same scope as root.','line_number':1228,'multiline':False]
['text':' 'constant' and 'x' is designed to be the same for all layers,','line_number':1254,'multiline':False]
['text':' hence `x + self.constant` is common sub expression.','line_number':1255,'multiline':False]
['text':' 'bias' is designed to be different for all layers,','line_number':1256,'multiline':False]
['text':' hence `* self.bias` is not common sub expression.','line_number':1257,'multiline':False]
['text':' Test aten export of op with no symbolic','line_number':1305,'multiline':False]
['text':' Test custom op','line_number':1323,'multiline':False]
['text':' Calling custom op','line_number':1344,'multiline':False]
['text':' Test aten export of op with symbolic for aten','line_number':1430,'multiline':False]
['text':' prim::ListConstruct is exported as onnx::SequenceConstruct for opset >= 11','line_number':1446,'multiline':False]
['text':' Test prim op','line_number':1449,'multiline':False]
['text':' Move import to local as caffe2 backend requires additional build flag,','line_number':1637,'multiline':False]
['text':' and is only used in this test case.','line_number':1638,'multiline':False]
['text':' Check that the prim::Constant node in the graph for representing the','line_number':1706,'multiline':False]
['text':' scripted function `f` is removed and the following prim::CallFunction','line_number':1707,'multiline':False]
['text':' is replced by inline graph, with onnx::Sub and onnx::Add nodes.','line_number':1708,'multiline':False]
['text':' onnx::Sub and onnx::Add nodes only.','line_number':1713,'multiline':False]
['text':' Reusing layers.','line_number':1794,'multiline':False]
['text':' Reusing parameters.','line_number':1797,'multiline':False]
['text':' Parameter with different tensors equal in value.','line_number':1801,'multiline':False]
['text':' Test training mode.','line_number':1815,'multiline':False]
['text':' Test eval mode.','line_number':1840,'multiline':False]
['text':' upsample scale is a constant, not a model parameter,','line_number':1927,'multiline':False]
['text':' therefore should be ignored by shared weight deduplication.','line_number':1928,'multiline':False]
['text':' aten::upsample converts to onnx::resize','line_number':1942,'multiline':False]
