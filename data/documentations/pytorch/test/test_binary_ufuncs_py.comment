['text':' Owner(s): ["module: tests"]','line_number':1,'multiline':False]
['text':' TODO: update to use opinfos consistently','line_number':74,'multiline':False]
['text':' Generic tests for elementwise binary (AKA binary universal (u) functions (funcs))','line_number':76,'multiline':False]
['text':' TODO: below contiguous tensor results are compared with a variety of noncontiguous results.','line_number':77,'multiline':False]
['text':'   It would be interesting to have the lhs and rhs have different discontiguities.','line_number':78,'multiline':False]
['text':' Helper for comparing torch tensors and NumPy arrays','line_number':80,'multiline':False]
['text':' TODO: should this or assertEqual also validate that strides are equal?','line_number':81,'multiline':False]
['text':' Some NumPy functions return scalars, not arrays','line_number':87,'multiline':False]
['text':' Handles exact dtype comparisons between arrays and tensors','line_number':91,'multiline':False]
['text':' Allows array dtype to be float32 when comparing with bfloat16 tensors','line_number':93,'multiline':False]
['text':'   since NumPy doesn't support the bfloat16 dtype','line_number':94,'multiline':False]
['text':' Also ops like scipy.special.erf, scipy.special.erfc, etc, promote float16','line_number':95,'multiline':False]
['text':' to float32','line_number':96,'multiline':False]
['text':' Tests that the function and its (array-accepting) reference produce the same','line_number':116,'multiline':False]
['text':'   values on given tensors','line_number':117,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_utils.py#L1149','line_number':128,'multiline':False]
['text':' Each sample input acquired from the generator is just one lhs tensor','line_number':149,'multiline':False]
['text':'   and one rhs tensor','line_number':150,'multiline':False]
['text':' Crafts a custom error message for smaller, printable tensors','line_number':161,'multiline':False]
['text':' Assumes x is a scalar','line_number':165,'multiline':False]
['text':' testing multi-outputs results','line_number':184,'multiline':False]
['text':' The following tests only apply to elementwise binary operators with references','line_number':187,'multiline':False]
['text':' tests broadcasting and noncontiguous broadcasting behavior','line_number':244,'multiline':False]
['text':' Tests that elementwise binary operators participate in type promotion properly','line_number':474,'multiline':False]
['text':' NOTE: because the cross-product of all possible type promotion tests is huge, this','line_number':475,'multiline':False]
['text':'   just spot checks some handwritten cases.','line_number':476,'multiline':False]
['text':' NOTE: It may be possible to refactor this test into something simpler','line_number':477,'multiline':False]
['text':' int x int type promotion','line_number':496,'multiline':False]
['text':' standard type promotion','line_number':522,'multiline':False]
['text':' Integers can be safely cast to other integer types','line_number':535,'multiline':False]
['text':' Float outs cannot be safely cast to integer types','line_number':543,'multiline':False]
['text':' Neither integer nor float outs can be cast to bool','line_number':548,'multiline':False]
['text':' All these output types can be cast to any float or complex type','line_number':556,'multiline':False]
['text':' float x float type promotion','line_number':571,'multiline':False]
['text':' normal float type promotion','line_number':581,'multiline':False]
['text':' All these output types can be cast to any float or complex type','line_number':588,'multiline':False]
['text':' float outs can't be cast to an integer dtype','line_number':605,'multiline':False]
['text':' bool outs can be cast to an integer dtype','line_number':613,'multiline':False]
['text':' complex x complex type promotion','line_number':618,'multiline':False]
['text':' normal complex type promotion','line_number':628,'multiline':False]
['text':' All these output types can be cast to any or complex type','line_number':635,'multiline':False]
['text':' complex outs can't be cast to float types','line_number':643,'multiline':False]
['text':' complex outs can't be cast to an integer dtype','line_number':650,'multiline':False]
['text':' bool outs can be cast to a float type','line_number':658,'multiline':False]
['text':' bool outs can be cast to an integer dtype','line_number':665,'multiline':False]
['text':' int x float type promotion','line_number':670,'multiline':False]
['text':' Note: float type is the result dtype','line_number':671,'multiline':False]
['text':' float x complex type promotion','line_number':680,'multiline':False]
['text':' Note: complex type with highest "value type" is the result dtype','line_number':681,'multiline':False]
['text':' int x float scalar type promotion','line_number':692,'multiline':False]
['text':' Note: default float dtype is the result dtype','line_number':693,'multiline':False]
['text':' repeats with a scalar float tensor, which should set the dtype','line_number':704,'multiline':False]
['text':' Additional test with double','line_number':710,'multiline':False]
['text':' float x complex scalar type promotion','line_number':719,'multiline':False]
['text':' Note: result dtype is complex with highest "value type" among all tensors','line_number':720,'multiline':False]
['text':' repeats with a scalar complex tensor','line_number':734,'multiline':False]
['text':' Additional test with complexdouble','line_number':742,'multiline':False]
['text':' Value type of 1D+ Tensor (lhs_f32) takes priority over scalar tensor (rhs_c128).','line_number':746,'multiline':False]
['text':' float x float scalar tensor','line_number':752,'multiline':False]
['text':' Note: result dtype is the type of the float tensor','line_number':753,'multiline':False]
['text':' complex x complex scalar tensor','line_number':762,'multiline':False]
['text':' Note: result dtype is the type of the complex tensor','line_number':763,'multiline':False]
['text':' scalar  x scalar','line_number':777,'multiline':False]
['text':' Note: result dtype is default float type','line_number':778,'multiline':False]
['text':' TODO: move to error input test','line_number':786,'multiline':False]
['text':' empty + empty','line_number':814,'multiline':False]
['text':' scalar + empty','line_number':828,'multiline':False]
['text':' non-empty, empty','line_number':834,'multiline':False]
['text':' zero-dim variables that don't require grad should bind to scalar arguments','line_number':850,'multiline':False]
['text':' 3 + (3 * 3) * 2','line_number':853,'multiline':False]
['text':' Tests that the binary operators and, or, and xor (as well as their reflected and inplace versions)','line_number':859,'multiline':False]
['text':' work properly (AKA &, ||, ^ and &=, |=, ^=)','line_number':860,'multiline':False]
['text':' Tensor x Tensor and Tensor x Scalar ops','line_number':863,'multiline':False]
['text':' Tests tensor x tensor case','line_number':876,'multiline':False]
['text':' Tests tensor x scalar case','line_number':883,'multiline':False]
['text':' Tests scalar x tensor case','line_number':889,'multiline':False]
['text':' Tests scalar x tensor case (for ops which aren't inplace)','line_number':895,'multiline':False]
['text':' Tests tensor x tensor case','line_number':897,'multiline':False]
['text':' Tests tensor x scalar case','line_number':906,'multiline':False]
['text':' Avoid division by zero so we can test (a / b) * b == a','line_number':932,'multiline':False]
['text':' floor(a / b) * b can be < a, so fixup slightly to avoid underflow','line_number':940,'multiline':False]
['text':' Compare division of special floating point values against NumPy','line_number':970,'multiline':False]
['text':' Divide by zero is tested separately','line_number':975,'multiline':False]
['text':' Compare bfloat16 against NumPy float','line_number':980,'multiline':False]
['text':' Compare contiguous (likely vectorized) against non-contiguous (not vectorized)','line_number':999,'multiline':False]
['text':' NOTE: NumPy's floor_divide rounding changed in 1.20.0 to be consistent with divide','line_number':1028,'multiline':False]
['text':' CPU scalar','line_number':1031,'multiline':False]
['text':' Device tensor','line_number':1034,'multiline':False]
['text':' Compare division of random values against NumPy','line_number':1043,'multiline':False]
['text':' Avoid division by zero which raises for integers and, for floats,','line_number':1047,'multiline':False]
['text':' NumPy 1.20 changed floor_divide to follow IEEE rules for inf/nan','line_number':1048,'multiline':False]
['text':' after dividing by zero.','line_number':1049,'multiline':False]
['text':' Compare bfloat16 against NumPy float','line_number':1052,'multiline':False]
['text':' Contiguous (likely vectorized)','line_number':1068,'multiline':False]
['text':' Non-contiguous (not vectorized)','line_number':1075,'multiline':False]
['text':' test to make sure the complex division does not produce underflow or overflow','line_number':1086,'multiline':False]
['text':' in the intermediate of its calculations','line_number':1087,'multiline':False]
['text':' NOTE: the calculation still produces an error if the number is greater than','line_number':1088,'multiline':False]
['text':' finfo.max / 2, but hopefully people realized that it's a dangerous region to work with','line_number':1089,'multiline':False]
['text':' Tests that trying to add, inplace, a CUDA tensor to a CPU tensor','line_number':1112,'multiline':False]
['text':'   throws the correct error message','line_number':1113,'multiline':False]
['text':' TODO: refactor this test into a more generic one, it's parked here currently','line_number':1123,'multiline':False]
['text':' No warnings','line_number':1138,'multiline':False]
['text':' Cases that throw warnings','line_number':1143,'multiline':False]
['text':' test that multi-d out doesn't trigger segfault','line_number':1146,'multiline':False]
['text':' Verifies that the inplace dunders (like idiv) actually are in place','line_number':1157,'multiline':False]
['text':' UserWarning not triggered','line_number':1158,'multiline':False]
['text':' output is identical to input:','line_number':1195,'multiline':False]
['text':' output and input are independent:','line_number':1197,'multiline':False]
['text':' output partially overlaps with input:','line_number':1199,'multiline':False]
['text':' base - tensor, exponent - number','line_number':1308,'multiline':False]
['text':' contiguous','line_number':1309,'multiline':False]
['text':' `math.pow` has issues with complex exponentiation so we need to resort to normal `pow`.','line_number':1312,'multiline':False]
['text':' non-contiguous','line_number':1318,'multiline':False]
['text':' scalar ** tensor to enforce correct handling of dtypes for __rpow__().','line_number':1325,'multiline':False]
['text':' math.pow will overflow and throw exceptions for large integers','line_number':1342,'multiline':False]
['text':' On CPU,','line_number':1365,'multiline':False]
['text':' Half Tensor with complex exponents leads to computation dtype','line_number':1366,'multiline':False]
['text':' of ComplexHalf for which this ops is not supported yet','line_number':1367,'multiline':False]
['text':' base - number, exponent - tensor','line_number':1373,'multiline':False]
['text':' contiguous','line_number':1374,'multiline':False]
['text':' non-contiguous','line_number':1381,'multiline':False]
['text':' TODO: refactor all these tests using opinfos properly','line_number':1388,'multiline':False]
['text':' When base is a 0-dim cpu tensor and exp is a cuda tensor, we exp `pow` to work but `pow_` to fail, since','line_number':1422,'multiline':False]
['text':' `pow` will try to create the output tensor on a cuda device, but `pow_` needs to use the cpu tensor as the output','line_number':1423,'multiline':False]
['text':' We can potentially merge this into OpInfo, but one blocker is that the','line_number':1450,'multiline':False]
['text':' first input must be a scalar. It is not as simple as just wrapping this in','line_number':1451,'multiline':False]
['text':' a lambada that switches the inputs, because we also want to test samples inputs','line_number':1452,'multiline':False]
['text':' where the second input is a scalar. The wrapper would need some more logic.','line_number':1453,'multiline':False]
['text':' Tests pow() for integral, floating-type tensors, with integral, floating-type','line_number':1462,'multiline':False]
['text':' exponents (tensor or scalar), respectively. noncontiguous tensors are also tested.','line_number':1463,'multiline':False]
['text':' int tensors don't take negative exponents','line_number':1479,'multiline':False]
['text':' test non-contiguous tensors as well','line_number':1496,'multiline':False]
['text':' pow's output would have some NaNs as well','line_number':1540,'multiline':False]
['text':' Tests that a Runtime error occurs when a base tensor cannot be resized','line_number':1544,'multiline':False]
['text':' by pow's inplace variant due to PyTorch's broadcasting semantics.','line_number':1545,'multiline':False]
['text':' Binary ops with a cpu + cuda tensor are allowed if the cpu tensor has 0 dimension','line_number':1636,'multiline':False]
['text':' On CPU,','line_number':1660,'multiline':False]
['text':' Half Tensor with complex base leads to computation dtype','line_number':1661,'multiline':False]
['text':' of ComplexHalf for which this ops is not supported yet','line_number':1662,'multiline':False]
['text':' NOTE: pow has fast-path when base is 1 which supports','line_number':1663,'multiline':False]
['text':' ComplexHalf','line_number':1664,'multiline':False]
['text':' Test against a scalar and non-scalar input','line_number':1678,'multiline':False]
['text':' We expect the computation to be performed in uint8 (overflowing to 0), and then cast to int64','line_number':1681,'multiline':False]
['text':' Computation should run in int64, and not overflow','line_number':1689,'multiline':False]
['text':' test tensor that is not aligned to multiple of 16 bytes','line_number':1727,'multiline':False]
['text':' test tensor where there is a tail which is not a multiple','line_number':1745,'multiline':False]
['text':' of GPU warp size','line_number':1746,'multiline':False]
['text':' Tests that CUDA tensors on different devices cannot be used in the same','line_number':1755,'multiline':False]
['text':' binary operation, and that CUDA "scalars" cannot be used in the same','line_number':1756,'multiline':False]
['text':' binary operation as non-scalar CPU tensors.','line_number':1757,'multiline':False]
['text':' This test ensures that a scalar Tensor can be safely used','line_number':1792,'multiline':False]
['text':' in a binary operation in conjunction with a Tensor on all','line_number':1793,'multiline':False]
['text':' available CUDA devices','line_number':1794,'multiline':False]
['text':' Tests torch division ops which can handle both arguments being','line_number':1811,'multiline':False]
['text':'   scalars.','line_number':1812,'multiline':False]
['text':' Skips zero divisors','line_number':1819,'multiline':False]
['text':' Creates jitted functions of two tensors','line_number':1848,'multiline':False]
['text':' Skips zero divisors','line_number':1862,'multiline':False]
['text':' Creates jitted functions of one tensor','line_number':1874,'multiline':False]
['text':' NOTE: the JIT implements division as torch.reciprocal(a) * 5','line_number':1878,'multiline':False]
['text':' NOTE: this fails if the input is not an integer tensor','line_number':1885,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/45199','line_number':1886,'multiline':False]
['text':' Skips zero divisors','line_number':1903,'multiline':False]
['text':' Handles Issue 45199 (see comment above)','line_number':1909,'multiline':False]
['text':' This should emit a UserWarning, why doesn't it?','line_number':1914,'multiline':False]
['text':' See issue gh-52387','line_number':1915,'multiline':False]
['text':' The following functions are unsupported by the JIT','line_number':1945,'multiline':False]
['text':' Skips zero divisors','line_number':1972,'multiline':False]
['text':' Inplace modification fails because a float tensor is required','line_number':2010,'multiline':False]
['text':'   if the divisor is a float tensor','line_number':2011,'multiline':False]
['text':' Inplace modification is OK when both or neither tensor is','line_number':2017,'multiline':False]
['text':'   a float tensor','line_number':2018,'multiline':False]
['text':' Tests binary op equivalence with Python builtin ops','line_number':2034,'multiline':False]
['text':' Also tests that reverse operations are equivalent to forward ops','line_number':2035,'multiline':False]
['text':' NOTE: division ops are tested separately above','line_number':2036,'multiline':False]
['text':' Skips zero divisors','line_number':2050,'multiline':False]
['text':' np.maximum and np.minimum functions compare input arrays element-wisely.','line_number':2166,'multiline':False]
['text':' if one of the elements being compared is a NaN, then that element is returned.','line_number':2167,'multiline':False]
['text':' test cuda tensor and cpu scalar','line_number':2266,'multiline':False]
['text':' TODO: This should really be covered by OpInfo but it isn't. The problem','line_number':2313,'multiline':False]
['text':' is that our gradient tests test using float64 but it should also test','line_number':2314,'multiline':False]
['text':' float32','line_number':2315,'multiline':False]
['text':' TODO: tests like this should be generic','line_number':2339,'multiline':False]
['text':' Before Python 3.10, floats were implicitly converted to ints, but with','line_number':2360,'multiline':False]
['text':'   DeprecationWarning: an integer is required (got type float).','line_number':2361,'multiline':False]
['text':'   Implicit conversion to integers using __int__ is deprecated,','line_number':2362,'multiline':False]
['text':'   and may be removed in a future version of Python.','line_number':2363,'multiline':False]
['text':' Since Python 3.10, that attempt gives an error.','line_number':2364,'multiline':False]
['text':' bfloat16 has a lower precision so we have to have a separate check for it','line_number':2376,'multiline':False]
['text':' TODO: what is this test testing?','line_number':2381,'multiline':False]
['text':' with a tensor','line_number':2385,'multiline':False]
['text':' with a scalar','line_number':2394,'multiline':False]
['text':' TODO: reconcile with minimum/maximum tests','line_number':2403,'multiline':False]
['text':' 0:250: a -- nan, b -- not nan','line_number':2410,'multiline':False]
['text':' 250:500: a -- not nan, b -- nan','line_number':2412,'multiline':False]
['text':' 500:750: a and b both nan','line_number':2414,'multiline':False]
['text':' 750:1000: neither nan','line_number':2417,'multiline':False]
['text':' To handle inconsistencies of type promotion between PyTorch and Numpy','line_number':2462,'multiline':False]
['text':' Applied for both arguments having integral precision and bfloat16','line_number':2463,'multiline':False]
['text':' Verify Value','line_number':2470,'multiline':False]
['text':' Verify Sign','line_number':2472,'multiline':False]
['text':' Use double copysign to verify the correctnes of 0.0 and -0.0, since','line_number':2473,'multiline':False]
['text':' it always True for self.assertEqual(0.0 == -0.0). So, we use 1 as the','line_number':2474,'multiline':False]
['text':' magnitude to verify the sign between torch and numpy results, elementwise.','line_number':2475,'multiline':False]
['text':' Special case: NaN conversions between FP32 and FP16 is not bitwise','line_number':2476,'multiline':False]
['text':' equivalent to pass this assertion.','line_number':2477,'multiline':False]
['text':' Compare Result with NumPy','line_number':2484,'multiline':False]
['text':' Type promotion','line_number':2485,'multiline':False]
['text':' Broadcast','line_number':2490,'multiline':False]
['text':' 0.0/-0.0/inf/-inf/nan','line_number':2499,'multiline':False]
['text':' torch.bfloat16 can not hold '-nan'','line_number':2501,'multiline':False]
['text':' torch.half can not hold '-nan' on CUDA','line_number':2502,'multiline':False]
['text':' Input is 0.0','line_number':2527,'multiline':False]
['text':' Input is -0.0','line_number':2539,'multiline':False]
['text':' Other is 0.0','line_number':2551,'multiline':False]
['text':' Other is -0.0','line_number':2563,'multiline':False]
['text':' check floating-point tensor fmod/remainder to zero is nan on both CPU and GPU','line_number':2674,'multiline':False]
['text':' Check Issue https://github.com/pytorch/pytorch/issues/48130','line_number':2680,'multiline':False]
['text':' check integral tensor fmod/remainder to zero','line_number':2685,'multiline':False]
['text':' RuntimeError on CPU','line_number':2688,'multiline':False]
['text':' ROCm behavior: x % 0 is a no-op; x is returned','line_number':2693,'multiline':False]
['text':' CUDA behavior: Different value for different dtype','line_number':2696,'multiline':False]
['text':' Due to it's an undefined behavior, CUDA returns a pattern of all 1s','line_number':2697,'multiline':False]
['text':' for integral dividend (other than int64) divided by zero. For int64,','line_number':2698,'multiline':False]
['text':' CUDA returns all 1s for negative dividend, half 1s for positive dividend.','line_number':2699,'multiline':False]
['text':' uint8: 0xff -> 255','line_number':2700,'multiline':False]
['text':' int32: 0xffffffff -> -1','line_number':2701,'multiline':False]
['text':' Use numpy as reference','line_number':2711,'multiline':False]
['text':' out','line_number':2723,'multiline':False]
['text':' in-place (Type cast runtime error)','line_number':2728,'multiline':False]
['text':' mod with same dtype as x','line_number':2741,'multiline':False]
['text':' Exclude 0','line_number':2743,'multiline':False]
['text':' Mods: Integer, Float, Tensor, Non-contiguous Tensor','line_number':2746,'multiline':False]
['text':' mod with floating-point dtype','line_number':2748,'multiline':False]
['text':' Tests for torch.remainder(scalar, tensor)','line_number':2766,'multiline':False]
['text':' remainder has same sign as divisor','line_number':2787,'multiline':False]
['text':' fmod has same sign as dividend','line_number':2790,'multiline':False]
['text':' remainder is within range of divisor','line_number':2793,'multiline':False]
['text':' fmod is within range of divisor','line_number':2796,'multiline':False]
['text':' remainder is same as fmod','line_number':2798,'multiline':False]
['text':' differ by one divisor','line_number':2802,'multiline':False]
['text':' Tests gcd(0, 0), gcd(0, a) cases','line_number':2836,'multiline':False]
['text':' Test unsigned integers with potential sign issues (i.e., uint8 with value >= 128)','line_number':2844,'multiline':False]
['text':' Compares with NumPy','line_number':2851,'multiline':False]
['text':' Tests lcm(0, 0), lcm(0, a) cases','line_number':2861,'multiline':False]
['text':' Compares with NumPy','line_number':2868,'multiline':False]
['text':' Test special cases','line_number':2879,'multiline':False]
['text':' (from, to, expected)','line_number':2906,'multiline':False]
['text':' contiguous','line_number':2950,'multiline':False]
['text':' non-contiguous','line_number':2960,'multiline':False]
['text':' view as sm1.size()','line_number':2965,'multiline':False]
['text':' reference_implementation assumes 1-d sm2','line_number':2973,'multiline':False]
['text':' TODO: update make_tensor to support extremal additions and remove this in favor of make_tensor','line_number':3021,'multiline':False]
['text':' work around torch.randn not being implemented for bfloat16','line_number':3027,'multiline':False]
['text':' Use extremal values','line_number':3037,'multiline':False]
['text':' issue #42660','line_number':3061,'multiline':False]
['text':' testing all combinations of broadcasting and type promotion','line_number':3062,'multiline':False]
['text':' with a range of dtypes and input shapes, and with extremal values','line_number':3063,'multiline':False]
['text':' working around the fact that numpy doesn't support bfloat16','line_number':3065,'multiline':False]
['text':' by letting numpy treat them as float32's','line_number':3066,'multiline':False]
['text':' complex not supported','line_number':3084,'multiline':False]
['text':' perform broadcasting','line_number':3099,'multiline':False]
['text':' functional version of op','line_number':3108,'multiline':False]
['text':' functional comparison ops always return bool tensors','line_number':3111,'multiline':False]
['text':' out version of op','line_number':3114,'multiline':False]
['text':' all casts to complex128 are safe','line_number':3117,'multiline':False]
['text':' [11...1110110, 1010]','line_number':3124,'multiline':False]
['text':' [11...11011000, 101000]','line_number':3127,'multiline':False]
['text':' [1111...111011, 101]','line_number':3132,'multiline':False]
['text':' Issue #70904','line_number':3140,'multiline':False]
['text':' numpy changes dtype from uint8 to int16 for some out-of-limits shift values','line_number':3145,'multiline':False]
['text':' small for non-vectorized operation','line_number':3147,'multiline':False]
['text':' small for non-vectorized operation','line_number':3148,'multiline':False]
['text':' large for vectorized operation','line_number':3149,'multiline':False]
['text':' new tensor','line_number':3297,'multiline':False]
['text':' out','line_number':3299,'multiline':False]
['text':' Check Integer Overflows','line_number':3341,'multiline':False]
['text':' test with scalar','line_number':3353,'multiline':False]
['text':' random values','line_number':3360,'multiline':False]
['text':' basic test','line_number':3364,'multiline':False]
['text':' test bounds','line_number':3373,'multiline':False]
['text':' Tensor weights','line_number':3391,'multiline':False]
['text':' numpy has not implemented logaddexp for complex','line_number':3447,'multiline':False]
['text':' simple test','line_number':3466,'multiline':False]
['text':' large value test for numerical stability','line_number':3472,'multiline':False]
['text':' complex infs/nans differ under Dynamo/Inductor','line_number':3490,'multiline':False]
['text':' [res] torch.add([res,] tensor1, tensor2)','line_number':3503,'multiline':False]
['text':' contiguous','line_number':3507,'multiline':False]
['text':' non-contiguous','line_number':3517,'multiline':False]
['text':' [res] torch.add([res,] tensor, value)','line_number':3524,'multiline':False]
['text':' contiguous','line_number':3527,'multiline':False]
['text':' non-contiguous','line_number':3535,'multiline':False]
['text':' inter-type','line_number':3544,'multiline':False]
['text':' contiguous + non-contiguous','line_number':3549,'multiline':False]
['text':' 1d + empty','line_number':3556,'multiline':False]
['text':' inter-type unint8','line_number':3561,'multiline':False]
['text':' bool','line_number':3566,'multiline':False]
['text':' fused multiply add','line_number':3578,'multiline':False]
['text':' bfloat16','line_number':3584,'multiline':False]
['text':' different alpha types','line_number':3589,'multiline':False]
['text':' add complex numbers with float alpha','line_number':3592,'multiline':False]
['text':' add complex numbers with complex alpha','line_number':3599,'multiline':False]
['text':' add complex numbers with integer alpha','line_number':3606,'multiline':False]
['text':' mismatched alpha','line_number':3613,'multiline':False]
['text':' mismatched alpha, float / double tensor and complex alpha','line_number':3627,'multiline':False]
['text':' complex','line_number':3641,'multiline':False]
['text':' mismatched alpha','line_number':3688,'multiline':False]
['text':' bfloat16/float16','line_number':3808,'multiline':False]
['text':' Older version of SciPy uses a different name','line_number':3893,'multiline':False]
['text':' SciPy failing when x == [], but our version returns empty','line_number':3948,'multiline':False]
['text':' Complex and real results do not agree between PyTorch and NumPy when computing negative and zero power of 0','line_number':4006,'multiline':False]
['text':' Related: https://github.com/pytorch/pytorch/issues/48000','line_number':4007,'multiline':False]
['text':' base[0] = base[3] = base[7] = 0','line_number':4008,'multiline':False]
['text':' Case of Tensor x Tensor','line_number':4031,'multiline':False]
['text':' Case of Tensor x Scalar','line_number':4046,'multiline':False]
['text':' Case of Scalar x Tensor','line_number':4074,'multiline':False]
['text':' Tensor-Tensor Test (tensor of same and different shape)','line_number':4173,'multiline':False]
['text':' Scalar-Tensor Test','line_number':4195,'multiline':False]
['text':' Special Values Tensor-Tensor','line_number':4199,'multiline':False]
['text':' Special Values Scalar-Tensor','line_number':4220,'multiline':False]
['text':' Test that python numbers don't participate in type promotion at the same','line_number':4241,'multiline':False]
['text':' priority level as 0-dim tensors','line_number':4242,'multiline':False]
['text':' Tensor-Tensor Test (tensor of same and different shape)','line_number':4266,'multiline':False]
['text':' Special Values Tensor-Tensor','line_number':4296,'multiline':False]
['text':' x tensor - q tensor same size','line_number':4326,'multiline':False]
['text':' x tensor - q tensor broadcast lhs','line_number':4331,'multiline':False]
['text':' x tensor - q tensor broadcast rhs','line_number':4336,'multiline':False]
['text':' x tensor - q tensor broadcast all','line_number':4341,'multiline':False]
['text':' x scalar - q tensor','line_number':4346,'multiline':False]
['text':' x tensor - q scalar','line_number':4353,'multiline':False]
['text':' Tests that Tensor and CPU Scalar work for `mul` for chalf.','line_number':4365,'multiline':False]
['text':' Ideally, this should be covered by `test_complex_half_reference_testing`','line_number':4366,'multiline':False]
['text':' from test_ops.py by checking reference_samples from the OpInfo.','line_number':4367,'multiline':False]
['text':' But currently that doesn't work as sample generation requires support of','line_number':4368,'multiline':False]
['text':' `index_select` which is not implemented for `complex32` at the','line_number':4369,'multiline':False]
['text':' time of writing this test.','line_number':4370,'multiline':False]
['text':' TODO: Remove this test once above issue is fixed.','line_number':4371,'multiline':False]
['text':' Ref: https://github.com/pytorch/pytorch/pull/76364','line_number':4372,'multiline':False]
['text':' Unsupported operators','line_number':4422,'multiline':False]
['text':' '__imatmul__',','line_number':4423,'multiline':False]
['text':' '__divmod__', '__rdivmod__', '__idivmod__',','line_number':4424,'multiline':False]
['text':' Test that binary math operations return NotImplemented for unknown types.','line_number':4427,'multiline':False]
['text':' TODO: refactor to inline these','line_number':4432,'multiline':False]
['text':' Generate the inputs','line_number':4447,'multiline':False]
['text':' Runs the tensor op on the device','line_number':4450,'multiline':False]
