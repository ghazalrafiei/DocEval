['text':' Owner(s): ["oncall: pt2"]','line_number':1,'multiline':False]
['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':3,'multiline':False]
['text':' All rights reserved.','line_number':4,'multiline':False]
['text':'','line_number':5,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':6,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':7,'multiline':False]
['text':' noqa: F401','line_number':75,'multiline':False]
['text':' NB: numpy is a testing dependency!','line_number':81,'multiline':False]
['text':' Also verifies fix for https://github.com/pytorch/pytorch/issues/84570','line_number':157,'multiline':False]
['text':' FIXME','line_number':165,'multiline':False]
['text':' test_mutation will:','line_number':257,'multiline':False]
['text':' - Ensure that inputs are non-leaves, so our graphs can mutate them','line_number':258,'multiline':False]
['text':' - try to mutate outputs of the graph (to ensure that autograd meta is set properly on outputs)','line_number':259,'multiline':False]
['text':' Only active when inp_ is Callable.','line_number':270,'multiline':False]
['text':' TODO: probably consolidate all tests to make inp a Callable.','line_number':271,'multiline':False]
['text':' Some tests pass in a callable for inp, to generate the inputs','line_number':275,'multiline':False]
['text':' (useful if we want to generate complicated aliasing inputs)','line_number':276,'multiline':False]
['text':' The callable should return a tuple of f_inputs, f_graph_inputs','line_number':279,'multiline':False]
['text':' (The idea is that we might want to compile a function with the graph inputs,','line_number':280,'multiline':False]
['text':' but test autograd backprop all the way through the actual inputs)','line_number':281,'multiline':False]
['text':' Our input clones need to mimic when inputs are duplicates of one another','line_number':288,'multiline':False]
['text':' For graphs where we mutate inputs, need our test to make sure inputs aren't leaves','line_number':310,'multiline':False]
['text':' This tests that autograd meta is set properly on the output we can','line_number':351,'multiline':False]
['text':' mutate it.','line_number':352,'multiline':False]
['text':' int, None, Tensor','line_number':363,'multiline':False]
['text':' Test for bug occurring at the intersection of fake tensors & functionalization.','line_number':395,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/93424','line_number':408,'multiline':False]
['text':' Backwards pass tries to wrap a sparse tensor in a FunctionalTensorWrapper;','line_number':422,'multiline':False]
['text':' test that this works even though the sparse tensor has no storage.','line_number':423,'multiline':False]
['text':' Things to note:','line_number':448,'multiline':False]
['text':' - the extra clone is because we need to pass the pre-mutated input to grad(),','line_number':449,'multiline':False]
['text':'   but autograd operates above functionalization so we need to manually clone.','line_number':450,'multiline':False]
['text':'   Hopefully backends can optimize this easily.','line_number':451,'multiline':False]
['text':' - The extra return arg is because the compiled forward returns (mutated inputs + outputs)','line_number':452,'multiline':False]
['text':' a_clone should inherit the view chain from b_slice','line_number':476,'multiline':False]
['text':' Also mutates b_,','line_number':478,'multiline':False]
['text':' The data mutation happens *after* the set_(). This is ok (see the graph below)','line_number':486,'multiline':False]
['text':' Important things to note:','line_number':495,'multiline':False]
['text':' - "return a.set_(b)" desugars into "return b"','line_number':496,'multiline':False]
['text':' - Both a and b are recorded as experiencing mutations,','line_number':497,'multiline':False]
['text':'   which is why we see "b_updated" (output of the mul) twice in the graph outputs.','line_number':498,'multiline':False]
['text':'   a is recorded as both a data mutation and a metadata mutation (due to set_ swapping its storage).','line_number':499,'multiline':False]
['text':' - the runtime epilogue for a is "a.set_(mul)"','line_number':500,'multiline':False]
['text':' - the runtime epilogue for b is "b.copy_(mul)"','line_number':501,'multiline':False]
['text':' This is a (hopefully) extremely rare case that is difficult to handle,','line_number':509,'multiline':False]
['text':' so we ban it.','line_number':510,'multiline':False]
['text':' Now, any mutations on either tmp','line_number':515,'multiline':False]
['text':' will be tracked as graph input mutations.','line_number':516,'multiline':False]
['text':' BAD: a_view is now detached from every graph input,','line_number':519,'multiline':False]
['text':' so we won't recognize that this caused an input mutation!','line_number':520,'multiline':False]
['text':' Things to note:','line_number':539,'multiline':False]
['text':' - There are no set_() calls in the graph (we functionalize a.set_(b) into "b")','line_number':540,'multiline':False]
['text':' - There is only **1** graph output. We properly realized that the two set_() calls','line_number':541,'multiline':False]
['text':'   undo each other, and so effectively no inputs are mutated.','line_number':542,'multiline':False]
['text':' Tensor, None, int','line_number':552,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/93363','line_number':562,'multiline':False]
['text':' f will mutate aliases of the input, including its autograd metadata!','line_number':596,'multiline':False]
['text':' y.grad_fn is AsStridedBackward','line_number':597,'multiline':False]
['text':' Test the actual gradients are correct','line_number':601,'multiline':False]
['text':' Tensor, None, int','line_number':609,'multiline':False]
['text':' Try mutating one of the outputs, which is aliased.','line_number':621,'multiline':False]
['text':' Assert that the aliasing relationship was preserved','line_number':624,'multiline':False]
['text':' Here, "a" requires grad, and gets mutated, so we append a copy_() to the end of the graph.','line_number':700,'multiline':False]
['text':' Its mutation doesn't take part in autograd though, because we mutated a detach'd view.','line_number':701,'multiline':False]
['text':' Need to make sure that this copy_() doesn't error, and doesn't participate in autograd either.','line_number':702,'multiline':False]
['text':' test_mutation=True will first do some compute on inp, so it is no longer an autograd leaf','line_number':709,'multiline':False]
['text':' by the time it becomes a graph input. Good to test both cases.','line_number':710,'multiline':False]
['text':' Even though the input requires_grad, we expect the keep the input mutation in the graph','line_number':720,'multiline':False]
['text':' (Even though this is a training graph!)','line_number':721,'multiline':False]
['text':' Even though the input requires_grad, we expect the keep the input mutation in the graph','line_number':735,'multiline':False]
['text':' Perform a mix of mutations on a:','line_number':746,'multiline':False]
['text':' 1 normal, 1 in no_grad, 1 on a detach'd tensor.','line_number':747,'multiline':False]
['text':' Only the first should participate in gradient computation.','line_number':748,'multiline':False]
['text':' This is additionally a good test, because the input tensors that we mutate','line_number':770,'multiline':False]
['text':' are *also* saved for backwards.','line_number':771,'multiline':False]
['text':' This tests that what we save for the backward is actually cloned inputs,','line_number':772,'multiline':False]
['text':' and not the original inputs that got mutated.','line_number':773,'multiline':False]
['text':' This simulates what inductor does (running the fw + bw decompositions)','line_number':786,'multiline':False]
['text':' expectation: there are no copy_() calls in the decomposed batch norm when running under training=False (eval mode)','line_number':817,'multiline':False]
['text':' Outputs that alias inputs are pulled out of the graph entirely, so we don't compile anything here','line_number':828,'multiline':False]
['text':' The original function returned two outputs, both of which aliased inputs.','line_number':849,'multiline':False]
['text':' We expect two outputs in the functional graph, a_updated and c_updated.','line_number':850,'multiline':False]
['text':' The actual aliased outputs themselves aren't in the compiled forward graph;','line_number':851,'multiline':False]
['text':' Instead, they're generated outside of  the graph.','line_number':852,'multiline':False]
['text':' Important thing to check here: of the three inputs:','line_number':878,'multiline':False]
['text':' Only the b.mul_(3) should show up in the graph (we functionalize it and return it).','line_number':879,'multiline':False]
['text':' Everything else that does not show up in the graph includes:','line_number':880,'multiline':False]
['text':' - The metadata mutation on c (we do it outside the graph)','line_number':881,'multiline':False]
['text':' - All 3 original fw outputs, which are aliases of inputs (we regenerate them outside of the graph)','line_number':882,'multiline':False]
['text':' Here, total # of outputs is 1 because:','line_number':902,'multiline':False]
['text':' - num_mutated_inps = 1 (a_updated)','line_number':903,'multiline':False]
['text':' - num_fw_outputs = 0 (the output is an alias of the input, so we move it outside the compiled fw)','line_number':904,'multiline':False]
['text':' In AOTAutograd, we are obligated to make the compiled forward directly return `out`,','line_number':948,'multiline':False]
['text':' and reconstruct `out.view(-1)` as a fresh output.','line_number':949,'multiline':False]
['text':' This raises a runtime error from autograd in eager mode','line_number':968,'multiline':False]
['text':' In eager mode, if we mutate a tensor, any multi-output-view aliases','line_number':974,'multiline':False]
['text':' get their grad_fn replaced with error nodes, so accessing grad_fn should error','line_number':975,'multiline':False]
['text':' The above case also applies to detached aliases (they turn the multi-output-view','line_number':981,'multiline':False]
['text':' alias's grad_fns into error nodes)','line_number':982,'multiline':False]
['text':' All aliased outs are from multi-output views, so AOTAutograd will hide the aliasing from autograd.','line_number':987,'multiline':False]
['text':' Assert that we get CompiledFunctionBackward in the backward graph,','line_number':997,'multiline':False]
['text':' and not AsStridedBackward. No view-regeneration necessary for this mult-output view case.','line_number':998,'multiline':False]
['text':' See Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]','line_number':999,'multiline':False]
['text':' Several of the outputs are from multi-output views.','line_number':1006,'multiline':False]
['text':' However: they are part of the same alias set as "a", and "a.view(out.shape)",','line_number':1007,'multiline':False]
['text':' which are both user-visible.','line_number':1008,'multiline':False]
['text':' AOTAutograd will not try to be smart here and hide the aliasing relationships from autograd.','line_number':1009,'multiline':False]
['text':' Instead, it will perform its "output aliases input" logic, and regenerate all aliases.','line_number':1010,'multiline':False]
['text':' We rely on autograd's view-replay here, and view-replay gives up and uses as_strided','line_number':1022,'multiline':False]
['text':' for multi-output views','line_number':1023,'multiline':False]
['text':' The last output is not from a multi-output view, so autograd will let us mutate it.','line_number':1026,'multiline':False]
['text':' Also mutate the input, which should affect the aliased output.','line_number':1029,'multiline':False]
['text':' Do backward','line_number':1032,'multiline':False]
['text':' All aliased outs are from multi-output views, so AOTAutograd will hide the aliasing from autograd.','line_number':1039,'multiline':False]
['text':' Assert that we get CompiledFunctionBackward in the backward graph,','line_number':1050,'multiline':False]
['text':' and not AsStridedBackward. No view-regeneration necessary for this mult-output view case.','line_number':1051,'multiline':False]
['text':' See Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]','line_number':1052,'multiline':False]
['text':' All aliased outs but one are from multi-output views, so AOTAutograd will hide the aliasing from autograd.','line_number':1059,'multiline':False]
['text':' Assert that we get CompiledFunctionBackward in the backward graph,','line_number':1070,'multiline':False]
['text':' and not AsStridedBackward. No view-regeneration necessary for this mult-output view case.','line_number':1071,'multiline':False]
['text':' See Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]','line_number':1072,'multiline':False]
['text':' The last output is not from a multi-output view, so autograd will let us mutate it.','line_number':1075,'multiline':False]
['text':' All aliased outs but one are from multi-output views, so AOTAutograd will hide the aliasing from autograd.','line_number':1082,'multiline':False]
['text':' Assert that we get CompiledFunctionBackward in the backward graph,','line_number':1093,'multiline':False]
['text':' and not AsStridedBackward. No view-regeneration necessary for this mult-output view case.','line_number':1094,'multiline':False]
['text':' See Note: [AOTAutograd: differentiable outputs that alias each other from a multi-output view call]','line_number':1095,'multiline':False]
['text':' The last output is not from a multi-output view, so autograd will let us mutate it.','line_number':1098,'multiline':False]
['text':' There are 5 outputs that all alias each other.','line_number':1105,'multiline':False]
['text':' 3 of them come from multi-output views, but the other 3 are "ordinary" aliases.','line_number':1106,'multiline':False]
['text':' Therefore, AOTAutograd will not attempt the multi-output-view optimization,','line_number':1107,'multiline':False]
['text':' and apply the intermediate_base logic to all aliases.','line_number':1108,'multiline':False]
['text':' (In theory we could probably get AOTAutograd to only apply the intermediate base','line_number':1109,'multiline':False]
['text':' logic to the last 2 outputs and not the first 3. We should probably','line_number':1110,'multiline':False]
['text':' just do the graph partitioning defined in this doc instead though).','line_number':1111,'multiline':False]
['text':' https://docs.google.com/document/d/1DlfFq8TKbuAn2zyJxLfoW-X1qkkm5PLdHFtySo03QAk/edit','line_number':1112,'multiline':False]
['text':' also return the graph intermediate directly,','line_number':1115,'multiline':False]
['text':' which will force AOTAutograd to do the "intermediate base" logic.','line_number':1116,'multiline':False]
['text':' (Why? The user can mutate "out", which should change the autograd metadata','line_number':1117,'multiline':False]
['text':'  of the other aliased outputs)','line_number':1118,'multiline':False]
['text':' Mutate the last output of f4 (autograd will allow this, since it is not a multi-output view,','line_number':1127,'multiline':False]
['text':' as long as *only* the non-multi-output views participate in the backward)','line_number':1128,'multiline':False]
['text':' Note: We could probably try to hide **only** the multi-output views from autograd here','line_number':1129,'multiline':False]
['text':' and only do the intermediate base logic for the last two aliases.','line_number':1130,'multiline':False]
['text':' Longer term solution of graph partitioning is probably cleaner though (see the note).','line_number':1131,'multiline':False]
['text':' use inductor's decomps (which will e.g. turn _unsafe_view() into view())','line_number':1147,'multiline':False]
['text':' First output is an alias of an intermediate that doesn't require grad','line_number':1161,'multiline':False]
['text':' important bit: we don't bother generating an intermediate base as an output in the graph,','line_number':1167,'multiline':False]
['text':' because the intermediate base itself didn't require gradients.','line_number':1168,'multiline':False]
['text':' (the only problematic case is when both the base and the aliasesed output require gradients).','line_number':1169,'multiline':False]
['text':' AOTAutograd should manually generate these two output views in the epilogue.','line_number':1190,'multiline':False]
['text':' AOTAutograd should manually generate the first output (a view of an intermediate)','line_number':1206,'multiline':False]
['text':' but not the second (which is itself the intermediate for the first)','line_number':1207,'multiline':False]
['text':' AOTAutograd should manually generate the first output (a view of an intermediate)','line_number':1222,'multiline':False]
['text':' but not the second (which is itself the intermediate for the first)','line_number':1223,'multiline':False]
['text':' AOTAutograd should manually generate the first output (a view of an intermediate)','line_number':1238,'multiline':False]
['text':' but not the second (which is itself the intermediate for the first)','line_number':1239,'multiline':False]
['text':' TODO: fix this test.','line_number':1261,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/90507','line_number':1262,'multiline':False]
['text':' self.verify_aot_autograd(f, inp, test_mutation=True)','line_number':1263,'multiline':False]
['text':' Thanks to the detach_() AOT Autograd doesn't need to do anything.','line_number':1270,'multiline':False]
['text':' `out` will show up as having OutputType.non_alias,','line_number':1271,'multiline':False]
['text':' and ._is_view() == False','line_number':1272,'multiline':False]
['text':' TODO: fix this test.','line_number':1295,'multiline':False]
['text':' See <github issue link>','line_number':1296,'multiline':False]
['text':' self.verify_aot_autograd(f, inp, test_mutation=True)','line_number':1297,'multiline':False]
['text':' AOTAutograd should manually generate these two output views in the epilogue.','line_number':1303,'multiline':False]
['text':' There are 3 types of aliasing that require us to return metadata in the compiled fw:','line_number':1319,'multiline':False]
['text':' (1) outputs that are views of inputs','line_number':1320,'multiline':False]
['text':' (2) outputs that are views of intermediates','line_number':1321,'multiline':False]
['text':' (3) inputs that get metadata mutations','line_number':1322,'multiline':False]
['text':' test all 3 of them here','line_number':1323,'multiline':False]
['text':' TODO: make this test run with dynamic shapes so it is more meaningful','line_number':1335,'multiline':False]
['text':' metadata output order: (a_updated_meta, out1_meta, out2_meta, out3_meta)','line_number':1336,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/114975','line_number':1358,'multiline':False]
['text':' First inp doesnt require grad, but we switch it on','line_number':1421,'multiline':False]
['text':' This is a torture test:','line_number':1433,'multiline':False]
['text':' a and b get turned into a synthetic base in the compiled graph','line_number':1434,'multiline':False]
['text':' One gets a data mutation, the other gets a metadata mutation.','line_number':1435,'multiline':False]
['text':' We need to make sure that the metadata mutation gets propagated','line_number':1436,'multiline':False]
['text':' back to the original input.','line_number':1437,'multiline':False]
['text':' a and b are aliased','line_number':1439,'multiline':False]
['text':' Note: in our test, the add() is important because we need the graph inputs to be non-leaves so we can mutate them.','line_number':1447,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/106456','line_number':1460,'multiline':False]
['text':' create a non-contiguous view to pass as an input to the compiler','line_number':1469,'multiline':False]
['text':' Mutations in the backward are allowed as long as the mutated object does not require grad','line_number':1479,'multiline':False]
['text':' bw mutation','line_number':1490,'multiline':False]
['text':' Mutation on buffer that does not require grad during the backward is allowed','line_number':1503,'multiline':False]
['text':' bw metadata mutation','line_number':1523,'multiline':False]
['text':' Partially addresses https://github.com/pytorch/pytorch/issues/106457','line_number':1564,'multiline':False]
['text':' No overlap, contiguous','line_number':1571,'multiline':False]
['text':' create two non-contiguous views that share storage, but are actually non-overlapping','line_number':1575,'multiline':False]
['text':' Input mutations on subclasses with training graphs fail backward guards today.','line_number':1583,'multiline':False]
['text':' Important characteristic: the graph takes in 2 inputs!','line_number':1587,'multiline':False]
['text':' That shows that we didn't try to run our complicated synthetic base logic,','line_number':1588,'multiline':False]
['text':' because we successfully detected false aliasing across the two inputs.','line_number':1589,'multiline':False]
['text':' No overlap, non-contiguous: first tensor ends before second tensor start','line_number':1597,'multiline':False]
['text':' No overlap, non-contiguous: tensors are perfectly interleaved','line_number':1605,'multiline':False]
['text':' No overlap, non-contiguous','line_number':1613,'multiline':False]
['text':' No overlap, non-contiguous','line_number':1621,'multiline':False]
['text':' overlap! non-contiguous','line_number':1629,'multiline':False]
['text':' overlap! non-contiguous','line_number':1637,'multiline':False]
['text':' All non-overlap graphs should be the same since we detected false aliasing','line_number':1653,'multiline':False]
['text':' All overlap graphs should be the same since we detected real aliasing','line_number':1659,'multiline':False]
['text':' See a full diagnosis at this issue: https://github.com/pytorch/pytorch/issues/94990','line_number':1668,'multiline':False]
['text':' Note [Detaching saved tensors in AOTAutograd]','line_number':1669,'multiline':False]
['text':' This program creates a ref-cycle. Long term, we should fix this ref cycle','line_number':1670,'multiline':False]
['text':' (since it can arise, naturally albeit rarely, from uses of autograd.Function).','line_number':1671,'multiline':False]
['text':' But AOTAutograd makes it more likely to show up from tracing user programs,','line_number':1672,'multiline':False]
['text':' so we deal with it by manually detaching the tensors that we save for backward.','line_number':1673,'multiline':False]
['text':' This is completely wrong and would give wrong results if we were to do double backward.','line_number':1674,'multiline':False]
['text':' Fortunately today, double backward is explicitly banned in AOTAutograd.','line_number':1675,'multiline':False]
['text':' a and b are aliased, but have different shapes','line_number':1695,'multiline':False]
['text':' The first output should view off the first input, the 2nd output should view off the 2nd input','line_number':1696,'multiline':False]
['text':' Note: in our test, the add() is important because we need the graph inputs to be non-leaves so we can mutate them.','line_number':1702,'multiline':False]
['text':' Note: in our test, the add() is important because we need the graph inputs to be non-leaves so we can mutate them.','line_number':1720,'multiline':False]
['text':' Important parts of the graph:','line_number':1728,'multiline':False]
['text':' - the compiled graph takes in a base, and we generate a and b (the views) off of the base','line_number':1729,'multiline':False]
['text':' - clone() is still in the graph, because we need to call grad() on the original (non-mutated) inputs','line_number':1730,'multiline':False]
['text':' - We re-generate the views *after* the clone, to preserve view relationships.','line_number':1731,'multiline':False]
['text':' noqa: B950','line_number':1741,'multiline':False]
['text':' Here, one of the aliased inputs is the base itself','line_number':1752,'multiline':False]
['text':' noqa: B950','line_number':1767,'multiline':False]
['text':' Here, we need to take care:that because and b are aliased','line_number':1771,'multiline':False]
['text':' since a and b are aliased, we generate a view off of "updated b"','line_number':1772,'multiline':False]
['text':' noqa: B950','line_number':1791,'multiline':False]
['text':' a and c alias','line_number':1795,'multiline':False]
['text':' The main thing we're testing here is that','line_number':1797,'multiline':False]
['text':' (1) We need to reconstruct c.view(-1) from the 3rd input to the forward','line_number':1798,'multiline':False]
['text':' (2) But we need to be careful to do this *before* converting aliased inputs into synthetic bases.','line_number':1799,'multiline':False]
['text':'     The original fw takes in 3 args, but the compiled fw takes in only 2 args.','line_number':1800,'multiline':False]
['text':' noqa: B950','line_number':1821,'multiline':False]
['text':' a and b alias, and we do a metadata mutation on a','line_number':1825,'multiline':False]
['text':' Since we're not mutating data, then b isn't affected at all.','line_number':1826,'multiline':False]
['text':' We expect aot autograd to not bother with constructing a synthetic base.','line_number':1827,'multiline':False]
['text':' Expectation: fwd() takes in 2 args, and we don't construct a synthetic base.','line_number':1838,'multiline':False]
['text':' a and b alias, but neither require gradients (so they don't have a _base)','line_number':1847,'multiline':False]
['text':' aot autograd should construct the synthetic base from `torch.Tensor(a.storage())`','line_number':1848,'multiline':False]
['text':' noqa: B950','line_number':1872,'multiline':False]
['text':' This tests our calling convention: if b and d are aliased, then the outer calling convention','line_number':1875,'multiline':False]
['text':' that we send to the compiled forward becomes:','line_number':1876,'multiline':False]
['text':' (b_d_base, a, c)','line_number':1877,'multiline':False]
['text':' Importantly, even though a and c alias in our test, neither inputs are mutated,','line_number':1878,'multiline':False]
['text':' So we don't need to do the base construction / deconstruction','line_number':1879,'multiline':False]
['text':' a and c alias, b and d alias','line_number':1890,'multiline':False]
['text':' 3 graph inputs: (b_d_base, a, c)','line_number':1899,'multiline':False]
['text':' 2 returns: (b_updated, a+c+d)','line_number':1900,'multiline':False]
['text':' (there are 2 original fw outs, but one is a view of b so it's not part of the graph)','line_number':1901,'multiline':False]
['text':' (there are also 2 input mutations, but one is a metadata-only mutation so the compiled forward doesn't return it)','line_number':1902,'multiline':False]
['text':' noqa: B950','line_number':1915,'multiline':False]
['text':' detach() so that none of the inputs have a ._base attribute.','line_number':1925,'multiline':False]
['text':' Mondo test that tests a combination of:','line_number':1935,'multiline':False]
['text':' input is mutated, that aliases another input (so we make a synthetic base)','line_number':1936,'multiline':False]
['text':' an output is an alias of another output','line_number':1937,'multiline':False]
['text':' an output is an alias of an intermediate','line_number':1938,'multiline':False]
['text':' a and c are aliased','line_number':1939,'multiline':False]
['text':' mutates c','line_number':1941,'multiline':False]
['text':' metadata mutate b','line_number':1942,'multiline':False]
['text':' out1 and out3 are aliases of an intermediate, and alias each other!','line_number':1947,'multiline':False]
['text':' out2 aliases an input, so we don't return it','line_number':1948,'multiline':False]
['text':' Note: in our test, the add() is important because we need the graph inputs to be non-leaves so we can mutate them.','line_number':1954,'multiline':False]
['text':' Expected:','line_number':1964,'multiline':False]
['text':' - 2 inputs in the forward: synthetic_base_a_c, b','line_number':1965,'multiline':False]
['text':' - 1 output in the forward: "tmp"','line_number':1966,'multiline':False]
['text':'   out2 is an alias of an input, and will be generated off of b outside of the compiled fn','line_number':1967,'multiline':False]
['text':'   out1 and out3 are aliases of tmp, that we generate outside of the compiled function','line_number':1968,'multiline':False]
['text':' noqa: B950','line_number':1983,'multiline':False]
['text':' The important bit: the forward fn returns 2 outputs,','line_number':2000,'multiline':False]
['text':' but one of them is a symint so we should only see','line_number':2001,'multiline':False]
['text':' 1 grad_output as an input to the backward graph.','line_number':2002,'multiline':False]
['text':' (Otherwise, autograd will plumb a None as the value of the grad_output,','line_number':2003,'multiline':False]
['text':' which causes inductor to complain).','line_number':2004,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/100224','line_number':2159,'multiline':False]
['text':' This test checks that, just because only the first','line_number':2179,'multiline':False]
['text':' argument did a metadata mutation, we still correctly','line_number':2180,'multiline':False]
['text':' switch to strategy 2 (deduplicate)','line_number':2181,'multiline':False]
['text':' See: https://github.com/pytorch/pytorch/pull/89896#discussion_r1036224447','line_number':2182,'multiline':False]
['text':' noqa: B950','line_number':2195,'multiline':False]
['text':' See Note: Dynamo recompilation guarding invalid grad for why this test exists','line_number':2204,'multiline':False]
['text':' is ok!','line_number':2236,'multiline':False]
['text':' Note This should not raise! Once we have guards in place here,','line_number':2246,'multiline':False]
['text':' we will have this working correctly, as it should recompile.','line_number':2247,'multiline':False]
['text':' noqa: B950','line_number':2252,'multiline':False]
['text':' See Note: Dynamo recompilation guarding invalid grad for why this test exists','line_number':2261,'multiline':False]
['text':' noqa: B950','line_number':2301,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':2343,'multiline':False]
['text':' TODO(whc) make this work (test setup is wrong somehow)','line_number':2385,'multiline':False]
['text':' joint_forward_backward = create_joint_forward_backward(f)','line_number':2386,'multiline':False]
['text':' out = f(inp)','line_number':2387,'multiline':False]
['text':' joint_inputs =  ([inp], [out.detach().contiguous()])','line_number':2388,'multiline':False]
['text':' fx_g = make_fx(joint_forward_backward)(*joint_inputs)','line_number':2389,'multiline':False]
['text':' TODO: assert outputs of fwd graph trace to correct symint','line_number':2390,'multiline':False]
['text':' e2e test that fails without symint clone fix','line_number':2392,'multiline':False]
['text':' noqa: B950','line_number':2578,'multiline':False]
['text':' noqa: B950','line_number':2595,'multiline':False]
['text':' There is no copy_ method','line_number':2618,'multiline':False]
['text':' noqa: B950','line_number':2623,'multiline':False]
['text':' noqa: B950','line_number':2632,'multiline':False]
['text':' No more param lifting','line_number':2661,'multiline':False]
['text':' No more param lifting','line_number':2669,'multiline':False]
['text':' inplace mutation on attr is not allowed','line_number':2690,'multiline':False]
['text':' Some important characteristics of the exported graph below:','line_number':2756,'multiline':False]
['text':' 8 arguments: 2 params from conv, 2 params from batchnorm, 2 buffers from 1 batchnorm, 1 user input','line_number':2757,'multiline':False]
['text':' 9 outputs: 3 mutated buffers (from batchnorm), 2 user outputs and 4 gradients (since there were 4 parameters)','line_number':2758,'multiline':False]
['text':' noqa: B950','line_number':2794,'multiline':False]
['text':' noqa: B950','line_number':2800,'multiline':False]
['text':' noqa: B950','line_number':2801,'multiline':False]
['text':' noqa: B950','line_number':2802,'multiline':False]
['text':' noqa: B950','line_number':2803,'multiline':False]
['text':' Also check the inference graph','line_number':2807,'multiline':False]
['text':' Main important thing here is that there are 5 total outputs: 3 total mutated buffers (from batchnorm), 2 user outputs.','line_number':2808,'multiline':False]
['text':' noqa: B950','line_number':2825,'multiline':False]
['text':' Some important characteristics of the exported graph below:','line_number':2826,'multiline':False]
['text':' 8 arguments: 2 params from conv, 2 params from batchnorm, 2 buffers from 1 batchnorm, 1 user input','line_number':2827,'multiline':False]
['text':' 9 outputs: 2 mutated buffers (from batchnorm), 2 user outputs and 4 gradients (since there were 4 parameters)','line_number':2828,'multiline':False]
['text':' No calling convention changes necessary to invoke the traced graph','line_number':2839,'multiline':False]
['text':' Now test the backward','line_number':2843,'multiline':False]
['text':' Test running the traced backward graph with a mocked-up grad_output','line_number':2858,'multiline':False]
['text':' noqa: B950','line_number':2986,'multiline':False]
['text':' Reference calculation','line_number':3044,'multiline':False]
['text':' Compiled function calculation','line_number':3050,'multiline':False]
['text':' Following module results in inplace ops while tracing. The test checks','line_number':3065,'multiline':False]
['text':' that the meta tensor information is stored for inplace ops.','line_number':3066,'multiline':False]
['text':' make sure we don't do the suboptimal thing of saving the bigger primals input to sum,','line_number':3114,'multiline':False]
['text':' rather than saving the sizes of the primals input for use in backward expand','line_number':3115,'multiline':False]
['text':' tried to test what happens if we save a size tuple in the graph;','line_number':3126,'multiline':False]
['text':' turns out we never will due to how we trace, but this is probably','line_number':3127,'multiline':False]
['text':' still a good test case for various size manipulations','line_number':3128,'multiline':False]
['text':' Try to force symints intermixed with outputs in the function's returns','line_number':3150,'multiline':False]
['text':' this saves 4 new ints for backward. why?','line_number':3157,'multiline':False]
['text':' and what do i have to do to make it save a tensor for backward?','line_number':3158,'multiline':False]
['text':' in the fwd graph, 13 outs because:','line_number':3174,'multiline':False]
['text':' - 5 original outputs (sb is a tuple, gets expanded to 2 symints)','line_number':3175,'multiline':False]
['text':' - 8 saved outputs for backward: 5 tensors, 3 symints','line_number':3176,'multiline':False]
['text':' in the bwd graph, 10 inputs (grad outs) because:','line_number':3178,'multiline':False]
['text':' - The fwd graph had 13 outputs','line_number':3179,'multiline':False]
['text':' - 1 was a view of an input, which gets regenerated outside of the graph','line_number':3180,'multiline':False]
['text':'   and doesn't participate in the backward','line_number':3181,'multiline':False]
['text':' - 2 user outs were symints (b.size()), which don't get tangents in the backward','line_number':3182,'multiline':False]
['text':' fw outputs include b.size() which expands to 2 symints,','line_number':3186,'multiline':False]
['text':'','line_number':3187,'multiline':False]
['text':' TODO(whc)- are the saved-tensors/saved-symints correct here?','line_number':3188,'multiline':False]
['text':' i just made the test pass based on what default partition did','line_number':3189,'multiline':False]
['text':' Of the 5 original forward outputs, the 4th (c) is an input,','line_number':3190,'multiline':False]
['text':' which won't show up in the compiled forward graph','line_number':3191,'multiline':False]
['text':' TODO(whc) we should learn to return torch.Sizes','line_number':3200,'multiline':False]
['text':' Try to force symints intermixed with outputs in the function's returns','line_number':3214,'multiline':False]
['text':' this saves 4 new ints for backward. why?','line_number':3221,'multiline':False]
['text':' and what do i have to do to make it save a tensor for backward?','line_number':3222,'multiline':False]
['text':' fw outputs include b.size() which expands to 2 symints,','line_number':3242,'multiline':False]
['text':' then 4 tensors (transposes of matricies used for mm) are saved','line_number':3243,'multiline':False]
['text':' finally 3 symints are saved','line_number':3244,'multiline':False]
['text':' TODO(whc) we should learn to return torch.Sizes','line_number':3253,'multiline':False]
['text':' Expected forward graph:','line_number':3282,'multiline':False]
['text':' opcode         name       target           args                        kwargs','line_number':3283,'multiline':False]
['text':' -------------  ---------  ---------------  --------------------------  --------','line_number':3284,'multiline':False]
['text':' placeholder    primals_1  primals_1        ()                          {}','line_number':3285,'multiline':False]
['text':' call_function  mul        aten.mul.Tensor  (primals_1, primals_1)      {}','line_number':3286,'multiline':False]
['text':' call_function  mul_1      aten.mul.Tensor  (mul, primals_1)            {}','line_number':3287,'multiline':False]
['text':' output         output     output           ([mul_1, primals_1, mul],)  {}','line_number':3288,'multiline':False]
['text':' Expected backward graph:','line_number':3290,'multiline':False]
['text':' opcode         name        target           args                     kwargs','line_number':3291,'multiline':False]
['text':' -------------  ----------  ---------------  -----------------------  --------','line_number':3292,'multiline':False]
['text':' placeholder    primals_1   primals_1        ()                       {}','line_number':3293,'multiline':False]
['text':' placeholder    mul         mul              ()                       {}','line_number':3294,'multiline':False]
['text':' placeholder    tangents_1  tangents_1       ()                       {}','line_number':3295,'multiline':False]
['text':' call_function  mul_2       aten.mul.Tensor  (tangents_1, mul)        {}','line_number':3296,'multiline':False]
['text':' call_function  mul_3       aten.mul.Tensor  (tangents_1, primals_1)  {}','line_number':3297,'multiline':False]
['text':' call_function  mul_4       aten.mul.Tensor  (mul_3, primals_1)       {}','line_number':3298,'multiline':False]
['text':' call_function  add         aten.add.Tensor  (mul_2, mul_4)           {}','line_number':3299,'multiline':False]
['text':' call_function  add_1       aten.add.Tensor  (add, mul_4)             {}','line_number':3300,'multiline':False]
['text':' output         output      output           ([add_1],)               {}','line_number':3301,'multiline':False]
['text':' Expected forward graph:','line_number':3307,'multiline':False]
['text':' opcode         name       target           args                    kwargs','line_number':3308,'multiline':False]
['text':' -------------  ---------  ---------------  ----------------------  --------','line_number':3309,'multiline':False]
['text':' placeholder    primals_1  primals_1        ()                      {}','line_number':3310,'multiline':False]
['text':' call_function  mul        aten.mul.Tensor  (primals_1, primals_1)  {}','line_number':3311,'multiline':False]
['text':' call_function  mul_1      aten.mul.Tensor  (mul, primals_1)        {}','line_number':3312,'multiline':False]
['text':' output         output     output           ([mul_1, primals_1],)   {}','line_number':3313,'multiline':False]
['text':' Expected backward graph:','line_number':3315,'multiline':False]
['text':' opcode         name        target           args                     kwargs','line_number':3316,'multiline':False]
['text':' -------------  ----------  ---------------  -----------------------  --------','line_number':3317,'multiline':False]
['text':' placeholder    primals_1   primals_1        ()                       {}','line_number':3318,'multiline':False]
['text':' placeholder    tangents_1  tangents_1       ()                       {}','line_number':3319,'multiline':False]
['text':' call_function  mul         aten.mul.Tensor  (primals_1, primals_1)   {} # RECOMPUTED','line_number':3320,'multiline':False]
['text':' call_function  mul_2       aten.mul.Tensor  (tangents_1, mul)        {}','line_number':3321,'multiline':False]
['text':' call_function  mul_3       aten.mul.Tensor  (tangents_1, primals_1)  {}','line_number':3322,'multiline':False]
['text':' call_function  mul_4       aten.mul.Tensor  (mul_3, primals_1)       {}','line_number':3323,'multiline':False]
['text':' call_function  add         aten.add.Tensor  (mul_2, mul_4)           {}','line_number':3324,'multiline':False]
['text':' call_function  add_1       aten.add.Tensor  (add, mul_4)             {}','line_number':3325,'multiline':False]
['text':' output         output      output           ([add_1],)               {}','line_number':3326,'multiline':False]
['text':' The test simulates the condition where transpose followed by view','line_number':3330,'multiline':False]
['text':' happens in the backward pass.','line_number':3331,'multiline':False]
['text':' https://discuss.pytorch.org/t/error-on-transpose-and-view/434','line_number':3332,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/110666','line_number':3355,'multiline':False]
['text':' We expect this to give an inference graph','line_number':3357,'multiline':False]
['text':' Even though x requires grad, we should still get an inference graph','line_number':3365,'multiline':False]
['text':' Ensure that AOT Autograd works with AMP','line_number':3380,'multiline':False]
['text':' Tests to add cases for (non-exhaustive list, mostly for my notes):','line_number':3388,'multiline':False]
['text':' - subclass / mode introduced in the middle of the compiled fn','line_number':3389,'multiline':False]
['text':' - various input mutation / intermediate base tests','line_number':3390,'multiline':False]
['text':' - input mutation that changes a tensor into a subclass','line_number':3391,'multiline':False]
['text':' - metadata mutation? (TBD)','line_number':3392,'multiline':False]
['text':' - guard tests (fw guards *and* bw guards)','line_number':3393,'multiline':False]
['text':' - subclass test involving _indices_of_inps_to_detach','line_number':3394,'multiline':False]
['text':' a is a subclass, b is not','line_number':3396,'multiline':False]
['text':' Output is a TwoTensor (check both inner tensors)','line_number':3424,'multiline':False]
['text':' Both grad_inputs are TwoTensor','line_number':3430,'multiline':False]
['text':' Important pieces of the graph:','line_number':3436,'multiline':False]
['text':' - mul() and div() show up twice, because we called them on a TwoTensor','line_number':3437,'multiline':False]
['text':' - add() shows up once, because we called it on a plain Tensor','line_number':3438,'multiline':False]
['text':' - The user forward() fn returns 1 output (the result of add),','line_number':3439,'multiline':False]
['text':'   while the graph itself returns two outputs (add, add_1)','line_number':3440,'multiline':False]
['text':' - add, add_1 correspond to the two inner dense tensors that will be wrapped','line_number':3441,'multiline':False]
['text':' - into a single TwoTensor output.','line_number':3442,'multiline':False]
['text':' Important pieces of the graph:','line_number':3452,'multiline':False]
['text':' - 4 total dense outputs.','line_number':3453,'multiline':False]
['text':'   This corresponds to the fact that each user fwd inpt (a, b)','line_number':3454,'multiline':False]
['text':'   will get a gradient that is a TwoTensor subclass,','line_number':3455,'multiline':False]
['text':'   so (mul_2, mul_3) will be wrapped into a.grad','line_number':3456,'multiline':False]
['text':'   and (div_1, div_2) will be wrapped into b.grad','line_number':3457,'multiline':False]
['text':' - 4 total dense outputs,','line_number':3458,'multiline':False]
['text':' a is a subclass, b is not','line_number':3468,'multiline':False]
['text':' Output is a TwoTensor (check both inner tensors)','line_number':3493,'multiline':False]
['text':' a is a subclass, b is not','line_number':3498,'multiline':False]
['text':' When creating the joint, we assume that the second grad_out','line_number':3504,'multiline':False]
['text':' is not a subclass.','line_number':3505,'multiline':False]
['text':' In the below test case though, we end up being wrong.','line_number':3506,'multiline':False]
['text':' This would require re-tracing and recompiling the backward.','line_number':3507,'multiline':False]
['text':' First out is a TwoTensor, second is an ordinary tensor','line_number':3528,'multiline':False]
['text':' We compiled our graph assuming type(grad_out[1]) == torch.Tensor,','line_number':3533,'multiline':False]
['text':' but we were wrong: in the below tests, it is a subclass.','line_number':3534,'multiline':False]
['text':' This will eventually require a repartition + recompile','line_number':3535,'multiline':False]
['text':' a is a tensor, b is a TwoTensor','line_number':3543,'multiline':False]
['text':' Both grad_inputs are TwoTensor','line_number':3571,'multiline':False]
['text':' confirm input mutations worked','line_number':3608,'multiline':False]
['text':' NOTE: we need to use b in our gradient compute. Otherwise we will need to recompile teh backward.','line_number':3613,'multiline':False]
['text':' Both grad_inputs are TwoTensor','line_number':3616,'multiline':False]
['text':' NB: Metadata mutation for subclasses is currently broken and disabled','line_number':3622,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/114975','line_number':3623,'multiline':False]
['text':' confirm input mutations worked','line_number':3656,'multiline':False]
['text':' NOTE: we need to use b in our gradient compute. Otherwise we will need to recompile the backward.','line_number':3661,'multiline':False]
['text':' Both grad_inputs are TwoTensor','line_number':3664,'multiline':False]
['text':' NB: Metadata mutation for subclasses is currently broken and disabled','line_number':3670,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/114975','line_number':3671,'multiline':False]
['text':' confirm input mutations worked','line_number':3706,'multiline':False]
['text':' NOTE: we need to use b in our gradient compute. Otherwise we will need to recompile the backward.','line_number':3711,'multiline':False]
['text':' Both grad_inputs are TwoTensor','line_number':3714,'multiline':False]
['text':' confirm input mutations worked','line_number':3753,'multiline':False]
['text':' Both grad_inputs are TwoTensors','line_number':3760,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/105327','line_number':3831,'multiline':False]
['text':' Extracted from unet','line_number':3853,'multiline':False]
['text':' NB: must not require grad','line_number':3865,'multiline':False]
['text':' return a python callable','line_number':3899,'multiline':False]
['text':' Accessing a free variable fake tensor will look like a','line_number':3918,'multiline':False]
['text':' constant to make_fx, and result in the tensor being traced','line_number':3919,'multiline':False]
['text':' into the graph, which is an error condition.  Make sure we','line_number':3920,'multiline':False]
['text':' report adequately in this case.','line_number':3921,'multiline':False]
['text':' entries in here don't work and need to be fixed.','line_number':3930,'multiline':False]
['text':' Each one of these is a bug (or needs to be investigated)','line_number':3931,'multiline':False]
['text':' data-dependent control flow','line_number':3933,'multiline':False]
['text':' flaky','line_number':3945,'multiline':False]
['text':' Given input size: (s0xs1x2). Calculated output size: ...','line_number':3947,'multiline':False]
['text':' UBSAN failure!','line_number':3950,'multiline':False]
['text':' Misc','line_number':3952,'multiline':False]
['text':' RuntimeError: "sum_cpu" not implemented for 'ComplexHalf'','line_number':3956,'multiline':False]
['text':' seems to fail sometimes?','line_number':3959,'multiline':False]
['text':' seems flaky','line_number':3960,'multiline':False]
['text':' flaky','line_number':3961,'multiline':False]
['text':' overrides atol=1e-4, rtol=1e-5 would do as well','line_number':3964,'multiline':False]
['text':' conv2d sometimes nondeterministic in this config?','line_number':3969,'multiline':False]
['text':' aten.masked_select.default','line_number':3974,'multiline':False]
['text':' aten.frexp.Tensor - couldn't find symbolic meta function/decomposition','line_number':3975,'multiline':False]
['text':' aten.i0.default - couldn't find symbolic meta function/decomposition','line_number':3976,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3977,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3978,'multiline':False]
['text':' aten.linalg_eig.default - couldn't find symbolic meta function/decomposition','line_number':3979,'multiline':False]
['text':' aten.linalg_lstsq.default - couldn't find symbolic meta function/decomposition','line_number':3980,'multiline':False]
['text':' aten.linalg_lstsq.default - couldn't find symbolic meta funct...','line_number':3981,'multiline':False]
['text':' aten.linalg_lu_solve.default - couldn't find symbolic meta function/deco...','line_number':3982,'multiline':False]
['text':' '0 is not tracked with proxy for <torch.fx.experimental.proxy_te..','line_number':3983,'multiline':False]
['text':' aten.fill_.Scalar - couldn't find symbolic meta funct...','line_number':3984,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3985,'multiline':False]
['text':' aten._ctc_loss.Tensor - couldn't find symbolic meta function/deco...','line_number':3986,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3987,'multiline':False]
['text':' rand() received an invalid combination of arguments - g...','line_number':3988,'multiline':False]
['text':' rand() received an invalid combination of arguments - g...','line_number':3989,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3990,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3991,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/st...','line_number':3992,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3993,'multiline':False]
['text':' aten.pixel_shuffle.default - couldn't find symbolic meta fun...','line_number':3994,'multiline':False]
['text':' aten.pixel_unshuffle.default - couldn't find symbolic meta...','line_number':3995,'multiline':False]
['text':' aten.segment_reduce.default - couldn't find symbolic meta functio...','line_number':3996,'multiline':False]
['text':' aten.segment_reduce.default - couldn't find symbolic meta functio...','line_number':3997,'multiline':False]
['text':' aten.i0.default - couldn't find symbolic meta function/decomposition','line_number':3998,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':3999,'multiline':False]
['text':' RuntimeError: isIntList() INTERNAL ASSERT FAILED  Expected IntList but got GenericList','line_number':4000,'multiline':False]
['text':' many complex operators incorrect striding, metadata','line_number':4003,'multiline':False]
['text':' Cannot call sizes() on tensor with symbolic sizes/strides','line_number':4019,'multiline':False]
['text':' aot_autograd_check is able to check data specialization by','line_number':4026,'multiline':False]
['text':' randomizing the inputs. Here's a list of ops that really do not','line_number':4027,'multiline':False]
['text':' like random inputs for which we want to disable that.','line_number':4028,'multiline':False]
['text':' Carveout for getitem; I don't want to xfail the entire test','line_number':4049,'multiline':False]
['text':' because that will reject known to be good tests see','line_number':4050,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/94705','line_number':4051,'multiline':False]
['text':' Lazy modules need to see an input first to initialize params.','line_number':4070,'multiline':False]
['text':' PackedSequence is only used for RNNs. It might be possible to fake-ify if they're pytrees but','line_number':4074,'multiline':False]
['text':' torchdynamo already doesn't support RNNs','line_number':4075,'multiline':False]
['text':' torch._subclasses.fake_tensor.DynamicOutputShapeException: aten._ctc_loss.default','line_number':4124,'multiline':False]
['text':' RuntimeError: It appears that you're trying to get value out','line_number':4125,'multiline':False]
['text':' of a tracing tensor with aten._local_scalar_dense.default -','line_number':4126,'multiline':False]
['text':' erroring out! It's likely that this is caused by data-dependent','line_number':4127,'multiline':False]
['text':' control flow or similar.','line_number':4128,'multiline':False]
['text':' AssertionError: The values for attribute 'shape' do not match:','line_number':4129,'multiline':False]
['text':' torch.Size([1]) != torch.Size([]). Outputs of the operator are different in','line_number':4130,'multiline':False]
['text':' eager-mode PyTorch vs AOTAutograd. This means the operator will have incorrect','line_number':4131,'multiline':False]
['text':' output underneath torch.compile. This could be because the operator's','line_number':4132,'multiline':False]
['text':' implementation not traceable or that there is a bug in AOTAutograd.','line_number':4133,'multiline':False]
['text':' DataDependentOutputException: aten.eq compares a mask input','line_number':4134,'multiline':False]
['text':' to a causal mask tensor, to see if Boolean is_causal should be set','line_number':4135,'multiline':False]
['text':' for TrnasformerEncoder layers, MHA and sdp custom kernels','line_number':4136,'multiline':False]
['text':' DataDependentOutputException: aten.equal compares a mask input','line_number':4137,'multiline':False]
['text':' to a causal mask tensor, to see if Boolean is_causal should be set','line_number':4138,'multiline':False]
['text':' for TransformerEncoder layers, MHA and sdp custom kernels','line_number':4139,'multiline':False]
['text':' (this bubbles up to Transformer)','line_number':4140,'multiline':False]
['text':' DataDependentOutputException: aten.equal compares a mask input to a mask producing a bool','line_number':4144,'multiline':False]
['text':' DataDependentOutputException: aten.equal compares a mask input to a mask producing a bool','line_number':4145,'multiline':False]
['text':' NotImplementedError: local_scalar_dense/item NYI for torch.bool','line_number':4146,'multiline':False]
['text':' in native_group_norm_backward cpg, _rem = divmod(C, group)','line_number':4147,'multiline':False]
['text':' TypeError: unsupported operand type(s) for divmod(): 'SymInt' and 'int'','line_number':4148,'multiline':False]
['text':' int() argument must be a string, a bytes-like object or a number, not 'SymFloat'','line_number':4149,'multiline':False]
['text':' int() argument must be a string, a bytes-like object or a number, not 'SymFloat'','line_number':4150,'multiline':False]
['text':' new_size = _infer_size(target.size(), weight.size())','line_number':4151,'multiline':False]
['text':' RuntimeError: expected int at position 0, but got: SymInt','line_number':4152,'multiline':False]
['text':' RuntimeError: Cannot call numel() on tensor with symbolic sizes/strides','line_number':4153,'multiline':False]
['text':' RuntimeError: Cannot call numel() on tensor with symbolic sizes/strides','line_number':4154,'multiline':False]
