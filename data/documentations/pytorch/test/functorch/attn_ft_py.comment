['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':1,'multiline':False]
['text':' All rights reserved.','line_number':2,'multiline':False]
['text':'','line_number':3,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':4,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':5,'multiline':False]
['text':' first run the encoding linear layers for q, k, v normally','line_number':51,'multiline':False]
['text':' the meaning of a linear layer is well understood, so no need to use explicit dimensions','line_number':52,'multiline':False]
['text':' introduce values that represent each dimension. dimensions are 'first class'','line_number':57,'multiline':False]
['text':' because they are actual python values introduced here','line_number':58,'multiline':False]
['text':' bind the positional dimensions in k, q, and v against','line_number':62,'multiline':False]
['text':' our values. the sizes of each dimension are determined by this binding','line_number':63,'multiline':False]
['text':' and when a dimension is used twice (e.g. batch), its size against both','line_number':64,'multiline':False]
['text':' uses is checked for consistency.','line_number':65,'multiline':False]
['text':' The group (heads, features) splits apart a single positional dimension','line_number':66,'multiline':False]
['text':' into two dimensions. Since heads.size*features.size == q.size(2)','line_number':67,'multiline':False]
['text':' and we specified heads.size, features.size is inferred here.','line_number':68,'multiline':False]
['text':' this option allows the model to attend to not just the elements of the current sequence','line_number':74,'multiline':False]
['text':' but the previous elements as well as additional tokens.','line_number':75,'multiline':False]
['text':' cat introduces a new dimension extended_key_sequence, because it is twice as long','line_number':80,'multiline':False]
['text':' as the original key_sequence','line_number':81,'multiline':False]
['text':' for the rest of the function, we will just use extended_key_sequence in lieu of','line_number':84,'multiline':False]
['text':' key_sequence','line_number':85,'multiline':False]
['text':' Take the dot product between "query" and "key" to get the raw attention scores.','line_number':89,'multiline':False]
['text':' The actual outer-product and summation are explicitly represented here,','line_number':90,'multiline':False]
['text':' and like einsum, will be pattern matched to an efficient matrix multiply op.','line_number':91,'multiline':False]
['text':' relative positional embeddings gave a unique embedding based on the distance between','line_number':94,'multiline':False]
['text':' key and value tokens in the sequence, e.g.','line_number':95,'multiline':False]
['text':'  0  1  2  3','line_number':96,'multiline':False]
['text':' -1  0  1  2','line_number':97,'multiline':False]
['text':' -2 -1  0  1','line_number':98,'multiline':False]
['text':' -3 -2 -1  0','line_number':99,'multiline':False]
['text':' the value of a dimension object when used as a tensor is the indices along its dimension','line_number':101,'multiline':False]
['text':' so we can directly subtract the two dimensions to get a 2D tensor of (query_sequence x key_sequence)','line_number':102,'multiline':False]
['text':' with the distance between them','line_number':103,'multiline':False]
['text':' we can then use that as an indirect index into the embedding table values to look up the features for that index','line_number':108,'multiline':False]
['text':' this is just a `gather` primitive op. The resulting tensor will','line_number':109,'multiline':False]
['text':' have all the dimensions of embeddeding_idx (query_sequence x key_sequence),','line_number':110,'multiline':False]
['text':' plus all the dimensions of `embed` that were not indirectly accessed (`embedding_range`).','line_number':111,'multiline':False]
['text':' this form of indirect indexing is more straightforward than either advanced indexing or torch.gather which both','line_number':112,'multiline':False]
['text':' have a lot of dependencies on the positions of indexing tensors.','line_number':113,'multiline':False]
['text':' these were einsum ops in the positional code because they are not easy to fit to existing matmul operators','line_number':118,'multiline':False]
['text':' eventhough they are degenerate matmuls','line_number':119,'multiline':False]
['text':' Normalize the attention scores to probabilities.','line_number':128,'multiline':False]
['text':' # This is actually dropping out entire tokens to attend to, which might','line_number':130,'multiline':False]
['text':' # seem a bit unusual, but is taken from the original Transformer paper.','line_number':131,'multiline':False]
['text':' similarly, we can replace the matmul with a direct listing of the outer product, which makes it clear','line_number':134,'multiline':False]
['text':' we are weighting the values v across all keys with the attention scores.','line_number':135,'multiline':False]
['text':' finally, we convert back to a standard tensor by describing the layout of dimensions.','line_number':138,'multiline':False]
['text':' working in reverse to with_dims, the (heads, features) group flattens the dimensions into a single one.','line_number':139,'multiline':False]
