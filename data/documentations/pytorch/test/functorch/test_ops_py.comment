['text':' Owner(s): ["module: functorch"]','line_number':1,'multiline':False]
['text':' Copyright (c) Facebook, Inc. and its affiliates.','line_number':3,'multiline':False]
['text':' All rights reserved.','line_number':4,'multiline':False]
['text':'','line_number':5,'multiline':False]
['text':' This source code is licensed under the BSD-style license found in the','line_number':6,'multiline':False]
['text':' LICENSE file in the root directory of this source tree.','line_number':7,'multiline':False]
['text':' Version of autograd.grad with some differences:','line_number':56,'multiline':False]
['text':'   - pytree inputs is allowed (but leaves of the pytree have to all','line_number':57,'multiline':False]
['text':'     be tensors)','line_number':58,'multiline':False]
['text':'   - if an input is not used as part of derivatives, we will return a','line_number':59,'multiline':False]
['text':'     zero-filled tensor for the result','line_number':60,'multiline':False]
['text':' Given f, returns an f' such that:','line_number':113,'multiline':False]
['text':' - f' takes only positional arguments','line_number':114,'multiline':False]
['text':' - All arguments to f' are floating-point Tensors','line_number':115,'multiline':False]
['text':' - All outputs of f' are floating-point Tensors','line_number':116,'multiline':False]
['text':' TODO: consolidate with normalize_op_input_output2','line_number':139,'multiline':False]
['text':' returns a new function g(*args, *cotangents)','line_number':200,'multiline':False]
['text':' that computes vjps and (*args, cotangents)','line_number':201,'multiline':False]
['text':' Returns a new function g(*args, *cotangents) that computes vjps and','line_number':225,'multiline':False]
['text':' sample (*args, *cotangents)','line_number':226,'multiline':False]
['text':' We want this higher-order variant of jvp, so that it can','line_number':258,'multiline':False]
['text':' be used to wrap vmap','line_number':259,'multiline':False]
['text':' We want this higher-order variant of jvp, so that it can','line_number':289,'multiline':False]
['text':' be used to wrap vmap','line_number':290,'multiline':False]
['text':' data_ptr composite compliance','line_number':321,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':324,'multiline':False]
['text':' linalg.diagonal is an alias','line_number':333,'multiline':False]
['text':' adjoint is an alias','line_number':337,'multiline':False]
['text':' moveaxis is an alias','line_number':339,'multiline':False]
['text':' 'ravel', is composite implicit autograd and may call clone','line_number':343,'multiline':False]
['text':' swapdims and swapaxes are aliases','line_number':350,'multiline':False]
['text':' 'tensor_split' not composite compliant, see vjp_fail','line_number':367,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':373,'multiline':False]
['text':' RuntimeError: "sum_cpu" not implemented for 'ComplexHalf'','line_number':376,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':377,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':378,'multiline':False]
['text':' Non-contiguous Bugs','line_number':380,'multiline':False]
['text':'','line_number':381,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':382,'multiline':False]
['text':' RuntimeError: !self.requires_grad() || self.is_contiguous()','line_number':387,'multiline':False]
['text':' RuntimeError: Tensor must have a last dimension with stride 1','line_number':390,'multiline':False]
['text':' query: last dimension must be contiguous','line_number':392,'multiline':False]
['text':' Fused attention kernels require last dim to be contiguous','line_number':393,'multiline':False]
['text':' Reduce into single value for grad','line_number':452,'multiline':False]
['text':' Composite ops that do bad things. Need to be fixed in PyTorch core.','line_number':467,'multiline':False]
['text':' RuntimeError: Cannot access data pointer of Tensor that doesn't have storage','line_number':468,'multiline':False]
['text':' BUG: silent incorrectness: runs and produces numerical differences','line_number':471,'multiline':False]
['text':' fails everywhere except on mac','line_number':472,'multiline':False]
['text':' fails everywhere except on windows','line_number':473,'multiline':False]
['text':' fails everywhere except on mac','line_number':474,'multiline':False]
['text':' TODO: fails comparing None to tensor of 0s for saved_mean/var tangents','line_number':475,'multiline':False]
['text':' TODO: fails comparing None to tensor of 0s for saved_mean/var tangents','line_number':476,'multiline':False]
['text':' in-place test errors out with no formula implemented','line_number':482,'multiline':False]
['text':' TODO: https://github.com/pytorch/pytorch/issues/91280','line_number':483,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':485,'multiline':False]
['text':' ROCm: NotImplementedError','line_number':486,'multiline':False]
['text':' ROCm: NotImplementedError','line_number':488,'multiline':False]
['text':' --- Non-Contiguous Failures! ---','line_number':491,'multiline':False]
['text':' This is expected to fail as the operator','line_number':492,'multiline':False]
['text':' expects last dim to have stride=1','line_number':493,'multiline':False]
['text':' BUG','line_number':495,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':496,'multiline':False]
['text':' TODO: get rid of vjp_decomp when we add decomposition support to','line_number':520,'multiline':False]
['text':' PyTorch's forward-mode ad. Currently the decomposition support only','line_number':521,'multiline':False]
['text':' works for functorch.jvp','line_number':522,'multiline':False]
['text':' NB: we used requires_grad=True to determine where the primals are,','line_number':554,'multiline':False]
['text':' but don't need that information otherwise','line_number':555,'multiline':False]
['text':' ---- Non-Contiguous Failures ----','line_number':601,'multiline':False]
['text':' This is expected to fail as the operator','line_number':602,'multiline':False]
['text':' expects last dim to have stride=1','line_number':603,'multiline':False]
['text':' RuntimeError: query: last dimension must be contiguous','line_number':605,'multiline':False]
['text':' The fused attention kernels require the last dim to be contiguous','line_number':606,'multiline':False]
['text':' BUG','line_number':610,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':611,'multiline':False]
['text':' silent incorrectness; Flaky','line_number':681,'multiline':False]
['text':' silent incorrectness; Flaky','line_number':682,'multiline':False]
['text':' Not Implemented','line_number':683,'multiline':False]
['text':' Expected a proper Tensor but got None for argument #1 'other'','line_number':684,'multiline':False]
['text':' sparse tensors have no strides','line_number':685,'multiline':False]
['text':' sparse tensors have no strides','line_number':686,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':690,'multiline':False]
['text':' Mismatched elements: 1 / 15 (6.7%)','line_number':691,'multiline':False]
['text':' Greatest absolute difference: 24.0 at index (2, 4) (up to 1e-05 allowed)','line_number':692,'multiline':False]
['text':' Greatest relative difference: 1.7933241714393998e-06 at index (2, 4) (up to 1.3e-06 allowed)','line_number':693,'multiline':False]
['text':' The failure occurred for item [0]','line_number':694,'multiline':False]
['text':' Compute vjp of vjp','line_number':729,'multiline':False]
['text':' Compute ref_vjp of vjp. We could have done ref_vjp of ref_vjp,','line_number':733,'multiline':False]
['text':' but since we're confident that vjp works by itself, this is','line_number':734,'multiline':False]
['text':' an equivalent way to test that.','line_number':735,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':747,'multiline':False]
['text':' Takes too long','line_number':749,'multiline':False]
['text':' Takes too long','line_number':750,'multiline':False]
['text':' Takes too long','line_number':751,'multiline':False]
['text':' Takes too long','line_number':752,'multiline':False]
['text':' incorrect output','line_number':753,'multiline':False]
['text':' incorrect output','line_number':754,'multiline':False]
['text':' incorrect output','line_number':755,'multiline':False]
['text':' calls random op','line_number':756,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':757,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':758,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':759,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':760,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':761,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':762,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':763,'multiline':False]
['text':' Not composable autograd.Function','line_number':764,'multiline':False]
['text':' It looks like you're either (1) calling .item() on a Tensor or','line_number':765,'multiline':False]
['text':' (2) attempting to use a Tensor in some data-dependent control flow or','line_number':766,'multiline':False]
['text':' (3) encountering this error in PyTorch internals.','line_number':767,'multiline':False]
['text':' works on ROCm','line_number':769,'multiline':False]
['text':' vmap not implemented for at::equal.','line_number':770,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':771,'multiline':False]
['text':' got a batched tensor as input while the running_mean or running_var,','line_number':772,'multiline':False]
['text':' which will be updated in place, were not batched.','line_number':773,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':775,'multiline':False]
['text':' derivate not implemented for _ctc_loss_backward','line_number':776,'multiline':False]
['text':' calls random op','line_number':777,'multiline':False]
['text':' calls random op','line_number':778,'multiline':False]
['text':' calls random op','line_number':779,'multiline':False]
['text':' calls random op','line_number':780,'multiline':False]
['text':' calls random op','line_number':781,'multiline':False]
['text':' calls random op','line_number':782,'multiline':False]
['text':' calls random op','line_number':783,'multiline':False]
['text':' randomness','line_number':784,'multiline':False]
['text':' outputs ints','line_number':785,'multiline':False]
['text':' randomness','line_number':786,'multiline':False]
['text':' It looks like you're either (1) calling .item() on a Tensor or','line_number':787,'multiline':False]
['text':' (2) attempting to use a Tensor in some data-dependent control flow or','line_number':788,'multiline':False]
['text':' (3) encountering this error in PyTorch internals.','line_number':789,'multiline':False]
['text':' got a batched tensor as input while the running_mean or running_var,','line_number':791,'multiline':False]
['text':' which will be updated in place, were not batched.','line_number':792,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':794,'multiline':False]
['text':' RuntimeError: NYI: querying is_contiguous inside of vmap','line_number':795,'multiline':False]
['text':' for memory_format other than torch.contiguous_formats','line_number':796,'multiline':False]
['text':' RuntimeError: NYI: Tensor.clone(memory_format) inside vmap is only','line_number':798,'multiline':False]
['text':' supported with memory_format torch.preserve_format or','line_number':799,'multiline':False]
['text':' torch.contiguous_format (got ChannelsLast)','line_number':800,'multiline':False]
['text':' RuntimeError: NYI: Tensor.clone(memory_format) inside vmap is only','line_number':802,'multiline':False]
['text':' supported with memory_format torch.preserve_format','line_number':803,'multiline':False]
['text':' or torch.contiguous_format (got ChannelsLast)s','line_number':804,'multiline':False]
['text':' RuntimeError: vmap: we do not yet support aten::rrelu_with_noise.','line_number':806,'multiline':False]
['text':' calls random op','line_number':807,'multiline':False]
['text':' calls random op','line_number':808,'multiline':False]
['text':' calls random op','line_number':809,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':810,'multiline':False]
['text':' Batching rule not implemented for `at::equal`','line_number':812,'multiline':False]
['text':' vmap (looks like you are calling item/data-dependent)','line_number':813,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':814,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':815,'multiline':False]
['text':' calls random op','line_number':816,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':817,'multiline':False]
['text':' rank 4 tensor for channels_last','line_number':818,'multiline':False]
['text':' RuntimeError: Tensor must have a last dimension with stride 1','line_number':819,'multiline':False]
['text':' got a batched tensor as input while the running_mean or running_var,','line_number':820,'multiline':False]
['text':' which will be updated in place, were not batched.','line_number':821,'multiline':False]
['text':' view doesn't work on sparse','line_number':823,'multiline':False]
['text':' Since, we test `vjpvjp` independently,','line_number':844,'multiline':False]
['text':' for this test, we just verify that vmap','line_number':845,'multiline':False]
['text':' of `vjpvjp` is correct.','line_number':846,'multiline':False]
['text':' TODO: test in-place','line_number':856,'multiline':False]
['text':' -------------------- ALLOWED FAILURES --------------------------------','line_number':886,'multiline':False]
['text':' The following are not bugs and are expected behavior','line_number':887,'multiline':False]
['text':' Not possible due to dynamic shapes','line_number':888,'multiline':False]
['text':' randomness','line_number':889,'multiline':False]
['text':' randomness','line_number':890,'multiline':False]
['text':' randomness','line_number':891,'multiline':False]
['text':' randomness','line_number':892,'multiline':False]
['text':' randomness','line_number':893,'multiline':False]
['text':' randomness','line_number':894,'multiline':False]
['text':' randomness','line_number':895,'multiline':False]
['text':' randomness','line_number':896,'multiline':False]
['text':' randomness','line_number':897,'multiline':False]
['text':' randomness','line_number':898,'multiline':False]
['text':' randomness','line_number':899,'multiline':False]
['text':' outputs ints','line_number':900,'multiline':False]
['text':' randomness','line_number':901,'multiline':False]
['text':' not possible due to dynamic shapes; we support a subset','line_number':902,'multiline':False]
['text':' random','line_number':903,'multiline':False]
['text':' random','line_number':904,'multiline':False]
['text':' randomness','line_number':905,'multiline':False]
['text':' randomness','line_number':906,'multiline':False]
['text':' non-dense output','line_number':907,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':908,'multiline':False]
['text':' Not composable autograd.Function','line_number':910,'multiline':False]
['text':' ----------------------------------------------------------------------','line_number':911,'multiline':False]
['text':' ---------------------------- BUGS ------------------------------------','line_number':913,'multiline':False]
['text':' All of the following are bugs and need to be fixed','line_number':914,'multiline':False]
['text':' # really annoying thing where it passes correctness check but not has_batch_rule','line_number':915,'multiline':False]
['text':' dynamic error','line_number':918,'multiline':False]
['text':' checks q via a .item() call','line_number':919,'multiline':False]
['text':' checks var for if any value < 0','line_number':920,'multiline':False]
['text':' .item() call','line_number':921,'multiline':False]
['text':' checks q via a .item() call','line_number':922,'multiline':False]
['text':' Tensor must have a last dimension with stride 1','line_number':923,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':925,'multiline':False]
['text':' item call','line_number':934,'multiline':False]
['text':' Batching rule not implemented for aten::_use_cudnn_ctc_loss.Tensor','line_number':936,'multiline':False]
['text':' NYI: querying is_contiguous inside of vmap for memory_format other than torch.contiguous_format','line_number':938,'multiline':False]
['text':' calls as_strided','line_number':944,'multiline':False]
['text':' .item() call','line_number':945,'multiline':False]
['text':' ---------------------------------------------------------------------','line_number':946,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':949,'multiline':False]
['text':' TODO: test in-place','line_number':973,'multiline':False]
['text':' -------------------- ALLOWED FAILURES --------------------------------','line_number':987,'multiline':False]
['text':' The following are expected (not a bug)','line_number':988,'multiline':False]
['text':' randomness','line_number':989,'multiline':False]
['text':' randomness','line_number':990,'multiline':False]
['text':' randomness','line_number':991,'multiline':False]
['text':' randomness','line_number':994,'multiline':False]
['text':' outputs ints','line_number':995,'multiline':False]
['text':' randomness','line_number':996,'multiline':False]
['text':' randomness','line_number':997,'multiline':False]
['text':' Cannot access data pointer of Tensor that doesn't have storage','line_number':1000,'multiline':False]
['text':' Cannot access data pointer of Tensor that doesn't have storage','line_number':1001,'multiline':False]
['text':' Not actually a problem: embedding with max_norm mutates the weight','line_number':1002,'multiline':False]
['text':' and causes different runs to produce different results.','line_number':1003,'multiline':False]
['text':' skip because this is flaky depending on what the max_norm is!','line_number':1004,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1006,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1007,'multiline':False]
['text':' ----------------------------------------------------------------------','line_number':1008,'multiline':False]
['text':' ---------------------------- BUGS ------------------------------------','line_number':1010,'multiline':False]
['text':' The following are bugs that we should fix','line_number':1011,'multiline':False]
['text':' silent incorrectness (nan difference)','line_number':1012,'multiline':False]
['text':' Tensor-likes are not close!','line_number':1013,'multiline':False]
['text':' soft_margin_loss_backward does not support forward-ad','line_number':1015,'multiline':False]
['text':' data_ptr composite compliance','line_number':1016,'multiline':False]
['text':' at::equal batching rule (cpu), also, in-place vmap (cuda)','line_number':1017,'multiline':False]
['text':' Test runner cannot handle this','line_number':1018,'multiline':False]
['text':' requires special handling, and does not yet have a batching rule. Feel free to file a github issue!','line_number':1019,'multiline':False]
['text':' .item or data-dependent control flow','line_number':1021,'multiline':False]
['text':' forward-mode AD does not support at::scatter','line_number':1022,'multiline':False]
['text':' at::equal batching rule (cpu), also, in-place vmap (cuda)','line_number':1023,'multiline':False]
['text':' Tensor must have a last dimension with stride 1','line_number':1024,'multiline':False]
['text':' randomness','line_number':1026,'multiline':False]
['text':' randomness','line_number':1027,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1029,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1030,'multiline':False]
['text':' potential silent incorrectness','line_number':1032,'multiline':False]
['text':' Flaky, seems to sometimes his max_unpool2d','line_number':1033,'multiline':False]
['text':' fails everywhere except on mac','line_number':1034,'multiline':False]
['text':' fails everywhere except on mac','line_number':1035,'multiline':False]
['text':' erroring because running_mean and running_var aren't differentiable','line_number':1037,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':1043,'multiline':False]
['text':' ROCm: NotImplementedError','line_number':1044,'multiline':False]
['text':' ----------------------------------------------------------------------','line_number':1046,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':1049,'multiline':False]
['text':' This is technically a superset of test_vmapjvp. We should either delete test_vmapjvp','line_number':1061,'multiline':False]
['text':' or figure out if we can split vmapjvpall. It's useful to keep test_vmapjvp intact','line_number':1062,'multiline':False]
['text':' because that corresponds to "batched forward-mode AD" testing in PyTorch core','line_number':1063,'multiline':False]
['text':' TODO: test in-place','line_number':1066,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1089,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1090,'multiline':False]
['text':' ???','line_number':1095,'multiline':False]
['text':' conj_physical fallback','line_number':1107,'multiline':False]
['text':' conj_physical fallback','line_number':1108,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1114,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1115,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1116,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1117,'multiline':False]
['text':' trilinear doesn't have batching rule','line_number':1120,'multiline':False]
['text':' hit vmap fallback, which is disabled','line_number':1126,'multiline':False]
['text':' TODO: test in-place','line_number':1131,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1156,'multiline':False]
['text':' Batching rule not implemented for `narrow.Tensor` (and view op)','line_number':1161,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1173,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1174,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1175,'multiline':False]
['text':' aten::scatter_reduce.two hit the vmap fallback','line_number':1176,'multiline':False]
['text':' aten::_unique hit the vmap fallback which is currently disabled','line_number':1228,'multiline':False]
['text':' TODO: test in-place','line_number':1237,'multiline':False]
['text':' vjpvmap testing can't handle randomness','line_number':1260,'multiline':False]
['text':' vjpvmap testing can't handle randomness','line_number':1261,'multiline':False]
['text':' vjpvmap testing can't handle randomness','line_number':1262,'multiline':False]
['text':' randomness','line_number':1263,'multiline':False]
['text':' randomness','line_number':1264,'multiline':False]
['text':' randomness','line_number':1265,'multiline':False]
['text':' outputs ints','line_number':1267,'multiline':False]
['text':' randomness','line_number':1268,'multiline':False]
['text':' randomness','line_number':1269,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1270,'multiline':False]
['text':' non-dense output','line_number':1271,'multiline':False]
['text':' takes too long','line_number':1272,'multiline':False]
['text':' Not composable autograd.Function','line_number':1273,'multiline':False]
['text':' fallback path doesn't work','line_number':1275,'multiline':False]
['text':' All of the following are bugs and need to be fixed','line_number':1276,'multiline':False]
['text':' Batching rule not implemented for `narrow.Tensor` (and view op)','line_number':1282,'multiline':False]
['text':' generator works on cpu, fails on cuda','line_number':1283,'multiline':False]
['text':' generator works on cpu, fails on cuda','line_number':1284,'multiline':False]
['text':' something weird happening with channels_last','line_number':1290,'multiline':False]
['text':' NB: there is no vjpvmap_has_batch_rule test because that is almost','line_number':1306,'multiline':False]
['text':' certainly redundant with the vmap_has_batch_rule test in test_vmap.py','line_number':1307,'multiline':False]
['text':' one-off skip','line_number':1309,'multiline':False]
['text':' If the op doesn't support autograd, vmap(op) won't either','line_number':1314,'multiline':False]
['text':' TODO: test in-place','line_number':1318,'multiline':False]
['text':' instance norm calls batch norm','line_number':1324,'multiline':False]
['text':' For dtype changing operations, the jacobians have different dtype.','line_number':1361,'multiline':False]
['text':' NYI','line_number':1373,'multiline':False]
['text':' RuntimeError: Trying to set a forward gradient that has a different size than that of the original Tensor,','line_number':1374,'multiline':False]
['text':' this is not supported. Tensor is of size [5, 2, 3] while the given forward gradient is of size [1, 2, 3].','line_number':1375,'multiline':False]
['text':' NYI: forward-AD for _cdist_forward','line_number':1377,'multiline':False]
['text':' NYI: forward-AD for cholesky','line_number':1378,'multiline':False]
['text':' NYI: forward-AD for _embedding_bag','line_number':1379,'multiline':False]
['text':' NYI: forward AD for grid_sampler_2d','line_number':1380,'multiline':False]
['text':' NYI: forward AD for grid_sampler_2d','line_number':1381,'multiline':False]
['text':' NYI: forward AD for hardsigmoid_backward','line_number':1382,'multiline':False]
['text':' NYI: forward AD for huber_loss_backward','line_number':1383,'multiline':False]
['text':' not composable','line_number':1384,'multiline':False]
['text':' NYI: forward AD for ormqr','line_number':1385,'multiline':False]
['text':' NYI: multilabel_margin_loss_forward','line_number':1386,'multiline':False]
['text':' NYI: forward-AD for soft_margin_loss_backward','line_number':1387,'multiline':False]
['text':' NYI: forward-AD for _ctc_loss','line_number':1388,'multiline':False]
['text':' NYI: forward-AD with _pdist_forward','line_number':1389,'multiline':False]
['text':' outputs ints','line_number':1391,'multiline':False]
['text':' NYI: forward AD with multi_margin_loss','line_number':1392,'multiline':False]
['text':' flaky, I'm not sure why','line_number':1393,'multiline':False]
['text':' Sparse tensors have no strides','line_number':1394,'multiline':False]
['text':' NYI: forward-AD for _segment_reduce','line_number':1395,'multiline':False]
['text':' Sparse tensors have no strides','line_number':1396,'multiline':False]
['text':' NYI: forward-AD for index_reduce','line_number':1397,'multiline':False]
['text':' NYI: forward-AD for _segment_reduce','line_number':1398,'multiline':False]
['text':' NYI','line_number':1399,'multiline':False]
['text':' TODO: test in-place','line_number':1423,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':1468,'multiline':False]
['text':' Following operators take too long, hence skipped','line_number':1470,'multiline':False]
['text':' Not actually a problem','line_number':1482,'multiline':False]
['text':' not composable','line_number':1483,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1484,'multiline':False]
['text':' Potential bugs/errors','line_number':1486,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':1487,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':1488,'multiline':False]
['text':' AssertionError: Tensor-likes are not close!','line_number':1489,'multiline':False]
['text':' calls random op','line_number':1490,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1491,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1492,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1493,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1494,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1495,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1496,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1497,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1498,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1499,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':1500,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1501,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1502,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1503,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1504,'multiline':False]
['text':' Batching rule not implemented for aten::equal','line_number':1505,'multiline':False]
['text':' RuntimeError: Batch norm got a batched tensor as input while the','line_number':1506,'multiline':False]
['text':' running_mean or running_var, which will be updated in place,','line_number':1507,'multiline':False]
['text':' were not batched.','line_number':1508,'multiline':False]
['text':' ForwardAD not implemented and no decomposition','line_number':1511,'multiline':False]
['text':' calls random op','line_number':1512,'multiline':False]
['text':' calls random op','line_number':1513,'multiline':False]
['text':' calls random op','line_number':1514,'multiline':False]
['text':' randomness','line_number':1515,'multiline':False]
['text':' outputs ints','line_number':1516,'multiline':False]
['text':' randomness','line_number':1517,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1518,'multiline':False]
['text':' calls randomn op','line_number':1519,'multiline':False]
['text':' calls random op','line_number':1520,'multiline':False]
['text':' calls random op','line_number':1521,'multiline':False]
['text':' calls random op','line_number':1522,'multiline':False]
['text':' data depenedant flow','line_number':1523,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1524,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1525,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1526,'multiline':False]
['text':' vmap: inplace into a regular tensor','line_number':1527,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1528,'multiline':False]
['text':' RuntimeError: Batch norm got a batched tensor as input while the','line_number':1529,'multiline':False]
['text':' running_mean or running_var, which will be updated in place,','line_number':1530,'multiline':False]
['text':' were not batched.','line_number':1531,'multiline':False]
['text':' NYI: Tensor.clone(memory_format) inside vmap is only supported with','line_number':1533,'multiline':False]
['text':' memory_format torch.preserve_format or torch.contiguous_format (got ChannelsLast)','line_number':1534,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1537,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1538,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1539,'multiline':False]
['text':' vmap: we do not yet support aten::rrelu_with_noise.','line_number':1540,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1541,'multiline':False]
['text':' calls random op','line_number':1542,'multiline':False]
['text':' calls random op','line_number':1543,'multiline':False]
['text':' calls random op','line_number':1544,'multiline':False]
['text':' Batching rule not implemented for aten::equal','line_number':1545,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1546,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1547,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1548,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':1549,'multiline':False]
['text':' RuntimeError: Sparse CSR tensors do not have strides','line_number':1550,'multiline':False]
['text':' calls random op','line_number':1551,'multiline':False]
['text':' vmap: inplace into regular tensor','line_number':1552,'multiline':False]
['text':' RuntimeError: required rank 4 tensor to use channels_last format','line_number':1553,'multiline':False]
['text':' Forward AD not implemented and no decomposition','line_number':1554,'multiline':False]
['text':' RuntimeError: Tensor must have a last dimension with stride 1','line_number':1555,'multiline':False]
['text':' RuntimeError: Batch norm got a batched tensor as','line_number':1556,'multiline':False]
['text':' input while the running_mean or running_var, which will be updated in','line_number':1557,'multiline':False]
['text':' place, were not batched.','line_number':1558,'multiline':False]
['text':' Since we test `jvpvjp` separately,','line_number':1580,'multiline':False]
['text':' in this we just check that vmap of `jvpvjp`','line_number':1581,'multiline':False]
['text':' is correct.','line_number':1582,'multiline':False]
['text':' TODO: test in-place','line_number':1589,'multiline':False]
['text':' since we're ignoring index 0, at least one element must be non-zero','line_number':1652,'multiline':False]
['text':' must be non-zero since ignore_index may be 0','line_number':1737,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':1777,'multiline':False]
['text':' The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0','line_number':1780,'multiline':False]
['text':' contiguous call','line_number':1782,'multiline':False]
['text':' contiguous call','line_number':1783,'multiline':False]
['text':' dispatch key issue','line_number':1784,'multiline':False]
['text':' outputs ints','line_number':1785,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/96560','line_number':1787,'multiline':False]
['text':' numerical inconsistencies, look like bugs','line_number':1790,'multiline':False]
['text':' fails on linux, passes on windows','line_number':1791,'multiline':False]
['text':' fails on all but mac','line_number':1792,'multiline':False]
['text':' flaky needs investigation','line_number':1793,'multiline':False]
['text':' flaky needs investigation','line_number':1794,'multiline':False]
['text':' flaky needs investigation','line_number':1795,'multiline':False]
['text':' flaky needs investigation','line_number':1796,'multiline':False]
['text':' flaky needs investigation','line_number':1797,'multiline':False]
['text':' fails on windows','line_number':1798,'multiline':False]
['text':' fails on all but windows','line_number':1799,'multiline':False]
['text':' fails on all but windows','line_number':1800,'multiline':False]
['text':' this specializes a lot of code from the get_fallback_and_vmap_exhaustive test. If we need this more','line_number':1868,'multiline':False]
['text':' generally, this could go for a refactor','line_number':1869,'multiline':False]
['text':' we want to check the case where A will be seen as contiguous by jvp but during the vmap calls will become','line_number':1874,'multiline':False]
['text':' non-contiguous because vmap will expand. This will happen during both levels of vmap','line_number':1875,'multiline':False]
['text':' some things in __getitem__ use at::index, which doesn't alias, so this tests a subset of them that do alias','line_number':1920,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':1944,'multiline':False]
['text':' NOTE: [three-transform testing]','line_number':1945,'multiline':False]
['text':' We only test the autograd_function_db tests here.','line_number':1946,'multiline':False]
['text':'','line_number':1947,'multiline':False]
['text':' Usually testing the composition of two transforms is sufficient to convince','line_number':1948,'multiline':False]
['text':' ourselves that an operator is correctly implemented. For the following cases,','line_number':1949,'multiline':False]
['text':' we want to be extra sure, so we send those through some three-transform tests:','line_number':1950,'multiline':False]
['text':' - autograd.Function. The mechanism is via PyDispatcher/HigherOrderOperator, not the','line_number':1951,'multiline':False]
['text':'   regular PyTorch dispatcher, so it's good to exercise more caution.','line_number':1952,'multiline':False]
['text':' Not composable','line_number':1955,'multiline':False]
['text':' strategy: compare vmap(vjp(vmap(op)) vs map(vjp(map(op))','line_number':1987,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':1995,'multiline':False]
['text':' Not composable','line_number':1998,'multiline':False]
['text':' strategy: compare vjp(vmap(vmap(op)) vs vjp(map(map(op))','line_number':2012,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':2032,'multiline':False]
['text':' Not composable','line_number':2035,'multiline':False]
['text':' We're generally convinced that jvp x vmap works (vmap turns an operator','line_number':2058,'multiline':False]
['text':' into another operator and we test jvp support for operators). So','line_number':2059,'multiline':False]
['text':' we only test it on the things we're not sure about:','line_number':2060,'multiline':False]
['text':' - the autograd.Function <> functorch interaction','line_number':2061,'multiline':False]
['text':' Not composable','line_number':2064,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':2088,'multiline':False]
['text':' Not composable','line_number':2091,'multiline':False]
['text':' strategy: compare jvp(vmap(vmap(op)) vs jvp(map(map(op))','line_number':2105,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':2120,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/86798','line_number':2121,'multiline':False]
['text':' Not composable','line_number':2124,'multiline':False]
['text':' strategy: compare vmap(jvp(vmap(op)) vs map(jvp(map(op))','line_number':2147,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':2155,'multiline':False]
['text':' Not composable','line_number':2158,'multiline':False]
['text':' See NOTE: [three-transform testing]','line_number':2183,'multiline':False]
['text':' Not composable','line_number':2186,'multiline':False]
