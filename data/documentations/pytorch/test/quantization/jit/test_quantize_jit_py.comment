['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]
['text':' torch','line_number':3,'multiline':False]
['text':' torch.ao.quantization','line_number':10,'multiline':False]
['text':' torch.ao.quantization.quantize_jit','line_number':30,'multiline':False]
['text':' Testing utils','line_number':40,'multiline':False]
['text':' Annotated models','line_number':61,'multiline':False]
['text':' Standard library','line_number':80,'multiline':False]
['text':' After freezing, weight becomes Constant.','line_number':113,'multiline':False]
['text':' We have this pattern in the original graph: Constant f32_weight -> quant -> dequant','line_number':114,'multiline':False]
['text':' After skipping dequant during Constant Propagation, the resulting graph will be:','line_number':115,'multiline':False]
['text':' Constant int8_weight -> dequant','line_number':116,'multiline':False]
['text':' Test trivial case','line_number':134,'multiline':False]
['text':' Check that the transformation doesn't change numerics','line_number':149,'multiline':False]
['text':' Check that in the original script module's forward we have two','line_number':154,'multiline':False]
['text':' CallMethod nodes. One of them should be for conv.forward and the other','line_number':155,'multiline':False]
['text':' for bn.forward.','line_number':156,'multiline':False]
['text':' Run FoldConvBatchnorm pass.','line_number':161,'multiline':False]
['text':' Check that after the pass one of the CallMethods is gone (supposedly,','line_number':164,'multiline':False]
['text':' the bn.forward).','line_number':165,'multiline':False]
['text':' Check that the transformation doesn't change numerics','line_number':170,'multiline':False]
['text':' Test trivial case','line_number':177,'multiline':False]
['text':' to make sure new bias is not zero','line_number':183,'multiline':False]
['text':' Check that in the original script module's forward we have two','line_number':198,'multiline':False]
['text':' CallMethod nodes. One of them should be for conv.forward and the other','line_number':199,'multiline':False]
['text':' for bn.forward.','line_number':200,'multiline':False]
['text':' Run FoldConvBatchnorm pass.','line_number':205,'multiline':False]
['text':' Check that after the pass one of the CallMethods is gone (supposedly,','line_number':208,'multiline':False]
['text':' the bn.forward).','line_number':209,'multiline':False]
['text':' Check that the transformation doesn't change numerics','line_number':214,'multiline':False]
['text':' Test that we find Conv-BN patterns in submodules','line_number':221,'multiline':False]
['text':' to make sure new bias is not zero','line_number':271,'multiline':False]
['text':' This test case attempt to try combinations of conv2d/conv3d with bias/nobias','line_number':322,'multiline':False]
['text':' as well as BatchNorm with affine/no-affine along with varying the','line_number':323,'multiline':False]
['text':' number of layers.','line_number':324,'multiline':False]
['text':' this only works when default dtype is double','line_number':325,'multiline':False]
['text':' make sure it runs','line_number':419,'multiline':False]
['text':' check matmuls are not fused','line_number':423,'multiline':False]
['text':' check 3d matmul is not fused','line_number':436,'multiline':False]
['text':' make sure it runs','line_number':439,'multiline':False]
['text':' for input and output of conv','line_number':454,'multiline':False]
['text':' for weight','line_number':456,'multiline':False]
['text':' make sure it runs','line_number':528,'multiline':False]
['text':' input and output of sub','line_number':552,'multiline':False]
['text':' not quantized','line_number':554,'multiline':False]
['text':' no observers since we observe in the outer most call site','line_number':556,'multiline':False]
['text':' weight of linear','line_number':558,'multiline':False]
['text':' observer for weight of conv','line_number':611,'multiline':False]
['text':' observer for input of conv and output of relu','line_number':613,'multiline':False]
['text':' observer for input of conv and output of relu','line_number':618,'multiline':False]
['text':' observer for weight of conv','line_number':620,'multiline':False]
['text':' observer for output of relu','line_number':622,'multiline':False]
['text':' 3 for x, y, weight, one for output of each F.conv2d and one for output of add','line_number':682,'multiline':False]
['text':' conv1 and conv2 shares the same type, we need to','line_number':698,'multiline':False]
['text':' make sure we didn't quantize the type twice','line_number':699,'multiline':False]
['text':' input and output of conv','line_number':726,'multiline':False]
['text':' TODO: this is too long, split this to test_insert_observers.py and remove','line_number':740,'multiline':False]
['text':' insrt_observers prefix','line_number':741,'multiline':False]
['text':' we don't want to insert observer for input of self.conv2','line_number':754,'multiline':False]
['text':' because output of self.conv1 is already observed','line_number':755,'multiline':False]
['text':' input and output of conv','line_number':762,'multiline':False]
['text':' we don't want to insert observer for input of self.conv2','line_number':793,'multiline':False]
['text':' because output of self.conv1 is already observed','line_number':794,'multiline':False]
['text':' input and output of conv','line_number':801,'multiline':False]
['text':' reshape','line_number':823,'multiline':False]
['text':' flatten','line_number':826,'multiline':False]
['text':' we want to test that channel_shuffle is going to pass','line_number':851,'multiline':False]
['text':' the observed property from the output of conv1 to input of conv2','line_number':852,'multiline':False]
['text':' so that we don't insert observers for input of conv2','line_number':853,'multiline':False]
['text':' x is already observed','line_number':989,'multiline':False]
['text':' x will be observed in the branch','line_number':1005,'multiline':False]
['text':' since output for both branch are quantized','line_number':1008,'multiline':False]
['text':' the if node is quantized consistently','line_number':1009,'multiline':False]
['text':' make sure the quantized model is executable','line_number':1056,'multiline':False]
['text':' observers for input, output and value between conv1/conv2','line_number':1085,'multiline':False]
['text':' observer for weight','line_number':1089,'multiline':False]
['text':' observer for weight','line_number':1093,'multiline':False]
['text':' check all observers have been removed','line_number':1104,'multiline':False]
['text':' quantize weight','line_number':1122,'multiline':False]
['text':' no quantize node in _conv_forward','line_number':1126,'multiline':False]
['text':' check dequantize is right before CallMethod of conv','line_number':1201,'multiline':False]
['text':' check dequantize is right before add','line_number':1203,'multiline':False]
['text':' TODO: This pass replaces any function called "linear" with "aten::linear"','line_number':1211,'multiline':False]
['text':' No longer necessary, and also quite surprising','line_number':1212,'multiline':False]
['text':' to avoid being frozen','line_number':1255,'multiline':False]
['text':' make sure patterns in both branches are fused','line_number':1277,'multiline':False]
['text':' make sure there is only one quantize_per_tensor for input','line_number':1293,'multiline':False]
['text':' and linear_prepack is folded','line_number':1294,'multiline':False]
['text':' Make sure model save works','line_number':1607,'multiline':False]
['text':' make sure there is only one quantize_per_tensor for input','line_number':1697,'multiline':False]
['text':' and conv2d_prepack is folded','line_number':1698,'multiline':False]
['text':' TODO: remove after refactor of checkGraphModeOp','line_number':1963,'multiline':False]
['text':' we don't check the numerical consistency for add_scalar','line_number':2010,'multiline':False]
['text':' since it's not supported','line_number':2011,'multiline':False]
['text':' TODO: remove after refactor of checkGraphModeOp','line_number':2013,'multiline':False]
['text':' quantized::add_scalar_relu or quantized::add_scalar_relu_out','line_number':2196,'multiline':False]
['text':' TODO: split this after refactor of checkGraphModeOp','line_number':2197,'multiline':False]
['text':' TODO: remove after refactor of checkGraphModeOp','line_number':2385,'multiline':False]
['text':' we don't check the numerical consistency for add_scalar','line_number':2432,'multiline':False]
['text':' since it's not supported','line_number':2433,'multiline':False]
['text':' TODO: remove after refactor of checkGraphModeOp','line_number':2435,'multiline':False]
['text':' quantized::mul_scalar_relu or quantized::mul_scalar_relu_out','line_number':2618,'multiline':False]
['text':' x = x.clamp_(-2, 2)  # Enable when quantized `clamp_` is ready','line_number':2750,'multiline':False]
['text':' add_scalar','line_number':2788,'multiline':False]
['text':' mul_scalar','line_number':2790,'multiline':False]
['text':' add_scalar_out','line_number':2792,'multiline':False]
['text':' mul_scalar_out','line_number':2794,'multiline':False]
['text':' add_scalar_relu','line_number':2796,'multiline':False]
['text':' add_scalar_relu_out','line_number':2799,'multiline':False]
['text':' mul_scalar_relu','line_number':2802,'multiline':False]
['text':' mul_scalar_relu_out','line_number':2805,'multiline':False]
['text':' prim::ListConstruct','line_number':2817,'multiline':False]
['text':' prim::ListUnpack','line_number':2819,'multiline':False]
['text':' prim::TupleConstruct','line_number':2821,'multiline':False]
['text':' prim::TupleUnpack','line_number':2823,'multiline':False]
['text':' This model is not executable since we just put all ops','line_number':2854,'multiline':False]
['text':' in the same forward, therefore we only test scripting','line_number':2855,'multiline':False]
['text':' dummy data to suppress warning','line_number':2858,'multiline':False]
['text':' This checks that the dequantize from the output of first conv','line_number':2868,'multiline':False]
['text':' is being propagated to the end, so that we don't insert extra','line_number':2869,'multiline':False]
['text':' observers and also successfully fused two quantized::conv2d','line_number':2870,'multiline':False]
['text':' patterns','line_number':2871,'multiline':False]
['text':' one quantize_per_tensor for input','line_number':2872,'multiline':False]
['text':' interpolate node will introduce 3 quantize_per_tensor ops','line_number':2924,'multiline':False]
['text':' interpolate node','line_number':2925,'multiline':False]
['text':' interpolate node','line_number':2926,'multiline':False]
['text':' interpolate node','line_number':2927,'multiline':False]
['text':' common node','line_number':2928,'multiline':False]
['text':' common node','line_number':2929,'multiline':False]
['text':' F.sigmoid is deprecated','line_number':2938,'multiline':False]
['text':' F.tanh is deprecated','line_number':2942,'multiline':False]
['text':' This model is not executable since we just put all ops','line_number':2949,'multiline':False]
['text':' in the same forward, therefore we only test scripting','line_number':2950,'multiline':False]
['text':' dummy data to suppress warning','line_number':2953,'multiline':False]
['text':' Checking the model before fianlize contain unfused patterns','line_number':2963,'multiline':False]
['text':' that numerically matches the model after quantize by checking','line_number':2964,'multiline':False]
['text':' number of aten::quantize_per_tensor functions','line_number':2965,'multiline':False]
['text':' conv has 3 quantize_per_tensor for activations and 1 for weight','line_number':2966,'multiline':False]
['text':' and for N general value op between conv we should have','line_number':2967,'multiline':False]
['text':' N + 1 quantize_per_tensor between these ops','line_number':2969,'multiline':False]
['text':' NB: This Needs to be updated when we add more ops to test','line_number':2971,'multiline':False]
['text':' mapping from number of quant for the op to the number of these ops','line_number':2972,'multiline':False]
['text':' for example, for `3` in the key means for this type of op','line_number':2973,'multiline':False]
['text':' we'll have 3 quantize_per_tensor','line_number':2974,'multiline':False]
['text':' for output','line_number':2976,'multiline':False]
['text':' constant propagation removes some prepacks','line_number':2979,'multiline':False]
['text':' This checks that the dequantize from the output of first conv','line_number':2984,'multiline':False]
['text':' is being propagated to the end, so that we don't insert extra','line_number':2985,'multiline':False]
['text':' observers and also successfully fused two quantized::conv2d','line_number':2986,'multiline':False]
['text':' patterns','line_number':2987,'multiline':False]
['text':' one quantize_per_tensor for input','line_number':2988,'multiline':False]
['text':' observer for weight','line_number':3052,'multiline':False]
['text':' for input of FC for dynamic quant','line_number':3059,'multiline':False]
['text':' only quantize child module.','line_number':3085,'multiline':False]
['text':' input of sub for dynamic quant','line_number':3088,'multiline':False]
['text':' not quantized','line_number':3090,'multiline':False]
['text':' no observers since we observe in the outer most call site','line_number':3092,'multiline':False]
['text':' weight of linear','line_number':3094,'multiline':False]
['text':' quantizing activations','line_number':3133,'multiline':False]
['text':' add op is not dynamically quantized.','line_number':3170,'multiline':False]
['text':' Explicitly call forward on model before convert','line_number':3227,'multiline':False]
['text':' Check to make sure weight observers run correctly','line_number':3269,'multiline':False]
['text':' Ensure that attempting to quantize an EmbeddingBag throws an error if','line_number':3490,'multiline':False]
['text':' padding_idx is not None','line_number':3491,'multiline':False]
['text':' eager mode','line_number':3552,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3557,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3558,'multiline':False]
['text':' eager mode','line_number':3588,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3601,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3602,'multiline':False]
['text':' eager mode','line_number':3632,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3637,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3638,'multiline':False]
['text':' Currently only qnnpack is supported','line_number':3665,'multiline':False]
['text':' eager mode','line_number':3666,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3671,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3672,'multiline':False]
['text':' eager mode','line_number':3698,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3701,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3702,'multiline':False]
['text':' Eager mode','line_number':3722,'multiline':False]
['text':' Graph mode','line_number':3725,'multiline':False]
['text':' Copy weights for eager_model','line_number':3727,'multiline':False]
['text':' Eager mode','line_number':3774,'multiline':False]
['text':' Graph mode','line_number':3777,'multiline':False]
['text':' Copy weights for eager_model','line_number':3779,'multiline':False]
['text':' eager mode','line_number':3821,'multiline':False]
['text':' copy the weight from eager mode so that we can','line_number':3824,'multiline':False]
['text':' compare the result of the two quantized models later','line_number':3825,'multiline':False]
['text':' Check to make sure choose_qparams->quant->dequant->linear is numerically','line_number':3843,'multiline':False]
['text':' equivalent to the final quantized model.','line_number':3844,'multiline':False]
['text':' Create weight tensor values that are beyond fp16 max','line_number':3855,'multiline':False]
['text':' compare result with eager mode','line_number':3872,'multiline':False]
