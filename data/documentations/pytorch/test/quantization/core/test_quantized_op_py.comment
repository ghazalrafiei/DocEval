['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]
['text':' Make sure we won't have overflows from vpmaddubsw instruction used in FBGEMM.','line_number':52,'multiline':False]
['text':' On the current Intel x86 architecture, we need to utilize vpmaddubsw instruction','line_number':53,'multiline':False]
['text':' for the 8-bit int multiplication. This instruction vertically multiplies each','line_number':54,'multiline':False]
['text':' unsigned 8-bit integer from a with the corresponding signed 8-bit integer from','line_number':55,'multiline':False]
['text':' b, producing intermediate signed 16-bit integers. This function modifies the','line_number':56,'multiline':False]
['text':' weights to eliminate the overflow on the signed 16-bit integers.','line_number':57,'multiline':False]
['text':' Go through the same loop again to double check we don't have any overflow','line_number':74,'multiline':False]
['text':' Reference quantized Linear operator','line_number':84,'multiline':False]
['text':' Calculate reasonable quantization params','line_number':121,'multiline':False]
['text':' torch.quint8','line_number':132,'multiline':False]
['text':' Retrives the default parameters from X.','line_number':173,'multiline':False]
['text':' Quantizes the reference to account for max error.','line_number':179,'multiline':False]
['text':' q_min and q_max only depend on the initial torch_type.','line_number':180,'multiline':False]
['text':' Retrieves the inplace keyword arguments','line_number':192,'multiline':False]
['text':' some functions require inplace=True to test in-place.','line_number':193,'multiline':False]
['text':' copy.copy is needed because these are modified in place','line_number':194,'multiline':False]
['text':' Quantizes and dequantizes to account for max error.','line_number':200,'multiline':False]
['text':' Adjusts output_scale if needed.','line_number':206,'multiline':False]
['text':' The output_scale determines the quantization scale for functions that','line_number':207,'multiline':False]
['text':' have a constrained output range. e.x. sigmoid ranges from 0 to 1.','line_number':208,'multiline':False]
['text':' Adjusts output_zero_point if needed (see explanation for the','line_number':214,'multiline':False]
['text':' change_zero_point parameter above).','line_number':215,'multiline':False]
['text':' output_zero_point determines the additional offset that will be','line_number':216,'multiline':False]
['text':' added to a scaled value during quantization.','line_number':217,'multiline':False]
['text':' Quantizes the dequantized version of Y_hat.','line_number':223,'multiline':False]
['text':' Finds qY using in-place or non-in-place quantized operators.','line_number':231,'multiline':False]
['text':' TODO: enable after observed output is supported in qnnpack','line_number':312,'multiline':False]
['text':' @override_qengines','line_number':313,'multiline':False]
['text':' issue #107030','line_number':333,'multiline':False]
['text':' torch.nn.functional','line_number':415,'multiline':False]
['text':' calculate ELU(dqX) and quantize','line_number':438,'multiline':False]
['text':' calculate CELU(dqX) and quantize','line_number':464,'multiline':False]
['text':' test regular','line_number':470,'multiline':False]
['text':' 0: num_parameter = num_channels','line_number':507,'multiline':False]
['text':' hypothesis is flaky for this test, create test cases manually','line_number':541,'multiline':False]
['text':' In the FP kernel, mean and variance are calculated in floating point.','line_number':559,'multiline':False]
['text':' In the quantized kernel, they are calculated in integer arithmetic.','line_number':560,'multiline':False]
['text':' Because of this, the numerics do not always match exactly which is','line_number':561,'multiline':False]
['text':' expected and acceptable. We do two things to allow this failure','line_number':562,'multiline':False]
['text':' in this test:','line_number':563,'multiline':False]
['text':' 1. do not use Hypothesis to generate the input tensor.  Hypothesis','line_number':564,'multiline':False]
['text':'    favors homogeneous inputs in its search strategies which isn't','line_number':565,'multiline':False]
['text':'    representative of the inputs we care about, and tends to maximize','line_number':566,'multiline':False]
['text':'    this particular numerics difference.','line_number':567,'multiline':False]
['text':' 2. allow a small % of off by Y_scale errors.  Even when the','line_number':568,'multiline':False]
['text':'    variance of the input is high, there can be off by one errors','line_number':569,'multiline':False]
['text':'    in the result if the input value happens to fall exactly on','line_number':570,'multiline':False]
['text':'    the bin boundary of the output scale.','line_number':571,'multiline':False]
['text':'','line_number':572,'multiline':False]
['text':' If we want the numerics to match we could switch to calculating','line_number':573,'multiline':False]
['text':' mean+var in floating point in the future, at the cost of speed.','line_number':574,'multiline':False]
['text':' Enforce non-homogeneous inputs','line_number':585,'multiline':False]
['text':' Initialize the weights non-randomly for reproducibility, to avoid','line_number':595,'multiline':False]
['text':' flaky tests','line_number':596,'multiline':False]
['text':' Due to the numerics difference mentioned above between calculating','line_number':614,'multiline':False]
['text':' the variance in float vs int, the results can still be slightly','line_number':615,'multiline':False]
['text':' different.','line_number':616,'multiline':False]
['text':' off-by-one errors are magnitude of Y_scale','line_number':621,'multiline':False]
['text':' Note: QNNPACK is tested separately in TestQNNPackOps','line_number':638,'multiline':False]
['text':' Quantize the reference to account for max error.','line_number':648,'multiline':False]
['text':' Note that the output scale has +1, because we use scale of 2.0/2^BITS','line_number':649,'multiline':False]
['text':' in the implementations.','line_number':650,'multiline':False]
['text':' calculate threshold(dqX) and quantize','line_number':674,'multiline':False]
['text':' NB: This is a strange size so that we exercise both the vectorized','line_number':864,'multiline':False]
['text':' implementation (64-element chunks at at time) as well as the scalar','line_number':865,'multiline':False]
['text':' implementation','line_number':866,'multiline':False]
['text':' Add ReLU ground truth','line_number':876,'multiline':False]
['text':' Add + ReLU ground truth','line_number':889,'multiline':False]
['text':' Add ground truth','line_number':925,'multiline':False]
['text':' Add + ReLU ground truth','line_number':932,'multiline':False]
['text':' Add ground truth','line_number':960,'multiline':False]
['text':' Add + ReLU ground truth','line_number':967,'multiline':False]
['text':' NB: This is a strange size so that we exercise both the vectorized','line_number':983,'multiline':False]
['text':' implementation (64-element chunks at at time) as well as the scalar','line_number':984,'multiline':False]
['text':' implementation','line_number':985,'multiline':False]
['text':' Add ground truth','line_number':1001,'multiline':False]
['text':' Add + ReLU ground truth','line_number':1014,'multiline':False]
['text':' mul ReLU ground truth','line_number':1046,'multiline':False]
['text':' mul + ReLU ground truth','line_number':1059,'multiline':False]
['text':' Scalar multiplication','line_number':1074,'multiline':False]
['text':' Scalar multiplication + relu','line_number':1081,'multiline':False]
['text':' mul ground truth','line_number':1112,'multiline':False]
['text':' mul + ReLU ground truth','line_number':1125,'multiline':False]
['text':' matmul ground truth','line_number':1173,'multiline':False]
['text':' Using per channel quantization fails','line_number':1183,'multiline':False]
['text':' 2d softmax over last dim','line_number':1213,'multiline':False]
['text':' >2 dims, softmax along last dim','line_number':1214,'multiline':False]
['text':' >2 dims, softmax along not last dim (requires permute)','line_number':1215,'multiline':False]
['text':' >2 dims, softmax along last dim, but not contiguous','line_number':1216,'multiline':False]
['text':' Channels Last, doesn't require permute','line_number':1217,'multiline':False]
['text':' Channels Last 3D, doesn't require permute','line_number':1218,'multiline':False]
['text':' softmax ground truth','line_number':1238,'multiline':False]
['text':' A = torch.arange(-25, 25, dtype=torch.float)','line_number':1262,'multiline':False]
['text':' B = torch.arange(-25, 25, dtype=torch.float)','line_number':1263,'multiline':False]
['text':' mul ground truth','line_number':1279,'multiline':False]
['text':' ground truth','line_number':1296,'multiline':False]
['text':' quantized','line_number':1300,'multiline':False]
['text':' Check constraints','line_number':1343,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1344,'multiline':False]
['text':' Test the ops.quantized separately, because None is not treated.','line_number':1372,'multiline':False]
['text':' TODO: merge this test with test_max_pool2d when USE_EXPERIMENTAL_CUDNN_V8_API flag is enabled in CI','line_number':1381,'multiline':False]
['text':' cudnn's support for quantized pooling is limited to','line_number':1385,'multiline':False]
['text':' int8 currently','line_number':1386,'multiline':False]
['text':' currently there is no support for dilation for cudnn','line_number':1390,'multiline':False]
['text':' pooling','line_number':1391,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1401,'multiline':False]
['text':' Test the ops.quantized separately, because None is not treated.','line_number':1419,'multiline':False]
['text':' Check constraints','line_number':1438,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1439,'multiline':False]
['text':' Test the ops.quantized separately, because None is not treated.','line_number':1468,'multiline':False]
['text':' Continue with invalid input','line_number':1487,'multiline':False]
['text':' Check constraints for invalid input','line_number':1517,'multiline':False]
['text':' Ensure we hit the vectorized paths','line_number':1561,'multiline':False]
['text':' 176 = 128 + 32 + 16','line_number':1562,'multiline':False]
['text':' 128 hits the interleaved path','line_number':1563,'multiline':False]
['text':' 32 hits the non-interleaved path','line_number':1564,'multiline':False]
['text':' 16 hits the scalar path','line_number':1565,'multiline':False]
['text':' Check constraints','line_number':1568,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1569,'multiline':False]
['text':' Test the ops.quantized separately, because None is not treated.','line_number':1601,'multiline':False]
['text':' Check constraints for invalid input','line_number':1624,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1674,'multiline':False]
['text':' Run reference on float tensor and then quantize the result for comparison','line_number':1684,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1730,'multiline':False]
['text':' Run reference on int_repr + round to avoid double rounding error.','line_number':1743,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1785,'multiline':False]
['text':' Run reference on float tensor and then quantize the result for comparison','line_number':1798,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':1844,'multiline':False]
['text':' Run reference on int_repr + round to avoid double rounding error.','line_number':1859,'multiline':False]
['text':' ndim == 3','line_number':1920,'multiline':False]
['text':' Run reference on int_repr + round to avoid double rounding error.','line_number':1928,'multiline':False]
['text':' Run reference on int_repr + round to avoid double rounding error.','line_number':1997,'multiline':False]
['text':' TODO: torch.cuda.is_available() should be swapped for a flag that checks if cudnn','line_number':2013,'multiline':False]
['text':' is enabled in the build when cudnn supports adaptive average pooling','line_number':2014,'multiline':False]
['text':' ndim == 4','line_number':2067,'multiline':False]
['text':' Run reference on int_repr + round to avoid double rounding error.','line_number':2075,'multiline':False]
['text':' Num elements in the shape','line_number':2100,'multiline':False]
['text':' Side of the tensor generated','line_number':2101,'multiline':False]
['text':' dimension over which to perform topk','line_number':2102,'multiline':False]
['text':' Return largest or smallest element','line_number':2103,'multiline':False]
['text':' Return sorted or not','line_number':2104,'multiline':False]
['text':' Is input in the NHWC format?','line_number':2106,'multiline':False]
['text':' NHWC requires 4 dimensions','line_number':2111,'multiline':False]
['text':' Dimension to find top-k for should exist','line_number':2113,'multiline':False]
['text':' Test the cat on per-channel quantized tensor.','line_number':2179,'multiline':False]
['text':' X is NHWC','line_number':2303,'multiline':False]
['text':' Tile out X so # channels is > 64','line_number':2306,'multiline':False]
['text':' We add a fast path in qcat: when inputs share the same scale and zero_point,','line_number':2311,'multiline':False]
['text':' it will go direct memcpy instead of dequant-cat-quant.','line_number':2312,'multiline':False]
['text':' Here, we quantize and get quantized tensors in NHWC for both dims and strides. The','line_number':2314,'multiline':False]
['text':' permute switches it so that the tensor looks like NCHW but it laid out in memory as','line_number':2315,'multiline':False]
['text':' NHWC.','line_number':2316,'multiline':False]
['text':' using multiple of 4 sizes to satisfy pytorch_q8gavgpool_ukernel_up8xm__sse2() 4-byte alignment demand under ASAN','line_number':2357,'multiline':False]
['text':' hypothesis is flaky for this test, create test cases manually','line_number':2472,'multiline':False]
['text':' minimum rank for channels_last','line_number':2493,'multiline':False]
['text':' In the FP kernel, sums and sums of squares are calculated in floating point.','line_number':2496,'multiline':False]
['text':' In the int8 and uint8 versions of the quantized kernel, they are','line_number':2497,'multiline':False]
['text':' calculated in integer arithmetic (which is exact).','line_number':2498,'multiline':False]
['text':' Because of this, the numerics do not always match exactly which is','line_number':2499,'multiline':False]
['text':' expected and acceptable. We do the following to allow this failure','line_number':2500,'multiline':False]
['text':' in this test:','line_number':2501,'multiline':False]
['text':' 1. do not use Hypothesis to generate the input tensor.  Hypothesis','line_number':2502,'multiline':False]
['text':'    favors homogeneous inputs in its search strategies which isn't','line_number':2503,'multiline':False]
['text':'    representative of the inputs we care about, and tends to maximize','line_number':2504,'multiline':False]
['text':'    this particular numerics difference.','line_number':2505,'multiline':False]
['text':' 2. allow a small % of off by Y_scale errors.  Even when the','line_number':2506,'multiline':False]
['text':'    variance of the input is high, there can be off by one errors','line_number':2507,'multiline':False]
['text':'    in the result if the input value happens to fall exactly on','line_number':2508,'multiline':False]
['text':'    the bin boundary of the output scale.','line_number':2509,'multiline':False]
['text':'','line_number':2510,'multiline':False]
['text':' If we want the numerics to match we could switch to calculating','line_number':2511,'multiline':False]
['text':' mean+var in floating point in the future, at the cost of speed.','line_number':2512,'multiline':False]
['text':' Initialize the weights non-randomly for reproducibility','line_number':2516,'multiline':False]
['text':' Enforce non-homogeneous inputs','line_number':2534,'multiline':False]
['text':' Due to the numerics difference mentioned above between calculating','line_number':2549,'multiline':False]
['text':' the variance in float vs int, the results can still be slightly','line_number':2550,'multiline':False]
['text':' different.','line_number':2551,'multiline':False]
['text':' off-by-one errors are magnitude of Y_scale','line_number':2556,'multiline':False]
['text':' NB: Add just one test case to test overflow, but this case is too slow to run','line_number':2577,'multiline':False]
['text':' internally in @fbcode//mode/dev, the long pole is the 4x calls to torch.sort','line_number':2578,'multiline':False]
['text':' inside torch.unique current implementation','line_number':2579,'multiline':False]
['text':' shape,','line_number':2582,'multiline':False]
['text':' torch_type','line_number':2583,'multiline':False]
['text':' scale','line_number':2584,'multiline':False]
['text':' zero_point','line_number':2585,'multiline':False]
['text':' channels_last','line_number':2586,'multiline':False]
['text':' affine','line_number':2587,'multiline':False]
['text':' required rank 4 tensor to use channels_last format','line_number':2594,'multiline':False]
['text':' In the FP kernel, sums and sums of squares are calculated in floating point.','line_number':2597,'multiline':False]
['text':' In the int8 and uint8 versions of the quantized kernel, they are','line_number':2598,'multiline':False]
['text':' calculated in integer arithmetic (which is exact).','line_number':2599,'multiline':False]
['text':' Because of this, the numerics do not always match exactly which is','line_number':2600,'multiline':False]
['text':' expected and acceptable. We do the following to allow this failure','line_number':2601,'multiline':False]
['text':' in this test:','line_number':2602,'multiline':False]
['text':' 1. do not use Hypothesis to generate the input tensor.  Hypothesis','line_number':2603,'multiline':False]
['text':'    favors homogeneous inputs in its search strategies which isn't','line_number':2604,'multiline':False]
['text':'    representative of the inputs we care about, and tends to maximize','line_number':2605,'multiline':False]
['text':'    this particular numerics difference.','line_number':2606,'multiline':False]
['text':' 2. allow a small % of off by Y_scale errors.  Even when the','line_number':2607,'multiline':False]
['text':'    variance of the input is high, there can be off by one errors','line_number':2608,'multiline':False]
['text':'    in the result if the input value happens to fall exactly on','line_number':2609,'multiline':False]
['text':'    the bin boundary of the output scale.','line_number':2610,'multiline':False]
['text':'','line_number':2611,'multiline':False]
['text':' If we want the numerics to match we could switch to calculating','line_number':2612,'multiline':False]
['text':' mean+var in floating point in the future, at the cost of speed.','line_number':2613,'multiline':False]
['text':' Enforce non-homogeneous inputs','line_number':2634,'multiline':False]
['text':' Due to the numerics difference mentioned above between calculating','line_number':2648,'multiline':False]
['text':' the variance in float vs int, the results can still be slightly','line_number':2649,'multiline':False]
['text':' different.','line_number':2650,'multiline':False]
['text':' off-by-one errors are magnitude of Y_scale','line_number':2655,'multiline':False]
['text':' hypothesis too slow for this test, create test cases manually','line_number':2666,'multiline':False]
['text':' hypothesis too slow for this test, create test cases manually','line_number':2717,'multiline':False]
['text':' upsample_nearest2d','line_number':2768,'multiline':False]
['text':' relu','line_number':2773,'multiline':False]
['text':' tanh','line_number':2778,'multiline':False]
['text':' sigmoid','line_number':2782,'multiline':False]
['text':' interpolate','line_number':2787,'multiline':False]
['text':' avg_pool','line_number':2794,'multiline':False]
['text':' adaptive_avg_pool','line_number':2803,'multiline':False]
['text':' max_pool','line_number':2809,'multiline':False]
['text':' hardtanh','line_number':2817,'multiline':False]
['text':' mul','line_number':2822,'multiline':False]
['text':' add','line_number':2826,'multiline':False]
['text':' conv','line_number':2831,'multiline':False]
['text':' linear','line_number':2843,'multiline':False]
['text':' dynamic linear','line_number':2853,'multiline':False]
['text':' test bias()','line_number':2866,'multiline':False]
['text':' test unpack()','line_number':2868,'multiline':False]
['text':' reference','line_number':2880,'multiline':False]
['text':' single dim, single index','line_number':2883,'multiline':False]
['text':' multiple dim, single index','line_number':2890,'multiline':False]
['text':' single dim, multiple indices','line_number':2897,'multiline':False]
['text':' multiple dim, multiple indices','line_number':2904,'multiline':False]
['text':' This is not supported','line_number':2922,'multiline':False]
['text':' Assume 12dB is sufficient for functional equivalence','line_number':2941,'multiline':False]
['text':' Without the bias, linear performs poorly','line_number':2942,'multiline':False]
['text':' Prepare','line_number':2962,'multiline':False]
['text':' Calibrate','line_number':2969,'multiline':False]
['text':' Quantize','line_number':2973,'multiline':False]
['text':' Trace','line_number':2987,'multiline':False]
['text':' Script','line_number':2990,'multiline':False]
['text':' Must be divisible by the number of heads','line_number':3020,'multiline':False]
['text':' This is not supported','line_number':3024,'multiline':False]
['text':' Q','line_number':3035,'multiline':False]
['text':' K','line_number':3037,'multiline':False]
['text':' V','line_number':3039,'multiline':False]
['text':' Dequantize the data back for reference','line_number':3052,'multiline':False]
['text':' Prepare','line_number':3063,'multiline':False]
['text':' `reduce_range` is False by default for ONEDNN backend','line_number':3065,'multiline':False]
['text':' but the test fails on earlier CPUs without VNNI.','line_number':3066,'multiline':False]
['text':' So we use a default qconfig with `reduce_range=True` here','line_number':3067,'multiline':False]
['text':' Calibrate','line_number':3074,'multiline':False]
['text':' Check the result of the prepare','line_number':3077,'multiline':False]
['text':' Attention','line_number':3078,'multiline':False]
['text':' Weight','line_number':3079,'multiline':False]
['text':' Quantize','line_number':3081,'multiline':False]
['text':' Reference result','line_number':3089,'multiline':False]
['text':' Verify the result is scriptable','line_number':3103,'multiline':False]
['text':' Test the multi-dim input tensor','line_number':3131,'multiline':False]
['text':' W_scale = 1.0','line_number':3144,'multiline':False]
['text':' W_zp = 0','line_number':3145,'multiline':False]
['text':' W_scale, W_zp = _calculate_dynamic_qparams(W_fp32, torch.qint8)','line_number':3182,'multiline':False]
['text':' We currently only check the case where W_scale = 1.0, W_zp = 0.','line_number':3183,'multiline':False]
['text':' Observe X_fp32 and determine X_scale and X_zero_point, this should match','line_number':3202,'multiline':False]
['text':' internals of dynamic linear.','line_number':3203,'multiline':False]
['text':' Weight prepacking operator for dynamic quantized Linear','line_number':3207,'multiline':False]
['text':' Dynamic quantized Linear operator with prepacked weight','line_number':3209,'multiline':False]
['text':' Y_fp32 = qlinear_dynamic(X_fp32, W_prepack, b_fp32)','line_number':3211,'multiline':False]
['text':' Y_fp32_ref = F.linear(X_fp32, W_fp32, b_fp32)','line_number':3214,'multiline':False]
['text':' if use_multi_dim_input:','line_number':3215,'multiline':False]
['text':'     Y_fp32_ref = Y_fp32_ref.view(3, int(batch_size / 3), output_channels)','line_number':3216,'multiline':False]
['text':' Observe X_fp32 and determine X_scale and X_zero_point, this should match','line_number':3280,'multiline':False]
['text':' internals of dynamic linear.','line_number':3281,'multiline':False]
['text':' Quantized Linear operator with prepacked weight','line_number':3287,'multiline':False]
['text':' Y_fp32_ref = F.linear(X_fp32, W_fp32, b_fp32)','line_number':3293,'multiline':False]
['text':' batch_size','line_number':3315,'multiline':False]
['text':' input_channels','line_number':3316,'multiline':False]
['text':' output_channels','line_number':3317,'multiline':False]
['text':' use_bias','line_number':3318,'multiline':False]
['text':' use_relu','line_number':3319,'multiline':False]
['text':' qlinear_dynamic_fp16 uses FP32 activation tensors and FP16 weight tensors','line_number':3335,'multiline':False]
['text':' output is FP32','line_number':3336,'multiline':False]
['text':' For Input (seq_len, batch, input_size)','line_number':3347,'multiline':False]
['text':' For H and C: (num_layers(1) * num_directions, batch, hidden_size)','line_number':3352,'multiline':False]
['text':' We test only for seq length of 1 and num layers of 1 as dynamic quantization occurs multiple times','line_number':3395,'multiline':False]
['text':' within the LSTM op and we do not model the quantization between multiple calls of the linear op within the','line_number':3396,'multiline':False]
['text':' lstm op','line_number':3397,'multiline':False]
['text':' Fp16 quantization is not supported for qnnpack or onednn','line_number':3402,'multiline':False]
['text':' We test only for seq length of 1 and num layers of 1 as dynamic quantization occurs multiple times','line_number':3529,'multiline':False]
['text':' within the LSTM op and we do not model the quantization between multiple calls of the linear op within the','line_number':3530,'multiline':False]
['text':' lstm op','line_number':3531,'multiline':False]
['text':' Fp16 quantization is not supported for qnnpack or onednn','line_number':3536,'multiline':False]
['text':' The goal here is to show that the dynamic op is the same as','line_number':3580,'multiline':False]
['text':' calc params->quantize_input->quantized op->dequantize output','line_number':3581,'multiline':False]
['text':' not supported by QNNPACK','line_number':3584,'multiline':False]
['text':' TODO: fix MakeDeConvOutputShape overflowing for convT3d with qnnpack','line_number':3661,'multiline':False]
['text':' QNNPACK supports uint8 in the kernels. In the op we shift the int8','line_number':3671,'multiline':False]
['text':' weight values to uint8 to be on par with fbgemm. However, this causes','line_number':3672,'multiline':False]
['text':' some rounding issues in rare cases. So, we relax the check to allow','line_number':3673,'multiline':False]
['text':' off by one results.','line_number':3674,'multiline':False]
['text':' only qnnpack qengine supports qint8 when xnnpack is available','line_number':3677,'multiline':False]
['text':' No support for channelwise in xnnpack (int8)','line_number':3682,'multiline':False]
['text':' ONEDNN does not support qint8','line_number':3683,'multiline':False]
['text':' Test the multi-dim input tensor','line_number':3696,'multiline':False]
['text':' xnnpack forces W_zp to 0 when using symmetric quantization','line_number':3708,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':3709,'multiline':False]
['text':' when using symmetric quantization','line_number':3714,'multiline':False]
['text':' special restriction for xnnpack fully connected op weight','line_number':3715,'multiline':False]
['text':' [-127, 127] instead of [-128, 127]','line_number':3716,'multiline':False]
['text':' weight is always int8_t','line_number':3723,'multiline':False]
['text':' Compare X_scale * W_scale * input_channels * X_value_max * W_value_max with','line_number':3765,'multiline':False]
['text':' Y_scale * 255 (max for uint8).','line_number':3766,'multiline':False]
['text':' Weight prepacking operator for quantized Linear','line_number':3769,'multiline':False]
['text':' Quantized Linear operator with prepacked weight','line_number':3774,'multiline':False]
['text':' Test the per-tensor quantization only','line_number':3777,'multiline':False]
['text':' Reference quantized Linear operator','line_number':3778,'multiline':False]
['text':' Assert equal','line_number':3786,'multiline':False]
['text':' Test both per-tensor and per-channel quantization','line_number':3788,'multiline':False]
['text':' Reference quantized result from PyTorch Linear operator','line_number':3789,'multiline':False]
['text':' Assert equal','line_number':3800,'multiline':False]
['text':' No support for channelwise in xnnpack (int8)','line_number':3852,'multiline':False]
['text':' ONEDNN does not support qint8','line_number':3853,'multiline':False]
['text':' Test the multi-dim input tensor','line_number':3864,'multiline':False]
['text':' xnnpack forces W_zp to 0 when using symmetric quantization','line_number':3876,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':3877,'multiline':False]
['text':' when using symmetric quantization','line_number':3882,'multiline':False]
['text':' special restriction for xnnpack fully connected op weight','line_number':3883,'multiline':False]
['text':' [-127, 127] instead of [-128, 127]','line_number':3884,'multiline':False]
['text':' weight is always int8_t','line_number':3891,'multiline':False]
['text':' Compare X_scale * W_scale * input_channels * X_value_max * W_value_max with','line_number':3933,'multiline':False]
['text':' Y_scale * 255 (max for uint8).','line_number':3934,'multiline':False]
['text':' Weight prepacking operator for quantized Linear','line_number':3937,'multiline':False]
['text':' Quantized Linear operator with prepacked weight','line_number':3943,'multiline':False]
['text':' Test both per-tensor and per-channel quantization','line_number':3945,'multiline':False]
['text':' Reference quantized result from PyTorch Linear operator','line_number':3946,'multiline':False]
['text':' in cudnn v. 8.4.0, there is a limitation that input channels','line_number':3957,'multiline':False]
['text':' should be a multiple of 4 for int8 tensors. in cudnn v.8.3.3','line_number':3958,'multiline':False]
['text':' this should be a multiple of 16','line_number':3959,'multiline':False]
['text':' constraints on output channels appear to be relax, as it seems we can use any positive integer here','line_number':3961,'multiline':False]
['text':' except 1. It is not clear why 1 will not work. TODO: check with Yang','line_number':3962,'multiline':False]
['text':' channelwise currently not supported for qlinear cudnn','line_number':3967,'multiline':False]
['text':' TODO: check with yang regarding CUDNN flags','line_number':3973,'multiline':False]
['text':' Weight prepacking operator for quantized Linear','line_number':4038,'multiline':False]
['text':' Quantized Linear operator with prepacked weight','line_number':4041,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':4065,'multiline':False]
['text':' Weight prepacking operator for quantized Linear','line_number':4079,'multiline':False]
['text':' Weight unpack operator for quantized Linear (Used for serialization)','line_number':4081,'multiline':False]
['text':' Assert equal','line_number':4083,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':4110,'multiline':False]
['text':' Weight prepacking operator for quantized Linear','line_number':4114,'multiline':False]
['text':' Make sure we free original tensor by running matrix multiplication in backend.','line_number':4117,'multiline':False]
['text':' At this step, original tensor should be recovered from a data_ptr','line_number':4120,'multiline':False]
['text':' Assert equal','line_number':4122,'multiline':False]
['text':' compute with CPU tensors','line_number':4217,'multiline':False]
['text':' Reference','line_number':4224,'multiline':False]
['text':' Compare results','line_number':4229,'multiline':False]
['text':' torch.quantize_per_channel does not support float16 yet.','line_number':4263,'multiline':False]
['text':' Combine 3D embeddings (e.g. stacked combination of embeddings)','line_number':4266,'multiline':False]
['text':' in a dimension orthogonal to channels.','line_number':4267,'multiline':False]
['text':' Check numerics of prepack function that accepts qtensor as input.','line_number':4273,'multiline':False]
['text':' We use min-max observer to mimic the quantization performed in the original function.','line_number':4274,'multiline':False]
['text':' Get the scale and zero point for the weight tensor','line_number':4277,'multiline':False]
['text':' Quantize the weights to 8bits','line_number':4281,'multiline':False]
['text':' compare against C2 to ensure numerical equivalency.','line_number':4290,'multiline':False]
['text':' C2 quantization needs the memory format of Tensor to be `continuous`, otherwise it will','line_number':4329,'multiline':False]
['text':' throw exceptions. torch.clone() will make the memory format to be `continuous`','line_number':4330,'multiline':False]
['text':' Compare packed weights against C2.','line_number':4334,'multiline':False]
['text':' Compare unpacked weights against C2','line_number':4336,'multiline':False]
['text':' when num_batches = 1, it will create a 2D tensor','line_number':4343,'multiline':False]
['text':' test unsplit weight (memory format is `contiguous`)','line_number':4347,'multiline':False]
['text':' test split weights (memory format is not `contiguous`)','line_number':4350,'multiline':False]
['text':' 4bit and 2bit quantization right now only works for 2D Tensor so we set the num_batches to 1','line_number':4380,'multiline':False]
['text':' 4bit and 2bit quantization right now only works for 2D Tensor so we set the num_batches to 1','line_number':4394,'multiline':False]
['text':' Reference result will be the floating point torch.nn.EmbeddingBag.','line_number':4448,'multiline':False]
['text':' Testing that prune_weight with mapping_table {0} will','line_number':4467,'multiline':False]
['text':' fallback to non sparse embedding look up kernel.','line_number':4468,'multiline':False]
['text':' Prune and generate mapping table','line_number':4471,'multiline':False]
['text':' Test operator that accepts TorchBind packed weights.','line_number':4503,'multiline':False]
['text':' Get the scale and zero point for the weight tensor','line_number':4512,'multiline':False]
['text':' Quantize the weights to 8bits','line_number':4514,'multiline':False]
['text':' Get the scale and zero point for the weight tensor','line_number':4618,'multiline':False]
['text':' Quantize the weights to 8bits','line_number':4621,'multiline':False]
['text':' Test TorchBind based embedding_bag operator','line_number':4682,'multiline':False]
['text':' Get the scale and zero point for the weight tensor','line_number':4685,'multiline':False]
['text':' Quantize the weights to 8bits','line_number':4688,'multiline':False]
['text':' currently transposed conv and per-channel per quantization does not work','line_number':4708,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight and zero output padding','line_number':4710,'multiline':False]
['text':' IC OC/G','line_number':4716,'multiline':False]
['text':' OC IC/G','line_number':4718,'multiline':False]
['text':' Assert equal','line_number':4741,'multiline':False]
['text':' Padded input size should be at least as big as dilated kernel','line_number':4770,'multiline':False]
['text':' Resize W_scale and W_zero_points arrays equal to output_channels','line_number':4780,'multiline':False]
['text':' For testing, we use small values for weights and for activations','line_number':4783,'multiline':False]
['text':' so that no overflow occurs in vpmaddubsw instruction. If the','line_number':4784,'multiline':False]
['text':' overflow occurs in qconv implementation and if there is no','line_number':4785,'multiline':False]
['text':' overflow','line_number':4786,'multiline':False]
['text':' In reference we can't exactly match the results with reference.','line_number':4787,'multiline':False]
['text':' Please see the comment in qconv implementation file','line_number':4788,'multiline':False]
['text':' aten/src/ATen/native/quantized/cpu/qconv.cpp for more details.','line_number':4789,'multiline':False]
['text':' the operator expects them in the format','line_number':4791,'multiline':False]
['text':' (output_channels, input_channels/groups, kernel_d, kernel_h, kernel_w)','line_number':4792,'multiline':False]
['text':' (input_channels, output_channels/groups, kernel_d, kernel_h, kernel_w)','line_number':4793,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':4854,'multiline':False]
['text':' Assign weights','line_number':4865,'multiline':False]
['text':' Quantize reference results for comparison','line_number':4902,'multiline':False]
['text':' quantized conv op without prepacking','line_number':4930,'multiline':False]
['text':' Make sure the results match','line_number':4933,'multiline':False]
['text':' assert_array_almost_equal compares using the following formula:','line_number':4934,'multiline':False]
['text':'     abs(desired-actual) < 1.5 * 10**(-decimal)','line_number':4935,'multiline':False]
['text':' (https://docs.scipy.org/doc/numpy/reference/generated/numpy.testing.assert_almost_equal.html)','line_number':4936,'multiline':False]
['text':' We use decimal = 0 to ignore off-by-1 differences between','line_number':4937,'multiline':False]
['text':' reference and test. Off-by-1 differences arise due to the order of','line_number':4938,'multiline':False]
['text':' round and zero_point addition operation, i.e., if addition','line_number':4939,'multiline':False]
['text':' followed by round is used by reference and round followed by','line_number':4940,'multiline':False]
['text':' addition is used by test, the results may differ by 1.','line_number':4941,'multiline':False]
['text':' For example, the result of round(2.5) + 1 is 3 while','line_number':4942,'multiline':False]
['text':' round(2.5 + 1) is 4 assuming the rounding mode is','line_number':4943,'multiline':False]
['text':' round-to-nearest, ties-to-even.','line_number':4944,'multiline':False]
['text':' Return the quantized data for later reuse','line_number':4951,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':5021,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':5102,'multiline':False]
['text':' TODO: merge this test with test_qconv2d when CUDNN runtime flags becomes available','line_number':5230,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':5233,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':5237,'multiline':False]
['text':' currently padding only supports groups=1','line_number':5239,'multiline':False]
['text':' result for dilation == 2 is not correct','line_number':5246,'multiline':False]
['text':' dilation=st.integers(1, 2),','line_number':5247,'multiline':False]
['text':' currently cudnn has only been verified to work for dilation = 1','line_number':5248,'multiline':False]
['text':' TODO: check backend works for dilation > 1','line_number':5249,'multiline':False]
['text':' TODO: enable channelwise','line_number':5258,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':5316,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':5320,'multiline':False]
['text':' currently padding only supports groups=1','line_number':5322,'multiline':False]
['text':' result for dilation == 2 is not correct','line_number':5329,'multiline':False]
['text':' dilation=st.integers(1, 2),','line_number':5330,'multiline':False]
['text':' currently cudnn has only been verified to work for dilation = 1','line_number':5331,'multiline':False]
['text':' TODO: check backend works for dilation > 1','line_number':5332,'multiline':False]
['text':' TODO: enable channelwise','line_number':5341,'multiline':False]
['text':' profile','line_number':5423,'multiline':False]
['text':' fp32 benchmark','line_number':5435,'multiline':False]
['text':' fp16 benchmark','line_number':5447,'multiline':False]
['text':' Currently only the QNNPACK is supported','line_number':5483,'multiline':False]
['text':' QNNPACK doesn't support these','line_number':5485,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':5533,'multiline':False]
['text':' check that this doesn't error','line_number':5549,'multiline':False]
['text':' Test the module implementation','line_number':5554,'multiline':False]
['text':' QNNPACK doesn't support these','line_number':5630,'multiline':False]
['text':' ONEDNN does not support output paddings','line_number':5631,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':5659,'multiline':False]
['text':' check that this doesn't error','line_number':5675,'multiline':False]
['text':' Test the module implementation','line_number':5680,'multiline':False]
['text':' QNNPACK doesn't support this','line_number':5765,'multiline':False]
['text':' ONEDNN doesn't support output paddings','line_number':5766,'multiline':False]
['text':' check that this doesn't error','line_number':5802,'multiline':False]
['text':' Test the module implementation','line_number':5807,'multiline':False]
['text':' QNNPACK doesn't support channelwise','line_number':5857,'multiline':False]
['text':' Only QNNPACK supports transposed conv','line_number':5859,'multiline':False]
['text':' QNNPACK doesn't support channelwise','line_number':5897,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':5964,'multiline':False]
['text':' Only qnnpack qengine supportes qint8','line_number':6035,'multiline':False]
['text':' TODO: merge this test with test_qconv1d when CUDNN runtime flags becomes available','line_number':6051,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':6054,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':6056,'multiline':False]
['text':' currently padding only supports groups=1','line_number':6058,'multiline':False]
['text':' currently cudnn has only been verified to work for dilation = 1','line_number':6063,'multiline':False]
['text':' TODO: check backend works for dilation > 1','line_number':6064,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6067,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6070,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6073,'multiline':False]
['text':' TODO: enable channelwise','line_number':6076,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':6128,'multiline':False]
['text':' cudnn only supports multiples of 4, but we have explicitly added padding on the backend','line_number':6130,'multiline':False]
['text':' currently padding only supports groups=1','line_number':6132,'multiline':False]
['text':' currently cudnn has only been verified to work for dilation = 1','line_number':6137,'multiline':False]
['text':' TODO: check backend works for dilation > 1','line_number':6138,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6141,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6144,'multiline':False]
['text':' currently conv cudnn backend is only implemented for int8 symmetric','line_number':6147,'multiline':False]
['text':' TODO: enable channelwise','line_number':6150,'multiline':False]
['text':' QNNPACK doesn't support this','line_number':6397,'multiline':False]
['text':' The following should pass when input shape is changed','line_number':6437,'multiline':False]
['text':' The following should pass when input shape is changed','line_number':6460,'multiline':False]
['text':' None, torch.float32, torch.bfloat16','line_number':6487,'multiline':False]
['text':' ONEDNN only supports symmetric quantization of weight','line_number':6491,'multiline':False]
['text':' Assign weights','line_number':6531,'multiline':False]
['text':' Quantize reference results for comparison','line_number':6564,'multiline':False]
['text':' Calculate the result for 2.X path','line_number':6569,'multiline':False]
['text':' Kernel expects pass in reciprocal of scale in fake quant','line_number':6621,'multiline':False]
['text':' Kernel expects pass in reciprocal of scale in fake quant','line_number':6643,'multiline':False]
['text':' Make sure the results match','line_number':6658,'multiline':False]
['text':' assert_array_almost_equal compares using the following formula:','line_number':6659,'multiline':False]
['text':'     abs(desired-actual) < 1.5 * 10**(-decimal)','line_number':6660,'multiline':False]
['text':' (https://docs.scipy.org/doc/numpy/reference/generated/numpy.testing.assert_almost_equal.html)','line_number':6661,'multiline':False]
['text':' We use decimal = 0 to ignore off-by-1 differences between','line_number':6662,'multiline':False]
['text':' reference and test. Off-by-1 differences arise due to the order of','line_number':6663,'multiline':False]
['text':' round and zero_point addition operation, i.e., if addition','line_number':6664,'multiline':False]
['text':' followed by round is used by reference and round followed by','line_number':6665,'multiline':False]
['text':' addition is used by test, the results may differ by 1.','line_number':6666,'multiline':False]
['text':' For example, the result of round(2.5) + 1 is 3 while','line_number':6667,'multiline':False]
['text':' round(2.5 + 1) is 4 assuming the rounding mode is','line_number':6668,'multiline':False]
['text':' round-to-nearest, ties-to-even.','line_number':6669,'multiline':False]
['text':' Return the quantized data for later reuse','line_number':6680,'multiline':False]
['text':' Remove some test combination to reduce UT test time','line_number':6701,'multiline':False]
['text':' Remove some test combination to reduce UT test time','line_number':6760,'multiline':False]
['text':' Remove some test combination to reduce UT test time','line_number':6820,'multiline':False]
['text':' Test qconv with post op relu','line_number':6855,'multiline':False]
['text':' Test qconv with post op hardtanh','line_number':6905,'multiline':False]
['text':' Test qconv with post op add','line_number':6955,'multiline':False]
['text':' Test qconv with post op add relu','line_number':7010,'multiline':False]
['text':' Test qconv with post op add','line_number':7062,'multiline':False]
['text':' Per-Tensor test','line_number':7133,'multiline':False]
['text':' Out variant','line_number':7144,'multiline':False]
['text':' Per-Tensor test','line_number':7158,'multiline':False]
['text':' Out variant','line_number':7169,'multiline':False]
['text':' For 3D, max input size would be 16x16x16','line_number':7175,'multiline':False]
['text':' Per-Tensor test','line_number':7194,'multiline':False]
['text':' Note: In QNNPACK the output scale and zero_point can only be','line_number':7232,'multiline':False]
['text':'       2.0/256, 128 respectively, as it uses a LUT with 256 bins.','line_number':7233,'multiline':False]
['text':' Floating point reference','line_number':7247,'multiline':False]
['text':' Note: In QNNPACK the output scale and zero_point can only be','line_number':7265,'multiline':False]
['text':'       1.0/256, 0 respectively, as it uses a LUT with 256 bins.','line_number':7266,'multiline':False]
['text':' Floating point reference','line_number':7279,'multiline':False]
['text':' Input parameters','line_number':7296,'multiline':False]
['text':' Floating point reference','line_number':7311,'multiline':False]
['text':' Add ground truth','line_number':7366,'multiline':False]
['text':' Add ground truth','line_number':7426,'multiline':False]
['text':' ground truth','line_number':7453,'multiline':False]
['text':' quantized','line_number':7457,'multiline':False]
['text':' 4d','line_number':7470,'multiline':False]
['text':' 5d','line_number':7478,'multiline':False]
['text':' Check constraints','line_number':7501,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':7502,'multiline':False]
['text':' Check constraints','line_number':7564,'multiline':False]
['text':' Kernel cannot be overhanging!','line_number':7565,'multiline':False]
['text':' Quantize Ref Output','line_number':7584,'multiline':False]
['text':' Check constraints','line_number':7613,'multiline':False]
['text':' Quantize Ref Output','line_number':7632,'multiline':False]
['text':' Reversed broadcasting.','line_number':7714,'multiline':False]
['text':' Reversed broadcasting.','line_number':7742,'multiline':False]
