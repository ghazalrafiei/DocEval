['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]
['text':' dtype == torch.quint8','line_number':38,'multiline':False]
['text':' Note we explicitly cast variables to np.float32 in a couple of places to avoid','line_number':72,'multiline':False]
['text':' the default casting in Python often resulting in double precision and to make','line_number':73,'multiline':False]
['text':' sure we're doing the same numerics as C++ code.','line_number':74,'multiline':False]
['text':' [(left, right, loss)] # local optima solution','line_number':81,'multiline':False]
['text':' move left','line_number':86,'multiline':False]
['text':' move right','line_number':90,'multiline':False]
['text':' found a local optima','line_number':96,'multiline':False]
['text':' xmax, xmin','line_number':107,'multiline':False]
['text':' affine transform to put Xq in [0,2**bit_rate - 1]','line_number':112,'multiline':False]
['text':' Xq = (2 ** bit_rate - 1) * (Xq - xmin) / data_range','line_number':113,'multiline':False]
['text':' Manually compute loss instead of using np.linalg.norm to use the same','line_number':126,'multiline':False]
['text':' accumulation order used by C++ code','line_number':127,'multiline':False]
['text':' When the last two dimensions of a 4D tensor are both size 1 or if c == 1, we have a degenerate case','line_number':159,'multiline':False]
['text':' see https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html','line_number':160,'multiline':False]
['text':' In this case, the output of torch.Tensor.to and torch.Tensor.contiguous should not be the same','line_number':161,'multiline':False]
['text':' max number of tensor dimensions','line_number':202,'multiline':False]
['text':' max size for any tensor dimension','line_number':204,'multiline':False]
['text':' slicing and int_repr','line_number':233,'multiline':False]
['text':' dequantize','line_number':239,'multiline':False]
['text':' we can also print a qtensor','line_number':243,'multiline':False]
['text':' to catch edge case when num elements * bit rate < 8, make sure at lease allocate one byte to hold the int repr','line_number':255,'multiline':False]
['text':' Packed one entry looks like 00000011','line_number':265,'multiline':False]
['text':' Packed 4 entries, each of value 3, look like 00110011, 00110011 for torch.qunit4x2, or 11111111 for torch.quint2x4','line_number':269,'multiline':False]
['text':' Packed 5 entries, each of value 3, look like 00110011, 00110011, 00000011 for torch.qunit4x2,','line_number':274,'multiline':False]
['text':' or 11111111, 00000011 for torch.quint2x4','line_number':275,'multiline':False]
['text':' Test tensor creation','line_number':297,'multiline':False]
['text':' Test save/load','line_number':301,'multiline':False]
['text':' scalar assignment verification','line_number':324,'multiline':False]
['text':' 1D tensor assignment verification','line_number':327,'multiline':False]
['text':' 2D tensor assignment verification','line_number':332,'multiline':False]
['text':' 3D tensor assignment verification','line_number':337,'multiline':False]
['text':' 4D tensor assignment verification','line_number':342,'multiline':False]
['text':' non-contiguous case **this should raise an exception**','line_number':347,'multiline':False]
['text':' Scalar Tensor','line_number':352,'multiline':False]
['text':' item','line_number':353,'multiline':False]
['text':' assignment','line_number':363,'multiline':False]
['text':' float assignment','line_number':365,'multiline':False]
['text':' Copying from a float Tensor','line_number':368,'multiline':False]
['text':' Also check 5D tensors work.','line_number':395,'multiline':False]
['text':' legacy constructor/new doesn't support qtensors','line_number':403,'multiline':False]
['text':' create Tensor from uint8_t Tensor, scales and zero_points','line_number':450,'multiline':False]
['text':' create Tensor from uint8_t Tensor, scale and zero_point','line_number':469,'multiline':False]
['text':' create via empty_like','line_number':476,'multiline':False]
['text':' create via empty_like but change the dtype (currently not supported)','line_number':484,'multiline':False]
['text':'  (torch.qint32, torch.float) not supported for quantize_per_channel','line_number':522,'multiline':False]
['text':'  (torch.qint32, torch.float) not supported for quantize_per_channel','line_number':574,'multiline':False]
['text':' Check 4D tensor with 2 different memory formats.','line_number':643,'multiline':False]
['text':' Check 5D tensor.','line_number':653,'multiline':False]
['text':' Reference quantize function with FP zero_point.','line_number':669,'multiline':False]
['text':' Check 4D tensor with 2 different memory formats.','line_number':685,'multiline':False]
['text':' Check 5D tensor.','line_number':695,'multiline':False]
['text':' Check 4D tensor with non-zero axis.','line_number':743,'multiline':False]
['text':' compare transpose + dequantized result with orignal transposed result','line_number':760,'multiline':False]
['text':' compare int representation after transformations','line_number':766,'multiline':False]
['text':' compare dequantized result','line_number':770,'multiline':False]
['text':' compare permuted + dequantized result with original transposed result','line_number':772,'multiline':False]
['text':' make permuted result contiguous','line_number':775,'multiline':False]
['text':' change memory format','line_number':778,'multiline':False]
['text':' permuting larger tensors','line_number':787,'multiline':False]
['text':' should work','line_number':790,'multiline':False]
['text':' we can't reorder the axis','line_number':801,'multiline':False]
['text':' but we can change memory format','line_number':805,'multiline':False]
['text':' storage is not accessible on the cuda right now','line_number':818,'multiline':False]
['text':' Serializing and Deserializing Tensor','line_number':825,'multiline':False]
['text':' quint32, cuda is not supported yet','line_number':837,'multiline':False]
['text':' Serializing and Deserializing Tensor','line_number':843,'multiline':False]
['text':' copy from same scale and zero_point','line_number':855,'multiline':False]
['text':' copying from different scale and zero_point','line_number':864,'multiline':False]
['text':' check original scale and zero_points are set correctly','line_number':869,'multiline':False]
['text':' check scale and zero_points has been copied','line_number':873,'multiline':False]
['text':' can't copy from quantized tensor to non-quantized tensor','line_number':875,'multiline':False]
['text':' copy from float doesn't support cuda','line_number':880,'multiline':False]
['text':' check copy from non-quantized to quantized','line_number':882,'multiline':False]
['text':' cuda is not supported yet','line_number':890,'multiline':False]
['text':' Check to make sure the scale and zero_point has been copied.','line_number':923,'multiline':False]
['text':' positive, negative, overflow','line_number':934,'multiline':False]
['text':' reference tensor for comparing q_filled','line_number':943,'multiline':False]
['text':' Make sure the scale and zero_point don't change','line_number':948,'multiline':False]
['text':' Adapted from test_qtensor_fill_per_tensor but for a NHWC tensor (requires 4D)','line_number':952,'multiline':False]
['text':' positive, negative, overflow','line_number':961,'multiline':False]
['text':' reference tensor for comparing q_filled','line_number':969,'multiline':False]
['text':' Make sure the scale and zero_point don't change','line_number':974,'multiline':False]
['text':' adapted from test_qtensor_fill_per_tensor','line_number':978,'multiline':False]
['text':' adding a constant to avoid too small of a scale','line_number':982,'multiline':False]
['text':' positive, negative, overflow','line_number':989,'multiline':False]
['text':' reference tensor for comparing q_filled','line_number':1000,'multiline':False]
['text':' Make sure the scale and zero_point don't change','line_number':1005,'multiline':False]
['text':' adapted from test_qtensor_fill_per_tensor','line_number':1016,'multiline':False]
['text':' positive, negative, overflow','line_number':1025,'multiline':False]
['text':' mask fill the whole tensor, equivalent to calling plain vanilla fill','line_number':1035,'multiline':False]
['text':' Make sure the scale and zero_point don't change','line_number':1046,'multiline':False]
['text':' the above loop does the same test as test_qtensor_fill','line_number':1050,'multiline':False]
['text':' now we will check masked_fill for subset of indices','line_number':1051,'multiline':False]
['text':' this assignment doesn't end up calling masked_fill, allowing us to compare the different implementations','line_number':1063,'multiline':False]
['text':' for the scalar tensor case, index_put routes to masked_fill','line_number':1084,'multiline':False]
['text':' adapted from test_qtensor_fill_per_channel and test_qtensor_fill_per_tensor_nhwc','line_number':1124,'multiline':False]
['text':' adding a constant to avoid too small of a scale','line_number':1128,'multiline':False]
['text':' positive, negative, overflow','line_number':1135,'multiline':False]
['text':' reference tensor for comparing q_filled','line_number':1146,'multiline':False]
['text':' Make sure the scale and zero_point don't change','line_number':1151,'multiline':False]
['text':' testing -1','line_number':1186,'multiline':False]
['text':' swaps 2nd and 3rd dimension','line_number':1191,'multiline':False]
['text':' does not change tensor layout in memory','line_number':1192,'multiline':False]
['text':' size is the same but the underlying data is different','line_number':1197,'multiline':False]
['text':' torch.equal is not supported for the cuda backend','line_number':1199,'multiline':False]
['text':' a case can't view non-contiguos Tensor','line_number':1203,'multiline':False]
['text':' swaps 2nd and 3rd dimension','line_number':1206,'multiline':False]
['text':' view on contiguous tensor is fine','line_number':1210,'multiline':False]
['text':' Compare original and post-transpose','line_number':1234,'multiline':False]
['text':' swaps 2nd and 3rd dimension','line_number':1237,'multiline':False]
['text':' Change the sizes back to the original','line_number':1238,'multiline':False]
['text':' size is the same but the underlying data is different','line_number':1244,'multiline':False]
['text':' torch.equal is not supported for the cuda backend','line_number':1246,'multiline':False]
['text':' Throws an error if numel is wrong','line_number':1250,'multiline':False]
['text':' resize on both contiguous and non-contiguous tensor should be fine','line_number':1256,'multiline':False]
['text':' testing -1','line_number':1268,'multiline':False]
['text':' swaps 2nd and 3rd dimension','line_number':1273,'multiline':False]
['text':' does not change tensor layout','line_number':1274,'multiline':False]
['text':' torch.equal is not supported for the cuda backend','line_number':1280,'multiline':False]
['text':' we can use reshape for non-contiguous Tensor','line_number':1284,'multiline':False]
['text':' swaps 2nd and 3rd dimension','line_number':1287,'multiline':False]
['text':' Per channel qtensor','line_number':1299,'multiline':False]
['text':' squeeze without dim specified','line_number':1313,'multiline':False]
['text':' Context - https://github.com/pytorch/pytorch/issues/41115','line_number':1363,'multiline':False]
['text':' There's no way to actually pin the memory of a quantized tensor','line_number':1373,'multiline':False]
['text':' range of fp16 value is [-65504, + 65504]','line_number':1387,'multiline':False]
['text':' register the ops','line_number':1464,'multiline':False]
['text':' register the ops','line_number':1482,'multiline':False]
['text':' Now try decomposed pattern','line_number':1526,'multiline':False]
['text':' register the ops','line_number':1539,'multiline':False]
['text':' register the ops','line_number':1557,'multiline':False]
['text':' register the ops','line_number':1575,'multiline':False]
