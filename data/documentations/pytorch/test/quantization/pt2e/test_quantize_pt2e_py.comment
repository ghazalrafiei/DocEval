['text':' Owner(s): ["oncall: quantization"]','line_number':1,'multiline':False]
['text':' noqa: F811','line_number':29,'multiline':False]
['text':' noqa: F811','line_number':32,'multiline':False]
['text':' resetting dynamo cache','line_number':95,'multiline':False]
['text':' program capture','line_number':99,'multiline':False]
['text':' Calibrate','line_number':111,'multiline':False]
['text':' TODO: use OP_TO_ANNOTATOR','line_number':178,'multiline':False]
['text':' two for input of the first conv, one for output for the first conv','line_number':228,'multiline':False]
['text':' TODO: use OP_TO_ANNOTATOR','line_number':247,'multiline':False]
['text':' Ensure the conv has no observer inserted at output','line_number':298,'multiline':False]
['text':' two for input of conv','line_number':300,'multiline':False]
['text':' TODO: use OP_TO_ANNOTATOR','line_number':314,'multiline':False]
['text':' two for input of conv','line_number':384,'multiline':False]
['text':' one for input of maxpool','line_number':385,'multiline':False]
['text':' one for output of maxpool','line_number':386,'multiline':False]
['text':' TODO: use OP_TO_ANNOTATOR','line_number':403,'multiline':False]
['text':' input, weight, bias, output for the conv','line_number':477,'multiline':False]
['text':' note: quantize op for weight and bias are const propagated','line_number':478,'multiline':False]
['text':' input, output for the conv','line_number':578,'multiline':False]
['text':' weight and bias for conv','line_number':585,'multiline':False]
['text':' note: quantize op for weight and bias are const propagated','line_number':586,'multiline':False]
['text':' two for input of the first conv, one for output for the first conv','line_number':668,'multiline':False]
['text':' program capture','line_number':766,'multiline':False]
['text':' make sure the two observers for input are shared','line_number':772,'multiline':False]
['text':' checking that the output observers for the two convs are shared as well','line_number':787,'multiline':False]
['text':' two for input of the first conv, one for output for the first conv','line_number':794,'multiline':False]
['text':' program capture','line_number':822,'multiline':False]
['text':' make sure the two input observers and output are shared','line_number':829,'multiline':False]
['text':' checking that the output observers for the two convs are shared as well','line_number':850,'multiline':False]
['text':' two for input of the first conv, one for output for the first conv','line_number':857,'multiline':False]
['text':' TODO: refactor this to a common util','line_number':901,'multiline':False]
['text':' TODO: refactor this to a common util','line_number':992,'multiline':False]
['text':' TODO: refactor this to a common util','line_number':1086,'multiline':False]
['text':' program capture','line_number':1122,'multiline':False]
['text':' using int32 to simulate int16','line_number':1147,'multiline':False]
['text':' one for input of the first conv, one for output for the first conv','line_number':1185,'multiline':False]
['text':' quantize op for weight node is folded','line_number':1209,'multiline':False]
['text':' quantize op for weight node is folded','line_number':1220,'multiline':False]
['text':' only quantize linear, so add is not quantized and the constant Tensor','line_number':1243,'multiline':False]
['text':' should not be folded','line_number':1244,'multiline':False]
['text':' quantize op for weight node is folded','line_number':1250,'multiline':False]
['text':' transpose op not folded','line_number':1253,'multiline':False]
['text':' quantize op for weight node is folded','line_number':1281,'multiline':False]
['text':' serialization','line_number':1333,'multiline':False]
['text':' deserialization','line_number':1336,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':1437,'multiline':False]
['text':' Compare against short term workflow','line_number':1445,'multiline':False]
['text':' cannot compare against fx quant because of the numerical differences coming','line_number':1446,'multiline':False]
['text':' from quantize and dequantize ops','line_number':1447,'multiline':False]
['text':' Note that dynamic quantization must be applied first here.','line_number':1472,'multiline':False]
['text':' this is because static quantizer also quantizes linear with static qspec','line_number':1473,'multiline':False]
['text':' and if we apply static_quantizer first then dynamic_quantizer cannot be applied','line_number':1474,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':1483,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':1486,'multiline':False]
['text':' Test with 2d inputs','line_number':1502,'multiline':False]
['text':' Had to turn off check against fx because fx quant workflow does not seem','line_number':1507,'multiline':False]
['text':' to propagate observers for permute node for this model.','line_number':1508,'multiline':False]
['text':' Suprisingly it does propagate it for EmbeddingConvLinearModule','line_number':1509,'multiline':False]
['text':' TODO: Figure out the right behavior for propagation','line_number':1510,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':1599,'multiline':False]
['text':' Assert that dropout op exists and is in train mode','line_number':1626,'multiline':False]
['text':' Do the subgraph rewriting','line_number':1635,'multiline':False]
['text':' Assert that dropout op is now replaced with a clone op','line_number':1638,'multiline':False]
['text':' Assert that bn op exists and is in train mode','line_number':1674,'multiline':False]
['text':' Do the subgraph rewriting','line_number':1683,'multiline':False]
['text':' Assert that bn op is now in eval mode','line_number':1686,'multiline':False]
['text':' Before export: this is OK','line_number':1695,'multiline':False]
['text':' After export: this is not OK','line_number':1699,'multiline':False]
['text':' After prepare: still not OK','line_number':1706,'multiline':False]
['text':' After convert: still not OK','line_number':1714,'multiline':False]
['text':' one for weight','line_number':1739,'multiline':False]
['text':' make sure it runs','line_number':1763,'multiline':False]
['text':' just faking a dtype here','line_number':1794,'multiline':False]
['text':' two for input of the first conv, one for output for the first conv','line_number':1851,'multiline':False]
