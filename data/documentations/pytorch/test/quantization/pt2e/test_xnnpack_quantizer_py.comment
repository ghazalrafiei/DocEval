['text':' Owner(s): ["oncall: mobile"]','line_number':1,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':52,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':77,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':80,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':102,'multiline':False]
['text':' Test with 2d inputs','line_number':131,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':136,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':139,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':161,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':164,'multiline':False]
['text':' Test with 2d inputs','line_number':170,'multiline':False]
['text':' Test with 2d inputs','line_number':187,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':192,'multiline':False]
['text':' Test with 2d inputs','line_number':214,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':217,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':220,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':244,'multiline':False]
['text':' quantize_per_channel for weights are const propagated','line_number':247,'multiline':False]
['text':' input and output for the second linear','line_number':295,'multiline':False]
['text':' first linear is not quantized','line_number':300,'multiline':False]
['text':' second linear is quantized','line_number':302,'multiline':False]
['text':' input and output for the second linear','line_number':338,'multiline':False]
['text':' first linear is not quantized','line_number':343,'multiline':False]
['text':' second linear is quantized','line_number':345,'multiline':False]
['text':' We only want to annotate Linear type','line_number':398,'multiline':False]
['text':' input and output for the linear','line_number':403,'multiline':False]
['text':' only the linear is quantized','line_number':408,'multiline':False]
['text':' program capture','line_number':424,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':444,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':451,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':470,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':473,'multiline':False]
['text':' Test with 2d inputs','line_number':490,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':516,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':519,'multiline':False]
['text':' Test with 2d inputs','line_number':529,'multiline':False]
['text':' input and output are using quantize_per_tensor and weight is using quantize_per_channel','line_number':553,'multiline':False]
['text':' note: quantize op for weights are const propagated','line_number':556,'multiline':False]
['text':' Test with 2d inputs','line_number':572,'multiline':False]
['text':' input_tensor','line_number':611,'multiline':False]
['text':' hidden_tensor','line_number':615,'multiline':False]
['text':' (D * num_layers, N, H_out)','line_number':616,'multiline':False]
['text':' input_tensor','line_number':673,'multiline':False]
['text':' hidden_tensor','line_number':677,'multiline':False]
['text':' (D * num_layers, N, H_out)','line_number':678,'multiline':False]
['text':' two input and one output for first add, and output for second add','line_number':721,'multiline':False]
['text':' two input and one output for first add, and output for second add','line_number':751,'multiline':False]
['text':' two input and one output for first add, and output for second add','line_number':778,'multiline':False]
['text':' not quantized','line_number':814,'multiline':False]
['text':' not quantized','line_number':845,'multiline':False]
['text':' TODO: express this using self._test_quantizer, add test for inception_v4','line_number':863,'multiline':False]
['text':' program capture','line_number':874,'multiline':False]
['text':' checking that we inserted observers correctly for maxpool operator (input and','line_number':884,'multiline':False]
['text':' output share observer instance)','line_number':885,'multiline':False]
['text':' comparing with existing fx graph mode quantization reference flow','line_number':894,'multiline':False]
['text':' the result matches exactly after prepare','line_number':906,'multiline':False]
['text':' Note: this currently will always be true since we are inserting observers','line_number':907,'multiline':False]
['text':' the check becomes useful when we add qat examples','line_number':908,'multiline':False]
['text':' but we can still manully inspect the printed observers to make sure','line_number':909,'multiline':False]
['text':' it matches','line_number':910,'multiline':False]
['text':' there are slight differences after convert due to different implementations','line_number':916,'multiline':False]
['text':' of quant/dequant','line_number':917,'multiline':False]
