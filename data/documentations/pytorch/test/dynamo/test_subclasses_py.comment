['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' Wrapper subclass with two inner tensors: data and scale','line_number':81,'multiline':False]
['text':' data has same shape as outer, and scale has single dim size','line_number':82,'multiline':False]
['text':' Returns True if the function recompiles between inputs1 and inputs2 with the','line_number':145,'multiline':False]
['text':' specified dynamic setting.','line_number':146,'multiline':False]
['text':' Should reset to the outer state (disabled) after exiting ctx manager','line_number':191,'multiline':False]
['text':' When inputs' DimDynamic is DYNAMIC or DUCK, the inputs','line_number':496,'multiline':False]
['text':' to opt_f will be tensors with SymInt sizes. Dynamo will treat input','line_number':497,'multiline':False]
['text':' as dynamic automatically and will only compile once','line_number':498,'multiline':False]
['text':' Recompile once, first with dim 0 and 1 become Dynamic','line_number':505,'multiline':False]
['text':' Recompile 2 times, first with dim 1 become Dynamic, second with dim 0 becomes Dynamic.','line_number':507,'multiline':False]
['text':' Recompile 2 times, first with dim 0 become Dynamic, second with dim 1 becomes Dynamic.','line_number':509,'multiline':False]
['text':' Cannot re-use the version from AOTAutograd, since that uses python functional tensors.','line_number':553,'multiline':False]
['text':' frame count and op count are incremented due to re-compilation','line_number':638,'multiline':False]
['text':' frame count and op count are incremented due to re-compilation','line_number':649,'multiline':False]
['text':' Holds an inner tensor, that has a distinct shape from the outer wrapper tensor.','line_number':686,'multiline':False]
['text':' Also adds additional guards on the inner tensor's sizes.','line_number':687,'multiline':False]
['text':' When the first input to an op has x.shape[0] > 5, we insert an extra add node.','line_number':688,'multiline':False]
['text':' Double the outer-most dimension','line_number':692,'multiline':False]
['text':' TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.','line_number':695,'multiline':False]
['text':' Calling the overload that has kwargs causes us to go down the first overload path,','line_number':696,'multiline':False]
['text':' which will **always** specialize sizes.','line_number':697,'multiline':False]
['text':' We should probably eventually fix this so that the first overload can just handle dynamic shapes.','line_number':698,'multiline':False]
['text':' Add guards on the  inner tensor's sizes','line_number':734,'multiline':False]
['text':' Grab info on sources and guards from the shapeenv','line_number':750,'multiline':False]
['text':' During fakeifying, we end up allocating a separate symint','line_number':779,'multiline':False]
['text':' for the outer and inner tensor (in this test, s0 is unused).','line_number':780,'multiline':False]
['text':' lower bound comes from code underneath torch_dispatch  (operating on the inner tensor size)','line_number':789,'multiline':False]
['text':' upper bound comes from user code (operating on the wrapper size)','line_number':791,'multiline':False]
['text':' shouldn't recompile for different sizes when dynamic=True','line_number':799,'multiline':False]
['text':' should recompile for different data size when dynamic=False','line_number':804,'multiline':False]
['text':' avoid recompile using manual mark_dynamic() for different data size','line_number':809,'multiline':False]
['text':' NB: mark_dynamic() on outer tensor should translate to inner tensors of the same size','line_number':811,'multiline':False]
['text':' should recompile for different scale size when dynamic=False','line_number':818,'multiline':False]
['text':' still recompiles using manual mark_dynamic() on outer for different scale size','line_number':823,'multiline':False]
['text':' NB: mark_dynamic() on outer tensor doesn't translate to inner tensors of different size','line_number':825,'multiline':False]
['text':' s0 is specialized and guarded in outter shape_env when dynamo checks the guards','line_number':888,'multiline':False]
['text':' s0 is specialized and guarded in outter shape_env when dynamo checks the guards','line_number':910,'multiline':False]
['text':' noqa: B024','line_number':935,'multiline':False]
['text':' Makes a jagged tensor with N constituent tensors with size','line_number':952,'multiline':False]
['text':' as specified ((S0, S1, S2), D)','line_number':953,'multiline':False]
['text':' Makes a jagged tensor with N constituent tensors with size','line_number':963,'multiline':False]
['text':' as specified ((S0, S1, S2), D)','line_number':964,'multiline':False]
['text':' NB: If we have shape e.g. (3, j0, 3), duck sizing will give us (s0, s1, s0).','line_number':991,'multiline':False]
['text':' This causes a recompile later on when it realizes the batch and last dim','line_number':992,'multiline':False]
['text':' should not always be equal. To avoid that, we use (3, j0, 5) here.','line_number':993,'multiline':False]
['text':' Binary recompiles because singleton ints no longer match','line_number':1007,'multiline':False]
['text':' TODO: cannot parametrize this test class with device for some reason','line_number':1013,'multiline':False]
['text':' NB: If we have shape e.g. (3, j0, 3), duck sizing will give us (s0, s1, s0).','line_number':1045,'multiline':False]
['text':' This causes a recompile later on when it realizes the batch and last dim','line_number':1046,'multiline':False]
['text':' should not always be equal. To avoid that, we use (3, j0, 5) here.','line_number':1047,'multiline':False]
['text':' correctness','line_number':1060,'multiline':False]
['text':' We specialize on the length of offsets, e.g. (1) we recompile if the','line_number':1065,'multiline':False]
['text':' length of the offsets is different. (2) we don't recompile if the','line_number':1066,'multiline':False]
['text':' length of the offsets is the same, even if the size of the constituent','line_number':1067,'multiline':False]
['text':' tensors are different.','line_number':1068,'multiline':False]
['text':' There are three cases to consider here based on the logic in','line_number':1073,'multiline':False]
['text':' meta_utils.py','line_number':1074,'multiline':False]
['text':'','line_number':1075,'multiline':False]
['text':' (1) basic case:','line_number':1076,'multiline':False]
['text':' view is not a leaf and has the same requires grad as its basic case','line_number':1077,'multiline':False]
['text':' (2) leaf view case:','line_number':1082,'multiline':False]
['text':' the view has to be a leaf (w/ requires_grad True or requires_grad False)','line_number':1083,'multiline':False]
['text':' base w/ requires_grad True or requires_grad False','line_number':1084,'multiline':False]
['text':' The issue is this doesn't quite work','line_number':1093,'multiline':False]
['text':' (3) obscure case:','line_number':1097,'multiline':False]
['text':' view is not a leaf (implies requires_grad True)','line_number':1098,'multiline':False]
['text':' base w/ requires_grad False)','line_number':1099,'multiline':False]
['text':' intermediate leaf view','line_number':1101,'multiline':False]
['text':' Check metadata and values are correct','line_number':1121,'multiline':False]
['text':' Check that no guards are incurred','line_number':1126,'multiline':False]
