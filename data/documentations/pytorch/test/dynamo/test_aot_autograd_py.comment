['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' https://github.com/pytorch/torchdynamo/issues/1147','line_number':36,'multiline':False]
['text':' https://github.com/pytorch/torchdynamo/issues/1301','line_number':63,'multiline':False]
['text':' This should not error: we mutated an autograd leaf under no_grad mode.','line_number':76,'multiline':False]
['text':' Init mod','line_number':155,'multiline':False]
['text':' Run it for real','line_number':159,'multiline':False]
['text':' Run it in export','line_number':162,'multiline':False]
['text':' Run exported graph with AOT','line_number':165,'multiline':False]
['text':' Init mod','line_number':185,'multiline':False]
['text':' Run it for real','line_number':190,'multiline':False]
['text':' Run it in export','line_number':193,'multiline':False]
['text':' Assert equal','line_number':196,'multiline':False]
['text':' Run exported graph with AOT','line_number':199,'multiline':False]
['text':' This should not error: we mutated an autograd leaf under no_grad mode.','line_number':201,'multiline':False]
['text':' Init mod','line_number':217,'multiline':False]
['text':' Run it for real','line_number':222,'multiline':False]
['text':' Run it through optimize, with our capturing fn','line_number':225,'multiline':False]
['text':' Assert equal','line_number':237,'multiline':False]
['text':' Uncomment to reproduce commented out graphs below.','line_number':240,'multiline':False]
['text':' for gm in gms:','line_number':241,'multiline':False]
['text':'     print("GM CODE", gm.code)','line_number':242,'multiline':False]
['text':' Graph 1','line_number':246,'multiline':False]
['text':' def forward(self, x : torch.nn.parameter.Parameter, y : torch.Tensor):','line_number':247,'multiline':False]
['text':'     mul = x * y;  x = y = None','line_number':248,'multiline':False]
['text':'     return (mul,)','line_number':249,'multiline':False]
['text':' BREAK','line_number':250,'multiline':False]
['text':' Graph 2','line_number':251,'multiline':False]
['text':' def forward(self, y : torch.Tensor):','line_number':252,'multiline':False]
['text':'     getitem = y[0];  y = None','line_number':253,'multiline':False]
['text':'     getitem_1 = getitem[0];  getitem = None','line_number':254,'multiline':False]
['text':'     lt = getitem_1 < 3;  getitem_1 = None','line_number':255,'multiline':False]
['text':'     return (lt,)','line_number':256,'multiline':False]
['text':' BREAK','line_number':257,'multiline':False]
['text':' Graph 3','line_number':258,'multiline':False]
['text':' def forward(self, param : torch.Tensor, y : torch.Tensor):','line_number':259,'multiline':False]
['text':'     add = y + param;  y = param = None','line_number':260,'multiline':False]
['text':'     return (add,)','line_number':261,'multiline':False]
['text':' BREAK','line_number':262,'multiline':False]
['text':' Graph 4','line_number':263,'multiline':False]
['text':' def forward(self, _stack0 : torch.Tensor, x : torch.nn.parameter.Parameter, y : torch.Tensor):','line_number':264,'multiline':False]
['text':'     add = x + y;  x = y = None','line_number':265,'multiline':False]
['text':'     mul = _stack0 * add;  _stack0 = add = None','line_number':266,'multiline':False]
['text':'     return (mul,)','line_number':267,'multiline':False]
['text':' Run fn with AOT','line_number':269,'multiline':False]
['text':' Note: Dynamo recompilation guarding invalid grad','line_number':275,'multiline':False]
['text':'','line_number':276,'multiline':False]
['text':' This test is a spiritual equivalent to test_invalid_requires_grad_fake in test_autodispatch.py','line_number':277,'multiline':False]
['text':' The point of this test is to invoke aot_autograd in a way that would normally trigger an assertion','line_number':278,'multiline':False]
['text':' (This is what test_invalid_requires_grad_fake) does. However, the point of this test is to prove','line_number':279,'multiline':False]
['text':' that we do not hit this assertion, as dynamo recompiles correctly and protects this condition.','line_number':280,'multiline':False]
['text':'','line_number':281,'multiline':False]
['text':' Subnote: The reason for us having test_invalid_requires_grad_fake utilizing fake tensors','line_number':282,'multiline':False]
['text':' is because dynamo sends fake tensors down to aot_autograd.','line_number':283,'multiline':False]
['text':' Reset failure reason','line_number':310,'multiline':False]
['text':' for new backend','line_number':315,'multiline':False]
['text':' Remove this test after we get double backward to actually work','line_number':325,'multiline':False]
['text':' The following cases should be equivalent:','line_number':330,'multiline':False]
['text':' (1) double backward entirely inside compiled function','line_number':332,'multiline':False]
['text':' (2) the second half of double backward outside compiled function','line_number':346,'multiline':False]
['text':' (3) double backward entirely outside compiled function','line_number':359,'multiline':False]
['text':' create_graph=False','line_number':372,'multiline':False]
['text':' Note: to prevent a recompilation between the two calls,','line_number':404,'multiline':False]
['text':' we need to clone x and y on each use.','line_number':405,'multiline':False]
['text':' fxy mutates the input's metadata, so otherwise dynamo will end up recompiling.','line_number':406,'multiline':False]
['text':' Reset failure reason','line_number':412,'multiline':False]
['text':' for new backend','line_number':417,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/103539','line_number':690,'multiline':False]
['text':' this is just dealing with the fact that','line_number':694,'multiline':False]
['text':' aot_module_simplified expects submods to always return tuples/lists','line_number':695,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/99569','line_number':744,'multiline':False]
['text':' Use dynamo export to get the fx graph module','line_number':789,'multiline':False]
['text':' aot_export requires a graph mod input of fwd graph','line_number':810,'multiline':False]
['text':' returns the full fwd/bwd graph in graph mod format','line_number':811,'multiline':False]
['text':' Walk all the nodes in fx graph.','line_number':821,'multiline':False]
['text':' Write the resulting ops to a table','line_number':822,'multiline':False]
['text':' Make all seq_nr relative so it starts at 0','line_number':837,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/110121','line_number':886,'multiline':False]
['text':' Set create_graph=True to ensure that the sequence_nr','line_number':943,'multiline':False]
['text':' for backward ops continues to count down.','line_number':944,'multiline':False]
['text':' Check that the grad computed for the inputs, given the input, is the same','line_number':1000,'multiline':False]
['text':' The tangent to `y[0]`, which has grad_required=False, is irrelevant','line_number':1001,'multiline':False]
['text':' We still want to make sure that this raises','line_number':1016,'multiline':False]
