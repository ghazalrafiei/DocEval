['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' The intention of this test file is you should put test cases specifically','line_number':16,'multiline':False]
['text':' for assume_static_by_default=False, aka you want to YOLO make everything as','line_number':17,'multiline':False]
['text':' dynamic as possible.  If you want to test the more normal situation where','line_number':18,'multiline':False]
['text':' you assume static by default, put it in a regular test file and','line_number':19,'multiline':False]
['text':' test_dynamic_shapes will cover both the YOLO and non-YOLO cases.','line_number':20,'multiline':False]
['text':' no recompilations if passing on different numpy int values','line_number':50,'multiline':False]
['text':' array scalars decay to 0D arrays','line_number':62,'multiline':False]
['text':' test unspecialized primitive max/min','line_number':64,'multiline':False]
['text':' no graph output in this frame','line_number':98,'multiline':False]
['text':' Really annoying intersection of specialization and RandomValueSource','line_number':111,'multiline':False]
['text':' If we get a RandomValueSource with a single element tensor, we should return a ConstantVariable like other','line_number':112,'multiline':False]
['text':' unspects... but if we do, we break the bytecode assumptions and guards will not work as we will be referring','line_number':113,'multiline':False]
['text':' to a name from a source that is not there. If we call .item() and take the wrapped_value out, where we do','line_number':114,'multiline':False]
['text':' wrapped_value = wrapped_value.item() where we send unspec down to wrap_fx_proxy, this test passes and then','line_number':115,'multiline':False]
['text':' some models fail on missing codegen.tx.output.random_values_var. If we let the tensor value go into wrap as','line_number':116,'multiline':False]
['text':' it is, this test fails.','line_number':117,'multiline':False]
['text':' The real solution here is to rewrite RandomValueSource and all the codegen it does from the ground up.','line_number':118,'multiline':False]
['text':' For compiled functions with random calls,','line_number':137,'multiline':False]
['text':' it should return different values for every iteration.','line_number':138,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/95425','line_number':139,'multiline':False]
['text':' builtin getitem args[0] is python list and args[1] is unspec','line_number':173,'multiline':False]
['text':' 48 is unspecialized','line_number':178,'multiline':False]
['text':' 0.23 is unspecialized','line_number':223,'multiline':False]
['text':' fails as long as numpy scalars are 0D arrays','line_number':252,'multiline':False]
['text':' np.float64 is unspecialized by default,','line_number':254,'multiline':False]
['text':' but it should be specialized when used in control flow.','line_number':255,'multiline':False]
['text':' This will fail to compile a generic kernel, but we should not','line_number':301,'multiline':False]
['text':' complain about it (mark dynamic will try its best but 0/1','line_number':302,'multiline':False]
['text':' specialization is allowed)','line_number':303,'multiline':False]
['text':' TODO: NameError: name 's1' is not defined when dynamic=True','line_number':315,'multiline':False]
['text':' passes','line_number':319,'multiline':False]
['text':' crashes','line_number':321,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/104812','line_number':387,'multiline':False]
['text':' the python arg parser coerces dim into a vector<int>','line_number':390,'multiline':False]
['text':' To ensure that the continuation frame is compiled,','line_number':412,'multiline':False]
['text':' have to write the test function in this funny way.','line_number':413,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/111918','line_number':414,'multiline':False]
