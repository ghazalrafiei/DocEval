['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' Specializes a test to run only if translation validation is set.','line_number':86,'multiline':False]
['text':' These are used for test_{cond/map}_with_quantization','line_number':108,'multiline':False]
['text':' TODO(jansel): FX doesn't support this, should add upstream support','line_number':271,'multiline':False]
['text':' Test reversal by putting int arg first.','line_number':276,'multiline':False]
['text':' No error: the overload is PT2 compliant','line_number':402,'multiline':False]
['text':' graph break on incorrect parsing','line_number':410,'multiline':False]
['text':' view','line_number':585,'multiline':False]
['text':' Test reversal by putting constant first','line_number':665,'multiline':False]
['text':' Ensure support for constant on right side','line_number':709,'multiline':False]
['text':' expect for dynamic: size, index, 6 comparison ops, add','line_number':726,'multiline':False]
['text':' Ensure support for constant on left side','line_number':734,'multiline':False]
['text':' expect for dynamic: size, index, 6 comparison ops, add','line_number':751,'multiline':False]
['text':' Test reversal by putting param shape arg first.','line_number':763,'multiline':False]
['text':' Test both ListVariable and ShapeVariable','line_number':827,'multiline':False]
['text':' Test both ListVariable and ShapeVariable','line_number':866,'multiline':False]
['text':' Can't run yet due to other unimplemented instructions','line_number':923,'multiline':False]
['text':' res += [isinstance(torch.nn.LazyLinear(2, 3), torch.nn.Linear)]','line_number':924,'multiline':False]
['text':' Filter out id-matches that won't reproduce run to run','line_number':951,'multiline':False]
['text':' expect extra size node for dynamic','line_number':1005,'multiline':False]
['text':' try all of list, iterator, tuple, vararg.','line_number':1030,'multiline':False]
['text':' unsound 0/1 specialization!','line_number':1096,'multiline':False]
['text':' 3','line_number':1123,'multiline':False]
['text':' 4.5','line_number':1124,'multiline':False]
['text':' 5','line_number':1126,'multiline':False]
['text':' 7','line_number':1128,'multiline':False]
['text':' One recompile per differing input type','line_number':1170,'multiline':False]
['text':' expect 1 more op (size call) for dynamic','line_number':1481,'multiline':False]
['text':' TODO: the captured frame here is a bit goofy, because we don't','line_number':1514,'multiline':False]
['text':' output anything and none of the traced operations have side','line_number':1515,'multiline':False]
['text':' effects.  Probably need better heuristic for bailing on','line_number':1516,'multiline':False]
['text':' dynamo if there are no outputs','line_number':1517,'multiline':False]
['text':' expect 3 ops post folding for dynamic case: size, index, add','line_number':1562,'multiline':False]
['text':' tuple += tuple','line_number':1570,'multiline':False]
['text':' tuple += constant tuple','line_number':1572,'multiline':False]
['text':' expect 4 add / subs for static, 4 * 3 (size, index, math op) for dynamic','line_number':1576,'multiline':False]
['text':' list += list','line_number':1584,'multiline':False]
['text':' list += tuple','line_number':1586,'multiline':False]
['text':' expect 6 add / subs for static, 6 * 3 (size, index, math op) for dynamic','line_number':1590,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/97480','line_number':1748,'multiline':False]
['text':' guard failure','line_number':1823,'multiline':False]
['text':' seen in fastNLP_Bert','line_number':1854,'multiline':False]
['text':' if axis is None, second argument is treated as 1d array','line_number':1897,'multiline':False]
['text':' returns equivalent of torch.eq/ne','line_number':1915,'multiline':False]
['text':' skip','line_number':1918,'multiline':False]
['text':' Did you know that tensor[ndarray_of_floats] works?','line_number':1919,'multiline':False]
['text':' skip','line_number':1922,'multiline':False]
['text':' in numpy, in place matmul does not work single','line_number':1923,'multiline':False]
['text':' dimensional arrays','line_number':1924,'multiline':False]
['text':' TODO try a bit harder','line_number':1933,'multiline':False]
['text':' XXX: differs from numpy','line_number':2079,'multiline':False]
['text':' shape, dims format','line_number':2115,'multiline':False]
['text':' Just a side effect so that compilation kicks in','line_number':2127,'multiline':False]
['text':' Define the boundaries of the complex plane','line_number':2136,'multiline':False]
['text':' Create the grid of complex numbers','line_number':2144,'multiline':False]
['text':' We need to specialise the number as it's in a forloop','line_number':2169,'multiline':False]
['text':' from transformers/models/big_bird/modeling_big_bird.py','line_number':2210,'multiline':False]
['text':' It's all traced once with x = 1, x = 2 and then x = ks0','line_number':2225,'multiline':False]
['text':' For dynamic it's x=1 and x=ks0','line_number':2226,'multiline':False]
['text':' graph break','line_number':2275,'multiline':False]
['text':' repeat with a different function','line_number':2278,'multiline':False]
['text':' no tensors/ndarray as inputs in the frame','line_number':2303,'multiline':False]
['text':' test that we gracefully graph break on dtypes','line_number':2316,'multiline':False]
['text':' that do not have pytorch equivalents.','line_number':2317,'multiline':False]
['text':' torch does not have the `uint16` dtype','line_number':2324,'multiline':False]
['text':' graph break','line_number':2329,'multiline':False]
['text':' test that iteration over an ndarray produces ndarrays not bare tensors','line_number':2332,'multiline':False]
['text':' cache size limit needs to be larger than the `dtypes` list size','line_number':2346,'multiline':False]
['text':' floats','line_number':2350,'multiline':False]
['text':' np.dtype('float64')   # XXX: this is not supported, yet','line_number':2356,'multiline':False]
['text':' integers','line_number':2357,'multiline':False]
['text':' np.dtype('int')       # XXX: as above','line_number':2363,'multiline':False]
['text':' no graph break','line_number':2376,'multiline':False]
['text':' setting the config value makes the PRNG identical to numpy's','line_number':2378,'multiline':False]
['text':' NB this may involve a graph break','line_number':2379,'multiline':False]
['text':' graph break when calling methods with inplace_view tag on graph input','line_number':2389,'multiline':False]
['text':' mul_','line_number':2409,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/107460','line_number':2439,'multiline':False]
['text':' noqa: B903','line_number':2575,'multiline':False]
['text':' TODO(jansel): dead code eliminate the object creation','line_number':2630,'multiline':False]
['text':' Force the constant guard to test source in guards','line_number':2696,'multiline':False]
['text':' Test closure with out-of-scope cell variable, used in a cond','line_number':2958,'multiline':False]
['text':' where the two branches read different closure variables','line_number':2959,'multiline':False]
['text':' y = g(x)','line_number':2983,'multiline':False]
['text':' Just for checking that Dynamo returned mod object can redirect','line_number':3057,'multiline':False]
['text':' to this method','line_number':3058,'multiline':False]
['text':' Test that run works on a decorated fn','line_number':3110,'multiline':False]
['text':' The first optimize in the nesting should be ignored','line_number':3125,'multiline':False]
['text':' Since the fn code object is already compiled, calling fn1 should','line_number':3130,'multiline':False]
['text':' directly call the compiled_fn callable.','line_number':3131,'multiline':False]
['text':' Test same behavior by reversing the calls','line_number':3135,'multiline':False]
['text':' Python code is needed here, since torch.manual_seed graph-breaks.','line_number':3230,'multiline':False]
['text':' Refs: https://github.com/pytorch/pytorch/issues/107187','line_number':3231,'multiline':False]
['text':' Only the torch.seed call is turned into an FX graph.','line_number':3236,'multiline':False]
['text':' Graph breaks at manual_seed.','line_number':3239,'multiline':False]
['text':' temporary test to check that the ci torch version is set correctly','line_number':3312,'multiline':False]
['text':' noqa: B903','line_number':3596,'multiline':False]
['text':' no breaks on self_int','line_number':3692,'multiline':False]
['text':' breaks on self_Tensor','line_number':3699,'multiline':False]
['text':' Extra op is the recorded equality test (although once','line_number':3733,'multiline':False]
['text':' the trace is flattened this is dead!)','line_number':3734,'multiline':False]
['text':' Modules that are passed into torch._dynamo optimized functions','line_number':3886,'multiline':False]
['text':' will normally be held onto through the generated GraphModule,','line_number':3887,'multiline':False]
['text':' which contains the modules. remove the reference in this backend','line_number':3888,'multiline':False]
['text':' and test that no additional references are being held.','line_number':3889,'multiline':False]
['text':' codegen to update mutated variables with side effect','line_number':3944,'multiline':False]
['text':' should after stack value's codegen','line_number':3945,'multiline':False]
['text':' graph break','line_number':3949,'multiline':False]
['text':' graph break','line_number':3951,'multiline':False]
['text':' graph break','line_number':3957,'multiline':False]
['text':' graph break','line_number':3959,'multiline':False]
['text':' Regular','line_number':4183,'multiline':False]
['text':' Prefix','line_number':4201,'multiline':False]
['text':' forward the GPT model','line_number':4243,'multiline':False]
['text':' each index maps to a (learnable) vector','line_number':4246,'multiline':False]
['text':' each position maps to a (learnable) vector','line_number':4249,'multiline':False]
['text':' if we are given some desired targets also calculate the loss','line_number':4255,'multiline':False]
['text':' Test plain recurse','line_number':4272,'multiline':False]
['text':' Test with prefix','line_number':4283,'multiline':False]
['text':' Test now the other path','line_number':4309,'multiline':False]
['text':' We intentionally don't reset dynamo for each backend so that we can test','line_number':4369,'multiline':False]
['text':' 1. dynamo doesn't recompile when backend stays the same, i.e. frame_count doesn't increase','line_number':4370,'multiline':False]
['text':' 2. dynamo recompiles when backend changes, i.e. frame_count is non-zero for next backend','line_number':4371,'multiline':False]
['text':' Run opt_f multiple times to make sure dynamo doesn't recompile.','line_number':4376,'multiline':False]
['text':' Specifically, frame_count doesn't increase','line_number':4377,'multiline':False]
['text':' the number of cached backends is i + 2 because we have the optimizing backend + None','line_number':4378,'multiline':False]
['text':' Test dynamo recompiles but only caches a single backend for each thread','line_number':4409,'multiline':False]
['text':' cnt and None','line_number':4411,'multiline':False]
['text':' Wait for all threads to finish','line_number':4430,'multiline':False]
['text':' Threads are sharing the backend cache. We see two cnt backends and one None backend','line_number':4434,'multiline':False]
['text':' invalid size','line_number':4490,'multiline':False]
['text':' nonzero_static.out: out dtype mismatch','line_number':4501,'multiline':False]
['text':' nonzero_static.out: out resize (shrink)','line_number':4510,'multiline':False]
['text':' nonzero_static.out: out resize (enlarge)','line_number':4527,'multiline':False]
['text':' 0 rank','line_number':4544,'multiline':False]
['text':' 0 size','line_number':4554,'multiline':False]
['text':' 1D input','line_number':4564,'multiline':False]
['text':' padded with default fill_value "-1"','line_number':4579,'multiline':False]
['text':' 2D input','line_number':4583,'multiline':False]
['text':' 3D input','line_number':4615,'multiline':False]
['text':' * 10 then add x','line_number':4767,'multiline':False]
['text':' * -1 then add x','line_number':4774,'multiline':False]
['text':' * 10 then add x','line_number':4813,'multiline':False]
['text':' * -1 then add x','line_number':4820,'multiline':False]
['text':' reset logging state','line_number':4899,'multiline':False]
['text':' Test sth like torch.LongTensor(list(np.int64, np.int64, ...))','line_number':4975,'multiline':False]
['text':' noqa: B027','line_number':5032,'multiline':False]
['text':' Test user defined function default arguments can be:','line_number':5058,'multiline':False]
['text':' 1, user defined functions (e.g, add1)','line_number':5059,'multiline':False]
['text':' 2, torch functions (e.g, torch.sin)','line_number':5060,'multiline':False]
['text':' 3, python builtin functions (e.g, operator.neg)','line_number':5061,'multiline':False]
['text':' should not error','line_number':5179,'multiline':False]
['text':' obj.__bool__ is not existed','line_number':5275,'multiline':False]
['text':' noqa: B903','line_number':5276,'multiline':False]
['text':' obj.__bool__ is function and returns bool type','line_number':5280,'multiline':False]
['text':' obj.__bool__ is non-function','line_number':5288,'multiline':False]
['text':' obj.__bool__ is function and returns non-bool type','line_number':5314,'multiline':False]
['text':' obj.__bool__ is not existed, but obj.__len__ exists','line_number':5339,'multiline':False]
['text':' noqa: B903','line_number':5340,'multiline':False]
['text':' obj.__bool__ takes precedence over obj.__len__','line_number':5347,'multiline':False]
['text':' VariableTracker.recursively_contains should be updated correctly when mutation happens','line_number':5413,'multiline':False]
['text':' we didn't actually test guard_failure_fn here but whatever,','line_number':5561,'multiline':False]
['text':' nice to see no guard failure on the test','line_number':5562,'multiline':False]
['text':' guard is expected for both static and dynamic shapes','line_number':5665,'multiline':False]
['text':' This function does some guard accumulation,','line_number':5673,'multiline':False]
['text':' and then rolls back due to control flow.','line_number':5674,'multiline':False]
['text':' The idea is that if one were printing guards as they appear,','line_number':5675,'multiline':False]
['text':' they would see this insert a guard that does not show up in the final set of','line_number':5676,'multiline':False]
['text':' guards as we rolled back from it.','line_number':5677,'multiline':False]
['text':' This guard was created','line_number':5702,'multiline':False]
['text':' Check recompilation','line_number':5746,'multiline':False]
['text':' Ensure that super guard checks are working as expected','line_number':5751,'multiline':False]
['text':' from torch._dynamo.test_case import run_tests','line_number':5793,'multiline':False]
['text':' if assume_static_by_default == True: spec int list','line_number':5829,'multiline':False]
['text':' otherwise: unspec int list','line_number':5830,'multiline':False]
['text':' Cache the original builtin function ids','line_number':5853,'multiline':False]
['text':' Monkey patch builtin function','line_number':5878,'multiline':False]
['text':' specifically test for tensor.attribute -> torch.something()','line_number':5887,'multiline':False]
['text':' user-defined functions which are object's attributes are not converted to bound methods','line_number':5947,'multiline':False]
['text':' user-defined functions which are class's attributes are converted to bound methods','line_number':5968,'multiline':False]
['text':' test with LOAD_GLOBAL since it has a different instruction size','line_number':6130,'multiline':False]
['text':' format: jump_opname, target_idx, expected forward jump, expected return value','line_number':6176,'multiline':False]
['text':' check if offset of latest jump instruction is forward/backward','line_number':6186,'multiline':False]
['text':' run the code and check result','line_number':6194,'multiline':False]
['text':' these numbers have no real meaning to them','line_number':6206,'multiline':False]
['text':' last instructions of an exn_table entry is a large instruction','line_number':6265,'multiline':False]
['text':' i.e., LOAD_GLOBAL a','line_number':6266,'multiline':False]
['text':' dead','line_number':6366,'multiline':False]
['text':' dead','line_number':6369,'multiline':False]
['text':' traceback.format_exc() approximates an unhandled exception','line_number':6391,'multiline':False]
['text':' segfaults in python 3.11 if shadow frame is freed improperly','line_number':6405,'multiline':False]
['text':' test that the errors are the same for dense and sparse versions','line_number':6409,'multiline':False]
['text':' shapes must be compatible for matrix multiplication','line_number':6411,'multiline':False]
['text':' flake8: noqa','line_number':6465,'multiline':False]
['text':' fmt: off','line_number':6466,'multiline':False]
['text':' test binary ops','line_number':6467,'multiline':False]
['text':' test','line_number':6472,'multiline':False]
['text':' test +','line_number':6474,'multiline':False]
['text':' +','line_number':6476,'multiline':False]
['text':' test','line_number':6479,'multiline':False]
['text':' test','line_number':6481,'multiline':False]
['text':' test slice','line_number':6484,'multiline':False]
['text':' test','line_number':6487,'multiline':False]
['text':' test','line_number':6491,'multiline':False]
['text':' test nested and multiline function calls','line_number':6494,'multiline':False]
['text':' test chained function calls','line_number':6501,'multiline':False]
['text':' test unicode (match traceback behavior)','line_number':6506,'multiline':False]
['text':' uncomment to determine offsets','line_number':6514,'multiline':False]
['text':' print(*enumerate(insts), sep="\n")','line_number':6515,'multiline':False]
['text':' test unicode (since assertExpectedInline doesn't support unicode)','line_number':6591,'multiline':False]
['text':' Sadly, this does not throw - we do not prop correctly across the graph break','line_number':6659,'multiline':False]
['text':' Trigger an exception in backwards','line_number':6726,'multiline':False]
['text':' Suppress unrelated pkg_resources warnings','line_number':6739,'multiline':False]
['text':' Run with dynamic','line_number':6750,'multiline':False]
['text':' Run without dynamic, no recompile','line_number':6756,'multiline':False]
['text':' Mark a new dim, 1, as dynamic','line_number':6761,'multiline':False]
['text':' Recompile triggered because we marked a new dym as dynamic','line_number':6765,'multiline':False]
['text':' Reset','line_number':6768,'multiline':False]
['text':' Reset counter','line_number':6770,'multiline':False]
['text':' Run with dynamic 1','line_number':6773,'multiline':False]
['text':' Run with dynamic 0, not subset','line_number':6777,'multiline':False]
['text':' Run with dynamic 0, 1, 2, not subset','line_number':6781,'multiline':False]
['text':' Not an exhaustive test of dynamic shapes behavior, but some sanity','line_number':7027,'multiline':False]
['text':' strip_function_call should extract the object from the string.','line_number':7057,'multiline':False]
['text':' s and s2 should hash the  same','line_number':7082,'multiline':False]
['text':' s3 should be different','line_number':7084,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/99665','line_number':7138,'multiline':False]
['text':' Nothing gets CSE-d, since the only repeated sub-expression is 'x'.','line_number':7181,'multiline':False]
['text':' i.e. not a node type we are interested on.','line_number':7182,'multiline':False]
['text':' 'a.b.c' gets CSE-d, since it's a sub-expression used more than 'PyExprCSEPass.USE_THRESHOLD'.','line_number':7186,'multiline':False]
['text':' 'm.n[0]' gets CSE-d, since it is a sub-expression used more than 'PyExprCSEPass.USE_THRESHOLD'.','line_number':7194,'multiline':False]
['text':' The whole expressiong gets CSE-d, as well as all of its sub-expressions.','line_number':7202,'multiline':False]
['text':' use numpy int so it's wrapped as fake tensor in dynamo','line_number':7342,'multiline':False]
['text':' test shape as fake tensor, which param type is','line_number':7344,'multiline':False]
['text':' Sequence[Union[_int, SymInt]]','line_number':7345,'multiline':False]
['text':' use checkpoint to trigger a "sourceless" tensor subclass','line_number':7598,'multiline':False]
['text':' There's a lot of stuff about sets that cannot work without a good deal of exertion on our part.','line_number':7704,'multiline':False]
['text':' Specifically, getting a set as input won't ever work with how GetItemSource works (Can't arbitrary access set contents)','line_number':7705,'multiline':False]
['text':' and so the guard story for the objects passed into input just isn't there atm.','line_number':7706,'multiline':False]
['text':' first call with no aliasing','line_number':7744,'multiline':False]
['text':' no aliasing again','line_number':7748,'multiline':False]
['text':' assert no recompile','line_number':7750,'multiline':False]
['text':' aliasing changes, we should recompile','line_number':7753,'multiline':False]
['text':' same aliasing, different tensor','line_number':7757,'multiline':False]
['text':' aliasing between global and arg, should recompile again','line_number':7761,'multiline':False]
['text':' Reset','line_number':7765,'multiline':False]
['text':' aliasing between global and arg, first call','line_number':7768,'multiline':False]
['text':' same aliasing, different tensor, all local, recompile','line_number':7772,'multiline':False]
['text':' aliasing same tensor, we shouldn't recompile','line_number':7776,'multiline':False]
['text':' No aliasing','line_number':7780,'multiline':False]
['text':' No aliasing again','line_number':7784,'multiline':False]
['text':' assert no recompile','line_number':7786,'multiline':False]
['text':' trigger a recompile and graph break','line_number':7841,'multiline':False]
['text':' Value change, no recompiles','line_number':7919,'multiline':False]
['text':' Size change, forced recompiles','line_number':7924,'multiline':False]
['text':' Nothing to compile here','line_number':7943,'multiline':False]
['text':' Inline generator','line_number':8027,'multiline':False]
['text':' Use 'yield from' to iterate over tensors and multiply','line_number':8032,'multiline':False]
['text':' Constant assert at trace time','line_number':8123,'multiline':False]
['text':' Constant assert at trace time','line_number':8146,'multiline':False]
['text':' https://github.com/pytorch/pytorch/issues/110287','line_number':8279,'multiline':False]
['text':' Initialize the accumulator with the first value from the iterable','line_number':8402,'multiline':False]
['text':' If the iterable is empty, return an empty generator','line_number':8405,'multiline':False]
['text':' The main ShapeEnv should have no event recorded.','line_number':8472,'multiline':False]
['text':' Call create_symbolic_sizes_strides_storage_offset on both of them.','line_number':8475,'multiline':False]
['text':' Create a guard: size[0] == 3 (call evaluate_expr)','line_number':8480,'multiline':False]
['text':'   - +1 guard entry','line_number':8481,'multiline':False]
['text':'   - +1 replacement entry','line_number':8482,'multiline':False]
['text':' The main ShapeEnv should remain with no event recorded.','line_number':8486,'multiline':False]
['text':' Check that we don't store any recording metadata on nodes','line_number':8495,'multiline':False]
['text':' from the symbolic shape FX graph.','line_number':8496,'multiline':False]
['text':' Call create_symbolic_sizes_strides_storage_offset on both of them.','line_number':8596,'multiline':False]
['text':' Create a guard: size[0] % 3 == 0 (only in the main ShapeEnv)','line_number':8604,'multiline':False]
['text':'   - +1 guard entry','line_number':8605,'multiline':False]
['text':'   - +1 divisible entry','line_number':8606,'multiline':False]
['text':' Call create_symbolic_sizes_strides_storage_offset on both of them.','line_number':8633,'multiline':False]
['text':' Create a guard: size[0] == 3 (only in the main ShapeEnv)','line_number':8641,'multiline':False]
['text':'   - +1 guard entry','line_number':8642,'multiline':False]
['text':'   - +1 replacement entry','line_number':8643,'multiline':False]
['text':' Call create_symbolic_sizes_strides_storage_offset on both of them.','line_number':8670,'multiline':False]
['text':' Create a guard: size[0] >= 3 (only in the main ShapeEnv)','line_number':8678,'multiline':False]
['text':'   - +1 guard entry','line_number':8679,'multiline':False]
['text':'   - +1 var_to_guard entry','line_number':8680,'multiline':False]
['text':'   - Change: var_to_range','line_number':8681,'multiline':False]
['text':' Call create_unbacked_symint on both of them.','line_number':8711,'multiline':False]
['text':' Create a runtime assert: r % 3 == 0 (only in the main ShapeEnv)','line_number':8715,'multiline':False]
['text':'   - +1 deferred_runtime_asserts entry','line_number':8716,'multiline':False]
['text':'   - Change: num_deferred_runtime_asserts','line_number':8717,'multiline':False]
['text':' Make sure the record/replay mechanism for ShapeEnv will fallback','line_number':8743,'multiline':False]
['text':' if no ShapeEnv instance is found.','line_number':8744,'multiline':False]
