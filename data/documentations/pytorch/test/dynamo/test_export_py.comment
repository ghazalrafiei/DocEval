['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' TODO(voz): Refactor to a shared test function.','line_number':40,'multiline':False]
['text':' The tests in this file are a little redundant,','line_number':41,'multiline':False]
['text':' They all take a func, run it with eager, then export it, then compare','line_number':42,'multiline':False]
['text':' Tensor input can be almost anything here, and the result will capture what we','line_number':1254,'multiline':False]
['text':' made constant at compile time.','line_number':1255,'multiline':False]
['text':' Tensor input can be almost anything here, and the result will capture what we','line_number':1278,'multiline':False]
['text':' made constant at compile time.','line_number':1279,'multiline':False]
['text':' Tensor input can be almost anything here, and the result will capture what we','line_number':1302,'multiline':False]
['text':' made constant at compile time.','line_number':1303,'multiline':False]
['text':' Tensor input can be almost anything here, and the result will capture what we','line_number':1324,'multiline':False]
['text':' made constant at compile time.','line_number':1325,'multiline':False]
['text':' X is negative, so .item() < 0, which means we return y','line_number':1348,'multiline':False]
['text':' X is positive, but we compiled helper_fn to return None, so it will still return y','line_number':1353,'multiline':False]
['text':' X is positive, so .item() > 0, which means we return y * x','line_number':1375,'multiline':False]
['text':' X is negative, but we compiled helper_fn to return x, so it will still return y * x','line_number':1380,'multiline':False]
['text':' X is negative, so .item() < 0, which means we return y','line_number':1402,'multiline':False]
['text':' X is positive, but we compiled helper_fn to return None, so it will still return y','line_number':1407,'multiline':False]
['text':' X is positive, so .item() > 0, which means we return y * x','line_number':1429,'multiline':False]
['text':' X is negative, but we compiled helper_fn to return x, so it will still return y * x','line_number':1434,'multiline':False]
['text':' X is positive, so .item() > 0, which means we return y * x','line_number':1456,'multiline':False]
['text':' X is negative, but we compiled helper_fn to return x, so it will still return y * x','line_number':1461,'multiline':False]
['text':' New X, just to show we did not specialize','line_number':1556,'multiline':False]
['text':' ok','line_number':1568,'multiline':False]
['text':' True branch and false branch return tensors of different shape','line_number':1736,'multiline':False]
['text':' True branch and false branch return tensors of different shape','line_number':1742,'multiline':False]
['text':' aten graph should flatten getitem calls to actual','line_number':1846,'multiline':False]
['text':' slice kernel call.','line_number':1847,'multiline':False]
['text':' In torch mode, the graph should contain 3 getitem methods','line_number':1859,'multiline':False]
['text':' one for x.shape[0]-2 and one for x.shape[1]-1 and one for slice','line_number':1860,'multiline':False]
['text':' this is because Tensor class has its' own getitem method','line_number':1861,'multiline':False]
['text':' which gets translated to aten.Slice later.','line_number':1862,'multiline':False]
['text':' FIXME: AssertionError: Dynamo input and output is a strict subset of traced input/output','line_number':2022,'multiline':False]
['text':' FIXME: AssertionError: Dynamo input and output is a strict subset of traced input/output','line_number':2028,'multiline':False]
['text':' FIXME: AssertionError: Dynamo input and output is a strict subset of traced input/output','line_number':2057,'multiline':False]
['text':' FIXME: AssertionError: Dynamo input and output is a strict subset of traced input/output','line_number':2063,'multiline':False]
['text':' To ensure dynamo.export is robust to wrapped functions','line_number':2094,'multiline':False]
['text':' when it cannot use `inspect` to retrieve original signature','line_number':2095,'multiline':False]
['text':' info.','line_number':2096,'multiline':False]
['text':' 3rd unnamed positional argument','line_number':2159,'multiline':False]
['text':' Check that the exported graph preserves same argument names.','line_number':2218,'multiline':False]
['text':' return a constant of primitive type','line_number':2265,'multiline':False]
['text':' new behavior','line_number':2278,'multiline':False]
['text':' Tests if we can restore saved nn.Parameters when we load them again','line_number':2316,'multiline':False]
['text':' no specialization error','line_number':2410,'multiline':False]
['text':' Ensure the exported graph module with metadata is serializable,','line_number':2621,'multiline':False]
['text':' metadata won't be saved in the serialized module','line_number':2622,'multiline':False]
['text':' noqa: B015','line_number':2666,'multiline':False]
['text':' ok','line_number':2676,'multiline':False]
['text':' error','line_number':2687,'multiline':False]
['text':' non-trivial divisibility condition','line_number':2700,'multiline':False]
['text':' trivially true','line_number':2707,'multiline':False]
['text':' In export, int & float in forward should always be specialized','line_number':2790,'multiline':False]
['text':' See https://github.com/pytorch/pytorch/issues/100455','line_number':3226,'multiline':False]
['text':' Outter shape env should have no guards in it because we never specialize on the outter symbool.','line_number':3375,'multiline':False]
['text':' NB: This doesn't actually work (it only reports the first usage),','line_number':3410,'multiline':False]
['text':' but I'm leaving the test here in case we fix it later','line_number':3411,'multiline':False]
['text':' Since there are no parameters we can do this','line_number':3477,'multiline':False]
['text':' TODO: Seems to choke if you don't make a fresh model and','line_number':3486,'multiline':False]
['text':' just try to export Linear directly...','line_number':3487,'multiline':False]
['text':' User-instantiated FakeTensorMode','line_number':3526,'multiline':False]
['text':' Fakefy input+model before exporting it','line_number':3535,'multiline':False]
['text':' Export the model with fake inputs and parameters','line_number':3540,'multiline':False]
['text':' noqa: B950,E122','line_number':3720,'multiline':False]
['text':' replace relu with fn','line_number':4117,'multiline':False]
['text':' check for other metadata','line_number':4162,'multiline':False]
['text':' replace abs with graph break','line_number':4239,'multiline':False]
