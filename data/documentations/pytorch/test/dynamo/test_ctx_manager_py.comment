['text':' Owner(s): ["module: dynamo"]','line_number':1,'multiline':False]
['text':' redundant no_grad should get ignored','line_number':31,'multiline':False]
['text':' coalesced noop','line_number':62,'multiline':False]
['text':' coalesced noop','line_number':65,'multiline':False]
['text':' coalesced noop','line_number':73,'multiline':False]
['text':' coalesced noop','line_number':76,'multiline':False]
['text':' graph break','line_number':83,'multiline':False]
['text':' wrap torch.profiler.* as NullContextVariable and do nothing','line_number':119,'multiline':False]
['text':' graph break','line_number':126,'multiline':False]
['text':' wrap torch.autograd.profiler.* as NullContextVariable and do nothing','line_number':139,'multiline':False]
['text':' graph break','line_number':146,'multiline':False]
['text':' use new event to sync','line_number':270,'multiline':False]
['text':' Regression for: https://github.com/pytorch/pytorch/issues/93890','line_number':476,'multiline':False]
['text':' Check that nested with non-inlineable function with graph break','line_number':512,'multiline':False]
['text':' We remember to exit the inner autocast correctly to outer','line_number':515,'multiline':False]
['text':' even after graph breaks','line_number':516,'multiline':False]
['text':' Autocast doesn't work on addition, so we need the bias to be `bfloat16`','line_number':572,'multiline':False]
['text':' Calling no_grad or enabled_grad should not mutate global state','line_number':1048,'multiline':False]
['text':' Calling no_grad or enabled_grad should not mutate global state','line_number':1083,'multiline':False]
['text':' Consuming set_grad_enabled by calling it on a function','line_number':1118,'multiline':False]
['text':' should not mutate global state','line_number':1119,'multiline':False]
