['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]
['text':' folded','line_number':43,'multiline':False]
['text':' folded','line_number':44,'multiline':False]
['text':' folded','line_number':45,'multiline':False]
['text':' not folded','line_number':46,'multiline':False]
['text':' folded','line_number':47,'multiline':False]
['text':' folded','line_number':48,'multiline':False]
['text':' folded','line_number':49,'multiline':False]
['text':' folded','line_number':51,'multiline':False]
['text':' folded','line_number':54,'multiline':False]
['text':' folded','line_number':55,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':73,'multiline':False]
['text':' module m {','line_number':74,'multiline':False]
['text':'   attributes {','line_number':75,'multiline':False]
['text':'     tt = ...','line_number':76,'multiline':False]
['text':'   }','line_number':77,'multiline':False]
['text':'   ...','line_number':78,'multiline':False]
['text':' }','line_number':79,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':135,'multiline':False]
['text':' module m {','line_number':136,'multiline':False]
['text':'   attributes {','line_number':137,'multiline':False]
['text':'     sub2 = ...','line_number':138,'multiline':False]
['text':'      b =','line_number':139,'multiline':False]
['text':'   }','line_number':140,'multiline':False]
['text':'   ...','line_number':141,'multiline':False]
['text':'   submodule {','line_number':142,'multiline':False]
['text':'     module m {','line_number':143,'multiline':False]
['text':'       attributes {','line_number':144,'multiline':False]
['text':'         sub2 = ...','line_number':145,'multiline':False]
['text':'         b =','line_number':146,'multiline':False]
['text':'       }','line_number':147,'multiline':False]
['text':'       ...','line_number':148,'multiline':False]
['text':'     }','line_number':149,'multiline':False]
['text':'   }','line_number':150,'multiline':False]
['text':' }','line_number':151,'multiline':False]
['text':' verify b is preserved in sub2','line_number':157,'multiline':False]
['text':' verify a is removed in sub2','line_number':158,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':189,'multiline':False]
['text':' module m {','line_number':190,'multiline':False]
['text':'   attributes {','line_number':191,'multiline':False]
['text':'   }','line_number':192,'multiline':False]
['text':'   ...','line_number':193,'multiline':False]
['text':'   submodule {','line_number':194,'multiline':False]
['text':'   }','line_number':195,'multiline':False]
['text':' }','line_number':196,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':242,'multiline':False]
['text':' module m {','line_number':243,'multiline':False]
['text':'   attributes {','line_number':244,'multiline':False]
['text':'   }','line_number':245,'multiline':False]
['text':'   ...','line_number':246,'multiline':False]
['text':'   submodule {','line_number':247,'multiline':False]
['text':'   }','line_number':248,'multiline':False]
['text':' }','line_number':249,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':281,'multiline':False]
['text':' module m {','line_number':282,'multiline':False]
['text':'   attributes {','line_number':283,'multiline':False]
['text':'     self.a = ...','line_number':284,'multiline':False]
['text':'     self.b = ..','line_number':285,'multiline':False]
['text':'   }','line_number':286,'multiline':False]
['text':'   ...','line_number':287,'multiline':False]
['text':'   submodule {','line_number':288,'multiline':False]
['text':'   }','line_number':289,'multiline':False]
['text':' }','line_number':290,'multiline':False]
['text':' TODO:  Although there are no mutation, the alias analysis','line_number':291,'multiline':False]
['text':' conservatively assumes there is a mutation because attributes are','line_number':292,'multiline':False]
['text':' passed to fork subgraph. both 'a' and 'b' are preserved.','line_number':293,'multiline':False]
['text':' Check if frozen module looks as below:','line_number':329,'multiline':False]
['text':' module m {','line_number':330,'multiline':False]
['text':'   attributes {','line_number':331,'multiline':False]
['text':'     self.b = ..','line_number':332,'multiline':False]
['text':'   }','line_number':333,'multiline':False]
['text':'   ...','line_number':334,'multiline':False]
['text':' TODO:  Although there are no mutation, the alias analysis','line_number':335,'multiline':False]
['text':' conservatively assumes there is a mutation because attributes are','line_number':336,'multiline':False]
['text':' passed to fork subgraph. 'b' is preserved.','line_number':337,'multiline':False]
['text':' sub1 and sub2.sub shared same class type.','line_number':376,'multiline':False]
['text':' Checking if  Frozen module looks as  below','line_number':390,'multiline':False]
['text':' module mf {','line_number':391,'multiline':False]
['text':'   attributes {','line_number':392,'multiline':False]
['text':'     sub1 = ...','line_number':393,'multiline':False]
['text':'     sub2 = ...','line_number':394,'multiline':False]
['text':'   }','line_number':395,'multiline':False]
['text':'   ...','line_number':396,'multiline':False]
['text':'   submodules {','line_number':397,'multiline':False]
['text':'     module sub1 {','line_number':398,'multiline':False]
['text':'       attributes {','line_number':399,'multiline':False]
['text':'         a = ...','line_number':400,'multiline':False]
['text':'         b = ...','line_number':401,'multiline':False]
['text':'       }','line_number':402,'multiline':False]
['text':'       ...','line_number':403,'multiline':False]
['text':'     }','line_number':404,'multiline':False]
['text':'     module sub2 {','line_number':405,'multiline':False]
['text':'       attributes {','line_number':406,'multiline':False]
['text':'         sub = ...','line_number':407,'multiline':False]
['text':'       }','line_number':408,'multiline':False]
['text':'       ...','line_number':409,'multiline':False]
['text':'       submodule {','line_number':410,'multiline':False]
['text':'         module sub {','line_number':411,'multiline':False]
['text':'           attributes {','line_number':412,'multiline':False]
['text':'             a = ...','line_number':413,'multiline':False]
['text':'             b = ...','line_number':414,'multiline':False]
['text':'           }','line_number':415,'multiline':False]
['text':'           ...','line_number':416,'multiline':False]
['text':'         }','line_number':417,'multiline':False]
['text':'       }','line_number':418,'multiline':False]
['text':'     }','line_number':419,'multiline':False]
['text':'   }','line_number':420,'multiline':False]
['text':' }','line_number':421,'multiline':False]
['text':' aliasing','line_number':459,'multiline':False]
['text':' aliasing','line_number':467,'multiline':False]
['text':' Freezing detects that self.sub2.sub.a and self.sub1.a are alias','line_number':482,'multiline':False]
['text':' FIXME: JIT is not honoring aliasing. 'Sub' module is copied. As a result','line_number':489,'multiline':False]
['text':' Eager and Script modules produce different output.','line_number':490,'multiline':False]
['text':' aliasing','line_number':515,'multiline':False]
['text':' aliasing','line_number':523,'multiline':False]
['text':' sub2 is fully folded becasue self.sub1 and self.sub2.sub are not alias (Scripting bug)','line_number':536,'multiline':False]
['text':' Should be equal','line_number':542,'multiline':False]
['text':' aliasing','line_number':560,'multiline':False]
['text':' Test that 'sub1' is preserved entirely and 'sub2' is completely folded','line_number':570,'multiline':False]
['text':' aliasing','line_number':594,'multiline':False]
['text':' Test that be both sub1 and sub1 are preserved and 'b' is preserved','line_number':604,'multiline':False]
['text':' even if it is not used. To fulfill user request to preserve 'sub1'','line_number':605,'multiline':False]
['text':' Mutable attributes','line_number':672,'multiline':False]
['text':' Post-freezing mutating m_s.a  does not affect m_f (m_f has its own copy).','line_number':691,'multiline':False]
['text':' Post-freezing tensor attribute mutations affect m_f.','line_number':743,'multiline':False]
['text':' FIXME: deep copy all folded attributes so that m_f has full ownership.','line_number':744,'multiline':False]
['text':' 1+2+3+14+15+16','line_number':834,'multiline':False]
['text':' account for  self.a += 10.','line_number':878,'multiline':False]
['text':' FIXME: It should be assertTrue. Currently scripting is making a copy for setting self.b (see #33034)','line_number':939,'multiline':False]
['text':' Check attribute a is preserved. Alias analysis detects that 'a' has output writers.','line_number':947,'multiline':False]
['text':' In this example, 'a' is not mutated. However, we do not track which sub','line_number':948,'multiline':False]
['text':' values of a composite ivalue is mutated.','line_number':949,'multiline':False]
['text':' noqa: B903','line_number':1014,'multiline':False]
['text':' noqa: B903','line_number':1015,'multiline':False]
['text':' verify mTrain_freezed looks exactly as:','line_number':1123,'multiline':False]
['text':' module {','line_number':1124,'multiline':False]
['text':'   attributes {','line_number':1125,'multiline':False]
['text':'     conv1 = ...','line_number':1126,'multiline':False]
['text':'     conv2 = ...','line_number':1127,'multiline':False]
['text':'     dropout1 = ...','line_number':1128,'multiline':False]
['text':'     dropout2 = ...','line_number':1129,'multiline':False]
['text':'     fc1 = ...','line_number':1130,'multiline':False]
['text':'     fc2 = ...','line_number':1131,'multiline':False]
['text':'   }','line_number':1132,'multiline':False]
['text':'   ...','line_number':1133,'multiline':False]
['text':'   submodules {','line_number':1134,'multiline':False]
['text':'     module conv1 {','line_number':1135,'multiline':False]
['text':'       attributes {','line_number':1136,'multiline':False]
['text':'          weight = ...','line_number':1137,'multiline':False]
['text':'          bias = ...','line_number':1138,'multiline':False]
['text':'       }','line_number':1139,'multiline':False]
['text':'       ...','line_number':1140,'multiline':False]
['text':'     }','line_number':1141,'multiline':False]
['text':'     module conv2 {','line_number':1142,'multiline':False]
['text':'       attributes {','line_number':1143,'multiline':False]
['text':'          weight = ...','line_number':1144,'multiline':False]
['text':'          bias = ...','line_number':1145,'multiline':False]
['text':'       }','line_number':1146,'multiline':False]
['text':'       ...','line_number':1147,'multiline':False]
['text':'     }','line_number':1148,'multiline':False]
['text':'     module dropout1 {','line_number':1149,'multiline':False]
['text':'       attributes {','line_number':1150,'multiline':False]
['text':'          training = ...','line_number':1151,'multiline':False]
['text':'       }','line_number':1152,'multiline':False]
['text':'       ...','line_number':1153,'multiline':False]
['text':'     }','line_number':1154,'multiline':False]
['text':'     module dropout2 {','line_number':1155,'multiline':False]
['text':'       attributes {','line_number':1156,'multiline':False]
['text':'          training = ...','line_number':1157,'multiline':False]
['text':'       }','line_number':1158,'multiline':False]
['text':'       ...','line_number':1159,'multiline':False]
['text':'     }','line_number':1160,'multiline':False]
['text':'     module fc1 {','line_number':1161,'multiline':False]
['text':'       attributes {','line_number':1162,'multiline':False]
['text':'          weight = ...','line_number':1163,'multiline':False]
['text':'          bias = ...','line_number':1164,'multiline':False]
['text':'       }','line_number':1165,'multiline':False]
['text':'       ...','line_number':1166,'multiline':False]
['text':'     }','line_number':1167,'multiline':False]
['text':'     module fc2 {','line_number':1168,'multiline':False]
['text':'       attributes {','line_number':1169,'multiline':False]
['text':'          weight = ...','line_number':1170,'multiline':False]
['text':'          bias = ...','line_number':1171,'multiline':False]
['text':'       }','line_number':1172,'multiline':False]
['text':'       ...','line_number':1173,'multiline':False]
['text':'     }','line_number':1174,'multiline':False]
['text':' FIXME: frozen module mutated from outside (original module).','line_number':1231,'multiline':False]
['text':' Attribute "a" is preserved','line_number':1252,'multiline':False]
['text':' Both attribute "a" and method "modify_a" are preserved','line_number':1279,'multiline':False]
['text':' Earlier bug resulted in _packed_params set to false.','line_number':1445,'multiline':False]
['text':' It used to segfault while running frozen module.','line_number':1449,'multiline':False]
['text':' I don't think there's any way to create a plain python object that','line_number':1869,'multiline':False]
['text':' contains a torch.nn.Module inside it, but just in case... I'm not','line_number':1870,'multiline':False]
['text':' sure freezing would handle this case correctly, so marking as xfail','line_number':1871,'multiline':False]
['text':' so that if this ever _does_ start working someone will need to','line_number':1872,'multiline':False]
['text':' investigate to make sure this is handled correctly.','line_number':1873,'multiline':False]
['text':' type: (float) -> None','line_number':1912,'multiline':False]
['text':' Check if prim::TupleConstruct and prim::TupleUnpack','line_number':1958,'multiline':False]
['text':' Don't exist in frozen graph','line_number':1959,'multiline':False]
['text':' this method will change during freezing','line_number':1971,'multiline':False]
['text':' successfully no-ops with non-const inputs','line_number':2038,'multiline':False]
['text':' _jit_pass_optimize_frozen_graph should not be called on non-method attributes (e.g. "amt")','line_number':2073,'multiline':False]
['text':' During freezing this creates tensors constants that are attached to the frozen graph,','line_number':2077,'multiline':False]
['text':' which is then kept alive by the compilation unit (which causes a leak)','line_number':2078,'multiline':False]
['text':' CUDA conv takes input tensors which must all be the same dtype,','line_number':2082,'multiline':False]
['text':' which can cause issues if folding produces inputs of different dtypes.','line_number':2083,'multiline':False]
['text':' successively no-ops with non-const inputs','line_number':2155,'multiline':False]
['text':' broadcasting add','line_number':2182,'multiline':False]
['text':' broadcasting add','line_number':2186,'multiline':False]
['text':' add with different dtype','line_number':2189,'multiline':False]
['text':' successfully no-ops with non-const inputs','line_number':2257,'multiline':False]
['text':' Freezing requires that the graph be a module','line_number':2376,'multiline':False]
['text':' successively no-ops with non-const inputs','line_number':2433,'multiline':False]
['text':' set optimize to False here, by default freezing runs run_frozen_optimizations','line_number':2453,'multiline':False]
['text':' inspect frozen mod','line_number':2455,'multiline':False]
['text':' run_frozen_optimizations should be run','line_number':2460,'multiline':False]
['text':' inspect mod','line_number':2474,'multiline':False]
['text':' inspect mod','line_number':2495,'multiline':False]
['text':' successfully no-ops with non-const inputs','line_number':2535,'multiline':False]
['text':' TODO: merge with check_linear_optimizations once both diffs land','line_number':2575,'multiline':False]
['text':' successively no-ops with non-const inputs','line_number':2583,'multiline':False]
['text':' Generic composable conv for testing purposes','line_number':2596,'multiline':False]
['text':' for good measure, check that broadcasting does not work without this op','line_number':2639,'multiline':False]
['text':' so we can remove the op if it ever gets supported','line_number':2640,'multiline':False]
['text':' add gets uninplaced and reinplaced','line_number':2659,'multiline':False]
['text':' test no error when mkldnn not available','line_number':2681,'multiline':False]
['text':' torch.nn.AdaptiveMaxPool2d(4), # return tuples','line_number':2828,'multiline':False]
['text':' these two passes needed to remove','line_number':2839,'multiline':False]
['text':' a size check in BatchNorm2d','line_number':2840,'multiline':False]
['text':' torch.nn.AdaptiveAvgPool3d(4), # no ideep bindings','line_number':2852,'multiline':False]
['text':' torch.nn.AdaptiveMaxPool3d(4), # return tuples','line_number':2853,'multiline':False]
['text':' these two passes needed to remove','line_number':2863,'multiline':False]
['text':' a size check in BatchNorm2d','line_number':2864,'multiline':False]
['text':' `inplace=False` is intentional, otherwise we modify the input','line_number':2934,'multiline':False]
['text':' and we aren't testing aten impls anyways','line_number':2935,'multiline':False]
['text':' a1 cant be inplaced for first use, can for second','line_number':2954,'multiline':False]
['text':' simple conv-relu','line_number':3009,'multiline':False]
['text':' this mul can be inplaced since x is dead after this use','line_number':3023,'multiline':False]
['text':' temporary livespan is the return node,','line_number':3025,'multiline':False]
['text':' add can not be inplaced','line_number':3026,'multiline':False]
['text':' x can't be inplaced because its a return value,','line_number':3041,'multiline':False]
['text':' check that the inplacing pass doesnt try to inplace','line_number':3042,'multiline':False]
['text':' self.tensor because its always alive','line_number':3043,'multiline':False]
['text':' the shapes dont add up on this just testing a particular pattern','line_number':3060,'multiline':False]
['text':' x is an input to the graph, and so it should not be inplaced','line_number':3065,'multiline':False]
['text':' in the torch.add(x, x) call','line_number':3066,'multiline':False]
['text':' self.tensor cannot be inplaced, however x can,','line_number':3076,'multiline':False]
['text':' and bc add is commutative we can reverse inputs to add_','line_number':3077,'multiline':False]
