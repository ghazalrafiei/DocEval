['text':' Owner(s): ["oncall: jit"]','line_number':1,'multiline':False]
['text':' backward_fn expects 2 inputs: (grad_output, current_grad_r)','line_number':33,'multiline':False]
['text':' current_grad_r is provided because we need to add this contribution','line_number':34,'multiline':False]
['text':' to grad_r when we return it.','line_number':35,'multiline':False]
['text':' check behavior with defined tensor','line_number':38,'multiline':False]
['text':' expect 3 tensors: grad_y, grad_a, grad_b','line_number':42,'multiline':False]
['text':' now test with undefined grad_out','line_number':47,'multiline':False]
['text':' expect all of them to be None','line_number':50,'multiline':False]
['text':' outputs should require_grad only if eager outputs would require_grad.','line_number':57,'multiline':False]
['text':' the value "r" is used twice, by gammaln and by entr, so it is profiled twice.','line_number':73,'multiline':False]
['text':' So during autodiff graph formation the profile nodes are unmerged because','line_number':74,'multiline':False]
['text':' they are aliasing. Then the DifferentiableGraph doesn't have a profile','line_number':75,'multiline':False]
['text':' node on the output. The requires_grad info should then be added onto the','line_number':76,'multiline':False]
['text':' output value (otherwise autodiff will make the output require_grad).','line_number':77,'multiline':False]
['text':' Note: this relies on gammaln and entr not having autodiff implementations.','line_number':78,'multiline':False]
['text':' same as above, but also add a CallFunction in between.','line_number':98,'multiline':False]
