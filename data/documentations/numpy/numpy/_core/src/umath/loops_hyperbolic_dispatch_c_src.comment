['text':'@targets
 ** $maxopt baseline
 ** (avx2 fma3) AVX512_SKX
 ** vsx2 vsx4
 ** neon_vfpv4
 ** vx vxe
 *','line_number':1,'multiline':True]
['text':' native support','line_number':13,'multiline':False]
['text':'
 * NOTE: The following implementation of tanh(f32, f64) have been converted from
 * Intel SVML to universal intrinsics, and the original code can be found in:
 *
 * - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_d_la.s
 * - https://github.com/numpy/SVML/blob/main/linux/avx512/svml_z0_tanh_s_la.s
 *
 * ALGORITHM DESCRIPTION:
 *
 *   NOTE: Since the hyperbolic tangent function is odd
 *         (tanh(x) = -tanh(-x)), below algorithm deals with the absolute
 *         value of the argument |x|: tanh(x) = sign(x) * tanh(|x|)
 *
 *   We use a table lookup method to compute tanh(|x|).
 *   The basic idea is to split the input range into a number of subintervals
 *   and to approximate tanh(.) with a polynomial on each of them.
 *
 *   IEEE SPECIAL CONDITIONS:
 *   x = [+,-]0, r = [+,-]0
 *   x = +Inf,   r = +1
 *   x = -Inf,   r = -1
 *   x = QNaN,   r = QNaN
 *   x = SNaN,   r = QNaN
 *
 *
 *  ALGORITHM DETAILS
 *
 *  SVML handle |x| > HUGE_THRESHOLD, INF and NaNs by scalar callout as following:
 *  1. check special cases
 *  2. return `+-1` for `|x| > HUGE_THRESHOLD`  otherwise return `x`
 *
 *  It wasn't clear to us the reason behind using callout instead of using
 *  AVX512 directly for single-precision.
 *  However, we saw it's better to use SIMD instead of following SVML.
 *
 *  Main path computations are organized as follows:
 *  Actually we split the interval [0, SATURATION_THRESHOLD)
 *  into a number of subintervals.  On each subinterval we approximate tanh(.)
 *   with a minimax polynomial of pre-defined degree. Polynomial coefficients
 *  are computed beforehand and stored in table. We also use
 *
 *       y := |x| + B,
 *
 *  here B depends on subinterval and is used to make argument
 *   closer to zero.
 *   We also add large fake interval [SATURATION_THRESHOLD, HUGE_THRESHOLD],
 *   where 1.0 + 0.0*y + 0.0*y^2 ... coefficients are stored - just to
 *   preserve main path computation logic but return 1.0 for all arguments.
 *
 *   Hence reconstruction looks as follows:
 *   we extract proper polynomial and range reduction coefficients
 *        (Pj and B), corresponding to subinterval, to which |x| belongs,
 *        and return
 *
 *       r := sign(x) * (P0 + P1 * y + ... + Pn * y^n)
 *
 *   NOTE: we use multiprecision technique to multiply and sum the first
 *         K terms of the polynomial. So Pj, j = 0..K are stored in
 *         table each as a pair of target precision numbers (Pj and PLj) to
 *         achieve wider than target precision.
 *
 ','line_number':14,'multiline':True]
['text':' For architectures without efficient gather / scatter instructions, it is','line_number':78,'multiline':False]
['text':' better to use a transposed LUT where we can load all coefficients for an','line_number':79,'multiline':False]
['text':' index linearly.  In order to keep the same vertical calculation, we','line_number':80,'multiline':False]
['text':' transpose the coef. into lanes.  2 lane transpose is all that's','line_number':81,'multiline':False]
['text':' implemented so we require `npyv_nlanes_f64` == 2.','line_number':82,'multiline':False]
['text':' npyv_nlanes_f64 == 2','line_number':85,'multiline':False]
['text':' 0','line_number':92,'multiline':False]
['text':' b,   c0,  c1,  c2','line_number':93,'multiline':False]
['text':' c3,  c4,  c5,  c6','line_number':94,'multiline':False]
['text':' c7,  c8,  c9,  c10','line_number':95,'multiline':False]
['text':' c11, c12, c13, c14','line_number':96,'multiline':False]
['text':' c15, c16','line_number':97,'multiline':False]
['text':' 1','line_number':98,'multiline':False]
['text':' 2','line_number':104,'multiline':False]
['text':' 3','line_number':110,'multiline':False]
['text':' 4','line_number':116,'multiline':False]
['text':' b,   c0,  c1,  c2','line_number':117,'multiline':False]
['text':' c3,  c4,  c5,  c6','line_number':118,'multiline':False]
['text':' c7,  c8,  c9,  c10','line_number':119,'multiline':False]
['text':' c11, c12, c13, c14','line_number':120,'multiline':False]
['text':' c15, c16','line_number':121,'multiline':False]
['text':' 5','line_number':122,'multiline':False]
['text':' 6','line_number':128,'multiline':False]
['text':' 7','line_number':134,'multiline':False]
['text':' 8','line_number':140,'multiline':False]
['text':' b,   c0,  c1,  c2','line_number':141,'multiline':False]
['text':' c3,  c4,  c5,  c6','line_number':142,'multiline':False]
['text':' c7,  c8,  c9,  c10','line_number':143,'multiline':False]
['text':' c11, c12, c13, c14','line_number':144,'multiline':False]
['text':' c15, c16','line_number':145,'multiline':False]
['text':' 9','line_number':146,'multiline':False]
['text':' 10','line_number':152,'multiline':False]
['text':' 11','line_number':158,'multiline':False]
['text':' 12','line_number':164,'multiline':False]
['text':' b,   c0,  c1,  c2','line_number':165,'multiline':False]
['text':' c3,  c4,  c5,  c6','line_number':166,'multiline':False]
['text':' c7,  c8,  c9,  c10','line_number':167,'multiline':False]
['text':' c11, c12, c13, c14','line_number':168,'multiline':False]
['text':' c15, c16','line_number':169,'multiline':False]
['text':' 13','line_number':170,'multiline':False]
['text':' 14','line_number':176,'multiline':False]
['text':' 15','line_number':182,'multiline':False]
['text':' 0','line_number':191,'multiline':False]
['text':' 1','line_number':196,'multiline':False]
['text':' 2','line_number':201,'multiline':False]
['text':' 3','line_number':206,'multiline':False]
['text':' 4','line_number':211,'multiline':False]
['text':' 5','line_number':216,'multiline':False]
['text':' 6','line_number':221,'multiline':False]
['text':' 7','line_number':226,'multiline':False]
['text':' 8','line_number':231,'multiline':False]
['text':' 9','line_number':236,'multiline':False]
['text':' 10','line_number':241,'multiline':False]
['text':' 11','line_number':246,'multiline':False]
['text':' 12','line_number':251,'multiline':False]
['text':' 13','line_number':256,'multiline':False]
['text':' 14','line_number':261,'multiline':False]
['text':' 15','line_number':266,'multiline':False]
['text':' 16','line_number':271,'multiline':False]
['text':' 17','line_number':276,'multiline':False]
['text':' defined(TANH_TRANSPOSED_LUT)','line_number':282,'multiline':False]
['text':' |x| > HUGE_THRESHOLD, INF and NaNs.','line_number':294,'multiline':False]
['text':' no native 64-bit for max/min and its fine to use 32-bit max/min','line_number':298,'multiline':False]
['text':' since we're not crossing 32-bit edge','line_number':299,'multiline':False]
['text':'*begin repeat
         * #off= 0, 2,  4,  6,  8,  10, 12,  14,  16#
         * #e0 = b, c1, c3, c5, c7, c9, c11, c13, c15#
         * #e1 = c0,c2, c4, c6, c8, c10,c12, c14, c16#
         ','line_number':309,'multiline':True]
['text':'*begin repeat1
         * #lane = 0, 1#
         ','line_number':314,'multiline':True]
['text':'*end repeat1*','line_number':318,'multiline':True]
['text':'*end repeat*','line_number':321,'multiline':True]
['text':' defined(TANH_TRANSPOSED_LUT)','line_number':341,'multiline':False]
['text':' no need to zerofy nans or avoid FP exceptions by NO_EXC like SVML does','line_number':343,'multiline':False]
['text':' since we're clearing the FP status anyway.','line_number':344,'multiline':False]
['text':' 1.0 if |x| > HUGE_THRESHOLD || INF','line_number':363,'multiline':False]
['text':' qnan if nan','line_number':366,'multiline':False]
['text':' NPY_SIMD_F64','line_number':378,'multiline':False]
['text':' For architectures without efficient gather / scatter instructions, it is','line_number':383,'multiline':False]
['text':' better to use a transposed LUT where we can load all coefficients for an','line_number':384,'multiline':False]
['text':' index linearly.  In order to keep the same vertical calculation, we','line_number':385,'multiline':False]
['text':' transpose the coef. into lanes.  A 4x4 transpose is all that's','line_number':386,'multiline':False]
['text':' supported so we require `npyv_nlanes_f32` == 4.','line_number':387,'multiline':False]
['text':' Define missing universal intrinsics used below','line_number':390,'multiline':False]
['text':' !defined(npyv_get_lane_u32)','line_number':401,'multiline':False]
['text':' npyv_nlanes_f32 == 4','line_number':402,'multiline':False]
['text':' c6       c5          c4          c3          c2          c1          c0          b','line_number':409,'multiline':False]
['text':' 4','line_number':414,'multiline':False]
['text':' 8','line_number':419,'multiline':False]
['text':' 12','line_number':424,'multiline':False]
['text':' 16','line_number':429,'multiline':False]
['text':' 20','line_number':434,'multiline':False]
['text':' 24','line_number':439,'multiline':False]
['text':' 28','line_number':444,'multiline':False]
['text':' 0','line_number':452,'multiline':False]
['text':' 1','line_number':457,'multiline':False]
['text':' 2','line_number':462,'multiline':False]
['text':' 3','line_number':467,'multiline':False]
['text':' 4','line_number':472,'multiline':False]
['text':' 5','line_number':477,'multiline':False]
['text':' 6','line_number':482,'multiline':False]
['text':' 7','line_number':487,'multiline':False]
['text':' defined(TANHF_TRANSPOSED_LUT)','line_number':493,'multiline':False]
['text':' check |x| > HUGE_THRESHOLD, INF and NaNs.','line_number':505,'multiline':False]
['text':'*begin repeat
         * #lane = 0, 1, 2, 3#
         ','line_number':518,'multiline':True]
['text':'*end repeat*','line_number':524,'multiline':True]
['text':' lane0: {c6, c5, c4, c3},  {c2, c1, c0, b}','line_number':526,'multiline':False]
['text':' lane1: {c6, c5, c4, c3},  {c2, c1, c0, b}','line_number':527,'multiline':False]
['text':' lane2: {c6, c5, c4, c3},  {c2, c1, c0, b}','line_number':528,'multiline':False]
['text':' lane3: {c6, c5, c4, c3},  {c2, c1, c0, b}','line_number':529,'multiline':False]
['text':'','line_number':530,'multiline':False]
['text':' transposed:','line_number':531,'multiline':False]
['text':' c6: {lane0, lane1, lane2, lane3}','line_number':532,'multiline':False]
['text':' c5: {lane0, lane1, lane2, lane3}','line_number':533,'multiline':False]
['text':' c4: {lane0, lane1, lane2, lane3}','line_number':534,'multiline':False]
['text':' c3: {lane0, lane1, lane2, lane3}','line_number':535,'multiline':False]
['text':' c2: {lane0, lane1, lane2, lane3}','line_number':536,'multiline':False]
['text':' c1: {lane0, lane1, lane2, lane3}','line_number':537,'multiline':False]
['text':' c0: {lane0, lane1, lane2, lane3}','line_number':538,'multiline':False]
['text':' b : {lane0, lane1, lane2, lane3}','line_number':539,'multiline':False]
['text':' defined(TANHF_TRANSPOSED_LUT)','line_number':563,'multiline':False]
['text':' no need to zerofy nans or avoid FP exceptions by NO_EXC like SVML does','line_number':565,'multiline':False]
['text':' since we're clearing the FP status anyway.','line_number':566,'multiline':False]
['text':' 1.0 if |x| > HUGE_THRESHOLD || INF','line_number':575,'multiline':False]
['text':' qnan if nan','line_number':578,'multiline':False]
['text':' NPY_SIMD_F32','line_number':594,'multiline':False]
['text':' NPY_SIMD_FMA3','line_number':595,'multiline':False]
['text':'*begin repeat
 * #TYPE = FLOAT, DOUBLE#
 * #type = float, double#
 * #sfx  = f32,   f64#
 * #ssfx = f,     #
 * #simd = NPY_SIMD_FMA3 && NPY_SIMD_F32, NPY_SIMD_FMA3 && NPY_SIMD_F64#
 ','line_number':597,'multiline':True]
['text':'*begin repeat1
 *  #func = tanh#
 *  #simd_req_clear = 1#
 ','line_number':604,'multiline':True]
['text':'*end repeat1*','line_number':640,'multiline':True]
['text':'*end repeat*','line_number':641,'multiline':True]
