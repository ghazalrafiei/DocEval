['text':'*
 * the NumPy C SIMD vectorization interface "NPYV" are types and functions intended
 * to simplify vectorization of code on different platforms, currently supports
 * the following SIMD extensions SSE, AVX2, AVX512, VSX and NEON.
 *
 * TODO: Add an independent sphinx doc.
','line_number':3,'multiline':True]
['text':'
 * clang commit an aggressive optimization behaviour when flag `-ftrapping-math`
 * isn't fully supported that's present at -O1 or greater. When partially loading a
 * vector register for a operations that requires to fill up the remaining lanes
 * with certain value for example divide operation needs to fill the remaining value
 * with non-zero integer to avoid fp exception divide-by-zero.
 * clang optimizer notices that the entire register is not needed for the store
 * and optimizes out the fill of non-zero integer to the remaining
 * elements. As workaround we mark the returned register with `volatile`
 * followed by symmetric operand operation e.g. `or`
 * to convince the compiler that the entire vector is needed.
 ','line_number':21,'multiline':True]
['text':'
 * Avoid using any of the following intrinsics with MSVC 32-bit,
 * even if they are apparently work on newer versions.
 * They had bad impact on the generated instructions,
 * sometimes the compiler deal with them without the respect
 * of 32-bit mode which lead to crush due to execute 64-bit
 * instructions and other times generate bad emulated instructions.
 ','line_number':40,'multiline':True]
['text':' lane type by intrin suffix','line_number':59,'multiline':False]
['text':' TODO: Add support for VSX(2.06) and BE Mode for VSX','line_number':79,'multiline':False]
['text':'/ SIMD width in bits or 0 if there's no SIMD extension available.','line_number':89,'multiline':False]
['text':'/ SIMD width in bytes or 0 if there's no SIMD extension available.','line_number':91,'multiline':False]
['text':'/ 1 if the enabled SIMD extension supports single-precision otherwise 0.','line_number':93,'multiline':False]
['text':'/ 1 if the enabled SIMD extension supports double-precision otherwise 0.','line_number':95,'multiline':False]
['text':'/ 1 if the enabled SIMD extension supports native FMA otherwise 0.','line_number':97,'multiline':False]
['text':'/ note: we still emulate(fast) FMA intrinsics even if they','line_number':98,'multiline':False]
['text':'/ aren't supported but they shouldn't be used if the precision is matters.','line_number':99,'multiline':False]
['text':'/ 1 if the enabled SIMD extension is running on big-endian mode otherwise 0.','line_number':101,'multiline':False]
['text':'/ 1 if the supported comparison intrinsics(lt, le, gt, ge)','line_number':103,'multiline':False]
['text':'/ raises FP invalid exception for quite NaNs.','line_number':104,'multiline':False]
['text':' enable emulated mask operations for all SIMD extension except for AVX512','line_number':108,'multiline':False]
['text':' enable integer divisor generator for all SIMD extensions','line_number':113,'multiline':False]
['text':'*
 * Some SIMD extensions currently(AVX2, AVX512F) require (de facto)
 * a maximum number of strides sizes when dealing with non-contiguous memory access.
 *
 * Therefore the following functions must be used to check the maximum
 * acceptable limit of strides before using any of non-contiguous load/store intrinsics.
 *
 * For instance:
 *  npy_intp ld_stride = step[0] / sizeof(float);
 *  npy_intp st_stride = step[1] / sizeof(float);
 *
 *  if (npyv_loadable_stride_f32(ld_stride) && npyv_storable_stride_f32(st_stride)) {
 *      for (;;)
 *          npyv_f32 a = npyv_loadn_f32(ld_pointer, ld_stride);
 *          // ...
 *          npyv_storen_f32(st_pointer, st_stride, a);
 *  }
 *  else {
 *      for (;;)
 *          // C scalars
 *  }
 ','line_number':118,'multiline':True]
['text':' _NPY_SIMD_H_','line_number':171,'multiline':False]
