['text':'-
 * Public Domain 2014-present MongoDB, Inc.
 * Public Domain 2008-2014 WiredTiger, Inc.
 *
 * This is free and unencumbered software released into the public domain.
 *
 * Anyone is free to copy, modify, publish, use, compile, sell, or
 * distribute this software, either in source code form or as a compiled
 * binary, for any purpose, commercial or non-commercial, and by any
 * means.
 *
 * In jurisdictions that recognize copyright laws, the author or authors
 * of this software dedicate any and all copyright interest in the
 * software to the public domain. We make this dedication for the benefit
 * of the public at large and to the detriment of our heirs and
 * successors. We intend this dedication to be an overt act of
 * relinquishment in perpetuity of all present and future rights to this
 * software under copyright law.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 ','line_number':1,'multiline':True]
['text':'
 * Predictable replay is the ability to do test runs multiple times and always have predictable
 * changes made at every timestamp. Two predictable runs with the same starting data seed executed
 * up to the same timestamp will always have their data compare identically. Predictable replay only
 * works with timestamped transactions and to avoid complexity, only a single operation is allowed
 * in a transaction.
 *
 * To achieve the predictability we use two random number generators (the data RNG and the extra
 * RNG) with known start seeds, the data seed and the extra seed. Every single-threaded modification
 * (like bulk loading) when deciding on a random course, uses the global data RNG, which is seeded
 * by the data seed. Global decisions that don't affect data, like whether to turn on verbose, or
 * even the rate of checkpointing, use the global extra RNG, which is seeded by the extra seed.
 * Changing the extra seed may change some characteristics of how a workload is tested, but should
 * not change any data on disk. When worker threads run, they have their own data and extra RNGs,
 * and these are seeded by the timestamp they are working on.
 *
 * Before a worker thread can decide on what operation to do on which key in which table, it must
 * obtain the next timestamp. Timestamps are doled out atomically, so no two worker threads can ever
 * perform operations using the same timestamp. The timestamp is XOR-ed with the data seed, the
 * result is the seed of the thread's private data RNG for the duration of that operation. Likewise,
 * a private extra RNG is seeded from the timestamp and the extra seed. This ensures that all
 * decisions about what is committed at that timestamp are predictable based on the timestamp. As
 * you might expect, the thread's data RNG is used to decide what operation to do, which table to
 * use, and which key within the table. Other random decisions, like whether to reopen a session, or
 * whether to repeat a read from the snap list, use the extra RNG.
 *
 * Note that once a thread has started to work on an operation at a timestamp, it cannot give up on
 * the effort. If, for example, a rollback error naturally happens, we can rollback the transaction.
 * However, immediately getting a new timestamp would mean that we would lose the consequences of
 * the previous timestamp, perhaps a record would not be updated in a particular way. Thus, after a
 * rollback, a thread starts again, using the same timestamp it had before, and it seeds its RNGs
 * again using this timestamp. This gives full predictability, even in the face of temporary
 * failures.
 *
 * To avoid the possibility that two threads work on the same key at the same time, we have the
 * concept of lanes, and only one thread can be working in a lane at once. There are LANE_COUNT
 * lanes, where LANE_COUNT is 2^k for some k. A thread uses a data RNG to choose the top bits of a
 * key number, but the bottom k bits of the key number are set to the bottom k bits of the timestamp
 * being worked. Those bottom k bits also determine the lane we are in. Each lane has a flag that
 * determines whether the lane is in use by some operation. If thread T1 working an operation at
 * timestamp X takes a sufficiently long time relative to other operations, it may be that the
 * current timestamp has advanced to X + LANE_COUNT. If that is the case, a different thread T2 that
 * gets that larger timestamp will see that the lane is occupied. Rather than using that timestamp
 * and potentially getting the same key number, the T2 leaves that timestamp, knowing that T1 will
 * do it, and advances to another timestamp to work on. When T1 finishes its long operation, it will
 * notice if there are other timestamps that have been left for it. If so, it keeps the lane
 * occupied, and works on the new timestamp. At some point, it will notice that all the timestamps
 * in the lane have been processed up to that point, and it can release the lane, and go back to
 * choosing the next available timestamp to process.
 *
 * Having some operations lag behind is a natural part of processing. This leads to a stable
 * timestamp that may lag significantly. Due to the possibility of dependencies between operations,
 * the more lag, the more chance that a rollback error occurs. Without predictable replay, this is
 * not a problem, any operation that produces a rollback can be freely abandoned, and threads
 * generally continue moving quickly ahead with more work. However, with predictable replay, no
 * operation can be abandoned, and an operation that failed because of a dependency will repeatedly
 * fail until the stable timestamp advances. For that reason, we keep calculating and moving the
 * stable timestamp ahead at a much faster pace when predictable replay is configured. We also use
 * an algorithm that only uses lanes that are in use to calculate the stable timestamp. This is safe
 * and more responsive than the default calculation. And when there is a rollback error, we try to
 * be smart whether we need to yield or pause. These modifications allow predictable performance to
 * be on par with regular performance.
 ','line_number':31,'multiline':True]
['text':'
 * replay_end_timed_run --
 *     In a timed run, get everyone to stop.
 ','line_number':95,'multiline':True]
['text':'
     * We'll post a stop timestamp that all worker threads should abide by. There's a potential race
     * between when we read the current timestamp and before we publish the stop timestamp. During
     * that time, other threads could do work and advance the current timestamp, potentially beyond
     * the intended stop timestamp. We pick a stop timestamp far enough in the future that it's
     * rather unlikely to happen.
     ','line_number':102,'multiline':True]
['text':'
 * replay_maximum_committed --
 *     For predictable replay runs, return the largest timestamp that's no longer in use.
 ','line_number':112,'multiline':True]
['text':'
     * The calculation is expensive, and does not need to be accurate all the time, and it's okay to
     * be behind. So we use a cached value most of the time.
     ','line_number':122,'multiline':True]
['text':'
 * replay_operation_enabled --
 *     Return whether an operation type should be enabled in the configuration.
 ','line_number':145,'multiline':True]
['text':'
     * We don't permit modify operations with predictable replay.
     *
     * The problem is read timestamps. As currently implemented, the read timestamp selected is
     * variable, based on the state of other threads and their progress with other timestamped
     * operations. And if two changes are made to the same key in a short amount of time, if the
     * second operation were to be performed sometimes with a read timestamp before the first
     * operation, and sometimes with a read timestamp after the first operation, then the results
     * would be variable.
     *
     * We could track recent operations on a key (in its lane, for instance), but when we realize
     * the read timestamp isn't recent enough, we would need to wait for the stable timestamp to
     * move forward (and our waiting can affect/delay other thread's operations as well). Having the
     * stable timestamp move forward is the only way our read timestamp can progress.
     *
     * Another possibility that also involves tracking recent operations on a key would be to
     * disallow modifies that occur within, say 10000 timestamps of a previous write operation on
     * the same key. Those modifies could be silently converted to reads, for instance. If our read
     * timestamp was greater than 10000 timestamps behind, we'd still need to wait for the stable
     * timestamp to catch up.
     ','line_number':155,'multiline':True]
['text':'
     * FIXME-WT-10570. We don't permit remove operations with predictable replay.
     *
     * This should be something we can and should fix. The problem may be similar to the problem
     * with modify, where having a varying read timestamp can cause different results for different
     * runs.
     ','line_number':179,'multiline':True]
['text':'
     * We don't permit truncate operations with predictable replay.
     *
     * Currently, we use an operation's timestamp to help derive the operation's key.
     * The last N bits of the timestamp are used as the last bits of the key (where
     * 2^N == LANE_COUNT). These last N bits give the lane number, and within each
     * lane we track the progress of operations for that lane. Using lanes, we can
     * track and guarantee that only a single operation is active in a lane at once,
     * and therefore we can't have multiple operations on a single key performed out
     * of order or simultaneously. The truncate operation, for a small set of keys,
     * would reserve multiple consecutive lanes (probably okay) and for larger sets,
     * would reserve the entire set of lanes. This would effectively require all
     * threads to get into a holding state, waiting for the truncate to start and then
     * complete before continuing with their next operation. While we could fudge this
     * in certain ways (e.g. operations with 10000 timestamps of a truncate would be
     * forced to stay out of its table), there still would be a lot of details, and
     * some rethink of our lane strategy. Even getting this to work, we would have
     * a truncate that had the whole table to itself, which doesn't seem like an
     * effective test.
     ','line_number':189,'multiline':True]
['text':'
 * replay_pick_timestamp --
 *     Pick the next timestamp for this operation. That timestamp is used for any commits and also
 *     determines which lane we are in, to prevent races from occurring on operations on a single
 *     key. Also, by using the timestamp to seed the random number generators, it also determines
 *     precisely the nature of the operation.
 ','line_number':215,'multiline':True]
['text':'
     * Choose a unique timestamp for commits. When we do predictable replay. If the field for
     * replaying again is set, we already have a timestamp picked for us.
     ','line_number':229,'multiline':True]
['text':'
         * Timestamp is already picked for us.
         ','line_number':234,'multiline':True]
['text':'
             * For predictable replay, this is the only place we increment the timestamp. We keep a
             * copy to check that assumption. If we were to mistakenly change the timestamp
             * elsewhere (as might be done in non-predictable runs), we would lose the integrity of
             * the predictable run.
             ','line_number':250,'multiline':True]
['text':'
     * For this operation, seed the RNG used for data operations according to the timestamp and the
     * global data seed. This allows us to have a predictable set of actions related to commits at
     * this timestamp, so long as we are running with the same global data seed.
     ','line_number':272,'multiline':True]
['text':'
 * replay_loop_begin --
 *     Called at the top of the operation loop.
 ','line_number':283,'multiline':True]
['text':'
         * Predictable replay, as it works now, requires that we're not in transaction when we start
         * the loop.
         ','line_number':291,'multiline':True]
['text':'
         * We're here at the start of the loop for one of four reasons:
         *   1) We needed to rollback the transaction, so we didn't give up our replay timestamp,
         * and we set the again flag.
         *   2) We successfully committed the last transaction, but our lane was behind,
         * and was skipped over, so we're obligated to perform the next timestamp in our lane.
         * In that case, we have a replay timestamp and the again flag is set.
         *   3) We successfully committed the last transaction, and our lane was not behind.
         * We don't have a replay timestamp and the again flag is off.
         *   4) It's our first time through the loop, this is equivalent to the previous case.
         ','line_number':297,'multiline':True]
['text':'
         * Choose a unique timestamp for commits, based on the conditions above.
         ','line_number':309,'multiline':True]
['text':'
 * replay_run_reset --
 *     Called at beginning and end of runs to set up the lanes.
 ','line_number':318,'multiline':True]
['text':' Set every lane's commit timestamp to the current timestamp. ','line_number':329,'multiline':True]
['text':' Reset fields in tinfo. ','line_number':336,'multiline':True]
['text':'
 * replay_run_begin --
 *     Called at the beginning of a run.
 ','line_number':347,'multiline':True]
['text':'
 * replay_run_end --
 *     Called when finishing processing for a run.
 ','line_number':360,'multiline':True]
['text':'
 * replay_read_ts --
 *     Return a read timestamp for a begin transaction call.
 ','line_number':373,'multiline':True]
['text':'
 * replay_prepare_ts --
 *     Return a timestamp to be used for prepare.
 ','line_number':390,'multiline':True]
['text':' See if we're just starting a run. ','line_number':401,'multiline':True]
['text':'
         * When we're starting a run, we'll just use the final commit timestamp for our prepare
         * timestamp. We know that's safe.
         ','line_number':403,'multiline':True]
['text':'
         * Our lane's current operation will have a commit timestamp tinfo->replay_ts. Our lane's
         * previous commit timestamp was that number minus LANE_COUNT. The global stable timestamp
         * generally should not be advanced past our lane's previous commit timestamp. So a prepare
         * timestamp halfway between the lane's previous commit timestamp and the current commit
         * timestamp should be valid.
         ','line_number':409,'multiline':True]
['text':' As a sanity check, make sure the timestamp hasn't completely aged out. ','line_number':418,'multiline':True]
['text':'
 * replay_commit_ts --
 *     Return the commit timestamp.
 ','line_number':427,'multiline':True]
['text':'
 * replay_committed --
 *     Called when a transaction was successfully committed. We can give up a lane if appropriate.
 ','line_number':440,'multiline':True]
['text':'
     * Updating the last commit timestamp for a lane in use allows read, oldest and stable
     * timestamps to advance.
     ','line_number':458,'multiline':True]
['text':'
 * replay_adjust_key --
 *     Given a fully random key number, modify the key that is in our lane.
 ','line_number':474,'multiline':True]
['text':'
 * replay_rollback --
 *     Called after a rollback.
 ','line_number':497,'multiline':True]
['text':'
     * After a rollback, we don't give up our timestamp or our lane, we need to retry at the top of
     * the operations loop.
     ','line_number':507,'multiline':True]
['text':'
 * replay_pause_after_rollback --
 *     Called after a rollback, allowing us to yield or pause.
 ','line_number':518,'multiline':True]
['text':' Generally, the more behind we are, the less we want to wait. ','line_number':530,'multiline':True]
['text':' If we're in the furthest group behind, don't wait at all. ','line_number':535,'multiline':True]
['text':'
     * If we're in the last half, don't sleep. If we're in the front half, occasionally sleep.
     ','line_number':539,'multiline':True]
['text':' Never sleep more than .1 seconds ','line_number':545,'multiline':True]
