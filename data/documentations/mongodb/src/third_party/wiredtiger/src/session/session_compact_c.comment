['text':'-
 * Copyright (c) 2014-present MongoDB, Inc.
 * Copyright (c) 2008-2014 WiredTiger, Inc.
 *	All rights reserved.
 *
 * See the file LICENSE for redistribution information.
 ','line_number':1,'multiline':True]
['text':'
 * Compaction is the place where the underlying block manager becomes visible
 * in the higher engine btree and API layers.  As there is currently only one
 * block manager, this code is written with it in mind: other block managers
 * may need changes to support compaction, and a smart block manager might need
 * far less support from the engine.
 *
 * First, the default block manager cannot entirely own compaction because it
 * has no way to find a block after it moves other than a request from the
 * btree layer with the new address.  In other words, if internal page X points
 * to leaf page Y, and page Y moves, the address of page Y has to be updated in
 * page X.  Generally, this is solved by building a translation layer in the
 * block manager so internal pages don't require updates to relocate blocks:
 * however, the translation table must be durable, has its own garbage
 * collection issues and might be slower, all of which have their own problems.
 *
 * Second, the btree layer cannot entirely own compaction because page
 * addresses are opaque, it cannot know where a page is in the file from the
 * address cookie.
 *
 * For these reasons, compaction is a cooperative process between the btree
 * layer and the block manager.  The btree layer walks files, and asks the
 * block manager if rewriting a particular block would reduce the file
 * footprint: if writing the page will help, the page is marked dirty so it
 * will eventually be written.  As pages are written, the original page
 * potentially becomes available for reuse and if enough pages at the end of
 * the file are available for reuse, the file can be truncated, and compaction
 * succeeds.
 *
 * However, writing a page is not by itself sufficient to make a page available
 * for reuse.  The original version of the page is still referenced by at least
 * the most recent checkpoint in the file.  To make a page available for reuse,
 * we have to checkpoint the file so we can discard the checkpoint referencing
 * the original version of the block; once no checkpoint references a block, it
 * becomes available for reuse.
 *
 * Compaction is not necessarily possible in WiredTiger, even in a file with
 * lots of available space.  If a block at the end of the file is referenced by
 * a named checkpoint, there is nothing we can do to compact the file, no
 * matter how many times we rewrite the block, the named checkpoint can't be
 * discarded and so the reference count on the original block will never go to
 * zero. What's worse, because the block manager doesn't reference count
 * blocks, it can't easily know this is the case, and so we'll waste a lot of
 * effort trying to compact files that can't be compacted.
 *
 * Finally, compaction checkpoints are database-wide, otherwise we can corrupt
 * file relationships, for example, an index checkpointed by compaction could
 * be out of sync with the primary after a crash.
 *
 * Now, to the actual process.  First, we checkpoint the database: there are
 * potentially many dirty blocks in the cache, and we want to write them out
 * and then discard previous checkpoints so we have as many blocks as possible
 * on the file's "available for reuse" list when we start compaction.
 *
 * Then, we compact the high-level object.
 *
 * Compacting the object is done 10% at a time, that is, we try and move blocks
 * from the last 10% of the file into the beginning of the file (the 10% is
 * hard coded in the block manager).  The reason for this is because we are
 * walking the file in logical order, not block offset order, and we can fail
 * to compact a file if we write the wrong blocks first.
 *
 * For example, imagine a file with 10 blocks in the first 10% of a file, 1,000
 * blocks in the 3rd quartile of the file, and 10 blocks in the last 10% of the
 * file.  If we were to rewrite blocks from more than the last 10% of the file,
 * and found the 1,000 blocks in the 3rd quartile of the file first, we'd copy
 * 10 of them without ever rewriting the blocks from the end of the file which
 * would allow us to compact the file.  So, we compact the last 10% of the
 * file, and if that works, we compact the last 10% of the file again, and so
 * on.  Note the block manager uses a first-fit block selection algorithm
 * during compaction to maximize block movement.
 *
 * After each 10% compaction, we checkpoint two more times (seriously, twice).
 * The second and third checkpoints are because the block manager checkpoints
 * in two steps: blocks made available for reuse during a checkpoint are put on
 * a special checkpoint-available list and only moved to the real available
 * list after the metadata has been updated with the new checkpoint's
 * information.  (Otherwise it is possible to allocate a rewritten block, crash
 * before the metadata is updated, and see corruption.)  For this reason,
 * blocks allocated to write the checkpoint itself cannot be taken from the
 * blocks made available by the checkpoint.
 *
 * To say it another way, the second checkpoint puts the blocks from the end of
 * the file that were made available by compaction onto the checkpoint-available
 * list, but then potentially writes the checkpoint itself at the end of the
 * file, which would prevent any file truncation.  When the metadata is updated
 * for the second checkpoint, the blocks freed by compaction become available
 * for the third checkpoint, so the third checkpoint's blocks are written
 * towards the beginning of the file, and then the file can be truncated. Since
 * the second checkpoint made the btree clean, mark it as dirty again to ensure
 * the third checkpoint rewrites blocks too. Otherwise, the btree is skipped.
 ','line_number':11,'multiline':True]
['text':'
 * __compact_start --
 *     Start object compaction.
 ','line_number':104,'multiline':True]
['text':'
 * __compact_end --
 *     End object compaction.
 ','line_number':117,'multiline':True]
['text':'
 * __compact_uri_analyze --
 *     Extract information relevant to deciding what work compact needs to do from a URI that is
 *     part of a table schema. Called via the schema_worker function.
 ','line_number':130,'multiline':True]
['text':'
     * Add references to schema URI objects to the list of objects to be compacted. Skip over LSM
     * trees or we will get false positives on the "file:" URIs for the chunks.
     ','line_number':138,'multiline':True]
['text':'
 * __compact_handle_append --
 *     Gather a file handle to be compacted. Called via the schema_worker function.
 ','line_number':153,'multiline':True]
['text':' Set compact active on the handle. ','line_number':168,'multiline':True]
['text':' Make sure there is space for the next entry. ','line_number':174,'multiline':True]
['text':'
 * __session_compact_check_timeout --
 *     Check if the timeout has been exceeded.
 ','line_number':182,'multiline':True]
['text':'
 * __wt_session_compact_check_interrupted --
 *     Check if compaction has been interrupted. Foreground compaction can be interrupted through an
 *     event handler while background compaction can be disabled at any time using the compact API.
 ','line_number':210,'multiline':True]
['text':' If compaction has been interrupted, we return WT_ERROR to the caller. ','line_number':226,'multiline':True]
['text':'
         * Always log a warning when:
         * - Interrupting foreground compaction
         * - Interrupting background compaction and the connection is not being closed/open.
         * Otherwise, it is expected to potentially interrupt background compaction and should not
         * be exposed as a warning.
         ','line_number':243,'multiline':True]
['text':' Compaction can be interrupted if the timeout has exceeded. ','line_number':257,'multiline':True]
['text':'
 * __compact_checkpoint --
 *     This function waits and triggers a checkpoint.
 ','line_number':263,'multiline':True]
['text':' Checkpoints may take a lot of time, check if compaction has been interrupted. ','line_number':272,'multiline':True]
['text':'
 * __compact_worker --
 *     Function to alternate between checkpoints and compaction calls.
 ','line_number':279,'multiline':True]
['text':'
     * Reset the handles' compaction skip flag (we don't bother setting or resetting it when we
     * finish compaction, it's simpler to do it once, here).
     ','line_number':290,'multiline':True]
['text':' Perform an initial checkpoint (see this file's leading comment for details). ','line_number':297,'multiline':True]
['text':'
     * We compact 10% of a file on each pass (but the overall size of the file is decreasing each
     * time, so we're not compacting 10% of the original file each time). Try 100 times (which is
     * clearly more than we need); quit if we make no progress.
     ','line_number':300,'multiline':True]
['text':' Step through the list of files being compacted. ','line_number':306,'multiline':True]
['text':' Skip objects where there's no more work. ','line_number':308,'multiline':True]
['text':'
             * If successful and we did work, schedule another pass. If successful and we did no
             * work, skip this file in the future.
             ','line_number':319,'multiline':True]
['text':'
             * If compaction failed because checkpoint was running, continue with the next handle.
             * We might continue to race with checkpoint on each handle, but that's OK, we'll step
             * through all the handles, and then we'll block until a checkpoint completes.
             *
             * Just quit if eviction is the problem.
             ','line_number':332,'multiline':True]
['text':'
         * Perform two checkpoints. Mark the trees impacted by compaction to ensure the last
         * checkpoint processes them (see this file's leading comment for details).
         ','line_number':358,'multiline':True]
['text':'
 * __wt_compact_check_eligibility --
 *     Function to check whether the specified URI is eligible for compaction.
 ','line_number':374,'multiline':True]
['text':' Tiered tables cannot be compacted. ','line_number':383,'multiline':True]
['text':'
 * __wt_session_compact --
 *     WT_SESSION.compact method.
 ','line_number':390,'multiline':True]
['text':' Trigger the background server. ','line_number':410,'multiline':True]
['text':'
         * We shouldn't provide any other configurations when explicitly disabling the background
         * compaction server.
         ','line_number':415,'multiline':True]
['text':'
     * The compaction thread should not block when the cache is full: it is holding locks blocking
     * checkpoints and once the cache is full, it can spend a long time doing eviction.
     ','line_number':453,'multiline':True]
['text':' In-memory ignores compaction operations. ','line_number':462,'multiline':True]
['text':'
     * Non-LSM object compaction requires checkpoints, which are impossible in transactional
     * contexts. Disallow in all contexts (there's no reason for LSM to allow this, possible or
     * not), and check now so the error message isn't confusing.
     ','line_number':469,'multiline':True]
['text':' Disallow objects in the WiredTiger name space. ','line_number':476,'multiline':True]
['text':' Check the file is eligible for compaction. ','line_number':491,'multiline':True]
['text':' Setup the session handle's compaction state structure. ','line_number':495,'multiline':True]
['text':' Configure the minimum amount of space recoverable. ','line_number':499,'multiline':True]
['text':' Compaction can be time-limited. ','line_number':503,'multiline':True]
['text':'
     * Find the types of data sources being compacted. This could involve opening indexes for a
     * table, so acquire the table lock in write mode.
     ','line_number':509,'multiline':True]
['text':'
     * Release common session resources (for example, checkpoint may acquire significant
     * reconciliation structures/memory).
     ','line_number':536,'multiline':True]
['text':' Map prepare-conflict to rollback. ','line_number':551,'multiline':True]
['text':'
 * __wt_session_compact_readonly --
 *     WT_SESSION.compact method; readonly version.
 ','line_number':559,'multiline':True]
