['text':'-
 * Copyright (c) 2014-present MongoDB, Inc.
 * Copyright (c) 2008-2014 WiredTiger, Inc.
 *	All rights reserved.
 *
 * See the file LICENSE for redistribution information.
 ','line_number':1,'multiline':True]
['text':'
 * __btree_clear --
 *     Clear a Btree, either on handle discard or re-open.
 ','line_number':17,'multiline':True]
['text':'
     * If the tree hasn't gone through an open/close cycle, there's no cleanup to be done.
     ','line_number':29,'multiline':True]
['text':' Close the Huffman tree. ','line_number':35,'multiline':True]
['text':' Terminate any associated collator. ','line_number':38,'multiline':True]
['text':' Destroy locks. ','line_number':42,'multiline':True]
['text':' Free allocated memory. ','line_number':46,'multiline':True]
['text':'
 * __wt_btree_open --
 *     Open a Btree.
 ','line_number':53,'multiline':True]
['text':'
     * This may be a re-open, clean up the btree structure. Clear the fields that don't persist
     * across a re-open. Clear all flags other than the operation flags (which are set by the
     * connection handle software that called us).
     ','line_number':74,'multiline':True]
['text':' Set the data handle first, our called functions reasonably use it. ','line_number':83,'multiline':True]
['text':' Checkpoint and verify files are readonly. ','line_number':86,'multiline':True]
['text':' Get the checkpoint information for this name/checkpoint pair. ','line_number':91,'multiline':True]
['text':' Set the order number. ','line_number':94,'multiline':True]
['text':'
     * Bulk-load is only permitted on newly created files, not any empty file -- see the checkpoint
     * code for a discussion.
     ','line_number':97,'multiline':True]
['text':' Handle salvage configuration. ','line_number':105,'multiline':True]
['text':' Initialize and configure the WT_BTREE structure. ','line_number':112,'multiline':True]
['text':' Connect to the underlying block manager. ','line_number':115,'multiline':True]
['text':'
     * !!!
     * As part of block-manager configuration, we need to return the maximum
     * sized address cookie that a block manager will ever return.  There's
     * a limit of WT_BTREE_MAX_ADDR_COOKIE, but at 255B, it's too large for
     * a Btree with 512B internal pages.  The default block manager packs
     * a wt_off_t and 2 uint32_t's into its cookie, so there's no problem
     * now, but when we create a block manager extension API, we need some
     * way to consider the block manager's maximum cookie size versus the
     * minimum Btree internal node size.
     ','line_number':121,'multiline':True]
['text':'
     * Open the specified checkpoint unless it's a special command (special commands are responsible
     * for loading their own checkpoints, if any).
     ','line_number':134,'multiline':True]
['text':'
         * There are two reasons to load an empty tree rather than a checkpoint: either there is no
         * checkpoint (the file is being created), or the load call returns no root page (the
         * checkpoint is for an empty file).
         ','line_number':139,'multiline':True]
['text':' Warm the cache, if possible. ','line_number':151,'multiline':True]
['text':' Get the last record number in a column-store file. ','line_number':155,'multiline':True]
['text':'
     * Eviction ignores trees until the handle's open flag is set, configure eviction before that
     * happens.
     *
     * Files that can still be bulk-loaded cannot be evicted. Permanently cache-resident files can
     * never be evicted. Special operations don't enable eviction. The underlying commands may turn
     * on eviction (for example, verify turns on eviction while working a file to keep from
     * consuming the cache), but it's their decision. If an underlying command reconfigures
     * eviction, it must either clear the evict-disabled-open flag or restore the eviction
     * configuration when finished so that handle close behaves correctly.
     ','line_number':161,'multiline':True]
['text':'
 * __wt_btree_close --
 *     Close a Btree.
 ','line_number':188,'multiline':True]
['text':'
     * The close process isn't the same as discarding the handle: we might re-open the handle, which
     * isn't a big deal, but the backing blocks for the handle may not yet have been discarded from
     * the cache, and eviction uses WT_BTREE structure elements. Free backing resources but leave
     * the rest alone, and we'll discard the structure when we discard the data handle.
     *
     * Handles can be closed multiple times, ignore all but the first.
     ','line_number':201,'multiline':True]
['text':'
     * Verify the history store state. If the history store is open and this btree has history store
     * entries, it can't be a metadata file, nor can it be the history store file.
     ','line_number':213,'multiline':True]
['text':' Clear the saved checkpoint information. ','line_number':221,'multiline':True]
['text':'
     * If we turned eviction off and never turned it back on, do that now, otherwise the counter
     * will be off.
     ','line_number':224,'multiline':True]
['text':' Discard any underlying block manager resources. ','line_number':233,'multiline':True]
['text':' Unload the checkpoint, unless it's a special command. ','line_number':237,'multiline':True]
['text':' Close the underlying block manager reference. ','line_number':241,'multiline':True]
['text':'
 * __wt_btree_discard --
 *     Discard a Btree.
 ','line_number':248,'multiline':True]
['text':'
 * __wt_btree_config_encryptor --
 *     Return an encryptor handle based on the configuration.
 ','line_number':267,'multiline':True]
['text':'
     * We do not use __wt_config_gets_none here because "none" and the empty string have different
     * meanings. The empty string means inherit the system encryption setting and "none" means this
     * table is in the clear even if the database is encrypted.
     ','line_number':279,'multiline':True]
['text':'
 * __btree_conf --
 *     Configure a WT_BTREE structure.
 ','line_number':301,'multiline':True]
['text':' Dump out format information. ','line_number':321,'multiline':True]
['text':' Get the file ID. ','line_number':331,'multiline':True]
['text':' Validate file types and check the data format plan. ','line_number':335,'multiline':True]
['text':' Row-store key comparison. ','line_number':348,'multiline':True]
['text':' Column-store: check for fixed-size data. ','line_number':358,'multiline':True]
['text':' Page sizes ','line_number':370,'multiline':True]
['text':'
     * Turn on logging when it's enabled in the database and not disabled for the tree. Timestamp
     * behavior is described by the logging configurations for historical reasons; logged objects
     * imply commit-level durability and ignored timestamps, not-logged objects imply checkpoint-
     * level durability and supported timestamps. In-memory configurations default to ignoring all
     * timestamps, and the application uses the logging configuration flag to turn on timestamps.
     ','line_number':389,'multiline':True]
['text':'
     * The metadata isn't blocked by in-memory cache limits because metadata "unroll" is performed
     * by updates that are potentially blocked by the cache-full checks.
     *
     * The metadata file ignores timestamps and is logged if at all possible.
     ','line_number':408,'multiline':True]
['text':' The history store file is never logged and supports timestamps. ','line_number':419,'multiline':True]
['text':' Get the last flush times for tiered storage, if applicable. ','line_number':431,'multiline':True]
['text':' Checksums ','line_number':445,'multiline':True]
['text':' Huffman encoding ','line_number':456,'multiline':True]
['text':'
     * Reconciliation configuration:
     *	Block compression (all)
     *	Dictionary compression (variable-length column-store, row-store)
     *	Page-split percentage
     *	Prefix compression (row-store)
     *	Suffix compression (row-store)
     ','line_number':459,'multiline':True]
['text':' FALLTHROUGH ','line_number':478,'multiline':True]
['text':'
     * Configure compression adjustment.
     * When doing compression, assume compression rates that will result in
     * pages larger than the maximum in-memory images allowed. If we're
     * wrong, we adjust downward (but we're almost certainly correct, the
     * maximum in-memory images allowed are only 4x the maximum page size,
     * and compression always gives us more than 4x).
     *	Don't do compression adjustment for fixed-size column store, the
     * leaf page sizes don't change. (We could adjust internal pages but not
     * internal pages, but that seems an unlikely use case.)
     ','line_number':488,'multiline':True]
['text':'
         * Don't do compression adjustment when on-disk page sizes are less than 16KB. There's not
         * enough compression going on to fine-tune the size, all we end up doing is hammering
         * shared memory.
         *
         * Don't do compression adjustment when on-disk page sizes are equal to the maximum
         * in-memory page image, the bytes taken for compression can't grow past the base value.
         ','line_number':505,'multiline':True]
['text':' Configure encryption. ','line_number':523,'multiline':True]
['text':' Configure read-only. ','line_number':526,'multiline':True]
['text':' Initialize locks. ','line_number':531,'multiline':True]
['text':' Clean ','line_number':535,'multiline':True]
['text':' Not syncing ','line_number':537,'multiline':True]
['text':' Checkpoint generation ','line_number':538,'multiline':True]
['text':'
     * The first time we open a btree, we'll be initializing the write gen to the connection-wide
     * base write generation since this is the largest of all btree write generations from the
     * previous run. This has the nice property of ensuring that the range of write generations used
     * by consecutive runs do not overlap which aids with debugging.
     *
     * If we're reopening a btree or importing a new one to a running system, the btree write
     * generation from the last run may actually be ahead of the connection-wide base write
     * generation. In that case, we should initialize our write gen just ahead of our btree specific
     * write generation.
     *
     * The runtime write generation is important since it's going to determine what we're going to
     * use as the base write generation (and thus what pages to wipe transaction ids from). The idea
     * is that we want to initialize it once the first time we open the btree during a run and then
     * for every subsequent open, we want to reuse it. This so that we're still able to read
     * transaction ids from the previous time a btree was open in the same run.
     ','line_number':540,'multiline':True]
['text':' If this is the first time opening the tree this run. ','line_number':560,'multiline':True]
['text':'
     * In recovery use the last checkpointed run write generation number as base write generation
     * number to reset the transaction ids of the pages that were modified before the restart. The
     * transaction ids are retained only on the pages that are written after the restart.
     *
     * Rollback to stable does not operate on logged tables and metadata, so it is skipped.
     *
     * The only scenarios where the checkpoint run write generation number is less than the
     * connection last checkpoint base write generation number are when rollback to stable doesn't
     * happen during the recovery due to the unavailability of history store file, or when reading a
     * checkpoint.
     ','line_number':566,'multiline':True]
['text':'
     * We've just overwritten the runtime write generation based off the fact that know that we're
     * importing and therefore, the checkpoint data's runtime write generation is meaningless. We
     * need to ensure that the underlying dhandle doesn't get discarded without being included in a
     * subsequent checkpoint including the new overwritten runtime write generation. Otherwise,
     * we'll reopen, won't know that we're in the import case and will incorrectly use the old
     * system's runtime write generation.
     ','line_number':585,'multiline':True]
['text':'
 * __wt_root_ref_init --
 *     Initialize a tree root reference, and link in the root page.
 ','line_number':599,'multiline':True]
['text':' Used in a macro for diagnostic builds ','line_number':606,'multiline':True]
['text':'
 * __wt_btree_tree_open --
 *     Read in a tree from disk.
 ','line_number':618,'multiline':True]
['text':'
     * A buffer into which we read a root page; don't use a scratch buffer, the buffer's allocated
     * memory becomes the persistent in-memory page.
     ','line_number':635,'multiline':True]
['text':'
     * Read and verify the page (verify to catch encrypted objects we can't decrypt, where we read
     * the object successfully but we can't decrypt it, and we want to fail gracefully).
     *
     * Create a printable version of the address to pass to verify.
     ','line_number':641,'multiline':True]
['text':'
     * Flag any failed read or verification: if we're in startup, it may be fatal.
     ','line_number':653,'multiline':True]
['text':'
     * Failure to open metadata means that the database is unavailable. Try to provide a helpful
     * failure message.
     ','line_number':661,'multiline':True]
['text':'
     * Build the in-memory version of the page. Clear our local reference to the allocated copy of
     * the disk image on return, the in-memory object steals it.
     ','line_number':676,'multiline':True]
['text':' Finish initializing the root, root reference links. ','line_number':684,'multiline':True]
['text':'
 * __btree_tree_open_empty --
 *     Create an empty in-memory tree.
 ','line_number':694,'multiline':True]
['text':'
     * Newly created objects can be used for cursor inserts or for bulk loads; set a flag that's
     * cleared when a row is inserted into the tree.
     ','line_number':711,'multiline':True]
['text':'
     * A note about empty trees: the initial tree is a single root page. It has a single reference
     * to a leaf page, marked deleted. The leaf page will be created by the first update. If the
     * root is evicted without being modified, that's OK, nothing is ever written.
     *
     * !!!
     * Be cautious about changing the order of updates in this code: to call __wt_page_out on error,
     * we require a correct page setup at each point where we might fail.
     ','line_number':718,'multiline':True]
['text':' Bulk loads require a leaf page for reconciliation: create it now. ','line_number':757,'multiline':True]
['text':' Finish initializing the root, root reference links. ','line_number':766,'multiline':True]
['text':'
 * __wt_btree_new_leaf_page --
 *     Create an empty leaf page.
 ','line_number':779,'multiline':True]
['text':'
     * When deleting a chunk of the name-space, we can delete internal pages. However, if we are
     * ever forced to re-instantiate that piece of the namespace, it comes back as a leaf page.
     * Reset the WT_REF type as it's possible that it has changed.
     ','line_number':802,'multiline':True]
['text':'
 * __btree_preload --
 *     Pre-load internal pages.
 ','line_number':813,'multiline':True]
['text':' Pre-load the second-level internal pages. ','line_number':832,'multiline':True]
['text':'
 * __btree_get_last_recno --
 *     Set the last record number for a column-store. Note that this is used to handle appending to
 *     a column store after a truncate operation. It is not related to the WT_CURSOR::largest_key
 *     API.
 ','line_number':847,'multiline':True]
['text':'
     * The last record number is used to support appending to a column store tree that has had a
     * final page truncated. Since checkpoint trees are read-only they don't need the value.
     ','line_number':863,'multiline':True]
['text':'
     * The endpoint for append is global; read the last page with global visibility (even if it's
     * deleted) to make sure that if the end of the tree is truncated, the tree walk finds the
     * correct page. (Note that this path does not examine the visibility of individual data items;
     * it only checks whether whole pages are deleted.)
     ','line_number':872,'multiline':True]
['text':'
 * __btree_page_sizes --
 *     Verify the page sizes. Some of these sizes are automatically checked using limits defined in
 *     the API, don't duplicate the logic here.
 ','line_number':892,'multiline':True]
['text':'
     * Get the allocation size. Allocation sizes must be a power-of-two, nothing else makes sense.
     ','line_number':911,'multiline':True]
['text':'
     * Get the internal/leaf page sizes. All page sizes must be in units of the allocation size.
     ','line_number':918,'multiline':True]
['text':'
     * FLCS leaf pages have a lower size limit than the default, because the size configures the
     * bitmap data size and the timestamp data adds on to that. Each time window can be up to 63
     * bytes and the total page size must not exceed 4G. Thus for an 8t table there can be 64M
     * entries (so 64M of bitmap data and up to 63*64M == 4032M of time windows), less a bit for
     * headers. For a 1t table there can be (64 7/8)M entries because the bitmap takes less space,
     * but that corresponds to a configured page size of a bit over 8M. Consequently the absolute
     * limit on the page size is 8M, but since pages this large make no sense and perform poorly
     * even if they don't get bloated out with timestamp data, we'll cut down by a factor of 16 and
     * set the limit to 128KB.
     ','line_number':929,'multiline':True]
['text':'
     * Default in-memory page image size for compression is 4x the maximum internal or leaf page
     * size, and enforce the on-disk page sizes as a lower-limit for the in-memory image size.
     ','line_number':943,'multiline':True]
['text':'
     * Don't let pages grow large compared to the cache size or we can end
     * up in a situation where nothing can be evicted.  Make sure at least
     * 10 pages fit in cache when it is at the dirty trigger where threads
     * stall.
     *
     * Take care getting the cache size: with a shared cache, it may not
     * have been set.  Don't forget to update the API documentation if you
     * alter the bounds for any of the parameters here.
     ','line_number':958,'multiline':True]
['text':' Enforce a lower bound of a single disk leaf page ','line_number':976,'multiline':True]
['text':'
     * Try in-memory splits once we hit 80% of the maximum in-memory page size. This gives
     * multi-threaded append workloads a better chance of not stalling.
     ','line_number':979,'multiline':True]
['text':'
     * Get the split percentage (reconciliation splits pages into smaller than the maximum page size
     * chunks so we don't split every time a new entry is added). Determine how large newly split
     * pages will be. Set to the minimum, if the read value is less than that.
     ','line_number':985,'multiline':True]
['text':'
     * In-memory split configuration.
     ','line_number':1000,'multiline':True]
['text':'
     * Get the maximum internal/leaf page key/value sizes.
     *
     * In-memory configuration overrides any key/value sizes, there's no such thing as an overflow
     * item in an in-memory configuration.
     ','line_number':1014,'multiline':True]
['text':'
     * Default max for leaf keys: split-page / 10. Default max for leaf values: split-page / 2.
     *
     * It's difficult for applications to configure this in any exact way as they have to duplicate
     * our calculation of how many keys must fit on a page, and given a split-percentage and page
     * header, that isn't easy to do.
     ','line_number':1031,'multiline':True]
['text':'
 * __wt_btree_switch_object --
 *     Switch to a writeable object for a tiered btree.
 ','line_number':1046,'multiline':True]
['text':' If the btree is readonly, there is nothing to do. ','line_number':1057,'multiline':True]
['text':'
     * When initially opening a tiered Btree, a tier switch is done internally without the btree
     * being fully opened. That's okay, the btree will be told later about the current object
     * number.
     ','line_number':1061,'multiline':True]
