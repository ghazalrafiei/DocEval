['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]
['text':' This value is initialized only if the node is running as a config server','line_number':145,'multiline':False]
['text':'*
 * Runs the BatchedCommandRequest 'request' on namespace 'nss' It transforms the request to BSON
 * and then uses a DBDirectClient to run the command locally.
 ','line_number':173,'multiline':True]
['text':'startTransaction','line_number':196,'multiline':True]
['text':' Swap out the clients in order to get a fresh opCtx. Previous operations in this transaction','line_number':207,'multiline':False]
['text':' that have been run on this opCtx would have set the timeout in the locker on the opCtx, but','line_number':208,'multiline':False]
['text':' commit should not have a lock timeout.','line_number':209,'multiline':False]
['text':' Runs commit for the transaction with 'txnNumber'.','line_number':246,'multiline':False]
['text':' Runs abort for the transaction with 'txnNumber'.','line_number':255,'multiline':False]
['text':' It is safe to ignore write concern errors in the presence of a NoSuchTransaction command','line_number':261,'multiline':False]
['text':' error because the transaction being aborted was both generated by and run locally on this','line_number':262,'multiline':False]
['text':' replica set primary. The NoSuchTransaction decision couldn't end up being rolled back.','line_number':263,'multiline':False]
['text':' unique ','line_number':305,'multiline':True]
['text':' creates a vector of a vector of BSONObj (one for each batch) from the docs vector','line_number':314,'multiline':False]
['text':' each batch can only be as big as the maximum BSON Object size and be below the maximum','line_number':315,'multiline':False]
['text':' document count','line_number':316,'multiline':False]
['text':' creates a vector of a vector of BSONObj (one for each batch) from the docs vector','line_number':322,'multiline':False]
['text':' each batch can only be as big as the maximum BSON Object size and be below the maximum','line_number':323,'multiline':False]
['text':' document count','line_number':324,'multiline':False]
['text':'collator','line_number':355,'multiline':True]
['text':' pipeline ','line_number':360,'multiline':True]
['text':' Compose the pipeline to generate a NamespacePlacementType for each existing collection and
     * database in the cluster based on the content of the sharding catalog.
     *
     * 1. Join config.collections with config.chunks to extract
     * - the collection name and uuid
     * - the list of shards containing one or more chunks of the collection
     * - the timestamp of the most recent collection chunk migration received by each shard
     *
     * 2. Project the output to
     * - select the most recent collection chunk migration across shards (using initTimestamp as a
     *   fallback in case no timestamp could be retrieved on stage 1)
     * - fit each document to the  NamespacePlacementType schema
     *
     * 3. Add to the previous results a projection of the config.databases entries that fits the
     *    NamespacePlacementType schema
     *
     * 4. merge everything into config.placementHistory.
     *
     db.collections.aggregate([
     {
         $lookup: {
         from: "chunks",
         localField: "uuid",
         foreignField: "uuid",
         as: "timestampByShard",
         pipeline: [
             {
              $group: {
                 _id: "$shard",
                 value: {
                 $max: "$onCurrentShardSince"
                 }
             }
             }
         ],
         }
     },
     {
         $project: {
         _id: 0,
         nss: "$_id",
         shards: "$timestampByShard._id",
         uuid: 1,
         timestamp: {
             $ifNull: [
             {
                 $max: "$timestampByShard.timestamp"
             },
             <initTimestamp>
             ]
         },
         }
     },
     {
         $unionWith: {
          coll: "databases",
          pipeline: [
             {
             $project: {
                 _id: 0,
                 nss: "$_id",
                 shards: [
                 "$primary"
                 ],
                 timestamp: "$version.timestamp"
             }
             }
         ]
         }
     },
     {
         $merge:
         {
             into: "config.placementHistory",
             on: ["nss", "timestamp"],
             whenMatched: "replace",
             whenNotMatched: "insert"
         }
     }
     ])
     ','line_number':401,'multiline':True]
['text':' Aliases for the field names of the the final projections','line_number':488,'multiline':False]
['text':' Stage 1. Join config.collections and config.chunks using the collection UUID to create the','line_number':501,'multiline':False]
['text':' placement-by-shard info documents','line_number':502,'multiline':False]
['text':' Stage 2. Adapt the info on collections to the config.placementHistory entry format','line_number':519,'multiline':False]
['text':' Get the most recent collection placement timestamp among all the shards: if not found,','line_number':521,'multiline':False]
['text':' apply initTimestamp as a fallback.','line_number':522,'multiline':False]
['text':' Stage 3 Add placement info on each database of the cluster','line_number':533,'multiline':False]
['text':' Stage 4. Merge into the placementHistory collection','line_number':544,'multiline':False]
['text':'
     * The initialization metadata of config.placementHistory is composed by two special docs,
     * identified by kConfigPlacementHistoryInitializationMarker:
     * - initializationTimeInfo: contains the time of the initialization and an empty set of shards.
     *   It will allow ShardingCatalogClient to serve accurate responses to historical placement
     *   queries within the [initializationTime, +inf) range.
     * - approximatedPlacementForPreInitQueries:  contains the cluster topology at the time of the
     *   initialization and is marked with Timestamp(0,1).
     *   It will be used by ShardingCatalogClient to serve approximated responses to historical
     *   placement queries within the [-inf, initializationTime) range.
     ','line_number':561,'multiline':True]
['text':' Delete the current initialization metadata','line_number':589,'multiline':False]
['text':' Insert the new initialization metadata','line_number':603,'multiline':False]
['text':' resourceYielder ','line_number':626,'multiline':True]
['text':' namespace','line_number':633,'multiline':False]
['text':' Make sure to write config.version last since we detect rollbacks of config.version and','line_number':733,'multiline':False]
['text':' will re-run initializeConfigDatabaseIfNeeded if that happens, but we don't detect rollback','line_number':734,'multiline':False]
['text':' of the index builds.','line_number':735,'multiline':False]
['text':'unique','line_number':817,'multiline':True]
['text':'*
 * Ensure that config.collections exists upon configsvr startup
 ','line_number':827,'multiline':True]
['text':' Ensure that config.collections exist so that snapshot reads on it don't fail with','line_number':831,'multiline':False]
['text':' SnapshotUnavailable error when it is implicitly created (when sharding a','line_number':832,'multiline':False]
['text':' collection for the first time) but not in yet in the committed snapshot).','line_number':833,'multiline':False]
['text':' create returns error NamespaceExists if collection already exists','line_number':839,'multiline':False]
['text':' TODO (SERVER-83264): Move new validator to _initConfigSettings and remove old validator once 8.0','line_number':848,'multiline':False]
['text':' becomes last LTS.','line_number':849,'multiline':False]
['text':' create returns error NamespaceExists if collection already exists','line_number':882,'multiline':False]
['text':' Collection already exists, create validator on that collection','line_number':889,'multiline':False]
['text':' No shards should be added until we have forwarded featureCompatibilityVersion to all shards.','line_number':901,'multiline':False]
['text':' We do a direct read of the shards collection with local readConcern so no shards are missed,','line_number':904,'multiline':False]
['text':' but don't go through the ShardRegistry to prevent it from caching data that may be rolled','line_number':905,'multiline':False]
['text':' back.','line_number':906,'multiline':False]
['text':' The config server will run shard upgrade/downgrade tasks directly instead of sending','line_number':919,'multiline':False]
['text':' a command to itself.','line_number':920,'multiline':False]
['text':' The zone doesn't exists.','line_number':965,'multiline':False]
['text':' The last shard that belongs to this zone is a different shard.','line_number':977,'multiline':False]
['text':' Nothing to be notified.','line_number':1004,'multiline':False]
['text':' Setup an AlternativeClientRegion and a non-interruptible Operation Context to ensure that','line_number':1008,'multiline':False]
['text':' the notification may be also sent out while the node is stepping down.','line_number':1009,'multiline':False]
['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':1013,'multiline':False]
['text':' Compose the request and decorate it with the needed write concern and auth parameters.','line_number':1022,'multiline':False]
['text':' send cmd','line_number':1030,'multiline':False]
['text':'throwOnError','line_number':1037,'multiline':True]
['text':'
         * The notification is considered succesful when at least one instantiation of the command
         * is succesfully completed, assuming that:
         * - each recipient of the notification is reacting with the emission of an entry in its
         * oplog before returning an OK status
         * - other processes interested in events of new database creations (e.g, a mongos that
         * serves a change stream targeting the namespace being created) are tailing the oplogs of
         * all the shards of the cluster.
         *
         * If all the failures reported by the remote nodes are classified as retryable, an error
         * code of the same category will be returned back to the caller of this function to allow
         * the re-execution of the original request.
         *
         * (Failures caused by recipients running a legacy FCV are ignored).
         ','line_number':1070,'multiline':True]
['text':' startTransaction ','line_number':1106,'multiline':True]
['text':' if the operation is in a transaction then the overhead for each document is different.','line_number':1120,'multiline':False]
['text':'startTransaction','line_number':1158,'multiline':True]
['text':' resourceYielder ','line_number':1193,'multiline':True]
['text':' Begin the transaction with a noop find.','line_number':1198,'multiline':False]
['text':' We retry on transient transaction errors like LockTimeout and detect whether','line_number':1245,'multiline':False]
['text':' asr.opCtx() was killed by explicitly checking if it has been interrupted.','line_number':1246,'multiline':False]
['text':' We stop retrying on ErrorCategory::NotPrimaryError and ErrorCategory::ShutdownError','line_number':1250,'multiline':False]
['text':' exceptions because it is expected for another attempt on this same server to keep','line_number':1251,'multiline':False]
['text':' receiving that error.','line_number':1252,'multiline':False]
['text':' hasWriteConcernError ','line_number':1262,'multiline':True]
['text':' isCommitOrAbort ','line_number':1262,'multiline':True]
['text':' isCommitOrAbort ','line_number':1279,'multiline':True]
['text':' commitTransaction() specifies {writeConcern: {w: "majority"}} without a wtimeout, so','line_number':1291,'multiline':False]
['text':' it isn't expected to have a write concern error unless the primary is stepping down','line_number':1292,'multiline':False]
['text':' or shutting down or asr.opCtx() is killed. We throw because all of those cases are','line_number':1293,'multiline':False]
['text':' terminal for the caller running a local replica set transaction anyway.','line_number':1294,'multiline':False]
['text':' Simulates the case described in the above comment where the transaction commits, but','line_number':1297,'multiline':False]
['text':' fails to replicate due to some interruption.','line_number':1298,'multiline':False]
['text':'*
     * This function will establish an initialization time to collect a consistent description of
     * the placement of each existing namespace through a snapshot read of the sharding catalog.
     * Such description will then be persisted in config.placementHistory.
     *
     * Concurrently, sharding DDL operations and chunk may also commit - and insert new documents
     * into config.placementHistory if they alter the distribution of the targeted namespace. All
     * these writes operations are not supposed to collide, since:
     * - initializePlacementHistory() will make use of the config time to access already
     *   majority-committed information
     * - incoming (or not yet materialized) DDLs will insert more recent placement information,
     *   which will have the effect of "updating" the snapshot produced by this function.
     ','line_number':1312,'multiline':True]
['text':' Suspend the periodic cleanup job that runs in background.','line_number':1327,'multiline':False]
['text':' Delete any existing document that has been already majority committed.','line_number':1333,'multiline':False]
['text':' Set the time of the initialization.','line_number':1360,'multiline':False]
['text':' Ensure isolation from concurrent add/removeShards while the initializationTime is','line_number':1366,'multiline':False]
['text':' set. Also, retrieve the content of config.shards (it will later form part of the','line_number':1367,'multiline':False]
['text':' metadata describing the initialization of config.placementHistor).','line_number':1368,'multiline':False]
['text':' Setup and run the aggregation that will perform the snapshot read of the sharding catalog and','line_number':1392,'multiline':False]
['text':' persist its output into config.placementHistory.','line_number':1393,'multiline':False]
['text':' (This operation includes a $merge stage writing into the config database, which requires','line_number':1394,'multiline':False]
['text':' internal client credentials).','line_number':1395,'multiline':False]
['text':' TODO(SERVER-74658): Please revisit if this thread could be made killable.','line_number':1400,'multiline':False]
['text':' Failpoint to hang the operation after setting the snapshot read concern and before','line_number':1424,'multiline':False]
['text':' running the aggregation.','line_number':1425,'multiline':False]
['text':'
     * config.placementHistory has now a full representation of the cluster at initializationTime.
     * As a final step, persist also the initialization metadata so that the whole content may be
     * consistently queried.
     ','line_number':1432,'multiline':True]
['text':'
     * The method implements the following optimistic approach for data cleanup:
     * 1. Set earliestOpTime as the new initialization time of config.placementHistory;
     this will have the effect of hiding older(deletable) documents when the collection is queried
     by the ShardingCatalogClient.','line_number':1445,'multiline':True]
['text':'
     * 2. Build up and execute the delete request to remove the disposable documents. This
     * operation is not atomic and it may be interrupted by a stepdown event, but we rely on the
     * fact that the cleanup is periodically invoked to ensure that the content in excess will be
     *    eventually deleted.
     *
     * 2.1 For each namespace represented in config.placementHistory, collect the timestamp of its
     *     most recent placement doc (initialization markers are not part of the output).
     *
     *     config.placementHistory.aggregate([
     *      {
     *          $group : {
     *              _id : "$nss",
     *              mostRecentTimestamp: {$max : "$timestamp"},
     *          }
     *      },
     *      {
     *          $match : {
     *              _id : { $ne : "kConfigPlacementHistoryInitializationMarker"}
     *          }
     *      }
     *  ])
     ','line_number':1458,'multiline':True]
['text':'
     * 2.2 For each namespace found, compose a delete statement.
     ','line_number':1500,'multiline':True]
['text':'
     * Send the delete request.
     ','line_number':1533,'multiline':True]
['text':' namespace mongo','line_number':1573,'multiline':False]
