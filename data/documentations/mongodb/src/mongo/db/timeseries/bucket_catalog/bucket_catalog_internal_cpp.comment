['text':'*
 *    Copyright (C) 2020-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]
['text':' IWYU pragma: keep','line_number':50,'multiline':False]
['text':'*
 * Updates stats to reflect the status of bucket fetches and queries based off of the
 * 'ReopeningContext' (which is populated when attempting to reopen a bucket).
 ','line_number':111,'multiline':True]
['text':'*
 * Abandons the write batch and notifies any waiters that the bucket has been cleared.
 ','line_number':134,'multiline':True]
['text':' Bucket is not compressed, likely because compression failed.','line_number':146,'multiline':False]
['text':' TODO SERVER-80653: This should no longer be possible with a retry mechanism on bucket','line_number':147,'multiline':False]
['text':' compression failure.','line_number':148,'multiline':False]
['text':'*
 * Returns a prepared write batch matching the specified 'key' if one exists, by searching the set
 * of open buckets associated with 'key'.
 ','line_number':158,'multiline':True]
['text':' No open bucket for this metadata.','line_number':168,'multiline':False]
['text':'*
 * Finds an outstanding disk access operation that could conflict with reopening a bucket for the
 * series defined by 'key'. If one exists, this function returns an associated awaitable object.
 ','line_number':183,'multiline':True]
['text':' We are trying to perform a query-based reopening. This conflicts with any reopening','line_number':201,'multiline':False]
['text':' for the key.','line_number':202,'multiline':False]
['text':' We are about to attempt an archive-based reopening. This conflicts with any query-based','line_number':206,'multiline':False]
['text':' reopening for the key, or another archive-based reopening for this bucket.','line_number':207,'multiline':False]
['text':' namespace','line_number':218,'multiline':False]
['text':' Buckets are spread across independently-lockable stripes to improve parallelism. We map a','line_number':251,'multiline':False]
['text':' bucket to a stripe by hashing the BucketKey.','line_number':252,'multiline':False]
['text':' No open bucket for this metadata.','line_number':310,'multiline':False]
['text':' No open bucket for this metadata.','line_number':354,'multiline':False]
['text':' In order to potentially erase elements of the set while we iterate it (via _abort), we need','line_number':359,'multiline':False]
['text':' to advance the iterator before we call erase. This means we can't use the more','line_number':360,'multiline':False]
['text':' straightforward range iteration, and use the somewhat awkward pattern below.','line_number':361,'multiline':False]
['text':' Clean up the bucket if it has been cleared.','line_number':383,'multiline':False]
['text':' Validate the bucket document against the schema.','line_number':420,'multiline':False]
['text':' Buckets are spread across independently-lockable stripes to improve parallelism. We map a','line_number':439,'multiline':False]
['text':' bucket to a stripe by hashing the BucketKey.','line_number':440,'multiline':False]
['text':' Initialize the remaining member variables from the bucket document.','line_number':455,'multiline':False]
['text':' Populate the top-level data field names.','line_number':468,'multiline':False]
['text':' The namespace is stored two times: the bucket itself and openBucketsByKey. The bucket','line_number':503,'multiline':False]
['text':' consists of minmax and schema data so add their memory usage. Since the metadata is stored in','line_number':504,'multiline':False]
['text':' the bucket, we need to add that as well. A unique pointer to the bucket is stored once:','line_number':505,'multiline':False]
['text':' openBucketsById. A raw pointer to the bucket is stored at most twice: openBucketsByKey,','line_number':506,'multiline':False]
['text':' idleBuckets.','line_number':507,'multiline':False]
['text':' Forward the WriteConflict if the bucket has been cleared or has a pending direct write.','line_number':533,'multiline':False]
['text':' If this bucket was archived, we need to remove it from the set of archived buckets.','line_number':538,'multiline':False]
['text':' Pass ownership of the reopened bucket to the bucket catalog.','line_number':557,'multiline':False]
['text':' If we already have an open bucket for this key, we need to close it.','line_number':563,'multiline':False]
['text':' We should only have one open bucket at a time.','line_number':575,'multiline':False]
['text':' Now actually mark this bucket as open.','line_number':581,'multiline':False]
['text':' If we have an existing bucket, passing the Bucket* will let us check if the bucket was','line_number':598,'multiline':False]
['text':' cleared as part of a set since the last time it was used. If we were to just check by OID, we','line_number':599,'multiline':False]
['text':' may miss if e.g. there was a move chunk operation.','line_number':600,'multiline':False]
['text':' Avoid reusing the bucket if it conflicts with reopening.','line_number':613,'multiline':False]
['text':' It's possible to have two buckets with the same ID in different collections, so let's make','line_number':617,'multiline':False]
['text':' extra sure the existing bucket is the right one.','line_number':618,'multiline':False]
['text':' If the bucket was already open, wasn't cleared, the state didn't conflict with reopening, and','line_number':623,'multiline':False]
['text':' the namespace matches, then we can simply return it.','line_number':624,'multiline':False]
['text':' We don't actually want to roll this bucket over yet, bail out.','line_number':658,'multiline':False]
['text':' The namespace is stored two times: the bucket itself and openBucketsByKey.','line_number':688,'multiline':False]
['text':' We don't have a great approximation for the','line_number':689,'multiline':False]
['text':' _schema size, so we use initial document size minus metadata as an approximation. Since','line_number':690,'multiline':False]
['text':' the metadata itself is stored once, in the bucket, we can combine the two and just use','line_number':691,'multiline':False]
['text':' the initial document size. A unique pointer to the bucket is stored once:','line_number':692,'multiline':False]
['text':' openBucketsById. A raw pointer to the bucket is stored at most twice: openBucketsByKey,','line_number':693,'multiline':False]
['text':' idleBuckets.','line_number':694,'multiline':False]
['text':' Buckets are spread across independently-lockable stripes to improve parallelism. We map a','line_number':732,'multiline':False]
['text':' bucket to a stripe by hashing the BucketKey.','line_number':733,'multiline':False]
['text':' Save the catalog era value from before we make any further checks. This guarantees that we','line_number':736,'multiline':False]
['text':' don't miss a direct write that happens sometime in between our decision to potentially reopen','line_number':737,'multiline':False]
['text':' a bucket below, and actually reopening it in a subsequent reentrant call. Any direct write','line_number':738,'multiline':False]
['text':' will increment the era, so the reentrant call can check the current value and return a write','line_number':739,'multiline':False]
['text':' conflict if it sees a newer era.','line_number':740,'multiline':False]
['text':' Can safely clear reentrant coordination state now that we have acquired the lock.','line_number':764,'multiline':False]
['text':' First let's check the existing bucket if we have one.','line_number':776,'multiline':False]
['text':' No existing bucket to use, go ahead and try to reopen our rehydrated bucket.','line_number':786,'multiline':False]
['text':' If we had a different type of error, then we should fall through and proceed to open','line_number':810,'multiline':False]
['text':' a new bucket.','line_number':811,'multiline':False]
['text':' If we were time forward or backward, we might be able to "reopen" a bucket we still have','line_number':835,'multiline':False]
['text':' in memory that's set to be closed when pending operations finish.','line_number':836,'multiline':False]
['text':' We weren't able to insert into the other bucket, so fall through to the regular','line_number':853,'multiline':False]
['text':' reopening procedure.','line_number':854,'multiline':False]
['text':' Conflict with other prepared batch on this bucket.','line_number':883,'multiline':False]
['text':' Conflict with any query-based reopening request on this series (metaField value)','line_number':887,'multiline':False]
['text':' or an archive-based request on this bucket.','line_number':888,'multiline':False]
['text':' No other batches for this bucket are currently committing, so we can proceed.','line_number':900,'multiline':False]
['text':' We only hit this failpoint when there are conflicting operations on the same series.','line_number':908,'multiline':False]
['text':' We have to wait for someone else to finish.','line_number':911,'multiline':False]
['text':' We don't care about the result.','line_number':912,'multiline':False]
['text':' If the bucket was rolled over, then there may be a different open bucket for this metadata.','line_number':928,'multiline':False]
['text':' If we are cleaning up while archiving a bucket, then we want to preserve its state. Otherwise','line_number':942,'multiline':False]
['text':' we can remove the state from the catalog altogether.','line_number':943,'multiline':False]
['text':' When removing a closed bucket, the BucketStateRegistry may contain state for this','line_number':949,'multiline':False]
['text':' bucket due to an untracked ongoing direct write (such as TTL delete).','line_number':950,'multiline':False]
['text':' Ensure that we are in a state of pending compression (represented by a negative','line_number':958,'multiline':False]
['text':' direct write counter).','line_number':959,'multiline':False]
['text':' No state change','line_number':970,'multiline':False]
['text':' If we have an archived bucket, we still want to account for it in numberOfActiveBuckets','line_number':999,'multiline':False]
['text':' so we will increase it here since removeBucket decrements the count.','line_number':1000,'multiline':False]
['text':' We had a meta hash collision, and already have a bucket archived with the same meta hash','line_number':1004,'multiline':False]
['text':' and timestamp as this bucket. Since it's somewhat arbitrary which bucket we keep, we'll','line_number':1005,'multiline':False]
['text':' keep the one that's already archived and just plain close this one.','line_number':1006,'multiline':False]
['text':' We want to find the largest time that is not greater than info.time. Generally lower_bound','line_number':1022,'multiline':False]
['text':' will return the smallest element not less than the search value, but we are using','line_number':1023,'multiline':False]
['text':' std::greater instead of std::less for the map's comparisons. This means the order of keys','line_number':1024,'multiline':False]
['text':' will be reversed, and lower_bound will return what we want.','line_number':1025,'multiline':False]
['text':' We need to make sure our measurement can fit without violating max span. If not, we','line_number':1033,'multiline':False]
['text':' can't use this bucket.','line_number':1034,'multiline':False]
['text':' If the bucket is represented by a state in the registry, it conflicts with','line_number':1041,'multiline':False]
['text':' reopening so we can mark it as untracked to drop the state once the directWrite','line_number':1042,'multiline':False]
['text':' finishes.','line_number':1043,'multiline':False]
['text':' Synchronize concurrent disk accesses. An outstanding query-based reopening request for','line_number':1084,'multiline':False]
['text':' this series or an outstanding archived-based reopening request or prepared batch for this','line_number':1085,'multiline':False]
['text':' bucket would potentially conflict with our choice to reopen here, so we must wait for any','line_number':1086,'multiline':False]
['text':' such operation to finish and retry.','line_number':1087,'multiline':False]
['text':' Synchronize concurrent disk accesses. An outstanding reopening request or prepared batch for','line_number':1100,'multiline':False]
['text':' this series would potentially conflict with our choice to reopen here, so we must wait for','line_number':1101,'multiline':False]
['text':' any such operation to finish and retry.','line_number':1102,'multiline':False]
['text':' Derive the maximum bucket size.','line_number':1116,'multiline':False]
['text':' Before we access the bucket, make sure it's still there.','line_number':1142,'multiline':False]
['text':' Special case, bucket has already been cleared, and we need only abort this batch.','line_number':1149,'multiline':False]
['text':' Proceed to abort any unprepared batches and remove the bucket if possible','line_number':1154,'multiline':False]
['text':' Abort any unprepared batches. This should be safe since we have a lock on the stripe,','line_number':1164,'multiline':False]
['text':' preventing anyone else from using these.','line_number':1165,'multiline':False]
['text':' We shouldn't remove the bucket if there's a prepared batch outstanding','line_number':1171,'multiline':False]
['text':' and it's not the one we manage. In that case, we don't know what the','line_number':1172,'multiline':False]
['text':' user is doing with it, but we need to keep the bucket around until','line_number':1173,'multiline':False]
['text':' that batch is finished.','line_number':1174,'multiline':False]
['text':' We own the prepared batch, so we can go ahead and abort it and remove the bucket.','line_number':1177,'multiline':False]
['text':' As long as we still need space and have entries and remaining attempts, close idle buckets.','line_number':1212,'multiline':False]
['text':' Can archive a bucket if it's still eligible for insertions.','line_number':1222,'multiline':False]
['text':' Bucket was cleared and just needs to be removed from catalog.','line_number':1226,'multiline':False]
['text':' If this is the only entry, erase the whole map so we don't leave it empty.','line_number':1250,'multiline':False]
['text':' Otherwise just erase this bucket from the map.','line_number':1253,'multiline':False]
['text':' We round the measurement timestamp down to the nearest minute, hour, or day depending on the','line_number':1267,'multiline':False]
['text':' granularity. We do this for two reasons. The first is so that if measurements come in','line_number':1268,'multiline':False]
['text':' slightly out of order, we don't have to close the current bucket due to going backwards in','line_number':1269,'multiline':False]
['text':' time. The second, and more important reason, is so that we reliably group measurements','line_number':1270,'multiline':False]
['text':' together into predictable chunks for sharding. This way we know from a measurement timestamp','line_number':1271,'multiline':False]
['text':' what the bucket timestamp will be, so we can route measurements to the right shard chunk.','line_number':1272,'multiline':False]
['text':' Now, if we used the standard OID generation method for the remaining bytes we could end up','line_number':1277,'multiline':False]
['text':' with lots of bucket OID collisions. Consider the case where we have the granularity set to','line_number':1278,'multiline':False]
['text':' 'Hours'. This means we will round down to the nearest day, so any bucket generated on the','line_number':1279,'multiline':False]
['text':' same machine on the same day will have the same timestamp portion and unique instance portion','line_number':1280,'multiline':False]
['text':' of the OID. Only the increment would differ. Since we only use 3 bytes for the increment','line_number':1281,'multiline':False]
['text':' portion, we run a serious risk of overflow if we are generating lots of buckets.','line_number':1282,'multiline':False]
['text':'','line_number':1283,'multiline':False]
['text':' To address this, we'll instead use a PRNG to generate the rest of the bytes. With 8 bytes of','line_number':1284,'multiline':False]
['text':' randomness, we should have a pretty low chance of collisions. The limit of the birthday','line_number':1285,'multiline':False]
['text':' paradox converges to roughly the square root of the size of the space, so we would need a few','line_number':1286,'multiline':False]
['text':' billion buckets with the same timestamp to expect collisions. In the rare case that we do get','line_number':1287,'multiline':False]
['text':' a collision, we can (and do) simply regenerate the bucket _id at a higher level.','line_number':1288,'multiline':False]
['text':' In rare cases duplicate bucket _id fields can be generated in the same stripe and fail to be','line_number':1318,'multiline':False]
['text':' inserted. We will perform a limited number of retries to minimize the probability of','line_number':1319,'multiline':False]
['text':' collision.','line_number':1320,'multiline':False]
['text':' Make sure we set the control.min time field to match the rounded _id timestamp.','line_number':1356,'multiline':False]
['text':'metaField=','line_number':1359,'multiline':True]
['text':' The bucket does not contain any measurements that are yet to be committed, so we can take','line_number':1372,'multiline':False]
['text':' action now.','line_number':1373,'multiline':False]
['text':' We must keep the bucket around until all measurements are committed committed, just mark','line_number':1380,'multiline':False]
['text':' the action we chose now so it we know what to do when the last batch finishes.','line_number':1381,'multiline':False]
['text':' If the mode is enabled to create new buckets, then we should update stats for soft closures','line_number':1397,'multiline':False]
['text':' accordingly. If we specify the mode to not allow bucket creation, it means we are not sure if','line_number':1398,'multiline':False]
['text':' we want to soft close the bucket yet and should wait to update closure stats.','line_number':1399,'multiline':False]
['text':' In scenarios where we have a high cardinality workload and face increased cache pressure we','line_number':1420,'multiline':False]
['text':' will decrease the size of buckets before we close them.','line_number':1421,'multiline':False]
['text':' Before we hit our bucket minimum count, we will allow for large measurements to be inserted','line_number':1428,'multiline':False]
['text':' into buckets. Instead of packing the bucket to the BSON size limit, 16MB, we'll limit the max','line_number':1429,'multiline':False]
['text':' bucket size to 12MB. This is to leave some space in the bucket if we need to add new internal','line_number':1430,'multiline':False]
['text':' fields to existing, full buckets.','line_number':1431,'multiline':False]
['text':' We restrict the ceiling of the bucket max size under cache pressure.','line_number':1434,'multiline':False]
['text':' There's enough space to add this measurement and we're still below the large','line_number':1452,'multiline':False]
['text':' measurement threshold.','line_number':1453,'multiline':False]
['text':' Only increment this metric once per bucket.','line_number':1455,'multiline':False]
['text':' Remove the bucket from the bucket state registry.','line_number':1532,'multiline':False]
['text':' Remove the bucket from the bucket state registry.','line_number':1562,'multiline':False]
['text':' Remove the bucket from the bucket state registry.','line_number':1590,'multiline':False]
['text':' Check in-memory and disk state, caller still has commit rights.','line_number':1608,'multiline':False]
['text':' namespace mongo::timeseries::bucket_catalog::internal','line_number':1629,'multiline':False]
