['text':'*
 *    Copyright (C) 2018-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]
['text':' IWYU pragma: no_include "cxxabi.h"','line_number':46,'multiline':False]
['text':' Fail setMaintenanceMode with ErrorCodes::NotSecondary to simulate a concurrent election.','line_number':172,'multiline':False]
['text':' Signals that a hello request has started waiting.','line_number':175,'multiline':False]
['text':' Will cause a hello request to hang as it starts waiting.','line_number':177,'multiline':False]
['text':' Will cause a hello request to hang after it times out waiting for a topology change.','line_number':179,'multiline':False]
['text':' Skip sending heartbeats to pre-check that a quorum is available before a reconfig.','line_number':182,'multiline':False]
['text':' Will cause signal drain complete to hang right before acquiring the RSTL.','line_number':184,'multiline':False]
['text':' Will cause signal drain complete to hang before reconfig.','line_number':186,'multiline':False]
['text':' Will cause signal drain complete to hang after reconfig.','line_number':188,'multiline':False]
['text':' Will hang right after setting the currentOp info associated with an automatic reconfig.','line_number':191,'multiline':False]
['text':' Make reconfig command hang before validating new config.','line_number':193,'multiline':False]
['text':' Blocks after reconfig runs.','line_number':195,'multiline':False]
['text':' Allows skipping fetching the config from ping sender.','line_number':197,'multiline':False]
['text':' Hang after grabbing the RSTL but before we start rejecting writes.','line_number':199,'multiline':False]
['text':' Hang before making checks on the new config relative to the current one.','line_number':201,'multiline':False]
['text':' Simulates returning a specified error in the hello response.','line_number':203,'multiline':False]
['text':' Throws right before the call into recoverTenantMigrationAccessBlockers.','line_number':205,'multiline':False]
['text':' Number of times we tried to go live as a secondary.','line_number':208,'multiline':False]
['text':' Tracks the last state transition performed in this replica set.','line_number':211,'multiline':False]
['text':' Tracks the number of operations killed on state transition.','line_number':215,'multiline':False]
['text':' Tracks the number of operations left running on state transition.','line_number':218,'multiline':False]
['text':' Tracks the number of times we have successfully performed automatic reconfigs to remove','line_number':221,'multiline':False]
['text':' 'newlyAdded' fields.','line_number':222,'multiline':False]
['text':' namespace','line_number':260,'multiline':False]
['text':' Note that setting the last applied opTime forward also advances the global timestamp.','line_number':327,'multiline':False]
['text':' The oplog application phase of initial sync starts timestamping writes, causing','line_number':330,'multiline':False]
['text':' WiredTiger to pin this data in memory. Advancing the oldest timestamp in step with the','line_number':331,'multiline':False]
['text':' last applied optime here will permit WiredTiger to evict this data as it sees fit.','line_number':332,'multiline':False]
['text':' If this failpoint is set, override the default sync source retry interval for initial sync.','line_number':343,'multiline':False]
['text':' namespace','line_number':351,'multiline':False]
['text':' If this is a config server, then we set the periodic no-op interval to 1 second. This is to','line_number':396,'multiline':False]
['text':' ensure that the config server will not unduly hold up change streams running on the cluster.','line_number':397,'multiline':False]
['text':' Create necessary replication collections to guarantee that if a checkpoint sees data after','line_number':483,'multiline':False]
['text':' initial sync has completed, it also sees these collections.','line_number':484,'multiline':False]
['text':' Ensure (update if needed) the in-memory count for the oplogTruncateAfterPoint collection','line_number':487,'multiline':False]
['text':' matches the collection contents.','line_number':488,'multiline':False]
['text':' This log line is checked in unit tests.','line_number':503,'multiline':False]
['text':' Check that we have a local Rollback ID. If we do not have one, create one.','line_number':512,'multiline':False]
['text':' Read the last op from the oplog after cleaning up any partially applied batches.','line_number':561,'multiline':False]
['text':' Use a callback here, because _finishLoadLocalConfig calls isself() which requires','line_number':584,'multiline':False]
['text':' that the server's networking layer be up and running and accepting connections, which','line_number':585,'multiline':False]
['text':' doesn't happen until startReplication finishes.','line_number':586,'multiline':False]
['text':' Create a new horizon to promise mapping since it is possible for the horizons','line_number':604,'multiline':False]
['text':' to change after a replica set reconfig.','line_number':605,'multiline':False]
['text':' ourSetName can be empty if user didn't pass in --replSet. In that case, just use the local','line_number':649,'multiline':False]
['text':' config.','line_number':650,'multiline':False]
['text':' Do not check optime, if this node is an arbiter.','line_number':663,'multiline':False]
['text':' Restore the current term according to the terms of last oplog entry and last vote.','line_number':682,'multiline':False]
['text':' The initial term of OpTime() is 0.','line_number':683,'multiline':False]
['text':' Update the global timestamp before setting the last applied opTime forward so the last','line_number':692,'multiline':False]
['text':' applied optime is never greater than the latest cluster time in the logical clock.','line_number':693,'multiline':False]
['text':' Set our last applied and durable optimes to the top of the oplog, if we have one.','line_number':702,'multiline':False]
['text':' unlocks _mutex.','line_number':710,'multiline':False]
['text':' Step down is impossible, so we don't need to wait for the returned event.','line_number':717,'multiline':False]
['text':' Initial sync may take locks during startup; make sure there is no possibility of conflict.','line_number':737,'multiline':False]
['text':' Must take the lock to set _initialSyncer, but not call it.','line_number':741,'multiline':False]
['text':' InitialSyncer::startup() must be called outside lock because it uses features (eg.','line_number':779,'multiline':False]
['text':' setting the initial sync flag) which depend on the ReplicationCoordinatorImpl.','line_number':780,'multiline':False]
['text':' fallbackToLogical ','line_number':822,'multiline':True]
['text':' isMajorityDataAvailable ','line_number':842,'multiline':True]
['text':' Transition from STARTUP2 to RECOVERING and start the producer and the applier.','line_number':844,'multiline':False]
['text':' If the member state is REMOVED, this will do nothing until we receive a config with','line_number':845,'multiline':False]
['text':' ourself in it.','line_number':846,'multiline':False]
['text':' This log is used in tests to ensure we made it to this point.','line_number':852,'multiline':False]
['text':' This is not the first call.','line_number':858,'multiline':False]
['text':' Make sure we're not holding any locks; existing locks might conflict with operations','line_number':862,'multiline':False]
['text':' we take during initial sync or replication steady state startup.','line_number':863,'multiline':False]
['text':' Check to see if we need to do an initial sync.','line_number':866,'multiline':False]
['text':' Start steady replication, since we already have data.','line_number':876,'multiline':False]
['text':' ReplSetConfig has been installed, so it's either in STARTUP2 or REMOVED.','line_number':877,'multiline':False]
['text':' Set an initial sync ID, in case we were upgraded or restored from backup without doing','line_number':881,'multiline':False]
['text':' an initial sync.','line_number':882,'multiline':False]
['text':' Do initial sync.','line_number':892,'multiline':False]
['text':' Initialize the cached pointer to the oplog collection. We want to do this even as standalone','line_number':905,'multiline':False]
['text':' so accesses to the cached pointer in replica set nodes started as standalone still work','line_number':906,'multiline':False]
['text':' (mainly AutoGetOplog). In case the oplog doesn't exist, it is just initialized to null.','line_number':907,'multiline':False]
['text':' Need to perform replication recovery up to and including the given timestamp.','line_number':937,'multiline':False]
['text':' We are expected to be able to transition out of the kConfigStartingUp state by the end','line_number':952,'multiline':False]
['text':' of this function. Any uncaught exceptions here leave us in an invalid state and we will','line_number':953,'multiline':False]
['text':' not be able to shut down by normal means, as clean shutdown assumes we can leave that state.','line_number':954,'multiline':False]
['text':' If we're not done loading the config, then the config state will be set by','line_number':970,'multiline':False]
['text':' _finishLoadLocalConfig.','line_number':971,'multiline':False]
['text':' Shutting down the initial syncer early works around an issue where an initial syncer may not','line_number':1007,'multiline':False]
['text':' be able to shut down with an opCtx active.  No opCtx is active when enterTerminalShutdown is','line_number':1008,'multiline':False]
['text':' called due to a signal.','line_number':1009,'multiline':False]
['text':' Cancel any ongoing election so that the node cannot become primary once in quiesce mode,','line_number':1027,'multiline':False]
['text':' and do not wait for cancellation to complete.','line_number':1028,'multiline':False]
['text':' Increment the topology version and respond to all waiting hello requests with an error.','line_number':1034,'multiline':False]
['text':' Shutdown must:','line_number':1047,'multiline':False]
['text':' * prevent new threads from blocking in awaitReplication','line_number':1048,'multiline':False]
['text':' * wake up all existing threads blocking in awaitReplication','line_number':1049,'multiline':False]
['text':' * Shut down and join the execution resources it owns.','line_number':1050,'multiline':False]
['text':' Used to shut down outside of the lock.','line_number':1067,'multiline':False]
['text':' Wait until we are finished starting up, so that we can cleanly shut everything down.','line_number':1080,'multiline':False]
['text':' joining the replication executor is blocking so it must be run outside of the mutex','line_number':1102,'multiline':False]
['text':' We aren't currently in the set. Return 0 seconds so we can clear out the applier's','line_number':1197,'multiline':False]
['text':' queue of work.','line_number':1198,'multiline':False]
['text':' Switching to rollback should call setFollowerModeRollback instead.','line_number':1216,'multiline':False]
['text':' We were a candidate, which means _topCoord believed us to be in state RS_SECONDARY, and','line_number':1233,'multiline':False]
['text':' we know that newState != RS_SECONDARY because we would have returned early, above if','line_number':1234,'multiline':False]
['text':' the old and new state were equal. So, try again after the election is over to','line_number':1235,'multiline':False]
['text':' finish setting the follower mode.  We cannot wait for the election to finish here as we','line_number':1236,'multiline':False]
['text':' may be holding a global X lock, so we return a bad status and rely on the caller to','line_number':1237,'multiline':False]
['text':' retry.','line_number':1238,'multiline':False]
['text':' If we are switching out of SECONDARY and to ROLLBACK, we must make sure that we hold the','line_number':1247,'multiline':False]
['text':' RSTL in mode X to prevent readers that have the RSTL in intent mode from reading.','line_number':1248,'multiline':False]
['text':' This logic is a little complicated in order to avoid acquiring the RSTL in mode X','line_number':1283,'multiline':False]
['text':' unnecessarily.  This is important because the applier may call signalDrainComplete()','line_number':1284,'multiline':False]
['text':' whenever it wants, not only when the ReplicationCoordinator is expecting it.','line_number':1285,'multiline':False]
['text':'','line_number':1286,'multiline':False]
['text':' The steps are:','line_number':1287,'multiline':False]
['text':' 1.) Check to see if we're waiting for this signal.  If not, return early.','line_number':1288,'multiline':False]
['text':' 2.) Otherwise, release the mutex while acquiring the RSTL in mode X, since that might take a','line_number':1289,'multiline':False]
['text':'     while (NB there's a deadlock cycle otherwise, too).','line_number':1290,'multiline':False]
['text':' 3.) Re-check to see if we've somehow left drain mode.  If we have not, clear','line_number':1291,'multiline':False]
['text':'     producer and applier's states, set the flag allowing non-local database writes and','line_number':1292,'multiline':False]
['text':'     drop the mutex.  At this point, no writes can occur from other threads, due to the RSTL','line_number':1293,'multiline':False]
['text':'     in mode X.','line_number':1294,'multiline':False]
['text':' 4.) Drop all temp collections, and log the drops to the oplog.','line_number':1295,'multiline':False]
['text':' 5.) Log transition to primary in the oplog and set that OpTime as the floor for what we will','line_number':1296,'multiline':False]
['text':'     consider to be committed.','line_number':1297,'multiline':False]
['text':' 6.) Drop the RSTL.','line_number':1298,'multiline':False]
['text':'','line_number':1299,'multiline':False]
['text':' Because replicatable writes are forbidden while in drain mode, and we don't exit drain','line_number':1300,'multiline':False]
['text':' mode until we have the RSTL in mode X, which forbids all other threads from making','line_number':1301,'multiline':False]
['text':' writes, we know that from the time that _canAcceptNonLocalWrites is set until','line_number':1302,'multiline':False]
['text':' this method returns, no external writes will be processed.  This is important so that a new','line_number':1303,'multiline':False]
['text':' temp collection isn't introduced on the new primary before we drop all the temp collections.','line_number':1304,'multiline':False]
['text':' When we go to drop all temp collections, we must replicate the drops.','line_number':1306,'multiline':False]
['text':' Kill all user writes and user reads that encounter a prepare conflict. Also kills select','line_number':1324,'multiline':False]
['text':' internal operations. Although secondaries cannot accept writes, a step up can kill writes','line_number':1325,'multiline':False]
['text':' that were blocked behind the RSTL lock held by a step down attempt. These writes will be','line_number':1326,'multiline':False]
['text':' killed with a retryable error code during step up.','line_number':1327,'multiline':False]
['text':' Exit drain mode only if we're actually in draining mode, the apply buffer is empty in the','line_number':1332,'multiline':False]
['text':' current term, and we're allowed to become the writable primary.','line_number':1333,'multiline':False]
['text':' If the config doesn't have a term, don't change it.','line_number':1346,'multiline':False]
['text':' We re-write the term but keep version the same. This conceptually a no-op','line_number':1356,'multiline':False]
['text':' in the config consensus group, analogous to writing a new oplog entry','line_number':1357,'multiline':False]
['text':' in Raft log state machine on step up.','line_number':1358,'multiline':False]
['text':' Since we are only bumping the config term, we can skip the config replication and','line_number':1370,'multiline':False]
['text':' quorum checks in reconfig.','line_number':1371,'multiline':False]
['text':' If the node stepped down after we released the lock, we can just return.','line_number':1377,'multiline':False]
['text':' Writing this new config with a new term is somewhat "best effort", and if we get','line_number':1381,'multiline':False]
['text':' preempted by a concurrent reconfig, that is fine since that new config will have','line_number':1382,'multiline':False]
['text':' occurred after the node became primary and so the concurrent reconfig has updated','line_number':1383,'multiline':False]
['text':' the term appropriately.','line_number':1384,'multiline':False]
['text':' Must calculate the commit level again because firstOpTimeOfMyTerm wasn't set when we logged','line_number':1410,'multiline':False]
['text':' our election in onTransitionToPrimary(), above.','line_number':1411,'multiline':False]
['text':' Update _canAcceptNonLocalWrites.','line_number':1415,'multiline':False]
['text':' Update the global timestamp before setting the last applied opTime forward so the last','line_number':1443,'multiline':False]
['text':' applied optime is never greater than the latest cluster time in the logical clock.','line_number':1444,'multiline':False]
['text':' The caller may have already advanced the global timestamp, so they may request that we skip','line_number':1447,'multiline':False]
['text':' this step.','line_number':1448,'multiline':False]
['text':' In pv1, oplog entries are ordered by non-decreasing term and strictly increasing','line_number':1460,'multiline':False]
['text':' timestamp. So, in pv1, its not possible for us to get opTime with lower term and','line_number':1461,'multiline':False]
['text':' timestamp higher than or equal to our current lastAppliedOptime.','line_number':1462,'multiline':False]
['text':' Single vote primaries may have a lagged stable timestamp due to paring back the','line_number':1469,'multiline':False]
['text':' stable timestamp to the all committed timestamp.','line_number':1470,'multiline':False]
['text':' Reset to uninitialized OpTime','line_number':1498,'multiline':False]
['text':' Must do this outside _mutex','line_number':1517,'multiline':False]
['text':' The last applied opTime should never advance beyond the global timestamp (i.e. the latest','line_number':1530,'multiline':False]
['text':' cluster time). Not enforced if the logical clock is disabled, e.g. for arbiters.','line_number':1531,'multiline':False]
['text':' If we are using applied times to calculate the commit level, update it now.','line_number':1537,'multiline':False]
['text':' No need to wake up replication waiters because there should not be any replication waiters','line_number':1541,'multiline':False]
['text':' waiting on our own lastApplied.','line_number':1542,'multiline':False]
['text':' Update the storage engine's lastApplied snapshot before updating the stable timestamp on the','line_number':1544,'multiline':False]
['text':' storage engine. New transactions reading from the lastApplied snapshot should start before','line_number':1545,'multiline':False]
['text':' the oldest timestamp is advanced to avoid races. Additionally, update this snapshot before','line_number':1546,'multiline':False]
['text':' signaling optime waiters. This avoids a race that would allow optime waiters to open','line_number':1547,'multiline':False]
['text':' transactions on stale lastApplied values because they do not hold or reacquire the','line_number':1548,'multiline':False]
['text':' replication coordinator mutex when signaled.','line_number':1549,'multiline':False]
['text':' Signal anyone waiting on optime changes.','line_number':1552,'multiline':False]
['text':' Advance the stable timestamp if necessary. Stable timestamps are used to determine the latest','line_number':1563,'multiline':False]
['text':' timestamp that it is safe to revert the database to, in the event of a rollback via the','line_number':1564,'multiline':False]
['text':' 'recover to timestamp' method.','line_number':1565,'multiline':False]
['text':' If we are lagged behind the commit optime, set a new stable timestamp here.','line_number':1568,'multiline':False]
['text':' On secondary it is not possible for lastDurable to be set beyond lastApplied, because','line_number':1576,'multiline':False]
['text':' lastApplied is only updated at the completion of an oplog batch. But on primary it is','line_number':1577,'multiline':False]
['text':' possible because we only update lastApplied as part of the onCommit hook of the storage','line_number':1578,'multiline':False]
['text':' transaction, which may be delayed, but this should be fine.','line_number':1579,'multiline':False]
['text':' If we are using durable times to calculate the commit level, update it now.','line_number':1582,'multiline':False]
['text':' There could be replication waiters waiting for our lastDurable for {j: true}, wake up those','line_number':1586,'multiline':False]
['text':' that now have their write concern satisfied.','line_number':1587,'multiline':False]
['text':' nothing to wait for','line_number':1661,'multiline':False]
['text':' We should never wait for replication if we are holding any locks, because this can','line_number':1667,'multiline':False]
['text':' potentially block for long time while doing network activity.','line_number':1668,'multiline':False]
['text':' 'afterOpTime', 'afterClusterTime', and 'atClusterTime' are only supported for replica','line_number':1681,'multiline':False]
['text':' sets.','line_number':1682,'multiline':False]
['text':' We just need to wait for the opTime to catch up to what we need (not majority RC).','line_number':1718,'multiline':False]
['text':' We need to wait for all committed writes to be visible, even in the oplog (which uses','line_number':1741,'multiline':False]
['text':' special visibility rules).  We must do this after waiting for our target optime, because','line_number':1742,'multiline':False]
['text':' only then do we know that it will fill in all "holes" before that time.  If we do it','line_number':1743,'multiline':False]
['text':' earlier, we may return when the requested optime has been reached, but other writes','line_number':1744,'multiline':False]
['text':' at optimes before that time are not yet visible.','line_number':1745,'multiline':False]
['text':'','line_number':1746,'multiline':False]
['text':' We wait only on primaries, because on secondaries, other mechanisms assure that the','line_number':1747,'multiline':False]
['text':' last applied optime is always hole-free, and waiting for all earlier writes to be visible','line_number':1748,'multiline':False]
['text':' can deadlock against secondary command application.','line_number':1749,'multiline':False]
['text':'','line_number':1750,'multiline':False]
['text':' Note that oplog queries by secondary nodes depend on this behavior to wait for','line_number':1751,'multiline':False]
['text':' all oplog holes to be filled in, despite providing an afterClusterTime field','line_number':1752,'multiline':False]
['text':' with Timestamp(0,1).','line_number':1753,'multiline':False]
['text':' primaryOnly =','line_number':1754,'multiline':True]
['text':' It seems that targetOpTime can sometimes be default OpTime{}. When there is no','line_number':1799,'multiline':False]
['text':' _currentCommittedSnapshot, _getCurrentCommittedSnapshotOpTime_inlock() also returns','line_number':1800,'multiline':False]
['text':' default OpTime{}. Hence this branch only runs if _currentCommittedSnapshot actually','line_number':1801,'multiline':False]
['text':' exists.','line_number':1802,'multiline':False]
['text':' convert clusterTime to opTime so it can be used by the _opTimeWaiterList for wait on','line_number':1821,'multiline':False]
['text':' readConcern level local.','line_number':1822,'multiline':False]
['text':' We don't set isMajorityCommittedRead for transactions because snapshots are always','line_number':1826,'multiline':False]
['text':' speculative; we wait for majority when the transaction commits.','line_number':1827,'multiline':False]
['text':'','line_number':1828,'multiline':False]
['text':' Majority and snapshot reads outside of transactions should non-speculatively wait for the','line_number':1829,'multiline':False]
['text':' majority committed snapshot.','line_number':1830,'multiline':False]
['text':' TODO: remove when SERVER-29729 is done','line_number':1842,'multiline':False]
['text':' Using an uninitialized term means that this optime will be compared to other optimes only by','line_number':1857,'multiline':False]
['text':' its timestamp. This allows us to wait only on the timestamp of the commit point surpassing','line_number':1858,'multiline':False]
['text':' this timestamp, without worrying about terms.','line_number':1859,'multiline':False]
['text':' Only update committed optime if the remote optimes increased.','line_number':1936,'multiline':False]
['text':' Wait up replication waiters on optime changes.','line_number':1939,'multiline':False]
['text':' We do not count arbiters and members that aren't part of replica set config,','line_number':1969,'multiline':False]
['text':' towards the commit quorum.','line_number':1970,'multiline':False]
['text':' We do not count arbiters and members that aren't part of replica set config,','line_number':1991,'multiline':False]
['text':' towards the commit quorum.','line_number':1992,'multiline':False]
['text':' The syncMode cannot be unset.','line_number':2006,'multiline':False]
['text':' Make sure we have a valid "committed" snapshot up to the needed optime.','line_number':2024,'multiline':False]
['text':' Wait for the "current" snapshot to advance to/past the opTime.','line_number':2029,'multiline':False]
['text':' The following is an optimization when we are checking for opTime, but not config.','line_number':2041,'multiline':False]
['text':' For majority write concern, in addition to waiting for the committed snapshot to','line_number':2042,'multiline':False]
['text':' advance past the write, it also waits for a majority of nodes to have their','line_number':2043,'multiline':False]
['text':' lasDurable (j: true) or lastApplied (j: false) to advance past the write.','line_number':2044,'multiline':False]
['text':' Waiting for the committed snapshot is sufficient in most cases and so the additional','line_number':2045,'multiline':False]
['text':' wait is usually a no-op and only needed','line_number':2046,'multiline':False]
['text':' when writeConcernMajorityJournalDefault is false and j: true.','line_number':2047,'multiline':False]
['text':' When writeConcernMajorityShouldJournal is true,','line_number':2048,'multiline':False]
['text':' waiting for committedSnapshot is enough regardless of the j value.','line_number':2049,'multiline':False]
['text':' When writeConcernMajorityShouldJournal is false,','line_number':2050,'multiline':False]
['text':' committedSnapshot also cover j: false. Otherwise, fall through.','line_number':2051,'multiline':False]
['text':' At this stage all the tagged nodes should have reached the opTime, except','line_number':2056,'multiline':False]
['text':' after the reconfig(see SERVER-47205). If the OpTime is greater than','line_number':2057,'multiline':False]
['text':' _committedSnapshotAfterReconfig, check for that.','line_number':2058,'multiline':False]
['text':' Fallthrough to wait for "majority" write concern.','line_number':2069,'multiline':False]
['text':' Continue and wait for replication to the majority (of voters).','line_number':2072,'multiline':False]
['text':' It is illegal to wait for replication with a session checked out because it can lead to','line_number':2090,'multiline':False]
['text':' deadlocks.','line_number':2091,'multiline':False]
['text':' We should never wait for replication if we are holding any locks, because this can','line_number':2096,'multiline':False]
['text':' potentially block for long time while doing network activity.','line_number':2097,'multiline':False]
['text':' If we get a timeout error and the opCtx deadline is >= the writeConcern wtimeout, then we','line_number':2129,'multiline':False]
['text':' know the timeout was due to wtimeout (not opCtx deadline) and thus we return','line_number':2130,'multiline':False]
['text':' ErrorCodes::WriteConcernFailed.','line_number':2131,'multiline':False]
['text':' The returned future won't account for wTimeout or wDeadline, so reject any write concerns','line_number':2157,'multiline':False]
['text':' with either option to avoid misuse.','line_number':2158,'multiline':False]
['text':' no replication check needed (validated above)','line_number':2189,'multiline':False]
['text':' If waiting for the empty optime, always say it's been replicated.','line_number':2193,'multiline':False]
['text':' Check if the given write concern is satisfiable before we add ourself to','line_number':2229,'multiline':False]
['text':' _replicationWaiterList. On replSetReconfig, waiters that are no longer satisfiable will be','line_number':2230,'multiline':False]
['text':' notified. See _setCurrentRSConfig.','line_number':2231,'multiline':False]
['text':' We are only waiting for our own lastApplied, add this to _opTimeWaiterList instead. This','line_number':2247,'multiline':False]
['text':' is because waiters in _replicationWaiterList are not notified on self's lastApplied','line_number':2248,'multiline':False]
['text':' updates.','line_number':2249,'multiline':False]
['text':' From now on, we are either waiting for replication or local journaling. And waiters in','line_number':2253,'multiline':False]
['text':' _replicationWaiterList will be checked and notified on remote opTime updates and on self's','line_number':2254,'multiline':False]
['text':' lastDurable updates (but not on self's lastApplied updates, in which case use','line_number':2255,'multiline':False]
['text':' _opTimeWaiterList instead).','line_number':2256,'multiline':False]
['text':' If true, we know that a stepdown is underway.','line_number':2263,'multiline':False]
['text':' Clear the current metrics before setting.','line_number':2277,'multiline':False]
['text':' Turn remainingQuiesceTimeMillis into an int64 so that it's a supported BSONElement.','line_number':2309,'multiline':False]
['text':' horizonString must be passed in if we are a valid member of the config.','line_number':2328,'multiline':False]
['text':' Report that we are secondary and not accepting writes until drain completes.','line_number':2343,'multiline':False]
['text':' The client is not using awaitable hello so we respond immediately.','line_number':2373,'multiline':False]
['text':' Getting a different process id indicates that the server has restarted so we return','line_number':2380,'multiline':False]
['text':' immediately with the updated process id.','line_number':2381,'multiline':False]
['text':' The received hello command contains a stale topology version so we respond','line_number':2395,'multiline':False]
['text':' immediately with a more current topology version.','line_number':2396,'multiline':False]
['text':' An empty SNI will correspond to kDefaultHorizon.','line_number':2402,'multiline':False]
['text':' Each awaitable hello will wait on their specific horizon. We always expect horizonString','line_number':2411,'multiline':False]
['text':' to exist in _horizonToTopologyChangePromiseMap.','line_number':2412,'multiline':False]
['text':' A horizonString that is boost::none indicates that we do not have a valid config.','line_number':2436,'multiline':False]
['text':' If clientTopologyVersion is not none, deadline must also be not none.','line_number':2453,'multiline':False]
['text':' Used in tests that wait for this failpoint to be entered before triggering a topology','line_number':2461,'multiline':False]
['text':' change.','line_number':2462,'multiline':False]
['text':' Wait for a topology change with timeout set to deadline.','line_number':2470,'multiline':False]
['text':' We decrement the counter on most errors. Note that some errors may already be covered','line_number':2498,'multiline':False]
['text':' by calls to resetNumAwaitingTopologyChanges(), which sets the counter to zero, so we','line_number':2499,'multiline':False]
['text':' only decrement non-zero counters. This is safe so long as:','line_number':2500,'multiline':False]
['text':' 1) Increment + decrement calls always occur at a 1:1 ratio and in that order.','line_number':2501,'multiline':False]
['text':' 2) All callers to increment/decrement/reset take locks.','line_number':2502,'multiline':False]
['text':' Return a HelloResponse with the current topology version on timeout when waiting for','line_number':2509,'multiline':False]
['text':' a topology change.','line_number':2510,'multiline':False]
['text':' A topology change has not occured within the deadline so horizonString is still a','line_number':2512,'multiline':False]
['text':' good indicator of whether we have a valid config.','line_number':2513,'multiline':False]
['text':' A topology change has happened so we return a HelloResponse with the updated','line_number':2519,'multiline':False]
['text':' topology version.','line_number':2520,'multiline':False]
['text':' Check if the node is primary after acquiring global IS lock.','line_number':2528,'multiline':False]
['text':' About to make network and DBDirectClient (recursive) calls, so we should not hold any locks.','line_number':2561,'multiline':False]
['text':' Run the command via AsyncDBClient which performs a network call. This is also the desired','line_number':2569,'multiline':False]
['text':' behaviour when running this command locally as to avoid using the DBDirectClient which would','line_number':2570,'multiline':False]
['text':' provide additional management when trying to cancel the request with differing clients.','line_number':2571,'multiline':False]
['text':' Schedule the remote command.','line_number':2576,'multiline':False]
['text':' Wait for the response in an interruptible mode.','line_number':2588,'multiline':False]
['text':' If waiting for the response is interrupted, then we still have a callback out and','line_number':2591,'multiline':False]
['text':' registered with the TaskExecutor to run when the response finally does come back. Since','line_number':2592,'multiline':False]
['text':' the callback references local state, cbkResponse, it would be invalid for the callback to','line_number':2593,'multiline':False]
['text':' run after leaving the this function. Therefore, we cancel the callback and wait','line_number':2594,'multiline':False]
['text':' uninterruptably for the callback to be run.','line_number':2595,'multiline':False]
['text':' Don't kill step up/step down thread.','line_number':2620,'multiline':False]
['text':' The state transition should never be rollback within this class.','line_number':2643,'multiline':False]
['text':' cap deadline','line_number':2655,'multiline':False]
['text':' Wait for RSTL to be acquired.','line_number':2663,'multiline':False]
['text':' Dump all locks to identify which thread(s) are holding RSTL.','line_number':2669,'multiline':False]
['text':' Dump the stack of each thread.','line_number':2684,'multiline':False]
['text':' Set the reason for killing operations.','line_number':2718,'multiline':False]
['text':' Reset the value before killing user operations as we only want to track the number','line_number':2722,'multiline':False]
['text':' of operations that's running after step down.','line_number':2723,'multiline':False]
['text':' Destroy all stashed transaction resources, in order to release locks.','line_number':2727,'multiline':False]
['text':' Operations (like batch insert) that have currently yielded the global lock during step','line_number':2732,'multiline':False]
['text':' down can reacquire global lock in IX mode when this node steps back up after a brief','line_number':2733,'multiline':False]
['text':' network partition. And, this can lead to data inconsistency (see SERVER-27534). So,','line_number':2734,'multiline':False]
['text':' its important we mark operations killed at least once after enqueuing the RSTL lock in','line_number':2735,'multiline':False]
['text':' X mode for the first time. This ensures that no writing operations will continue','line_number':2736,'multiline':False]
['text':' after the node's term change.','line_number':2737,'multiline':False]
['text':' Ensure that we are not holding the RSTL lock in any mode.','line_number':2786,'multiline':False]
['text':' Since we have released the RSTL lock at this point, there can be some conflicting','line_number':2789,'multiline':False]
['text':' operations sneaked in here. We need to kill those operations to acquire the RSTL lock.','line_number':2790,'multiline':False]
['text':' Also, its ok to start "RstlKillOpthread" thread before RSTL lock enqueue as we kill','line_number':2791,'multiline':False]
['text':' operations in a loop.','line_number':2792,'multiline':False]
['text':' This makes us tell the 'hello' command we can't accept writes (though in fact we can,','line_number':2804,'multiline':False]
['text':' it is not valid to disable writes until we actually acquire the RSTL).','line_number':2805,'multiline':False]
['text':' Once we release the RSTL, we announce either that we can accept writes or that we're now','line_number':2812,'multiline':False]
['text':' a real secondary.','line_number':2813,'multiline':False]
['text':' Note this check is inherently racy - it's always possible for the node to stepdown from some','line_number':2828,'multiline':False]
['text':' other path before we acquire the global exclusive lock.  This check is just to try to save us','line_number':2829,'multiline':False]
['text':' from acquiring the global X lock unnecessarily.','line_number':2830,'multiline':False]
['text':' Using 'force' sets the default for the wait time to zero, which means the stepdown will','line_number':2838,'multiline':False]
['text':' fail if it does not acquire the lock immediately. In such a scenario, we use the','line_number':2839,'multiline':False]
['text':' stepDownUntil deadline instead.','line_number':2840,'multiline':False]
['text':' This will cause us to fail if we're already in the process of stepping down, or if we've','line_number':2853,'multiline':False]
['text':' already successfully stepped down via another path.','line_number':2854,'multiline':False]
['text':' Update _canAcceptNonLocalWrites from the TopologyCoordinator now that we're in the middle','line_number':2857,'multiline':False]
['text':' of a stepdown attempt.  This will prevent us from accepting writes so that if our stepdown','line_number':2858,'multiline':False]
['text':' attempt fails later we can release the RSTL and go to sleep to allow secondaries to','line_number':2859,'multiline':False]
['text':' catch up without allowing new writes in.','line_number':2860,'multiline':False]
['text':' Make sure that we leave _canAcceptNonLocalWrites in the proper state.','line_number':2868,'multiline':False]
['text':' Set up a waiter which will be signaled when we process a heartbeat or updatePosition','line_number':2899,'multiline':False]
['text':' and have a majority of nodes at our optime.','line_number':2900,'multiline':False]
['text':' If attemptStepDown() succeeds, we are guaranteed that no concurrent step up or','line_number':2904,'multiline':False]
['text':' step down can happen afterwards. So, it's safe to release the mutex before','line_number':2905,'multiline':False]
['text':' yieldLocksForPreparedTransactions().','line_number':2906,'multiline':False]
['text':' The stepdown attempt failed. We now release the RSTL to allow secondaries to read the','line_number':2910,'multiline':False]
['text':' oplog, then wait until enough secondaries are caught up for us to finish stepdown.','line_number':2911,'multiline':False]
['text':' Make sure we re-acquire the RSTL before returning so that we're always holding the','line_number':2915,'multiline':False]
['text':' RSTL when the onExitGuard set up earlier runs.','line_number':2916,'multiline':False]
['text':' Need to release _mutex before re-acquiring the RSTL to preserve lock acquisition','line_number':2918,'multiline':False]
['text':' order rules.','line_number':2919,'multiline':False]
['text':' Need to re-acquire the RSTL before re-attempting stepdown. We use no timeout here','line_number':2922,'multiline':False]
['text':' even though that means the lock acquisition could take longer than the stepdown','line_number':2923,'multiline':False]
['text':' window. Since we'll need the RSTL no matter what to clean up a failed stepdown','line_number':2924,'multiline':False]
['text':' attempt, we might as well spend whatever time we need to acquire it now.  For','line_number':2925,'multiline':False]
['text':' the same reason, we also disable lock acquisition interruption, to guarantee that','line_number':2926,'multiline':False]
['text':' we get the lock eventually.','line_number':2927,'multiline':False]
['text':' NOLINT.','line_number':2928,'multiline':False]
['text':' Since we have released the RSTL lock at this point, there can be some read','line_number':2930,'multiline':False]
['text':' operations sneaked in here, that might hold global lock in S mode or blocked on','line_number':2931,'multiline':False]
['text':' prepare conflict. We need to kill those operations to avoid 3-way deadlock','line_number':2932,'multiline':False]
['text':' between read, prepared transaction and step down thread. And, any write','line_number':2933,'multiline':False]
['text':' operations that gets sneaked in here will fail as we have updated','line_number':2934,'multiline':False]
['text':' _canAcceptNonLocalWrites to false after our first successful RSTL lock','line_number':2935,'multiline':False]
['text':' acquisition. So, we won't get into problems like SERVER-27534.','line_number':2936,'multiline':False]
['text':' If termAtStart != currentTerm, tryToStartStepDown would have thrown.','line_number':2943,'multiline':False]
['text':' As we should not wait for secondaries to catch up if this node has not yet written in','line_number':2945,'multiline':False]
['text':' this term, invariant that the lastAppliedOpTime we will wait for has the same term as the','line_number':2946,'multiline':False]
['text':' current term. Also see TopologyCoordinator::isSafeToStepDown.','line_number':2947,'multiline':False]
['text':' We ignore the case where runWithDeadline returns timeoutError because in that case','line_number':2957,'multiline':False]
['text':' coming back around the loop and calling tryToStartStepDown again will cause','line_number':2958,'multiline':False]
['text':' tryToStartStepDown to return ExceededTimeLimit with the proper error message.','line_number':2959,'multiline':False]
['text':' Prepare for unconditional stepdown success!','line_number':2965,'multiline':False]
['text':' We need to release the mutex before yielding locks for prepared transactions, which might','line_number':2966,'multiline':False]
['text':' check out sessions, to avoid deadlocks with checked-out sessions accessing this mutex.','line_number':2967,'multiline':False]
['text':' Clear the node's election candidate metrics since it is no longer primary.','line_number':2975,'multiline':False]
['text':' Schedule work to (potentially) step back up once the stepdown period has ended.','line_number':2983,'multiline':False]
['text':' If election handoff is enabled, schedule a step-up immediately instead of waiting for the','line_number':2988,'multiline':False]
['text':' election timeout to expire.','line_number':2989,'multiline':False]
['text':' For election protocol v1, call _startElectSelfIfEligibleV1 to avoid race','line_number':3042,'multiline':False]
['text':' against other elections caused by events like election timeout, replSetStepUp etc.','line_number':3043,'multiline':False]
['text':' The answer isn't meaningful unless we hold the ReplicationStateTransitionLock.','line_number':3058,'multiline':False]
['text':' _canAcceptReplicatedWrites_UNSAFE is always true for standalone nodes, and adjusted based on','line_number':3065,'multiline':False]
['text':' primary+drain state in replica sets.','line_number':3066,'multiline':False]
['text':'','line_number':3067,'multiline':False]
['text':' Stand-alone nodes and drained replica set primaries can always accept writes.  Writes are','line_number':3068,'multiline':False]
['text':' always permitted to the "local" database.','line_number':3069,'multiline':False]
['text':' namespace','line_number':3096,'multiline':False]
['text':' Writes on unreplicated collections are always permitted.','line_number':3100,'multiline':False]
['text':' Assert that we are holding the RSTL, meaning the value returned from','line_number':3104,'multiline':False]
['text':' `_canAcceptReplicatedWrites_UNSAFE` is guaranteed to be accurate.','line_number':3105,'multiline':False]
['text':' Otherwise, check whether we can currently accept replicated writes.','line_number':3107,'multiline':False]
['text':' Writes on unreplicated collections are always permitted.','line_number':3113,'multiline':False]
['text':' Otherwise, check whether we can currently accept replicated writes.','line_number':3117,'multiline':False]
['text':' Writes are never replicated for a standalone.','line_number':3127,'multiline':False]
['text':' Note that we can only reliably translate a UUID to a namespace via the collection catalog','line_number':3132,'multiline':False]
['text':' after acquiring a consistent catalog/storage snapshot. However, it is ok to skip that in','line_number':3133,'multiline':False]
['text':' this case because we do not allow renames between replicated and unreplicated collections.','line_number':3134,'multiline':False]
['text':' So even if the namespace string is incorrect, whether or not the collection is replicated','line_number':3135,'multiline':False]
['text':' will be accurate.','line_number':3136,'multiline':False]
['text':' If ns is null, we failed to find it in the catalog. In that case, we fall back to checking','line_number':3145,'multiline':False]
['text':' the DB name.','line_number':3146,'multiline':False]
['text':' Otherwise, if it is not in the catalog and not the local DB, assume it is replicated. Note','line_number':3151,'multiline':False]
['text':' this could be incorrect if it is system.profile since that is the one unreplicated collection','line_number':3152,'multiline':False]
['text':' not in the local DB.','line_number':3153,'multiline':False]
['text':' Always allow reads from the direct client, no matter what.','line_number':3170,'multiline':False]
['text':' Oplog reads are not allowed during STARTUP state, but we make an exception for internal','line_number':3175,'multiline':False]
['text':' reads. Internal reads are required for cleaning up unfinished apply batches.','line_number':3176,'multiline':False]
['text':' Non-oplog local reads from the user are not allowed during initial sync when the initial','line_number':3187,'multiline':False]
['text':' sync method disallows it.  "isFromUserConnection" means DBDirectClient reads are not blocked;','line_number':3188,'multiline':False]
['text':' "isInternalClient" means reads from other cluster members are not blocked.','line_number':3189,'multiline':False]
['text':' getInitialSyncProgress must be called outside the ReplicationCoordinatorImpl::_mutex','line_number':3288,'multiline':False]
['text':' lock. Else it might deadlock with InitialSyncer::_multiApplierCallback where it first','line_number':3289,'multiline':False]
['text':' acquires InitialSyncer::_mutex and then ReplicationCoordinatorImpl::_mutex.','line_number':3290,'multiline':False]
['text':' We need to hold the lock so that we don't run when storage is being shutdown.','line_number':3305,'multiline':False]
['text':' OpTime isn't used when checking for config replication.','line_number':3469,'multiline':False]
['text':' Note that public method processReplSetMetadata() above depends on this method not needing','line_number':3499,'multiline':False]
['text':' to do anything when the term is up to date.  If that changes, be sure to update that','line_number':3500,'multiline':False]
['text':' method as well.','line_number':3501,'multiline':False]
['text':' It is possible that we change state to or from RECOVERING. Thus, we need the RSTL in X mode.','line_number':3516,'multiline':False]
['text':' _initialSyncer must not be called with repl mutex held.','line_number':3572,'multiline':False]
['text':' If we are in the middle of an initial sync, do a resync.','line_number':3576,'multiline':False]
['text':' For election protocol v1, call _startElectSelfIfEligibleV1 to avoid race','line_number':3594,'multiline':False]
['text':' against other elections caused by events like election timeout, replSetStepUp etc.','line_number':3595,'multiline':False]
['text':' Only explicitly set configTerm to this node's term for non-force reconfigs.','line_number':3613,'multiline':False]
['text':' Otherwise, use -1.','line_number':3614,'multiline':False]
['text':' When initializing a new config through the replSetReconfig command, ignore the term','line_number':3617,'multiline':False]
['text':' field passed in through its args. Instead, use this node's term.','line_number':3618,'multiline':False]
['text':' Increase the config version for force reconfig.','line_number':3644,'multiline':False]
['text':' Set the 'newlyAdded' field to true for all new voting nodes.','line_number':3654,'multiline':False]
['text':' In a reconfig, the 'newlyAdded' flag should never already be set for','line_number':3658,'multiline':False]
['text':' this member. If it is set, throw an error.','line_number':3659,'multiline':False]
['text':' We should never set the 'newlyAdded' field for arbiters or during force reconfigs.','line_number':3670,'multiline':False]
['text':' Append the 'newlyAdded' field if the node:','line_number':3681,'multiline':False]
['text':' 1) Is a new, voting node','line_number':3682,'multiline':False]
['text':' 2) Already has a 'newlyAdded' field in the old config','line_number':3683,'multiline':False]
['text':' force ','line_number':3711,'multiline':True]
['text':' skipSafetyChecks','line_number':3711,'multiline':True]
['text':' skipSafetyChecks','line_number':3717,'multiline':True]
['text':' should be unreachable due to !_settings.isReplSet() check above','line_number':3737,'multiline':False]
['text':' Placate clang.','line_number':3738,'multiline':False]
['text':' For safety of reconfig, since we must commit a config in our own term before executing a','line_number':3763,'multiline':False]
['text':' reconfig, so we should never have a config in an older term. If the current config was','line_number':3764,'multiline':False]
['text':' installed via a force reconfig, we aren't concerned about this safety guarantee.','line_number':3765,'multiline':False]
['text':' Construct a fake OpTime that can be accepted but isn't used.','line_number':3771,'multiline':False]
['text':' Make sure that the latest committed optime from the previous config is committed in the','line_number':3783,'multiline':False]
['text':' current config. If this is the initial reconfig, then we don't need to check this','line_number':3784,'multiline':False]
['text':' condition, since there were no prior configs. Also, for force reconfigs we bypass this','line_number':3785,'multiline':False]
['text':' safety check condition.','line_number':3786,'multiline':False]
['text':' If our config was installed via a "force" reconfig, we bypass the oplog commitment check.','line_number':3788,'multiline':False]
['text':' Call the callback to get the new config given the old one.','line_number':3814,'multiline':False]
['text':' Synchronize this change with potential changes to the default write concern.','line_number':3821,'multiline':False]
['text':' Reservation OK.','line_number':3830,'multiline':False]
['text':' Excluding reconfigs that bump the config term during step-up from checking against changing','line_number':3842,'multiline':False]
['text':' the implicit default write concern, as it is not needed.','line_number':3843,'multiline':False]
['text':' skipping step-up reconfig ','line_number':3844,'multiline':True]
['text':' If the new config changes the replica set's implicit default write concern, we fail the','line_number':3848,'multiline':False]
['text':' reconfig command. This includes force reconfigs.','line_number':3849,'multiline':False]
['text':' The user should set a cluster-wide write concern and attempt the reconfig command again.','line_number':3850,'multiline':False]
['text':' Allow all reconfigs if the shard is not part of a sharded cluster yet, however','line_number':3862,'multiline':False]
['text':' prevent changing the implicit default write concern to (w: 1) after it becomes part','line_number':3863,'multiline':False]
['text':' of a sharded cluster and CWWC is not set on the cluster.','line_number':3864,'multiline':False]
['text':' Remote call to the configServer should be done to check if CWWC is set on the','line_number':3865,'multiline':False]
['text':' cluster.','line_number':3866,'multiline':False]
['text':' Initiates a remote call to the config server.','line_number':3870,'multiline':False]
['text':' If we are currently using a custom write concern as the default, check that the','line_number':3893,'multiline':False]
['text':' corresponding definition still exists in the new config.','line_number':3894,'multiline':False]
['text':' Default WC can be 'boost::none' if the implicit default is used and set to 'w:1'.','line_number':3900,'multiline':False]
['text':' Make sure we can find ourselves in the config. If the config contents have not changed, then','line_number':3943,'multiline':False]
['text':' we bypass the check for finding ourselves in the config, since we know it should already be','line_number':3944,'multiline':False]
['text':' satisfied. There is also one further optimization here: if we have a valid _selfIndex, we can','line_number':3945,'multiline':False]
['text':' do a quick and cheap pass first to see if host and port exist in the new config. This is safe','line_number':3946,'multiline':False]
['text':' as we are not allowed to have the same HostAndPort in the config twice. Matching HostandPort','line_number':3947,'multiline':False]
['text':' implies matching isSelf, and it is actually preferrable to avoid checking the latter as it is','line_number':3948,'multiline':False]
['text':' succeptible to transient DNS errors.','line_number':3949,'multiline':False]
['text':' Either our HostAndPort changed in the config or we didn't have a _selfIndex.','line_number':3969,'multiline':False]
['text':' Always debug assert if we reach this point.','line_number':3976,'multiline':False]
['text':' Don't write no-op for internal and external force reconfig.','line_number':4016,'multiline':False]
['text':' For non-force reconfigs with 'skipSafetyChecks' set to false, we are guaranteed that the','line_number':4017,'multiline':False]
['text':' node is a writable primary.','line_number':4018,'multiline':False]
['text':' When 'skipSafetyChecks' is true, it is possible the node is not yet a writable primary','line_number':4019,'multiline':False]
['text':' (eg. in the case where reconfig is called during stepup). In all other cases, we should','line_number':4020,'multiline':False]
['text':' still do the no-op write when possible.','line_number':4021,'multiline':False]
['text':' writeOplog ','line_number':4025,'multiline':True]
['text':' Wait for durability of the new config document.','line_number':4032,'multiline':False]
['text':' Do not conduct an election during a reconfig, as the node may not be electable post-reconfig.','line_number':4051,'multiline':False]
['text':' If there is an election in-progress, there can be at most one. No new election can happen as','line_number':4058,'multiline':False]
['text':' we have already set our ReplicationCoordinatorImpl::_rsConfigState state to','line_number':4059,'multiline':False]
['text':' "kConfigReconfiguring" which prevents new elections from happening.','line_number':4060,'multiline':False]
['text':' Wait for the election to complete and the node's Role to be set to follower.','line_number':4065,'multiline':False]
['text':' Primary node won't be electable or removed after the configuration change.','line_number':4075,'multiline':False]
['text':' So, finish the reconfig under RSTL, so that the step down occurs safely.','line_number':4076,'multiline':False]
['text':' We need to release the mutex before yielding locks for prepared transactions, which','line_number':4083,'multiline':False]
['text':' might check out sessions, to avoid deadlocks with checked-out sessions accessing','line_number':4084,'multiline':False]
['text':' this mutex.','line_number':4085,'multiline':False]
['text':' Clear the node's election candidate metrics since it is no longer primary.','line_number':4093,'multiline':False]
['text':' Update _canAcceptNonLocalWrites.','line_number':4096,'multiline':False]
['text':' Release the rstl lock as the node might have stepped down due to','line_number':4099,'multiline':False]
['text':' other unconditional step down code paths like learning new term via heartbeat &','line_number':4100,'multiline':False]
['text':' liveness timeout. And, no new election can happen as we have already set our','line_number':4101,'multiline':False]
['text':' ReplicationCoordinatorImpl::_rsConfigState state to "kConfigReconfiguring" which','line_number':4102,'multiline':False]
['text':' prevents new elections from happening. So, its safe to release the RSTL lock.','line_number':4103,'multiline':False]
['text':' Record the latest committed optime in the current config atomically with the new config','line_number':4116,'multiline':False]
['text':' taking effect. Once we have acquired the replication mutex above, we are ensured that no new','line_number':4117,'multiline':False]
['text':' writes will be committed in the previous config, since any other system operation must','line_number':4118,'multiline':False]
['text':' acquire the mutex to advance the commit point.','line_number':4119,'multiline':False]
['text':' Safe reconfig guarantees that all committed entries are safe, so we can keep our commit','line_number':4122,'multiline':False]
['text':' point. One exception is when we change the meaning of the "committed" snapshot from applied','line_number':4123,'multiline':False]
['text':' -> durable. We have to drop all snapshots so we don't mistakenly read from the wrong one.','line_number':4124,'multiline':False]
['text':' If the new config has the same content but different version and term, like on stepup, we','line_number':4127,'multiline':False]
['text':' don't need to drop snapshots either, since the quorum condition is still the same.','line_number':4128,'multiline':False]
['text':' If we have a split config, schedule heartbeats to each recipient member. It informs them of','line_number':4138,'multiline':False]
['text':' the new split config.','line_number':4139,'multiline':False]
['text':' Check writable primary before waiting.','line_number':4159,'multiline':False]
['text':' Wait for the config document to be replicated to a majority of nodes in the current config.','line_number':4172,'multiline':False]
['text':' In an edge case, if this node has received and installed new config with a higher term via','line_number':4189,'multiline':False]
['text':' a heartbeat reconfig while still waiting for an earlier config to propagate, we can','line_number':4190,'multiline':False]
['text':' erroneously return the earlier reconfig request successfully. Therefore, if a node sees that','line_number':4191,'multiline':False]
['text':' its installed config term is higher than the config term it is waiting on, it means a new','line_number':4192,'multiline':False]
['text':' primary has been elected, and we should fail the original reconfig request.','line_number':4193,'multiline':False]
['text':' Wait for the latest committed optime in the previous config to be committed in the','line_number':4207,'multiline':False]
['text':' current config.','line_number':4208,'multiline':False]
['text':' We will retry on the next heartbeat.','line_number':4243,'multiline':False]
['text':' Even though memberIds should properly identify nodes across config changes, to be safe we','line_number':4261,'multiline':False]
['text':' only want to do an automatic reconfig where the base config is the one that specified','line_number':4262,'multiline':False]
['text':' this memberId.','line_number':4263,'multiline':False]
['text':' Set info for currentOp to display if called while this is still running.','line_number':4288,'multiline':False]
['text':' force ','line_number':4311,'multiline':True]
['text':' It is safe to do nothing here as we will retry this on the next heartbeat, or we may','line_number':4319,'multiline':False]
['text':' instead find out the reconfig already took place and is no longer necessary.','line_number':4320,'multiline':False]
['text':' We intentionally do not wait for config commitment. If the config does not get committed, we','line_number':4326,'multiline':False]
['text':' will try again on the next heartbeat.','line_number':4327,'multiline':False]
['text':' Initiate FCV in local storage. This will propagate to other nodes via initial sync.','line_number':4357,'multiline':False]
['text':' The setname is not provided as a command line argument in serverless mode.','line_number':4377,'multiline':False]
['text':' In pv1, the TopologyCoordinator has not set the term yet. It will be set to kInitialTerm if','line_number':4405,'multiline':False]
['text':' the initiate succeeds so we pass that here.','line_number':4406,'multiline':False]
['text':' Since the JournalListener has not yet been set up, we must manually set our','line_number':4427,'multiline':False]
['text':' durableOpTime.','line_number':4428,'multiline':False]
['text':' Sets the initial data timestamp on the storage engine so it can assign a timestamp','line_number':4431,'multiline':False]
['text':' to data on disk. We do this after writing the "initiating set" oplog entry.','line_number':4432,'multiline':False]
['text':' Advance the commit point and take a stable checkpoint, to make sure that we can recover if we','line_number':4436,'multiline':False]
['text':' happen to roll back our first entries after replSetInitiate.','line_number':4437,'multiline':False]
['text':' Will call _setStableTimestampForStorage() on success.','line_number':4441,'multiline':False]
['text':' fromSyncSource ','line_number':4443,'multiline':True]
['text':' forInitiate ','line_number':4443,'multiline':True]
['text':'stableCheckpoint','line_number':4445,'multiline':True]
['text':' A configuration passed to replSetInitiate() with the current node as an arbiter','line_number':4449,'multiline':False]
['text':' will fail validation with a "replSet initiate got ... while validating" reason.','line_number':4450,'multiline':False]
['text':' When a node is removed, always return a hello response indicating the server has no','line_number':4512,'multiline':False]
['text':' config set.','line_number':4513,'multiline':False]
['text':' We were previously removed but are now rejoining the replica set.','line_number':4517,'multiline':False]
['text':' Reply with an error to hello requests received while the node had an invalid config.','line_number':4519,'multiline':False]
['text':' Create a hello response for each horizon the server is knowledgeable about.','line_number':4551,'multiline':False]
['text':' Fulfill the promise and replace with a new one for future waiters.','line_number':4562,'multiline':False]
['text':' We are joining the replica set for the first time. Send back an error to hello','line_number':4568,'multiline':False]
['text':' requests that are waiting on a horizon that does not exist in the new config. Otherwise,','line_number':4569,'multiline':False]
['text':' reply with an updated hello response.','line_number':4570,'multiline':False]
['text':' No more hello requests will wait for a topology change, so clear _horizonToPromiseMap.','line_number':4590,'multiline':False]
['text':' We want to respond to any waiting hellos even if our current and target state are the','line_number':4608,'multiline':False]
['text':' same as it is possible writes have been disabled during a stepDown but the primary has yet','line_number':4609,'multiline':False]
['text':' to transition to SECONDARY state.  We do not do so when _stepDownPending is true','line_number':4610,'multiline':False]
['text':' because in that case we have already said we cannot accept writes in the hello response','line_number':4611,'multiline':False]
['text':' and explictly incremented the toplogy version.','line_number':4612,'multiline':False]
['text':' Wake up any threads blocked in awaitReplication, close connections, etc.','line_number':4627,'multiline':False]
['text':' Wake up the optime waiter that is waiting for primary catch-up to finish.','line_number':4630,'multiline':False]
['text':' _canAcceptNonLocalWrites should already be set.','line_number':4634,'multiline':False]
['text':' Exit catchup mode if we're in it and enable replication producer and applier on stepdown.','line_number':4644,'multiline':False]
['text':' _pendingTermUpdateDuringStepDown is set before stepping down due to hearing about a','line_number':4647,'multiline':False]
['text':' higher term, so that we can remember the term we heard and update our term as part of','line_number':4648,'multiline':False]
['text':' finishing stepdown. It is then unset toward the end of stepdown, after the function','line_number':4649,'multiline':False]
['text':' we are in is called. Thus we must be stepping down due to seeing a higher term if and','line_number':4650,'multiline':False]
['text':' only if _pendingTermUpdateDuringStepDown is set here.','line_number':4651,'multiline':False]
['text':' Switching out of SECONDARY, but not to PRIMARY or ROLLBACK. Note that ROLLBACK case is','line_number':4663,'multiline':False]
['text':' handled separately and requires RSTL lock held, see setFollowerModeRollback.','line_number':4664,'multiline':False]
['text':' Switching into SECONDARY, but not from PRIMARY.','line_number':4667,'multiline':False]
['text':' When transitioning from other follower states to SECONDARY, run for election on a','line_number':4673,'multiline':False]
['text':' single-node replica set.','line_number':4674,'multiline':False]
['text':' If we are transitioning from secondary, cancel any scheduled takeovers.','line_number':4678,'multiline':False]
['text':' Ensure replication is running if we are no longer REMOVED.','line_number':4684,'multiline':False]
['text':' Initializes the featureCompatibilityVersion to the latest value, because arbiters do not','line_number':4700,'multiline':False]
['text':' receive the replicated version. This is to avoid bugs like SERVER-32639.','line_number':4701,'multiline':False]
['text':' (Generic FCV reference): This FCV check should exist across LTS binary versions.','line_number':4703,'multiline':False]
['text':' Notifies waiters blocked in waitForMemberState().','line_number':4713,'multiline':False]
['text':' For testing only.','line_number':4714,'multiline':False]
['text':' Get the term from the topology coordinator, which we then use to generate the election ID.','line_number':4748,'multiline':False]
['text':' We intentionally wait until the end of this function','line_number':4749,'multiline':False]
['text':' Clear the sync source.','line_number':4763,'multiline':False]
['text':' Notify all secondaries of the election win by cancelling all current heartbeats and sending','line_number':4766,'multiline':False]
['text':' new heartbeat requests to all nodes. We must cancel and start instead of restarting scheduled','line_number':4767,'multiline':False]
['text':' heartbeats because all heartbeats must be restarted upon election succeeding.','line_number':4768,'multiline':False]
['text':' Reset the number of catchup operations performed before starting catchup.','line_number':4784,'multiline':False]
['text':' No catchup in single node replica set.','line_number':4787,'multiline':False]
['text':' When catchUpTimeoutMillis is 0, we skip doing catchup entirely.','line_number':4796,'multiline':False]
['text':' Check whether the callback has been cancelled while holding mutex.','line_number':4809,'multiline':False]
['text':' Deal with infinity and overflow - no timeout.','line_number':4817,'multiline':False]
['text':' Schedule timeout callback.','line_number':4822,'multiline':False]
['text':' Clean up its own members.','line_number':4840,'multiline':False]
['text':' Enter primary drain mode.','line_number':4849,'multiline':False]
['text':' Destroy the state itself.','line_number':4851,'multiline':False]
['text':' Haven't collected all heartbeat responses.','line_number':4857,'multiline':False]
['text':' We've caught up.','line_number':4866,'multiline':False]
['text':' Report the number of ops applied during catchup in replSetGetStatus once the primary is','line_number':4873,'multiline':False]
['text':' caught up.','line_number':4874,'multiline':False]
['text':' Reset the target optime if it has changed.','line_number':4880,'multiline':False]
['text':' Only increment the 'numCatchUps' election metric the first time we add a waiter, so that','line_number':4902,'multiline':False]
['text':' we only increment it once each time a primary has to catch up. If there is already an','line_number':4903,'multiline':False]
['text':' existing waiter, then the node is catching up and has already been counted.','line_number':4904,'multiline':False]
['text':' Double check the target time since stepdown may signal us too.','line_number':4909,'multiline':False]
['text':' Report the number of ops applied during catchup in replSetGetStatus once the primary','line_number':4916,'multiline':False]
['text':' is caught up.','line_number':4917,'multiline':False]
['text':' It is only necessary to check if an arbiter is running on a quarterly binary version when a','line_number':4979,'multiline':False]
['text':' fresh node is added to the replica set as an arbiter and when an old secondary node is','line_number':4980,'multiline':False]
['text':' removed and then re-added to the replica set as an arbiter. That's why we only need to warn','line_number':4981,'multiline':False]
['text':' once per process as converting from secondary to arbiter normally requires a server shutdown.','line_number':4982,'multiline':False]
['text':' Warn if an arbiter is running on a quarterly binary version.','line_number':4985,'multiline':False]
['text':' updateConfig() can change terms, so update our term shadow to match.','line_number':4994,'multiline':False]
['text':' We allow the IDWC to be set only once after initial configuration is loaded.','line_number':5002,'multiline':False]
['text':' If 'enableDefaultWriteConcernUpdatesForInitiate' is enabled, we allow the IDWC to be','line_number':5006,'multiline':False]
['text':' recalculated after a reconfig. However, this logic is only relevant for testing,','line_number':5007,'multiline':False]
['text':' and should not be executed outside of our test infrastructure. This is needed due to an','line_number':5008,'multiline':False]
['text':' optimization in our ReplSetTest jstest fixture that initiates replica sets with only the','line_number':5009,'multiline':False]
['text':' primary, and then reconfigs the full membership set in. As a result, we must calculate','line_number':5010,'multiline':False]
['text':' the final IDWC only after the last node has been added to the set.','line_number':5011,'multiline':False]
['text':' Warn if using the in-memory (ephemeral) storage engine with','line_number':5017,'multiline':False]
['text':' writeConcernMajorityJournalDefault=true.','line_number':5018,'multiline':False]
['text':' Check that getLastErrorDefaults has not been changed from the default settings of','line_number':5053,'multiline':False]
['text':' { w: 1, wtimeout: 0 }.','line_number':5054,'multiline':False]
['text':' Emit a warning at startup if there are IP addresses in the SplitHorizon field of the','line_number':5070,'multiline':False]
['text':' replset config.','line_number':5071,'multiline':False]
['text':' Check that no horizon mappings contain IP addresses','line_number':5074,'multiline':False]
['text':' Emit a startup warning for any SplitHorizon mapping that can be parsed as','line_number':5076,'multiline':False]
['text':' a valid CIDR range, except for the default horizon.','line_number':5077,'multiline':False]
['text':' If the SplitHorizon has changed, reply to all waiting hellos with an error.','line_number':5093,'multiline':False]
['text':' Wake up writeConcern waiters that are no longer satisfiable due to the rsConfig change.','line_number':5106,'multiline':False]
['text':' This throws if a waiter's writeConcern is no longer satisfiable, in which case','line_number':5110,'multiline':False]
['text':' setValueIf_inlock will fulfill the waiter's promise with the error status.','line_number':5111,'multiline':False]
['text':' Return false meaning that the waiter is still satisfiable and thus can remain in the','line_number':5113,'multiline':False]
['text':' waiter list.','line_number':5114,'multiline':False]
['text':' If the new config describes an electable one-node replica set, we need to start an','line_number':5124,'multiline':False]
['text':' election.','line_number':5125,'multiline':False]
['text':' Don't send heartbeats if we're not in the config, if we get re-added one of the','line_number':5130,'multiline':False]
['text':' nodes in the set will contact us.','line_number':5131,'multiline':False]
['text':' We should only create a new horizon-to-promise mapping for nodes that are members of','line_number':5135,'multiline':False]
['text':' the config.','line_number':5136,'multiline':False]
['text':' Clear the horizon promise mappings of removed nodes so they can be recreated if the','line_number':5140,'multiline':False]
['text':' node later rejoins the set.','line_number':5141,'multiline':False]
['text':' If we're still REMOVED, clear the seedList.','line_number':5144,'multiline':False]
['text':' If we become primary after the unlock below, the forwardSecondaryProgress will do nothing','line_number':5182,'multiline':False]
['text':' (slightly expensively).  If we become secondary after the unlock below, BackgroundSync','line_number':5183,'multiline':False]
['text':' will take care of forwarding our progress by calling signalUpstreamUpdater() once we','line_number':5184,'multiline':False]
['text':' select a new sync source.  So it's OK to depend on the stale value of wasPrimary here.','line_number':5185,'multiline':False]
['text':' maxRemoteOpTime is null here if we got valid updates but no downstream node had','line_number':5188,'multiline':False]
['text':' actually advanced any optime.','line_number':5189,'multiline':False]
['text':' Must do this outside _mutex','line_number':5193,'multiline':False]
['text':' We need to ensure that the 'commitQuorum' can be satisfied by all the members of this','line_number':5244,'multiline':False]
['text':' replica set.','line_number':5245,'multiline':False]
['text':' for shell prompt','line_number':5260,'multiline':False]
['text':' Always allow chaining while in catchup and drain mode.','line_number':5274,'multiline':False]
['text':' Handle special case of initial sync source read preference.','line_number':5279,'multiline':False]
['text':' This sync source will be cleared when we go to secondary mode, because we will perform','line_number':5280,'multiline':False]
['text':' a postMemberState action of kOnFollowerModeStateChange which calls chooseNewSyncSource().','line_number':5281,'multiline':False]
['text':' Voting nodes prefer to sync from the primary.  A voting node that is initial syncing','line_number':5293,'multiline':False]
['text':' may have acknowledged writes which are part of the set's write majority; if it then','line_number':5294,'multiline':False]
['text':' resyncs from a node which does not have those writes, and (before it replicates them','line_number':5295,'multiline':False]
['text':' again) helps elect a new primary which also does not have those writes, the writes','line_number':5296,'multiline':False]
['text':' may be lost.  By resyncing from the primary (if possible), which always has the','line_number':5297,'multiline':False]
['text':' majority-commited writes, the probability of this scenario is reduced.','line_number':5298,'multiline':False]
['text':' If we are not the primary and chaining is disabled in the config (without overrides), we','line_number':5304,'multiline':False]
['text':' should only be syncing from the primary.','line_number':5305,'multiline':False]
['text':' If read preference is SecondaryOnly, we should never choose the primary.','line_number':5321,'multiline':False]
['text':' If we lost our sync source, schedule new heartbeats immediately to update our knowledge','line_number':5325,'multiline':False]
['text':' of other members's state, allowing us to make informed sync source decisions.','line_number':5326,'multiline':False]
['text':' Update the global timestamp before setting last applied opTime forward so the last applied','line_number':5364,'multiline':False]
['text':' optime is never greater than the latest in-memory cluster time.','line_number':5365,'multiline':False]
['text':' Drop the last batch of message following a change of replica set due to a shard split.','line_number':5383,'multiline':False]
['text':' We should drop the last batch if we find a significantly closer node. This is to','line_number':5402,'multiline':False]
['text':' avoid advancing our 'lastFetched', which makes it more likely that we will be able to','line_number':5403,'multiline':False]
['text':' choose the closer node as our sync source.','line_number':5404,'multiline':False]
['text':'','line_number':5454,'multiline':False]
['text':' The stable timestamp must be a "consistent" timestamp with respect to the oplog. Intuitively,','line_number':5455,'multiline':False]
['text':' it must be a timestamp at which the oplog history is "set in stone" i.e. no writes will','line_number':5456,'multiline':False]
['text':' commit at earlier timestamps. More precisely, it must be a timestamp T such that future','line_number':5457,'multiline':False]
['text':' writers only commit at times greater than T and readers only read at, or earlier than, T.  We','line_number':5458,'multiline':False]
['text':' refer to this timestamp as the "no-overlap" point, since it is the timestamp that delineates','line_number':5459,'multiline':False]
['text':' these non overlapping readers and writers. The calculation of this value differs on primary','line_number':5460,'multiline':False]
['text':' and secondary nodes due to their distinct behaviors, as described below.','line_number':5461,'multiline':False]
['text':'','line_number':5462,'multiline':False]
['text':' On a primary node, oplog writes may commit out of timestamp order, which can lead to the','line_number':5464,'multiline':False]
['text':' creation of oplog "holes". On a primary the all_durable timestamp tracks the newest timestamp','line_number':5465,'multiline':False]
['text':' T such that no future transactions will commit behind T. Since all_durable is a timestamp,','line_number':5466,'multiline':False]
['text':' however, without a term, we need to construct an optime with a proper term. If we are','line_number':5467,'multiline':False]
['text':' primary, then the all_durable should always correspond to a timestamp at or newer than the','line_number':5468,'multiline':False]
['text':' first write completed by this node as primary, since we write down a new oplog entry before','line_number':5469,'multiline':False]
['text':' allowing writes as a new primary. Thus, it can be assigned the current term of this primary.','line_number':5470,'multiline':False]
['text':' On a secondary, oplog entries are written in parallel, and so may be written out of timestamp','line_number':5477,'multiline':False]
['text':' order. Because of this, the stable timestamp must not fall in the middle of a batch while it','line_number':5478,'multiline':False]
['text':' is being applied. To prevent this we ensure the no-overlap point does not surpass the','line_number':5479,'multiline':False]
['text':' lastApplied, which is only advanced at the end of secondary batch application.','line_number':5480,'multiline':False]
['text':' The stable optime must always be less than or equal to the no overlap point. When majority','line_number':5483,'multiline':False]
['text':' reads are enabled, the stable optime must also not surpass the majority commit point. When','line_number':5484,'multiline':False]
['text':' majority reads are disabled, the stable optime is not required to be majority committed.','line_number':5485,'multiline':False]
['text':' Make sure the stable optime does not surpass its maximum.','line_number':5489,'multiline':False]
['text':' Check that the selected stable optime does not exceed our maximum and that it does not','line_number':5492,'multiline':False]
['text':' surpass the no-overlap point.','line_number':5493,'multiline':False]
['text':' Don't update the stable optime if we are in initial sync. We advance the oldest timestamp','line_number':5515,'multiline':False]
['text':' continually to the lastApplied optime during initial sync oplog application, so if we learned','line_number':5516,'multiline':False]
['text':' about an earlier commit point during this period, we would risk setting the stable timestamp','line_number':5517,'multiline':False]
['text':' behind the oldest timestamp, which is prohibited in the storage engine. Note that we don't','line_number':5518,'multiline':False]
['text':' take stable checkpoints during initial sync, so the stable timestamp during this period','line_number':5519,'multiline':False]
['text':' doesn't play a functionally important role anyway.','line_number':5520,'multiline':False]
['text':' Get the current stable optime.','line_number':5528,'multiline':False]
['text':' Don't update the stable timestamp if it is earlier than the initial data timestamp.','line_number':5531,'multiline':False]
['text':' Timestamps before the initialDataTimestamp are not consistent and so are not safe to use for','line_number':5532,'multiline':False]
['text':' the stable timestamp or the committed snapshot, which is the timestamp used by majority','line_number':5533,'multiline':False]
['text':' readers. This also prevents us from setting the stable timestamp behind the oldest timestamp','line_number':5534,'multiline':False]
['text':' after leaving initial sync, since the initialDataTimestamp and oldest timestamp will be equal','line_number':5535,'multiline':False]
['text':' after initial sync oplog application has completed.','line_number':5536,'multiline':False]
['text':' Set the stable timestamp and update the committed snapshot.','line_number':5556,'multiline':False]
['text':' As arbiters aren't data bearing nodes, the all durable timestamp does not get advanced. To','line_number':5560,'multiline':False]
['text':' advance the all durable timestamp when setting the stable timestamp we use 'force=true'.','line_number':5561,'multiline':False]
['text':' Update committed snapshot and wake up any threads waiting on read concern or write concern.','line_number':5564,'multiline':False]
['text':' Set the committed snapshot to the new stable optime. The wall time of the committed snapshot','line_number':5565,'multiline':False]
['text':' is not used for anything so we can create a fake one.','line_number':5566,'multiline':False]
['text':' Update the stable timestamp for the storage engine.','line_number':5569,'multiline':False]
['text':' Check to see if we can immediately return without taking any locks.','line_number':5575,'multiline':False]
['text':' This needs to happen after the attempt so readers can be sure we've already tried.','line_number':5580,'multiline':False]
['text':' Need the RSTL in mode X to transition to SECONDARY','line_number':5583,'multiline':False]
['text':' We can only transition to SECONDARY from RECOVERING state.','line_number':5586,'multiline':False]
['text':' Maintenance mode will force us to remain in RECOVERING state, no matter what.','line_number':5597,'multiline':False]
['text':' Execute the transition to SECONDARY.','line_number':5603,'multiline':False]
['text':' Arbiters do not store replicated data, so we consider their data trivially','line_number':5628,'multiline':False]
['text':' consistent.','line_number':5629,'multiline':False]
['text':' Even if we have no new snapshot, we need to notify waiters that the commit point moved.','line_number':5634,'multiline':False]
['text':' We should only enter terminal shutdown from global terminal exit.  In that case, rather','line_number':5661,'multiline':False]
['text':' than voting in a term we don't plan to stay alive in, refuse to vote.','line_number':5662,'multiline':False]
['text':' It's safe to store lastVote outside of _mutex. The topology coordinator grants only one','line_number':5704,'multiline':False]
['text':' vote per term, and storeLocalLastVoteDocument does nothing unless lastVote has a higher term','line_number':5705,'multiline':False]
['text':' than the previous lastVote, so threads racing to store votes from different terms will','line_number':5706,'multiline':False]
['text':' eventually store the latest vote.','line_number':5707,'multiline':False]
['text':' Note the topology coordinator has already advanced its last vote at this point,','line_number':5712,'multiline':False]
['text':' so this node will not be able to vote in this election; this is a "spoiled" vote.','line_number':5713,'multiline':False]
['text':' Don't take any locks if we do not need to.','line_number':5729,'multiline':False]
['text':' Avoid retrieving Rollback ID if we do not need it for _prepareOplogQueryMetadata_inlock().','line_number':5734,'multiline':False]
['text':' Do BSON serialization outside lock.','line_number':5757,'multiline':False]
['text':' Fail point to block and optionally skip fetching config. Supported arguments:','line_number':5774,'multiline':False]
['text':'   versionAndTerm: [ v, t ]','line_number':5775,'multiline':False]
['text':' namespace','line_number':5794,'multiline':False]
['text':' ourSetName is empty if this was in auto initiate mode.','line_number':5811,'multiline':False]
['text':' In serverless mode before having an initialized config, simply use the replica set','line_number':5823,'multiline':False]
['text':' name provided in the hearbeat request.','line_number':5824,'multiline':False]
['text':' If the latest heartbeat indicates that the remote node's config has changed, we want to','line_number':5834,'multiline':False]
['text':' update it's member data as soon as possible. Send an immediate hearbeat, and update the','line_number':5835,'multiline':False]
['text':' member data on processing its response.','line_number':5836,'multiline':False]
['text':' If this node does not belong to the configuration it knows about, send heartbeats','line_number':5841,'multiline':False]
['text':' back to any node that sends us a heartbeat, in case one of those remote nodes has','line_number':5842,'multiline':False]
['text':' a configuration that contains us.  Chances are excellent that it will, since that','line_number':5843,'multiline':False]
['text':' is the only reason for a remote node to send this node a heartbeat request.','line_number':5844,'multiline':False]
['text':' If we are currently in drain mode, we won't allow installing newer configs, so we don't','line_number':5860,'multiline':False]
['text':' schedule a heartbeat to fetch one. We do allow force reconfigs to proceed even if we are','line_number':5861,'multiline':False]
['text':' in drain mode.','line_number':5862,'multiline':False]
['text':' Schedule a heartbeat to the sender to fetch the new config.','line_number':5870,'multiline':False]
['text':' Only send this if the sender's config is newer.','line_number':5871,'multiline':False]
['text':' We cannot cancel the enqueued heartbeat, but either this one or the enqueued heartbeat','line_number':5872,'multiline':False]
['text':' will trigger reconfig, which cancels and reschedules all heartbeats.','line_number':5873,'multiline':False]
['text':' If the sender thinks the primary is different from what we think and if the sender itself','line_number':5888,'multiline':False]
['text':' is the primary, then we want to update our view of primary by immediately sending out a','line_number':5889,'multiline':False]
['text':' new round of heartbeats, whose responses should inform us of the new primary. We only do','line_number':5890,'multiline':False]
['text':' this if the term of the heartbeat is greater than or equal to our own, to prevent','line_number':5891,'multiline':False]
['text':' updating our view to a stale primary.','line_number':5892,'multiline':False]
['text':' Note: no mutex acquisition here, as we are reading an Atomic variable.','line_number':5910,'multiline':False]
['text':' Term is only valid if we are replicating.','line_number':5928,'multiline':False]
['text':' Check we haven't acquired any lock, because potential stepdown needs global lock.','line_number':5933,'multiline':False]
['text':' If the term is already up to date, we can skip the update and the mutex acquisition.','line_number':5936,'multiline':False]
['text':' Wait for potential stepdown to finish.','line_number':5948,'multiline':False]
['text':' When the node discovers a new term, the new term date metrics are now out-of-date, so we','line_number':5971,'multiline':False]
['text':' clear them.','line_number':5972,'multiline':False]
['text':' The timeout isn't used by _doneWaitingForReplication_inlock.','line_number':6023,'multiline':False]
['text':' Only setWMajorityWriteAvailabilityDate if the wait was successful.','line_number':6028,'multiline':False]
['text':' Runs callback and returns early if the writeConcern is immediately satisfied.','line_number':6036,'multiline':False]
['text':' If we are in ROLLBACK state, do not set any new _currentCommittedSnapshot, as it will be','line_number':6053,'multiline':False]
['text':' cleared at the end of rollback anyway.','line_number':6054,'multiline':False]
['text':' The new committed snapshot should be <= the current replication commit point.','line_number':6061,'multiline':False]
['text':' The new committed snapshot should be >= the current snapshot.','line_number':6066,'multiline':False]
['text':' Wake up any threads waiting for read concern or write concern.','line_number':6078,'multiline':False]
['text':' A null _electionState indicates that the election has already completed.','line_number':6180,'multiline':False]
['text':' Step up is considered successful only if we are currently a primary and we are not in the','line_number':6190,'multiline':False]
['text':' process of stepping down. If we know we are going to step down, we should fail the','line_number':6191,'multiline':False]
['text':' replSetStepUp command so caller can retry if necessary.','line_number':6192,'multiline':False]
['text':' We must be holding the RSTL in mode X to change _canAcceptNonLocalWrites.','line_number':6223,'multiline':False]
['text':' We must be holding the RSTL.','line_number':6242,'multiline':False]
['text':' We must be holding the RSTL.','line_number':6254,'multiline':False]
['text':' We must be holding the RSTL in mode X to change _canServeNonLocalReads.','line_number':6262,'multiline':False]
['text':' Checking whether the shard is part of a sharded cluster or not by checking if CWWC','line_number':6283,'multiline':False]
['text':' flag is set as we record it during sharding initialization phase, as on restarting a','line_number':6284,'multiline':False]
['text':' shard node for upgrading or any other reason, sharding initialization happens before','line_number':6285,'multiline':False]
['text':' config initialization.','line_number':6286,'multiline':False]
['text':' namespace repl','line_number':6327,'multiline':False]
['text':' namespace mongo','line_number':6328,'multiline':False]
