['text':'*
 *    Copyright (C) 2020-present MongoDB, Inc.
 *
 *    This program is free software: you can redistribute it and/or modify
 *    it under the terms of the Server Side Public License, version 1,
 *    as published by MongoDB, Inc.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    Server Side Public License for more details.
 *
 *    You should have received a copy of the Server Side Public License
 *    along with this program. If not, see
 *    <http://www.mongodb.com/licensing/server-side-public-license>.
 *
 *    As a special exception, the copyright holders give permission to link the
 *    code of portions of this program with the OpenSSL library under certain
 *    conditions as described in each individual source file and distribute
 *    linked combinations including the program with the OpenSSL library. You
 *    must comply with the Server Side Public License in all respects for
 *    all of the code used other than as permitted herein. If you modify file(s)
 *    with this exception, you may extend this exception to your version of the
 *    file(s), but you are not obligated to do so. If you do not wish to do so,
 *    delete this exception statement from your version. If you delete this
 *    exception statement from all source files in the program, then also delete
 *    it in the license file.
 ','line_number':1,'multiline':True]
['text':' IWYU pragma: keep','line_number':59,'multiline':False]
['text':' Add kTenantMigrationOplogView, kSessionTransactionsTableNamespace, and kRsOplogNamespace','line_number':174,'multiline':False]
['text':' to resolvedNamespaces since they are all used during different pipeline stages.','line_number':175,'multiline':False]
['text':' explain ','line_number':186,'multiline':True]
['text':' fromMongos ','line_number':187,'multiline':True]
['text':' needsMerge ','line_number':188,'multiline':True]
['text':' allowDiskUse ','line_number':189,'multiline':True]
['text':' bypassDocumentValidation ','line_number':190,'multiline':True]
['text':' isMapReduceCommand ','line_number':191,'multiline':True]
['text':' runtimeConstants ','line_number':193,'multiline':True]
['text':' collator ','line_number':194,'multiline':True]
['text':' collUUID ','line_number':197,'multiline':True]
['text':' We allow retrying on the following oplog fetcher errors:','line_number':200,'multiline':False]
['text':' 1) InvalidSyncSource - we cannot sync from the chosen sync source, potentially because the sync','line_number':201,'multiline':False]
['text':'    source is too stale or there was a network error when connecting to the sync source.','line_number':202,'multiline':False]
['text':' 2) ShudownInProgress - the current sync source is shutting down','line_number':203,'multiline':False]
['text':' Creates the oplog buffer that will be populated by donor oplog entries from the retryable','line_number':209,'multiline':False]
['text':' writes fetching stage and oplog fetching stage.','line_number':210,'multiline':False]
['text':' namespace','line_number':223,'multiline':False]
['text':' We never restart just the oplog fetcher.  If a failure occurs, we restart the whole state machine','line_number':229,'multiline':False]
['text':' and recover from there.  So the restart decision is always "no".','line_number':230,'multiline':False]
['text':' The oplog fetcher requires some of the methods in DataReplicatorExternalState to operate.','line_number':241,'multiline':False]
['text':' The oplog fetcher is passed its executor directly and does not use the one from the','line_number':244,'multiline':False]
['text':' DataReplicatorExternalState.','line_number':245,'multiline':False]
['text':' The oplog fetcher uses the current term and opTime to inform the sync source of term changes.','line_number':253,'multiline':False]
['text':' As the term on the donor and the term on the recipient have nothing to do with each other,','line_number':254,'multiline':False]
['text':' we do not want to do that.','line_number':255,'multiline':False]
['text':' Tenant migration does not require the metadata from the oplog query.','line_number':260,'multiline':False]
['text':' Tenant migration does not change sync source depending on metadata.','line_number':264,'multiline':False]
['text':' Tenant migration does not re-evaluate sync source on error.','line_number':273,'multiline':False]
['text':' The oplog fetcher should never call the rest of the methods.','line_number':279,'multiline':False]
['text':' namespace','line_number':315,'multiline':False]
['text':' Perform the multiplication first to avoid rounding errors, and add one to avoid division','line_number':438,'multiline':False]
['text':' by 0.','line_number':439,'multiline':False]
['text':' Don't allow reading before the opTime timestamp of the final write on the recipient','line_number':504,'multiline':False]
['text':' associated with cloning the donor's data so the client can't see an inconsistent state. The','line_number':505,'multiline':False]
['text':' oplog applier timestamp may be null if no oplog entries were copied, but data may still have','line_number':506,'multiline':False]
['text':' been cloned, so use the last applied opTime in that case.','line_number':507,'multiline':False]
['text':'','line_number':508,'multiline':False]
['text':' Note the cloning writes happen on a separate thread, but the last applied opTime in the','line_number':509,'multiline':False]
['text':' replication coordinator is guaranteed to be inclusive of those writes because this function','line_number':510,'multiline':False]
['text':' is called after waiting for the _dataConsistentPromise to resolve, which happens after the','line_number':511,'multiline':False]
['text':' last write for cloning completes (and all of its WUOW onCommit() handlers).','line_number':512,'multiline':False]
['text':' Also don't allow reading before the returnAfterReachingTimestamp (aka the blockTimestamp) to','line_number':517,'multiline':False]
['text':' prevent readers from possibly seeing data in a point in time snapshot on the recipient that','line_number':518,'multiline':False]
['text':' would not have been seen at the same point in time on the donor if the donor's cluster time','line_number':519,'multiline':False]
['text':' is ahead of the recipient's.','line_number':520,'multiline':False]
['text':' This gives assurance that _tenantOplogApplier pointer won't be empty, and that it has been','line_number':527,'multiline':False]
['text':' started. Additionally, we must have finished processing the recipientSyncData command that','line_number':528,'multiline':False]
['text':' waits on _dataConsistentPromise.','line_number':529,'multiline':False]
['text':' In the event of a donor failover, it is possible that a new donor has stepped up and','line_number':534,'multiline':False]
['text':' initiated this 'recipientSyncData' cmd. Make sure the recipient is not in the middle of','line_number':535,'multiline':False]
['text':' restarting the oplog applier to retry the future chain.','line_number':536,'multiline':False]
['text':' When the data sync is done, we reset _tenantOplogApplier, so just throw the data sync','line_number':541,'multiline':False]
['text':' completion future result.','line_number':542,'multiline':False]
['text':' Sanity checks.','line_number':547,'multiline':False]
['text':' A cancellation error may occur due to an interrupt. If that is the case, replace the error','line_number':571,'multiline':False]
['text':' code with the interrupt code, the true reason for interruption.','line_number':572,'multiline':False]
['text':' Make sure that the recipient logical clock has advanced to at least the donor timestamp','line_number':583,'multiline':False]
['text':' before returning success for recipientSyncData.','line_number':584,'multiline':False]
['text':' Note: tickClusterTimeTo() will not tick the recipient clock backwards in time.','line_number':585,'multiline':False]
['text':' updateStateDoc was a no-op, but we still must ensure it's all-replicated.','line_number':600,'multiline':False]
['text':' ConnectionString::connect() always returns a DBClientConnection in a unique_ptr of','line_number':633,'multiline':False]
['text':' DBClientBase type.','line_number':634,'multiline':False]
['text':' Authenticate connection to the donor.','line_number':637,'multiline':False]
['text':' Only ever used to cancel when the setTenantMigrationRecipientInstanceHostTimeout failpoint is','line_number':674,'multiline':False]
['text':' set.','line_number':675,'multiline':False]
['text':' Cancel the find host request after a timeout. Ignore callback handle.','line_number':681,'multiline':False]
['text':' Get all donor hosts that we have excluded.','line_number':698,'multiline':False]
['text':' Application name is constructed such that it doesn't exceeds','line_number':710,'multiline':False]
['text':' kMaxApplicationNameByteLength (128 bytes).','line_number':711,'multiline':False]
['text':' "TenantMigration_" (16 bytes) + <tenantId> (61 bytes) + "_" (1 byte) +','line_number':712,'multiline':False]
['text':' <migrationUuid> (36 bytes) =  114 bytes length.','line_number':713,'multiline':False]
['text':' Note: Since the total length of tenant database name (<tenantId>_<user','line_number':714,'multiline':False]
['text':' provided db name>) can't exceed 63 bytes and the user provided db name','line_number':715,'multiline':False]
['text':' should be at least one character long, the maximum length of tenantId can','line_number':716,'multiline':False]
['text':' only be 61 bytes.','line_number':717,'multiline':False]
['text':' Application name is constructed such that it doesn't exceed','line_number':766,'multiline':False]
['text':' kMaxApplicationNameByteLength (128 bytes).','line_number':767,'multiline':False]
['text':' "TenantMigration_" (16 bytes) + <tenantId> (61 bytes) + "_" (1 byte) +','line_number':768,'multiline':False]
['text':' <migrationUuid> (36 bytes) + _oplogFetcher" (13 bytes) =  127 bytes','line_number':769,'multiline':False]
['text':' length.','line_number':770,'multiline':False]
['text':' Make sure we don't end up with a partially initialized set of connections.','line_number':797,'multiline':False]
['text':' If the future chain has been interrupted, stop retrying.','line_number':802,'multiline':False]
['text':'
             * Retry sync source selection if we encountered any of the following errors:
             * 1) The RSM couldn't find a suitable donor host
             * 2) The majority snapshot OpTime on the donor host was not ahead of our stored
             * 'startApplyingDonorOpTime'
             * 3) Some other retriable error
             ','line_number':814,'multiline':True]
['text':' Clean up any hosts that have had their exclusion duration expired.','line_number':849,'multiline':False]
['text':' Return the list of currently excluded donor hosts.','line_number':856,'multiline':False]
['text':' If the instance state has a startAt field, then the instance is restarted by step up. So,','line_number':866,'multiline':False]
['text':' skip persisting the state doc. And, PrimaryOnlyService::onStepUp() waits for majority commit','line_number':867,'multiline':False]
['text':' of the primary no-op oplog entry written by the node in the newer term before scheduling the','line_number':868,'multiline':False]
['text':' Instance::run(). So, it's also safe to assume that instance's state document written in an','line_number':869,'multiline':False]
['text':' older term on disk won't get rolled back for step up case.','line_number':870,'multiline':False]
['text':' If we don't have a startAt field, we shouldn't have an expireAt either.','line_number':886,'multiline':False]
['text':' If the state is 'kDone' without the expireAt field, it means a recipientForgetMigration','line_number':887,'multiline':False]
['text':' command is received before a recipientSyncData command or after the state doc is garbage','line_number':888,'multiline':False]
['text':' collected. We want to re-initialize the state doc but immediately mark it garbage','line_number':889,'multiline':False]
['text':' collectable to account for delayed recipientSyncData commands.','line_number':890,'multiline':False]
['text':' Persist the state doc before starting the data sync.','line_number':893,'multiline':False]
['text':' Wait for the state doc to be majority replicated to make sure that the state doc','line_number':916,'multiline':False]
['text':' doesn't rollback.','line_number':917,'multiline':False]
['text':' Generally, snapshot reads on config.transactions table have some risks.','line_number':935,'multiline':False]
['text':' But for this case, it is safe because we query only for multi-statement transaction entries','line_number':936,'multiline':False]
['text':' (and "state" field is set only for multi-statement transaction transactions) and writes to','line_number':937,'multiline':False]
['text':' config.transactions collection aren't coalesced for multi-statement transactions during','line_number':938,'multiline':False]
['text':' secondary oplog application, unlike the retryable writes where updates to config.transactions','line_number':939,'multiline':False]
['text':' collection are coalesced on secondaries.','line_number':940,'multiline':False]
['text':' Get the last oplog entry at the read concern majority optime in the remote oplog. It','line_number':968,'multiline':False]
['text':' does not matter which tenant it is for.','line_number':969,'multiline':False]
['text':' We are resuming a migration.','line_number':971,'multiline':False]
['text':' We must set a writeConcern on internal commands.','line_number':1025,'multiline':False]
['text':' If the tenantMigrationInfo is set on the opCtx, we will set the','line_number':1050,'multiline':False]
['text':' 'fromTenantMigration' field when writing oplog entries. That field is used to help recipient','line_number':1051,'multiline':False]
['text':' secondaries determine if a no-op entry is related to a transaction entry.','line_number':1052,'multiline':False]
['text':' The in-memory transaction state may have been updated past the on-disk transaction state. For','line_number':1079,'multiline':False]
['text':' instance, this might happen in an unprepared read-only transaction, which updates in-memory','line_number':1080,'multiline':False]
['text':' but not on-disk. To prevent potential errors, we use the on-disk state for the following','line_number':1081,'multiline':False]
['text':' transaction number checks.','line_number':1082,'multiline':False]
['text':' If the entry's transaction number is stale/older than the current active transaction number','line_number':1086,'multiline':False]
['text':' on the participant, fail the migration.','line_number':1087,'multiline':False]
['text':' If the txn numbers are equal, move on to the next entry.','line_number':1097,'multiline':False]
['text':' Write a fake applyOps with the tenantId as the namespace so that this will be picked','line_number':1111,'multiline':False]
['text':' up by the committed transaction prefetch pipeline in subsequent migrations.','line_number':1112,'multiline':False]
['text':' Use the same wallclock time as the noop entry.','line_number':1122,'multiline':False]
['text':' Write the no-op entry and update 'config.transactions'.','line_number':1131,'multiline':False]
['text':' Invalidate in-memory state so that the next time the session is checked out, it would reload','line_number':1140,'multiline':False]
['text':' the transaction state from 'config.transactions'.','line_number':1141,'multiline':False]
['text':' Simulate the sync source shutting down/restarting.','line_number':1148,'multiline':False]
['text':' Test-only.','line_number':1162,'multiline':False]
['text':' secondaryOk ','line_number':1208,'multiline':True]
['text':' useExhaust ','line_number':1208,'multiline':True]
['text':' Test-only.','line_number':1225,'multiline':False]
['text':' If the oplog buffer contains entries at this point, it indicates that the recipient went','line_number':1250,'multiline':False]
['text':' through failover before it finished writing all oplog entries to the buffer. Clear it and','line_number':1251,'multiline':False]
['text':' redo the work.','line_number':1252,'multiline':False]
['text':' Ensure we are primary when trying to clear the oplog buffer since it will drop and','line_number':1255,'multiline':False]
['text':' re-create the collection.','line_number':1256,'multiline':False]
['text':' Fetch the oplog chains of all retryable writes that occurred before startFetchingTimestamp.','line_number':1274,'multiline':False]
['text':' Use local read concern. This is because secondary oplog application coalesces multiple','line_number':1282,'multiline':False]
['text':' updates to the same config.transactions record into a single update of the most recent','line_number':1283,'multiline':False]
['text':' retryable write statement, and since after SERVER-47844, the committed snapshot of a','line_number':1284,'multiline':False]
['text':' secondary can be in the middle of batch, the combination of these two makes secondary','line_number':1285,'multiline':False]
['text':' majority reads on config.transactions not always reflect committed retryable writes at','line_number':1286,'multiline':False]
['text':' that majority commit point. So we need to do a local read to fetch the retryable writes','line_number':1287,'multiline':False]
['text':' so that we don't miss the config.transactions record and later do a majority read on the','line_number':1288,'multiline':False]
['text':' donor's last applied operationTime to make sure the fetched results are majority committed.','line_number':1289,'multiline':False]
['text':' We must set a writeConcern on internal commands.','line_number':1293,'multiline':False]
['text':' Allow aggregation to write to temporary files in case it reaches memory restriction.','line_number':1295,'multiline':False]
['text':' Failpoint to set a small batch size on the aggregation request.','line_number':1298,'multiline':False]
['text':' secondaryOk ','line_number':1307,'multiline':True]
['text':' useExhaust ','line_number':1307,'multiline':True]
['text':' cursor->more() will automatically request more from the server if necessary.','line_number':1310,'multiline':False]
['text':' Similar to the OplogFetcher, we keep track of each oplog entry to apply and the number of','line_number':1312,'multiline':False]
['text':' the bytes of the documents read off the network.','line_number':1313,'multiline':False]
['text':' Gather entries from current batch.','line_number':1319,'multiline':False]
['text':' Wait for enough space.','line_number':1326,'multiline':False]
['text':' Buffer retryable writes entries.','line_number':1328,'multiline':False]
['text':' In between batches, check for recipient failover.','line_number':1335,'multiline':False]
['text':' Do a majority read on the sync source to make sure the pre-fetch result exists on a','line_number':1344,'multiline':False]
['text':' majority of nodes in the set. The timestamp we wait on is the donor's last applied','line_number':1345,'multiline':False]
['text':' operationTime, which is guaranteed to be at batch boundary if the sync source is a','line_number':1346,'multiline':False]
['text':' secondary. We do not check the rollbackId - rollback would lead to the sync source','line_number':1347,'multiline':False]
['text':' closing connections so the migration would fail and retry.','line_number':1348,'multiline':False]
['text':' Update _stateDoc to indicate that we've finished the retryable writes oplog entry fetching','line_number':1367,'multiline':False]
['text':' stage.','line_number':1368,'multiline':False]
['text':' If the oplog buffer already contains fetched documents, we must be resuming a','line_number':1396,'multiline':False]
['text':' migration.','line_number':1397,'multiline':False]
['text':' The config is only used for setting the awaitData timeout; the defaults are fine.','line_number':1428,'multiline':False]
['text':' We do not need to check the rollback ID.','line_number':1432,'multiline':False]
['text':' forTenantMigration ','line_number':1436,'multiline':True]
['text':' Wait for enough space.','line_number':1481,'multiline':False]
['text':' Buffer docs for later application.','line_number':1484,'multiline':False]
['text':' We don't want to insert a resume token noop if it would be a duplicate.','line_number':1494,'multiline':False]
['text':' This term is not used for anything.','line_number':1506,'multiline':False]
['text':' Use an empty namespace string so this op is ignored by the applier.','line_number':1509,'multiline':False]
['text':' Use an empty wall clock time since we have no wall clock time, but we must give it one, and','line_number':1511,'multiline':False]
['text':' we want it to be clearly fake.','line_number':1512,'multiline':False]
['text':' The oplog fetcher is normally canceled when migration is done; any other error','line_number':1521,'multiline':False]
['text':' indicates failure.','line_number':1522,'multiline':False]
['text':' Oplog fetcher status of "OK" means the stopReplProducer failpoint is set.  Migration','line_number':1524,'multiline':False]
['text':' cannot continue in this state so force a failure.','line_number':1525,'multiline':False]
['text':'skipWaitingForForgetMigration=','line_number':1534,'multiline':True]
['text':'skipWaitingForForgetMigration=','line_number':1558,'multiline':True]
['text':' fp is locked. If we call pauseWhileSet here, another thread can't disable fp.','line_number':1574,'multiline':False]
['text':' Find the most recent no-op oplog entry from the current migration.','line_number':1650,'multiline':False]
['text':' If the state is data consistent, do not start the cloner.','line_number':1668,'multiline':False]
['text':' runOnExecutorEvent ensures the future is not ready unless an error has occurred.','line_number':1689,'multiline':False]
['text':' Signal the cloner to start.','line_number':1696,'multiline':False]
['text':' PrimaryOnlyService::onStepUp() before starting instance makes sure that the state doc','line_number':1704,'multiline':False]
['text':' is majority committed, so we can also skip waiting for it to be majority replicated.','line_number':1705,'multiline':False]
['text':' PrimaryOnlyService::onStepUp() before starting instance makes sure that the state doc','line_number':1726,'multiline':False]
['text':' is majority committed, so we can also skip waiting for it to be majority replicated.','line_number':1727,'multiline':False]
['text':' PrimaryOnlyService::onStepUp() before starting instance makes sure that the state doc','line_number':1739,'multiline':False]
['text':' is majority committed, so we can also skip waiting for it to be majority replicated.','line_number':1740,'multiline':False]
['text':' Persist the state that tenant migration instance has reached','line_number':1745,'multiline':False]
['text':' consistent state.','line_number':1746,'multiline':False]
['text':'
 * Acceptable classes for the 'Target' are AbstractAsyncComponent and RandomAccessOplogBuffer.
 ','line_number':1775,'multiline':True]
['text':' namespace','line_number':1823,'multiline':False]
['text':' Throws if we have failed to persist the state doc at the first place. This can only happen in','line_number':1828,'multiline':False]
['text':' unittests where we enable the autoRecipientForgetMigration failpoint. Otherwise,','line_number':1829,'multiline':False]
['text':' recipientForgetMigration will always wait for the state doc to be persisted first and thus','line_number':1830,'multiline':False]
['text':' this will only be called with _stateDocPersistedPromise resolved OK.','line_number':1831,'multiline':False]
['text':' Nothing to do if the state doc already has the expireAt set.','line_number':1840,'multiline':False]
['text':' indexesAffected ','line_number':1910,'multiline':True]
['text':' OpDebug* ','line_number':1911,'multiline':True]
['text':' We assume that we only fail with shutDown/stepDown errors (i.e. for','line_number':1925,'multiline':False]
['text':' failovers). Otherwise, the whole chain would stop running without marking the','line_number':1926,'multiline':False]
['text':' state doc garbage collectable while we are still the primary','line_number':1927,'multiline':False]
['text':' Prevents the tenant cloner from getting retried on retryable errors.','line_number':1937,'multiline':False]
['text':' interrupts running tenant cloner.','line_number':1943,'multiline':False]
['text':' interrupts running tenant oplog fetcher.','line_number':1948,'multiline':False]
['text':' Interrupts running oplog applier.','line_number':1952,'multiline':False]
['text':' We only get here on stepDown/shutDown.','line_number':1963,'multiline':False]
['text':' nothing to do.','line_number':1968,'multiline':False]
['text':' If the task is running, then setting promise result will be taken care by the main task','line_number':1974,'multiline':False]
['text':' continuation chain.','line_number':1975,'multiline':False]
['text':' The interrupt() is called before the instance is scheduled to run. If the state doc has','line_number':1982,'multiline':False]
['text':' already been marked garbage collectable, resolve the forget migration durable promise','line_number':1983,'multiline':False]
['text':' with OK.','line_number':1984,'multiline':False]
['text':'skipWaitingForForgetMigration=','line_number':1996,'multiline':True]
['text':'skipWaitingForForgetMigration=','line_number':2002,'multiline':True]
['text':' Wait until the state doc is initialized and persisted.','line_number':2013,'multiline':False]
['text':' Interrupt the chain to mark the state doc garbage collectable.','line_number':2024,'multiline':False]
['text':'skipWaitingForForgetMigration=','line_number':2028,'multiline':True]
['text':' Save them to join() with it outside of _mutex.','line_number':2054,'multiline':False]
['text':' Either the namespace belongs to the tenant, or it's an applyOps in the admin namespace','line_number':2070,'multiline':False]
['text':' and the first operation belongs to the tenant.  A transaction with mixed tenant/non-tenant','line_number':2071,'multiline':False]
['text':' operations should not be possible and will fail in the TenantOplogApplier.','line_number':2072,'multiline':False]
['text':'','line_number':2073,'multiline':False]
['text':' Commit of prepared transactions is not handled here; we'd need to handle them in the applier','line_number':2074,'multiline':False]
['text':' by allowing all commits through here and ignoring those not corresponding to active','line_number':2075,'multiline':False]
['text':' transactions.','line_number':2076,'multiline':False]
['text':' expireAt ','line_number':2107,'multiline':True]
['text':' Record the FCV at the start of a migration and check for changes in every','line_number':2118,'multiline':False]
['text':' subsequent attempt. Fail if there is any mismatch in FCV or','line_number':2119,'multiline':False]
['text':' upgrade/downgrade state. (Generic FCV reference): This FCV check should','line_number':2120,'multiline':False]
['text':' exist across LTS binary versions.','line_number':2121,'multiline':False]
['text':' Test-only.','line_number':2144,'multiline':False]
['text':' We avoid holding the mutex while scanning the local oplog which','line_number':2184,'multiline':False]
['text':' acquires the RSTL in IX mode. This is to allow us to be interruptable','line_number':2185,'multiline':False]
['text':' via a concurrent stepDown which acquires the RSTL in X mode.','line_number':2186,'multiline':False]
['text':' Throwing error when cloner is canceled externally via interrupt(),','line_number':2191,'multiline':False]
['text':' makes the instance to skip the remaining task (i.e., starting oplog','line_number':2192,'multiline':False]
['text':' applier) in the sync process. This step is necessary to prevent race','line_number':2193,'multiline':False]
['text':' between interrupt() and starting oplog applier for the failover','line_number':2194,'multiline':False]
['text':' scenarios where we don't start the cloner if the tenant data is','line_number':2195,'multiline':False]
['text':' already in consistent state.','line_number':2196,'multiline':False]
['text':' Do not set the internal states if the migration is already interrupted.','line_number':2239,'multiline':False]
['text':' Reuse the _writerPool for retry of the future chain.','line_number':2244,'multiline':False]
['text':' Start the oplog buffer outside the mutex to avoid deadlock on a concurrent stepdown.','line_number':2269,'multiline':False]
['text':' It is illegal to start the replicated donor buffer when the node is not primary.','line_number':2271,'multiline':False]
['text':' So ensure we are primary before trying to startup the oplog buffer.','line_number':2272,'multiline':False]
['text':' wait for oplog applier to complete/stop.','line_number':2294,'multiline':False]
['text':' The oplog applier does not exit normally; it must be shut down externally,','line_number':2295,'multiline':False]
['text':' e.g. by recipientForgetMigration.','line_number':2296,'multiline':False]
['text':' The oplog buffer collection can be safely dropped at this point. In case either collection','line_number':2335,'multiline':False]
['text':' does not exist, dropping will be a no-op. It isn't necessary that a given drop is','line_number':2336,'multiline':False]
['text':' majority-committed. A new primary will attempt to drop the collection anyway.','line_number':2337,'multiline':False]
['text':' We must abort the migration if we try to start or resume while upgrading or downgrading.','line_number':2360,'multiline':False]
['text':' We defer this until after the state doc is persisted in a started so as to make sure it','line_number':2361,'multiline':False]
['text':' it safe to abort and forget the migration. (Generic FCV reference): This FCV check should','line_number':2362,'multiline':False]
['text':' exist across LTS binary versions.','line_number':2363,'multiline':False]
['text':' Tenant migrations gets aborted on FCV upgrading or downgrading state. But,','line_number':2372,'multiline':False]
['text':' due to race between between Instance::getOrCreate() and','line_number':2373,'multiline':False]
['text':' SetFeatureCompatibilityVersionCommand::_cancelTenantMigrations(), we might miss aborting this','line_number':2374,'multiline':False]
['text':' tenant migration and FCV might have updated or downgraded at this point. So, need to ensure','line_number':2375,'multiline':False]
['text':' that the protocol is still compatible with FCV.','line_number':2376,'multiline':False]
['text':' Any FCV changes after this point will abort this migration.','line_number':2381,'multiline':False]
['text':'','line_number':2382,'multiline':False]
['text':' The 'AsyncTry' is run on the cleanup executor as opposed to the scoped executor as we rely','line_number':2383,'multiline':False]
['text':' on the 'PrimaryService' to interrupt the operation contexts based on thread pool and not the','line_number':2384,'multiline':False]
['text':' executor.','line_number':2385,'multiline':False]
['text':' Instance task can be started only once for the current term on a primary.','line_number':2392,'multiline':False]
['text':' If the task state is interrupted, then don't start the task.','line_number':2394,'multiline':False]
['text':' The task state will already have been set to 'kRunning' if we restarted','line_number':2399,'multiline':False]
['text':' the future chain on donor failover.','line_number':2400,'multiline':False]
['text':' There is a conflicting migration. If its state doc has already','line_number':2412,'multiline':False]
['text':' been marked as garbage collectable, this instance must correspond','line_number':2413,'multiline':False]
['text':' to a retry and we can delete immediately to allow the migration to','line_number':2414,'multiline':False]
['text':' restart. Otherwise, there is a real conflict so we should throw','line_number':2415,'multiline':False]
['text':' ConflictingInProgress.','line_number':2416,'multiline':False]
['text':' The doc has an expireAt but was deleted before we had time to','line_number':2437,'multiline':False]
['text':' delete it above therefore it's safe to pursue since it has been','line_number':2438,'multiline':False]
['text':' cleaned up.','line_number':2439,'multiline':False]
['text':' If our state is initialized and we haven't fulfilled the','line_number':2453,'multiline':False]
['text':' '_stateDocPersistedPromise' yet, it means we are restarting the future','line_number':2454,'multiline':False]
['text':' chain due to recipient failover.','line_number':2455,'multiline':False]
['text':' This is a retry of the future chain due to donor failure.','line_number':2466,'multiline':False]
['text':' Avoid fulfilling the promise twice on restart of the future chain.','line_number':2478,'multiline':False]
['text':' Must abort if flagged for cancellation above.','line_number':2491,'multiline':False]
['text':' Sets up internal state to begin migration.','line_number':2514,'multiline':False]
['text':' We shouldn't retry migration after receiving the recipientForgetMigration command','line_number':2524,'multiline':False]
['text':' or on stepDown/shutDown.','line_number':2525,'multiline':False]
['text':' Reset the task state and clear the interrupt status.','line_number':2540,'multiline':False]
['text':' Clean up the async components before retrying the future chain.','line_number':2545,'multiline':False]
['text':' Swap the oplog fetcher and applier to join() outside of '_mutex'.','line_number':2552,'multiline':False]
['text':' Perform join outside the lock to avoid deadlocks.','line_number':2558,'multiline':False]
['text':' On shutDown/stepDown, the _scopedExecutor may have already been shut down. So we','line_number':2571,'multiline':False]
['text':' need to schedule the clean up work on the parent executor.','line_number':2572,'multiline':False]
['text':' We don't need the final optime from the oplog applier. The data sync does not','line_number':2574,'multiline':False]
['text':' normally stop by itself on success. It completes only on errors or on external','line_number':2575,'multiline':False]
['text':' interruption (e.g. by shutDown/stepDown or by recipientForgetMigration command).','line_number':2576,'multiline':False]
['text':' If we were interrupted during oplog application, replace oplog application','line_number':2579,'multiline':False]
['text':' status with error state.','line_number':2580,'multiline':False]
['text':' Network and cancellation errors can be caused due to interrupt() (which shuts','line_number':2581,'multiline':False]
['text':' down the cloner/fetcher dbClientConnection & oplog applier), so replace those','line_number':2582,'multiline':False]
['text':' error status with interrupt status, if set.','line_number':2583,'multiline':False]
['text':' All of our async components don't exit with CallbackCanceled normally unless','line_number':2595,'multiline':False]
['text':' they are shut down by the instance itself via interrupt. If we get a','line_number':2596,'multiline':False]
['text':' CallbackCanceled error without an interrupt, it is coming from the service's','line_number':2597,'multiline':False]
['text':' cancellation token on failovers. It is possible for the token to get canceled','line_number':2598,'multiline':False]
['text':' before the instance is interrupted, so we replace the CallbackCanceled error','line_number':2599,'multiline':False]
['text':' with InterruptedDueToReplStateChange and treat it as a retryable error.','line_number':2600,'multiline':False]
['text':' Handle recipientForgetMigration.','line_number':2621,'multiline':False]
['text':' Schedule on the _scopedExecutor to make sure we are still the primary when','line_number':2638,'multiline':False]
['text':' waiting for the recipientForgetMigration command.','line_number':2639,'multiline':False]
['text':' Note marking the keys as garbage collectable is not atomic with marking the','line_number':2651,'multiline':False]
['text':' state document garbage collectable, so an interleaved failover can lead the','line_number':2652,'multiline':False]
['text':' keys to be deleted before the state document has an expiration date. This is','line_number':2653,'multiline':False]
['text':' acceptable because the decision to forget a migration is not reversible.','line_number':2654,'multiline':False]
['text':' Schedule on the parent executor to mark the completion of the whole chain so this','line_number':2675,'multiline':False]
['text':' is safe even on shutDown/stepDown.','line_number':2676,'multiline':False]
['text':' Replace the CallbackCanceled error with InterruptedDueToReplStateChange to','line_number':2691,'multiline':False]
['text':' support retry behavior. We can receive a CallbackCanceled error here during','line_number':2692,'multiline':False]
['text':' a failover if the ScopedTaskExecutor is shut down and rejects a task before','line_number':2693,'multiline':False]
['text':' the OperationContext is interrupted.','line_number':2694,'multiline':False]
['text':' namespace repl','line_number':2774,'multiline':False]
['text':' namespace mongo','line_number':2775,'multiline':False]
